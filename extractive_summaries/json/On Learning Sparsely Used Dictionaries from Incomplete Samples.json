{
  "sections": [{
    "heading": "1. Introduction",
    "text": ""
  }, {
    "heading": "1.1. Motivation",
    "text": "In this paper, we consider a variant of the problem of dictionary learning, a widely used unsupervised technique for learning compact (sparse) representations of high dimensional data. At its core, the challenge in dictionary learning is to discover a basis (or dictionary) that can sparsely represent a given set of data samples with as little empirical representation error as possible. The study of sparse coding enjoys a rich history in image processing, machine learning, and compressive sensing (Elad & Aharon, 2006; Aharon et al., 2006; Olshausen & Field, 1997; Candes & Tao, 2005; Rubinstein et al., 2010; Gregor & LeCun, 2010; Boureau et al., 2010). While the majority of these aforementioned works involved heuristics, several exciting re-\n1Iowa State University 2Yahoo! Research. Correspondence to: Thanh V. Nguyen <thanhng@iastate.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ncent results (Spielman et al., 2012; Agarwal et al., 2013; 2014; Arora et al., 2014; 2015; Sun et al., 2015; Chatterji & Bartlett, 2017; Nguyen et al., 2018) have established rigorous conditions under which their algorithms recover the true dictionary under suitable generative models for the data.\nAn important underlying assumption that guides the success of all existing dictionary learning algorithms is the availability of (sufficiently many) data samples that are fully observed. Our focus, on the other hand, is on the special case where the given data points are only partially observed, that is, we are given access to only a small fraction of the coordinates of the data samples.\nSuch a setting of incomplete observations is natural in many applications like image-inpainting and demosaicing (Rubinstein et al., 2010). For example, this routinely appears in hyper-spectral imaging (Xing et al., 2012) where entire spectral bands of signals could be missing or unobserved. Moreover, in other applications, collecting fully observed samples can be expensive (or in some cases, even infeasible). Examples include the highly unreliable continuous blood glucose (CBG) monitoring systems that suffer from signal dropouts, where often the task is to learn a dictionary from partially observed signals (Naumova & Schnass, 2017a).\nEarlier works that tackle the incomplete variant of the dictionary learning problem only offer heuristic solutions (Xing et al., 2012; Naumova & Schnass, 2017a) or involve constructing intractable statistical estimators (Soni et al., 2016). Indeed, the recovery of the true dictionary involves analyzing an extremely non-convex optimization problem that is, in general, not solvable in polynomial time (Loh & Wainwright, 2011). To our knowledge, our work is the first to give a theoretically sound as well as tractable algorithm to recover the exact dictionary from missing data (provided certain natural assumptions are met)."
  }, {
    "heading": "1.2. Our Contributions",
    "text": "In this paper, we make concrete theoretical algorithmic progress to the dictionary learning problem with incomplete samples. Inspired by recent algorithmic advances in dictionary learning (Arora et al., 2014; 2015), we adopt a learning-theoretic setup. Specifically, we assume that each data sample is synthesized from a generative model with an unknown dictionary and a random k-sparse coefficient\nvector (or sparse code). Mathematically, the data samples Y = [y(1), y(2), . . . , y(p)] 2 Rn⇥p are of the form\nY = A⇤X⇤ ,\nwhere A⇤ 2 Rn⇥m denotes the dictionary and X⇤ 2 Rm⇥p denotes the (column-wise) k-sparse codes.\nHowever, we do not have direct access to the data; instead, each high-dimensional data sample is further subsampled such that only a small fraction of the entries are observed. The assumption we make is that each entry of Y is observed independently with probability ⇢ 2 (0, 1]. For reasons that will become clear, we also assume that the ground truth dictionary A⇤ is both incoherent (i.e., the columns of A⇤ are sufficiently close to orthogonal) and democratic (i.e., the energy of each atom is well spread). Both these assumptions are standard in the compressive-sensing literature. We clarify the generative model more precisely in the sequel.\nGiven a set of such (partially observed) data samples, our goal is to recover the true dictionary A⇤. Towards this goal, we make the following contributions:\n1. Let us assume, for a moment, that we are given a coarse estimate A0 that is sufficiently close to the true dictionary. We devise a descent-style algorithm that leverages the given incomplete data to iteratively refine the dictionary estimate; moreover, we show that it converges rapidly to an estimate within a small ball of the ground truth A⇤ (whose radius decreases given more samples). Our result can be informally summarized as follows:\nTheorem 1 (Informal, descent). When given a “sufficientlyclose” initial estimate A 0 , there exists an iterative gradient\ndescent-type algorithm that linearly converges to the true dictionary with O(mk polylog(n)) incomplete samples.\nOur above result mirrors several recent results in non-convex learning that all develop a descent algorithm which succeeds given a good enough initialization (Yuan & Zhang, 2013; Cai et al., 2016; Tu et al., 2016). Indeed, similar guarantees for descent-style algorithms (such as alternating minimization) exist for the related problem of matrix completion (Jain et al., 2013), which coincides with our setting if m ⌧ n. However, our setting is distinct, since we are interested in learning overcomplete dictionaries, where m > n.\n2. Having established the efficiency of the above refinement procedure, we then address the challenge of actually coming up with a coarse estimate of A⇤. We do not know of a provable procedure that produces a good enough initial estimate using partial samples. To circumvent this issue, we assume availability of O(m) fully observed samples along with the partial samples1. Given this setting, we show\n1While this might be a limitation of our analysis, we emphasize\nthat we can provide a “sufficiently close” initial estimate in polynomial time. Our result can be summarized as follows:\nTheorem 2 (Informal, initialization). There exists an initialization algorithm that, given O(m polylog(n)) fully observed samples and an additional O(mk polylog(n)) partially observed samples, returns an initial estimate A 0 that is sufficiently close to A ⇤ in a column-wise sense."
  }, {
    "heading": "1.3. Techniques",
    "text": "The majority of our theoretical contributions are fairly technical, so for clarity, we provide some non-rigorous intuition.\nAt a high level, our approach merges ideas from two main themes in the algorithmic learning theory literature. We build upon recent seminal, theoretically-sound algorithms for sparse coding (specifically, the framework of Arora et al. (2015)). Their approach consists of a descent-based algorithm performed over the surface of a suitably defined loss function of the dictionary parameters. The descent is achieved by alternating between updating the dictionary estimate and updating the sparse codes of the data samples. The authors prove that this algorithm succeeds provided that the codes are sparse enough, the columns of A⇤ are incoherent, and that we are given sufficiently many samples.\nHowever, a direct application of the above framework to the partially observed setting does not seem to succeed. To resolve this, we leverage a specific property that is commonly assumed in the matrix completion literature: we suppose that the dictionaries are not “spiky” and that the energy of each atom is spread out among its coordinates; specifically, the sub-dictionaries formed by randomly sub-selecting rows are still incoherent. We call such dictionaries democratic, following the terminology of Davenport et al. (2009). (In matrix completion papers, this property is also sometimes referred to incoherence, but we avoid doing so since that overloads the term.) Our main contribution is to show that democratic, incoherent dictionaries can be learned via a similar alternating descent scheme if only a small fraction of the data entries are available. Our analysis is novel and distinct than that provided in (Arora et al., 2015).\nOf course, the above analysis is somewhat local in nature since we are using a descent-style method. In order to get global guarantees for recovery of A⇤, we need to initialize carefully. Here too, the spectral initialization strategies suggested in earlier dictionary learning papers (Arora et al., 2014; 2015) do not succeed. To resolve this, we again appeal to the democracy property of A⇤. We also need\nthat the number of full samples needed by our method is relatively small. Indeed, the state-of-the-art approach for dictionary learning (Arora et al., 2015) requires O(mk polylog(n)) fully observed samples, while our method needs only O(m polylog(n)) samples, which represents a polynomial improvement since k can be as large as p n.\nto assume that provided a small hold-out set of additional, fully observed samples is available2. Using this hold-out set (which can be construed as additional prior information or “side” information) together with the available samples gives us a spectral initialization strategy that provably gives a good enough initial estimate.\nPutting the above two pieces together: if we are provided O(mk/⇢4 polylog n) partially observed samples from the generative model, together with an additional O(m polylog n) full samples, then we can guarantee a fast, provable algorithm for learning A⇤. See Table 1 for a summary of our results, and comparison with existing work. We remark that while our algorithms only succeed up to sparsity level k  O(⇢ p n), we obtain a running time improvement over the best available dictionary learning approaches."
  }, {
    "heading": "1.4. Relation to Prior Work",
    "text": "The literature on dictionary learning (or sparse coding) is very vast and hence our references to prior work will necessarily be incomplete; we refer to the seminal work of Rubinstein et al. (2010) for a list of applications. Dictionary learning with incompletely observed data, however, is far less well-understood. Initial attempts in this direction (Xing et al., 2012) involve Bayesian-style techniques; more recent attempts have focused on alternating minimization techniques, along with incoherence- and democracy-type assumptions akin to our framework (Naumova & Schnass, 2017b;a). However, none of these methods provide rigorous polynomial-time algorithms that provably succeed in recovering the dictionary parameters.\nOur setup can also be viewed as an instance of matrix completion, which has been a source of intense interest in the machine learning community over the last decade (Candès & Recht, 2009; Keshavan et al., 2010). The typical assumption in such approaches is that the data matrix Y = A⇤X⇤ is low-rank (i.e., A⇤ typically spans a low-dimensional subspace). This assumption leads to either feasible convex relaxations, or a bilinear form that can be solved approximately via alternating minimization. However, our work differs significantly from this setup, since we are interested in the case where A⇤ is over-complete; moreover, our guarantees are not in terms of estimating the missing entries of Y , but rather obtaining the atoms in A⇤. Note that our generative model also differs from the setup of high-rank matrix completion (Eriksson et al., 2012), where the data is sampled randomly from a finite union-of-subspaces. In contrast, our data samples are synthesized via sparse linear combinations of a given dictionary.\n2We do not know how to remove this assumption, and it appears that techniques stronger than spectral initialization (e.g., involving higher-order moments) are required.\nIn the context of matrix-completion, perhaps the most related work to ours is the statistical analysis of matrixcompletion under the sparse-factor model of Soni et al. (2016), which employs a similar generative data model to ours. (Similar sparse-factor models have been studied in the work of Lan et al. (2014), but no complexity guarantees are provided.) For this model, Soni et al. (2016) propose a highly non-convex statistical estimator for estimate Y and provide error bounds for this estimator under various noise models. However, they do not discuss an efficient algorithm to realize that estimator. In contrast, we provide rigorous polynomial time algorithms, together with error bounds on the estimation quality of A⇤. Overall, we anticipate that our work can shed some light on the design of provable algorithms for matrix-completion in such more general settings."
  }, {
    "heading": "2. Preliminaries",
    "text": "Notation. Given a vector x 2 Rm and a subset S ✓ [m], we denote xS 2 Rm as a vector which equals x in indices belonging to S and equals zero elsewhere. We use A•i and A T\nj• respectively to denote the ith column and the jth row of matrix A 2 Rn⇥m. We use A•S as the submatrix of A with columns in S. In contrast, we use A • to indicate the submatrix of A with rows not in set to zero. Let supp(x) and sgn(x) be the support and element-wise sign of x. Let thresholdK(x) be the hard-thresholding operator that sets all entries of x with magnitude less than K to zero. The symbol k·k refers to the `2-norm, unless otherwise specified.\nFor asymptotic analysis, we use e⌦(·) and eO(·) to represent ⌦(·) and O(·) up to (unspecified) poly-logarithmic factors depending on n. Besides, g(n) = O⇤(f(n)) denotes g(n)  Kf(n) for some sufficiently small constant K. Finally, the terms “with high probability” (abbreviated to w.h.p.) is used to indicate an event with failure probability O(n !(1)). We make use of the following definitions.\nDefinition 1 (Incoherence). The matrix A is incoherent with parameter µ if the following holds for all columns i 6= j:\n|hA•i, A•ji| kA•ikkA•jk  µp n .\nThe incoherence property requires the columns of A to be approximately orthogonal, and is a canonical property to resolve identifiability issues in dictionary learning and sparse recovery. We distinguish this from the conventional notion of “incoherence” widely used in the matrix completion literature. This notion is related to a notion that we call democracy, which we define next.\nDefinition 2 (Democracy). Suppose that the matrix A is µ-incoherent. A is further said to be democratic if the submatrix A • is µ-incoherent for any subset ⇢ [n] of size p n  | |  n.\nThis property tells us that the rows of A have roughly the same amount of “information”, and that the submatrix of A restricted to any subset of rows is also incoherent. A similar concept (stated in terms of the restricted isometry property) is well-known in the compressive sensing literature (Davenport et al., 2009). Several probabilistic constructions of dictionaries satisfy this property; typical examples include random matrices drawn from i.i.d. Gaussian or Rademacher distributions. The p n lower bound on | | is to ensure that the submatrix of A including only the rows in is balanced in terms of dimensions.\nWe seek an algorithm that provides a provably “good” estimate of A⇤. For this, we need a suitable measure of “goodness”. The following notion of distance records the maximal column-wise difference between any estimate A and A⇤ in `2-norm under a suitable permutation and sign flip. Definition 3 (( ,)-nearness). The matrix A is said to be -close to A\n⇤ if k (i)A•⇡(i) A⇤•ik  holds for every i =\n1, 2, . . . ,m and some permutation ⇡ : [m]! [m] and sign flip : [m] : {±1}. In addition, if kA•⇡ A⇤k  kA⇤k holds, then A is said to be ( ,)-near to A⇤.\nTo keep notation simple, in our convergence theorems below, whenever we discuss nearness, we simply replace the transformations ⇡ and in the above definition with the identity mapping ⇡(i) = i and the positive sign (·) = +1 while keeping in mind that in reality, we are referring to finding one element in the equivalence class of all permutations and sign flips of A⇤.\nArmed with the above concepts, we now posit a generative model for our observed data. Suppose that the data samples Y = [y(1), y(2), . . . , y(p)] are such that each column is generated according to the rule:\ny = P (A⇤x⇤), (1)\nwhere A⇤ is an unknown, ground truth dictionary; x⇤ and are drawn from some distribution D and P is the sampling\noperator that keeps entries in untouched and zeroes out everything else. We emphasize that is independently chosen for each y(i), so more precisely, y(i) = y(i)\n(i) 2 Rn.\nWe ignore the superscript to keep the notation simple. We also make the following assumptions: Assumption 1. The true dictionary A⇤ is over-complete with m  Kn for some constant K > 1, and democratic with parameter µ. All columns of A ⇤ have unit norms.\nAssumption 2. The true dictionary A⇤ has bounded spectral and max (`1-) norms such that kA⇤k  O( p m/n) and kA⇤kmax  O(1/ p n). Assumption 3. The code vector x⇤ is k-sparse random with uniform support S. The nonzero entries of x ⇤ are pairwise\nindependent sub-Gaussian with variance 1, and bounded\nbelow by some known constant C.\nAssumption 4. Each entry of the sample A⇤x⇤ is independently observed with constant probability ⇢ 2 (0, 1].\nThe incoherence and spectral bound are ubiquitous in the dictionary learning literature (Arora et al., 2014; 2015). For the incomplete setting, we further require the democracy and max-norm bounds to control the spread of energy of the entries of A⇤, so that A⇤ is not “spiky”. Such conditions are often encountered in the matrix completion literature (Candès & Recht, 2009; Keshavan et al., 2010). The distributional assumptions on the code vectors x⇤ are standard in theoretical dictionary learning (Agarwal et al., 2014; Arora et al., 2014; Gribonval et al., 2015; Arora et al., 2015). Finally, we also require the sparsity k  O⇤(⇢ p n/ log n) throughout the paper."
  }, {
    "heading": "3. A Descent-Style Learning Algorithm",
    "text": "We now design and analyze an algorithm for learning the dictionary A⇤ given incomplete samples of the form (1). Our strategy will be to use a descent-like scheme to construct a sequence of estimates A which successively gets closer to\nA ⇤ in the sense of ( ,)-nearness.\nLet us first provide some intuition. The natural approach to solve this problem is to perform gradient descent over an appropriate empirical loss of the dictionary parameters. More precisely, we consider the squared loss between observed entries of Y and their estimates (which is the typical loss function used in the incomplete observations setting (Jain et al., 2013)):\nL(A) = 1 2\nX\ni,j2⌦ (Yij (AX)ij)2, (2)\nwhere ⌦ is the set of locations of observed entries in the samples Y . However, straightforward gradient descent over A is not possible for several reasons: (i) the gradient depends on the finite sample variability of Y ; (ii) the gradient with respect to A depends on the optimal code vectors of the data samples, x⇤\ni , which are unknown a priori; (iii) since\nwe are working in the overcomplete setting, care has to be taken to ensure that the code vectors (i.e., columns of X) obey the sparsity model (as specified in Assumption 2).\nThe neurally-plausible sparse coding algorithm of Arora et al. (2015) provides a crucial insight into the understanding of the loss surface of LA in the fully observed setting. Basically, within a small ball around the ground truth A⇤, the surface is well behaved such that a noisy version of X⇤ is sufficient to construct a good enough approximation to the gradient of L. Moreover, given an estimate within a small ball around A⇤, a noisy (but good enough) estimate of X⇤ can be quickly computed using a thresholding operation.\nWe extend this understanding to the (much more challenging) setting of incomplete observations. Specifically, we show the loss surface in (2) behaves well even with missing data. This enables us to devise an algorithm similar to that of Arora et al. (2015) and obtain a descent property directly related to (the population parameter) A⇤. The full procedure is detailed as Algorithm 1.\nWe now analyze our proposed algorithm. Specifically, we can show that if initialized properly and with proper choice of step size, Algorithm 1 exhibits linear convergence to a ball of radius O( p k/n) around A⇤. Formally, we have:\nTheorem 3. Suppose that the initial estimate A0 is ( , 2)- near to A ⇤ with = O⇤(1/ log n) and the sampling prob-\nability satisfies ⇢ 1/(k + 1). If Algorithm 1 is given p = e⌦(mk) fresh partial samples at each step and uses learning rate ⌘ = ⇥(m/⇢k), then\nE[kAs•i A⇤•ik 2]  (1 ⌧)skA0•i A⇤•ik\n2 +O( p k/n)\nfor some 0 < ⌧ < 1/2 and s = 1, 2, . . . , T . As a corollary, A s converges geometrically to A ⇤ until column-wise O( p k/n) error.\nAlgorithm 1 Gradient descent-style algorithm Input: Partial samples Y with observed entry set (i) Initial A0 that is ( , 2)-near to A⇤ for s = 0, 1, . . . , T do\n/* Encoding step */ for i = 1, 2, . . . , p do\nx (i) thresholdC/2( 1⇢ (A s)T y(i))\nend /* Update step */ bgs 1\np\nP p\ni=1(P (i)(Asx(i)) y(i))sgn(x(i))T\nA s+1 As ⌘bgs\nend Output: A AT as a learned dictionary\nWe defer the full proof of Theorem 3 to Appendix C. To understand the working of the algorithm and its correctness, let us consider the setting where we have access to infinitely many samples. This setting is, of course, fictional; however, expectations are easier to analyze than empirical averages, and moreover, this exercise reveals several key elements for proving Theorem 3. More precisely, we first provide bounds on the expected value of bgs, denoted as\ng s , Ey[(P (Asx) y)sgn(x)T ],\nto establish the descent property for the infinite sample case. The sample complexity argument emerges when we control the concentration of bgs, detailed in Appendix C. Here, we separately discuss the encoding and update steps in Algorithm 1.\nEncoding step. The first main result is to show that the hard-thresholding (or pooling)-based rule for estimating the sparse code vectors is sufficiently accurate. This rule adapts the encoding step of the dictionary learning algorithm proposed in (Arora et al., 2015), with an additional scaling factor 1/⇢. This scaling is necessary to avoid biases arising due to the presence of incomplete information.\nThe primary novelty is in our analysis. Specifically, we prove that the estimate of X obtained via the encoding step (even under partial observations) enables a good enough identification of the support of the true X⇤. The key, here, is to leverage the fact that A⇤ is democratic and that As is near A⇤. We call this property support consistency and establish it as follows.\nLemma 1. Suppose that As is ( , 2)-near to A⇤ with = O\n⇤(1/ log n). With high probability over y = P (A⇤x⇤), the estimate x obtained by the encoding step of Algorithm 1 has the same sign as the true x ⇤ ; that is,\nsgn thresholdC/2 1 ⇢ (As)T y = sgn(x⇤), (3)\nThis holds true for incoherence parameter µ  p n\n2k , sparsity\nparameter k ⌦(logm) and subsampling probability ⇢ 1/(k + 1).\nLemma 1 implies that when the “mass” of A⇤ is spread out across entries, within a small neighborhood of A⇤ the estimate x is reliable even if y is incompletely observed. This lemma is the main ingredient for bounding the behavior of the update rule.\nUpdate step. The support consistency property of the estimated x arising in the encoding step is key to rigorously analyzing the expected gradient gs. This relatively ‘simple’ encoding enables an explicit form of the update rule, and gives an intuitive reasoning on how the descent property can be achieved. In fact, we will see that\ng s i = ⇢piqi( s i A s •i A⇤•i) + o(⇢piqi)\nfor pi = E[|x⇤i ||i 2 S], qi = P[i 2 S] and si = hA•i, A⇤•ii. Since we assume that the current estimate As is (columnwise) sufficiently close to A⇤, each s\ni is approximately\nequal to 1, and hence gs i ⇡ ⇢piqi(As•i A⇤•i), i.e., the gradient points in the desired direction. Combining this with standard analysis of gradient descent, we can prove that the overall algorithm geometrically decreases the error in each step s as long as the learning rate ⌘ is properly chosen. Specifically, we get the following theoretical result. Theorem 4. Suppose that A0 is ( , 2)-near to A⇤ with = O\n⇤(1/ log n) and the sampling probability satisfies ⇢ 1/(k + 1). Assuming infinitely many partial samples at each step, Algorithm 1 geometrically converges to A ⇤ until\ncolumn-wise error O(k/⇢n). More precisely,\nkAs+1•i A ⇤ •ik 2  (1 ⌧)kAs•i A⇤•ik 2 +O k 2 /⇢ 2 n 2\nfor some 0 < ⌧ < 1/2 and for s = 1, 2, . . . , T provided the learning rate obeys ⌘ = ⇥(m/⇢k).\nWe provide the mathematical proof for the form of gs as well as the descent in Appendix A.2. We also argue that the ( , 2)-nearness of As+1 and A⇤ is maintained after each update. This is studied in Lemma 7 in Appendix A."
  }, {
    "heading": "4. An Initialization Algorithm",
    "text": "In the previous section, we provided an algorithm that (accurately) recovers A⇤ in an iterative descent-style approach. In order to establish correctness guarantees, the algorithm requires a coarse estimate A0 that is -close to the ground truth with closeness parameter = O⇤(1/ log n). This section presents an initialization strategy to obtain such a good starting point for A⇤.\nAgain, we begin with some intuition. At a high level, our algorithm mimics the spectral initialization strategy for dictionary learning proposed by (Arora et al., 2015). In essence,\nthe idea is to re-weight the data samples (which are fully observed) appropriately. When this is the case, analyzing the spectral properties of the covariance matrix of the new re-weighted samples gives us the desired initialization. The re-weighting itself relies upon the computation of pairwise correlations between the samples with two fixed samples (say, u and v) chosen from an independent hold-out set. This strategy is appealing in both from the standpoint of statistical efficiency as well as computational ease.\nUnfortunately, a straightforward application of this strategy to our setting of incomplete observations does not work. The major issue, of course, is that pairwise correlation (the inner product) of two high dimensional vectors is highly uninformative if each vector is only partially observed. We circumvent this issue via the following simple (but key) observation: provided the underlying dictionary is democratic and the representation is sufficiently sparse, the correlation between a partially observed data sample y with a fully observed sample u is indeed proportional to the actual correlation between y and u. Therefore, assuming that we are given a hold-out set that is fully observed, an adaptation of the spectral approach of Arora et al. (2015) provably succeeds. Moreover, the size of the hold-out set need not be large; in particular, we need only O(m polylog(n)) fully-observed samples, as opposed to the O(mk polylog(n)) samples required by the analysis of Arora et al. (2015). The parameter k can be as big as p n, so in fact we require polynomially fewer fully-observed samples.\nIn summary: in order to initialize our descent procedure, we assume the availability of a small (but fully observed) hold-out set. In practice, we can imagine expending some amount of effort in the beginning to collect all the entries of a small subset of the available data samples. The availability of such additional information (or “side-information”) has been made in the literature on matrix completion (Natarajan & Dhillon, 2014).\nThe full procedure is described in pseudocode form as Algorithm 2. Our main theoretical result (Theorem 5) summarizes its performance.\nTheorem 5. Suppose that the available training dataset consists of p1 fully observed samples, together with p2 in-\ncompletely observed samples according to the observation model (1). Suppose µ = O⇤ p n\nk log3 n\n, 1 ⇢ 1  k \nO ⇤( ⇢\np n\nlogn ). When p1 = e⌦(m) and p2 = e⌦(mk/⇢4), then with high probability, Algorithm 2 returns an initial estimate A 0 whose columns share the same support as A ⇤ and is ( , 2)-near to A⇤ with = O⇤(1/ log n).\nThe full proof is provided in Appendix B. To provide some intuition about the working of the algorithm and its proof, let us again consider the setting where we have access to infinitely many samples. These analyses result in key lemmas,\nAlgorithm 2 Spectral initialization algorithm Input: P1: p1 fully observed samples P2: p2 partially observed samples Set L = ; while |L| < m do\nPick u and v from P1 at random Construct the weighted covariance matrix cMu,v using samples y(i) from P2\ncMu,v 1\np2⇢ 4\np2X\ni=1\nhy(i), uihy(i), viy(i)(y(i))T\n1, 2 top singular values if 1 ⌦(k/m) and 2 < O⇤(k/m log n) then\nz top singular vector if z is not within distance 1/ log n of vectors in L even with sign flip then\nL L [ {z} end\nend end Output: A0 ProjB(Ã) where Ã is the matrix whose columns in L and B = {A : kAk  2kA⇤k}\nwhich we will reuse extensively for proving Theorem 5.\nFirst, consider two fully observed data samples u = A⇤↵ and v = A⇤↵0 drawn from the hold-out set. (Here, A⇤,↵,↵0 are unknown.) Consider also a partially observed sample y = A⇤ •x ⇤ under a random subset ✓ [n]. Define:\n= 1\n⇢ A\n⇤T •u, and\n0 = 1\n⇢ A\n⇤T •v\nrespectively as (crude) estimates of ↵ and ↵0, simply obtained by applying a (scaled) adjoint of A • to u and v respectively. It follows from the above definition that:\n= 1\n⇢ A\n⇤T •A ⇤ ↵, and hy, ui = ⇢h , x⇤i.\nOur main claim is that since A⇤ is assumed to satisfy the democracy property, 1\n⇢ A ⇤T •A ⇤ resembles the identity, and hence “looks” like the true code vector ↵. In particular, we have the following lemma.\nLemma 2. With high probability over the randomness in u and , we have: (a) | i ↵i|  µk lognpn + q 1 ⇢ ⇢n1/2 for each i = 1, 2, . . . ,m and (b) k k  p k logn ⇢ .\nProof. Denote U = supp(↵) and W = U\\{i}, then\n| i ↵i| = 1\n⇢ A\n⇤T ,iA ⇤ •W↵W + 1 ⇢ hA⇤ ,i, A⇤•ii 1 ↵i\n 1 ⇢ A⇤T ,iA⇤•W↵W + ( 1 ⇢ A ⇤T ,iA ⇤ •i 1)↵i .\n(4)\nWe will bound these terms on the right hand side of (4) using the properties of A⇤ and ↵. First, we notice that for any ⇢ [n]:\nkA⇤T ,iA⇤•W k 2 =\nX j2W hA⇤ ,i, A⇤•ji 2  µ 2 n X j2W kA⇤ ,ik 2kA⇤ ,jk 2 ,\nwhere we have used the democracy of A⇤ with respect to . Moreover, using the Chernoff bound for kA⇤ ,ik\n2 = P\nn i=1 A ⇤2 li 1[l 2 ], we have kA⇤ ,ik 2  ⇢ + o(⇢) w.h.p. Hence, kA⇤T ,iA⇤•W k\n2  ⇢2µ2k/n with high probability. In addition, k↵W k  p k log n w.h.p. because ↵W is ksparse sub-Gaussian. Therefore, the first term in (4) gives 1 ⇢ |A⇤T ,iA⇤•W↵W |  µk lognp n with high probability.\nFor the second term in (4), consider a random variable T = ( 1\n⇢ A ⇤T ,iA ⇤ •i 1)↵i over and ↵i. We first observe for\nany vector w 2 Rn that:\nE[(wT w)2] = nX\ni=1\nE[w4 i 1i2 ] +\nnX i 6=j E[w2 i w 2 j 1i,j2 ]\n= ⇢(1 ⇢) nX\ni=1\nw 4 i + ⇢2.\nHence, T has mean 0 and variance 2 T\n= (1 ⇢)/⇢ P n\nj=1 A 4 ji , which is bounded by O( 1 ⇢ ⇢n ) because kA⇤kmax  O(1/ p n). By Chebyshev’s inequality, we\nhave |T |  q\n1 ⇢ ⇢n1/2\nwith failure probability 1/ p n. Com-\nbining everything, we get\n| i ↵i|  µk log np\nn + s 1 ⇢ ⇢n1/2 ,\nw.h.p., which is the first part of the claim.\nFor the second part, we bound k k by expanding it as:\nk k = 1 ⇢ kA⇤T •A⇤•U↵Uk  1 ⇢ kA⇤ •kkA⇤•Ukk↵Uk,\nand again, if we use k↵Uk  p k log n w.h.p.and kA⇤k \nO(1), then k k  p k log n/⇢.\nWe briefly compare the above result with that of Arora et al. (2015). Our upper bounds are more general, and are stated in terms of the incompleteness factor ⇢. Indeed,\nand reconstruction error in sample size and sampling probability.\nour results match the previous bounds when ⇢ = 1. The above lemma suggests the following interesting regime of parameters. Specifically, for µ = O⇤ p n\nk log3 n\nand 1 ⇢ 1 \nk  O⇤( ⇢ p n\nlogn ), one can see that | i ↵i|  O ⇤(1/ log2 n)\nw.h.p., which implies that is a good estimate of ↵ even when a subset of rows in A⇤ is given.\nIn the next lemma, we show that that the pairwise correlation of u and any sample y is sufficiently informative for the same re-weighted spectral estimation strategy of Arora et al. (2015) to succeed in the incomplete setting.\nLemma 3. Suppose that u, v are a pair of fully observed samples and y is an incomplete sample independent of u, v.\nThe weighted covariance matrix Mu,v has the form:\nMu,v , 1\n⇢4 Ey[hy, uihy, viyyT ]\n= X\ni2U\\V qici i\n0 i A ⇤ •iA ⇤T •i +O ⇤(k/m log n),\nwhere ci = E[x⇤4i |i 2 S] and qi = P[i 2 S].\nThe complete proof is relegated to Appendix B. We will instead discuss some implications of this Lemma. Recall that ci is a constant with 0 < c < 1 and qi = ⇥(k/m).\nSuppose, for a moment, that the sparse representations of u and v share exactly one common dictionary element, say A\n⇤ •i (i.e., if U = supp(u) and V = supp(v) then U \\ V = {i}.) The first term, qici i 0iA⇤•iA⇤T•i , has norm |qici i 0i|. From Claim 2, | i| |↵i| | i ↵i| C o(1). Therefore, qici i 0 i A ⇤ •iA ⇤T •i has norm at least ⌦(k/m) whereas the perturbation terms are at most O⇤(k/m log n). According to Wedin’s theorem, we conclude that the top singular vector of Mu,v must be O⇤(k/m log n)/⌦(k/m) = O⇤(1/ log n) -close to A⇤•i. This gives us a coarse estimate of A⇤•i.\nThe question remains when and how whether we can a priori certify whether u, v share a unique dictionary atom among their sparse representations. Fortunately, the following Lemma provides a simple test for this via examining the decay of the singular vectors of the cross-covariance matrix Mu,v. The proof follows directly from that of Lemma 37 in (Arora et al., 2015).\nLemma 4. When the top singular value of Mu,v is at least ⌦(k/m) and the second largest one is at most O\n⇤(k/m log n), then u and v share a unique dictionary element with high probability.\nThe above discussion isolates one of the columns of A⇤. We can repeat this procedure several times by randomly choosing pairs of samples u and v from the hold-out set. Using the result of Arora et al. (2015), if |P1| is p1 = eO(m), then we can estimate all the m dictionary atoms. Overall, the sample complexity of Algorithm 2 is dominated by p2 = eO(mk/⇢4)."
  }, {
    "heading": "5. Experiments",
    "text": "We corroborate our theory by demonstrating some representative numerical benefits of our proposed algorithms. We generate a synthetic dataset based on the generative model described in Section 2. The ground truth dictionary A⇤ is of size 256⇥ 256 with independent standard Gaussian entries. We normalize columns of A⇤ to be unit norm. Then, we generate 6-sparse code vectors x⇤ with support drawn uniformly at random. Entries in the support are sampled from ±1 with equal probability. We generate all full samples, and isolate 5000 samples as “side information” for the initialization step. The remaining are then subsampled with different parameters ⇢.\nWe set the number of iterations to T = 3000 in the initialization procedure and the number of descent steps T = 50 for the descent scheme. Besides, we slightly modify the thresholding operator in the encoding step of Algorithm 1. We use another operator that keeps k largest entries of the input untouched and sets everything else to zero due to its stability. For each Monte Carlo trial, we uniformly draw p partial samples. The task, for our algorithm, is to learn A⇤. An implementation of our method is available online3.\nWe evaluate our algorithm on two metrics against p and ⇢: (i) recovery rate, i.e., the fraction of trials in which each algorithm successfully recovers the ground truth A⇤; and (ii) reconstruction error. All the metrics are averaged over 50 Monte Carlo simulations. “Successful recovery” is defined according to a threshold ⌧ = 6 on the Frobenius norm of the difference between the estimate bA and the ground truth A⇤. (Since we can only estimate bA modulo a permutation and sign flip, the optimal column and sign matching is computed using the Hungarian algorithm.)\nFigure 1 shows our experimental results. Here, sample size refers to the number of incomplete samples. Our algorithms are able to recover the dictionary for ⇢ = 0.6, 0.8, 1.0. For ⇢ = 0.4, we can observe a “phase transition” in sample complexity of successful recovery around p = 10, 000 samples.\n3https://github.com/thanh-isu\nAcknowledgements The authors thank the anonymous reviewers for many insightful comments and suggestions during the review process. This work was supported in part by the National Science Foundation under grants CCF-1566281 and CCF-1750920, and in part by a Faculty Fellowship from the Black and Veatch Foundation."
  }],
  "year": 2018,
  "references": [{
    "title": "Exact recovery of sparsely used overcomplete dictionaries",
    "authors": ["A. References Agarwal", "A. Anandkumar", "P. Netrapalli"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2013
  }, {
    "title": "Learning sparsely used overcomplete dictionaries",
    "authors": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2014
  }, {
    "title": "k-svd: An algorithm for designing overcomplete dictionaries for sparse representation",
    "authors": ["M. Aharon", "M. Elad", "A. Bruckstein"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2006
  }, {
    "title": "New algorithms for learning incoherent and overcomplete dictionaries",
    "authors": ["S. Arora", "R. Ge", "A. Moitra"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2014
  }, {
    "title": "Simple, efficient, and neural algorithms for sparse coding",
    "authors": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2015
  }, {
    "title": "Learning mid-level features for recognition",
    "authors": ["Boureau", "Y.-L", "F. Bach", "Y. LeCun", "J. Ponce"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2010
  }, {
    "title": "Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow",
    "authors": ["T.T. Cai", "X. Li", "Z Ma"],
    "venue": "The Annals of Statistics,",
    "year": 2016
  }, {
    "title": "Exact matrix completion via convex optimization",
    "authors": ["E.J. Candès", "B. Recht"],
    "venue": "Foundations of Computational mathematics,",
    "year": 2009
  }, {
    "title": "Decoding by linear programming",
    "authors": ["E.J. Candes", "T. Tao"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2005
  }, {
    "title": "Alternating minimization for dictionary learning with random initialization",
    "authors": ["N. Chatterji", "P. Bartlett"],
    "year": 2017
  }, {
    "title": "A simple proof that random matrices are democratic",
    "authors": ["M.A. Davenport", "J.N. Laska", "P.T. Boufounos", "R.G. Baraniuk"],
    "venue": "arXiv preprint arXiv:0911.0736,",
    "year": 2009
  }, {
    "title": "High-rank matrix completion",
    "authors": ["B. Eriksson", "L. Balzano", "R. Nowak"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2012
  }, {
    "title": "Learning fast approximations of sparse coding",
    "authors": ["K. Gregor", "Y. LeCun"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2010
  }, {
    "title": "Sample complexity of dictionary learning and other matrix factorizations",
    "authors": ["R. Gribonval", "R. Jenatton", "F. Bach", "M. Kleinsteuber", "M. Seibert"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Low-rank matrix completion using alternating minimization",
    "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"],
    "venue": "In ACM Symposium on Theory of Computing,",
    "year": 2013
  }, {
    "title": "Matrix completion from a few entries",
    "authors": ["R.H. Keshavan", "A. Montanari", "S. Oh"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2010
  }, {
    "title": "Sparse factor analysis for learning and content analytics",
    "authors": ["A.S. Lan", "A.E. Waters", "C. Studer", "R.G. Baraniuk"],
    "venue": "Journal of Machine Learning Research,",
    "year": 1959
  }, {
    "title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
    "authors": ["Loh", "P.-L", "M.J. Wainwright"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Inductive matrix completion for predicting gene–disease",
    "authors": ["N. Natarajan", "I.S. Dhillon"],
    "venue": "associations. Bioinformatics,",
    "year": 2014
  }, {
    "title": "Dictionary learning from incomplete data",
    "authors": ["V. Naumova", "K. Schnass"],
    "venue": "arXiv preprint arXiv:1701.03655,",
    "year": 2017
  }, {
    "title": "Dictionary learning from incomplete data for efficient image restoration",
    "authors": ["V. Naumova", "K. Schnass"],
    "venue": "In European Signal Processing Conference (EUSIPCO),",
    "year": 2017
  }, {
    "title": "A provable approach for double-sparse coding",
    "authors": ["T.V. Nguyen", "R.K.W. Wong", "C. Hegde"],
    "venue": "In Proc. Conf. American Assoc. Artificial Intelligence (AAAI),",
    "year": 2018
  }, {
    "title": "Sparse coding with an overcomplete basis set: A strategy employed by v1",
    "authors": ["B.A. Olshausen", "D.J. Field"],
    "venue": "Vision research,",
    "year": 1997
  }, {
    "title": "Dictionaries for sparse representation modeling",
    "authors": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"],
    "venue": "Proceedings of the IEEE,",
    "year": 2010
  }, {
    "title": "Noisy matrix completion under sparse factor models",
    "authors": ["A. Soni", "S. Jain", "J. Haupt", "S. Gonella"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2016
  }, {
    "title": "Exact recovery of sparsely-used dictionaries",
    "authors": ["D.A. Spielman", "H. Wang", "J. Wright"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2012
  }, {
    "title": "Complete dictionary recovery using nonconvex optimization",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Low-rank solutions of linear matrix equations via procrustes flow",
    "authors": ["S. Tu", "R. Boczar", "M. Simchowitz", "M. Soltanolkotabi", "B. Recht"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Dictionary learning for noisy and incomplete hyperspectral images",
    "authors": ["Z. Xing", "M. Zhou", "A. Castrodad", "G. Sapiro", "L. Carin"],
    "venue": "SIAM Journal on Imaging Sciences,",
    "year": 2012
  }, {
    "title": "Truncated power method for sparse eigenvalue problems",
    "authors": ["Yuan", "X.-T", "T. Zhang"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }],
  "id": "SP:4c91bc2b79950f59efeda7e4897ca0f2c4bff530",
  "authors": [{
    "name": "Thanh V. Nguyen",
    "affiliations": []
  }, {
    "name": "Akshay Soni",
    "affiliations": []
  }, {
    "name": "Chinmay Hegde",
    "affiliations": []
  }],
  "abstractText": "Existing algorithms for dictionary learning assume that the entries of the (high-dimensional) input data are fully observed. However, in several practical applications, only an incomplete fraction of the data entries may be available. For incomplete settings, no provably correct and polynomialtime algorithm has been reported in the dictionary learning literature. In this paper, we provide provable approaches for learning – from incomplete samples – a family of dictionaries whose atoms have sufficiently “spread-out” mass. First, we propose a descent-style iterative algorithm that linearly converges to the true dictionary when provided a sufficiently coarse initial estimate. Second, we propose an initialization algorithm that utilizes a small number of extra fully observed samples to produce such a coarse initial estimate. Finally, we theoretically analyze their performance and provide asymptotic statistical and computational guarantees.",
  "title": "On Learning Sparsely Used Dictionaries from Incomplete Samples"
}