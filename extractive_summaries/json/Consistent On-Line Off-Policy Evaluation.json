{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013). We consider the problem of off-policy evaluation (OPE) – assessing the performance of a complex strategy without applying it. An OPE formulation is often considered in domains with limited sampling capability. For example, marketing and recommender systems (Theocharous and Hallak, 2013; Theocharous et al., 2015) directly relate policies to revenue. A more extreme example is drug administration, as there are only few patients in\n1The Technion, Haifa, Israel. Correspondence to: Assaf Hallak <ifogph@gmail.com>, Shie Mannor <shie@ee.technion.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe testing population, and sub-optimal policies can have life threatening effects (Hochberg et al., 2016). OPE can also be useful as a module for policy optimization in a policy improvement scheme (Thomas et al., 2015a).\nIn this paper, we consider the OPE problem in an on-line setup where each new sample is immediately used to update our current value estimate of some previously unseen policy. We propose and analyze a new algorithm called COP-TD(λ,β) for estimating the value of the target policy; COP-TD(λ,β) has the following properties:\n1. Easy to understand and implement on-line.\n2. Allows closing the gap to consistency such that the limit point is the same that would have been obtained by on-policy learning with the target policy.\n3. Empirically comparable to state-of-the art algorithms.\nOur algorithm resembles (Sutton et al., 2015)’s Emphatic TD that was extended by (Hallak et al., 2015) to the general parametric form ETD(λ,β). We clarify the connection between the algorithms and compare them empirically. Finally, we introduce an additional related heuristic called Log-COP-TD(λ,β) and motivate it."
  }, {
    "heading": "2. Notations and Background",
    "text": "We consider the standard discounted Markov Decision Process (MDP) formulation (Bertsekas and Tsitsiklis, 1996) with a single long trajectory. Let M = (S,A,P,R, ζ, γ) be an MDP where S is the finite state space and A is the finite action space. The parameter P sets the transition probabilities Pr(s′|s, a) given the previous state s ∈ S and action a ∈ A, where the first state is determined by the distribution ζ. The parameter R sets the reward distribution r(s, a) obtained by taking action a in state s and γ is the discount factor specifying the exponential reduction in reward with time. The process advances as follows:\nA state s0 is sampled according to the distribution ζ(s). Then, at each time step t starting from t = 0 the agent draws an action at according to the stochastic behavior policy µ(a|st), a reward rt . = r(st, at) is accumulated by the agent, and the next state st+1 is sampled using the transition probability Pr(s′|st, at).\nThe expected discounted accumulated reward starting from a specific state and choosing an action by some policy π is called the value function, which is also known to satisfy the Bellman equation in a vector form:\nV π(s) = Eπ [ ∞∑ t=0 γtrt ∣∣∣ s0 = s] , TπV .= Rπ + γPπV, where [Rπ]s . = Eπ [r(s, π(s))] and [Pπ]s,s′ . = Eπ [Pr(s′|s, π(s))] are the policy induced reward vector and transition probability matrix respectively; Tπ is called the Bellman operator. The problem of estimating V π(s) from samples is called policy evaluation. If the target policy π is different than the behavior policy µ which generated the samples, the problem is called off-policy evaluation (OPE). The TD(λ) (Sutton, 1988) algorithm is a standard solution to on-line on-policy evaluation: Each time step the temporal difference error updates the current value function estimate, such that eventually the stochastic approximation process will converge to the true value function. The standard form of TD(λ) is given by:\nR (n) t,st = n−1∑ i=0 γirt+i + γ nV̂t(st+n),\nRλt,st =(1− λ) ∞∑ n=0 λnR(n+1)st ,\nV̂t+1(st) =V̂t(st) + αt ( Rλt,st − V̂t(st) ) ,\n(1)\nwhere αt is the step size. The value R (n) t,st is an estimate of the current state’s V (st), looking forward n steps, and Rλt,st is an exponentially weighted average of all of these estimates going forward till infinity. Notice that Equation 1 does not specify an on-line implementation since R(n)t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas and Tsitsiklis (1996) for on-line TD(λ), and Sutton et al. (2014), Sutton et al. (2015) for off-policy TD(λ)). The underlying operator of TD(λ) is given by:\nTλπ V = (1− λ) ∞∑ n=0 λn ( n∑ i=0 γiP iπRπ + γ n+1Pn+1π V ) = (1− λ)(I − λTπ)−1TπV,\nand is a γ(1−λ)1−λγ -contraction (Bertsekas, 2012).\nWe denote by dµ(s) the stationary distribution over states induced by taking the policy µ and mark Dµ = diag(dµ). Since we are concerned with the behavior at infinite horizon, we assume ζ(s) = dµ(s). In addition, we assume that the MDP is ergodic for the two specified policies µ, π so ∀s ∈ S : dµ(s) > 0, dπ(s) > 0 and that the OPE problem is proper – π(a|s) > 0⇒ µ(a|s) > 0.\nWhen the state space is too large to hold V π(s), a linear function approximation scheme is used: V π(s) ≈ θ>π φ(s), where θ is the optimized weight vector and φ(s) is the feature vector of state s composed of k features. We denote by Πdπ the projection to the subspace spanned by the features with respect to the dπ-weighted norm, and by Φ ∈ RS,k the matrix whose lines consist of the feature vectors for each state and assume its columns are linearly independent.\nTD(λ) can be adjusted to find the fixed point of ΠdπT λ π (Sutton and Barto, 1998):\nR (n) t,st = n−1∑ i=0 γirt+i + γ nθ>t φ(st+n),\nRλt,st =(1− λ) ∞∑ n=0 λnR(n+1)st ,\nθt+1 =θt + αt ( Rλt,st − θ > t φ(st) ) φ(st).\nFinally, we define OPE-related quantities:\nρt . = π(at|st) µ(at|st) , Γnt . = n−1∏ i=0 ρt−1−i, ρd(s) . = dπ(s) dµ(s) ,\nwe call ρd the covariate shift ratio (as denoted under different settings by (Hachiya et al., 2012)).\nWe summarize the assumptions used in the proofs:\n1. For both policies the induced Markov chain is ergodic.\n2. The first state s0 is distributed according to the stationary distribution of the behavior policy dµ(s).\n3. The problem is proper: π(a|s) > 0⇒ µ(a|s) > 0.\n4. The feature matrix Φ has full rank k.\nAssumption 1 is commonly used for convergence theorems as it verifies the value function is well defined on all states regardless of the initial sampled state. Assumption 2 can be relaxed since we are concerned with the long-term properties of the algorithm past its mixing time – we require it for clarity of the proofs. Assumption 3 is required so the importance sampling ratios will be well defined. Assumption 4 guarantees the optimal θ is unique which greatly simplifies the proofs."
  }, {
    "heading": "3. Previous Work",
    "text": "We can roughly categorize previous OPE algorithms to two main families. Gradient based methods that perform stochastic gradient descent on error terms they want to minimize. These include GTD (Sutton et al., 2009a), GTD-2,\nTDC (Sutton et al., 2009b) and HTD (White and White, 2016). The main disadvantages of gradient based methods are (A) they usually update an additional error correcting term, which means another time-step parameter needs to be controlled; and (B) they rely on estimating non-trivial terms, an estimate that tends to converge slowly. The other family uses importance sampling (IS) methods that correct the gains between on-policy and off-policy updates using the IS-ratios ρt’s. Among these are full IS (Precup et al., 2001) and ETD(λ,β) (Sutton et al., 2015). These methods are characterized by the bias-variance trade-off they resort to – navigating between biased convergent values (or even divergent), and very slow convergence stemming from the high variance of IS correcting factors (the ρt products). There are also a few algorithms that fall between the two, for example TO-GTD (van Hasselt et al., 2014) and WISTD(λ) (Mahmood and Sutton, 2015).\nA comparison of these algorithms in terms of convergence rate, synergy with function approximation and more is available in (White and White, 2016; Geist and Scherrer, 2014). We focus in this paper on the limit point of the convergence. For most of the aforementioned algorithms, the process was shown to converge almost surely to the fixed point of the projected Bellman operator ΠdTπ where d is some stationary distribution (usually dµ), however the d in question was never1 dπ as we would have obtained from running on-policy TD with the target policy (also see (Kolter, 2011) for relevant discussion). The algorithm achieving the closest result is ETD(λ,β) which replaced d with f = ( I − βP>π )−1 dµ, where β trades-off some of the process’ variance with the bias in the limit point. Hence, our main contribution is a consistent algorithm which can converge to the same value that would have been obtained by running an on-policy scheme with the same policy."
  }, {
    "heading": "4. Motivation",
    "text": "Here we provide a motivating example showing that even in simple cases with “close” behavior and target policies, the two induced stationary distributions can differ greatly. Choosing a specific linear parameterization further emphasizes the difference between applying on-policy TD with the target policy, and applying inconsistent off-policy TD.\nAssume a chain MDP with numbered states 1, 2, ..|S|, where from each state s you can either move left to state s − 1, or right to state s + 1. If you’ve reached the beginning or the end of the chain (states 1 or |S|) then taking a step further does not affect your location. Assume the behavior policy moves left with probability 0.5 + , while the target policy moves right with probability 0.5+ . It is easy\n1Except full IS, however its variance is too high to be applicable in practice.\nto see that the stationary distributions are given by: dµ(s) ∝ (\n0.5− 0.5 +\n)s , dπ(s) ∝ ( 0.5 +\n0.5−\n)s .\nFor instance, if we have a length 100 chain with = 0.01, for the rightmost state we have dµ(|S|) ≈ 8 · 10−4, dπ(|S|) ≈ 0.04. Let’s set the reward to be 1 for the right half of the chain, so the target policy is better since it spends more time in the right half. The value of the target policy in the edges of the chain for γ = 0.99 is V π(1) = 0.21, V π(100) = 99.97.\nNow what happens if we try to approximate the value function using one constant feature φ(s) ≡ 1? The fixed point of ΠdµTπ is θ = 11.92, while the fixed point of ΠdπTπ is θ = 88.08 – a substantial difference. The reason for this difference lies in the emphasis each projection puts on the states: according to Πdµ , the important states are in the left half of the chain – these with low value function, and therefore the value estimation of all states is low. However, according to Πdπ the important states are concentrated on the right part of the chain since the target policy will visit these more often. Hence, the estimation error is emphasized on the right part of the chain and the value estimation is higher. When we wish to estimate the value of the target policy, we want to know what will happen if we deploy it instead of the behavior policy, thus taking the fixed point of ΠdπTπ better represents the off-policy evaluation solution.\n5. COP-TD(λ, β) Most off-policy algorithms multiply the TD summand of TD(λ) with some value that depends on the history and the current state. For example, full IS-TD by (Precup et al., 2001) examines the ratio between the probabilities of the trajectory under both policies:\nPπ(s0, a0, s1, . . . , st, at) Pµ(s0, a0, s1, . . . , st, at) = t∏ m=0 ρm = Γ t tρt. (2)\nIn problems with a long horizon, or these that start from the stationary distribution, we suggest using the time-invariant covariate shift ρd multiplied by the current ρt. The intuition is the following: We would prefer using the probabilities ratio given in Equation 2, but it has very high variance, and after many time steps we might as well look at the stationary distribution ratio instead. This direction leads us to the following update equations:\nθt+1 = θt + αtρd(st)ρt ( rt + θ > t (γφ(st+1)− φ(st)) ) φ(st).\n(3) Lemma 1. If the αt satisfy ∑∞ t=0 αt =∞, ∑∞ t=0 α 2 t <∞ then the process described by Eq. (3) converges almost surely to the fixed point of ΠπTπV = V .\nThe proof follows the ODE method (Kushner and Yin, 2003) similarly to Tsitsiklis and Van Roy (1997) (see the appendix for more details).\nSince ρd(s) is generally unknown, it is estimated using an additional stochastic approximation process. In order to do so, we note the following Lemma:\nLemma 2. Let ρ̂d be an unbiased estimate of ρd, and for every n = 0, 1, . . . , t define Γ̃nt . = ρ̂d(st−n)Γ n t . Then:\nEµ [ Γ̃nt |st ] = ρd(st).\nFor any state st there are t→∞ such quantities {Γ̃nt }tn=0, where we propose to weight them similarly to TD(λ):\nΓ̃βt = (1− β) ∞∑ n=0 βnΓ̃n+1t .\nNote that ρd(s), unlike V (s), is restricted to a close set since its dµ-weighted linear combination is equal to 1 and all of its entries are non-negative; We denote this dµweighted simplex by ∆dµ , and let Π∆dµ be the (non-linear) projection to this set with respect to the Euclidean norm (Π∆dµ can be calculated efficiently, (Chen and Ye, 2011)). Now, we can devise a TD algorithm which estimates ρd and uses it to find θ, which we call COP-TD(0, β) (Consistent Off-Policy TD).\nAlgorithm 1 COP-TD(0,β), Input: θ0, ρ̂d,0,\n1: Init: F0 = 0, n β 0 = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: N(st) = N(st) + 1, ∀s ∈ S : d̂µ(s) = N(s)t 6: nβt = βn β t + 1 7: Update Γnt ’s weighted average: 8: Ft = ρt−1(βFt−1 + est−1) 9: Update & project by ρd’s TD error:\n10: δdt = F>t ρ̂d,t\nnβt︸ ︷︷ ︸ →Γ̃βt\n−ρ̂d,t(st)\n11: ρ̂d,t+1 = Π∆d̂µ ( ρ̂d,t + α d t δ d t est ) 12: Off-policy TD(0): 13: δt = rt + θ>t (γφ(st+1)− φ(st)) 14: θt+1 = θt + αtρ̂d,t+1(st)ρtδtφ(st) 15: end for\nSimilarly to the Bellman operator for TD-learning, we define the underlying COP-operator Y and its β extension:\nY u = D−1µ P > π Dµu,\nY βu = (1− β)D−1µ P>π (I − βP>π )−1Dµu.\nThe following Lemma may give some intuition on the convergence of the ρd estimation process: Lemma 3. Under the ergodicity assumption, denote the eigenvalues of Pπ by 0 ≤ · · · ≤ |ξ2| < ξ1 = 1. Then Y β is a maxi 6=1\n(1−β)|ξi| |1−βξi| < 1-contraction in the L2-norm on the\northogonal subspace to ρd, and ρd is a fixed point of Y β .\nThe technical proof is given in the appendix. Theorem 1. If the step sizes satisfy ∑ t αt = ∑ t α d t =\n∞, ∑ t(α 2 t + (α d t )\n2) < ∞, αt αdt → 0, tαdt → 0, and E [ (βnΓnt ) 2|st ] ≤ C for some constant C and every t and n, then after applying COP-TD(0, β), ρ̂d,t converges to ρd almost surely, and θt converges to the fixed point of ΠπTπV .\nNotice that COP-TD(0, β) given in Alg. 1 is infeasible in problems with large state spaces since ρd ∈ R|S|. Like TD(λ), we can introduce linear function approximation: represent ρd(s) ≈ θ>ρ φρ(s) where θρ is a weight vector and φρ(s) is the off-policy feature vector and adjust the algorithm accordingly. For ρ̂d to still be contained in the set ∆dµ , we pose the requirement on the feature vectors: φρ(s) ∈ Rk+, and ∑ s dµ(s)θ > ρ φρ(s) = 1 ( noted as\nthe simplex projection Π∆Eµ[φρ(s)] ) . In practice, the latter\nrequirement can be approximated: ∑ s dµ(s)θ > ρ φρ(s) ≈ 1 t θ > ρ ∑ t φρ(st) = 1 resulting in an extension of the previously applied dµ estimation (step 5 in COP-TD(0, β)). We provide the full details in Algorithm 2, which also incorporates non-zero λ ( similarly to ETD(λ,β) ) .\nAlgorithm 2 COP-TD(λ,β) with Function Approximation, Input: θ0, θρ,0\n1: Init: F0 = 0, n β 0 = 1, Nφ = 0, e0 = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: nβt = βn β t + 1, Nφ = Nφ + φρ(st), d̂φρ = Nφ t 6: Update Γnt ’s weighted average: 7: Ft = ρt−1(βFt−1 + φρ(st−1)) 8: Update & project by ρd’s TD error: 9: δdt = θ > ρ,t−1 ( Ft nβt − φρ(st)\n) 10: θρ,t+1 = Π∆d̂φρ ( θρ,t + α d t δ d t φρ(st)\n) 11: Off-policy TD(λ): 12: Mt = λ+ (1− λ)θ>ρ,t+1φρ(st) 13: et = ρt (λγet +Mtφ(st+1)) 14: δt = rt + θ>t (γφ(st+1)− φ(st)) 15: θt+1 = θt + αtδtet 16: end for\nTheorem 2. If the step sizes satisfy ∑ t αt = ∑ t α d t =\n∞, ∑ t(α 2 t + (α d t )\n2) < ∞, αt αdt → 0, tαdt → 0, and E [ (βnΓnt ) 2|st ] ≤ C for some constant C and every t, n,\nthen after applying COP-TD(0, β) with function approximation satisfying φρ(s) ∈ Rk+, ρ̂d,t converges to the fixed point of Π∆Eµ[φρ]ΠφρY\nβ denoted by ρCOPd almost surely, and if θt converges it is to the fixed point of Πdµ◦ρCOPd TπV , where ◦ is a coordinate-wise product of vectors.\nThe proof is given in the appendix and also follows the ODE method. Notice that a theorem is only given for λ = 0, convergence results for general λ should follow the work by Yu (2015).\nA possible criticism on COP-TD(0,β) is that it is not actually consistent, since in order to be consistent the original state space has to be small, in which case every off-policy algorithm is consistent as well. Still, the dependence on another set of features allows to trade-off accuracy with computational power in estimating ρd and subsequently V . Moreover, smart feature selection may further reduce this gap, and COP-TD(0, β) is still the first algorithm addressing this issue. We conclude with linking the error in ρd’s estimate with the difference in the resulting θ, which suggests that a well estimated ρd results in consistency: Corollary 1. Let 0 < < 1. If (1 − )ρd ≤ ρCOPd ≤ (1 + )ρd, then the fixed point of COP-TD(0,β) with function approximation θCOP satisfies the following, where ‖ · ‖∞ is the L∞ induced norm:\n‖θ∗ − θCOP‖∞ ≤ ‖A−1π Φ>‖∞ ( Rmax + (1 + γ)‖Φ‖∞‖θCOP‖∞ ) ,\nwhereAπ = Φ>Dπ(I−γPπ)Φ, and θ∗ sets the fixed point of the operator ΠdπTπV ."
  }, {
    "heading": "5.1. Relation to ETD(λ, β)",
    "text": "Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD. Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(λ, β), which was shown to perform extremely well empirically by White and White (2016). ETD(0, β) can be represented as:\nFt = (1− β) ∞∑ n=0 βnΓnt ,\nθt+1 = θt + αtFtρt ( rt + θ > t (γφ(st+1)− φ(st)) ) . (4)\nAs mentioned before, ETD(λ, β) converges to the fixed point of ΠfTλπ (Yu, 2015), where f = E [Ft|st] = (I − βPπ)\n−1dµ. Error bounds can be achieved by showing that the operator ΠfTλπ is a contraction under certain requirements on β and that the variance of Ft is directly related to β as well (Hallak et al., 2015) (and thus affects the convergence rate of the process).\nWhen comparing ETD(λ,β)’s form to COP-TD(λ,β)’s, instead of spending memory and time resources on a\nstate/feature-dependent Ft, ETD(λ,β) uses a one-variable approximation. The resulting Ft is in fact a one-step estimate of ρd, starting from ρ̂d(s) ≡ 1 (see Equations 9, 4), up to a minor difference: F ETDt = βF COP-TD t + 1 (which following our logic adds bias to the estimate 2).\nUnlike ETD(λ, β), COP-TD(λ,β)’s effectiveness depends on the available resources. The number of features φρ(s) can be adjusted accordingly to provide the most affordable approximation. The added cost is fine-tuning another stepsize, though β’s effect is less prominent."
  }, {
    "heading": "6. The Logarithm Approach for Handling Long Products",
    "text": "We now present a heuristic algorithm which works similarly to COP-TD(λ, β). Before presenting the algorithm, we explain the motivation behind it."
  }, {
    "heading": "6.1. Statistical Interpretation of TD(λ)",
    "text": "Konidaris et al. (2011) suggested a statistical interpretation of TD(λ). They show that under several assumptions the TD(λ) estimate Rλst is the maximum likelihood estimator of V (st) given Rnst : (1) Each R n st is an unbiased estimator of V (st); (2) The random variables Rnst are independent and specifically uncorrelated; (3) The random variables Rnst are jointly normally distributed; and (4) The variance of each Rnst is proportional to λ n.\nUnder Assumptions 1-3 the maximum likelihood estimator of V (s) given its previous estimate can be represented as a linear convex combination of Rnst with weights:\nwn =\n[ Var ( R (n) st )]−1 ∑∞ m=0 [ Var ( R (m) st\n)]−1 . Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed. In a follow-up paper by Thomas et al. (2015b), the second assumption was also removed and the weights were instead given as: wn = 1>cov(Rst )en 1>cov(Rst )1\n, where the covariance matrix can be estimated from the data, or otherwise learned through some parametric form.\nWhile both the approximated variance and learned covariance matrix solutions improve performance on several benchmarks, the first uses a rather crude approximation, and the second solution is both state-dependent and based on noisy estimates of the covariance matrix. In addition, there aren’t efficient on-line implementations since all past\n2We have conducted several experiments with an altered ETD and indeed obtained better results compared with the original, these experiments are outside the scope of the paper.\nweights should be recalculated to match a new sample. Still, the suggested statistical justification is a valuable tool in assessing the similar role of β in ETD(λ, β)."
  }, {
    "heading": "6.2. Variance Weighted Γnt",
    "text": "As was shown by Konidaris et al. (2011), we can use statedependent weights instead of β exponents to obtain better estimates. The second moments are given explicitly as\nfollows3: E [ (Γnt ) 2 |st ] = d>µ P̃ n−1est dµ(st) , where [ P̃ ] s,s′\n=∑ a∈A π2(a|s) µ(a|s) P (s ′|s, a).\nThese can be estimated for each state separately. Notice that the variances increase exponentially depending on the largest eigenvalue of P̃ (as Assumption 4 dictates), but this is merely an asymptotic behavior and may be relevant only when the weights are already negligible. Hence, implementing this solution on-line should not be a problem with the varying weights, as generally only the first few of these are non-zero. While this solution is impractical in problems with large state spaces parameterizing or approximating these variances (similarly to Thomas et al. (2015b)) could improve performance in specific applications."
  }, {
    "heading": "6.3. Log-COP-TD(λ, β)",
    "text": "Assumption 3 in the previous section is that the sampled estimators (R(n),Γnt ) are normally distributed. For on policy TD(λ), this assumption might seem not too harsh as the estimators R(n) represent growing sums of random variables. However, in our case the estimators Γnt are growing products of random variables. To correct this issue we can define new estimators using a logarithm on each Γ̃nt :\nlog [ρd(st)] = log\n[ E [ ρ̂d(st−m)\nt−1∏ k=t−m\nρk ∣∣ st]]\n≈ log [ρ̂d(st−m)] + t−1∑\nk=t−m\nE [log [ρk] |st] .\n(5)\nThis approximation is crude – we could add terms reducing the error through Taylor expansion, but these would be complicated to deal with. Hence, we can relate to this method mainly as a well-motivated heuristic.\nNotice that this formulation resembles the standard MDP formulation, only with the corresponding ”reward” terms log[ρt] going backward instead of forward, and no discount factor. Unfortunately, without a discount factor we\n3The covariances can be expressed analytically as well, for clarity we drop this immediate result.\ncannot expect the estimated value to converge, so we propose using an artificial one γlog. We can incorporate function approximation for this formulation as well. Unlike COP-TD(λ, β), we can choose the features and weights as we wish with no restriction, besides the linear constraint on the resulting ρd through the weight vector θρ. This can be approximately enforced by normalizing θρ using X t . = 1t ∑ t exp(θ > ρ,tφ(st)) (which should equal 1 if we were exactly correct). We call the resulting algorithm LogCOP-TD(λ,β).\nAlgorithm 3 Log-COP-TD(λ,β) with Function Approximation, Input: θ0,θρ,0\n1: Init: F0 = 0, n0(β) = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: nβt = βn β t + 1, Nφ = γlog(βNφ +\nφρ(st)), X = X + exp(θ > ρ,tφ(st))\n6: Update log(Γnt )’s weighted average: 7: Ft = βγlogFt−1 + n β t log[ρ(st−1)] 8: Update & project by log(ρd)’s TD error: 9: δdt =\nFt nβt + θ>ρ,t\n( Nφ\nnβt − φρ(st) ) 10: θρ,t+1 = θρ,t + αdt δ d t φρ(st) 11: Off-policy TD(λ): 12: Mt = λ+ (1− λ) exp ( θ>ρ,t+1φρ(st) ) /(X/t)"
  }, {
    "heading": "13: et = ρt (λγet +Mtφ(st+1))",
    "text": ""
  }, {
    "heading": "14: δt = rt + θ>t (γφ(st+1)− φ(st))",
    "text": "15: θt+1 = θt + αtδtet 16: end for"
  }, {
    "heading": "6.4. Using the Original Features",
    "text": "An interesting phenomenon occurs when the behavior and target policies employ a feature based Boltzmann distribution for choosing the actions: µ(a|s) = exp ( θ>a,µφ(s) ) ,\nand π(a|s) = exp ( θ>a,πφ(s) ) , where a constant feature is added to remove the (possibly different) normalizing constant. Thus, log(ρt) = (θa,π − θa,µ)>φ(st), and LogCOP-TD(λ,β) obtains a parametric form that depends on the original features instead of a different set."
  }, {
    "heading": "6.5. Approximation Hardness",
    "text": "As we propose to use linear function approximation for ρd(s) and log (ρd(s)) one cannot help but wonder how hard it is to approximate these quantities, especially compared to the value function. The comparison between V (s) and ρd(s) is problematic for several reasons:\n1. The ultimate goal is estimating V π(s), approximation errors in ρd(s) are second order terms.\n2. The value function V π(s) depends on the policy-\ninduced reward function and transition probability matrix, while ρd(s) depends on the stationary distributions induced by both policies. Since each depends on at least one distinct factor - we can expect different setups to result in varied approximation hardness. For example, if the reward function has a poor approximation then so will V π(s), while extremely different behavior and target policies can cause ρd(s) to behave erratically.\n3. Subsequently, the choice of features for approximating V π(s) and ρd(s) can differ significantly depending on the problem at hand.\nIf we would still like to compare V π(s) and ρd(s), we could think of extreme examples:\n• When π = µ, ρd(s) ≡ 1, when R(s) ≡ 0 then V π(s) ≡ 0.\n• In the chain MDP example in Section 4 we saw that ρd(s) is an exponential function of the location in the chain. Setting reward in one end to 1 will result in an exponential form for V π(s) as well. Subsequently, in the chain MDP example approximating log (ρd(s)) is easier than ρd(s) as we obtain a linear function of the position; This is not the general case."
  }, {
    "heading": "7. Experiments",
    "text": "We have performed 3 types of experiments. Our first batch of experiments (Figure 1) demonstrates the accuracy of predicting ρd by both COP-TD(λ, β) and Log-COP-TD(λ, β). We show two types of setups in which visualization of ρd is relatively clear - the chain MDP example mentioned in Section 4 and the mountain car domain (Sutton and Barto, 1998) in which the state is determined by only two continuous variables - the car’s position and speed. The parameters λ and β exhibited low sensitivity in these tasks so they were simply set to 0, we show the estimated ρd after 106 iterations. For the chain MDP (top two plots, notice the logarithmic scale) we first approximate ρd without any function approximation (top-left) and we can see COP-TD manages to converge to the correct value while Log-COPTD is much less exact. When we use linear feature space (constant parameter and position) Log-COP-TD captures the true behavior of ρd much better as expected. The two lower plots show the error (in color) in ρd estimated for the mountain car with a pure exploration behavior policy vs. a target policy oriented at moving right. The z-axis is the same for both plots and it describes a much more accurate estimate of ρd obtained through simulations. The features used were local state aggregation. We can see that both algorithms succeed similarly on the position-speed pairs which are sampled often due to the behavior policy and the\nmountain. When looking at more rarely observed states, the estimate becomes worse for both algorithms, though Log-COP-TD seems to be better performing on the spike at position > 0.\nNext we test the sensitivity of COP-TD(λ, β) and LogCOP-TD(λ,β) to the parameters β and γlog (Figure 2) on two distinct toy examples - the chain MDP introduced before but with only 30 states with the position-linear features, and a random MDP with 32 states, 2 actions and a 5-bit binary feature vector along with a free parameter (this compact representation was suggested by White and White (2016) to approximate real world problems). The policies on the chain MDP were taken as described before, and on the random MDP a state independent 0.75/0.25 probability to choose an action by the behavior/target policy. As we can see, larger values of β cause noisier estimations in the random MDP for COP-TD(λ, β), but has little effect in other venues. As for γlog - we can see that if it is too large or too small the error behaves sub-optimally, as expected for the crude approximation of Equation 5. In conclusion, unlike ETD(λ, β), Log/COP-TD(λ, β) are much less effected by β, though γlog should be tuned to improve results.\nOur final experiment (Figure 3) compares our algorithms to ETD(λ, β) and GTD(λ, β) over 4 setups: chain MDP with 100 states with right half rewards 1 with linear features, a 2 action random MDP with 256 states and binary features, acrobot (3 actions) and cart-pole balancing (21 actions) (Sutton and Barto, 1998) with reset at success and state aggregation to 100 states. In all problems we used the same features for ρd and V π(s) estimation, γ = 0.99, constant step size 0.05 for the TD process and results were averaged over 10 trajectories, other parameters (λ, β, other step sizes, γlog) were swiped over to find the best ones. To\nreduce figure clutter we have not included standard deviations though the noisy averages still reflect the variance in the process. Our method of comparison on the first 2 setups estimates the value function using the suggested algorithm, and finds the dπ weighted average of the error between V and the on-policy fixed point ΠπTVπ:\n‖V̂ −ΠπTVπ‖2dπ = ∑ s dπ(s) [ (θ∗ − θ̂)>φ(s) ]2 ,\nwhere θ∗ is the optimal θ obtained by on-policy TD using the target policy. On the latter continuous state problems we applied on-line TD on a different trajectory following the target policy, used the resulting θ value as ground truth and taken the sum of squared errors with respect to it. The behavior and target policies for the chain MDP and random MDP are as specified before. For the acrobot problem the behavior policy is uniform over the 3 actions and the target policy chooses between these with probabilities ( 16 , 1 3 , 1 2 ). For the cart-pole the action space is divided to 21 actions from -1 to 1 equally, the behavior policy chooses among these uniformly while the target policy is 1.5 times more prone to choosing a positive action than a negative one.\nThe experiments show that COP-TD(λ, β) and Log-COPTD(λ, β) have comparable performance to ETD(λ, β) where at least one is better in every setup. The advantage in the new algorithms is especially seen in the chain MDP corresponding to a large discrepancy between the stationary distribution of the behavior and target policy. GTD(λ) is consistently worse on the tested setups, this might be due to the large difference between the chosen behavior and target policies which affects GTD(λ) the most."
  }, {
    "heading": "8. Conclusion",
    "text": "Research on off-policy evaluation has flourished in the last decade. While a plethora of algorithms were suggested so far, ETD(λ, β) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties. Unfortunately, ETD(λ, β) does not converge to the same point achieved by on-line TD when linear function approximation is applied.\nWe address this issue with COP-TD(λ,β) and proved it can achieve consistency when used with a correct set of features, or at least allow trading-off some of the bias by adding or removing features. Despite requiring a new set of features and calibrating an additional update function, COP-TD(λ,β)’s performance does not depend as much on β as ETD(λ,β), and shows promising empirical results.\nWe offer a connection to the statistical interpretation of TD(λ) that motivates our entire formulation. This interpretation leads to two additional approaches: (a) weight the Γnt using estimated variances instead of β exponents and (b) approximating log[ρd] instead of ρd; both approaches deserve consideration when facing a real application."
  }, {
    "heading": "9. Acknowledgments",
    "text": "This Research was supported in part by the Israel Science Foundation (grant No. 920/12) and by the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013)/ ERC Grant Agreement n.306638."
  }],
  "year": 2017,
  "references": [{
    "title": "Application of reinforcement learning to routing in distributed wireless networks: a review",
    "authors": ["Hasan AA Al-Rawi", "Ming Ann Ng", "Kok-Lim Alvin Yau"],
    "venue": "Artificial Intelligence Review,",
    "year": 2015
  }, {
    "title": "Applying reinforcement learning towards automating resource allocation and application scalability in the cloud",
    "authors": ["Enda Barrett", "Enda Howley", "Jim Duggan"],
    "venue": "Concurrency and Computation: Practice and Experience,",
    "year": 2013
  }, {
    "title": "Dynamic Programming and Optimal Control, Vol II",
    "authors": ["D. Bertsekas"],
    "venue": "Athena Scientific,",
    "year": 2012
  }, {
    "title": "Projected equation methods for approximate solution of large linear systems",
    "authors": ["D. Bertsekas", "H. Yu"],
    "venue": "Journal of Computational and Applied Mathematics,",
    "year": 2009
  }, {
    "title": "Adaptive feature pursuit: Online adaptation of features in reinforcement learning. Reinforcement Learning and Approximate Dynamic Programming for Feedback Control, pages",
    "authors": ["Shalabh Bhatnagar", "Vivek S Borkar", "LA Prashanth"],
    "year": 2012
  }, {
    "title": "Feature search in the grassmanian in online reinforcement learning",
    "authors": ["Shalabh Bhatnagar", "Vivek S Borkar", "KJ Prabuchandran"],
    "venue": "IEEE Journal of Selected Topics in Signal Processing,",
    "year": 2013
  }, {
    "title": "Construction of approximation spaces for reinforcement learning",
    "authors": ["Wendelin Böhmer", "Steffen Grünewälder", "Yun Shen", "Marek Musial", "Klaus Obermayer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Least-squares temporal difference learning",
    "authors": ["Justin A Boyan"],
    "venue": "In ICML, pages",
    "year": 1999
  }, {
    "title": "Linear least-squares algorithms for temporal difference learning",
    "authors": ["Steven J Bradtke", "Andrew G Barto"],
    "venue": "Machine learning,",
    "year": 1996
  }, {
    "title": "Projection onto a simplex",
    "authors": ["Yunmei Chen", "Xiaojing Ye"],
    "venue": "arXiv preprint arXiv:1101.6081,",
    "year": 2011
  }, {
    "title": "Policy evaluation with temporal differences: a survey and comparison",
    "authors": ["Christoph Dann", "Gerhard Neumann", "Jan Peters"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Adaptive bases for reinforcement learning",
    "authors": ["Dotan Di Castro", "Shie Mannor"],
    "venue": "Machine Learning and Knowledge Discovery in Databases,",
    "year": 2010
  }, {
    "title": "Regularized policy iteration",
    "authors": ["Amir M Farahmand", "Mohammad Ghavamzadeh", "Shie Mannor", "Csaba Szepesvári"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Incremental truncated lstd",
    "authors": ["Clement Gehring", "Yangchen Pan", "Martha White"],
    "venue": "arXiv preprint arXiv:1511.08495,",
    "year": 2015
  }, {
    "title": "l1-penalized projected bellman residual",
    "authors": ["Matthieu Geist", "Bruno Scherrer"],
    "venue": "In European Workshop on Reinforcement Learning,",
    "year": 2011
  }, {
    "title": "Off-policy learning with eligibility traces: A survey",
    "authors": ["Matthieu Geist", "Bruno Scherrer"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "A dantzig selector approach to temporal difference learning",
    "authors": ["Matthieu Geist", "Bruno Scherrer", "Alessandro Lazaric", "Mohammad Ghavamzadeh"],
    "venue": "arXiv preprint arXiv:1206.6480,",
    "year": 2012
  }, {
    "title": "Lstd with random projections",
    "authors": ["Mohammad Ghavamzadeh", "Alessandro Lazaric", "Odalric Maillard", "Rémi Munos"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Basis expansion in natural actor critic methods",
    "authors": ["Sertan Girgin", "Philippe Preux"],
    "venue": "In European Workshop on Reinforcement Learning,",
    "year": 2008
  }, {
    "title": "Off-policy temporal difference learning with distribution adaptation in fast mixing chains",
    "authors": ["Arash Givchi", "Maziar Palhang"],
    "venue": "Soft Computing,",
    "year": 2017
  }, {
    "title": "Feature selection for reinforcement learning: Evaluating implicit state-reward dependency via conditional mutual information",
    "authors": ["Hirotaka Hachiya", "Masashi Sugiyama"],
    "venue": "Machine Learning and Knowledge Discovery in Databases,",
    "year": 2010
  }, {
    "title": "Importance-weighted least-squares probabilistic classifier for covariate shift adaptation with application to human activity",
    "authors": ["Hirotaka Hachiya", "Masashi Sugiyama", "Naonori Ueda"],
    "venue": "recognition. Neurocomputing,",
    "year": 2012
  }, {
    "title": "Generalized emphatic temporal difference learning: Bias-variance analysis",
    "authors": ["Assaf Hallak", "Aviv Tamar", "Remi Munos", "Shie Mannor"],
    "venue": "arXiv preprint arXiv:1509.05172,",
    "year": 2015
  }, {
    "title": "Regularized least squares temporal difference learning with nested l2 and l1 penalization",
    "authors": ["Matthew W Hoffman", "Alessandro Lazaric", "Mohammad Ghavamzadeh", "Rémi Munos"],
    "venue": "In European Workshop on Reinforcement Learning,",
    "year": 2011
  }, {
    "title": "Constructing basis functions from directed graphs for value function approximation",
    "authors": ["Jeff Johns", "Sridhar Mahadevan"],
    "venue": "In Proceedings of the 24th international conference on Machine learning,",
    "year": 2007
  }, {
    "title": "Linear complementarity for regularized policy evaluation and improvement",
    "authors": ["Jeffrey Johns", "Christopher Painter-Wakefield", "Ronald Parr"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2010
  }, {
    "title": "A least-squares approach to direct importance estimation",
    "authors": ["Takafumi Kanamori", "Shohei Hido", "Masashi Sugiyama"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Reinforcement learning in robotics: A survey",
    "authors": ["Jens Kober", "J Andrew Bagnell", "Jan Peters"],
    "venue": "The International Journal of Robotics Research,",
    "year": 2013
  }, {
    "title": "The fixed points of off-policy TD",
    "authors": ["J Zico Kolter"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "Regularization and feature selection in least-squares temporal difference learning",
    "authors": ["J Zico Kolter", "Andrew Y Ng"],
    "venue": "In Proceedings of the 26th annual international conference on machine learning,",
    "year": 2009
  }, {
    "title": "Td-gamma: Re-evaluating complex backups in temporal difference learning",
    "authors": ["George Konidaris", "Scott Niekum", "Philip S Thomas"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2011
  }, {
    "title": "Automatic feature selection for model-based reinforcement learning in factored mdps",
    "authors": ["Mark Kroon", "Shimon Whiteson"],
    "venue": "In Machine Learning and Applications,",
    "year": 2009
  }, {
    "title": "Stochastic approximation and recursive algorithms and applications, volume 35",
    "authors": ["Harold Kushner", "G George Yin"],
    "venue": "Springer Science & Business Media,",
    "year": 2003
  }, {
    "title": "Regularized offpolicy td-learning",
    "authors": ["Bo Liu", "Sridhar Mahadevan", "Ji Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Feature selection and feature learning for high-dimensional batch reinforcement learning: a survey",
    "authors": ["De-Rong Liu", "Hong-Liang Li", "Ding Wang"],
    "venue": "International Journal of Automation and Computing,",
    "year": 2015
  }, {
    "title": "Sparse temporal difference learning using lasso",
    "authors": ["Manuel Loth", "Manuel Davy", "Philippe Preux"],
    "venue": "In Approximate Dynamic Programming and Reinforcement Learning,",
    "year": 2007
  }, {
    "title": "Samuel meets amarel: Automating value function approximation using global state space analysis",
    "authors": ["Sridhar Mahadevan"],
    "venue": "In AAAI,",
    "year": 2005
  }, {
    "title": "Sparse q-learning with mirror descent",
    "authors": ["Sridhar Mahadevan", "Bo Liu"],
    "venue": "arXiv preprint arXiv:1210.4893,",
    "year": 2012
  }, {
    "title": "Proto-value functions: A laplacian framework for learning representation and control in markov decision processes",
    "authors": ["Sridhar Mahadevan", "Mauro Maggioni"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2007
  }, {
    "title": "Off-policy learning based on weighted importance sampling with linear computational complexity",
    "authors": ["A Rupam Mahmood", "Richard S Sutton"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Weighted importance sampling for off-policy learning with linear function approximation",
    "authors": ["A Rupam Mahmood", "Hado P van Hasselt", "Richard S Sutton"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Basis function adaptation in temporal difference reinforcement learning",
    "authors": ["Ishai Menache", "Shie Mannor", "Nahum Shimkin"],
    "venue": "Annals of Operations Research,",
    "year": 2005
  }, {
    "title": "Human-level control through deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"],
    "venue": "Nature, 518(7540):529–533,",
    "year": 2015
  }, {
    "title": "Greedy algorithms for sparse reinforcement learning",
    "authors": ["Christopher Painter-Wakefield", "Ronald Parr"],
    "venue": "arXiv preprint arXiv:1206.6485,",
    "year": 2012
  }, {
    "title": "An analysis of laplacian methods for value function approximation in mdps",
    "authors": ["Marek Petrik"],
    "venue": "In IJCAI,",
    "year": 2007
  }, {
    "title": "Feature selection using regularization in approximate linear programs for markov decision processes",
    "authors": ["Marek Petrik", "Gavin Taylor", "Ron Parr", "Shlomo Zilberstein"],
    "venue": "arXiv preprint arXiv:1005.1860,",
    "year": 2010
  }, {
    "title": "Off-policy temporal-difference learning with function approximation",
    "authors": ["Doina Precup", "Richard S Sutton", "Sanjoy Dasgupta"],
    "venue": "In ICML,",
    "year": 2001
  }, {
    "title": "Sparse reinforcement learning via convex optimization",
    "authors": ["Zhiwei Qin", "Weichang Li", "Firdaus Janoos"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Stochastic approximation: A dynamical systems viewpoint",
    "authors": ["Zeev Schuss", "Vivek S Borkar"],
    "year": 2009
  }, {
    "title": "Explicit manifold representations for value-function approximation in reinforcement learning",
    "authors": ["William D Smart"],
    "venue": "ISAIM,",
    "year": 2004
  }, {
    "title": "Incremental basis construction from temporal difference error",
    "authors": ["Yi Sun", "Mark Ring", "Jürgen Schmidhuber", "Faustino J Gomez"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning",
    "year": 2011
  }, {
    "title": "Reinforcement learning: An introduction",
    "authors": ["R.S. Sutton", "A. Barto"],
    "year": 1998
  }, {
    "title": "An emphatic approach to the problem of off-policy temporaldifference learning",
    "authors": ["R.S. Sutton", "A.R. Mahmood", "M White"],
    "year": 2015
  }, {
    "title": "A new q (lambda) with interim forward view and monte carlo equivalence",
    "authors": ["Rich Sutton", "Ashique R Mahmood", "Doina Precup", "Hado V Hasselt"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Learning to predict by the methods of temporal differences",
    "authors": ["Richard S Sutton"],
    "venue": "Machine learning,",
    "year": 1988
  }, {
    "title": "A convergent o(n) temporal-difference algorithm for off-policy learning with linear function approximation",
    "authors": ["Richard S Sutton", "Hamid R Maei", "Csaba Szepesvári"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Lifetime value marketing using reinforcement learning",
    "authors": ["Georgios Theocharous", "Assaf Hallak"],
    "venue": "RLDM",
    "year": 2013
  }, {
    "title": "Personalized ad recommendation systems for life-time value optimization with guarantees",
    "authors": ["Georgios Theocharous", "Philip S Thomas", "Mohammad Ghavamzadeh"],
    "venue": "In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence",
    "year": 2015
  }, {
    "title": "High confidence policy improvement",
    "authors": ["Philip Thomas", "Georgios Theocharous", "Mohammad Ghavamzadeh"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "Policy evaluation using the omega-return",
    "authors": ["Philip S Thomas", "Scott Niekum", "Georgios Theocharous", "George Konidaris"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "An analysis of temporal-difference learning with function approximation",
    "authors": ["John N Tsitsiklis", "Benjamin Van Roy"],
    "venue": "Automatic Control, IEEE Transactions on,",
    "year": 1997
  }, {
    "title": "Off-policy td (λ) with a true online equivalence",
    "authors": ["Hado van Hasselt", "A Rupam Mahmood", "Richard S Sutton"],
    "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, Quebec City,",
    "year": 2014
  }, {
    "title": "A novel approach for constructing basis functions in approximate dynamic programming for feedback control",
    "authors": ["Jian Wang", "Zhenhua Huang", "Xin Xu"],
    "venue": "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),",
    "year": 2013
  }, {
    "title": "Investigating practical, linear temporal difference learning",
    "authors": ["Adam White", "Martha White"],
    "venue": "arXiv preprint arXiv:1602.08771,",
    "year": 2016
  }, {
    "title": "Regularized feature selection in reinforcement learning",
    "authors": ["Dean S Wookey", "George D Konidaris"],
    "venue": "Machine Learning,",
    "year": 2015
  }, {
    "title": "Representation discovery using a fixed basis in reinforcement learning",
    "authors": ["Dean Stephen Wookey"],
    "venue": "PhD thesis, University of the Witwatersrand South Africa,",
    "year": 2016
  }, {
    "title": "On convergence of emphatic temporal-difference learning",
    "authors": ["H. Yu"],
    "venue": "In COLT,",
    "year": 2015
  }, {
    "title": "Convergence of least squares temporal difference methods under general conditions",
    "authors": ["Huizhen Yu"],
    "venue": "In Proceedings of the 27th International Conference on Machine Learning",
    "year": 2010
  }, {
    "title": "Graying the black box: Understanding dqns",
    "authors": ["Tom Zahavy", "Nir Ben-Zrihem", "Shie Mannor"],
    "venue": "arXiv preprint arXiv:1602.02658,",
    "year": 2016
  }],
  "id": "SP:affdeb920a9c60e1586d55e40bcbc0e4be49fc4f",
  "authors": [{
    "name": "Assaf Hallak",
    "affiliations": []
  }, {
    "name": "Shie Mannor",
    "affiliations": []
  }],
  "abstractText": "The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(λ, β)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD(λ, β) can be designed to converge to the same value that would have been obtained by using on-policy TD(λ) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call logCOP-TD(λ, β). Both algorithms have favorable empirical results to the current state of the art online OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.",
  "title": "Consistent On-Line Off-Policy Evaluation"
}