{
  "sections": [{
    "text": "setting, our results demonstrate that \"-differential privacy may be ensured for free – in particular, the regret bounds scale as O( p T ) + ˜O 1\n\"\n. For\nbandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of ˜O ⇣ 1\n\"\np T ⌘\n, while the previously known best regret bound was ˜O ⇣ 1\n\"\nT 2 3 ⌘ ."
  }, {
    "heading": "1. Introduction",
    "text": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.\nIn this paper, we design differentially private algorithms for online linear optimization with near-optimal regret, both in\n*Equal contribution 1Computer Science, Princeton University, Princeton, NJ, USA. Correspondence to: Naman Agarwal <namana@cs.princeton.edu>, Karan Singh <karans@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n1Here the ˜O(·) notation hides polylog(T ) factors.\nthe full information and partial information (bandit) settings. This result improves the known best regret bounds for a number of important online learning problems – including prediction from expert advice and non-stochastic multi-armed bandits."
  }, {
    "heading": "1.1. Full-Information Setting: Privacy for Free",
    "text": "For the full-information setting where the algorithm gets to see the complete loss vector every round, we design \"-differentially private algorithms with regret bounds that scale as O ⇣p T ⌘ + ˜O 1\n\"\n(Theorem 3.1), partially resolv-\ning an open question to improve the previously best known bound of O ⇣ 1\n\"\np T ⌘\nposed in (Smith & Thakurta, 2013). A decomposition of the bound on the regret bound of this form implies that when \" 1p\nT , the regret incurred by the differentially private algorithm matches the optimal regret in the non-private setting, i.e. differential privacy is free. Moreover even when \"  1p\nT , our results guarantee a sub-constant regret per round in contrast to the vacuous constant regret per round guaranteed by existing results.\nConcretely, consider the case of online linear optimization over the cube, with unit l1-norm-bounded loss vectors. In this setting, (Smith & Thakurta, 2013) achieves a regret bound of O( 1\n\"\np NT ), which is meaningful only if T N\n\" 2 . Our theorems imply a regret bound of ˜O( p NT + N\n\" ). This is an improvement on the previous bound regardless of the value of \". Furthermore, when T is between N\n\" and N \" 2 , the previous bounds are vacuous whereas our results are still meaningful. Note that the above arguments show an improvement over existing results even for moderate value of \". Indeed, when \" is very small, the magnitude of improvements are more pronounced.\nBeyond the separation between T and \", the key point of our analysis is that we obtain bounds for general regularization based algorithms which adapt to the geometry of the underlying problem optimally, unlike the previous algorithms (Smith & Thakurta, 2013) which utilizes euclidean regularization. This allows our results to get rid of a polynomial dependence on N (in the p T term) in some cases. Online linear optimization over the sphere and prediction with expert advice are notable examples.\nWe summarize our results in Table 1.1."
  }, {
    "heading": "1.2. Bandits: Reduction to the Non-private Setting",
    "text": "In the partial-information (bandit) setting, the online learning algorithm only gets to observe the loss of the prediction it prescribed. We outline a reduction technique that translates a non-private bandit algorithm to a differentially private bandit algorithm, while retaining the ˜O( p T ) dependency of the regret bound on the number of rounds of play (Theorem 4.5). This allows us to derive the first \"- differentially private algorithm for bandit linear optimization achieving ˜O( p T ) regret, using the algorithm for the non-private setting from (Abernethy et al., 2012). This answers a question from (Smith & Thakurta, 2013) asking if ˜O( p T ) regret is attainable for differentially private linear bandits .\nAn important case of the general bandit linear optimization framework is the non-stochastic multi-armed bandits problem(Bubeck et al., 2012b), with applications for website optimization, personalized medicine, advertisement placement and recommendation systems. Here, we propose an \"-differentially private algorithm which enjoys a regret of ˜O( 1\n\"\np NT logN) (Theorem 4.1), improving on the\npreviously best attainable regret of ˜O( 1 \"\nNT 2 3 )(Smith &\nThakurta, 2013).\nWe summarize our results in Table 1.2."
  }, {
    "heading": "1.3. Related Work",
    "text": "The problem of differentially private online learning was first considered in (Dwork et al., 2010), albeit guaranteeing privacy in a weaker setting – ensuring the privacy of the individual entries of the loss vectors. (Dwork et al., 2010) also introduced the tree-based aggregation scheme for releasing the cumulative sums of vectors in a differentially private manner, while ensuring that the total amount of noise added for each cumulative sum is only polylogarithmically dependent on the number of vectors. The stronger notion of privacy protecting entire loss vectors was first studied in (Jain et al., 2012), where gradient-based algorithms were proposed that achieve (\", )-differntial privacy and regret bounds of ˜O ⇣ 1\n\"\np T log 1 ⌘ . (Smith &\nThakurta, 2013) proposed a modification of Follow-theApproximate-Leader template to achieve ˜O 1\n\"\nlog 2.5 T\nregret for strongly convex loss functions, implying a regret bound of ˜O ⇣ 1\n\"\np T ⌘\nfor general convex functions. In addition, they also demonstrated that under bandit feedback, it is possible to obtain regret bounds that scale as ˜O ⇣ 1\n\"\nT 2 3 ⌘ .\n(Dwork et al., 2014a; Jain & Thakurta, 2014) proved that in the special case of prediction with expert advice setting, it is possible to achieve a regret of O 1\n\"\np T logN . While\nmost algorithms for differentially private online learning are based on the regularization template, (Dwork et al., 2014b) used a perturbation-based algorithm to guarantee (\", )-differential privacy for the problem of online PCA. (Tossou & Dimitrakakis, 2016) showed that it is possible to design \"-differentially private algorithms for the stochastic multi-armed bandit problem with a separation of \", T for the regret bound. Recently, an independent work due to (Tossou & Dimitrakakis, 2017), which we were made aware of after the first manuscript, also demonstrated a ˜O ⇣ 1\n\"\np T ⌘\nregret bound in the non-stochastic multi-armed bandits setting. We match their results (Theorem 4.1), as well as provide a generalization to arbitrary convex sets (Theorem 4.5)."
  }, {
    "heading": "1.4. Overview of Our Techniques",
    "text": "Full Information Setting: We consider the two well known paradigms for online learning, Folllow-theRegularized-Leader (FTRL) and Folllow-the-PerturbedLeader (FTPL). In both cases, we ensure differential privacy by restricting the mode of access to the inputs (the loss vectors). In particular, the algorithm can only retrieve estimates of the loss vectors released by a tree based aggregation protocol (Algorithm 2) which is a slight modification of the protocol used in (Jain et al., 2012; Smith & Thakurta, 2013). We outline a tighter analysis of the regret minimization framework by crucially observing that in case of linear losses, the expected regret of an algorithm that injects identically (though not necessarily independently) distributed noise per step is the same as one that injects a single copy of the noise at the very start of the algorithm.\nThe regret analysis of Follow-the-Leader based algorithm involves two components, a bias term due to the regularization and a stability term which bounds the change in the output of the algorithm per step. In the analysis due to (Smith & Thakurta, 2013), the stability term is affected by the variance of the noise as it changes from step to step. However in our analysis, since we treat the noise to have been sampled just once, the stability analysis does not factor in the variance and the magnitude of the noise essentially appears as an additive term in the bias.\nBandit Feedback: In the bandit feedback setting, we show a general reduction that takes a non-private algorithm and outputs a private algorithm (Algorithm 4). Our key observation here (presented as Lemma 4.3) is that on linear functions, in expectation the regret of an algorithm on a noisy sequence of loss vectors is the same as its regret on the original loss sequence as long as noise is zero mean. We now bound the regret on the noisy sequence by conditioning out the case when the noise can be large and using exploration techniques from (Bubeck et al., 2012a) and (Abernethy et al., 2008)."
  }, {
    "heading": "2. Model and Preliminaries",
    "text": "This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.\nFull-Information Setting: Online linear optimization (Hazan et al., 2016; Shalev-Shwartz, 2011) involves repeated decision making over T rounds of play. At the beginning of every round (say round t), the algorithm chooses a point in x\nt 2 X , where X ✓ RN is a (compact) convex set. Subsequently, it observes the loss l\nt 2 Y ✓ RN and suffers a loss of hl\nt , x t i. The success of such an algorithm, across T rounds of play, is measured though regret, which is defined as\nRegret = E  TX\nt=1\nhl t , x t i min x2K\nTX\nt=1\nhl t , xi\nwhere the expectation is over the randomness of the algorithm. In particular, achieving a sub-linear regret (o(T )) corresponds to doing almost as good (averaging across T rounds) as the fixed decision with the least loss in hindsight. In the non-private setting, a number of algorithms have been devised to achieve O( p T ) regret, with additional dependencies on other parameters dependent on the properties of the specific decision set X and loss set Y . (See (Hazan et al., 2016) for a survey of results.)\nFollowing are three important instantiations of the above\nframework.\n• Prediction with Expert Advice: Here the underlying decision set is the simplex X =\nN = {x 2 Rn : x i 0, P n\ni=1\nx i = 1} and the loss vectors are constrained to the unit cube Y = {l\nt 2 RN : kl t k1  1}.\n• OLO over the Sphere: Here the underlying decision is the euclidean ball X = {x 2 Rn : kxk\n2  1} and the loss vectors are constrained to the unit euclidean ball Y = {l\nt 2 RN : kl t k 2  1}.\n• OLO over the Cube: The decision is the unit cube X = {x 2 Rn : kxk1  1}, while the loss vectors are constrained to the set Y = {l\nt 2 RN : kl t k 1  1}.\nPartial-Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets to observe the value hl\nt , x t i, in contrast to the complete loss vector l\nt 2 RN as in the full information scenario. Therefore, the only feedback the algorithm receives is the value of the loss it incurs for the decision it takes. This makes designing algorithms for this feedback model challenging. Nevertheless for the general problem of bandit linear optimization, (Abernethy et al., 2008) introduced a computationally efficient algorithm that achieves an optimal dependence of the incurred regret of O( p T ) on the number of rounds of play. The non-stochastic multi-armed\nbandit (Auer et al., 2002) problem is the bandit version of the prediction with expert advice framework.\nDifferential Privacy: Differential Privacy (Dwork et al., 2006) is a rigorous framework for establishing guarantees on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under composition and robustness to linkage acts (Dwork et al., 2014a).\nDefinition 2.1 ((\", )-Differential Privacy). A randomized online learning algorithm A on the action set X and the loss set Y is (\", )-differentially private if for any two sequence of loss vectors L = (l\n1 , . . . l T ) ✓ YT and L0 = (l0 1 , . . . l0 T\n) ✓ YT differing in at most one vector – that is to say 9t\n0 2 [T ], 8t 2 [T ] {t 0 }, l t = l0 t – for all S ✓ X T , it holds that\nP(A(L) 2 S)  e\"P(A(L0) 2 S) +\nRemark 2.2. The above definition of Differential Privacy is specific to the online learning scenario in the sense that it assumes the change of a complete loss vector. This has been the standard notion considered earlier in (Jain et al., 2012; Smith & Thakurta, 2013). Note that the definition entails that the entire sequence of predictions produced by the algorithm is differentially private.\nNotation: We define kYk p = max{kl t k p : l t 2 Y}, kXk\np = max{kxk p : x 2 X}, and M = max\nl2Y,x2X |hl, xi|, where k · kp is the lp norm. By Holder’s inequality, it is easy to see that M  kYk\np kXk q\nfor all p, q 1 with 1 p + 1 q = 1. We define the distribution LapN ( ) to be the distribution over RN such that each coordinate is drawn independently from the Laplace distribution with parameter ."
  }, {
    "heading": "3. Full-Information Setting: Privacy for Free",
    "text": "In this section, we describe an algorithmic template (Algorithm 1) for differentially private online linear optimization, based on Follow-the-Regularized-Leader scheme. Subsequently, we outline the noise injection scheme (Algorithm 2), based on the Tree-based Aggregation Protocol (Dwork et al., 2010), used as a subroutine by Algorithm 1 to ensure input differential privacy. The following is our main theorem in this setting.\nTheorem 3.1. Algorithm 1 when run with D = LapN ( ) where = kYk1 log T\n\" , regularization R(x), decision set X and loss vectors l\n1 , . . . l t , the regret of Algorithm 1 is bounded by\nRegret  vuutD R TX\nt=1\nmax x2X (kl t k⇤r2R(x))2 +DLap\nwhere\nD Lap = E Z⇠D0  max\nx2X hZ, xi min x2X hZ, xi\nD R = max x2X R(x) min x2X R(x)\nand D0 is the distribution induced by the sum of dlog T e independent samples from D, k · k⇤r2R(x) represents the dual of the norm with respect to the hessian of R. Moreover, the algorithm is \"-differentially private, i.e. the sequence of predictions produced (x\nt : t 2 [T ]) is \"-differentially private.\nAlgorithm 1 FTRL Template for OLO input Noise distribution D, Regularization R(x)\n1: Initialize an empty binary tree B to compute differentially private estimates of P t\ns=1\nl s . 2: Sample n1\n0 , . . . ndlog Te 0\nindependently from D. 3: ˜L\n0\nPdlog Te\ni=1\nni 0 . 4: for t = 1 to T do 5: Choose x\nt = argmin x2X\n⇣ ⌘hx, ˜L t 1i+R(x) ⌘\n. 6: Observe l\nt 2 Y , and suffer a loss of hl t , x t i. 7: (˜L\nt , B) TreeBasedAgg(l t , B, t,D, T ). 8: end for\nThe above theorem leads to following corollary where we show the bounds obtained in specific instantiations of online linear optimization.\nCorollary 3.2. Substituting the choices of , R(x) listed below, we specify the regret bounds in each case.\n1. Prediction with Expert Advice: Choosing = N log T\n\"\nand R(x) = P N\ni=1\nx i log(x i ),\nRegret  O p T logN + N log2 T logN\n\"\n!\n2. OLO over the Sphere Choosing = p N log T\n\" and R(x) = kxk2\n2\nRegret  O p T + N log2 T\n\"\n!\n3. OLO over the Cube With = log T \" and R(x) = kxk2 2\nRegret  O p NT + N log2 T\n\"\n!\nAlgorithm 2 TreeBasedAgg(l t , B, t,D, T ) input Loss vector l\nt , Binary tree B, Round t, Noise distribution D, Time horizon T\n1: ( ˜L0 t , B) PrivateSum(l t , B, t,D, T ) – Algorithm 5 ((Jain et al., 2012)) with the noise added at each node – be it internal or leaf – sampled independently from the distribution D.\n2: s t the binary representation of t as a string. 3: Find the minimum set S of already populated nodes in\nB that can compute P t\ns=1\nl s . 4: Define Q = |S|  dlog T e. Define r\nt = dlog T e Q. 5: Sample n1\nt\n, . . . nrt t independently from D. 6: ˜L\nt\n˜L0 t +\nP rt\ni=1\nni t . output ( ˜L\nt\n, B)."
  }, {
    "heading": "3.1. Proof of Theorem 3.1",
    "text": "We first prove the privacy guarantee, and then prove the claimed bound on the regret. For the analysis, we define the random variable Z\nt to be the net amount of noise injected by the TreeBasedAggregation (Algorithm 2) on the true partial sums. Formally, Z\nt is the difference between cumulative sum of loss vectors and its differentially private estimate used as input to the arg-min oracle.\nZ t = ˜L t\ntX\ni=1\nl i\nFurther, let D0 be the distribution induced by summing of dlog T e independent samples from D.\nPrivacy : To make formal claims about the quality of privacy, we ensure input differential privacy for the algorithm – that is, we ensure that the entire sequence of partial sums of the loss vectors ( P t\ns=1\nl s : t 2 [T ]) is \"- differentially private. Since the outputs of Algorithm 1 are strictly determined by the prefix sum estimates produced by TreeBasedAgg, by the post-processing theorem, this certifies that the entire sequence of choices made by the algorithm (across all T rounds of play) (x\nt : t 2 [T ]) is \"- differentially private. We modify the standard Tree-based Aggregation protocol to make sure that the noise on each output (partial sum) is distributed identically (though not necessarily independently) across time. While this modification is not essential for ensuring privacy, it simplifies the regret analysis. Lemma 3.3 (Privacy Guarantees with Laplacian Noise). Choose any kYk1 log T\n\" . When Algorithm 2 A(D, T ) is run with D = LapN ( ), the following claims hold true:\n• Privacy: The sequence (˜L t : t 2 [T ]) is \"- differentially private.\n• Distribution: 8t 2 [T ], Z t ⇠ Pdlog Te\ni=1\nn i , where\neach n i is independently sampled from LapN ( ).\nProof. By Theorem 9 ((Jain et al., 2012)), we have that the sequence ( ˜L0\nt : t 2 [T ]) is \"-differentially private. Now the sequence (˜L\nt : t 2 [T ]) is \"-differentially private because differential privacy is immune to post-processing(Dwork et al., 2014a).\nNote that the PrivateSum algorithm adds exactly |S| independent draws from the distribution D to P t\ns=1\nl s , where S is the minimum set of already populated nodes in the tree that can compute the required prefix sum. Due to Line 6 in Algorithm 2, it is made certain that every prefix sum released is a sum of the true prefix sum and dlog T e independent draws from D.\nRegret Analysis: In this section, we show that for linear loss functions any instantiation of the Follow-theRegularized-Leader algorithm can be made differentially private with an additive loss in regret. Theorem 3.4. For any noise distribution D, regularization R(x), decision set X and loss vectors l\n1 , . . . l t , the regret of Algorithm 1 is bounded by\nRegret  vuutD R TX\nt=1\nmax x2X (kl t k⇤r2R(x))2 +DD0\nwhere DD0 = EZ⇠D0 [maxx2X hZ, xi minx2X hZ, xi], D\nR\n= max x2X R(x) minx2X R(x), and k · k⇤r2R(x) represents the dual of the norm with respect to the hessian of R.\nProof. To analyze the regret suffered by Algorithm 1, we consider an alternative algorithm that performs a one-shot noise injection – this alternate algorithm may not be differentially private. The observation here is that the alternate algorithm and Algorithm 1 suffer the same loss in expectation and therefore the same expected regret which we bound in the analysis below.\nConsider the following alternate algorithm which instead of sampling noise Z\nt at each step instead samples noise at the beginning of the algorithm and plays with respect to that. Formally consider the sequence of iterates x̂\nt defined as follows. Let Z ⇠ D.\nx̂ 1 , x 1 , x̂ t , argmin x2X ⌘hx, Z +\nX\ni\nl i\ni+R(x)\nWe have that\nE Z1...ZT⇠D\n\" TX\nt=1\nhl t , x t i # = E Z⇠D \" TX\nt=1\nhl t , x̂ t i # (1)\nTo see the above equation note that E Zt⇠D [hlt, x̂ti] = E Z⇠D [hlt, xti] since x, x̂t have the same distribution.\nTherefore it is sufficient to bound the regret of the sequence x̂ 1 . . . x̂ t\n. The key idea now is to notice that the addition of one shot noise does not affect the stability term of the FTRL analysis and therefore the effect of the noise need not be paid at every time step. Our proof will follow the standard template of using the FTL-BTL (Kalai & Vempala, 2005) lemma and then bounding the stability term in the standard way. Formally define the augmented series of loss functions by defining\nl 0\n(x) = 1 ⌘ R(x) + hZ, xi\nwhere Z ⇠ D is a sample. Now invoking the Follow the Leader, Be the Leader Lemma (Lemma 5.3, (Hazan et al., 2016)) we get that for any fixed u 2 X\nTX\nt=0\nl t\n(u) TX\nt=0\nl t (x̂ t+1 )\nTherefore we can conclude that TX\nt=1\n[l t (x̂ t ) l t (u)] (2)\n TX\nt=1\n[l t (x̂ t ) l t (x̂ t+1 )] + l 0 (u) l 0 (x̂ 1 )\n TX\nt=1\n[l t (x̂ t ) l t (x̂ t+1 )] +\n1 ⌘ D R +D Z\n(3)\nwhere D Z , max x2X(hZ, xi) minx2X(hZ, xi) Therefore we now need to bound the stability term l t (x̂ t ) l t (x̂ t+1\n). Now, the regret bound follows from the standard analysis for the stability term in the FTRL scheme (see for instance (Hazan et al., 2016)). Notice that the bound only depends on the change in the cumulative loss per step i.e. ⌘ ( P\nt\nl t + Z), for which the change is the loss vector ⌘l t+1\nacross time steps. Therefore we get that\nl t (x̂ t ) l t (x̂ t+1 )  max x2X k⌘l t k2 ⌘r 2R(x) (4)\nCombining Equations (1), (3), (4) we get the regret bound in Theorem 3.4."
  }, {
    "heading": "3.2. Regret Bounds for FTPL",
    "text": "In this section, we outline algorithms based on the Followthe-Perturbed-Leader template(Kalai & Vempala, 2005). FTPL-based algorithms ensure low-regret by perturbing the cumulative sum of loss vectors with noise from a suitably chosen distribution. We show that the noise added in the process of FTPL is sufficient to ensure differential privacy. More concretely, using the regret guarantees due to\n(Abernethy et al., 2014), for the full-information setting, we establish that the regret guarantees obtained scale as O( p T )+ ˜O( 1\n\"\nlog\n1 ). While Theorem 3.5 is valid for all instances of online linear optimization and achieves O( p T ) regret, it yields sub-optimal dependence on the dimension of the problem. The advantage of FTPL-based approaches over FTRL is that FTPL performs linear optimization over the decision set every round, which is possibly computationally less expensive than solving a convex program every round, as FTRL requires.\nAlgorithm 3 FTPL Template for OLO – A(D, T ) on the action set X , the loss set Y .\n1: Initialize an empty binary tree B to compute differentially private estimates of P t\ns=1\nl s . 2: Sample n1\n0 , . . . ndlog Te 0\nindependently from D. 3: ˜L\n0\nPdlog Te\ni=1\nni 0 . 4: for t = 1 to T do 5: Choose x\nt = argmin x2X hx, ˜Lt 1i.\n6: Observe the loss vector l t 2 Y , and suffer hl t , x t i. 7: (˜L\nt , B) TreeBasedAgg(l t , B, t,D, T ). 8: end for\nTheorem 3.5 (FTPL: Online Linear Optimization). Let kXk\n2\n= sup x2X kxk2 and kYk2 = suplt2Y kltk2. Choosing = max{kYk\n2\nq Tp\nN log T\n, p N\n\"\nlog T log log T } and D = N (0, 2I\nN ), we have that RegretA(D,T )(T ) is\nO N 1 4 kXk\n2 kYk 2 p T + NkXk 2\n\" log\n1.5 T log log T\n!\nMoreover the algorithm is \"-differentially private.\nThe proof of the theorem is deferred to the appendix."
  }, {
    "heading": "4. Differentially Private Multi-Armed Bandits",
    "text": "In this section, we state our main results regarding bandit linear optimization, the algorithms that achieve it and prove the associated regret bounds. The following is our main theorem concerning non-stochastic multi-armed bandits. Theorem 4.1 (Differentially Private Multi-Armed Bandits). Fix loss vectors (l\n1 . . . l T ) such that kl t k1  1. When Algorithm 4 is run with parameters D = LapN ( ) where = 1\n\" and algorithm A = Algorithm 5 with the following parameters: ⌘ = q logN\n2NT (1+2 2 logNT )\n,\n= ⌘N p\n1 + 2 2 logNT and the exploration distribution µ(i) = 1\nN\n. The regret of the Algorithm 4 is\nO\n✓p NT log T logN\n\"\n◆\nMoreover, Algorithm 4 is \"-differentially private\nBandit Feedback: Reduction to the Non-private Setting\nWe begin by describing an algorithmic reduction that takes as input a non-private bandit algorithm and translates it into an \"-differentially private bandit algorithm. The reduction works in a straight-forward manner by adding the requisite magnitude of Laplace noise to ensure differential privacy. For the rest of this section, for ease of exposition we will assume that both T and N are sufficiently large.\nAlgorithm 4 A0(A,D) – Reduction to the Non-private Setting for Bandit Feedback Input: Online Algorithm A, Noise Distribution D.\n1: for t = 0 to T do 2: Receive x̃\nt 2 X from A and output x̃ t . 3: Receive a loss value hl\nt , x̃ t i from the adversary. 4: Sample Z\nt ⇠ D. 5: Forward hl\nt x̃ t i+ hZ t , x̃ t i as input to A. 6: end for\nAlgorithm 5 EXP2 with exploration µ Input: learning rate ⌘; mixing coefficient ; distribution µ\n1: q 1 = 1\nN . . . 1 N\n2 RN .\n2: for t = 1,2 . . . T do 3: Let p\nt = (1 )q t + µ and play i t ⇠ p t\n4: Estimate loss vector l t by ˜l t = P+ t e ite T it l t , with P t = E i⇠pt ⇥ e i eT i ⇤ 5: Update the exponential weights,\nq t+1\n(i) = e ⌘hei, ˜ ltiq t (i) P\ni\n0 e ⌘hei0 , ˜ ltiq t (i0)\n6: end for\nThe following Lemma characterizes the conditions under which Algorithm 4 is \" differentially private Lemma 4.2 (Privacy Guarantees). Assume that each loss vector l\nt is in the set Y ✓ RN , such that max\nt,l2Y | hl,x̃tikx̃tk1 |  B. For D = Lap N ( ) where = B \"\n, the sequence of outputs (x̃\nt : t 2 [T ]) produced by the Algorithm A0(A,D) is \"-differentially private.\nThe following lemma charaterizes the regret of Algorithm 4. In particular we show that the regret of Algorithm 4 is, in expectation, same as that of the regret of the input algorithm A on a perturbed version of loss vectors. Lemma 4.3 (Noisy Online Optimization). Consider a loss sequence (l\n1 . . . l T ) and a convex set X . Define a perturbed version of the sequence as random vectors (˜l\nt : t 2 [T ]) as ˜l\nt = l t + Z t where Z t is a random vector such that {Z\n1 , . . . Z t } are independent and E[Z t ] = 0 for all t 2 [T ].\nLet A be a full information (or bandit) online algorithm which outputs a sequence (x̃\nt\n2 X : t 2 [T ]) and takes as\ninput ˜l t (respectively h˜l t , x̃ t i) at time t. Let x⇤ 2 K be a fixed point in the convex set. Then we have that\nE{Zt} \" EA \" TX\nt=1\n(hl t , x̃ t i hl t , x⇤i) ##\n= E{Zt} \" EA \" TX\nt=1\n⇣ h˜l t .x̃ t i h˜l t , x⇤i ⌘##\nWe provide the proof of Lemma 4.2 and defer the proof of Lemma 4.3 to the Appendix Section B.\nProof of Lemma 4.2. Consider a pair of sequence of loss vectors that differ at exactly one time step – say L = (l 1 , . . . l t0 . . . , lT ) and L0 = (l1, . . . , l0t0 , . . . lT ). Since the prediction of produced by the algorithm at time step any time t can only depend on the loss vectors in the past (l 1 , . . . l t 1), it is clear that the distribution of the output of the algorithm for the first t 0 rounds (x̃ 1 , . . . x̃ t0) is unaltered. We claim that 8I ✓ R, it holds that\nP(hl t0 + Zt0 , x̃t0i 2 I)  e\"P(hl0t0 + Zt0 , x̃t0i 2 I)\nBefore we justify the claim, let us see how this implies that desired statement. To see this, note that conditioned on the value fed to the inner algorithm A at time t\n0 , the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at other time steps (discounting t\n0 ) stays the same (in distribution). By the above discussion, it is sufficient to demonstrate \"-differential privacy for each input fed (as feedback) to the algorithm A.\nFor the sake of analysis, define lFict t as follows. If x̃ t = 0, define lFict\nt\n= 0 2 RN . Else, define lFict t 2 RN to be such that (lFict\nt\n)\ni\n= hlt,x̃ti x̃i if and only if i = argmax i2[d]|x̃i|\nand 0 otherwise, where argmax breaks ties arbitrarily. Define ˜lFict\nt\n= lFict t + Z t . Now note that h˜lFict t , x̃ t i = hl t x̃ t i+ hZ t , x̃ t i.\nIt suffices to establish that each ˜lFict t is \"-differentially private. To argue for this, note that Laplace mechanism (Dwork et al., 2014a) ensures the same, since the l\n1 norm of ˜lFict\nt\nis bounded by B."
  }, {
    "heading": "4.1. Proof of Theorem 4.1",
    "text": "Privacy: Note that since max t,l2Y | hl,x̃tikx̃tk1 |  kYk1  1 as x̃ t 2 {e i\n: i 2 [N ]}. Therefore by Lemma 4.2, setting = 1\n\"\nis sufficient to ensure \"-differential privacy.\nRegret Analysis: For the purpose of analysis we define the following pseudo loss vectors.\n˜l t = l t + Z t\nwhere by definition Z t ⇠ LapN ( ). The following follows from Fact C.1 proved in the appendix.\nP(kZ t k21 10 2 log 2 NT )  1\nT 2\nTaking a union bound, we have\nP(9t kZ t k21 10 2 log 2 NT )  1 T (5)\nTo bound the norm of the loss we define the event F , {9t : kZ\nt k21 10 2 log 2 NT}. We have from (5) that\nP(F )  1 T . We now have that\nE[Regret]  E[Regret| ¯F ] + P(F )E[Regret|F ]\nSince the regret is always bounded by T we get that the second term above is at most 1. Therefore we will concern ourselves with bounding the first term above. Note that Z t\nremains independent and symmetric even when conditioned on the event ¯F . Moreover the following statements also hold.\n8t E[Z t | ¯F ] = 0 (6)\n8t E[kZ t k21| ¯F ]  10 2 log 2 NT (7)\nEquation (6) follows by noting that Z t remains symmetric around the origin even after conditioning. It can now be seen that Lemma 4.3 still applies even when the noise is sampled from LapN ( ) conditioned under the event ¯F (due to Equation 6). Therefore we have that\nE[Regret| ¯F ] = E{Zt} \" EA \" TX\nt=1\n⇣ h˜l t , x̃ t i h˜l t , x⇤i ⌘# ¯F #\n(8)\nTo bound the above quantity we make use of the following lemma which is a specialization of Theorem 1 in (Bubeck et al., 2012a) to the case of multi-armed bandits. Lemma 4.4 (Regret Guarantee for Algorithm 5). If ⌘ is such that ⌘|he\ni , ˜l t i|  1, we have that the regret of Algorithm 5 is bounded by\nRegret  2 T + logN ⌘\n+ ⌘E X\nt\nX\ni\np t (i)he i , ˜l t i2\nNow note that due to the conditioning kZ t k21  10 2 log2 NT and therefore we have that\nmax t,x2 N |hZt, xi|  4 logNT.\nIt can be seen that the condition ⌘|he i , ˜l t i|  1 in Theorem 4.4 is satisfied for exploration µ(i) = 1\nN and under the condition ¯F as long as\n⌘N(1 + 4 logNT ) \nwhich holds by the choice of these parameters. Finally\nE[Regret| ¯F ]\n= E{Zt} \" EA \" TX\nt=1\n⇣ h˜l t , x̃ t i h˜l t , x⇤i ⌘# ¯F #\n E{Zt}\n\" logN\n⌘ + ⌘\nTX\nt=1\nNk˜l t k21 + 2T ¯F #\n E{Zt}\n\" logN\n⌘ + 2⌘\nTX\nt=1\nN(kl t k21 + kZtk21) + 2T ¯F #\n logN ⌘ + 2⌘TN(1 + 2 log2 NT ) + 2T  O ✓q TN logN(1 + 2 log2 NT ) ◆\n O ✓p NT log T logN\n\"\n◆"
  }, {
    "heading": "4.2. Differentially Private Bandit Linear Optimization",
    "text": "In this section we prove a general result about bandit linear optimization over general convex sets, the proof of which is deferred to the appendix.\nTheorem 4.5 (Bandit Linear Optimization). Let X ✓ RN be a convex set. Fix loss vectors (l\n1 , . . . l T ) such that max\nt,x2X |hlt, xi|  M . We have that Algorithm 4 when run with parameters D = LapN ( ) (with = kYk1\n\" ) and algorithm A = SCRiBLe(Abernethy et al., 2012) with step parameter ⌘ = q ⌫ log T\n2N 2 T (M 2 + 2 NkXk22) we have the following guarantees that the regret of the algorithm is bounded by\nO\np\nT log T\ns\nN2⌫ ✓ M2 + NkXk2 2 kYk2 1\n\"2\n◆!\nwhere ⌫ is the self-concordance parameter of the convex body X . Moreover the algorithm is \"-differentially private."
  }, {
    "heading": "5. Conclusion",
    "text": "In this work, we demonstrate that ensuring differential privacy leads to only a constant additive increase in the incurred regret for online linear optimization in the full feedback setting. We also show nearly optimal bounds (in terms of T) in the bandit feedback setting. Multiple avenues for future research arise, including extending our bandit results to other challenging partial-information models such as semi-bandit, combinatorial bandit and contextual bandits. Another important unresolved question is whether it is possible to achieve an additive separation in \", T in the adversarial bandit setting."
  }],
  "year": 2017,
  "references": [{
    "title": "Competing in the dark: An efficient algorithm for bandit linear optimization",
    "authors": ["Abernethy", "Jacob", "Hazan", "Elad", "Rakhlin", "Alexander"],
    "venue": "In COLT, pp",
    "year": 2008
  }, {
    "title": "Online linear optimization via smoothing",
    "authors": ["Abernethy", "Jacob", "Lee", "Chansoo", "Sinha", "Abhinav", "Tewari", "Ambuj"],
    "venue": "In COLT, pp",
    "year": 2014
  }, {
    "title": "Interior-point methods for full-information and bandit online learning",
    "authors": ["Abernethy", "Jacob D", "Hazan", "Elad", "Rakhlin", "Alexander"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2012
  }, {
    "title": "The nonstochastic multiarmed bandit problem",
    "authors": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"],
    "venue": "SIAM journal on computing,",
    "year": 2002
  }, {
    "title": "Towards minimax policies for online linear optimization with bandit feedback",
    "authors": ["Bubeck", "Sébastien", "Cesa-Bianchi", "Nicolo", "Kakade", "Sham M", "Mannor", "Shie", "Srebro", "Nathan", "Williamson", "Robert C"],
    "venue": "In COLT,",
    "year": 2012
  }, {
    "title": "Calibrating noise to sensitivity in private data analysis",
    "authors": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"],
    "venue": "In Theory of Cryptography Conference,",
    "year": 2006
  }, {
    "title": "Differential privacy under continual observation",
    "authors": ["Dwork", "Cynthia", "Naor", "Moni", "Pitassi", "Toniann", "Rothblum", "Guy N"],
    "venue": "In Proceedings of the forty-second ACM symposium on Theory of computing,",
    "year": 2010
  }, {
    "title": "Analyze gauss: optimal bounds for privacypreserving principal component analysis",
    "authors": ["Dwork", "Cynthia", "Talwar", "Kunal", "Thakurta", "Abhradeep", "Zhang", "Li"],
    "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
    "year": 2014
  }, {
    "title": "near) dimension independent risk bounds for differentially private learning",
    "authors": ["Jain", "Prateek", "Thakurta", "Abhradeep G"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Differentially private online learning",
    "authors": ["Jain", "Prateek", "Kothari", "Pravesh", "Thakurta", "Abhradeep"],
    "venue": "In COLT,",
    "year": 2012
  }, {
    "title": "Efficient algorithms for online decision problems",
    "authors": ["Kalai", "Adam", "Vempala", "Santosh"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 2005
  }, {
    "title": "Online learning and online convex optimization",
    "authors": ["Shalev-Shwartz", "Shai"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "nearly) optimal algorithms for private online learning in fullinformation and bandit settings",
    "authors": ["Smith", "Adam", "Thakurta", "Abhradeep Guha"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Algorithms for differentially private multi-armed bandits",
    "authors": ["Tossou", "Aristide", "Dimitrakakis", "Christos"],
    "venue": "In AAAI 2016,",
    "year": 2016
  }, {
    "title": "Achieving privacy in the adversarial multi-armed bandit",
    "authors": ["Tossou", "Aristide C. Y", "Dimitrakakis", "Christos"],
    "venue": "In 14th International Conference on Artificial Intelligence",
    "year": 2017
  }],
  "id": "SP:28d32ac12beadb0b2a293d4d59a4918cd339c0c2",
  "authors": [{
    "name": "Naman Agarwal",
    "affiliations": []
  }, {
    "name": "Karan Singh",
    "affiliations": []
  }],
  "abstractText": "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.",
  "title": "The Price of Differential Privacy for Online Learning"
}