{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 889–898, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Text compression exploits redundancy in human language to store documents compactly, and transmit them quickly. It is natural to think about compressing bilingual texts, which have even more redundancy:\n“From an information theoretic point of view, accurately translated copies of the original text would be expected to contain almost no extra information if the original text is available, so in principle it should be possible to store and transmit these texts with very little extra cost.” (Nevill and Bell, 1992)\nOf course, if we look at actual translation data (Figure 1), we see that there is quite a bit of unpredictability. But the intuition is sound. If there were a million equally-likely translations of a short sentence, it would only take us log2(1m) = 20 bits to specify which one.\nBy finding and exploiting patterns in bilingual data, we want to provide an upper bound for this question: How much information does a human translator add to the original? We do this in the context of building a practical compressor for bilingual text.\n上个星期的战斗至少夺取12个人的生命。 At least 12 people were killed in the battle last week. Last week’s fight took at least 12 lives. The fighting last week killed at least 12. The battle of last week killed at least 12 persons. At least 12 people lost their lives in last week’s fighting. At least 12 persons died in the fighting last week. At least 12 died in the battle last week. At least 12 people were killed in the fighting last week. During last week’s fighting, at least 12 people died. Last week at least twelve people died in the fighting. Last week’s fighting took the lives of twelve people.\nFigure 1: Eleven human translations of the same source sentence (LDC2002T01).\nWe adopt the same scheme used in monolingual text compression benchmark evaluations, such as the Hutter Prize (Hutter, 2006), a competition to compress a 100m-word extract of English Wikipedia. A valid entry is an executable, or self-extracting archive, that prints out Wikipedia, byte-for-byte. Decompression code, dictionaries, and/or other resources must be embedded in the executable—we cannot assume that the recipient of the compressed file has access to those resources. This view of compression goes by the name of algorithmic information theory (or Kolmogorov complexity).\nAny executable is permitted. For example, if our job were to compress the first million digits of π, then we might submit a very short piece of code that prints those digits. The brevity of that compression would demonstrate our understanding of the sequence. Of course, in our application, we will find it useful to develop generic algorithms that can compress any text.\nOur approach will be as follows. Given a bilingual text (file1 and file2), we develop this compression interface:\n% compress file1 > file1.exe\n% bicompress file2 file1 > file2.exe\nThe second command compresses file2 while looking at file1. We take the size of file1.exe\n889\nas the amount of information in the original text. We bound how much information the translator adds to the original by: |file2.exe| / |file1.exe| We can say that bilingual compression is more effective that monolingual compression if: |file2.exe| < |file3.exe|, where % compress file2 > file3.exe Our decompression interface is: % file1.exe > file1\n% file2.exe file1 > file2\nThe second command decompresses file2 while looking at (uncompressed) file1.\nThe contributions of this paper are: 1. We provide a new quantitative bound for how\nmuch information a translator adds to an original text. 2. We present practical software to compress bilingual text with compression rates that exceed the previous state-of-the-art. 3. We set up a public benchmark bilingual text compression challenge to stimulate new researchers to find and exploit patterns in bilingual text. Ultimately, we want to feed those ideas into practical machine translation systems."
  }, {
    "heading": "2 Data",
    "text": "We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2). Portions of this large corpus have been used in previous compression work (Sánchez-Martı́nez et al., 2012). The Spanish side is in UTF-8. For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline.\nOur task is to compress the data “as is”: un-\ntokenized, but already segment aligned. We also include a tokenized version with 334 manually word-aligned segment pairs (Lambert et al., 2005) distributed throughout the corpus.\nFor rapid development and testing, we have arranged a smaller corpus that is 10% the size of the full corpus (Figure 3)."
  }, {
    "heading": "3 Monolingual compression",
    "text": "Compression captures patterns in data. Language modeling also captures patterns, but at first blush, these two areas seem distinct. In compression, we seek a small executable that prints out a text, while in language modeling, we seek an executable that assigns low perplexity to held-out test data.1 Actually, the two areas have much more in common, as a review of compression algorithms reveals.\nHuffman coding. A well-known compression technique is to create a binary Huffman tree whose leaves are characters in the text,2 and whose edges are labeled 0 or 1 (Huffman and others, 1952). The tree is arranged so that frequent characters have short binary codes (edge sequences). It is very important that the Huffman tree for a particular text be included at the beginning of the compressed file, so that decompression knows how to process the compressed bit string.\nAdaptive Huffman. Actually, we can avoid shipping the Huffman tree inside the compressed file, by building the tree adaptively, as the compressor processes the input text. If we start with a uniform distribution, the first few characters may not compress very well, but soon we will converge onto a good tree and good compression. It is very\n1File size has advantages, as perplexity computations are often buggy, and they usually gloss over how probability is apportioned to out-of-vocabulary items.\n2Or other symbols, such as words, bytes, or unicode sequences.\nimportant that the decompressor exactly recapitulate the same sequence of Huffman trees that the compressor made. It can do this by counting characters as it outputs them, just as the compressor counted characters as it consumed them.\nAdaptive compression can also nicely accommodate shifting topics in text, if we give higher counts to recent events. By its single-pass nature, it is also good for streaming data.\nArithmetic coding. Huffman coding exploits a predictive unigram distribution over the next character. If we use more context, we can make sharper distributions. An n-gram table is one way to map contexts onto predictions.\nHow do we convert good predictions into good compression? The solution is called arithmetic coding (Rissanen and Langdon Jr., 1981; Witten et al., 1987). Figure 4 sketches the technique. We produce context-dependent probability intervals, and each time we observe a character, we move to its interval. Our working interval becomes smaller and smaller, but the better our predictions, the wider it stays. A document’s compression is the shortest bit string that fits inside the final interval. In practice, we do the bit-coding as we navigate probability intervals.\nArithmetic coding separates modeling and compression, making our job similar to language modeling, where we use try to use context to predict the next symbol."
  }, {
    "heading": "3.1 PPM",
    "text": "PPM is the most well-known adaptive, predictive compression technique (Cleary and Witten, 1984). PPM updates character n-gram tables (usually n=1..5) as it compresses. In a given context, an n-gram table may predict only a subset of characters, so PPM reserves some probability mass for\nan escape (ESC), after which it executes a hard backoff to the (n-1)-gram table. In PPMA, P(ESC) is 1/(1+D), where D is the number of times the context has been seen. PPMB uses q/D, where q is the number of distinct character types seen in the context. PPMC uses q/(q+D), aka Witten-Bell. PPMD uses q/2D.\nPPM* uses the shortest previously-seen deterministic context, which may be quite long. If there is no deterministic context, PPM* goes to the longest matching context and starts PPMD. Instead of the longest context, PPMZ rates all contexts between lengths 0 and 12 according to each context’s most probable character. PPMZ also implements an adaptive P(ESC) that combines context length, number of previous ESC in the context, etc.\nWe use our own C++ implementation of PPMC for monolingual compression experiments in this paper. When we pass over a set of characters in favor of ESC, we remove those characters from the hard backoff."
  }, {
    "heading": "3.2 PAQ",
    "text": "PAQ (Mahoney, 2005) is a family of state-of-theart compression algorithms and a perennial Hutter Prize winner. PAQ combines hundreds of models with a logistic unit when making a prediction. This is most efficient when predictions are at the bit-level instead of the character-level. The unit’s model weights are adaptively updated by:\nwi ← wi + ηxi(correct− P(1)), where xi = ln(Pi(1)/(1− Pi(1)) η = fixed learning rate Pi(1) = ith model’s prediction\nPAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII > 32), a bigram model, and a skip-bigram model."
  }, {
    "heading": "4 Bilingual Compression: Prior Work",
    "text": "Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase corpora, such as different English versions of the Bible.\nConley and Klein (2008) and Conley and Klein (2013) compress a target text that has been wordaligned to a source text, to which they add a lemmatizer and bilingual glossary. They obtain a 1%- 6% improvement over monolingual compression,\nwithout counting the cost of auxiliary files needed for decompression.\nMartı́nez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, then compressing this sequence of biwords. Sánchez-Martı́nez et al. (2012) improve the interleaving scheme and include offsets to enable decompression to reconstruct the original word order. They also compare several characterbased and word-based compression schemes for biword sequences. On Spanish-English Europarl data, they reach an 18.7% compression rate on word-interleaved text, compared to 20.1% for concatenated texts, a 7.2% improvement.\nAl-Onaizan et al. (1999) study the perplexity of learned translation models, i.e., the probability assigned to the target corpus given the source corpus. They observed iterative training to improve training-set perplexity (as guaranteed) but degrade test-set perplexity. They hypothesized that an increasingly tight, unsmoothed translation dictionary might exclude word translations needed to explain test-set data. Subsequently, research moved to extrinsic evaluation of translation models, in the context of end-to-end machine translation.\nFoster et al. (2002) and others have used prediction to propose auto-completions to speed up human translation. As we have seen, prediction and compression are highly related."
  }, {
    "heading": "5 Predictive Bilingual Compression",
    "text": "Our algorithm compresses target-language file2 while looking at source-language file1:\n% bicompress file2 file1 > file2.exe\nTo make use of arithmetic coding, we consider the task of predicting the next target character, given the source sentence and target string so far:3\nP(ej |f1 . . . fl, e1 . . . ej−1) If we are able to accurately predict what a human translator will type next, then we should be able to build a good machine translator. Here is an example of the task:\nSpanish: Pido que hagamos un minuto de silencio. English so far: I should like to ob\n3We predict e from f in this paper, reversed from Brown et al. (1993), who predict f from e."
  }, {
    "heading": "5.1 Word alignment",
    "text": "Let us first work at the word level instead of the character level. If we are predicting the jth English word, and we know that it translates fi (“aligns to fi”), and if fi has only a handful of translations, then we may be able to specify ej with just a few bits. We may therefore suppose that a set of Viterbi word alignments may be useful for compression (Conley and Klein, 2008; SánchezMartı́nez et al., 2012).\nWe consider unidirectional alignments that link each target position j to a single source position i (including the null word at i = 0). Such alignments can be computed automatically using EM (Brown et al., 1993), and stored in one of two formats:\nAbsolute: 1 2 5 5 7 0 3 6 . . . Relative: +1 +1 +3 0 +2 null -4 +3 . . .\nIn order to interpret the bits produced by the compressor, our decompressor must also have access to the same Viterbi alignments. Therefore, we must include those alignments at the beginning of the compressed file. So let’s compress them too.\nHow compressible are alignment sequences? Figure 5 gives results for Viterbi alignments derived from our large parallel Spanish/English corpus. First, some interesting facts: • Huffman works better on relative offsets, be-\ncause the common “+1” gets a short bit code. • PPMC’s use of context makes it impressively\ninsensitive to alignment format. • PPMC beats Huffman on relative offsets.\nThis would not happen if relative offset integers were independent of one another, as assumed by (Brown et al., 1993) and (Vogel et al., 1996). Bigram statistics bear this out:\nP(+1 | -2) = 0.20 P(+1 | +1) = 0.59 P(+1 | -1) = 0.20 P(+1 | +2) = 0.49 P(+1 | 0) = 0.52\nSo this small compression experiment already\nsuggests that translation aligners might want to model more context than just P(offset).\nHowever, the main point of Figure 5 is that the compressed alignment file requires 12.4 Mb! This is too large for us to prepend to our compressed file, for the sake of enabling decompression."
  }, {
    "heading": "5.2 Translation dictionary",
    "text": "Another approach is to forget Viterbi alignments and instead exploit a probabilistic translation dictionary table t(e|f). To predict the next target word ej , we admit the possibility that ej might be translating any of the source tokens. IBM Model 2 (Brown et al., 1993) tells us how to do this:\nGiven f1 . . . fl: 1. Choose English length m (m|l) 2. For j = 1..m, choose alignment aj a(aj |j, l) 3. For j = 1..m, choose translation ej t(ej |faj )\nwhich, via the “IBM trick” implies: P(e1 . . . em|f1 . . . fl) = (m|l) ∏mj=1 ∑li=0 a(i|j, l)t(ej |fi) In compression, we must predict English words incrementally, before seeing the whole string. Furthermore, we must predict P(STOP ) to end the English sentence. We can adapt IBM Model 2 to make incremental predictions:\nP(STOP |f1 . . . fl, e1 . . . ej−1) ∼ P(STOP |j, l) = (j − 1|l)/∑maxk=j−1 (k|l)\nP(ej |f1 . . . fl, e1 . . . ej−1) ∼ P(ej |f1 . . . fl) =\n[1− P(STOP |j, l)] ∑li=0 a(i|j, l)t(ej |fi) We can train t, a, and on our bilingual text using EM (Brown et al., 1993). However, the t-table is still too large to prepend to the compressed English file."
  }, {
    "heading": "5.3 Adaptive translation modeling",
    "text": "Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string.\nNeal and Hinton (1998) describe online EM, which updates probability tables after each training example. Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case.\nWe initialize a uniform translation model, use it to collect fractional counts from the first segment\npair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on. Because we pass through the data only once, we hope to converge quickly to high-quality tables for compressing the bulk of the text.\nUnlike in batch EM, we need not keep separate count and probability tables. We only need count tables, including summary counts for normalization groups, so memory savings are significant. Whenever we need a probability, we compute it on the fly. To avoid zeroes being immediately locked in, we invoke add-λ smoothing every time we compute a probability from counts:4\nt(e|f) = count(e,f)+λtcount(f)+λt|VE | a(i|j, l) = count(i,j,l)+λacount(j,l)+λa(l+1)\nwhere |VE | is the size of the English vocabulary. We determine |VE | via a quick initial pass through the data, then include it at the top of our compressed file.\nIn batch EM, we usually run IBM Model 1 for a few iterations before Model 2, gripped by an atavistic fear that the a probabilities will enforce rigid alignments before word co-occurrences have a chance to settle in. It turns out this fear is justified in online EM! Because the a table initially learns to align most words to null, we smooth it more heavily (λa = 102, λt = 10−4).\nWe also implement a single-pass HMM alignment model (Vogel et al., 1996). In the IBM models, we can either collect fractional counts after we have compressed a whole sentence, or we can do it word-by-word. In the HMM model, alignment choices are no longer independent of one another:\nGiven f1 . . . fl: 1. Choose English length m w/prob (m|l) 2. For j = 1..m:\n2a. set aj to null w/prob p1, or\n2b. choose non-null aj w/prob (1− p1)o(aj − ak) 3. For j = 1..m, choose translation ej w/prob t(ej |faj ) In the expression o(aj − ak), k is the maximum English index (k < j) such that ak 6= 0. The relative offset o-table learns to encourage adjacent English words to align to adjacent Spanish words.\nBatch HMM performs poorly under uniform initialization, with two causes of failure. First, EM training sets o(0) too high, leading to absolute alignments like “1 2 2 2 5 5 5 5 . . . ”. We avoid\n4In their online EM Model 1 aligner, Liang and Klein (p.c.) skirt the smoothing issue by running an epoch of batch EM to initialize a full set of probabilities before starting.\nthis with a standard schedule of 5 IBM1 iterations, 5 IBM2 iterations, then 5 HMM iterations. However, HMM still learns a very high value for p1, aligning most tokens to null, so we fix p1 = 0.1 for the duration of training.\nSingle-pass, online HMM suffers the same two problems, both solved when we smooth differentially (λo = 102, λt = 10−4) and fix p1 = 0.1.\nTwo quick asides before we examine the effectiveness of our online methods: • Translation researchers often drop long seg-\nment pairs that slow down HMM model processing. In compression, we cannot drop any of the text. Therefore, if the source segment contains more than 50 words, we use only monolingual PPMC to compress the target. This affects 26.5% of our word tokens. • We might assist an online aligner by permut-\ning our n segment pairs to place shorter, less ambiguous ones at the top. However, we would have to communicate the permutation to the decompressor, at a prohibitive cost of log2(n!)/(8 · 106) = 4.8 Mb. We next look at alignment accuracy (f-score) on our large Spanish/English corpus (Figure 6). We evaluate against both a silver standard (Batch EM Viterbi alignments5) and a gold standard of 334 human-aligned segment pairs distributed throughout the corpus. We see that online methods generate competitive translation dictionaries. Because single-pass alignment is significantly faster than traditional multi-pass, we also investigate its impact on an overall Moses pipeline for phrase-based\n5We confirm that our Batch HMM implementation gives f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2, p=85.5, r=61.0), and its differently parameterized HMM.\nmachine translation (Koehn et al., 2007). Figure 7 shows that we can achieve competitive translation accuracy using fast, single-pass alignment, speeding up the system development cycle. For this use case, we can get an additional +0.3 alignment fscore (just as fast) if we print Viterbi alignments in a second pass instead of during training."
  }, {
    "heading": "5.4 Word tokenization",
    "text": "We now want our continuously-improving translation model (TM) to predict target text, and to combine its predictions with PPM’s. For that to happen, our TM will need to predict the exact text, including spurious double-spaces, how parentheses combine with quotation marks, and so on.\nWe devise a tokenization scheme that records spacing information in the word tokens, which allows us to recover the original text uniquely. First, we identify word tokens as subsequences of [a-zAZ]*, [0-9]*, and [other]*, appending to each token the number of spaces following it (e.g., “...@2”). Next, we remove all “@1”, which leaves unique\nrecoverability intact. Finally, we move any suffix on an alpha-numeric word i to become a prefix on a non-alpha-numeric word i+ 1. This reduces the vocabulary size for TM learning. An example: \"String-theory?\" he asked.\n<=> S@0 \"@0 String@0 -@0 theory@0 ?@0 \"@1 he@2 asked@0 .@0\n<=> S@0 \"@0 String@0 -@0 theory@0 ?@0 \" he@2 asked@0 .@0\n<=> S@0 \"@0 String @0-@0 theory @0?@0 \" he@2 asked @0.@0"
  }, {
    "heading": "5.5 Predicting target words",
    "text": "Under this tokenization scheme, we now ask our TM to give us a probability distribution over possible next words. The TM knows the entire source word sequence f1...fl and the target words e1...ej−1 seen so far. As candidates, we consider target words that can be produced, via the current t-table, from any (non-NULL) source words with probability greater than 10−4.\nFor HMM, we compute a prediction lattice that gives a distribution over possible source alignment positions for the current word we are predicting. Intuitively, the prediction lattice tells us “where we currently are” in translating the source string, and it prefers translations of source words in that vicinity. We efficiently reuse the lattice as we make predictions for each subsequent target word.\nTo make the TM’s prediction more accurate, we weight its prediction for each word with a smoothed, adapted English bigram word language model (LM). This discourages the TM from trying to predict the first character of a word by simply using the most frequent source words. We found that exponentiating the LM’s score by 0.2 before weighting keeps it from overpowering the HMM predictions."
  }, {
    "heading": "5.6 Predicting target characters",
    "text": "To convert word predictions into character predictions, we combine scores for words that share the next character. For example, if the TM predicts ”monkey 0.4, car 0.3, cat 0.2, dog 0.1”, then we have ”P(c) 0.5, P(m) 0.4, P(d) 0.1”. Additionally, we restrict ourselves to words prefixed by the portion of ej already observed. The TM predicts the space character when a predicted word fully matches the observed prefix.\nWe also adjust PPM to produce a full distribution over the 96 possible next characters. PPM\nnormally computes a distribution over only characters previously seen in the current context (plus ESC). We now back off to the lowest context for every prediction.\nWe interpolate PPM and TM probabilities: P(ek|f1 . . . fl, e1 . . . ek−1) = µ PPPM (ek|e1 . . . ek−1)+ (1− µ) PTM (ek|f1 . . . fl, e1 . . . ek−1)\nWe adjust µ dynamically based on the relative confidence of the models:\nµ = max(PPM) 2.5\nmax(PPM)2.5+max(HMM)2.5 Here, max(model) refers to the highest probability assigned to any character in the current context by the model. This yields better compression rates than simply setting µ to a constant. When the TM is unable to extend a word, we set µ = 1."
  }, {
    "heading": "6 Results",
    "text": "Figure 8 shows that monolingual PPM compresses the Spanish side of our corpus to 15.8% of the original. Figure 9 (Main results) shows results for the English side of the corpus. Monolingual PPM compresses to 16.5%, while our HMM-based bilingual compression compresses to 11.9%.6\nWe can say that a human translation is characterized by an additional 0.95 bits per byte on top of the original, rather than the 1.32 bits per byte we\n6For this result, we divide the English corpus into two pieces and compress them in parallel, and we further increase the sentence length threshold from 50 to 60, incurring a speed penalty. Our fictional Weissman score is 0.676.\nwould need if the English were independent text. Assuming our Spanish compression is good, we can also say that the human translator produces at most 68.1% (35.0/51.4) of the information that the original Spanish author produced. Intuitively, we feel this bound is high and should be reduced with better translation modeling.\nFigure 9 also reports our Shannon game experiments in which bilingual humans guessed subsequent characters of the English text. As suggested by Shannon, we upper-bound bpb as the crossentropy of a unigram model over a human guess sequence (e.g., 1 1 2 5 17 1 1 . . . ), which records how many guesses it took to identify each subsequent English character, given context. For a 502- character English sequence, a team of four bilinguals working together gave us an upper-bound bpb of 0.51. This team had access to the original Spanish, plus a Google translation. Monolinguals guessing on the same data (minus the Spanish and Google translation) yielded an upper-bound bpb of 1.61. These human-level models indicate that human translators are actually only adding ∼ 32% more information on top of the original, and that our current translation models are only capturing some fraction of this redundancy.7\nFigure 10 shows compression of the entire bilingual corpus, allowing us to compare with the previous state-of-the-art (Sánchez-Martı́nez et al., 2012), which compresses a single, wordinterleaved bilingual corpus. It shows how PPMC\n7Machine models can also generate guess sequences, and we see that entropy of a 30m-character PPMC guess sequence (1.43) upper-bounds actual PPMC bpb (1.28).\ndoes on a concatenated Spanish/English file. Uncompressed English (294.5 Mb) is 90.6% the size of uncompressed Spanish (324.9 Mb). Huffman narrows this gap to 93.0%, and PPM narrows it further to 94.4%, consistent with Behr et al. (2003) and Liberman (2008). Spanish redundancies like adjective-noun agreement and balanced question marks (“¿ . . . ?”) may remain unexploited."
  }, {
    "heading": "7 Conclusion",
    "text": "We have created a bilingual text compression challenge web site.8 This web site contains standard bilingual data, specifies what a valid compression is, and maintains benchmark results.\nThere are many future directions to pursue. First, we would like to develop and exploit better predictive translation modeling. We have so far adapted machine translation technology circa only 1996. For example, the HMM alignment model cannot “cross off” a source word and stop trying to translate it. Also possible are phrase-based translation, neural nets, or as-yet-unanticipated patternfinding algorithms. We only require an executable that prints the bilingual text.\nOur current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text. Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.”\nMore broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana & Lang, unpub. 1995).\nTranslation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas.\n8www.isi.edu/natural-language/compression"
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported by a USC Provost Fellowship and ARO grant W911NF-10-1-0533."
  }],
  "year": 2015,
  "references": [{
    "title": "A two-level structure for compressing aligned bitexts",
    "authors": ["Joaquı́n Adiego", "Nieves R. Brisaboa", "Miguel A. Martı́nez-Prieto", "Felipe Sánchez-Martı́nez"],
    "venue": "In String Processing and Information Retrieval",
    "year": 2009
  }, {
    "title": "Modelling parallel texts for boosting compression",
    "authors": ["Joaquı́n Adiego", "Miguel A. Martı́nez-Prieto", "Javier E. Hoyos-Torı́o", "Felipe Sánchez-Martı́nez"],
    "venue": "In Proc. Data Compression Conference (DCC)",
    "year": 2010
  }, {
    "title": "Statistical machine translation",
    "authors": ["Yaser Al-Onaizan", "Jan Curin", "Michael Jahr", "Kevin Knight", "John Lafferty", "Dan Melamed", "Franz-Josef Och", "David Purdy", "Noah A. Smith", "David Yarowsky."],
    "venue": "Technical Report http://bit.ly/1u9jJsx, Johns Hop-",
    "year": 1999
  }, {
    "title": "Estimating and comparing entropies across written natural languages using PPM compression",
    "authors": ["F. Behr", "Victoria Fossum", "Michael Mitzenmacher", "David Xiao."],
    "venue": "Proc. Data Compression Conference (DCC).",
    "year": 2003
  }, {
    "title": "Native language identification with PPM",
    "authors": ["Victoria Bobicev."],
    "venue": "Proc. NAACL.",
    "year": 2013
  }, {
    "title": "An estimate of an upper bound for the entropy of English",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Robert L. Mercer", "Stephen A Della Pietra", "Jennifer C. Lai."],
    "venue": "Computational Linguistics, 18(1):31–",
    "year": 1992
  }, {
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."],
    "venue": "Computational linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Data compression using adaptive coding and partial string matching",
    "authors": ["John G. Cleary", "Ian Witten."],
    "venue": "IEEE Transactions on Communications, 32(4):396–402.",
    "year": 1984
  }, {
    "title": "Using alignment for multilingual text compression",
    "authors": ["Ehud S. Conley", "Shmuel T. Klein."],
    "venue": "International Journal of Foundations of Computer Science, 19(01):89–101.",
    "year": 2008
  }, {
    "title": "Improved alignment-based algorithm for multilingual text compression",
    "authors": ["Ehud S. Conley", "Shmuel T. Klein."],
    "venue": "Mathematics in Computer Science, 7(2):137–153.",
    "year": 2013
  }, {
    "title": "User-friendly text prediction for translators",
    "authors": ["George Foster", "Philippe Langlais", "Guy Lapalme."],
    "venue": "Proc. EMNLP.",
    "year": 2002
  }, {
    "title": "A method for the construction of minimum redundancy codes",
    "authors": ["David A. Huffman"],
    "venue": "Proc. IRE, 40(9):1098–1101.",
    "year": 1952
  }, {
    "title": "Finding structure via compression",
    "authors": ["Jason L. Hutchens", "Michael D Alder."],
    "venue": "Proc. Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning.",
    "year": 1998
  }, {
    "title": "50,000 Euro prize for compressing human knowledge",
    "authors": ["Marcus Hutter."],
    "venue": "http://prize. hutter1.net. Accessed: 2015-02-04.",
    "year": 2006
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proc. NAACL.",
    "year": 2003
  }, {
    "title": "Moses: Open source toolkit for statistical machine translation",
    "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"],
    "year": 2007
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Proc. MT Summit X.",
    "year": 2005
  }, {
    "title": "Guidelines for word alignment evaluation and manual alignment",
    "authors": ["Patrik Lambert", "Adrià De Gispert", "Rafael Banchs", "José B Mariño."],
    "venue": "Language Resources and Evaluation, 39.",
    "year": 2005
  }, {
    "title": "Stream-based translation models for statistical machine translation",
    "authors": ["Abby Levenberg", "Chris Callison-Burch", "Miles Osborne."],
    "venue": "Proc. NAACL, pages 394–402. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Online EM for unsupervised models",
    "authors": ["Percy Liang", "Dan Klein."],
    "venue": "Proc. HLT-NAACL.",
    "year": 2009
  }, {
    "title": "Is English more efficient than Chinese after all? http://languagelog",
    "authors": ["Mark Liberman."],
    "venue": "ldc.upenn.edu/nll/?p=93. Accessed: 2015-02-04.",
    "year": 2008
  }, {
    "title": "Experiments with a PPM compressionbased method for English-Chinese bilingual sentence alignment",
    "authors": ["Wei Liu", "Zhipeng Chang", "William J. Teahan."],
    "venue": "Statistical Language and Speech Processing, pages 70–81. Springer.",
    "year": 2014
  }, {
    "title": "Adaptive weighting of context models for lossless data compression",
    "authors": ["M.V. Mahoney."],
    "venue": "Technical Report CS-2005-16, Florida Institute of Technology.",
    "year": 2005
  }, {
    "title": "On the use of word alignments to enhance bitext compression",
    "authors": ["M.A. Martı́nez-Prieto", "J. Adiego", "F. SánchezMartı́nez", "P. de la Fuente", "R.C. Carrasco"],
    "venue": "In Proc. Data Compression Conference (DCC)",
    "year": 2009
  }, {
    "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants",
    "authors": ["Radford M. Neal", "Geoffrey E Hinton."],
    "venue": "Learning in Graphical Models, pages 355–368. Springer.",
    "year": 1998
  }, {
    "title": "Compression of parallel texts",
    "authors": ["Craig Nevill", "Timothy Bell."],
    "venue": "Information Processing & Management, 28(6):781–793.",
    "year": 1992
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proc. ACL.",
    "year": 2002
  }, {
    "title": "Adaptive compression-based approach for chinese pinyin input",
    "authors": ["David Martin Powers", "Jin Hu Huang."],
    "venue": "Proc. Third SIGHAN Workshop on Chinese Language Learning.",
    "year": 2004
  }, {
    "title": "Universal modeling and coding",
    "authors": ["Jorma Rissanen", "Glen G Langdon Jr."],
    "venue": "Information Theory, IEEE Transactions on, 27(1):12–23.",
    "year": 1981
  }, {
    "title": "Generalized biwords for bitext compression and translation spotting",
    "authors": ["Felipe Sánchez-Martı́nez", "Rafael C. Carrasco", "Miguel A. Martı́nez-Prieto", "Joaquin Adiego"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2012
  }, {
    "title": "Combining prediction by partial matching and logistic regression for thai word segmentation",
    "authors": ["Ohm Sornil", "Paweena Chaiwanarom."],
    "venue": "Proc. COLING, page 1208. Association for Computational Linguistics.",
    "year": 2004
  }, {
    "title": "A compression-based algorithm for Chinese word segmentation",
    "authors": ["William John Teahan", "Yingying Wen", "Rodger McNab", "Ian H. Witten."],
    "venue": "Computational Linguistics, 26(3):375–393.",
    "year": 2000
  }, {
    "title": "HMM-based word alignment in statistical translation",
    "authors": ["Stephan Vogel", "Hermann Ney", "Christoph Tillmann."],
    "venue": "Proc. COLING.",
    "year": 1996
  }, {
    "title": "Arithmetic coding for data compression",
    "authors": ["Ian H. Witten", "Radford M. Neal", "John G. Cleary."],
    "venue": "Communications of the ACM, 30(6):520–540.",
    "year": 1987
  }],
  "id": "SP:2e406d198004898e19e63fed379d9d8a7a77af26",
  "authors": [{
    "name": "Barret Zoph",
    "affiliations": []
  }, {
    "name": "Marjan Ghazvininejad",
    "affiliations": []
  }, {
    "name": "Kevin Knight",
    "affiliations": []
  }],
  "abstractText": "We ask how much information a human translator adds to an original text, and we provide a bound. We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator? We develop new compression algorithms and establish a benchmark task.",
  "title": "How Much Information Does a Human Translator Add to the Original?"
}