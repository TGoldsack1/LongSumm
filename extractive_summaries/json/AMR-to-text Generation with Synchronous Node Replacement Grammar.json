{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2002\nAMR-to-text Generation with Synchronous Node Replacement Grammar\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea Department of Computer Science, University of Rochester, Rochester, NY 14627\nIBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Singapore University of Technology and Design\nAbstract\nThis paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result."
  }, {
    "heading": "1 Introduction",
    "text": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).\nAMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).\nFlanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.\nWe propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input\n7\nAMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset."
  }, {
    "heading": "2 Synchronous Node Replacement Grammar",
    "text": ""
  }, {
    "heading": "2.1 Grammar Definition",
    "text": "A synchronous node replacement grammar (NRG) is a rewriting formalism: G = 〈N,Σ,∆, P, S〉, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (〈F,E〉,∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪∆ and ∼ denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing Xi with F on the graph. Here we omit defining C and allow arbitrary connections.1 Following Chiang\n1This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is replaced with Xi, nodes previously connected to F are reconnected to Xi\nData: training corpus C Result: rule instances R\n1 R← []; 2 for (Sent,AMR,∼) in C do 3 Rcur ← FRAGMENTEXTRACT(Sent,AMR,∼); 4 for ri in Rcur do 5 R.APPEND(ri) ; 6 for rj in Rcur/{ri} do 7 if ri.CONTAINS(rj) then 8 rij ← ri.COLLAPSE(rj); 9 R.APPEND(rij) ;\n10 end 11 end 12 end 13 end\nAlgorithm 1: Rule extraction\n(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.\nFigure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1, rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2. Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3. Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013)."
  }, {
    "heading": "2.2 Induced Rules",
    "text": "There are three types of rules in our system, namely induced rules, concept rules and graph glue rules. Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus. Shown in Algorithm 1, the first step is to extract a set of initial rules from training 〈sentence, AMR, ∼〉2 pairs (Line 2) using the phrase-to-graph-fragment extraction algorithm of Peng et al. (2015) (Line 3). Here an initial rule\n2∼ denotes alignment between words and AMR labels.\ncontains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj , and generate rij by collapsing ri with rj , if ri contains rj (Line 6-8). Here ri contains rj , if rj .F is a subgraph of ri.F and rj .E is a sub-phrase of ri.E. When collapsing ri with rj , we replace the corresponding subgraph in ri.F with a new non-terminal node, and the sub-phrase in ri.E with the same non-terminal. For example, we obtain rule (b) by collapsing (d) with (a) in Table 1. All initial and generated rules are stored in a rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set."
  }, {
    "heading": "2.3 Concept Rules and Glue Rules",
    "text": "In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of the node concept. A concept rule is used in case no induced rule can cover the node. We refer to the verbalization list3 and AMR guidelines4 for creating more complex concept rules. For example, one concept rule created from the verbalization list is “(k / keep-01 :ARG1 (p / peace)) ||| peacekeeping”.\nInspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied. Three glue rules are defined for each type of edge label. Taking the edge label “ARG0” as an example, we create the following glue rules:\nID. F E r1 (X1 / #X1# :ARG0 (X2 / #X2#)) #X1# #X2# r2 (X1 / #X1# :ARG0 (X2 / #X2#)) #X2# #X1# r3 (X1 / #X1# :ARG0 X1) #X1#\nwhere for both r1 and r2, F contains two nonterminal nodes with a directed edge connecting them, and E is the concatenation the two nonterminals in either the monotonic or the inverse order. For r3, F contains one non-terminal node with a self-pointing edge, and E is the nonterminal. With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph."
  }, {
    "heading": "3 Model",
    "text": "We adopt a log-linear model for scoring search hypotheses. Given an input AMR graph, we find\n3http://amr.isi.edu/download/lists/verbalization-listv1.06.txt\n4https://github.com/amrisi/amr-guidelines\nthe highest scored derivation t∗ from all possible derivations t:\nt∗ = argmax t\nexp ∑\ni\nwifi(g, t), (1)\nwhere g denotes the input AMR, fi(·, ·) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).\nWe perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam."
  }, {
    "heading": "3.1 Translation Probabilities",
    "text": "Production rules serve as a basis for scoring hypotheses. We associate each synchronous NRG rule n → (〈F,E〉,∼) with a set of probabilities. First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where c〈F,E〉 is the fractional count of 〈F,E〉.\np(F |E) = c〈F,E〉∑ F ′ c〈F ′,E〉\n(2)\nIn addition, lexicalized translation probabilities are defined as:\npw(F |E) = ∏\nl∈F\n∑ w∈E p(l|w) (3)\nHere l is a label (including both edge labels such as “ARG0” and concept labels such as “want-01”) in the AMR fragment F , and w is a word in the phrase E. Equation 3 can be regarded as a “soft” version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule. In addition to p(F |E) and pw(F |E), we use features in the reverse direction, namely p(E|F ) and pw(E|F ), the definitions of which are omitted as they are consistent with\nEquations 2 and 3, respectively. The probabilities associated with concept rules and glue rules are manually set to 0.0001."
  }, {
    "heading": "3.2 Reordering Model",
    "text": "Although the word order is defined for induced rules, it is not the case for glue rules. We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label. The probabilistic model using smoothed counts is defined as:\np(M |h, l, t) = 1.0 + ∑ h ∑ t c(h, l, t,M)\n2.0 + ∑ o∈{M,I} ∑ h ∑ t c(h, l, t, o) (4)\nc(h, l, t,M) is the count of monotonic translations of head h and tail t, connected by edge l."
  }, {
    "heading": "3.3 Moving Distance",
    "text": "The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Setup",
    "text": "We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50.\nWe investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate\nexisting translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output. We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset."
  }, {
    "heading": "4.2 Main results",
    "text": "The results are shown in Table 2. First, All outperforms all baselines. NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system. On the other hand, NoConceptRule does not lead to much performance drop. This observation is consistent with the observation of Song et al. (2016) for their TSP-based system. NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close. Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules. Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62, showing the advantage of our model. To our knowledge, this is the best result reported so far on the task."
  }, {
    "heading": "4.3 Grammar analysis",
    "text": "We have shown the effectiveness of our synchronous node replacement grammar (SNRG) on the AMR-to-text generation task. Here we further analyze our grammar as it is relatively less studied than the hyperedge replacement grammar (HRG) (Drewes et al., 1997).\nStatistics on the whole rule set We first categorize our rule set by the number of terminals and nonterminals in the AMR fragment F , and show the percentages of each type in Figure 3. Each rule contains at most 1 nonterminal, as we collapse each initial rule only once. First\nof all, the percentage of rules containing nonterminals are much more than those without nonterminals, as we collapse each pair of initial rules (in Algorithm 1) and the results can be quadratic the number of initial rules. In addition, most rules are small containing 1 to 3 terminals, meaning that they represent small pieces of meaning and are easier to matched on a new AMR graph. Finally, there are a few large rules, which represent complex meaning.\nStatistics on the rules used for decoding In addition, we collect the rules that our well-tuned system used for generating the 1-best output on the testset, and categorize them into 3 types: (1) glue rules, (2) nonterminal rules, which are not glue rules but contain nonterminals on the righthand side and (3) terminal rules, whose right-hand side only contain terminals. Over the rules used on the 1-best result, more than 30% are non-terminal rules, showing that the induced rules play an important role. On the other hand, 30% are glue rules. The reason is that the data sparsity for graph grammars is more severe than string-based grammars (such as CFG), as the graph structures are more complex than strings. Finally, terminal rules take the largest percentage, while most are induced rules, but not concept rules.\nRule examples Finally, we show some rules in Table 4, where F and E are the right-hand-side AMR fragment and phrase, respectively. For the first rule, the root of F is a verb (“give-01”) whose subject is a nonterminal and object is a AMR fragment “(p / person :ARG0-of (u / use-01))”, which means “user”. So it is easy to see that the corresponding phrase E conveys the same meaning. For the second rule, “(s3 / stay-01 :accompanier (i / i))” means “stay\nwith me”, which is also covered by its phrase."
  }, {
    "heading": "4.4 Generation example",
    "text": "Finally, we show an example in Table 5, where the top is the input AMR graph, and the bottom is the generation result. Generally, most of the meaning of the input AMR are correctly translated, such as “:example”, which means “such as”, and “thing”, which is an abstract concept and should not be translated, while there are a few errors, such as “that” in the result should be “what”, and there should be an “in” between “tmt” and “fairfax”."
  }, {
    "heading": "5 Conclusion",
    "text": "We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time. Our method performs better than the previous systems, empirically proving the advantages of our graph-to-string rules."
  }, {
    "heading": "Acknowledgement",
    "text": "This work was funded by a Google Faculty Research Award. Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education."
  }],
  "year": 2017,
  "references": [{
    "title": "Broad-coverage CCG semantic parsing with AMR",
    "authors": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-15). pages 1699– 1710.",
    "year": 2015
  }, {
    "title": "Abstract meaning representation for sembanking",
    "authors": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."],
    "venue": "Proceedings of the 7th Linguis-",
    "year": 2013
  }, {
    "title": "A hierarchical phrase-based model for statistical machine translation",
    "authors": ["David Chiang."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). Ann Arbor, Michigan, pages 263–270.",
    "year": 2005
  }, {
    "title": "Hyperedge replacement, graph grammars",
    "authors": ["Frank Drewes", "Hans-Jörg Kreowski", "Annegret Habel."],
    "venue": "Handbook of Graph Grammars 1:95–162.",
    "year": 1997
  }, {
    "title": "Node replacement graph grammars",
    "authors": ["J. Engelfriet", "G. Rozenberg."],
    "venue": "Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation, World Scientific Publishing Co., Inc., River Edge, NJ, USA, pages 1–94.",
    "year": 1997
  }, {
    "title": "Generation from abstract meaning representation using tree transducers",
    "authors": ["Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell."],
    "venue": "Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational",
    "year": 2016
  }, {
    "title": "A discriminative graph-based parser for the abstract meaning representation",
    "authors": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-",
    "year": 2014
  }, {
    "title": "Loosely tree-based alignment for machine translation",
    "authors": ["Daniel Gildea."],
    "venue": "Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03). Sapporo, Japan, pages 80–87.",
    "year": 2003
  }, {
    "title": "Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing",
    "authors": ["James Goodman", "Andreas Vlachos", "Jason Naradowsky."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational",
    "year": 2016
  }, {
    "title": "Graph parsing with s-graph grammars",
    "authors": ["Jonas Groschwitz", "Alexander Koller", "Christoph Teichmann."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15). Beijing, China, pages 1481–1490.",
    "year": 2015
  }, {
    "title": "Statistical syntax-directed translation with extended domain of locality",
    "authors": ["Liang Huang", "Kevin Knight", "Aravind Joshi."],
    "venue": "Proceedings of Association for Machine Translation in the Americas (AMTA2006). pages 66–73.",
    "year": 2006
  }, {
    "title": "Semantics-based machine translation with hyperedge replacement grammars",
    "authors": ["Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight."],
    "venue": "Proceedings of the International Conference on Computational",
    "year": 2012
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03). pages 48–54.",
    "year": 2003
  }, {
    "title": "Improving event detection with abstract meaning representation",
    "authors": ["Xiang Li", "Thien Huu Nguyen", "Kai Cao", "Ralph Grishman."],
    "venue": "Proceedings of the First Workshop on Computing News Storylines. Beijing, China, pages 11–15.",
    "year": 2015
  }, {
    "title": "Treeto-string alignment template for statistical machine translation",
    "authors": ["Yang Liu", "Qun Liu", "Shouxun Lin."],
    "venue": "Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL-06). Sydney, Australia, pages 609–616.",
    "year": 2006
  }, {
    "title": "Translation with source constituency and dependency trees",
    "authors": ["Fandong Meng", "Jun Xie", "Linfeng Song", "Yajuan Lü", "Qun Liu."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-13). Seattle, Washington, USA, pages",
    "year": 2013
  }, {
    "title": "Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning",
    "authors": ["Arindam Mitra", "Chitta Baral."],
    "venue": "Proceedings of the National Conference on Artificial Intelligence (AAAI-16).",
    "year": 2015
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["Franz Josef Och."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03). Sapporo, Japan, pages 160–167.",
    "year": 2003
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02). pages 311–318.",
    "year": 2002
  }, {
    "title": "A synchronous hyperedge replacement grammar based approach for AMR parsing",
    "authors": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."],
    "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15).",
    "year": 2015
  }, {
    "title": "Addressing the data sparsity issue in neural amr parsing",
    "authors": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."],
    "venue": "Proceedings of the 12",
    "year": 2017
  }, {
    "title": "A decoder for syntax-based statistical MT",
    "authors": ["Kenji Yamada", "Kevin Knight."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02). Philadelphia, Pennsylvania, USA, pages 303–310.",
    "year": 2002
  }, {
    "title": "AMR parsing with an incremental joint model",
    "authors": ["Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Austin, Texas, pages 680–689.",
    "year": 2016
  }],
  "id": "SP:da4ba4383295a23eff2a5324aee28c5879780564",
  "authors": [{
    "name": "Linfeng Song",
    "affiliations": []
  }, {
    "name": "Xiaochang Peng",
    "affiliations": []
  }, {
    "name": "Yue Zhang",
    "affiliations": []
  }, {
    "name": "Zhiguo Wang",
    "affiliations": []
  }, {
    "name": "Daniel Gildea",
    "affiliations": []
  }],
  "abstractText": "This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.",
  "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar"
}