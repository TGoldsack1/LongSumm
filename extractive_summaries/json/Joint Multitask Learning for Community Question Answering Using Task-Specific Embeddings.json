{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4196–4207 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4196"
  }, {
    "heading": "1 Introduction and Motivation",
    "text": "Question answering web forums such as StackOverflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question–comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content.\nWithin community Question Answering (cQA) forums, two subtasks are of special relevance when a user poses a new question to the website (Hoogeveen et al., 2018; Lai et al., 2018): (i) finding similar questions (question-question relatedness), and (ii) finding relevant answers to the new question, if they already exist (answer selection).\n∗Work conducted while this author was at QCRI, HBKU.\nBoth subtasks have been the focus of recent research as they result in end-user applications. The former is interesting for a user who wants to explore the space of similar questions in the forum and to decide whether to post a new question. It can also be relevant for the forum owners as it can help detect redundancy, eliminate question duplicates, and improve the overall forum structure. Subtask (ii) on the other hand is useful for a user who just wants a quick answer to a specific question, without the need of digging through the long answer threads and winnowing good from bad comments or without having to post a question and then wait for an answer.\nObviously, the two subtasks are interrelated as the information needed to answer a new question is usually found in the threads of highly related questions. Here, we focus on jointly solving the two subtasks with the help of yet another related subtask, i.e., determining whether a comment within a question-comment thread is a good answer to the question heading that thread.\nAn example is shown in Figure 1. A new question q is posed for which several potentially related questions are identified in the forum (e.g., by using an information retrieval system); qi in the example is one of these existing questions. Each retrieved question comes with an associated thread of comments; cim represents one comment from the thread of question qi. Here, cim is a good answer for qi, qi is indeed a question related to q, and consequently cim is a relevant answer for the new question q. This is the setting of SemEval-2016 Task 3, and we use its benchmark datasets.\nOur approach has two steps. First, a deep neural network (DNN) in the form of a feed-forward neural network is trained to solve each of the three subtasks separately, and the subtask-specific hidden layer activations are taken as embedded feature representations to be used in the second step.\nThen, a conditional random field (CRF) model uses these embeddings and performs joint learning with global inference to exploit the dependencies between the subtasks.\nA key strength of DNNs is their ability to learn nonlinear interactions between underlying features through specifically-designed hidden layers, and also to learn the features (e.g., vectors for words and documents) automatically. This capability has led to gains in many unstructured output problems. DNNs are also powerful for structured output problems. Previous work has mostly relied on recurrent or recursive architectures to propagate information through hidden layers, but has been disregarding the modeling strength of structured conditional models, which use global inference to model consistency in the output structure (i.e., the class labels of all nodes in a graph). In this work, we explore the idea that combining simple DNNs with structured conditional models can be an effective and efficient approach for cQA subtasks that offers the best of both worlds.\nOur experimental results show that: (i) DNNs already perform very well on the questionquestion similarity and answer selection subtasks; (ii) strong dependencies exist between the subtasks under study, especially answer-goodness and question-question-relatedness influence answerselection significantly; (iii) the CRFs exploit the dependencies between subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF."
  }, {
    "heading": "2 Related Work",
    "text": "Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddings, and we defer the joint learning with global inference to the structured model.\nFrom the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these models, we use DNNs to induce taskspecific embeddings, and, more importantly, we perform multitask learning of three different cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the\ndatasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SUPER TEAM (Mihaylova et al., 2016) and KELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features.\nIn work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model. Moreover, they focus on one task only, while we use multitask learning. Bonadiman et al. (2017) proposed a multitask neural architecture where the three tasks are trained together with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do.\nThe general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. Huang et al. (2015) proposed an LSTM-CRF framework for such tasks. Ma and Hovy (2016) included a CNN in the framework to compute word representations from character-level embeddings. While these studies consider tasks related to constituents in a sentence, e.g., words and phrases, we focus on methods to represent comments and to model dependencies between comment-level tags. We also experiment with arbitrary graph structures in our CRF model to model dependencies at different levels."
  }, {
    "heading": "3 Learning Approach",
    "text": "Let q be a newly-posed question, and cim denote the m-th comment (m ∈ {1, 2, . . . ,M}) in the\nanswer thread for the i-th potentially related question qi (i ∈ {1, 2, . . . , I}) retrieved from the forum. We can define three cQA subtasks: (A) classify each comment cim in the thread for question qi as Good vs. Bad with respect to qi; (B) determine, for each retrieved question qi, whether it is Related to the new question q in the sense that a good answer to qi might also be a good answer to q; and finally, (C) classify each comment cim in each answer thread as either Relevant or Irrelevant with respect to the new question q.\nLet yai,m ∈ {Good,Bad}, ybi ∈ {Related, Notrelated}, and yci,m ∈ {Relevant, Irrelevant} denote the corresponding output labels for subtasks A, B, and C, respectively. As argued before, subtask C depends on the other two subtasks. Intuitively, if cim is a good comment with respect to the existing question qi, and qi is related to the new question q (subtask A), then cim is likely to be a relevant answer to q. Similarly, subtask B can benefit from subtask C: if comment cim in the answer thread of qi is relevant with respect to q, then qi is likely to be related to q.\nWe propose to exploit these inherent correlations between the cQA subtasks as follows: (i) by modeling their interactions in the input representations, i.e., in the feature space of (q, qi, cim), and more importantly, (ii) by capturing the dependencies between the output variables (yai,m, y b i , y c i,m). Moreover, we cast each cQA subtask as a structured prediction problem in order to model the dependencies between output variables of the same type. Our intuition is that if two comments cim and cin in the same thread are similar, then they are likely to have the same labels for both subtask A and subtask C, i.e., yai,m ≈ yai,n, and yci,m ≈ yci,n. Similarly, if two pre-existing questions qi and qj are similar, they are also likely to have the same labels, i.e., ybi ≈ ybj .\nOur framework works in two steps. First, we use a DNN, specifically, a feed-forward NN, to learn task-specific embeddings for the three subtasks, i.e., output embeddings xai,m, x b i and x c i,m for subtasks A, B and C (Figure 2a). The DNN uses syntactic and semantic embeddings of the input elements, their interactions, and other similarity features between them and, as a by-product, learns the output embeddings for each subtask.\nIn the second step, a structured conditional model operates on subtask-specific embeddings from the DNNs and captures the dependencies between the\nsubtasks, between existing questions, and between comments for an existing question (Figure 2b). Below, we describe the two steps in detail."
  }, {
    "heading": "3.1 Neural Models for cQA Subtasks",
    "text": "Figure 2a depicts our complete neural framework for the three subtasks. The input is a tuple (q, qi, c i m) consisting of a new question q, a retrieved question qi, and a comment cim from qi’s answer thread. We first map the input elements to fixed-length vectors (zq, zqi , zcim) using their syntactic and semantic embeddings. Depending on the requirements of the subtasks, the network then models the interactions between the inputs by passing their embeddings through non-linear hidden layers ν(·). Additionally, the network also considers pairwise similarity features φ(·) between two input elements that go directly to the output layer, and also through the last hidden layer. The pairwise features together with the activations at the final hidden layer constitute the task-specific embeddings for each subtask t: xti = [ν\nt(·), φt(·)]. The final layer defines a Bernoulli distribution for each subtask t ∈ {a, b, c}:\np(yti |q, qi, cim, θ) = Ber(yti | sig(wTt xti)) (1)\nwhere xti, wt, and y t i are the task-specific embedding, the output layer weights, and the prediction variable for subtask t, respectively, and sig(·) refers to the sigmoid function. We train the models by minimizing the crossentropy between the predicted distribution and the\ngold labels. The main difference between the models is how they compute the task-specific embeddings xti for subtask t.\nNeural Model for Subtask A. The feedforward network for subtask A is shown in the lower part of Figure 2a. To determine whether a comment cim is good with respect to the thread question qi, we model the interactions between cim and qi by merging their embeddings zcim and zqi , and passing them through a hidden layer:\nha1 = f(U a[zqi , zcim ]) (2)\nwhere Ua is the weight matrix from the inputs to the first hidden units, f is a non-linear activation function. The activations are then fed to a final subtask-specific hidden layer, which combines these signals with the pairwise similarity features φa(qi, c i m). Formally,\nha2 = f(V a[ha1, φ a(qi, c i m)]) (3)\nwhere V a is the weight matrix. The task-specific output embedding is formed by merging ha2 and φa(qi, c i m); x a i,m = [h a 2, φ a(qi, c i m)].\nNeural Model for Subtask B. To determine whether an existing question qi is related to the new question q, we model the interactions between q and qi using their embeddings and pairwise similarity features similarly to subtask A. The upper part of Figure 2a shows the network. The transformation is defined as follows:\nhb1 = f(U b[zq, zqi ]);h b 2 = f(V b[hb1, φ b(q, qi)])\nwhere U b and V b are the weight matrices in the first and second hidden layer. The task-specific embedding is formed by xbi = [h b 2, φ b(q, qi)].\nNeural Model for Subtask C. The network for subtask C is shown in the middle of Figure 2a. To decide if a comment cim in the thread of qi is relevant to q, we consider how related qi is to q, and how useful cim is to answer qi. Again, we model the direct interactions between q and cim using pairwise features φc(q, cim) and a hidden layer transformation hc1 = f(U c[zq, zcim ]), where U c is a weight matrix. We then include a second hidden layer to combine the activations from different inputs and pairwise similarity features. Formally,\nhc2 = f(V c[ha1 ,h b 1,h c 1, φ a(qi, c i m), φ b(q, qi), φ c(q, cim)])\nThe final task-specific embedding for subtask C is formed as xci = [hc2, φa(qi, cim), φb(q, qi), φc(q, cim)]."
  }, {
    "heading": "3.2 Joint Learning with Global Inference",
    "text": "One simple way to exploit the interdependencies between the subtask-specific embeddings (xai,m, xbi , x c i,m) is to precompute the predictions for some subtasks (A and B), and then to use the predictions as features for the other subtask (C). However, as shown later in Section 6, such a pipeline approach propagates errors from one subtask to the subsequent ones. A more robust way is to build a joint model for all subtasks.\nWe could use the full DNN network in Figure 2a to learn the classification functions for the three subtasks jointly as follows:\np(yai,m, y b i , y c i,m|θ) = p(yai,m|θa)p(ybi |θb)p(yci,m|θc) (4)\nwhere θ = [θa, θb, θc] are the model parameters. However, this has two key limitations: (i) it assumes conditional independence between the subtasks given the parameters; (ii) the scores are normalized locally, which leads to the so-called label bias problem (Lafferty et al., 2001), i.e., the features for one subtask would have no influence on the other subtasks.\nThus, we model the dependencies between the output variables by learning (globally normalized) node and edge factor functions that jointly optimize a global performance criterion. In particular, we represent the cQA setting as a large undirected graph G=(V,E)=(Va∪Vb∪Vc, Eaa∪Ebb∪Ecc∪Eac∪Ebc∪Eab). As shown in Figure 2b, the graph contains six subgraphs:\nGa=(Va, Eaa), Gb=(Vb, Ebb) and Gc=(Vc, Ecc) are associated with the three subtasks, while the bipartite subgraphs Gac=(Va ∪ Vc, Eac), Gbc=(Vb ∪ Vc, Ebc) and Gab=(Va ∪ Vb, Eab) connect nodes across tasks.\nWe associate each node u ∈ Vt with an input vector xu, representing the embedding for subtask t, and an output variable yu, representing the class label for subtask t. Similarly, each edge (u, v) ∈ Est is associated with an input feature vector µ(xu,xv), derived from the node-level features, and an output variable yuv ∈ {1, 2, · · · , L}, representing the state transitions for the pair of nodes.1 For notational simplicity, here we do not distinguish between comment and question nodes, rather we use u and v as general indices. We define the following joint conditional distribution:\np(y|θ,x) = 1 Z(θ,x) ∏ t∈τ [ ∏ u∈Vt ψn(yu|x,wtn) ]\n∏ (s,t)∈τ×τ [ ∏ (u,v)∈Est ψe(yuv|x,wste ) ]\n(5)\nwhere τ = {a, b, c}, ψn(·) and ψe(·) are node and edge factors, respectively, andZ(·) is a global normalization constant. We use log-linear factors:\nψn(yu|x,wtn) = exp(σ(yu,x)Twtn) (6) ψe(yuv|x,wste ) = exp(σ(yuv,x)Twste ) (7)\nwhere σ(·) is a feature vector derived from the inputs and the labels.\nThis model is essentially a pairwise conditional random field (Murphy, 2012). The global normalization allows CRFs to surmount the label bias problem, allowing them to take long-range interactions into account. The objective in Equation 5 is a convex function, and thus we can use gradientbased methods to find the global optimum. The gradients have the following form:\nf ′(wtn) = ∑ u∈Vt σ(yu,x)− E[σ(yu,x)] (8)\nf ′(wste ) = ∑\n(u,v)∈Est\nσ(yuv,x)− E[σ(yuv,x)] (9)\nwhere E[φ(·)] is the expected feature vector.\nTraining and Inference. Traditionally, CRFs have been trained using offline methods like LBFGS (Murphy, 2012). Online training using\n1To avoid visual clutter, the input features and the output variables for the edges are not shown in Figure 2b.\nfirst-order methods such as stochastic gradient descent was proposed by Vishwanathan et al. (2006). Since our DNNs are trained with the RMSprop online adaptive algorithm (Tieleman and Hinton, 2012), in order to compare our two models, we use RMSprop to train our CRFs as well.\nFor our CRF models, we use Belief Propagation, or BP, (Pearl, 1988) for inference. BP converges to an exact solution for trees. However, exact inference is intractable for graphs with loops. Despite this, Pearl (1988) advocated for the use of BP in loopy graphs as an approximation. Even though BP only gives approximate solutions, it often works well in practice for loopy graphs (Murphy et al., 1999), outperforming other methods such as mean field (Weiss, 2001).\nVariations of Graph Structures. A crucial advantage of our CRFs is that we can use arbitrary graph structures, which allows us to capture dependencies between different types of variables: (i) intra-subtask, for variables of the same subtask, e.g., ybi and y b j in Figure 2b, and (ii) acrosssubtask, for variables of different subtasks. For intra-subtask, we explore null (i.e., no connection between nodes) and fully-connected relations. For subtasks A and C, the intra-subtask connections are restricted to the nodes inside a thread, e.g., we do not connect yci,m and y c j,m in Figure 2b.\nFor across-subtask, we explored three types of connections depending on the subtasks involved: (i) null or no connection between subtasks, (ii) 1:1 connection for A-C, where the corresponding nodes of the two subtasks in a thread are connected, e.g., yai,m and y c i,m in Figure 2b, and (iii) M:1 connection to B, where we connect all the nodes of C or A to the thread-level B node. Each configuration of intra- and acrossconnections yields a different CRF model. Figure 2b shows one such model for two threads each containing two comments, where all subtasks have fully-connected intra-subtask links, 1:1 connection for A-C, and M:1 for C-B and A-B."
  }, {
    "heading": "4 Features for the DNN Models",
    "text": "We have two types of features: (i) input embeddings, for q, qi and cim, and (ii) pairwise features, for (q, qi), (q, cim), and (qi, c i m) — see Figure 2a."
  }, {
    "heading": "4.1 Input Embeddings",
    "text": "We use three types of pre-trained vectors to represent a question (q or qi) or a comment (cim):\nGOOGLE VECTORS. 300-dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The embedding for a question (or comment) is the average of the word embeddings it is composed of.\nSYNTAX. We parse the question (or comment) using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector produced internally as a by-product of parsing.\nQL VECTORS. We use fine-tuned word embeddings pretrained on all the available in-domain Qatar Living data (Mihaylov and Nakov, 2016)."
  }, {
    "heading": "4.2 Pairwise Features",
    "text": "We extract pairwise features for each of (q, qi), (q, cim), and (qi, c i m) pairs. These include:\nCOSINES. We compute cosines using the above vectors: cos(q, qi), cos(q, cim) and cos(qi, c i m).\nMT FEATURES. We use the following machine translation evaluation metrics: (1) BLEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover et al., 2006); (4) METEOR v1.4 (Lavie and Denkowski, 2009); (5) Unigram PRECISION; (6) Unigram RECALL.\nBLEU COMPONENTS. We further use various components involved in the computation of BLEU:2 n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty.\nQUESTION-COMMENT RATIO. (1) questionto-comment count ratio in terms of sentences/tokens/nouns/verbs/adjectives/adverbs/pronouns; (2) question-to-comment count ratio of words that are not in WORD2VEC’s Google News vocabulary."
  }, {
    "heading": "4.3 Node Features",
    "text": "COMMENT FEATURES. These include number of (1) nouns/verbs/adjectives/adverbs/pronouns, (2) URLs/images/emails/phone numbers, (3) tokens/sentences, (4) positive/negative smileys, (5) single/double/triple exclamation/interrogation symbols, (6) interrogative sentences, (7) ‘thank’ mentions, (8) words that are not in WORD2VEC’s Google News vocabulary. Also, (9) average number of tokens, and (10) word type-to-token ratio.\nMETA FEATURES. (1) is the person answering the question the one who asked it; (2) reciprocal rank of comment cim in the thread of qi, i.e., 1/m;\n2BLEU FEATURES and BLEU COMPONENTS (Guzmán et al., 2016a,b) are ported from an MT evaluation framework (Guzmán et al., 2015; Guzmán et al., 2017) to cQA.\n(3) reciprocal rank of cim in the list of comments for q, i.e., 1/[m+10×(i − 1)]; and (4) reciprocal rank of question qi in the list for q, i.e., 1/i."
  }, {
    "heading": "5 Data and Settings",
    "text": "We experiment with the data from SemEval-2016 Task 3 (Nakov et al., 2016b). Consistently with our notation from Section 3, it features three subtasks: subtask A (i.e., whether a comment cim is a good answer to the question qi in the thread), subtask B (i.e., whether the retrieved question qi is related to the new question q), and subtask C (i.e., whether the comment cim is a relevant answer for the new question q). Note that the two main subtasks we are interested in are B and C. DNN Setting. We preprocess the data using min-max scaling. We use RMSprop3 for learning, with parameters set to the values suggested by Tieleman and Hinton (2012). We use up to 100 epochs with patience of 25, rectified linear units (ReLU) as activation functions, l2 regularization on weights, and dropout (Srivastava et al., 2014) of hidden units. See Table 1 for more detail. CRF Setting. For the CRF model, we initialize the node-level weights from the output layer weights of the DNNs, and we set the edge-level weights to 0. Then, we train using RMSprop with loopy BP. We regularize the node parameters according to the best settings of the DNN: 0.001, 0.05, and 0.0001 for A, B, and C, respectively."
  }, {
    "heading": "6 Results and Discussion",
    "text": "Below, we first present the evaluation results using DNN models (Section 6.1). Then, we discuss the performance of the joint models (Section 6.2)."
  }, {
    "heading": "6.1 Results for the DNN Models",
    "text": "Table 2 shows the results for our individual DNN models (rows in boldface) for subtasks A, B and C on the TEST set. We report three ranking-based measures that are commonly accepted in the IR community: mean average precision (MAP), which was the official\n3Other adaptive algorithms such as ADAM (Kingma and Ba, 2014) or ADADELTA (Zeiler, 2012) were slightly worse.\nevaluation measure of SemEval-2016, average recall (AvgRec), and mean reciprocal rank (MRR).\nFor each subtask, we show two baselines and the results of the top-2 systems at SemEval. The first baseline is a random ordering of the questions/comments, assuming no knowledge about the subtask. The second baseline keeps the chronological order of the comments for subtask A, of the question ranking from the IR engine for subtask B, and both for subtask C.\nWe can see that the individual DNN models for subtasks B and C are very competitive, falling between the first and the second best at SemEval2016. For subtask A, our model is weaker, but, as we will see below, it can help improve the results for subtasks B and C, which are our focus here.\nLooking at the results for subtask C, we can see that sizeable gains are possible when using gold labels for subtasks A and B as features to DNNC , e.g., adding gold A labels yields +6.90 MAP points. Similarly, using gold labels for subtask B adds +2.05 MAP points absolute. Moreover, the gain is cumulative: using the two gold labels together\nyields +9.25 MAP points. The same behavior is observed for the other evaluation measures. Of course, as we use gold labels, this is an upper bound on performance, but it justifies our efforts towards a joint multitask learning model."
  }, {
    "heading": "6.2 Results for the Joint Model",
    "text": "Below we discuss the evaluation results for the joint model. We focus on subtasks B and C, which are the main target of our study.\nResults for Subtask C. Table 3 compares several variants of the CRF model for joint learning, which we described in Section 3.2 above.\nRow 1 shows the results for our individual DNNC model. The following rows 2–4 present a pipeline approach, where we first predict labels for subtasks A and B and then we add these predictions as features to DNNC . This is prone to error propagation, and improvements are moderate and inconsistent across the evaluation measures.\nThe remaining rows correspond to variants of our CRF model with different graph structures. Overall, the improvements over DNNC are more sizeable than for the pipeline approach (with one single exception out of 24 cases); they are also more consistent across the evaluation measures, and the improvements in MAP over the baseline range from +0.96 to +1.76 points absolute.\nRows 5–8 show the impact of adding connections to subtasks A and B when solving subtask C (see Figure 2b). Interestingly, we observe the same pattern as with the gold labels: the A-C and B-C connections help individually and in combination, with A-C being more helpful. Yet, further adding A-B does not improve the results (row 8).\nNote that the locally normalized joint model in Eq. 4 yields much lower results than the globally normalized CRFall (row 8): 54.32, 59.87, and 61.76 in MAP, AvgRec and MRR (figures not included in the table for brevity). This evinces the problems with the conditional independence assumption and the local normalization in the model.\nFinally, rows 9–12 explore variants of the best system from the previous set (row 7), which has connections between subtasks only. Rows 9–12 show the results when using subgraphs for A, B and C that are fully connected (i.e., for all pairs). We can see that none of these variants yields improvements over the model from row 7, i.e., the fine-grained relations between comments in the threads and between the different related questions\ndo not seem to help solve subtask C in the joint model. Note that our scores from row 7 are better than the best results achieved by a system at SemEval-2016 Task 3 subtask C: 56.00 vs. 55.41 on MAP, and 63.25 vs. 61.48 on MRR.\nResults for Subtask B. Next, we present in Table 4 similar experiments, but this time with subtask B as the target, and we show some more measures (accuracy, precision, recall, and F1).\nGiven the insights from Table 2 (where we used gold labels), we did not expect to see much improvements for subtask B. Indeed, as rows 2–4 show, using the pipeline approach, the IR measures are basically unaltered. However, classification accuracy improves by almost one point absolute, recall is also higher (trading for lower precision), and F1 is better by a sizeable margin.\nComing to the joint models (rows 6–9), we can see that the IR measures improve consistently over the pipeline approach, even though not by much. The effect on accuracy-P-R-F1 is the same as observed with the pipeline approach but with larger differences.4 In particular, accuracy improves by more than two points absolute, and recall increases, which boosts F1 to almost 60.\nRow 5 is a special case where we only consider subtask B, but we do the learning and the inference over the set of ten related questions, exploiting their relations. This yields a slight increase in all measures; more importantly, it is crucial for obtaining better results with the joint models.\nRows 6–9 show results for various variants of the A-C and B-C architecture with fully connected B nodes, playing with the fine-grained connection of the A and C nodes. The best results are in this block, with increases over DNNB in MAP (+0.61), AvgRec (+0.69) and MRR (+1.05), and especially in accuracy (+2.18) and F1 (+11.25 points). This is remarkable given the low expectation we had about improving subtask B.\nNote that the best architecture for subtask C from Table 3 (A-C and B-C with no fully connected B layer) does not yield good results for subtask B. We speculate that subtask B is overlooked by the architecture, which has many more connections and parameters on the nodes for subtasks A and C (ten comments are to be classified for both subtask\n4Note that we have a classification approach, which favors accuracy-P-R-F1; if we want to improve the ranking measures, we should optimize for them directly.\nA and C, while only one decision is to be made for the related question B).\nFinally, note that our best results for subtask B are also slightly better than those for the best system at SemEval-2016 Task 3, especially on MRR."
  }, {
    "heading": "7 Conclusion",
    "text": "We have presented a framework for multitask learning of two community Question Answering problems: question-question relatedness and answer selection. We further used a third, auxiliary one, i.e., finding the good comments in a question-comment thread. We proposed a twostep framework based on deep neural networks and structured conditional models, with a feedforward neural network to learn task-specific embeddings, which are then used in a pairwise CRF as part of a multitask model for all three subtasks.\nThe DNN model has its strength in generating compact embedded representations for the subtasks by modeling interactions between different input elements. On the other hand, the CRF is able to perform global inference over arbitrary graph structures accounting for the dependencies between subtasks to provide globally good solutions. The experi-\nmental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results.\nIn future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction."
  }, {
    "heading": "Acknowledgments",
    "text": "The first author would like to thank the funding support from MOE Tier-1."
  }],
  "year": 2018,
  "references": [{
    "title": "PMI-cool at SemEval-2016 Task 3: Experiments with PMI and goodness polarity lexicons for community question answering",
    "authors": ["Daniel Balchev", "Yasen Kiprov", "Ivan Koychev", "Preslav Nakov."],
    "venue": "Proceedings of the 10th International Workshop on",
    "year": 2016
  }, {
    "title": "Threadlevel information for comment classification in community question answering",
    "authors": ["Alberto Barrón-Cedeño", "Simone Filice", "Giovanni Da San Martino", "Shafiq Joty", "Lluís Màrquez", "Preslav Nakov", "Alessandro Moschitti."],
    "venue": "Proceedings of the",
    "year": 2015
  }, {
    "title": "Effective shared representations with multitask learning for community question answering",
    "authors": ["Daniele Bonadiman", "Antonio Uva", "Alessandro Moschitti."],
    "venue": "Proceedings of the Conference of the European Chapter of the Association for Computa-",
    "year": 2017
  }, {
    "title": "An exploration of data augmentation and RNN architectures for question ranking in community question answering",
    "authors": ["Charles Chen", "Razvan Bunescu."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJC-",
    "year": 2017
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "The Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Cross-language question re-ranking",
    "authors": ["Giovanni Da San Martino", "Salvatore Romeo", "Alberto Barrón-Cedeño", "Shafiq Joty", "Lluís Màrquez", "Alessandro Moschitti", "Preslav Nakov."],
    "venue": "Proceedings of the 40th International ACM SIGIR Conference",
    "year": 2017
  }, {
    "title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics",
    "authors": ["George Doddington."],
    "venue": "Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02, pages 138–145, San",
    "year": 2002
  }, {
    "title": "KeLP at SemEval-2016 Task 3: Learning semantic relations between questions and answers",
    "authors": ["Simone Filice", "Danilo Croce", "Alessandro Moschitti", "Roberto Basili."],
    "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation, Se-",
    "year": 2016
  }, {
    "title": "Pairwise neural machine",
    "authors": ["Francisco Guzmán", "Shafiq Joty", "Lluís Màrquez", "Preslav Nakov"],
    "year": 2015
  }, {
    "title": "Machine translation evaluation with neural networks",
    "authors": ["Francisco Guzmán", "Shafiq R. Joty", "Lluís Màrquez", "Preslav Nakov."],
    "venue": "Computer Speech & Language, 45:180–200.",
    "year": 2017
  }, {
    "title": "Machine translation evaluation meets community question answering",
    "authors": ["Francisco Guzmán", "Lluís Màrquez", "Preslav Nakov."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL ’16, pages 460–466,",
    "year": 2016
  }, {
    "title": "2016b. MTE-NN at SemEval-2016 Task 3: Can machine translation evaluation help community question answering",
    "authors": ["Francisco Guzmán", "Preslav Nakov", "Lluís Màrquez"],
    "venue": "In Proceedings of the International Workshop on Semantic Evaluation,",
    "year": 2016
  }, {
    "title": "Web forum retrieval and text analytics: A survey",
    "authors": ["Doris Hoogeveen", "Li Wang", "Timothy Baldwin", "Karin M. Verspoor."],
    "venue": "Foundations and Trends in Information Retrieval, 12(1):1–163.",
    "year": 2018
  }, {
    "title": "Bidirectional LSTM-CRF models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "CoRR, abs/1508.01991.",
    "year": 2015
  }, {
    "title": "Global thread-level inference for comment classification in community question answering",
    "authors": ["Shafiq Joty", "Alberto Barrón-Cedeño", "Giovanni Da San Martino", "Simone Filice", "Lluís Màrquez", "Alessandro Moschitti", "Preslav Nakov."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Speech act modeling of written asynchronous conversations with task-specific embeddings and conditional structured models",
    "authors": ["Shafiq Joty", "Enamul Hoque."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-",
    "year": 2016
  }, {
    "title": "Joint learning with global inference for comment classification in community question answering",
    "authors": ["Shafiq Joty", "Lluís Màrquez", "Preslav Nakov."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Cross-language learning with adversarial neural networks",
    "authors": ["Shafiq Joty", "Preslav Nakov", "Lluís Màrquez", "Israa Jaradat."],
    "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning, CoNLL ’17, pages 226–37, Van-",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning,",
    "year": 2001
  }, {
    "title": "A review on deep learning techniques applied to answer selection",
    "authors": ["Tuan Manh Lai", "Trung Bui", "Sheng Li."],
    "venue": "Proceedings of the 27th International Conference on Computational Linguistics, COLING ’18, pages 2132–2144, Santa Fe, New",
    "year": 2018
  }, {
    "title": "The METEOR metric for automatic evaluation of machine translation",
    "authors": ["Alon Lavie", "Michael Denkowski."],
    "venue": "Machine Translation, 23(2–3):105–115.",
    "year": 2009
  }, {
    "title": "Semi-supervised question retrieval with gated convolutions",
    "authors": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi Jaakkola", "Kateryna Tymoshenko", "Alessandro Moschitti", "Lluís Màrquez."],
    "venue": "Proceedings of the 2016 Conference of the North",
    "year": 2016
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL ’16, pages 1064–1074, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Large-scale goodness polarity lexicons for community question answering",
    "authors": ["Todor Mihaylov", "Daniel Balchev", "Yasen Kiprov", "Ivan Koychev", "Preslav Nakov."],
    "venue": "Proceedings of the 40th International Conference on Research and Development in Infor-",
    "year": 2017
  }, {
    "title": "Finding opinion manipulation trolls in news community forums",
    "authors": ["Todor Mihaylov", "Georgi Georgiev", "Preslav Nakov."],
    "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning, CoNLL ’15, pages 310–314, Bei-",
    "year": 2015
  }, {
    "title": "The dark side of news community forums: Opinion manipulation trolls",
    "authors": ["Todor Mihaylov", "Tsvetomila Mihaylova", "Preslav Nakov", "Lluís Màrquez", "Georgi Georgiev", "Ivan Koychev."],
    "venue": "Internet Research.",
    "year": 2018
  }, {
    "title": "SemanticZ at SemEval-2016 Task 3: Ranking relevant answers in community question answering using semantic similarity based on fine-tuned word embeddings",
    "authors": ["Todor Mihaylov", "Preslav Nakov."],
    "venue": "Proceedings of the 10th International Workshop on",
    "year": 2016
  }, {
    "title": "SUper Team at SemEval-2016",
    "authors": ["Tsvetomila Mihaylova", "Pepa Gencheva", "Martin Boyanov", "Ivana Yovcheva", "Todor Mihaylov", "Momchil Hardalov", "Yasen Kiprov", "Daniel Balchev", "Ivan Koychev", "Preslav Nakov", "Ivelina Nikolova", "Galia Angelova"],
    "year": 2016
  }, {
    "title": "Fact checking in community forums",
    "authors": ["Tsvetomila Mihaylova", "Preslav Nakov", "Lluís Màrquez", "Alberto Barrón-Cedeño", "Mitra Mohtarami", "Georgi Karadjov", "James Glass."],
    "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,",
    "year": 2018
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
    "year": 2013
  }, {
    "title": "Machine Learning A Probabilistic Perspective",
    "authors": ["Kevin Murphy."],
    "venue": "The MIT Press.",
    "year": 2012
  }, {
    "title": "Loopy belief propagation for approximate inference: An empirical study",
    "authors": ["Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan."],
    "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI’99, pages 467–475, Stockholm,",
    "year": 1999
  }, {
    "title": "It takes three to tango: Triangulation approach to answer ranking in community question answering",
    "authors": ["Preslav Nakov", "Lluís Màrquez", "Francisco Guzmán."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-",
    "year": 2016
  }, {
    "title": "2016b. SemEval-2016 task 3: Community question answering",
    "authors": ["Preslav Nakov", "Lluís Màrquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "abed Alhakim Freihat", "James Glass", "Bilal Randeree"],
    "venue": "In Proceedings of the 10th International Work-",
    "year": 2016
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL ’02, pages 311–318,",
    "year": 2002
  }, {
    "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
    "authors": ["Judea Pearl."],
    "venue": "Morgan Kaufmann Publishers Inc., San Francisco, California, USA.",
    "year": 1988
  }, {
    "title": "Convolutional neural tensor network architecture for communitybased question answering",
    "authors": ["Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of International Joint Conference on Artificial Intelligence, IJCAI ’15, pages 1305–1311, Buenos Aires, Ar-",
    "year": 2015
  }, {
    "title": "Learning hybrid representations to retrieve semantically equivalent questions",
    "authors": ["Cicero dos Santos", "Luciano Barbosa", "Dasha Bogdanova", "Bianca Zadrozny."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational",
    "year": 2015
  }, {
    "title": "Adversarial domain adaptation for duplicate question detection",
    "authors": ["Darsh J Shah", "Tao Lei", "Alessandro Moschitti", "Salvatore Romeo", "Preslav Nakov."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’18,",
    "year": 2018
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Ameri-",
    "year": 2006
  }, {
    "title": "Parsing with compositional vector grammars",
    "authors": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, pages 455–465, Sofia,",
    "year": 2013
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15:1929–1958.",
    "year": 2014
  }, {
    "title": "LSTM-based deep learning models for non-factoid answer selection",
    "authors": ["Ming Tan", "Bing Xiang", "Bowen Zhou."],
    "venue": "arXiv preprint arXiv:1511.04108.",
    "year": 2015
  }, {
    "title": "RMSprop",
    "authors": ["T. Tieleman", "G Hinton."],
    "venue": "COURSERA: Neural Networks",
    "year": 2012
  }, {
    "title": "Accelerated training of conditional random fields with stochastic gradient methods",
    "authors": ["S.V.N. Vishwanathan", "Nicol N. Schraudolph", "Mark W. Schmidt", "Kevin P. Murphy."],
    "venue": "Proceedings of the 23rd International Conference on Machine Learning, ICML",
    "year": 2006
  }, {
    "title": "A long short-term memory model for answer sentence selection in question answering",
    "authors": ["Di Wang", "Eric Nyberg."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "Concept and attention-based cnn for question retrieval in multi-view learning",
    "authors": ["Pengwei Wang", "Lei Ji", "Jun Yan", "Dejing Dou", "Nisansa De Silva", "Yong Zhang", "Lianwen Jin."],
    "venue": "ACM Trans. Intell. Syst. Technol., 9(4):41:1–41:24.",
    "year": 2018
  }, {
    "title": "Comparing the mean field method and belief propagation for approximate inference in MRFs",
    "authors": ["Yair Weiss."],
    "venue": "Advanced Mean Field Methods, pages 229–239, Cambridge, Massachusetts, USA. MIT Press.",
    "year": 2001
  }, {
    "title": "Question condensing networks for answer selection in community question answering",
    "authors": ["Wei Wu", "Xu SUN", "Houfeng WANG."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL ’18, pages 1746–1755,",
    "year": 2018
  }, {
    "title": "ADADELTA: an adaptive learning rate method",
    "authors": ["Matthew D. Zeiler."],
    "venue": "CoRR, abs/1212.5701.",
    "year": 2012
  }, {
    "title": "Answer sequence learning with neural networks for answer selection in community question answering",
    "authors": ["Xiaoqiang Zhou", "Baotian Hu", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for",
    "year": 2015
  }],
  "id": "SP:1e5e874846f5c34fd0aff3df0b93d973c38afd71",
  "authors": [{
    "name": "Shafiq Joty",
    "affiliations": []
  }, {
    "name": "Lluís Màrquez",
    "affiliations": []
  }, {
    "name": "Preslav Nakov",
    "affiliations": []
  }],
  "abstractText": "We address jointly two important tasks for Question Answering in community forums: given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure. While DNNs alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complementarity of DNNs and structured learning.",
  "title": "Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings"
}