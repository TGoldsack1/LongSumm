{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 284–290 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017). Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set). However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017). In this work, we combine both MT flavors within a hybrid GEC system. Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased CoNLL-2014 and the fluency-based JFLEG benchmark. Moreover, comparison with human gold standards shows that the created systems are\ncloser to reaching human-level performance than any other GEC system described in the literature so far.\nUsing consistent training data and preprocessing (§ 2), we first create strong SMT (§ 3) and NMT (§ 4) baseline systems. Then, we experiment with system combinations through pipelining and reranking (§ 5). Finally, we compare the performance with human annotations and identify issues with current state-of-the-art systems (§ 6)."
  }, {
    "heading": "2 Data and preprocessing",
    "text": "Our main training data is NUCLE (Dahlmeier et al., 2013). English sentences from the publicly available Lang-8 Corpora (Mizumoto et al., 2012) serve as additional training data.\nWe use official test sets from two CoNLL shared tasks from 2013 and 2014 (Ng et al., 2013, 2014) as development and test data, and evaluate using M2 (Dahlmeier and Ng, 2012). We also report results on JFLEG (Napoles et al., 2017) with the\n284\nGLEU metric (Napoles et al., 2015). The data set is provided with a development and test set split. All data sets are listed in Table 1.\nWe preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG. Sentences are truecased with scripts from Moses (Koehn et al., 2007). For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b). BPE codes are extracted only from correct sentences from Lang-8 and NUCLE."
  }, {
    "heading": "3 SMT systems",
    "text": "For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features. We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse). We also experiment with additional character-level dense features (Char. ops). All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014).\nExperiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++ (Gao and Vogel, 2008), and no reordering models are used. Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1.\nWe tune the systems separately for M2 and GLEU metrics. MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse features. For M2 tunning\n1https://github.com/dav/word2vec\nwe follow the 4-fold cross-validation on NUCLE with adapted error rate recommended by JunczysDowmunt and Grundkiewicz (2016). Models evaluated on GLEU are optimized on JFLEG Dev using the GLEU scorer, which we added to Moses. We report results for models using feature weights averaged over 4 tuning runs.\nResults Other things being equal, using the original tokenization, applying subword units, and extending edit-based features result in a similar system to Junczys-Dowmunt and Grundkiewicz (2016): 49.82 vs 49.49 M2 (Table 2).\nThe phrase-based SMT systems do not deal well with orthographic errors (Napoles et al., 2017) — if a source word has not been seen in the training corpus, it is likely copied as a target word. Subword units can help to solve this problem partially. Adding features based on character-level edit counts increases the results on both test sets.\nA result of 55.79 GLEU on JFLEG Test is already 2 points better than the GLEU-tuned NMT system of Sakaguchi et al. (2017) and only 1 point worse than the best reported result by Chollampatt and Ng (2017) with their M2-tuned SMT system, even though no additional spelling correction has been used at this point. We experiment with specialized spell-checking methods in later sections."
  }, {
    "heading": "4 NMT systems",
    "text": "The model architecture we choose for our NMTbased systems is an attentional encoder-decoder model with a bidirectional single-layer encoder and decoder, both using GRUs as their RNN variants (Sennrich et al., 2017). A similar architecture has been already tested for the GEC task by Sakaguchi et al. (2017), but we use different hyperparameters.\nTo improve the performance of our NMT models, similarly to Xie et al. (2016) and Ji et al. (2017), we combine them with an additional large-scale language model. In contrast to previous studies, which use an n-gram probabilistic LM, we build a 2-layer Recurrent Neural Network Language Model (RNN\nLM) with GRU cells which we train again on English Common Crawl data (Buck et al., 2014).\nExperimental settings We train with the Marian toolkit (Junczys-Dowmunt et al., 2018) on the same data we used for the SMT baselines, i.e. NUCLE and Lang-8. The RNN hidden state size is set to 1024, embedding size to 512. Source and target vocabularies as well as subword units are the same.\nOptimization is performed with Adam (Kingma and Ba, 2014) and the mini-batch size fitted into 4GB of GPU memory. We regularize the model with scaling dropout (Gal and Ghahramani, 2016) with a dropout probability of 0.2 on all RNN inputs and states. Apart from that we dropout entire source and target words with probabilities of 0.2 and 0.1 respectively. We use early stopping with a patience of 10 based on the cross-entropy cost on the CoNLL-2013 test set. Models are validated and saved every 10,000 mini-batches. As final models we choose the one with the best performance on the development set among the last ten model check-points based on the M2 or GLEU metrics.\nSize of RNN hidden state and embeddings, target vocabulary, and optimization options for the RNN LM are identical to those used for our NMT models. Decoding is done by beam search with a beam size of 12. We normalize scores for each hypothesis by sentence length.\nResults A single NMT model achieves lower performance than the SMT baselines (Table 3). However, the M2 score of 42.76 for CoNLL-2014 is already higher than the best published result of 41.53 M2 for a strictly neural GEC system of Ji et al. (2017) that has not been enhanced by an additional language model.\nOur RNN LM is integrated with NMT models through ensemble decoding (Sennrich et al., 2016a). Similarly to Ji et al. (2017), we choose the weight of the language model using grid search on the development set2. This strongly improves recall,\n2Used weights are 0.2 and 0.25 for M2 and GLEU evalua-\nand thus boosts the results significantly on both test sets (+5.8 M2 and +5.96 GLEU).\nAn ensemble of four independently trained models3 (NMT×4), on the other hand, increases precision at the expense of recall, which may even lead to a performance drop. Adding the RNN LM to that ensemble balances this negative effect, resulting in 50.19 M2. These are by far the highest results reported on both benchmarks for pure neural GEC systems.\nComparison to SMT systems With model ensembling, the neural systems achieve performance similar to SMT baselines (Figure 2). A strippeddown SMT system without CCLM, quite surprisingly gives better results on JFLEG than the NMT system, and the opposite is true for CoNLL-2014. The reason for the lower performance on JFLEG might be a large amount of spelling errors, which are more efficiently corrected by the SMT system using subword units.\nIf both systems are enhanced by a large-scale language model, the neural system outperforms the SMT system on JFLEG and it is competitive with SMT systems on CoNLL-2014. However, it is not known if the results would preserve if the NMT model is combined with a probabilistic ngram LM instead as it has been proposed in the previous works (Xie et al., 2016; Ji et al., 2017).\ntion, respectively. 3Each model is weighted equally during decoding."
  }, {
    "heading": "5 Hybrid SMT-NMT systems",
    "text": "We experiment with pipelining and rescoring methods in order to combine our best SMT and NMT GEC systems4.\nSMT-NMT pipelines The output corrected by an SMT system is passed as an input to the NMT ensemble with or without RNN LM5. In this case the NMT system serves as an automatic post-editing system. Pipelining improves the results on both test sets by increasing recall (Table 4). As the performance of the NMT system without a RNN LM is much lower than the performance of the SMT system alone, this implies that both approaches produce complementary corrections.\nRescoring with NMT Rescoring of an n-best list obtained from one system by another is a commonly used technique in GEC, which allows to combine multiple different systems or even different approaches (Hoang et al., 2016; Yannakoudakis et al., 2017; Chollampatt and Ng, 2017; Ji et al., 2017). In our experiments, we generate a 1000 n-best list with the SMT system and add separate scores from each neural component. Scores of NMT models and the RNN LM are added in the form of probabilities in negative log space. The re-scored weights are obtained from a single run of the Batch Mira algorithm (Cherry and Foster, 2012) on the development set.\nAs opposed to pipelining, rescoring improves precision at the expense of recall and is more effective for the CoNLL data resulting in up to 54.95 M2. On JFLEG, rescoring only with the RNN LM\n4The best system combinations are chosen again based on the development sets, i.e. CoNLL-2013 and JFLEG Dev. We omit these results as they are highly overestimated.\n5We did not observed any improvements if the order of the systems is reversed.\nproduces similar results as rescoring with the NMT ensemble. However, the best result for rescoring is lower than for pipelining on that test set. It seems the SMT system is not able to produce as diversified corrections in an n-best list as those generated by the NMT ensemble.\nSpelling correction and final results Pipelining the NMT-rescored SMT system and the NMT system leads to further improvement. We believe this can be explained by different contributions to precision and recall trade-offs for the two methods, similar to effects observed for the combination of the NMT ensemble and our RNN LM.\nOn top of our final hybrid system we add a spellchecking component, which is run before pipelining. We use a character-level SMT system following Chollampatt and Ng (2017) which they deploy for unknown words in their word-based SMT system. As our BPE-based SMT does not really suffer from unknown words, we run the spell-checking component on words that would have been segmented by the BPE algorithm. This last system achieves the best results reported in this paper: 56.25 M2 on CoNLL-2014 and 61.50 GLEU on JFLEG Test."
  }, {
    "heading": "6 Analysis and future work",
    "text": "For both benchmarks our systems are close to automatic evaluation results that have been claimed to correspond to human-level performance on the CoNLL-2014 test set and on JFLEG Test.\nExample outputs Table 5 shows system outputs for an example source sentence from the JFLEG Test corpus that illustrate the complementarity of the statistical and neural approaches. The SMT and NMT systems produce different corrections. Rescoring is able to generate a unique correction (is change→has changed), but it fails in generating some corrections from the neural system, e.g. misspellings (becom and dificullty). Pipelining, on the other hand, may not improve a local correction made by the SMT system (is changed). The combination of the two methods produces output, which is most similar to the references.\nComparison with human annotations Bryant and Ng (2015) created an extension of the CoNLL2014 test set with 10 annotators in total, JFLEG already incorporates corrections from 4 annotators. Human-level results for M2 and GLEU were calculated by averaging the scores for each annotator with regard to the remaining 9 (CoNLL) or 3 (JFLEG) annotators, respectively.\nFigure 3 contains human level scores, our results, and previously best reported results by Chollampatt and Ng (2017). Our best system reaches nearly 100% of the average human score according to M2 and nearly 99% for GLEU being much closer to that bound than previous works6.\n6During the camera-ready preparation, Chollampatt and Ng (2018) have published a GEC system based on a multilayer convolutional encoder-decoder neural network with a character-based spell-checking module improving the previous best result to 54.79 M2 on CoNLL-2014 and 57.47 GLEU on JFLEG Test.\nFurther inspection reveals, however, that the precision/recall trade-off for the automatic system indicates lower coverage compared to human corrections — lower recall is compensated with high precision7. Automatic systems might, for example, miss some obvious error corrections and therefore easily be distinguishable from human references. Future work would require a human evaluation effort to draw more conclusions."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was partially funded by Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Facebook."
  }],
  "year": 2018,
  "references": [{
    "title": "NLTK: the natural language toolkit",
    "authors": ["Steven Bird", "Edward Loper."],
    "venue": "Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics, page 31.",
    "year": 2004
  }, {
    "title": "How far are we from fully automatic high quality grammatical error correction",
    "authors": ["Christopher Bryant", "Hwee Tou Ng"],
    "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "N-gram counts and language models from the Common Crawl",
    "authors": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."],
    "venue": "Proceedings of the Language Resources and Evaluation Conference. Reykjavík, Iceland, pages 3579–3584.",
    "year": 2014
  }, {
    "title": "Batch tuning strategies for statistical machine translation",
    "authors": ["Colin Cherry", "George Foster."],
    "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-",
    "year": 2012
  }, {
    "title": "Connecting the dots: towards human-level grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational",
    "year": 2017
  }, {
    "title": "A multilayer convolutional encoder-decoder neural network for grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Better evaluation for grammatical error correction",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Associa-",
    "year": 2012
  }, {
    "title": "Building a large annotated corpus of learner english: The NUS corpus of learner english",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."],
    "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.",
    "year": 2013
  }, {
    "title": "A joint sequence translation model with integrated reordering",
    "authors": ["Nadir Durrani", "Helmut Schmid", "Alexander Fraser."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2011
  }, {
    "title": "A theoretically grounded application of dropout in recurrent neural networks",
    "authors": ["Yarin Gal", "Zoubin Ghahramani."],
    "venue": "Advances in neural information processing systems. pages 1019–1027.",
    "year": 2016
  }, {
    "title": "Parallel implementations of word alignment tool",
    "authors": ["Qin Gao", "Stephan Vogel."],
    "venue": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing. ACL, pages 49–57.",
    "year": 2008
  }, {
    "title": "KenLM: Faster and smaller language model queries",
    "authors": ["Kenneth Heafield."],
    "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, USA, WMT ’11, pages 187–197.",
    "year": 2011
  }, {
    "title": "Exploiting n-best hypotheses to improve an SMT approach to grammatical error correction",
    "authors": ["Duc Tam Hoang", "Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. IJ-",
    "year": 2016
  }, {
    "title": "A nested attention neural hybrid model for grammatical error correction",
    "authors": ["Jianshu Ji", "Qinlong Wang", "Kristina Toutanova", "Yongen Gong", "Steven Truong", "Jianfeng Gao."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
    "year": 2017
  }, {
    "title": "Phrase-based machine translation is stateof-the-art for automatic grammatical error correction",
    "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR) .",
    "year": 2014
  }, {
    "title": "The effect of learner corpus size in grammatical error correction of ESL writings",
    "authors": ["Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yu Matsumoto."],
    "venue": "Proceedings of COLING 2012. pages 863–872.",
    "year": 2012
  }, {
    "title": "Ground truth for grammatical error correction metrics",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "JFLEG: A fluency corpus and benchmark for grammatical error correction",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault."],
    "venue": "Proceedings of the 2017 Conference of the European Chapter of the Association for Computational Lin-",
    "year": 2017
  }, {
    "title": "The CoNLL-2014 shared task on grammatical error correction",
    "authors": ["Christopher Bryant."],
    "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational",
    "year": 2014
  }, {
    "title": "The CoNLL-2013 shared task on grammatical error correction",
    "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."],
    "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language",
    "year": 2013
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["Franz Josef Och."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1. Association for Computational Linguistics, Stroudsburg, USA, ACL ’03,",
    "year": 2003
  }, {
    "title": "Grammatical error correction: Machine translation and classifiers",
    "authors": ["Alla Rozovskaya", "Dan Roth."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Compu-",
    "year": 2016
  }, {
    "title": "Grammatical error correction with neural reinforcement learning",
    "authors": ["Keisuke Sakaguchi", "Matt Post", "Benjamin Van Durme."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short",
    "year": 2017
  }, {
    "title": "Adapting sequence models for sentence correction",
    "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander Rush", "Stuart Shieber."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
    "year": 2017
  }, {
    "title": "Nematus: a toolkit",
    "authors": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"],
    "year": 2017
  }, {
    "title": "2016a. Edinburgh neural machine",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"],
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
    "year": 2016
  }, {
    "title": "Neural language correction with character-based attention",
    "authors": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y Ng."],
    "venue": "arXiv preprint arXiv:1603.09727 .",
    "year": 2016
  }, {
    "title": "Neural sequencelabelling models for grammatical error correction",
    "authors": ["Helen Yannakoudakis", "Marek Rei", "Øistein E. Andersen", "Zheng Yuan."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro-",
    "year": 2017
  }, {
    "title": "Grammatical error correction using neural machine translation",
    "authors": ["Zheng Yuan", "Ted Briscoe."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-",
    "year": 2016
  }],
  "id": "SP:ca54d56d8942b02ed6e7d441d37372a65c1cc920",
  "authors": [{
    "name": "Roman Grundkiewicz",
    "affiliations": []
  }, {
    "name": "Marcin Junczys-Dowmunt",
    "affiliations": []
  }],
  "abstractText": "We combine two of the most popular approaches to automated Grammatical Error Correction (GEC): GEC based on Statistical Machine Translation (SMT) and GEC based on Neural Machine Translation (NMT). The hybrid system achieves new state-of-the-art results on the CoNLL-2014 and JFLEG benchmarks. This GEC system preserves the accuracy of SMT output and, at the same time, generates more fluent sentences as it typical for NMT. Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far.",
  "title": "Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation"
}