{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Estimating generative models from unlabeled data is one of the challenges in unsupervised learning. Recently, several latent variable approaches have been proposed to learn flexible density estimators together with efficient sampling, such as generative adversarial networks (GANs) (Goodfellow et al., 2014), variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014), iterative transformation of noise (Sohl-Dickstein et al., 2015), or non-volume preserving transformations (Dinh et al., 2017).\nIn this work we focus on GANs, currently the most con-\n*Equal contribution 1Université Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. 2Université Paris Sud, INRIA, équipe TAU, Gif-sur-Yvette, 91190, France. 3Facebook Artificial Intelligence Research Paris, France. Correspondence to: Corentin Tallec <corentin.tallec@inria.fr>, Thomas Lucas <thomas.lucas@inria.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nvincing source of samples of natural images (Karras et al., 2018). GANs consist of a generator and a discriminator network. The generator maps samples from a latent random variable with a basic prior, such as a multi-variate Gaussian, to the observation space. This defines a probability distribution over the observation space. A discriminator network is trained to distinguish between generated samples and true samples in the observation space. The generator, on the other hand, is trained to fool the discriminator. In an idealized setting with unbounded capacity of both networks and infinite training data, the generator should converge to the distribution from which the training data has been sampled.\nIn most adversarial setups, the discriminator classifies individual data samples. Consequently, it cannot directly detect discrepancies between the distribution of generated samples and global statistics of the training distribution, such as its moments or quantiles. For instance, if the generator models a restricted part of the support of the target distribution very well, this can fool the discriminator at the level of individual samples, a phenomenon known as mode dropping. In such a case there is little incentive for the generator to model other parts of the support of the target distribution. A more thorough explanation of this effect can be found in (Salimans et al., 2016).\nIn order to access global distributional statistics, imagine a discriminator that could somehow take full probability distributions as its input. This is impossible in practice. Still, it is possible to feed large batches of training or generated samples to the discriminator, as an approximation of the corresponding distributions. The discriminator can compute statistics on those batches and detect discrepancies between the two distributions. For instance, if a large batch exhibits only one mode from a multimodal distribution, the discriminator would notice the discrepancy right away. Even though a single batch may not encompass all modes of the distribution, it will still convey more information about missing modes than an individual example.\nTraining the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task too easy, as a single bad sample reveals the whole batch as synthetic. Instead, we introduce a “mixed” batch discrimination task in which the discriminator needs to predict the ratio of real samples in a batch.\n1\nThis use of batches differs from traditional minibatch learning. The batch is not used as a computational trick to increase parallelism, but as an approximate distribution, on which to compute global statistics.\nA naive way of doing so would be to concatenate the samples in the batch, feeding the discriminator a single tensor containing all the samples. However, this is parameterhungry, and the computed statistics are not automatically invariant to the order of samples in the batch. To compute functions that depend on the samples only through their distribution, it is necessary to restrict the class of discriminator networks to permutation-invariant functions of the batch. For this, we adapt and extend an architecture from McGregor (2007) to compute symmetric functions of the input. We show this can be done with minimal modification to existing architectures, at a negligible computational overhead w.r.t. ordinary batch processing.\nIn summary, our contributions are the following:\n• Naively training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task way too easy. We introduce a discrimination loss based on mixed batches of true and fake samples, that avoids this pitfall. We derive the associated optimal discriminator. • We provide a principled way of defining neural networks that are permutation-invariant over a batch of samples. We formally prove that the resulting class of functions comprises all symmetric continuous functions, and only symmetric functions. • We apply these insights to GANs, with good experimental results, both qualitatively and quantitatively.\nWe believe that discriminating between distributions at the batch level provides an equally principled alternative to approaches to GANs based on duality formulas (Nowozin et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017)."
  }, {
    "heading": "2. Related work",
    "text": "The training of generative models via distributional rather than pointwise information has been explored in several recent contributions. Batch discrimination (Salimans et al., 2016) uses a handmade layer to compute batch statistics which are then combined with sample-specific features to enhance individual sample discrimination. Karras et al. (2018) directly compute the standard deviation of features and feed it as an additional feature to the last layer of the network. Both methods use a single layer of handcrafted batch statistics, instead of letting the discriminator learn arbitrary batch statistics useful for discrimination as in our approach. Moreover, in both methods the discriminator still assesses single samples, rather than entire batches. Radford et al. (2015) reported improved results with batch normalization\nin the discriminator, which may also be due to reliance on batch statistics.\nOther works, such as (Li et al., 2015) and (Dziugaite et al., 2015), replace the discriminator with a fixed distributional loss between true and generated samples, the maximum mean discrepancy, as the criterion to train the generative model. This has the advantage of relieving the inherent instability of GANs, but lacks the flexibility of an adaptive discriminator.\nThe discriminator we introduce treats batches as sets of samples. Processing sets prescribes the use of permutation invariant networks. There has been a large body of work around permutation invariant networks, e.g (McGregor, 2007; 2008; Qi et al., 2016; Zaheer et al., 2017; Vaswani et al., 2017). Our processing is inspired by (McGregor, 2007; 2008) which designs a special kind of layer that provides the desired invariance property. The network from McGregor (2007) is a multi-layer perceptron in which the single hidden layer performs a batchwise computation that makes the result equivariant by permutation. Here we show that stacking such hidden layers and reducing the final layer with a permutation invariant reduction, covers the whole space of continuous permutation invariant functions.\nZaheer et al. (2017) first process each element of the set independently, then aggregate the resulting representation using a permutation invariant operation, and finally process the permutation invariant quantity. Qi et al. (2016) process 3D point cloud data, and interleave layers that process points independently, and layers that apply equivariant transformations. The output of their networks are either permutation equivariant for pointcloud segmentation, or permutation invariant for shape recognition. In our approach we stack permutation equivariant layers that combine batch\ninformation and sample information at every level, and aggregate these in the final layer using a permutation invariant operation.\nMore complex approaches to permutation invariance or equivariance appear in (Guttenberg et al., 2016). We prove, however, that our simpler architecture already covers the full space of permutation invariant functions.\nImproving the training of GANs has received a lot of recent attention. For instance, Arjovsky et al. (2017), Gulrajani et al. (2017) and Miyato et al. (2018) constrain the Lipschitz constant of the network and show that this stabilizes training and improves performance. Karras et al. (2018) achieved impressive results by gradually increasing the resolution of the generated images as training progresses."
  }, {
    "heading": "3. Adversarial learning with permutation-invariant batch features",
    "text": "Using a batch of samples rather than individual samples as input to the discriminator can provide global statistics about\nthe distributions of interest. Such statistics could be useful to avoid mode dropping. Adversarial learning (Goodfellow et al., 2014) can easily be extended to the batch discrimination case. For a fixed batch size B, the corresponding two-player optimization procedure becomes\nmin G max D Ex1,...,xB∼D [logD(x1, . . . , xB)] + (1)\nEz1,...,zB∼Z [log(1−D(G(z1), . . . , G(zB)))]\nwith D the empirical distribution over data, Z a distribution over the latent variable that is the input of the generator, G a pointwise generator and D a batch discriminator.1 This leads to a learning procedure similar to the usual GAN algorithm, except that the loss encourages the discriminator to output 1 when faced with an entire batch of real data, and 0 when faced with an entire batch of generated data.\nUnfortunately, this basic procedure makes the work of the discriminator too easy. As the discriminator is only faced with batches that consist of either only training samples or only generated samples, it can base its prediction on any subset of these samples. For example, a single poor generated sample would be enough to reject a batch. To cope with this deficiency, we propose to sample batches that mix both training and generated data. The discriminator’s task is to predict the proportion of real images in the batch, which is clearly a permutation invariant quantity."
  }, {
    "heading": "3.1. Batch smoothing as a regularizer",
    "text": "A naive approach to sampling mixed batches would be, for each batch index, to pick a datapoint from either real or generated images with probability 12 . This is necessarily ill behaved: as the batch size increases, the ratio of training data to generated data in the batch tends to 12 by the law of large numbers. Consequently, a discriminator always predicting 12 would achieve very low error with large batch sizes, and provide no training signal to the generator.\nInstead, for each batch we sample a ratio p from a distribution P on [0, 1], and construct a batch by picking real samples with probability p and generated samples with probability 1− p. This forces the discriminator to predict across an entire range of possible values of p.\nFormally, suppose we are given a batch of training data x ∈ RB×n and a batch of generated data x̃ ∈ RB×n. To mix x and x̃, a binary vector β is sampled from B (p)B , a B-dimensional Bernoulli distribution with parameter p. The mixed batch with mixing vector β is denoted\nmβ(x, x̃) := x β + x̃ (1− β). (2) 1The generator G could also be modified to produce batches of data, which can help to cover more modes per batch, but this deviates from the objective of learning a density estimator from which we can draw i.i.d. samples.\nThis apparently wastes some samples, but we can reuse the discarded samples by using 1− β in the next batch.\nThe discriminator has to predict the ratio of real images, #βB where #β is the sum of the components of β. As a loss on the predicted ratio, we use the Kullback–Leibler divergence between a Bernoulli distribution with the actual ratio of real images, and a Bernoulli distribution with the predicted ratio. The divergence between Bernoulli distributions with parameters u and v is\nKL(B (u) || B (v)) = u log u v + (1− u) log 1− u1− v . (3)\nFormally, the discriminator D will minimize the objective\nEp∼P, β∼B(p)B KL ( B (\n#β B\n) || B (D(mβ(x, x̃))) ) ,\n(4)\nwhere the expectation is over sampling p from a distribution P , typically uniform on [0, 1], then sampling a mixed minibatch. For clarity, we have omitted the expectation over the sampling of training and generated samples\nThe generator is trained with the loss\nEp∼P, β∼B(p)B log(D(mβ(x, x̃))). (5)\nThis loss, which is not the generator loss associated to the min-max optimization problem, is known to saturate less (Goodfellow et al., 2014).\nIn some experimental cases, using the discriminator loss (4) with P = U([0, 1]) made discriminator training too difficult. To alleviate some of the difficulty, we sampled the mixing variable p from a reduced symmetric union of intervals [0, γ] ∪ [1 − γ, 1]. With low γ, all generated batches are nearly purely taken from either real or fake data. We refer to this training method as batch smoothing-γ. Batch smoothing-0 corresponds to no mixing, while batch smoothing-0.5 corresponds to equation (4)."
  }, {
    "heading": "3.2. The optimal discriminator for batch smoothing",
    "text": "The optimal discriminator for batch smoothing can be computed explicitly, for p ∼ U([0, 1]), and extends the usual GAN discriminator when B = 1. Proposition 1. The optimal discriminator for the loss (4),\ngiven a batch y ∈ RB×N , is\nD∗(y) = 12 punbalanced(y) pbalanced(y)\n(6)\nwhere the distribution pbalanced and punbalanced on batches are defined as\npbalanced(y) = 1 B + 1 ∑\nβ∈{0,1}B\np1(y)βp2(y)1−β( B #β )\npunbalanced(y) = 2 B + 1 ∑\nβ∈{0,1}B\np1(y)βp2(y)1−β( B #β ) #β B .\n(7)\nin which p1 is the data distribution and p2 the distribution of generated samples, and where p1(y)β is shorthand for p1(y1)β1 . . . p1(yB)βB .\nThe proof is technical and is deferred to the supplementary material. For non-uniform beta distributions on p, a similar result holds, with different coefficients depending on #β and B in the sum.\nThese heavy expressions can be interpreted easily. First, in the case B = 1, the optimal discriminator reduces to the optimal discriminator for a standard GAN, D∗ = p1(y)p1(y)+p2(y) .\nActually pbalanced(y) is simply the distribution of batches y under our procedure of sampling p uniformly, then sampling β ∼ B (p)B . The binomial coefficients put on equal footing contributions with different true/fake ratios.\nThe generator loss (5), when faced with the optimal discriminator, is the Kullback–Leibler divergence between pbalanced and punbalanced (up to sign and a constant log(2)). Since punbalanced puts more weight on batches with higher #β (more true samples), this brings fake samples closer to true ones.\nSince pbalanced and punbalanced differ by a factor 2#β/B, the ratio D∗ = 12 punbalanced(y) pbalanced(y) is simply the expectation of #β/B under a probability distribution on β that is proportional to p1(y)βp2(y)1−β(\nB #β ) . But this is the posterior distribution on\nβ given the batch y and the uniform prior on the ratio p. Thus, the optimal discriminator is just the posterior mean of the ratio of true samples, D∗(y) = IEβ|y [ #β B ] . This is standard when minimizing the expected divergence between Bernoulli distributions and the approach can therefore be extended to non-uniform priors on p as shown in section 9."
  }, {
    "heading": "4. Permutation invariant networks",
    "text": "Computing statistics of probability distributions from batches of i.i.d. samples requires to compute quantities that\nare invariant to permuting the order of samples within the batch. In this section we propose a permutation equivariant layer that can be used together with a permutation invariant aggregation operation to build networks that are permutation invariant. We also provide a sketch of proof (fully developed in the supplementary material) that this architecture is able to reach all symmetric continuous functions, and only represents such functions."
  }, {
    "heading": "4.1. Building a permutation invariant architecture",
    "text": "A naive way of achieving invariance to batch permutations is to consider the batch dimension as a regular feature dimension, and to randomly reorder the batches at each step. This multiplies the input dimension by the batch size, and thus greatly increases the number of trainable parameters. Moreover, this only provides approximate invariance to batch permutation, as the network has to infer the invariance based on the training data.\nInstead, we propose to directly build invariance into the architecture. This method drastically reduces the number of parameters compared to the naive approach, bringing it back in line with ordinary networks, and ensures strict invariance to batch permutation.\nLet us first formalize the notion of batch permutation invariance and equivariance. A function f from RB×l to RB×L is batch permutation equivariant if permuting samples in the batch results in the same permutation of the outputs: for any permutation σ of the inputs,\nf(xσ(1), . . . , xσ(B)) = f(x)σ(1), . . . , f(x)σ(B). (8)\nFor instance, any regular neural network or other function treating the inputs x1, . . . , xB independently in parallel, is batch permutation equivariant.\nA function f from RB×l to RL is batch permutation invariant if permuting the inputs in the batch does not change the output: for any permutation on batch indices σ,\nf(xσ(1), . . . , xσ(B)) = f(x1, . . . , xB). (9)\nThe mean, the max or the standard deviation along the batch axis are all batch permutation invariant.\nPermutation equivariant and permutation invariant functions can be obtained by combining ordinary, parallel treatment of batch samples with an additional batch-averaging operation that performs an average of the activations across the batch direction. In our architecture, this averaging is the only form of interaction between different elements of the batch. It is one of our main results that such operations are sufficient to recover all invariant functions.\nFormally, on a batch of data x ∈ RB×n, our proposed batch\npermutation invariant network fθ is defined as\nfθ(x) = 1 B B∑ b=1 (φθp ◦ φθp−1 ◦ . . . ◦ φθ0(x))b (10)\nwhere each φθi is a batch permutation equivariant function from RB×li−1 to RB×li , where the li’s are the layer sizes.\nThe equivariant layer operation φθ with l input features and L output features comprises an ordinary weight matrix Λ ∈ Rl×L that treats each data point of the batch independently (“non-batch-mixing”), a batch-mixing weight matrix Γ ∈ Rl×L, and a bias vector β ∈ RL. As in regular neural networks, Λ processes each data point in the batch independently. On the other hand, the weight matrix Γ operates after computing an average across the whole batch. Defining ρ as the batch average for each feature,\nρ(x1, . . . , xB) := 1 B B∑ b=1 xb (11)\nthe permutation-equivariant layer φ is formally defined as φθ(x)b := µ ( β + xbΛ + ρ(x)Γ ) (12)\nwhere µ is a nonlinearity, b is a batch index, and the parameter of the layer is θ = (β,Λ,Γ)."
  }, {
    "heading": "4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions",
    "text": "The networks constructed above are permutation invariant by construction. However, it is unclear a priori that all permutation invariant functions can be represented this way: the functions that can be approximated to arbitrary precision by those networks could be a strict subset of the set of permutation invariant functions. The optimal solution for the discriminator could lie outside this subset, making our construction too restrictive. We now show this is not the case: our architecture satisfies a universal approximation theorem for permutation-invariant functions.\nTheorem 1. The set of networks that can be constructed by stacking as in Eq. (10) the layers φ defined in Eq. (12), with sigmoid nonlinearities except on the output layer, is dense in the set of permutation-invariant functions (for the topology of uniform convergence on compact sets).\nWhile the case of one-dimensional features is relatively simple, the multidimensional case is more intricate, and the detailed proof is given in the supplementary material. Let us describe the key ideas underlying the proof.\nThe standard universal approximation theorem for neural networks proves the following: for any continuous function f , we can find a network that given a batch x = (x1, . . . , xB), computes (f(x1), . . . , f(xB)). This is insufficient for our purpose as it provides no way of mixing information between samples in the batch.\nFirst, we prove that the set of functions that can be approximated to arbitrary precision by our networks is an algebra, i.e., a vector space stable under products. From this point on, it remains to be shown that this algebra contains a generative family of the continuous symmetric functions.\nTo prove that we can compute the sum of two functions f1 and f2, compute f1 and f2 on different channels (this is possible even if f1 and f2 require different numbers of layers, by filling in with the identity if necessary). Then sum across channels, which is possible in (12).\nTo compute products, first compute f1 and f2 on different channels, then apply the universal approximation theorem to turn this into log f1 and log f2, then add, then take the exponential thanks to the universal approximation theorem.\nThe key point is then the following: the algebra of all permutation-invariant polynomials over the components of (x1, . . . , xB) is generated as an algebra by the averages 1 B (f(x1) + . . .+ f(xB)) when f ranges over all functions of single batch elements. This non-trivial algebraic statement is proved in the supplementary material.\nBy construction, such functions 1B (f(x1)+. . .+f(xB)) are readily available in our architecture, by computing f as in an ordinary network and then applying the batch-averaging operation ρ in the next layer. Further layers provide sums and products of those thanks to the algebra property. We can conclude with a symmetric version of the Stone–Weierstrass theorem (polynomials are dense in continuous functions)."
  }, {
    "heading": "4.3. Practical architecture",
    "text": "In our experiments, we apply the constructions above to standard, deep convolutional neural networks. In practice, for the linear operations Λ and Γ in (12) we use convolutional kernels (of size 3× 3) acting over xb and ρ(x) respectively. Weight tensors Λ and Γ are also reweighted like so that at the start of training ρ(x) does not contribute disproportionately compared with other features: Λ̃ = |B||B|+1 Λ and Γ̃ = 1|B|+1 Γ where |B| denotes the size of batch B. While these coefficients could be learned, we have found this explicit initialization to improve training. Figure 1 shows how to modify standard CNN architectures to adapt each layer to our method.\nIn the first setup, which we refer to as BGAN, a permutation invariant reduction is done at the end of the discriminator, yielding a single prediction per batch, which is evaluated with the loss in (4). We also introduce a setup, M-BGAN, where we swap the order of averaging and applying the loss. 2 Namely, letting y be the single target for the batch (in our case, the proportion of real samples), the BGAN case translates into\nL((o1, . . . , oB), y) = ` (\n1 B B∑ i=1 oi, y\n) (13)\n2This was initially a bug that worked.\nwhile M-BGAN translates to\nL((o1, . . . , oB), y) = 1 B B∑ i=1 `(oi, y) (14)\nwhere L is the final loss function, ` is the KL loss function used in (4), (o1, . . . , ob) is the output of the last equivariant layer, and y is the target for the whole batch.\nBoth these losses are permutation invariant. A more detailled explanation of M-BGAN is given in Section 11."
  }, {
    "heading": "5. Experiments",
    "text": ""
  }, {
    "heading": "5.1. Synthetic 2D distributions",
    "text": "The synthetic dataset from Zhang et al. (2017) is explicitly designed to test mode dropping. The data are sampled from a mixture of concentrated Gaussians in the 2D plane. We compare standard GAN training, “mixup” training (Zhang et al., 2017), and batch smoothing using the BGAN from Section 4.3.\nIn all cases, the generators and discriminators are three-layer ReLU networks with 512 units per layer. The latent variables of the generator are 2-dimensional standard Gaussians. The models are trained on their respective losses using the Adam (Kingma & Ba, 2015) optimizer, with default parameters. The discriminator is trained for five steps for each generator step.\nThe results are summarized in Figure 3. Batch smoothing and mixup have similar effects. Results for BGAN and M-BGAN are qualitatively similar on this dataset and we only display results for BGAN. The standard GAN setting quickly diverges, due to its inability to fit several modes simultaneously, while both batch smoothing and mixup successfully fit the majority of modes of the distribution."
  }, {
    "heading": "5.2. Experimental results on CIFAR10",
    "text": "Next, we consider image generation on the CIFAR10 dataset. We use the simple architecture from (Miyato et al., 2018), minimally modified to obtain permutation invariance thanks to (12). All other architectural choices are unchanged. The same Adam hyperparameters from (Miyato et al., 2018) are used for all models: α = 2e−4, β1 = 0.5, β2 = 0.999, and no learning rate decay. We performed hyperparameter search for the number of discrimination steps between each generation step, ndisc, over the range {1, . . . , 5}, and for the batch smoothing parameter γ over [0.2, 0.5]. All models are trained for 400, 000 iterations, counting both generation and discrimination steps. We compare smoothed BGAN and M-BGAN, and the same network trained with spectral normalization (Miyato et al., 2018) (SN), and gradient penalty (Gulrajani et al., 2017) on both the Wasserstein (Arjovsky et al., 2017) (WGP) and the standard loss (GP). We also compare to a model using the batch-discrimination layer from (Salimans et al., 2016), adding a final batch discrimination layer to the architecture of (Miyato et al., 2018). All models are evaluated by reporting the Inception Score and the Fréchet Inception Distance (Heusel et al., 2017) and results are summarized in Table 2. Figure 4 displays sample images generated with our best model.\nFigure 5.2 highlights the training dynamics of each model3. On this architecture, M-BGAN heavily outperforms both batch discrimination and our other variants, and yields results similar to, or slightly better than (Miyato et al., 2018). Model trained with batch smoothing display results on par with batch discrimination, and much better than without batch smoothing.\n3For readability, a slight smoothing is performed on the curves."
  }, {
    "heading": "5.3. Effect of batch smoothing on the generator and discriminator losses",
    "text": "To check the effect of the batch smoothing parameter γ on the loss, we plot the discriminator and generator losses of the network for different γ’s. The smaller the γ, the purer the batches. We would expect discriminator training to be more difficult with larger γ. The results corroborate this insight (Fig. 2). BGAN and M-BGAN behave similarly and we only report on BGAN in the figure. The discriminator loss is not directly affected by an increase in γ, but the generator loss is lower for larger γ, revealing the relative advantage of the generator on the discriminator.\nThis suggests to increase γ if the discriminator dominates learning, and to decrease γ if the discriminator is stuck at a high value in spite of poor generated samples."
  }, {
    "heading": "5.4. Qualitative results on celebA",
    "text": "Finally, on the celebA face dataset, we adapt the simple architecture of (Miyato et al., 2018) to the increased resolution by adding a layer to both networks. For optimization we use Adam with β1 = 0, β2 = 0.9, α = 1e − 4, and ndisc = 1. Fig. 5 dislays BGAN samples with pure batches, and BGAN and M-BGAN samples with γ = .5. The visual quality of the samples is reasonable; we believe that an improvement is visible from pure batches to M-BGAN."
  }, {
    "heading": "6. Conclusion",
    "text": "We introduced a method to feed batches of samples to the discriminator of a GAN in an principled way, based on two observations: feeding all-fake or all-genuine batches to a discriminator makes its task too easy; second, a simple architectural trick makes it possible to provably recover all functions of the batch as an unordered set. Experimentally, this provides a new, alternative method to reduce mode dropping and reach good quantitative scores in GAN training."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "This work has been partially supported by the grant ANR-16CE23-0006 “Deep in France” and LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01)."
  }],
  "year": 2018,
  "references": [{
    "title": "Wasserstein generative adversarial networks",
    "authors": ["M. Arjovsky", "S. Chintala", "L. Bottou"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Approximation by superpositions of a sigmoidal function",
    "authors": ["G. Cybenko"],
    "venue": "Mathematics of Control, Signals and Systems,",
    "year": 1989
  }, {
    "title": "Density estimation using real NVP",
    "authors": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Training generative neural networks via maximum mean discrepancy optimization",
    "authors": ["G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani"],
    "venue": "arXiv preprint arXiv:1505.03906,",
    "year": 2015
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Improved training of wasserstein gans",
    "authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A.C. Courville"],
    "venue": "CoRR, abs/1704.00028,",
    "year": 2017
  }, {
    "title": "Permutation-equivariant neural networks applied to dynamics prediction",
    "authors": ["N. Guttenberg", "N. Virgo", "O. Witkowski", "H. Aoki", "R. Kanai"],
    "venue": "CoRR, abs/1612.04530,",
    "year": 2016
  }, {
    "title": "Gans trained by a two time-scale update rule converge to a nash equilibrium",
    "authors": ["M. Heusel", "H. Ramsauer", "T. Unterthiner", "B. Nessler", "G. Klambauer", "S. Hochreiter"],
    "venue": "CoRR, abs/1706.08500,",
    "year": 2017
  }, {
    "title": "Progressive growing of GANs for improved quality, stability, and variation",
    "authors": ["T. Karras", "T. Aila", "S.L. abd J. Lehtinen"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "iclr,",
    "year": 2015
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Generative moment matching networks",
    "authors": ["Y. Li", "K. Swersky", "R.S. Zemel"],
    "venue": "CoRR, abs/1502.02761,",
    "year": 2015
  }, {
    "title": "Neural network processing for multiset data",
    "authors": ["S. McGregor"],
    "venue": "In Proceedings of the 17th International Conference on Artificial Neural Networks,",
    "year": 2007
  }, {
    "title": "Further results in multiset processing with neural networks",
    "authors": ["S. McGregor"],
    "venue": "Neural Networks,",
    "year": 2008
  }, {
    "title": "Spectral normalization for generative adversarial networks",
    "authors": ["T. Miyato", "T. Kataoka", "M. Koyama", "Y. Yoshida"],
    "venue": "International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "f-gan: Training generative neural samplers using variational divergence minimization",
    "authors": ["S. Nowozin", "B. Cseke", "R. Tomioka"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
    "authors": ["C.R. Qi", "H. Su", "K. Mo", "L.J. Guibas"],
    "venue": "CoRR, abs/1612.00593,",
    "year": 2016
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "arXiv preprint arXiv:1511.06434,",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Improved techniques for training GANs",
    "authors": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
    "authors": ["J. Sohl-Dickstein", "E.A. Weiss", "N. Maheswaranathan", "S. Ganguli"],
    "venue": "arXiv preprint arXiv:1503.03585,",
    "year": 2015
  }],
  "id": "SP:321e0953ac3717b6700f1e594c0c21592d5db8f8",
  "authors": [{
    "name": "Thomas Lucas",
    "affiliations": []
  }, {
    "name": "Corentin Tallec",
    "affiliations": []
  }, {
    "name": "Jakob Verbeek",
    "affiliations": []
  }, {
    "name": "Yann Ollivier",
    "affiliations": []
  }],
  "abstractText": "Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network. However, the discriminator usually assesses individual samples. This prevents the discriminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture. This architecture is provably a universal approximator of all symmetric functions. Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.",
  "title": "Mixed batches and symmetric discriminators for GAN training"
}