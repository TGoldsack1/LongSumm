{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Establishing maps (e.g. pointwise correspondences) across object collections is a fundamental problem spanning many scientific domains. High-quality maps facilitating information propagation and transformation are key to applications ranging from 3D reconstruction with partial scans (Huber & Hebert, 2001), data-driven geometry completion and reconstruction (Pauly et al., 2005), texture transfer (Schreiner et al., 2004; Kraevoy & Sheffer, 2004), to comparative biology (Boyer et al., 2011; Gao et al., 2017), joint dataanalysis (Huang et al., 2011; Kim et al., 2012; Wang et al., 2013; 2014; Huang et al., 2014), and data exploration and organization (Kim et al., 2012; Huang et al., 2014).\nHigh quality object maps are generally difficult to compute. Prior work on map computation focused on optimizing maps between pairs of objects; see (van Kaick et al., 2010) for\n1Department of Computer Science, The University of Texas at Austin 2Department of Statistics, The University of Chicago 3Institute for Interdisciplinary Information Sciences, Tsinghua Univesity. Correspondence to: Chandrajit Bajaj <bajaj@cs.utexas.edu>, Qixing Huang <huangqx@cs.utexas.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\na standard survey and (Kim et al., 2011; Mandad et al., 2017) for some recent advances. Despite the significant progress, state-of-the-art techniques tends to hit a barrier on the quality of maps that are computed in a pairwise manner. Building upon the availability of big-data, a recent line of research (Kim et al., 2012; Nguyen et al., 2011; Ovsjanikov et al., 2012; Huang et al., 2012; Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Zhou et al., 2015a;b; Shen et al., 2016; Leonardos et al., 2017; Huang et al., 2017) considered computing many pairwise maps jointly among a collection of objects. The promise of these approaches hinges upon the observation that one way to obtain a high quality pairwise map between dissimilar objects is to choose a path connecting these objects but consisting of consecutive similar shapes: maps between similar objects are typically of higher quality, and so is the resulted composed map. From a regularization perspective, joint map computation leverages the generic consistency of a network of maps among multiple objects, in which composition of maps along cycles are expected to be close to the identity map.\nHowever, the performance of these data-driven approaches relies predominantly on the homogeneity of the object collection, i.e. the input objects fall into the same category or sub-category (e.g. Chairs, Cars, and Human models). In the presence of heterogeneous data, where the input objects fall into multiple underlying categories, applying existing data-driven approaches without the category label information tend to produce unsatisfactory results. In this setting, even though existing methods are able to suppress the noise in intra-cluster maps within a single cluster, jointly computed maps for the entire object collection leads are often significantly worse. One explanation is that high fraction of incorrect inter-cluster maps tends to “contaminate” the regularization effect of intra-cluster maps. A natural resolution is to employ a two-stage cascadic strategy that identifies the underlying clusters before computing the intra- and inter-cluster maps. Unfortunately, such clustering requires accurate quantification of the object similarities, which is a difficult problem in its own right. Meanwhile, the error produced in the clustering stage is unlikely remedied by the consistency-based regularization.\nIn this paper, we propose to solve the mapping and clustering problems simultaneously. Instead of explicitly relying on certain pairwise similarity and/or map distortion\nscores, we identify the underlying clusters based on the consistency of intra- and inter-cluster maps, inspired by the observation that maps tend to be more consistent along cycles within a cluster than across clusters. This discrepancy has been observed in many different contexts, and appears to be a consequence of the energy landscape of almost all optimization-based pairwise matching algorithms. The matching energy functional between objects in the same underlying cluster tends to have simple energy landscapes with easily identifiable global optimums, resulting in fairly cycle-consistent intra-cluster maps; in contrast, the highly non-convex energy landscape for dissimilar objects from different clusters leads to more “random” maps due to random initialization and/or multiple strong local minimums, for which cycle-consistency is much less often observed. This map consistency argument is the foundation of our simultaneous mapping and clustering (SMAC) algorithm."
  }, {
    "heading": "1.1. Motivating Example",
    "text": "We validate the map consistency argument through a motivating example (see Figure 1) on a real dataset from SHREC07 Watertight benchmark (Giorgi et al., 2007). This dataset consists of 38 shapes: the first 18 are Human models and the remaining 20 are Fourleg models (e.g., Horses and Dogs). Each shape is represented as a discrete metric space with 1024 sample points generated from farthest-point sampling (Eldar et al., 1997). We compute pairwise blended intrinsic maps (Kim et al., 2011) for all objects in this collection and use these maps to compute two similarity scores for each object pair: a map distortion score that measures the squared sum of geodesic distortions across all pointpairs (c.f. (Bronstein et al., 2006)), and a cycle-consistency score that is the median value (among other options) of the distortion scores of all 3-cycles to which the pair belongs, where the distortion of a 3-cycle is defined as the squared sum of the geodesic distances between each point and its image propagated along the 3-cycle.\nFigure 1 illustrates the distributions of the map distortion scores (Left) and the cycle-consistency scores (Right) on the 38 models. The cycle-consistency scores clearly reveal the\nunderlying cluster structure and in fact better separates the two clusters of models (Human vs. Fourleg) than the mapdistortion scores (intra-cluster blocks in the right figure are darker in blue). The superior cluster separation is verified by comparing the results of spectral clustering (Ng et al., 2002; Lei & Rinaldo, 2015) using the two similarity scores: spectral clustering based on the cycle-consistency scores recovers the two underlying clusters perfectly, whereas the same procedure using the map distortion scores incorrectly puts two Fourleg models in the cluster of Human models. This motivating example illustrates the effectiveness and superiority of the map consistency score as a quantification of object similarity."
  }, {
    "heading": "1.2. Approach Overview",
    "text": "Motivated by the example above, we propose an algorithm for simultaneous mapping and clustering (SMAC). Our SMAC algorithm takes as input (i) an object collection that falls into multiple clusters, and (ii) some noisy maps precomputed between object pairs, and outputs the underlying clusters together with improved maps between all pairs of objects. Our SMAC algorithm builds upon the equivalence between map consistency and the low-rank property of a data matrix with consistent maps in its blocks (c.f. (Huang & Guibas, 2013)). We show that this low-rank property still holds in the setting of multiple disjoint collections of consistent intra-cluster maps, though the rank is expected to be higher due to multiple clusters. Based on this observation, the first step of our approach simultaneously recovers the underlying clusters and intra-cluster maps by spectral decomposition. We show that properly “rounding off” the leading eigenvectors recovers the ground-truth clusters and intracluster maps in a single pass. We then construct inter-cluster maps from the recovered intra-cluster maps. Our theoretical analysis establishes sharp exact recovery conditions for both steps under a fairly general noise model, using some novel tight L∞-stability bounds for eigen-decompositions."
  }, {
    "heading": "1.3. Related Works",
    "text": "Joint object matching, i.e., simultaneously estimating maps among a collection of objects, is an emerging field across many scientific problems. Earlier works use combinatorial optimizations (Nguyen et al., 2011; Kim et al., 2012; Huang et al., 2012). More recent works (Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Wang & Singer, 2013) rely on convex or non-convex optimization techniques.However, all these methods assume that the underlying object collection is homogeneous (all objects belong to a single category). For a heterogeneous object collection where the objects fall into multiple distinctive clusters, existing methods usually rarely succeed in producing both high-quality intra- and inter-cluster maps.\nClustering and in particular spectral clustering is another well-studied topic. We refer to (Lei & Rinaldo, 2015; Rohe et al., 2011) for some recent advances and to (Filippone et al., 2008; Luxburg, 2007; Fortunato, 2010) for surveys. Our approach falls into the general category of graph-based clustering, but the pairwise information we utilize is of “functional” rather than “scalar” nature. Instead of the more common approach that derives affinity scores from the pairwise maps for clustering, our SMAC algorithm discovers the cluster structure based purely on the consistency of pairwise maps and demonstrates improved empirical performance. This strategy is reminiscent of heterogeneous multireference alignment (Boumal et al., 2017) and simultaneous alignment and classification (Lederman & Singer, 2016) for synchronization problems in Cryo-Electron Microscopy; in this context, our residue-based clustering strategy is closest in nature to learning group actions (Gao et al., 2016).\nOur approach relies on tight L∞-type bounds on leading eigenvectors of perturbed matrices. Though the stability of eigenvalues and eigenspaces are well-studied, element-wise eigenvector stability appears to be a much harder problem; see recent survey (O’Rourke et al., 2016). We introduce new stability bounds to tackle this technical difficulty."
  }, {
    "heading": "1.4. Mathematical Notation",
    "text": "We use lower bold letters a, b, c,u,v,w, · · · to denote vectors, and upper letters A,B,C, · · · for matrices. For a block matrix X ∈ Rn1m1×n2m2 , we use Xij ∈ Rm1×m2 to denote its ij-th block; the ij-th element of a matrix A is denoted as aij . With ⊗ we denote the Kronecker product. For a symmetric matrix A ∈ Rn×n, we always sort the eigenvalues in non-increasing order, i.e., λ1(A) ≥ · · · ≥ λn(A). Matrix norms ‖ · ‖F , ‖ · ‖1, ‖ · ‖2, and ‖ · ‖∞ will be used for a matrix A ∈ Rn1×n2 , of which ‖ · ‖2 = σmax(A) is the maximum singular value of A, and the spectral norm ‖ · ‖2 is sometimes simplified as ‖ · ‖. We denote Pm for the set of permutation matrices of dimension m×m."
  }, {
    "heading": "2. Algorithm",
    "text": "For simplicity, we focus on describing and analyzing our algorithm under the setting where pair-wise maps are given by permutations. In Section 4, we show how to modify the algorithm for partially similar objects.\nWe first describe the problem setup. Consider n objects S = {S1, · · · , Sn} each represented by m points. With G = (S, E) we denote an observation graph among S. An initial map X inij ∈ Pm is pre-computed on each edge (i, j) ∈ E using an off-the-shelf pairwise object matching algorithm from Si to Sj . We also assume the objects in S are partitioned into k ≥ 2 clusters, but k is unknown. Our goal is to identify the underlying clusters, and in the mean-\nAlgorithm 1 PermSMAC: Simultaneously mapping and clustering Input: Observation graph G = (S, E) and initial pairwise\nmaps X inij , (i, j) ∈ E Output: Underlying clusters S = c1 ∪ · · · ∪ ck and opti-\nmized pairwise maps Xij , 1 ≤ i, j ≤ n 1: {Step 1} Simultaneously compute the intra-cluster\nmaps and extract the underlying clusters: 2: {Step 1.1} Form data matrix based on (1). 3: {Step 1.2} Compute the critical value r =\nargmax 2≤i≤nm\nλi−λi+1 λi+λi+1 .\n4: {Step 1.3} LetU ∈ Rnm×r store the leading r eigenvectors of X . Compute pair-wise maps X?ij by solving (2) 5: {Step 1.4} Use fij(X?ij) as the affinity score and apply single-linkage clustering to obtain the underlying clusters 6: {Step 2} compute the inter-cluster maps by solving (6)\nwhile improve all pairwise maps between objects in S. As a basis for identifying the underlying clusters, we assume that the intra-cluster maps are more accurate (in terms of cycle-consistency) than inter-cluster maps.\nOur algorithm proceeds in two major steps. The first step simultaneously extracts the underlying clusters and computes intra-cluster maps. The second step then computes inter-cluster maps. Now we introduce these two steps in details. Algorithm 1 provides the pseudo code."
  }, {
    "heading": "2.1. Step I: Extract underlying clusters and compute intra-cluster maps.",
    "text": "Motivated from prior works for map synchronization (Pachauri et al., 2013; Shen et al., 2016), we use a block data matrix X ∈ Rnm×nm to encode the input maps:\nXij =\n{ (X inij − 1m11\nT ) (i, j) ∈ E 0 otherwise\n(1)\nOur approach is motivated by the empirical observation that the leading eigen-vectors of X reveal the underlying clusters and simultaneously denoise intra-cluster maps if intra-cluster maps are more accurate than inter-cluster maps. We provide the algorithmic details below; an analysis is provided in Section 3.\nGiven the data matrix, we first estimates the number of stable eigen-vectors as\nr = argmax m≤i≤nm λi − λi+1 |λi|+ |λi+1| .\nHere we search within the range of [m,nm], as we expect multiple underlying clusters. Let U ∈ Rnm×r store the\ntop r eigen-vectors of X , and divide U into n matrices U1, . . . , Un of shape m×r such that U = (UT1 , · · · , UTn )T . We then compute the estimated map X?ij along each edge (i, j) ∈ E by solving a linear assignment problem, i.e.,\nX?ij = min X∈Pm fij(X), fij(X) := ‖X · Ui − Uj‖2F (2)\nfor all 1 ≤ i, j ≤ n. Note that (2) admits an exact solution via linear assignment. In fact,\nX?ij = argmin X∈Pm ‖XUi − Uj‖2F (3)\n= argmin X∈Pm (‖Ui‖2F + ‖Uj‖2F − 2〈XUi, Uj〉) (4)\n= argmax X∈Pm 〈X,UjUTi 〉. (5)\nIntuitively, UjUTi provides an approximation of the underlying map Xij , and the linear assignment procedure projects this approximation onto the space of permutations.\nFor clustering, we treat the residual score fij(X?ij), 1 ≤ i, j ≤ n as the distance measure between Si and Sj , and apply single-linkage clustering (Gower & Ross, 1969) to obtain the underlying clusters. We set k = [ rm−1 ] as the number of desired clusters.\nEmpirically, we found that when the input inter-cluster maps are inaccurate, the quality of estimated inter-cluster maps appear to be much more noisy than estimated intra-cluster maps. This motivates us to re-estimate inter-cluster maps as a second step, described in Section 2.2 below."
  }, {
    "heading": "2.2. Step II: Estimate inter-cluster maps.",
    "text": "We estimate the inter-cluster maps between each pair of clusters independently. Specifically, consider two clusters cs and ct . Let Sis ∈ cs and Sit ∈ ct be a pair of objects selected from each cluster, respectively. We optimize the inter-cluster map X interst , represented as a pairwise object map Xisit , by solving the following linear assignment:\nX interst = argmin X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E ‖X −XjitX inij Xiis‖1\n= argmax X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E 〈X,XjitX inij Xiis〉. (6)\nNote that it is possible to jointly optimize X interst among all pairs of clusters. However, since the number of clusters is usually significantly smaller than the size of each cluster, we found the gain of doing so is insignificant."
  }, {
    "heading": "3. Analysis",
    "text": "We first describe our noise model in Section 3.1. We then analyze our method under this model and present the exact\nrecovery conditions for both underlying clusters and the pairwise maps in Section 3.2. Our analysis is based on a set of new stability bounds of eigen-decompositions. The proofs of all Lemma’s and Theorem’s with technical details can be referred to in the supplemental material."
  }, {
    "heading": "3.1. Model for Analysis",
    "text": "We consider two models, one for the pairwise map and the other one for the observation graph and the underlying cluster structure.\nMap model. We generalize the map model described in (Shen et al., 2016) to multiple clusters: Suppose there are k underlying clusters. With cs, 1 ≤ s ≤ k we denote the vertex indices of the s-th cluster. In other words, we have {1, · · · , n} = c1 ∪ c2 ∪ · · · ∪ ck. Given an observation graph, the input pairwise maps are independent, and they follow\nX inij = { Im with probability ηij UPm with probability 1− ηij\nwhere UPm denotes a random permutation matrix satisfying\nE[UPm ] = 1\nm 11T . (7)\nηij depends on the edge type: ηij = p if (i, j) is an intra-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s≤kcs × cs), and ηij = q if (i, j) is an inter-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s6=t≤kcs × ct). We assume p > q. Remark 1. Note that one can also generalize the model by assuming there exist underlying permutations Pi, 1 ≤ i ≤ n, so that the ground-truth map Xij = PjPTi . Nevertheless, it turns out that the two models are identical (Shen et al., 2016) . Hence we adopt the simpler form for convenience.\nModel for the observation graph and clusters. To obtain concise and interpretable exact recovery conditions, we are particularly interested in analyzing our algorithm when the observation graph and underlying clusters are generated from the following established model: assume n = n0k, and the size of each underlying cluster |cs| = n0, 1 ≤ s ≤ k; the observation graph is generated from the Erdős-Rényi G(n, t) with edge selection probability t. However, the stability bounds we introduce in the supplemental material can be used for analyzing more general noisy models, e.g., sizes of the underlying clusters are different."
  }, {
    "heading": "3.2. Map and Cluster Recovery",
    "text": "We begin by analyzing the leading eigenvalues and eigenvectors of E[X] to gain insights on the necessary conditions for map and cluster recovery. To make our discussion more general, we first assume the underlying cluster and the observation graph are fixed. Consider then the following (p, q)-\nreweighted normalized adjacency matrix A(p, q) ∈ Rn×n:\n(A(p, q))ij =  p (i, j) ∈ E ∩ ∪1≤s≤m(cs × cs),q (i, j) ∈ E ∩ ∪1≤s 6=t≤m(cs × ct). 0 otherwise\n(8) It is clear that\nE[X] = A(p, q)⊗ (Im − 1\nm 11T )\nand thus the non-zero eigenvalues of E[X] are non-zero eigenvalues ofA(p, q) with multiplicitym−1. Furthermore, let ( 1√\nm 1, Hm) be an orthonormal basis for Rm. Then\nthe leading k(m − 1) eigenvectors of E[X] are given by Sk ⊗Hm. This leads two conditions on the eigenvalues and eigenvectors of A(p, q) for map and cluster recovery:\n• Eigenvalue separation. Since our method leverages the largest eigengap, we assume that λk(A(p, q)) − λk+1(A(p, q)) has the largest eigengap. Define\nγ =\nmax 1≤i6=k≤n−1\nλi(A(p, q))− λi+1(A(p, q))\nλk(A(p, q))− λk+1(A(p, q)) ,\nThen a necessary condition for map recovery is γ < 1.\n• Eigenvector separation. We further assume that the underlying clusters can be recovered by reading off the rows of Sk. Formally, consider rows of Sk as coordinates of the corresponding objects, and define\ndintra = max 1≤s≤k max i,j∈cs\n‖(ei − ej)TSk‖, (9)\ndinter = min 1≤s<t≤k min i∈cs,j∈ct\n‖(ei − ej)TSk‖. (10)\ndintra and dinter essentially measure the maximum distance within each cluster and the minimum distance between different clusters, respectively. Thus, a necessary condition for cluster recovery is that dintra < µ · dinter for some small constant µ.\nUnder these two conditions, it is easy to see that when X ≈ E[X], we have Ui ≈ (eTi Sk) ⊗ Hm and UjUTi ≈ (eTj SkS T k ei)(Im− 1m11\nT ). It follows that both the underlying clusters and intra-cluster maps can be exactly recovered.\nThese two separation conditions are quite general. In fact, they hold for the noisy model described in Section 3.1. To gain some further insight, one can show that (c.f. (Le et al., 2017)) with high probability\nλ1(A(p, q)) =  q + p−qk +O( 1√ nt ) i = 1 p−q k +O( 1√ nt ) 2 ≤ i ≤ k O( 1√\nnt ) k + 1 ≤ i ≤ n\nindicating that γ = o(1). Moreover, under this model\nA(p, q) ≈ (p− q)Ik ⊗ (11T ) + q(11T ).\nIf p and q are well-separated, the top k eigenvectors of A(p, q) approximate Ik⊗1, meaning dintra ≈ 0 and dinter ≈ 1. Now we formally state the exact recovery conditions:\nTheorem 3.1. (Intra-Cluster and Map Recovery) Assume t = Ω( log(n)n ). Consider the noise model described in Section 3.1. There exists an absolute constant cintra such that PermSMAC recovers the underlying intra-cluster maps and the underlying clusters with high probability if\np− q ≥ cintrak √ log(n)\nnt . (11)\nRemark 2. Note that the gap between p and q is used to ensure the recovery of the underlying clusters. Moreover, the recovery rate matches the information theoretic lower bound established in (Chen et al., 2016) up to O( √ log(n), indicating the tightness of our condition for PermSMAC.\nThe following theorem provides an exact recovery condition for the inter-cluster maps. Compared with the previous lower bound, the lower bound on q for inter-cluster map recovery is significantly cruder. This shows the advantage of recovering inter-cluster maps as a separate step.\nTheorem 3.2. There exists an absolute constant cinter > 0, so that when q ≥ cinterk √ log(n)\nn2t ,\nPermSMAC recovers the underlying inter-cluster maps with high probability."
  }, {
    "heading": "4. Partial Matching",
    "text": "In this section we extend the algorithm to handle partially (as opposed to fully) similar objects. Each input object Si (1 ≤ i ≤ n) in this setting has mi ∈ N+ points, where the mi’s vary across the collection. Consequently, the pairwise maps X inij ∈ {0, 1}mj×mi are no longer permutation matrices since some rows and columns may only contain zero elements. We propose the following modified algorithm for SMAC in this partial matching setting.\nStep I: Extract underlying clusters and compute intracluster maps. Forming the data matrix and leading eigenvector computation stay the same as in the full matching setting, except we replace X inij − 1m11\nT by X inij in (1). The first difference occurring in the partial matching setting is that we cannot apply (2) to obtain pair-wise maps and affinity scores for clustering. Our strategy is to apply single linkage clustering on the rows of U . Specifically, the distance measure between two points p, p′ is given by\n‖up − up′‖2, where up is the corresponding row of p in U . The number of output clusters is set as r. Each output cluster of this single-linkage procedure collects a set of matched points among the input objects. We merge two clusters if the objects they belong to overlap. Suppose we finally obtain k clusters c1, · · · , ck. For each cluster ci, we introduce a binary matrix Yi ∈ {0, 1}ni×ri , whose columns encode the enclosed point clusters. Then it is easy to see that the blocks of YiY Ti describe intra-cluster maps. Note that ri = m in the full setting, but in the partial setting ri is usually larger than max1≤i≤nmi, due to partial similarity.\nStep II: Compute inter-cluster maps. In the partial setting, we encode the inter-cluster map from cluster cs to cluster ct as a matrix Xst ∈ {0, 1}rt×rs . Consider a object pair (i, j) ∈ E , where i ∈ cs and j ∈ ct. With Ei,s and Ej,t we denote the index matrices that extract the corresponding blocks in Ys and Yt. It is easy to see that the entries Y Tt Ej,tX in ij E T i,sYs provide cues for the inter-cluster map Xst. Similar to the full map setting, we compute\nCst = ∑\ni∈cs,j∈ct,(i,j)∈E\nY Tt Ej,tX in ij E T i,sYs.\nSince the inter-cluster maps may not be permutation matrices either, we apply a simple thresholding to obtain the inter-cluster maps:\nXst = Cst > βst,\nwhere βst is set as 0.9 times the maximum element of Cst in our experiments."
  }, {
    "heading": "5. Experimental Results",
    "text": "In this section, we evaluate our approach on both synthetic (Section 5.1) and real datasets (Section 5.2 and Section 5.3). For baseline comparison, we consider state-of-the-art approaches for clustering and joint mapping in each domain."
  }, {
    "heading": "5.1. Experimental Results on Synthetic Instances",
    "text": "We apply the model described in Section 3 to generate the synthetic data sets for our experiments. Below we summarize how the procedure depends on the model parameters:\n• The observation graph G. We employ a standard twocommunity stochastic block model (Abbe et al., 2016) which enables us to fully control the vertex degree and the spectral gap. We use this model to generate three observation graphs G1,G2,G3. All of them have n = 300 vertices, but the vertex degrees and spectral gaps vary. Specifically, G1 is the clique graph. G2 is a sparse graph, whose average vertex degree is 50 and the spectral gap is 0.1. The average vertex degree and spectral gap of G3 are 50 and 0.5, respectively. In\nour experiment, we treat G1 as the default observation graph. We also study the influence of the observation graphs on the performance of our algorithm.\n• Number of clusters k. Without loss of generality, we allocate each object into a underlying cluster with probability 1k . For each observation graph, we generate and fix one underlying cluster throughout our experiments.\n• Other parameters m, γ, p, q. We fix the number of points on each object as m = 30. In the partial matching setting, we follow the protocol (Chen et al., 2014) to generate the input objects so that the expected size of each object is mγ. We sample the ratio of correct inter-cluster maps q = exp(− i10 ), 15 ≤ i ≤ 50. Since p > q, we sample the ratio of correct intra-cluster maps so that p− q = i100 , 1 ≤ i ≤ 30.\nWe now study the empirical phase transition curves when varying p and q under different k, γ and observation graphs.\nVarying k. The first two rows of Figure 2 show the phase transition curves of map and cluster recovery for k = 2, 4, 6. Across all configurations, our approach tolerates a significant portion of noise in the input maps. The fraction of noise that our approach can handle reduces as k increases, which is consistent with the exact recovery conditions in Section 3. In addition, phase transitions with respect to mapping recovery and cluster recovery roughly align. The subtle differences are two-fold: when p and q are close, cluster recovery breaks as there is no cue for clustering; likewise, map recovery breaks when q approaches 0.\nVersus mapping only. Figure 2 compares our approach to state-of-the-art map recovery technique (Huang & Guibas, 2013). SMAC clearly exhibits a clear advantage in the regime, where q is small and p is significantly larger than q. This is expected through our exact recovery condition.\nVarying observation graph. Figure 3 shows phase transition curves of map and cluster recovery when varying the observation graphs (k = 2, γ = 1). Our approach tolerates a larger fraction of incorrect maps for larger vertex degrees. Moreover, when vertex degrees are comparable, a small spectral gap means higher recovery rate.\nVarying γ. Figure 4 shows the phase transition curves when varying the overlapping ratio γ. We again show three configurations, i.e., γ = 1, γ = 0.8 and γ = 0.6. Still, our approach can tolerate a large rate of noise in the input maps. Moreover, the rate reduces as γ becomes smaller. This is expected, as low overlapping ratio introduces weak signal for mapping and cluster recovery."
  }, {
    "heading": "5.2. Experimental Results on 3D Shapes",
    "text": "We proceed to evaluate SMAC on 3D shapes. We consider two datasets for this task. The first dataset collects four categories of 3D shapes from SHREC07-Watertight (Giorgi et al., 2007), namely, Human, Fourleg, Armadillo and Teddy. These categories appear to have both similar global structures and local geometric details. However, the intercategory variability is salient. The second dataset is more fine-grained. It has 10 underlying shape collections from FAUST training dataset (Bogo et al., 2014), where each collection consists of different poses of the same human subject (10 poses per collection). For evaluating shape maps, we follow the protocol of (Kim et al., 2011) by collecting statistics on the geodesic distortion of predicted correspondences with respect to human annotated feature correspondences.\nMapping performance. Figure 5(c) plots the accuracy of predicted correspondences of our approach versus the input. For a detailed assessment, we separate the statistics of intra-cluster maps and inter-cluster maps. We consider two approaches for baseline comparison: the first applies (Huang & Guibas, 2013) to the entire dataset, and the second applies (Huang & Guibas, 2013) to each category in isolation then applies the third step of our approach to compute intercluster maps. The second baseline may be considered as a performance upper bound. Our proposed SMAC is significantly better than mapping without clustering (which is seriously affected by the noise in inter-cluster maps) and competitive against the second baseline.\nClustering performance. As shown in Table 1, our approach correctly identifies all underlying clusters in SHREC07 and FAUST. We also applied two baseline clustering approaches on the same dataset. The first approach performs k-means on the spectral shape descriptors (Rustamov, 2007). This approach only yields 84.6% and 72.0%, respectively. The second approach utilizes the mapping distortion as an affinity measure and applies spectral clustering. This approach yields 94.9% and 74.0%, respectively, which are better than the first baseline. However, our approach is still better, which shows the advantage of using the cycle-consistency constraint for clustering."
  }, {
    "heading": "5.3. Experimental Results on 2D Images",
    "text": "Finally, we evaluate our approach on two datasets of 2D images. The first dataset (Figure 6(Left)) consists of 600 internet images of Notre Dame. These images naturally fall into 3 categories, each of which collects images from a similar camera pose (Snavely et al., 2006). The second dataset (Figure 6(Right)) collects 600 internet images of 4 landmark churches in Europe (Amiens Cathedral (200 im-\nages), York Minster (200 images), Duomo (100 images) and Westminster Abbey (100 images)). As inter-cluster maps do not make much sense here, we only evaluate clustering results and intra-cluster maps in this experiment. We sample 400 SIFT features (Lowe, 2004) for each image and apply SIFT flow (Liu et al., 2011) to establish pairwise correspondences between the features. We manually mark feature correspondences for evaluation.\nMapping performance. Figure 6 compares our approach with the two baseline approaches introduced in Section 5.2. The relative performance is consistent. Specifically, due to the small-overlapping region across different clusters, intercluster maps are rather noisy, so applying joint-mapping directly leads to sub-optimal results. In addition, our approach is competitive against the approach of computing intra-cluster and inter-cluster maps in a sequential manner.\nClustering performance. Finally, we evaluate our approach in terms of clustering accuracy. We choose two base-\nline approaches, where the first baseline approach performs k-means on image descriptors. In this case, we employ GIST (Oliva & Torralba, 2001). The second baseline uses the residual of SIFT flow as the affinity score for clustering. As shown in Table 1, our approach leads to a clustering accuracy of 99.3% and 96.1% on Notre Dame and Church, respectively. They are higher than those of the top performing baselines, i.e., 94.5% and 92.1%, respectively."
  }, {
    "heading": "6. Conclusions",
    "text": "We have introduced SMAC for simultaneously computing consistent maps across a heterogeneous data collection and identifying the underlying clusters. The key idea is to leverage the higher self-consistency within intra-cluster maps than inter-cluster maps. Enforcing this variation of consistency allows us to denoise the input maps in a sequential manner while simultaneously identifying the underlying cluster structures. The approach is based on spectral decomposition, for which we provided tight exact recovery conditions for both the input maps and the underling clusters. Experimental results on synthetic data sets justify our exact recovery conditions, and experimental results on real data sets demonstrate the efficacy of our approach.\nAcknowledgement. This research by Chandrajit Bajaj was supported in part by a grant from the NIH, R01 GM117594. Tingran Gao acknowledges support from Simons Math+X Investigators Award 400837, DARPA D15AP00109, and NSF IIS 1546413. Qixing Huang would like to acknowledge support for this research from NSF DMS-1700234, NSF CIP-1729486, and NSF IIS-1618648."
  }],
  "year": 2018,
  "references": [{
    "title": "Exact recovery in the stochastic block model",
    "authors": ["E. Abbe", "A.S. Bandeira", "G. Hall"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2016
  }, {
    "title": "FAUST: Dataset and evaluation for 3D mesh registration",
    "authors": ["F. Bogo", "J. Romero", "M. Loper", "M.J. Black"],
    "venue": "In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2014
  }, {
    "title": "Heterogeneous multireference alignment: a single pass approach",
    "authors": ["N. Boumal", "T. Bendory", "R.R. Lederman", "A. Singer"],
    "venue": "arXiv preprint arXiv:1710.02590,",
    "year": 2017
  }, {
    "title": "Generalized multidimensional scaling: A framework for isometry-invariant partial surface matching",
    "authors": ["A.M. Bronstein", "M.M. Bronstein", "R. Kimmel"],
    "venue": "In PNAS,",
    "year": 2006
  }, {
    "title": "Near-optimal joint object matching via convex relaxation",
    "authors": ["Y. Chen", "L.J. Guibas", "Q. Huang"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Information recovery from pairwise measurements",
    "authors": ["Y. Chen", "C. Suh", "A.J. Goldsmith"],
    "venue": "IEEE Trans. Information Theory,",
    "year": 2016
  }, {
    "title": "The farthest point strategy for progressive image sampling",
    "authors": ["Y. Eldar", "M. Lindenbaum", "M. Porat", "Y.Y. Zeevi"],
    "venue": "IEEE Trans. Image Processing,",
    "year": 1997
  }, {
    "title": "A survey of kernel and spectral methods for clustering",
    "authors": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"],
    "venue": "Pattern Recogn.,",
    "year": 2008
  }, {
    "title": "Community detection in graphs",
    "authors": ["S. Fortunato"],
    "venue": "Physics Reports,",
    "year": 2010
  }, {
    "title": "The Geometry of Synchronization Problems and Learning Group Actions",
    "authors": ["T. Gao", "J. Brodzki", "S. Mukherjee"],
    "venue": "arXiv preprint arXiv:1610.09051,",
    "year": 2016
  }, {
    "title": "Shape retrieval contest 2007: Watertight models track",
    "authors": ["D. Giorgi", "S. Biasotti", "L. Paraboschi"],
    "year": 2007
  }, {
    "title": "Minimum spanning trees and single linkage cluster analysis (ref: 69v18 p106-110)",
    "authors": ["J.C. Gower", "G.J.S. Ross"],
    "venue": "Applied Statistics,",
    "year": 1969
  }, {
    "title": "Consistent shape maps via semidefinite programming",
    "authors": ["Q. Huang", "L.J. Guibas"],
    "venue": "Comput. Graph. Forum,",
    "year": 2013
  }, {
    "title": "Joint shape segmentation with linear programming",
    "authors": ["Q. Huang", "V. Koltun", "L. Guibas"],
    "venue": "ACM Trans. Graph.,",
    "year": 2011
  }, {
    "title": "An optimization approach for extracting and encoding consistent maps in a shape collection",
    "authors": ["Q. Huang", "G. Zhang", "L. Gao", "S. Hu", "A. Butscher", "L.J. Guibas"],
    "venue": "ACM Trans. Graph.,",
    "year": 2012
  }, {
    "title": "Functional map networks for analyzing and exploring large shape collections",
    "authors": ["Q. Huang", "F. Wang", "L.J. Guibas"],
    "venue": "ACM Trans. Graph.,",
    "year": 2014
  }, {
    "title": "Translation synchronization via truncated least squares",
    "authors": ["X. Huang", "Z. Liang", "C. Bajaj", "Q. Huang"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Fully automatic registration of multiple 3d data sets",
    "authors": ["D.F. Huber", "M. Hebert"],
    "venue": "Image and Vision Computing,",
    "year": 2001
  }, {
    "title": "Exploring collections of 3d models using fuzzy correspondences",
    "authors": ["V. Kim", "W. Li", "N. Mitra", "S. DiVerdi", "T. Funkhouser"],
    "venue": "ACM Trans. Graph.,",
    "year": 2012
  }, {
    "title": "Blended intrinsic maps",
    "authors": ["V.G. Kim", "Y. Lipman", "T. Funkhouser"],
    "venue": "ACM Trans. Graph.,",
    "year": 2011
  }, {
    "title": "Cross-parameterization and compatible remeshing of 3d models",
    "authors": ["V. Kraevoy", "A. Sheffer"],
    "venue": "In ACM SIGGRAPH 2004 Papers,",
    "year": 2004
  }, {
    "title": "Concentration and regularization of random graphs",
    "authors": ["C.M. Le", "E. Levina", "R. Vershynin"],
    "venue": "Random Struct. Algorithms,",
    "year": 2017
  }, {
    "title": "A representation theory perspective on simultaneous alignment and classification",
    "authors": ["R.R. Lederman", "A. Singer"],
    "venue": "arXiv preprint arXiv:1607.03464,",
    "year": 2016
  }, {
    "title": "Consistency of spectral clustering in stochastic block models",
    "authors": ["J. Lei", "A. Rinaldo"],
    "venue": "Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Distributed consistent data association via permutation synchronization",
    "authors": ["S. Leonardos", "X. Zhou", "K. Daniilidis"],
    "venue": "In ICRA,",
    "year": 2017
  }, {
    "title": "Sift flow: Dense correspondence across scenes and its applications",
    "authors": ["C. Liu", "J. Yuen", "A. Torralba"],
    "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
    "year": 2011
  }, {
    "title": "Distinctive image features from scale-invariant keypoints",
    "authors": ["D.G. Lowe"],
    "venue": "Int. J. Comput. Vision,",
    "year": 2004
  }, {
    "title": "A tutorial on spectral clustering",
    "authors": ["U. Luxburg"],
    "venue": "Statistics and Computing,",
    "year": 2007
  }, {
    "title": "Variance-minimizing transport plans for inter-surface mapping",
    "authors": ["M. Mandad", "D. Cohen-Steiner", "L. Kobbelt", "P. Alliez", "M. Desbrun"],
    "venue": "ACM Transactions on Graphics,",
    "year": 2017
  }, {
    "title": "On spectral clustering: Analysis and an algorithm",
    "authors": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2002
  }, {
    "title": "An optimization approach to improving collections of shape maps",
    "authors": ["A. Nguyen", "M. Ben-Chen", "K. Welnicka", "Y. Ye", "L.J. Guibas"],
    "venue": "Comput. Graph. Forum,",
    "year": 2011
  }, {
    "title": "Modeling the shape of the scene: A holistic representation of the spatial envelope",
    "authors": ["A. Oliva", "A. Torralba"],
    "venue": "Int. J. Comput. Vision,",
    "year": 2001
  }, {
    "title": "Eigenvectors of random matrices: A survey",
    "authors": ["S. O’Rourke", "V. Vu", "K. Wang"],
    "venue": "J. Comb. Theory, Ser. A,",
    "year": 2016
  }, {
    "title": "Functional maps: a flexible representation of maps between shapes",
    "authors": ["M. Ovsjanikov", "M. Ben-Chen", "J. Solomon", "A. Butscher", "L.J. Guibas"],
    "venue": "ACM Trans. Graph.,",
    "year": 2012
  }, {
    "title": "Solving the multiway matching problem by permutation synchronization",
    "authors": ["D. Pachauri", "R. Kondor", "V. Singh"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Example-based 3d scan completion",
    "authors": ["M. Pauly", "N.J. Mitra", "J. Giesen", "M. Gross", "L.J. Guibas"],
    "venue": "In Proceedings of the Third Eurographics Symposium on Geometry Processing,",
    "year": 2005
  }, {
    "title": "Spectral clustering and the high-dimensional stochastic blockmodel",
    "authors": ["K. Rohe", "S. Chatterjee", "B. Yu"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "Laplace-beltrami eigenfunctions for deformation invariant shape representation",
    "authors": ["R.M. Rustamov"],
    "venue": "In Proceedings of the Fifth Eurographics Symposium on Geometry Processing,",
    "year": 2007
  }, {
    "title": "Inter-surface mapping",
    "authors": ["J. Schreiner", "A. Asirvatham", "E. Praun", "H. Hoppe"],
    "venue": "In ACM SIGGRAPH 2004 Papers,",
    "year": 2004
  }, {
    "title": "Normalized spectral map synchronization",
    "authors": ["Y. Shen", "Q. Huang", "N. Srebro", "S. Sanghavi"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Photo tourism: Exploring photo collections in 3d",
    "authors": ["N. Snavely", "S.M. Seitz", "R. Szeliski"],
    "venue": "ACM Trans. Graph.,",
    "year": 2006
  }, {
    "title": "A survey on shape correspondence",
    "authors": ["O. van Kaick", "H.R. Zhang", "G. Hamarneh", "D. Cohen-Or"],
    "venue": "In EuroGraphics: State-of-the-Art Report,",
    "year": 2010
  }, {
    "title": "Image cosegmentation via consistent functional maps",
    "authors": ["F. Wang", "Q. Huang", "L.J. Guibas"],
    "venue": "In ICCV, pp",
    "year": 2013
  }, {
    "title": "Unsupervised multi-class joint image segmentation",
    "authors": ["F. Wang", "Q. Huang", "M. Ovsjanikov", "L.J. Guibas"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Exact and stable recovery of rotations for robust synchronization",
    "authors": ["L. Wang", "A. Singer"],
    "venue": "Information and Inference: A Journal of the IMA,",
    "year": 2013
  }, {
    "title": "Flowweb: Joint image set alignment by weaving consistent, pixelwise correspondences",
    "authors": ["T. Zhou", "Y.J. Lee", "S.X. Yu", "A.A. Efros"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "Multi-image matching via fast alternating minimization",
    "authors": ["X. Zhou", "M. Zhu", "K. Daniilidis"],
    "venue": "CoRR, abs/1505.04845,",
    "year": 2015
  }],
  "id": "SP:4216ce1d861625829ef55fd6b322f9e5341ba30f",
  "authors": [{
    "name": "Chandrajit Bajaj",
    "affiliations": []
  }, {
    "name": "Tingran Gao",
    "affiliations": []
  }, {
    "name": "Zihang He",
    "affiliations": []
  }, {
    "name": "Qixing Huang",
    "affiliations": []
  }, {
    "name": "Zhenxiao Liang",
    "affiliations": []
  }],
  "abstractText": "We introduce a principled approach for simultaneous mapping and clustering (SMAC) for establishing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes). Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous object clustering together with a new set of maps possessing optimal intraand inter-cluster consistency. Our approach is based on the spectral decomposition of a data matrix storing all pairwise maps in its blocks. We additionally provide tight theoretical guarantees for the accuracy of SMAC under established noise models. We also demonstrate the usefulness of our approach on synthetic and real datasets.",
  "title": "SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions"
}