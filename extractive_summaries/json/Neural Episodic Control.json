{
  "sections": [{
    "text": "Keywords: Neural Episodic Control, Random Projection, Deep Reinforcement Learning, Atari"
  }, {
    "heading": "1. Introduction",
    "text": "Recent advances in deep learning have allowed more complex function approximation and feature extraction. Reinforcement learning has also benefited from this, and Deep QNetwork (DQN) (Mnih et al. (2015)) has been proposed, which enables end-to-end learning from only image. However, there are still problems that deep reinforcement learning has to solve. One of them is poor sample efficiency. Neural Episodic Control (NEC) (Pritzel et al. (2017)) has been proposed to solve it. NEC has introduced a differentiable dictionary, called Differentiable Neural Dictionary (DND), into the neural network architecture. This makes it possible to learn action values from a feature of a state stably and to learn with a small number of learning steps. One of the reasons is that it has reduced the parameters which have to learn by changing parametric prediction to non-parametric one. Furthermore, NEC’s architecture allows end-to-end learning, although extraction of features from images is parametric and output action values from them is non-parametric.\nIn this research, we propose a method that incorporates random projection, which is one of the dimensionality reduction methods without training parameters, into a part of NEC’s network in order to learn more stably. We also verify its effectiveness in video game experiments.\nc© 2019 D. Nishio & S. Yamane."
  }, {
    "heading": "2. Deep Reinforcement Learning",
    "text": "Reinforcement learning is an area of machine learning that an agent learns how to maximize the return of rewards obtained by interaction with the environment. The action value of a reinforcement learning agent taking the action a in the state s is defined by the following equation (1), where Gt = ∑ t ( γtrt ) is the sum of discounted rewards, and γ (0 ≤ γ < 1) is the discount factor that represents how important future rewards are.\nQπ(s, a) = Eπ [Gt|s, a] (1)\nThe action value is called Q-value, and Q-learning (Watkins and Dayan (1992)) is a method to estimate the value. It is a bootstrap estimation using Bellman equation (Bellman (1952)) as in the equation (2).\nQ(s, a)← Q(s, a) + α ( r + γmax a′ Q ( s′, a′ ) −Q(s, a) ) (2)\nIn value-based policy, the agent chooses an action that maximizes the estimated Q-value. However, it may not get better rewards for inexperienced states if it continues to choose the greedy action to maximize the value. In order to avoid this problem, there is a simple but powerful way called ε-greedy policy. It is written as the equation (3).\nπ(a|s) = {\n1− ε (a = argmaxaQ(s, a)) ε (otherwise)\n(3)\nClassically, Q-values are recorded in a Q-table, but in reality the state space is large and the action space may be continuous. Therefore, it is general to approximate an action value function.\nDeep learning is also a way of function approximation. It is able to extract features of states and to estimate the function which it is difficult for other approximation methods to express. DQN has attracted much attention for end-to-end learning by extracting embeddings from Convolutional Neural Network (CNN) (Krizhevsky et al. (2012)) only based on images and outputting action values in a Fully Connected (FC) layer based on the embeddings.\nHowever, Q-learning by nonlinear function approximation cannot generally guarantee the convergence, and it is difficult to learn stably. Therefore, DQN uses experience replay (Lin (1992)) which creates minibatches by taking experiences randomly, and has a network of the old parameters as the target value network. In particular, an efficient use of experience has been found to be important for fast and stable learning. Prioritized Experience Replay (PER) (Schaul et al. (2015)) has been proposed to weight experiences instead of conventional random sampling. The paper (Hessel et al. (2018a)) that has examined the importance and combination of various methods has also verified the effectiveness of PER, and it has shown that it has been very important to use experiences efficiently."
  }, {
    "heading": "3. Neural Episodic Control",
    "text": "Neural Episodic Control (NEC) (Pritzel et al. (2017)) has a memory stored experience inside the architecture to enhance sample efficiency further. NEC is based on Model-Free\nEpisodic Control (MFEC) (Blundell et al. (2016)). The main idea of MFEC is to store many experiences in a Q-table, and it estimates action values by a non-parametric method for embeddings extracted by Variational AutoEncoder (VAE) (Kingma and Welling (2013)) or random projection. In contrast, NEC adopts this idea as a part of the architecture by Differentiable Neural Dictionary (DND), which is a dictionary that enables to update by gradients for a Q-table of embeddings. This makes it possible to learn end-to-end while keeping the Q-table inside the network.\nSpecifically, the architecture of NEC can be divided into the following three parts.\n(a) Obtain an embedding h such as by convolutional layers.\n(b) Obtain an embedding h′ reduced dimensions of h by fully connected layers.\n(c) Lookup to output action values from h′ using a k-nearest neighbor algorithm and a kernel function.\nHere, we describe 2 and 3. In 2, NEC reduces the embedding dimensions obtained in 1. There are two reasons for doing it, the first is to reduce the space complexity of DND. The second is the reduction of time complexity of the k-nearest neighbor algorithm in 3.\nWe explain an idea for non-parametric estimation of action values in 3. It is assumed that the Q(h′, a) of each action a for the embedding h′i of a state si similar to the embedding h′ of a certain state s should be a similar value in many scenes.\nWhen h′ obtained in 2 and the corresponding action a are input, p keys h′i resembling h′ among the keys existing in DND are searched by k-nearest neighbor algorithm using kd-trees (Bentley (1975)). Let Qa be the weighted value vi corresponding to the p keys.\nwi = k (h′,h′i)∑ j k (h ′,h′j) (4)\nQa = p∑ i=1 wivi (5)\nThe function k in the equation (4) is a kernel function. For example, it is written as the equation (6) by the inverse kernel function.\nk ( h′,h′i ) =\n1\n‖h′ − h′i‖22 + δ (6)\nAlthough δ is the parameter to prevent division by zero, we should make it little larger such as δ = 10−3 because each value of p neighbors is referred.\nIn this way, NEC allows stable learning by estimating action values non-parametrically. While it has been possible to learn faster and stabler than many algorithms including DQN, it has been found that long-term training makes it inferior to other methods. The cause is that it is necessary to store a large number of embedding-action pairs in DND, and the insufficient buffer size cannot estimate action values well.\nNEC adopts multi-step Q-learning (Peng and Williams (1996)) as another technique for fast learning. One-step learning has the advantage that the variance of target value is low, but it also has the disadvantage that the propagation of rewards is slow. On the other hand, Monte Carlo Q-learning, which uses all the experiences of one episode, has the a rapid propagation of rewards, but unstable learning because of the high variance. Multi-step Qlearning has role as the trade-off the strengths and weaknesses of one-step Q-learning and Monte Carlo Q-learning.\nIn Rainbow (Hessel et al. (2018a)), it is better to set the value of this step number N to 3 or 5, but NEC sets N to 100 because of the stability of learning."
  }, {
    "heading": "4. Random Projection",
    "text": "Random Projection (RP) is a linear projection with a random matrix and is used to reduce dimensions of high-dimensional data. As the properties of the projection matrix, it is necessary to consider the time to construct the matrix and the quality of embedding after dimensionality reduction. The difference between the main methods of random projection is as shown in Table 1.\nFrom here, we describe Gaussian random projection (Hecht-Nielsen et al. (1994)) because it has the best Embedding quality. In this method, a dx dimensional vector x is multiplied by the Gaussian random matrix R to convert it into a dy dimensional vector y.\ny = Rx (7)\nIf the elements of the random matrix R are generated by random numbers in accordance with a Gaussian distribution (mean 0, variance 1/dy), the distance between the data is approximately maintained with high probability 1−O ( e−dxε 2 ) when any n training data\nx(j)(j = 1, . . . , n) are projected in the dy=O(ε−2 logn) dimensions[ Johnson and Lindenstrauss (1984), Vempala (2004)] where ε is the distortion.\n(1− ε) ‖xj − xi‖22 ≤ ‖yj − yi‖ 2 2 ≤ (1 + ε) ‖xj − xi‖ 2 2 (8)\nGaussian random projection is distinguished by its simplicity and high quality of embedding. The paper (Dasgupta (2013)) that actually experimented with random projection has shown that the relationship between the dimensions of the vector before projection and the dimensions of the vector after the projection is theoretically guaranteed. Moreover, it has been applied to the EM algorithm and the performance is improved. Another paper (Bingham and Mannila (2001)) applied to an image or text has reported to show good performance even if we have reduced dimensions under weaker conditions than the theoretically guaranteed the inequality (8). It has also shown that if the dimension is too small, the accuracy drops sharply.\nRandom projection has a good property that there is no restriction on the magnitude of each value of data. This means that the inequality (8) holds without normalization.\nAlso, random projection has good compatibility with the kd-trees because information other than Euclidean distance is redundant for the algorithm based on the closeness of the distance. In other words, we can use it for the data projected by random projection. In fact, RP-kd-Trees (Wu et al. (2011)), which uses random projection for dimension reduction, has been also proposed."
  }, {
    "heading": "5. Related Work",
    "text": "There are several researches that have improved or combined NEC. A representative one is Ephemerally Value Adjustments (EVA) (Hansen et al. (2018)). It has improved the performance by combining a value buffer like NEC and other planning algorithms only in decision making. Also, there are combines parametric methods such as DQN with nonparametric methods such as NEC. Semiparametric Reinforcement Learning (SRL) (Jain and Lindsey (2018)) has proposed a method using action values that combine the values output by neural network and the values estimated by NEC architecture. NEC2DQN (Nishio and Yamane (2018)) has also made use of learning efficiency of NEC to assist to learn about a parametric network.\nDeep reinforcement learning using random projection such as MFEC has been proposed. Episodic Memory Deep Q-Netwroks (EMDQN) (Lin et al. (2018)) has regularized action value with the values output using random projection. However, they differ from our\nproposed algorithm in that they use random projection outside their neural networks, in other words, they are not end-to-end networks.\nMore recent research has proposed an architecture using random projection for deep neural networks (Wójcik (2018)). Although the networks train with embeddings projected by random projection, we do not use it as the input of neural networks, but utilize the relationship of the embedding distance."
  }, {
    "heading": "6. Proposed Algorithm",
    "text": "As NEC has shown, non-parametric methods such as the k-nearest neighbor algorithm are good for increasing the learning speed of agents. One of the reasons is that there are no training parameters which have to learn. NEC reduces the dimensions of embeddings output by CNN with FC layers, calculates the Euclidean distance of nearby embeddings using k-nearest neighbor algorithm, then outputs the action value.\nHowever, is it required to use fully connected layer for reducing dimensions? We think the layer must have the two important abilities.\n(a) It is able to keep the relationships of Euclidean distances for its inputs as possible.\n(b) It is differentiable in order to perform backpropagation.\nIt is important for the inverse kernel function to calculate the value is that the relationship of the distance is maintained as in the equation (6). In addition, the (b) is essential for end-to-end learning. Moreover, we would not like to train the layer because changing the parameters could cause unstable learning if it does not have to train.\nFortunately, random projection meets these conditions as stated later. Therefore, we perform stable learning by incorporating random projection which can keep the Euclidean distance relationship without training parameters. At the same time, we make the proposed architecture more general by end-to-end learning.\nWe propose NEC-RP, which incorporates random projection into the architecture of NEC. We show the algorithm in Algorithm 1."
  }, {
    "heading": "6.1. Random Projection layers",
    "text": "Here we introduce Random Projection (RP) layer. It is a linear projection and differentiable as in the equation (7). The partial derivative of it is given by the equation (9).\n∂y ∂x = RT (9)\nIn addition, we can regard the equation (7) as in the following neural network’s equation (10) by substituting the random matrix R for W and a zero vector for b.\ny = Wx + b (10)\nIn other words, we define our RP layer by Gaussian random projection as a layer of which the weight matrix is generated by Gaussian random (mean 0, variance 1/dy) and the bias is a zero vector. Therefore, we regard our RP layer as a special case of the FC layer. We show the difference between these layers in Table 3.\nAlgorithm 1 NEC-RP\nFC RP\nInitializer (weight) e.g. Gaussian\n(Mean=0, Variance=1)\ne.g. Gaussian (Mean=0,\nVariance=|unitsout|−1) Initializer (bias) e.g. zeros zeros\nParameters update fix"
  }, {
    "heading": "6.2. Neural Episodic Control with a Random Projection layer",
    "text": "We show NEC-RP architecture in Fig. 2. Of the three parts of the NEC architecture mentioned in chapter 3, we use the RP layer in 2. In this way, the value of inverse kernel function is approximately maintained as in the inequality (11) from the inequality (8) with δ which is a constant that prevents division by zero.\n(1− ε) ‖h− hi‖22 ≤ ‖h′ − h′i‖ 2 2 ≤ (1 + ε) ‖h− hi‖ 2 2\n(1− ε) ‖h− hi‖22 + δ ≤ ‖h′ − h′i‖ 2 2 + δ ≤ (1 + ε) ‖h− hi‖ 2 2 + δ\n1\n(1 + ε) ‖h− hi‖22 + δ ≤ 1‖h′−h′i‖22+δ\n≤ 1 (1− ε) ‖h− hi‖22 + δ\n1\n(1 + ε) ‖h− hi‖22 + δ ≤ k(h′,h′i) ≤\n1\n(1− ε) ‖h− hi‖22 + δ (11)\nThe value of the kernel function may be unstable if we use nonlinear function approximation methods such as deep neural network, but in NEC-RP it is guaranteed as in the inequality (11), hence it is possible to learn stably without training parameters."
  }, {
    "heading": "6.3. Replacing a Random Projection layer with a Fully Connected layer",
    "text": "The RP layer does not learn its parameters as shown in Table 3. Although the methods which has no training parameters have the advantage of not requiring learning, they have the disadvantage that the performance is inferior to ones which have training parameters as the learning progresses. However, if we modify the parameters of the RP layer to learn, we can consider it as a FC layer because the RP layer is possible to learn from the middle of training. For example, the simplest method is to switch the RP layer to a FC layer according to the time step t and the hyperparameter CS.\nh′ = { fRP (h) (t < CS) fFC(h) (otherwise)\n(12)\nThis switching is the same as pre-training the other parameters than the layer for dimensionality reduction (Fig. 3)."
  }, {
    "heading": "6.4. Initialization",
    "text": "The initialization of the convolution layers and the fully connected layer is the same as DQN and NEC. As for a random matrix in the RP layer, we should consider from the Table 1, but in NEC-RP we consider that Embedding quality is the most important factor. Therefore, we adopt Gaussian random projection. In other words, we use a random matrix R generated by Gaussian random with mean 0, variance 1/dy, where y is the vector output by the previous layer. Naturally the bias is a zero vector."
  }, {
    "heading": "7. Experiments",
    "text": "Alike DQN and NEC, we experiment with Atari2600, a video game environment provided by OpenAI Gym Brockman et al. (2016). As for baseline NEC, we use the one reproduced in reinforcement learning library Coach Caspi et al. (2017), and implement NEC-RP 1 with this framework. We also use Faiss Johnson et al. (2017) as an approximate nearest\n1. Our implementation is available on https://github.com/dnishio/NEC-RP.\nneighbor library. The target video games are {MsPacman, SpaceInvaders, Bowling, Boxing, DoubleDunk}. We experiment three times with different random seeds for each game, on the other hand, we use a fixed seed for random matrix values in our RP layer. In each experiment, our agent learns for 10M frames. In order to prevent the learning from becoming unstable due to the variation in the scale of the rewards of the games, DQN clips the rewards r to −1 ≤ r ≤ 1. However, clipping them affects the final sum of total rewards because high rewards are treated the same as small rewards [van Hasselt et al. (2016), Hessel et al. (2018b)]. NEC-RP also does not clip them because NEC achieved stable learning without it. The parameters such as CNN and DND are the same as NEC. We show the details of our experiment parameters and our architecture in Table ??, ?? in Appendix ??."
  }, {
    "heading": "7.1. NEC vs. NEC-RP",
    "text": "Here we compare NEC and NEC-RP. NEC reduces the embedding dimensions to 32 dimensions (NEC32), and NEC-RP reduces to 32 dimensions (NEC-RP32) and reduces to 16 dimensions (NEC-RP16). In the comparison of NEC32 and NEC-RP32, we verify whether our proposed architecture that introduced random projection outperforms the performances of NEC. Additionally, in the comparison between NEC-RP32 and NEC-RP16, we examine how the wide range of the inequality (11) affects their performances.\nWe have shown the result in Fig. 4. Comparing NEC32 and NEC-RP32, we have found that NEC-RP has obtained higher scores in the case of four out of five games. From this fact, we have considered that not only it has been possible to introduce random projection into NEC, but also NEC-RP has learned faster because the parameters required to update have been reduced. In particular, we have shown that it has been possible to stably learn even in the games with the rewards r > 1 such as MsPacman and Bowling. However, in Boxing performance of NEC has been superior to ours in the early stages of learning. This has implied that there have been some tasks have been difficult for NEC-RP.\nWe have also found that random projection has had bad performance if we reduce embedding dimensions too much. In fact, we have seen that the score has been worse in all five games when we have reduced the dimensions to 16 dimensions. Since NEC-RP uses k-nearest neighbor algorithm, the computation time can be reduced when we reduce the dimensions as small as possible, but the performance is sacrificed. Therefore, it is important to consider the tradeoff."
  }, {
    "heading": "7.2. Switching NEC-RP to NEC",
    "text": "We compare the performance by switching NEC-RP to NEC, that is, we switch the RP layer to a FC layer with a heuristic step count CS. This comparison indicates experimentally\nthat it is possible to switch from NEC-RP to NEC, and we also verify whether the flexibility of learning about the neural networks can reduce the losses that random projection cannot minimize. We fix the embedding reduced dimensions to 32 dimensions, and we experiment in the case of switching in 2M frames (NEC-RP32 (2M)) and in 5M frames (NEC-RP32 (5M)).\nWe have shown the result in Fig. 5. Even when we have switched it in 2M frames or in 5M frames, it has learned without a sharp drop in their performances. From the result, we have found that it has been possible to switch the RP layer to a FC layer at any time. However, the performance has been about the same as NEC such as MsPacman and Bowling at 10M frames if the timing of switching is too early.\nOn the other hand, in the case of switching in 5M frames, we have shown that the result of exceeding the NEC-RP’s performance in Bowling even though it has been the architecture similar to NEC at 10M frames. We have also shown similar performance to NEC-RP in the other games, and we have found that it has been effective to use the RP layer only at the beginning of NEC’s training. However, in MsPacman we have observed that the score has dropped sharply around 10M frames. It is necessary to confirm whether we see this phenomenon in other games in long-term experiments in the future."
  }, {
    "heading": "8. Conclusion",
    "text": "In this research, we have proposed NEC-RP as the more stable and efficient learning architecture than NEC. We have experimented with the Atari games and actually have outperformed the performance in four out of five games and have shown that our agent has learned efficiently. In addition, we have experimented to switch from NEC-RP to NEC, and we have found that it is possible to improve the performance by switching an RP layer to a FC layer.\nAs future work, we should experiment with long-term training. In NEC’s paper, there is a report that NEC the performance is inferior to DQN one as learning progresses.\nWe also need to verify the performance of NEC-RP by long-term training because it may be inferior to DQN like NEC.\nMoreover, our architecture is easily available for other architectures that use NEC, and we can expect to improve the performance. We would like to research whether we can apply it to these methods in the future."
  }],
  "year": 2019,
  "references": [{
    "title": "Database-friendly random projections",
    "authors": ["Dimitris Achlioptas"],
    "venue": "In Proceedings of the Twentieth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, May 21-23,",
    "year": 2001
  }, {
    "title": "On the theory of dynamic programming",
    "authors": ["Richard Bellman"],
    "venue": "Proceedings of the National Academy of Sciences of the United States of America,",
    "year": 1952
  }, {
    "title": "Multidimensional binary search trees used for associative searching",
    "authors": ["Jon Louis Bentley"],
    "venue": "Commun. ACM,",
    "year": 1975
  }, {
    "title": "Random projection in dimensionality reduction: applications to image and text data",
    "authors": ["Ella Bingham", "Heikki Mannila"],
    "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2001
  }, {
    "title": "Model-free episodic control",
    "authors": ["Charles Blundell", "Benigno Uria", "Alexander Pritzel", "Yazhe Li", "Avraham Ruderman", "Joel Z. Leibo", "Jack W. Rae", "Daan Wierstra", "Demis Hassabis"],
    "venue": "CoRR, abs/1606.04460,",
    "year": 2016
  }, {
    "title": "Reinforcement learning coach, December 2017",
    "authors": ["Itai Caspi", "Gal Leibovich", "Gal Novik", "Shadi Endrawis"],
    "venue": "URL https://doi.org/10.5281/zenodo.1134899",
    "year": 2017
  }, {
    "title": "Experiments with random projection",
    "authors": ["Sanjoy Dasgupta"],
    "venue": "CoRR, abs/1301.3849,",
    "year": 2013
  }, {
    "title": "Fast deep reinforcement learning using online adjustments from the past",
    "authors": ["Steven Hansen", "Alexander Pritzel", "Pablo Sprechmann", "André Barreto", "Charles Blundell"],
    "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
    "year": 2018
  }, {
    "title": "Context vectors: general purpose approximate meaning representations self-organized from raw data",
    "authors": ["Robert Hecht-Nielsen"],
    "venue": "Computational intelligence: Imitating life,",
    "year": 1994
  }, {
    "title": "Multi-task deep reinforcement learning with popart",
    "authors": ["Matteo Hessel", "Hubert Soyer", "Lasse Espeholt", "Wojciech Czarnecki", "Simon Schmitt", "Hado van Hasselt"],
    "venue": "CoRR, abs/1809.04474,",
    "year": 2018
  }, {
    "title": "Semiparametric reinforcement learning",
    "authors": ["Mika Sarkin Jain", "Jack Lindsey"],
    "venue": "ICLR 2018 Workshop,",
    "year": 2018
  }, {
    "title": "Imagenet classification with deep",
    "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"],
    "year": 2013
  }, {
    "title": "Low-distortion subspace embeddings in input",
    "authors": ["2018/337. Xiangrui Meng", "Michael W. Mahoney"],
    "year": 2018
  }, {
    "title": "Incremental multi-step q-learning",
    "authors": ["Jing Peng", "Ronald J. Williams"],
    "venue": "Machine Learning,",
    "year": 1996
  }, {
    "title": "Neural episodic control",
    "authors": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adrià Puigdomènech Badia", "Oriol Vinyals", "Demis Hassabis", "Daan Wierstra", "Charles Blundell"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Prioritized experience replay",
    "authors": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"],
    "venue": "CoRR, abs/1511.05952,",
    "year": 2015
  }, {
    "title": "Learning values across many orders of magnitude",
    "authors": ["Hado P. van Hasselt", "Arthur Guez", "Matteo Hessel", "Volodymyr Mnih", "David Silver"],
    "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "The Random Projection Method, volume 65 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science",
    "authors": ["Santosh Srinivas Vempala"],
    "venue": "DIMACS/AMS,",
    "year": 2004
  }, {
    "title": "Technical note q-learning",
    "authors": ["Christopher J.C.H. Watkins", "Peter Dayan"],
    "venue": "Machine Learning,",
    "year": 1992
  }, {
    "title": "Random projection in deep neural networks",
    "authors": ["Piotr Iwo Wójcik"],
    "venue": "CoRR, abs/1812.09489,",
    "year": 2018
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["David P. Woodruff"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Randomly projected kd-trees with distance metric learning for image retrieval",
    "authors": ["Pengcheng Wu", "Steven C.H. Hoi", "Duc Dung Nguyen", "Ying He"],
    "venue": "In Advances in Multimedia Modeling - 17th International Multimedia Modeling Conference,",
    "year": 2011
  }],
  "id": "SP:7f9c8ac741d2ddf2f5ebe7efadf420ef9e863936",
  "authors": [{
    "name": "Daichi Nishio",
    "affiliations": []
  }, {
    "name": "Satoshi Yamane",
    "affiliations": []
  }, {
    "name": "S. Yamane",
    "affiliations": []
  }, {
    "name": "Nishio Yamane",
    "affiliations": []
  }],
  "abstractText": "End-to-end deep reinforcement learning has enabled agents to learn with little preprocessing by humans. However, it is still difficult to learn stably and efficiently because the learning method usually uses a nonlinear function approximation. Neural Episodic Control (NEC), which has been proposed in order to improve sample efficiency, is able to learn stably by estimating action values using a non-parametric method. In this paper, we propose an architecture that incorporates random projection into NEC to train with more stability. In addition, we verify the effectiveness of our architecture by Atari’s five games. The main idea is to reduce the number of parameters that have to learn by replacing neural networks with random projection in order to reduce dimensions while keeping the learning end-to-end.",
  "title": "Random Projection in Neural Episodic Control"
}