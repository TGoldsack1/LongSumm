{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep linear networks (DLN) are neural networks that have multiple hidden layers but have no nonlinearities between layers. That is, for given data points {x(i)}Ni=1 the outputs of such networks are computed via a series\nŷ(i) = WLWL−1 · · ·W1x(i)\nof matrix multiplications. Given a target y(i) for the ith data point and a pairwise loss function `(ŷ(i),y(i)), forming the usual summation\nL(W1, . . . ,WL) = 1\nN N∑ i=1 `(ŷ(i),y(i)) (1)\nthen yields the total loss.\nSuch networks have few direct applications, but they frequently appear as a class of toy models used to understand the loss surfaces of deep neural networks (Saxe et al., 2014; Kawaguchi, 2016; Lu & Kawaguchi, 2017; Hardt & Ma, 2017). For example, numerical experiments indicate that DLNs exhibit some behavior that resembles the behavior of\n*Equal contribution 1Department of Mathematics, Loyola Marymount University, Los Angeles, CA 90045, USA 2Department of Mathematics and Statistics, California State University, Long Beach, Long Beach, CA 90840, USA. Correspondence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von Brecht <james.vonbrecht@csulb.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndeep nonlinear networks during training (Saxe et al., 2014). Results of this sort provide a small piece of evidence that DLNs can provide a decent simplified model of more realistic networks with nonlinearities.\nFrom an analytical point-of-view, the simplicity of DLNs allows for a rigorous, in-depth study of their loss surfaces. These models typically employ a convex loss function `(ŷ,y), and so with one layer (i.e. L = 1) the loss L(W1) is convex and the resulting optimization problem (1) has no sub-optimal local minimizers. With multiple layers (i.e. L ≥ 2) the loss L(W1, . . . ,WL) is not longer convex, and so the question of paramount interest concerns whether this addition of depth and the subsequent loss of convexity creates sub-optimal local minimizers. Indeed, most analytical treatments of DLNs focus on this question.\nWe resolve this question in full for arbitrary convex differentiable loss functions. Specifically, we consider deep linear networks satisfying the two following hypotheses:\n(i) The loss function ŷ 7→ `(y, ŷ) is convex and differentiable.\n(ii) The thinnest layer is either the input layer or the output layer.\nMany networks of interest satisfy both of these hypotheses. The first hypothesis (i) holds for nearly all network criteria, such as the mean squared error loss, the logistic loss or the cross entropy loss, that appear in applications. In a classification scenario, the second hypothesis (ii) holds whenever each hidden layer has more neurons than the number of classes. Thus both hypotheses are often satisfied when using a deep linear network (1) to model its nonlinear counterpart. In any such situation we resolve the deep linear problem in its entirety. Theorem 1. If hypotheses (i) and (ii) hold then (1) has no sub-optimal minimizers, i.e. any local minimum is global.\nWe provide a short, transparent proof of this result. It is easily accessible to any reader with a basic understanding of the singular value decomposition, and in particular, it does not rely on any sophisticated machinery from either optimization or linear algebra. Moreover, this theorem is the strongest possible in the following sense — Theorem 2. There exists a convex, Lipschitz but not differentiable function ŷ 7→ `(y, ŷ) for which (1) has sub-optimal\nlocal minimizers.\nIn other words, we have a (perhaps surprising) hard limit on how far “local equals global” results can reach; differentiability of the loss is essential.\nMany prior analytical treatments of DLNs focus on similar questions. For instance, both (Baldi & Hornik, 1989) and (Baldi & Lu, 2012) consider “deep” linear networks with two layers (i.e. L = 2) and a mean squared error loss criterion. They provide a “local equals global” result under some relatively mild assumptions on the data and targets. More recently, (Kawaguchi, 2016) proved that deep linear networks with arbitrary number of layers and with mean squared error loss do not have sub-optimal local minima under certain structural assumptions on the data and targets. The follow-up (Lu & Kawaguchi, 2017) futher simplifies the proof of this result and weakens the structural assumptions. Specifically, this result shows that the loss (1) associated with a deep linear network has no sub-optimal local minima provided all of assumptions\n(i) The loss `(ŷ(i),y(i)) = ‖y(i) − ŷ(i)‖2 is the mean squared error loss criterion;\n(ii) The data matrixX = [x(1), . . . ,x(N)] has full row rank;\n(iii) The target matrix Y = [y(1), . . . ,y(N)] has full row rank;\nare satisfied. Compared to our result, (Lu & Kawaguchi, 2017) therefore allows for the hidden layers of the network to be thinner than the input and output layers. However, our result applies to network equipped with any differentiable convex loss (in fact any differentiable loss L for which firstorder optimality implies global optimality) and we do not require any assumption on the data and targets. Our proof is also shorter and much more elementary by comparison."
  }, {
    "heading": "2. Proof of Theorem 1",
    "text": "Theorem 1 follows as a simple consequence of a more general theorem concerning real-valued functions that take as input a product of matrices. That is, we view the deep linear problem as a specific instance of the following more general problem. Let Mm×n denote the space of m × n real matrices, and let f : MdL×d0 → R denote any differentiable function that takes dL × d0 matrices as input. For any such function we may then consider both the single-layer optimization\n(P1) { Minimize f(A) over all A in MdL×d0\nas well as the analogous problem\n(P2)  Minimize f(WLWL−1 · · ·W1) over all L-tuples (W1, . . . ,WL) in Md1×d0 × · · · ×MdL×dL−1\nthat corresponds to a multi-layer deep linear optimization. In other words, in (P2) we consider the task of optimizing f over those matrices A ∈MdL×d0 that can be realized by an L-fold product\nA = WLWL−1 · · ·W1 W` ∈Md`×d`−1 (2)\nof matrices. We may then ask how the parametrization (2) of A as a product of matrices affects the minimization of f, or in other words, whether the problems (P1) and (P2) have similar structure. At heart, the use of DLNs to model nonlinear neural networks centers around this question.\nAny notion of structural similarity between (P1) and (P2) should require that their global minima coincide. As a matrix of the form (2) has rank at most min{d0, . . . , dL}, we must impose the structural requirement\nmin{d1, . . . , dL−1} ≥ min{dL, d0} (3)\nin order to guarantee that (2) does, in fact, generate the full space of dL × d0 matrices. Under this assumption we shall prove the following quite general theorem.\nTheorem 3. Assume that f(A) is any differentiable function and that the structural condition (3) holds. Then at any local minimizer ( Ŵ1, . . . , ŴL ) of (P2) the optimality condition\n∇f ( Â ) = 0 Â := ŴLŴL−1 · · · Ŵ1\nis satisfied.\nTheorem 1 follows as a simple consequence of this theorem. The first hypothesis (i) of theorem 1 shows that the total loss (1) takes the form\nL(W1, . . . ,WL) = f(WL · · ·W1)\nfor f(A) some convex and differentiable function. The structural hypothesis (3) is equivalent to the second hypothesis (ii) of theorem 1, and so we can directly apply theorem 3 to conclude that a local minimum ( Ŵ1, . . . , ŴL ) of L corresponds to a critical point Â = ŴL · · · Ŵ1 of f(A), and since f(A) is convex, this critical point is necessarily a global minimum.\nBefore turning to the proof of theorem 3 we recall a bit of notation and provide a calculus lemma. Let\n〈A,B〉fro := Tr(ATB) = ∑ i ∑ j AijBij and\n‖A‖2fro := 〈A,A〉fro\ndenote the Frobenius dot product and the Frobenius norm, respectively. Also, recall that for a differentiable function φ : Mm×n 7→ R its gradient∇φ(A) ∈Mm×n is the unique matrix so that the equality\nφ(A+H) = φ(A) + 〈∇φ(A), H〉fro + o (‖H‖fro) (4)\nholds. If F (W1, . . . ,WL) := f(WL · · ·W1) denotes the objective of interest in (P2) the following lemma gives the partial derivatives of F as a function of its arguments.\nLemma 1. The partial derivatives of F are given by\n∇W1F (W1, . . . ,WL) = WT2,+∇f ( A ) ,\n∇WkF (W1, . . . ,WL) = WTk+1,+∇f(A)WTk−1,−, ∇WLF (W1, . . . ,WL) = ∇f ( A ) WTL−1,−,\nwhere A stands for the full product A := WL · · ·W1 and Wk,+,Wk,− are the truncated products\nWk,+ := WL · · ·Wk, Wk,− := Wk · · ·W1. (5)\nProof. The definition (4) implies\nF (W1, . . . ,Wk−1,Wk +H,Wk+1, . . . ,WL) = f ( A+Wk+1,+HWk−1,− ) = f(A) + 〈∇f(A),Wk+1,+HWk−1,−〉fro + o ( ‖H‖fro ) .\nUsing the cyclic property Tr(ABC) = Tr(CAB) of the trace then gives\n〈∇f(A) , Wk+1,+HWk−1,− 〉fro = Tr ( ∇f(A)TWk+1,+HWk−1,− ) = Tr ( Wk−1,−∇f(A)TWk+1,+H\n) = 〈WTk+1,+∇f(A)WTk−1,− , H 〉fro\nwhich, in light of (4), gives the desired formula for∇WkF . The formulas for∇W1F and∇WLF are obtained similarly.\nProof of Theorem 3: To prove theorem 3 it suffices to assume that dL ≥ d0 without loss of generality. This follows from the simple observation that\ng ( A ) := f ( AT )\ndefines a differentiable function of d0 × dL matrices for f(A) any differentiable function of dL × d0 matrices. As a point ( W1, . . . ,WL ) defines a local minimum of\nf ( WLWL−1 · · ·W1 ) if and only if ( WT1 , . . . ,W T L ) defines\na minimum of g ( V1 · · ·VL−1VL ) , the theorem for the case dL < d0 follows by appealing to its dL ≥ d0 instance. It\ntherefore suffices to assume that dL ≥ d0, and by the structural assumption that dk ≥ d0, throughout the remainder of the proof.\nConsider any local minimizer ( Ŵ1, . . . , ŴL ) of F and denote by Â, Ŵk,+ and Ŵk,− the corresponding full and truncated products (c.f. (5)). By definition of a local minimizer there exists some ε0 > 0 so that\nF (W1, . . . ,WL) ≥ F (Ŵ1, . . . , ŴL) = f ( Â ) (6)\nwhenever the family of inequalities\n‖W` − Ŵ`‖fro ≤ ε0 for all 1 ≤ ` ≤ L\nall hold. Moreover, lemma 1 yields (i) 0 = ŴT2,+∇f ( Â ) ,\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\n(iii) 0 = ∇f ( Â ) ŴTL−1,−. (7)\nsince all partial derivatives must vanish at a local minimum. If ŴL−1,− has a trivial kernel, i.e. ker(ŴL−1,−) = {0}, then the theorem follows easily. The critical point condition (7) part (iii) implies\nŴL−1,−∇f ( Â )T = 0,\nand since ŴL−1,− has a trivial kernel this implies ∇f ( Â ) = ∇f ( ŴLŴL−1 · · · Ŵ1 ) = 0 as desired.\nThe remainder of the proof concerns the case that ŴL−1,− has a nontrivial kernel. The main idea is to use this nontrivial kernel to construct a family of infinitesimal perturbations of the local minimizer ( Ŵ1, . . . , ŴL ) that leaves the overall product ŴL · · · Ŵ1 unchanged. In other words, the family of perturbations ( W̃1, . . . , W̃L ) satisfy\n‖W̃` − Ŵ`‖fro ≤ 0/2 ∀` = 1, . . . , L, (8) W̃LW̃L−1 · · · W̃1 = ŴLŴL−1 · · · Ŵ1. (9)\nAny such perturbation also defines a local minimizer. Claim 1. Any tuple of matrices ( W̃1, . . . , W̃L ) satisfying (8) and (9) is necessarily a local minimizer F .\nProof. For any matrixW` satisfying ‖W`−W̃`‖fro ≤ ε0/2, inequality (8) implies that\n‖W` − Ŵ`‖fro ≤ ‖W` − W̃`‖fro + ‖W̃` − Ŵ`‖fro ≤ ε0\nEquality (9) combined to (6) then leads to\nF ( W1, . . . ,WL ) ≥ f ( Â )\n= f(ŴL · · · Ŵ1) = f(W̃L · · · W̃1) = F ( W̃1, . . . , W̃L ) for any W` with ‖W` − W̃`‖fro ≤ ε0/2 and so the point (W̃1, . . . , W̃L) defines a local minimum.\nThe construction of such perturbations requires a preliminary observation and then an appeal to the singular value decomposition. Due to the definition of Ŵk,− it follows that ker(Ŵk+1,−) = ker(Ŵk+1Ŵk,−) ⊇ ker(Ŵk,−), and so the chain of inclusions\nker(Ŵ1,−) ⊆ ker(Ŵ2,−) ⊆ · · · ⊆ ker(ŴL−1,−) (10)\nholds. Since ŴL−1,− has a nontrivial kernel, the chain of inclusions (10) implies that there exists k∗ ∈ {1, . . . , L−1} such that\nker(Ŵk,−) = {0} if k < k∗ (11) ker(Ŵk,−) 6= {0} if k ≥ k∗ (12)\nIn other words, Ŵk∗,− is the first matrix appearing in (10) that has a nontrivial kernel.\nThe structural requirement (3) and the assumption that dL ≥ d0 imply that dk ≥ d0 for all k, and so the matrix Ŵk,− ∈ Mdk×d0 has more rows than columns. As a consequence its full singular value decomposition\nŴk,− = ÛkΣ̂kV̂ T k (13)\nhas the shape depicted in figure 1. The matrices Ûk ∈ Mdk×dk and V̂k ∈ Md0×d0 are orthogonal, whereas Σ̂k ∈ Mdk×d0 is a diagonal matrix containing the singular values of Ŵk,− in descending order. From (12) Ŵk,− has a nontrivial kernel for all k ≥ k∗, and so in particular its least singular value is zero. In particular, the (d0, d0) entry of Σ̂k vanishes if k ≥ k∗. Let ûk denote the corresponding dth0 column of Ûk, which exists since dk ≥ d0.\nClaim 2. Let wk∗+1, . . . ,wL denote any collection of vectors and δk∗+1, . . . , δL any collection of scalars satisfying\nwk ∈ Rdk , ‖wk‖2 = 1 and (14) 0 ≤ δk ≤ 0/2 (15)\nfor all k∗ + 1 ≤ k ≤ L. Then the tuple of matrices (W̃1, . . . , W̃L) defined by\nW̃k := { Ŵk if 1 ≤ k ≤ k∗ Ŵk + δkwkû T k−1 else,\n(16)\nsatisfies (8) and (9).\nProof. Inequality (8) follows from the fact that\n‖W̃k − Ŵk‖fro = δk‖wkûTk−1‖fro = δk‖wk‖2‖ûk−1‖2\nfor all k > k∗, together with the fact that ûk−1 and wk are unit vectors and that 0 ≤ δk ≤ 0/2.\nTo prove (9) let W̃k,− = W̃k · · · W̃1 and Ŵk,− = Ŵk · · · Ŵ1 denote the truncated products of the matrices\nW̃k and Ŵk. The equality W̃k∗,− = Ŵk∗,− is immediate from the definition (16). The equality (9) will then follow from showing that\nW̃k,− = Ŵk,− for all k∗ ≤ k ≤ L. (17)\nProceeding by induction, assume that W̃k,− = Ŵk,− for a given k ≥ k∗. Then\nW̃k+1,− = W̃k+1W̃k,−\n= W̃k+1Ŵk,− (induction hypothesis) = ( Ŵk+1 + δk+1wk+1û T k ) Ŵk,−\n= Ŵk+1,− + δk+1wk+1u T k Ŵk,−\nThe second term of the last line vanishes, since\nuTk Ŵk,− = u T k ÛkΣ̂kV̂ T k = e T d0Σ̂kV̂ T k = 0\nwith ed0 ∈ Rdk the dth0 standard basis vector. The second equality comes from the fact that the columns of Ûk are orthonormal, and the last equality comes from the fact that eTd0Σk∗ = 0 since the d th 0 row of Σ̂k∗ vanishes. Thus (17) holds, and so (9) holds as well.\nClaims 1 and claim 2 show that the perturbation( W̃1, . . . , W̃L ) defined by (16) is a local minimizer of F . The critical point conditions\n(i) 0 = W̃T2,+∇f ( Ã ) ,\n(ii) 0 = W̃Tk+1,+∇f ( Ã ) W̃Tk−1,− ∀ 2 ≤ k ≤ L− 1,\n(iii) 0 = ∇f ( Ã ) W̃TL−1,−\ntherefore hold as well for all choices of wk∗+1, . . . ,wL and δk∗+1, . . . , δL satisfying (14) and (15).\nThe proof concludes by appealing to this family of critical point relations. If k∗ > 1 the transpose of condition (ii) gives\nŴk∗−1,−∇f ( Â )T W̃k∗+1,+ = 0 (18)\nsince the equalities W̃k∗−1,− = Ŵk∗−1,− (c.f. (16)) and Ã = W̃L · · · W̃1 = ŴL · · · Ŵ1 = Â (c.f. (9)) both hold. But ker(Ŵk∗−1,−) = {0} by definition of k∗ (c.f. (11)), and so\n∇f ( Â )T W̃L · · · W̃k∗+1 = 0. (19)\nmust hold as well. If k∗ = 1 then (19) follows trivially from the critical point condition (i). Thus (19) holds for all choices of wk∗+1, . . . ,wL and δk∗+1, . . . , δL satisfying (14) and (15). First choose δk∗+1 = 0 so that W̃k∗+1 = Ŵk∗+1 and apply (19) to find\n∇f ( Â )T W̃L · · · W̃k∗+2Ŵk∗+1 = 0. (20)\nThen take any δk∗+1 > 0 and substract (20) from (19) to get\n1 δk∗+1 ∇f ( Â )T W̃L · · · W̃k∗+2 ( W̃k∗+1 − Ŵk∗+1 ) = ∇f ( Â )T W̃L · · · W̃k∗+2 ( wk∗+1û T k∗ ) = 0\nfor wk∗+1 an arbitrary vector with unit length. Right multiplying the last equality by ûk∗ and using the fact that (wk∗+1û T k∗ )ûk∗ = wk∗+1û T k∗ ûk∗ = wk∗+1 shows\n∇f ( Â )T W̃L · · · W̃k∗+2wk∗+1 = 0 (21)\nfor all choices of wk∗+1 with unit length. Thus (21) implies\n∇f ( Â )T W̃L · · · W̃k∗+2 = 0\nfor all choices of wk∗+2, . . . ,wL and δk∗+2, . . . , δL satisfying (14) and (15). The claim\n∇f ( Â ) = 0\nthen follows by induction."
  }, {
    "heading": "3. Concluding Remarks",
    "text": "Theorem 3 provides the mathematical basis for our analysis of deep linear problems. We therefore conclude by discussing its limits.\nFirst, theorem 3 fails if we refer to critical points rather than local minimizers. To see this, it suffices to observe that the critical point conditions for problem (P2),\n(i) 0 = ŴT2,+∇f ( Â ) ,\n(ii) 0 = ŴTk+1,+∇f ( Â ) ŴTk−1,− ∀ 2 ≤ k ≤ L− 1,\n(iii) 0 = ∇f ( Â ) ŴTL−1,−\nwhere Ŵk,+ := ŴL · · · Ŵk+1 and Ŵk,− := Ŵk−1 · · · Ŵ1, clearly hold if L ≥ 3 and all of the Ŵ` vanish. In other words, the collection of zero matrices always defines a critical point for (P2) but clearly ∇f ( 0 ) need not vanish. To\nput it otherwise, if L ≥ 3 the problem (P2) always has saddle-points even though all local optima are global.\nSecond, the assumption that f(A) is differentiable is necessary as well. More specifically, a function of the form\nF (W1, . . . ,WL) := f(WL · · ·W1)\ncan have sub-optimal local minima if f(A) is convex and globally Lipschitz but is not differentiable. A simple example demonstrates this, and therefore proves theorem 2. For instance, consider the bi-variate convex function\nf(x, y) := |x|+(1−y)+−1, (y)+ := max{y, 0}, (22)\nwhich is clearly globally Lipschitz but not differentiable. The set\narg min f := {(x, y) ∈ R2 : x = 0, y ≥ 1}\nfurnishes its global minimizers while fopt = −1 gives the optimal value. For this function even a two layer deep linear problem\nF ( W1,W2) := f(W2W1) W2 ∈ R2, W1 ∈ R\nhas sub-optimal local minimizers; the point\n(Ŵ1, Ŵ2) =\n( 0, [ 1 0 ]) (23)\nprovides an example of a sub-optimal solution. The set of all possible points in R2\nN (Ŵ1, Ŵ2) :={ W2W1 : ‖W2 − Ŵ2‖ ≤ 1\n4 , ‖W1 − Ŵ1‖ ≤\n1\n4 } generated by a 1/4-neighborhood of the optimum (23) lies in the two-sided, truncated cone\nN (Ŵ1, Ŵ2) ⊂ {\n(x, y) ∈ R2 : |x| ≤ 1 2 , |y| ≤ 1 2 |x| } ,\nand so if we let x ∈ R denote the first component of the product W2W1 then the inequality\nf(W2W1) ≥ 1\n2 |x| ≥ 0 = f(Ŵ2Ŵ1)\nholds on N (Ŵ1, Ŵ2) and so (Ŵ1, Ŵ2) is a sub-optimal local minimizer. Moreover, the minimizer (Ŵ1, Ŵ2) is a strict local minimizer in the only sense in which strict optimality can hold for a deep linear problem. Specifically, the strict inequality\nf(W2W1) > f(Ŵ2Ŵ1) (24)\nholds on N (Ŵ1, Ŵ2) unless W2W1 = Ŵ2Ŵ1 = 0; in the latter case (W1,W2) and (Ŵ1, Ŵ2) parametrize the same\npoint and so their objectives must coincide. We may identify the underlying issue easily. The proof of theorem 3 requires a single-valued derivative ∇f(Â) at a local optimum, but with f(x, y) as in (22) its subdifferential\n∂f(0) = {(x, y) ∈ R2 : −1 ≤ x ≤ 1, y = 0}\nis multi-valued at the sub-optimal local minimum (23). In other words, if a globally convex function f(A) induces sub-optimal local minima in the corresponding deep linear problem then ∇f(Â) cannot exist at any such sub-optimal solution (assuming the structural condition, of course).\nThird, the structural hypothesis\nd` ≥ min{dL, d0} for all ` ∈ {1, . . . , L}\nis necessary for theorem 3 to hold as well. If d` < min{d0, dL} for some ` the parametrization\nA = WL · · ·W1\ncannot recover full rank matrices. Let f(A) denote any function where∇f vanishes only at full rank matrices. Then\n∇f ( WL · · ·W1 ) 6= 0\nat all critical points of (P2), and so theorem 3 fails.\nFinally, if we do not require convexity of f(A) then it is not true, in general, that local minima of (P2) correspond to minima of the original problem. The functions\nf(x, y) = x2 − y2 F (W1,W2) = f(W2W1)\nand the minimizer (23) illustrate this point. While the origin is clearly a saddle point of the one layer problem, the argument leading to (24) shows that (23) is a local minimizer for the deep linear problem. So in the absence of additional structural assumptions on f(A), we may infer that a minimizer of the deep linear problem satisfies first-order optimality for the original problem, but nothing more."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural networks and principal component analysis: Learning from examples without local minima",
    "authors": ["P. Baldi", "K. Hornik"],
    "venue": "Neural networks,",
    "year": 1989
  }, {
    "title": "Complex-valued autoencoders",
    "authors": ["P. Baldi", "Z. Lu"],
    "venue": "Neural Networks,",
    "year": 2012
  }, {
    "title": "Identity matters in deep learning",
    "authors": ["M. Hardt", "T. Ma"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Depth creates no bad local minima",
    "authors": ["H. Lu", "K. Kawaguchi"],
    "venue": "arXiv preprint arXiv:1702.08580,",
    "year": 2017
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2014
  }],
  "id": "SP:c6f24674052cfaa1458ef7039fb44e006e2fc5ec",
  "authors": [{
    "name": "Thomas Laurent",
    "affiliations": []
  }, {
    "name": "James H. von Brecht",
    "affiliations": []
  }],
  "abstractText": "We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.",
  "title": "Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global"
}