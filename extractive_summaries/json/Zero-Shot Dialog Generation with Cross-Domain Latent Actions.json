{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2018 Conference, pages 1–10, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics\n1\nThis paper introduces zero-shot dialog generation (ZSDG), as a step towards neural dialog systems that can instantly generalize to new situations with minimal data. ZSDG enables an end-to-end generative dialog system to generalize to a new domain for which only a domain description is provided and no training dialogs are available. Then a novel learning framework, Action Matching, is proposed. This algorithm can learn a cross-domain embedding space that models the semantics of dialog responses which, in turn, lets a neural dialog generation model generalize to new domains. We evaluate our methods on a new synthetic dialog dataset, and an existing human-human dialog dataset. Results show that our method has superior performance in learning dialog models that rapidly adapt their behavior to new domains and suggests promising future research.1"
  }, {
    "heading": "1 Introduction",
    "text": "The generative end-to-end dialog model (GEDM) is one of the most powerful methods of learning dialog agents from raw conversational data in both chat-oriented and task-oriented domains (Serban et al., 2016; Wen et al., 2016; Zhao et al., 2017). Its base model is an encoder-decoder network (Cho et al., 2014) that uses an encoder network to encode the dialog context and generate the next response via a decoder network. Yet prior work in GEDMs has overlooked an important issue, i.e. the data scarcity problem. In fact, the data\n1Code and data are avaliable at https://github. com/snakeztc/NeuralDialog-ZSDG\nscarcity problem is extremely common in most dialog applications due to the wide range of potential domains that dialog systems can be applied to. To the best of our knowledge, current GEDMs are data-hungry and have only been successfully applied to domains with abundant training material. This limitation prohibits the possibility of using the GEDMs for rapid prototyping in new domains and is only useful for domains with large datasets.\nThe key idea of this paper lies in developing domain descriptions that can capture domain-specific information and a new type of GEDM model that can generalize to a new domain based on the domain description. Humans exhibit incredible efficiency in achieving this type of adaptation. Imagine that a customer service agent in the shoe department is transferred to the clothing department. After reading some relevant instructions and documentation, this agent can immediately begin to deal with clothes-related calls without the need for any example dialogs. We also argue that it is more efficient and natural for domain experts to express their knowledge in terms of domain descriptions rather than example dialogs. This is because creating example dialogs involves writing down imagined dialog exchanges that can be shared across multiple domains and are not relevant to the unique proprieties of a specific domain. However, current state-of-the-art GEDMs are not designed to incorporate such knowledge and are therefore incapable of adapting its behavior to unseen domains.\nThis paper introduces the use of zero-shot dialog generation (ZSDG) in order to enable GEDMs to generalize to unseen situations using minimal dialog data. Building on zero-shot classification (Palatucci et al., 2009), we formalize ZSDG as a learning problem where the training data contains dialog data from source domains along with domain descriptions from both the source and tar-\n2 get domains. Then at testing time, ZSDG models are evaluated on the target domain, where no training dialogs were available. We approach ZSDG by first discovering a dialog policy network that can be shared between the source and target domains. The output from this policy is distributed vectors which are referred to as latent actions. Then, in order to transform the latent actions from any domain back to natural language utterances, a novel Action Matching (AM) algorithm is proposed that learns a cross-domain latent action space that models the semantics of dialog responses. This in turns enables the GEDM to generate responses in the target domains even when it has never observed full dialogs in them. Finally the proposed methods and baselines are evaluated on two dialog datasets. The first one is a new synthetic dialog dataset generated by SimDial, which was developed for this study. SimDial enables us to easily generate task-oriented dialogs in a large number of domains, and provides a test bed to evaluate different ZSDG approaches. We further test our methods on a recently released multi-domain human-human corpus (Eric and Manning, 2017b) to validate whether performance can generalize to real-world conversations. Experimental results show that our methods are effective in incorporating knowledge from domain descriptions and achieve strong ZSDG performance."
  }, {
    "heading": "2 Related Work",
    "text": "Perhaps the most closely related topic is zeroshot learning (ZSL) for classification (Larochelle et al., 2008), which has focused on classifying unseen labels. A common approach is to represent the labels as attributes instead of class indexes (Palatucci et al., 2009). As a result, at test time, the model can first predict the semantic attributes in the input, then make the final prediction by comparing the predicted attributes with the candidate labels’ attributes. More recent work (Socher et al., 2013; Romera-Paredes and Torr, 2015) improved on this idea by learning parametric models, e.g. neural networks, to map the label and input data into a joint embedding space and then make predictions. Besides classification, prior art has explored the notion of task generalization in robotics, so that a robot can execute a new task that was not mentioned in training (Oh et al., 2017; Duan et al., 2017). In this case, a task is described by a demonstration or a sequence of instructions, and the system needs to learn to break down the instructions into previously learned skills. Also generating outof-vocabulary (OOV) words from recurrent neural networks (RNNs) can be seen as a form of ZSL, where the OOV words are unseen labels. Prior work has used delexicalized tags (Zhao et al., 2017) and copy-mechanism (Gu et al., 2016; Merity et al., 2016; Elsahar et al., 2018) to enable RNN output words that are not in its vocabulary. Finally, ZSL has been applied to individual components in the dialog system pipeline. Chen et al. (Chen et al., 2016) developed an intent classifier that can predict new intent labels that are not included in the training data. Bapna et al. (Bapna et al., 2017) extended that idea to the slot-filling module to track novel slot types. Both papers leverage a natural language description for the label (intent or slot-type) in order to learn a semantic embedding of the label space. Then, given any new labels, the model can still make predictions. There has also been extensive work on learning domain-adaptable dialog policy by first training a dialog policy on previous domains and testing the policy on a new domain. Gasic et al. (Gasic and Young, 2014) used the Gaussian Process with cross-domain kernel functions. The resulting policy can leverage experience from other domains to make educated decisions in a new one. In summary, past ZSL research in the dialog domain has mostly focused on the individual modules in a pipeline-based dialog system. We believe our proposal is the first step in exploring the notion of adapting an entire end-to-end dialog system to new domains for domain generalization."
  }, {
    "heading": "3 Problem Formulation",
    "text": "We begin by formalizing zero-shot dialog generation (ZSDG). Generative dialog models take a dialog context c as input and then generate the next response x. ZSDG uses the term domain to describe the difference between training and testing data. Let D = Ds ⋃ Dt be a set of domains, where Ds is a set of source domains, Dt is a set of target domains and Ds ∩Dt = ∅. During training, we are given a set of samples {c(n),x(n), d(n)} ∼ psource(c,x, d) drawn from the source domains. During testing, a ZSDG model will be given a dialog context c and a domain d drawn from the target domains and must generate the correct re-\n3 sponse x. Moreover, ZSDG assumes that every domain d has its own domain description φ(d) that is available at training for both source and target domains. The primary goal is to learn a generative dialog model F : C × D → X that can perform well in a target domain, by relating the unseen target domain description to the seen descriptions of the source domains. Our secondary goal is that F should perform similarly to a model that is designed to operate solely in the source domains. In short, the problem of ZSDG can be summarized as: Train Data: {c,x, d} ∼ psource(c,x, d) {φ(d)}, d ∈ D Test Data: {c,x, d} ∼ ptarget(c,x, d) Goal: F : C ×D → X"
  }, {
    "heading": "4 Proposed Method",
    "text": ""
  }, {
    "heading": "4.1 Seed Responses as Domain Descriptions",
    "text": "The design of the domain description φ is a crucial factor that decides whether robust performance in the target domains is achievable. This paper proposes seed response (SR) as a general-purpose domain description that can readily be applied to different dialog domains. SR needs for the developers to provide a list of example responses that the model can generate in this domain. SR’s assumption is that a dialog model can discover analogies between responses from different domains, so that its dialog policy trained on source domains can be reused in the target domain. Without losing generality, SRd defines φ(d) as {x(i),a(i), d}seed for domain d, where x is a seed response and a is its annotations. Annotations are salient features that help the system in infer the relationship amongst responses from different domains. This may be difficult to achieve using only words in x, e.g. two domains with distinct word distributions. For example, in a task-oriented weather domain, a seed response can be: The weather in New York is raining and the annotation is a semantic frame that contains domain general dialog acts and slot arguments, i.e. [Inform, loc=New York, type=rain]. The number of seed responses is often much smaller than the number of potential responses in the domain so it is best for SR to cover more responses that are unique to this domain. SRs assume that there is a discourselevel pattern that can be shared between the source and target domains, so that a system only needs\nsentence-level knowledge to adapt to the target. This assumption holds in many slot-filling dialog domains and it is easy to provide utterances in the target domain that are analogies to the ones from the source domains."
  }, {
    "heading": "4.2 Action Matching Encoder-Decoder",
    "text": "Figure 1 shows an overview of the model we use to tackle ZSDG. The base model is a standard encoder-decoder F where an encoder Fe maps c and d into a distributed representation zc = Fe(c, d) and the decoder Fd generates the response x given zc. We denote the embedding space that zc resides in as the latent action space. We follow the KB-as-an-environment approach (Zhao and Eskenazi, 2016) where the generated x include both system verbal utterances and API queries that interface with back-end databases. This base model has been proven to be effective in human interactive evaluation for taskoriented dialogs (Zhao et al., 2017). We have two high-level goals: (1) learn a crossdomain F that can be reused in all source domains and potentially shared with target domains as well. (2) create a mechanism to incorporate knowledge from the domain descriptions into F so that it can generate novel responses when tested on the target domains. To achieve the first goal, we combine c and d by appending d as a special word token at the beginning of every utterance in c. This simple approach performs well and enables the context encoder to take the domain into account when processing later word tokens. Also, this context domain integration can easily scale to dealing with a large number of domains. Then we encourage F\n4 to discover reusable dialog policy by training the same encoder decoder on dialog data generated from multiple source domains at the same time, which is a form of multi-task learning (Collobert and Weston, 2008). We achieve the second goal by projecting the response x from all domains into the same latent action space Z. Since x alone may not be sufficient to infer its semantics, we rely on their annotations a to learn meaningful semantic representations. Let zx and za be the projected latent actions from x and a. Our method encourages zd1x1 ≈ z d2 x2 when z d1 a1 ≈ z d2 a2 . Moreover, for a given z from any domain, we ensure that the decoder Fd can generate the corresponding response x by training on both SRd for d ∈ D and source dialogs. Specifically, we propose the Action Matching (AM) training procedure. We first introduce a recognition network R that can encode x and a into zx = R(x, d) and za = R(a, d) respectively. During training, the model receives two types of data. The first type is domain description data in the form of {x,a, d}seed for each domain. The second type of data is source domain dialog data in the form of {c,x, d}. For the first type of data, we update the parameters in R and Fd by minimizing the following loss function: Ldd(Fd,R) =− log pFd(x|R(a, d)) + λD[R(x, d)‖R(a, d)] (1) where λ is a constant hyperparameter and D is a distance function, e.g. mean square error (MSE), that measures the closeness of two input vectors. The first term in Ldd trains the decoder Fd to generate the response x given za = R(a, d) from all domains. The second term in Ldd enforces the recognition network R to encode a response and its annotation to nearby vectors in the latent action space from all domains, i.e. zdx ≈ zda for d ∈ D. Moreover, just optimizing Ldd does not ensure that the zc predicted by the encoder Fe will be related to the zx or za encoded by the recognition networkR. So when we receive the second type of data (source dialogs), we add a second term to the standard maximum likelihood objective to train F andR. Ldialog(F ,R) =− log pFd(x|Fe(c, d)) + λD(R(x, d)‖Fe(c, d)) (2) The second term in Ldialog completes the loop by encouraging zdc ≈ zdx, which resembles the regularization term used in variational autoencoders (Kingma and Welling, 2013). Assuming that annotation a provides a domain-agnostic semantic representation of x, then F trained on source domains can begin to operate in the target domains as well. During training, our AM algorithm alternates between these two types of data and optimizes Ldd or Ldialog accordingly. The resulting models effectively learn a latent action space that is shared by the the response annotation a, response x and predicted latent action based on c in all domains. AM training is summarized in Algorithm 1. Algorithm 1: Action Matching Training Initialize weights of Fe, Fd,R; Data = {c,x, d} ⋃ {x,a, d}seed while batch ∼ Data do if batch in the form {c,x, d} then Backpropagate loss Ldialog else Backpropagate loss Ldd end end"
  }, {
    "heading": "4.3 Architecture Details",
    "text": "We implement an AMED for later experiments as follows: Distance Functions: In this study, we assume that the latent actions are deterministic distributed vectors. Thus MSE is used: D(z, ẑ) = 1L ∑L l (zl− ẑl) 2, where L is the dimension size of the latent actions. Also, Ldialog and Ldd use the same distance function. Recognition Networks: we use a bidirectional GRU-RNN (Cho et al., 2014) as R to obtain utterance-level embedding. Since both x and a are sequences of word tokens, we combine them with the domain tag by appending the domain tag in the beginning of the original word sequence, i.e. {x, d} or {a, d} = [d,w1, ...wJ ], where J is the length of the word sequence. Then the R will encode [d,w1, ...wJ ] into hidden outputs in forward and backward directions, [( −→ h0, ←− hJ), ...( −→ hJ , ←− h0)]. We use the concatenation of the last hidden states from each direction, i.e. zx or za = [ −→ hJ , ←− hJ ] as utterance-level embedding for x or a respectively. Dialog Encoders: a hierarchical recurrent encoder (HRE) is used to encode the dialog context, which handles long contexts better than non-\n5\nhierarchical ones (Li et al., 2015). HRE first uses an utterance encoder to encode every utterance in the dialog and then uses a discourse-level LSTM-RNN to encode the dialog context by taking output from the utterance encoder as input. Instead of introducing a new utterance encoder, we reuse the recognition network R described above as the utterance encoder, which serves the purpose perfectly. Another advantage is that using zx predicted by R as input enables the discourselevel encoder to use knowledge from latent actions as well. Our discourse-level encoder is a 1- layer LSTM-RNN (Hochreiter and Schmidhuber, 1997), which takes in a list of output [z1, z2..zK ] from R and encodes them into [v1, v2, ...vK ], where K is the number of utterances in the context. The last hidden state vK is used as the predicted latent action zc. Response Decoders: we experiment with two types of LSTM-RNN decoders. The first is an RNN decoder with an attention mechanism (Luong et al., 2015), enabling the decoder to dynamically look up information from the context. Specifically, we flatten the dialog context into a sequence of words [w11, ...w1J ...wKJ ]. Using output from the R and the discourse-level LSTMRNN, each word here is represented by mkj = hkj +Wvvk. Let the hidden state of the decoder at step t be st, then our attention mechanism computes the Softmax output via: αkj,t = softmax(mTkj tanh(Wαst)) (3) s̃t = ∑ kj αkj,tmkj (4) pvocab(wt|st) = softmax(MLP(st, s̃t)) (5) The second type is the LSTM-RNN with a copy mechanism that can directly copy words from the context as output (Gu et al., 2016). Such a mechanism has already exhibited strong performance in task-oriented dialogs (Eric and Manning, 2017a) and is well suited for generating OOV word tokens (Elsahar et al., 2018). We implemented the Pointer Sentinel Mixture Model (PSM) (Merity et al., 2016) as our copy decoder. PSM defines the generation of the next word as a mixture of probabilities from either the Softmax output from the decoder LSTM or the attention Softmax for words in the context: p(wt|st) = gpvocab(wt|st) + (1 − g)pptr(wt|st), where g is the mixture weight computed from a sentinel vector u with st. pptr(wt|st) = ∑ kj∈I(w,x) αkj,t (6) g = softmax(uT tanh(Wαsi)) (7)"
  }, {
    "heading": "5 Datasets for ZSDG",
    "text": "Two dialog datasets were used for evaluation."
  }, {
    "heading": "5.1 SimDial Data",
    "text": "We developed SimDial2, which is a multi-domain dialog generator that can generate realistic conversations for slot-filling domains with configurable complexity. See Appendix A.3 for details. Compared to other synthetic dialog corpora used to test GEDMs, e.g. bAbI (Dodge et al., 2015), SimDial data is significantly more challenging. First since SimDial simulates communication noise, the dialogs that are generated can be very long (more than 50 turns) and the simulated agent can carry out error recovery strategies to correctly infer the users’ goals. This challenges end-to-end models 2https://github.com/snakeztc/SimDial\n6 to model long dialog contexts. SimDial also simulates spoken language phenomena, e.g. self-repair, hesitation. Prior work (Eshghi et al., 2017) has shown that this type of utterance-level noise deteriorates end-to-end dialog system performance. Data Details SimDial was used to generate dialogs for 6 domains: restaurant, movie, bus, restaurant-slot, restaurant-style and weather. For each domain, 900/100/500 dialogs were generated for training, validation and testing. On average, each dialog had 26 utterances and each utterance had 12.8 word tokens. The total vocabulary size was 651. We split the data such that the training data included dialogs from the restaurant, bus and weather domains and the test data included the restaurant, movie, restaurant-slot and restaurant style domains. This setup evaluates a ZSDG system from the following perspectives: Restaurant (in domain): evaluation on the restaurant test data checks if a dialog model is able to maintain its performance on the source domains. Restaurant-slot (unseen slots): restaurant-slot has the same slot types and natural language generation (NLG) templates as the restaurant domain, but has a completely different slot vocabulary, i.e. different location names and cuisine types. Thus this is designed to evaluate a model that can generalize to unseen slot values. Restaurant-style (unseen NLG): restaurant-style has the same slot type and vocabulary as restaurant, but its NLG templates are completely different, e.g. “which cuisine type?” → “please tell me what kind of food you prefer”. This part tests whether a model can learn to adapt to generate novel utterances with similar semantics. Movie (new domain): movie has completely different NLG templates and structure and shares few common traits with the source domains at the surface level. Movie is the hardest task in the SimDial data, which challenges a model to correctly generate next responses that are semantically different from the ones in source domains. Finally, we obtain SRs as domain descriptions by randomly selecting 100 unique utterances from each domain. The response annotation is a response’s internal semantic frame used by the SimDial generator. For example, “I believe you said Boston. Where are you going?” → [implicitconfirm loc=Boston; request location]."
  }, {
    "heading": "5.2 Stanford Multi-Domain Dialog Data",
    "text": "The second dataset is the Stanford multi-domain dialog (SMD) dataset (Eric and Manning, 2017b) of 3031 human-human dialogs in three domains: weather, navigation and scheduling. One speaker plays the role of a driver. The other plays the car’s AI assistant and talks to the driver to complete tasks, e.g. setting directions on a GPS. Average dialog length is 5.25 utterances; vocabulary size is 1601. We use SMD to validate whether our proposed methods generalize to human-generated dialogs. We generate SR by randomly selecting 150 unique utterances for each domain. An expert annotates the seed utterances with dialog acts and entities. For example “All right, I’ve set your next dentist appointment for 10am. Anything else?” → [ack; inform goal event=dentist appointment time=10am ; request needs]. Finally, in order to formulate a ZSDG problem, we use a leave-oneout approach with two domains as source domains and the third one as the target domain, which results in 3 possible configurations."
  }, {
    "heading": "6 Experiments and Results",
    "text": "The baseline models include 1. hierarchical recurrent encoder with attention decoder (+Attn) (Serban et al., 2016). 2. hierarchical recurrent encoder with copy decoder (Merity et al., 2016) (+Copy), which has achieved very good performance on task-oriented dialogs (Eric and Manning, 2017a). We then augment both baseline models with the proposed cross-domain AM training procedure and denote them as +Attn+AM and +Copy+AM. Evaluating generative dialog systems is challenging since the model can generate free-form responses. Fortunately, we have access to the internal semantic frames of the SimDial data, so we use the automatic measures used in (Zhao et al., 2017) that employ four metrics to quantify the performance of a task-oriented dialog model. BLEU is the corpus-level BLEU-4 between the generated response and the reference ones (Papineni et al., 2002). Entity F1 checks if a generated response contains the correct entities (slots) in the reference response. Act F1 measures whether the generated responses reflect the dialog acts in the reference responses, which compensates for BLEU’s limitation of looking for exact word choices. A onevs-rest support vector machine (Scholkopf and Smola, 2001) with bi-gram features is trained to\n7\ntag the dialogs in a response. KB F1 checks all the key words in a KB query that the system issues to the KB backend. Finally, we introduce BEAK = 4 √ bleu× ent× act× kb, the geometric mean of these four scores, to quantify a system’s overall performance. Meanwhile, since the oracle dialog acts and KB queries are not provided in the SMD data (Eric and Manning, 2017b), we only report BLEU and entity F1 results on SMD."
  }, {
    "heading": "6.1 Main Results",
    "text": "Table 1 shows results on the SimDial data. Although the standard +Attn model achieves good performance in the source domains, it doesn’t generalize to target domains, especially for entity F1 in the unseen-slot domain, BLEU score in the unseen-NLG domain, and all new domain metrics. The +Copy model has better, although still limited, generalization to target domains. The main benefit of the +Copy model is its ability to directly copy and output words from the context, reflected in its strong entity F1 in the unseen slot domain. However, +Copy can’t generalize to new domains where utterances are novel, e.g. the unseen NLG or the new domain. However, our AM algorithm substantially improves\nTable 2 summarizes the results on the SMD data. We also report the oracle performance, obtained by training +Copy on the full dataset. The AM algorithm can significantly improve Entity F1 and BLEU from the two baseline models. +Copy+AM also achieves competitive performance in terms of Entity F1 compared to the oracle scores, despite the fact that no target domain data was used in training."
  }, {
    "heading": "6.2 Model Analysis",
    "text": "Various types of performance improvement were also studied. Figure 3 shows the breakdown of the BLEU score according to the dialog acts of reference responses. Models with +Copy decoder can improve performance for all dialog acts except for the greet act, which occurs at the beginning of a dialog. In this case, the +Copy decoder has no context to copy and thus cannot generate any novel responses. This is one limitation of +Copy decoder since in real interactive testing with humans,\n8\neach system utterance must be generated from the model instead of copied from the context. However, models with AM training learn to generate novel utterances based on knowledge from the SR, so +Copy+AM can generate responses at the beginning of a dialog.\nA qualitative analysis was conducted to summarize typical responses from these models. Table 3 shows three types of typical situations in the SimDial data. The first type is general utterance utterances, e.g. “See you next time” that appear in all domains. All three models correctly generate them in the ZSDG setting. The second type is utterances with unseen slots. For example, explicit confirm “Do you mean xx?”. +Attn fails in this situation since the new slot values are not in its vocabulary. +Copy still performs well since it learns to copy entity-like words from the context, but the overall sentence is often incorrect, e.g. “Do you mean romance food”. The last one is unseen utterance where both +Attn and +Copy fail. The two baseline models can still generate responses with correct dialog acts, but the output words are in the source domains. Only the models trained with AM are able to infer that “Movie xx is a great movie” serves a function similar to “Bus xx can take you there”, and generates responses using the correct words from the target domain. Finally we investigate how the the size of SR affects AM performance. Figure 4 shows results in the SMD schedule domain. The number of seed\nresponses varies from 0 to 200. Performance in the target domains is positively correlated with the number of seed responses. We also observe that the model achieves sufficient SR performance at 100, compared to the ones trained on all of the 200 seed responses. This suggests that the amount of seeding needed by SR is relatively small, which shows the practicality of using SR as a domain description."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "This paper introduces ZSDG, dealing with neural dialog systems’ domain generalization ability. We formalize the ZSDG problem and propose an Action Matching framework that discovers crossdomain latent actions. We present a new simulated multi-domain dialog dataset, SimDial, to benchmark the ZSDG models. Our assessment validates the AM framework’s effectiveness and the AM encoder decoders perform well in the ZSDG setting. ZSDG provides promising future research questions. How can we reduce the annotation cost of learning the latent alignment between actions in different domains? How can we create ZSDG for new domains where the discourse-level patterns are significantly different? What are other potential domain description formats? In summary, solving ZSDG is an important step for future general-purpose conversational agents.\n9"
  }, {
    "heading": "A Supplemental Material",
    "text": "A.1 Seed Response Creation Process We follow the following process to create SR in a new slot-filling domain. First, we collect seed responses (including user/system utterances, KB queries and KB responses) from each source domain and annotate them with dialog acts, entity types and entity values. Then human experts with knowledge about the target domain can write up seed responses for the target domain by drawing ideas from the sources. For example, if the source domain is restaurants and the target domain is movies. The source may contain a system utterance with its annotation: “I believed you said Pittsburgh, what kind of food are you interested in? → [implicit-confirm, loc=Pittsburgh, request food type]”. Then the expert can come up with a similar utterance from the target domain, e.g. “Alright, Pittsburgh. what type of movie do you like? → [implicit-confirm, loc=Pittsburgh, request movie type]”. In this way, our proposed AM training can leverage the annotations to match these two actions as analogies in the latent action space. Another advantage of this process is that human experts do not have to directly label whether two utterances from two domains are direct analogies; this could be ambiguous and challenging. Instead, human experts only create domain shareable annotations and leave the difficult matching problem to our models.\nA.2 Model Details For all experiments, we use a word embedding with size 200. The recognition network uses bidirectional GRU-RNN with hidden size 256 for each\ndirection. The discourse-level LSTM-RNN and the decoder have hidden size 512. The models are trained with Adam (Kingma and Ba, 2014) with learning rate 0.001. All weights are initialized from uniform distribution between +/- 0.08. 30% dropouts are applied at the input and output of recurrent neural networks (Zaremba et al., 2014). For decoders with copy mechanisms, we also use the pointer loss proposed in (Merity et al., 2016) which minimizes− log(g+∑\nkj∈I(x,c) αkj), where I results in all positions of output x in the attention context.\nA.3 SimDial Details To generate data, SimDial expects: a domain specification (DS) and a complexity specification (CS). DS defines the content of the domain, e.g. restaurant or weather and CS defines complexity of dialogs for three aspects:\nEnvironmental: the complexity level of the communication channel, e.g. automatic speech recognition (ASR) error rate.\nPropositional: the complexity level in the propositional content of user utterances, e.g. the chance that a user will give multiple slot values in one utterance.\nInteraction: the complexity level in terms of real-time interaction, e.g. the percentage of selfrepair.\nThe following is an example dialog generated for the restaurant domain with all of the above complexity turned on. The number at the end of the speaker turn indicates ASR confidence."
  }],
  "year": 2018,
  "references": [{
    "title": "Towards zero-shot frame semantic parsing for domain scaling",
    "authors": ["References Ankur Bapna", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."],
    "venue": "arXiv preprint arXiv:1707.02363 .",
    "year": 2017
  }, {
    "title": "Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models",
    "authors": ["Yun-Nung Chen", "Dilek Hakkani-Tür", "Xiaodong He."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-",
    "year": 2016
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "arXiv preprint",
    "year": 2014
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160–167.",
    "year": 2008
  }, {
    "title": "Evaluating prerequisite qualities for learning end-to-end dialog systems",
    "authors": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1511.06931 .",
    "year": 2015
  }, {
    "title": "One-shot imitation learning",
    "authors": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba."],
    "venue": "arXiv preprint arXiv:1703.07326 .",
    "year": 2017
  }, {
    "title": "Zero-shot question generation from knowledge graphs for unseen predicates and entity types",
    "authors": ["Hady Elsahar", "Christophe Gravier", "Frederique Laforest."],
    "venue": "arXiv preprint arXiv:1802.06842 .",
    "year": 2018
  }, {
    "title": "A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue",
    "authors": ["Mihail Eric", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1701.04024 .",
    "year": 2017
  }, {
    "title": "Keyvalue retrieval networks for task-oriented dialogue",
    "authors": ["Mihail Eric", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1705.05414 .",
    "year": 2017
  }, {
    "title": "Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars",
    "authors": ["Arash Eshghi", "Igor Shalyminov", "Oliver Lemon."],
    "venue": "arXiv preprint arXiv:1709.07858 .",
    "year": 2017
  }, {
    "title": "Gaussian processes for pomdp-based dialogue manager optimization",
    "authors": ["Milica Gasic", "Steve Young."],
    "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 22(1):28–40.",
    "year": 2014
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."],
    "venue": "arXiv preprint arXiv:1603.06393 .",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "arXiv preprint arXiv:1312.6114 .",
    "year": 2013
  }, {
    "title": "Zero-data learning of new tasks",
    "authors": ["Hugo Larochelle", "Dumitru Erhan", "Yoshua Bengio."],
    "venue": "AAAI. 2, page 3.",
    "year": 2008
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1506.01057 .",
    "year": 2015
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1508.04025 .",
    "year": 2015
  }, {
    "title": "Pointer sentinel mixture models",
    "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."],
    "venue": "arXiv preprint arXiv:1609.07843 .",
    "year": 2016
  }, {
    "title": "Zero-shot task generalization with multi-task deep reinforcement learning",
    "authors": ["Junhyuk Oh", "Satinder Singh", "Honglak Lee", "Pushmeet Kohli."],
    "venue": "arXiv preprint arXiv:1706.05064 .",
    "year": 2017
  }, {
    "title": "Zero-shot learning with semantic output codes",
    "authors": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell."],
    "venue": "Advances in neural information processing systems. pages 1410–1418.",
    "year": 2009
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational",
    "year": 2002
  }, {
    "title": "An embarrassingly simple approach to zero-shot learning",
    "authors": ["Bernardino Romera-Paredes", "Philip Torr."],
    "venue": "International Conference on Machine Learning. pages 2152–2161.",
    "year": 2015
  }, {
    "title": "Learning with kernels: support vector machines, regularization, optimization, and beyond",
    "authors": ["Bernhard Scholkopf", "Alexander J Smola."],
    "venue": "MIT press.",
    "year": 2001
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1605.06069 .",
    "year": 2016
  }, {
    "title": "Zero-shot learning through cross-modal transfer",
    "authors": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng."],
    "venue": "Advances in neural information processing systems. pages 935–943.",
    "year": 2013
  }, {
    "title": "A networkbased end-to-end trainable task-oriented dialogue system",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."],
    "venue": "arXiv preprint arXiv:1604.04562 .",
    "year": 2016
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."],
    "venue": "arXiv preprint arXiv:1409.2329 .",
    "year": 2014
  }, {
    "title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning",
    "authors": ["Tiancheng Zhao", "Maxine Eskenazi."],
    "venue": "arXiv preprint arXiv:1606.02560 .",
    "year": 2016
  }, {
    "title": "Generative encoderdecoder models for task-oriented spoken dialog systems with chatting capability",
    "authors": ["Tiancheng Zhao", "Allen Lu", "Kyusong Lee", "Maxine Eskenazi."],
    "venue": "arXiv preprint arXiv:1706.08476 .",
    "year": 2017
  }],
  "id": "SP:751ef806f5dc01b9f84902f1e160eda5d5142857",
  "authors": [{
    "name": "Tiancheng Zhao",
    "affiliations": []
  }, {
    "name": "Maxine Eskenazi",
    "affiliations": []
  }],
  "abstractText": "This paper introduces zero-shot dialog generation (ZSDG), as a step towards neural dialog systems that can instantly generalize to new situations with minimal data. ZSDG enables an end-to-end generative dialog system to generalize to a new domain for which only a domain description is provided and no training dialogs are available. Then a novel learning framework, Action Matching, is proposed. This algorithm can learn a cross-domain embedding space that models the semantics of dialog responses which, in turn, lets a neural dialog generation model generalize to new domains. We evaluate our methods on a new synthetic dialog dataset, and an existing human-human dialog dataset. Results show that our method has superior performance in learning dialog models that rapidly adapt their behavior to new domains and suggests promising future research.1",
  "title": "Zero-Shot Dialog Generation with Cross-Domain Latent Actions"
}