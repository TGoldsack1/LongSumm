{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In Reinforcement Learning (RL) an agent seeks an optimal policy for a sequential decision making problem (Sutton & Barto, 1998). It does so by learning which action is optimal for each environment state. Over the course of time, many algorithms have been introduced for solving RL problems including Q-learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994; Sutton & Barto, 1998), and policy gradient methods (Sutton et al., 1999). These methods are often analyzed in the setup of linear function approximation, where convergence is guaranteed under mild assumptions (Tsitsiklis, 1994; Jaakkola et al., 1994; Tsitsiklis & Van Roy, 1997; Even-Dar & Mansour, 2003). In practice, real-world problems usually involve high-dimensional inputs forcing linear function approximation methods to rely upon hand engineered features\n1Department of Electrical Engineering, Haifa 32000, Israel. Correspondence to: Oron Anschel <oronanschel@campus.technion.ac.il>, Nir Baram <nirb@campus.technion.ac.il>, Nahum Shimkin <shimkin@ee.technion.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nfor problem-specific state representation. These problemspecific features diminish the agent flexibility, and so the need of an expressive and flexible non-linear function approximation emerges. Except for few successful attempts (e.g., TD-gammon, Tesauro (1995)), the combination of non-linear function approximation and RL was considered unstable and was shown to diverge even in simple domains (Boyan & Moore, 1995).\nThe recent Deep Q-Network (DQN) algorithm (Mnih et al., 2013), was the first to successfully combine a powerful non-linear function approximation technique known as Deep Neural Network (DNN) (LeCun et al., 1998; Krizhevsky et al., 2012) together with the Q-learning algorithm. DQN presented a remarkably flexible and stable algorithm, showing success in the majority of games within the Arcade Learning Environment (ALE) (Bellemare et al., 2013). DQN increased the training stability by breaking the RL problem into sequential supervised learning tasks. To do so, DQN introduces the concept of a target network and uses an Experience Replay buffer (ER) (Lin, 1993).\nFollowing the DQN work, additional modifications and extensions to the basic algorithm further increased training stability. Schaul et al. (2015) suggested sophisticated ER sampling strategy. Several works extended standard RL exploration techniques to deal with high-dimensional input (Bellemare et al., 2016; Tang et al., 2016; Osband et al., 2016). Mnih et al. (2016) showed that sampling from ER could be replaced with asynchronous updates from parallel environments (which enables the use of on-policy methods). Wang et al. (2015) suggested a network architecture base on the advantage function decomposition (Baird III, 1993).\nIn this work we address issues that arise from the combination of Q-learning and function approximation. Thrun & Schwartz (1993) were first to investigate one of these issues which they have termed as the overestimation phenomena. The max operator in Q-learning can lead to overestimation of state-action values in the presence of noise. Van Hasselt et al. (2015) suggest the Double-DQN that uses the Double Q-learning estimator (Van Hasselt, 2010) method as a solution to the problem. Additionally, Van Hasselt et al. (2015) showed that Q-learning overestimation do occur in practice\n(at least in the ALE).\nThis work suggests a different solution to the overestimation phenomena, named Averaged-DQN (Section 3), based on averaging previously learned Q-values estimates. The averaging reduces the target approximation error variance (Sections 4 and 5) which leads to stability and improved results. Additionally, we provide experimental results on selected games of the Arcade Learning Environment.\nWe summarize the main contributions of this paper as follows:\n• A novel extension to the DQN algorithm which stabilizes training, and improves the attained performance, by averaging over previously learned Q-values.\n• Variance analysis that explains some of the DQN problems, and how the proposed extension addresses them.\n• Experiments with several ALE games demonstrating the favorable effect of the proposed scheme."
  }, {
    "heading": "2. Background",
    "text": "In this section we elaborate on relevant RL background, and specifically on the Q-learning algorithm."
  }, {
    "heading": "2.1. Reinforcement Learning",
    "text": "We consider the usual RL learning framework (Sutton & Barto, 1998). An agent is faced with a sequential decision making problem, where interaction with the environment takes place at discrete time steps (t = 0, 1, . . .). At time t the agent observes state st ∈ S, selects an action at ∈ A, which results in a scalar reward rt ∈ R, and a transition to a next state st+1 ∈ S. We consider infinite horizon problems with a discounted cumulative reward objective Rt = �∞ t�=t γ\nt�−trt� , where γ ∈ [0, 1] is the discount factor. The goal of the agent is to find an optimal policy π : S → A that maximize its expected discounted cumulative reward.\nValue-based methods for solving RL problems encode policies through the use of value functions, which denote the expected discounted cumulative reward from a given state s, following a policy π. Specifically we are interested in state-action value functions:\nQπ(s, a) = Eπ � ∞�\nt=0\nγtrt |s0 = s, a0 = a � .\nThe optimal value function is denoted as Q∗(s, a) = maxπ Q\nπ(s, a), and an optimal policy π∗ can be easily derived by π∗(s) ∈ argmaxaQ∗(s, a)."
  }, {
    "heading": "2.2. Q-learning",
    "text": "One of the most popular RL algorithms is the Q-learning algorithm (Watkins & Dayan, 1992). This algorithm is based on a simple value iteration update (Bellman, 1957), directly estimating the optimal value function Q∗. Tabular Q-learning assumes a table that contains old action-value function estimates and preform updates using the following update rule:\nQ(s, a) ← Q(s, a) + α(r + γmax a� Q(s�, a�)−Q(s, a)), (1) where s� is the resulting state after applying action a in the state s, r is the immediate reward observed for action a at state s, γ is the discount factor, and α is a learning rate.\nWhen the number of states is large, maintaining a lookup table with all possible state-action pairs values in memory is impractical. A common solution to this issue is to use function approximation parametrized by θ, such that Q(s, a) ≈ Q(s, a; θ)."
  }, {
    "heading": "2.3. Deep Q Networks (DQN)",
    "text": "We present in Algorithm 1 a slightly different formulation of the DQN algorithm (Mnih et al., 2013). In iteration i the DQN algorithm solves a supervised learning problem to approximate the action-value function Q(s, a; θ) (line 6). This is an extension of implementing (1) in its function approximation form (Riedmiller, 2005).\nAlgorithm 1 DQN 1: Initialize Q(s, a; θ) with random weights θ0 2: Initialize Experience Replay (ER) buffer B 3: Initialize exploration procedure Explore(·) 4: for i = 1, 2, . . . , N do 5: yis,a = EB [r + γmaxa� Q(s�, a�; θi−1)| s, a] 6: θi ≈ argminθ EB � (yis,a −Q(s, a; θ))2 �\n7: Explore(·), update B 8: end for\noutput QDQN(s, a; θN )\nThe target values yis,a (line 5) are constructed using a designated target-network Q(s, a; θi−1) (using the previous iteration parameters θi−1), where the expectation (EB) is taken w.r.t. the sample distribution of experience transitions in the ER buffer (s, a, r, s�) ∼ B. The DQN loss (line 6) is minimized using a Stochastic Gradient Descent (SGD) variant, sampling mini-batches from the ER buffer. Additionally, DQN requires an exploration procedure (which we denote as Explore(·)) to interact with the environment (e.g., an �- greedy exploration procedure). The number of new experience transitions (s, a, r, s�) added by exploration to the ER\nbuffer in each iteration is small, relatively to the size of the ER buffer. Thereby, θi−1 can be used as a good initialization for θ in iteration i.\nNote that in the original implementation (Mnih et al., 2013; 2015), transitions are added to the ER buffer simultaneously with the minimization of the DQN loss (line 6). Using the hyperparameters employed by Mnih et al. (2013; 2015) (detailed for completeness in Appendix E), 1% of the experience transitions in ER buffer are replaced between target network parameter updates, and 8% are sampled for minimization."
  }, {
    "heading": "3. Averaged DQN",
    "text": "The Averaged-DQN algorithm (Algorithm 2) is an extension of the DQN algorithm. Averaged-DQN uses the K previously learned Q-values estimates to produce the current action-value estimate (line 5). The Averaged-DQN algorithm stabilizes the training process (see Figure 1), by reducing the variance of target approximation error as we elaborate in Section 5. The computational effort compared to DQN is, K-fold more forward passes through a Q-network while minimizing the DQN loss (line 7). The number of back-propagation updates remains the same as in DQN. Computational cost experiments are provided in Appedix D. The output of the algorithm is the average over the last K previously learned Q-networks.\nIn Figures 1 and 2 we can see the performance of Averaged-\nAlgorithm 2 Averaged DQN 1: Initialize Q(s, a; θ) with random weights θ0 2: Initialize Experience Replay (ER) buffer B 3: Initialize exploration procedure Explore(·) 4: for i = 1, 2, . . . , N do 5: QAi−1(s, a) = 1 K �K k=1 Q(s, a; θi−k)\n6: yis,a = EB � r + γmaxa� Q A i−1(s �, a�)| s, a �\n7: θi ≈ argminθ EB � (yis,a −Q(s, a; θ))2 � 8: Explore(·), update B 9: end for\noutput QAN (s, a) = 1 K �K−1 k=0 Q(s, a; θN−k)\nDQN compared to DQN (and Double-DQN), further experimental results are given in Section 6.\nWe note that recently-learned state-action value estimates are likely to be better than older ones, therefore we have also considered a recency-weighted average. In practice, a weighted average scheme did not improve performance and therefore is not presented here."
  }, {
    "heading": "4. Overestimation and Approximation Errors",
    "text": "Next, we discuss the various types of errors that arise due to the combination of Q-learning and function approximation in the DQN algorithm, and their effect on training stability. We refer to DQN’s performance in the BREAKOUT game in Figure 1. The source of the learning curve variance in DQN’s performance is an occasional sudden drop in the average score that is usually recovered in the next evaluation phase (for another illustration of the variance source see Appendix A). Another phenomenon can be observed in Figure 2, where DQN initially reaches a steady state (after 20 million frames), followed by a gradual deterioration in performance.\nFor the rest of this section, we list the above mentioned errors, and discuss our hypothesis as to the relations between each error and the instability phenomena depicted in Figures 1 and 2.\nWe follow terminology from Thrun & Schwartz (1993), and define some additional relevant quantities. Letting Q(s, a; θi) be the value function of DQN at iteration i, we denote Δi = Q(s, a; θi) − Q∗(s, a) and decompose it as follows:\nΔi = Q(s, a; θi)−Q∗(s, a) = Q(s, a; θi)− yis,a� �� �\nTarget Approximation Error\n+ yis,a − ŷis,a� �� � Overestimation\nError\n+ ŷis,a −Q∗(s, a)� �� � Optimality Difference .\nHere yis,a is the DQN target, and ŷ i s,a is the true target:\nyis,a = EB � r + γmax\na� Q(s�, a�; θi−1)| s, a\n� ,\nŷis,a = EB � r + γmax\na� (yi−1s�,a�)| s, a\n� .\nLet us denote by Zis,a the target approximation error, and by Ris,a the overestimation error, namely\nZis,a = Q(s, a; θi)− yis,a, Ris,a = y i s,a − ŷis,a.\nThe optimality difference can be seen as the error of a standard tabular Q-learning, here we address the other errors. We next discuss each error in turn."
  }, {
    "heading": "4.1. Target Approximation Error (TAE)",
    "text": "The TAE (Zis,a), is the error in the learned Q(s, a; θi) relative to yis,a, which is determined after minimizing the DQN loss (Algorithm 1 line 6, Algorithm 2 line 7). The TAE is a result of several factors: Firstly, the sub-optimality of θi due to inexact minimization. Secondly, the limited representation power of a neural net (model error). Lastly, the generalization error for unseen state-action pairs due to the finite size of the ER buffer.\nThe TAE can cause a deviations from a policy to a worse one. For example, such deviation to a sub-optimal policy occurs in case yis,a = ŷ i s,a = Q ∗(s, a) and,\nargmaxa[Q(s, a; θi)] �= argmaxa[Q(s, a; θi)− Zis,a] = argmaxa[y i s,a].\nWe hypothesize that the variability in DQN’s performance in Figure 1, that was discussed at the start of this section, is related to deviating from a steady-state policy induced by the TAE."
  }, {
    "heading": "4.2. Overestimation Error",
    "text": "The Q-learning overestimation phenomena were first investigated by Thrun & Schwartz (1993). In their work, Thrun and Schwartz considered the TAE Zis,a as a random variable uniformly distributed in the interval [−�, �]. Due to the max operator in the DQN target yis,a, the expected overestimation errors Ez[Ris,a] are upper bounded by γ�n−1n+1 (where n is the number of applicable actions in state s). The intuition for this upper bound is that in the worst case, all Q values are equal, and we get equality to the upper bound:\nEz[Ris,a] = γEz[max a� [Zi−1s�,a� ]] = γ� n− 1 n+ 1 .\nThe overestimation error is different in its nature from the TAE since it presents a positive bias that can cause asymptotically sub-optimal policies, as was shown by Thrun & Schwartz (1993), and later by Van Hasselt et al. (2015) in the ALE environment. Note that a uniform bias in the action-value function will not cause a change in the induced policy. Unfortunately, the overestimation bias is uneven and is bigger in states where the Q-values are similar for the different actions, or in states which are the start of a long trajectory (as we discuss in Section 5 on accumulation of TAE variance).\nFollowing from the above mentioned overestimation upper bound, the magnitude of the bias is controlled by the variance of the TAE.\nThe Double Q-learning and its DQN implementation (Double-DQN) (Van Hasselt et al., 2015; Van Hasselt, 2010) is one possible approach to tackle the overestimation problem, which replaces the positive bias with a negative one. Another possible remedy to the adverse effects of this error is to directly reduce the variance of the TAE, as in our proposed scheme (Section 5).\nIn Figure 2 we repeated the experiment presented in Van Hasselt et al. (2015) (along with the application of Averaged-DQN). This experiment is discussed in Van Hasselt et al. (2015) as an example of overestimation that leads to asymptotically sub-optimal policies. Since AveragedDQN reduces the TAE variance, this experiment supports an hypothesis that the main cause for overestimation in DQN is the TAE variance."
  }, {
    "heading": "5. TAE Variance Reduction",
    "text": "To analyse the TAE variance we first must assume a statistical model on the TAE, and we do so in a similar way to Thrun & Schwartz (1993). Suppose that the TAE Zis,a is a random process such that E[Zis,a] = 0, Var[Zis,a] = σ2s , and for i �= j: Cov[Zis,a, Zjs�,a� ] = 0. Furthermore, to focus only on the TAE we eliminate the overestimation error by considering a fixed policy for updating the target values. Also, we can conveniently consider a zero reward r = 0 everywhere since it has no effect on variance calculations.\nDenote by Qi � Q(s; θi)s∈S the vector of value estimates in iteration i (where the fixed action a is suppressed), and by Zi the vector of corresponding TAEs. For AveragedDQN we get:\nQi = Zi + γP 1\nK\nK�\nk=1\nQi−k,\nwhere P ∈ RS×S+ is the transition probabilities matrix for the given policy. The covariance of the above Vector Autoregressive (VAR) model is given by the discretetime Lyapunov equation, and can be solved directly or by specialized numerical algorithms (Arthur E Bryson, 1975). However, to obtain an explicit comparison, we further specialize the model to an M -state unidirectional MDP as in Figure 3"
  }, {
    "heading": "5.1. DQN Variance",
    "text": "We assume the statistical model mentioned at the start of this section. Consider a unidirectional Markov Decision Process (MDP) as in Figure 3, where the agent starts at state s0, state sM−1 is a terminal state, and the reward in any state is equal to zero.\nEmploying DQN on this MDP model, we get that for i > M :\nQDQN(s0, a; θi) = Z i s0,a + y i s0,a\n= Zis0,a + γQ(s1, a; θi−1) = Zis0,a + γ[Z i−1 s1,a + y i−1 s1,a] = · · · = = Zis0,a + γZ i−1 s1,a + · · ·+ γ(M−1)Zi−(M−1)sM−1,a ,\nwhere in the last equality we have used the fact yjM−1,a = 0 for all j (terminal state). Therefore,\nVar[QDQN(s0, a; θi)] = M−1�\nm=0\nγ2mσ2sm .\nThe above example gives intuition about the behavior of the TAE variance in DQN. The TAE is accumulated over the past DQN iterations on the updates trajectory. Accumulation of TAE errors results in bigger variance with its associated adverse effect, as was discussed in Section 4.\nAlgorithm 3 Ensemble DQN 1: Initialize K Q-networks Q(s, a; θk) with random\nweights θk0 for k ∈ {1, . . . ,K} 2: Initialize Experience Replay (ER) buffer B 3: Initialize exploration procedure Explore(·) 4: for i = 1, 2, . . . , N do 5: QEi−1(s, a) = 1 K �K k=1 Q(s, a; θ k i−1)\n6: yis,a = EB � r + γmaxa� Q E i−1(s �, a�))| s, a � 7: for k = 1, 2, . . . ,K do 8: θki ≈ argminθ EB � (yis,a −Q(s, a; θ))2 �\n9: end for 10: Explore(·), update B 11: end for output QEN (s, a) = 1 K �K k=1 Q(s, a; θ k i )"
  }, {
    "heading": "5.2. Ensemble DQN Variance",
    "text": "We consider two approaches for TAE variance reduction. The first one is the Averaged-DQN and the second we term Ensemble-DQN. We start with Ensemble-DQN which is a straightforward way to obtain a 1/K variance reduction,\nwith a computational effort of K-fold learning problems, compared to DQN. Ensemble-DQN (Algorithm 3) solves K DQN losses in parallel, then averages over the resulted Q-values estimates.\nFor Ensemble-DQN on the unidirectional MDP in Figure 3, we get for i > M :\nQEi (s0, a) =\nM−1�\nm=0\nγm 1\nK\nK�\nk=1\nZk,i−msm,a ,\nVar[QEi (s0, a)] = M−1�\nm=0\n1\nK γ2mσ2sm\n= 1\nK Var[QDQN(s0, a; θi)],\nwhere for k �= k�: Zk,is,a and Zk �,j\ns�,a� are two uncorrelated TAEs. The calculations of QE(s0, a) are detailed in Appendix B."
  }, {
    "heading": "5.3. Averaged DQN Variance",
    "text": "We continue with Averaged-DQN, and calculate the variance in state s0 for the unidirectional MDP in Figure 3. We get that for i > KM :\nVar[QAi (s0, a)] = M−1�\nm=0\nDK,mγ 2mσ2sm ,\nwhere DK,m = 1N �N−1\nn=0 |Un/K|2(m+1), with U = (Un) N−1 n=0 denoting a Discrete Fourier Transform (DFT) of a rectangle pulse, and |Un/K| ≤ 1. The calculations of QA(s0, a) and DK,m are more involved and are detailed in Appendix C.\nFurthermore, for K > 1,m > 0 we have that DK,m < 1/K (Appendix C) and therefore the following holds\nVar[QAi (s0, a)] < Var[Q E i (s0, a)]\n= 1\nK Var[QDQN(s0, a; θi)],\nmeaning that Averaged-DQN is theoretically more efficient in TAE variance reduction than Ensemble-DQN, and at least K times better than DQN. The intuition here is that Averaged-DQN averages over TAEs averages, which are the value estimates of the next states."
  }, {
    "heading": "6. Experiments",
    "text": "The experiments were designed to address the following questions:\n• How does the number K of averaged target networks affect the error in value estimates, and in particular the overestimation error.\n• How does the averaging affect the learned polices quality.\nTo that end, we ran Averaged-DQN and DQN on the ALE benchmark. Additionally, we ran Averaged-DQN, Ensemble-DQN, and DQN on a Gridworld toy problem where the optimal value function can be computed exactly."
  }, {
    "heading": "6.1. Arcade Learning Environment (ALE)",
    "text": "To evaluate Averaged-DQN, we adopt the typical RL methodology where agent performance is measured at the end of training. We refer the reader to Liang et al. (2016) for further discussion about DQN evaluation methods on the ALE benchmark. The hyperparameters used were taken from Mnih et al. (2015), and are presented for completeness in Appendix E. DQN code was taken from McGill University RLLAB, and is available online1 (together with Averaged-DQN implementation).\nWe have evaluated the Averaged-DQN algorithm on three Atari games from the Arcade Learning Environment (Bellemare et al., 2013). The game of BREAKOUT was selected due to its popularity and the relative ease of the DQN to reach a steady state policy. In contrast, the game of SEAQUEST was selected due to its relative complexity, and the significant improvement in performance obtained by other DQN variants (e.g., Schaul et al. (2015); Wang et al. (2015)). Finally, the game of ASTERIX was presented in Van Hasselt et al. (2015) as an example to overestimation in DQN that leads to divergence.\nAs can be seen in Figure 4 and in Table 1 for all three games, increasing the number of averaged networks in Averaged-DQN results in lower average values estimates, better-preforming policies, and less variability between the runs of independent learning trials. For the game of ASTERIX, we see similarly to Van Hasselt et al. (2015) that the divergence of DQN can be prevented by averaging.\nOverall, the results suggest that in practice Averaged-DQN reduces the TAE variance, which leads to smaller overestimation, stabilized learning curves and significantly improved performance."
  }, {
    "heading": "6.2. Gridworld",
    "text": "The Gridworld problem (Figure 5) is a common RL benchmark (e.g., Boyan & Moore (1995)). As opposed to the ALE, Gridworld has a smaller state space that allows the ER buffer to contain all possible state-action pairs. Additionally, it allows the optimal value function Q∗ to be ac-\n1McGill University RLLAB DQN Atari code: https: //bitbucket.org/rllabmcgill/atari_release. Averaged-DQN code https://bitbucket.org/ oronanschel/atari_release_averaged_dqn\ncurately computed.\nFor the experiments, we have used Averaged-DQN, and Ensemble-DQN with ER buffer containing all possible state-action pairs. The network architecture that was used composed of a small fully connected neural network with one hidden layer of 80 neurons. For minimization of the DQN loss, the ADAM optimizer (Kingma & Ba, 2014) was used on 100 mini-batches of 32 samples per target network parameters update in the first experiment, and 300 minibatches in the second."
  }, {
    "heading": "6.2.1. ENVIRONMENT SETUP",
    "text": "In this experiment on the problem of Gridworld (Figure 5), the state space contains pairs of points from a 2D discrete grid (S = {(x, y)}x,y∈1,...,20). The algorithm interacts with the environment through raw pixel features with a one-hot feature map φ(st) := (1{st = (x, y)})x,y∈1,...,20. There are four actions corresponding to steps in each compass direction, a reward of r = +1 in state st = (20, 20), and r = 0 otherwise. We consider the discounted return problem with a discount factor of γ = 0.9."
  }, {
    "heading": "6.2.2. OVERESTIMATION",
    "text": "In Figure 6 it can be seen that increasing the number K of averaged target networks leads to reduced overestimation eventually. Also, more averaged target networks seem to reduces the overshoot of the values, and leads to smoother and less inconsistent convergence."
  }, {
    "heading": "6.2.3. AVERAGED VERSUS ENSEMBLE DQN",
    "text": "In Figure 7, it can be seen that as was predicted by the analysis in Section 5, Ensemble-DQN is also inferior to Averaged-DQN regarding variance reduction, and as a con-\n200 400 600 800 1000 Iterations\n1.90\n1.92\n1.94\n1.96\n1.98\n2.00\n2.02\n2.04\n2.06\nA ve\nra ge\npr ed\nic te\nd va\nlu e\nA B C D\nGridworld\nEs[maxaQ ∗(s, a)] DQN (K=1) Averaged DQN, K=5 Averaged DQN, K=10 Averaged DQN, K=20\nFigure 6. Averaged-DQN average predicted value in Gridworld. Increasing the number K of averaged target networks leads to a faster convergence with less overestimation (positive-bias). The bold lines are averages over 40 independent learning trials, and the shaded area presents one standard deviation. In the figure, A,B,C,D present DQN, and Averaged-DQN for K=5,10,20 average overestimation.\nsequence far more overestimates the values. We note that Ensemble-DQN was not implemented for the ALE experiments due to its demanding computational effort, and the empirical evidence that was already obtained in this simple Gridworld domain."
  }, {
    "heading": "7. Discussion and Future Directions",
    "text": "In this work, we have presented the Averaged-DQN algorithm, an extension to DQN that stabilizes training, and improves performance by efficient TAE variance reduction. We have shown both in theory and in practice that the proposed scheme is superior in TAE variance reduction, compared to a straightforward but computationally demanding approach such as Ensemble-DQN (Algorithm 3). We have demonstrated in several games of Atari that increasing the number K of averaged target networks leads to better poli-\n0 200 400 600 800 1000 Iterations\n1.90\n1.92\n1.94\n1.96\n1.98\n2.00\n2.02\n2.04\nA ve\nra ge\npr ed\nic te\nd va\nlu e\nGridworld\nEs[maxaQ ∗(s, a)] DQN (K=1) Ensemble DQN, K=20 Averaged DQN, K=20\nFigure 7. Averaged-DQN and Ensemble-DQN predicted value in Gridworld. Averaging of past learned value is more beneficial than learning in parallel. The bold lines are averages over 20 independent learning trials, where the shaded area presents one standard deviation.\ncies while reducing overestimation. Averaged-DQN is a simple extension that can be easily integrated with other DQN variants such as Schaul et al. (2015); Van Hasselt et al. (2015); Wang et al. (2015); Bellemare et al. (2016); He et al. (2016). Indeed, it would be of interest to study the added value of averaging when combined with these variants. Also, since Averaged-DQN has variance reduction effect on the learning curve, a more systematic comparison between the different variants can be facilitated as discussed in (Liang et al., 2016).\nIn future work, we may dynamically learn when and how many networks to average for best results. One simple suggestion may be to correlate the number of networks with the state TD-error, similarly to Schaul et al. (2015). Finally, incorporating averaging techniques similar to AveragedDQN within on-policy methods such as SARSA and ActorCritic methods (Mnih et al., 2016) can further stabilize these algorithms."
  }],
  "references": [{
    "title": "Applied Optimal Control: Optimization Estimation and Control",
    "authors": ["Arthur E Bryson", "Yu Chi Ho"],
    "venue": "Hemisphere Publishing,",
    "year": 1975
  }, {
    "title": "Advantage updating",
    "authors": ["III Baird", "C. Leemon"],
    "venue": "Technical report, DTIC Document,",
    "year": 1993
  }, {
    "title": "The arcade learning environment: An evaluation platform for general agents",
    "authors": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2013
  }, {
    "title": "Unifying count-based exploration and intrinsic motivation",
    "authors": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"],
    "venue": "arXiv preprint arXiv:1606.01868,",
    "year": 2016
  }, {
    "title": "A Markovian decision process",
    "authors": ["Bellman", "Richard"],
    "venue": "Indiana Univ. Math. J.,",
    "year": 1957
  }, {
    "title": "Generalization in reinforcement learning: Safely approximating the value function",
    "authors": ["Boyan", "Justin", "Moore", "Andrew W"],
    "venue": "Advances in neural information processing systems,",
    "year": 1995
  }, {
    "title": "Learning rates for q-learning",
    "authors": ["Even-Dar", "Eyal", "Mansour", "Yishay"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "Learning to play in a day: Faster deep reinforcement learning by optimality tightening",
    "authors": ["He", "Frank S", "Yang Liu", "Alexander G. Schwing", "Peng", "Jian"],
    "venue": "arXiv preprint arXiv:1611.01606,",
    "year": 2016
  }, {
    "title": "On the convergence of stochastic iterative dynamic programming algorithms",
    "authors": ["Jaakkola", "Tommi", "Jordan", "Michael I", "Singh", "Satinder P"],
    "venue": "Neural Computation,",
    "year": 1994
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"],
    "venue": "arXiv preprint arXiv:",
    "year": 2014
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "venue": "In Advances in NIPS, pp",
    "year": 2012
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "State of the art control of Atari games using shallow reinforcement learning",
    "authors": ["Liang", "Yitao", "Machado", "Marlos C", "Talvitie", "Erik", "Bowling", "Michael"],
    "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,",
    "year": 2016
  }, {
    "title": "Reinforcement learning for robots using neural networks",
    "authors": ["Lin", "Long-Ji"],
    "venue": "Technical report, DTIC Document,",
    "year": 1993
  }, {
    "title": "Playing Atari with deep reinforcement learning",
    "authors": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"],
    "venue": "arXiv preprint arXiv:1312.5602,",
    "year": 2013
  }, {
    "title": "Asynchronous methods for deep reinforcement learning",
    "authors": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"],
    "venue": "arXiv preprint arXiv:1602.01783,",
    "year": 2016
  }, {
    "title": "Deep exploration via bootstrapped DQN",
    "authors": ["Osband", "Ian", "Blundell", "Charles", "Pritzel", "Alexander", "Van Roy", "Benjamin"],
    "venue": "arXiv preprint arXiv:1602.04621,",
    "year": 2016
  }, {
    "title": "Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method",
    "authors": ["Riedmiller", "Martin"],
    "venue": "In European Conference on Machine Learning,",
    "year": 2005
  }, {
    "title": "On-line Qlearning using connectionist systems",
    "authors": ["Rummery", "Gavin A", "Niranjan", "Mahesan"],
    "venue": "University of Cambridge, Department of Engineering,",
    "year": 1994
  }, {
    "title": "Prioritized experience replay",
    "authors": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"],
    "venue": "arXiv preprint arXiv:1511.05952,",
    "year": 2015
  }, {
    "title": "Reinforcement Learning: An Introduction",
    "authors": ["Sutton", "Richard S", "Barto", "Andrew G"],
    "year": 1998
  }, {
    "title": "Policy gradient methods for reinforcement learning with function approximation",
    "authors": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"],
    "venue": "In NIPS,",
    "year": 1999
  }, {
    "title": "exploration: A study of count-based exploration for deep reinforcement learning",
    "authors": ["Tang", "Haoran", "Rein Houthooft", "Davis Foote", "Adam Stooke", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"],
    "venue": "arXiv preprint arXiv:1611.04717,",
    "year": 2016
  }, {
    "title": "Temporal difference learning and tdgammon",
    "authors": ["Tesauro", "Gerald"],
    "venue": "Communications of the ACM,",
    "year": 1995
  }, {
    "title": "Asynchronous stochastic approximation and q-learning",
    "authors": ["Tsitsiklis", "John N"],
    "venue": "Machine Learning,",
    "year": 1994
  }, {
    "title": "An analysis of temporal-difference learning with function approximation",
    "authors": ["Tsitsiklis", "John N", "Van Roy", "Benjamin"],
    "venue": "IEEE transactions on automatic control,",
    "year": 1997
  }, {
    "title": "Double Q-learning",
    "authors": ["Van Hasselt", "Hado"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2010
  }, {
    "title": "Deep reinforcement learning with double Q-learning",
    "authors": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"],
    "venue": "arXiv preprint arXiv:",
    "year": 2015
  }, {
    "title": "Dueling network architectures for deep reinforcement learning",
    "authors": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"],
    "venue": "arXiv preprint arXiv:",
    "year": 2015
  }],
  "id": "SP:85c567d79aa66e9059396fa13dcb0dcc5f93aab7",
  "authors": [{
    "name": "Oron Anschel",
    "affiliations": []
  }, {
    "name": "Nir Baram",
    "affiliations": []
  }, {
    "name": "Nahum Shimkin",
    "affiliations": []
  }],
  "abstractText": "Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.",
  "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning"
}