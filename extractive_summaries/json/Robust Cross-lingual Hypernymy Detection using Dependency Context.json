{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 607–618 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Translation helps identify correspondences in bilingual texts, but other asymmetric semantic relationships can improve language understanding when translations are not exactly equivalent. One such relationship is cross-lingual hypernymy – identifying that écureuil (“squirrel” in French) is a kind of rodent, or ворона (“crow” in Russian) is a kind of bird. The ability to detect hypernyms across languages serves as a building block in a range of cross-lingual tasks, including Recognizing Textual Entailment (RTE) (Negri et al., 2012,\n∗ These authors contributed equally. 1https://github.com/yogarshi/\nbisparse-dep/\n2013), constructing multilingual taxonomies (Fu et al., 2014), event coreference across multilingual news sources (Vossen et al., 2015), and evaluating Machine Translation output (Padó et al., 2009).\nBuilding models that can robustly identify hypernymy across the spectrum of human languages is a challenging problem, that is further compounded in low resource settings. At first glance, translating words to English and then identifying hypernyms in a monolingual setting may appear to be a sufficient solution. However, this approach cannot capture many phenomena. For instance, the English words cook, leader and supervisor can all be hypernyms of the French word chef, as the French word does not have a exact translation in English covering its possible usages. However, translating chef to cook and then determining hypernymy monolingually precludes identifying leader or supervisor as a hypernyms of chef. Similarly, language-specific usage patterns can also influence hypernymy decisions. For instance, the French word chroniqueur translates to chronicler in English, but is more frequently used in French to refer to journalists (making journalist its hypernym).2\nThis motivates approaches that directly detect hypernymy in the cross-lingual setting by extending distributional methods for detecting monolingual hypernymy, as in our prior work (Vyas and Carpuat, 2016). State-of-the-art distributional approaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy require syntactic analysis (eg. dependency parsing), which may not available for many languages. Additionally, limited training resources make unsupervised methods more desirable than supervised hypernymy detection approaches (Roller and Erk,\n2All examples are from our dataset.\n607\n2016). Furthermore, monolingual distributional approaches cannot be applied directly to the crosslingual task, because the vector spaces of two languages need to be aligned using a cross-lingual resource (a bilingual dictionary, for instance).\nWe tackle these challenges by proposing BISPARSE-DEP - a family of robust, unsupervised approaches for identifying cross-lingual hypernymy. BISPARSE-DEP uses a cross-lingual word embedding model learned from a small bilingual dictionary and a variety of monolingual syntactic context extracted from a dependency parsed corpus. BISPARSE-DEP exhibits robust behavior along multiple dimensions. In the absence of a dependency treebank for a language, it can learn embeddings using a parser trained on related languages. When exposed to less monolingual data, or a lower quality bilingual dictionary, BISPARSEDEP degrades only marginally. In all these cases, it compares favorably with models that have been supplied with all necessary resources, showing promise for low-resource settings. We extensively evaluate BISPARSE-DEP on a new crowd-sourced cross-lingual dataset, with over 2900 hypernym pairs, spanning four languages from distinct families – French, Russian, Arabic and Chinese – and release the datasets for future evaluations."
  }, {
    "heading": "2 Related Work",
    "text": "Cross-lingual Distributional Semantics Cross-lingual word embeddings have been shown to encode semantics across languages in tasks such as word similarity (Faruqui and Dyer, 2014) and lexicon induction (Vulić and Moens, 2015). Our works stands apart in two aspects (1) In contrast to tasks involving similarity and synonymy (symmetric relations), the focus of our work is on detecting asymmetric relations across languages, using cross-lingual embeddings. (2) Unlike most previous work, we use dependency context instead of lexical context to induce crosslingual embeddings, which allows us to abstract away from language specific word order, and (as we show) improves hypernymy detection.\nMore closely related is our prior work (Vyas and Carpuat, 2016) where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previous approach by using dependency based embeddings (§6.1), and show that the improvements hold even when exposed to data scarce settings (§6.3).\nWe also do a more comprehensive evaluation on four languages paired with English, instead of just French.\nDependency Based Embeddings In monolingual settings, dependency based embeddings have been shown to outperform window based embeddings on many tasks (Bansal et al., 2014; Hill et al., 2014; Melamud et al., 2016). Roller and Erk (2016) showed that dependency embeddings can help in recovering Hearst patterns (Hearst, 1992) like “animals such as cats”, which are known to be indicative of hypernymy. Shwartz et al. (2017) demonstrated that dependency based embeddings are almost always superior to window based embeddings for identifying hypernyms in English. Our work uses dependency based embeddings in a cross-lingual setting, a less explored research direction. A key novelty of our work also lies in its use of syntactic transfer to derive dependency contexts. This scenario is more relevant in a cross-lingual setting, where treebanks might not be available for many languages.\n3 Our Approach – BISPARSE-DEP\nWe propose BISPARSE-DEP, a family of approaches that uses sparse, bilingual, dependency based word embeddings to identify cross-lingual hypernymy.\nFigure 1 shows an overview of the end-toend pipeline of BISPARSE-DEP. The two key components of this pipeline are: (1) Dependency based contexts (§3.1), which help us generalize across languages with minimal customization by abstracting away language-specific word order. We also discuss how to extract such contexts in the absence of a treebank in the language (§3.2) using a (weak) dependency parser trained on related languages. (2) Bilingual sparse coding (§3.3), which allows us to align dependency based word embeddings in a shared semantic space using a small bilingual dictionary. The resulting sparse bilingual embeddings can then be used with a unsupervised entailment scorer (§3.4) to predict hypernymy for cross-lingual word pairs."
  }, {
    "heading": "3.1 Dependency Based Context Extraction",
    "text": "The context of a word can be described in multiple ways using its syntactic neighborhood in a dependency graph. For instance, in Figure 2, we describe the context for a target word (traveler) in the following two ways:\n• FULL context (Padó and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014): Children and parent words, concatenated with the label and direction of the relation (eg. roamed#nsubj−1 and tired#amod are contexts for traveler). • JOINT context (Chersoni et al., 2016): Par-\nent concatenated with each of its siblings (eg. roamed#desert and roamed#seeking are contexts for traveler).\nThese two contexts exploit different amounts of syntactic information – JOINT does not require labeled parses, unlike FULL. The JOINT context combines parent and sibling information, while FULL keeps them as distinct contexts. Both encode directionality into the context, either through label direction or through sibling-parent relations.\nWe use word-context co-occurrences generated using these contexts in a distributional semantic model (DSM) in lieu of window based contexts to generate dependency based embeddings."
  }, {
    "heading": "3.2 Dependency Contexts without a Treebank",
    "text": "Using dependency contexts in multilingual settings may not always be possible, as dependency treebanks are not available for many languages. To circumvent this issue, we use related languages to train a weak dependency parser.\nWe train a delexicalized parser using treebanks of related languages, where the word form based\nfeatures are turned off, so that the parser is trained on purely non-lexical features (e.g. POS tags). The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delexicalized parsing (Zeman and Resnik, 2008; McDonald et al., 2011, inter alia) being one popular approach.3"
  }, {
    "heading": "3.3 Bilingual Sparse Coding",
    "text": "Given a dependency based co-occurrence matrix described in the previous section(s), we generate BISPARSE-DEP embeddings using the framework from our prior work (Vyas and Carpuat, 2016), which we henceforth call BISPARSE. BISPARSE generates sparse, bilingual word embeddings using a dictionary learning objective with a sparsity inducing l1 penalty. We give a brief overview of this approach, the full details of which can be found in our prior work.\nFor two languages with vocabularies ve and vf , and monolingual dependency embeddings Xe and Xf , BISPARSE solves the following objective:\nargmin Ae,De,Af ,Df\nve∑\ni=1\n1 2 ||AeiDeT −Xei||22 +λe||Aei||1\n+\nvf∑\nj=1\n1 2 ||Af jDfT −Xf j ||22 +λf ||Af j ||1\n+ ∑\ni,j\n1 2 λxSij ||Aei −Af j ||22 (1)\ns.t. Ak > 0 ‖Dki‖22≤ 1 k ∈ {e, f}\nwhere S is a translation matrix, and Ae and Af 3More sophisticated techniques for transferring syntactic knowledge have been proposed (Ammar et al., 2016; Rasooli and Collins, 2017), but we prioritize simplicity and show that a simple delexicalized parser is effective.\nare sparse matrices which are bilingual representations in a shared semantic space. The translation matrix S (of size ve×vf ) captures correspondences between the vocabularies (of size ve and vf ) of two languages. For instance, each row of S can be a one-hot vector that identifies the word in f that is most frequently aligned with the e word for that row in a large parallel corpus, thus building a one-to-many mapping between the two languages."
  }, {
    "heading": "3.4 Unsupervised Entailment Scorer",
    "text": "A variety of scorers can be used to quantify the directional relationship between two words, given feature representations of these words (Lin, 1998; Weeds and Weir, 2003; Lenci and Benotto, 2012). Once the BISPARSE-DEP embeddings are constructed, we use BalAPinc (Kotlerman et al., 2009) to score word pairs for hypernymy. BalAPinc is based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) and computes the geometric mean of 1) LIN (Lin, 1998), a symmetric score that captures similarity, and 2) APinc, an asymmetric score based on average precision."
  }, {
    "heading": "4 Crowd-Sourcing Annotations",
    "text": "There is no publicly available dataset to evaluate models of hypernymy detection across multiple languages. While ontologies like Open Multilingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) contain cross-lingual links, these resources are semiautomatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using CrowdFlower4. Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Arabic (Ar) and Chinese (Zh) - paired with English.\nTo begin the annotation process, we first pool candidate pairs using hypernymy edges across languages from OMW and BabelNet, along with translations from monolingual hypernymy datasets (Baroni and Lenci, 2011; Baroni et al., 2012; Kotlerman et al., 2010)."
  }, {
    "heading": "4.1 Annotation Setup",
    "text": "The annotation task requires annotators to be fluent in both English and the non-English language. To ensure only fluent speakers perform the task, for each language, we provide task instructions in the non-English language itself. Also, we restrict the task to annotators verified by CrowdFlower to have those language skills. Finally, annotators also\n4http://crowdflower.com\nneed to pass a quiz based on a small amount of gold standard data to gain access to the task.\nAnnotators choose between three options for each word pair (pf , qe), where pf is a non-English word and qe is a English word : “pf is a kind of qe”, “qe is a part of pf” and “none of the above”. Word pairs labeled with the first option are considered as positive examples while those labeled as “none of the above” are considered as negative.5 The second option was included to filter out meronymy examples that were part of the noisy pool. We leave it to the annotator to infer whether the relation holds between any senses of pf or qe, if either of them are polysemous.\nFor every candidate hypernym pair (pf , qe), we also ask annotators to judge its reversed and translated hyponym pair (qf , pe). For instance, if (citron, food) is a hypernym candidate, we also show annotators (aliments, lemon) which is a potential hyponym candidate (potential, because as mentioned in §1, translation need not preserve semantic relationships). The purpose of presenting the hyponym pair, (qf , pe), is two-fold. First, it emphasizes the directional nature of the task. Second, it identifies hyponym pairs, which we use as negative examples. The hyponym pairs are challenging since differentiating them from hypernyms truly requires detecting asymmetry.\nEach pair was judged by at least 5 annotators, and judgments with 80% agreement (at least 4 annotators agree) are considered for the final dataset. This is a stricter condition than certain monolingual hypernymy datasets - for instance, EVALution (Santus et al., 2015) - where agreement by 3 annotators is deemed sufficient. Inter-annotator agreement measured using Fleiss’ Kappa (Fleiss, 1971) was 58.1 (French), 53.7 (Russian), 53.2 (Arabic) and 55.8 (Chinese). This indicates moderate agreement, on par with agreement obtained on related fine-grained semantic tasks (Pavlick et al., 2015). We cannot compare with monolin-\n5We collected more negative pairs than positive, but sampled so as to keep a balanced dataset for ease of evaluation. We will release all annotated pairs along with the dataset.\ngual hypernymy annotator agreement as, to the best of our knowledge, such numbers are not available for existing test sets. Dataset statistics are shown in Table 1.\nWe observed that annotators were able to agree on pairs containing polysemous words where hypernymy holds for some sense. For instance, for the French-English pair (avocat, professional), the French word avocat can either mean lawyer or avocado, but the pair was annotated as a positive example. Hence, we leave it to the annotators to handle polysemy by choosing the most appropriate sense."
  }, {
    "heading": "4.2 Two Evaluation Test Sets",
    "text": "To verify if the crowdsourced hyponyms are challenging negative examples we create two evaluation sets. Both share the (crowdsourced) positive examples, but differ in their negatives:\n• HYPER-HYPO – negative examples are the crowdsourced hyponyms. • HYPER-COHYPO – negative examples are\ncohyponyms drawn from OMW.\nCohyponyms are words sharing a common hypernym. For instance, bière (“beer” in French) and vodka are cohyponyms since they share a common hypernym in alcool/alcohol. We choose cohyponyms for the second test set because: (a) They require differentiating between similarity (a symmetric relation) and hypernymy (an asymmetric relation). For instance, bière and vodka are highly similar yet, they do not have a hypernymy relationship. (b) Cohyponyms are a popular choice of negative examples in many entailment datasets (Baroni and Lenci, 2011)."
  }, {
    "heading": "5 Experimental Setup",
    "text": ""
  }, {
    "heading": "5.1 Data and Evaluation Setup",
    "text": "Training BISPARSE-DEP requires a dependency parsed monolingual corpus, and a translation matrix for jointly aligning the monolingual vectors. We compute the translation matrix using word alignments derived from parallel corpora (see corpus statistics in Table ??). While we use parallel corpora to generate the translation matrix to be comparable to baselines (§5.2), we can obtain the matrix from any bilingual dictionary.\nThe monolingual corpora are parsed using Yara Parser (Rasooli and Tetreault, 2015), trained on the corresponding treebank from the Universal Dependency Treebank (McDonald et al., 2013) (UDT-v1.4). Yara Parser was\nchosen as it is fast, and competitive with stateof-the-art parsers (Choi et al., 2015). The monolingual corpora was POS-tagged using TurboTagger (Martins et al., 2013). We induce dependency contexts for words by first thresholding the language vocabulary to the top 50,000 nouns, verbs and adjectives. A co-occurrence matrix is computed over this vocabulary using the context types in §3.1. Inducing Dependency Contexts The entries of the word-context co-occurrence matrix are reweighted using Positive Pointwise Mutual Information (Bullinaria and Levy, 2007). The resulting matrix is reduced to 1000 dimensions using SVD (Golub and Kahan, 1965).6 These vectors are used as Xe,Xf in the setup from §3.3 to generate 100 dimensional sparse bilingual vectors.\nEvaluation We use accuracy as our evaluation metric, as it is easy to interpret when the classes are balanced (Turney and Mohammad, 2015). Both evaluation datasets – HYPER-HYPO and HYPER-COHYPO – are split into 1:2 dev/test splits. BalAPinc has two tunable parameters - 1) a threshold that indicates the BalAPinc score above which all examples are labeled as positive, 2) the maximum number of features to consider for each word. We use the tuning set to tune the two parameters as well as the various hyper-parameters associated with the models."
  }, {
    "heading": "5.2 Contrastive Approaches",
    "text": "We compare our BISPARSE-DEP embeddings with the following approaches:\nMONO-DEP (Translation baseline) For word pair (pf , qe) in test data, we translate pf to English using the most common translation in the translation matrix. Hypernymy is then determined using sparse, dependency based embeddings in English.\nBISPARSE-LEX (Window context) Predecessor of the BISPARSE-DEP model from our previous work (Vyas and Carpuat, 2016). This model induces sparse, cross-lingual embeddings using window based context.\nBIVEC+ (Window context) Our extension of the BIVEC model of Luong et al. (2015). BIVEC generates dense, cross-lingual embeddings using window based context, by substituting aligned word pairs within a window in parallel sentences. By default, BIVEC only trains using parallel data,\n6Chosen based on preliminary experiments with {500,1000,2000,3000} dimensional vectors for En-Fr.\nand so we initialize it with monolingually trained window based embeddings to ensure fair comparison.\nCL-DEP (Dependency context) The model from Vulić (2017), which induces dense, dependency based cross-lingual embeddings by translating syntactic word-context pairs using the most common translation, and jointly training a word2vecf7 model for both languages. Vulić (2017) showed improvements for word similarity and bilingual lexicon induction. We report the first results using CL-DEP on this task.\n5.3 Evaluating Robustness of BISPARSE-DEP\nWe investigate how robust BISPARSE-DEP is when exposed to data scarce settings. Evaluating on a truly low resource language is complicated by the fact that obtaining an evaluation dataset for such a language is difficult. Therefore, we simulate such settings for the languages in our dataset in multiple ways.\nNo Treebank If a treebank is not available for a language, dependency contexts have to be induced using treebanks from other languages (§3.2), which can affect the quality of the dependencybased embeddings. To simulate this, we train a delexicalized parser for the languages in our dataset. We use treebanks from Slovenian, Ukrainian, Serbian, Polish, Bulgarian, Slovak and Czech (40k sentences) for training the Russian parser, and treebanks from English, Spanish, German, Portuguese, Swedish and Italian (66k sentences) for training the French parser. UDT does not (yet) have languages in the same family as Arabic or Chinese, so for the sake of completeness, we train Arabic and Chinese parsers on delexicalized treebanks of the language itself. Af-\n7bitbucket.org/yoavgo/word2vecf/\nter delexicalized training, the Labeled Attachment Score (LAS) on the UDT test set dropped by several points for all languages – from 76.6% to 60.0% for Russian, 83.7% to 71.1% for French, from 76.3% to 62.4% for Arabic and from 80.3% to 53.3% for Chinese. The monolingual corpora are then parsed with these weaker parsers, and coocurrences and dependency contexts are computed as before.\nSubsampling Monolingual Data To simulate low-resource behavior along another axis, we subsample the monolingual corpora used by BISPARSE-DEP to induce monolingual vectors, Xe,Xf . Specifically, we learn Xe and Xf using progressively smaller corpora.\nQuality of Bilingual Dictionary We study the impact of the quality of the bilingual dictionary used to create the translation matrix S. This experiment involves using increasingly smaller parallel corpora to induce the translation dictionary."
  }, {
    "heading": "6 Experiments",
    "text": "We aim to answer the following questions – (a) Are dependency based embeddings superior to window based embeddings for identifying crosslingual hypernymy? (§6.1) (b) Does directionality in the dependency context help cross-lingual hypernymy identification? (§6.2) (c) Are our models robust in data scarce settings (§6.3)? (d) Is the answer to (a) predicated on the choice of entailment scorer? (§6.4)?"
  }, {
    "heading": "6.1 Dependency v/s Window Contexts",
    "text": "We compare the performance of models described in §5.2 with the BISPARSE-DEP (FULL and JOINT) models. We evaluate the models on the two test splits described in §4.2 – HYPERHYPO and HYPER-COHYPO.\nHyper-Hypo Results Table 3a shows the results on HYPER-HYPO. First, the benefit of crosslingual modeling (as opposed to translation) is evident in that almost all models (except CL-DEP on French) outperform the translation baseline. Among dependency based models, BISPARSEDEP (FULL) and CL-DEP consistently outperform both window models, while BISPARSE-DEP (JOINT) outperforms them on all except Russian. BISPARSE-DEP (JOINT) is the best model overall for two languages (French and Chinese), CL-DEP for one (Arabic), with no statistically significant differences between BISPARSE-DEP (JOINT) and CL-DEP for Russian. This confirms that dependency context is more useful than window context for cross-lingual hypernymy detection.\nHyper-Cohypo Results The trends observed on HYPER-HYPO also hold on HYPER-COHYPO i.e. dependency based models continue to outperform window based models (Table 3b).\nOverall, BISPARSE-DEP (FULL) performs best in this setting, followed closely by BISPARSEDEP (JOINT). This suggests that the sibling information encoded in JOINT is useful to distinguish hypernyms from hyponyms (HYPER-HYPO results), while the dependency labels encoded in FULL help to distinguish hypernyms from cohyponyms. Also note that all models improve significantly on the HYPER-COHYPO set, suggesting that discriminating hypernyms from cohyponyms is easier than discriminating them from hyponyms.\nWhile the BISPARSE-DEP models were generally performing better than window models on both test sets, CL-DEP was not as consistent (e.g.,\nit was worse than the best window model on HYPER-COHYPO). As shown by Turney and Mohammad (2015), BalAPinc is designed for sparse embeddings and is likely to perform poorly with dense embeddings. This explains the relatively inconsistent performance of CL-DEP.\nBesides establishing the challenging nature of our crowd-sourced set, the experiments on HYPER-COHYPO and HYPER-HYPO also demonstrate the ability of the BISPARSE-DEP models to discriminate between different lexical semantic relations (viz. hypernymy and cohyponymy) in a cross-lingual setting. We will investigate this ability more carefully in future work."
  }, {
    "heading": "6.2 Ablating Directionality in Context",
    "text": "The context described by the FULL and JOINT BISPARSE models encodes directional information (§3.1) either in the form of label direction (FULL), or using sibling information (JOINT). Does such directionality in the context help to capture the asymmetric relationship inherent to hypernymy? To answer this, we evaluate a third BISPARSE-DEP model which uses UNLABELED dependency contexts. This is similar to the FULL context, except we do not concatenate the label of the relation to the context word (parent or children). For instance, for traveler in Fig. 2, contexts will be roamed and tired.\nExperiments on both HYPER-HYPO and HYPER-COHYPO (bottom row, Tables 3a and 3b) highlight that directional information is indeed essential - UNLABELED almost always performs worse than FULL and JOINT, and in many cases worse than even window based models.\n6.3 Evaluating Robustness of BISPARSE-DEP\nNo Treebank We run experiments (Table 4) for all languages with a version of BISPARSE-DEP that use the FULL context type for both English and the non-English (target) language, but the target language contexts are derived from a corpus parsed using a delexicalized parser (§5.3).\nThis model compares favorably on all language pairs against the best window based and the best dependency based model. In fact, it almost consistently outperforms the best window based model by several points, and is only slightly worse than the best dependency-based model.\nFurther analysis revealed that the good performance of the delexicalized model is due to the relative robustness of the delexicalized parser on frequent contexts in the co-occurrence matrix. Specifically, we found that in French and Russian, the most frequent contexts were derived from amod, nmod, nsubj and dobj edges.8 For instance, the nmod edge appears in 44% of Russian contexts and 33% of the French contexts. The delexicalized parser predicts both the label and direction of the nmod edge correctly with an F1 of 68.6 for Russian and 69.6 for French. In contrast, a fully-trained parser achieves a F1 of 76.7 for Russian and 76.8 for French for the same edge.\nSmall Monolingual Corpus In Figure 4, we use increasingly smaller monolingual corpora (10%, 20%, 40%, 60% and 80%) sampled at random to induce the monolingual vectors for BISPARSEDEP (FULL) model. Trends (Figure 4) indicate that BISSPARSE-DEP models that use only 40% of the original data remain competitive with the BISSPARSE-LEX model that has access to the full\n8Together they make up at least 70% of the contexts.\ndata. Robust performance with smaller monolingual corpora is helpful since large-enough monolingual corpora are not always easily available.\nQuality of Bilingual Dictionary Bilingual dictionaries derived from smaller amounts of parallel data are likely to be of lower quality than those derived from larger corpora. Hence, to analyze the impact of dictionary quality on BISPARSE-DEP (FULL), we use increasingly smaller parallel corpora to induce bilingual dictionaries used as the score matrix S (§3.3). We use the top 10%, 20%, 40%, 60% and 80% sentences from the parallel corpora. The trends in Figure 4 show that even with a lower quality dictionary, BISPARSE-DEP performs better than BISPARSE-LEX."
  }, {
    "heading": "6.4 Choice of Entailment Scorer",
    "text": "We change the entailment scorer from BalAPinc to SLQS (Santus et al., 2014) and redo experiments from §6.1 to see if the conclusions drawn depend\non the choice of the entailment scorer. SLQS is based on the distributional informativeness hypothesis, which states that hypernyms are less “informative” than hyponyms, because they occur in more general contexts. The informativeness Eu of a word u is defined to be the median entropy of its top N dimensions, Eu = medianNk=1H(ck), where H(ci) denotes the entropy of dimension ci. The SLQS score for a pair (u, v) is the relative difference in entropies,\nSLQS(u→ v) = 1− Eu Ev\nRecent work (Shwartz et al., 2017) has found SLQS to be more successful than other metrics in monolingual hypernymy detection.\nThe trends observed in these experiments are consistent with those in §6.1 – both BISPARSEDEP models still outperform window-based models. Also, the delexicalized version of BISPARSEDEP outperforms the window-based models, showing that the robust behavior demonstrated in §6.3 is also invariant across metrics.\nWe also found that using BalAPinc led to better results than SLQS . For both BISPARSE-DEP models, BalAPinc wins across the board for two languages (Russian and Chinese), and wins half the time for the other two languages compared to SLQS . We leave detailed comparison of these and other scores to future work."
  }, {
    "heading": "7 Conclusion",
    "text": "We introduced BISPARSE-DEP, a new distributional approach for identifying cross-lingual hypernymy, based on cross-lingual embeddings derived from dependency contexts. We showed that using BISPARSE-DEP is superior for the crosslingual hypernymy detection task, when compared to standard window based models and a translation baseline. Further analysis also showed that BISPARSE-DEP is robust to various low-resource settings. In principle, BISPARSE-DEP can be used for any language that has a bilingual dictionary with English and a “related” language with a treebank. We also introduced crowd-sourced crosslingual hypernymy datasets for four languages for future evaluations.\nOur approach has the potential to complement existing work on creating cross-lingual ontologies such as BabelNet and the Open Multilingual Wordnet, which are noisy because they are compiled semi-automatically, and have limited language coverage. In general, distributional approaches can help refine ontology construction for\nany language where sufficient resources are available.\nIt remains to be seen how our approach performs for other language pairs beyond simluated low-resource settings. We anticipate that replacing our delexicalized parser with more sophisticated transfer strategies (Rasooli and Collins, 2017; Aufrant et al., 2016) might be beneficial in such settings.While our delexicalized parsing based approach exhibits robustness, it can benefit from more sophisticated approaches for transfer parsing (Rasooli and Collins, 2017; Aufrant et al., 2016) to improve parser performance. We aim to explore these and other directions in the future."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank the members of the CLIP lab at the University of Maryland, members of the Cognitive Computation Group at the University of Pennsylvania, and the anonymous reviewers from EMNLP/CoNLL 2017 and NAACL 2018 for their constructive feedback. YV and MC were funded in part by research awards from Amazon, Google, and the Clare Boothe Luce Foundation. SU and DR were supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA)."
  }],
  "year": 2018,
  "references": [{
    "title": "Many languages, one parser",
    "authors": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."],
    "venue": "Transactions of the Association for Computational Linguistics 4:431–444.",
    "year": 2016
  }, {
    "title": "Zero-resource dependency parsing: Boosting delexicalized crosslingual transfer with linguistic knowledge",
    "authors": ["Lauriane Aufrant", "Guillaume Wisniewski", "François Yvon."],
    "venue": "Proc. of COLING. The COLING 2016 Organiz-",
    "year": 2016
  }, {
    "title": "Tailoring continuous word representations for dependency parsing",
    "authors": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, Baltimore, Maryland, pages 809–815.",
    "year": 2014
  }, {
    "title": "Entailment above the word level in distributional semantics",
    "authors": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, pages 23–32.",
    "year": 2012
  }, {
    "title": "Distributional memory: A general framework for corpus-based semantics",
    "authors": ["Marco Baroni", "Alessandro Lenci."],
    "venue": "Computational Linguistics 36(4):673–721.",
    "year": 2010
  }, {
    "title": "How we BLESSed distributional semantic evaluation",
    "authors": ["Marco Baroni", "Alessandro Lenci."],
    "venue": "Proceedings of the GEMS 2011 Workshop.",
    "year": 2011
  }, {
    "title": "Linking and extending an open multilingual wordnet",
    "authors": ["Francis Bond", "Ryan Foster."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, pages 1352–1362. http://www.aclweb.org/anthology/P13-1133.",
    "year": 2013
  }, {
    "title": "Extracting semantic representations from word cooccurrence statistics: A computational study",
    "authors": ["John A. Bullinaria", "Joseph P. Levy."],
    "venue": "Behavior Research Methods pages 510–526.",
    "year": 2007
  }, {
    "title": "How well can we predict hypernyms from word embeddings? a dataset-centric analysis",
    "authors": ["Vicente Ivan Sanchez Carmona", "Sebastian Riedel."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, Valencia, Spain, pages 401–407.",
    "year": 2017
  }, {
    "title": "Representing verbs with rich contexts: an evaluation on verb similarity",
    "authors": ["Emmanuele Chersoni", "Enrico Santus", "Alessandro Lenci", "Philippe Blache", "Chu-Ren Huang."],
    "venue": "Proc. of EMNLP. Association for Computational Linguistics, Austin, Texas, pages",
    "year": 2016
  }, {
    "title": "It depends: Dependency parser comparison using a web-based evaluation tool",
    "authors": ["Jinho D Choi", "Joel R Tetreault", "Amanda Stent."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, Beijing, China, pages 387–396.",
    "year": 2015
  }, {
    "title": "Improving vector space word representations using multilingual correlation",
    "authors": ["Manaal Faruqui", "Chris Dyer."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, Gothenburg, Sweden, pages 462–471. http://www.aclweb.org/anthology/E14-",
    "year": 2014
  }, {
    "title": "Measuring nominal scale agreement among many raters",
    "authors": ["Joseph L Fleiss."],
    "venue": "Psychological bulletin 76(5):378.",
    "year": 1971
  }, {
    "title": "Learning semantic hierarchies via word embeddings",
    "authors": ["Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, Baltimore, Maryland, pages 1199–1209.",
    "year": 2014
  }, {
    "title": "The Distributional Inclusion Hypotheses and Lexical Entailment",
    "authors": ["Maayan Geffet", "Ido Dagan."],
    "venue": "Proc. of ACL.",
    "year": 2005
  }, {
    "title": "A Field Guide to Forward-Backward Splitting with a FASTA Implementation",
    "authors": ["Tom Goldstein", "Christoph Studer", "Richard Baraniuk."],
    "venue": "arXiv eprint abs/1411.3.",
    "year": 2014
  }, {
    "title": "Calculating the singular values and pseudo-inverse of a matrix",
    "authors": ["Gene Golub", "William Kahan."],
    "venue": "Journal of the SIAM .",
    "year": 1965
  }, {
    "title": "Arabic gigaword 3rd edition, LDC2003T40",
    "authors": ["David Graff."],
    "venue": "LDC, University of Pennsylvania.",
    "year": 2007
  }, {
    "title": "Automatic acquisition of hyponyms from large text corpora",
    "authors": ["Marti A. Hearst."],
    "venue": "Proc. of COLING. Association for Computational Linguistics, pages 539–545. https://doi.org/10.3115/992133.992154.",
    "year": 1992
  }, {
    "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "arXiv preprint arXiv:1408.3456 .",
    "year": 2014
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Proc. of MT Summit.",
    "year": 2005
  }, {
    "title": "Directional distributional similarity for lexical expansion",
    "authors": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."],
    "venue": "Proc. of the ACL-IJCNLP.",
    "year": 2009
  }, {
    "title": "Directional distributional similarity for lexical inference",
    "authors": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."],
    "venue": "Natural Language Engineering .",
    "year": 2010
  }, {
    "title": "Identifying hypernyms in distributional semantic spaces",
    "authors": ["Alessandro Lenci", "Giulia Benotto."],
    "venue": "Proc. of the 6th Workshop on Semantic Evaluation.",
    "year": 2012
  }, {
    "title": "Dependencybased word embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, Baltimore, Maryland, pages 302–308. http://www.aclweb.org/anthology/P14-2050.",
    "year": 2014
  }, {
    "title": "Automatic retrieval and clustering of similar words",
    "authors": ["Dekang Lin."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, pages 768–774. https://doi.org/10.3115/980691.980696.",
    "year": 1998
  }, {
    "title": "Bilingual word representations with monolingual quality in mind",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proc. of the Workshop on Vector Space Modeling for NLP.",
    "year": 2015
  }, {
    "title": "Turning on the Turbo: Fast Third-Order NonProjective Turbo Parsers",
    "authors": ["Andre Martins", "Miguel Almeida", "Noah A. Smith."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, pages 617–622. http://www.aclweb.org/anthology/P13-2109.",
    "year": 2013
  }, {
    "title": "Universal dependency annotation for multilingual parsing",
    "authors": ["Zhang", "Oscar Täckström", "Claudia Bedini", "Núria Bertomeu Castelló", "Jungmee Lee."],
    "venue": "Proc. of ACL.",
    "year": 2013
  }, {
    "title": "Multi-source transfer of delexicalized dependency parsers",
    "authors": ["Ryan McDonald", "Slav Petrov", "Keith Hall."],
    "venue": "Proc. of EMNLP. Association for Computational Linguistics, pages 62–72. http://www.aclweb.org/anthology/D11-1006.",
    "year": 2011
  }, {
    "title": "Note on the sampling error of the difference between correlated proportions or percentages",
    "authors": ["Quinn McNemar."],
    "venue": "Psychometrika 12(2):153–157.",
    "year": 1947
  }, {
    "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings",
    "authors": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal."],
    "venue": "Proc. of NAACLHLT . Association for Computational Linguis-",
    "year": 2016
  }, {
    "title": "ISI Arabic-English Automatically Extracted Parallel Text LDC2007T08",
    "authors": ["Dragos Stefan Munteanu", "Daniel Marcu."],
    "venue": "LDC, University of Pennsylvania.",
    "year": 2007
  }, {
    "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
    "authors": ["Roberto Navigli", "Simone Paolo Ponzetto."],
    "venue": "Artificial Intelligence .",
    "year": 2012
  }, {
    "title": "Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization",
    "authors": ["Matteo Negri", "Alessandro Marchetti", "Yashar Mehdad", "Luisa Bentivogli", "Danilo Giampiccolo"],
    "year": 2012
  }, {
    "title": "Semeval-2013 Task 8: Cross-lingual Textual Entailment for Content Synchronization",
    "authors": ["Matteo Negri", "Alessandro Marchetti", "Yashar Mehdad", "Luisa Bentivogli", "Danilo Giampiccolo"],
    "year": 2013
  }, {
    "title": "Robust machine translation evaluation with entailment features",
    "authors": ["Sebastian Padó", "Michel Galley", "Dan Jurafsky", "Chris Manning."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint",
    "year": 2009
  }, {
    "title": "Dependency-based construction of semantic space models",
    "authors": ["Sebastian Padó", "Mirella Lapata."],
    "venue": "Computational Linguistics .",
    "year": 2007
  }, {
    "title": "Chinese Gigaword 5th Edition, LDC2011T13",
    "authors": ["Robert Parker."],
    "venue": "LDC, University of Pennsylvania.",
    "year": 2011
  }, {
    "title": "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
    "authors": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"],
    "venue": "Proceedings of ACL-IJCNLP",
    "year": 2015
  }, {
    "title": "Cross-lingual syntactic transfer with limited resources",
    "authors": ["Mohammad Sadegh Rasooli", "Michael Collins."],
    "venue": "Transactions of the Association for Computational Linguistics 5:279–293. https://transacl.org/ojs/index.php/tacl/article/view/922.",
    "year": 2017
  }, {
    "title": "Yara Parser: A Fast and Accurate Dependency Parser",
    "authors": ["Mohammad Sadegh Rasooli", "Joel R. Tetreault."],
    "venue": "CoRR abs/1503.06733.",
    "year": 2015
  }, {
    "title": "Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment",
    "authors": ["Stephen Roller", "Katrin Erk."],
    "venue": "Proc. of EMNLP. Association for Computational Linguistics, pages 2163–2172.",
    "year": 2016
  }, {
    "title": "Chasing hypernyms in vector spaces with entropy",
    "authors": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, Gothenburg, Sweden, pages 38–42.",
    "year": 2014
  }, {
    "title": "EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models",
    "authors": ["Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"],
    "venue": "In Proceedings of the 4th Workshop on Linked Data",
    "year": 2015
  }, {
    "title": "Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection",
    "authors": ["Vered Shwartz", "Enrico Santus", "Dominik Schlechtweg."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, Valencia, Spain, pages",
    "year": 2017
  }, {
    "title": "Parallel data, tools and interfaces in OPUS",
    "authors": ["Jörg Tiedemann."],
    "venue": "Proc. of LREC.",
    "year": 2012
  }, {
    "title": "Experiments with three approaches to recognizing lexical entailment",
    "authors": ["Peter D Turney", "Saif M Mohammad."],
    "venue": "Natural Language Engineering .",
    "year": 2015
  }, {
    "title": "Interoperability of cross-lingual and cross-document event detection",
    "authors": ["Piek Vossen", "Egoitz Laparra", "Itziar Aldabe", "German Rigau."],
    "venue": "Proc. of the 3rd Workshop on EVENTS at the NAACL-HLT .",
    "year": 2015
  }, {
    "title": "Cross-lingual syntactically informed distributed word representations",
    "authors": ["Ivan Vulić."],
    "venue": "Proc. of EACL. Association for Computational Linguistics, Valencia, Spain, pages 408–414. http://www.aclweb.org/anthology/E17-2065.",
    "year": 2017
  }, {
    "title": "Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction",
    "authors": ["Ivan Vulić", "Marie-Francine Moens."],
    "venue": "Proc. of ACL. Association for Computational Linguistics, Beijing, China, pages 719–725.",
    "year": 2015
  }, {
    "title": "Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment",
    "authors": ["Yogarshi Vyas", "Marine Carpuat."],
    "venue": "Association for Computational Linguistics, San Diego, California, pages 1187– 1197. http://www.aclweb.org/anthology/N16-1142.",
    "year": 2016
  }, {
    "title": "A general framework for distributional similarity",
    "authors": ["Julie Weeds", "David Weir."],
    "venue": "Proc. of EMNLP. http://www.aclweb.org/anthology/W031011.",
    "year": 2003
  }, {
    "title": "Cross-language parser adaptation between related languages",
    "authors": ["Daniel Zeman", "Philip Resnik."],
    "venue": "IJCNLP. http://www.aclweb.org/anthology/I08-3008.",
    "year": 2008
  }],
  "id": "SP:ea4c0f890be5f8d8a661b6aab65dc5fe9c29106d",
  "authors": [{
    "name": "Shyam Upadhyay",
    "affiliations": []
  }, {
    "name": "Yogarshi Vyas",
    "affiliations": []
  }, {
    "name": "Marine Carpuat",
    "affiliations": []
  }, {
    "name": "Dan Roth",
    "affiliations": []
  }],
  "abstractText": "Cross-lingual Hypernymy Detection involves determining if a word in one language (“fruit”) is a hypernym of a word in another language (“pomme” i.e. apple in French). The ability to detect hypernymy cross-lingually can aid in solving cross-lingual versions of tasks such as textual entailment and event coreference. We propose BISPARSE-DEP, a family of unsupervised approaches for cross-lingual hypernymy detection, which learns sparse, bilingual word embeddings based on dependency contexts. We show that BISPARSE-DEP can significantly improve performance on this task, compared to approaches based only on lexical context. Our approach is also robust, showing promise for low-resource settings: our dependency-based embeddings can be learned using a parser trained on related languages, with negligible loss in performance. We also crowd-source a challenging dataset for this task on four languages – Russian, French, Arabic, and Chinese. Our embeddings and datasets are publicly available.1",
  "title": "Robust Cross-lingual Hypernymy Detection using Dependency Context"
}