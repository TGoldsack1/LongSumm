{
  "sections": [{
    "text": "√ θ%̃/n) where θ\ndenotes freedom degree of the network parameters and %̃ = O(log( ∏l i=1 bi(ki − si + 1)/p) + log(bl+1)) encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi. To our best knowledge, this is the first generalization bound that only depends on O(log( ∏l+1 i=1 bi)), tighter than existing ones that all involve an exponential term likeO( ∏l+1 i=1 bi). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs."
  }, {
    "heading": "1. Introduction",
    "text": "Deep convolutional neural networks (CNNs) have been successfully applied to various fields, such as image classifica-\n1Department of Electrical & Computer Engineering (ECE), National University of Singapore, Singapore. Correspondence to: Pan Zhou <pzhou@u.nus.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ntion (Szegedy et al., 2015; He et al., 2016; Wang et al., 2017), speech recognition (Sainath et al., 2013; Abdel-Hamid et al., 2014), and classic games (Silver et al., 2016; Brown & Sandholm, 2017). However, theoretical analyses and understandings on deep CNNs still largely lag their practical applications. Recently, although many works establish theoretical understandings on deep feedforward neural networks (DNNs) from various aspects, e.g. (Neyshabur et al., 2015; Kawaguchi, 2016; Zhou & Feng, 2018; Tian, 2017; Lee et al., 2017), only a few (Sun et al., 2016; Kawaguchi et al., 2017; Du et al., 2017a;b) provide explanations on deep CNNs due to their more complex architectures and operations. Besides, these existing works all suffer certain discrepancy between their theories and practice. For example, the developed generalization error bound either exponentially grows along with the depth of a CNN model (Sun et al., 2016) or is data-dependent (Kawaguchi et al., 2017), and the convergence guarantees for optimization algorithms over CNNs are achieved by assuming an over-simplified CNN model consisting of only one non-overlapping convolutional layer (Du et al., 2017a;b).\nAs an attempt to explain the practical success of deep CNNs and mitigate the gap between theory and practice, this work aims to provide tighter data-independent generalization error bound and algorithmic optimization guarantees for the commonly used deep CNN models in practice. Specifically, we theoretically analyze the deep CNNs from following two aspects: (1) how their generalization performance varies with different network architecture choices and (2) why gradient descent based algorithms such as stochastic gradient descend (SGD) (Robbins & Monro, 1951), adam (Kingma & Ba, 2015) and RMSProp (Tieleman & Hinton, 2012), on minimizing empirical risk usually offer models with satisfactory performance. Moreover, we theoretically demonstrate the benefits of (stride) convolution and pooling operations, which are unique for CNNs, to the generalization performance, compared with feedforward networks.\nFormally, we consider a CNN model g(w;D) parameterized by w ∈ Rd, consisting of l convolutional layers and one subsequent fully connected layer. It maps the input D ∈ Rr0×c0 to an output vector v ∈ Rdl+1 . Its i-th convolutional layer takes Z(i−1) ∈ Rr̃i−1×c̃i−1×di−1 as input and outputs Z(i) ∈ Rr̃i×c̃i×di through spatial convolution, non-linear activation and pooling operations sequentially.\nHere r̃i × c̃i and di respectively denote resolution and the number of feature maps. Specifically, the computation with the i-th convolutional layer is described as\nX(i)(:, :, j) = Z(i−1) ~W j (i) ∈ R ri×ci ,∀j = 1, · · · , di, Y(i) = σ1(X(i)) ∈ Rri×ci×di , Z(i) = pool ( Y(i) ) ∈ Rr̃i×c̃i×di ,\nwhere X(i)(:, :, j) denotes the j-th feature map output by the i-th layer; W j(i) ∈ R\nki×ki×di−1 denotes the j-th convolutional kernel of size ki×ki and there are in total di kernels in the i-th layer; ~, pool (·) and σ1(·) respectively denote the convolutional operation with stride si, pooling operation with window size p × p without overlap and the sigmoid function. In particular, Z(0) = D is the input sample. The last layer is a fully connected one and formulated as\nu = W(l+1)z(l) ∈ Rdl+1 and v = σ2(u) ∈ Rdl+1 ,\nwhere z(l) ∈ Rr̃lc̃ldl is vectorization of the output Z(l) of the last convolutional layer; W(l+1) ∈ Rdl+1×r̃lc̃ldl denotes the connection weight matrix; σ2(·) is a softmax activation function (for classification) and dl+1 is the class number.\nIn practice, a deep CNN model is trained by minimizing the following empirical risk in terms of squared loss on the training data pairs (D(i),y(i)) drawn from an unknown distribution D,\nQ̃n(w) , 1\nn n∑ i=1 f(g(w;D(i)),y(i)), (1)\nwhere f(g(w;D),y) = 12‖v − y‖ 2 2 is the squared loss function. One can obtain the model parameter w̃ via SGD or its variants like adam and RMSProp. However, this empirical solution is different from the desired optimum w∗ that minimizes the population risk:\nQ(w) , E(D,y)∼D f(g(w;D),y).\nThis raises an important question: why CNNs trained by minimizing the empirical risk usually perform well in practice, considering the high model complexity and nonconvexity? This work answers this question by (1) establishing the generalization performance guarantee for CNNs and (2) expounding why the computed solution w̃ by gradient descent based algorithms for minimizing the empirical risk usually performs sufficiently well in practice.\nTo be specific, we present three new theoretical guarantees for CNNs. First, we prove that the generalization error of deep CNNs decreases at the rate of O( √ θ%̃/(2n)) where θ denotes parameter freedom degree1, and %̃ depends on the\n1We use the terminology of “parameter freedom degree” here for characterizing redundancy of parameters. For example, for a rank-r matrix A ∈ Rm1×m2 , the parameter freedom degree in this work is r(m1 +m2 + 1) instead of the commonly used one r(m1 +m2 − r).\nnetwork architecture parameters including the convolutional kernel size ki, stride si, pooling size p, channel number di and parameter magnitudes. It is worth mentioning that our generalization error bound is the first one that does not exponentially grow with depth.\nSecondly, we prove that for any gradient descent based optimization algorithm, e.g. SGD, RMSProp or adam, if its output w̃ is an approximate stationary point of the empirical risk Q̃n(w), w̃ is also an approximate stationary point of the population risk Q(w). This result is important as it explains why CNNs trained by minimizing the empirical risk have good generalization performance on test samples. We achieve such results by analyzing the convergence behavior of the empirical gradient to its population counterpart.\nFinally, we go further and quantitatively bound the distance between w̃ and w∗. We prove that when the samples are sufficient, a non-degenerate stationary point wn of Q̃n(w) uniquely corresponds to a non-degenerate stationary point w∗ of the population risk Q(w), with a distance shrinking at the rate of O((β/ζ) √ d%̃/n) where β also depends on the CNN architecture parameters (see Thereom 2). Here ζ accounts for the geometric topology of non-degenerate stationary points. Besides, the corresponding pair (wn,w∗) shares the same geometrical property—if one in (wn,w∗) is a local minimum or saddle point, so is the other one. All these results guarantee that for an arbitrary algorithm provided with sufficient samples, if the computed w̃ is close to the stationary point wn, then w̃ is also close to the optimum w∗ and they share the same geometrical property.\nTo sum up, we make multiple contributions to understand deep CNNs theoretically. To our best knowledge, this work presents the first theoretical guarantees on both generalization error bound without exponential growth over network depth and optimization performance for deep CNNs. We substantially extend prior works on CNNs (Du et al., 2017a;b) from the over-simplified single-layer models to the multi-layer ones, which is of more practical significance. Our generalization error bound is much tighter than the one derived from Rademacher complexity (Sun et al., 2016) and is also independent of data and specific training procedure, which distinguishes it from (Kawaguchi et al., 2017)."
  }, {
    "heading": "2. Related Works",
    "text": "Recently, many works have been devoted to explaining the remarkable success of deep neural networks. However, most works only focus on analyzing fully feedforward networks from aspects like generalization performance (Bartlett & Maass, 2003; Neyshabur et al., 2015), loss surface (Saxe et al., 2014; Dauphin et al., 2014; Choromanska et al., 2015; Kawaguchi, 2016; Nguyen & Hein, 2017; Zhou & Feng, 2018), optimization algorithm convergence (Tian, 2017; Li\n& Yuan, 2017) and expression ability (Eldan & Shamir, 2016; Soudry & Hoffer, 2017; Lee et al., 2017).\nThe literature targeting at analyzing CNNs is very limited, mainly because CNNs have much more complex architectures and computation. Among the few existing works, Du et al. (2017b) presented results for a simple and shallow CNN consisting of only one non-overlapping convolutional layer and ReLU activations, showing that gradient descent (GD) algorithms with weight normalization can converge to the global minimum. Similarly, Du et al. (2017a) also analyzed optimization performance of GD and SGD with nonGaussian inputs for CNNs with only one non-overlapping convolutional layer. By utilizing the kernel method, Zhang et al. (2017) transformed a CNN model into a single-layer convex model which has almost the same loss as the original CNN with high probability and proved that the transformed model has higher learning efficiency.\nRegarding generalization performance of CNNs, Sun et al. (2016) provided the Rademacher complexity of a deep CNN model which is then used to establish the generalization error bound. But the Rademacher complexity exponentially depends on the magnitude of total parameters per layer, leading to loose results. In contrast, the generalization error bound established in this work is much tighter, as discussed in details in Sec. 3. Kawaguchi et al. (2017) introduced two generalization error bounds of CNN, but both depend on a specific dataset as they involve the validation error or the intermediate outputs for the network model on a provided dataset. They also presented dataset-independent generalization error bound, but with a specific two-phase training procedure required, where the second phase need fix the states of ReLU activation functions. However, such two-phase training procedure is not used in practice.\nThere are also some works focusing on convergence behavior of nonconvex empirical risk of a single-layer model to the population risk. Our proof techniques essentially differ from theirs. For example, (Gonen & Shalev-Shwartz, 2017) proved that the empirical risk converges to the population risk for those nonconvex problems with no degenerated saddle points. Unfortunately, due to existence of degenerated saddle points in deep networks (Dauphin et al., 2014; Kawaguchi, 2016), their results are not applicable here. A very recent work (Mei et al., 2017) focuses on single-layer nonconvex problems but requires the gradient and Hessian of the empirical risk to be strong sub-Gaussian and subexponential respectively. Besides, it assumes a linearity property for the gradient which hardly holds in practice. Comparatively, our assumptions are much milder. We only assume magnitude of the parameters to be bounded. Furthermore, we also explore the parameter structures of optimized CNNs, i.e. the low-rankness property, and derive bounds matching empirical observations better. Finally, we analyze\nthe convergence rate of the empirical risk and generalization error of CNN which is absent in (Mei et al., 2017).\nOur work is also critically different from the recent work (Zhou & Feng, 2018) although we adopt a similar analysis road map with it. Zhou & Feng (2018) analyzed DNNs while this work focuses on CNNs with more complex architectures and operations which are more challenging and requires different analysis techniques. Besides, this work provides stronger results in the sense of several tighter bounds with much milder assumptions. (1) For nonlinear DNNs, Zhou & Feng (2018) assumed the input data to be Gaussian, while this work gets rid of such a restrictive assumption. (2) The generalization error bound O(r̂l+1 √ d/n) in (Zhou & Feng, 2018) exponentially depends on the upper magnitude bound r̂ of the weight matrix per layer and linearly depends on the total parameter number d, while ours is O( √ θ%̃/n) which only depends on the logarithm term\n%̃ = log( ∏l+1 i=1 bi) and the freedom degree θ of the network parameters, where bi and bl+1 respectively denote the upper magnitude bounds of each kernel per layer and the weight matrix of the fully connected layer. Note, the exponential term O(r̂l+1) in (Zhou & Feng, 2018) cannot be further improved due to their proof techniques. The results on empirical gradient and stationary point pairs in (Zhou & Feng, 2018) rely on O(r̂2(l+1)), while ours is O( ∏l+1 i=1 bi) which only depends on bi instead of bi2. (3) This work explores the parameter structures, i.e. the low-rankness property, and derives tighter bounds as the parameter freedom degree θ is usually smaller than the total parameter number d."
  }, {
    "heading": "3. Generalization Performance of Deep CNNs",
    "text": "In this section, we present the generalization error bound for deep CNNs and reveal effects of different architecture parameters on their generalization performance, providing some principles on model architecture design. We derive these results by establishing uniform convergence of the empirical risk Q̃n(w) to its population one Q(w).\nWe start with explaining our assumptions. Similar to (Xu & Mannor, 2012; Tian, 2017; Zhou & Feng, 2018), we assume that the parameters of the CNN have bounded magnitude. But we get rid of the Gaussian assumptions on the input data, meaning our assumption is milder than the ones in (Tian, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2018). Assumption 1. The magnitudes of the j-th kernel W j(i) ∈ Rki×ki×di−1 in the i-th convolutional layer and the weight matrix W(l+1) ∈ Rdl+1×r̃lc̃ldl in the the fully connected layer are respectively bounded as follows\n‖W j(i)‖F ≤bi (1≤j≤di; 1≤ i≤ l), ‖W(l+1)‖F ≤bl+1,\nwhere bi (1 ≤ i ≤ l) and bl+1 are positive constants.\nWe also assume that the entry value of the target output y\nalways falls in [0, 1], which can be achieved by scaling the entry value in y conveniently.\nIn this work, we also consider possible emerging structure of the learned parameters after training—the parameters usually present redundancy and low-rank structures (Lebedev et al., 2014; Jaderberg et al., 2014) due to high model complexity. So we incorporate low-rankness of the parameters or more concretely the parameter matrix consisting of kernels per layer, into our analysis. Denoting by vec(A) the vectorization of a matrix A, we have Assumption 2.\nAssumption 2. Assume the matrices W̃(i) andW(l+1) obey\nrank(W̃(i)) ≤ ai (1 ≤ i ≤ l) and rank(W(l+1)) ≤ al+1,\nwhere W̃(i) = [vec(W 1(i)), vec(W 2 (i)), · · · , vec(W di (i))] ∈ Rk2i di−1×di denotes the matrix consisting of all kernels in the i-th layer (1 ≤ i ≤ l).\nThe parameter low-rankness can also be defined on kernels individually by using the tensor rank (Tucker, 1966; Zhou et al., 2017; Zhou & Feng, 2017). Our proof techniques are extensible to this case and similar results can be expected."
  }, {
    "heading": "3.1. Generalization Error Bound for Deep CNNs",
    "text": "We now proceed to establish generalization error bound for deep CNNs. Let S = {(D(1),y(1)), · · · , (D(n),y(n))} denote the set of training samples i.i.d. drawn from D. When the optimal solution w̃ to problem (1) is computed by a deterministic algorithm, the generalization error is defined as g =\n∣∣Q̃n(w̃)−Q(w̃)∣∣. But in practice, a CNN model is usually optimized by randomized algorithms, e.g. SGD. So we adopt the following generalization error in expectation. Definition 1. (Generalization error) (Shalev-Shwartz et al., 2010) Assume a randomized algorithm A is employed for optimization over training samples S = {(D(1),y(1)), · · ·, (D(n),y(n))} ∼ D and w̃ = argminwQ̃n(w) is the empirical risk minimizer (ERM). Then if we have ES∼D\n∣∣EA(Q(w̃) − Q̃n(w̃))∣∣ ≤ k, the ERM is said to have generalization error with rate k under distribution D.\nWe bound the generalization error in expectation for deep CNNs by first establishing uniform convergence of the empirical risk to its corresponding population risk, as stated in Lemma 1 with proof in Sec. D.1 in supplement. Lemma 1. Assume that in CNNs, σ1 and σ2 are respectively the sigmoid and softmax activation functions and the loss function f(g(w;D),y) is squared loss. Under Assumptions 1 and 2, if n ≥ cf ′ l2(bl+1 +∑l\ni=1 dibi) 2 maxi √ rici/(θ%ε\n2) where cf ′ is a universal constant, then with probability at least 1− ε, we have\nsup w∈Ω\n∣∣∣Q̃n(w)−Q(w)∣∣∣ ≤ √ θ%+ log ( 4 ε ) 2n , (2)\nwhere the total freedom degree θ of the network is θ = al+1(dl+1 + r̃lc̃ldl + 1) + ∑l i=1 ai(ki 2di−1 + di + 1) and\n% = ∑l i=1log (√dibi(ki−si+1) 4p ) + log(bl+1) + log ( n 128p2 ) .\nTo our best knowledge, this generalization error rate is the first one that grows linearly (in contrast to exponentially) with depth l without needing any special training procedure. This can be observed from the fact that our result only depends on O( ∑l i=1 log(bi)), rather than an expo-\nnential factor O( ∏l+1 i=1 bi) which appears in some existing works, e.g. the uniform convergence of the empirical risk in deep CNNs (Sun et al., 2016) and fully feedforward networks (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018). This faster convergence rate is achieved by adopting similar analysis technique in (Mei et al., 2017; Zhou & Feng, 2018) but we derive tighter bounds on the related parameters featuring distributions of the empirical risk and its gradient, with milder assumptions. For instance, both (Zhou & Feng, 2018) and this work show that the empirical risk follows a sub-Gaussian distribution. But Zhou & Feng (2018) used Gaussian concentration inequality and thus need Lipschitz constant of loss which exponentially depends on the depth. In contrast, we use -net to decouple the dependence between input D and parameter w and then adopt Hoeffding’s inequality, only requiring the constant magnitude bound of loss and geting rid of exponential term.\nBased on Lemma 1, we derive generalization error of deep CNNs in Theorem 1 with proof in Sec. D.2 in supplement.\nTheorem 1. Assume that in CNNs, σ1 and σ2 are respectively the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then with probability at least 1− ε, the generalization error of a deep CNN model is bounded as\nES∼D ∣∣∣EA (Q(w̃)− Q̃n(w̃)) ∣∣∣ ≤\n√ θ%+ log ( 4 ε ) 2n ,\nwhere θ and % are given in Lemma 1.\nBy inspecting Theorem 1, one can find that the generalization error diminishes at the rate of O (1/ √ n) (up to a log factor). Besides, Theorem 1 explicitly reveals the roles of network parameters in determining model generalization performance. Such transparent results form stark contrast to the works (Sun et al., 2016) and (Kawaguchi et al., 2017) (see more comparison in Sec. 3.2). Notice, our technique also applies to other third-order differentiable activation functions, e.g. tanh, and other losses, e.g. cross entropy, with only slight difference in the results.\nFirst, the freedom degree θ of network parameters, which depends on the network size and the redundancy in parameters, plays an important role in the generalization error bound. More specifically, to obtain smaller generalization\nerror, more samples are needed to train a deep CNN model having larger freedom degree θ. As aforementioned, although the results in Theorem 1 are obtained under the low-rankness condition defined on the parameter matrix consisting of kernels per layer, they are easily extended to the (tensor) low-rankness defined on each kernel individually. The low-rankness captures common parameter redundancy in practice. For instance, (Lebedev et al., 2014; Jaderberg et al., 2014) showed that parameter redundancy exists in a trained network model and can be squeezed by low-rank tensor decomposition. The classic residual function (He et al., 2016; Zagoruyko & Komodakis, 2016) with three-layer bottleneck architecture (1× 1, 3× 3 and 1× 1 convs) has rank 1 in generalized block term decomposition (Chen et al., 2017; Cohen & Shashua, 2016). Similarly, inception networks (Szegedy et al., 2017) explicitly decomposes a convolutional kernel of large size into two separate convolutional kernels of smaller size (e.g. a 7× 7 kernel is replaced by two multiplying kernels of size 7×1 and 1×7). Employing these low-rank approximation techniques helps reduce the freedom degree and provides smaller generalization error. Notice, the low-rankness assumption only affects the freedom degree θ. Without this assumption, θ will be replaced by the total parameter number of the network.\nFrom the factor %, one can observe that the kernel size ki and its stride si determine the generalization error but in opposite ways. Larger kernel size ki leads to larger generalization error, while larger stride si provides smaller one. This is because both larger kernel and smaller stride increase the model complexity, since larger kernel means more trainable parameters and smaller stride implies larger size of feature maps in the subsequent layer. Also, the pooling operation in the first l convolutional layers helps reduce the generalization error, as reflected by the factor 1/p in %.\nFurthermore, the number of feature maps (i.e. channels) di appearing in the θ and % also affects the generalization error. A wider network with larger di requires more samples for training such that it can generalize well. This is because (1) a larger di indicates more trainable parameters, which usually increases the freedom degree θ, and (2) a larger di also requires larger kernels W j(i) with more channel-wise dimensions since there are more channels to convolve, leading to a larger magnitude bound bi for the kernel W j (i). Therefore, as suggested by Theorem 1, a thin network is more preferable than a fat network. Such an observation is consistent with other analysis works on the network expression ability (Eldan & Shamir, 2016; Lu et al., 2017) and the architecture-engineering practice, such as (He et al., 2016; Szegedy et al., 2015). By comparing contributions of the architecture and parameter magnitude to the generalization performance, we find that the generalization error usually depends on the network architecture parameters linearly or more heavily, and also on parameter magnitudes but\nwith a logarithm term log bi. This implies the architecture plays a more important role than the parameter magnitudes. Therefore, for achieving better generalization performance in practice, architecture engineering is indeed essential.\nFinally, by observing the factor %, we find that imposing certain regularization, such as ‖w‖22, on the trainable parameters is useful. The effectiveness of such a regularization will be more significant when imposing on the weight matrix of the fully connected layer due to its large size. Such a regularization technique, in deep learning literature, is well known as “weight decay”. This conclusion is consistent with other analysis works on the deep forward networks, such as (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018)."
  }, {
    "heading": "3.2. Discussions",
    "text": "Sun et al. (2016) also analyzed generalization error bound in deep CNNs but employing different techniques. They proved that the Rademacher complexity Rm(F) of a deep CNN model with sigmoid activation functions is O(̃bx(2pb̃)l+1 √ log(r0c0)/ √ n) where F denotes the function hypothesis that maps the input data D to v ∈ Rdl+1 by the analyzed CNN model. Here b̃x denotes the upper bound of the absolute entry values in the input datum D, i.e. b̃x ≥ |Di,j | (∀i, j), and b̃ obeys b̃ ≥ max{maxi ∑di j=1 ‖W j (i)‖1, ‖W(l+1)‖1}. Sun et al. (2016) showed that with probability at least 1 − ε, the difference between the empirical margin error errγe (g) (g ∈ F) and the population margin error errγp(g) can be bounded as\nerrγp(g) ≤ inf γ>0\n[ errγe (g) +\n8dl+1(2dl+1 − 1) γ Rm(F)\n+\n√ log log2(2/γ)\nn +\n√ log(2/ε)\nn\n] , (3)\nwhere γ controls the error margin since it obeys γ ≥ vy − maxk 6=y vk and y denotes the label of v. However, the bound in Eqn. (3) is practically loose, sinceRm(F) involves the exponential factor (2b̃)l+1 which is usually very large. In this case,Rm(F) is extremely large. By comparison, the bound provided in our Theorem 1 only depends on∑l+1 i=1 log(bi) which avoids the exponential growth along with the depth l, giving a much tighter and more practically meaningful bound. The generalization error bounds in (Kawaguchi et al., 2017) either depend on a specific dataset or rely on restrictive and rarely used training procedure, while our Theorem 1 is independent of any specific dataset or training procedure, rendering itself more general. More importantly, the results in Theorem 1 make the roles of network parameters transparent, which could benefit understanding and architecture design of CNNs."
  }, {
    "heading": "4. Optimization Guarantees for Deep CNNs",
    "text": "Although deep CNNs are highly non-convex, gradient descent based algorithms usually perform quite well on optimizing the models in practice. After characterizing the roles of different network parameters for the generalization performance, here we present optimization guarantees for gradient descent based algorithms in training CNNs.\nSpecifically, in practice one usually adopts SGD or its variants, such as adam and RMSProp, to optimize the CNN models. Such algorithms usually terminate when the gradient magnitude decreases to a low level and the training hardly proceeds. This implies that the algorithms in fact compute an -approximate stationary point w̃ for the loss function Q̃n(w), i.e. ‖∇wQ̃n(w̃)‖22 ≤ . Here we explore such a problem: by computing an -stationary point w̃ of the empirical risk Q̃n(w), can we also expect w̃ to be sufficiently good for generalization, or in other words expect that it is also an approximate stationary point for the population risk Q(w)? To answer this question, first we analyze the relationship between the empirical gradient∇wQ̃n(w) and its population counterpart∇wQ(w). Founded on this, we further establish convergence of the empirical gradient of the computed solution to its corresponding population gradient. Finally, we present the bounded distance between the computed solution w̃ and the optimum w∗.\nTo our best knowledge, this work is the first one that analyzes the optimization behavior of gradient descent based algorithms for training multi-layer CNN models with the commonly used convolutional and pooling operations."
  }, {
    "heading": "4.1. Convergence Guarantees on Gradients",
    "text": "Here we present guarantees on convergence of the empirical gradient to the population one in Theorem 2. As aforementioned, such results imply good generalization performance of the computed solution w̃ to the empirical risk Q̃n(w).\nTheorem 2. Assume that in CNNs, σ1 and σ2 respectively are the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then the empirical gradient uniformly converges to the population gradient in Euclidean norm. More specifically, there exist universal constants cg′ and cg such that if n ≥ cg′ l2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4\nd40b1 8(d log(6)+θ%)ε2 maxi(rici)\n, then\nsup w∈Ω ∥∥∥∇wQ̃n(w)−∇wQ(w)∥∥∥ 2 ≤cgβ\n√ 2d+θ%+log ( 4 ε ) 2n\nholds with probability at least 1 − ε, where % is provided in Lemma 1. Here β and d are defined as β =[ rlcldl 8p2 + ∑l i=1 bl+1 2di−1 8p2bi2di ri−1ci−1 ∏l j=i djbj 2(kj−sj+1)2 16p2 ]1/2 and d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, respectively.\nIts proof is given in Sec. D.3 in supplement. From Theorem 2, the empirical gradient converges to the population one at the rate of O(1/ √ n) (up to a log factor). In Sec. 3.1, we have discussed the roles of the network architecture parameters in %. Here we further analyze the effects of the network parameters on the optimization behavior through the factor β. The roles of the kernel size ki, the stride si, the pooling size p and the channel number di in β are consistent with those in Theorem 1. The extra factor rici advocates not building such CNN networks with extremely large feature map sizes. The total number of parameters d is involved here instead of the degree of freedom because the gradient ∇wQ̃n(w) may not have low-rank structures.\nBased on Theorem 2, we can further conclude that if the computed solution w̃ is an -approximate stationary point of the empirical risk, then it is also a 4 -approximate stationary point of the population risk. We state this result in Corollary 1 with proof in Sec. D.4 in supplement. Corollary 1. Suppose assumptions in Theorem 2 hold and we have n ≥ (d% + log(4/ε))β2/ . Then if the solution w̃ computed by minimizing the empirical risk obeys ‖∇Q̃n(w̃)‖22 ≤ , we have ‖∇Q(w̃)‖22 ≤ 4 with probability at least 1− ε.\nCorollary 1 shows that by using full gradient descent algorithms to minimize the empirical risk, the computed approximate stationary point w̃ is also close to the desired stationary point w∗ of the population risk. This guarantee is also applicable to other stochastic gradient descent based algorithms, like SGD, adam and RMSProp, by applying recent results on obtaining -approximate stationary point for nonconvex problems (Ghadimi & Lan, 2013; Tieleman & Hinton, 2012; Kingma & Ba, 2015). Accordingly, the computed solution w̃ has guaranteed generalization performance on new data. It partially explains the success of gradient descent based optimization algorithms for CNNs."
  }, {
    "heading": "4.2. Convergence of Stationary Points",
    "text": "Here we go further and directly characterize the distance between stationary points in the empirical risk Q̃n(w) and its population counterpart Q(w). Compared with the results for the risk and gradient, the results on stationary points give more direct performance guarantees for CNNs. Here we only analyze the non-degenerate stationary points including local minimum/maximum and non-degenerate saddle points, as they are geometrically isolated and thus are unique in local regions. We first introduce some necessary definitions. Definition 2. (Non-degenerate stationary points and saddle points) (Gromoll & Meyer, 1969) A stationary point w is said to be a non-degenerate stationary point of Q(w) if\ninf i ∣∣λi (∇2Q(w))∣∣ ≥ ζ, where λi ( ∇2Q(w) ) is the i-th eigenvalue of the Hessian\n∇2Q(w) and ζ is a positive constant. A stationary point is said to be a saddle point if the smallest eigenvalue of its Hessian∇2Q(w) has a negative value.\nSuppose Q(w) has m non-degenerate stationary points which are denoted as {w(1), w(2), · · · ,w(m)}. We have following results on the geometry of these stationary points in Theorem 3. The proof is given in Sec. D.5 in supplement.\nTheorem 3. Assume in CNNs, σ1 and σ2 are respectively the sigmoid and softmax activation functions and the loss f(g(w;D),y) is squared loss. Suppose Assumptions 1 and 2 hold. Then if n ≥ ch max ( d+θ% ζ2 , l2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4\nd40b1 8d%ε2 maxi(rici)\n) where ch is a constant,\nfor k ∈ {1, · · · ,m}, there exists a non-degenerate stationary point w(k)n of Q̃n(w) which uniquely corresponds to the non-degenerate stationary point w(k) of Q(w) with probability at least 1− ε. Moreover, with same probability the distance between w(k)n and w(k) is bounded as\n‖w(k)n −w(k)‖2≤ 2cgβ\nζ\n√ 2d+θ%+log ( 4 ε ) 2n , (1≤k≤m),\nwhere % and β are given in Lemma 1 and Theorem 2, respectively.\nTheorem 3 shows that there exists exact one-to-one correspondence between the non-degenerate stationary points of the empirical risk Q̃n(w) and the popular risk Q(w) for CNNs, if the sample size n is sufficiently large. Moreover, the non-degenerate stationary point w(k)n of Q̃n(w) is very close to its corresponding non-degenerate stationary point w(k) of Q(w). More importantly, their distance shrinks at the rate of O (1/ √ n) (up to a log factor). The network parameters have similar influence on the distance bounds as explained in the above subsection. Compared with gradient convergence rate in Theorem 2, the convergence rate of corresponding stationary point pairs in Theorem 3 has an extra factor 1/ζ that accounts for the geometric topology of non-degenerate stationary points, similar to other works like stochastic optimization analysis (Duchi & Ruan, 2016).\nFor degenerate stationary points to which the corresponding Hessian matrix has zero eigenvalues, one cannot expect to establish unique correspondence for stationary points in empirical and population risks, since they are not isolated anymore and may reside in flat regions. But Theorem 2 guarantees that the gradients of Q̃n(w) and Q(w) at these points are close. This implies a degenerate stationary point of Q(w) will also give a near-zero gradient for Q̃n(w), indicating it is also a stationary point for Q̃n(w).\nDu et al. (2017a;b) showed that for a simple and shallow CNN consisting of only one non-overlapping convolutional layer, (stochastic) gradient descent algorithms with weight\nnormalization can converge to the global minimum. In contrast to their simplified models, we analyze complex multi-layer CNNs with the commonly used convolutional and pooling operations. Besides, we provide results on both gradient and the distance between the computed solution and desired stationary points, which are applicable to arbitrary gradient descent based algorithms.\nNext, based on Theorem 3, we derive that the corresponding pair (w(k)n ,w(k)) in the empirical and population risks shares the same geometrical property stated in Corollary 2. Corollary 2. Suppose the assumptions in Theorem 3 hold. If any one in the pair (w(k)n ,w(k)) in Theorem 3 is a local minimum or saddle point, so is the other one.\nSee the proof of Corollary 2 in Sec. D.6 in supplement. Corollary 2 tells us that the corresponding pair, w(k)n and w(k), has the same geometric property. Namely, if either one in the pair is a local minimum or saddle point, so is the other one. This result is important for optimization. If the computed solution w̃ by minimizing the empirical risk Q̃n(w) is a local minimum, then it is also a local minimum of the population risk Q(w). Thus it partially explains why the computed solution w̃ can generalize well on new data. This property also benefits designing new optimization algorithms. For example, Saxe et al. (2014) and Kawaguchi (2016) pointed out that degenerate stationary points indeed exist for deep linear neural networks and Dauphin et al. (2014) empirically validated that saddle points are usually surrounded by high error plateaus in deep forward neural networks. So it is necessary to avoid the saddle points and find the local minimum of population risk. From Theorem 3, it is clear that one only needs to find the local minimum of empirical risk by using escaping saddle points algorithms, e.g. (Ge et al., 2015; Jin et al., 2017; Agarwal et al., 2017)."
  }, {
    "heading": "5. Comparison on DNNs And CNNs",
    "text": "Here we compare deep feedforward neural networks (DNNs) with deep CNNs from their generalization error and optimization guarantees to theoretically explain why CNNs are more preferable than DNNs, to some extent.\nBy assuming the input to be standard Gaussian N (0, τ2), Zhou & Feng (2018) proved that if n ≥ 18r2/(dτ2ε2 log(l+ 1)), with probability 1 − ε, the generalization error of an (l + 1)-layer DNN model with sigmoid activation functions is bounded by n:\nn , cnτ √\n(1 + crl) max i di\n√ d log(n(l+1)) + log(4/ε)\nn ,\nwhere cn is a universal constant; di denotes the width of the i-th layer; d is the total parameter number of the network; cr = max(r̂2/16, ( r̂2/16 )l ) where r̂ upper bounds Frobenius norm of the weight matrix in each\nlayer. Recall that the generalization bound of CNN provided in this work is O( √ θ%̃/(2n)), where %̃ =∑l\ni=1 log (√ dibi(ki − si + 1)/(4p) ) + log(bl+1).\nBy observing the above two generalization bounds, one can see when the layer number is fixed, CNN usually has smaller generalization error than DNN because: (1) CNN usually has much fewer parameters, i.e. smaller d, than DNN due to parameter sharing mechanism of convolutions. (2) The generalization error of CNN has a smaller factor than DNN in the network parameter magnitudes. Generalization error bound of CNN depends on a logarithm termO(log ∏l+1 i=1 bi) of the magnitude bi of each kernel/weight matrix, while the bound for DNN depends on O(r̂l+1). Since the kernel size is much smaller than that of the weight matrix in the fully connected layer by unfolding the convolutional layer, the upper magnitude bound bi (i = 1, · · · , l) of each kernel is usually much smaller than r̂. (3) The pooling operation and the stride in convolutional operation in CNN also benefit its generalization performance. This is because the factor %̃ involves O(2(l + 1) log(1/p)) and (ki − si + 1) which also reduce the generalization error. Notice, by applying our analysis technique, it might be possible to remove the exponential term in DNN. But as mentioned above, the unique operations, e.g. convolution, pooling and striding, still benefit CNN, making it generalize better than DNN.\nBecause of the above factors, the empirical gradient in CNN converges to its population counterpart faster, as well as the paired non-degenerate stationary points for empirical risk and population risk. All these results guarantee that for an arbitrary gradient descent based algorithm, it is faster to compute an approximate stationary point or a local minimum in population risk of CNN compared with DNN."
  }, {
    "heading": "6. Proof of Roadmap",
    "text": "Here we briefly introduce the proof roadmap. Due to space limitation, all the proofs of our theoretical results are deferred to the supplement. Firstly, our analysis relies on bounding the gradient magnitude and the spectral norm of Hessian of the loss f(g(w;D),y). By considering multilayer architecture of CNN, we establish recursive relation of their magnitudes in the k and k + 1 layers (Lemmas 9 ∼ 14 in supplement) and get their overall magnitude upper bound.\nFor the uniform convergence supw∈Ω |Q̃n(w) − Q(w)| in Lemma 1, we resort to bound three distances: A1 = supw∈Ω |Q̃n(w) − Q̃n(wkw)|, A2 = supwkw∈Θ |Q̃n(wkw)−EQ̃n(wkw)| and A3 = supw∈Ω |EQ̃n(wkw) −EQ(w)|, where wkw belongs to the -net Θ of parameter domain Ω. Using Markov inequality and Lipschitz property of loss, we can bound A1 and A3. To bound A2, we prove the empirical risk is sub-Gaussian. Considering the element wkw in -net Θ is independent of input D, we use Hoeffd-\ning’s inequality to prove empirical risk at point wkw to be sub-Gaussian for any wkw in Θ. By this decoupling of -net, our bound on A2 depends on the constant magnitude of loss and gets rid of exponential term. Combining these bounds together, we obtain the uniform convergence of empirical risk and can derive the generalization bound.\nWe use a similar decomposition and decoupling strategy mentioned above to bound gradient uniform convergence supw∈Ω‖∇wQ̃n(w)−∇wQ(w)‖2 in Theorem 2. But here we need to bound gradient and spectral norm of Hessian.\nTo prove correspondence and bounded distance of stationary points, we define a set G= {w∈Ω : ||∇Q̃n(w)||≤ and infi |λi(∇2Q̃n(w))| ≥ ζ} where λi is the i-th eigenvalue of∇2Q̃n(w). Then G is decomposed into countable components each of which has one or zero non-degenerate stationary point. Next we prove the uniform convergence between empirical and population Hessian by using a similar strategy as above. Combining uniform convergence of gradient and Hessian and the results in differential topology (Lemmas 4 & 5 in supplement), we obtain that for each component of G, if there is a unique non-degenerate stationary point in Q(w), then Q̃n(w) also has a unique one, and vice versa. This gives the one-to-one correspondence relation. Finally, the uniform convergence of gradient and Hessian can bound the distance between the corresponding points."
  }, {
    "heading": "7. Conclusion",
    "text": "In this work, we theoretically analyzed why deep CNNs can achieve remarkable success, from its generalization performance and the optimization guarantees of (stochastic) gradient descent based algorithms. We proved that the generalization error of deep CNNs can be bounded by a factor which depends on the network parameters. Moreover, we analyzed the relationship between the computed solution by minimizing the empirical risk and the optimum solutions in population risk from their gradient and their Euclidean distance. All these results show that with sufficient training samples, the generalization performance of deep CNN models can be guaranteed. Besides, these results also reveal that the kernel size ki, the stride si, the pooling size p, the channel number di and the freedom degree θ of the network parameters are critical to the generalization performance of deep CNNs. We also showed that the weight parameter magnitude is also important. These suggestions on network designs accord with the widely used network architectures."
  }, {
    "heading": "Acknowledgements",
    "text": "Jiashi Feng was partially supported by NUS startup R-263000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112."
  }],
  "year": 2018,
  "references": [{
    "title": "Convolutional neural networks for speech recognition",
    "authors": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"],
    "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
    "year": 2014
  }, {
    "title": "Finding approximate local minima faster than gradient descent",
    "authors": ["N. Agarwal", "Z. Allen-Zhu", "B. Bullins", "E. Hazan", "T. Ma"],
    "venue": "In STOC,",
    "year": 2017
  }, {
    "title": "Vapnik-chervonenkis dimension of neural nets",
    "authors": ["P. Bartlett", "W. Maass"],
    "venue": "The handbook of brain theory and neural networks,",
    "year": 2003
  }, {
    "title": "Safe and nested subgame solving for imperfect-information games",
    "authors": ["N. Brown", "T. Sandholm"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Sharing residual units through collective tensor factorization in deep neural networks",
    "authors": ["Y. Chen", "X. Jin", "B. Kang", "J. Feng", "S. Yan"],
    "venue": "arXiv preprint arXiv:1703.02180,",
    "year": 2017
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Arous", "Y. LeCun"],
    "venue": "In AISTATS, pp",
    "year": 2015
  }, {
    "title": "Convolutional rectifier networks as generalized tensor decompositions",
    "authors": ["N. Cohen", "A. Shashua"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "authors": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "When is a convolutional filter easy to learn",
    "authors": ["S. Du", "J. Lee", "Y. Tian"],
    "venue": "arXiv preprint arXiv:1709.06129,",
    "year": 2017
  }, {
    "title": "Gradient descent learns one-hidden-layer CNN: Don’t be afraid of spurious local minima",
    "authors": ["S. Du", "J. Lee", "Y. Tian", "B. Poczos", "A. Singh"],
    "venue": "arXiv preprint arXiv:1712.00779,",
    "year": 2017
  }, {
    "title": "Local asymptotics for some stochastic optimization problems: Optimality, constraint identification, and dual averaging",
    "authors": ["J. Duchi", "F. Ruan"],
    "venue": "arXiv preprint arXiv:1612.05612,",
    "year": 2016
  }, {
    "title": "The power of depth for feedforward neural networks",
    "authors": ["R. Eldan", "O. Shamir"],
    "venue": "In COLT,",
    "year": 2016
  }, {
    "title": "Escaping from saddle points—online stochastic gradient for tensor decomposition",
    "authors": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
    "authors": ["S. Ghadimi", "G. Lan"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Fast rates for empirical risk minimization of strict saddle problems",
    "authors": ["A. Gonen", "S. Shalev-Shwartz"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "On differentiable functions with isolated critical",
    "authors": ["D. Gromoll", "W. Meyer"],
    "venue": "points. Topology,",
    "year": 1969
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In CVPR,",
    "year": 2016
  }, {
    "title": "Speeding up convolutional neural networks with low rank expansions",
    "authors": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1405.3866,",
    "year": 2014
  }, {
    "title": "How to escape saddle points efficiently",
    "authors": ["C. Jin", "R. Ge", "P. Netrapalli", "S. Kakade", "M. Jordan"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Generalization in deep learning",
    "authors": ["K. Kawaguchi", "L. Kaelbling", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1710.05468,",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "In ICLR, pp",
    "year": 2015
  }, {
    "title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition",
    "authors": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"],
    "venue": "arXiv preprint arXiv:1412.6553,",
    "year": 2014
  }, {
    "title": "On the ability of neural nets to express distributions",
    "authors": ["H. Lee", "R. Ge", "A. Risteski", "T. Ma", "S. Arora"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "Convergence analysis of two-layer neural networks with ReLU activation",
    "authors": ["Y. Li", "Y. Yuan"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "The expressive power of neural networks: A view from the width",
    "authors": ["Z. Lu", "H. Pu", "F. Wang", "Z. Hu", "L. Wang"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "The landscape of empirical risk for non-convex losses",
    "authors": ["S. Mei", "Y. Bai", "A. Montanari"],
    "venue": "Annals of Statistics,",
    "year": 2017
  }, {
    "title": "Norm-based capacity control in neural networks",
    "authors": ["B. Neyshabur", "R. Tomioka", "N. Srebro"],
    "venue": "In COLT,",
    "year": 2015
  }, {
    "title": "The loss surface of deep and wide neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "A stochastic approximation method",
    "authors": ["H. Robbins", "S. Monro"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1951
  }, {
    "title": "Deep convolutional neural networks for LVCSR",
    "authors": ["T. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"],
    "venue": "In ICASSP,",
    "year": 2013
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["A. Saxe", "J. McClelland", "S. Ganguli"],
    "year": 2014
  }, {
    "title": "Learnability, stability and uniform convergence",
    "authors": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"],
    "venue": "JMLR, 11:2635–2670,",
    "year": 2010
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["D. Silver", "A. Huang", "C. Maddison", "A. Guez", "L. Sifre", "Driessche", "G. Van Den", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"],
    "venue": "search. Nature,",
    "year": 2016
  }, {
    "title": "Exponentially vanishing suboptimal local minima in multilayer neural networks",
    "authors": ["D. Soudry", "E. Hoffer"],
    "venue": "arXiv preprint arXiv:1702.05777,",
    "year": 2017
  }, {
    "title": "On the depth of deep neural networks: A theoretical view",
    "authors": ["S. Sun", "W. Chen", "L. Wang", "X. Liu", "T. Liu"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Going deeper with convolutions",
    "authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"],
    "venue": "In CVPR, pp",
    "year": 2015
  }, {
    "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
    "authors": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke", "A. Alemi"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis",
    "authors": ["Y. Tian"],
    "year": 2017
  }, {
    "title": "Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning",
    "authors": ["T. Tieleman", "G. Hinton"],
    "year": 2012
  }, {
    "title": "Some mathematical notes on three-mode factor analysis",
    "authors": ["L. Tucker"],
    "venue": "Psychometrika, 31(3):279–311,",
    "year": 1966
  }, {
    "title": "Beyond filters: Compact feature map for portable deep model",
    "authors": ["Y. Wang", "C. Xu", "D. Tao"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Robustness and generalization",
    "authors": ["H. Xu", "S. Mannor"],
    "venue": "Machine Learning,",
    "year": 2012
  }, {
    "title": "Convexified convolutional neural networks",
    "authors": ["Y. Zhang", "P. Liang", "M. Wainwright"],
    "year": 2017
  }, {
    "title": "Outlier-robust tensor PCA",
    "authors": ["P. Zhou", "J. Feng"],
    "venue": "In CVPR,",
    "year": 2017
  }, {
    "title": "Empirical risk landscape analysis for understanding deep neural networks",
    "authors": ["P. Zhou", "J. Feng"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "Tensor factorization for low-rank tensor completion",
    "authors": ["P. Zhou", "C. Lu", "Z. Lin", "C. Zhang"],
    "venue": "IEEE TIP,",
    "year": 2017
  }],
  "id": "SP:e616398242dce18e70a15f66ca39d7410c52ffe8",
  "authors": [{
    "name": "Pan Zhou",
    "affiliations": []
  }, {
    "name": "Jiashi Feng",
    "affiliations": []
  }],
  "abstractText": "This work aims to provide understandings on the remarkable success of deep convolutional neural networks (CNNs) by theoretically analyzing their generalization performance and establishing optimization guarantees for gradient descent based training algorithms. Specifically, for a CNN model consisting of l convolutional layers and one fully connected layer, we prove that its generalization error is bounded by O( √ θ%̃/n) where θ denotes freedom degree of the network parameters and %̃ = O(log( ∏l i=1 bi(ki − si + 1)/p) + log(bl+1)) encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi. To our best knowledge, this is the first generalization bound that only depends on O(log( ∏l+1 i=1 bi)), tighter than existing ones that all involve an exponential term likeO( ∏l+1 i=1 bi). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs.",
  "title": "Understanding Generalization and Optimization Performance of Deep CNNs"
}