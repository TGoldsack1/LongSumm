{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Determinantal Point Processes (DPPs) are a family of probabilistic models that arose from the study of quantum mechanics (Macchi, 1975) and random matrix theory (Dyson, 1962). Following the seminal work of Kulesza and Taskar (Kulesza & Taskar, 2012), discrete DPPs have found numerous applications in machine learning, including in document and timeline summarization (Lin & Bilmes, 2012; Yao et al., 2016), image search (Kulesza & Taskar, 2011; Affandi et al., 2014) and segmentation (Lee et al., 2016), audio signal processing (Xu & Ou, 2016), bioinformatics (Batmanghelich et al., 2014) and neuroscience (Snoek et al., 2013). What makes such models appealing is that they exhibit repulsive behavior and lend themselves naturally to tasks where returning a diverse set of objects is important.\nOne way to define a DPP is through an N × N symmetric positive semidefinite matrix K, called a kernel, whose\n1Department of Mathematics, MIT, USA. Correspondence to: John Urschel <urschel@mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\neigenvalues are bounded in the range [0, 1]. Then the DPP associated with K, which we denote by DPP(K), is the distribution on Y ⊆ [N ] = {1, . . . , N} that satisfies, for any J ⊆ [N ],\nP[J ⊆ Y ] = det(KJ),\nwhere KJ is the principal submatrix of K indexed by the set J . The graph induced by K is the graph G = ([N ], E) on the vertex set [N ] that connects i, j ∈ [N ] if and only if Ki,j 6= 0.\nThere are fast algorithms for sampling (or approximately sampling) from DPP(K) (Deshpande & Rademacher, 2010; Rebeschini & Karbasi, 2015; Li et al., 2016b;a). Marginalizing the distribution on a subset I ⊆ [N ] and conditioning on the event that J ⊆ Y both result in new DPPs and closed form expressions for their kernels are known (Borodin & Rains, 2005).\nThere has been much less work on the problem of learning the parameters of a DPP. A variety of heuristics have been proposed, including Expectation-Maximization (Gillenwater et al., 2014), MCMC (Affandi et al., 2014), and fixed point algorithms (Mariet & Sra, 2015). All of these attempt to solve a nonconvex optimization problem, and no guarantees on their statistical performance are known. Recently, Brunel et al. (Brunel et al., 2017) studied the rate of estimation achieved by the maximum likelihood estimator, but the question of efficient computation remains open.\nApart from positive results on sampling, marginalization and conditioning, most provable results about DPPs are actually negative. It is conjectured that the maximum likelihood estimator is NP-hard to compute (Kulesza, 2012). Actually, approximating the mode of size k of a DPP to within a ck factor is known to be NP-hard for some c > 1 (Çivril & Magdon-Ismail, 2009; Summa et al., 2015). The best known algorithms currently obtain a ek+o(k) approximation factor (Nikolov, 2015; Nikolov & Singh, 2016).\nIn this work, we bypass the difficulties associated with maximum likelihood estimation by using the method of moments to achieve optimal sample complexity. We introduce a parameter `, which we call the cycle sparsity of the graph induced by the kernelK, which governs the number of moments that need to be considered and, thus, the sample complexity. Moreover, we use a refined version of Horton’s al-\ngorithm (Horton, 1987; Amaldi et al., 2010) to implement the method of moments in polynomial time.\nThe cycle sparsity of a graph is the smallest integer ` so that the cycles of length at most ` yield a basis for the cycle space of the graph. Even though there are in general exponentially many cycles in a graph to consider, Horton’s algorithm constructs a minimum weight cycle basis and, in doing so, also reveals the parameter ` together with a collection of at most ` induced cycles spanning the cycle space.\nWe use such cycles in order to construct our method of moments estimator. For any fixed ` ≥ 2, our overall algorithm has sample complexity\nn = O ((C α )2` + logN α2ε2 ) for some constant C > 1 and runs in time polynomial in n and N , and learns the parameters up to an additive ε with high probability. The (C/α)2` term corresponds to the number of samples needed to recover the signs of the entries in K. We complement this result with a minimax lower bound (Theorem 2) to show that this sample complexity is in fact near optimal. In particular, we show that there is an infinite family of graphs with cycle sparsity ` (namely length ` cycles) on which any algorithm requires at least (C ′α)−2` samples to recover the signs of the entries of K for some constant C ′ > 1. Finally, we show experimental results that confirm many quantitative aspects of our theoretical predictions. Together, our upper bounds, lower bounds, and experiments present a nuanced understanding of which DPPs can be learned provably and efficiently."
  }, {
    "heading": "2. Estimation of the Kernel",
    "text": ""
  }, {
    "heading": "2.1. Model and definitions",
    "text": "Let Y1, . . . , Yn be n independent copies of Y ∼ DPP(K), for some unknown kernel K such that 0 K IN . It is well known that K is identified by DPP(K) only up to flips of the signs of its rows and columns: If K ′ is another symmetric matrix with 0 K ′ IN , then DPP(K ′)=DPP(K) if and only if K ′ = DKD for some D ∈ DN , where DN denotes the class of all N × N diagonal matrices with only 1 and −1 on their diagonals (Kulesza, 2012, Theorem 4.1). We call such a transform a DN -similarity of K.\nIn view of this equivalence class, we define the following pseudo-distance between kernels K and K ′:\nρ(K,K ′) = inf D∈DN |DKD −K ′|∞ ,\nwhere for any matrix K, |K|∞ = maxi,j∈[N ] |Ki,j | denotes the entrywise sup-norm.\nFor any S ⊂ [N ], we write ∆S = det(KS), where KS denotes the |S| × |S| submatrix of K obtained by keeping rows and colums with indices in S. Note that for 1 ≤ i 6= j ≤ N , we have the following relations:\nKi,i = P[i ∈ Y ], ∆{i,j} = P[{i, j} ⊆ Y ],\nand |Ki,j | = √ Ki,iKj,j −∆{i,j}. Therefore, the principal minors of size one and two of K determine K up to the sign of its off-diagonal entries. In fact, for any K, there exists an ` depending only on the graph GK induced by K, such that K can be recovered up to a DN -similarity with only the knowledge of its principal minors of size at most `. We will show that this ` is exactly the cycle sparsity."
  }, {
    "heading": "2.2. DPPs and graphs",
    "text": "In this section, we review some of the interplay between graphs and DPPs that plays a key role in the definition of our estimator.\nWe begin by recalling some standard graph theoretic notions. Let G = ([N ], E), |E| = m. A cycle C of G is any connected subgraph in which each vertex has even degree. Each cycle C is associated with an incidence vector x ∈ GF (2)m such that xe = 1 if e is an edge in C and xe = 0 otherwise. The cycle space C of G is the subspace of GF (2)m spanned by the incidence vectors of the cycles in G. The dimension νG of the cycle space is called cyclomatic number, and it is well known that νG := m−N + κ(G), where κ(G) denotes the number of connected components of G.\nRecall that a simple cycle is a graph where every vertex has either degree two or zero and the set of vertices with degree two form a connected set. A cycle basis is a basis of C ⊂ GF (2)m such that every element is a simple cycle. It is well known that every cycle space has a cycle basis of induced cycles.\nDefinition 1. The cycle sparsity of a graph G is the minimal ` for which G admits a cycle basis of induced cycles of length at most `, with the convention that ` = 2 whenever the cycle space is empty. A corresponding cycle basis is called a shortest maximal cycle basis.\nA shortest maximal cycle basis of the cycle space was also studied for other reasons by (Chickering et al., 1995). We defer a discussion of computing such a basis to Section 4.\nFor any subset S ⊆ [N ], denote by GK(S) = (S,E(S)) the subgraph of GK induced by S. A matching of GK(S) is a subset M ⊆ E(S) such that any two distinct edges in M are not adjacent in G(S). The set of vertices incident to some edge in M is denoted by V (M). We denote by M(S) the collection of all matchings of GK(S). Then, if GK(S) is an induced cycle, we can write the principal\nminor ∆S = det(KS) as follows: ∆S = ∑\nM∈M(S)\n(−1)|M | ∏\n{i,j}∈M\nK2i,j ∏\ni 6∈V (M)\nKi,i\n+ 2× (−1)|S|+1 ∏\n{i,j}∈E(S)\nKi,j . (1)\nOthers have considered the relationship between the principal minors of K and recovery of DPP(K). There has been work regarding the symmetric principal minor assignment problem, namely the problem of computing a matrix given an oracle that gives any principal minor in constant time (Rising et al., 2015).\nIn our setting, we can approximate the principal minors of K by empirical averages. However the accuracy of our estimator deteriorates with the size of the principal minor, and we must therefore estimate the smallest possible principal minors in order to achieve optimal sample complexity. Here, we prove a new result, namely, that the smallest ` such that all the principal minors of K are uniquely determined by those of size at most ` is exactly the cycle sparsity of the graph induced by K.\nProposition 1. Let K ∈ RN×N be a symmetric matrix, GK be the graph induced byK, and ` ≥ 3 be some integer. The kernelK is completely determined up toDN -similarity by its principal minors of size at most ` if and only if the cycle sparsity of GK is at most `.\nProof. Note first that all the principal minors of K completely determine K up to a DN -similarity (Rising et al., 2015, Theorem 3.14). Moreover, recall that principal minors of degree at most 2 determine the diagonal entries of K as well as the magnitude of its off-diagonal entries. In particular, given these principal minors, one only needs to recover the signs of the off-diagonal entries of K. Let the sign of cycle C in K be the product of the signs of the entries of K corresponding to the edges of C.\nSuppose GK has cycle sparsity ` and let (C1, . . . , Cν) be a cycle basis of GK where each Ci, i ∈ [ν] is an induced cycle of length at most `. By (1), the sign of any Ci, i ∈ [ν] is completely determined by the principal minor ∆S , where S is the set of vertices of Ci and is such that |S| ≤ `. Moreover, for i ∈ [ν], let xi ∈ GF (2)m denote the incidence vector of Ci. By definition, the incidence vector x of any cycle C is given by ∑ i∈I xi for some subset I ⊂ [ν]. The sign of C is then given by the product of the signs of Ci, i ∈ I and thus by corresponding principal minors. In particular, the signs of all cycles are determined by the principal minors ∆S with |S| ≤ `. In turn, by Theorem 3.12 in (Rising et al., 2015), the signs of all cycles completely determine K, up to a DN -similarity.\nNext, suppose the cycle sparsity of GK is at least ` + 1,\nand let C` be the subspace of GF (2)m spanned by the induced cycles of length at most ` in GK . Let x1, . . . , xν be a basis of C` made of the incidence column vectors of induced cycles of length at most ` inGK and form the matrix A ∈ GF (2)m×ν by concatenating the xi’s. Since C` does not span the cycle space of GK , ν < νGK ≤ m. Hence, the rank of A is less than m, so the null space of A> is non trivial. Let x̄ be the incidence column vector of an induced cycle C̄ that is not in C`, and let h ∈ GL(2)m with A>h = 0, h 6= 0 and x̄>h = 1. These three conditions are compatible because C̄ /∈ C`. We are now in a position to define an alternate kernel K ′ as follows: Let K ′i,i = Ki,i and |K ′i,j | = |Ki,j | for all i, j ∈ [N ]. We define the signs of the off-diagonal entries of K ′ as follows: For all edges e = {i, j}, i 6= j, sgn(K ′e) = sgn(Ke) if he = 0 and sgn(K ′e) = − sgn(Ke) otherwise. We now check that K and K ′ have the same principal minors of size at most ` but differ on a principal minor of size larger than `. To that end, let x be the incidence vector of a cycle C in C` so that x = Aw for some w ∈ GL(2)ν . Thus the sign of C in K is given by∏\ne : xe=1\nKe = (−1)x >h ∏ e : xe=1 K ′e\n= (−1)w >A>h ∏ e : xe=1 K ′e = ∏ e : xe=1 K ′e\nbecause A>h = 0. Therefore, the sign of any C ∈ C` is the same in K and K ′. Now, let S ⊆ [N ] with |S| ≤ `, and let G = GKS = GK′S be the graph corresponding to KS (or, equivalently, to K ′S). For any induced cycle C in G, C is also an induced cycle in GK and its length is at most `. Hence, C ∈ C` and the sign of C is the same in K and K ′. By (Rising et al., 2015, Theorem 3.12), det(KS) = det(K ′S). Next observe that the sign of C̄ in K is given by∏\ne : x̄e=1\nKe = (−1)x̄ >h ∏ e : x̄e=1 K ′e = − ∏ e : xe=1 K ′e.\nNote also that since C̄ is an induced cycle of GK = GK′ , the above quantity is nonzero. Let S̄ be the set of vertices in C̄. By (1) and the above display, we have det(KS̄) 6= det(K ′\nS̄ ). Together with (Rising et al., 2015, Theorem\n3.14), it yields K 6= DK ′D for all D ∈ DN ."
  }, {
    "heading": "2.3. Definition of the Estimator",
    "text": "Our procedure is based on the previous result and can be summarized as follows. We first estimate the diagonal entries (i.e., the principal minors of size one) of K by the method of moments. By the same method, we estimate the principal minors of size two ofK, and we deduce estimates of the magnitude of the off-diagonal entries. To use these estimates to deduce an estimate Ĝ of GK , we make the following assumption on the kernel K.\nAssumption 1. Fix α ∈ (0, 1). For all 1 ≤ i < j ≤ N , either Ki,j = 0, or |Ki,j | ≥ α.\nFinally, we find a shortest maximal cycle basis of Ĝ, and we set the signs of our non-zero off-diagonal entry estimates by using estimators of the principal minors induced by the elements of the basis, again obtained by the method of moments.\nFor S ⊆ [N ], set ∆̂S = 1\nn n∑ p=1 1S⊆Yp , and define\nK̂i,i = ∆̂{i} and B̂i,j = K̂i,iK̂j,j − ∆̂{i,j},\nwhere K̂i,i and B̂i,j are our estimators of Ki,i and K2i,j , respectively.\nDefine Ĝ = ([N ], Ê), where, for i 6= j, {i, j} ∈ Ê if and only if B̂i,j ≥ 12α\n2. The graph Ĝ is our estimator of GK . Let {Ĉ1, ..., ĈνĜ} be a shortest maximal cycle basis of the cycle space of Ĝ. Let Ŝi ⊆ [N ] be the subset of vertices of Ĉi, for 1 ≤ i ≤ νĜ. We define\nĤi = ∆̂Ŝi − ∑\nM∈M(Ŝi)\n(−1)|M | ∏\n{i,j}∈M\nB̂i,j ∏\ni 6∈V (M)\nK̂i,i,\nfor 1 ≤ i ≤ νĜ. In light of (1), for large enough n, this quantity should be close to\nHi = 2× (−1)|Ŝi|+1 ∏\n{i,j}∈E(Ŝi)\nKi,j .\nWe note that this definition is only symbolic in nature, and computing Ĥi in this fashion is extremely inefficient. Instead, to compute it in practice, we will use the determinant of an auxiliary matrix, computed via a matrix factorization. Namely, let us define the matrix K̃ ∈ RN×N such that K̃i,i = K̂i,i for 1 ≤ i ≤ N , and K̃i,j = B̂1/2i,j . We have\ndet K̃Ŝi = ∑ M∈M (−1)|M | ∏ {i,j}∈M B̂i,j ∏ i 6∈V (M) K̂i,i\n+ 2× (−1)|Ŝi|+1 ∏\n{i,j}∈Ê(Ŝi)\nB̂ 1/2 i,j ,\nso that we may equivalently write\nĤi = ∆̂Ŝi − det(K̃Ŝi) + 2× (−1) |Ŝi|+1 ∏ {i,j}∈Ê(Ŝi) B̂ 1/2 i,j .\nFinally, let m̂ = |Ê|. Set the matrix A ∈ GF (2)νĜ×m̂ with i-th row representing Ĉi in GF (2)m, 1 ≤ i ≤ νĜ, b = (b1, . . . , bνĜ) ∈ GF (2)\nνĜ with bi = 12 [sgn(Ĥi) + 1], 1 ≤ i ≤ νĜ, and let x ∈ GF (2)m be a solution to the linear system Ax = b if a solution exists, x = 1m otherwise.\nWe define K̂i,j = 0 if {i, j} /∈ Ê and K̂i,j = K̂j,i = (2x{i,j} − 1)B̂ 1/2 i,j for all {i, j} ∈ Ê."
  }, {
    "heading": "2.4. Geometry",
    "text": "The main result of this subsection is the following lemma which relates the quality of estimation of K in terms of ρ to the quality of estimation of the principal minors ∆S . Lemma 1. Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK . Let ε > 0. If |∆̂S −∆S | ≤ ε for all S ⊆ [N ] with |S| ≤ 2 and if |∆̂S −∆S | ≤ (α/4)|S| for all S ⊆ [N ] with 3 ≤ |S| ≤ `, then\nρ(K̂,K) < 4ε/α .\nProof. We can bound |B̂i,j −K2i,j |, namely,\nB̂i,j ≤ (Ki,i + α2/16)(Kj,j + α2/16)− (∆{i,j} − α2/16) ≤ K2i,j + α2/4\nand\nB̂i,j ≥ (Ki,i − α2/16)(Kj,j − α2/16)− (∆{i,j} + α2/16) ≥ K2i,j − 3α2/16,\ngiving |B̂i,j −K2i,j | < α2/4. Thus, we can correctly determine whether Ki,j = 0 or |Ki,j | ≥ α, yielding Ĝ = GK . In particular, the cycle basis Ĉ1, . . . , ĈνĜ of Ĝ is a cycle basis of GK . Let 1 ≤ i ≤ νĜ. Denote by t = (α/4)|Si|. We have∣∣∣Ĥi −Hi∣∣∣ ≤ |∆̂Ŝi −∆Ŝi |+ |M(Ŝi)|maxx∈±1 [ (1 + 4tx)|Ŝi| − 1\n] ≤ (α/4)|Ŝi| + |M(Ŝi)| [ (1 + 4t)|Ŝi| − 1\n] ≤ (α/4)|Ŝi| + T ( |Ŝi|, ⌊ |Ŝi| 2 ⌋) 4t T (|Ŝi|, |Ŝi|)\n≤ (α/4)|Ŝi| + 4t (2 |Ŝi| 2 − 1)(2|Ŝi| − 1)\n≤ (α/4)|Ŝi| + t22|Ŝi|\n< 2α|Ŝi| ≤ |Hi|,\nwhere, for positive integers p < q, we denote by T (q, p) = ∑p i=1 ( q i ) . Therefore, we can deter-\nmine the sign of the product ∏ {i,j}∈E(Ŝi)Ki,j for every element in the cycle basis and recover the signs of the non-zero off-diagonal entries of Ki,j . Hence,\nρ(K̂,K) = max1≤i,j≤N ∣∣∣|K̂i,j | − |Ki,j |∣∣∣. For i = j,∣∣∣|K̂i,j | − |Ki,j |∣∣∣ = |K̂i,i −Ki,i| ≤ ε. For i 6= j with {i, j} ∈ Ê = E, one can easily show that∣∣∣B̂i,j −K2i,j∣∣∣ ≤ 4ε, yielding\n|B̂1/2i,j − |Ki,j || ≤ 4ε∣∣B̂1/2i,j + |Ki,j |∣∣ ≤ 4ε α ,\nwhich completes the proof.\nWe are now in a position to establish a sufficient sample size to estimate K within distance ε. Theorem 1. Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK . Let ε > 0. For any A > 0, there exists C > 0 such that\nn ≥ C ( 1 α2ε2 + ` ( 4 α )2`) logN ,\nyields ρ(K̂,K) ≤ ε with probability at least 1−N−A.\nProof. Using the previous lemma, and applying a union bound,\nP [ ρ(K̂,K) > ε ] ≤ ∑ |S|≤2 P [ |∆̂S −∆S | > αε/4 ] +\n∑ 2≤|S|≤` P [ |∆̂S −∆S | > (α/4)|S| ] ≤ 2N2e−nα 2ε2/8 + 2N `+1e−2n(α/4) 2`\n, (2)\nwhere we used Hoeffding’s inequality."
  }, {
    "heading": "3. Information theoretic lower bound",
    "text": "We prove an information-theoretic lower bound that holds already if GK is an `-cycle. Let D(K‖K ′) and H(K,K ′) denote respectively the Kullback-Leibler divergence and the Hellinger distance between DPP(K) and DPP(K ′). Lemma 2. For η ∈ {−,+}, letKη be the `×`matrix with elements given by\nKi,j =  1/2 if j = i α if j = i± 1 ηα if (i, j) ∈ {(1, `), (`, 1)} 0 otherwise .\nThen, for any α ≤ 1/8, it holds\nD(K‖K ′) ≤ 4(6α)`, and H(K,K ′) ≤ (8α2)` .\nProof. It is straightforward to see that\ndet(K+J )− det(K − J ) = { 2α` if J = [`] 0 else .\nIf Y is sampled from DPP(Kη), we denote by pη(S) = P[Y = S], for S ⊆ [`]. It follows from the inclusionexclusion principle that for all S ⊆ [`],\np+(S)− p−(S) = ∑\nJ⊆[`]\\S\n(−1)|J|(detK+S∪J − detK − S∪J)\n= (−1)`−|S|(detK+ − detK−) = ±2α` , (3)\nwhere |J | denotes the cardinality of J . The inclusionexclusion principle also yields that pη(S) = |det(Kη − IS̄)| for all S ⊆ [l], where IS̄ stands for the ` × ` diagonal matrix with ones on its entries (i, i) for i /∈ S, zeros elsewhere.\nDenote by D(K+‖K−) the Kullback Leibler divergence between DPP(K+) and DPP(K−):\nD(K+‖K−) = ∑ S⊆[`] p+(S) log ( p+(S) p−(S) )\n≤ ∑ S⊆[`] p+(S) p−(S) (p+(S)− p−(S))\n≤ 2α` ∑ S⊆[`] |det(K+ − IS̄)| |det(K− − IS̄)| , (4)\nby (3). Using the fact that 0 < α ≤ 1/8 and the Gershgorin circle theorem, we conclude that the absolute value of all eigenvalues of Kη− IS̄ are between 1/4 and 3/4. Thus we obtain from (4) the bound D(K+‖K−) ≤ 4(6α)`.\nUsing the same arguments as above, the Hellinger distance H(K+,K−) between DPP(K+) and DPP(K−) satisfies\nH(K+,K−) = ∑ J⊆[`] ( p+(J)− p−(J)√ p+(J) + √ p−(J) )2\n≤ ∑ J⊆[`] 4α2` 2 · 4−` = (8α2)`\nwhich completes the proof.\nThe sample complexity lower bound now follows from standard arguments.\nTheorem 2. Let 0 < ε ≤ α ≤ 1/8 and 3 ≤ ` ≤ N . There exists a constant C > 0 such that if\nn ≤ C ( 8` α2` + log(N/`) (6α)` + logN ε2 ) ,\nthen the following holds: for any estimator K̂ based on n samples, there exists a kernelK that satisfies Assumption 1 and such that the cycle sparsity of GK is ` and for which ρ(K̂,K) ≥ ε with probability at least 1/3.\nProof. Recall the notation of Lemma 2. First consider the N ×N block diagonal matrix K (resp. K ′) where its first block isK+ (resp. K−) and its second block is IN−`. By a standard argument, the Hellinger distance Hn(K,K ′) between the product measures DPP(K)⊗n and DPP(K ′)⊗n satisfies\n1− H 2 n(K,K ′) 2 = ( 1− H 2(K,K ′) 2 )n ≥ (1− α2` 2× 8` )n ,\nwhich yields the first term in the desired lower bound.\nNext, by padding with zeros, we can assume that L = N/` is an integer. Let K(0) be a block diagonal matrix where each block is K+ (using the notation of Lemma 2). For j = 1, . . . , L, define the N × N block diagonal matrix K(j) as the matrix obtained from K(0) by replacing its jth block with K− (again using the notation of Lemma 2).\nSince DPP(K(j)) is the product measure of L lower dimensional DPPs that are each independent of each other, using Lemma 2 we have D(K(j)‖K(0)) ≤ 4(6α)`. Hence, by Fano’s lemma (see, e.g., Corollary 2.6 in (Tsybakov, 2009)), the sample complexity to learn the kernel of a DPP within a distance ε ≤ α is\nΩ\n( log(N/`)\n(6α)` ) which yields the second term.\nThe third term follows from considering K0 = (1/2)IN and letting Kj be obtained from K0 by adding ε to the jth entry along the diagonal. It is easy to see that D(Kj‖K0) ≤ 8ε2. Hence, a second application of Fano’s lemma yields that the sample complexity to learn the kernel of a DPP within a distance ε is Ω( logNε2 ).\nThe third term in the lower bound is the standard parametric term and is unavoidable in order to estimate the magnitude of the coefficients of K. The other terms are more interesting. They reveal that the cycle sparsity of GK , namely, `, plays a key role in the task of recovering the sign pattern of K. Moreover the theorem shows that the sample complexity of our method of moments estimator is near optimal."
  }, {
    "heading": "4. Algorithms",
    "text": ""
  }, {
    "heading": "4.1. Horton’s algorithm",
    "text": "We first give an algorithm to compute the estimator K̂ defined in Section 2. A well-known algorithm of Horton (Horton, 1987) computes a cycle basis of minimum total length in time O(m3N). Subsequently, the running time was improved toO(m2N/ logN) time (Amaldi et al., 2010). Also, it is known that a cycle basis of minimum total length is a shortest maximal cycle basis (Chickering et al., 1995). Together, these results imply the following.\nLemma 3. Let G = ([N ], E), |E| = m. There is an algorithm to compute a shortest maximal cycle basis in O(m2N/ logN) time.\nIn addition, we recall the following standard result regarding the complexity of Gaussian elimination (Golub & Van Loan, 2012).\nAlgorithm 1 Compute Estimator K̂ Input: samples Y1, ..., Yn, parameter α > 0.\nCompute ∆̂S for all |S| ≤ 2. Set K̂i,i = ∆̂{i} for 1 ≤ i ≤ N . Compute B̂i,j for 1 ≤ i < j ≤ N . Form K̃ ∈ RN×N and Ĝ = ([N ], Ê). Compute a shortest maximal cycle basis {v̂1, ..., v̂νĜ}. Compute ∆̂Ŝi for 1 ≤ i ≤ νĜ. Compute ĈŜi using det K̃Ŝi for 1 ≤ i ≤ νĜ. Construct A ∈ GF (2)νĜ×m, b ∈ GF (2)νĜ . Solve Ax = b using Gaussian elimination. Set K̂i,j = K̂j,i = (2x{i,j}−1)B̂ 1/2 i,j , for all {i, j} ∈ Ê.\nLemma 4. LetA ∈ GF (2)ν×m, b ∈ GF (2)ν . Then Gaussian elimination will find a vector x ∈ GF (2)m such that Ax = b or conclude that none exists in O(ν2m) time.\nWe give our procedure for computing the estimator K̂ in Algorithm 1. In the following theorem, we bound the running time of Algorithm 1 and establish an upper bound on the sample complexity needed to solve the recovery problem as well as the sample complexity needed to compute an estimate K̂ that is close to K.\nTheorem 3. Let K ∈ RN×N be a symmetric matrix satisfying 0 K I , and satisfying Assumption 1. Let GK be the graph induced by K and ` be the cycle sparsity of GK . Let Y1, ..., Yn be samples from DPP(K) and δ ∈ (0, 1). If\nn > log(N `+1/δ)\n(α/4) 2`\n,\nthen with probability at least 1 − δ, Algorithm 1 computes an estimator K̂ which recovers the signs of K up to a DN - similarity and satisfies\nρ(K, K̂) < 1\nα\n( 8 log(4N `+1/δ)\nn\n)1/2 (5)\nin O(m3 + nN2) time.\nProof. (5) follows directly from (2) in the proof of Theorem 1. That same proof also shows that with probability at least 1 − δ, the support of GK and the signs of K are recovered up to a DN -similarity. What remains is to upper bound the worst case run time of Algorithm 1. We will perform this analysis line by line. Initializing K̂ requires O(N2) operations. Computing ∆S for all subsets |S| ≤ 2 requires O(nN2) operations. Setting K̂i,i requires O(N) operations. Computing B̂i,j for 1 ≤ i < j ≤ N requires O(N2) operations. Forming K̃ requires O(N2) operations. Forming GK requires O(N2) operations. By\nLemma 3, computing a shortest maximal cycle basis requires O(mN) operations. Constructing the subsets Si, 1 ≤ i ≤ νĜ, requires O(mN) operations. Computing ∆̂Si for 1 ≤ i ≤ νĜ requires O(nm) operations. Computing ĈSi using det(K̃[Si]) for 1 ≤ i ≤ νĜ requires O(m`3) operations, where a factorization of each K̃[Si] is used to compute each determinant in O(`3) operations. Constructing A and b requires O(m`) operations. By Lemma 4, finding a solution x using Gaussian elimination requires O(m3) operations. Setting K̂i,j for all edges {i, j} ∈ E requires O(m) operations. Put this all together, Algorithm 1 runs in O(m3 + nN2) time."
  }, {
    "heading": "4.2. Chordal Graphs",
    "text": "Here we show that it is possible to obtain faster algorithms by exploiting the structure of GK . Specifically, in the case where GK chordal, we give an O(m) time algorithm to determine the signs of the off-diagonal entries of the estimator K̂, resulting in an improved overall runtime of O(m + nN2). Recall that a graph G = ([N ], E) is said to be chordal if every induced cycle in G is of length three. Moreover, a graph G = ([N ], E) has a perfect elimination ordering (PEO) if there exists an ordering of the vertex set {v1, ..., vN} such that, for all i, the graph induced by {vi} ∪ {vj |{i, j} ∈ E, j > i} is a clique. It is well known that a graph is chordal if and only if it has a PEO. A PEO of a chordal graph with m edges can be computed in O(m) operations using lexicographic breadth-first search (Rose et al., 1976).\nLemma 5. Let G = ([N ], E), be a chordal graph and {v1, ..., vn} be a PEO. Given i, let i∗ := min{j|j > i, {vi, vj} ∈ E}. Then the graph G′ = ([N ], E′), where E′ = {{vi, vi∗}}N−κ(G)i=1 , is a spanning forest of G.\nProof. We first show that there are no cycles in G′. Suppose to the contrary, that there is an induced cycle C of length k on the vertices {vj1 , ..., vjk}. Let v be the vertex of smallest index. Then v is connected to two other vertices in the cycle of larger index. This is a contradiction to the construction.\nWhat remains is to show that |E′| = N − κ(G). It suffices to prove the case κ(G) = 1. Suppose to the contrary, that there exists a vertex vi, i < N , with no neighbors of larger index. Let P be the shortest path in G from vi to vN . By connectivity, such a path exists. Let vk be the vertex of smallest index in the path. However, it has two neighbors in the path of larger index, which must be adjacent to each other. Therefore, there is a shorter path.\nNow, given the chordal graph GK induced by K and the estimates of principal minors of size at most three, we provide an algorithm to determine the signs of the edges of\nAlgorithm 2 Compute Signs of Edges in Chordal Graph\nInput: GK = ([N ], E) chordal, ∆̂S for |S| ≤ 3.\nCompute a PEO {v1, ..., vN}. Compute the spanning forest G′ = ([N ], E′). Set all edges in E′ to have positive sign. Compute Ĉ{i,j,i∗} for all {i, j} ∈ E \\ E′, j < i. Order edges E \\ E′ = {e1, ..., eν} such that i > j if max ei < max ej . Visit edges in sorted order and for e = {i, j}, j > i, set\nsgn({i, j}) = sgn(Ĉ{i,j,i∗}) sgn({i, i∗}) sgn({j, i∗}).\nGK , or, equivalently, the off-diagonal entries of K.\nTheorem 4. IfGK is chordal, Algorithm 2 correctly determines the signs of the edges of GK in O(m) time.\nProof. We will simultaneously perform a count of the operations and a proof of the correctness of the algorithm. Computing a PEO requires O(m) operations. Computing the spanning forest requires O(m) operations. The edges of the spanning tree can be given arbitrary sign, because it is a cycle-free graph. This assigns a sign to two edges of each 3-cycle. Computing each Ĉ{i,j,i∗} requires a constant number of operations because ` = 3, requiring a total of O(m −N) operations. Ordering the edges requires O(m) operations. Setting the signs of each remaining edge requires O(m) operations.\nTherefore, when GK is chordal, the overall complexity required by our algorithm to compute K̂ is reduced to O(m+ nN2)."
  }, {
    "heading": "5. Experiments",
    "text": "Here we present experiments to supplement the theoretical results of the paper. We test our algorithm on two types of random matrices. First, we consider the matrix K ∈ RN×N corresponding to the cycle on N vertices,\nK = 1\n2 I +\n1 4 A,\nwhere A is symmetric and has non-zero entries only on the edges of the cycle, either +1 or −1, each with probability 1/2. By the Gershgorin circle theorem, 0 K I . Next, we consider the matrix K ∈ RN×N corresponding to the clique on N vertices,\nK = 1\n2 I +\n1\n4 √ N A,\nwhere A is symmetric and has all entries either +1 or −1, each with probability 1/2. It is well known that −2 √ N A 2 √ N with high probability, implying 0 K I .\nFor both cases and for a range of values of matrix dimension N and samples n, we run our algorithm on 50 randomly generated instances. We record the proportion of trials where we recover the graph induced by K, and the proportion of the trials where we recover both the graph and correctly determine the signs of the entries.\nIn Figure 1, the shade of each box represents the proportion of trials where recovery was successful for a given pair N,n. A completely white box corresponds to zero success rate, black to a perfect success rate.\nThe plots corresponding to the cycle and the clique are telling. We note that for the clique, recovering the sparsity pattern and recovering the signs of the off-diagonal entries come hand-in-hand. However, for the cycle, there is a noticeable gap between the number of samples required to recover the sparsity pattern and the number of samples required to recover the signs of the off-diagonal entries. This empirically confirms the central role that cycle sparsity plays in parameter estimation, and further corroborates our theoretical results."
  }, {
    "heading": "6. Conclusion and open questions",
    "text": "In this paper, we gave the first provable guarantees for learning the parameters of a DPP. Our upper and lower bounds reveal the key role played by the parameter `, which is the cycle sparsity of graph induced by the kernel of the DPP. Our estimator does not need to know ` beforehand, but can adapt to the instance. Moreover, our procedure outputs an estimate of `, which could potentially be used for further inference questions such as testing and confidence intervals. An interesting open question is whether on a graph by graph basis, the parameter ` exactly determines the optimal sample complexity. Moreover when the number of samples is too small, can we exactly characterize which signs can be learned correctly and which cannot (up to a similarity transformation by D)? Such results would lend new theoretical insights into the output of algorithms for learning DPPs, and which individual parameters in the estimate we can be confident about and which we cannot.\nAcknowledgements. A.M. is supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF1565235, a David and Lucile Packard Fellowship and an Alfred P. Sloan Fellowship. P.R. is supported in part by NSF CAREER DMS-1541099, NSF DMS-1541100, DARPA W911NF-16-1-0551, ONR N00014-17-1-2147 and a grant from the MIT NEC Corporation."
  }],
  "year": 2017,
  "references": [{
    "title": "Learning the parameters of determinantal point process kernels",
    "authors": ["Affandi", "Raja Hafiz", "Fox", "Emily B", "Adams", "Ryan P", "Taskar", "Benjamin"],
    "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,",
    "year": 2014
  }, {
    "title": "Efficient deterministic algorithms for finding a minimum cycle basis in undirected graphs",
    "authors": ["Amaldi", "Edoardo", "Iuliano", "Claudio", "Rizzi", "Romeo"],
    "venue": "In International Conference on Integer Programming and Combinatorial Optimization,",
    "year": 2010
  }, {
    "title": "Diversifying sparsity using variational determinantal point processes",
    "authors": ["Batmanghelich", "Nematollah Kayhan", "Quon", "Gerald", "Kulesza", "Alex", "Kellis", "Manolis", "Golland", "Polina", "Bornn", "Luke"],
    "year": 2014
  }, {
    "title": "Eynard–mehta theorem, schur process, and their pfaffian analogs",
    "authors": ["Borodin", "Alexei", "Rains", "Eric M"],
    "venue": "Journal of statistical physics,",
    "year": 2005
  }, {
    "title": "Maximum likelihood estimation of determinantal point processes",
    "authors": ["Brunel", "Victor-Emmanuel", "Moitra", "Ankur", "Rigollet", "Philippe", "Urschel", "John"],
    "year": 2017
  }, {
    "title": "On finding a cycle basis with a shortest maximal cycle",
    "authors": ["Chickering", "David M", "Geiger", "Dan", "Heckerman", "David"],
    "venue": "Information Processing Letters,",
    "year": 1995
  }, {
    "title": "On selecting a maximum volume sub-matrix of a matrix and related problems",
    "authors": ["Çivril", "Ali", "Magdon-Ismail", "Malik"],
    "venue": "Theoretical Computer Science,",
    "year": 2009
  }, {
    "title": "Efficient volume sampling for row/column subset selection",
    "authors": ["Deshpande", "Amit", "Rademacher", "Luis"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2010
  }, {
    "title": "Statistical theory of the energy levels of complex systems",
    "authors": ["Dyson", "Freeman J"],
    "venue": "III. J. Mathematical Phys.,",
    "year": 1962
  }, {
    "title": "Expectation-maximization for learning determinantal point processes",
    "authors": ["Gillenwater", "Jennifer A", "Kulesza", "Alex", "Fox", "Emily", "Taskar", "Ben"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "A polynomial-time algorithm to find the shortest cycle basis of a graph",
    "authors": ["Horton", "Joseph Douglas"],
    "venue": "SIAM Journal on Computing,",
    "year": 1987
  }, {
    "title": "Learning with determinantal point processes",
    "authors": ["A. Kulesza"],
    "venue": "PhD thesis, University of Pennsylvania,",
    "year": 2012
  }, {
    "title": "k-DPPs: Fixed-size determinantal point processes",
    "authors": ["Kulesza", "Alex", "Taskar", "Ben"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Determinantal Point Processes for Machine Learning",
    "authors": ["Kulesza", "Alex", "Taskar", "Ben"],
    "venue": "Now Publishers Inc.,",
    "year": 2012
  }, {
    "title": "Individualness and determinantal point processes for pedestrian detection",
    "authors": ["Lee", "Donghoon", "Cha", "Geonho", "Yang", "Ming-Hsuan", "Oh", "Songhwai"],
    "venue": "In Computer Vision ECCV 2016 - 14th European Conference,",
    "year": 2016
  }, {
    "title": "Fast sampling for strongly rayleigh measures with application to determinantal point processes",
    "authors": ["Li", "Chengtao", "Jegelka", "Stefanie", "Sra", "Suvrit"],
    "year": 2016
  }, {
    "title": "Fast dpp sampling for nystrom with application to kernel methods",
    "authors": ["Li", "Chengtao", "Jegelka", "Stefanie", "Sra", "Suvrit"],
    "venue": "International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Learning mixtures of submodular shells with application to document summarization",
    "authors": ["Lin", "Hui", "Bilmes", "Jeff A"],
    "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA,",
    "year": 2012
  }, {
    "title": "The coincidence approach to stochastic point processes",
    "authors": ["Macchi", "Odile"],
    "venue": "Advances in Appl. Probability,",
    "year": 1975
  }, {
    "title": "Fixed-point algorithms for learning determinantal point processes",
    "authors": ["Mariet", "Zelda", "Sra", "Suvrit"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "Randomized rounding for the largest simplex problem",
    "authors": ["Nikolov", "Aleksandar"],
    "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
    "year": 2015
  }, {
    "title": "Maximizing determinants under partition constraints",
    "authors": ["Nikolov", "Aleksandar", "Singh", "Mohit"],
    "venue": "In STOC,",
    "year": 2016
  }, {
    "title": "Fast mixing for discrete point processes",
    "authors": ["Rebeschini", "Patrick", "Karbasi", "Amin"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "An efficient algorithm for the symmetric principal minor assignment problem",
    "authors": ["Rising", "Justin", "Kulesza", "Alex", "Taskar", "Ben"],
    "venue": "Linear Algebra and its Applications,",
    "year": 2015
  }, {
    "title": "Algorithmic aspects of vertex elimination on graphs",
    "authors": ["Rose", "Donald J", "Tarjan", "R Endre", "Lueker", "George S"],
    "venue": "SIAM Journal on computing,",
    "year": 1976
  }, {
    "title": "Introduction to nonparametric estimation",
    "authors": ["Tsybakov", "Alexandre B"],
    "year": 2009
  }, {
    "title": "Scalable discovery of audio fingerprint motifs in broadcast streams with determinantal point process based motif clustering",
    "authors": ["Xu", "Haotian", "Ou"],
    "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing,",
    "year": 2016
  }, {
    "title": "Tweet timeline generation with determinantal point processes",
    "authors": ["Yao", "Jin-ge", "Fan", "Feifan", "Zhao", "Wayne Xin", "Wan", "Xiaojun", "Chang", "Edward Y", "Xiao", "Jianguo"],
    "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
    "year": 2016
  }],
  "id": "SP:c9465f842b3ca9429e15728ba76be84b6df0fb50",
  "authors": [{
    "name": "John Urschel",
    "affiliations": []
  }, {
    "name": "Ankur Moitra",
    "affiliations": []
  }, {
    "name": "Philippe Rigollet",
    "affiliations": []
  }],
  "abstractText": "Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.",
  "title": "Learning Determinantal Point Processes with Moments and Cycles"
}