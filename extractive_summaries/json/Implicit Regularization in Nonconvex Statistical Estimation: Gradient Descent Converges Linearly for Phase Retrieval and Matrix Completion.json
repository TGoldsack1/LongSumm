{
  "sections": [{
    "text": "Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This “implicit regularization” feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e. solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors.\n1Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA 2Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA 3Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Cong Ma <congm@princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s)."
  }, {
    "heading": "1. Introduction",
    "text": "A wide spectrum of science and engineering applications calls for solutions to a nonlinear system of equations. Imagine we have collected a set of data points y = {yj}1≤j≤m, generated by a nonlinear sensing system,\nyj ≈ Aj ( x\\ ) , 1 ≤ j ≤ m,\nwhere x\\ is the unknown object of interest, and the Aj’s are certain nonlinear maps known a priori. Can we hope to reconstruct the underlying object x\\ in a faithful yet efficient manner? Problems of this kind abound in information and statistical science, prominent examples including low-rank matrix recovery (Keshavan et al., 2010; Candès & Recht, 2009), phase retrieval (Candès et al., 2013; Jaganathan et al., 2015), and learning neural networks (Soltanolkotabi et al., 2017; Zhong et al., 2017), to name just a few.\nIn principle, one can attempt reconstruction by seeking a solution that minimizes the empirical loss, namely,\nminimizex f(x) = m∑\nj=1\n∣∣yj −Aj(x) ∣∣2. (1)\nUnfortunately, this empirical loss minimization problem is, in many cases, highly nonconvex, making it NP-hard in general. For example, this non-convexity issue comes up in:\n• Solving random quadratic systems of equations (a.k.a. phase retrieval): where one wishes to solve for x\\\nin m quadratic equations yj = ( a>j x \\ )2\n, 1 ≤ j ≤ m, with {aj}1≤j≤m denoting the known design vectors. In this case, the empirical risk minimization is given by\nminimizex∈Rn f(x) = 1\n4m\nm∑\nj=1\n[ yj − ( a>j x )2]2 . (2)\n• Low-rank matrix completion: which aims to predict all entries of a low-rank matrix M \\ = X\\X\\> from partial entries (those from an index subset Ω), where X\\ ∈ Rn×r (r n). Here, the nonconvex problem to solve is\nminimize X∈Rn×r\nf(X) = n2\n4m\n∑\n(j,k)∈Ω\n( M \\j,k − e>j XX>ek )2 .\nImplicit Regularization in Nonconvex Statistical Estimation\nTable 1. Prior theory for gradient descent (with spectral initialization)\nVanilla gradient descent Regularized gradient descent sample iteration step sample iteration type of\ncomplexity complexity size complexity complexity regularization Phase\nn logn n log 1\n1 n\nn log 1 trimming retrieval e.g. (Chen & Candès, 2017)\nn/a n/a n/a nr7 nr log 1\nregularized loss Matrix e.g. (Sun & Luo, 2016)\ncompletion nr2 r2 log 1\nprojection e.g. (Chen & Wainwright, 2015)"
  }, {
    "heading": "1.1. Nonconvex Optimization via Regularized GD",
    "text": "First-order methods have been a popular heuristic in practice for solving nonconvex problems including (1). For instance, a widely adopted procedure is gradient descent (GD), which follows the update rule\nxt+1 = xt − ηt∇f ( xt ) , t ≥ 0, (3)\nwhere ηt is the learning rate (or step size) and x0 is some proper initial guess. Given that it only performs a single gradient calculation ∇f(·) per iteration (which typically can be completed within near-linear time), this paradigm emerges as a candidate for solving large-scale problems. The natural questions are: whether xt converges to the global solution and, if so, how long it takes for convergence, especially since (1) is highly nonconvex.\nFortunately, despite the worst-case hardness, appealing convergence properties have been discovered in various statistical estimation problems; the blessing being that the statistical models help rule out ill-behaved instances. For the average case, the empirical loss often enjoys benign geometry, particularly in a local region surrounding the global optimum. In light of this, an effective nonconvex iterative method typically consists of two parts:\n1. an initialization scheme (e.g. spectral methods); 2. an iterative refinement procedure (e.g. gradient descent).\nThis strategy has recently spurred a great deal of interest, owing to its promise of achieving computational efficiency and statistical accuracy at once for a growing list of problems, e.g. (Keshavan et al., 2010; Jain et al., 2013; Chen & Wainwright, 2015; Sun & Luo, 2016; Candès et al., 2015; Chen & Candès, 2017). However, rather than directly applying vanilla GD (3), existing theory often suggests enforcing proper regularization. Such explicit regularization enables improved computational convergence by properly “stabilizing” the search directions. The following regularization schemes, among others, have been suggested to obtain or improve computational guarantees. We refer to these algorithms collectively as Regularized Gradient Descent.\n• Trimming/truncation, which truncates a subset of the gradient components when forming the descent direction. For instance, when solving quadratic systems of equations, one can modify the gradient descent update rule as\nxt+1 = xt − ηtT ( ∇f ( xt )) , (4)\nwhere T is an operator that effectively drops samples bearing too much influence on the search direction (Chen & Candès, 2017; Zhang et al., 2016b; Wang et al., 2017). • Regularized loss, which attempts to optimize a regular-\nized empirical risk function through\nxt+1 = xt − ηt ( ∇f ( xt ) +∇R ( xt )) , (5)\nwhere R(x) stands for an additional penalty term in the empirical loss. For example, in matrix completion, R(·) penalizes the `2 row norm (Keshavan et al., 2010; Sun & Luo, 2016) as well as the Frobenius norm (Sun & Luo, 2016) of the decision matrix. • Projection, which projects the iterates onto certain sets based on prior knowledge, that is,\nxt+1 = P ( xt − ηt∇f ( xt )) , (6)\nwhere P is a certain projection operator used to enforce, for example, incoherence properties. This strategy has been employed in low-rank matrix completion (Chen & Wainwright, 2015; Zheng & Lafferty, 2016).\nEquipped with such regularization procedures, existing works uncover appealing computational and statistical properties under various statistical models. Table 1 summarizes the performance guarantees derived in the prior literature; for simplicity, only orderwise results are provided."
  }, {
    "heading": "1.2. Regularization-free Procedures?",
    "text": "The regularized gradient descent algorithms, while exhibiting appealing performance, usually introduce more tuning parameters depending on the assumed statistical models. In contrast, vanilla gradient descent (cf. (3)) — which is perhaps the very first method that comes into mind and requires minimal tuning parameters — is far less understood (cf. Table 1). Take matrix completion as an example: to the best of our knowledge, there is currently no theoretical guarantee derived for vanilla gradient descent.\nThe situation is better for phase retrieval: the local convergence of vanilla gradient descent, also known as Wirtinger flow (WF), has been investigated in (Candès et al., 2015).\nImplicit Regularization in Nonconvex Statistical Estimation\nUnder i.i.d. Gaussian design and with near-optimal sample complexity, WF (combined with spectral initialization) provably achieves -accuracy (in a relative sense) within O(n log(1/ε)) iterations. Nevertheless, the computational guarantee is significantly outperformed by the regularized version (called truncated Wirtinger flow (Chen & Candès, 2017)), which only requires O(log(1/ε)) iterations to converge with similar per-iteration cost. On closer inspection, the high computational cost of WF is largely due to the vanishingly small step size ηt = O(1/(n‖x\\‖22)) — and hence slow movement — suggested by the theory (Candès et al., 2015). While this is already the largest possible step size allowed in the theory published in (Candès et al., 2015), it is considerably more conservative than the choice ηt = O ( 1/‖x\\‖22 ) theoretically justified for the regularized version (Chen & Candès, 2017; Zhang et al., 2016b).\nThe lack of understanding and the suboptimal results about vanilla GD raise a natural question: are regularization-free iterative algorithms inherently suboptimal for solving nonconvex statistical estimation problems?"
  }, {
    "heading": "1.3. Numerical Surprise of Unregularized GD",
    "text": "To answer the preceding question, it is perhaps best to first collect some numerical evidence. In what follows, we test the performance of vanilla GD for solving random quadratic systems using a constant step size. The initial guess is obtained by means of the standard spectral method.\nFor each n, set m = 10n, take x\\ ∈ Rn to be a random vector with unit norm, and generate the design vectors aj\ni.i.d.∼ N (0, In), 1 ≤ j ≤ m. Figure 1 illustrates the relative `2 error min{‖xt−x\\‖2, ‖xt+x\\‖2}/‖x\\‖2 (modulo the unrecoverable global phase) vs. the iteration count. The results are shown for n = 20, 100, 200, 1000, with the step size taken to be ηt = 0.1 in all settings.\nIn all settings, vanilla gradient descent enjoys remarkable linear convergence, always yielding an accuracy of 10−5 (in a relative sense) within around 200 iterations. In particular, the step size is taken to be ηt = 0.1 although we vary the problem size from n = 20 to n = 1000. The consequence is that the convergence rates experience little changes when the\nproblem sizes vary. In comparison, the theory published in (Candès et al., 2015) seems overly pessimistic, as it suggests a diminishing step size inversely proportional to n and, as a result, an iteration complexity that worsens as the problem size grows.\nIn short, the above empirical results are surprisingly positive yet puzzling. Why was the computational efficiency of vanilla gradient descent unexplained or substantially underestimated in prior theory?"
  }, {
    "heading": "1.4. This Paper",
    "text": "The main contribution of this paper is towards demystifying the “unreasonable” effectiveness of regularization-free nonconvex gradient methods. As asserted in previous work, regularized gradient descent succeeds by properly enforcing/promoting certain incoherence conditions throughout the execution of the algorithm. In contrast, we discover that\nVanilla gradient descent automatically forces the iterates to stay incoherent with the measurement mechanism, thus implicitly regularizing the search directions.\nThis “implicit regularization” phenomenon is of fundamental importance, suggesting that vanilla gradient descent proceeds as if it were properly regularized. This explains the remarkably favorable performance of unregularized gradient descent in practice. Focusing on two fundamental statistical estimation problems, our theory guarantees both statistical and computational efficiency of vanilla gradient descent under random designs and spectral initialization. With near-optimal sample complexity, to attain -accuracy, vanilla gradient descent converges in an almost dimensionfree O(log(1/ )) iterations, possibly up to a log n factor. As a byproduct of our theory, we show that gradient descent provably controls the entrywise and spectral-norm estimation errors for noisy matrix completion."
  }, {
    "heading": "2. Implicit Regularization – A Case Study",
    "text": "To reveal reasons behind the effectiveness of vanilla gradient descent, we first examine the existing theory of gradient descent and identify the geometric properties that enable linear convergence. We then develop an understanding as to why prior theory is conservative, and describe the phenomenon of implicit regularization that helps explain the effectiveness of vanilla gradient descent. To facilitate discussion, we will use the problem of solving random quadratic systems of equations (or phase retrieval) and Wirtinger flow as a case study, but our diagnosis applies more generally."
  }, {
    "heading": "2.1. Gradient Descent Theory Revisited",
    "text": "It is well-known that for an unconstrained optimization problem, if the objective function f is both α-strongly convex and β-smooth, then vanilla gradient descent (3) enjoys `2\nImplicit Regularization in Nonconvex Statistical Estimation\nerror contraction (Bubeck, 2015), namely, for t ≥ 0 ∥∥xt+1 − x\\‖2 ≤ ( 1− 2\nβ/α+ 1\n)∥∥xt − x\\ ∥∥\n2 , (7)\nas long as the step size is chosen as ηt = 2/(α + β). Here, x\\ denotes the global minimum. This immediately reveals the iteration complexity for gradient descent: the number of iterations taken to attain -accuracy is bounded by O((β/α) log(1/ )). In other words, the iteration complexity is dictated by and scales linearly with the condition number — the ratio β/α of smoothness to strong convexity parameters.\nMoving beyond convex optimization, one can easily extend the above theory to nonconvex problems with local strong convexity and smoothness. More precisely, suppose the objective function f satisfies\n∇2f(x) αI and ∥∥∇2f(x) ∥∥ ≤ β\nover a local `2 ball surrounding the global minimum x\\:\nBδ(x) := { x | ‖x− x\\‖2 ≤ δ‖x\\‖2 } . (8)\nThe contraction result (7) continues to hold, as long as the algorithm starts with an initial point that falls inside Bδ(x)."
  }, {
    "heading": "2.2. Local Geometry for Solving Quadratic Systems",
    "text": "To invoke generic gradient descent theory, it is critical to characterize the local strong convexity and smoothness properties of the loss function. Take the problem of solving random quadratic systems as an example. Consider the i.i.d. Gaussian design in which aj\ni.i.d.∼ N (0, In), 1 ≤ j ≤ m, and suppose without loss of generality that the underlying signal obeys ‖x\\‖2 = 1. In the regime where m n log n (which is the regime considered in (Candès et al., 2015)), local strong convexity is present, in the sense that f(·) as defined in (2) obeys\n∇2f(x) (1/2) · In, ∀x : ∥∥x− x\\ ∥∥ 2 ≤ δ ∥∥x\\ ∥∥ 2\nwith high probability, provided that δ > 0 is sufficiently small (see (Soltanolkotabi, 2014; White et al., 2015) and (Ma et al., 2017)). The smoothness parameter, however, is not well-controlled. In fact, it can be as large as (up to logarithmic factors) ∥∥∇2f(x) ∥∥ . n even when we restrict attention to the local `2 ball (8) with δ > 0 being a fixed small constant. This means that the condition number β/α (defined in Section 2.1) may scale as O(n), leading to the step size recommendation ηt 1/n, and, as a consequence, a high iteration complexity O(n log(1/ )). This underpins the analysis in (Candès et al., 2015).\nIn summary, the geometric properties of the loss function — even in the local `2 ball centering around the global minimum — is not as favorable as one anticipates. A direct\napplication of generic gradient descent theory leads to an overly conservative learning rate and a pessimistic convergence rate, unless the number of samples is enormously larger than the number of unknowns."
  }, {
    "heading": "2.3. Which Region Enjoys Nicer Geometry?",
    "text": "Interestingly, our theory identifies a local region surrounding x\\ with a large diameter that enjoys much nicer geometry. This region does not mimic an `2 ball, but rather, the intersection of an `2 ball and a polytope. We term it the region of incoherence and contraction (RIC). For phase retrieval, the RIC includes all points x ∈ Rn obeying\n∥∥x− x\\ ∥∥ 2 ≤ δ ∥∥x\\ ∥∥ 2\nand (9a)\nmax 1≤j≤m\n∣∣a>j ( x− x\\ )∣∣ . √ log n ∥∥x\\ ∥∥ 2 , (9b)\nwhere δ > 0 is some small numerical constant. As will be formalized in (Ma et al., 2017), with high probability the Hessian matrix satisfies\n(1/2) · In ∇2f(x) O(log n) · In simultaneously for all x in the RIC. In words, the Hessian matrix is nearly well-conditioned (with the condition number bounded by O(log n)), as long as (i) the iterate is not very far from the global minimizer (cf. (9a)), and (ii) the iterate remains incoherent1 with respect to the sensing vectors (cf. (9b)). See Figure 2(a) for an illustration.\nThe following observation is thus immediate: one can safely adopt a far more aggressive step size (as large as ηt = O(1/ log n)) to achieve acceleration, as long as the iterates stay within the RIC. This, however, fails to be guaranteed by generic gradient descent theory. To be more precise, if the current iterate xt falls within the desired region, then in view of (7), we can ensure `2 error contraction after one iteration, namely,\n‖xt+1 − x\\‖2 ≤ ‖xt − x\\‖2 and hence xt+1 stays within the local `2 ball and hence satisfies (9a). However, it is not immediately obvious that xt+1 would still stay incoherent with the sensing vectors and satisfy (9b). If xt+1 leaves the RIC, then it no longer enjoys the benign local geometry of the loss function, and the algorithm has to slow down in order to avoid overshooting. See Fig. 2(b) for a visual illustration. In fact, in almost all regularized gradient descent algorithms mentioned in Section 1.1, the regularization procedures are mainly proposed to enforce such incoherence constraints."
  }, {
    "heading": "2.4. Implicit Regularization",
    "text": "However, is regularization really necessary for the iterates to stay within the RIC? To answer this question, we plot\n1If x is aligned with (and hence very coherent with) one vector aj , then with high probability one has ∣∣a>j (x−x\\)| & ∣∣a>j x| √ n‖x‖2, which is significantly larger than √ logn‖x‖2.\nImplicit Regularization in Nonconvex Statistical Estimation\n·\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2⌦ ⇣ e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 ⇣ e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn⇥r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 ⌦\nfind h, x 2 Cn s.t. b⇤i hix⇤i ai = b⇤i h\\ix\\⇤i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\n1\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2⌦ ⇣ e>i X >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 ⇣ e>i XX >ej e>i X\\X\\> j 2\nfind X 2 Rn⇥r s.t. e>j XX>ei = e>j X\\X\\> i, (i, j) 2 ⌦\nfind h, x 2 Cn s.t. b⇤i hix⇤i ai = b⇤i h\\ix\\⇤i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2⌦ ⇣ e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = m\ni=1\n⇣ e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn⇥r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 ⌦\nfind h, x 2 Cn s.t. b⇤i hix⇤i ai = b⇤i h\\ix\\⇤i ai, 1  i  m max(r2f( )) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 . log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 . log n\n1\na>1 (x x\\) .\np log n\na>2 (x x\\) .\np log n\n1\na>1 (x x\\) .\np log n\na>2 (x x\\) .\np log n\n1\n·x0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\nx0 1 x2 x3\n1\nP⇥(✓0)PX|⇥(x | ✓0) H0 > < H1 P⇥(✓1)PX|⇥(x | ✓1) L(x) = PX|⇥(x | ✓1) PX|⇥(x | ✓0) H1 > < H0 ⇠ H0 ! N (0, 1) H1 ! N (1, 1) L(x) = fX(x | H1) fX(x | H0) = 1p 2⇡ exp ⇣ (x 1) 2 2 ⌘ 1p 2⇡ exp x22 = exp ✓ x 1 2 ◆ L(x) H1 > < H0 ⇠ () x H1 > < H0 1 2 + log ⇠ Pe,MAP = P⇥(✓0)↵ + P⇥(✓1) ↵ S1,k = (Wk + c1Fk)e i 1,k\nS2,k = (Wk + c2Fk)e i 2,k\nS1,k = (Wk + c1Fk)e i 1,k S2,k = (Wk + c2Fk)e i 2,k , 8pixel k\nWk = f1(S1,k, S2,k)\nFk = f2(S1,k, S2,k)\nx\\\n1\n·x0 x1 x2 x3\n1\nx0 x1 x2 x3\n1\n0 1 2 x3\n1\nx0 x1 x2 x3\n1\nP⇥(✓0)PX|⇥(x | ✓0) H0 > < H1 P⇥(✓1)PX|⇥(x | ✓1) L(x) = PX|⇥(x | ✓1) PX|⇥(x | ✓0) H1 > < H0 ⇠ H0 ! N (0, 1) H1 ! N (1, 1) L(x) = fX(x | H1) fX(x | H0) = 1p 2⇡ exp ⇣ (x 1) 2 2 ⌘ 1p 2⇡ exp x22 = exp ✓ x 1 2 ◆ L(x) H1 > < H0 ⇠ () x H1 > < H0 1 2 + log ⇠ Pe,MAP = P⇥(✓0)↵ + P⇥(✓1) ↵ S1,k = (Wk + c1Fk)e i 1,k\nS2,k = (Wk + c2Fk)e i 2,k\nS1,k = (Wk + c1Fk)e i 1,k S2,k = (Wk + c2Fk)e i 2,k , 8pixel k\nWk = f1(S1,k, S2,k)\nFk = f2(S1,k, S2,k)\nx\\\n1\n(a) (b) (c)\nFigure 2. (a) The shaded region is an illustration of the incoherence region, which satisfies ∣∣a>j (x− x\\)∣∣ . √logn for all points x in the region. (b) When x0 resides in the desired region, we know that x1 remains within the `2 ball but might fall out of the incoherence region (the shaded region). Once x1 leaves the incoherence region, we lose control and may overshoot. (c) Our theory reveals that with high probability, all iterates will stay within the incoherence region, enabling fast convergence.\n0 5 10 15 20 25 30 0\n0.5\n1\n1.5\n2\n2.5\ngradient iterates vs. iteration count for the phase retrieval problem. The results are shown for n ∈ {20, 100, 200, 1000} and m = 10n, with the step size taken to be ηt = 0.1. The problem instances are generated in the same way as in Figure 1.\nin Fig. 3 the incoherence measure maxj|a>j (xt−x\\)|√\nlogn‖x\\‖2 vs. the\niteration count in a typical Monte Carlo trial, generated in the same way as for Figure 1. Interestingly, the incoherence measure remains bounded by 2 for all iterations t > 1. This important observation suggests that one may adopt a substantially more aggressive step size throughout the whole algorithm. The main objective of this paper is thus to provide a theoretical validation of the above empirical observation. As we will demonstrate shortly, with high probability all iterates throughout the execution of the algorithm (as well as the spectral initialization) are provably constrained within the RIC, implying fast convergence of vanilla gradient descent (cf. Figure 2(c)). The fact that the iterates stay incoherent with the measurement mechanism automatically, without explicit enforcement, is termed “implicit regularization” in the current work."
  }, {
    "heading": "2.5. A Glimpse of the Analysis: A Leave-one-out Trick",
    "text": "In order to rigorously establish (9b) for all iterates, the current paper develops a powerful mechanism based on the leave-one-out perturbation argument, a trick rooted and widely used in probability and random matrix theory (El Karoui, 2015; Javanmard & Montanari, 2015; Sur et al.,\n2017; Zhong & Boumal, 2017; Chen et al., 2017; Abbe et al., 2017). Note that the iterate xt is statistically dependent of the design vectors {aj}. Under such circumstances, one often resorts to generic bounds like the Cauchy-Schwarz inequality when bounding a>l (x\nt − x\\), which would not yield a desirable estimate. To address this issue, we introduce a sequence of auxiliary iterates {xt,(l)} for each 1 ≤ l ≤ m (for analytical purposes only), obtained by running vanilla gradient descent using all but the lth sample. As one expects, such auxiliary trajectories serve as extremely good surrogates of {xt} in the sense that\nxt ≈ xt,(l), 1 ≤ l ≤ m, t ≥ 0, (10) since their constructions only differ by a single sample. Most importantly, since xt,(l) is statistically independent of the lth design vector, it is much easier to control its incoherence w.r.t. al to the desired level:∣∣a>l ( xt,(l) − x\\ )∣∣ . √ log n ∥∥x\\ ∥∥ 2 . (11)\nCombining (10) and (11) then leads to (9b). See Figure 4 for a graphical illustration of this argument."
  }, {
    "heading": "3. Main Results",
    "text": "This section formalizes the implicit regularization phenomenon underlying unregularized GD, and presents its consequences, namely near-optimal statistical and computational guarantees for phase retrieval and matrix completion. The complete proofs can be found in (Ma et al., 2017)."
  }, {
    "heading": "3.1. Solving Random Quadratic Systems / Phase Retrieval",
    "text": "Suppose the m quadratic equations\nyj = ( a>j x \\ )2 , j = 1, 2, . . . ,m (12)\nare collected using random design vectors, namely, aj i.i.d.∼ N (0, In), and the nonconvex problem to solve is\nminimizex∈Rn f(x) := 1\n4m\nm∑\nj=1\n[( a>j x )2 − yj ]2 . (13)\nImplicit Regularization in Nonconvex Statistical Estimation\nA\nx\nAx\n1\n1 -3 2 -1 4\n-2 -1 3 4\n1 9 4 1 16\n4 1 9 16\nAx y = |Ax|2\n1\nAx y = |Ax|2\n1\n·\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m\nminimizeX f(X) = X\n(i,j)2⌦\n⇣ e>i XX >ej e>i X\\X\\>ej 2\nminimizeh,x f(h, x) = mX\ni=1\n⇣ e>i XX >ej e>i X\\X\\>ej 2\nfind X 2 Rn⇥r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 ⌦\nfind h, x 2 Cn s.t. b⇤i hix⇤i ai = b⇤i h\\ix\\⇤i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 .\np log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 .\np log n\nincoherence region w.r.t. a1\n1\nminimizex f(x) = 1 m mX i=1 (a>i x) 2 yi 2 s x 2 Rns.t. a>i x 2 = a>i x \\ 2 , 1  i  m minimizeX f(X) = X (i,j)2⌦ ⇣ e>i XX >ej e>i X\\X\\>ej 2 minimizeh,x f(h, x) = mX i=1 ⇣ e>i XX >ej e>i X\\X\\>ej 2 find X 2 Rn⇥r s.t. e>j XX>ei = e>j X\\X\\>ei, (i, j) 2 ⌦\nfind h, x 2 Cn s.t. b⇤i hix⇤i ai = b⇤i h\\ix\\⇤i ai, 1  i  m max(r2f(x)) max(r2f(x))\na1 a2 x \\\na>1 (x x\\)\nkx x\\k2 .\np log n\na1 a2 x \\\na>2 (x x\\)\nkx x\\k2 .\np log n\nincoherence region w.r.t. a1\n{xt,(1)} {xt}\n1\n{xt,(l)} al w.r.t. al\n1\n{xt,(l)} al w.r.t. al\n1\n{xt,(l)} al w.r.t. al\n1\n(a) (b)\nFigure 4. Illustration of the leave-one-out sequence w.r.t. al. (a) The sequence {xt,(l)}t≥0 is constructed without using the lth sample. (b) Since the auxiliary sequence {xt,(l)} is constructed without using al, the leave-one-out iterates stay within the incoherence region w.r.t. al with high probability. Meanwhile, {xt} and {xt,(l)} are expected to remain close as their construction differ only in one sample.\nThe Wirtinger flow (WF) algorithm, first introduced in (Candès et al., 2015), is a combination of spectral initialization and vanilla gradient descent; see Algorithm 1.\nAlgorithm 1 Wirtinger flow for phase retrieval Input: {aj}1≤j≤m and {yj}1≤j≤m. Spectral initialization: Let λ1 (Y ) and x̃0 be the leading eigenvalue and eigenvector of\nY = 1\nm ∑m j=1 yjaja > j , (14)\nrespectively, and set x0 = √ λ1 (Y ) /3 x̃\n0. Gradient updates: for t = 0, 1, 2, . . . , T − 1 do\nxt+1 = xt − ηt∇f ( xt ) . (15)\nRecognizing that the global phase/sign is unrecoverable from quadratic measurements, we introduce the `2 distance modulo the global phase as follows\ndist(x,x\\) := min { ‖x− x\\‖2, ‖x + x\\‖2 } . (16)\nOur finding is summarized in the following theorem. Theorem 1. Let x\\ ∈ Rn be a fixed vector. Suppose aj\ni.i.d.∼ N (0, In) for each 1 ≤ j ≤ m and m ≥ c0n log n for some sufficiently large constant c0 > 0. Assume the learning rate obeys ηt ≡ η = c1/ ( log n · ‖x0‖22 ) for any sufficiently small constant c1 > 0. Then there exist some absolute constants 0 < ε < 1 and c2 > 0 such that with probability at least 1−O ( mn−5 ) , the Wirtinger flow iterates (Algorithm 1) satisfy that for all t ≥ 0, dist(xt,x\\) ≤ ε(1− η‖x\\‖22/2)t‖x\\‖2, (17a)\nmax 1≤j≤m\n∣∣a>j ( xt − x\\ )∣∣ ≤ c2 √ log n‖x\\‖2. (17b)\nTheorem 1 reveals a few intriguing properties of WF.\n• Implicit regularization: Theorem 1 asserts that the incoherence properties are satisfied throughout the execution\nof the algorithm, including the spectral initialization (see (17b)), which formally justifies the implicit regularization feature we hypothesized. • Near-constant step size: Consider the case where ‖x\\‖2 = 1. Theorem 1 establishes near-linear convergence of WF with a substantially more aggressive step size η 1/ log n. Compared with the choice η . 1/n admissible in (Candès et al., 2015), Theorem 1 allows WF to attain -accuracy within O(log n log(1/ )) iterations. The resulting computational complexity of WF is O (mn log n log(1/ )) , which significantly improves upon the result O ( mn2 log (1/ ) ) derived in (Candès\net al., 2015). As a side note, if the sample size further increases to m n log2 n, then η 1 is also feasible, resulting in an iteration complexity log(1/ ). This follows since with high probability, the entire trajectory resides within a more refined incoherence region maxj ∣∣a>j ( xt −x\\\n)∣∣ . ‖x\\‖2. We omit the details here. Finally, we remark that similar implicit regularization phenomenon holds even in the presence of random initialization. See (Chen et al., 2018) for details."
  }, {
    "heading": "3.2. Low-rank Matrix Completion",
    "text": "We move on to the low-rank matrix completion problem.\nLet M \\ ∈ Rn×n be a positive semidefinite matrix2 with rank r, and suppose its eigendecomposition is\nM \\ = U \\Σ\\U \\>, (18)\nwhere U \\ ∈ Rn×r consists of orthonormal columns, and Σ\\ is an r × r diagonal matrix with eigenvalues in a descending order, i.e. σmax = σ1 ≥ · · · ≥ σr = σmin > 0. Throughout this paper, we assume the condition number κ := σmax/σmin is bounded by a fixed constant, independent of the problem size (i.e. n and r). Denoting\n2Here, we assume M \\ to be positive semidefinite to simplify the presentation, but note that our analysis easily extends to asymmetric low-rank matrices.\nImplicit Regularization in Nonconvex Statistical Estimation\nAlgorithm 2 Vanilla gradient descent for matrix completion (with spectral initialization)\nInput: Y = [Yj,k]1≤j,k≤n, r, p. Spectral initialization: Let U0Σ0U0> be the rank-r eigendecomposition of\nM0 := p−1PΩ(Y ) = p−1PΩ ( M \\ + E ) ,\nand set X0 = U0 ( Σ0 )1/2\n. Gradient updates: for t = 0, 1, 2, . . . , T − 1 do\nXt+1 = Xt − ηt∇f ( Xt ) . (21)\nX\\ = U \\(Σ\\)1/2 allows us to factorize M \\ as\nM \\ = X\\X\\>. (19)\nConsider a random sampling model such that each entry of M \\ is observed independently with probability 0 < p ≤ 1, i.e. for 1 ≤ j ≤ k ≤ n,\nYj,k = { M \\j,k + Ej,k with probability p, 0, else,\n(20)\nwhere the entries of E = [Ej,k]1≤j≤k≤n are independent sub-Gaussian noise with sub-Gaussian norm σ (see (Vershynin, 2012)). We denote by Ω the set of locations being sampled, and PΩ(Y ) represents the projection of Y onto the set of matrices supported in Ω. We note here that the sampling rate p, if not known, can be faithfully estimated by the sample proportion |Ω|/n2. To fix ideas, we consider the following nonconvex optimization problem\nminimize X∈Rn×r\nf (X) := 1\n4p\n∑\n(j,k)∈Ω\n( e>j XX >ek − Yj,k )2 .\nThe vanilla gradient descent algorithm (with spectral initialization) is summarized in Algorithm 2.\nBefore proceeding to the main theorem, we first introduce a standard incoherence parameter required for matrix completion (Candès & Recht, 2009). Definition 1 (Incoherence for matrix completion). A rank-r matrix M \\ with eigendecomposition M \\ = U \\Σ\\U \\> is said to be µ-incoherent if\n∥∥U \\ ∥∥ 2,∞ ≤ √ µ/n ∥∥U \\ ∥∥ F = √ µr/n, (22)\nwhere ‖ · ‖2,∞ denotes the largest `2 norm of the rows.\nIn addition, recognizing that X\\ is identifiable only up to orthogonal transformation, we define the optimal transform from the tth iterate Xt to X\\ as\nĤt := argmin R∈Or×r\n∥∥XtR−X\\ ∥∥\nF , (23)\nwhere Or×r is the set of r × r orthonormal matrices. With these definitions in place, we have the following theorem.\nTheorem 2. Let M \\ be a rank r, µ-incoherent PSD matrix, and its condition number κ is a fixed constant. Suppose the sample size satisfies n2p ≥ Cµ3r3n log3 n for some sufficiently large constant C > 0, and the noise satisfies\nσ\n√ n\np σmin√ κ3µr log3 n . (24)\nWith probability at least 1−O ( n−3 ) , the iterates of Algorithm 2 satisfy\n∥∥XtĤt −X\\∥∥ F ≤ ( C4ρ tµr 1 √ np + C1σ σmin √ n p )∥∥X\\∥∥ F ,\n∥∥XtĤt −X\\∥∥ 2,∞ ≤ ( C5ρ tµr √ logn np + C8σ σmin √ n logn p ) · ∥∥X\\∥∥\n2,∞,∥∥XtĤt −X\\∥∥ ≤ (C9ρtµr 1√ np + C10σ σmin √ n p )∥∥X\\∥∥ for all 0 ≤ t ≤ T = O(n5),3 where C1, C4, C5, C8, C9 and C10 are some absolute positive constants and 1− (σmin/5) · η ≤ ρ < 1, provided that 0 < ηt ≡ η ≤ 2/ (25κσmax).\nTheorem 2 provides the first theoretical guarantee of unregularized gradient descent for matrix completion, demonstrating near-optimal statistical accuracy and computational complexity, under near-minimal sample complexity.\n• Implicit regularization: In Theorem 2, we bound the `2/`∞ error of the iterates in a uniform manner. Note that ∥∥X −X\\ ∥∥ 2,∞ = maxj ∥∥e>j ( X −X\\ )∥∥ 2 , which\nimplies the iterates remain incoherent with the sensing vectors throughout and have small incoherence parameters, including the spectral initialization (cf. (22)). In comparison, prior works either include a penalty term on {‖e>j X‖2}1≤j≤n (Keshavan et al., 2010; Sun & Luo, 2016) and/or ‖X‖F (Sun & Luo, 2016) to encourage an incoherent and/or low-norm solution, or add an extra projection operation to enforce incoherence (Chen & Wainwright, 2015; Zheng & Lafferty, 2016). Our results demonstrate that such explicit regularization is unnecessary for the success of gradient descent. • Constant step size: Without loss of generality we may assume that σmax = ‖M \\‖ = O(1), which can be done by choosing proper scaling of M \\. Hence we have a constant step size ηt 1. Actually it is more convenient to consider the scale invariant parameter ρ: Theorem 2 guarantees linear convergence of the vanilla gradient descent at a constant rate ρ. Remarkably, the convergence\n3Theorem 2 remains valid if the total number T of iterations obeys T = nO(1). In the noiseless case where σ = 0, the theory allows arbitrarily large T .\nImplicit Regularization in Nonconvex Statistical Estimation\noccurs with respect to three different unitarily invariant norms: the Frobenius norm ‖·‖F, the `2/`∞ norm ‖·‖2,∞, and the spectral norm ‖ · ‖. As far as we know, the latter two are established for the first time. Note that our result even improves upon that for regularized GD; see Table 1. • Near-minimal Euclidean error: As the number of iterations t increases, the Euclidean error of vanilla GD converges to\n∥∥XtĤt −X\\ ∥∥\nF . σ σmin\n√ n\np\n∥∥X\\ ∥∥\nF , (25)\nwhich coincides with the theoretical guarantee in (Chen & Wainwright, 2015) and matches the minimax lower bound established in (Negahban & Wainwright, 2012; Koltchinskii et al., 2011). • Near-optimal entrywise error: The `2/`∞ error bound immediately yields entrywise control of the empirical risk. Specifically, as soon as the number of iterations t is sufficiently large, we have\n∥∥XtXt> −M \\ ∥∥ ∞ . σ\nσmin\n√ n log n\np\n∥∥M \\ ∥∥ ∞ .\nCompared with the Euclidean loss (25), this implies that when r = O(1), the entrywise error of XtXt> is uniformly spread out across all entries. As far as we know, this is the first result that reveals near-optimal entrywise error control for noisy matrix completion using nonconvex optimization, without resorting to sample splitting."
  }, {
    "heading": "4. Related Work",
    "text": "Convex relaxations have received much attention for solving nonlinear systems of equations in the past decade. Instead of directly attacking the nonconvex formulation, convex relaxation lifts the object of interest into a higher dimensional space and then attempts recovery via semidefinite programming (e.g. (Recht et al., 2010; Candès et al., 2013; Candès & Recht, 2009)). This has enjoyed great success in both theory and practice. Despite appealing statistical guarantees, SDP is in general prohibitively expensive when processing large-scale datasets.\nIn comparison, nonconvex approaches have been under extensive study in the last few years, due to their computational advantages. There is a growing list of statistical estimation problems for which nonconvex approaches are guaranteed to find global optimal solutions, including but not limited to phase retrieval (Netrapalli et al., 2013; Candès et al., 2015; Chen & Candès, 2017), low-rank matrix sensing and completion (Tu et al., 2016; Bhojanapalli et al., 2016; Park et al., 2016; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; Ge et al., 2016), dictionary learning (Sun et al., 2017), blind deconvolution (Li et al., 2016a; Cambareri & Jacques, 2016; Lee et al., 2017), tensor decomposition (Ge & Ma, 2017), joint alignment (Chen & Candès, 2018), learning shallow\nneural networks (Soltanolkotabi et al., 2017; Zhong et al., 2017). In several problems (Sun et al., 2016; 2017; Ge & Ma, 2017; Ge et al., 2016; Li et al., 2016b; Li & Tang, 2016; Mei et al., 2016; Maunu et al., 2017), it is further suggested that the optimization landscape is benign under sufficiently large sample complexity, in the sense that all local minima are globally optimal, and hence nonconvex iterative algorithms become promising in solving such problems.\nWhen it comes to noisy matrix completion, to the best of our knowledge, no rigorous guarantees have been established for gradient descent without explicit regularization. A notable exception is (Jin et al., 2016), which studies unregularized stochastic gradient descent for online matrix completion with fresh samples used in each iteration.\nFinally, we note that the notion of implicit regularization — broadly defined — arises in settings far beyond what considered herein. For instance, it has been in matrix factorization, over-parameterized stochastic gradient descent effectively enforces certain norm constraints, allowing it to converge to a minimal-norm solution as long as it starts from the origin (Li et al., 2017; Gunasekar et al., 2017). The stochastic gradient methods have also been shown to implicitly enforce Tikhonov regularization in several statistical learning settings (Lin et al., 2016). More broadly, this phenomenon seems crucial in enabling efficient training of deep neural networks (Neyshabur et al., 2017; Zhang et al., 2016a; Soudry et al., 2017; Keskar et al., 2016)."
  }, {
    "heading": "5. Discussions",
    "text": "This paper showcases an important phenomenon in nonconvex optimization: even without explicit enforcement of regularization, the vanilla form of gradient descent effectively achieves implicit regularization for a large family of statistical estimation problems. We believe this phenomenon arises in problems far beyond the two cases studied herein, and our results are initial steps towards understanding this fundamental phenomenon. That being said, there are numerous avenues that remain open. For instance, it remains unclear how to generalize the proposed leave-one-out tricks for more general designs beyond the i.i.d. Gaussian design. It would also be interesting to see whether the message conveyed in this paper can shed light on why simple forms of gradient descent and variants work so well in learning complicated neural networks. We leave these for future investigation."
  }, {
    "heading": "Acknowledgements",
    "text": "Y. Chi is supported in part by the grants AFOSR FA9550-151-0205, ONR N00014-18-1-2142, ARO grant W911NF-181-0303, NSF CCF-1527456 and ECCS-1818571. Y. Chen is supported by ARO grant W911NF-18-1-0303 and by Princeton SEAS innovation award.\nImplicit Regularization in Nonconvex Statistical Estimation"
  }],
  "year": 2018,
  "references": [{
    "title": "Entrywise eigenvector analysis of random matrices with low expected rank",
    "authors": ["E. Abbe", "J. Fan", "K. Wang", "Y. Zhong"],
    "venue": "arXiv preprint arXiv:1709.09565,",
    "year": 2017
  }, {
    "title": "Global optimality of local search for low rank matrix recovery",
    "authors": ["S. Bhojanapalli", "B. Neyshabur", "N. Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "A non-convex blind calibration method for randomised sensing strategies",
    "authors": ["V. Cambareri", "L. Jacques"],
    "venue": "arXiv preprint arXiv:1605.02615,",
    "year": 2016
  }, {
    "title": "Exact matrix completion via convex optimization",
    "authors": ["E.J. Candès", "B. Recht"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2009
  }, {
    "title": "Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming",
    "authors": ["E.J. Candès", "T. Strohmer", "V. Voroninski"],
    "venue": "Communications on Pure and Applied Mathematics,",
    "year": 2013
  }, {
    "title": "Phase retrieval via Wirtinger flow: Theory and algorithms",
    "authors": ["E.J. Candès", "X. Li", "M. Soltanolkotabi"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "The projected power method: An efficient algorithm for joint alignment from pairwise differences",
    "authors": ["Y. Chen", "E. Candès"],
    "venue": "Communications on Pure and Applied Mathematics,",
    "year": 2018
  }, {
    "title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems",
    "authors": ["Y. Chen", "E.J. Candès"],
    "venue": "Communications on Pure and Applied Mathematics,",
    "year": 2017
  }, {
    "title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
    "authors": ["Y. Chen", "M.J. Wainwright"],
    "year": 2015
  }, {
    "title": "Spectral method and regularized MLE are both optimal for top-K ranking",
    "authors": ["Y. Chen", "J. Fan", "C. Ma", "K. Wang"],
    "venue": "Annals of Statistics,",
    "year": 2017
  }, {
    "title": "Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval",
    "authors": ["Y. Chen", "Y. Chi", "J. Fan", "C. Ma"],
    "venue": "arXiv preprint arXiv:1803.07726,",
    "year": 2018
  }, {
    "title": "On the impact of predictor geometry on the performance on high-dimensional ridge-regularized generalized robust regression estimators",
    "authors": ["N. El Karoui"],
    "venue": "Probability Theory and Related Fields, pp",
    "year": 2015
  }, {
    "title": "On the optimization landscape of tensor decompositions",
    "authors": ["R. Ge", "T. Ma"],
    "venue": "arXiv preprint arXiv:1706.05598,",
    "year": 2017
  }, {
    "title": "Matrix completion has no spurious local minimum",
    "authors": ["R. Ge", "J.D. Lee", "T. Ma"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Implicit regularization in matrix factorization",
    "authors": ["S. Gunasekar", "B. Woodworth", "S. Bhojanapalli", "B. Neyshabur", "N. Srebro"],
    "venue": "arXiv preprint arXiv:1705.09280,",
    "year": 2017
  }, {
    "title": "Phase retrieval: An overview of recent developments",
    "authors": ["K. Jaganathan", "Y.C. Eldar", "B. Hassibi"],
    "venue": "arXiv preprint arXiv:1510.07713,",
    "year": 2015
  }, {
    "title": "Low-rank matrix completion using alternating minimization",
    "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"],
    "venue": "In ACM symposium on Theory of computing,",
    "year": 2013
  }, {
    "title": "De-biasing the lasso: Optimal sample size for Gaussian designs",
    "authors": ["A. Javanmard", "A. Montanari"],
    "venue": "arXiv preprint arXiv:1508.02757,",
    "year": 2015
  }, {
    "title": "Provable efficient online matrix completion via non-convex stochastic gradient descent",
    "authors": ["C. Jin", "S.M. Kakade", "P. Netrapalli"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Matrix completion from a few entries",
    "authors": ["R.H. Keshavan", "A. Montanari", "S. Oh"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2010
  }, {
    "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
    "authors": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"],
    "venue": "arXiv preprint arXiv:1609.04836,",
    "year": 2016
  }, {
    "title": "Nuclearnorm penalization and optimal rates for noisy low-rank matrix completion",
    "authors": ["V. Koltchinskii", "K. Lounici", "A.B. Tsybakov"],
    "venue": "Ann. Statist.,",
    "year": 2011
  }, {
    "title": "Blind recovery of sparse signals from subsampled convolution",
    "authors": ["K. Lee", "Y. Li", "M. Junge", "Y. Bresler"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2017
  }, {
    "title": "The nonconvex geometry of low-rank matrix optimizations with general objective functions",
    "authors": ["Q. Li", "G. Tang"],
    "venue": "arXiv preprint arXiv:1611.03060,",
    "year": 2016
  }, {
    "title": "Rapid, robust, and reliable blind deconvolution via nonconvex optimization. CoRR, abs/1606.04933, 2016a",
    "authors": ["X. Li", "S. Ling", "T. Strohmer", "K. Wei"],
    "venue": "URL http://arxiv. org/abs/1606.04933",
    "year": 2016
  }, {
    "title": "Symmetry, saddle points, and global geometry of nonconvex matrix factorization",
    "authors": ["X. Li", "Z. Wang", "J. Lu", "R. Arora", "J. Haupt", "H. Liu", "T. Zhao"],
    "venue": "arXiv preprint arXiv:1612.09296,",
    "year": 2016
  }, {
    "title": "Algorithmic regularization in over-parameterized matrix recovery",
    "authors": ["Y. Li", "T. Ma", "H. Zhang"],
    "venue": "pp. arXiv preprint arXiv:1712.09203,",
    "year": 2017
  }, {
    "title": "Generalization properties and implicit regularization for multiple passes SGM",
    "authors": ["J. Lin", "R. Camoriano", "L. Rosasco"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution",
    "authors": ["C. Ma", "K. Wang", "Y. Chi", "Y. Chen"],
    "venue": "arXiv preprint arXiv:1711.10467,",
    "year": 2017
  }, {
    "title": "A well-tempered landscape for non-convex robust subspace recovery",
    "authors": ["T. Maunu", "T. Zhang", "G. Lerman"],
    "venue": "arXiv preprint arXiv:1706.03896,",
    "year": 2017
  }, {
    "title": "The landscape of empirical risk for non-convex losses",
    "authors": ["S. Mei", "Y. Bai", "A. Montanari"],
    "venue": "arXiv preprint arXiv:1607.06534,",
    "year": 2016
  }, {
    "title": "Restricted strong convexity and weighted matrix completion: optimal bounds with noise",
    "authors": ["S. Negahban", "M.J. Wainwright"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2012
  }, {
    "title": "Phase retrieval using alternating minimization",
    "authors": ["P. Netrapalli", "P. Jain", "S. Sanghavi"],
    "venue": "Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Geometry of optimization and implicit regularization in deep learning",
    "authors": ["B. Neyshabur", "R. Tomioka", "R. Salakhutdinov", "N. Srebro"],
    "venue": "arXiv preprint arXiv:1705.03071,",
    "year": 2017
  }, {
    "title": "Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach",
    "authors": ["D. Park", "A. Kyrillidis", "C. Caramanis", "S. Sanghavi"],
    "venue": "arXiv preprint arXiv:1609.03240,",
    "year": 2016
  }, {
    "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
    "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"],
    "venue": "SIAM Review,",
    "year": 2010
  }, {
    "title": "Algorithms and Theory for Clustering and Nonconvex Quadratic Programming",
    "authors": ["M. Soltanolkotabi"],
    "venue": "PhD thesis, Stanford University,",
    "year": 2014
  }, {
    "title": "Theoretical insights into the optimization landscape of overparameterized shallow neural networks",
    "authors": ["M. Soltanolkotabi", "A. Javanmard", "J.D. Lee"],
    "venue": "arXiv preprint arXiv:1707.04926,",
    "year": 2017
  }, {
    "title": "The implicit bias of gradient descent on separable data",
    "authors": ["D. Soudry", "E. Hoffer", "N. Srebro"],
    "venue": "arXiv preprint arXiv:1710.10345,",
    "year": 2017
  }, {
    "title": "A geometric analysis of phase retrieval",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "In ISIT,",
    "year": 2016
  }, {
    "title": "Complete dictionary recovery over the sphere i: Overview and the geometric picture",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2017
  }, {
    "title": "Guaranteed matrix completion via non-convex factorization",
    "authors": ["R. Sun", "Luo", "Z.-Q"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2016
  }, {
    "title": "The likelihood ratio test in high-dimensional logistic regression is asymptotically a rescaled chi-square",
    "authors": ["P. Sur", "Y. Chen", "E.J. Candès"],
    "venue": "arXiv preprint arXiv:1706.01191,",
    "year": 2017
  }, {
    "title": "Introduction to the non-asymptotic analysis of random matrices",
    "authors": ["R. Vershynin"],
    "venue": "Compressed Sensing, Theory and Applications, pp",
    "year": 2012
  }, {
    "title": "Solving systems of random quadratic equations via truncated amplitude flow",
    "authors": ["G. Wang", "G.B. Giannakis", "Y.C. Eldar"],
    "venue": "IEEE Transactions on Information",
    "year": 2017
  }, {
    "title": "The local convexity of solving quadratic equations",
    "authors": ["C.D. White", "R. Ward", "S. Sanghavi"],
    "venue": "arXiv preprint arXiv:1506.07868,",
    "year": 2015
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "venue": "arXiv preprint arXiv:1611.03530,",
    "year": 2016
  }, {
    "title": "Provable non-convex phase retrieval with outliers: Median truncated Wirtinger flow",
    "authors": ["H. Zhang", "Y. Chi", "Y. Liang"],
    "venue": "In International conference on machine learning,",
    "year": 2016
  }, {
    "title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements",
    "authors": ["Q. Zheng", "J. Lafferty"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Convergence analysis for rectangular matrix completion using Burer-Monteiro factorization and gradient descent",
    "authors": ["Q. Zheng", "J. Lafferty"],
    "venue": "arXiv preprint arXiv:1605.07051,",
    "year": 2016
  }, {
    "title": "Recovery guarantees for one-hidden-layer neural networks",
    "authors": ["K. Zhong", "Z. Song", "P. Jain", "P.L. Bartlett", "I.S. Dhillon"],
    "year": 2017
  }, {
    "title": "Near-optimal bounds for phase synchronization",
    "authors": ["Y. Zhong", "N. Boumal"],
    "venue": "arXiv preprint arXiv:1703.06605,",
    "year": 2017
  }],
  "id": "SP:1e8e4468c5b2f3caa04234b7fa7f31a20aa8825d",
  "authors": [{
    "name": "Cong Ma",
    "affiliations": []
  }, {
    "name": "Kaizheng Wang",
    "affiliations": []
  }, {
    "name": "Yuejie Chi",
    "affiliations": []
  }, {
    "name": "Yuxin Chen",
    "affiliations": []
  }],
  "abstractText": "Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures require proper regularization (e.g. trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This “implicit regularization” feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e. solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors. Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Cong Ma <congm@princeton.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",
  "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion"
}