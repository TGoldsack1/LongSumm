{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Convolutional sparse coding (CSC) (Zeiler et al., 2010) has been successfully used in image processing (Gu et al., 2015; Heide et al., 2015), signal processing (Cogliati et al., 2016), and biomedical applications (Pachitariu et al., 2013; Andilla & Hamprecht, 2014; Chang et al., 2017; Jas et al., 2017; Peter et al., 2017). It is closely related to sparse coding (Aharon et al., 2006), but CSC is advantageous in that its shift-invariant dictionary can capture shifted local patterns common in signals and images. Each data sample is then represented by the sum of a set of filters from the dictionary convolved with the corresponding codes.\nTraditional CSC algorithms operate in the batch mode 1Department of Computer Science and Engineering, Hong Kong University of Science and Technology University, Hong Kong 24Paradigm Inc, Beijing, China. 3Department of Computer and Information Science, University of Macau, Macau. Correspondence to: jamesk@cse.ust.hk <James T. Kwok>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n(Kavukcuoglu et al., 2010; Zeiler et al., 2010; Bristow et al., 2013; Heide et al., 2015; Šorel & Šroubek, 2016; Wohlberg, 2016; Papyan et al., 2017), which take O(NK2P + NKP logP ) time and O(NKP ) space (where N is the number of samples, K is the number of filters, and P is the dimensionality. Recently, a number of online CSC algorithms have been proposed for better scalability (Degraux et al., 2017; Liu et al., 2017; Wang et al., 2018). As data samples arrive, relevant information is compressed into small history statistics, and the model is incrementally updated. In particular, the state-of-the-art OCSC algorithm (Wang et al., 2018) has the much smaller time and space complexities of O(K2P +KP logP ) and O(K2P ), respectively.\nHowever, the complexities of OCSC still depend quadratically on K, and cannot be used with a large number of filters. The number of local patterns that can be captured is thus limited, and may lead to inferior performance especially on higher-dimensional data sets. Besides, the use of more filters also leads to a larger number of expensive convolution operations. Rigamonti et al. (2013) and Sironi et al. (2015) proposed to post-process the learned filters by approximating them with separable filters, making the convolutions less expensive. However, as learning and post-processing are then two independent procedures, the resultant filters may not be optimal. Moreover, these separate filters cannot be updated online with the arrival of new samples.\nAnother direction to scale up CSC is via distributed computation (Bertsekas & Tsitsiklis, 1997). By distributing the data and workload onto multiple machines, the recent consensus CSC algorithm (Choudhury et al., 2017) can handle large, higher-dimensional data sets such as videos, multispectral images and light field images. However, the heavy computational demands of the CSC problem are only shared over the computing platform, but not fundamentally reduced.\nIn this paper, we propose to approximate the possibly large number of filters by a sample-dependent combination of a small set of base filters learned from the data. While the standard CSC dictionary is shared by all samples, we propose each sample to have its own “personal” dictionary to compensate for the reduced flexibility of using these\nbase filters. In this way, the representation power can remain the same but with a reduced number of parameters. Computationally, this structure also allows efficient online learning algorithms to be developed. Specifically, the base filter can be updated by the alternating direction method of multipliers (ADMM) (Boyd et al., 2011), while the codes and combination weights can be learned by accelerated proximal algorithms (Yao et al., 2017). Extensive experimental results on a variety of data sets show that the proposed algorithm is more efficient in both time and space, and outperforms existing batch, online and distributed CSC algorithms.\nThe rest of the paper is organized as follows. Section 2 briefly reviews convolutional sparse coding. Section 3 describes the proposed algorithm. Experimental results are presented in Section 4, and the last section gives some concluding remarks.\nNotations: For a vector a, its ith element is a(i), `1-norm is kak1 = P i |a(i)| and `2-norm is kak2 = pP i a\n2(i). The convolution of two vectors a and b is denoted a ⇤ b. For a matrix A, A? is its complex conjugate, and A† = (A>)? its conjugate transpose. The Hadamard product of two matrices A and B is (A B)(i, j) = A(i, j)B(i, j). The identity matrix is denoted I . F(x) is the Fourier transform that maps x from the spatial domain to the frequency domain, while F 1(x) is the inverse operator which maps F(x) back to x."
  }, {
    "heading": "2. Review: Convolutional Sparse Coding",
    "text": "Given samples {x1, . . . , xN} in RP , CSC learns a shiftinvariant dictionary D 2 RM⇥K , with the columns {D(: , k)} representing the K filters. Each sample xi is encoded as Zi 2 RP⇥K , with the kth column being the code convolved with filter D(:, k). The dictionary and codes are learned together by solving the optimization problem:\nmin D2D,{Zi}\n1\nN\nNX\ni=1\n1\n2\nxi KX\nk=1\nD(:, k)⇤Zi(:, k)\n2\n2\n+ kZik1,\n(1) where the first term measures the signal reconstruction error, D = {D : kD(:, k)k2  1, 8k = 1, . . . ,K} ensures that the filters are normalized, and > 0 is a regularization parameter controlling the sparsity of Zi’s.\nConvolution in (1) is performed in the spatial domain. This takes O(KPM) time, and is expensive. In contrast, recent CSC methods perform convolution in the frequency domain, which takes O(KP logP ) time (Mallat, 1999) and is faster for typical choices of M and P . Let x̃i ⌘ F(xi), D̃(:, k) ⌘ F(D(:, k)), and Z̃i(:, k) ⌘ F(Zi(:, k)) be the Fourier-transformed counterparts of xi, D(:, k) and Zi(:, k). The codes and filters are updated in an alternating manner by block coordinate descent, as:\nUpdate Codes: Given D̃, each Z̃i is independently obtained as\nmin Z̃i,Ui\n1\n2P\nx̃i KX\nk=1\nD̃(:, k) Z̃i(:, k)\n2\n2\n+ kUik1 (2)\ns.t. F(Ui(:, k)) = Z̃i(:, k), k = 1, . . . ,K,\nwhere Ui is an auxiliary variable.\nUpdate Dictionary: D̃ is updated by solving\nminD̃,V 1\n2NP\nNX\ni=1\nx̃i KX\nk=1\nD̃(:, k) Z̃i(:, k)\n2\n2\n(3)\ns.t. F(V (:, k)) = D̃(:, k), k = 1, . . . ,K, kC(V (:, k))k22  1, k = 1, . . . ,K,\nwhere V is an auxiliary variable, and C(·) crops the extra P M dimensions in V (:, k).\nBoth (2) and (3) can be solved by the alternating direction method of multipliers (ADMM) (Boyd et al., 2011). Subsequently, {Z̃i} and D̃ can be transformed back to the spatial domain as Zi(:, k) = F 1(Z̃i(:, k)) and D(: , k) = C(F 1(D̃(:, k))). Note that while Zi’s (in the spatial domain) are sparse, the FFT-transformed Z̃i’s are not.\nOn inference, given the learned dictionary D, the testing sample xj is reconstructed as PK k=1 D(:, k) ⇤ Zj(:, k), where Zj is the obtained code."
  }, {
    "heading": "2.1. Post-Processing for Separable Filters",
    "text": "Filters obtained by CSC are non-separable and subsequent convolutions may be slow. To speed this up, they can be postprocessed and approximated by separable filters (Rigamonti et al., 2013; Sironi et al., 2015). Specifically, the learned D is approximated by SW , where S 2 RM⇥R contains R rank-1 base filters {S(:, 1), . . . , S(:, R)}, and W 2 RR⇥K contains the combination weights. However, this often leads to performance deterioration."
  }, {
    "heading": "2.2. Online CSC",
    "text": "An online CSC algorithm (OCSC) is recently proposed in (Wang et al., 2018). Given the Fourier-transformed sample x̃t and dictionary D̃t 1 from the last iteration, the corresponding {Z̃t, Ut} are obtained as in (2). The following Proposition updates D̃t and Vt by reformulating (3) for use with smaller history statistics.\nProposition 1 ((Wang et al., 2018)). D̃t, Vt can be obtained\nby solving the optimization problem:\nminD̃,V 1\n2P\nPX\np=1\nD̃(p, :)Ht(:, :, p)D̃ †(:, p)\n2D̃(p, :)Gt(:, p) (4) s.t. F(V (:, k)) = D̃(:, k), k = 1, . . . ,K,\nkC(V (:, k))k22  1, k = 1, . . . ,K,\nwhere Ht(:, :, p)= 1 t Pt i=1 Z̃ > i (:, p)Z̃ ? i (p, :)2RK⇥K , and Gt(:, p)= 1 t Pt i=1 x̃ ? i (p)Z̃ > i (:, p)2RK .\nProblem (4) can be solved by ADMM. The total space complexity is only O(K2P ), which is independent of N . Moreover, Ht and Gt can be updated incrementally.\nTwo other online CSC reformulations have also been proposed recently. Degraux et al. (2017) performs convolution in the spatial domain, and is slow. Liu et al. (2017) performs convolution in the frequency domain, but requires expensive huge sparse matrix operations."
  }, {
    "heading": "3. Online CSC with Sample-Dependent Dictionary",
    "text": "Though OCSC scales well with sample size N , its space complexity still depends on K quadratically. This limits the number of filters that can be used and can impact performance. Motivated by the ideas of separable filters in Section 2.1, we enable learning with more filters by approximating the K filters with R base filters, where R ⌧ K. In contrast to the separable filters, which are obtained by post-processing and may not be optimal, we propose to learn the dictionary directly during signal reconstruction. Moreover, filters in the dictionary are combined from the base filters in a sample-dependent manner."
  }, {
    "heading": "3.1. Problem Formulation",
    "text": "Recall that each xi in (1) is represented by PK\nk=1 D(:, k) ⇤ Zi(:, k). Let B 2 RM⇥R, with columns {B(:, r)} being the base filters. We propose to represent xi as:\nxi = KX\nk=1\nRX\nr=1\nWi(r, k)B(:, r) ! ⇤ Zi(:, k), (5)\nwhere Wi 2 RR⇥K is the matrix for the filter combination weights. In other words, each D(:, k) in (1) is replaced byPR\nr=1 Wi(r, k)B(:, r), or equivalently,\nDi = BWi, (6)\nwhich is sample-dependent. As will be seen, this allows the Wi’s to be learned independently (Section 3.3). This also leads to more sample-dependent patterns being captured and thus better performance (Section 4.4).\nSample-dependent filters have been recently studied in convolutional neural networks (CNN) (Jia et al., 2016). Empirically, this outperforms standard CNNs in one-shot learning (Bertinetto et al., 2016), video prediction (Jia et al., 2016) and image deblurring (Kang et al., 2017). Jia et al. (2016) uses a specially designed neural network to learn the filters, and does not consider the CSC model. In contrast, the sample-dependent filters here are integrated into CSC.\nThe dictionary can also be adapted to individual samples by fine-tuning (Donahue et al., 2014). However, learning the initial shared dictionary is still expensive when K is large. Besides, as will be shown in Section 4.2, the proposed method outperforms fine-tuning empirically."
  }, {
    "heading": "3.2. Learning",
    "text": "Plugging (6) into the CSC formulation in (1), we obtain\nminB,{Wi,Zi} 1\nN\nNX\ni=1\nfi(B,Wi, Zi) + kZik1 (7)\ns.t. BWi 2 D, i = 1, . . . , N, (8) B 2 B, (9)\nwhere\nfi(B,Wi, Zi)⌘ 1\n2\nxi KX\nk=1\nRX\nr=1\nWi(r, k)B(:, r) ! ⇤Zi(:, k) 2\n2\n,\nand B ⌘ {B : kB(:, r)k2  1, 8 r = 1, . . . , R}. As B and Wi are coupled together in (8), this makes the optimization problem difficult. The following Proposition decouples B and Wi. All the proofs are in the Appendix.\nProposition 2. For B 2 B, we have BWi 2 D if (i) Wi 2 W`1 ⌘ {W : kWi(:, k)k1  1, k = 1, . . . ,K}, or (ii) Wi 2 W`2 ⌘ {W : kWi(:, k)k2  1/ p R, k = 1, . . . ,K}.\nTo simplify notations, we use W to denote W`1 or W`2. By imposing either one of the above structures on {Wi}, we have the following optimization problem:\nminB,{Wi,Zi} 1\nN\nNX\ni=1\nfi(B,Wi, Zi) + kZik1 (10)\ns.t. B 2 B, and Wi 2 W, i = 1, . . . , N.\nOn inference with sample xj , the corresponding (Wj , Zj) can be obtained by solving (10) with the learned B fixed."
  }, {
    "heading": "3.3. Online Learning Algorithm for (10)",
    "text": "As in Section 2.2, we propose an online algorithm for better scalability. At the tth iteration, consider\nminB,{Wi,Zi} 1\nt\ntX\ni=1\nfi(B,Wi, Zi) + kZik1 (11)\ns.t. B 2 B, and Wi 2 W, i = 1, . . . , N.\nLet B̃(:, r) ⌘ F(B(:, r)), where B(:, r) 2 RM is zeropadded to be P -dimensional. Note that the number of convolutions can be reduced from K to R by rewriting the summation above as PR r=1 B(:, r) ⇤ ( PK k=1 Wi(r, k)Zi(: , k)). The following Proposition rewrites (11) and performs convolutions in the frequency domain. Proposition 3. Problem (11) can be rewritten as\nminB̃,{Wi,Zi} 1\nt\ntX\ni=1\nf̃i(B̃,Wi, Zi)+ kZik1 (12)\ns.t. kC(F 1(B̃(:, r)))k21, r = 1, . . . , R, Wi 2 W, i = 1, . . . , N,\nwhere f̃i(B̃,Wi, Zi) ⌘ 12P kx̃i PR\nr=1 B̃(:, r) Ỹi(:, r)k22, and Ỹi(:, r) ⌘ F(ZiW>i (:, r)).\nThe spatial-domain base filters can be recovered from B̃ as B(:, r) = C(F 1(B̃(:, r))).\n3.3.1. OBTAINING B̃t\nFrom (12), B̃t can be obtained by solving the subproblem:\nminB̃,V̄ 1\n2tP\ntX\ni=1\nx̃i RX\nr=1\nB̃(:, r) Ỹi(:, r)\n2\n2\ns.t. F(V̄ (:, r)) = B̃(:, r), r = 1, . . . , R, kC(V̄ (:, r))k2  1, r = 1, . . . , R,\nwhere V̄ is an auxiliary variable. This is of the same form as (3). Hence, analogous to (4), B̃t can be obtained as:\nminB̃,V̄ 1\n2tP\ntX\ni=1\nPX\np=1\nB̃(p, :)H̄t(:, :, p)B̃ †(:, p)\n2B̃(p, :)Ḡt(:, p) (13) s.t. F(V̄ (:, r)) = B̃(:, r), r = 1, . . . , R,\nkC(V̄ (:, r))k2  1, r = 1, . . . , R, where H̄t(:, :, p)=1t Pt i=1 Ỹ > i (:, p)Ỹ ? i (p, :) 2 RR⇥R, and Ḡt(:, p) = 1 t Pt i=1 x̃ ? i (p)Ỹ > i (:, p) 2 RR. They can be incrementally updated as\nH̄t(:, :, p) = t 1 t H̄t 1(:, :, p)+ 1 t Ỹ >t (:, p)Ỹ ? t (p, :), (14)\nḠt(:, p) = t 1 t Ḡt 1(:, p) + 1 t x̃?t (p)Ỹ > t (:, p). (15)\nProblem (13) can then be solved using ADMM as in (4).\n3.3.2. OBTAINING Wt AND Zt\nWith the arrival of xt, we fix the base filters to B̃t 1 learned at the last iteration, and obtain (Wt, Zt) from (12) as:\nmin W,Z F (W,Z)⌘ f̃t(B̃t 1,W,Z)+IW(W )+ kZk1, (16)\nwhere IW(W ) is the indicator function on W (i.e., IW(W ) = 0 if W 2 W and 1 otherwise).\nAs in the CSC literature, it can be shown that ADMM can also be used to solve (16). While CSC’s code update subproblem in (2) is convex, problem (16) is nonconvex and existing convergence results for ADMM (Wang et al., 2015) do not apply.\nIn this paper, we will instead use the nonconvex and inexact accelerated proximal gradient (niAPG) algorithm (Yao et al., 2017). This is a recent proximal algorithm for nonconvex problems. As the regularizers on W and Z in (16) are independent, the proximal step w.r.t. the two blocks can be performed separately as: (proxIW(·)(W ), prox k·k1(Z)) (Parikh & Boyd, 2014). As shown in (Parikh & Boyd, 2014), these individual proximal steps can be easily computed (for W = W`1 or W`2)."
  }, {
    "heading": "3.3.3. COMPLETE ALGORITHM",
    "text": "The whole procedure, which will be called “Sampledependent Convolutional Sparse Coding (SCSC)”, is shown in Algorithm 1. Its space complexity, which is dominated by H̄t and Ḡt, is O(R2P ). Its per-iteration time complexity is O(RKP + RP logP ), where the O(RKP ) term is due to gradient computation, and O(RP logP ) is due to FFT/inverse FFT. Table 1 compares its complexities with those of the other online and distributed CSC algorithms. As can be seen, SCSC has much lower time and space complexities as R ⌧ K."
  }, {
    "heading": "4. Experiments",
    "text": "Experiments are performed on a number of data sets (Table 2). Fruit and City are two small image data sets that have been commonly used in the CSC literature (Zeiler et al., 2010; Bristow et al., 2013; Heide et al., 2015; Papyan et al., 2017). We use the default training and testing splits provided in (Bristow et al., 2013). The images are preprocessed as in (Zeiler et al., 2010; Heide et al., 2015; Wang et al., 2018), which includes conversion to grayscale, feature standardization, local contrast normalization and edge tapering. These two data sets are small. In some experiments, we will also use two larger data sets, CIFAR10 (Krizhevsky & Hinton, 2009) and Flower (Nilsback & Zisserman, 2008). Following (Heide et al., 2015; Choudhury et al., 2017; Papyan et al., 2017; Wang et al., 2018), we set the filter size M as 11⇥11, and the regularization parameter\nAlgorithm 1 Sample-dependent CSC (SCSC). 1: Initialize W0 2 W , B0 2 B, H̄0 = 0, Ḡ0 = 0; 2: for t = 1, 2, . . . , T do 3: draw xt from {xi}; 4: x̃t = F(xt); 5: obtain Wt, Zt using niAPG; 6: for r = 1, 2, . . . , R do 7: Ỹt(:, r) = F(ZtW>t (:, r)); 8: end for 9: update {H̄t(:, :, 1), . . . , H̄t(:, :, P )} using (14); 10: update {Ḡt(:, 1), . . . , Ḡt(:, P )} using (15); 11: update B̃t by (13) using ADMM; 12: end for 13: for r = 1, 2, . . . , R do 14: BT (:, r) = C(F 1(B̃T (:, r))); 15: end for output BT .\nin (1) as 1.\nTo evaluate efficacy of the learned dictionary, we will mainly consider the task of image reconstruction as in (Aharon et al., 2006; Heide et al., 2015; Sironi et al., 2015). The reconstructed image quality is evaluated by the testing peak signal-to-noise ratio (Papyan et al., 2017): PSNR = 1|⌦| P xj2⌦ 20 log10 ⇣ p P kx̂j xjk2 ⌘ , where x̂j is the reconstruction of xj from test set ⌦. The experiment is repeated five times with different dictionary initializations.\n4.1. Choice of W : W`1 versus W`2\nFirst, we study the choice of W in Proposition 2. We compare SCSC-L1, which uses W = W`1, with SCSCL2, which uses W = W`2. Experiments are performed on Fruit and City. As in (Heide et al., 2015; Papyan et al., 2017; Wang et al., 2018), the number of filters K is set to\n100. Recall the space complexity results in Table 1, we define the compression ratio of SCSC relative to OCSC (using the same K) as CR = (K/R)2. We vary R in {K/20,K/10,K/9, . . . ,K/2,K}. The corresponding CR is {400, 100, 81, . . . , 1}.\nResults are shown in Figure 1. As can be seen, SCSC-L1 is much inferior. Figure 2(a) shows the weight Wj obtained with K = 100 and R = 10 by SCSC-L1 on a test sample xj from City (results on the other data sets are similar). As can be seen, most of its entries are zero because of the sparsity induced by the `1 norm. The expressive power is severely limited as typically only one base filter is used to approximate the original filter. On the other hand, the Wj learned by SCSC-L2 is dense and has more nonzero entries (Figure 2(b)). In the sequel, we will only focus on SCSC-L2, which will be simply denoted as SCSC."
  }, {
    "heading": "4.2. Sample-Dependent Dictionary",
    "text": "In this experiment, we set K = 100, and compare SCSC with the following algorithms that use sample-independent dictionaries: (i) SCSC (shared): This is a SCSC variant in which all Wi’s in (5) are the same. Its optimization is based on alternating minimization. (ii) Separable filters learned by tensor decomposition (SEP-TD) (Sironi et al., 2015), which\nis based on post-processing the (shared) dictionary learned by OCSC as reviewed in Section 2.1; (iii) OCSC (Wang et al., 2018): the state-of-the-art online CSC algorithm.\nResults are shown in Figure 3. As can be seen, SCSC always outperforms SCSC(shared) and SEP-TD, and outperforms OCSC when R = 10 (corresponding to CR = 100) or above. This demonstrates the advantage of using a sampledependent dictionary.\nNext, we compare against OCSC with fine-tuned filters, which are also sample-dependent. Specifically, given test sample xj , we first obtain its code Zj from (2) with the learned dictionary D, and then fine-tune D by solving (3) using the newly computed Zj . As in (Donahue et al., 2014), this is repeated for a few iterations.1 We set OCSC’s K to be equal to SCSC’s R, so that the two methods take the same space (Table 1). The K used in SCSC is still 100. Results are shown in Figure 4. As can be seen, though fine-tuning improves the performance of OCSC slightly, this approach of generating sample-dependent filters is still much worse than SCSC."
  }, {
    "heading": "4.3. Learning with More Filters",
    "text": "Recall that SCSC allows the use of more filters (i.e., a larger K) because of its lower time and space complexities. In this Section, we demonstrate that this can lead to better performance. We compare SCSC with two most recent batch and online CSC methods, namely, slice-based CSC\n1In the experiments, we stop after five iterations.\n(SBCSC) (Papyan et al., 2017) and OCSC. For SCSC, we set R = 10 for Fruit and City, and R = 30 for CIFAR-10 and Flower.\nFigure 5 shows the testing PSNR’s at different K’s. As can be seen, a larger K consistently leads to better performance for all methods. SCSC allows the use of a larger K because of its much smaller memory footprint. For example, on CIFAR-10, CR = 1024 at K = 800; on Flower, CR = 1600 at K = 400."
  }, {
    "heading": "4.4. Comparison with the State-of-the-Art",
    "text": "First, we perform experiments on the two smaller data sets of Fruit and City, with K = 100. We set R = 10 (i.e., CR = 100) for SCSC. This is compared with the batch CSC algorithms, including (i) deconvolution network (DeconvNet) (Zeiler et al., 2010), (ii) fast CSC (FCSC) (Bristow et al., 2013), (iii) fast and flexible CSC (FFCSC) (Heide et al., 2015), (iv) convolutional basis pursuit denoising (CBPDN) (Wohlberg, 2016), (v) the CONSENSUS algorithm (Šorel & Šroubek, 2016), and (vi) slice-based CSC (SBCSC) (Papyan et al., 2017). We also compare with the online CSC algorithms, including (vii) OCSC (Wang et al., 2018), (viii) OCDL-Degraux (Degraux et al., 2017), and (ix) OCDL-Liu (Liu et al., 2017).\nFigure 6 shows convergence of the testing PSNR with clock time. As also demonstrated in (Degraux et al., 2017; Liu et al., 2017; Wang et al., 2018), online CSC methods converge faster and have better PSNR than batch CSC methods. Among the online methods, SCSC has comparable PSNR as OCSC, but is faster and requires much less storage (CR = 100).\nNext, we perform experiments on the two large data sets, CIFAR-10 and Flower. All the batch CSC algorithms and two online CSC algorithms, OCDL-Degraux and OCDLLiu, cannot handle such large data sets. Hence, we will only compare SCSC with OCSC. On CIFAR-10, we set K = 300, and the corresponding CR for SCSC is 100. On Flower, K is still 300 for SCSC. However, OCSC can only use K = 50 because of its much larger memory footprint. Figure 7 shows convergence of the testing PSNR. In both cases, SCSC significantly outperforms OCSC."
  }, {
    "heading": "4.5. Higher-Dimensional Data",
    "text": "In this section, we perform experiments on data sets with dimensionalities larger than two. To alleviate the large memory problem, Choudhury et al. (2017) proposed the use of distributed algorithms. Here, we show that SCSC can effectively handle these data sets using one single machine.\nExperiments are performed on three data sets (Table 3) in (Choudhury et al., 2017). The Video data set contains image subsequences recorded in an airport (Li et al., 2004). The length of each video is 7, and each image frame is of size 100⇥ 100. The Multispectral data contains 60⇥ 60 patches from multispectral images (covering 31 wavelengths) of real-world objects and materials (Yasuma et al., 2010). The Light field data contains 60⇥60 patches of light field images on objects and scenes (Kalantari et al., 2016). For each pixel, the light rays are from 8 ⇥ 8 different directions. Following (Choudhury et al., 2017), we set the filter size M\nto 11⇥ 11⇥ 11 for Video, 11⇥ 11⇥ 31 for Multispectral, and 11⇥ 11⇥ 8⇥ 8 for Light field.\nWe compare SCSC with OCSC and the concensus CSC (CCSC) (Choudhury et al., 2017) algorithms, with K = 50. For fair comparison, only one machine is used for all methods. We do not compare with the batch methods and the two online methods (OCDL-Degraux and OCDL-Liu) as they are not scalable (as already shown in Section 4.4).\nBecause of the small memory footprint of SCSC, we run it on a GTX 1080 Ti GPU in this experiment. OCSC is also run on GPU for Video. However, OCSC can only run on CPU for Multispectral and Light field. CCSC, which needs to access all the samples and codes during processing, can only be on CPU.2\nResults are shown in Table 4. Note that SCSC is the only method that can handle the whole of Video, Multispectral and Light field data sets on a single machine. In comparison, CCSC can only handle a maximum of 30 Video samples, 40 Multispectral samples, and 35 Light field samples. OCSC can handle the whole of Video and Multispectral, but cannot converge in 2 days when the whole Light field data set is used. Again, SCSC outperforms OCSC and CCSC.\nAs for speed, SCSC is the fastest. However, note that this is for reference only as SCSC is run on GPU while the others (except for OCSC on Video) are run on CPU. Nevertheless, this still demonstrates an important advantage of SCSC, namely that its small memory footprint can benefit from the use of GPU, while the others cannot."
  }, {
    "heading": "4.6. Image Denoising and Inpainting",
    "text": "In previous experiments, superiority of the learned dictionary is demonstrated by reconstruction of clean images. In this section, we further examine the learned dictionary on two applications: image denoising and inpainting. Ten test images provided by (Choudhury et al., 2017) are used. In denoising, we add Gaussian noise with zero mean and variance 0.01 to the test images (the average input PSNR is 10dB). In inpainting, we random sub-sample 50% of the pixels as 0 (the average input PSNR is 9.12dB). Following (Heide et al., 2015; Choudhury et al., 2017; Papyan et al.,\n2For Video, the memory used (in GB) by CCSC, OCSC, SCSC (with R = 5) and SCSC (with R = 10) are 28.73, 7.58, 2.66, and 2.87, respectively. On Multispectral, they are 28.26, 11.09, 0.73 and 0.76; on Light field, they are 29.79, 15.94, 7.26 and 8.88, respectively.\n2017), we use a binary weight matrix to mask out positions of the missing pixels. We use the filters learned from Fruit in Section 4.4. SCSC is compared with (batch) SBCSC and (online) OCSC.\nResults are shown in Table 5. As can be seen, the PSNRs obtained by SCSC are consistently higher than those by the other methods. This shows that the dictionary, which yields high PSNR on image reconstruction, also leads to better performance in other image processing applications.\n4.7. Solving (16): niAPG vs ADMM\nFinally, we compare the performance of ADMM and niAPG in solving subproblem (16). We use a training sample xi from City. The experiment is repeated five times with different (Wi, Zi) initializations. Figure 8 shows convergence of the objective in (16) with time. As can be seen, niAPG has fast convergence while ADMM fails to converge. Figure 9 shows kỸi F(ZiW>i )k2F , which measures violation of the ADMM constraints, with the number of iterations. As can be seen, the violation does not go to zero, which indicates that ADMM does not converge."
  }, {
    "heading": "5. Conclusion",
    "text": "In this paper, we proposed a novel CSC extension, in which each sample has its own sample-dependent dictionary constructed from a small set of shared base filters. Using online learning, the model can be efficiently updated with low time and space complexities. Extensive experiments on a variety of data sets including large image data sets and\nFigure 8. Convergence of niAPG and ADMM on solving (16).\nFigure 9. Constraint violation in ADMM.\nhigher-dimensional data sets all demonstrate its efficiency and scalability."
  }, {
    "heading": "Acknowledgements",
    "text": "The second author especially thanks Weiwei Tu and Yuqiang Chen from 4Paradigm Inc. This research was supported in part by the Research Grants Council, Hong Kong, under Grant 614513, and by the University of Macau Grant SRG2015-00050-FST."
  }],
  "year": 2018,
  "references": [{
    "title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation",
    "authors": ["M. Aharon", "M. Elad", "A. Bruckstein"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2006
  }, {
    "title": "Sparse space-time deconvolution for calcium image analysis",
    "authors": ["F. Andilla", "F. Hamprecht"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Learning feed-forward one-shot learners",
    "authors": ["L. Bertinetto", "J.F. Henriques", "J. Valmadre", "P. Torr", "A. Vedaldi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Parallel and Distributed Computation: Numerical Methods",
    "authors": ["D. Bertsekas", "J. Tsitsiklis"],
    "venue": "Athena Scientific,",
    "year": 1997
  }, {
    "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
    "authors": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "Fast convolutional sparse coding",
    "authors": ["H. Bristow", "A. Eriksson", "S. Lucey"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2013
  }, {
    "title": "Consensus convolutional sparse coding",
    "authors": ["B. Choudhury", "R. Swanson", "F. Heide", "G. Wetzstein", "W. Heidrich"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Context-dependent piano music transcription with convolutional sparse coding",
    "authors": ["A. Cogliati", "Z. Duan", "B. Wohlberg"],
    "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing,",
    "year": 2016
  }, {
    "title": "Online convolutional dictionary learning for multimodal imaging",
    "authors": ["K. Degraux", "U.S. Kamilov", "P.T. Boufounos", "D. Liu"],
    "venue": "In IEEE International Conference on Image Processing,",
    "year": 2017
  }, {
    "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
    "authors": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Convolutional sparse coding for image super-resolution",
    "authors": ["S. Gu", "W. Zuo", "Q. Xie", "D. Meng", "X. Feng", "L. Zhang"],
    "venue": "In International Conference on Computer Vision, pp",
    "year": 2015
  }, {
    "title": "Fast and flexible convolutional sparse coding",
    "authors": ["F. Heide", "W. Heidrich", "G. Wetzstein"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Learning the morphology of brain signals using alphastable convolutional sparse coding",
    "authors": ["M. Jas", "T.D. La Tour", "U. Simsekli", "A. Gramfort"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Dynamic filter networks",
    "authors": ["X. Jia", "B. De Brabandere", "T. Tuytelaars", "L.V. Gool"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Learningbased view synthesis for light field cameras",
    "authors": ["N.K. Kalantari", "T. Wang", "R. Ramamoorthi"],
    "venue": "ACM Transactions on Graphics,",
    "year": 2016
  }, {
    "title": "Incorporating side information by adaptive convolution",
    "authors": ["D. Kang", "D. Dhar", "A. Chan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Learning convolutional feature hierarchies for visual recognition",
    "authors": ["K. Kavukcuoglu", "P. Sermanet", "Y. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "venue": "Technical report, University of Toronto,",
    "year": 2009
  }, {
    "title": "Statistical modeling of complex backgrounds for foreground object detection",
    "authors": ["L. Li", "W. Huang", "I.Y. Gu", "Q. Tian"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2004
  }, {
    "title": "Online convolutional dictionary learning",
    "authors": ["J. Liu", "C. Garcia-Cardona", "B. Wohlberg", "W. Yin"],
    "venue": "In IEEE International Conference on Image Processing,",
    "year": 2017
  }, {
    "title": "A Wavelet Tour of Signal Processing",
    "authors": ["S. Mallat"],
    "year": 1999
  }, {
    "title": "Convolutional dictionary learning via local processing",
    "authors": ["V. Papyan", "Y. Romano", "J. Sulam", "M. Elad"],
    "venue": "In International Conference on Computer Vision,",
    "year": 2017
  }, {
    "title": "Sparse convolutional coding for neuronal assembly detection",
    "authors": ["S. Peter", "E. Kirschbaum", "M. Both", "L. Campbell", "B. Harvey", "C. Heins", "D. Durstewitz", "F. Diego", "F.A. Hamprecht"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Learning separable filters",
    "authors": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2013
  }, {
    "title": "Learning separable filters",
    "authors": ["A. Sironi", "B. Tekin", "R. Rigamonti", "V. Lepetit", "P. Fua"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }, {
    "title": "Fast convolutional sparse coding using matrix inversion lemma",
    "authors": ["M. Šorel", "F. Šroubek"],
    "venue": "Digital Signal Processing,",
    "year": 2016
  }, {
    "title": "Global convergence of ADMM in nonconvex nonsmooth optimization",
    "authors": ["Y. Wang", "W. Yin", "J. Zeng"],
    "venue": "arXiv preprint arXiv:1511.06324,",
    "year": 2015
  }, {
    "title": "Scalable online convolutional sparse coding",
    "authors": ["Y. Wang", "Q. Yao", "J.T. Kwok", "L.M. Ni"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2018
  }, {
    "title": "Efficient algorithms for convolutional sparse representations",
    "authors": ["B. Wohlberg"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2016
  }, {
    "title": "Efficient inexact proximal gradient algorithm for nonconvex problems",
    "authors": ["Q. Yao", "J. Kwok", "F. Gao", "W. Chen", "T.Y. Liu"],
    "venue": "In International Joint Conferences on Artifical Intelligence,",
    "year": 2017
  }, {
    "title": "Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum",
    "authors": ["F. Yasuma", "T. Mitsunaga", "D. Iso", "S.K. Nayar"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2010
  }],
  "id": "SP:41229b5495f37bcc4b6ddbfaebbee14377860655",
  "authors": [{
    "name": "Yaqing Wang",
    "affiliations": []
  }, {
    "name": "Quanming Yao",
    "affiliations": []
  }, {
    "name": "James T. Kwok",
    "affiliations": []
  }, {
    "name": "Lionel M. Ni",
    "affiliations": []
  }],
  "abstractText": "Convolutional sparse coding (CSC) has been popularly used for the learning of shift-invariant dictionaries in image and signal processing. However, existing methods have limited scalability. In this paper, instead of convolving with a dictionary shared by all samples, we propose the use of a sample-dependent dictionary in which each filter is a linear combination of a small set of base filters learned from data. This added flexibility allows a large number of sample-dependent patterns to be captured, which is especially useful in the handling of large or high-dimensional data sets. Computationally, the resultant model can be efficiently learned by online learning. Extensive experimental results on a number of data sets show that the proposed method outperforms existing CSC algorithms with significantly reduced time and space complexities.",
  "title": "Online Convolutional Sparse Coding with Sample-Dependent Dictionary"
}