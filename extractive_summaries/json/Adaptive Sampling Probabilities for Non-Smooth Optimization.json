{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Identifying and adapting to structural aspects of problem data can often improve performance of optimization algorithms. In this paper, we study two forms of such structure: variance in the relative importance of different features and observations (as well as blocks thereof). As a motivating concrete example, consider the `\np\nregression problem\nminimize\nx\n(\nf(x) := kAx bkp p =\nn\nX\ni=1\n|aT i x b i\n|p ) , (1)\nwhere a i denote the rows of A 2 Rn⇥d. When the columns (features) of A have highly varying norms—say because\n1Management Science & Engineering, Stanford University, USA 2Electrical Engineering, Stanford University, USA 3Statistics, Stanford University, USA. Correspondence to: Hongseok Namkoong <hnamk@stanford.edu>, Aman Sinha <amans@stanford.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ncertain features are infrequent—we wish to leverage this during optimization. Likewise, when rows a\ni have disparate norms, “heavy” rows of A influence the objective more than others. We develop optimization algorithms that automatically adapt to such irregularities for general nonsmooth convex optimization problems.\nStandard (stochastic) subgradient methods (Nemirovski et al., 2009), as well as more recent accelerated variants for smooth, strongly convex incremental optimization problems (e.g. Johnson and Zhang, 2013; Defazio et al., 2014), follow deterministic or random procedures that choose data to use to compute updates in ways that are oblivious to conditioning and structure. As our experiments demonstrate, choosing blocks of features or observations—for instance, all examples belonging to a particular class in classification problems—can be advantageous. Adapting to such structure can lead to substantial gains, and we propose a method that adaptively updates the sampling probabilities from which it draws blocks of features/observations (columns/rows in problem (1)) as it performs subgradient updates. Our method applies to both coordinate descent (feature/column sampling) and mirror descent (observation/row sampling). Heuristically, our algorithm learns to sample informative features/observations using their gradient values and requires overhead only logarithmic in the number of blocks over which it samples. We show that our method optimizes a particular bound on convergence, roughly sampling from the optimal stationary probability distribution in hindsight, and leading to substantial improvements when the data has pronounced irregularity.\nWhen the objective f(·) is smooth and the desired solution accuracy is reasonably low, (block) coordinate descent methods are attractive because of their tractability (Nesterov, 2012; Necoara et al., 2011; Beck and Tetruashvili, 2013; Lee and Sidford, 2013; Richtárik and Takáč, 2014; Lu and Xiao, 2015). In this paper, we consider potentially non-smooth functions and present an adaptive block coordinate descent method, which iterates over b blocks of coordinates, reminiscent of AdaGrad (Duchi et al., 2011). Choosing a good sampling distribution for coordinates in coordinate descent procedures is nontrivial (Lee and Sidford, 2013; Necoara et al., 2011; Shalev-Shwartz and Zhang, 2012; Richtárik and Takáč, 2015; Csiba et al., 2015; Allen-Zhu and Yuan, 2015). Most work focuses on choos-\ning a good stationary distribution using problem-specific knowledge, which may not be feasible; this motivates automatically adapting to individual problem instances. For example, Csiba et al. (2015) provide an updating scheme for the probabilities in stochastic dual ascent. However, the update requires O(b) time per iteration, making it impractical for large-scale problems. Similarly, Nutini et al. (2015) observe that the Gauss-Southwell rule (choosing the coordinate with maximum gradient value) achieves better performance, but this also requires O(b) time per iteration. Our method roughly emulates this behavior via careful adaptive sampling and bandit optimization, and we are able to provide a number of a posteriori optimality guarantees.\nIn addition to coordinate descent methods, we also consider the finite-sum minimization problem\nminimize\nx2X\n1\nn\nn\nX\ni=1\nf i (x),\nwhere the f i are convex and may be non-smooth. Variancereduction techniques for finite-sum problems often yield substantial gains (Johnson and Zhang, 2013; Defazio et al., 2014), but they generally require smoothness. More broadly, importance sampling estimates (Strohmer and Vershynin, 2009; Needell et al., 2014; Zhao and Zhang, 2014; 2015; Csiba and Richtárik, 2016) can yield improved convergence, but the only work that allows online, problemspecific adaptation of sampling probabilities of which we are aware is Gopal (2016). However, these updates require O(b) computation and do not have optimality guarantees.\nWe develop these ideas in the coming sections, focusing first in Section 2 on adaptive procedures for (non-smooth) coordinate descent methods and developing the necessary bandit optimization and adaptivity machinery. In Section 3, we translate our development into convergence results for finite-sum convex optimization problems. Complementing our theoretical results, we provide a number of experiments in Section 4 that show the importance of our algorithmic development and the advantages of exploiting block structures in problem data."
  }, {
    "heading": "2. Adaptive sampling for coordinate descent",
    "text": "We begin with the convex optimization problem\nminimize x2X f(x) (2)\nwhere X = X 1 ⇥ · · · ⇥ X b ⇢ Rd is a Cartesian product of closed convex sets X\nj\n⇢ Rdj with P\nj\nd j = d, and f is convex and Lipschitz. When there is a natural block structure in the problem, some blocks have larger gradient norms than others, and we wish to sample these blocks more often in the coordinate descent algorithm. To that\nend, we develop an adaptive procedure that exploits variability in block “importance” online. In the coming sections, we show that we obtain certain near-optimal guarantees, and that the computational overhead over a simple random choice of block j 2 [b] is at most O(log b). In addition, under some natural structural assumptions on the blocks and problem data, we show how our adaptive sampling scheme provides convergence guarantees polynomially better in the dimension than those of naive uniform sampling or gradient descent.\nNotation for coordinate descent Without loss of genrality we assume that the first d\n1 coordinates of x 2 Rd correspond to X\n1 , the second d 2 to X 2 , and so on. We let U j\n2 {0, 1}d⇥dj be the matrix identifying the jth block, so that I\nd = [U 1 · · · U d ]. We define the projected subgradient vectors for each block j by\nG j (x) = U j U> j f 0(x) 2 Rd,\nwhere f 0(x) 2 @f(x) is a fixed element of the subdifferential @f(x). Define x\n[j] := U> j x 2 Rdj and G [j] (x) =\nU> j G j (x) = U> j f 0(x) 2 Rdj . Let j denote a differentiable 1-strongly convex function on X\nj with respect to the norm k·k\nj\n, meaning for all 2 Rdj we have\nj x [j] +\nj x [j] +r j (x [j] )\n> + 1\n2\nk k2 j ,\nand let k·k j,⇤ be the dual norm of k·kj . Let Bj(u, v) =\nj (u) j (v) r j (v)>(u v) be the Bregman divergence associated with\nj , and define the tensorized divergence B(x, y) := P b\nj=1\nB j (x [j] , y [j] ). Throughout the paper, we assume the following. Assumption 1. For all x, y 2 X , we have B(x, y)  R2\nand G [j] (x) 2 j,⇤  L 2/b for j = 1, . . . , b."
  }, {
    "heading": "2.1. Coordinate descent for non-smooth functions",
    "text": "The starting point of our analysis is the simple observation that if a coordinate J 2 [b] is chosen according to a probability vector p > 0, then the importance sampling estimator\nG J (x)/p J satisfies E p [G J (x)/p J ] = f 0(x) 2 @f(x).\nThus the randomized coordinate subgradient method of Algorithm 1 is essentially a stochastic mirror descent method (Nemirovski and Yudin, 1983; Beck and Teboulle, 2003; Nemirovski et al., 2009), and as long as sup\nx2X E[kp 1 J G J (x)k2⇤] < 1 it converges at rate O(1/ p T ). With this insight, a variant of standard stochastic mirror descent analysis yields the following convergence guarantee for Algorithm 1 with non-stationary probabilities (cf. Dang and Lan (2015), who do not quite as carefully track dependence on the sampling distribution\nAlgorithm 1 Non-smooth Coordinate Descent Input: Stepsize ↵\nx > 0, Probabilities p1, . . . , pT . Initialize: x1 = x for t 1, . . . , T\nSample J t ⇠ pt Update x: x\nt+1 [J\nt\n] argmin x2X J\nt\n(* G[J\nt\n](x t)\np\nt J\nt\n, x + +\n1 ↵\nx\nB\nJ\nt\n⇣ x, x t\n[J t ]\n⌘)\nreturn x̄ T\n1 T\nP\nT\nt=1\nxt\np). Throughout, we define the expected sub-optimality gap of an algorithm outputing an estimate bx by S(f, bx) := E[f(bx)] inf\nx ⇤2X f(x⇤). See Section A.1 for the proof. Proposition 1. Under Assumption 1, Algorithm 1 achieves\nS(f, xT )  R 2\n↵ x\nT +\n↵ x\n2T\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG [j] (xt)\n2\nj,⇤ pt j\n3\n5 . (3)\nwhere S(f, x̄ T ) = E[f(x̄ T )] inf x2X f(x).\nAs an immediate consequence, if pt p min > 0 and ↵ x =\nR\nL\nq\n2pmin\nT\n, then S(f, x̄ T )  RL q 2\nTpmin . To make this\nmore concrete, we consider sampling from the uniform distribution pt ⌘ 1\nb\n1 so that p min = 1/b, and assume homogeneous block sizes d\nj = d/b for simplicity. Algorithm 1 solves problem (2) to ✏-accuracy within O(bR2L2/✏2) iterations, where each iteration approximately costs O(d/b) plus the cost of projecting into X\nj . In contrast, mirror descent with the same constraints and divergence B achieves the same accuracy within O(R2L2/✏2) iterations, taking O(d) time plus the cost of projecting to X per iteration. As the projection costs are linear in the number b of blocks, the two algorithms are comparable.\nIn practice, coordinate descent procedures can significantly outperform full gradient updates through efficient memory usage. For huge problems, coordinate descent methods can leverage data locality by choosing appropriate block sizes so that each gradient block fits in local memory."
  }, {
    "heading": "2.2. Optimal stepsizes by doubling",
    "text": "In the the upper bound (3), we wish to choose the optimal stepsize ↵\nx that minimizes this bound. However, the term P\nT\nt=1\nE ⇥ P b\nj=1\nkG[j](xt)k2 j,⇤\np\nt j\n⇤\nis unknown a priori. We circumvent this issue by using the doubling trick (e.g. ShalevShwartz, 2012, Section 2.3.1) to achieve the best possible rate in hindsight. To simplify our analysis, we assume that there is some p\nmin\n> 0 such that\npt 2 b := p 2 Rb + : p>1 = 1, p p min .\nMaintaining the running sum P t\nl=1 p 2 l,J\nl\nG [J\nl\n]\n(x l )\n2\nJ\nl\n,⇤\nAlgorithm 2 Stepsize Doubling Coordinate Descent Initialize: x1 = x, p1 = p, k = 1 while t  T do\nwhile P t\nl=1\n(pl J\nl\n)\n2 G [J\nl\n]\n(xl)\n2\nJ\nl\n,⇤  4 k, t  T do\nRun inner loop of Algorithm 1 with\n↵ x,k = p 2R ⇣ 4 k + L 2\nbp 2 min\n⌘ 12\nt t+ 1 k k + 1\nreturn x̄ T\n1 T\nP\nT\nt=1\nxt\nrequires incremental time O(d J\nt ) at each iteration t, choosing the stepsizes adaptively via Algorithm 2 only requires a constant factor of extra computation over using a fixed step size. The below result shows that the doubling trick in Algorithm 2 acheives (up to log factors) the performance of the optimal stepsize that minimizes the regret bound (3).\nProposition 2. Under Assumption 1, Algorithm 2 achieves\nS(f, x̄ T )  6R T\n0\n@\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG [j] (xt)\n2\nj,⇤ pt j\n3\n5\n1\nA\n1 2\n+\nr\n2\nb\nRL\np min\nT log 4 log\n✓\n4bTL2\np min\n◆\nwhere S(f, x̄ T ) = E[f(x̄ T )] inf x2X f(x)."
  }, {
    "heading": "2.3. Adaptive probabilities",
    "text": "We now present an adaptive updating scheme for pt, the sampling probabilities. From Proposition 2, the stationary distribution achieving the smallest regret upper bound minimizes the criterion\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG [j] (xt)\n2\nj,⇤ p j\n3\n5\n=\nT\nX\nt=1\nE\n2\n4\nG [J\nt\n]\n(xt)\n2\nJ\nt ,⇤ p2 J\nt\n3\n5 ,\nwhere the equality follows from the tower property. Since xt depends on the pt, we view this as an online convex optimization problem and choose p1, . . . , pT to minimize the regret\nmax p2 b\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG [j] (xt)\n2 j,⇤\n1\npt j\n1 p j\n!\n3\n5 . (4)\nNote that due to the block coordinate nature of Algorithm 1, we only compute\nG [j] (xt)\n2 j,⇤ for the sampled j = Jt at each iteration. Hence, we treat this as a multi-armed bandit problem where the arms are the blocks j = 1, . . . , b and we only observe the loss\nG [j] (xt)\n2 j,⇤ /(p t J t ) 2 associated with the arm J\nt\npulled at time t.\nAlgorithm 3 Coordinate Descent with Adaptive Sampling Input: Stepsize ↵\np > 0, Threshold p min > 0 with P = {p 2 Rb\n+\n: p>1 = 1, p p min } Initialize: x1 = x, p1 = p for t 1, . . . , T\nSample J t ⇠ pt Choose ↵\nx,k according to Algorithm 2 Update x: x\nt+1 [J\nt\n] argmin x2X J\nt\n(* G[J\nt\n](x t)\np\nt J\nt\n, x + +\n1 ↵\nx,k\nB ⇣ x, x t\n[J t ]\n⌘)\nUpdate p: for b` t,j (x) defined in (5), wt+1 pt exp( (↵\np b` t,J\nt (xt)/pt J\nt\n)e J\nt ), pt+1 argmin\nq2P Dkl\nq||wt+1\nreturn x̄ T\n1 T\nP\nT\nt=1\nxt\nBy using a bandit algorithm—another coordinate descent method— to update p, we show that our updates achieve performance comparable to the best stationary probability in\nb in hindsight. To this end, we first bound the regret (4) by the regret of a linear bandit problem. By convexity of x 7! 1/x and d\ndx\nx 1 = x 2, we have\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG [j] (xt)\n2 j,⇤\n1\npt j\n1 p j\n!\n3\n5\n T X\nt=1\nE\n2\n6 6 6\n4\n*\nn\nG [j] (xt)\n2 j,⇤ /(p t j ) 2\no\nb\nj=1\n| {z }\n(⇤)\n, pt p +\n3\n7 7 7\n5\n.\nNow, let us view the vector (⇤) as the loss vector for a constrained linear bandit problem with feasibility region\nb . We wish to apply EXP3 (due to Auer et al. (2002)) or equivalently, a 1-sparse mirror descent to p with\nP (p) = p log p (see, for example, Section 5.3 of Bubeck and Cesa-Bianchi (2012) for the connections). However, EXP3 requires the loss values be positive in order to be in the region where P\nis strongly convex, so we scale our problem using the fact that p and pt’s are probability vectors. Namely,\nT\nX\nt=1\nE ⌧ n\nG[j](x t )\n2 j,⇤ /(p t j )\n2 o b\nj=1 , pt p\n=\nT\nX\nt=1\nE hD b` t (xt), pt p Ei ,\nwhere\nb` t,j (x) :=\nG[j](x)\n2 j,⇤\n(pt j )\n2 +\nL2\nbp2min . (5)\nUsing scaled loss values, we perform EXP3 (Algorithm 3). Intuitively, we penalize the probability of the sampled block by the strength of the signal on the block. The\nscaling (5) ensures that we penalize blocks with low signal (as opposed to rewarding those with high signal) which enforces diversity in the sampled coordinates as well. In Section A.3, we will see how this scaling plays a key role in proving optimality of Algorithm 3. Here, the signal is measured by the relative size of the gradient in the block against the probability of sampling the block. This means that blocks with large “surprises”—those with higher gradient norms relative to their sampling probability—will get sampled more frequently in the subsequent iterations. Algorithm 3 guarantees low regret for the online convex optimization problem (4) which in turn yields the following guarantee for Algorithm 3. Theorem 3. Under Assumption 1, the adaptive updates in Algorithm 3 with ↵\np\n=\np 2 min L 2\nq\n2b log b\nT\nachieve\nS(f, x̄ T )  6R T\nv u u u t\nmin p2 b\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nkG [j] (xt)k2 j,⇤\np j\n3\n5\n| {z }\n(a):best in hindsight\n(6)\n+\n8LR\nTp min\n✓\nT log b\nb\n◆ 1 4\n| {z } (b):regret for bandit problem\n+ 2RLp bTp\nmin\nlog\n✓\n4bTL2\np min\n◆\n.\nwhere S(f, x̄ T ) = E[f(x̄ T )] inf x2X f(x).\nSee Section A.3 for the proof. Note that there is a trade-off in the regret bound (6) in terms of p\nmin : for small p min , the first term is small, as the the set\nb is large, but second (regret) term is large, and vice versa. To interpret the bound (6), take p\nmin = /b for some 2 (0, 1). The first term dominates the remainder as long as T = ⌦(b log b); we require T ⇣ (bR2L2/✏2) to guarantee convergence of coordinate descent in Proposition 1, so that we roughly expect the first term in the bound (6) to dominate. Thus, Algorithm 3 attains the best convergence guarantee for the optimal stationary sampling distribution in hindsight."
  }, {
    "heading": "2.4. Efficient updates for p",
    "text": "The updates for p in Algorithm 3 can be done in O(log b) time by using a balanced binary tree. Let D\nkl (p||q) := P\nd\ni=1\np i log p i\nq\ni denote the Kullback-Leibler divergence between p and q. Ignoring the subscript on t so that w = wt+1, p = pt and J = J\nt , the new probability vector q is given by the minimizer of\nD kl (q||w) s.t. q>1 = 1, q p min , (7)\nwhere w is the previous probability vector p modified only at the index J . We store w in a binary tree, keeping values up to their normalization factor. At each node, we also store the sum of elements in the left/right subtree for\nAlgorithm 4 KL Projection 1: Input: J , p\nJ , w J , mass = P\ni\nw i\n2: wcand pJ ·mass. 3: if wcand/(mass wJ + wcand)  pmin then 4: wcand pmin\n1 pmin (mass wJ) 5: Update(wcand, J)\nefficient sampling (for completeness, the pseudo-code for sampling from the binary tree in O(log b) time is given in Section B.3). The total mass of the tree can be accessed by inspecting the root of the tree alone.\nThe following proposition shows that it suffices to touch at most one element in the tree to do the update. See Section B for the proof. Proposition 4. The solution to (7) is given by\nq j 6=J =\n(\n1\n1 p J +w J\nw j if w J\npmin(1 pJ ) 1 pmin\n1 pmin 1 p\nJ\nw j\notherwise ,\nq J =\n(\n1\n1 p J +w J\nw if w J\npmin(1 pJ ) 1 pmin\np min\notherwise.\nAs seen in Algorithm 4, we need to modify at most one element in the binary tree. Here, the update function modifies the value at index J and propagates the value up the tree so that the sum of left/right subtrees are appropriately updated. We provide the full pseudocode in Section B.2."
  }, {
    "heading": "2.5. Example",
    "text": "The optimality guarantee given in Theorem 3 is not directly interpretable since the term (a) in the upper bound (6) is only optimal given the iterates x1, . . . , xT despite the fact that xt’s themselves depend on the sampling probabilities. Hence, we now study a setting where we can further bound (6) to get a explicit regret bound for Algorithm 3 that is provably better than non-adaptive counterparts. Indeed, under certain structural assumptions on the problem similar to those of McMahan and Streeter (2010) and Duchi et al. (2011), our adaptive sampling algorithm provably achieves regret polynomially better in the dimension than either using a uniform sampling distribution or gradient descent.\nConsider the SVM objective\nf(x) = 1\nn\nn\nX\ni=1\n1 y i z> i x\n+\nwhere n is small and d is large. Here, @f(x) = 1\nn\nP\nn\ni=1\n1 1 y i z> i x 0 z i . Assume that for some fixed ↵ 2 (1,1) and L\nj := j ↵, we have |@ j f(x)|2  1\nn\nP\nn\ni=1\n|z i,j |2  L2 j . In particular, this is the case if we have sparse features z\nU\n2 { 1, 0,+1}d with power law\ntails P (|z U,j | = 1) = j ↵ where U is a uniform random variable over {1, . . . , n}.\nTake C j = {j} for j = 1, . . . , d (and b = d). First, we show that although for the uniform distribution p = 1/d\nd\nX\nj=1\nE[kG j (xt)k2⇤] 1/d  d d X\nj=1\nL2 j = O(d log d),\nthe term (a) in (6) can be orders of magnitude smaller. Proposition 5. Let b = d, p\nmin = /d for some 2 (0, 1), and C\nj = {j}. If kG j (x)k2⇤  L 2 j := j ↵ for some ↵ 2 (1,1), then\nmin\np2 b ,p pmin\nd\nX\nj=1\nE[ G j (xt)\n2 ⇤]\np j\n=\n(\nO(log d), if ↵ 2 [2,1) O(d2 ↵), if ↵ 2 (1, 2).\nWe defer the proof of the proposition to Section A.5. Using this bound, we can show explicit regret bounds for Algorithm 3. From Theorem 3 and Proposition 5, we have that Algorithm 3 attains\nS(f, x̄ T\n)  ( O(R log dp T ), if ↵ 2 [2,1) Rp T O(d1 ↵ 2 ), if ↵ 2 (1, 2)\n+O ⇣ Rd3/4T 3/4 log5/4 d ⌘ .\nSetting above to be less than ✏ and inverting with respect to T , we obtain the iteration complexity in Table 1.\nTo see the runtime bounds for uniformly sampled coordinate descent and gradient descent, recall the regret bound (3) given in Proposition 1. Plugging pt\nj = 1/d in the bound, we obtain\nS(f, x̄ T\n)  O(R p log d p 2dT ).\nfor ↵ x =\np\n2R2/(L2Td) where L2 := P d\nj=1\nL2 j . Similarly, gradient descent with ↵\nx\n=\np\n2R2/(L2T ) attains\nS(f, x̄ T\n)  O(R p log d p 2T ).\nSince each gradient descent update takes O(d), we obtain the same runtime bound.\nWhile non-adaptive algorithms such as uniformly-sampled coordinate descent or gradient descent have the same runtime for all ↵, our adaptive sampling method automatically tunes to the value of ↵. Note that for ↵ 2 (1,1), the first term in the runtime bound for our adaptive method given in Table 1 is strictly better than that of uniform coordinate descent or gradient descent. In particular, for ↵ 2 [2,1) the best stationary sampling distribution in hindsight yields an improvement that is at most O(d) better in the dimension. However, due to the remainder terms for the bandit problem, this improvement only matters (i.e.first term is larger than second) when\n✏ =\n8\n<\n:\nO ⇣ Rd 3 2 p log d ⌘\nif ↵ 2 [2,1)\nO ⇣ Rd 3 2 (1 ↵) log 5/2 d ⌘ if ↵ 2 (1, 2).\nIn Section 4, we show that these remainder terms can be made smaller than what their upper bounds indicate. Empirically, our adaptive method outperforms the uniformlysampled counterpart for larger values of ✏ than above."
  }, {
    "heading": "3. Adaptive probabilities for stochastic gradient descent",
    "text": "Consider the empirical risk minimization problem\nminimize\nx2X\n(\n1\nn\nn\nX\ni=1\nf i (x) =: f(x)\n)\nwhere X 2 Rd is a closed convex set and f i (·) are convex functions. Let C\n1 , . . . , C b be a partition of the n samples so that each example belongs to some C\nj , a set of size n j := |C j\n| (note that the index j now refers to blocks of examples instead of coordinates). These block structures naturally arise, for example, when C\nj ’s are the examples with the same label in a multi-class classification problem. In this stochastic optimization setting, we now sample a block J t\n⇠ pt at each iteration t, and perform gradient updates using a gradient estimate on the block C\nJ\nt . We show how a similar adaptive updating scheme for pt’s again achieves the optimality guarantees given in Section 2."
  }, {
    "heading": "3.1. Mirror descent with non-stationary probabilities",
    "text": "Following the approach of (Nemirovski et al., 2009), we run mirror descent for the updates on x. At iteration t, a block J\nt is drawn from a b-dimensional probability vector pt. We assume that we have access to unbiased stochastic gradients G\nj (x) for each block. That is, E[G\nj (x)] = 1 n\nj\nP\ni2C j\n@f i (x). In particular, the estimate G\nJ\nt\n(xt) := @f I\nt\n(x) where I t is drawn uniformly in C J\nt\ngives the usual unbiased stochastic gradient of minibatch size 1. The other extreme is obtained by using a minibatch size of n\nj where G J\nt (xt) := 1 n\nJ\nt\nP\ni2C J\nt\n@f i (x). Then,\nthe importance sampling estimator nJt np t\nJ\nt\nG J\nt (xt) is an unbiased estimate for the subgradient of the objective.\nLet be a differentiable 1-strongly convex function on X with respect to the norm k·k as before and denote by k·k⇤ the dual norm of k·k. Let B(x, y) = (x) (y) r (y)>(x y) be the Bregman divergence associated with . In this section, we assume the below (standard) bound. Assumption 2. For all x, y 2 X , we have B(x, y)  R2 and kG\nj (x)k2⇤  L for j = 1, . . . , b.\nWe use these stochastic gradients to perform mirror updates, replacing the update in Algorithm 1 with the update\nxt+1 argmin x2X\n⇢\nn J\nt npt J\nt\n⌦\nG J\nt\n(xt), x ↵ + 1\n↵ x\nB(x, xt) . (8)\nFrom a standard argument (e.g., (Nemirovski et al., 2009)), we obtain the following convergence guarantee. The proof follows an argument similar to that of Proposition 1. Proposition 6. Under Assumption 2, the updates (8) attain\nS(f, x̄ T\n)  R 2\n↵ x\nT +\n↵ x\n2T\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nn2 j kG j (xt)k2⇤ n2pt\nj\n3\n5 . (9)\nwhere S(f, x̄ T ) = E[f(x̄ T )] inf x2X f(x).\nAgain, we wish to choose the optimal step size ↵ x that minimizes the regret bound (9). To this end, modify the doubling trick given in Algorithm 2 as follows: use P\nt\nl=1\nn 2 J\nl\nn 2 p 2 l,J\nl\nG J\nl\n(xl)\n2 ⇤ for the second while condition,\nand stepsizes ↵ x,k = p 2R ⇣ 4 k + L 2 max j n 2 j\nn 2 p 2 min\n⌘ 12 . Then,\nsimilar to Proposition 2, we have\nS(f, x̄ T )  6R T\n0\n@\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nn2 j\nn2pt j\nG j (xt)\n2 ⇤\n3\n5\n1\nA\n1 2\n+\np 2RL\np min T log 4\nmax\nj n j\nn log\n0\n@\n4TL2\np min\nb\nX\nj=1\nn2 j\nn2\n1\nA ."
  }, {
    "heading": "3.2. Adaptive probabilities",
    "text": "Now, we consider an adaptive updating scheme for pt’s similar to Section 2.3. Using the scaled gradient estimate\nb` t,j (x) := ✓ n j\nnpt j\nkG j (x)k⇤ ◆2 + L2 max j n2 j\nn2p2min (10)\nto run EXP3, we obtain Algorithm 5. Again, the additive scaling L2(max\nj n j /np min ) 2 is to ensure that b` 0. As in Section 2.4, the updates for p in Algorithm 5 can be done in O(log b) time. We can also show similar optimality guarantees for Algorithm 5 as before. The proof is essentially the same to that given in Section A.3.\nAlgorithm 5 Mirror Descent with Adaptive Sampling Input: Stepsize ↵\np > 0 Initialize: x1 = x, p1 = p for t 1, . . . , T\nSample J t ⇠ pt Choose ↵\nx,k according to (modified) Algorithm 2. Update x: x\nt+1 J\nt\nargmin x2X\n( 1\np\nt J\nt\n⌦ G\nJ\nt\n(x\nt\n), x ↵ +\n1 ↵\nx,k\nB ⇣ x, x t\nJ\nt\n⌘)\nUpdate p: wt+1 pt exp( (↵\np b` t,J\nt (xt)/pt J\nt\n)e J\nt\n)\npt+1 argmin q2P Dkl\nq||wt+1\nreturn x̄ T\n1 T\nP\nT\nt=1\nxt\nTheorem 7. Let W := Lmaxj nj pminn . Under Assumption 2,\nAlgorithm 5 with ↵ p = 1\nW\n2\nq\n2 log b\nbT\nachieves\nS(f, x̄ T )  6R T min p2 b\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nn2 j\nn2p j\nG J\nt\n(xt)\n2 ⇤\n3\n5\n+W (2Tb log b) 1 4 +\np 2RW\nT log 4 log\n4TL2\np min\nb\nX\nj=1\nn2 j\nn2\n!\nwhere S(f, x̄ T ) = E[f(x̄ T )] inf x2X f(x).\nWith equal block sizes n j = n/b and p min = /b for some 2 (0, 1), the first term in the boudn of Theorem 7 is O(TL2) which dominates the second term if T = ⌦(b log b). Since we usually have T = ⇥(n) for SGD, as long as n = ⌦(b log b) we have\nS(f, x̄ T )  O\n0\nB @ R T\nv u u u t\nmin p2 b\nT\nX\nt=1\nE\n2\n4\nb\nX\nj=1\nG[j](xt)\n2 j,⇤\np j\n3\n5\n1\nC A .\nThat is, Algorithm 5 attains the best regret bound achieved by the optimal stationary distribution in hindsight had the xt’s had remained the same. Further, under similar structural assumptions kG\nj (x)k2⇤ / j ↵ as in Section 2.5, we\ncan prove that the regret bound for our algorithm is better than that of the uniform distribution."
  }, {
    "heading": "4. Experiments",
    "text": "We compare performance of our adaptive approach with stationary sampling distributions on real and synthetic datasets. To minimize parameter tuning, we fix ↵\np at the value suggested by theory in Theorems 3 and 7. However, we make a heuristic modification to our adaptive algorithm since rescaling the bandit gradient (5) and (10) dwarfs the signals in gradient values if L is too large. We present performance of our algorithm with respect to multiple estimates of the Lipschitz constant ˆL = L/c for c > 1, where\nL is the actual upper bound.1 We tune the stepsize ↵ x for both methods, using the form / p t and tuning .\nFor all our experiments, we compare our method against the uniform distribution and blockwise Lipschitz sampling distribution p\nj / L j where L j is the Lipschitz constant of the j-th block (Zhao and Zhang, 2015). We observe that the latter method often performs very well with respect to iteration count. However, since computing the blockwise Lipschitz sampling distribution takes O(nd), the method is not competitive in large-scale settings. Our algorithm, on the other hand, adaptively learns the latent structure and often outperforms stationary counterparts with respect to runtime. While all of our plots are for a single run with a random seed, we can reject the null hypothesis f(xT\nuniform ) < f(xT adaptive ) at 99% confidence for all instances where our theory guarantees it. We take k·k = k·k\n2\nthroughout this section."
  }, {
    "heading": "4.1. Adaptive sampling for coordinate descent",
    "text": "Synthetic Data We begin with coordinate descent, first verifying the intuition of Section 2.5 on a synthetic dataset. We consider the problem minimizekxk11 1 n kAx bk 1\n, and we endow A 2 Rn⇥d with the following block structure: the columns are drawn as a\nj ⇠ j ↵/2N(0, I). Thus, the gradients of the columns decay in a heavy-tailed manner as in Section 2.5 so that L2\nj = j ↵. We set n = d = b = 256; the effects of changing ratios n/d and b/d manifest themselves via relative norms of the gradients in the columns, which we control via ↵ instead. We run all experiments with p\nmin\n= 0.1/b and multiple values of c.\nResults are shown in Figure 1, where we show the optimality gap vs. runtime in (a) and the learned sampling distribution in (b). Increasing ↵ (stronger block structure) improves our relative performance with respect to uniform sampling and our ability to accurately learn the underlying block structure. Experiments over more ↵ and c in Section C further elucidate the phase transition from uniform-like behavior to regimes learning/exploiting structure.\nWe also compare our method with (non-preconditioned) SGD using leverage scores p\nj / ka j k 1 given by (Yang et al., 2016). The leverage scores (i.e., sampling distribution proportional to blockwise Lipschitz constants) roughly correpond to using p\nj / j ↵/2, which is the stationary distribution that minimizes the bound (3); in this synthetic setting, this sampling probability coincides with the actual block structure. Although this is expensive to compute, taking O(nd) time, it exploits the latent block structure very well as expected. Our method quickly learns the structure and performs comparably with this “optimal” distribution.\n1We guarantee a positive loss by taking max(b` t,j (x), 0).\nModel selection Our algorithm’s ability to learn underlying block structure can be useful in its own right as an online feature selection mechanism. We present one example of this task, studying an aptamer selection problem (Cho et al., 2013), which consists of n = 2900 nucleotide sequences (aptamers) that are one-hot-encoded with all kgrams of the sequence, where 1  k  5 so that d = 105, 476. We train an l\n1 -regularized SVM on the binary labels, which denote (thresholded) binding affinity of the aptamer. We set the blocksize as 50 features (b = 2110) and p\nmin = 0.01/b. Results are shown in Figure 2, where we see that adaptive feature selection certainly improves training time in (a). The learned sampling distribution depicted in (b) for the best case (c = 107) places larger weight on features known as G-complexes; these features are wellknown to affect binding affinities (Cho et al., 2013)."
  }, {
    "heading": "4.2. Adaptive sampling for SGD",
    "text": "Synthetic data We use the same setup as in Section 4.1 but now endow block structure on the rows of A rather than the columns. In Figure 3, we see that when there is little block structure (↵ = 0.4) all sampling schemes perform similarly. When the block structure is apparent (↵ = 6), our adaptive method again learns the underlying structure\nand outperforms uniform sampling. We provide more experiments in Section C to illustrate behaviors over more c and ↵. We note that our method is able to handle online data streams unlike stationary methods such as leverage scores.\nCUB-200-2011/ALOI We apply our method to two multi-class object detection datasets: Caltech-UCSD Birds-200-2011 (Wah et al., 2011) and ALOI (Geusebroek et al., 2005). Labels are used to form blocks so that b = 200 for CUB and b = 1000 for ALOI. We use softmax loss for CUB-200-2011 and a binary SVM loss for ALOI, where in the latter we do binary classification between shells and non-shell objects. We set p\nmin = 0.5/b to enforce enough exploration. For the features, outputs of the last fullyconnected layer of ResNet-50 (He et al., 2016) are used for CUB so that we have 2049-dimensional features. Since our classifier x is (b · d)-dimensional, this is a fairly large scale problem. For ALOI, we use default histogram features (d = 128). In each case, we have n = 5994 and n = 108, 000 respectively. We use X := {x 2 Rm : kxk\n2  r} where r = 100 for CUB and r = 10 for ALOI. We observe in Figure 4 that our adaptive sampling method outperforms stationary counterparts."
  }, {
    "heading": "Acknowledgements",
    "text": "HN was supported by the Samsung Scholarship. AS and SY were supported by Stanford Graduate Fellowships and AS was also supported by a Fannie & John Hertz Foundation Fellowship. JCD was supported by NSF-CAREER1553086."
  }],
  "year": 2017,
  "references": [{
    "title": "Even faster accelerated coordinate descent using non-uniform sampling",
    "authors": ["Z. Allen-Zhu", "Y. Yuan"],
    "venue": "arXiv preprint arXiv:1512.09103,",
    "year": 2015
  }, {
    "title": "Finite-time analysis of the multiarmed bandit problem",
    "authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"],
    "venue": "Machine Learning,",
    "year": 2002
  }, {
    "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
    "authors": ["A. Beck", "M. Teboulle"],
    "venue": "Operations Research Letters,",
    "year": 2003
  }, {
    "title": "On the convergence of block coordinate descent type methods",
    "authors": ["A. Beck", "L. Tetruashvili"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Prediction, learning, and games",
    "authors": ["N. Cesa-Bianchi", "G. Lugosi"],
    "year": 2006
  }, {
    "title": "Quantitative selection and parallel characterization of aptamers",
    "authors": ["M. Cho", "S.S. Oh", "J. Nie", "R. Stewart", "M. Eisenstein", "J. Chambers", "J.D. Marth", "F. Walker", "J.A. Thomson", "H.T. Soh"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2013
  }, {
    "title": "Importance sampling for minibatches",
    "authors": ["D. Csiba", "P. Richtárik"],
    "venue": "arXiv preprint arXiv:1602.02283,",
    "year": 2016
  }, {
    "title": "Stochastic dual coordinate ascent with adaptive probabilities",
    "authors": ["D. Csiba", "Z. Qu", "P. Richtárik"],
    "venue": "arXiv preprint arXiv:1502.08053,",
    "year": 2015
  }, {
    "title": "Stochastic block mirror descent methods for nonsmooth and stochastic optimization",
    "authors": ["C.D. Dang", "G. Lan"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "SAGA: A fast incremental gradient method with support for nonstrongly convex composite objectives",
    "authors": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J.C. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "The amsterdam library of object images",
    "authors": ["J.-M. Geusebroek", "G.J. Burghouts", "A.W. Smeulders"],
    "venue": "International Journal of Computer Vision,",
    "year": 2005
  }, {
    "title": "Adaptive sampling for sgd by exploiting side information",
    "authors": ["S. Gopal"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["R. Johnson", "T. Zhang"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems",
    "authors": ["Y.T. Lee", "A. Sidford"],
    "venue": "In 54th Annual Symposium on Foundations of Computer Science,",
    "year": 2013
  }, {
    "title": "On the complexity analysis of randomized block-coordinate descent methods",
    "authors": ["Z. Lu", "L. Xiao"],
    "venue": "Mathematical Programming,",
    "year": 2015
  }, {
    "title": "Adaptive bound optimization for online convex optimization",
    "authors": ["B. McMahan", "M. Streeter"],
    "venue": "In Proceedings of the Twenty Third Annual Conference on Computational Learning Theory,",
    "year": 2010
  }, {
    "title": "A random coordinate descent method on large optimization problems with linear constraints",
    "authors": ["I. Necoara", "Y. Nesterov", "F. Glineur"],
    "year": 2011
  }, {
    "title": "Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm",
    "authors": ["D. Needell", "R. Ward", "N. Srebro"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Problem Complexity and Method Efficiency in Optimization",
    "authors": ["A. Nemirovski", "D. Yudin"],
    "year": 1983
  }, {
    "title": "Robust stochastic approximation approach to stochastic programming",
    "authors": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2009
  }, {
    "title": "Efficiency of coordinate descent methods on huge-scale optimization problems",
    "authors": ["Y. Nesterov"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2012
  }, {
    "title": "Coordinate descent converges faster with the gauss-southwell rule than random selection",
    "authors": ["J. Nutini", "M. Schmidt", "I.H. Laradji", "M. Friedlander", "H. Koepke"],
    "venue": "arXiv preprint arXiv:1506.00552,",
    "year": 2015
  }, {
    "title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
    "authors": ["P. Richtárik", "M. Takáč"],
    "venue": "Mathematical Programming,",
    "year": 2014
  }, {
    "title": "Parallel coordinate descent methods for big data optimization",
    "authors": ["P. Richtárik", "M. Takáč"],
    "venue": "Mathematical Programming, page Online first,",
    "year": 2015
  }, {
    "title": "Online learning and online convex optimization",
    "authors": ["S. Shalev-Shwartz"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Proximal stochastic dual coordinate ascent",
    "authors": ["S. Shalev-Shwartz", "T. Zhang"],
    "venue": "arXiv preprint arXiv:1211.2717,",
    "year": 2012
  }, {
    "title": "A randomized Kaczmarz algorithm with exponential convergence",
    "authors": ["T. Strohmer", "R. Vershynin"],
    "venue": "Journal of Fourier Analysis and Applications,",
    "year": 2009
  }, {
    "title": "The Caltech-UCSD Birds-200-2011 Dataset",
    "authors": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"],
    "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,",
    "year": 2011
  }, {
    "title": "Accelerating minibatch stochastic gradient descent using stratified sampling",
    "authors": ["P. Zhao", "T. Zhang"],
    "venue": "[stat.ML],",
    "year": 2014
  }, {
    "title": "Stochastic optimization with importance sampling",
    "authors": ["P. Zhao", "T. Zhang"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
    "year": 2015
  }],
  "id": "SP:08b1570bda45960186622c1a3c29eb59a8aefdac",
  "authors": [{
    "name": "Hongseok Namkoong",
    "affiliations": []
  }, {
    "name": "Aman Sinha",
    "affiliations": []
  }, {
    "name": "Steve Yadlowsky",
    "affiliations": []
  }, {
    "name": "John C. Duchi",
    "affiliations": []
  }],
  "abstractText": "Abstract Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be exploited for quicker convergence. Yet adaptive variants often suffer nontrivial computational overhead. We present a framework that discovers and leverages such structural properties at a low computational cost. We employ a bandit optimization procedure that “learns” probabilities for sampling coordinates or examples in (nonsmooth) optimization problems, allowing us to guarantee performance close to that of the optimal stationary sampling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adaptive counterparts, and we complement our analysis with experiments on several datasets.",
  "title": "Adaptive Sampling Probabilities for Non-Smooth Optimization"
}