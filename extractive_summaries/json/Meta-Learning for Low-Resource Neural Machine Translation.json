{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3622"
  }, {
    "heading": "1 Introduction",
    "text": "Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017). In the past few years, various approaches have been proposed to address this issue. The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre\n* Equal contribution.\net al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b). Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair.\nIn this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation. We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pairs as separate tasks. This view enables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples (§3). Furthermore, the vanilla MAML however cannot handle tasks with mismatched input and output. We overcome this limitation by incorporating the universal lexical representation (Gu et al., 2018b) and adapting it for the meta-learning scenario (§3.3).\nWe extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation. We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English. Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation\napproach across all the target language pairs, and the gap grows as the number of training examples decreases."
  }, {
    "heading": "2 Background",
    "text": "Neural Machine Translation (NMT) Given a source sentence X = {x1, ..., xT 0}, a neural machine translation model factors the distribution over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a leftto-right causal structure:\np(Y |X; ✓) = T+1Y\nt=1\np(yt|y0:t 1, x1:T 0 ; ✓), (1)\nwhere special tokens y0 (hbosi) and yT+1 (heosi) are used to represent the beginning and the end of a target sentence. These conditional probabilities are parameterized using a neural network. Typically, an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) with a RNN-based decoder is used. More recently, architectures without any recurrent structures (Gehring et al., 2017; Vaswani et al., 2017) have been proposed and shown to speed up training while achieving state-of-the-art performance.\nLow Resource Translation NMT is known to easily over-fit and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017). In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs. Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).\nFor the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks. For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource. The pivot can be a third language or even an image in multimodal domains. When pivots are\nnot easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation. Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages.\nAll the previous work for multilingual NMT assume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.\nMeta Learning In the machine learning community, meta-learning, or learning-to-learn, has recently received interests. Meta-learning tries to solve the problem of “fast adaptation on new training data.” One of the most successful applications of meta-learning has been on few-shot (or oneshot) learning (Lake et al., 2015), where a neural network is trained to readily learn to classify inputs based on only one or a few training examples. There are two categories of meta-learning:\n1. learning a meta-policy for updating model parameters (see, e.g., Andrychowicz et al., 2016; Ha et al., 2016a; Mishra et al., 2017)\n2. learning a good parameter initialization for fast adaptation (see, e.g., Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017).\nIn this paper, we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category. More specifically, we extend the idea of model-agnostic metalearning (MAML, Finn et al., 2017) in the multilingual scenario."
  }, {
    "heading": "3 Meta Learning for Low-Resource Neural Machine Translation",
    "text": "The underlying idea of MAML is to use a set of source tasks T 1, . . . , T K to find the initialization of parameters ✓0 from which learning a target task T 0 would require only a small number of training examples. In the context of machine translation, this amounts to using many high-resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-\nters. This process can be understood as\n✓⇤ = Learn(T 0;MetaLearn(T 1, . . . , T K)).\nThat is, we meta-learn the initialization from auxiliary tasks and continue to learn the target task. We refer the proposed meta-learning method for NMT to MetaNMT. See Fig. 1 for the overall illustration."
  }, {
    "heading": "3.1 Learn: language-specific learning",
    "text": "Given any initial parameters ✓0 (which can be either random or meta-learned),\nthe prior distribution of the parameters of a desired NMT model can be defined as an isotropic Guassian:\n✓i ⇠ N (✓0i , 1/ ),\nwhere 1/ is a variance. With this prior distribution, we formulate the language-specific learning process Learn(DT ; ✓0) as maximizing the logposterior of the model parameters given data DT :\nLearn(DT ; ✓0) = argmax ✓ LDT (✓)\n= argmax\n✓\nX\n(X,Y )2DT\nlog p(Y |X, ✓) k✓ ✓0k2,\nwhere we assume p(X|✓) to be uniform. The first term above corresponds to the maximum likelihood criterion often used for training a usual NMT system. The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of overfitting when there is not enough training data. In practice, we solve the problem above by maximizing the first term with gradient-based optimization and early-stopping after only a few update steps.\nThus, in the low-resource scenario, finding a good initialization ✓0 strongly correlates the final performance of the resulting model."
  }, {
    "heading": "3.2 MetaLearn",
    "text": "We find the initialization ✓0 by repeatedly simulating low-resource translation scenarios using auxiliary, high-resource language pairs. Following Finn et al. (2017), we achieve this goal by defining the meta-objective function as\nL(✓) =EkEDT k ,D0T k (2)2\n64 X\n(X,Y )2D0 T k\nlog p(Y |X;Learn(DT k ; ✓))\n3\n75 ,\nwhere k ⇠ U({1, . . . ,K}) refers to one metalearning episode, and DT , D0T follow the uniform distribution over T ’s data.\nWe maximize the meta-objective function using stochastic approximation (Robbins and Monro, 1951) with gradient descent. For each episode, we uniformly sample one source task at random, T k. We then sample two subsets of training examples independently from the chosen task, DT k and D0T k . We use the former to simulate languagespecific learning and the latter to evaluate its outcome. Assuming a single gradient step is taken only the with learning rate ⌘, the simulation is:\n✓0k = Learn(DT k ; ✓) = ✓ ⌘r✓LDT k (✓).\nOnce the simulation of learning is done, we evaluate the updated parameters ✓0k on D 0 T k , The gradient computed from this evaluation, which we refer to as meta-gradient, is used to update the\nmeta model ✓. It is possible to aggregate multiple episodes of source tasks before updating ✓:\n✓ ✓ ⌘0 X\nk\nr✓LD 0 T k (✓0k),\nwhere ⌘0 is the meta learning rate. Unlike a usual learning scenario, the resulting model ✓0 from this meta-learning procedure is not necessarily a good model on its own. It is however a good starting point for training a good model using only a few steps of learning. In the context of machine translation, this procedure can be understood as finding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource language pairs.\nMeta-Gradient We use the following approximation property\nH(x)v ⇡ r(x+ ⌫v) r(x) ⌫\nto approximate the meta-gradient:1\nr✓LD 0 (✓0) = r✓0LD 0 (✓0)r✓(✓ ⌘r✓LD(✓))\n= r✓0LD 0 (✓0) ⌘r✓0LD 0 (✓0)H✓(LD(✓))\n⇡ r✓0LD 0 (✓0) ⌘\n⌫\n r✓LD(✓)\n✓̂ r✓LD(✓) ✓ ,\nwhere ⌫ is a small constant and\nˆ✓ = ✓ + ⌫r✓0LD 0 (✓0).\nIn practice, we find that it is also possible to ignore the second-order term, ending up with the following simplified update rule:\nr✓LD 0 (✓0) ⇡ r✓0LD 0 (✓0). (3)\n1We omit the subscript k for simplicity.\nRelated Work: Multilingual Transfer Learning The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016). The latter can be thought of as solving the following problem:\nmax ✓ Lmulti(✓) = Ek\n2 4 X\n(X,Y )2Dk\nlog p(Y |X; ✓)\n3\n5 ,\nwhere Dk is the training set of the k-th task, or language pair. The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ✓0 found from solving the above problem.\nThe major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair. The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq. (2). As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.\nIllustration In Fig. 2, we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En). Transfer learning trains an NMT system specifically for a source language pair (Es-En) and finetunes the system for each target language pair (RoEn, Lv-En). Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn). If not, it finetunes the system for each target pair, similarly to transfer learning. Both of these however aim at directly solving the source tasks. On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks. This is done by repeatedly simulating the learning process on\nlow-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En)."
  }, {
    "heading": "3.3 Unified Lexical Representation",
    "text": "I/O mismatch across language pairs One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks. This, however, does not apply to machine translation in general due to the vocabulary mismatch across different languages. In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages. This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.)\nUniversal Lexical Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b). We start with multilingual word embedding matrices ✏kquery 2 R|Vk|⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language. These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018). We take one of these languages k0 to build universal lexical representation consisting of a universal embedding matrix ✏u 2 RM⇥d and a corresponding key matrix ✏key 2 RM⇥d, where M < |V 0k|. Both ✏kquery and ✏key are fixed during meta-learning. We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by\n✏0[x] = MX\ni=1\n↵i✏u[i],\nwhere ↵i / exp 1⌧ ✏key[i] >A✏kquery[x] and ⌧ is set to 0.05. This approach allows us to handle languages with different vocabularies using a fixed number of shared parameters (✏u, ✏key and A.)\nLearning of ULR It is not desirable to update the universal embedding matrix ✏u when fine-\ntuning on a small corpus which contains a limited set of unique tokens in the target language, as it could adversely influence the other tokens’ embedding vectors. We thus estimate the change to each embedding vector induced by languagespecific learning by a separate parameter ✏k[x]:\n✏k[x] = ✏0[x] + ✏k[x].\nDuring language-specific learning, the ULR ✏0[x] is held constant, while only ✏k[x] is updated, starting from an all-zero vector. On the other hand, we hold ✏k[x]’s constant while updating ✏u and A during the meta-learning stage."
  }, {
    "heading": "4 Experimental Settings",
    "text": ""
  }, {
    "heading": "4.1 Dataset",
    "text": "Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages: Romanian (Ro) from WMT’16,2 Latvian (Lv), Finnish (Fi), Turkish (Tr) from WMT’17,3 and Korean (Ko) from Korean Parallel Dataset.4 We use the officially provided train, dev and test splits for all these languages. The statistics of these languages are presented in Table 1. We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.\nSource Tasks We use the following languages from Europarl5: Bulgarian (Bg), Czech (Cs), Danish (Da), German (De), Greek (El), Spanish (Es), Estonian (Et), French (Fr), Hungarian (Hu), Italian (It), Lithuanian (Lt), Dutch (Nl), Polish (Pl), Portuguese (Pt), Slovak (Sk), Slovene (Sl) and\n2 http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/\nSwedish (Sv), in addition to Russian (Ru)6 to learn the intilization for fine-tuning. In our experiments, different combinations of source tasks are explored to see the effects from the source tasks.\nValidation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks. This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection.\nPreprocessing and ULR Initialization As described in §3.3, we initialize the query embedding vectors ✏kquery of all the languages. For each language, we use the monolingual corpora built from Wikipedia7 and the parallel corpus. The concatenated corpus is first tokenized and segmented using byte-pair encoding (BPE, Sennrich et al., 2016), resulting in 40, 000 subwords for each language. We then estimate word vectors using fastText (Bojanowski et al., 2016) and align them across all the languages in an unsupervised way\n6 A subsample of approximately 2M pairs from WMT’17. 7 We use the most recent Wikipedia dump (2018.5) from\nhttps://dumps.wikimedia.org/backup-index.html.\nusing MUSE (Conneau et al., 2018) to get multilingual word vectors. We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏u."
  }, {
    "heading": "4.2 Model and Learning",
    "text": "Model We utilize the recently proposed Transformer (Vaswani et al., 2017) as an underlying NMT system. We implement Transformer in this paper based on (Gu et al., 2018a)8 and modify it to use the universal lexical representation from §3.3. We use the default set of hyperparameters (dmodel = dhidden = 512, nlayer = 6, nhead = 8, nbatch = 4000, twarmup = 16000) for all the language pairs and across all the experimental settings. We refer the readers to (Vaswani et al., 2017; Gu et al., 2018a) for the details of the model. However, since the proposed metalearning method is model-agnostic, it can be easily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with attention (Bahdanau et al., 2015).\n8 https://github.com/salesforce/nonauto-nmt\nLearning We meta-learn using various sets of source languages to investigate the effect of source task choice. For each episode, by default, we use a single gradient step of language-specific learning with Adam (Kingma and Ba, 2014) per computing the meta-gradient, which is computed by the first-order approximation in Eq. (3).\nFor each target task, we sample training examples to form a low-resource task. We build tasks of 4k, 16k, 40k and 160k English tokens for each language. We randomly sample the training set five times for each experiment and report the average score and its standard deviation. Each fine-tuning is done on a training set, early-stopped on a validation set and evaluated on a test set. In default without notation, datasets of 16k tokens are used.\nFine-tuning Strategies The transformer consists of three modules; embedding, encoder and decoder. We update all three modules during metalearning, but during fine-tuning, we can selectively tune only a subset of these modules. Following (Zoph et al., 2016), we consider three fine-tuning\nstrategies; (1) fine-tuning all the modules (all), (2) fine-tuning the embedding and encoder, but freezing the parameters of the decoder (emb+enc) and (3) fine-tuning the embedding only (emb)."
  }, {
    "heading": "5 Results",
    "text": "vs. Multilingual Transfer Learning We metalearn the initial models on all the source tasks using either Ro-En or Lv-En as a validation task. We also train the initial models to be multilingual translation systems. We fine-tune them using the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the proposed meta-learning strategy and the multilingual, transfer learning strategy. As presented in Fig. 3, the proposed learning approach significantly outperforms the multilingual, transfer learning strategy across all the target tasks regardless of which target task was used for early stopping. We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches. With the proposed meta-learning and emb+enc fine-tuning, the final NMT systems trained using only a fraction of all available training examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.\nvs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the final performance. We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT performance (0 BLEU). It is however still lower than both the multi-NMT and meta-NMT.\nImpact of Validation Tasks Similarly to training any other neural network, meta-learning still requires early-stopping to avoid overfitting to a\nspecific set of source tasks. In doing so, we observe that the choice of a validation task has nonnegligible impact on the final performance. For instance, as shown in Fig. 3, Fi-En benefits more when Ro-En is used for validation, while the opposite happens with Tr-En. The relationship between the task similarity and the impact of a validation task must be investigated further in the future.\nTraining Set Size We vary the size of the target task’s training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy. We use the emb+enc fine-tuning on Ro-En and Fi-En. Fig. 4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task’s training set. The gap between the meta-learning and transfer learning grows as the size shrinks, confirming the effectiveness of the proposed approach on extremely lowresource language pairs.\nImpact of Source Tasks In Table 2, we present the results on all five target tasks obtained while varying the source task set. We first see that it is always beneficial to use more source tasks. Although the impact of adding more source tasks varies from one language to another, there is up to 2⇥ improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En). The same trend can be observed even without any fine-tuning (i.e., unsupervised translation, (Lample et al., 2017; Artetxe et al., 2017b)). In addition, the choice of source languages has different implications for different target languages. For instance, Ro-En benefits more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.\nTraining Curves The benefit of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig. 5. With the multilingual, transfer learning ap-\nproach, we observe that training rapidly saturates and eventually degrades, as the model overfits to the source tasks. MetaNMT on the other hand continues to improve and never degrades, as the metaobjective ensures that the model is adequate for fine-tuning on target tasks rather than for solving the source tasks.\nSample Translations We present some sample translations from the tested models in Table 3. Inspecting these examples provides the insight into the proposed meta-learning algorithm. For instance, we observe that the meta-learned model without any fine-tuning produces a word-by-word translation in the first example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization. The system however cannot reorder tokens from Turkish to English, as it has not seen any training example of Tr-En. After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation. A similar phenomenon is observed in the Ko-En example. These cases could be found across different language pairs."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high-resource languages pairs. We based the proposed algorithm on the recently proposed model-agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation, resulting in MetaNMT. Our extensive evaluation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the proposed MetaNMT significantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.\nThe proposed approach opens new opportunities for neural machine translation. First, it is a principled framework for incorporating various extra sources of data, such as source- and targetside monolingual corpora. Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems."
  }, {
    "heading": "Acknowledgement",
    "text": "This research was supported in part by the Facebook Low Resource Neural Machine Translation Award. This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure). KC thanks support by eBay, TenCent, NVIDIA and CIFAR."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning to learn by gradient descent by gradient descent",
    "authors": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W Hoffman", "David Pfau", "Tom Schaul", "Nando de Freitas."],
    "venue": "Advances in Neural Information Processing Systems, pages",
    "year": 2016
  }, {
    "title": "Learning bilingual word embeddings with (almost) no bilingual data",
    "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol-",
    "year": 2017
  }, {
    "title": "Unsupervised neural machine translation",
    "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre", "Kyunghyun Cho."],
    "venue": "arXiv preprint arXiv:1710.11041.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.04606.",
    "year": 2016
  }, {
    "title": "A teacher-student framework for zeroresource neural machine translation",
    "authors": ["Yun Chen", "Yang Liu", "Yong Cheng", "Victor OK Li."],
    "venue": "arXiv preprint arXiv:1705.00753.",
    "year": 2017
  }, {
    "title": "Zeroresource neural machine translation with multiagent communication game",
    "authors": ["Yun Chen", "Yang Liu", "Victor OK Li."],
    "venue": "arXiv preprint arXiv:1802.03116.",
    "year": 2018
  }, {
    "title": "Neural machine translation with pivot languages",
    "authors": ["Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu."],
    "venue": "arXiv preprint arXiv:1611.04928.",
    "year": 2016
  }, {
    "title": "On the properties of neural machine translation: Encoder–Decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.",
    "year": 2014
  }, {
    "title": "Word translation without parallel data",
    "authors": ["Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou"],
    "venue": "International Conference on Learning Representations",
    "year": 2018
  }, {
    "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
    "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine."],
    "venue": "arXiv preprint arXiv:1703.03400.",
    "year": 2017
  }, {
    "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism",
    "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Zero-resource translation with multi-lingual neural machine translation",
    "authors": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann Dauphin."],
    "venue": "arXiv preprint arXiv:1705.03122.",
    "year": 2017
  }, {
    "title": "Nonautoregressive neural machine translation",
    "authors": ["Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher."],
    "venue": "ICLR.",
    "year": 2018
  }, {
    "title": "Universal neural machine translation for extremely low resource languages",
    "authors": ["Jiatao Gu", "Hany Hassan", "Jacob Devlin", "Victor OK Li."],
    "venue": "arXiv preprint arXiv:1802.05368.",
    "year": 2018
  }, {
    "title": "Dynamic neural turing machine with continuous and discrete addressing schemes",
    "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Neural computation, 30(4):857–884.",
    "year": 2018
  }, {
    "title": "On using monolingual corpora in neural machine translation",
    "authors": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1503.03535.",
    "year": 2015
  }, {
    "title": "Hypernetworks",
    "authors": ["David Ha", "Andrew Dai", "Quoc V Le."],
    "venue": "arXiv preprint arXiv:1609.09106.",
    "year": 2016
  }, {
    "title": "Toward multilingual neural machine translation with universal encoder and decoder",
    "authors": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel."],
    "venue": "arXiv preprint arXiv:1611.04798.",
    "year": 2016
  }, {
    "title": "Dual learning for machine translation",
    "authors": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tieyan Liu", "Wei-Ying Ma."],
    "venue": "Advances in Neural Information Processing Systems, pages 820–828.",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Six challenges for neural machine translation",
    "authors": ["Philipp Koehn", "Rebecca Knowles."],
    "venue": "arXiv preprint arXiv:1706.03872.",
    "year": 2017
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-",
    "year": 2003
  }, {
    "title": "Human-level concept learning through probabilistic program induction",
    "authors": ["Brenden M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum."],
    "venue": "Science, 350(6266):1332–1338.",
    "year": 2015
  }, {
    "title": "Unsupervised machine translation using monolingual corpora only",
    "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc’Aurelio Ranzato"],
    "venue": "arXiv preprint arXiv:1711.00043",
    "year": 2017
  }, {
    "title": "Fully character-level neural machine translation without explicit segmentation",
    "authors": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."],
    "venue": "arXiv preprint arXiv:1610.03017.",
    "year": 2016
  }, {
    "title": "Emergent translation in multi-agent communication",
    "authors": ["Jason Lee", "Kyunghyun Cho", "Jason Weston", "Douwe Kiela."],
    "venue": "arXiv preprint arXiv:1710.06922.",
    "year": 2017
  }, {
    "title": "Key-value memory networks for directly reading documents",
    "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1606.03126.",
    "year": 2016
  }, {
    "title": "Meta-learning with temporal convolutions",
    "authors": ["Nikhil Mishra", "Mostafa Rohaninejad", "Xi Chen", "Pieter Abbeel."],
    "venue": "arXiv preprint arXiv:1707.03141.",
    "year": 2017
  }, {
    "title": "A stochastic approximation method",
    "authors": ["Herbert Robbins", "Sutton Monro."],
    "venue": "The annals of mathematical statistics, pages 400–407.",
    "year": 1951
  }, {
    "title": "Improving neural machine translation models with monolingual data",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "arXiv preprint arXiv:1511.06709.",
    "year": 2015
  }, {
    "title": "Edinburgh neural machine translation systems for wmt 16",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "arXiv preprint arXiv:1606.02891.",
    "year": 2016
  }, {
    "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "authors": ["Samuel L Smith", "David HP Turban", "Steven Hamblin", "Nils Y Hammerla."],
    "venue": "arXiv preprint arXiv:1702.03859.",
    "year": 2017
  }, {
    "title": "Prototypical networks for few-shot learning",
    "authors": ["Jake Snell", "Kevin Swersky", "Richard Zemel."],
    "venue": "Advances in Neural Information Processing Systems, pages 4080–4090.",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quôc Lê."],
    "venue": "NIPS.",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "arXiv preprint arXiv:1706.03762.",
    "year": 2017
  }, {
    "title": "Matching networks for one shot learning",
    "authors": ["Oriol Vinyals", "Charles Blundell", "Tim Lillicrap", "Daan Wierstra"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Unsupervised neural machine translation with weight sharing",
    "authors": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu."],
    "venue": "arXiv preprint arXiv:1804.09057.",
    "year": 2018
  }, {
    "title": "Exploiting source-side monolingual data in neural machine translation",
    "authors": ["Jiajun Zhang", "Chengqing Zong."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545.",
    "year": 2016
  }, {
    "title": "Earth mover’s distance minimization for unsupervised bilingual lexicon induction",
    "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1934–",
    "year": 2017
  }, {
    "title": "Transfer learning for lowresource neural machine translation",
    "authors": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."],
    "venue": "arXiv preprint arXiv:1604.02201.",
    "year": 2016
  }],
  "id": "SP:a75869d69cc86f501939c237ae4711aa2885f6a6",
  "authors": [{
    "name": "Jiatao Gu",
    "affiliations": []
  }, {
    "name": "Yong Wang",
    "affiliations": []
  }, {
    "name": "Yun Chen",
    "affiliations": []
  }, {
    "name": "Kyunghyun Cho",
    "affiliations": []
  }, {
    "name": "Victor O.K. Li",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT). We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences).",
  "title": "Meta-Learning for Low-Resource Neural Machine Translation"
}