{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Probabilistic graphical models provide a general framework to conveniently build probabilistic models in a modular and compact way. They are commonly used in statistics, computer vision, natural language processing, machine learning and many related fields (Wainwright & Jordan, 2008). The success of graphical models depends largely on the availability of efficient inference algorithms. Unfortunately, exact inference is intractable in general, making approximate inference an important research topic.\nApproximate inference algorithms generally adopt a variational approach or a sampling approach. The variational approach formulates the inference problem as an optimi-\n1Australian National University, Canberra, Australia. 2National University of Singapore, Singapore. 3Queensland University of Technology, Brisbane, Australia. Correspondence to: Andrew Wrigley <andrew.wrigley@anu.edu.au>, Wee Sun Lee <leews@comp.nus.edu.sg>, Nan Ye <n.ye@qut.edu.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nsation problem and constructs approximations by solving relaxations of the optimisation problem. A number of wellknown inference algorithms can be seen as variational algorithms, such as loopy belief propagation, mean-field variational inference, and generalized belief propagation (Wainwright & Jordan, 2008). The sampling approach uses sampling to approximate either the underlying distribution or key quantities of interest. Commonly used sampling methods include particle filters and Markov-chain Monte Carlo (MCMC) algorithms (Andrieu et al., 2003).\nOur proposed algorithm, tensor belief propagation (TBP), can be seen as a sampling-based algorithm. Unlike particle filters or MCMC methods, which sample states (also known as particles), TBP samples functions in the form of rank-1 tensors. Specifically, we use a data structure commonly used in exact inference, the junction tree, and perform approximate message passing on the junction tree using messages represented as mixtures of rank-1 tensors.\nWe assume that each factor in the graphical model is originally represented as a tensor decomposition (mixture of rank-1 tensors). Under this assumption, all messages and intermediate factors also have the same representation. Our key observation is that marginalisation can be performed efficiently for mixtures of rank-1 tensors, and multiplication can be approximated by sampling. This leads to an approximate message passing algorithm where messages and intermediate factors are approximated by low-rank tensors.\nWe provide analysis, giving conditions under which the method performs well. We compare TBP experimentally with several existing approximate inference methods using Ising models, random MRFs and two real-world datasets, demonstrating promising results."
  }, {
    "heading": "2. Related Work",
    "text": "Exact inference on tree-structured graphical models can be performed efficiently using belief propagation (Pearl, 1982), a dynamic programming algorithm that involves passing messages between nodes containing the results of intermediate computations. For arbitrary graphical models, the well-known junction tree algorithm (Shafer & Shenoy, 1990; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990) is commonly used. The model is first compiled into a junc-\ntion tree data structure and a similar message passing algorithm is then run over the junction tree. Unfortunately, for non-tree models the time and space complexity of the junction tree algorithm grows exponentially with a property of the graph called its treewidth. For high-treewidth graphical models, exact inference is intractable in general.\nOur work approximates the messages passed in the junction tree algorithm to avoid the exponential runtime caused by exact computations in high-treewidth models. Various previous work has taken the same approach. Expectation propagation (EP) (Minka, 2001) approximates messages by minimizing the Kullback-Leiber (KL) divergence between the actual message and its approximation. Structured message passing (Gogate & Domingos, 2013) can be considered as a special case of EP where structured representations, in particular algebraic decision diagrams (ADDs) and sparse hash tables, are used so that EP can be performed efficiently. In contrast to ADDs, the tensor decompositions used for TBP may provide a more compact representation for some problems. An ADD partitions a tensor into axis-aligned hyper-rectangles – it is possible to represent a hyper-rectangle using a rank-1 tensor but a rank-1 tensor is generally not representable as an axis-aligned rectangle. Furthermore, the supports of the rank-1 tensors in the mixture may overlap. However, ADD compresses hyperrectangles that share sub-structures and this may result in the methods having different strengths. Sparse tables, on the other hand, work well for cases with extreme sparsity.\nSeveral methods use sampled particles to approximate messages (Koller et al., 1999; Ihler & McAllester, 2009; Sudderth et al., 2010). To allow their algorithms to work well on problems with less sparsity, Koller et al. (1999) and Sudderth et al. (2010) use non-parametric methods to smooth the particle representation of messages. In contrast, we decompose each tensor into a mixture of rank-1 tensors and sample the rank-1 tensors directly, instead of through the intermediate step of sampling particles. Another approach, which pre-samples the particles at each node and passes messages between these pre-sampled particles was taken by Ihler & McAllester (2009). The methods of Ihler & McAllester (2009) and Sudderth et al. (2010) were also applied on graphs with loops using loopy belief propagation.\nXue et al. (2016) use discrete Fourier representations for inference via the elimination algorithm. The discrete Fourier representation is a special type of tensor decomposition. Instead of sampling, the authors perform approximations by truncating the Fourier coefficients, giving different approximation properties. Other related works include (Darwiche, 2000; Park & Darwiche, 2002; Chavira & Darwiche, 2005), where belief networks are compiled into compact arithmetic circuits (ACs). On the related problem of MAP inference, McAuley & Caetano (2011) show that junction tree\nclusters that factor over subcliques or consist only of latent variables yield improved complexity properties."
  }, {
    "heading": "3. Preliminaries",
    "text": "For simplicity we limit our discussion to Markov random fields (MRFs), but our results apply equally to Bayesian networks and general factor graphs. We focus only on discrete models. A Markov random field G is an undirected graph representing a probability distribution P (X1, . . . , XN ), such that P factorises over the maxcliques in G, i.e.\nP (X1, . . . , XN ) = 1\nZ ∏ c∈cl(G) φc(Xc) (1)\nwhere cl(G) is the set of max-cliques in G and Z =∑ X ∏ c∈cl(G) φc(Xc) ensures normalisation. We call the factors φc clique potentials or potentials.\nTBP is based on the junction tree algorithm (see e.g. (Koller & Friedman, 2009)). A junction tree is a special type of cluster graph, i.e. an undirected graph with nodes called clusters that are associated with sets of variables rather than single variables. Specifically, a junction tree is a cluster graph that is a tree and which also satisfies the running intersection property. The running intersection property states that if a variable is in two clusters, it must also be in every cluster on the path that connects the two clusters.\nThe junction tree algorithm is essentially the well-known belief propagation algorithm applied to the junction tree after the cluster potentials have been initialized. At initialisation, each clique potential is first associated with a cluster. Each cluster potential Φt(Xt) is computed by multiplying all the clique potentials φc(Xc) associated with the cluster Xt. Thereafter, the algorithm is defined recursively in terms of messages passed between neighbouring clusters. A message is always a function of the variables in the receiving cluster, and represents an intermediate marginalisation over a partial set of factors. The messagemt→s(Xs) sent from a cluster t to a neighbouring cluster s is defined recursively by\nmt→s(Xs) = ∑\nXt\\Xs\nΦt(Xt) ∏\nu∈N(t)\\{s}\nmu→t(Xt), (2)\nwhere N(t) is the set of neighbours of t. Since the junction tree is singly connected, this recursion is well-defined. After all messages have been computed, the marginal distribution on a cluster of variables Xs is computed using\nPs(Xs) ∝ Φs(Xs) ∏\nt∈N(s)\nmt→s(Xs), (3)\nand univariate marginals can be computed by summation over cluster marginals. The space and time complexity of\nthe junction tree inference algorithm is exponential in the induced width of the graph, i.e. the number of variables in the largest tree cluster minus 1 (Koller & Friedman, 2009). The lowest possible induced width (over all possible junction trees for the graph) is defined as the treewidth of the graph1."
  }, {
    "heading": "4. Tensor Belief Propagation",
    "text": "The TBP algorithm we propose (Algorithm 1) is the same as the junction tree algorithm except for the approximations at line 1 and line 4, which we describe below.\nAlgorithm 1 Tensor Belief Propagation input Clique potentials {φc(Xc)}, junction tree J . output Approximate cluster marginals {P̃s(Xs)} for all s.\n1: For each cluster Xt, compute Φ̃t(Xt) ≈ ∏ c φc(Xc),\nwhere the product is over all cliques c associated with t. 2: while there is any unsent message do 3: Pick an unsent message mt→s(Xs) with all messages to t from neighbours other than s sent. 4: Send m̃t→s(Xs) ≈\n∑ Xt\\Xs Φ̃t(Xt) ∏ u∈N(t)\\{s} m̃u→t(Xt).\n5: end while 6: return P̃s(Xs) ∝ Φ̃s(Xs) ∏ t∈N(s) m̃t→s(Xs).\nThere are two challenges in applying the junction tree algorithm to high-treewidth graphical models: representing the intermediate potential functions, and computing them. For representation, using tables to represent the cluster potentials and messages requires space exponential in the induced width. For computation, the two operations relevant to the complexity of the algorithm are marginalisation over a subset of variables in a cluster and multiplication of multiple factors. When clusters become large, these operations become intractable unless additional structure can be exploited.\nTBP alleviates these difficulties by representing all potential functions as mixtures of rank-1 tensors. We show how to perform exact marginalisation of a mixture (required in line 4) efficiently, and how to perform approximate multiplication of mixtures using sampling (used for approximation in lines 1 and 4)."
  }, {
    "heading": "4.1. Mixture of Rank-1 Tensors",
    "text": "As we are concerned with discrete distributions, each potential φc can be represented by a multidimensional array, i.e. a tensor. Furthermore, a d-dimensional tensor T ∈ RN1×···×Nd can be decomposed into a weighted sum\n1Finding the treewidth of a graph is NP-hard; in practice we use various heuristics to construct the junction tree.\nof outer products of vectors as\nT = r∑ k=1 wk a 1 k ⊗ a2k ⊗ · · · ⊗ adk, (4)\nwhere wk ∈ R, aik ∈ RNi and ⊗ is the outer product, i.e.( a1k ⊗ a2k ⊗ · · · ⊗ adk ) i1,...,id = ( a1k ) i1 · · · · · ( adk ) id\n. We denote the vector of weights {wk} as w. The smallest r for which an exact r-term decomposition exists is called the rank of T and a decomposition (4) using this r is a tensor rank decomposition. This decomposition is known by several names including CANDECOMP/PARAFAC (CP) decomposition and Hitchcock decomposition (Kolda & Bader, 2009). In this paper, we assume without loss of generality that the weights are non-negative and sum to 1, giving a mixture of rank-1 tensors. Such a mixture forms a probability distribution over rank-1 tensors, which we refer to as a tensor belief. We also assume that the decomposition is non-negative, i.e. the rank-1 tensors are nonnegative, although the method can be extended to allow negative values.\nFor a (clique or cluster) potential function φs(Xs) over |Xs| = Ns variables, (4) is equivalent to decomposing φs into a sum of fully-factorised terms, i.e.\nφs(Xs) = r∑ k=1 wsk ψ s k(Xs)\n= r∑ k=1 wsk ψ s k,1(Xs1) · · ·ψsk,Ns(XsNs ). (5)\nThis observation allows us to perform marginalisation and multiplication operations efficiently."
  }, {
    "heading": "4.2. Marginalisation",
    "text": "Marginalising out a variable Xi from a cluster Xs simplifies in the same manner as if the distribution was fullyfactorised, namely∑ Xi φs(Xs) = r∑ k=1 wsk (∑ Xi ψsk,j(Xi) ) · (6)\nψsk,1(Xs1)ψ s k,2(Xs2) · · ·ψsk,Ns(XsNs )︸ ︷︷ ︸\nexcluding ψsk,j(Xi) where we simply push the sum ∑ Xi\ninside and only evaluate it over the univariate factor ψsk,j(Xi). We can then absorb this sum into the weights {wsk} and the result stays in decomposed form (5). To marginalise over multiple variables, we evaluate (6) for each variable in turn."
  }, {
    "heading": "4.3. Multiplication",
    "text": "The key observation for multiplication is that a mixture of rank-1 tensors can be treated as a probability distribution\nover the rank-1 tensors with expectation equal to the true function, by considering the weight of each rank-1 term wk as its probability.\nTo multiply two potentials φi and φj , we repeatedly sample rank-1 terms from each multiplicand to build the product. We draw a sample of K pairs of indices {(kr, lr)}Kr=1 independently from wi and wj respectively, and use the approximation\nφi(Xi) · φj(Xj) ≈ 1\nK K∑ r=1 ψikr (Xi) · ψ j lr (Xj). (7)\nThe approximation is also a mixture of rank-1 tensors, with the rank-1 tensors being the ψikr (Xi) · ψ j lr\n(Xj), and their weights being the frequencies of the sampled (kr, lr) pairs. This process is equivalent to drawing a sample of the same size from the distribution representing φi(Xi) ·φj(Xj) and hence provides an unbiased estimate of the product function. Multiplication of each pair ψikr (Xi) · ψ j lr\n(Xj) can be performed efficiently due to their factored forms. It is possible to extend the method to allow multiplication of more potential functions simultaneously but we only use pairwise multiplication in this paper, repeating the process as necessary to multiply more functions."
  }, {
    "heading": "4.4. Theoretical Analysis",
    "text": "For simplicity, we give results for binary MRFs. Extension to non-binary variables is straightforward. We assume that exact tensor decompositions are given for all initial clique potentials.\nTheorem 1. Consider a binary MRF with C max-cliques with potential functions represented as non-negative tensor decompositions. Consider the unnormalised distribution D(X) = ∏ c∈cl(G) φc(Xc). Let Di(Xi) = ∑ X\\Xi D(X) be the unnormalised marginal for variableXi. Assume that the values of the rank-1 tensors in any mixture resulting from pairwise multiplication is upper bounded by M , and that the approximation target for any cell in any multiplication operation is lower bounded by B. Let D̃i(Xi) be the estimates produced by TBP using a junction tree with an induced width T . With probability at least 1 − δ, for all i and xi,\n(1− )Di(xi) ≤ D̃i(xi) ≤ (1 + )Di(xi)\nif the sample size used for all multiplication operations is at least\nKmin( , δ) ∈ O ( C2 2 M\nB\n( logC + T + log 2\nδ\n)) .\nFurthermore, D̃i(Xi) remains a consistent estimator for Di(Xi) if B = 0.\nProof. We give the proof for the case B 6= 0 here. The consistency proof for the case B = 0 can be found in the supplementary material.\nThe errors in the unnormalised marginals are due to the errors in approximate pairwise multiplication defined by Equation (7). We first give bounds for the errors in all the pairwise multiplication by sampling operations, then derive the bounds for the errors in the unnormalised marginals.\nRecall that by the multiplicative Chernoff bound (see e.g. (Koller & Friedman, 2009)), for K i.i.d. random variables Y1, . . . , YK in range [0,M ] with expected value µ, we have\nP\n( 1\nK K∑ i=1 Yi /∈ [(1− ζ)µ, (1 + ζ)µ]\n) ≤ 2 exp ( −ζ 2µK\n3M\n) .\nThe number of pairwise multiplications required to initialize the cluster potentials {Φ̃t(Xt)} is at most C (line 1). The junction tree has at most C clusters, and thus at most 2(C − 1) messages need to be computed. Each message requires at most C pairwise multiplications (line 4). Hence the total number of pairwise multiplications needed is at most 2C2. Each pairwise multiplication involves functions defined on at most T binary variables, and thus it simultaneously estimates at most 2T values. Hence at most 2T+1C2 values are estimated by sampling in the algorithm.\nSince each estimate satisfies the Chernoff bound, we can apply a union bound to bound the probability that the estimate for any µj is outside [(1− ζ)µj , (1 + ζ)µj ], giving\nPζ\n( 1\nK K∑ i=1 Xji /∈ [(1− ζ)µj , (1 + ζ)µj ] for\nany estimate j ) ≤ 2T+2C2 exp ( −ζ 2BK\n3M\n) ,\nwhereB is a lower bound on the minimum value of all cells estimated during the algorithm. If we set an upper-bound on this error probability of δ and rearrange for K, we have that with probability at least δ, all estimates are within a factor of (1± ζ) of their true values when\nK ≥ 3 ζ2 M B ln\n2T+2C2\nδ . (8)\nWe now seek an expression for the sample size required for the unnormalised marginals to have small errors. First we argue that at most C + Q − 1 pairwise multiplications are used in the process of constructing the marginals at any node u, where Q is the number of clusters. This is true for the base case of a tree of size 1 as no messages need to be passed. As the inductive hypothesis, assume that the statement is true for trees of size less than n. The node u is\nconsidered as the root of the tree and by the inductive hypothesis the number of multiplications used in each subtree i is at most Ci + Qi − 1 where Ci and Qi are the number of clique potentials and clusters associated with subtree i. Summing them up and noting that we need to perform at most one additional multiplication for each clique potential associated with u for initialisation, and one additional multiplication for each subtree, gives us the required result. To simplify analysis, we bound C + Q − 1 by 2C from here onwards.\nAt worst, each pairwise multiplication results in an extra (1 ± ζ) factor in the bound. Since we are using a multiplicative bound, marginalisation operations have no effect on the bound. As we do no more than 2C multiplications, the final marginal estimates are all within a factor (1±ζ)2C of their true value.\nTo bound the marginal so that it is within a factor (1 ± ) of its true value for a chosen > 0, we note that choosing ζ = ln(1+ )2C implies (1 − ζ)\n2C ≥ 1 − and (1 + ζ)2C ≤ (1 + ):\n(1− ζ)2C = (\n1− ln(1 + ) 2C\n)2C (9)\n≥ (\n1− 2C\n)2C ≥ 1−\n(1 + ζ)2C = ( 1 + ln(1 + )\n2C\n)2C (10)\n≤ exp (ln(1 + )) = 1 + .\nIn (9) we use Bernoulli’s inequality together with ln(1 + ) ≤ , and in (10) we use (1 + x)r ≤ exp(rx).\nSubstituting this ζ into (8), we have that all marginal estimates are accurate within factor (1± ) with probability at least 1− δ, when\nK ≥ 12C 2 (ln(1 + ))2 M B ln 2T+2C2 δ (11)\nUsing the fact that ln(1 + ) ≥ · ln 2 for 0 ≤ ≤ 1, and 24 ln 2 < 35, we can relax this bound to\nK ≥ 35C 2 2 M\nB\n( logC + T + log 2\nδ\n) .\nCorollary 1. Under the same conditions as Theorem 1, with probability at least 1− δ, the normalised marginal estimates p̃i(xi) satisfy\n(1− γ)pi(xi) ≤ p̃i(xi) ≤ (1 + γ)pi(xi)\nfor all i and xi, if the sample size used for all multiplication operations is at least\nK ′min(γ, δ) ∈ O ( C2\nγ2 M B\n( logC + T + log 1\nδ\n)) .\nProof. Suppose the unnormalised marginals have relative error bounded by (1± ), i.e.\n(1− )Di(xi) ≤ D̃i(xi) ≤ (1 + )Di(xi)\nfor all i and xi. Then we have\np̃i(xi) = D̃i(xi)∑ xi D̃i(xi) ≤ (1 + )Di(xi)∑ xi (1− )Di(xi) = 1 + 1− pi(xi).\nTo bound the relative error on p̃i(xi) to (1 + γ), we set\n1 + 1− = 1 + γ =⇒ = γ γ + 2 .\nSince 1 2 = ( γ+2 γ )2 < ( 3 γ )2 = O ( 1 γ2 ) for γ < 1, the increase in Kmin required to bound the normalised estimates rather than the unnormalised estimates is at most a constant. Thus,\nK ′min(γ, δ) ∈ O ( C2\nγ2 M B\n( logC + T + log 1\nδ\n)) .\nas required. The negative side of the bound is similar.\nInterestingly, the sample size does not grow exponentially with the induced width, and hence the treewidth of the graph. As the inference problem is NP-hard, we expect the ratio M/B to be large in difficult problems. The M/B ratio comes from bounding the relative error when sampling a mixture φ(x) = ∑r k=1 wkψk(x). A more refined bound can be obtained by bounding maxk maxx ψk(x)/φ(x) instead; this bound would not grow as quickly if ψk(x) is always small whenever φ(x) is small. Understanding the properties of function classes where these bounds are small may help us understand when TBP works well.\nTheorem 1 suggests that that it may be useful to reweight the rank-1 tensors in a multiplicand to give a smaller M/B ratio. The following proposition gives a reweighting scheme that minimises the maximum value of the rank-1 tensors in a multiplicand, which leads to a smaller M with B fixed. Theorem 1 still holds with this reweighting.\nProposition 1. Let φ(x) = ∑r k=1 wkψk(x) where wk ≥\n0, ∑r k=1 wk = 1 and ψk(x) ≥ 0 for all x. Con-\nsider a reweighted representation ∑r k=1 w ′ kψ ′ k(x), where ψ′k(x) = wk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k = 1. Then maxk maxx ψ′k(x) is minimized when w ′ k ∝ wk maxx ψk(x).\nProof. Let ak = wk maxx ψk(x), and let v = maxk maxx ψ ′ k(x) = maxk\nak w′k . For any choice of w′, we have v ≥ ∑\nk ak∑ k w ′ k = ∑ k ak. The first inequality holds be-\ncause vw′k ≥ ak for any k by the definition of v. Since\nTensor Belief Propagation∑ k ak is a constant lower bound for v, and this is clearly achieved by setting w′k ∝ ak, we have the claimed result.\nNote that with ψk(x) represented as a rank-1 tensor, the maximum value over x can be computed quickly with the help of the factored structure.\nReweighting the rank-1 tensors as described in Proposition 1 and then sampling gives a form of importance sampling. Importance sampling is often formulated instead with the objective of minimizing the variance of the estimates. Since we expect lowering the variance of intermediate estimates to lead to lower variance on the final marginal estimates, we also examine the following alternative reweighting scheme. Proposition 2. Let φ(x) = ∑r k=1 wkψk(x) where wk ≥\n0, ∑r k=1 wk = 1. Consider a reweighted representa-\ntion ∑r k=1 w ′ kψ ′ k(x), where ψ ′ k(x) =\nwk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k = 1. Let φ̃(x) = 1 K ∑K i=1 ψ ′ Ki(x) be an estimator for φ(x), where each Ki is drawn independently from the categorical distribution with parameters {w′1, . . . , w′r}. Then φ̃(x) is unbiased, and the total variance ∑ x Var[φ̃\n′(x)] is minimized when w′k ∝ wk √∑ x ψk(x) 2.\nProof. Clearly φ̃(x) is an unbiased estimator for φ(x). The variance of this estimator is\nVar[φ̃(x)] = 1\nK Var[ψ′K′i(x)]\n= 1\nK\n( E[ψ′Ki(x) 2]− E[ψ′Ki(x)] 2 )\n= 1\nK (∑ k w′kψ ′ k(x) 2 − φ(x)2 )\n= 1\nK (∑ k w′k w2k w′2k ψk(x) 2 − φ(x)2 )\n= 1\nK (∑ k w2k w′k ψk(x) 2 − φ(x)2 ) .\nSince K and φ(x) are constant, we have\n{w′k}∗ = argmin {w′k} ∑ x Var[φ̃′(x)]\n= argmin {w′k} ∑ x ∑ k 1 w′k (wkψk(x)) 2,\nwhere each w′k ≥ 0, ∑ k w ′ k = 1. A straightforward application of the method of Lagrange multipliers yields\nw′k = wk √∑ x ψk(x) 2∑\nl wl √∑ x ψl(x) 2 .\nThe factored forms of rank-1 tensors again allows the importance reweighting to be computed efficiently.\nPropositions 1 and 2 could be applied directly to the algorithm by multiplying two potential functions fully before sampling. If each potential function has K terms, the multiplication would result in K2 terms which is computationally expensive. We only partially exploit the results of Propositions 1 and 2 in our algorithm. When multiplying two potential functions, we draw K pairs of rank-1 tensors from their respective distributions and multiply them as described earlier but re-weight the resulting mixture using either Proposition 1 or 2. We call the former max-norm reweighting, and the latter min-variance reweighting."
  }, {
    "heading": "5. Experiments",
    "text": "We present experiments on grid-structured Ising models, random graphs with pairwise Ising potentials, and two real-world datasets from the UAI 2014 Inference Competition (Gogate, 2014). We test our algorithm against commonly used approximate inference methods, namely loopy belief propagation (labelled BP), mean-field (MF), treereweighted BP (TRW) and Gibbs sampling using existing implementations from the libDAI package (Mooij, 2010). TBP was implemented in C++ inside the libDAI framework using Eigen (Guennebaud et al., 2010) and all tests were executed on a single core of a 1.4 GHz Intel Core i5 processor. In each experiment, we time the execution of TBP and allow Gibbs sampling the same wall-clock time. We run the other algorithms until convergence, which occurs quickly in each case (hence only the final performance is shown). Parameters used for BP, MF, TRW and Gibbs are given in the supplementary material. To build the junction tree, we use the min-fill heuristic2 implemented in libDAI."
  }, {
    "heading": "5.1. Grid-structured Ising Models",
    "text": "Figure 1 gives results for 10×10 Ising models. TheN×N Ising model is a planar grid-structured MRF described by the joint distribution\np(x1, . . . , xN2) = 1\nZ exp ∑ (i,j) wijxixj + ∑ i bixi  where (i, j) runs over all edges in the grid. Each variable Xi takes value either 1 or −1. In our experiments, we choose the wij uniformly from [−2, 2] (mixed interactions) or [0, 2] (attractive interactions), and the b uniformly from [−1, 1]. We use a symmetric rank-2 tensor decomposition for the pairwise potentials. We measure performance\n2Min-fill repeatedly eliminates variables that result in the lowest number of additional edges in the graph (Koller & Friedman, 2009).\nby the mean L1 error of marginal estimates over the N2 variables,\n1\nN2 N2∑ i=1 |Pexact(Xi = 1)− Papprox(Xi = 1)|.\nExact marginals were obtained using the exact junction tree algorithm (possible because the grid size is small). Results are all averages over 100 model instances generated randomly with the specified parameters; error bars show standard error. We give a description of the tensor decomposition and additional results for a range of grid sizes N and interaction strengths wij in the supplementary material.\nAs we increase the number of samples used for multiplication K, and hence running time, the performance of TBP improves as expected. On both attractive and mixed interaction models, loopy BP, mean-field and tree-reweighted BP perform poorly. Gibbs sampling performs poorly on models with attractive interactions but performs better on models with mixed interactions. This is expected since models with mixed interactions mix faster. Reweighting gives a noticeable improvement over not using reweighting; both max-norm reweighting and min-variance reweighting give very similar results (in Figure 1, they not easily differentiated). For the remainder of our experiments, we show results for max-norm reweighting only."
  }, {
    "heading": "5.2. Random Pairwise MRFs",
    "text": "We also test TBP on random binary N -node MRFs with pairwise Ising potentials. We construct the graphs by independently adding each edge (i, j) with probability 0.5, with the requirement that the resulting graph is connected. Pairwise potentials are of the same form as Ising models (i.e. exp[wijxixj ]), where interaction strengths wij are chosen uniformly from [0, 2] (attractive) or [−2, 2] (mixed) and the\nbi are chosen uniformly from [−1, 1].\nFigure 2a shows the performance of TBP with increasing sample size K on 15-node models. Similar to gridstructured Ising models, TBP performs well as the sample size is increased. Notably, TBP performs very well on attractive models where other methods perform poorly.\nFigure 2b shows the performance of the algorithms as the graph size is increased to 30 nodes. Interestingly, TBP starts to fail for mixed interaction models as the graph size is increased but remains very effective for attractive interaction models while other methods perform poorly."
  }, {
    "heading": "5.3. UAI Competition Problems",
    "text": "Finally, we show results of TBP on two real-world datasets from the UAI 2014 Inference Competition, namely the Promedus and Linkage datasets. We chose these two problems following Zhu & Ermon (2015). To compute the initial tensor rank decompositions, we use the non-negative cp nmu method from Bader et al. (2015), an iterative optimisation method based on (Lee & Seung, 2001). The initial potential functions are decomposed into mixtures with r components. We show results for r = 2 and r = 4. We measure performance by the mean L1 error over all states and all variables\n1\nNSi N∑ i=1 ∑ xi |Pexact(Xi = xi)− Papprox(Xi = xi)|,\nwhere Si is the number of states of variableXi. Results are averages over all problem instances in (Gogate, 2014).\nWe see that TBP outperforms Gibbs sampling on both problems3. On the Linkage dataset, using r = 2 mix-\n3We omit results for BP, MF and TRW for these problems because the libDAI package was unable to run them.\nture components for the initial decompositions performs best and increasing r does not improve performance. On the Promedus dataset r = 4 performs better; in this case, increasing r may improve the accuracy of the decomposition of the initial potential functions. For reference, the marginal error achieved by the random projection method of Zhu & Ermon (2015) is 0.09 ± 0.02 for Linkage and 0.21 ± 0.06 for Promedus. TBP with K = 105 achieved error of 0.08±0.01 for Linkage and 0.12±0.01 for Promedus despite using substantially less running time 4."
  }, {
    "heading": "6. Conclusion and Future Work",
    "text": "We proposed a new sampling-based approximate inference algorithm, tensor belief propagation, which uses a mixtureof-rank-1-tensors representation to approximate cluster potentials and messages in the junction tree algorithm. It\n4TBP with K = 105 takes an average across all problem instances of approximately 10 minutes (Linkage) and 3 minutes (Promedus) per problem on our machine, and does not differ substantially for r = 2 vs r = 4. The results described by Zhu & Ermon (2015) were comparable in running time to 10 million iterations of Gibbs sampling, which we estimate would take several hours on our machine without parallelism.\ngives consistent estimators, and performs well on a range of problems against well-known algorithms.\nIn this paper, we have not addressed how to perform the initial tensor decomposition. There exist well-known optimisation algorithms for this problem to minimize Euclidean error (Kolda & Bader, 2009), though it is not clear that this is the best objective for the purpose of TBP. We are currently investigating the best way of formulating and solving this optimisation problem to yield accurate results during inference. Further, when constructing the junction tree, it is not clear whether min-fill and other well-known heuristics remain appropriate for TBP. In particular, these cost functions aim to minimise the induced width of the junction tree which is no longer directly relevant to the performance of the algorithm. Further work may investigate other heuristics, for example with the goal of minimising cluster variance.\nFinally, approximate inference algorithms have applications in learning (for example, in the gradient computations when training restricted Boltzmann machines) and thus TBP may improve learning as well as inference in some applications."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the anonymous reviewers for their helpful comments. This work is supported by NUS AcRF Tier 1 grant R-252-000-639-114 and a QUT Vice-Chancellor’s Research Fellowship Grant."
  }],
  "year": 2017,
  "references": [{
    "title": "An introduction to MCMC for machine learning",
    "authors": ["Andrieu", "Christophe", "De Freitas", "Nando", "Doucet", "Arnaud", "Jordan", "Michael I"],
    "venue": "Machine learning,",
    "year": 2003
  }, {
    "title": "MATLAB Tensor Toolbox Version 2.6. http://www.sandia.gov/ ̃tgkolda/TensorToolbox",
    "authors": ["Bader", "Brett W", "Kolda", "Tamara G"],
    "venue": "In IJCAI,",
    "year": 2015
  }, {
    "title": "A Differential Approach to Inference in Bayesian Networks",
    "authors": ["Darwiche", "Adnan"],
    "venue": "In Uncertainty in Artificial Intelligence (UAI),",
    "year": 2000
  }, {
    "title": "Structured Message Passing",
    "authors": ["Gogate", "Vibhav", "Domingos", "Pedro"],
    "venue": "In Uncertainty in Artificial Intelligence (UAI),",
    "year": 2013
  }, {
    "title": "Particle Belief Propagation",
    "authors": ["Ihler", "Alexander T", "McAllester", "David A"],
    "venue": "In AISTATS, pp",
    "year": 2009
  }, {
    "title": "An algebra of Bayesian belief universes for knowledge-based systems",
    "authors": ["Jensen", "Finn Verner", "Olesen", "Kristian G", "Andersen", "Stig Kjaer"],
    "year": 1990
  }, {
    "title": "Tensor decompositions and applications",
    "authors": ["Kolda", "Tamara G", "Bader", "Brett W"],
    "venue": "SIAM review,",
    "year": 2009
  }, {
    "title": "Probabilistic Graphical Models: Principles and Techniques. Adaptive Computation and Machine Learning",
    "authors": ["D. Koller", "N. Friedman"],
    "year": 2009
  }, {
    "title": "Local computations with probabilities on graphical structures and their application to expert systems",
    "authors": ["Lauritzen", "Steffen L", "Spiegelhalter", "David J"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
    "year": 1988
  }, {
    "title": "Algorithms for non-negative matrix factorization",
    "authors": ["Lee", "Daniel D", "Seung", "H Sebastian"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2001
  }, {
    "title": "Faster Algorithms for Max-Product Message-Passing",
    "authors": ["McAuley", "Julian J", "Caetano", "Tibério S"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Expectation propagation for approximate Bayesian inference",
    "authors": ["Minka", "Thomas P"],
    "venue": "In Proceedings of the Seventeenth conference on Uncertainty in Artificial Intelligence,",
    "year": 2001
  }, {
    "title": "libDAI: A Free and Open Source C++ Library for Discrete Approximate Inference in Graphical Models",
    "authors": ["Mooij", "Joris M"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "A Differential Semantics for Jointree Algorithms",
    "authors": ["Park", "James D", "Darwiche", "Adnan"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2002
  }, {
    "title": "Reverend Bayes on inference engines: A distributed hierarchical approach",
    "authors": ["Pearl", "Judea"],
    "venue": "In AAAI,",
    "year": 1982
  }, {
    "title": "Nonparametric Belief Propagation",
    "authors": ["Sudderth", "Erik B", "Ihler", "Alexander T", "Isard", "Michael", "Freeman", "William T", "Willsky", "Alan S"],
    "venue": "Communications of the ACM,",
    "year": 2010
  }, {
    "title": "Graphical models, exponential families, and variational inference",
    "authors": ["Wainwright", "Martin J", "Jordan", "Michael I"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2008
  }, {
    "title": "Variable Elimination in Fourier Domain",
    "authors": ["Xue", "Yexiang", "Ermon", "Stefano", "Lebras", "Ronan", "Gomes", "Carla P", "Selman", "Bart"],
    "venue": "In Proc. 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "A Hybrid Approach for Probabilistic Inference using Random Projections",
    "authors": ["Zhu", "Michael", "Ermon", "Stefano"],
    "venue": "In ICML, pp. 2039–2047,",
    "year": 2015
  }],
  "id": "SP:a463e504827e93172076b41105abaaad1315b875",
  "authors": [{
    "name": "Andrew Wrigley",
    "affiliations": []
  }, {
    "name": "Wee Sun Lee",
    "affiliations": []
  }, {
    "name": "Nan Ye",
    "affiliations": []
  }],
  "abstractText": "We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm. Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors. Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling. Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable. We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.",
  "title": "Tensor Belief Propagation"
}