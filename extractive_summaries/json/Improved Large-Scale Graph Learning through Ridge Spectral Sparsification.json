{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Graphs are a very effective data structure to represent relationships between entities (e.g., social and collaboration networks, influence graphs). Over the years, many machine learning problems have been defined and solved exploiting the graph representation, such as graph-regularized least squares (LAPRLS, Belkin et al. 2005), Laplacian smoothing (LAPSMO, Sadhanala et al. 2016) graph semi-supervised learning (SSL, Chapelle et al. 2010; Zhu et al. 2003), laplacian embedding (LE, Belkin & Niyogi 2001, and spectral\n1SequeL team, INRIA Lille - Nord Europe, France 2LCSL, IIT, Italy, and MIT, USA. 3New Jersey Institute of Technology, USA 4Facebook AI Research, Paris, France. Correspondence to: Daniele Calandriello <daniele.calandriello@iit.it>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nclustering (SC, Von Luxburg 2007). The intuition behind graph-based learning is that the information expressed by the graph helps to capture the underlying structure of the problem (e.g., a manifold), thus improving the learning. For instance, LAPSMO and SSL rely on the assumption that nodes that are close in the graph are more likely to have similar labels. Similarly, LE and SC try to find a lowdimensional representation of the nodes using the eigenvectors of the Laplacian of the graph. In general, given a graph G of n nodes and m edges, most of graph-based learning tasks require computing the minimum of a cost function based on the associated n× n Laplacian matrix LG , which contains m non-zero entries. Solving exactly such optimization problems amounts to O(n3) time and O(n2) space complexity in the worst case and they become infeasible even for mildly large/dense graphs.\nA complete review of the literature on large-scale graph learning is beyond the scope of this paper and we only consider methods that reduce learning space and time complexity starting from a given graph received as input.1 We identify mainly three possible approaches. We can (1) reduce runtime replacing the pseudo-inverse operator L+G with an iterative solver, (2) reduce time and space complexity replacing the large graph G with a sparser approximationH, or (3) reduce runtime and increase memory capacity by distributing the computation across multiple machines.\nIterative solvers. Iterative methods can solve a number of learning problems without explicitly constructing L+G (e.g., gradient descent, GD, for LAPSMO, iterative averaging for SSL, and the power method for SC). In this case we only need O(m) time per iteration. Unfortunately, all simple iterative methods (e.g., GD) converge in a number of iterations proportional to the condition number of the Laplacian, κ = λmax(LG)/λmin(LG), which may grow linearly with the number of nodes n, thus removing the advantage of the iterative method, whose complexity tends to O(n3) in the worst case. Advanced iterative methods, such as the preconditioned conjugate gradient, use preconditioning to find an accurate solution in a number of iterations independent of κ. Koutis et al. (2011) gives a nearly-linear solver for Laplacians or strongly diagonally dominant (SDD) matri-\n1Many algorithms reduce the complexity of graph learning at construction time but they cannot be applied to natural graphs (e.g., social graphs) and therefore we do not review them.\nces, that using a chain of preconditioners, converges in only O(m log(n)) time. As space and time costs scale with the number of edges, a natural desire is to reduce m by sparsifying and distributing the graph.\nGraph sparsification. The objective of sparsification methods is to remove redundant edges, so that the resulting sparse sub-graph can be easily stored in memory and efficiently manipulated to compute final solutions. A simple graph-sparsification technique is to sample nq (with q > 1) edges from G with probabilities proportional to the edge weights with replacement. While computationally very efficient, uniform sampling requires sampling a number of edges proportional to O(nµ(G)) (i.e., q ∝ µ(G)), where µ(G) is the coherence of the Laplacian matrix, and it can grow as large as n when the graph is highly structured (e.g., if there is a single edge e connecting two components of the graph we need to sample all of the edges of the graph— potentially O(n2)—to guarantee that we do not exclude e and generate an inappropriate H). A more refined approach is the k-neighbors (kN) sparsifier (Sadhanala et al., 2016), which performs local sparsifications node-by-node by keeping all edges at nodes with degree smaller than q, and samples them proportionally to their weights whenever the degree is bigger than q. While in certain structured graphs, this method may perform much better than uniform (Von Luxburg et al., 2014), in the general case q, still needs to scale with the coherence µ(G). A more effective method is to sample edges proportionally to their effective resistance, which intuitively measures the importance of an edge in preserving the minimum distance between two nodes. As a result, only relevant edges are kept and the sparsified graph could be reduced to O(npolylog(n)) edges. Nonetheless, computing effective resistances also requires the pseudo-inverse L+G , thus being as expensive as solving any graph-Laplacian learning problem.\nDistributed computing. When the number of edges m is too large to fit the whole graph in a single machine, we are forced to distribute the edges across multiple machines. At the same time, if the sparsifier construction or the downstream inference can be parallelized, we can also reduce their runtime. Unfortunately, distributing data and computation across multiple machines can cause large communication costs. For example, simple GD or label propagation methods require O(κ) iterations (and communication rounds) to converge and access to non-local (e.g., neighbors in a graph) data. While preconditioned solvers reduce the number of iterations, almost none of their memory access is local, thus making difficult to have efficient distributed implementations.\nContribution. In this paper, we propose a new approach that aims at integrating the benefits of the three different methods above. Using the large memory and computational capacity of distributed computing and leveraging the sequen-\ntial sparsification methods of Kelner & Levin (2013) and Calandriello et al. (2017), we show how to compute an accurate sparsifierH of graph G inO(n log3(n)) time,O(n log2(n)) work and O(n log(n)) memory, using only log(n) rounds of communication. Afterwards, learning tasks can be solved directly on LH on a single machine using near-linear time solvers, resulting in an overallO(n log3(n)) runtime. Moreover, we show that the regularization used in some graphbased learning algorithms allows using even sparser graphs. In particular, we introduce the notion of ridge effective resistance to obtain sparsifiers that are better adapted to solve Laplacian-regularized learning tasks (e.g., LAPSMO, SSL) and are smaller than standard spectral sparsifiers without compromising the performance of downstream tasks."
  }, {
    "heading": "2. Background",
    "text": "We use lowercase letters a for scalars, bold lowercase letters a for vectors and uppercase bold letters A for matrices. We use A B to denote that B − A is positive semidefinite (PSD), [A]i,j to indicate the (i, j)-th entry of A, and ordered the eigenvalues as λ1(A) ≤ . . . ≤ λn(A)."
  }, {
    "heading": "2.1. Graphs and graph Laplacian",
    "text": "We denote with G = (V, E), an undirected weighted graph with n nodes V and m edges E . Each edge ei,j ∈ E has a weight aei,j measuring the “similarity” between nodes i and j. Given graphs G and G′ over the same set of nodes V , G + G′ denotes the graph obtained by summing the weights of their edges. For graph G, we introduce the weighted adjacency matrix AG with entries [AG ]i,j = aei,j , the total weights A = ∑ e ae , and the diagonal degree matrix DG\nwith entries [DG ]i,i , ∑ j aei,j . The Laplacian of G is the PSD matrix LG , DG − AG . Furthermore, we assume that G is connected and thus LG has only one eigenvalue equal to 0 and Ker(LG) = 1. Let L+G be the pseudoinverse of LG and L −1/2 G = (L + G )\n1/2. For any node i = 1, . . . , n, we denote with χi ∈ Rn, the indicator vector, so that be ,√ ae(χi − χj) is the “edge” vector. If we denote with BG the m × n signed edge-vertex incidence matrix, then the Laplacian can be written as LG = ∑ e beb T e = B T GBG ."
  }, {
    "heading": "2.2. Learning on graphs",
    "text": "Given graph G and its Laplacian LG , we denote with f ∈ Rn, a labeling of its nodes, where [f ]i is the value associated with the i-th node. Many graph learning algorithms assume that the optimal labeling f? is smooth w.r.t. the graph, i.e., the quantity ∑ e ae([f\n?]ei − [f?]ej )2 = f?TLGf? is small. In the following, we review examples from the supervised, semi-supervised and unsupervised learning with graphs.\nLaplacian smoothing (LAPSMO) with Gaussian noise. Given a graph G on n nodes, let y , f? + ξ be a noisy\nmeasurement of f? with [ξ]i ∼ N (0, σ2). The goal of LAPSMO is to find a vector f̂ that accurately reconstructs f? under the graph smoothness assumption by solving\nf̂ , argmin f∈Rn\n(f − y)T(f − y) + λfTLGf\n= (λLG + I) −1y, (1)\nwhere λ is a regularization parameter.\nGraph semi-supervised learning (SSL). In SSL, the input f` is a partial observation of the labels f? for a subset S ⊂ [n] of nodes. The goal is to predict the labels fu of the unrevealed nodes. The harmonic function solution (HFS) by Zhu et al. (2003) solves the optimization problem\nf̂HFS , argmin f∈Rn\n1 ` (f − y)T`S(f − y) + λfTLGf\n= (λ`LG + IS) +yS , (2)\nwhere ` , |S| is the number of labeled nodes received as input, IS ∈ Rn×n is the identity matrix with zeros at nodes not in S , and yS , ISy ∈ Rn. Similarly, in local transductive regression (LTR) (Cortes et al., 2008), the optimization problem is\nf̂LTR , argmin f∈Rn\n(f − y)TC(f − y) + fT(LG + λI)f\n= (C−1(LG + λI) + I) −1yS , (3)\nwhere C is a diagonal matrix with entries c` for nodes in S, cu for entries not in S, and c` ≥ cu > 0. Spectral clustering (SC). Applying the Laplacian smoothness assumption, the goal of SC is to find k disjoint subset assignments such that the clusters are smooth w.r.t. the Laplacian. Let {fc}kc=1 be the cluster indicator vectors such that [fc]i , 1 if node i is in the c-th cluster and [fc]i , 0 otherwise. Denote with F ∈ Rn×k, the matrix containing the assignments, and let C be the space of feasible clustering, such that all fc are binary and each row of F contains only one non-zero entry. Since computing the minimum ratio-cut is NP-hard (Von Luxburg, 2007; Lee et al., 2014), even under constraints (Cucuringu et al., 2016), SC defines instead the relaxed problem\nF̂ , argmin F:FTF=Ik,fc⊥1\nTr(FTLF).\nOnce the relaxed solution is computed, we can use different heuristics to recover the clustering, such as thresholding or performing a k-means clustering on the F̂ matrix.\nComputational complexity. The problems above require either to compute an eigendecomposition of the Laplacian LG or to solve a linear system involving LG . Computing these exactly is not feasible when the number of nodes n and edges m grows. In particular, (a) storing LG in memory\nrequires O(m) space, and it is not feasible when m is large, (b) even if LG is sparse and m is small, the pseudo-inverse L+G might be dense, and thus computing and storing L + G exactly requires up to O(n3) time and O(n2) space."
  }, {
    "heading": "3. Distributed Spectral Sparsification",
    "text": "In this section, we describe a new, sequential, distributed, and efficient algorithm for graph sparsification that can be used as a preprocessing step to solve a large variety of downstream learning tasks, without significantly affecting their performance. We point out that while distributing data-agnostic sparsifiers (e.g. uniform sampling) is straightforward, distributing the computation of sparsifiers based on effective resistances requires a careful merging procedure to guarantee satisfactory memory vs. accuracy tradeoff, which is what we provide in this section."
  }, {
    "heading": "3.1. (ε, γ)-spectral sparsifiers",
    "text": "We start with the introduction of the notion of (ε, γ)sparsifier that is adapted for the learning tasks that use sparsified graph Laplacian.\nDefinition 1. A (ε, γ)-spectral sparsifier of G is a reweighted sub-graphH ⊆ G whose Laplacian LH satisfies\n(1− ε)LG − εγI LH (1 + ε)LG + εγI. (4)\nFor γ = 0, this definition reduces to the standard notion of ε-spectral sparsifier (Spielman & Teng, 2011). The main difference is that an (ε, γ)-spectral sparsifier allows for an extra additive error of order εγ. This change is directly motivated by the fact that the sparsifier H may be used in learning tasks whose solution may not be sensitive to small (additive) errors. As a result, (ε, γ)-spectral sparsifiers are able to further reduce the size of H w.r.t. (ε, 0)- sparsifiers, without significantly affecting the final learning performance. Formally, an ε-sparsifier preserves all the quadratic forms up to a small multiplicative (constant) error, and thus can be used to provide an accurate approximation to many important quantities such as graph cuts or eigenvalues. In fact, for all i ∈ [n], an ε-sparsifier guarantees that (1 − ε)λi(LG) ≤ λi(LH) ≤ (1 + ε)λi(LG). Nonetheless, in many learning tasks (e.g., LTR) the noise level in the signal f requires regularizing the solution so that the Laplacian LG itself is eventually replaced by LG + λI (e.g., Eq. 1). This corresponds to soft-thresholding the eigenvalues of the Laplacian, so that eigenvalues below λ are partially ignored. If λ is properly tuned w.r.t. the noise, the regularization increases stability and improves the learning performance. Therefore, constructing a sparsifier that accurately reconstructs all eigenvalues of LG may be wasteful, as it may require keeping most of the edges. As a result, in tasks where LG is regularized, it is better to use (ε, γ)-sparsifiers, as their additive error γI is homogeneous with the regu-\nlarization λI and their smaller size allows scaling to up.2 We now extend the results of Spielman & Srivastava (2011) for the construction of ε-spectral sparsifiers to the general case of (ε, γ)-sparsifiers. We redefine the edge effective resistance to account for the regularization. Definition 2. The γ-effective resistance of an edge e in graph G is defined as\nre(γ) , b T e ( LG + γI )−1 be. (5)\nThe “effective dimension” of the graph is the total sum of the γ-effective resistances, deff(γ) , ∑ e re(γ).\nWe can now construct a sparsifier H by sampling q times each edge with a probability proportional to its γ-effective resistance. More formally, the resulting (random) graph contains qe ∼ B(re(λ); q) copies of each edge, where B is the Binomial distribution, and its associated Laplacian is LH = ∑ e∈H qe/(qre(γ))beb T e , which is an unbiased estimator of LG . We can then apply existing results from sketching of PSD matrices (Alaoui & Mahoney, 2015) to prove thatH is a valid (ε, γ)-sparsifier. Proposition 1 (Cohen et al. 2017). Let ε > 0 and γ ≥ 0 be the accuracy parameters and 0 ≤ δ ≤ 1 the probability of error. Let H be the graph obtained by sampling edges in G with a probability proportional to their γ-effective resistances. If q ≥ 4 log(4n/δ)/ε2, then w.p. 1− δ,H is an (ε, γ)-sparsifier with O(deff(γ)q) edges.\nWe first notice that this result reduces to the one of Spielman & Srivastava (2011) for γ = 0. In fact, deff(0) = n − 1 for all graphs, thus matching the space requirement q for ε-sparsifiers. Nonetheless, as γ increases, the size of H reduces significantly. Using LG = BTGBG , the effective dimension deff(γ) can be conveniently rewritten as deff(γ) = Tr ( BTGBG(B T GBG+γI)\n−1) = n∑ i=2 λi(LG) λi(LG) + γ ,\nthus showing that deff(γ) is the “soft” rank of the Laplacian, where γ significantly reduces the contribution of small eigenvalues to the total sum. While in the worst case deff(γ) can be as large as n− 1, for a variety of graphs with rapidly decaying spectrum (Jamakovic & Mieghem, 2006; Samukhin et al., 2008; Zhan et al., 2010; Akoglu et al., 2015), deff(γ) may be significantly smaller than n− 1, thus reducing the number of edges q required to obtain an (ε, γ)-sparsifier."
  }, {
    "heading": "3.2. The algorithm",
    "text": "As pointed out in the introduction, the main limitation of effective-resistance-based sparsification is that the computation of re requires inverting the Laplacian matrix, thus resulting in a computational cost that already matches the cost\n2Whenever no regularization is required in the learning task (i.e., HFS, SC), we set γ = 0 and consider “standard” ε-sparsifiers.\nAlgorithm 1 The DiSRe algorithm. Input: G Output: HG\n1: Partition G into k sub-graphs: 2: H1,` ← G` ← {(ei,j , qe = 1, p̃1,e = 1)} 3: Initialize set S1 = {H1,`}k`=1 4: for h = 1, . . . , k − 1 do 5: Pick two sparsifiersHh,i′ ,Hh,i′ from Sh 6: H ← Merge-Resparsify(Hh,i,Hh,i′) 7: PlaceH back into Sh+1 8: end for 9: ReturnHG , the last sparsifier in Sk\nAlgorithm 2 Merge-Resparsify Require: (ε, γ)-sparsifiersHh,i,Hh,i′ of graphs Gh,i,Gh,i′ Ensure: H, an (ε, γ) sparsifier of Gh,i + Gh,i′\n1: InitializeH = Hh,i +Hh,i′ 2: For all e ∈ H, use a fast SDD solver to compute\nr̃h+1,e(γ)← (1− ε)bTe (LH + (1 + ε)γI)−1be\n3: Set probabilities p̃h+1,e ← min{r̃h+1,e(γ), p̃h,e} 4: Sample qh+1,e from B(p̃h+1,e/p̃h,e, qh,e) 5: ReturnH ← {(ei,j , qh+1,e, p̃h+1,e)} for all qh+1,e > 0\nof the learning tasks themselves. Moreover, large graphs cannot be stored in memory, and multiple passes over the graph would result in a disk access overhead larger than the computational cost. In order to avoid these problems, we adapt our previous work (Calandriello et al., 2017) in online sparsification and randomized linear algebra (see a thorough discussion and comparison at the end of the section) to obtain the distributed sequential resparsification (DiSRe) algorithm (Alg. 1).3\nThe structure. We represent a sparsifierH as a collection of weighted edgesH , {(ei,j , qe, p̃e)}, and the Laplacian can be reconstructed as LH , ∑ e∈H 1/p̃e(qe/q)beb T e . Intuitively, each edge e has an associated weight based on its probability p̃e, and a number of included copies qe. Keeping multiple copies of each edge helps the random LH to concentrate towards LG , where the maximum number of copies q for an edge trades-off success probability and the size ofH. We assume we have k machines. DiSRe begins by partitioning the graph G into k sub-graphs G` on n vertices and m` ≥ n edges, such that G = {G`}ki=` In other words, it splits the matrix BG into submatrices BGi by arbitrarily selecting a subset of rows. The sub-graphs are small enough that they can be stored in memory,4 and they are\n3Whenever the original graph contains m ≤ Õ(deff(γ))edges, there is no need to run DiSRe as the (ε, γ)-sparsifiers would not reduce the size of the graph.\n4Whenever this is not possible (i.e., m/k is too large to be\nalso obviously sparsifiers of themselves, therefore we can define an initial set of sparsifiers S1 , {H1,`}k`=1, with H1,` , {(ei,j , q1,e = q, p̃1,e = 1)}e∈Gl . With this definition,H1,` contains edges ei,j with unit weight p̃1,e = 1 and H1,` = G`. Starting from these initial sparsifiers, DiSRe proceeds through a sequence of merge and sparsify operations where two sparsifiers are first combined and then sparsified again to keep having manageable-size graphs at each step. While DiSRe can run on any arbitrary sequence of merges, we consider the most (computationally) effective scheme, where sparsifiers are merged two-by-two in parallel, thus inducing a balanced full binary merge tree (see\nFig. 2). For notational convenience, we consider that at each iteration h, the inner loop of Alg. 1 only merges two arbitrary sparsifiers from the pool of available sub-graphs Sh and merges them into a new sparsifier. In practice, multiple merge-and-sparsify operations can be executed in a parallel and asynchronous way. The size of Sh, number of sparsifiers present at layer h, is |Sh| = k−h+1. Therefore, a node in the tree corresponding to a sparsifier is uniquely identified by two indices {h, `} where h is the height of the layer and ` ≤ |Sh| is the index of the node in the layer. We also define the graph G{h,`} as the union of all sub-graphs G`′ that are reachable from node {h, `} as leaves (descendants of {h, `}). For example, in Fig. 2, sparsifier H3,1 in node {3, 1} approximates the graph G{3,2} = G3 + G4, where we highlight in red the descendant tree.\nThe resparsification. In Alg. 2 we detail how two arbitrary sparsifiers are combined to obtain a temporary graph H. While the merge operation simply combinesHh,i andHh,i′ by summing their weights, the resparsification aims at generating a valid sparsifier from the “original” sub-graph (Gh,i+Gh,i′), as if it was directly sparsified at the beginning. We first compute estimates r̃(γ) of the γ-effective resistance by using fast solvers to invert the strongly diagonal dominant LH + γI matrix. Instead of sampling edges in H directly proportionally to r̃(γ) (more precisely p̃h+1,e), we perform a “resampling” scheme where an edge e is preserved with a “reweighted” probability p̃h+1,e/p̃h,e. Intuitively, the overall\nstored on a single machine), we can simply apply the same merging scheme of DiSRe by loading small enough chunks of the graph and sparsifying them sequentially.\nsequence of resampling guarantees that at each step h+ 1, an edge e ∈ (Gh,i + Gh,i′) has the “correct” probability p̃h+1,e of being included in the sparsifier.\nPerformance. We now study the performance of DiSRe and its complexity. Time complexity refers to the amount of time necessary to compute the final solution and work complexity refers to the total amount of operations carried out by all machines to compute the final solution.\nTheorem 1. Let ε > 0 be the accuracy, 0 ≤ δ ≤ 1 the probability of error, and ρ , (1 + 3ε)/(1 − ε). Given an arbitrary graph G and an arbitrary merge tree structure, if DiSRe is run with parameter q , 26ρ log(3n/δ)/ε2, then each sub-graphsH{h,`} is an (ε, γ)-sparsifier of G{h,`} with at most 3qdeff(γ) edges with probability 1 − δ. Whenever the merge tree is balanced and k is big enough such that m/k ≤ 3qdeff(γ),5 then merge operations can be run in parallel across the machines with an overall time complexity of O(deff(γ) log3(n)), a total work O(m log3(n)), and O(log(n)) rounds of communication.\nDiscussion. Kelner & Levin (2013) proposed a sequential algorithm for graph sparsification that closely emulates the batch sampling of Spielman & Srivastava (2011) in a semi-streaming setting and incrementally constructs an ε-sparsifier. However, their proof had a flaw since they treated dependent variables as independent (Calandriello et al., 2016). Kyng et al. (2016) resolved the issues in the proof of Kelner & Levin (2013) and showed that a slightly modified algorithm can construct a sparsifier with O(n log(n)/ε2) edges in O(m log2(n)/ε2) time, matching the space complexity of batch sampling. The method proposed by Kyng et al. (2016) can be further improved by parallelizing its computation over multiple machines. Using the parallel sparsification algorithm of Koutis & Xu (2016), the time complexity can be reduced up to Õ(log6(n)). Nonetheless, since these methods require random access to the edges, they cannot be easily distributed (it would have O(m polylog(n)) communication cost) and scaled to graphs that cannot be stored on a single machine. Furthermore, the algorithm of Kyng et al. (2016) accurately reconstructs the whole spectrum of the Laplacian, which leads to sparsifiers whose number of edges scales linearly with n. On the other hand, in regularized learning tasks, the presence of multiplicative and additive spectral error allows creating smaller sparsifiers whose size scales with deff(γ). Notice that, for γ large enough, this possibly means sparsifiers with less than n− 1 edges, necessarily leading to disconnected graphs. Finally, note that merging two traditional ε-sparsifiers gives an ε-sparsifier, merging two (γ, ε)sparsifiers produces a less accurate (2γ, ε)-sparsifier. Therefore simple merge-and-reduce strategies (Feldman et al.,\n5This implies that there are enough machines so that the leaves in the merge tree already have relatively sparse sub-graphs.\n2013), which address every resparsification as independent, would either cumulate errors or require multiple passes over the data. Similarly to Kyng et al. (2016), DiSRe’s sequential Merge-Resparsify solves this problem (Appendix B).\nMixed additive-multiplicative reconstruction is studied more extensively in randomized matrix algebra (Drineas & Mahoney, 2017). Cohen et al. (2016) developed an efficient method to spectrally sparsify generic matrices up to (1± ε) multiplicative and γ-additive errors using an incremental sampling method based on ridge leverage scores (i.e., the analog of γ-effective resistances for matrices). If applied to graph Laplacians, their method adds edges incrementally and returns an (ε, γ)-sparsified graph with O(deff(γ) log2(n)) edges in O(m log(n)) time. Nonetheless, Cohen et al. (2016) provided only ε-sparsifiers, suggesting to set γ as small as possible, and did not explore the advantages possible in machine learning. Moreover, no existing (ε, γ)-sparsifier construction method can leverage both distribution and fast solvers. Cohen et al. (2016) can only add edges (but not remove them as DiSRe), preventing repeated merge-and-resparsify. Other streaming RLS sampling methods, such as by Cohen et al. (2017), use dense intermediate sketches, such as frequent directions (Ghashami et al., 2016), that are not Laplacians of a subgraph and cannot be easily paired with near-linear solvers for Laplacians."
  }, {
    "heading": "4. Downstream Guarantees",
    "text": "We now show how the spectral reconstruction guarantees provided by (ε, γ)-sparsifiers translate into guarantees on the quality of the approximate solutions computed usingH instead of G. We first introduce a result for ε-sparsifiers in SSL and then show how for regularized problems, (ε, γ)sparsification can further improve computational performance without loss in accuracy in LAPSMO."
  }, {
    "heading": "4.1. Generalization bounds for SSL",
    "text": "Given the closed form solutions of HFS (Eq. 2) and LTR (Eq. 3), we simply replace LG with LH and then run a nearlylinear time solver to obtain approximate solutions f̃HFS and f̃LTR. We compare approximate solutions to their exact counterparts in the context of algorithmic stability.\nDefinition 3. Let L be a transductive learning algorithm. We denote by f and f ′ the solutions obtained by running L on datasets V , (S, T ) and V , (S ′, T ′) respectively. L is uniformly β-stable w.r.t. the squared loss if there exists β ≥ 0 such that for any two partitions (S, T ) and (S ′, T ′) that differ by exactly one training (and test) point and for all i ∈ [n], we have |([f ]i − [y]i)2 − ([f ′]i − [y]i)2| ≤ β.\nThe stability of LTR was proven by Cortes et al. (2008). On the other hand, the singularity of the Laplacian may\nlead to unstable behavior in HFS due to the (γ`LG + IS)+ pseudo-inverse, with drastically different results for small perturbations of the dataset. For this reason, we take the Stable-HFS algorithm by Belkin et al. (2004), where an additional regularization term is introduced to restrict the space of admissible solutions to the space F , {f : 〈f ,1〉 = 0} of solutions orthogonal to the null space of LG (i.e., centered functions). As shown by Belkin et al. (2004), to satisfy the constraint, it is sufficient to set an additional regularization parameter µ to µ , ((γ`LG + IS)+yS)T1/((γ`LG + IS)+1)T1, and compute the solution f̂STA as f̂STA , (γ`LG + IS)+(yS − µ1). While Stable-HFS is more stable and thus more suited for theoretical analysis, its space and time requirement remains O(m) and cannot be applied to graphs with a large number of edges. Therefore, we again replace f̂STA with an approximate solution f̃STA computed using LH. Define R̂(f) , 1` ∑` i=1(f(xi)− y(xi))2 as the\nempirical error and R(f) , 1u ∑u i=1(f(xi)−y(xi))2 as the generalization.\nTheorem 2. Let G be a fixed (connected) graph with eigenvalues 0 = λ1(G) < λ2(G) ≤ . . . ≤ λn(G), and H an ε-sparsifier of G. Let y ∈ Rn be the labels of the nodes in G with |y(x)| ≤ c and F be the set of centered functions such that |f(x) − y(x)| ≤ 2c. Let S ⊂ V be a random subset of labeled nodes, if the labels yS are centered, then w.p. at least 1− δ (w.r.t. the random generation of the sparsifierH and the random subset of labeled points S) the resulting Stable-HFS solution satisfies\nR(f̃) ≤ R̂(f̂) + β + ( 2β + 4c2(`+ u)\n`u\n)√ π(`, u) ln 1δ\n2\n+ 1\n1− ε\n( 2(1 + ε)ε`γλ2(G)c ((1− ε)`γλ2(G)− 1)2 )2 , (6)\nwhere f̃ and f̂ are computed onH and G,\nπ(`, u) , `u `+ u− 0.5 2max{`, u} 2max{`, u} − 1 and β ≤ 3c √ `\n((1− ε)`γλ2(G)− 1)2 +\n4c\n(1− ε)`γλ2(G)− 1 ·\nThm. 2 (full proof in Appendix A) shows how approximating G withH impacts the generalization error as the number of labeled samples ` increases. If we set ε = 0, we recover the bound of Cortes et al. (2008), which depends only on R̂(f̂) and β. When ε > 0, we see from Eq. 6 that the two terms already present in the exact case are either unchanged (R̂(f̂)) or increase only by a constant factor β. Because of the approximation, a new error term (the last one in Eq. 6) is added to the bound, but we can see that it is negligible compared to β. In fact, it converges to zero asO(ε2/`2(1− ε)4) as ` grows and it is dominated by β for any constant value\nof ε. This means that increasing ε corresponds to a constant increase in the bound, regardless of the size of the problem. Consequently, ε can be freely chosen to trade off accuracy and space complexity (Thm. 1) depending on the problem constraints. Finally, because the eigenvalues present in the bound are the ones of the original graph, any additional knowledge on the spectral properties of the input graph can be easily included in the analysis. Therefore, it is straightforward to provide stronger guarantees for Sparse-HFS when combined with assumptions on the graph generating model. Finally, we remark the level of generality of this result that holds for the integration between HFS and any ε-accurate spectral sparsification method. We postpone computational considerations to the following subsection."
  }, {
    "heading": "4.2. Generalization bounds for LAPSMO",
    "text": "Starting from the closed form solution of LAPSMO (Eq. 1) we can replace the LG matrix with a sparsified Laplacian LH and using a fast linear solver, compute an approximate solution f̃ = (λLH + I)−1y in O(n log2(n)) time and O(n log(n)) space. Finally, we can decompose the error as ‖f? − f̃‖22 ≤ ‖f? − f̂‖22 + ‖f̂ − f̃‖22. The first term can be bounded using classical results from empirical process theory (Bühlmann & Van De Geer, 2011). We bound the second term in the following theorem.\nTheorem 3. For an arbitrary graph G and its (ε, γ)sparsifier, let f̂ be the LAPSMO solution computed using LG and f̃ the solution computed using LH. Then,\n‖f̃ − f̂‖22 ≤ ε2 1− ε (0.25 + λγ) ( λf̂TLG f̂ + λγ‖f̂‖22 ) ,\nwhere λ is the regularization of LAPSMO.\nFor ε-sparsifiers, Sadhanala et al. (2016) derive a similar bound ‖f̃ − f̂‖22 ≤ O(λf̂TLG f̂). Setting γ = 0, we recover their bound up to constants. When γ > 0 instead, additional error terms emerge due to the introduced bias. In particular, the term λγ‖f̂‖22 depends on the norm of the exact solution f̂ , which in turn depends on the value of λ. Nonetheless, when ‖f?‖22 is small, as is the case in our experiments, setting γ = 1/λ makes this term a constant, which is reflected by the good empirical performance. Computationally, for both Stable-HFS and LAPSMO, passing from computing a solution on the full graph to computing a solution on the sparsifier reduces the number of edges, which makes the memory and runtime plummet. Moreover, carefully distributing the sparsification process across multiple machines allows computing a final solution in a time independent from the number of edges, since the preprocessing sparsification step takes only O(n log3(n)) time, and the solution step only O(n log2(n)). Up to logarithmic terms, this results in an overall Õ(n) near-linear runtime,\nwithout any assumptions on the input graph. For graphs with a particularly favorable spectrum and problems with enough regularization, this is only Õ(deff(γ)), resulting in a potentially sub-linear runtime. This result, only possible due to a particular structure of learning problems, opens up unexplored possibilities that would not be possible for general graph problems."
  }, {
    "heading": "4.3. Bounds for other problems",
    "text": "Many other problems can be well approximated using (ε, γ)sparsifiers. For example, the cost of a SC solution evaluated on LH is very close to the cost evaluated on LG .\nProposition 2. For any rank k orthogonal projection FTF, ifH is an (ε, γ)-sparsifier of G, we have\nTr(FTLHF) ≤ (1 + ε) Tr(FTLGF) + εγk.\nTherefore, a clustering that well separates the sparsifier will also separate well the true graph. Similarly, we can obtain strong approximation guarantees for a variety of other Laplacian-based algorithms. Regularized problems such as LTR (Cortes et al., 2008), Laplacian-regularized least squares, and Laplacian SVM (Belkin et al., 2005) are of particular interest since the additive γ error is absorbed by the regularization and it is possible to provide strong generalization guarantees."
  }, {
    "heading": "5. Experiments",
    "text": "We empirically validate our theoretical findings by testing how (ε, γ)-sparsifiers improves computational complexity without sacrificing final accuracy.\nDataset. We run experiments on the Amazon co-purchase graph (Sadhanala et al., 2016). This graph fits our setting: It cannot be generated from vectorial data and is only artificially sparse, since the crawler that created it had no access to the true private co-purchase network held by Amazon. To compensate, Gleich & Mahoney (2015) use a densification procedure that given the graph adjacency matrix AG , computes all k-step neighbors AG,k , ∑k s=1 A s G . We make the graph unweighted for numerical stability. The final graph has n = 334, 863 nodes and m = 98, 465, 352 edges, with an average degree of 294. We followed an approach similar to Sadhanala et al. (2016) and introduce a hand-designed smooth signal as a target. We then perform 2000 iterations of the power method to compute an approximation of the smallest eigenvector vmin, which is used as a smooth function over the graph.\nBaselines. For all setups, we compute an “exact” solution (up to convergence error) using a fast linear solver. Computing this EXACT baseline requires O(m log(n)) time and O(m) space and achieves the best performance. Afterwards, we compare three different sparsification procedures to eval-\nuate if they can accelerate computation while preserving accuracy. We run DiSRe with different values of γ depending on the setting. For empirically strong heuristics, we attempted to uniformly subsample the edges, but at the sparsity level achieved by the other methods, the uniformly sampled sparsifier is disconnected and highly inaccurate. Instead, we compare to the state-of-the-art k-neighbors (kN) heuristic by Sadhanala et al. (2016), which is just as fast as uniform sampling and more accurate in practice.\nExperimental procedure. We repeat each experiment 10 times with different sparsifiers and report the average performance of f̃ on the specific task and its standard deviation. More details on experiments are given in the Appendix C."
  }, {
    "heading": "5.1. Laplacian smoothing with Gaussian noise",
    "text": "We set f? = vmin and test different levels of noise, log10(σ) ∈ {−3,−2,−1, 0}. After constructing the sparsifier H, we compute an approximate solution f̃ using LAPSMO (Eq. 2) with λ ∈ {10−3, 10−2, 10−1, 1, 10}. We measure the performance by the squared error D(f̃) = ‖f? − f̃‖22. As ‖f?‖22 = ‖vmin‖22 = 1, good values of D(f̃) should be below 1.\nAccuracy. In the interest of space, in Tab. 1, we report results for σ = {0.001, 0.01} and the best regularization λ for each method. We first notice that all sparsifiers are considerably smaller than the original graph, keeping only a small fraction of its edges. The smallest sparsifiers are obtained by DiSRe when γ is large. The comparison with DiSRe with γ = 0 (i.e., ε-sparsifier) confirms that the additive error translates into an extra compression of the resulting sparsifier. This also impacts the accuracy which degrades as γ increases. Nonetheless, we notice that while ε-sparsifiers perfectly match the accuracy of the exact method, even for large γ (and thus much smaller graph), DiSRe still outperforms kN, which has a significantly worse accuracy. Finally, we note that for γ = 0, the impact of q is as expected: Increasing q increases the size of the sparsifier and slightly improves the performance.\nComputational complexity. All algorithms require 90s to load the graph from disk. The preprocessing phase of kN takes slightly less than 1min, while DiSRe’s takes 12min\non 4 machines. For the solving step, EXACT is unsurprisingly the slowest, requiring 12min to compute an f̂ solution. Both kN and (ε, γ)-sparsifiers require 1–2min, depending on the number of edges preserved. Overall, preprocessing the graph with DiSRe before computing a solution does not introduce any overhead compared to EXACT (both take roughly 12min). We notice that while kN is overall faster, the time for DiSRe could be easily reduced by increasing the number of parallel processes when computing effective resistances or with a better network topology allowing pointto-point communication. Moreover, once we have access to an accurate ε-sparsifier, it is easier to solve problem repeatedly, e.g., to cross-validate regularization. For example, computing a solution for 4 different values of λ (see the appendix) is crucial for good performance and requires 48min for EXACT and only 20min for DiSRe. Finally, memory usage is reduced by a factor of 3 as EXACT requires over 30GB of memory to execute while DiSRe never exceeds 10GB. We expect these advantages to only grow larger as we scale to larger graphs."
  }, {
    "heading": "5.2. SSL with harmonic function solution",
    "text": "We also test DiSRe on a SSL problem. The labels are generated taking the sign of f? = vmin and ` ∈ {20, 346, 672, 1000} labels are revealed. The labeled nodes are chosen at random so that 0 and 1 labels are balanced in the dataset. We run Stable-HFS with λ ∈ {10−6, 10−4, 10−2, 1}. In Tab. 1, we report results for ` = {346, 672} and the best λ for each method. We run DiSRe with γ = 0 as Stable-HFS does not have any regularization and ε-sparsifiers are preferable. The average size of the sparsifiers is the same as before as they are agnostic to the learning task. Similar to the smoothing case, DiSRe achieves a performance that closely approximates the exact solution, despite the significant compression of the original graph. Furthermore, the effectiveness of the ε-sparsifier returned by DiSRe is confirmed by its comparison with kN, whose error is significantly worse. Finally, we notice that the computational analysis in the previous section holds for SSL as well. In fact, although the learning task is different, we use the same SSD solver to compute the HFS and thus the running time are comparable in the two tasks."
  }, {
    "heading": "Acknowledgements",
    "text": "Experiments presented in this paper were carried out using the Grid’5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER, and several universities as well as other organizations (see https://www.grid5000.fr). The research presented was also supported by European CHIST-ERA project DELTA, French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Otto-von-Guericke-Universität Magdeburg associated-team north-european project Allocate, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE240010-01) and BoB (n.ANR-16-CE23-0003). I. Koutis is supported by NSF CAREER award CCF-1149048."
  }],
  "year": 2018,
  "references": [{
    "title": "Graph based anomaly detection and description: a survey",
    "authors": ["L. Akoglu", "H. Tong", "D. Koutra"],
    "venue": "Data Mining and Knowledge Discovery,",
    "year": 2015
  }, {
    "title": "Fast randomized kernel methods with statistical guarantees",
    "authors": ["A.E. Alaoui", "M.W. Mahoney"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
    "authors": ["M. Belkin", "P. Niyogi"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2001
  }, {
    "title": "Regularization and Semi-Supervised Learning on Large Graphs",
    "authors": ["M. Belkin", "I. Matveeva", "P. Niyogi"],
    "venue": "In Conference on Learning Theory,",
    "year": 2004
  }, {
    "title": "On manifold regularization",
    "authors": ["M. Belkin", "P. Niyogi", "V. Sindhwani"],
    "venue": "In International conference on Artificial Intelligence and Statistics,",
    "year": 2005
  }, {
    "title": "Statistics for highdimensional data: methods, theory and applications",
    "authors": ["P. Bühlmann", "S. Van De Geer"],
    "venue": "Springer Science & Business Media,",
    "year": 2011
  }, {
    "title": "Analysis of kelner and levin graph sparsification algorithm for a streaming setting",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "year": 2016
  }, {
    "title": "Distributed sequential sampling for kernel matrix approximation",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "venue": "In International conference on Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Semi-supervised learning",
    "authors": ["O. Chapelle", "B. Schlkopf", "A. Zien"],
    "year": 2010
  }, {
    "title": "Online row sampling. In Approximation, Randomization, and Combinatorial Optimization",
    "authors": ["M.B. Cohen", "C. Musco", "J.W. Pachocki"],
    "venue": "Algorithms and Techniques,",
    "year": 2016
  }, {
    "title": "Input sparsity time low-rank approximation via ridge leverage score sampling",
    "authors": ["M.B. Cohen", "C. Musco"],
    "venue": "In Symposium on Discrete Algorithms,",
    "year": 2017
  }, {
    "title": "Stability of transductive regression algorithms",
    "authors": ["C. Cortes", "M. Mohri", "D. Pechyony", "A. Rastogi"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2008
  }, {
    "title": "Simple and scalable constrained clustering: a generalized spectral method",
    "authors": ["M. Cucuringu", "I. Koutis", "S. Chawla", "G. Miller", "R. Peng"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering",
    "authors": ["D. Feldman", "M. Schmidt", "C. Sohler"],
    "venue": "In Symposium on Discrete Algorithms,",
    "year": 2013
  }, {
    "title": "Frequent directions: Simple and deterministic matrix sketching",
    "authors": ["M. Ghashami", "E. Liberty", "J.M. Phillips", "D.P. Woodruff"],
    "venue": "SIAM Journal on Computing,",
    "year": 2016
  }, {
    "title": "Using local spectral methods to robustify graph-based learning algorithms",
    "authors": ["D.F. Gleich", "M.W. Mahoney"],
    "venue": "In Knowledge Discovery and Data Mining,",
    "year": 2015
  }, {
    "title": "The Laplacian spectrum of complex networks",
    "authors": ["A. Jamakovic", "P.V. Mieghem"],
    "venue": "In European Conference on Complex Systems,",
    "year": 2006
  }, {
    "title": "Spectral sparsification in the semi-streaming setting",
    "authors": ["J.A. Kelner", "A. Levin"],
    "venue": "Theory of Computing Systems,",
    "year": 2013
  }, {
    "title": "Simple parallel and distributed algorithms for spectral graph sparsification",
    "authors": ["I. Koutis", "S.C. Xu"],
    "venue": "Transactions on Parallel Computing,",
    "year": 2016
  }, {
    "title": "A nearly-m log n time solver for SDD linear systems",
    "authors": ["I. Koutis", "G.L. Miller", "R. Peng"],
    "venue": "In Symposium on Foundations of Computer Science,",
    "year": 2011
  }, {
    "title": "Approximate gaussian elimination for laplacians-fast, sparse, and simple",
    "authors": ["R. Kyng", "S. Sachdeva"],
    "venue": "In Foundations of Computer Science,",
    "year": 2016
  }, {
    "title": "A framework for analyzing resparsification algorithms",
    "authors": ["R. Kyng", "J. Pachocki", "R. Peng", "S. Sachdeva"],
    "venue": "In Symposium on Theory of Computing,",
    "year": 2016
  }, {
    "title": "Multiway spectral partitioning and higher-order cheeger inequalities",
    "authors": ["J.R. Lee", "S.O. Gharan", "L. Trevisan"],
    "venue": "Journal of the ACM,",
    "year": 2014
  }, {
    "title": "Stochastic dominance: Investment decision making under uncertainty",
    "authors": ["H. Levy"],
    "year": 2015
  }, {
    "title": "Graph sparsification approaches for laplacian smoothing",
    "authors": ["V. Sadhanala", "Wang", "Y.-X", "R. Tibshirani"],
    "venue": "In International conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Laplacian spectra of, and random walks on, complex networks: Are scale-free architectures really important",
    "authors": ["A.N. Samukhin", "S.N. Dorogovtsev", "J.F.F. Mendes"],
    "venue": "Physical Review E,",
    "year": 2008
  }, {
    "title": "Graph sparsification by effective resistances",
    "authors": ["D.A. Spielman", "N. Srivastava"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Spectral sparsification of graphs",
    "authors": ["D.A. Spielman", "Teng", "S.-H"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Freedman’s inequality for matrix martingales",
    "authors": ["J.A. Tropp"],
    "venue": "Electronic Communications in Probability,",
    "year": 2011
  }, {
    "title": "A tutorial on spectral clustering",
    "authors": ["U. Von Luxburg"],
    "venue": "Statistics and computing,",
    "year": 2007
  }, {
    "title": "Hitting and commute times in large random neighborhood graphs",
    "authors": ["U. Von Luxburg", "A. Radl", "M. Hein"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "On the distributions of Laplacian eigenvalues versus node degrees in complex networks",
    "authors": ["C. Zhan", "G. Chen", "L.F. Yeung"],
    "venue": "Physica A: Statistical Mechanics and its Applications,",
    "year": 2010
  }, {
    "title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions",
    "authors": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2003
  }],
  "id": "SP:b09ae5a34d8d338b67e466e6d5c2cdff6c798fc7",
  "authors": [{
    "name": "Daniele Calandriello",
    "affiliations": []
  }, {
    "name": "Ioannis Koutis",
    "affiliations": []
  }, {
    "name": "Alessandro Lazaric",
    "affiliations": []
  }, {
    "name": "Michal Valko",
    "affiliations": []
  }],
  "abstractText": "The representation and learning benefits of methods based on graph Laplacians, such as Laplacian smoothing or harmonic function solution for semi-supervised learning (SSL), are empirically and theoretically well supported. Nonetheless, the exact versions of these methods scale poorly with the number of nodes n of the graph. In this paper, we combine a spectral sparsification routine with Laplacian learning. Given a graph G as input, our algorithm computes a sparsifier in a distributed way in O(n log(n)) time, O(m log(n)) work and O(n log(n)) memory, using only log(n) rounds of communication. Furthermore, motivated by the regularization often employed in learning algorithms, we show that constructing sparsifiers that preserve the spectrum of the Laplacian only up to the regularization level may drastically reduce the size of the final graph. By constructing a spectrally-similar graph, we are able to bound the error induced by the sparsification for a variety of downstream tasks (e.g., SSL). We empirically validate the theoretical guarantees on Amazon co-purchase graph and compare to the state-of-the-art heuristics.",
  "title": "Improved Large-Scale Graph Learning through Ridge Spectral Sparsification"
}