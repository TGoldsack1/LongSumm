{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Sparse support vector machine (SVM) (Bi et al., 2003; Wang et al., 2006) is a powerful technique that can simultaneously perform classification by margin maximiza-\n*Equal contribution 1State Key Lab of CAD&CG, Zhejiang University, China 2Tencent AI Lab, Shenzhen, China 3University of Michigan, USA. Correspondence to: Jie Wang <jiewangustc@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntion and variable selection by `1-norm penalty. The last few years have witnessed many successful applications of sparse SVMs, such as text mining (Joachims, 1998; Yoshikawa et al., 2014), bioinformatics (Narasimhan & Agarwal, 2013) and image processing (Mohr & Obermayer, 2004; Kotsia & Pitas, 2007). Many algorithms (Hastie et al., 2004; Fan et al., 2008; Catanzaro et al., 2008; Hsieh et al., 2008; Shalev-Shwartz et al., 2011) have been proposed to efficiently solve sparse SVM problems. However, the applications of sparse SVMs to large-scale learning problems, which involve a huge number of samples and extremely high-dimensional features, remain challenging.\nAn emerging technique, called screening (El Ghaoui et al., 2012), has been shown to be promising in accelerating large-scale sparse learning. The essential idea of screening is to quickly identify the zero coefficients in the sparse solutions without solving any optimization problems such that the corresponding features or samples—that are called inactive features or samples—can be removed from the training phase. Then, we only need to perform optimization on the reduced datasets instead of the full datasets, leading to substantial savings in the computational cost and memory usage. Here, we need to emphasize that screening differs greatly from feature selection methods, although they look similar at the first glance. To be precise, screening is devoted to accelerating the training of many sparse models including Lasso, Sparse SVM, etc., while feature selection is the goal of these models. In the past few years, many screening methods are proposed for a large set of sparse learning techniques, such as Lasso (Tibshirani et al., 2012; Xiang & Ramadge, 2012; Wang et al., 2013), group Lasso (Ndiaye et al., 2016), `1-regularized logistic regression (Wang et al., 2014), and SVM (Ogawa et al., 2013). Empirical studies indicate that screening methods can lead to orders of magnitude of speedup in computation time.\nHowever, most existing screening methods study either feature screening or sample screening individually (Shibagaki et al., 2016) and their applications have very different scenarios. Specifically, to achieve better performance (say, in terms of speedup), we favor feature screening methods when the number of features p is much larger than the number of samples n, while sample screening methods are\npreferable when n p. Note that there is another class of sparse learning techniques, like sparse SVMs, which induce sparsities in both feature and sample spaces. All these screening methods are helpless in accelerating the training of these models with large n and p. We also cannot address this problem by simply combining the existing feature and sample screening methods. The reason is that they could mistakenly discard relevant data as they are specifically designed for different sparse models. Recently, Shibagaki et al. (Shibagaki et al., 2016) consider this problem and propose a method to simultaneously identify the inactive features and samples in a dynamic manner (Bonnefoy et al., 2014); that is, during the optimization process, they trigger their testing rule when there is a sufficient decrease in the duality gap. Thus, the method in (Shibagaki et al., 2016) can discard more inactive features and samples as the optimization proceeds and one has small-scale problems to solve in the late stage of the optimization. Nevertheless, the overall speedup can be limited as the problems’ size can be large in the early stage of the optimization. To be specific, the method in (Shibagaki et al., 2016) depends heavily on the duality gap during the optimization process. The duality gap in the early stage can always be large, which makes the dual and primal estimations inaccurate and finally results in ineffective screening rules. Hence, it is essentially solving a large problem in the early stage.\nIn this paper, to address the limitations in the dynamic screening method, we propose a novel screening method that can Simultaneously identify Inactive Features and Samples (SIFS) for sparse SVMs in a static manner, that is, we only need to perform SIFS once before (instead of during) optimization. Thus, we only need to run the optimization algorithm on small-scale problems. The major technical challenge in developing SIFS is that we need to accurately estimate the primal and dual optima. The more accurate the estimations are, the more effective SIFS is in detecting inactive features and samples. Thus, our major technical contribution is a novel framework, which is based on the strong convexity of the primal and dual problems of sparse SVMs [see problems (P∗) and (D∗) in Section 2] for deriving accurate estimations of the primal and dual optima (see Section 3). Another appealing feature of SIFS is the so-called synergy effect (Shibagaki et al., 2016). Specifically, the proposed SIFS consists of two parts, i.e., Inactive Feature Screening (IFS) and Inactive Samples Screening (ISS). We show that discarding inactive features (samples) identified by IFS (ISS) leads to a more accurate estimation of the primal (dual) optimum, which in turn dramatically enhances the capability of ISS (IFS) in detecting inactive samples (features). Thus, SIFS applies IFS and ISS in an alternating manner until no more inactive features and samples can be identified, leading to much better performance in scaling up large-scale problems than the application of\nISS or IFS individually. Moreover, SIFS (see Section 4) is safe in the sense that the detected features and samples are guaranteed to be absent from the sparse representations. To the best of our knowledge, SIFS is the first static screening rule for sparse SVM that is able to simultaneously detect inactive features and samples. Experiments (see Section 5) on both synthetic and real datasets demonstrate that SIFS significantly outperforms the state-of-the-art (Shibagaki et al., 2016) in improving the efficiency of sparse SVMs and the speedup can be orders of magnitude. Detailed proofs of theoretical results in the main text are in the supplementary supplements.\nNotations: Let ‖ · ‖1, ‖ · ‖, and ‖ · ‖∞ be the `1, `2, and `∞ norms, respectively. We denote the inner product of vectors x and y by 〈x,y〉, and the i-th component of x by [x]i. Let [p] = {1, 2..., p} for a positive integer p. Given a subset J := {j1, ..., jk} of [p], let |J | be the cardinality of J . For a vector x, let [x]J = ([x]j1 , ..., [x]jk)T . For a matrix X, let [X]J = (xj1 , ...,xjk) and J [X] = ((xj1)T , ..., (xjk)T )T , where xi and xj are the ith row and jth column of X, respectively. For a scalar t, we denote max{0, t} by [t]+."
  }, {
    "heading": "2. Basics and Motivations",
    "text": "In this section, we briefly review some basics of sparse SVMs and then motivate SIFS via the KKT conditions. Specifically, we focus on the `1-regularized SVM with a smoothed hinged loss that has strong theoretical guarantees (Shalev-Shwartz & Zhang, 2016), which takes the form of\nmin w∈Rp\nP (w;α, β) = 1\nn n∑ i=1 `(1− 〈x̄i,w〉) + α 2 ‖w‖2 + β||w||1, (P∗)\nwhere w is the parameter vector to be estimated, {xi, yi}ni=1 is the training set, xi ∈ Rp, yi ∈ {−1,+1}, x̄i = yixi, α and β are positive parameters, and the loss function `(·) : R→ R is\n`(t) =  0, if t < 0, t2\n2γ , if 0 ≤ t ≤ γ, t− γ2 , if t > γ,\nwhere γ ∈ (0, 1). We present the Lagrangian dual problem of problem (P∗) and the KKT conditions in the following theorem, which plays a fundamentally important role in developing our screening rule.\nTheorem 1. Let X̄ = (x̄1, x̄2, ..., x̄n) and Sβ(·) be the soft-thresholding operator (Hastie et al., 2015), i.e., [Sβ(u)]i = sign([u]i)(|[u]i| − β)+. Then, for problem (P∗), the followings hold:\n(i) : The dual problem of (P∗) is\nmin θ∈[0,1]n\nD(θ;α, β) = 1\n2α ∥∥∥∥Sβ ( 1nX̄θ )∥∥∥∥2 + γ2n‖θ‖2\n− 1 n 〈1, θ〉, (D∗)\nwhere 1 ∈ Rn is a vector with all components equal to 1. (ii) : Denote the optima of (P∗) and (D∗) by w∗(α, β) and θ∗(α, β), respectively. Then,\nw∗(α, β) = 1 α Sβ ( 1 n X̄θ∗(α, β) ) , (KKT-1)\n[θ∗(α, β)]i =  0, if 1− 〈x̄i,w∗(α, β)〉 < 0; 1, if 1− 〈x̄i,w∗(α, β)〉 > γ; 1 γ (1− 〈x̄i,w\n∗(α, β)〉), otherwise . (KKT-2)\nAccording to KKT-1 and KKT-2, we define 4 index sets:\nF = { j ∈ [p] : 1\nn |[X̄θ∗(α, β)]j | ≤ β\n} ,\nR = {i ∈ [n] : 1− 〈w∗(α, β), x̄i〉 < 0}, E = {i ∈ [n] : 1− 〈w∗(α, β), x̄i〉 ∈ [0, γ]}, L = {i ∈ [n] : 1− 〈w∗(α, β), x̄i〉 > γ},\nwhich imply that\n(i): i ∈ F ⇒ [w∗(α, β)]i = 0, (ii): { i ∈ R ⇒ [θ∗(α, β)]i = 0, i ∈ L ⇒ [θ∗(α, β)]i = 1.\n(R)\nThus, we call the jth feature inactive if j ∈ F . The samples in E are the so-called support vectors and we call the samples inR and L inactive samples.\nSuppose that we are given subsets of F , R, and L, then by (R), we can see that many coefficients of w∗(α, β) and θ∗(α, β) are known. Thus, we may have much less unknowns to solve and the problem size can be dramatically reduced. We formalize this idea in Lemma 1.\nLemma 1. Given index sets F̂ ⊆ F , R̂ ⊆ R, and L̂ ⊆ L, the followings hold (i) : [w∗(α, β)]F̂ = 0, [θ ∗(α, β)]R̂ = 0, [θ ∗(α, β)]L̂ = 1. (ii) : Let D̂ = R̂ ∪ L̂, Ĝ1 = F̂c [X̄]D̂c , and Ĝ2 = F̂c [X̄]L̂, where F̂c = [p]\\F̂ , D̂c = [n]\\D̂, and L̂c = [n]\\L̂. Then, [θ∗(α, β)]D̂c solves the following scaled dual problem:\nmin θ̂∈[0,1]|D̂c| { 1 2α ∥∥∥∥Sβ ( 1nĜ1θ̂ + 1nĜ21 )∥∥∥∥2 + γ2n‖θ̂‖2\n− 1 n 〈1, θ̂〉\n} . (scaled-D∗)\n(iii) : Suppose that θ∗(α, β) is known. Then,\n[w∗(α, β)]F̂c = 1 α Sβ ( 1 n F̂ c [X̄]θ ∗(α, β) ) .\nLemma 1 indicates that, if we can identify index sets F̂ and D̂ and the cardinalities of F̂c and D̂c are much smaller than the feature dimension p and the dataset size n, we only need to solve a problem (scaled-D∗) that may be much smaller than problem (D∗) to exactly recover the optima w∗(α, β) and θ∗(α, β) without sacrificing any accuracy.\nHowever, we cannot directly apply the rules in (R) to identify subsets of F , R, and L, as they require the knowledge of w∗(α, β) and θ∗(α, β) that are usually unavailable. Inspired by the idea in (El Ghaoui et al., 2012), we can first estimate regions W and Θ that contain w∗(α, β) and θ∗(α, β), respectively. Then, by denoting\nF̂ := { j ∈ [p] : max\nθ∈Θ {∣∣∣∣ 1n [X̄θ]j ∣∣∣∣} ≤ β} , (1)\nR̂ := { i ∈ [n] : max\nw∈W {1− 〈w, x̄i〉} < 0\n} , (2)\nL̂ := { i ∈ [n] : min\nw∈W {1− 〈w, x̄i〉} > γ\n} , (3)\nsince it is easy to know that F̂ ⊂ F , R̂ ⊂ R and L̂ ⊂ L, the rules in (R) can be relaxed as follows:\n(i): j ∈ F̂ ⇒ [w∗(α, β)]j = 0, (R1) (ii): { i ∈ R̂ ⇒ [θ∗(α, β)]i = 0, i ∈ L̂ ⇒ [θ∗(α, β)]i = 1. (R2)\nIn view of R1 and R2, we sketch the development of SIFS as follows.\nStep 1: Derive estimationsW and Θ such that w∗(α, β) ∈ W and θ∗(α, β) ∈ Θ, respectively.\nStep 2: Develop SIFS by deriving the relaxed screening rules R1 and R2, i.e., by solving the optimization problems in Eq. (1), Eq. (2) and Eq. (3)."
  }, {
    "heading": "3. Estimate the Primal and Dual Optima",
    "text": "In this section, we first show that the primal and dual optima admit closed form solutions for specific values of α and β (see Section 3.1). Then, in Sections 3.2 and 3.3, we present accurate estimations of the primal and dual optima, respectively."
  }, {
    "heading": "3.1. Effective Intervals of the Parameters α and β",
    "text": "We first show that, if the value of β is sufficiently large, no matter what α is, the primal solution is 0.\nTheorem 2. Let βmax = ‖ 1nX̄1‖∞. Then, for α > 0 and β ≥ βmax, we have\nw∗(α, β) = 0, θ∗(α, β) = 1.\nFor any β, the next result shows that, if α is large enough, the primal and dual optima admit closed form solutions.\nTheorem 3. If we denote\nαmax(β) = 1\n1− γ max i∈[n]\n{ 〈x̄i,Sβ( 1\nn X̄1)〉\n} ,\nthen for all α ∈ [max{αmax(β), 0},∞)∩ (0,∞), we have\nw∗(α, β) = 1 α Sβ ( 1 n X̄1 ) , θ∗(α, β) = 1. (4)\nBy Theorems 2 and 3, we only need to consider the cases with β ∈ (0, βmax] and α ∈ (0, αmax(β)]."
  }, {
    "heading": "3.2. Primal Optimum Estimation",
    "text": "In Section 1, we mention that the proposed SIFS consists of IFS and ISS, and an alternating application of IFS and ISS can improve the estimation of the primal and dual optima, which can in turn make ISS and IFS more effective in identifying inactive samples and features, respectively. Lemma 2 shows that discarding inactive features by IFS leads to a more accurate estimation of the primal optimum.\nLemma 2. Suppose that the reference solution w∗(α0, β0) with β0 ∈ (0, βmax] and α0 ∈ (0, αmax(β0)] is known. Consider problem (P∗) with parameters α > 0 and β0. Let F̂ be the index set of the inactive features identified by the previous IFS steps, i.e., [w∗(α, β0)]F̂ = 0. We define\nc = α0 + α\n2α [w∗(α0, β0)]F̂c , (5)\nr2 = (α0 − α)2\n4α2 ‖w∗(α0, β0)‖2\n− (α0 + α) 2\n4α2 ‖[w∗(α0, β0)]F̂‖ 2. (6)\nThen, the following holds:\n[w∗(α, β0)]F̂c ∈ W := {w : ‖w − c‖ ≤ r}. (7)\nAs F̂ is the index set of identified inactive features, we have [w∗(α, β0)]F̂ = 0. Hence, we only need to find an accurate estimation of [w∗(α, β0)]F̂c . Lemma 2 shows that [w∗(α, β0)]F̂c lies in a ball of radius r centered at c. Note that, before we perform IFS, the set F̂ is empty and thus the second term on the right hand side (RHS) of Eq. (6) is 0. If we apply IFS multiple times (alternating with ISS), the set F̂ will be monotonically increasing. Thus, Eq. (6) implies that the radius will be monotonically decreasing, leading to a more accurate primal optimum estimation."
  }, {
    "heading": "3.3. Dual Optimum Estimation",
    "text": "Similar to Lemma 2, the next result shows that ISS can improve the estimation of the dual optimum.\nLemma 3. Suppose that the reference solution θ∗(α0, β0) with β0 ∈ (0, βmax] and α0 ∈ (0, αmax(β0)] is known. Consider problem (D∗) with parameters α > 0 and β0. Let R̂ and L̂ be the index sets of inactive samples identified by the previous ISS steps, i.e., [θ∗(α, β0)]R̂ = 0, [θ∗(α, β0)]L̂ = 1, and D̂ = R̂ ∪ L̂. We define\nc = α− α0\n2γα 1 +\nα0 + α\n2α [θ∗(α0, β0)]D̂c , (8)\nr2 = (α0 − α)2\n4α2\n∥∥∥∥θ∗(α0, β0)− 1γ 1 ∥∥∥∥2\n− ∥∥∥∥ (2γ − 1)α+ α02γα 1− α0 + α2α [θ∗(α0, β0)]L̂ ∥∥∥∥2 − ∥∥∥∥α− α02γα 1 + α0 + α2α [θ∗(α0, β0)]R̂\n∥∥∥∥2 . (9) Then, the following holds:\n[θ∗(α, β0)]D̂c ∈ Θ := ‖θ : θ − c‖ ≤ r. (10)\nSimilar to Lemma 2, Lemma 3 also bounds [θ∗(α, β0)]D̂c by a ball. In view of Eq. (9), a similar discussion of Lemma 2—that is, the index sets L̂ and R̂ monotonically increase and thus the last two terms on the RHS of Eq. (9) monotonically increase when we perform ISS multiple times (alternating with IFS)—implies that the ISS steps can reduce the radius and thus improve the dual optimum estimation.\nRemark 1. To estimate w∗(α, β0) and θ∗(α, β0) by Lemmas 2 and 3, we have a free reference solution pair w∗(α0, β0) and θ∗(α0, β0) with α0 = αmax(β0). From Theorems 2 and 3, we know that in this setting, w∗(α0, β0) and θ∗(α0, β0) admit closed form solutions."
  }, {
    "heading": "4. The Proposed SIFS Screening Rule",
    "text": "We first present the IFS and ISS rules in Sections 4.1 and 4.2, respectively. Then, in Section 4.3, we develop the SIFS screening rule by an alternating application of IFS and ISS."
  }, {
    "heading": "4.1. Inactive Feature Screening (IFS)",
    "text": "Suppose that w∗(α0, β0) and θ∗(α0, β0) are known, we derive IFS to identify inactive features for problem (P∗) at (α, β0) by solving the optimization problem in Eq. (1) (see Section E in the supplementary material):\nsi(α, β0) = max θ∈Θ\n{ 1\nn |〈[x̄i]D̂c , θ〉+ 〈[x̄\ni]L̂, 1〉| } , i ∈ F̂c,\n(11)\nwhere Θ is given by Eq. (10) and F̂ and D̂ = R̂∪L̂ are the index sets of inactive features and samples that have been identified in previous screening processes, respectively. The next result shows the closed form solution of problem (11). Lemma 4. Consider problem (11). Let c and r be given by Eq. (8) and Eq. (9). Then, for all i ∈ F̂c, we have\nsi(α, β0) = 1\nn (|〈[x̄i]D̂c , c〉+ 〈[x̄ i]L̂, 1〉|+ ‖[x̄ i]D̂c‖r).\nWe are now ready to present the IFS rule. Theorem 4. Consider problem (P∗). We suppose that w∗(α0, β0) and θ∗(α0, β0) are known. Then,\n(1): The feature screening rule IFS takes the form of\nsi(α, β0) ≤ β0 ⇒ [w∗(α, β0)]i = 0,∀i ∈ F̂c (IFS)\n(2): We update the index set F̂ by\nF̂ ← F̂ ∪ {i : si ≤ β0, i ∈ F̂c}. (12)\nRecall that (Lemma 3), previous sample screening results give us a more tighter dual estimation, i.e., a smaller feasible region Θ for problem (11), which results in a smaller si(α, β0). It finally leads us to a more powerful feature screening rule IFS. This is the so called synergy effect."
  }, {
    "heading": "4.2. Inactive Sample Screening (ISS)",
    "text": "Similar to IFS, we derive ISS to identify inactive samples by solving the optimization problems in Eq. (2) and Eq. (3) (see Section G in the supplementary material for details):\nui(α, β0) = max w∈W\n{1− 〈[x̄i]F̂c ,w〉}, i ∈ D̂ c, (13)\nli(α, β0) = min w∈W\n{1− 〈[x̄i]F̂c ,w〉}, i ∈ D̂ c, (14)\nwhereW is given by Eq. (7) and F̂ and D̂ = R̂∪ L̂ are the index sets of inactive features and samples that have been identified in previous screening processes. We show that problems (13) and (14) admit closed form solutions. Lemma 5. Consider problems (13) and (14). Let c and r be given by Eq. (5) and Eq. (6). Then,\nui(α, β0) = 1− 〈[x̄i]F̂c , c〉+ ‖[x̄i]F̂c‖r, i ∈ D̂ c,\nli(α, β0) = 1− 〈[x̄i]F̂c , c〉 − ‖[x̄i]F̂c‖r, i ∈ D̂ c.\nWe are now ready to present the ISS rule. Theorem 5. Consider problem (D∗). We suppose that w∗(α0, β0) and θ∗(α0, β0) are known. Then,\n(1): The sample screening rule ISS takes the form of\nui(α, β0) < 0⇒ [θ∗(α, β0)]i = 0, li(α, β0) > γ ⇒ [θ∗(α, β0)]i = 1, ∀i ∈ D̂c (ISS)\n(2): We update the the index sets R̂ and L̂ by\nR̂ ← R̂ ∪ {i : ui(α, β0) < 0, i ∈ D̂c}, (15) L̂ ← L̂ ∪ {i : li(α, β0) > γ, i ∈ D̂c}. (16)\nThe synergy effect also exists here. Recall that (Lemma 2), previous feature screening results lead a smaller feasible regionW for the problems (13) and (14), which results in smaller ui(α, β0) and bigger li(α, β0). It finally leads us to a more accurate sample screening rule ISS."
  }, {
    "heading": "4.3. The Proposed SIFS Rule by An Alternating Application of IFS and ISS",
    "text": "In real applications, the optimal parameter values of α and β are usually unknown. To determine appropriate parameter values, common approaches, like cross validation and stability selection, need to solve the model over a grid of parameter values {(αi,j , βj) : i ∈ [M ], j ∈ [N ]} with βmax > β1 > ... > βN > 0 and αmax(βj) > α1,j > ... > αM,j > 0. This can be very time-consuming. Inspired by Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al., 2012), we develop a sequential version of SIFS in Algorithm 1. Specifically, given the primal and dual opti-\nAlgorithm 1 SIFS 1: Input: βmax > β1 > ... > βN > 0 and αmax(βj) = α0,j > α1,j > ... > αM,j > 0.\n2: for j = 1 to N do 3: Compute the first reference solution w∗(α0,j , βj) and θ∗(α0,j , βj) using the close-form formula (4). 4: for i = 1 to M do 5: Initialization: F̂ = R̂ = L̂ = ∅ 6: repeat 7: Run sample screening using rule ISS based on w∗(αi−1,j , βj). 8: Update R̂ and L̂ by Eq. (15) and Eq. (16), respectively. 9: Run feature screening using rule IFS based on\nθ∗(αi−1,j , βj). 10: Update F̂ by Eq. (12). 11: until No new inactive features or samples are identified 12: Compute w∗(αi,j , βj) and θ∗(αi,j , βj) by solving the scaled problem. 13: end for 14: end for 15: Output:w∗(αi,j , βj) and θ∗(αi,j , βj), i ∈ [M ], j ∈\n[N ].\nma w∗(αi−1,j , βj) and θ∗(αi−1,j , βj) at (αi−1,j , βj), we apply SIFS to identify the inactive features and samples for problem (P∗) at (αi,j , βj). Then, we perform optimization on the reduced dataset and solve the primal and dual optima\nat (αi,j , βj). We repeat this process until we solve problem (P∗) at all pairs of parameter values.\nNote that we insert α0,j into every sequence {αi,j : i ∈ [M ]} ( see line 1 in Algorithm 1) to obtain a closed-form solution as the first reference solution. In this way, we can avoid solving problem at (α1,j , βj), j ∈ [N ] directly (without screening), which is time consuming. At last, we would like to point out that the values {(αi,j , βj) : i ∈ [M ], j ∈ [N ]} in SIFS can be specified by users arbitrarily.\nSIFS applies ISS and IFS in an alternating manner to reinforce their capability in identifying inactive samples and features. In Algorithm 1, we apply ISS first. Of course, we can also apply IFS first. The theorem below demonstrates that the orders have no impact on the performance of SIFS.\nTheorem 6. Given the optimal solutions w∗(αi−1,j , βj) and θ∗(αi−1,j , βj) at (αi−1,j , βj) as the reference solution pair at (αi,j , βj) for SIFS, we assume SIFS with ISS first stops after applying IFS and ISS for p times and denote the identified inactive features and samples as F̂Ap , R̂Ap and L̂Ap . Similarly, when we apply IFS first, the results are denoted as F̂Bq , R̂Bq and L̂Bq . Then, the followings hold: (1) F̂Ap = F̂Bq , R̂Ap = R̂Bq and L̂Ap = L̂Bq . (2) With different orders of applying ISS and IFS, the difference of the times of ISS and IFS we need to apply in SIFS can never be larger than 1, that is, |p− q| ≤ 1.\nRemark 2. From Remark 1, we can see that our SIFS can also be applied to solve a single problem, due to the existence of the free reference solution pair."
  }, {
    "heading": "5. Experiments",
    "text": "We evaluate SIFS on both synthetic and real datasets in terms of three measurements. The first one is the scaling ratio: 1− (n−ñ)(p−p̃)np , where ñ, p̃, n, and p are the numbers of inactive samples and features identified by SIFS, sample size, and feature dimension of the datasets. The second measure is rejection ratios of each triggering of ISS and IFS in SIFS: ñin0 and p̃i p0\n, where ñi and p̃i are the numbers of inactive samples and features identified in i-th triggering of ISS and IFS in SIFS. n0 and p0 are the numbers of inactive samples and features in the solution. The third measure is speedup, i.e., the ratio of the running time of the solver without screening to that with screening.\nRecall that, we can integrate SIFS with any solvers for problem (P∗). In this experiment, we use Accelerated Proximal Stochastic Dual Coordinate Ascent (AcceleratedProx-SDCA) (Shalev-Shwartz & Zhang, 2016), as it is one of the state-of-the-arts. As we mentioned in the introduction section that screening differs greatly from features selection methods, it is not appropriate to make comparisons with feature selection methods. To this end, we only\nchoose the state-of-art screening method for Sparse SVMs in (Shibagaki et al., 2016) as a baseline in the experiments.\nFor each dataset, we solve problem (P∗) at a grid of turning parameter values. Specifically, we first compute βmax by Theorem 2 and then select 10 values of β that are equally spaced on the logarithmic scale of β/βmax from 1 to 0.05. Then, for each value of β, we first compute αmax(β) by Theorem 3 and then select 100 values of α that are equally spaced on the logarithmic scale of α/αmax(β) from 1 to 0.01. Thus, for each dataset, we solve problem (P∗) at 1000 pairs of parameter values in total. We write the code in C++ along with Eigen library for some numerical computations. We perform all the computations on a single core of Intel(R) Core(TM) i7-5930K 3.50GHz, 128GB MEM."
  }, {
    "heading": "5.1. Simulation Studies",
    "text": "We evaluate SIFS on 3 synthetic datasets named syn1, syn2 and syn3 with sample and feature size (n, p) ∈ {(10000, 1000), (10000, 10000), (1000, 10000)}. We present each data point as x = [x1;x2] with x1 ∈ R0.02p and x2 ∈ R0.98p. We use Gaussian distributions G1 = N(u, 0.75I),G2 = N(−u, 0.75I) and G3 = N(0, 1) to generate the data points, where u = 1.51 and I ∈ R0.02p×0.02p is the identity matrix. To be precise, x1 for positive and negative points are sampled from G1 and G2, respectively. For each entry in x2, it has chance η = 0.02 to be sampled from G3 and chance 1− η to be 0.\nFig. 1 shows the scaling ratios by ISS, IFS, and SIFS on the synthetic datasets at 1000 parameter values. We can see that IFS is more effective in scaling problem size than ISS, with scaling ratios roughly 98% against 70 − 90%. Moreover, SIFS, which is an alternating application of IFS and ISS, significantly outperforms ISS and IFS, with scal-\ning ratios roughly 99.9%. This high scaling ratios imply that SIFS can lead to a significant speedup.\nDue to the space limitation, we only report the rejection ratios of SIFS on syn2. Other results can be found in the supplementary material. Fig. 2 shows that SIFS can identify most of the inactive features and samples. However, few features and samples are identified in the second and later triggerings of ISS and IFS. The reason may be that the task here is so simple that one triggering is enough.\nTable 1 reports the running time of solver without and with IFS, ISS and SIFS for solving problem (P∗) at 1000 pairs of parameter values. We can see that SIFS leads to significant speedups, that is, up to 76.8 times. Taking syn2 for example, without SIFS, the solver takes more than two hours to solve problem (P∗) at 1000 pairs of parameter values. However, combined with SIFS, the solver only needs less than three minutes for solving the same set of problems. From the theoretical analysis in (Shalev-Shwartz & Zhang, 2016) for Accelerated-Prox-SDCA, we can see that its computational complexity rises proportionately to the sample size n and the feature dimension p. From this theoretical result, we can see that the results in Figure 1 are roughly consistent with the speedups we achieved shown in Table 1."
  }, {
    "heading": "5.2. Experiments on Real Datasets",
    "text": "In this experiment, we evaluate the performance of SIFS on 5 large-scale real datasets: real-sim, rcv1-train, rcv1-\ntest, url, and kddb, which are all collected from the project page of LibSVM (Chang & Lin, 2011). See Table 2 for a brief summary. We note that, the kddb dataset has about 20 million samples with 30 million features.\nRecall that, SIFS detects the inactive features and samples in a static manner, i.e., we perform SIFS only once before the optimization and thus the size of the problem we need to perform optimization on is fixed. However, the method in (Shibagaki et al., 2016) detects inactive features and samples in a dynamic manner (Bonnefoy et al., 2014), i.e., they perform their method along with the optimization and thus the size of the problem would keep decreasing during the iterative process. Thus, comparing SIFS with the method in (Shibagaki et al., 2016) in terms of rejection ratios is inapplicable. We compare the performance of SIFS with the method in (Shibagaki et al., 2016) in terms of speedup. Specifically, we compare the speedup gained by SIFS and the method in (Shibagaki et al., 2016) for solving problem (P∗) at 1000 pairs of parameter values. The code of the method in (Shibagaki et al., 2016) is obtained from (https://github.com/husk214/s3fs).\nFig. 3 shows the rejection ratios of SIFS on the real-sim dataset (other results are in the supplementary material). In Fig. 3, we can see that some inactive features and samples are identified in the 2nd and 3rd triggering of ISS and IFS, which verifies the necessity of the alternating application of ISS and IFS. SIFS is efficient since it always stops in 3 times of triggering. In addition, most of (> 98%) the inactive features can be identified in the 1st triggering of IFS while identifying inactive samples needs to apply ISS two or more times. It may result from two reasons: 1) We run ISS first, which reinforces the capability of IFS due to the synergy effect (see Sections 4.1 and 4.2), see Section L.1 in the supplementary material for further verification; 2) Feature screening here may be easier than sample screening.\nTable 3 reports the running time of solver without and with the method in (Shibagaki et al., 2016) and SIFS for solving problem (P∗) at 1000 pairs of parameter values on real datasets. The speedup gained by SIFS is up to 300 times on real-sim, rcv1-train and rcv1-test. Moreover, SIFS significantly outperforms the method in (Shibagaki et al., 2016) in terms of speedup—by about 30 to 40 times faster on the aforementioned three datasets. For datasets url and kddb, we do not report the results of the solver as the sizes of the datasets are huge and the computational cost is prohibitive. Instead, we can see that the solver with SIFS is about 25\ntimes faster than the solver with the method in (Shibagaki et al., 2016) on both datasets url and kddb. Take the dataset kddb as an example. The solver with SIFS takes about 13 hours to solve problem (P∗) for all 1000 pairs of parameter values, while the solver with the method in (Shibagaki et al., 2016) needs 11 days to finish the same task."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we develop a novel data reduction method SIFS to simultaneously identify inactive features and samples for sparse SVM. Our major contribution is a novel framework for an accurate estimation of the primal and dual optima based on strong convexity. To the best of our knowledge, the proposed SIFS is the first static screening method that is able to simultaneously identify inactive features and samples for sparse SVMs. An appealing feature of SIFS is that all detected features and samples are guaranteed to be irrelevant to the outputs. Thus, the model learned on the reduced data is identical to the one learned on the full data. Experiments on both synthetic and real datasets demonstrate that SIFS can dramatically reduce the problem size and the resulting speedup can be orders of magnitude. We plan to generalize SIFS to more complicated models, e.g., SVM with a structured sparsity-inducing penalty."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported by the National Basic Research Program of China (973 Program) under Grant 2013CB336500, National Natural Science Foundation of China under Grant 61233011 and National Youth Topnotch Talent Support Program."
  }],
  "year": 2017,
  "references": [{
    "title": "Dimensionality reduction via sparse support vector machines",
    "authors": ["Bi", "Jinbo", "Bennett", "Kristin", "Embrechts", "Mark", "Breneman", "Curt", "Song", "Minghu"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "A dynamic screening principle for the lasso",
    "authors": ["Bonnefoy", "Antoine", "Emiya", "Valentin", "Ralaivola", "Liva", "Gribonval", "Rémi"],
    "venue": "In Signal Processing Conference (EUSIPCO),",
    "year": 2014
  }, {
    "title": "Fast support vector machine training and classification on graphics processors",
    "authors": ["Catanzaro", "Bryan", "Sundaram", "Narayanan", "Keutzer", "Kurt"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Libsvm: a library for support vector machines",
    "authors": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
    "year": 2011
  }, {
    "title": "Safe feature elimination in sparse supervised learning",
    "authors": ["El Ghaoui", "Laurent", "Viallon", "Vivian", "Rabbani", "Tarek"],
    "venue": "Pacific Journal of Optimization,",
    "year": 2012
  }, {
    "title": "Liblinear: A library for large linear classification",
    "authors": ["Fan", "Rong-En", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Wang", "Xiang-Rui", "Lin", "Chih-Jen"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "The entire regularization path for the support vector machine",
    "authors": ["Hastie", "Trevor", "Rosset", "Saharon", "Tibshirani", "Robert", "Zhu", "Ji"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2004
  }, {
    "title": "Statistical learning with sparsity: the lasso and generalizations",
    "authors": ["Hastie", "Trevor", "Tibshirani", "Robert", "Wainwright", "Martin"],
    "year": 2015
  }, {
    "title": "A dual coordinate descent method for large-scale linear svm",
    "authors": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S Sathiya", "Sundararajan", "Sellamanickam"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Text categorization with support vector machines: Learning with many relevant features",
    "authors": ["Joachims", "Thorsten"],
    "year": 1998
  }, {
    "title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines",
    "authors": ["Kotsia", "Irene", "Pitas", "Ioannis"],
    "venue": "Image Processing, IEEE Transactions on,",
    "year": 2007
  }, {
    "title": "A topographic support vector machine: Classification using local label configurations",
    "authors": ["Mohr", "Johannes", "Obermayer", "Klaus"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2004
  }, {
    "title": "Safe screening of non-support vectors in pathwise svm computation",
    "authors": ["Ogawa", "Kohei", "Suzuki", "Yoshiki", "Takeuchi", "Ichiro"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "Mathematical Programming,",
    "year": 2016
  }, {
    "title": "Pegasos: Primal estimated sub-gradient solver for svm",
    "authors": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"],
    "venue": "Mathematical programming,",
    "year": 2011
  }, {
    "title": "Simultaneous safe screening of features and samples in doubly sparse modeling",
    "authors": ["Shibagaki", "Atsushi", "Karasuyama", "Masayuki", "Hatano", "Kohei", "Takeuchi", "Ichiro"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Lasso screening rules via dual polytope projection",
    "authors": ["Wang", "Jie", "Zhou", "Jiayu", "Wonka", "Peter", "Ye", "Jieping"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A safe screening rule for sparse logistic regression",
    "authors": ["Wang", "Jie", "Zhou", "Jiayu", "Liu", "Jun", "Wonka", "Peter", "Ye", "Jieping"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "The doubly regularized support vector machine",
    "authors": ["Wang", "Li", "Zhu", "Ji", "Zou", "Hui"],
    "venue": "Statistica Sinica, pp",
    "year": 2006
  }, {
    "title": "Fast lasso screening tests based on correlations",
    "authors": ["Xiang", "Zhen James", "Ramadge", "Peter J"],
    "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2012
  }, {
    "title": "Latent support measure machines for bag-of-words data classification",
    "authors": ["Yoshikawa", "Yuya", "Iwata", "Tomoharu", "Sawada", "Hiroshi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1961
  }],
  "id": "SP:f535868f14c392f94cf1513d8fa164e2b2241862",
  "authors": [{
    "name": "Weizhong Zhang",
    "affiliations": []
  }, {
    "name": "Bin Hong",
    "affiliations": []
  }, {
    "name": "Wei Liu",
    "affiliations": []
  }, {
    "name": "Jieping Ye",
    "affiliations": []
  }, {
    "name": "Deng Cai",
    "affiliations": []
  }, {
    "name": "Xiaofei He",
    "affiliations": []
  }, {
    "name": "Jie Wang",
    "affiliations": []
  }],
  "abstractText": "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVM. Experiments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million features) demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.",
  "title": "Scaling Up Sparse Support Vector Machinesby Simultaneous Feature and Sample Reduction"
}