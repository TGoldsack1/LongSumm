{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics\nIn this paper, we describe domainindependent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text.\nOur work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts."
  }, {
    "heading": "1 Introduction",
    "text": "Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvevåg et al., 2007; Bedi et al., 2015).\nVarious frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and\nAsher, 1991) define relations that hierarchically structure texts. The entity grid model (Barzilay and Lapata, 2008) and its extensions1 capture the referential coherence of entities moving in and out of focus across a text. Each captures only a single aspect of coherence, and all focus on scoring existing sentences, rather than on generating coherent discourse for tasks like abstractive summarization.\nHere we introduce two classes of neural models for discourse coherence. Our discriminative models induce coherence by treating human generated texts as coherent examples and texts with random sentence replacements as negative examples, feeding LSTM sentence embeddings of pairs of consecutive sentences to a classifier. These achieve stateof-the-art (96% accuracy) on the standard domainspecific sentence-pair-ordering dataset (Barzilay and Lapata, 2008), but suffer in a larger opendomain setting due to the small semantic space that negative sampling is able to cover.\nOur generative models are based on augumenting encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMMLDA topic model into the generative model and (2) an end-to-end model that introduces a Markovstructured neural latent variable, inspired by recent work on training latent-variable recurrent nets (Bowman et al., 2015; Serban et al., 2016b). These generative models obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text generated by the model.\nOur work marks an initial step in building endto-end systems to evaluate open-domain discourse coherence, and more importantly, generating coherent texts given discourse contexts.\n1Adding coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011) and entity graphs (Guinaudeau and Strube, 2013).\n198"
  }, {
    "heading": "2 The Discriminative Model",
    "text": "The discriminative model treats cliques (sets of sentences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples. The discriminative model can be viewed as an extended version of Li and Hovy’s (2014) model but is practical at large scale2. We thus make this section succinct.\nNotations Let C denote a sequence of coherent texts taken from original articles generated by humans. C is comprised of a sequence of sentences C = {sn−L, ..., sn−1, sn, sn+1, ..., sn+L} where L denotes the half size of the context window. Suppose each sentence sn consists of a sequence of words wn1, ..., wnt, ..., wnM , where M is the number of tokens in sn. Each word w is associated with a K dimensional vector hw and each sentence is associated with a K dimensional vector xs.\nEach C contains 2L + 1 sentences, and is associated with a (2L+ 1)×K dimensional vector obtained by concatenating the representations of its constituent sentences. The sentence representation is obtained from LSTMs. After word compositions, we use the representation output from the final time step to represent the entire sentence. Another neural network model with a sigmoid function on the very top layer is employed to map the concatenation of representations of its constituent sentences to a scalar, indicating the probability of the current clique being a coherent one or an incoherent one.\nWeakness Two problems with the discriminative model stand out: First, it relies on negative sampling to generate negative examples. Since the sentence-level semantic space in the open-domain setting is huge, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don’t cover the space of possible meanings. As we will show in the experiments section, the discriminative model performs competitively in specific domains, but not in the open domain setting. Secondly and more importantly, discriminative models are only able to tell whether an already-given chunk of text is coherent or not. While they can thus be used in tasks like extractive summarization for sentence re-ordering, they cannot be used for coherent text generation\n2Li and Hovy’s (2014) recursive neural model operates on parse trees, which does not support batched computation and is therefore hard to scale up.\nin tasks like dialogue generation or abstractive text summarization."
  }, {
    "heading": "3 The Generative Model",
    "text": "We therefore introduce three neural generative models of discourse coherence."
  }, {
    "heading": "3.1 Model 1: the SEQ2SEQ Model and its Variations",
    "text": "In a coherent context, a machine should be able to guess the next utterance given the preceding ones. A straightforward way to do that is to train a SEQ2SEQ model to predict a sentence given its contexts (Sutskever et al., 2014). Generating sentences based on neighboring sentences resembles skip-thought models (Kiros et al., 2015), which build an encoder-decoder model by predicting tokens in neighboring sentences.\nAs shown in Figure 1a, given two consecutive sentences [si, si+1], one can measure the coherence by the likelihood of generating si+1 given its preceding sentence si (denoted by uni). This likelihood is scaled by the number of words in si+1 (denoted by Ni+1) to avoid favoring short sequences.\nL(si, si+1) = 1\nNi+1 log p(si+1|si) (1)\nThe probability can be directly computed using a pretrained SEQ2SEQ model (Sutskever et al., 2014) or an attention-based model (Bahdanau et al., 2015; Luong et al., 2015).\nIn a coherent context, a machine should not only be able to guess the next utterance given the preceding ones, but also the preceding one given the following ones. This gives rise to the coherence model (denoted by bi) that measures the bidirectional dependency between the two consecutive sentences:\nL(si, si+1) = 1 Ni log pB(si|si+1)\n+ log 1\nNi+1 pF (si+1|si)\n(2)\nWe separately train two models: a forward model pF (si+1|si) that predicts the next sentence based on the previous one and a backward model pB(si|si+1) that predicts the previous sentence given the next sentence. pB(si|si+1) can be trained in a way similar to pF (si+1|si) with sources and targets swapped. It is worth noting that pB and pF are separate models and do not share parameters.\nOne problem with the described uni and bi models is that sentences with higher language model probability (e.g., sentences without rare words) also tend to have higher conditional probability given their preceding or succeeding sentences. We are interested in measuring the informational gain from the contexts rather than how fluent the current sentence is. We thus propose eliminating the influence of the language model, which yields the following coherence score:\nL(si, si+1)\n= 1 Ni [log pB(si|si+1)− log pL(si)] + 1\nNi+1 [log pB(si+1|si)− log pL(si+1)]\n(3) where pL(s) is the language model probability for generating sentence s. We train an LSTM language model, which can be thought of as a SEQ2SEQ model with an empty source. A closer look at Eq. 3 shows that it is of the same form as the mutual information between si+1 and si, namely log[p(si+1, si)/p(si+1)p(si)].\nGeneration The scoring functions in Eqs. 1, 2, and 3 are discriminative, generating coherence scores for an already-given chunk of text. Eqs. 2 and 3 can not be directly used for generation purposes, since they requires the completion of si+1 before the score can be computed. A normal strategy is to generate a big N-best list using Eq. 1 and then rerank the N-best list using Eq. 2 or 3 (Li et al., 2015a). The N-best list can be generated using standard beam search, or other algorithmic variations that promote diversity, coherence, etc. (Shao et al., 2017).\nWeakness (1) The SEQ2SEQ model generates words sequentially based on an evolving hidden vector, which is updated by combining the current word representation with previously built hidden vectors. The generation process is thus not exposed to more global features of the discourse like topics. As the hidden vector evolves, the influence from contexts gradually diminishes, with language models quickly dominating. (2) By predicting a sentence conditioning only on its left or right neighbor, the model lacks the ability to handle the longerterm discourse dependencies across the sentences of a text.\nTo tackle these two issues, we need a model that is able to constantly remind the decoder about the\nglobal meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables."
  }, {
    "heading": "3.2 HMM-LDA based Generative Models (HMM-LDA-GM)",
    "text": "In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA.\nOur first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the SEQ2SEQ model with global information for token generation, with two components:\n(1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the topic from previous sentence. Let tn denote the distribution of topics for the current sentence, where tn ∈ R1×T . We also associate each LDA\ntopic with a K dimensional vector, representing the semantics embedded in this topic. The topicrepresentation matrix is denoted by V ∈ RT×K , where T is the pre-specified number of topics in LDA. V is learned in the word predicting process when training encoder-decoder models.\n(2) Training encoder-decoder models: For the current sentence sn, given its topic distribution tn, we first compute the topic representation zn for sn using the weighted sum of LDA topic vectors:\nzn = tn × V (4) zn can be thought of as a discourse state vector that stores the information the current sentence needs to convey in the discourse, and is used to guide every step of word generation in sn. We run the encoderdecoder model, which subsequently predicts tokens in sn given sn−1. This process is the same as the vanilla version of SEQ2SEQ models, the only difference being that zn is incorporated into each step of decoding for hidden vector updates:\np(sn|zn, sn−1) = M∏ t=1 p(wt|ht−1, zn) (5)\nV is updated along with parameters in the encoderdecoder model. zn influences each time step of decoding, and thus addresses the problem that vanilla SEQ2SEQ models gradually lose global information as the hidden representations evolve. zn is computed based on the topic distribution tn, which is obtained from the HMM-LDA model, thus modeling the global Markov discourse dependency between sentences of the text.3 The model can be adapted to the bi-directional setting, in which we separately train two models to handle the forward probability log p(tn|sn−1, ...) and the backward one log p(tn|sn+1). The bi-directional (bi) strategy described in Eq. 3 can also be incorporated to remove the influence of language models.\nWeakness Topic models (either vanilla or HMM versions) focus on word co-occurrences at the document-level and are thus very lexicon-based. Furthermore, given the diversity of topics in a dataset like Wikipedia but the small number of topic clusters, the LDA model usually produces very coarse-grained topics (politics, sports, history,\n3This pipelined approach is closely related to recent work that incorporates LDA topic information into generation models in an attempt to leverage context information (Ghosh et al., 2016; Xing et al., 2016; Mei et al., 2016)\netc.), assigning very similar topic distributions to consecutive sentences. These topics thus capture topical coherence but are too coarse-grained to capture all the more fine-grained aspects of discourse coherence relationships."
  }, {
    "heading": "3.3 Variational Latent Variable Generative Models (VLV-GM)",
    "text": "We therefore propose instead to train an end-to-end system, in which the meaning transitions between sentences can be naturally learned from the data. Inspired by recent work on generating sentences from a latent space (Serban et al., 2016b; Bowman et al., 2015; Chung et al., 2015), we propose the VSV-GM model in Fig. 1c. Each sentence sn is again associated with a hidden vector representation zn ∈ RK which stores the global information that the current sentence needs to talk about, but instead of obtaining zn from an upstream model like LDA, zn is learned from the training data. zn is a stochastic latent variable conditioned on all previous sentences and zn−1:\np(zn|zn−1, sn−1, sn−2, ...) = N(µtruezn ,Σtruezn ) µtruezn = f(zn−1, sn−1, sn−2, ...) Σtruezn = g(zn−1, sn−1, sn−2, ...)\n(6) where N(µ,Σ) is a multivariate normal distribution with mean µ ∈ RK and covariance matrix Σ ∈ RK×K . Σ is a diagonal matrix. As can be seen, the global information zn for the current sentence depends on the information zn−1 for its previous sentence as well as the text of the context sentences. This forms a Markov chain across all sentences. f and g are neural network models that take previous sentences and zn−1, and map them to a real-valued representation using hierarchical LSTMs (Li et al., 2015b)4.\nEach word wnt from sn is predicted using the concatenation of the representation previously build by the LSTMs (the same vector used in word prediction in vanilla SEQ2SEQ models) and zn, as shown in Eq.5.\nWe are interested in the posterior distribution p(zn|s1, s2, ..., sn−1), namely, the information that the current sentence needs to convey given the preceding ones. Unfortunately, a highly non-linear mapping from zn to tokens in sn results in in-\n4Sentences are first mapped to vector representations using a LSTM model. Another level of LSTM at the sentence level then composes representations of the multiple sentences to a single vector.\ntractable inference of the posterior. A common solution is to use variational inference to learn another distribution, denoted by q(zn|s1, s2, ..., sN ), to approximate the true posterior p(zn|s1, s2, ..., sn−1). The model’s latent variables are obtained by maximizing the variational lower-bound of observing the dataset:\nlog p(s1, .., sN ) ≤ N∑\nt=1\n−DKL(q(zn|sn, sn−1, ...)||p(zn|sn−1, sn−2, ...))\n+ Eq(zn|sn,sn−1,...) log p(sn|zn, sn−1, sn−2, ...) (7) This objective to optimize consists of two parts; the first is the KL divergence between the approximate distribution q and the true posterior p(sn|zn, sn−1, sn−2, ...), in which we want to approximate the true posterior using q. The second part Eq(zn|sn,sn−1,...) log p(sn|zn, sn−1, sn−2, ...), predicts tokens in sn in the same way as in SEQ2SEQ models with the difference that it considers the global information zn.\nThe approximate posterior distribution q(zn|sn, sn−1, ...) takes a form similar to p(zn|sn−1, sn−2, ...):\nq(zn|sn, sn−1, ...) = N(µ approxzn ,Σ approxzn ) µ approxzn = fq(zn−1, sn, sn−1, ...) Σ approxzn = gq(zn−1, sn, sn−1, ...) (8)\nfq and gq are of similar structures to f and g, using a hierarchical neural network model to map context tokens to vector representations.\nLearning and Testing At training time, the approximate posterior q(zn|zn−1, sn, sn−1, ...), the true distribution p(zn|zn−1, sn−1, sn−2, ...), and the generative probability p(sn|zn, sn−1, sn−2, ...) are trained jointly by maximizing the variational lower bound with respect to their parameters: a sample zn is first drawn from the posterior distribution q, namely N(µ approxzn ,Σ approx zn ). This sample is used to approximate the expectation Eq log p(sn|zn, sn−1, sn−2, ...). Using zn, we can update the encoder-decoder model using SGD in a way similar to the standard SEQ2SEQ model, the only difference being that the current token to predict not only depends on the LSTM output ht, but also zn. Given the sampled zn, the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix).\nThe proposed VLV-GM model can be adapted to the bi-directional setting and the bi setting similarly to the way LDA-based models are adapted.\nThe proposed model is closely related to many recent attempts in training variational autoencoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014), variational or latent-variable recurrent nets (Bowman et al., 2015; Chung et al., 2015; Ji et al., 2016; Bayer and Osendorfer, 2014), hierarchical latent variable encoder-decoder models (Serban et al., 2016b,a)."
  }, {
    "heading": "4 Experimental Results",
    "text": "In this section, we describe experimental results. We first evaluate the proposed models on discriminative tasks such as sentence-pair ordering and full paragraph ordering reconstruction. Then we look at the task of coherent text generation."
  }, {
    "heading": "4.1 Sentence Ordering, Domain-specific Data",
    "text": "Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed description of this commonly used dataset and training/testing are found in the Appendix.\nWe report the performance of the following baselines widely used in the coherence literature.\n(1) Entity Grid Model: The grid model presented in Barzilay and Lapata (2008). Results are directly taken from Barzilay and Lapata’s (2008) paper. We also consider variations of entity grid models, such as Louis and Nenkova (2012) which models the\ncluster transition probability and the Graph Based Approach which uses a graph to represent the entity transitions needed for local coherence computation (Guinaudeau and Strube, 2013).\n(2) Li and Hovy (2014): A recursive neural model computes sentence representations based on parse trees. Negative sampling is used to construct negative incoherent examples. Results are from their papers.\n(3) Foltz et al. (1998) computes the semantic relatedness of two text units as the cosine similarity between their LSA vectors. The coherence of a discourse is the average of the cosine of adjacent sentences. We used this intuition, but with more modern embedding models: (1) 300-dimensional Glove word vectors (Pennington et al., 2014), embeddings for a sentence computed by averaging the embeddings of its words (2) Sentence representations obtained from LDA (Blei et al., 2003) with 300 topics, trained on the Wikipedia dataset. Results are reported in Table 2. The extended version of the discriminative model described in this work significantly outperforms the parse-tree based recursive models presented in Li and Hovy (2014) as well as all non-neural baselines. It achieves almost perfect accuracy on the earthquake dataset and 93% on the accident dataset, marking a significant advancement in the benchmark. Generative models (both vanilla SEQ2SEQ and the proposed variational model) do not perform competitively on this dataset. We conjecture that this is due to the small size of the dataset, leading the generative model to overfit."
  }, {
    "heading": "4.2 Evaluating Ordering on Open-domain",
    "text": "Since the dataset presented in Barzilay and Lapata (2008) is quite domain-specific, we propose testing coherence with a much larger, open-domain dataset: Wikipedia. We created a test set by randomly selecting 984 paragraphs from Wikipedia dump 2014, each paragraph consisting of at least 16 sentences. The training set is 30 million sentences not overlapping with the test set."
  }, {
    "heading": "4.2.1 Binary Permutation Classification",
    "text": "We adopt the same strategy as in Barzilay and Lapata (2008), in which we generate pairs of sentence permutations from the original Wikipedia paragraphs. We follow the protocols described in the subsection and each pair whose original paragraph’s score is higher than its permutation is treated as being correctly classified, else incorrectly\nclassified. Models are evaluated using accuracy. We implement the Entity Grid Model (Barzilay and Lapata, 2008) using the Wikipedia training set as a baseline, the detail of which is presented in the Appendix. Other baselines consist of the Glove and LDA updates of the lexical coherence baselines (Foltz et al., 1998).\nResults Table 2 presents results on the binary classification task. Contrary to the findings on the domain specific dataset in the previous subsection, the discriminative model does not yield compelling results, performing only slightly better than the entity grid model. We believe the poor performance is due to the sentence-level negative sampling used by the discriminative model. Due to the huge semantic space in the open-domain setting, the sampled instances can only cover a tiny proportion of the possible negative candidates, and therefore don’t cover the space of possible meanings. By contrast the dataset in Barzilay and Lapata (2008) is very domain-specific, and the semantic space is thus relatively small. By treating all other sentences in the document as negative, the discriminative strategy’s negative samples form a much larger proportion of the semantic space, leading to good performance.\nGenerative models perform significantly better than all other baselines. Compared with the dataset in Barzilay and Lapata (2008), overfitting is not an issue here due to the great amount of training data. In line with our expectation, the MMI model outperforms the bidirectional model, which in turn outperforms the unidirectional model across all three generative model settings. We thus only report MMI results for experiments below. The VLV-GM model outperforms that the LDA-HMM-GM model, which is slightly better than the vanila SEQ2SEQ models."
  }, {
    "heading": "4.2.2 Paragraph Reconstruction",
    "text": "The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large open-domain version. We therefore believe it is time for the community to move to a more difficult task for measuring coherence.\nWe suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003). More formally, given a set of permuted sentences s1, s2, ..., sN (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s.\nBecause the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don’t know the surrounding context. Hence, we only use the generative model. The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partially constructed document. We use beam search with beam size 10. We use the Entity Grid model as a baseline for both the settings.\nEvaluating the absolute positions of sentences would be too harsh, penalizing orderings that maintain relative position between sentences through which local coherence can be manifested. We therefore use Kendall’s τ (Lapata, 2003, 2006), a metric of rank correlation for evaluation. See the Appendix for details of Kendall’s τ computation. We observe a pattern similar to the results on the binary classification task, where the VLV-GM model performs the best."
  }, {
    "heading": "4.3 Adversarial evaluation on Text Generation Quality",
    "text": "Both the tasks above are discriminative ones. We also want to evaluate different models’ ability to generate coherent text chunks. The experiment is set up as follow: each encoder-decoder model is first given a set of context sentences (3 sentences). The model then generates a succeeding sentence using beam-search given the contexts. For the unidirectional setting, we directly take the most probable sequence and for the bi-directional and MMI, we rerank the N-best list using the backward probability and language model probability.\nWe conduct experiments on multi-sentence generation, in which we repeat the generative process described above for N times, where N=1,2,3. At the end of each turn, the context is updated by adding in the newly generated sequence, and this sequence is used as the source input to the encoderdecoder model for next sequence generation. For example, when N is set to 2, given the three context sentences context-a, context-b and context-c, we first generate sen-d given the three context sentences and then generate sen-e given the sen-d, context-a, context-b and context-c.\nFor evaluation, standard word overlap metrics such as BLEU or ROUGE are not suited for our task, and we use adversarial evaluation Bowman et al. (2015); Anjuli and Vinyals (2016). In adversarial evaluation, we train a binary discriminant function to classify a sequence as machine generated or human generated, in an attempt to evaluate the model’s sentence generation capability. The evaluator takes as input the concatenation of the contexts and the generated sentences (i.e., context-a, context-b and context-c, sen-d , sen-e in the example described above),5 and outputs a scalar, indicating the probability of the current text chunk being human-generated. Training/dev/test sets are held-out sets from the one on which generative models are trained. They respectively contain 128,000/12,800/12,800 instances. Since discriminative models cannot generate sentences, and thus cannot be used for adversarial evaluation, they are skipped in this section.\nWe report Adversarial Success (AdverSuc for short), which is the fraction of instances in which a model is capable of fooling the evaluator. Adver-\n5The model uses a hierarchical neural structure that first maps each sentence to a vector representation, with another level of LSTM on top of the constituent sentences, producing a single vector to represent the entire chunk of texts.\nSuc is the difference between 1 and the accuracy achieved by the evaluator. Higher values of AdverSuc for a dialogue generation model are better. AdverSuc-N denotes the adversarial accuracy value on machine-generated texts with N turns.\nTable 4 show AdverSuc numbers for different models. As can be seen, the latent variable model VLV-GM is able to generate chunk of texts that are most indistinguishable from coherent texts from humans. This is due to its ability to handle the dependency between neighboring sentences. Performance declines as the number of turns increases due to the accumulation of errors and current models’ inability to model long-term sentence-level dependency. All models perform poorly on the adver-3 evaluation metric, with the best adversarial success value being 0.081 (the trained evaluator is able to distinguish between human-generated and machine generated dialogues with greater than 90 percent accuracy for all models)."
  }, {
    "heading": "4.4 Qualitative Analysis",
    "text": "With the aim of guiding future investigations, we also briefly explore our model qualitatively, examining the coherence scores assigned to some artificial miniature discourses that exhibit various kinds of coherence.\nCase 1: Lexical Coherence Pinochet was arrested. His arrest was unexpected. 1.79 Pinochet was arrested. His death was unexpected. 0.84 Mary ate some apples. She likes apples. 2.03 Mary ate some apples. She likes pears. 0.27 Mary ate some apples. She likes Paris. -1.35\nThe examples suggest that the model handles lexical coherence, correctly favoring the 1st over\nthe 2nd, and the 3rd over the 4th examples. Note that the coherence score for the final example is negative, which means conditioning on the first sentence actually decreases the likelihood of generating the second one.\nCase 2: Temporal Order Washington was unanimously elected president in the first two national elections. He oversaw the creation of a strong, wellfinanced national government. 1.48 Washington oversaw the creation of a strong, well-financed national government. He was unanimously elected president in the first two national elections. 0.72\nCase 3: Causal Relationship Bret enjoys video games; therefore, he sometimes is late to appointments. 0.69 Bret sometimes is late to appointments; therefore, he enjoys video games. -0.07\nCases 2 and 3 suggest the model may, at least in these simple cases, be capable of addressing the much more complex task of dealing with temporal and causal relationships. Presumably this is because the model is exposed in training to the general preference of natural text for temporal order, and even for the more subtle causal links.\nCase 4: Centering/Referential Coherence Mary ate some apples. She likes apples. 3.06 She ate some apples. Mary likes apples. 2.41\nThe model seems to deal with simple cases of referential coherence.\nExample3: 2.40 John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. Example4: 1.62 John went to his favorite music store to buy a piano. It was a store John had frequented for many years He was excited that he could finally buy a piano.. It was closing just as John arrived.\nIn these final examples from Miltsakaki and Kukich (2004), the model successfully captures the fact that the second text is less coherent due to rough shifts. This suggests that the discourse embedding space may be able to capture a representation of entity focus.\nOf course all of these these qualitative evaluations are only suggestive, and a deeper understanding of what the discourse embedding space is capturing will likely require more sophisticated visualizations."
  }, {
    "heading": "5 Conclusion",
    "text": "We investigate the problem of open-domain discourse coherence, training discriminative models that treating natural texts as coherent and permutations as non-coherent, and Markov generative models that can predict sentences given their neighbors.\nOur work shows that the traditional evaluation metric (ordering pairs of sentences in small domains) is completely solvable by our discriminative models, and we therefore suggest the community move to the harder task of open-domain full-paragraph sentence ordering.\nThe proposed models also offer an initial step in generating coherent texts given contexts, which has the potential to benefit a wide range of generation tasks in NLP. Our latent variable neural models, by offering a new way to learn latent discourse-level features of a text, also suggest new directions in discourse representation that may bring benefits to any discourse-aware NLP task.\nAcknowledgements The authors thank Will Monroe, Sida Wang, Kelvin Guu and the other members of the Stanford NLP Group for helpful discussions and comments. Jiwei Li is supported by a Facebook Fellowship, which we gratefully acknowledge. This work is also partially supported by the NSF under award IIS-1514268, and the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF- 15-1- 0462. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, the NSF, or Facebook."
  }, {
    "heading": "6 Supplemental Material",
    "text": "Details for the domain specific dataset (Barzilay and Lapata, 2008) The corpus consists of 200 articles each from two domains: NTSB airplane accident reports (V=4758, 10.6 sentences/document) and AP earthquake reports (V=3287, 11.5 sentences/document), split into training and testing. For each document, pairs of permutations are generated6. Each pair contains the original document order and a random permutation of the sentences from the same document.\nTraining/Testing details for models on the domain specific dataset We use reduced versions of both generative and discriminative models to allow fair comparison with baselines. For the discriminative model, we generate noise negative examples from random replacements in the training set, with the only difference that random replacements only come from the same document. We use 300 dimensional embeddings borrowed from GLOVE (Pennington et al., 2014) to initialize word embeddings. Word embeddings are kept fixed during training and we update LSTM parameters using AdaGrad (Duchi et al., 2011). For the generative model, due to the small size of the dataset, we train a one layer SEQ2SEQ model with word dimensionality and number of hidden neurons set to 100. The model is trained using SGD with AdaGrad (Zeiler, 2012).\nThe task requires a coherence score for the whole document, which is comprised of multiple cliques. We adopt the strategy described in Li and Hovy (2014) by breaking the document into a series of cliques which is comprised of a sequence of\n6Permutations downloaded from people.csail.mit. edu/regina/coherence/CLsubmission/.\nconsecutive sentences. The document-level coherence score is attained by averaging its constituent cliques. We say a document is more coherent if it achieves a higher average score within its constituent cliques.\nImplementation of Entity Grid Model For each noun in a sentence, we extract its syntactic role (subject, object or other). We use a wikipedia dump parsed using the Fanse Parser (Tratz and Hovy, 2011). Subjects and objects are extracted based on nsubj and dobj relations in the dependency trees. (Barzilay and Lapata, 2008) define two versions of the Entity Grid Model, one using full coreference and a simpler method using only exact-string coreference; Due to the difficulty of running full coreference resolution tens of millions of Wikipedia sentences, we follow other researchers in using Barzilay and Lapata’s simpler method (Feng and Hirst, 2012; Burstein et al., 2010; Barzilay and Lapata, 2008).7\nKendall’s τ Kendall’s τ is computed based on the number of inversions in the rankings as follows:\nτ = 1− 2# of inversions N × (N − 1) (9)\nwhere N denotes the number of sentences in the original document and inversions denote the number of interchanges of consecutive elements needed to reconstruct the original document. Kendall’s τ can be efficiently computed by counting the number of intersections of lines when aligning the original document and the generated document. We refer the readers to Lapata (2003) for more details.\nDerivation for Variation Inference For simplicity, we use µpost and Σapprox to denote µ approx(zn) and Σ approx(zn), µtrue and Σtrue to denote µtrue(zn) and Σtrue(zn). The KLdivergence between the approximate distribution q(zn|zn−1, sn, sn−1, ...) and the true distribution p(zn|zn−1, sn−1, sn−2, ...) in the variational inference is given by:\nDKL(q(zn|zn−1, sn, sn−1, ...)||p(zn|zn−1, sn−1, sn−2, ...) =\n1 2 (tr(Σ−1trueΣapprox)− k + log detΣtrue\ndetΣapprox +(µtrue − µapprox)−1Σ−1true(µtrue − µapprox))\n(10) 7Our implementation of the Entity Grid Model is built upon public available code at https://github.com/ karins/CoherenceFramework.\nwhere k denotes the dimensionality of the vector. Since zn has already been sampled and thus known, µapprox, Σapprox, µtrue, Σtrue and consequently Eq10 can be readily computed. The gradient with respect to µapprox, Σapprox, µtrue, Σtrue can be respectively computed, and the error is then backpropagated to the hierarchical neural models that are used to compute them. We refer the readers to Doersch (2016) for more details about how a general VAE model can be trained.\nOur generate models offer a powerful way to represent the latent discourse structure in a complex embedding space, but one that is hard to visualize. To help understand what the model is doing, we examine some relevant examples, annotated with the (log-likelihood) coherence score from the MMI generative model, with the goal of seeing (qualitatively) the kinds of coherence the model seems to be representing. (The MMI can be viewed as the informational gain from conditioning the generation of the current sentence on its neighbors.)"
  }],
  "year": 2017,
  "references": [{
    "title": "Computing locally coherent discourses",
    "authors": ["Ernst Althaus", "Nikiforos Karamanis", "Alexander Koller."],
    "venue": "Proceedings of ACL 2004.",
    "year": 2004
  }, {
    "title": "Adversarial evaluation of dialogue models",
    "authors": ["Kannan Anjuli", "Oriol Vinyals."],
    "venue": "NIPS 2016 Workshop on Adversarial Training .",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proc. of the International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Modeling local coherence: An entity-based approach",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Computational Linguistics 34(1):1–34.",
    "year": 2008
  }, {
    "title": "Catching the drift: Probabilistic content models, with applications to generation and summarization",
    "authors": ["Regina Barzilay", "Lillian Lee."],
    "venue": "HLT-NAACL. pages 113–120.",
    "year": 2004
  }, {
    "title": "Sentence fusion for multidocument news summarization",
    "authors": ["Regina Barzilay", "Kathleen R McKeown."],
    "venue": "Computational Linguistics 31(3):297–328.",
    "year": 2005
  }, {
    "title": "Learning stochastic recurrent networks",
    "authors": ["Justin Bayer", "Christian Osendorfer."],
    "venue": "arXiv preprint arXiv:1411.7610 .",
    "year": 2014
  }, {
    "title": "Automated analysis of free speech predicts psychosis",
    "authors": ["Gillinder Bedi", "Facundo Carrillo", "Guillermo A Cecchi", "Diego Fernández Slezak", "Mariano Sigman", "Natália B Mota", "Sidarta Ribeiro", "Daniel C Javitt", "Mauro Copelli", "Cheryl M Corcoran"],
    "year": 2015
  }, {
    "title": "Latent dirichlet allocation",
    "authors": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."],
    "venue": "Journal of machine Learning research 3(Jan):993–1022.",
    "year": 2003
  }, {
    "title": "Generating sentences from a continuous space",
    "authors": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."],
    "venue": "arXiv preprint arXiv:1511.06349 .",
    "year": 2015
  }, {
    "title": "Using entity-based features to model coherence in student essays",
    "authors": ["Jill Burstein", "Joel Tetreault", "Slava Andreyev."],
    "venue": "Human language technologies: The 2010 annual conference of the North American chapter of the Association for Computational Lin-",
    "year": 2010
  }, {
    "title": "A recurrent latent variable model for sequential data",
    "authors": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "Advances in neural information processing systems. pages 2980–2988.",
    "year": 2015
  }, {
    "title": "Tutorial on variational autoencoders",
    "authors": ["Carl Doersch."],
    "venue": "arXiv preprint arXiv:1606.05908 .",
    "year": 2016
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Extending the entity grid with entity-specific features",
    "authors": ["Micha Eisner", "Eugene Charniak."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Associ-",
    "year": 2011
  }, {
    "title": "A unified local and global model for discourse coherence",
    "authors": ["Micha Elsner", "Joseph L Austerweil", "Eugene Charniak."],
    "venue": "HLT-NAACL. pages 436–443.",
    "year": 2007
  }, {
    "title": "Coreference-inspired coherence modeling",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human",
    "year": 2008
  }, {
    "title": "Quantifying incoherence in speech: An automated methodology and novel application to schizophrenia",
    "authors": ["Brita Elvevåg", "Peter W Foltz", "Daniel R Weinberger", "Terry E Goldberg."],
    "venue": "Schizophrenia research 93(1):304–316.",
    "year": 2007
  }, {
    "title": "Extending the entity-based coherence model with multiple ranks",
    "authors": ["Vanessa Wei Feng", "Graeme Hirst."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational",
    "year": 2012
  }, {
    "title": "Discourse coherence and lsa",
    "authors": ["Peter W Foltz."],
    "venue": "Handbook of latent semantic analysis pages 167– 184.",
    "year": 2007
  }, {
    "title": "The measurement of textual coherence with latent semantic analysis",
    "authors": ["Peter W Foltz", "Walter Kintsch", "Thomas K Landauer."],
    "venue": "Discourse processes 25(2-3):285–307.",
    "year": 1998
  }, {
    "title": "Contextual lstm (clstm) models for large scale nlp tasks",
    "authors": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck."],
    "venue": "arXiv preprint arXiv:1602.06291 .",
    "year": 2016
  }, {
    "title": "Hidden topic markov models",
    "authors": ["Amit Gruber", "Yair Weiss", "Michal Rosen-Zvi."],
    "venue": "AISTATS. volume 2, pages 163–170.",
    "year": 2007
  }, {
    "title": "Graphbased local coherence modeling",
    "authors": ["Camille Guinaudeau", "Michael Strube."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. pages 93–103.",
    "year": 2013
  }, {
    "title": "Cohesion in English",
    "authors": ["M.A.K. Halliday", "Ruqaiya Hasan."],
    "venue": "Longman.",
    "year": 1976
  }, {
    "title": "Planning coherent multisentential text",
    "authors": ["Eduard H Hovy."],
    "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 163– 169.",
    "year": 1988
  }, {
    "title": "A latent variable recurrent neural network for discourse relation language models",
    "authors": ["Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein."],
    "venue": "arXiv preprint arXiv:1603.01913 .",
    "year": 2016
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "arXiv preprint arXiv:1312.6114 .",
    "year": 2013
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Advances in Neural Information Processing Systems. pages 3276–3284.",
    "year": 2015
  }, {
    "title": "Probabilistic text structuring: Experiments with sentence ordering",
    "authors": ["Mirella Lapata."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, pages 545–552.",
    "year": 2003
  }, {
    "title": "Automatic evaluation of information ordering: Kendall’s tau",
    "authors": ["Mirella Lapata."],
    "venue": "Computational Linguistics 32(4):471–484.",
    "year": 2006
  }, {
    "title": "Discourse relations and defeasible knowledge",
    "authors": ["Alex Lascarides", "Nicholas Asher."],
    "venue": "Proceedings of the 29th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 55–62.",
    "year": 1991
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "arXiv preprint arXiv:1510.03055 .",
    "year": 2015
  }, {
    "title": "A model of coherence based on distributed sentence representation",
    "authors": ["Jiwei Li", "Eduard Hovy."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2014
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1506.01057 .",
    "year": 2015
  }, {
    "title": "Automatically evaluating text coherence using discourse relations",
    "authors": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
    "year": 2011
  }, {
    "title": "A coherence model based on syntactic patterns",
    "authors": ["Annie Louis", "Ani Nenkova."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Com-",
    "year": 2012
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "EMNLP .",
    "year": 2015
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C Mann", "Sandra A Thompson."],
    "venue": "Text 8(3):243–281.",
    "year": 1988
  }, {
    "title": "From local to global coherence: A bottom-up approach to text planning",
    "authors": ["Daniel Marcu."],
    "venue": "AAAI/IAAI. Citeseer, pages 629–635.",
    "year": 1997
  }, {
    "title": "Cohmetrix: Capturing linguistic features of cohesion",
    "authors": ["Danielle S. McNamara", "Max M. Louwerse", "Philip M. McCarthy", "Arthur C. Graesser."],
    "venue": "Discourse Processes 47(4):292–330.",
    "year": 2010
  }, {
    "title": "Coherent dialogue with attention-based language models",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter."],
    "venue": "arXiv preprint arXiv:1611.06997 .",
    "year": 2016
  }, {
    "title": "Evaluation of text coherence for electronic essay scoring systems",
    "authors": ["Eleni Miltsakaki", "Karen Kukich."],
    "venue": "Natural Language Engineering 10(01):25–55.",
    "year": 2004
  }, {
    "title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text",
    "authors": ["J. Morris", "G. Hirst."],
    "venue": "Computational Linguistics 17(1):21–48.",
    "year": 1991
  }, {
    "title": "A twodimensional topic-aspect model for discovering multi-faceted topics",
    "authors": ["Michael Paul", "Roxana Girju."],
    "venue": "Urbana 51(61801):36.",
    "year": 2010
  }, {
    "title": "Mixed membership markov models for unsupervised conversation modeling",
    "authors": ["Michael J Paul."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Associ-",
    "year": 2012
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP. volume 14, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."],
    "venue": "arXiv preprint arXiv:1401.4082 .",
    "year": 2014
  }, {
    "title": "Unsupervised modeling of twitter conversations",
    "authors": ["Alan Ritter", "Colin Cherry", "Bill Dolan."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for",
    "year": 2010
  }, {
    "title": "Multiresolution recurrent neural networks: An application to dialogue response generation",
    "authors": ["Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron Courville."],
    "venue": "arXiv preprint",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1605.06069 .",
    "year": 2016
  }, {
    "title": "Generating long and diverse responses with neural conversation models",
    "authors": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil."],
    "venue": "arXiv preprint arXiv:1701.03185 .",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."],
    "venue": "Advances in neural information processing systems. pages 3104–3112.",
    "year": 2014
  }, {
    "title": "A fast, accurate, non-projective, semantically-enriched parser",
    "authors": ["Stephen Tratz", "Eduard Hovy."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1257–1268.",
    "year": 2011
  }, {
    "title": "Evaluating discoursebased answer extraction for why-question answering",
    "authors": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "Peter-Arno Coppen."],
    "venue": "Proceedings of the 30th annual international",
    "year": 2007
  }, {
    "title": "Structural topic model for latent topical structure analysis",
    "authors": ["Hongning Wang", "Duo Zhang", "ChengXiang Zhai."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.",
    "year": 2011
  }, {
    "title": "Topic augmented neural response generation with a joint attention mechanism",
    "authors": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."],
    "venue": "arXiv preprint arXiv:1606.08340 .",
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701 .",
    "year": 2012
  }],
  "id": "SP:ff45849811e8ed80448283e2e97a68a6483e44b9",
  "authors": [{
    "name": "Jiwei Li",
    "affiliations": []
  }, {
    "name": "Dan Jurafsky",
    "affiliations": []
  }],
  "abstractText": "Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding. Yet existing models of coherence focus on measuring individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domainindependent neural models of discourse coherence that are capable of measuring multiple aspects of coherence in existing sentences and can maintain coherence while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.",
  "title": "Neural Net Models of Open-domain Discourse Coherence"
}