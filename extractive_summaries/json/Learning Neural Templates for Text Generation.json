{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3174–3187 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3174"
  }, {
    "heading": "1 Introduction",
    "text": "With the continued success of encoder-decoder models for machine translation and related tasks, there has been great interest in extending these methods to build general-purpose, data-driven natural language generation (NLG) systems (Mei et al., 2016; Dušek and Jurcıcek, 2016; Lebret et al., 2016; Chisholm et al., 2017; Wiseman et al., 2017). These encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) use a neural encoder model to represent a source knowledge base, and a decoder model to emit a textual description word-by-word, conditioned on the source encoding. This style of generation contrasts with the more traditional division of labor in NLG, which famously emphasizes addressing the two questions of “what to say” and “how to say it” separately, and which leads to systems with explicit content selection, macro- and micro-planning, and surface realization components (Reiter and Dale, 1997; Jurafsky and Martin, 2014).\nEncoder-decoder generation systems appear to have increased the fluency of NLG outputs, while reducing the manual effort required. However, due to the black-box nature of generic encoderdecoder models, these systems have also largely sacrificed two important desiderata that are often found in more traditional systems, namely (a) interpretable outputs that (b) can be easily controlled in terms of form and content.\nThis work considers building interpretable and controllable neural generation systems, and proposes a specific first step: a new data-driven generation model for learning discrete, template-like\nstructures for conditional text generation. The core system uses a novel, neural hidden semimarkov model (HSMM) decoder, which provides a principled approach to template-like text generation. We further describe efficient methods for training this model in an entirely data-driven way by backpropagation through inference. Generating with the template-like structures induced by the neural HSMM allows for the explicit representation of what the system intends to say (in the form of a learned template) and how it is attempting to say it (in the form of an instantiated template).\nWe show that we can achieve performance competitive with other neural NLG approaches, while making progress satisfying the above two desiderata. Concretely, our experiments indicate that we can induce explicit templates (as shown in Figure 1) while achieving competitive automatic scores, and that we can control and interpret our generations by manipulating these templates. Finally, while our experiments focus on the data-to-text regime, we believe the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text."
  }, {
    "heading": "2 Related Work",
    "text": "A core task of NLG is to generate textual descriptions of knowledge base records. A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner. For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013).\nThere has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach. However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013). In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over generation segments.\nRecently, a new paradigm has emerged around neural text generation systems based on machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). Most of this work has used unconstrained black-box encoderdecoder approaches. There has been some work on discrete variables in this context, including extracting representations (Shen et al., 2018), incorporating discrete latent variables in text modeling (Yang et al., 2018), and using non-HSMM segmental models for machine translation or summarization (Yu et al., 2016; Wang et al., 2017; Huang et al., 2018). Dai et al. (2017) develop an approximate inference scheme for a neural HSMM using RNNs for continuous emissions; in contrast we maximize the exact log-marginal, and use RNNs to parameterize a discrete emission distribution. Finally, there has also been much recent interest in segmental RNN models for non-generative tasks in NLP (Tang et al., 2016; Kong et al., 2016; Lu et al., 2016).\nThe neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated or transferred (Shen et al., 2017; Zhao et al., 2018; Li et al., 2018). In contrast, here we focus on controlling either the content of a generation or the way it is expressed by manipulating the (latent) template used in realizing the generation."
  }, {
    "heading": "3 Overview: Data-Driven NLG",
    "text": "Our focus is on generating a textual description of a knowledge base or meaning representation. Following standard notation (Liang et al., 2009; Wiseman et al., 2017), let x= {r1 . . . rJ} be a collection of records. A record is made up of a type (r.t), an entity (r.e), and a value (r.m). For example, a knowledge base of restaurants might have a record with r.t = Cuisine, r.e = Denny’s, and r.m = American. The aim is to generate an adequate and fluent text description ŷ1:T = ŷ1, . . . , ŷT of x. Concretely, we consider the E2E Dataset (Novikova et al., 2017) and the WikiBio Dataset (Lebret et al., 2016). We show an example E2E knowledge base x in the top of Figure 1. The top of Figure 2 shows an example knowledge base x from the WikiBio dataset, where it is paired with a reference text y= y1:T at the bottom.\nThe dominant approach in neural NLG has been\n3176\nsopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences.\nData-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).\nAt the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011).\nOur approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).\nOur model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the WEATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design.\nFigure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist,\nplant pathologist, computer scientist, mathematician, mystic, and mycologist.”.\n3 Language Modeling for Constrained Sentence generation\nConditional language models are a popular choice to generate sentences. We introduce a tableconditioned language model for constraining text generation to include elements from fact tables.\n3.1 Language model Given a sentence s = w1, . . . , wT with T words from vocabularyW , a language model estimates:\nP (s) =\nT∏\nt=1\nP (wt|w1, . . . , wt−1) . (1)\nLet ct = wt−(n−1), . . . , wt−1 be the sequence of n − 1 context words preceding wt. An n-gram language model makes an order n Markov assumption,\nP (s) ≈ T∏\nt=1\nP (wt|ct) . (2)\n3.2 Language model conditioned on tables A table is a set of field/value pairs, where values are sequences of words. We therefore propose language models that are conditioned on these pairs.\nLocal conditioning refers to the information from the table that is applied to the description of the words which have already generated, i.e. the previous words that constitute the context of the language\nFrederick Parker-Rhodes (21 March 1914 - 21 November\n1987) was an English linguist, plant pathologist, computer\nscientist, mathematician, mystic, and mycologist.\nFigure 2: An example from the WikiBio dataset (Lebret et al., 2016), with a database x (top) for Frederick ParkerRhodes and corresponding reference generation y (bottom).\nto use an encoder network over x and then a conditio al decoder network to generate y, training the whole system in an end-to-end manner. To generate a description for a given example, a black-box network (such as an RNN) is used to produce a distribution over the n xt w rd, from which a choice is made and fed back into the system. The entire distribution is driven by the internal states of the neural network.\nWhile effective, relying on a neural decoder makes it difficult to understand what aspects of x are correlated with a particular system output. This leads to problems both in controlling finegrained aspects of the generation process and in interpreting model mistakes.\nAs an example of why controllability is important, consider the records in Figure 1. Given these inputs an end-user might ant to gene te an output meeting specific co straints, such as not mentioning any information relating to customer rating. Under a standard encoder-decoder style model, one could filter out this information either from the encoder or decoder, but in practice this would lead to unexpected changes in output that might propagate through the whole system.\nAs n xampl of the difficulty of interpreting mistakes, consider following actual generation from an encoder-decoder style system for\nthe records in Figure 2: ”frederick parker-rhodes (21 november 1914 - 2 march 1987) was an english mycology and plant pathology, mathematics at the university of uk.” In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempting to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or perhaps it is simply failing to fluently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence).\nTraditional NLG systems (Kukich, 1983; McKeown, 1992; Belz, 2008; Gatt and Reiter, 2009), in contrast, largely avoid these problems. Since they typically employ an explicit planning component, which decides which knowledge base records to\nfocus on, and a surface realization component,\nwhich realizes the chosen records, the intent of the\nystem is always explicit, and it may be modified\nto meet constraints. The goal of this work is to propose an approach to neural NLG that addresses these issues in a principled way. We target this goal by proposing a new model that generates with template-like objects induced by a neural HSMM (see Figure 1). Templates are useful here because they represent a fixed plan for the generation’s content, and because they make it clear what part of the generation is associated with which record in the knowledge base."
  }, {
    "heading": "4 Background: Semi-Markov Models",
    "text": "What does it mean to learn a template? It is natural to think of a template as a sequence of typed text-segments, perhaps with some segments acting as the template’s “backbone” (Wang and Cardie, 2013), and the remaining segments filled in from the knowledge base.\nA natural probabilistic model conforming with this intuition is the hidden semi-markov model (HSMM) (Gales and Young, 1993; Ostendorf et al., 1996), which models latent segmentations in an output sequence. Informally, an HSMM is much like an HMM, except emissions may last multiple time-steps, and multi-step emissions need not be independent of each other conditioned on the state.\nWe briefly review HSMMs following Murphy (2002). Assume we have a sequence of obs rved tokens y1 . . . yT and a discrete, latent state zt ∈{1, . . . ,K} for each timestep. We addition-\nally use two per-timestep variables to model multistep segments: a length variable lt ∈{1, . . . , L} specifying the length of the current segment, and a deterministic binary variable ft indicating whether a segment finishes at time t. We will consider in particular conditional HSMMs, which condition on a source x, essentially giving us an HSMM decoder.\nAn HSMM specifies a joint distribution on the observations and latent segmentations. Letting θ denote all the parameters of the model, and using the variables introduced above, we can write the corresponding joint-likelihood as follows\np(y, z, l, f |x; θ) = T−1∏ t=0 p(zt+1, lt+1 | zt, lt, x)ft\n× T∏ t=1 p(yt−lt+1:t | zt, lt, x)ft ,\nwhere we take z0 to be a distinguished startstate, and the deterministic ft variables are used for excluding non-segment log probabilities. We further assume p(zt+1, lt+1 | zt, lt, x) factors as p(zt+1 | zt, x) × p(lt+1 | zt+1). Thus, the likelihood is given by the product of the probabilities of each discrete state transition made, the probability of the length of each segment given its discrete state, and the probability of the observations in each segment, given its state and length."
  }, {
    "heading": "5 A Neural HSMM Decoder",
    "text": "We use a novel, neural parameterization of an HSMM to specify the probabilities in the likelihood above. This full model, sketched out in Figure 3, allows us to incorporate the modeling components, such as LSTMs and attention, that make neural text generation effective, while maintaining the HSMM structure."
  }, {
    "heading": "5.1 Parameterization",
    "text": "Since our model must condition on x, let rj ∈Rd represent a real embedding of record rj ∈x, and let xa ∈Rd represent a real embedding of the entire knowledge base x, obtained by max-pooling coordinate-wise over all the rj . It is also useful to have a representation of just the unique types of records that appear in x, and so we also define xu ∈Rd to be the sum of the embeddings of the unique types appearing in x, plus a bias vector and followed by a ReLU nonlinearity.\nTransition Distribution The transition distribution p(zt+1 | zt, x) may be viewed as aK ×K matrix of probabilities, where each row sums to 1. We define this matrix to be\np(zt+1 | zt, x) ∝ AB +C(xu)D(xu),\nwhere A∈RK×m1 , B ∈Rm1×K are state embeddings, and where C : Rd → RK×m2 and D : Rd → Rm2×K are parameterized non-linear functions of xu. We apply a row-wise softmax to the resulting matrix to obtain the desired probabilities.\nLength Distribution We simply fix all length probabilities p(lt+1 | zt+1) to be uniform up to a maximum length L.1\nEmission Distribution The emission model models the generation of a text segment conditioned on a latent state and source information, and so requires a richer parameterization. Inspired by the models used for neural NLG, we base this model on an RNN decoder, and write a segment’s probability as a product over token-level probabilities,\np(yt−lt+1:t | zt= k, lt= l, x) = lt∏ i=1 p(yt−lt+i | yt−lt+1:t−lt+i−1, zt= k, x)\n× p(</seg> | yt−lt+1:t, zt= k, x)× 1{lt = l},\n1We experimented with parameterizing the length distribution, but found that it led to inferior performance. Forcing the length probabilities to be uniform encourages the model to cluster together functionally similar emissions of different lengths, while parameterizing them can lead to states that specialize to specific emission lengths.\nwhere </seg> is an end of segment token. The RNN decoder uses attention and copy-attention over the embedded records rj , and is conditioned on zt= k by concatenating an embedding corresponding to the k’th latent state to the RNN’s input; the RNN is also conditioned on the entire x by initializing its hidden state with xa.\nMore concretely, let hki−1 ∈Rd be the state of an RNN conditioned on x and zt= k (as above) run over the sequence yt−lt+1:t−lt+i−1. We let the model attend over records rj using hki−1 (in the style of Luong et al. (2015)), producing a context vector cki−1. We may then obtain scores vi−1 for each word in the output vocabulary,\nvi−1=W tanh(g k 1 ◦ [hki−1, cki−1]),\nwith parameters gk1 ∈R2d and W ∈RV×2d. Note that there is a gk1 vector for each of K discrete states. To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records rj using copy attention (Gülçehre et al., 2016; Gu et al., 2016; Yang et al., 2016). Define copy scores,\nρj = r T j tanh(g k 2 ◦ hki−1),\nwhere gk2 ∈Rd. We then normalize the outputvocabulary and copy scores together, to arrive at\nṽi−1=softmax([vi−1, ρ1, . . . , ρJ ]),\nand thus\np(yt−lt+i=w | yt−lt+1:t−lt+i−1, zt= k, x) = ṽi−1,w + ∑\nj:rj .m=w\nṽi−1,V+j .\nAn Autoregressive Variant The model as specified assumes segments are independent conditioned on the associated latent state and x. While this assumption still allows for reasonable performance, we can tractably allow interdependence between tokens (but not segments) by having each next-token distribution depend on all the previously generated tokens, giving us an autoregressive HSMM. For this model, we will in fact use p(yt−lt+i=w | y1:t−lt+i−1, zt= k, x) in defining our emission model, which is easily implemented by using an additional RNN run over all the preceding tokens. We will report scores for both non-autoregressive and autoregressive HSMM decoders below."
  }, {
    "heading": "5.2 Learning",
    "text": "The model requires fitting a large set of neural network parameters. Since we assume z, l, and f are unobserved, we marginalize over these variables to maximize the log marginal-likelihood of the observed tokens y given x. The HSMM marginal-likelihood calculation can be carried out efficiently with a dynamic program analogous to either the forward- or backward-algorithm familiar from HMMs (Rabiner, 1989).\nIt is actually more convenient to use the backward-algorithm formulation when using RNNs to parameterize the emission distributions, and we briefly review the backward recurrences here, again following Murphy (2002). We have:\nβt(j) = p(yt+1:T | zt= j, ft=1, x)\n= K∑ k=1 β∗t (k) p(zt+1= k | zt = j)\nβ∗t (k) = p(yt+1:T | zt+1 = k, ft = 1, x)\n= L∑ l=1 [ βt+l(k) p(lt+1= l | zt+1= k)\np(yt+1:t+l | zt+1= k, lt+1= l) ] ,\nwith base case βT (j)= 1. We can now obtain the marginal probability of y as p(y |x)= ∑K k=1 β ∗ 0(k) p(z1= k), where we have used the fact that f0 must be 1, and we therefore train to maximize the log-marginal likelihood of the observed y:\nln p(y |x; θ) = ln K∑ k=1 β∗0(k) p(z1= k). (1)\nSince the quantities in (1) are obtained from a dynamic program, which is itself differentiable, we may simply maximize with respect to the parameters θ by back-propagating through the dynamic program; this is easily accomplished with automatic differentiation packages, and we use pytorch (Paszke et al., 2017) in all experiments."
  }, {
    "heading": "5.3 Extracting Templates and Generating",
    "text": "After training, we could simply condition on a new database and generate with beam search, as is standard with encoder-decoder models. However, the structured approach we have developed allows us to generate in a more template-like way, giving us more interpretable and controllable generations.\n[The Golden Palace]55 [is a]59 [coffee shop]12 [providing]3 [Indian]50 [food]1 [in the]17 [£20- 25]26 [price range]16 [.]2 [It is]8 [located in the]25 [riverside]40 [.]53 [Its customer rating is]19 [high]23 [.]2\nFigure 4: A sample Viterbi segmentation of a training text; subscripted numbers indicate the corresponding latent state. From this we can extract a template with S=17 segments; compare with the template used at the bottom of Figure 1.\nFirst, note that given a database x and reference generation y we can obtain the MAP assignment to the variables z, l, and f with a dynamic program similar to the Viterbi algorithm familiar from HMMs. These assignments will give us a typed segmentation of y, and we show an example Viterbi segmentation of some training text in Figure 4. Computing MAP segmentations allows us to associate text-segments (i.e., phrases) with the discrete labels zt that frequently generate them. These MAP segmentations can be used in an exploratory way, as a sort of dimensionality reduction of the generations in the corpus. More importantly for us, however, they can also be used to guide generation.\nIn particular, since each MAP segmentation implies a sequence of hidden states z, we may run a template extraction step, where we collect the most common “templates” (i.e., sequences of hidden states) seen in the training data. Each “template” z(i) consists of a sequence of latent states, with z(i)= z(i)1 , . . . z (i) S representing the S distinct segments in the i’th extracted template (recall that we will technically have a zt for each time-step, and so z(i) is obtained by collapsing adjacent zt’s with the same value); see Figure 4 for an example template (with S=17) that can be extracted from the E2E corpus. The bottom of Figure 1 shows a visualization of this extracted template, where discrete states are replaced by the phrases they frequently generate in the training data.\nWith our templates z(i) in hand, we can then restrict the model to using (one of) them during generation. In particular, given a new input x, we may generate by computing\nŷ(i) = argmax y′ p(y′, z(i) |x), (2)\nwhich gives us a generation ŷ(i) for each extracted template z(i). For example, the generation in Figure 1 is obtained by maximizing (2) with x set to the database in Figure 1 and z(i) set to the template\nextracted in Figure 4. In practice, the argmax in (2) will be intractable to calculate exactly due to the use of RNNs in defining the emission distribution, and so we approximate it with a constrained beam search. This beam search looks very similar to that typically used with RNN decoders, except the search occurs only over a segment, for a particular latent state k."
  }, {
    "heading": "5.4 Discussion",
    "text": "Returning to the discussion of controllability and interpretability, we note that with the proposed model (a) it is possible to explicitly force the generation to use a chosen template z(i), which is itself automatically learned from training data, and (b) that every segment in the generated ŷ(i) is typed by its corresponding latent variable. We explore these issues empirically in Section 7.1.\nWe also note that these properties may be useful for other text applications, and that they offer an additional perspective on how to approach latent variable modeling for text. Whereas there has been much recent interest in learning continuous latent variable representations for text (see Section 2), it has been somewhat unclear what the latent variables to be learned are intended to capture. On the other hand, the latent, template-like structures we induce here represent a plausible, probabilistic latent variable story, and allow for a more controllable method of generation.\nFinally, we highlight one significant possible issue with this model – the assumption that segments are independent of each other given the corresponding latent variable and x. Here we note that the fact that we are allowed to condition on x is quite powerful. Indeed, a clever encoder could capture much of the necessary interdependence between the segments to be generated (e.g., the correct determiner for an upcoming noun phrase) in its encoding, allowing the segments themselves to be decoded more or less independently, given x."
  }, {
    "heading": "6 Data and Methods",
    "text": "Our experiments apply the approach outlined above to two recent, data-driven NLG tasks."
  }, {
    "heading": "6.1 Datasets",
    "text": "Experiments use the E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016) datasets, examples of which are shown in Figures 1 and 2, respectively. The former dataset, used for the\n2018 E2E-Gen Shared Task, contains approximately 50K total examples, and uses 945 distinct word types, and the latter dataset contains approximately 500K examples and uses approximately 400K word types. Because our emission model uses a word-level copy mechanism, any record with a phrase consisting of n words as its value is replaced with n positional records having a single word value, following the preprocessing of Lebret et al. (2016). For example, “type[coffee shop]” in Figure 1 becomes “type-1[coffee]” and “type2[shop].”\nFor both datasets we compare with published encoder-decoder models, as well as with direct template-style baselines. The E2E task is evaluated in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015), and METEOR (Banerjee and Lavie, 2005).2 The benchmark system for the task is an encoder-decoder style system followed by a reranker, proposed by Dušek and Jurcıcek (2016). We compare to this baseline, as well as to a simple but competitive non-parametric template-like baseline (“SUB” in tables), which selects a training sentence with records that maximally overlap (without including extraneous records) the unseen set of records we wish to generate from; ties are broken at random. Then, word-spans in the chosen training sentence are aligned with records by string-match, and replaced with the corresponding fields of the new set of records.3\nThe WikiBio dataset is evaluated in terms of BLEU, NIST, and ROUGE, and we compare with the systems and baselines implemented by Lebret et al. (2016), which include two neural, encoderdecoder style models, as well as a Kneser-Ney, templated baseline."
  }, {
    "heading": "6.2 Model and Training Details",
    "text": "We first emphasize two additional methodological details important for obtaining good performance.\nConstraining Learning We were able to learn more plausible segmentations of y by constraining the model to respect word spans yt+1:t+l that appear in some record rj ∈x. We accomplish this by giving zero probability (within the backward re-\n2We use the official E2E NLG Challenge scoring scripts at https://github.com/tuetschek/e2e-metrics.\n3For categorical records, like “familyFriendly”, which cannot easily be aligned with a phrase, we simply select only candidate training sentences with the same categorical value.\ncurrences in Section 5) to any segmentation that splits up a sequence yt+1:t+l that appears in some rj , or that includes yt+1:t+l as a subsequence of another sequence. Thus, we maximize (1) subject to these hard constraints.\nIncreasing the Number of Hidden States While a larger K allows for a more expressive latent model, computing K emission distributions over the vocabulary can be prohibitively expensive. We therefore tie the emission distribution between multiple states, while allowing them to have a different transition distributions.\nWe give additional architectural details of our model in the Supplemental Material; here we note that we use an MLP to embed rj ∈Rd, and a 1- layer LSTM (Hochreiter and Schmidhuber, 1997) in defining our emission distributions. In order to reduce the amount of memory used, we restrict our output vocabulary (and thus the height of the matrix W in Section 5) to only contain words in y that are not present in x; any word in y present in x is assumed to be copied. In the case where a word yt appears in a record rj (and could therefore have been copied), the input to the LSTM at time t+1 is computed using information from rj ; if there are multiple rj from which yt could have been copied, the computed representations are simply averaged.\nFor all experiments, we set d=300 and L=4. At generation time, we select the 100 most common templates z(i), perform beam search with a beam of size 5, and select the generation with the highest overall joint probability.\nFor our E2E experiments, our best nonautoregressive model has 55 “base” states, duplicated 5 times, for a total of K =275 states, and our best autoregressive model uses K =60 states, without any duplication. For our WikiBio experiments, both our best non-autoregressive and autoregressive models uses 45 base states duplicated 3 times, for a total of K =135 states. In all cases, K was chosen based on BLEU performance on held-out validation data. Code implementing our models is available at https://github.com/ harvardnlp/neural-template-gen."
  }, {
    "heading": "7 Results",
    "text": "Our results on automatic metrics are shown in Tables 1 and 2. In general, we find that the templated baselines underperform neural models, whereas our proposed model is fairly competitive with neural models, and sometimes even out-\nperforms them. On the E2E data, for example, we see in Table 1 that the SUB baseline, despite having fairly impressive performance for a nonparametric model, fares the worst. The neural HSMM models are largely competitive with the encoder-decoder system on the validation data, despite offering the benefits of interpretability and controllability; however, the gap increases on test.\nTable 2 evaluates our system’s performance on the test portion of the WikiBio dataset, comparing with the systems and baselines implemented by Lebret et al. (2016). Again for this dataset we see that their templated Kneser-Ney model underperforms on the automatic metrics, and that neural models improve on these results. Here the HSMMs are competitive with the best model of Lebret et al. (2016), and even outperform it on ROUGE. We emphasize, however, that recent, sophisticated approaches to encoder-decoder style\nTravellers Rest Beefeater\nname[Travellers Rest Beefeater], customerRating[3 out of 5], area[riverside], near[Raja Indian Cuisine]\n1. [Travellers Rest Beefeater]55 [is a]59 [3 star]43 [restaurant]11 [located near]25 [Raja Indian Cuisine]40 [.]53 2. [Near]31 [riverside]29 [,]44 [Travellers Rest Beefeater]55 [serves]3 [3 star]50 [food]1 [.]2 3. [Travellers Rest Beefeater]55 [is a]59 [restaurant]12 [providing]3 [riverside]50 [food]1 [and has a]17 [3 out of 5]26 [customer rating]16 [.]2 [It is]8 [near]25 [Raja Indian Cuisine]40 [.]53 4. [Travellers Rest Beefeater]55 [is a]59 [place to eat]12 [located near]25 [Raja Indian Cuisine]40 [.]53 5. [Travellers Rest Beefeater]55 [is a]59 [3 out of 5]5 [rated]32 [riverside]43 [restaurant]11 [near]25 [Raja Indian Cuisine]40 [.]53\nTable 3: Impact of varying the template z(i) for a single x from the E2E validation data; generations are annotated with the segmentations of the chosen z(i). Results were obtained using the NTemp+AR model from Table 1.\ndatabase-to-text generation have since surpassed the results of Lebret et al. (2016) and our own, and we show the recent seq2seq style results of Liu et al. (2018), who use a somewhat larger model, at the bottom of Table 2."
  }, {
    "heading": "7.1 Qualitative Evaluation",
    "text": "We now qualitatively demonstrate that our generations are controllable and interpretable.\nControllable Diversity One of the powerful aspects of the proposed approach to generation is that we can manipulate the template z(i) while leaving the database x constant, which allows for easily controlling aspects of the generation. In Table 3 we show the generations produced by our model for five different neural template sequences z(i), while fixing x. There, the segments in each generation are annotated with the latent states determined by the corresponding z(i). We see that these templates can be used to affect the wordordering, as well as which fields are mentioned in the generated text. Moreover, because the discrete states align with particular fields (see below), it is generally simple to automatically infer to which fields particular latent states correspond, allowing users to choose which template best meets their requirements. We emphasize that this level of controllability is much harder to obtain for encoderdecoder models, since, at best, a large amount of sampling would be required to avoid generating around a particular mode in the conditional distribution, and even then it would be difficult to control the sort of generations obtained.\nInterpretable States Discrete states also provide a method for interpreting the generations produced by the system, since each segment is explicitly typed by the current hidden state of the model. Table 4 shows the impact of varying the template z(i) for a single x from the WikiBio dataset. While there is in general surprisingly little stylistic variation in the WikiBio data itself, there is variation in the information discussed, and the templates capture this. Moreover, we see that particular discrete states correspond in a consistent way to particular pieces of information, allowing us to align states with particular field types. For instance, birth names have the same hidden state (132), as do names (117), nationalities (82), birth dates (101), and occupations (20).\nTo demonstrate empirically that the learned states indeed align with field types, we calculate the average purity of the discrete states learned for both datasets in Table 5. In particular, for each discrete state for which the majority of its generated words appear in some rj , the purity of a state’s record type alignment is calculated as the percentage of the state’s words that come from the most frequent record type the state represents. This calculation was carried out over training examples that belonged to one of the top 100 most frequent templates. Table 5 indicates that discrete states learned on the E2E data are quite pure. Discrete states learned on the WikiBio data are less pure, though still rather impressive given that there are approximately 1700 record types represented in the WikiBio data, and we limit the number of states to 135. Unsurprisingly, adding autoregressiveness to the model decreases purity on both datasets, since the model may rely on the autoregressive RNN for typing, in addition to the state’s identity."
  }, {
    "heading": "8 Conclusion and Future Work",
    "text": "We have developed a neural, template-like generation model based on an HSMM decoder, which can be learned tractably by backpropagating through a dynamic program. The method allows us to extract template-like latent objects in a principled way in the form of state sequences, and then generate with them. This approach scales to large-scale text datasets and is nearly competitive with encoder-decoder models. More importantly, this approach allows for controlling the diversity of generation and for producing interpretable states during generation. We view this work both as the first step towards learning discrete latent variable template models for more difficult generation tasks, as well as a different perspective on learning latent variable text models in general. Future work will examine encouraging the model to learn maximally different (or minimal) templates, which our objective does not explicitly encourage, templates of larger textual phenomena, such as paragraphs and documents, and hierarchical templates."
  }, {
    "heading": "Acknowledgments",
    "text": "SW gratefully acknowledges the support of a Siebel Scholars award. AMR gratefully acknowledges the support of NSF CCF-1704834, Intel Research, and Amazon AWS Research grants."
  }, {
    "heading": "A Supplemental Material",
    "text": "A.1 Additional Model and Training Details Computing rj A record rj is represented by embedding a feature for its type, its position, and its word value in Rd, and applying an MLP with ReLU nonlinearity (Nair and Hinton, 2010) to form rj ∈Rd, similar to Yang et al. (2016) and Wiseman et al. (2017).\nLSTM Details The initial cell and hiddenstate values for the decoder LSTM are given by Q1xa and tanh(Q2xa), respectively, where Q1,Q2 ∈Rd×d.\nWhen a word yt appears in a record rj , the input to the LSTM at time t + 1 is computed using an MLP with ReLU nonlinearity over the concatenation of the embeddings for rj’s record type, word value, position, and a feature for whether it is the final position for the type. If there are multiple rj from which yt could have been copied, the computed representations are averaged. At test time, we use the MAP rj to compute the input, even if there are multiple matches. For yt which could not have been copied, the input to the LSTM at time t+1 is computed using the same MLP over yt and three dummy features.\nFor the autoregressive HSMM, an additional 1- layer LSTM with d hidden units is used. We experimented with having the autoregressive HSMM consume either tokens y1:t in predicting yt+1, or the average embedding of the field types corresponding to copied tokens in y1:t. The former worked slightly better for the WikiBio dataset (where field types are more ambiguous), while the latter worked slightly better for the E2E dataset.\nTransition Distribution The function C(xu), which produces hidden state embeddings conditional on the source, is defined as C(xu)=U2(ReLU(U1xu)), where\nU1 ∈Rm3×d and U2 ∈RK×m2×m3 ; D(x) is defined analogously. For all experiments, m1=64, m2=32, and m3=64.\nOptimization We train with SGD, using a learning rate of 0.5 and decaying by 0.5 each epoch after the first epoch in which validation loglikelihood fails to increase. When using an autoregressive HSMM, the additional LSTM is optimized only after the learning rate has been decayed. We regularize with Dropout (Srivastava et al., 2014).\nA.2 Additional Learned Templates In Tables 6 and 7 we show visualizations of additional templates learned on the E2E and WikiBio data, respectively, by both the non-autoregressive and autoregressive HSMM models presented in the paper. For each model, we select a set of five dissimilar templates in an iterative way by greedily selecting the next template (out of the 200 most frequent) that has the highest percentage of states that do not appear in the previously selected templates; ties are broken randomly. Individual states within a template are visualized using the three most common segments they generate.\n1. | The Waterman\nThe Golden Palace Browns Cambridge\n...\n| is a\nis an is a family friendly\n...\n| Italian French\nfast food ...\n| restaurant\npub place ...\n| with a with\nwith an ...\n| average\nhigh low ...\n| customer rating\nprice range rating ... |.\n2. | There is a\nThere is a cheap There is an\n...\n| restaurant\ncoffee shop French restaurant\n...\n| The Mill\nBibimbap House The Twenty Two\n...\n| located in the located on the\nlocated north of the ...\n| centre of the city\nriver city centre\n...\n| that serves\nserving that provides\n...\n| fast food\nsushi take-away deliveries\n...\n|.\n3. | The Olive Grove\nThe Punter The Cambridge Blue\n...\n| restaurantpub ... | serves offers has ... |\nfast food sushi\ntake-away deliveries ...\n|.\n4. | The\nChild friendly The average priced\n...\n| restaurant\ncoffee shop French restaurant\n...\n| The Mill\nBibimbap House The Twenty Two\n...\n| serves offers has ... | English Indian Italian ... | food cuisine dishes ... |.\n5. | The Strada\nThe Dumpling Tree Alimentum\n...\n| provides serves offers ... | Indian Chinese English ... | food in the food at a food and has a ... | customer rating of price range of rating of\n...\n| 1 out of 5 average\n5 out of 5 ...\n|.\n1. | The Eagle\nThe Golden Curry Zizzi ...\n| provides providing\nserves ...\n| Indian\nChinese English\n...\n| food\ncuisine Food ...\n| in the with a\nand has a ...\n| high\nmoderate average\n...\n| price range\ncustomer rating rating ...\n|. | It is\nThey are It’s ...\n| near\nlocated in the located near\n...\n| riverside\ncity centre Cafe Sicilia\n...\n|. | Its customer rating is\nIt has a The price range is\n...\n| 1 out of 5 average\nhigh ...\n|.\n2. | Located near\nLocated in the Near ...\n| The Portland Arms\nriverside city centre\n...\n| is an\nis a family friendly there is a\n...\n| Italian\nfast food French ...\n| restaurant called\nplace called restaurant named\n...\n| The Waterman\nCocum Loch Fyne\n...\n|.\n3. | A An\nA family friendly ...\n| Italian\nfast food French ...\n| restaurant\npub coffee shop\n...\n| is\ncalled named ...\n| The Waterman\nCocum Loch Fyne\n...\n|.\n4. | Located near\nLocated in the Near ...\n| The Portland Arms\nriverside city centre\n...\n| , | The Eagle\nThe Golden Curry Zizzi ...\n| is a\nis a family friendly is an ...\n| cheap\nfamily-friendly family friendly\n...\n| Italian\nfast food French ...\n| restaurant\npub coffee shop\n...\n|.\n5. | A An\nA family friendly ...\n| Italian\nfast food French ...\n| restaurant\npub coffee shop\n...\n| near\nlocated in the located near ... |\nriverside city centre Cafe Sicilia ... | is called named ... | The Waterman Cocum Loch Fyne ... |.\nTable 6: Five templates extracted from the E2E data with the NTemp model (top) and the Ntemp+AR model (bottom).\n1. | william henry\ngeorge augustus frederick marie anne de bourbon\n...\n| (\nwas ( ; ...\n| born\nborn on born 1 ... | 1968 1960 1970 ... | ) ]) ] ... | is an american is a russian was an american ... | politician actor football player ... |.\n2. | sir\ncaptain lieutenant\n...\n| john herbert\nhartley donald charles cameron\n...\n| was a\nwas a british was an english ... |\nworld war i world war\nfirst world war ...\n| national team organization super league\n...\n|.\n3. | john herbert\nhartley donald charles cameron\n...\n| is a\nwas a is an ...\n| indie rock\ndeath metal ska ...\n| band\nmidfielder defenceman\n...\n| from for\nbased in ...\n| australia\nlos angeles, california chicago\n...\n|.\n4. | john herbert\nhartley donald charles cameron\n...\n| was a is a\nis a former ...\n| american\nmajor league baseball australian\n...\n| football\nprofessional baseball professional ice hockey\n...\n| midfielder defender\ngoalkeeper ...\n|.\n5. | james\nwilliam john william\n...\n| “ billy ” wilson\nsmith “ jack ” henry\n...\n| ( | 1900\nc. 1894 1913 ...\n| – | france\nbudapest buenos aires\n...\n| ) | is an american is an english\nwas an american ...\n| footballer\nprofessional footballer rules footballer\n...\n| who plays for\nwho currently plays for who played with\n...\n| paganese\nsouth melbourne fc dynamo kyiv\n...\n| in the of the\nand the ...\n| vicotiral football league national football league\naustralian football league ... | ( | vfl nfl afl ... | ) |.\n1. | aftab ahmed\nanderson da silva david jones\n...\n| (; ... | born born on born 1 ... | 1951 1970 1974 ... | )] ... | is an american was an american is an english ... | actor actress cricketer ... |.\n2. | aftab ahmed\nanderson da silva david jones\n...\n| was a\nis a former is a ...\n| world war i\nliberal baseball\n...\n| member of the\nparty member of the recipient of the\n...\n| austrian\npennsylvania montana\n...\n| house of representatives\nlegislature senate ...\n|.\n3. | adjutant\nlieutenant captain\n...\n| aftab ahmed\nanderson da silva david jones\n...\n| was a\nis a former is a ...\n| world war i\nliberal baseball\n...\n| member of the\nparty member of the recipient of the\n...\n| knesset\nscottish parliament fc lokomotiv liski\n...\n|.\n4. | william\njohn william james “\n...\n| “ billy ” watson\nsmith jim ” edward\n...\n| ( | 1913\nc. 1900 1913 ... | – in - ... | 1917 surrey, england british columbia ... | ) | was an american was an australian is an american ... | football player rules footballer defenceman ...\n| who plays for\nwho currently plays for who played with\n...\n| collingwood\nst kilda carlton ...\n| in the of the\nand the ...\n| victorial football league national football league\naustralian football league ... | ( | vfl afl nfl ... | ) |.\n5. | aftab ahmed\nanderson da silva david jones\n...\n| is a\nis a former is a female\n...\n| member of the\nparty member of the recipient of the\n...\n| knesset\nscottish parliament fc lokomotiv liski\n...\n|.\nTable 7: Five templates extracted from the WikiBio data with the NTemp model (top) and the Ntemp+AR model (bottom)."
  }],
  "year": 2018,
  "references": [{
    "title": "A simple domain-independent probabilistic approach to generation",
    "authors": ["Gabor Angeli", "Percy Liang", "Dan Klein."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512. Association for Com-",
    "year": 2010
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
    "authors": ["Satanjeev Banerjee", "Alon Lavie."],
    "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
    "year": 2005
  }, {
    "title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models",
    "authors": ["Anja Belz."],
    "venue": "Natural Language Engineering, 14(04):431–455.",
    "year": 2008
  }, {
    "title": "Comparing automatic and human evaluation of nlg systems",
    "authors": ["Anja Belz", "Ehud Reiter."],
    "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics.",
    "year": 2006
  }, {
    "title": "Learning to generate one-sentence biographies from wikidata",
    "authors": ["Andrew Chisholm", "Will Radford", "Ben Hachey."],
    "venue": "CoRR, abs/1702.06235.",
    "year": 2017
  }, {
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.",
    "year": 2014
  }, {
    "title": "Recurrent hidden semi-markov model",
    "authors": ["Hanjun Dai", "Bo Dai", "Yan-Ming Zhang", "Shuang Li", "Le Song."],
    "venue": "International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings",
    "authors": ["Ondrej Dušek", "Filip Jurcıcek."],
    "venue": "The 54th Annual Meeting of the Association for Computational Linguistics, page 45.",
    "year": 2016
  }, {
    "title": "The theory of segmental hidden Markov models",
    "authors": ["Mark JF Gales", "Steve J Young."],
    "venue": "University of Cambridge, Department of Engineering.",
    "year": 1993
  }, {
    "title": "Simplenlg: A realisation engine for practical applications",
    "authors": ["Albert Gatt", "Ehud Reiter."],
    "venue": "Proceedings of the 12th European Workshop on Natural Language Generation, pages 90–93. Association for Computational Linguistics.",
    "year": 2009
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Pointing the unknown words",
    "authors": ["Çaglar Gülçehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Comput., 9:1735–1780.",
    "year": 1997
  }, {
    "title": "Domain adaptable semantic clustering in statistical nlg",
    "authors": ["Blake Howald", "Ravikumar Kondadadi", "Frank Schilder."],
    "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)–Long Papers, pages 143–154.",
    "year": 2013
  }, {
    "title": "Toward controlled generation of text",
    "authors": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing."],
    "venue": "International Conference on Machine Learning, pages 1587–1596.",
    "year": 2017
  }, {
    "title": "Towards neural phrasebased machine translation",
    "authors": ["Po-Sen Huang", "Chong Wang", "Sitao Huang", "Dengyong Zhou", "Li Deng."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Speech and language processing",
    "authors": ["Dan Jurafsky", "James H Martin."],
    "venue": "Pearson London.",
    "year": 2014
  }, {
    "title": "A statistical nlg framework for aggregated planning and realization",
    "authors": ["Ravi Kondadadi", "Blake Howald", "Frank Schilder."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol-",
    "year": 2013
  }, {
    "title": "Segmental recurrent neural networks",
    "authors": ["Lingpeng Kong", "Chris Dyer", "Noah A Smith."],
    "venue": "International Conference on Learning Representations.",
    "year": 2016
  }, {
    "title": "A global model for concept-to-text generation",
    "authors": ["Ioannis Konstas", "Mirella Lapata."],
    "venue": "J. Artif. Intell. Res.(JAIR), 48:305–346.",
    "year": 2013
  }, {
    "title": "Design of a knowledge-based report generator",
    "authors": ["Karen Kukich."],
    "venue": "ACL, pages 145–150.",
    "year": 1983
  }, {
    "title": "Neural text generation from structured data with application to the biography domain",
    "authors": ["Rémi Lebret", "David Grangier", "Michael Auli."],
    "venue": "EMNLP, pages 1203–1213.",
    "year": 2016
  }, {
    "title": "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
    "authors": ["J. Li", "R. Jia", "H. He", "P. Liang."],
    "venue": "North American Association for Computational Linguistics (NAACL).",
    "year": 2018
  }, {
    "title": "Learning semantic correspondences with less supervision",
    "authors": ["Percy Liang", "Michael I Jordan", "Dan Klein."],
    "venue": "ACL, pages 91–99. Association for Computational Linguistics.",
    "year": 2009
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out.",
    "year": 2004
  }, {
    "title": "Table-to-text generation by structure-aware seq2seq learning",
    "authors": ["Tianyu Liu", "Kexiang Wang", "Lei Sha", "Baobao Chang", "Zhifang Sui."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Segmental recurrent neural networks for end-to-end speech recognition",
    "authors": ["Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A Smith", "Steve Renals."],
    "venue": "Interspeech 2016, pages 385–389.",
    "year": 2016
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 1412–",
    "year": 2015
  }, {
    "title": "Text generation - using discourse strategies and focus constraints to generate natural language text",
    "authors": ["Kathleen McKeown."],
    "venue": "Studies in natural language processing. Cambridge University Press.",
    "year": 1992
  }, {
    "title": "Yag: A template-based generator for realtime systems",
    "authors": ["Susan W McRoy", "Songsak Channarukul", "Syed S Ali."],
    "venue": "Proceedings of the first international conference on Natural language generationVolume 14, pages 264–267. Association for Compu-",
    "year": 2000
  }, {
    "title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."],
    "venue": "NAACL HLT, pages 720–730.",
    "year": 2016
  }, {
    "title": "Hidden semi-markov models (hsmms)",
    "authors": ["Kevin P Murphy."],
    "venue": "unpublished notes, 2.",
    "year": 2002
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["Vinod Nair", "Geoffrey E Hinton."],
    "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814.",
    "year": 2010
  }, {
    "title": "The E2E dataset: New challenges for end-toend generation",
    "authors": ["Jekaterina Novikova", "Ondrej Dušek", "Verena Rieser."],
    "venue": "Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saarbrücken, Germany.",
    "year": 2017
  }, {
    "title": "From hmm’s to segment models: A unified view of stochastic modeling for speech recognition",
    "authors": ["Mari Ostendorf", "Vassilios V Digalakis", "Owen A Kimball."],
    "venue": "IEEE Transactions on speech and audio processing, 4(5):360–378.",
    "year": 1996
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Automatic differentiation in pytorch",
    "authors": ["Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer."],
    "venue": "NIPS 2017 Autodiff Workshop.",
    "year": 2017
  }, {
    "title": "A tutorial on hidden markov models and selected applications in speech recognition",
    "authors": ["Lawrence R Rabiner."],
    "venue": "Proceedings of the IEEE, 77(2):257– 286.",
    "year": 1989
  }, {
    "title": "Building applied natural language generation systems",
    "authors": ["Ehud Reiter", "Robert Dale."],
    "venue": "Natural Language Engineering, 3(1):57–87.",
    "year": 1997
  }, {
    "title": "Style transfer from non-parallel text by cross-alignment",
    "authors": ["Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Advances in Neural Information Processing Systems, pages 6833–6844.",
    "year": 2017
  }, {
    "title": "Neural language modeling by jointly learning syntax and lexicon",
    "authors": ["Yikang Shen", "Zhouhan Lin", "Chin wei Huang", "Aaron Courville."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3104–3112.",
    "year": 2014
  }, {
    "title": "End-to-end training approaches for discriminative segmental models",
    "authors": ["Hao Tang", "Weiran Wang", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE, pages 496–502. IEEE.",
    "year": 2016
  }, {
    "title": "Unsupervised neural hidden markov models",
    "authors": ["Ke M Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight."],
    "venue": "Proceedings of the Workshop on Structured Prediction for NLP, pages 63–71.",
    "year": 2016
  }, {
    "title": "Cider: Consensus-based image description evaluation",
    "authors": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.",
    "year": 2015
  }, {
    "title": "Sequence modeling via segmentations",
    "authors": ["Chong Wang", "Yining Wang", "Po-Sen Huang", "Abdelrahman Mohamed", "Dengyong Zhou", "Li Deng."],
    "venue": "International Conference on Machine Learning, pages 3674–3683.",
    "year": 2017
  }, {
    "title": "Domainindependent abstract generation for focused meeting summarization",
    "authors": ["Lu Wang", "Claire Cardie."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages",
    "year": 2013
  }, {
    "title": "Challenges in data-to-document generation",
    "authors": ["Sam Wiseman", "Stuart Shieber", "Alexander Rush."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263.",
    "year": 2017
  }, {
    "title": "Breaking the softmax bottleneck: A high-rank RNN language model",
    "authors": ["Zhilin Yang", "Zihang Dai", "Ruslan Salakhutdinov", "William W. Cohen."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Reference-aware language models",
    "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling."],
    "venue": "CoRR, abs/1611.01628.",
    "year": 2016
  }, {
    "title": "Online segment to segment neural transduction",
    "authors": ["Lei Yu", "Jan Buys", "Phil Blunsom."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1307–1316.",
    "year": 2016
  }, {
    "title": "Adversarially regularized autoencoders",
    "authors": ["Junbo Jake Zhao", "Yoon Kim", "Kelly Zhang", "Alexander M. Rush", "Yann LeCun."],
    "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 5897–5906.",
    "year": 2018
  }],
  "id": "SP:fccf3e3aee4802aa6a0813d66afbe1d19573edba",
  "authors": [{
    "name": "Sam Wiseman",
    "affiliations": []
  }, {
    "name": "Stuart M. Shieber",
    "affiliations": []
  }, {
    "name": "Alexander M. Rush",
    "affiliations": []
  }],
  "abstractText": "While neural, encoder-decoder models have had significant empirical success in text generation, there remain several unaddressed problems with this style of generation. Encoderdecoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content. This work proposes a neural generation system using a hidden semimarkov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate. We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable. Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoderdecoder text generation models.",
  "title": "Learning Neural Templates for Text Generation"
}