{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Clustering is one of the most widely used techniques in data analysis (Xu & Wunsch, 2005; Jain, 2010). Despite a rich literature on pure continuous data or pure categorical data, the clustering problem remains challenging for mixed-type data, i.e., data with both types of attributes (Everitt et al., 2001). Mixed-type data are ubiquitous in real world domains, e.g., social science, biomedicine and finance, where categorical attributes often describe demographic information or questionnaire responses, and continuous attributes often correspond to quantitative measurements. However, only a very limited number of clustering methods have been proposed for such data (Everitt et al., 2001; Huang, 1998). The major challenge is the lack of a good geometric intuition of data on the mixed-type domain;\n1City University of New York (CUNY), New York, USA 2University of Sussex, Falmer, United Kingdom 3National Research University Higher School of Economics, Moscow, Russia 4Ohio State University, Columbus, USA. Correspondence to: Chao Chen <chao.chen.cchen@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nsuch intuition is the important basis of many successful geometric clustering methods, e.g., k-means (MacQueen et al., 1967), Ward’s method (Ward Jr, 1963), DBSCAN (Ester et al., 1996), to name a few. In practice, what usually being done is to convert mixed-type data to either pure continuous or pure categorical domain, and subsequently use existing geometric clustering methods. A metric for directly dealing with mixed-type data is also available, based on Gower’s coefficient (1971). The uptake of geometric clustering methods is mostly driven by their lightweight computational requirements. However, these methods lack a well justified underlying probabilistic model, are sensitive to the choice of underlying metric, and do not give a principled answer to the fundamental question of required number of clusters for the data at hand.\nIn this paper, we propose a probabilistic clustering method for mixed-type data, which admits at least four attractive properties. First, our probabilistic method goes beyond the widely-adopted class conditional independence assumption of feature variables, e.g., as in the latent class model (McCutcheon, 1987). Second, our method is based on the global topographical features, i.e., peaks and mountains, of the density function, rather than the distances between data points. The argument for topographical features is to sidestep a premature specification of the metric space in which our mixed-type data will achieve the best grouping. Third, our method is able to utilize a persistent homology theory to automatically determine the number of clusters in the data. Fourth, the proposed method can be easily parallelized to achieve a competitive running time with respect to many lightweight geometric clustering methods.\nFrom the modeling perspective, we compose tree graphical models with topographical features to achieve a probabilistic mixed-type clustering model. Graphical models provide a way of factorizing a joint probability distribution into a product of local interactions. These local interactions capture dependency among feature variables. While a Bayesian network or a Markov random field can be built with a set of nodes representing each feature variable. The graph structure and parameter estimation can be computationally expensive. By constraining the graph to be a tree, the structure and parameter can be learned efficiently. Other than computational benefits, tree-structured\ngraphical models also provide a modeling elegance; with a tree structure, we have a factorization that explicitly corresponds to empirical univariate and bivariate marginal distributions. For the bivariate distributions, we can then adapt the product kernel density estimation (Scott, 2015) to capture interaction between continuous-continuous variables, between categorical-categorical variables, and between categorical-continuous variables.\nHaving modeled the data generation process via a tree graphical model, we are left with finding a robust approach for assigning each data point to its cluster. To achieve this, we adopt a topological perspective, namely, we view a probability distribution as a terrain function, called the density landscape, and capture its topographical features as the basis for defining clusters. The topographical features include modes (peaks) and their attractive basins. For high-dimension and sparse data, it is natural to have many modes. To avoid over-segmentation of the data and generation of many clusters with only few members, we employ a persistent homology theory (Edelsbrunner & Harer, 2010) to measure the saliency of all modes and merge the trivial ones. Our principled method for clustering mixed-type data respects the underlying topographical features of the density landscape and achieves competitive performance on real data."
  }, {
    "heading": "1.1. Related Work",
    "text": "Clustering has been extensively studied in machine learning and data mining. Many comprehensive surveys have been produced detailing the landscape of clustering problems and models. Here we will review related work in the context of geometric versus probabilistic clustering methods for mixed-type data and clustering methods that rely on topographical features such as modes and their attractive basins.\nGeometric clustering methods A straightforward approach for mixed-type data clustering is to map them into either pure continuous or pure categorical domains before applying a standard clustering method. A metric based on Gower’s coefficient (Gower, 1971) has been proposed for mixed-type data, which rescales the difference in all dimensions, continuous or categorical, and take the average. One can apply any distance based method using these metrics. However, all these methods are heuristic; there is no good justification for the underlying geometric intuition of these methods on such a counter-intuitive metric space, despite some successful stories in practice. For example, K-Prototypes algorithm (Huang, 1998) uses a weighted sum of the Euclidean distance and Hamming distance and adopts the K-Means method (Faber, 1994), which iteratively finds the mean of each cluster and re-associates data to different clusters. When the data is pure categorical, the method is called K-Modes (Huang, 1997). Chiu et al. (2001) proposed a hierarchical clustering method, in\nwhich distance between clusters are measured using their log-likelihood, which treat continuous and categorical domain separately.\nProbabilistic clustering methods Graphical models have been applied to clustering before. Zhang (2004) proposed a latent tree model, i.e., a Bayesian tree whose leaf nodes correspond to all observed dimensions and internal nodes are latent variables determining different clusters. Such tree structure can be learned using efficient algorithms (Chen et al., 2012; Liu et al., 2015). However, this method is only restricted to categorical data. Lee & Hastie (2015) proposed a loopy graphical model to model mixed-type data. Their model reduces to a discrete Markov random field when all attributes are categorical, and a Gaussian graphical model when all attributes are continuous. Parameters are learned using pseudo-likelihood estimation (Besag, 1975) and edges are selected using group sparsity penalties (Yuan & Lin, 2006; Huang & Zhang, 2010). However, an efficient inference model is missing in order to apply such model to clustering.\nClustering by mode-seeking The density landscape has been exploited before to extract global properties of the data and to achieve better clustering quality. Mode-seeking methods, i.e., associating data to modes representing clusters, have been proposed before in continuous domain (Cheng, 1995; Comaniciu & Meer, 2002b). But such methods rely on a kernel density estimation, which suffers from the curse of dimensionality and thus do not scale to high dimensions (Wasserman, 2013, chap. 20). Chen & Quadrianto (2016) proposed a mode-seeking method for categorical data clustering. However, their method tends to produce trivial modes/clusters and thus over-segments the data, mainly due to the lack a principled way to merge modes into clusters of proper size.\nPersistent homology for merging clusters In recent years, novel approaches have been proposed to merge modes/clusters based on the topographical landscape of the density function. Chazal et al. (2013) used topological persistence to guide the merging of data into clusters. Their method, although theoretically sound, relies on a k-nearest neighbor graph of the data and a given density function, e.g., a kernel density estimation (Silverman, 1986) or a distance from measure (Chazal et al., 2011). This method assumes that the data is a high quality sample of the domain and the k-nearest neighbor graph faithfully captures the topographical characteristics of the distribution. However, this condition is often too strong to assume in practice, where most datasets are relatively sparse. In this paper, we propose to start with mode-seeking, and leverage these modes and the gradient paths as a more accurate account of the density landscape. Our idea proves to be a better solution and a good complement to the theoretical tool. We also refer to other topological and geometrical studies into the\nglobal structures of hierarchical clustering (Eldridge et al., 2015; Carlsson & Mémoli, 2010)."
  }, {
    "heading": "2. Background",
    "text": "A probabilistic graphical model (Koller & Friedman, 2009) consists of a set of inter-dependent random variables X = (X1, . . . , XD), a potential function f , and a graph G = (V, E). Each element in the node set V represents one random variable from X . The edges represents the dependence relations between pairs of variables. There are two different kinds of variables in our setting: continuous ones and discrete ones variables. For simplification, we assume each discrete variable takes discrete values Xi ∈ L = {1, . . . , L}. In this paper, we use discrete and categorical interchangeably and focus on non-ordinal discrete variables, although ordinal discrete variables are of interest in practice as well. In our setting, only Hamming distance can be used for discrete variables.\nA value assignment to all random variables x = (x1, . . . , xD) is called a configuration. A potential function f : x → R assigns to each configuration a real value, which is inversely proportional to the logarithm of the probability distribution, p(x) = exp(−f(x) − A), where A is the log-partition function. In this paper, we focus on tree structured graphical models, represented by T = (V, E). For a tree model, the probability and potential of a configuration can be factorized into a product (Bach & Jordan, 2003):\np(x) = ∏\n(i,j)∈E\np(xi, xj)\np(xi)p(xj) ∏ k∈V p(xk), (2.1)\nwhere p(xi, xj) is the bivariate marginal density of the variable Xi and Xj , and p(xk) is the univariate marginal density of the variable Xk.\nWhen the true distribution can be represented by a tree, we can use the algorithm by Chow & Liu (1968) to reconstruct the tree model. First, we compute the mutual information between all pairs of variables:\nMIij = ∫ xi,xj p(xi, xj) log p(xi, xj) p(xi)p(xj) dxidxj ,\nusing empirical univariate and bivariate marginals. The integral is replaced by sum when Xi and Xj have discrete values. Next, we compute the maximum spanning tree of a complete graph with D nodes, using the mutual information as edge weights. The computed tree is the desired tree model with the optimal KL-divergence from the true tree distribution (Liu et al., 2011). More details of the selection of the models for univariate and bivariate densities will be given in Section 3."
  }, {
    "heading": "3. Method",
    "text": "Our method first estimates the underlying probabilistic density function from given data. We choose tree-models as they strike a elegant balance between computational efficiency and flexibility of the model. Next, we propose to cluster data based on the density landscape: associating data with modes/peaks of the density, and merge them based on advanced persistent homology theory. First, we formalize the definition of modes in the mixed-type domain. Then we present algorithms for modes-seeking (Section 3.2) and for modes-merging (Section 3.3).\nWe first formalize what a mode is in a D-dimensional mixed-type data domain. Our definition is not restricted to the underlying model. Denote by Id and Ic the index sets of discrete- and continuous-valued random variables. Denote by distH(x, x′) the Hamming distance between x and x′ within the discrete dimensions, and distL2(x, x′) the L2 distance within the continuous dimensions. We call a discrete neighborhood of x with radius δ > 0 as all elements with no more than δ Hamming distance and zero Euclidean distance from x, formally,\nN dδ (x) = {x′ | distd(x, x′) ≤ δ ∧ distc(x, x′) = 0}.\nSimilarly, we define a continuous neighborhood of x with radius > 0 as\nN c (x) = {x′ | distd(x, x′) = 0 ∧ distc(x, x′) ≤ }.\nGiven a probability density function, p(X), a mode is a local maximum in both the continuous neighborhood and discrete neighborhood, formally: Definition 1 (Modes). A point x ∈ X is a mode if and only if there exists positive numbers > 0 and δ > 0 such that (1) p(x) ≥ p(x′) for any x′ ∈ N c (x); and (2) p(x) ≥ p(x′) for any x′ ∈ N dδ (x). It suffices to use the smallest positive integer for the discrete neighborhood, δ = 1. In this paper, we focus on a tree-structured graphical model. Next, we describe our tree model in details within the mixed-type setting."
  }, {
    "heading": "3.1. Instantiating the Tree Model",
    "text": "We formalize the univariate and bivariate marginal densities p(xi) and p(xi, xj) in the tree model (Eq. (2.1)). We assume a set of N data {y1, y2, · · · , yN} is given. For discrete dimensions, we use Multinoulli distribution with Dirichlet prior α = 1, ∀i, j ∈ Id:\np(xi) = Nxi + 1\nN + L , with Nxi = N∑ n=1 Jyni = xiK,\np(xi, xj) = Nxi,xj + 1\nN + L2 ,\nwith Nxi,xj = N∑ n=1 Jyni = xi ∧ ynj = xjK.\nFor continuous variables, we use one-dimensional kernel density estimation for univariate density, and product kernel (Scott, 2015) for univariate and bivariate marginal density. Formally, ∀i, j ∈ Ic,\np(xi) = 1\nN N∑ n=1 Kh1i(y n i − xi), and\np(xi, xj) = 1\nN N∑ n=1 { Kh2i(y n i − xi)Kh2j (ynj − xj) } ,\n(3.1)\nWe use a one-dimensional Gaussian kernel, denoted as Kh(z) =\n1√ 2πh\nexp ( − z 2\n2h2\n) . Following standard non-\nparametric statistics literature (Fan & Gijbels, 1996; Tsybakov, 2009), the kernel bandwidths for univariate and bivariate density are chosen as\nhti = 1.06·min { σ∗i , q∗i,0.75 − q∗i,0.25 1.34 } ·N− 1 2β+t , t = 1, 2,\nwhere σ∗i , q ∗ i,0.75 and q ∗ i,0.25 are the standard deviation, the 75% and 25% sample quantiles of Xi, respectively. The variable β is the order of the kernel (Fan & Gijbels, 1996) and is set to 2 by default.\nThe choice of a product kernel is justified by two reasons. First, a product kernel reduces to the product of onedimensional kernels, which are more reliable that a direct 2D kernel density estimation. Second, the product kernel proves to be convenient to be adopt to bivariate densities for variables with mixed-type as follows. For a mixed-type pair of variables, (Xi, Xj), i ∈ Ic, j ∈ Id, we take the limit of h2i to zero in the product kernel formula (Equation (3.1)). The first kernel becomes the Dirac-delta function, leading to the following bivariate marginal\np(xi, xj) = 1\nN N∑ n=1 { Jynj = xjKKh2i(yni − xi) } .\nBuilding the tree model. Using these empirical univariate and bivariate marginal densities, we estimate all pairwise mutual information, and then compute the tree (V, E) using the Chow-Liu algorithm. Plugging the univariate and bivariate marginal densities into Eq. (2.1), we have the complete density distribution (the tree model). Next, we present our algorithm for finding the modes over the density landscape of the computed model."
  }, {
    "heading": "3.2. Mode-Seeking Algorithm",
    "text": "Our algorithm assigns each data to a mode via a gradient ascent procedure. For a mixed-domain, a gradient is not well defined. Following the definition of modes (Def. 1), we formulate a gradient step as an optimization within either the continuous neighborhood N c (x) or the discrete\nneighborhood N dδ (x), with δ = 1. The two procedures have to be taken alternatively in order to continue increasing the probability until a mode is reached.\nOur algorithm starts at each data, s, iteratively walks to a nearby point with bigger probability until convergence. The final position is the mode of interest and will be associated with the data, s. For ease of computation, we use the potential function f(x) instead of the probability density function:\nf(x) = − ∑\n(i,j)∈E log p(xi, xj)− ∑ i∈V (1− di) log p(xi),\n(3.2) in which di is the degree of node i in the tree. It is easy to verify that p(x) ∝ −f(x). Therefore, modes of p(x) are the local minima of f(x), following the same definition in Def. 1. We follow the aforementioned iterative procedure, except at each step, we find a nearby point with smaller potential.\nAt each step of the algorithm, we first update all discrete variables until no better elements exist within the discrete neighborhood N dδ (x) with δ = 1. Next, we update all continuous variables using gradient descent, until the gradient of f at continuous dimensions∇cf becomes zero. Our main algorithm is summarized in Alg. 1.\nAlgorithm 1 Mode-Seeking Algorithm 1: Input: Data D = {si | i = 1, · · · , N}; a potential\nfunction f . 2: Output: A set of modes,M; mode indices associated\nto each data {ci | i = 1, · · · , N} 3: M← ∅ 4: for i = 1 to N do 5: x← si 6: repeat 7: repeat 8: x← argminz∈Nd1 (x) f(z) 9: until x converges\n10: repeat 11: x← x− η∇cf 12: until x converges 13: until x converges 14: if x /∈M then 15: M←M∪ {x} 16: end if 17: ci ← the index of x inM 18: end for\nHere η is the stepsize. The best neighbor within Hamming distance one, argminz∈Nd1 (x) f(z), can be computed using dynamic programming. This can be achieved by directly adapting the algorithm by (Chen & Quadrianto, 2016).\nIt remains to compute the gradient of f in the contin-\nuous domain, ∇cf . For each continuous variable i ∈ Ic, relevant terms in the energy function (Eq. (3.2)) can be divided into three groups, the univariate term, the bivariate terms with a continuous neighbor, j ∈ Ic, and the bivariate terms with a discrete neighbor, j ∈ Id. Treating them differently, the partial derivative:\n∂f(x)\n∂xi = −(1− di)\n∑N n=1 Kh1i(y n i − xi)\nyni −xi h21i∑N\nn=1 Kh1i(y n i − xi)\n− ∑\nj∈Ic:(i,j)∈E\n∑N n=1 Kh2i(y n i − xi)Kh2j (ynj − xj)\nyni −xi h22i∑N\nn=1 Kh2i(y n i − xi)Kh2j (ynj − xj)\n− ∑\nk∈Id:(i,j)∈E\n∑N n=1 Kh2i(y n i − xi)Jynk = xkK\nyni −xi h22i∑N\nn=1 Kh2i(y n i − xi)\n(3.3)\nAlgorithm 2 Merging Data Using Topological Persistence\n1: Input: Ĝ = (V̂, Ê), density function p : V̂ → R+, persistence threshold τ 2: Output: Clusters C 3: C ← ∅ 4: Sort elements in V̂ according to the density function\nvalues, so that p(vi) ≥ p(vi+1), ∀vi, vi+1 ∈ V̂ . 5: for i = 1 to |V̂| do 6: nbd← {vj | (vi, vj) ∈ Ê ∧ j < i} 7: // neighbors of vi with smaller indices (bigger p) 8: if nbd = ∅ then 9: create a new cluster c = {vi}\n10: birth(c)← p(vi) 11: C ← C ∪ {c} 12: else 13: Cnbd ← all clusters containing nodes in nbd 14: cmax ← argmaxc∈Cnbd birth(c) 15: for all c ∈ Cnbd and c 6= cmax do 16: persistence(c)← birth(c)− p(vi) 17: if persistence(c) < τ then 18: // merge c into cmax 19: cmax ← cmax ∪ c 20: C ← C\\{c} 21: end if 22: end for 23: // assign vi to cmax 24: cmax ← cmax ∪ {vi} 25: end if 26: end for"
  }, {
    "heading": "3.3. Merging Clusters Using Topological Persistence",
    "text": "The modes computed in Alg. 1 provide a clustering of the data. However, in practice, the data is often relatively sparse. In such cases, the method tends to produce a large\nnumber of modes, and thus over-segments the data into small clusters. There are ways to merge these small clusters (Ward Jr, 1963; Day & Edelsbrunner, 1984). But they rely on a distance metric to measure similarities between clusters. Instead, we propose a principled approach that is only based on the density landscape, i.e., the topographical features such as peaks, ridges, valleys. Our method is built on the theory of persistent homology. We focus on zerodimensional topological structures in this paper, although the theory is much more general.\nPersistence of modes. We estimate the saliency of a peak (mode) using its “relative height”, namely, the difference between its height and the level at which its basin of attraction meets the one of another higher mode. Formally, we filter the domain using a function value threshold t from +∞ to −∞. As t decreases, we monitor the topological changes of the progressively growing superlevel set, X t = {x ∈ X | p(x) ≥ t}, that is, the domain whose probability density value is no smaller than t. Each mode attributes to the birth of a new connected component in the superlevel set and the component is killed when it meets another component created by a higher mode. The density value of the creating mode and the density value of the point at which the two components meet (called a saddle) are called the birth and death times, and their difference, called the persistence, measures the saliency of this mode. See Figure 1 for an illustration.\nThe merging of connected components as we decrease the threshold t provides a natural way to merge modes; when two connected components meet, we merge them if one of them has≤ τ persistence (Figure 1). This gives us a principled way to merge modes. Based on the convergence of tree-model estimation (Liu et al., 2011) and the stability\nof persistent homology (Cohen-Steiner et al., 2007), this method is guaranteed to be robust to noise and L∞ perturbation of the density function.\nSample-based persistence computation. Given a dense uniform sampling of the whole domain X , we can trust these samples will describe the density landscape faithfully. In practice, however, a uniform sampling will have exponential size to the dimension. Chazal et al. (2013) used the k-nearest neighbor graph of the input data, D, assuming they are good samples from the density function. However, in practice, the data is often relatively sparse and cannot represent the landscape well enough to produce a high quality mode-merging hierarchy. In fact, it is very likely that the modes are not included in the data and thus the birth time (as well as the persistence) will be under-estimated. See Figure 2(left) for an illustration.\nIn this paper, we propose to compute persistence based on all points we encountered during the mode-seeking procedure. In Algorithm 1, we collect the point x computed after each iteration (after line 12). The gradient step also provides a natural edge connecting these points. This tree structured graph give us a high-quality description of the attractive basin of each mode. This provides us a well-suited underlying graph describing the density landscape. See Figure 2(right). Finally, to ensure the graph is fully connected, and the space between modes are well described, we add edges (green edges) connecting points from neighboring attractive basins, as well as the lowest point along these edges (green markers). Note that this is the only time when the distance metric plays a role in our model. We use a sum of the Hamming distance and Euclidean distance.\nAlgorithm. Given a graph Ĝ = (V̂, Ê), in which each node is assigned a probability density, we compute the persistence-based merge tree as follows. Sort all nodes in decreasing order of their density function values. Add them into the superlevel set one-by-one. To add a node vi, we check whether it is adjacent to any nodes that have been included. If not, vi, which must be a mode itself, creates a new connected component with the birth time p(vi). If vi is connected to multiple existing connected components, we keep the one with the earliest birth time, cmax, and merge some others into cmax. In particular, for each other adjacent connected component, we check whether its life length so far is less than τ . The ones with ≤ τ life length will be merged into cmax. We add vi into the connected component cmax See Figure 3 for an illustration. See Alg. 2 for the pseudocode."
  }, {
    "heading": "4. Experiments",
    "text": "We compare our methods with existing clustering methods on several real world mixed-type datasets from UCI repository (Lichman, 2013): Contraceptive Method Choice dataset (CMC), Credit Approval dataset (CRX),\nGerman Credit Approval (German), and Statlog Heart Disease dataset (Heart). See the table below for more details. All datasets have 60% to 70% of the features being discrete.\nTable 1. Datasets\nData # of samples Dimension # of clusters CMC 1473 9 3 Heart 297 13 5 CRX 653 15 2\nGerman 1000 20 2 Our method can be straightforwardly parallelized. We run the mode-seeking for all data points (the for-loop in Alg. 1) in parallel. On average, the mode-seeking of a single data takes 6 gradient ascent steps and 5.87 seconds. On a cluster with 48 cores, our program finishes within 3 minutes for any of the datasets. If running in a sequential manner, the time will be linear to the dataset size. After all data are processed, we collect all relevant points and run a persistence-based merging sequentially. This step takes less than 20 seconds for any of the datasets. The persistence-based merging depends on a threshold τ . It is hard to select a universal one due to the large variation among datasets. Instead, we choose the τ for each dataset so that the desired the nubmer of clusters remain after merging. This is a fair comparison; all clustering methods we compare with use an oracle number of clusters. We empirically set the parameter δ to one. Using a bigger δ hurts the performance as it would try to ‘smooth’ the landscape in the categorical domain.\nAll methods can be grouped into five different groups, based on the underlying domain and the approach. The first group assumes a continuous domain and an Euclidean metric. We project the mixed-type data into the continuous domain and directly apply such methods, including k-means (Faber, 1994), Affinity Propagation (Frey & Dueck, 2007), Mean Shift (Cheng, 1995; Comaniciu & Meer, 2002a), Spectral Clustering (Kamvar et al., 2003), Ward’s algorithm (Ward Jr, 1963), Agglomerative clustering (Day & Edelsbrunner, 1984) and DBSCAN (Ester et al., 1996).\nThe second group are methods designed for pure categorical domain, e.g., K-Modes (Huang, 1997), ROCK (Guha et al., 1999), mixture of multinoulli (latent class analysis) (McCutcheon, 1987). We convert mixed-type data into categorical data by thresholding continuous values at the median. We also include Affinity Propagation, Spectral Clustering and DBSCAN in this group; these methods can be applied to any distance metrics. We compute pairwise Hamming distance between data as the input of these three methods.\nFor the third group, we use these three methods, but using a distance matrix based on Gower’s coefficient (Gower, 1971), which was designed specifically for mixed-domain. The fourth group uses a simply sum of the Euclidean distance (restricted to continuous dimensions) and Hamming distance (restricted to categorical dimensions). A good rep-\nresentative in such group is K-Prototypes (Huang, 1998). We again applied the three methods (Affinity, Spectral and DBSCAN) on this new metric.\nIn the last group, we compare our method and a few other topological methods. We compare to the method using only modes for clustering. This is essentially an adaptation of (Chen & Quadrianto, 2016) to the mixed-type domain. We also compare to (Chazal et al., 2013) by computing the persistence on the k-nearest neighbor graph, using our tree-model as the underlying density estimation. Finally, we also show the result of our method.\nThe results are listed in Table 2. We use the Adjusted Mutual Information (AMI) (Vinh et al., 2010) and Adjusted Rand Score (ARS) (Hubert & Arabie, 1985) to evaluate all\nmethods. For all methods requiring random initializations, we run each one for 10 times and take the average performance. When necessary, we provide a true number of clusters as an oracle. The cells with N/A correspond to the cases when the program crashes. It is most likely because the Gower’s coefficient and Hamming distance does not give us a well-conditioned distance matrix for the spectral clustering method.\nDiscussion. Our method outperforms most methods from all other four groups, using different types of metrics. We also observe that a few methods based on pure categorical domain are quite competitive. Similarly, K-prototype, a popular tool for mixed-type data, has good performance on some data. Outperforming other topological methods (modes only and persistence only) demonstrate the significance of our contribution.\nOur current experiments assume the correct number of clusters is given. It is possible to prove that with sufficient samples and the correct threshold τ , the persistence-based clustering can find the correct number of cluster and the right clustering for most data points in a sense similar to the elegant result in (Chazal et al., 2013). A closely related theoretical result is in (Eldridge et al., 2015), which shows that the hierarchical clustering tree constructed by a similar merging procedure is consistent for points sampled from a nice density distribution over RD."
  }, {
    "heading": "5. Conclusions",
    "text": "In this paper, we propose a probabilistic clustering method for mixed-type data. We design a tree-structured graphical model for the mixed-type domain. We also develop methods based on a topographical view of the density landscape. We design algorithms to capture modes of the density landscape and merge trivial modes based on the theory of persistent homology.\nAcknowledgments. XN and CC have been partly funded by the grant PSC-CUNY 69844-00 47. NQ has been partly funded by the Russian Academic Excellence Project ‘5- 100’. YW has been partly supported by the grant NSF DMS-1547357. The authors gratefully acknowledge use of the services and facilities of CUNY Queens Colleges Center for Computational Infrastructure for the Sciences (CCIS)."
  }],
  "year": 2017,
  "references": [{
    "title": "Beyond independent components: trees and clusters",
    "authors": ["Bach", "Francis R", "Jordan", "Michael I"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "Statistical analysis of non-lattice data",
    "authors": ["Besag", "Julian"],
    "venue": "The statistician, pp",
    "year": 1975
  }, {
    "title": "Characterization, stability and convergence of hierarchical clustering methods",
    "authors": ["Carlsson", "Gunnar", "Mémoli", "Facundo"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Geometric inference for measures based on distance functions",
    "authors": ["Chazal", "Frédéric", "Cohen-Steiner", "David", "Mérigot", "Quentin"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2011
  }, {
    "title": "Clustering high dimensional categorical data via topographical features",
    "authors": ["Chen", "Chao", "Quadrianto", "Novi"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Model-based multidimensional clustering of categorical data",
    "authors": ["Chen", "Tao", "Zhang", "Nevin L", "Liu", "Tengfei", "Poon", "Kin Man", "Wang", "Yi"],
    "venue": "Artificial Intelligence,",
    "year": 2012
  }, {
    "title": "Mean shift, mode seeking, and clustering",
    "authors": ["Cheng", "Yizong"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 1995
  }, {
    "title": "Approximating discrete probability distributions with dependence trees",
    "authors": ["C Chow", "C. Liu"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1968
  }, {
    "title": "Stability of persistence diagrams",
    "authors": ["Cohen-Steiner", "David", "Edelsbrunner", "Herbert", "Harer", "John"],
    "venue": "Discrete & Computational Geometry,",
    "year": 2007
  }, {
    "title": "Mean shift: A robust approach toward feature space analysis",
    "authors": ["D. Comaniciu", "P. Meer"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (PAMI),",
    "year": 2002
  }, {
    "title": "Mean shift: A robust approach toward feature space analysis",
    "authors": ["Comaniciu", "Dorin", "Meer", "Peter"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2002
  }, {
    "title": "Efficient algorithms for agglomerative hierarchical clustering methods",
    "authors": ["Day", "William HE", "Edelsbrunner", "Herbert"],
    "venue": "Journal of classification,",
    "year": 1984
  }, {
    "title": "Computational Topology: an Introduction",
    "authors": ["Edelsbrunner", "Herbert", "Harer", "John"],
    "venue": "AMS,",
    "year": 2010
  }, {
    "title": "Beyond hartigan consistency: Merge distortion metric for hierarchical clustering",
    "authors": ["Eldridge", "Justin", "Belkin", "Mikhail", "Wang", "Yusu"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
    "authors": ["Ester", "Martin", "Kriegel", "Hans-Peter", "Sander", "Jörg", "Xu", "Xiaowei"],
    "venue": "In Kdd,",
    "year": 1996
  }, {
    "title": "Cluster Analysis (5th Edition)",
    "authors": ["Everitt", "Brian S", "Landau", "Sabine", "Leese", "Morven", "Stahl", "Daniel"],
    "year": 2001
  }, {
    "title": "Clustering and the continuous k-means algorithm",
    "authors": ["Faber", "Vance"],
    "venue": "Los Alamos Science,",
    "year": 1994
  }, {
    "title": "Local polynomial modelling and its applications: monographs on statistics and applied probability 66, volume 66",
    "authors": ["Fan", "Jianqing", "Gijbels", "Irene"],
    "year": 1996
  }, {
    "title": "Clustering by passing messages between data",
    "authors": ["Frey", "Brendan J", "Dueck", "Delbert"],
    "venue": "points. Science,",
    "year": 2007
  }, {
    "title": "A general coefficient of similarity and some of its properties",
    "authors": ["Gower", "John C"],
    "year": 1971
  }, {
    "title": "ROCK: A robust clustering algorithm for categorical attributes",
    "authors": ["Guha", "Sudipto", "Rastogi", "Rajeev", "Shim", "Kyuseok"],
    "venue": "In International Conference on Data Engineering (ICDE),",
    "year": 1999
  }, {
    "title": "The benefit of group sparsity",
    "authors": ["Huang", "Junzhou", "Zhang", "Tong"],
    "venue": "The Annals of Statistics,",
    "year": 1978
  }, {
    "title": "A fast clustering algorithm to cluster very large categorical data sets in data mining",
    "authors": ["Huang", "Zhexue"],
    "venue": "In DMKD, pp",
    "year": 1997
  }, {
    "title": "Extensions to the k-means algorithm for clustering large data sets with categorical values",
    "authors": ["Huang", "Zhexue"],
    "venue": "Data mining and knowledge discovery,",
    "year": 1998
  }, {
    "title": "Comparing partitions",
    "authors": ["Hubert", "Lawrence", "Arabie", "Phipps"],
    "venue": "Journal of classification,",
    "year": 1985
  }, {
    "title": "Data clustering: 50 years beyond k-means",
    "authors": ["Jain", "Anil K"],
    "venue": "Pattern recognition letters,",
    "year": 2010
  }, {
    "title": "Probabilistic graphical models: principles and techniques",
    "authors": ["Koller", "Daphne", "Friedman", "Nir"],
    "venue": "MIT press,",
    "year": 2009
  }, {
    "title": "Learning the structure of mixed graphical models",
    "authors": ["Lee", "Jason D", "Hastie", "Trevor J"],
    "venue": "Journal of Computational and Graphical Statistics,",
    "year": 2015
  }, {
    "title": "Forest density estimation",
    "authors": ["Liu", "Han", "Xu", "Min", "Gu", "Haijie", "Gupta", "Anupam", "Lafferty", "John", "Wasserman", "Larry"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Greedy learning of latent tree models for multidimensional clustering",
    "authors": ["Liu", "Teng-Fei", "Zhang", "Nevin L", "Chen", "Peixian", "April Hua", "Poon", "Leonard KM", "Wang", "Yi"],
    "venue": "Machine learning,",
    "year": 2015
  }, {
    "title": "Some methods for classification and analysis of multivariate observations",
    "authors": ["MacQueen", "James"],
    "venue": "In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,",
    "year": 1967
  }, {
    "title": "Latent class analysis",
    "authors": ["McCutcheon", "Allan L"],
    "venue": "Number 64. Sage,",
    "year": 1987
  }, {
    "title": "Multivariate density estimation: theory, practice, and visualization",
    "authors": ["Scott", "David W"],
    "year": 2015
  }, {
    "title": "Density estimation for statistics and data analysis, volume 26",
    "authors": ["Silverman", "Bernard W"],
    "venue": "CRC press,",
    "year": 1986
  }, {
    "title": "Introduction to nonparametric estimation. revised and extended from the 2004 french original. translated by vladimir zaiats",
    "authors": ["Tsybakov", "Alexandre B"],
    "year": 2009
  }, {
    "title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance",
    "authors": ["Vinh", "Nguyen Xuan", "Epps", "Julien", "Bailey", "James"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Hierarchical grouping to optimize an objective function",
    "authors": ["Ward Jr.", "Joe H"],
    "venue": "Journal of the American statistical association,",
    "year": 1963
  }, {
    "title": "All of statistics: a concise course in statistical inference",
    "authors": ["Wasserman", "Larry"],
    "venue": "Springer Science & Business Media,",
    "year": 2013
  }, {
    "title": "Survey of clustering algorithms",
    "authors": ["Xu", "Rui", "Wunsch", "Donald"],
    "venue": "IEEE Transactions on neural networks,",
    "year": 2005
  }, {
    "title": "Model selection and estimation in regression with grouped variables",
    "authors": ["Yuan", "Ming", "Lin", "Yi"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2006
  }, {
    "title": "Hierarchical latent class models for cluster analysis",
    "authors": ["Zhang", "Nevin L"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2004
  }],
  "id": "SP:392eac83c74b704e6960a184814de20e93e64559",
  "authors": [{
    "name": "Xiuyan Ni",
    "affiliations": []
  }, {
    "name": "Novi Quadrianto",
    "affiliations": []
  }, {
    "name": "Yusu Wang",
    "affiliations": []
  }, {
    "name": "Chao Chen",
    "affiliations": []
  }],
  "abstractText": "Clustering data with both continuous and discrete attributes is a challenging task. Existing methods often lack a principled probabilistic formulation. In this paper, we propose a clustering method based on a tree-structured graphical model to describe the generation process of mixed-type data. Our tree-structured model factorizes into a product of pairwise interactions, and thus localizes the interaction between feature variables of different types. To provide a robust clustering method based on the tree-model, we adopt a topographical view and compute peaks of the density function and their attractive basins for clustering. Furthermore, we leverage the theory from topology data analysis to adaptively merge trivial peaks into large ones in order to achieve meaningful clusterings. Our method outperforms state-of-the-art methods on mixed-type data.",
  "title": "Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data"
}