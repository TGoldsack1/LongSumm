{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 168–178 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1016\nWe propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the most human-like amongst all evaluated."
  }, {
    "heading": "1 Introduction",
    "text": "Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.\nPoetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem’s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud.\nStatistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015). Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form).\nWe experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.\n168"
  }, {
    "heading": "2 Related Work",
    "text": "Automatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches.\nRule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013; Barbieri et al., 2012) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry.\nStatistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used to generate new poetic variants (Yi et al., 2016; Greene et al., 2010). Neural language models have been increasingly applied to the task of poetry generation. The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generation systems with both manual and automatic evaluation. Ghazvininejad et al. (2016) and Goyal et al. (2016) apply neural language models with regularising finite state machines. However, in the former case the rhythm of the output cannot be defined at sample time, and in the latter case the finite state machine is not trained on rhythm at all, as it is trained on dialogue acts. McGregor et al. (2016) construct a phonological model for generating prosodic texts, however there is no attempt to embed semantics into this model."
  }, {
    "heading": "3 Phonetic-level Model",
    "text": "Our first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm.\nHowever, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes.\nThis means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. 2) The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own internal consistency.\nThus, the model comprises three steps: transliterating an orthographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.\nPhonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon. Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters.\nAlthough this approach offers full coverage over the training corpus—even for abbreviated words like ask’d and archaic words like renewest—it has several limitations. Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.\nIn addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes\n1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.\ndecipherment, when converting back into an orthographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.\nNeural language model We train a Long-Short Term Memory network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we maximize a multinomial logistic regression objective over the final softmax prediction. Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256. We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over multiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.\nOrthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. That is, we compute the most probable hypothesis word W given a phoneme sequence ρ:\narg maxi P ( Wi | ρ ) (1)\nWe can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010).\nTransliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a\nprevious word sequence (wt−n, ..., wt).\narg maxwt+1 P ( wt+1 | w1, ... , wt ) (2)\nIf a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that n maximises the n-gram frequency of the subsequences.\nOutput A popular form of poetry with strict internal structure is the sonnet. Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010). Since the 17,134 word tokens in Shakespeare’s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters.\nAn example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme."
  }, {
    "heading": "4 Constrained Character-level Model",
    "text": "As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of\nadequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically.\nWe now explore an alternative approach. Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model representing content, and a discriminative model representing form. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.\nCharacter Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words.\nAs we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers, with embeddings in 128 dimensions. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. We attenuate the learning rate over time, and by 20 epochs the model converges.\nRhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such\nas rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus.\nPronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations for constructing a classifier. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3. Furthermore, they are constructed from American English, meaning that British English may be misclassified.\nThese issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used.\nWe represent each line of poetry as a cascade of Weighted Finite State Transducers (WFST). A WFST is a finite-state automaton that maps between two sets of symbols. It is defined as an eight-tuple where ⟨Q, Σ, ρ, I, F, ∆, λ, p⟩:\nQ : A set of states\nΣ : An input alphabet of symbols\nρ : An output alphabet of symbols\nI : A set of initial states\nF : A set of final states, or sinks\n∆ : A transition function mapping pairs of states and symbols to sets of states\nλ : A set of weights for initial states\nP : A set of weights for final states 3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91% when the following word is the (Greene et al., 2010)\nA WFST assigns a probability (or weight, in the general case) to each path through it, going from an initial state to an end state. Every path corresponds to an input and output label sequence, and there can be many such paths for each sequence.\nWFSTs are often used in a cascade, where a number of machines are executed in series, such that the output tape of one machine is the input tape for the next. Formally, a cascade is represented by the functional composition of several machines.\nW (x, z) = A(x|y) ◦ B(y|z) ◦ C(z) (3)\nWhere W (x, z) is defined as the ⊕ sum of the path probabilities through the cascade, and x and z are an input sequence and output sequence respectively. In the real semiring (where the product of probabilities are taken in series, and the sum of the probabilities are taken in parallel), we can rewrite the definition of weighted composition to produce the following:\nW (x, z) = ⊕\ny\nA(x | y) ⊗ B(y | z) ⊗ C(z) (4)\nAs we are dealing with probabilities, this can be rewritten as:\nP (x, z) = ∑\ny\nP (x | y)P (y | z)P (z) (5)\nWe can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer.\nIn each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length4. We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.\n4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).\n× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×\nConstraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word tokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens.\nWe can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the last formulaically acceptable state of the line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content."
  }, {
    "heading": "4.1 Themes and Poetic devices",
    "text": "It is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about ‘love’, for instance, we would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like ‘love’. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.\nThemes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of these character strings by their cosine similarity to the key word, plus a constant. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.\nPoetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the same heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c."
  }, {
    "heading": "5 Evaluation",
    "text": "In order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry."
  }, {
    "heading": "5.1 Intrinsic evaluation",
    "text": "To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry. The second set was sampled from the characterlevel model, constrained to Iambic form. For com-\nparison, and to act as a baseline, we also sampled from the unconstrained character model.\nWe created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses.\nAs Table 1 shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry.\nIn order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5. Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction. Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.\n5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case."
  }, {
    "heading": "5.2 Extrinsic evaluation",
    "text": "We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e: the model generates good Iambic form, but not very good English), we only evaluate on the constrained characterlevel model. Poetry was generated with a variety of themes and poetic devices (see supplementary material).\nThe aim of the study was to determine whether participants could distinguish between human and machine-generated poetry, and if so to what extent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated. Participants were recruited from friends and people within poetry communities within the University of Cambridge, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.\nIn addition to the classification task, each participant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit). We naively consider the overall quality of a poem to be the mean of these three measures. We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans.\nTo select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948). This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them.\nFor our human poems, we focused explicitly on poetry where greater consideration is placed on\n6http://neuralpoetry.getforge.io/\nprosodic elements like rhythm and rhyme than semantic content (known as “nonsense verse”). We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which eight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in Figure 4d. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts.\nThe performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3. For the human poems,\nour group of participants guessed correctly that they were human 51.4% of the time. For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated. To determine whether our results were statistically significant, we performed a Chi2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more ‘human-like’ than those actually written by humans. In addition, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse.\nIt should be noted that the poems that were most ‘human-like’ and most aesthetic respectively were generated by the neural character model. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material, and our code is made available online7.\n7https://github.com/JackHopkins/ACLPoetry"
  }, {
    "heading": "6 Conclusions",
    "text": "Our contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices.\nAn indistinguishability test, where participants were asked to classify a randomly selected set of human “nonsense verse” and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans. In addition, the poems that were deemed most ‘humanlike’ and most aesthetic were both machinegenerated.\nIn future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry."
  }],
  "year": 2017,
  "references": [{
    "title": "Word length frequency and distribution in english: Observations, theory, and implications for the construction of verse lines",
    "authors": ["Hideaki Aoyama", "John Constable."],
    "venue": "arXiv preprint cmp-lg/9808004 .",
    "year": 1998
  }, {
    "title": "Markov constraints for generating lyrics with style",
    "authors": ["Mirko Degli Esposti."],
    "venue": "Proceedings of the 20th European Conference on Artificial Intelligence. IOS Press, pages 115–120.",
    "year": 2012
  }, {
    "title": "Neural probabilistic language models",
    "authors": ["Yoshua Bengio", "Holger Schwenk", "Jean-Sébastien Senécal", "Fréderic Morin", "Jean-Luc Gauvain."],
    "venue": "Innovations in Machine Learning, Springer, pages 137–186.",
    "year": 2006
  }, {
    "title": "Issues in building general letter to sound rules",
    "authors": ["Alan W Black", "Kevin Lenzo", "Vincent Pagel"],
    "year": 1998
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Full face poetry generation",
    "authors": ["Simon Colton", "Jacob Goodwin", "Tony Veale."],
    "venue": "Proceedings of the Third International Conference on Computational Creativity. pages 95–102.",
    "year": 2012
  }, {
    "title": "An exact a* method for deciphering letter-substitution ciphers",
    "authors": ["Eric Corlett", "Gerald Penn."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 1040–",
    "year": 2010
  }, {
    "title": "A formula for predicting readability: Instructions",
    "authors": ["Edgar Dale", "Jeanne S Chall."],
    "venue": "Educational research bulletin pages 37–54.",
    "year": 1948
  }, {
    "title": "Wasp: Evaluation of different strategies for the automatic generation of spanish verse",
    "authors": ["Pablo Gervás."],
    "venue": "Proceedings of the AISB-00 Symposium on Creative & Cultural Aspects of AI. pages 93–100.",
    "year": 2000
  }, {
    "title": "Generating topical poetry",
    "authors": ["Marjan Ghazvininejad", "Xing Shi", "Yejin Choi", "Kevin Knight."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 1183–1191.",
    "year": 2016
  }, {
    "title": "Natural language generation through character-based rnns with finite-state prior knowledge",
    "authors": ["Raghav Goyal", "Marc Dymetman", "Eric Gaussier."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-",
    "year": 2016
  }, {
    "title": "Automatic analysis of rhythmic poetry with applications to generation and translation",
    "authors": ["Erica Greene", "Tugba Bodrumlu", "Kevin Knight."],
    "venue": "Proceedings of the 2010 conference on empirical methods in natural language processing. Association for",
    "year": 2010
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Character-aware neural language models",
    "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."],
    "venue": "arXiv preprint arXiv:1508.06615 .",
    "year": 2015
  }, {
    "title": "Unsupervised analysis for decipherment problems",
    "authors": ["Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada."],
    "venue": "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 499–",
    "year": 2006
  }, {
    "title": "N-gram similarity and distance",
    "authors": ["Grzegorz Kondrak."],
    "venue": "String processing and information retrieval. Springer, pages 115–126.",
    "year": 2005
  }, {
    "title": "Better word representations with recursive neural networks for morphology",
    "authors": ["Thang Luong", "Richard Socher", "Christopher D Manning."],
    "venue": "CoNLL. pages 104–113.",
    "year": 2013
  }, {
    "title": "Towards a computational model of poetry generation",
    "authors": ["Hisar Manurung", "Graeme Ritchie", "Henry Thompson."],
    "venue": "Technical report, The University of Edinburgh.",
    "year": 2000
  }, {
    "title": "Process based evaluation of computer generated poetry",
    "authors": ["Stephen McGregor", "Matthew Purver", "Geraint Wiggins."],
    "venue": "The INLG 2016 Workshop on Computational Creativity in Natural Language Generation. page 51.",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781 .",
    "year": 2013
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur."],
    "venue": "INTERSPEECH. volume 2, page 3.",
    "year": 2010
  }, {
    "title": "Poetry generation system with an emotional personality",
    "authors": ["Joanna Misztal", "Bipin Indurkhya."],
    "venue": "Proceedings of the Fourth International Conference on Computational Creativity.",
    "year": 2014
  }, {
    "title": "Performance tradeoffs in dynamic time warping algorithms for isolated word recognition",
    "authors": ["Cory Myers", "Lawrence R Rabiner", "Aaron E Rosenberg."],
    "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on 28(6):623–635.",
    "year": 1980
  }, {
    "title": "Gaiku: Generating haiku with word associations norms",
    "authors": ["Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad."],
    "venue": "Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Lin-",
    "year": 2009
  }, {
    "title": "Learning phoneme mappings for transliteration without parallel data",
    "authors": ["Sujith Ravi", "Kevin Knight."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-",
    "year": 2009
  }, {
    "title": "Training neural network language models on very large corpora",
    "authors": ["Holger Schwenk", "Jean-Luc Gauvain."],
    "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for",
    "year": 2005
  }, {
    "title": "Generating text with recurrent neural networks",
    "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017–1024.",
    "year": 2011
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems. pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Harnessing constraint programming for poetry composition",
    "authors": ["Jukka M Toivanen", "Matti Järvisalo", "Hannu Toivonen"],
    "venue": "In Proceedings of the Fourth International Conference on Computational Creativity",
    "year": 2013
  }, {
    "title": "Freetts 1.2: A speech synthesizer written entirely in the java programming language",
    "authors": ["Willie Walker", "Paul Lamere", "Philip Kwok"],
    "year": 2010
  }, {
    "title": "The carnegie mellon pronouncing dictionary [cmudict",
    "authors": ["R Weide."],
    "venue": "0.6].",
    "year": 2005
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["Paul J Werbos."],
    "venue": "Proceedings of the IEEE 78(10):1550–1560.",
    "year": 1990
  }, {
    "title": "Generating chinese classical poems with rnn encoderdecoder",
    "authors": ["Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun."],
    "venue": "arXiv preprint arXiv:1604.01537 .",
    "year": 2016
  }, {
    "title": "Chinese poetry generation with recurrent neural networks",
    "authors": ["Xingxing Zhang", "Mirella Lapata."],
    "venue": "EMNLP. pages 670–680. 178",
    "year": 2014
  }],
  "id": "SP:9d0fc3b70e7c90b77dcdf726e0d9efe35c6ac7e9",
  "authors": [{
    "name": "Jack Hopkins",
    "affiliations": []
  }, {
    "name": "Douwe Kiela",
    "affiliations": []
  }],
  "abstractText": "We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the most human-like amongst all evaluated.",
  "title": "Automatically Generating Rhythmic Verse with Neural Networks"
}