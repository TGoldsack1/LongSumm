{
  "sections": [{
    "text": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework to estimate the optimal transport mapping as the gradient of a convex function that is trained via minimax optimization. Numerical experiments confirm the accuracy of the learned transport map. Our approach can be readily used to train a deep generative model. When trained between a simple distribution in the latent space and a target distribution, the learned optimal transport map acts as a deep generative model. Although scaling this to a large dataset is challenging, we demonstrate two important strengths over standard adversarial training: robustness and discontinuity. As we seek the optimal transport, the learned generative model provides the same mapping regardless of how we initialize the neural networks. Further, a gradient of a neural network can easily represent discontinuous mappings, unlike standard neural networks that are constrained to be continuous. This allows the learned transport map to match any target distribution with many discontinuous supports and achieve sharp boundaries.\n*Equal contribution. Order decided by a coin flip. 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign. 2Department of Mechanical and Aerospace Engineering, University of California, Irvine. 3Department of Electrical Engineering, Princeton University, 4Allen School of Computer Science & Engineering, University of Washington. Correspondence to: Ashok <makkuva2@illinois.edu>, Amir <amirhoseintghv@gmail.com>.\nProceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s)."
  }, {
    "heading": "1. Introduction",
    "text": "Finding a mapping that transports mass from one distribution Q to another distribution P is an important task in various machine learning applications, such as deep generative models (Goodfellow et al., 2014; Kingma & Welling, 2013) and domain adaptation (Gopalan et al., 2011; BenDavid et al., 2010). Among infinitely many transport maps T that can map a random variableX fromQ such that T (X) is distributed as P , several recent advances focus on discovering some inductive bias to find a transport map with desirable properties. Research in optimal transport has been leading such efforts, in applications such as color transfer (Ferradans et al., 2014), shape matching (Su et al., 2015), data assimilation (Reich, 2013), and Bayesian inference (El Moselhy & Marzouk, 2012). Searching for an optimal transport encourages a mapping that minimizes the total cost of transporting mass from Q to P , as originally formulated in Monge (1781), and provides the inductive bias needed in many such applications. However, finding the optimal transport map in general is a challenging task, especially in high dimensions where efficient approaches are critical.\nAlgorithmic solutions are well-established for discrete variables; the optimal transport can be found as a solution to linear program. Building upon this mature area, typical approaches for general distributions use quantization, and this becomes intractable for high-dimensional variables we encounter in modern applications (Evans & Gangbo, 1999; Benamou & Brenier, 2000; Papadakis et al., 2014).\nTo this end, we propose a novel minimax optimization approach to search for the optimal transport under the quadratic distance (i.e. 2-Wassertstein metric). A major challenge in a minimax formulation of optimal transport is that the constraints in the Kantorovich dual formulation (3) are notoriously challenging. They require the evaluation of the functions at every point in the domain, which is not tractable. A common straightforward heuristics sample some points and add those sampled constraints as regularizers. Such regularizations create biases that hinder learning the true optimal transport.\nOur key innovation is to depart from this common practice; we instead eliminate the constraints by restricting our search to the set of all convex functions, building upon the fundamental connection from Theorem 3.1. This leads to\nar X\niv :1\n90 8.\n10 96\n2v 2\n[ cs\n.L G\n] 1\n7 Ju\nn 20\n20\na novel minimax formulation in (5). Leveraging on recent advances in input convex neural networks, we propose a new architecture and a training algorithm for solving this minimax optimization. We establish the consistency of our proposed minimax formulation in Theorem 3.3. In particular, we show that the solution to this optimization problems yields the exact optimal transport map. We provide stability analysis for the proposed estimator in Theorem 3.6.\nFurther, when used to train deep generative models, our approach can be viewed as a novel framework to train a generator that is modeled as a gradient of a convex function. We provide a principled training rule based on the optimal transport theory. This ensures that (i) the generator converges to the optimal transport, independent of how we initialize the neural network; and (ii) represent sharp boundaries when the target has multiple disconnected supports. Gradient of a neural network naturally represents discontinuous functions, which is critical in mapping from a single connected support to disconnected supports.\nTo model convex functions, we leverage Input Convex Neural Networks (ICNNs), a class of scalar-valued neural networks f(x; θ) such that the function x 7→ f(x; θ) ∈ R is convex. These neural networks were introduced by Amos et al. (2016) to provide efficient inference and optimization procedures for structured prediction, data imputation and reinforcement learning tasks. In this paper, we show that ICNNs can be efficiently trained to learn the optimal transport map between two distributions P and Q. To the best of our knowledge, this is the first such instance where ICNNs are leveraged for the well-known task of learning optimal transport maps in a scalable fashion. This framework opens up a new realm for understanding problems in optimal transport theory using parametric convex neural networks, both in theory and practice. Figure 1 provides an example where the optimal transport map has been learned via our proposed Algorithm 1 from the orange distribution to the green distribution.\nNotation. P(X ) denotes the set of probability measures on a Polish space X , and B(X ) denotes the Borel sub-\nsets of X . For P ∈ P(X ) and Q ∈ P(Y), P ⊗ Q denotes the product measure on X × Y . For measurable map T : X → Y , T#P denotes the push-forward of P under T , i.e. (T#P )(A) = P (T−1(A)), ∀A ∈ B(Y). L1(P ) , {f is measurable & ∫ f dP < ∞} denotes the set of integrable functions with respect to P . CVX(P ) denotes the set of all convex functions in L1(P ). Id : x 7→ x denotes the identity function. 〈·, ·〉 and ‖ · ‖ denote the inner-product and `2-Euclidean norm."
  }, {
    "heading": "2. Background on optimal transport",
    "text": "Let P and Q be two probability distributions on Rd with finite second order moments. The Monge’s optimal transportation problem is to transport the probability mass under Q to P with the least amount of cost1, i.e.\nminimize T :T#Q=P\n1 2 EX∼Q‖X − T (X)‖2. (1)\nAny transport map T achieving the minimum in (1) is called optimal transport map. Optimal transport map may not exist. In fact, the feasible set in the above optimization problem may itself be empty, for example when Q is a Dirac distribution and P is any non-Dirac distribution.\nTo resolve the existence issue of the Monge problem (1), Kantorovich introduced a relaxation of the problem,\nW 22 (P,Q) , inf π∈Π(P,Q)\n1 2 E(X,Y )∼π‖X − Y ‖2, (2)\nwhere Π(P,Q) denotes the set of all joint probability distributions (or equivalently, couplings) whose first and second marginals are P and Q, respectively. The optimal value in (2) is the 2-Wasserstein distance W2(·, ·) squared. Any coupling π achieving the infimum is called the optimal coupling. Optimization problem (2) is also referred to as the primal formulation for 2-Wasserstein distance.\n1In general, Monge’s problem is defined in terms of cost function c(x, y). This paper is concerned with quadratic cost function c(x, y) = 1\n2 ‖x − y‖2 because of its nice geometrical properties\nand connection to convex analysis (Villani, 2003, Ch. 2).\nKantorovich also provided a dual formulation for (2), known as the Kantorovich duality (Villani, 2003, Theorem 1.3),\nW 22 (P,Q) = sup (f,g)∈Φc EP [f(X)] + EQ[g(Y )], (3)\nwhere Φc denotes the constrained space of functions, defined as Φc , { (f, g) ∈ L1(P )× L1(Q) : f(x) + g(y) ≤ 1 2‖x− y‖ 2 2, ∀(x, y) dP ⊗ dQ a.e. } .\nThe dual problem (3) can be recast as an stochastic optimization problem by approximating the expectations using independent samples from P and Q. However, there is no easy way to ensure the feasibility of the constraint (f, g) ∈ Φc along the gradient updates. Common approach is to translate the optimization into a tractable form, while sacrificing the original goal of finding the optimal transport map. Concretely, an entropic or a quadratic regularizer is added to the primal problem (2) (Cuturi, 2013; Essid & Solomon, 2018; Peyré et al., 2019; Blondel et al., 2017). Then, the dual to the regularized primal problem is an unconstrained version of (3) with additional penalty term. The unconstrained problem can be numerically solved using Sinkhorn algorithm in discrete setting (Cuturi, 2013) or stochastic gradient methods with suitable function representation in continuous setting (Genevay et al., 2016; Seguy et al., 2017). The optimal transport can then be obtained from f and g, using the first-order optimality conditions of the FenchelRockafellar’s duality theorem (Seguy et al., 2017), or by training a generator through an adversarial computational procedure (Leygonie et al., 2019).\nIn this paper, we take a different approach: solve the dual problem without introducing a regularization. This builds upon (Taghvaei & Jalali, 2019), where ICNN for the task of approximating the Wasserstein distance and optimal transport map is originally proposed. We bring the idea proposed (Taghvaei & Jalali, 2019) into practice by introducing a novel minimax optimization formulation. We describe our proposed method in Section 3 and provide a detailed comparison in Remark 3.5. Discussion about other related works (Lei et al., 2017; Guo et al., 2019; Xie et al., 2019; Muzellec & Cuturi, 2019; Rabin et al., 2011; Korotin et al., 2019) appears in Appendix D."
  }, {
    "heading": "3. A novel minimax formulation to learn optimal transport",
    "text": "Our goal is to learn the optimal transport map T ∗ from Q to P , from samples drawn from P and Q, respectively. We use the fundamental connection between optimal transport and Kantorovich dual in Theorem 3.1, to formulate learning T ∗ as a problem of estimating W 22 (P,Q). However, W 2 2 (P,Q) is notoriously hard to estimate. The standard Kantorovich dual formulation in Eq. (3) involves a supremum over a set Φc with infinite constraints, which is challenging to even approximately project onto. To this end, we derive an\nalternative optimization formulation in Eq. (5), inspired by the convexification trick (Villani, 2003, Section 2.1.2). This allows us to eliminate the distance constraint of Φc, and instead constrain our search over all convex functions. This constrained optimization can now be seamlessly integrated with recent advances in designing deep neural architectures with convexity guarantees. This leads to a novel minimax optimization to learn the optimal transport.\nWe exploit the fundamental properties of W 22 (P,Q) and the corresponding optimal transport to reparametrize the optimization formulation. Note that for any (f, g) ∈ Φc,\nf(x) + g(y) ≤ 1 2 ‖x− y‖22 ⇐⇒[\n1 2 ‖x‖22 − f(x)\n] + [ 1\n2 ‖y‖22 − g(y)\n] ≥ 〈x, y〉.\nHence reparametrizing 12‖ · ‖ 2 2 − f(·) and 12‖ · ‖ 2 2 − g(·) by f and g respectively, and substituting them in (3) yields\nW 22 (P,Q) = CP,Q − inf (f,g)∈Φ̃c\n{ EP [f(X)] + EQ[g(Y )] } ,\nwhere CP,Q = (1/2)E[‖X‖22 + ‖Y ‖22] is a constant independent of (f, g) and Φ̃c , {(f, g) ∈ L1(P ) × L1(Q) : f(x) + g(y) ≥ 〈x, y〉, ∀(x, y) dP ⊗ dQ a.e.}. While the above constrained optimization problem involves a pair of functions (f, g), it can be transformed into the following form involving only a single convex function f , thanks to Villani (2003, Theorem 2.9):\nW 22 (P,Q)=CP,Q− inf f∈CVX(P ) EP [f(X)]+EQ[f∗(Y )], (4)\nwhere f∗(y) = supx〈x, y〉 − f(x) is the convex conjugate of f(·).\nThe crucial tools behind our formulation are the following celebrated results due to Knott-Smith and Brenier (Villani, 2003), which relate the optimal solutions for the dual form in (4) and the primal form in (2). Theorem 3.1 ((Villani, 2003, Theorem 2.12)). Let P,Q be two probability distributions on Rd with finite second order moments. Then,\n1. (Knott-Smith optimality criterion) A coupling π ∈ Π(P,Q) is optimal for the primal (2) if and only if there exists a convex function f ∈ CVX(Rd) such that Supp(π) ⊂ Graph(∂f). Or equivalently, for all dπalmost (x, y), y ∈ ∂f(x). Moreover, the pair (f, f∗) achieves the minimum in the dual form (4).\n2. (Brenier’s theorem) If Q admits a density with respect to the Lebesgue measure on Rd, then there is a unique optimal coupling π for the primal problem. In particular, the optimal coupling satisfies that\ndπ(x, y) = dQ(y)δx=∇f∗(y),\nwhere the convex pair (f, f∗) achieves the minimum in the dual problem (4). Equivalently, π = (∇f∗ × Id)#Q.\n3. Under the above assumptions of Brenier’s theorem, ∇f∗ in the unique solution to Monge transportation problem from Q to P , i.e.\nEQ‖∇f∗(Y )− Y ‖2 = inf T :T#Q=P EQ‖T (Y )− Y ‖2.\nRemark 3.2. Whenever Q admits a density, we refer to ∇f∗ as the optimal transport map.\nHenceforth, throughout the paper we assume that the distribution Q admits a density in Rd. Note that in view of Theorem 3.1, any optimal pair (f, f∗) from the dual formulation in (4) provides us an optimal transport map ∇f∗ pushing forward Q onto P . However, the objective (4) is not amenable to standard stochastic optimization schemes due to the conjugate function f∗. To this end, we propose a novel minimax formulation in the following theorem where we replace the conjugate with a new convex function.\nTheorem 3.3. WheneverQ admits a density in Rd, we have\nW 22 (P,Q) = sup f∈CVX(P ), f∗∈L1(Q) inf g∈CVX(Q) VP,Q(f, g) + CP,Q, (5)\nwhere VP,Q(f, g) is a functional of f, g defined as\nVP,Q(f, g) = −EP [f(X)]−EQ[〈Y,∇g(Y )〉−f(∇g(Y ))].\nIn addition, there exists an optimal pair (f0, g0) achieving the infimum and supremum respectively, where ∇g0 is the optimal transport map from Q to P .\nProof sketch. The proof follows from the inequality 〈y,∇g(y)〉 − f(∇g(y)) ≤ f∗(y) for all functions g, and then taking the expectation over Q, and observing that the equality is achieved with g = f∗. The technical details appear in Appendix A.\nRemark 3.4. For any convex function f , the function g ∈ L1(Q) that achieves the infimum in (5) is convex and equals f∗. Therefore, the constraint g ∈ CVX(Q) can be relaxed to g ∈ L1(Q) without changing the optimal value and optimizing functions. We numerically observe that the optimization algorithm performs better under this relaxation.\nFormulation (5) now provides a principled approach to learn the optimal transport mapping∇g(·) as a solution of a minimax optimization. Since the optimization involves the search over the space of convex functions, we utilize the recent advances in input convex neural networks (ICNNs) to parametrize them as discussed in the following section."
  }, {
    "heading": "W1 W2 WL-1...",
    "text": ""
  }, {
    "heading": "3.1. Minimax optimization over ICNNs",
    "text": "We propose using parametric models based on deep neural networks to approximate the set of convex functions. This is known as input convex neural networks (Amos et al., 2016), denoted by ICNN(Rd). We propose estimating the following approximate Wasserstein-2 distance, from samples:\nW̃ 22 (P,Q)= sup f∈ICNN(Rd) inf g∈ICNN(Rd) VP,Q(f, g)+CP,Q. (6)\nICNNs are a class of scalar-valued neural networks f(x; θ) such that the function x 7→ f(x; θ) ∈ R is convex.\nThe neural network architecture for an ICNN is as follows. Given an input x ∈ Rd, the mapping x 7→ f(x; θ) is given by a L-layer feed-forward NN using the following equations for l = 0, 1, . . . , L− 1:\nzl+1 = σl(Wlzl +Alx+ bl), f(x; θ) = zL,\nwhere {Wl}, {Al} are weight matrices (with the convention that W0 = 0), and {bl} are the bias terms. σl denotes the entry-wise activation function at the layer l. This is illustrated in Figure 2. We denote the total set of parameters by θ = ({Wl}, {Al}, {bl}). It follows from Amos et al. (2016, Proposition 1) that f(x; θ) is convex in x provided\n(i) all entries of the weights Wl are non-negative;\n(ii) activation function σ0 is convex;\n(iii) σl is convex and non-decreasing, for l = 1, . . . , L− 1.\nWhile ICNNs are a specific parametric class of convex functions, it is important to understand if this class is rich enough representationally. This is answered positively by Chen et al. (2018, Theorem 1). In particular, they show that any convex function over a compact domain can be approximated in sup norm by a ICNN to the desired accuracy. This justifies the choice of ICNNs as a suitable approximating class for the convex functions.\nThe proposed framework for learning the optimal transport provides a novel training method for deep generative models, where (a) the generator is modeled as a gradient of a convex function and (b) the minimax optimization in (6) (and more concretely, Algorithm 1) provides the training methodology. On the surface, Eq. (6) resembles the minimax optimization of generative adversarial networks based on Wasserstein-1\ndistance (Arjovsky et al., 2017), called WGAN. However, there are several critical differences making our approach attractive.\nFirst, because WGANs use optimal transportation distance only as a measure of distance, the learned generator map from the latent source to the target is arbitrary and sensitive to the initialization (see Figure 4) (Jacob et al., 2018). On the other hand, our proposed approach aims to find the optimal transport map and learns the same mapping regardless of the initialization (see Figure 1).\nSecondly, in a WGAN architecture (Arjovsky et al., 2017; Petzka et al., 2017), the transport map (which is the generator) is represented with neural network that is a continuous mapping. Although, a discontinuous map can be approximated arbitrarily close with continuous neural networks, such a construction requires large weights making training unstable. On the other hand, through our proposed method, by representing the transport map with gradient of a neural network (equipped with ReLU type activation functions), we obtain a naturally discontinuous map. As a consequence we have sharp transition from one part of the support to the other, whereas GANs (including WGANs) suffer from spurious probability masses that are not present in the target. This is illustrated in Section 4.3. The same holds for regularization-based methods for learning optimal transport (Genevay et al., 2016; Seguy et al., 2017; Leygonie et al., 2019), where transport map is parametrized by continuous neural nets.\nRemark 3.5. In a recent work, Taghvaei & Jalali (2019)\nproposed to solve the semi-dual optimization problem (4) by representing the function f with an ICNN and learning it using a stochastic optimization algorithm. However, each step of this algorithm requires computing the conjugate f∗ for all samples in the batch via solving a inner convex optimization problem for each sample which makes it slow and challenging to scale to large datasets. Further it is memory intensive as each inner optimization step requires a copy of all the samples in the dataset. In contrast, we represent the convex conjugate f∗ using ICNN and present a novel minimax formulation to learn it, in a scalable manner."
  }, {
    "heading": "3.2. Stability analysis of the learned transport map",
    "text": "Theorem 3.3 establishes the consistency of our proposed optimization: if the objective (5) is solved exactly with a pair of functions (f0, g0), then ∇g0 is the exact optimal transport map from Q to P . In this section, we study the error in approximating the optimal transport map∇g0, when the objective (5) is solved up to a small error. To this end, we build upon the recent results from Hütter & Rigollet (2019, Prop. 8) regarding the stability of optimal transport maps.\nRecall that the optimization objective (5) involves a minimization and a maximization. For any pair (f, g), let 1(f, g) denote the minimization gap and 2(g) denote the\nmaximization gap, defined according to:\n1(f, g) = V(f, g)− inf g̃∈CVX(Q) V(f, g̃), (7)\n2(f) = sup f̃∈CVX(P ) inf g̃∈CVX(Q) V(f̃ , g̃)− inf g̃∈CVX(Q) V(f, g̃)\nThen, the following theorem bounds the the error between ∇g and the optimal transport map ∇g0 as a function 1 and 2. We defer its proof to Appendix B.\nTheorem 3.6. Consider the optimization problem (5). Assume Q admits a density and let ∇g0(·) denote the optimal transport map from Q to P . Then for any pair (f, g) such that f is α-strongly convex, we have\n‖∇g −∇g0‖2L2(Q) ≤ 2\nα ( 1(f, g) + 2(f)),\nwhere 1 and 2 are defined in (7), and ‖ · ‖L2(Q) denotes the L2-norm with respect to measure Q."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, first we qualitatively illustrate our proposed approach (see Figure 3) on the following two-dimensional synthetic datasets: (a) Checkerboard, (b) Mixture of eight Gaussians. We compare our method with the following three baselines: (i) Barycentric-OT (Seguy et al., 2017), (ii) W1-LP, which is the state-of-the-art Wasserstein GAN introduced by (Petzka et al., 2017), (iii) W2GAN (Leygonie et al., 2019). Note that while the goal of W1-LP is not to learn the optimal transport map, the generator obtained at the end of its training can be viewed as a transport map. For all these baselines, we use the implementations (publicly available) of Leygonie et al. (2019) which has the best set of parameters for each of these methods. In Section 4.2 and Section 4.3, we highlight the respective robustness and the discontinuity of our transport maps as opposed to other approaches. Finally, in Section 4.4, we show the effectiveness of our approach on the challenging task of learning the optimal transport map on a variety of synthetic and real world high-dimensional data. Full experimental details are provided in Appendix C.\nTraining methodology. We utilize our minimax formulation in (6) to learn the optimal transport map. We parametrize the convex functions f and g using the same ICNN architecture (Figure 2). Recall that to ensure convexity, we need to restrict all weights W`’s to be non-negative (Assumption (i) in ICNN). We enforce it strictly for f , as the maximization over g can be unbounded, making optimization unstable, whenever f is non-convex. However, we relax this constraint for g (as permitted according to Remark 3.4) and instead introduce a regularization term\nR(θg) = λ ∑ Wl∈θg ‖max(−Wl, 0)‖2F , (8)\nwhere λ > 0 is a regularization constant and the maximum is taken entry-wise for all the weight parameters {Wl} ⊂ θg. We empirically observe that this relaxation makes the optimization converge faster.\nFor both the maximization and minimization updates in (6), we use Adam (Kingma & Ba, 2014). At each iteration, we draw a batch of samples from P and Q denoted by {Xi}Mi=1 and {Yj}Mj=1 respectively. Then, we use the following objective for optimization which is an empirical counterpart of (6):\nmax θf :W`≥0,∀`∈[L−1] min θg J(θf , θg) +R(θg), (9)\nwhere θf , θg are the parameters of f and g, respectively, W` ≥ 0 is an entry-wise constraint, and\nJ(θf , θg) = 1\nM M∑ i=1 f(∇g(Yi))− 〈Yi,∇g(Yi)〉 − f(Xi).\nThis is summarized in Algorithm 1. In the remainder of the paper, we interchangeably refer to Algorithm 1 as either ‘Our approach’ or ‘Our algorithm’.\nAlgorithm 1 The numerical procedure to solve the optimization problem (9).\nInput: Source dist. Q, Target dist. P , Batch size M , Generator iterations K, Total iteratioins T for t = 1, . . . , T do\nSample batch {Xi}Mi=1 ∼ P for k = 1, . . . ,K do\nSample batch {Yi}Mi=1 ∼ Q Update θg to minimize (9) using Adam method\nend for Update θf to maximize (9) using Adam method Projection: w ← max(w, 0), for all w ∈ {W l} ∈ θf\nend for\nRemark 4.1. Note that the regularization term R(θg) is data-independent and does not introduce any bias to the optimization problem. For any convex function f , the minimizer of the problem (9) is still a convex function g as discussed in Remark 3.4. We use this regularization to guide the algorithm towards neural networks that are convex."
  }, {
    "heading": "4.1. Learning the optimal transport map",
    "text": "As highlighted in Figure 1 and Figure 3d, qualitatively, we observe that our proposed procedure indeed learns the optimal transport map on both the Checkerboard and Mixture of eight Gaussians datasets. In particular, our transport map is able to cut the continuous mass symmetrically and transport it to the nearest target support in both these examples. Also, Figure 3 illustrates the qualitative difference of our approach compared to other approaches, in terms of non-optimality\nand existence of trailing dots. The existence of trailing dots is due to representing the transport map with continuous neural networks, discussed in Section 4.3."
  }, {
    "heading": "4.2. Robustness of learning transport maps",
    "text": "In this section we numerically illustrate that the generator in W1-LP and W2GAN finds arbitrary transport maps, and it is sensitive to initialization as discussed in Section 3. This is in stark contrast with our proposed approach which finds the optimal transport independent of the initialization. We consider the previous Checkerboard example (Figure 1a) and train W1-LP and W2GAN with different random initializations. The resulting transport maps for two different random trials are depicted in Figure 4a and Figure 4b for W1-LP, and Figure 4c and Figure 4d for W2-GAN. In addition to the fact that the learned transport map is very sensitive to initializations, the quality of the samples generated by thus trained models are also sensitive. This is a major challenge in training GANs (Lin et al., 2018)."
  }, {
    "heading": "4.3. Learning discontinuous transport maps",
    "text": "The power to represent a discontinuous transport mapping is what fundamentally sets our proposed method apart from the existing approaches, as discussed in Section 3. Two prominent approaches for learning transport maps are generative adversarial networks (Arjovsky et al., 2017; Petzka et al., 2017) and regularized optimal transport (Genevay et al., 2016; Seguy et al., 2017). In both cases, the transport map is modeled by a standard neural network with finite depth and width, which is a continuous function. As a consequence, continuous transport maps suffer from unintended and undesired spurious probability mass that connects disjoint supports of the target probability distribution.\nFirst, standard GANs including the original GAN (Goodfellow et al., 2014) and variants of WGAN (Arjovsky et al., 2017; Gulrajani et al., 2017; Wei et al., 2018) all suffer from spurious probability masses. Even those designed to tackle such spurious probability masses, like PacGAN (Lin et al., 2018), cannot overcome the barrier of continuous\nneural networks. This suggests that fundamental change in the architecture, like the one we propose, is necessary. Figure 3b illustrates the same scenario for the transport map learned through the WGAN framework. We can observe the trailing dots of spurious probability masses, resulting from undesired continuity of the learned transport maps.\nSimilarly, regularization methods to approximate optimal transport maps, explained in Section 2, suffer from the same phenomenon. Representing a transport map with an inherently continuous function class results in spurious probability masses connecting disjoint supports. Figure 3a, corresponding to Barycentric-OT, illustrates those trailing dots of spurious masses for the learned transport map from algorithm introduced in Seguy et al. (2017). We also observe a similar phenomenon with Leygonie et al. (2019) as illustrated in Figure 3c.\nOn the other hand, we represent the transport map with the gradient of a neural network (equipped with non-smooth ReLU type activation functions). The resulting transport map can naturally represent discontinuous transport maps, as illustrated in Figure 1b and Figure 3d. The vector field of the learned transport map in Figure 1c clearly shows the discontinuity of the learned optimal transport."
  }, {
    "heading": "4.4. High dimensional experiments",
    "text": "We consider the challenging task of learning optimal transport maps on high dimensional distributions. In particular, we consider both synthetic and real world high dimensional datasets and provide quantitative and qualitative illustration of the performance of our proposed approach.\nGaussian to Gaussian. Source distribution Q = N (0, Id) and target distribution P = N (µ, Id), for some fixed µ ∈ Rd and d = 784. The mean vector µ = α(1, . . . , 1)> for some parameter α > 0. Because both distributions are Gaussian, the optimal transport map is explicitly known: T ∗(x) = x+ µ and hence W 22 (P,Q) = ‖µ‖2/2 = α2d/2. In Figure 5a, we compare our estimated distance W̃ 22 (P,Q), defined in (6), with the exact value W 22 (P,Q), as the training progresses for various values of α ∈ {1, 5, 10}. Intu-\nitively, learning is more challenging when α is larger. Further, error in learning the optimal transport map, quantified with the metric ‖µT (Q) − µ‖2, where µT (Q) is the mean of the transported distribution T#Q, is reported in Table 1.\nHigh-dim. Gaussian to low-dim. mixture. Source distribution Q is standard Gaussian N (0, Id) with d = 784, and the target distribution P is a mixture of four Gaussians that lie in in the two-dimensional subspace of the highdimensional space Rd, i.e. the first two components of the random vector X ∼ P is mixture of four Gaussians, and the rest of the components are zero. The projection of the learned optimal transport map onto the first four components is depicted in Figure 5b. As illustrated in the left panel of 5b, our transport map correctly maps the source distribution to the mixture of four Gaussians in the first two components. And it maps the rest of the components to zero, as highlighted by a red blob at zero in the right panel.\nMNIST {0, 1, 2, 3, 4} to MNIST {5, 6, 7, 8, 9}. We consider the standard MNIST dataset (LeCun et al., 1998) with the goal of learning the optimal transport map from the set of images corresponding to first five digits {0, 1, 2, 3, 4} to the last five digits {5, 6, 7, 8, 9}. To achieve this, we embed the images into the a space where the Euclidean norm ‖ · ‖ between the embedded images is meaningful. This is in alignment with the reported results in the literature for learning the L2-optimal transport map (Yang & Karniadakis, 2019, Sec. 4.1). We consider the embeddings into a 16-dimensional latent feature space given by a pre-trained Variational Autoencoder (VAE). We simulate our algorithm\non this feature space. The results of the learned transport map are depicted in Figure 5. Figure 5c presents samples from the source distribution and Figure 5d illustrates the source samples after transportation under the learned optimal transport map. We observe that the digits that look alike are coupled via the optimal transport map, e.g. 1→ 9, 2→ 8, and 4→ 9.\nGaussian to MNIST. The source is 16-dimensional standard Gaussian distribution, and the target is the 16- dimensional latent embeddings of all the MNIST digits. The MNIST like samples that are generated from the learned optimal transport map are depicted in Figure 6.\nThese experiments serve as a proof of concept that the algorithm scales to high-dimensional setting and real-world dataset. We believe that further improvements on the performance of the proposed algorithm requires careful tuning of hyper-parameters which takes time to develop (similar to initial WGAN) and is a subject of ongoing work."
  }, {
    "heading": "5. Conclusion",
    "text": "We presented a novel minimax framework to learn the optimal transport map under W2-metric. Our framework is\nin contrast to regularization-based approaches, where the constraint of the dual Kantorovich problem is replaced with a penalty term. Instead, we represent the dual functions with ICNN, so that the constraint is automatically satisfied. Further, the transport map is expressed as gradient of a convex function, which is able to represent discontinuous maps. We believe that our framework paves way for bridging the optimal transport theory and practice."
  }, {
    "heading": "A. Proof of Theorem 3.3",
    "text": "Define Vf (g) , EQ[〈Y,∇g(Y )〉−f(∇g(Y ))]. The main step of the proof is to show that supg∈CVX(Q) Vf (g) = EQ[f∗(Y )]. Then the conclusion follows from (4). To prove this, note that for all g ∈ CVX(Q), we have\n〈y,∇g(y)〉 − f(∇g(y)) ≤ 〈y,∇f∗(y)〉 − f(∇f∗(y)) = f∗(y),\nfor all y ∈ Rd such that g and f∗ are differentiable at y. We now claim that both g and f∗ are differentiable Q-almost everywhere (a.e). If the claim is true, upon taking the expectation w.r.t Q:\nVf (g) ≤ Vf (f∗) = EQ[f∗(Y )], ∀g ∈ CVX(Q) and the inequality is achieved with g = f∗. Now we prove the claim as follows: Since ∫ g dQ <∞, we haveQ(g =∞) = 0. ThusQ(Dom(g)) = 1, where Dom(g) is the domain of the function g. Moreover, Q(Int(Dom(g)) = 1, where Int(·) denotes the interior, because the boundary hasQ-measure zero (Q has a density). Since g is convex, it is differentiable on Int(Dom(g)) except at points of Lebesgue measure zero which have Q-measure zero too. Therefore, g is Q-a.e differentiable. Similar arguments hold for f∗."
  }, {
    "heading": "B. Proof of Theorem 3.6",
    "text": "The proof follows from the bounds\n‖∇g −∇f∗‖2L2(Q) ≤ 2\nα 1, (10a)\n‖∇f∗ −∇g0‖2L2(Q) ≤ 2\nα 2, (10b)\nand using the triangle inequality. The proof for the first bound is as follows. If f is α-strongly convex, then f∗ is 1α smooth. By definition of smoothness,\nf∗(z) ≤ f∗(y) + 〈∇f∗(y), z − y〉+ 1 2α ‖z − y‖2 , hy(z), ∀y, z ∈ Rd,\nwhere hy(z) is defined to be the quadratic function of z that appears on the right-hand side of the inequality. From f∗(z) ≤ hy(z), it follows that the convex conjugate f(x) ≥ h∗y(x). As a result,\nf(x) ≥ h∗y(x) = −f∗(y) + 〈y, x〉+ α\n2 ‖x−∇f∗(y)‖2, ∀x, y ∈ Rd. (11)\nWe use this inequality to control the optimality gap 1(f, g):\n1(f, g) = V(f, g)− inf g̃ V(f, g̃)\n= V(f, g)− V(f, f∗) = EQ[f∗(Y )− 〈Y,∇g(Y )〉+ f(∇g(Y ))] ≥ α 2 EQ[‖∇g(Y )−∇f∗(Y )‖2],\nwhere the last step follows from (11), with x = ∇g(y). This concludes the proof of the bound (10a). It remains to prove (10b). To this end, note that the optimality gap 2(f) is given by\n2(f) = V(f0, g0)− inf g̃ V(f, g̃)\n= V(f0, f∗0 )− V(f, f∗) = −(EP [f0(X)] + EQ[f∗0 (Y )]) + (EP [f(X)] + EQ[f∗(Y )]) = −EQ[f0(∇f∗0 (Y )) + f∗0 (Y )] + EQ[f(∇f∗0 (Y )) + f∗(Y )] = −EQ[〈Y,∇f∗0 (Y )〉] + EQ[f(∇f∗0 (Y )) + f∗(Y )]\nUsing the inequality (11) with x = ∇f∗0 (y) yields:\n2(f) ≥ α\n2 EQ[|∇f∗0 (Y )−∇f∗(Y )|2]\nconcluding (10b) noting that f∗0 = g0."
  }, {
    "heading": "C. Experimental set-up",
    "text": "C.1. Two-dimensional experiments\nDatasets. We use the following synthetic datasets: (i) Checkerboard, and (ii) Mixture of eight Gaussians. For the Checkerboard dataset, the source distribution Q is the law of the random variable Y = X + Z, where X ∼ Unif({(0, 0), (1, 1), (1,−1), (−1, 1), (−1,−1)}) and Z ∼ Unif([−0.5, 0.5] × [−0.5, 0.5]). Similarly, P is the distribution of random variable Y = X + Z, where X ∼ Unif({(0, 1), (0,−1), (1, 0), (−1, 0)}) and Z ∼ Unif([−0.5, 0.5] × [−0.5, 0.5]). Note that Unif(B) denotes the uniform distribution over any set B. For the mixture of eight Gaussians dataset, we have Q = N (0, I2) and P is the law of random variable Y , where Y = X + Z with X ∼ Unif({(1, 0), ( 1√\n2 , 1√ 2 )}, (0, 1), (−1√ 2 , 1√ 2 ), (−1, 0), (−1√ 2 , −1√ 2 ), (0,−1), ( 1√ 2 , −1√ 2 )}) and Z ∼ N (0, 0.5I2).\nArchitecture details. For our Algorithm 1, we parametrize both the convex functions f and g by ICNNs. Both these ICNN networks have equal number of nodes for all the hidden layers followed by a final output layer. We choose a square of leaky ReLU function, i.e σ0(x) = (max(βx, x))\n2 with a small positive constant β as the convex activation function for the first layer σ0. For the remaining layers, we use the leaky ReLU function, i.e σl(x) = max(βx, x) for l = 1, . . . , L− 1, as the monotonically non-decreasing and convex activation function. Note that the assumptions (ii)-(iii) of the ICNN are satisfied. In all of our experiments, we set the parameter β = 0.2. In some of the experiments as explained below, we chose the SELU activation function which also obeys the convexity assumptions.\nFor the three baselines, Barycentric-OT, W1-LP, and W2GAN, we use the implementations of Leygonie et al. (2019), made publicly available at https://github.com/jshe/wasserstein-2. For all these methods, we use the default settings of hyperparameters which were fixed to be the best values from the respective papers. Further, for a fair comparison we allow the number of parameters in each of these baselines to be larger than ours; in fact, for W2GAN and Barycentric-OT, the default number of neural network parameters is much larger than ours.\nHyperparameters. For reproducibility, we provide the details of the numerical experiments for each of the figures. For the Checkerboard dataset in Figure 3 (same as Figure 1), we run Algorithm 1 with the following parameters: For both the ICNNs f and g, we set the hidden size m = 64, number of layers L = 4, regularization constant λ = 1.0, Leaky ReLU activation and for training we use batch size M = 1024, learning rate 10−4, generator iterations K = 10, total number of iterations T = 105, and the Adam optimizer with β1 = 0.5, and β2 = 0.9. For each of the baselines, the following are the values of the parameters: (a) Barycentric-OT: 3 (1 corresponding to the dual stage and the rest for the map step) neural networks each with m = 128, L = 3,M = 512, T = 2× 105 and l2-entropy penalty, (b) W1-LP: Both the discriminator and the generator neural networks with m = 128, L = 3,K = 5 and M = 512, T = 2× 105, and (c) W2GAN: 3 neural networks (1 corresponding to the generator whereas the remaining are for two functions in the dual formulation (3)) each with m = 128, L = 3,K = 5,M = 512, T = 2 × 105. W2GAN also uses six additional regularization terms which set to default values as provided in the code. Also, all these baselines use ReLU activation and Adam optimizer with β1 = 0.9 and β2 = 0.990 and the learning rate for generator parameters being 0.0001 and 0.0005 for the rest. For the mixture of eight Gaussians dataset, we use the same parameters except batch-size M = 256, whereas all the baselines use the same parameters as the above setting. Also, for the multiple trials in Figure 4 for W1-LP and W2GAN, we use the above parameters but with a different random initialization of the neural network weights and biases.\nC.2. High dimensional experiments\nGaussian to Gaussian. Source distribution Q = N (0, Id) and target distribution P = N (µ, Id), for some fixed µ ∈ Rd and d = 784. The mean vector µ = α(1, . . . , 1)> with α ∈ {1, 5, 10}. For both the ICNNs f and g, we have d = 784,m = 1024, L = 3, Leaky ReLU activation, batch size M = 60, K = 16, λ = 0.1, T = 40, 000, Adam optimizer with β1 = 0.5 and β2 = 0.99, learning rate decay by a factor of 0.5 for every 2, 000 iterations. Note that in Figure 5a, 1 epoch corresponds to 1000 iterations.\nHigh-dim. Gaussian to low-dim. mixture. Source distribution Q = N (0, Id) with d = 784. The target distribution is a mixture of four Gaussians P = ∑4 i=1 1 4N (µi,Σ), where µi = (±1.4,±1.4, 0, . . . , 0) ∈ R\n784 and Σ = diag(0.2, 0.2, 0, . . . , 0). For both the ICNNs f and g, we have d = 784,m = 1024, L = 3, Leaky ReLU activation, batch size M = 60, K = 25, λ = 0.01, Adam optimizer with β1 = 0.5 and β2 = 0.99, learning rate decay by a factor of 0.5 for every two epochs. The algorithm is simulated for 30 epochs, where each epoch corresponds to 1000 iterations.\nMNIST {0, 1, 2, 3, 4} to MNIST {5, 6, 7, 8, 9}. To obtain the latent embeddings of the MNIST dataset, we first train a VAE with both the encoder and decoder having 3 hidden layers with 256 neurons and the size of latent vector being 16 dimensional. We then use ICNNs f and g to learn the optimal transport between the embeddings of digits {0, 1, 2, 3, 4} to that of {5, 6, 7, 8, 9}. For both these ICNNs we have d = 16,m = 1024, L = 3, CELU activation, batch size = 128, K = 16, λ = 1, T = 100, 000, Adam optimizer with β1 = 0.9 and β2 = 0.99, learning rate decay by a factor of 0.5 for every 4, 000 iterations.\nGaussian to MNIST. To obtain the latent embeddings for the MNIST, we use the same pre-trained VAE models as above. Also we use the same hyperparameter settings as that of the “MNIST {0, 1, 2, 3, 4} to MNIST {5, 6, 7, 8, 9}\" experiment with the only change of batch size being 64."
  }, {
    "heading": "D. Further discussion of related work",
    "text": "The idea of solving the semi-dual optimization problem (4) is classically considered in (Chartrand et al., 2009), where the authors derive a formula for the functional derivative of the objective function with respect to f and propose to solve the optimization problem with the gradient descent method. Their approach is based on the discretization of the space and knowledge of the explicit form of the probability density functions, that is not applicable to real-world high dimensional problems.\nMore recently, the authors in (Lei et al., 2017; Guo et al., 2019) propose to learn the function f in a semi-discrete setting, where one of the marginals is assumed to be a discrete distribution supported on a set of N points {y1, . . . , yN} ⊂ Rd, and the other marginal is assumed to have a continuous density with compact convex support Ω ⊂ Rd. They show that the problem of learning the function f is similar to the variational formulation of the Alexandrov problem: constructing a convex polytope with prescribed face normals and volumes. Moreover, they show that, in the semi-distrete setting, the optimal f is of the form f(x) = max1≤i≤1{〈x, yi〉+ bi} and simplify the problem of learning f to the problem learning N real numbers bi ∈ R. However, the objective function involves computing polygonal partition of Ω into N convex cells, induced by the function f , which is computationally challenging. Moreover, the learned optimal transport map∇f , transports the probability distribution from each convex cell to a single point yi, which results in generalization issues. Additionally, the proposed approach is semi-discrete, and as a result, does not scale with the number of samples.\nStatistical analysis of learning the optimal transport map through the semi-dual optimization problem (4) is studied in (Hütter & Rigollet, 2019; Rigollet & Weed, 2018), where the authors establish a minimax convergence rate with respect to number of samples for certain classes of regular probability distributions. They also propose a procedure that achieves the optimal convergence rate, that involves representing the function f with span of wavelet basis functions up to a certain order, and also requiring the function f to be convex. However, they do not provide a computational algorithm to implement the procedure.\nThere are also other alternative approaches to approximate the optimal transport map that are not based on solving the semi-dual optimization problem (4). In (Leygonie et al., 2019), the authors propose to approximate the optimal transport map, through an adversarial computational procedure, by considering the dual optimization problem (3), and replacing the constraint with a quadratic penalty term. However, in contrast to the other regularization-based approaches such as (Seguy et al., 2017), they consider a GAN architecture, and propose to take the generator, after the training is finished, as the optimal transport map. They also provide a theoretical justification for their proposal, however the theoretical justification is valid in an ideal setting where the generator has infinite capacity, the discriminator is optimal at each update step, and the cost is equal to the exact Wasserstein distance. These ideal conditions are far from being true in a practical setting.\nAnother approach, proposed in (Xie et al., 2019), is to learn the optimal coupling from primal formulation (2), instead of solving the dual problem (3). The approach involves representing the coupling with two generators that map a Gaussian random variable to Rd, and two discriminators to ensure the coupling satisfies the marginal constraints. Although, the proposed approach is attractive when an optimal transport map does not exists, it is computationally expensive because it involves learning four deep neural networks. Finally, a procedure is recently proposed to approximate the optimal transport map that is optimal only on a subspace projection instead of the entire space (Muzellec & Cuturi, 2019). This approach is inspired by the sliced Wasserstein distance method to approximate the Wasserstein distance (Rabin et al., 2011; Deshpande et al., 2018). However, selection of the subspace to project on is a non-trivial task, and optimally selecting the projection is an optimization over the Grassmann manifold which is computationally challenging.\nIn a recent work, Korotin et al. (2019) too model the convex conjugate function f∗ with an ICNN, denoted here by g, and a\npenalty term of the form ‖∇f(∇g(y))− y‖2 is added to the semi-dual optimization (4). The penalty term serves to ensure that∇g is inverse of∇f and hence g = f∗. The additional penalty term makes the problem non-convex, even in the infinite capacity case, where the function representation is not restricted."
  }],
  "year": 2020,
  "references": [{
    "title": "Input convex neural networks",
    "authors": ["B. Amos", "L. Xu", "J.Z. Kolter"],
    "venue": "arXiv preprint arXiv:1609.07152,",
    "year": 2016
  }, {
    "title": "A theory of learning from different domains",
    "authors": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"],
    "venue": "Machine learning,",
    "year": 2010
  }, {
    "title": "A computational fluid mechanics solution to the monge-kantorovich mass transfer problem",
    "authors": ["Benamou", "J.-D", "Y. Brenier"],
    "venue": "Numerische Mathematik,",
    "year": 2000
  }, {
    "title": "Smooth and sparse optimal transport",
    "authors": ["M. Blondel", "V. Seguy", "A. Rolet"],
    "venue": "arXiv preprint arXiv:1710.06276,",
    "year": 2017
  }, {
    "title": "A gradient descent solution to the monge-kantorovich problem",
    "authors": ["R. Chartrand", "B. Wohlberg", "K. Vixie", "E. Bollt"],
    "venue": "Applied Mathematical Sciences,",
    "year": 2009
  }, {
    "title": "Optimal control via neural networks: A convex approach",
    "authors": ["Y. Chen", "Y. Shi", "B. Zhang"],
    "venue": "arXiv preprint arXiv:1805.11835,",
    "year": 2018
  }, {
    "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
    "authors": ["M. Cuturi"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2013
  }, {
    "title": "Generative modeling using the sliced wasserstein distance",
    "authors": ["I. Deshpande", "Z. Zhang", "A.G. Schwing"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2018
  }, {
    "title": "Bayesian inference with optimal maps",
    "authors": ["T.A. El Moselhy", "Y.M. Marzouk"],
    "venue": "Journal of Computational Physics,",
    "year": 2012
  }, {
    "title": "Quadratically regularized optimal transport on graphs",
    "authors": ["M. Essid", "J. Solomon"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2018
  }, {
    "title": "Differential equations methods for the Monge-Kantorovich mass transfer problem, volume 653",
    "authors": ["L.C. Evans", "W. Gangbo"],
    "venue": "American Mathematical Soc.,",
    "year": 1999
  }, {
    "title": "Regularized discrete optimal transport",
    "authors": ["S. Ferradans", "N. Papadakis", "G. Peyré", "Aujol", "J.-F"],
    "venue": "SIAM Journal on Imaging Sciences,",
    "year": 2014
  }, {
    "title": "Stochastic optimization for large-scale optimal transport",
    "authors": ["A. Genevay", "M. Cuturi", "G. Peyré", "F. Bach"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Domain adaptation for object recognition: An unsupervised approach",
    "authors": ["R. Gopalan", "R. Li", "R. Chellappa"],
    "venue": "In 2011 international conference on computer vision,",
    "year": 2011
  }, {
    "title": "Improved training of wasserstein gans",
    "authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A.C. Courville"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Mode collapse and regularity of optimal transportation",
    "authors": ["Y. Guo", "D. An", "X. Qi", "Z. Luo", "Yau", "S.-T", "X Gu"],
    "year": 1902
  }, {
    "title": "Minimax rates of estimation for smooth optimal transport",
    "authors": ["Hütter", "J.-C", "P. Rigollet"],
    "venue": "maps. arXiv preprint arXiv:1905.05828,",
    "year": 2019
  }, {
    "title": "W2gan: Recovering an optimal transport map with a gan",
    "authors": ["L. Jacob", "J. She", "A. Almahairi", "S. Rajeswar", "A. Courville"],
    "year": 2018
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "A geometric view of optimal transportation and generative model",
    "authors": ["N. Lei", "K. Su", "L. Cui", "Yau", "S.-T", "D.X. Gu"],
    "venue": "arXiv preprint arXiv:1710.05488,",
    "year": 2017
  }, {
    "title": "Adversarial computation of optimal transport maps",
    "authors": ["J. Leygonie", "J. She", "A. Almahairi", "S. Rajeswar", "A. Courville"],
    "venue": "arXiv preprint arXiv:1906.09691,",
    "year": 2019
  }, {
    "title": "Pacgan: The power of two samples in generative adversarial networks",
    "authors": ["Z. Lin", "A. Khetan", "G. Fanti", "S. Oh"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2018
  }, {
    "title": "Subspace detours: Building transport plans that are optimal on subspace projections",
    "authors": ["B. Muzellec", "M. Cuturi"],
    "venue": "arXiv preprint arXiv:1905.10099,",
    "year": 2019
  }, {
    "title": "Optimal transport with proximal splitting",
    "authors": ["N. Papadakis", "G. Peyré", "E. Oudet"],
    "venue": "SIAM Journal on Imaging Sciences,",
    "year": 2014
  }, {
    "title": "On the regularization of wasserstein gans",
    "authors": ["H. Petzka", "A. Fischer", "D. Lukovnicov"],
    "venue": "arXiv preprint arXiv:1709.08894,",
    "year": 2017
  }, {
    "title": "Wasserstein barycenter and its application to texture mixing",
    "authors": ["J. Rabin", "G. Peyré", "J. Delon", "M. Bernot"],
    "venue": "In International Conference on Scale Space and Variational Methods in Computer Vision,",
    "year": 2011
  }, {
    "title": "A nonparametric ensemble transform method for bayesian inference",
    "authors": ["S. Reich"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2013
  }, {
    "title": "Uncoupled isotonic regression via minimum wasserstein deconvolution",
    "authors": ["P. Rigollet", "J. Weed"],
    "venue": "arXiv preprint arXiv:1806.10648,",
    "year": 2018
  }, {
    "title": "Large-scale optimal transport and mapping estimation",
    "authors": ["V. Seguy", "B.B. Damodaran", "R. Flamary", "N. Courty", "A. Rolet", "M. Blondel"],
    "venue": "arXiv preprint arXiv:1711.02283,",
    "year": 2017
  }, {
    "title": "Optimal mass transport for shape matching and comparison",
    "authors": ["Z. Su", "Y. Wang", "R. Shi", "W. Zeng", "J. Sun", "F. Luo", "X. Gu"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2015
  }, {
    "title": "2-wasserstein approximation via restricted convex potentials with application to improved training for gans",
    "authors": ["A. Taghvaei", "A. Jalali"],
    "year": 1902
  }, {
    "title": "Topics in optimal transportation",
    "authors": ["C. Villani"],
    "venue": "Number 58. American Mathematical Soc.,",
    "year": 2003
  }, {
    "title": "Improving the improved training of wasserstein gans: A consistency term and its dual effect",
    "authors": ["X. Wei", "B. Gong", "Z. Liu", "W. Lu", "L. Wang"],
    "venue": "arXiv preprint arXiv:1803.01541,",
    "year": 2018
  }, {
    "title": "On scalable and efficient computation of large scale optimal transport",
    "authors": ["Y. Xie", "M. Chen", "H. Jiang", "T. Zhao", "H. Zha"],
    "year": 1905
  }, {
    "title": "Potential flow generator with l_2 optimal transport regularity for generative models",
    "authors": ["L. Yang", "G.E. Karniadakis"],
    "venue": "arXiv preprint arXiv:1908.11462,",
    "year": 2019
  }],
  "id": "SP:f9e4668a5111ade2dfe4097db4743c4ce1c04493",
  "authors": [{
    "name": "Ashok Vardhan Makkuva",
    "affiliations": []
  }, {
    "name": "Amirhossein Taghvaei",
    "affiliations": []
  }, {
    "name": "Jason D. Lee",
    "affiliations": []
  }, {
    "name": "Sewoong Oh",
    "affiliations": []
  }],
  "abstractText": "In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework to estimate the optimal transport mapping as the gradient of a convex function that is trained via minimax optimization. Numerical experiments confirm the accuracy of the learned transport map. Our approach can be readily used to train a deep generative model. When trained between a simple distribution in the latent space and a target distribution, the learned optimal transport map acts as a deep generative model. Although scaling this to a large dataset is challenging, we demonstrate two important strengths over standard adversarial training: robustness and discontinuity. As we seek the optimal transport, the learned generative model provides the same mapping regardless of how we initialize the neural networks. Further, a gradient of a neural network can easily represent discontinuous mappings, unlike standard neural networks that are constrained to be continuous. This allows the learned transport map to match any target distribution with many discontinuous supports and achieve sharp boundaries. Equal contribution. Order decided by a coin flip. Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign. Department of Mechanical and Aerospace Engineering, University of California, Irvine. Department of Electrical Engineering, Princeton University, Allen School of Computer Science & Engineering, University of Washington. Correspondence to: Ashok <makkuva2@illinois.edu>, Amir <amirhoseintghv@gmail.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s).",
  "title": "Optimal transport mapping via input convex neural networks"
}