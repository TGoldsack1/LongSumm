{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2074–2080, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution.\nThose semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish\nrival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses.\n(2) The Palestinian Authority condemned the attack(E3), saying it(E4) would divert international sympathy away from the far higher Palestinian civilian death toll.\nE1 corefers to E2, and E3 does to E4. E2 is more abstract than E1, and has less evidence of being an event. E4 is a pronoun, and thus may seem to refer to an entity rather than an event. Thus, E2 and E4 are relatively difficult to be recognized as events by themselves. However, event coreference E1-E2, which is supported primarily by E2’s participants Barclays and Zaragozano shared with E1, helps determine that E2 is an event. The same logic applies to E3 and E4. On the other hand, previous works typically rely on a pipelined model that extracts events (e.g., E1 and E3) at the first stage, and then resolves event coreference at the second stage. Although this modularity is preferable from development perspectives, the pipelined model limits the interactions. That is, the first stage alone is unlikely to detect E2 and E4 as events due to the difficulties described above. These missing events make it impossible for the second stage to resolve event coreference E1-E2 and E3-E4.\nIn this work, we address the problem using the ProcessBank corpus (Berant et al., 2014). Following the terminology defined in the corpus, we introduce several terms:\n• Event: an abstract representation of a change of state, independent from particular texts. • Event trigger: main word(s) in text, typically a verb or a noun that most clearly expresses an event. • Event arguments: participants or attributes in text, typically nouns, that are involved in an event. • Event mention: a clause in text that describes an event, and includes both a trigger and arguments. • Event coreference: a linguistic phenomenon that two event mentions refer to the same event.\nWe aim to explore the interactions between event mentions and event coreference. As a first step toward the goal, we focus on the task of identifying event triggers and resolving event coreference, and\n2074\npropose a document-level joint learning model using structured perceptron (Collins, 2002) that simultaneously predicts them. Our assumption is that the joint model is able to capture the interactions between event triggers and event coreference adequately, and such comprehensive decision improves the system performance. For instance, the joint model is likely to extract E2 as well as E1 successfully via their event coreference by simultaneously looking at coreference features.\nOur contributions are as follows: 1. This is the first work that simultaneously pre-\ndicts event triggers and event coreference using a single joint model. At the core of the model is a document-level structured perceptron algorithm that learns event triggers and event coreference jointly. 2. The incremental token-based prediction in joint decoding poses a challenge of synchronizing the assignments of event triggers and coreference. To avoid this problem, we propose an incremental decoding algorithm that combines the segment-based decoding and best-first clustering algorithm. 3. Our experiments indicate that the joint model achieves a substantial performance gain in event coreference resolution with a corpus in the biology domain, as compared to a pipelined model."
  }, {
    "heading": "2 Related Work",
    "text": "No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks.\nEvent extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference\ninto their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly.\nJoint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution."
  }, {
    "heading": "3 Approach",
    "text": "We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers."
  }, {
    "heading": "3.1 Corpus",
    "text": "The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corpus provides 150 paragraphs as training data, and we split them into 120 and 30 for our training and development, respectively. We chose ProcessBank instead of a larger corpus such as the Automatic Content Extraction (ACE) 2005 corpus for the following two reasons. First, the human annotation of event coreference links in ProcessBank enables us to apply the bestfirst clustering directly; on the other hand, this is\nnot readily feasible in ACE 2005 since it annotates event coreference as clusters, and gold standard event coreference links required for the bestfirst clustering are not available. Second, event coreference resolution using ProcessBank is novel since almost no previous work on the task used that corpus. The only exception could be (Berant et al., 2014), where they extracted several types of relations between event triggers, including event coreference. However, they did not report any performance scores of their system specifically on event coreference, and thus their work is not comparable to ours.\nUnlike previous work (Berant et al., 2014; Li et al., 2013), we explicitly allow an event trigger to have multiple tokens, such as verb phrase ‘look into’ and compound proper noun ‘World War II’. This is a more realistic setting for event trigger identification since in general there are a considerable number of multi-token event triggers1."
  }, {
    "heading": "3.2 Event Graph Learning",
    "text": "Let x denote an input document with n tokens where xi is the i-th token in the document. For event graph learning, we use structured perceptron (Collins, 2002), and average weights to reduce overfitting as suggested in (Collins, 2002). The algorithm involves decoding to generate the best event graph for each input document. We elaborate on our decoding algorithm in Section 3.3. Since an event graph has an exponentially large search space, we use beam search to approximate exact inference. We extract a range of features by using Stanford CoreNLP (Manning et al., 2014), MATE (Björkelund et al., 2009), OpenNLP2, Nomlex (Macleod et al., 1998), and Levin verb classes (Levin, 1993). For brevity, we provide details of the structured perceptron algorithm and features in the supplementary material.\nWe use the standard-update strategy in our structured perceptron model. As variants of structured perceptron, one could employ the early up-\n1For example, around 13.4% of the 1403 event triggers in ProcessBank have multiple tokens.\n2http://opennlp.apache.org/\ndate (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger identification and event coreference resolution is a much more difficult task since they require more complex knowledge and argument structures. Due to the difficultly of the task, it is also very difficult to develop such an effective feature set that beam search can explore the search space of an entire document thoroughly with early updates. This observation follows (Björkelund and Kuhn, 2014) on entity coreference resolution. In contrast, the maxviolation update showed almost the same performance as the standard update on the development data. From these results, we chose the standardupdate strategy for simplicity."
  }, {
    "heading": "3.3 Joint Decoding",
    "text": "Given that an event trigger has one or more tokens, event trigger identification could be solved as a token-level sequential labeling problem with BIO or BILOU scheme in the same way as named entity recognition (Ratinov and Roth, 2009). If one uses this approach, a beam state may represent a partial assignment of an event trigger. However, event coreference can be explored only from complete assignments of an event trigger. Thus, one would need to synchronize the search process of event coreference by comparing event coreferences from the complete assignment at a certain position with those from complete assignments at following positions. This makes it complicated to implement the formalization of token-level sequential labeling for joint decoding in our task. One possible way to avoid this problem is to extract event trigger candidates with a preference on high recall first, and then search event coreference from those candidates, regarding them as complete assignments of an event trigger. This recalloriented pre-filtering is often used in entity coreference resolution (Lee et al., 2013; Björkelund\nAlgorithm 1 Joint decoding for event triggers and coreference with beam search. Input: input document x = (x1, x2, . . . , xn) Input: beam width k, max length of event trigger lmax Output: best event graph ŷ for x 1: initialize empty beam history B[1..n] 2: for i← 1..n do 3: for l← 1..lmax do 4: for y ∈ B[i− l] do 5: e← CREATEEVENTTRIGGER(l, i). 6: APPENDEVENTTRIGGER(y, e) 7: B[i]← k-BEST(B[i] ∪ y) 8: for j ← 1..i− 1 do 9: c← CREATEEVENTCOREF(j, e). 10: ADDEVENTCOREF(y, c) 11: B[i]← k-BEST(B[i] ∪ y) 12: return B[n][0]\nand Farkas, 2012). In our initial experiments, we observed that our rule-based filter gained around 97% recall, but extracted around 12,400 false positives against 823 true positives in the training data. This made it difficult for our structured perceptron to learn event triggers, which underperformed on event coreference resolution.\nWe, therefore, employ segment-based decoding with multiple-beam search (Zhang and Clark, 2008a; Li and Ji, 2014) for event trigger identification, and combine it with the best-first clustering (Ng and Cardie, 2002) for event coreference resolution in document-level joint decoding. The key idea of segment-based decoding with multiple-beam search is to keep previous beam states available, and use them to form segments from previous positions to the current position. Let lmax denote the upper bound on the number of tokens in one event trigger. The k-best partial structures (event subgraphs) in beam B at the j-th token is computed as follows:\nB[j] = k-BEST y∈{y[1:j−l]∈B[j−l], y[j−l+1,j]=s}\nΦ(x, y) ·w\nwhere 1 ≤ l ≤ lmax, y[1:j] is an event subgraph ending at the j-th token, and y[j−l+1,j] = s means that partial structure y[j−l+1,j] is a segment, i.e., an event trigger candidate with a subsequence of tokens x[j−l+1,j]. This approximates Viterbi decoding with beam search.\nThe best-first clustering incrementally makes coreference decisions by selecting the most likely antecedent for each trigger. Our joint decoding algorithm makes use of the incremental process to combine the segment-based decoding and bestfirst clustering. Algorithm 1 shows the summary of the joint decoding algorithm. Line 3 - 7 implements the segment-based decoding, and line 8 - 11\nimplements the best-first clustering. Once a new event trigger is appended to an event subgraph at line 6, the decoder uses it as a referring mention regardless of whether the event subgraph is in the beam, and seeks the best antecedent for it. This enables the joint model to make a more global decision on event trigger identification and event coreference decision, as described in Section 1."
  }, {
    "heading": "4 Experimental Settings",
    "text": "When training our model, we observed that 20- iteration training almost reached convergence, and thus we set the number of iterations to 20. We set lmax to 6 because we observed that the longest event trigger in the entire ProcessBank corpus has six tokens. When tuning beam width k on the development set, large beam width did not give us a significant performance difference. We attribute this result to the small size of the development data. In particular, the development data has only 28 event coreferences, which makes it difficult to reveal the effect of beam width. We thus set k to 1 in our experiments."
  }, {
    "heading": "4.1 Baseline Systems",
    "text": "Our baseline is a pipelined model that divides the event trigger decoding and event coreference decoding in Algorithm 1 into two separate stages. It uses the same structured perceptron with the same hyperparameters and feature templates. We choose this baseline because it clearly reveals the effectiveness of the joint model by focusing only on the architectural difference. One could develop other baseline systems. One of them is a deterministic sieve-based approach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work."
  }, {
    "heading": "4.2 Evaluation",
    "text": "We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and\nBLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the average of MUC F1, B3 F1, and CEAFe F1."
  }, {
    "heading": "5 Results and Discussions",
    "text": "We first show the result of event coreference resolution on the test data in Table 2. The joint model outperforms the baseline by 6.9 BLANC F1 and 1.8 CoNLL F1 points. We observed that this overall performance gain comes largely from a precision gain, more specifically, substantially reduced false positives. We explain the superiority of the joint model as follows. In the baseline, the second stage uses the output of the first stage. Since event triggers are fixed at this point, the baseline explores coreference links only between these event triggers. In contrast, the joint model seeks event triggers and event coreference simultaneously, and thus it explores a larger number of false positives in the search process, thereby learning to penalize false positives more adequately than the baseline.\nTable 3 shows the results of event trigger identification on the test data. We observed that the joint model also reduced false positives, similarly in event coreference resolution. However, its improvement on precision is small, ending up with almost the same F1 point as the baseline. We speculate that this is due to the small size of the corpus, and the joint model was unable to show its advantages in event trigger identification.\nBelow are two error cases in event coreference resolution, where our model fails to resolve E5E6 and E7-E8. The model was unable to adequately extract features for both event triggers and event coreference, particularly because their surface strings are not present in training data, they are lexically and syntactically different, and they\ndo not share key semantic roles (e.g., agents and patients) in a clear argument structure.\n(3) When the cell is stimulated, gated channels open that facilitate Na+ diffusion(E5). Sodium ions then ”fall”(E6) down their electrochemical gradient, . . .\n(4) The next seven steps decompose(E7) the citrate back to oxaloacetate. It is this regeneration(E8) of oxaloacetate that makes this process a cycle."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "We present a joint structured prediction model for event trigger identification and event coreference resolution. To our knowledge, this is the first work that solves these two tasks simultaneously. Our experiment shows that the proposed method effectively penalizes false positives in joint search, thereby outperforming a pipelined model substantially in event coreference resolution.\nThere are a number of avenues for future work. One can further ensure the advantage of the joint model using a larger corpus. Our preliminary experiment on the ACE 2005 corpus shows that due to its larger document size and event types, one will need to reduce training time by a distributed learning algorithm such as mini-batches (Zhao and Huang, 2013). Another future work is to incorporate other components of events into the model. These include event types, event arguments, and other relations such as subevents. One could leverage them as other learning targets or constraints, and investigate further benefits of joint modeling."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the Deep Exploration and Filtering of Text (DEFT) Program, and by U.S. Army Research Office (ARO) grant W911NF-14-1-0436 under the Reading, Extraction, and Assembly of Pathways for Evidentiary Reading (REAPER) Program. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, ARO, or the U.S. government. Jun Araki is partly supported by a Funai Overseas Scholarship."
  }],
  "year": 2015,
  "references": [{
    "title": "Topic Detection and Tracking: Event-based Information Organization",
    "authors": ["James Allan."],
    "venue": "Kluwer Academic Publishers.",
    "year": 2002
  }, {
    "title": "Algorithms for scoring coreference chains",
    "authors": ["Amit Bagga", "Breck Baldwin."],
    "venue": "Proceedings of LREC 1998 Workshop on Linguistics Coreference, pages 563–566.",
    "year": 1998
  }, {
    "title": "Unsupervised event coreference resolution",
    "authors": ["Cosmin Adrian Bejan", "Sanda M. Harabagiu."],
    "venue": "Computational Linguistics, 40(2):311–347.",
    "year": 2014
  }, {
    "title": "Modeling biological processes for reading comprehension",
    "authors": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning."],
    "venue": "Proceedings of EMNLP 2014,",
    "year": 2014
  }, {
    "title": "Event matching using the transitive closure of dependency relations",
    "authors": ["Daniel M. Bikel", "Vittorio Castelli."],
    "venue": "Proceedings of ACL 2008, pages 145– 148.",
    "year": 2008
  }, {
    "title": "Datadriven multilingual coreference resolution using resolver stacking",
    "authors": ["Anders Björkelund", "Richárd Farkas."],
    "venue": "Proceedings of EMNLP/CoNLL 2012, pages 49–55.",
    "year": 2012
  }, {
    "title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features",
    "authors": ["Anders Björkelund", "Jonas Kuhn."],
    "venue": "Proceedings of ACL 2014, pages 47–57.",
    "year": 2014
  }, {
    "title": "Multilingual semantic role labeling",
    "authors": ["Anders Björkelund", "Love Hafdell", "Pierre Nugues."],
    "venue": "Proceedings of CoNLL 2009, pages 43–48.",
    "year": 2009
  }, {
    "title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing",
    "authors": ["Bernd Bohnet", "Joakim Nivre."],
    "venue": "Proceedings of EMNLP/CoNLL 2012, pages 1455– 1465.",
    "year": 2012
  }, {
    "title": "Biology",
    "authors": ["Neil Campbell", "Jane Reece."],
    "venue": "Benjamin Cummings.",
    "year": 2005
  }, {
    "title": "Incremental parsing with the perceptron algorithm",
    "authors": ["Michael Collins", "Brian Roark."],
    "venue": "Proceedings of ACL 2004, pages 111–118.",
    "year": 2004
  }, {
    "title": "Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "Proceedings of EMNLP 2002, pages 1–8.",
    "year": 2002
  }, {
    "title": "Finding contradictions in text",
    "authors": ["Marie-Catherine de Marneffe", "Anna N. Rafferty", "Christopher D. Manning."],
    "venue": "Proceedings of ACL-HLT 2008, pages 1039–1047.",
    "year": 2008
  }, {
    "title": "Global joint models for coreference resolution and named entity classification",
    "authors": ["Pascal Denis", "Jason Baldridge."],
    "venue": "Procesamiento del Lenguaje Natural, 42:87–96.",
    "year": 2009
  }, {
    "title": "Using cross-entity inference to improve event extraction",
    "authors": ["Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu."],
    "venue": "Proceedings of ACL-HLT 2011, pages 1127–1136.",
    "year": 2011
  }, {
    "title": "Structured perceptron with inexact search",
    "authors": ["Liang Huang", "Suphan Fayong", "Yang Guo."],
    "venue": "Proceedings of NAACL-HLT 2012, pages 142–151.",
    "year": 2012
  }, {
    "title": "Refining event extraction through cross-document inference",
    "authors": ["Heng Ji", "Ralph Grishman."],
    "venue": "Proceedings of ACL-HLT 2008, pages 254–262.",
    "year": 2008
  }, {
    "title": "Knowledge base population: Successful approaches and challenges",
    "authors": ["Heng Ji", "Ralph Grishman."],
    "venue": "Proceedings of ACL-HLT 2011, pages 1148–1158.",
    "year": 2011
  }, {
    "title": "Dependency-based semantic role labeling of PropBank",
    "authors": ["Richard Johansson", "Pierre Nugues."],
    "venue": "Proceedings of EMNLP 2008, pages 69–",
    "year": 2008
  }, {
    "title": "Joint entity and event coreference resolution across documents",
    "authors": ["Heeyoung Lee", "Marta Recasens", "Angel Chang", "Mihai Surdeanu", "Dan Jurafsky."],
    "venue": "Proceedings of EMNLP/CoNLL 2012, pages 489– 500.",
    "year": 2012
  }, {
    "title": "Deterministic coreference resolution based on entity-centric, precision-ranked rules",
    "authors": ["Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky."],
    "venue": "Computational Linguistics, 39(4):885–916.",
    "year": 2013
  }, {
    "title": "English Verb Classes and Alternation: A Preliminary Investigation",
    "authors": ["Beth Levin."],
    "venue": "The University of Chicago Press.",
    "year": 1993
  }, {
    "title": "Incremental joint extraction of entity mentions and relations",
    "authors": ["Qi Li", "Heng Ji."],
    "venue": "Proceedings of ACL 2014, pages 402–412.",
    "year": 2014
  }, {
    "title": "Extractive summarization using inter- and intra- event relevance",
    "authors": ["Wenjie Li", "Mingli Wu", "Qin Lu", "Wei Xu", "Chunfa Yuan."],
    "venue": "Proceedings of ACL/COLING 2006, pages 369–376.",
    "year": 2006
  }, {
    "title": "Joint event extraction via structured prediction with global features",
    "authors": ["Qi Li", "Heng Ji", "Liang Huang."],
    "venue": "Proceedings of ACL 2013, pages 73–82.",
    "year": 2013
  }, {
    "title": "Using document level cross-event inference to improve event extraction",
    "authors": ["Shasha Liao", "Ralph Grishman."],
    "venue": "Proceedings of ACL 2010, pages 789– 797.",
    "year": 2010
  }, {
    "title": "Supervised within-document event coreference using information propagation",
    "authors": ["Zhengzhong Liu", "Jun Araki", "Eduard Hovy", "Teruko Mitamura."],
    "venue": "Proceedings of LREC 2014.",
    "year": 2014
  }, {
    "title": "An extension of BLANC to system mentions",
    "authors": ["Xiaoqiang Luo", "Sameer Pradhan", "Marta Recasens", "Eduard Hovy."],
    "venue": "Proceedings of ACL 2014, pages 24–29.",
    "year": 2014
  }, {
    "title": "On coreference resolution performance metrics",
    "authors": ["Xiaoqiang Luo."],
    "venue": "Proceedings of HLT/EMNLP 2005, pages 25–32.",
    "year": 2005
  }, {
    "title": "Nomlex: A lexicon of nominalizations",
    "authors": ["Catherine Macleod", "Ralph Grishman", "Adam Meyers", "Leslie Barrett", "Ruth Reeves."],
    "venue": "Proceedings of EURALEX 1998, pages 187–193.",
    "year": 1998
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."],
    "venue": "Proceedings ACL 2014: System Demonstrations, pages 55–60.",
    "year": 2014
  }, {
    "title": "Event extraction as dependency parsing",
    "authors": ["David McClosky", "Mihai Surdeanu", "Christopher Manning."],
    "venue": "Proceedings of ACL-HLT 2011, pages 1626–1635.",
    "year": 2011
  }, {
    "title": "Improving machine learning approaches to coreference resolution",
    "authors": ["Vincent Ng", "Claire Cardie."],
    "venue": "Proceedings of ACL 2002, pages 104–111.",
    "year": 2002
  }, {
    "title": "Joint inference for knowledge extraction from biomedical literature",
    "authors": ["Hoifung Poon", "Lucy Vanderwende."],
    "venue": "Proceedings of NAACL-HLT 2010, pages 813–821.",
    "year": 2010
  }, {
    "title": "Scoring coreference partitions of predicted mentions: A reference implementation",
    "authors": ["Sameer Pradhan", "Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube."],
    "venue": "Proceedings of ACL 2014, pages 30–35.",
    "year": 2014
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["Lev Ratinov", "Dan Roth."],
    "venue": "Proceedings of CoNLL 2009, pages 147–155.",
    "year": 2009
  }, {
    "title": "BLANC: Implementing the Rand index for coreference evaluation",
    "authors": ["Marta Recasens", "Eduard Hovy."],
    "venue": "Natural Language Engineering, 17(4):485– 510.",
    "year": 2011
  }, {
    "title": "Fast and robust joint models for biomedical event extraction",
    "authors": ["Sebastian Riedel", "Andrew McCallum."],
    "venue": "Proceedings of EMNLP 2011, pages 1–12.",
    "year": 2011
  }, {
    "title": "Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features",
    "authors": ["Deepak Venugopal", "Chen Chen", "Vibhav Gogate", "Vincent Ng."],
    "venue": "Proceedings of EMNLP 2014, pages 831–843.",
    "year": 2014
  }, {
    "title": "A modeltheoretic coreference scoring scheme",
    "authors": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."],
    "venue": "Proceedings of MUC-6, pages 45–52.",
    "year": 1995
  }, {
    "title": "Joint word segmentation and POS tagging using a single perceptron",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of ACL-HLT 2008, pages 888– 896.",
    "year": 2008
  }, {
    "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of EMNLP 2008, pages 562–571.",
    "year": 2008
  }, {
    "title": "Minibatch and parallelization for online large margin structured learning",
    "authors": ["Kai Zhao", "Liang Huang."],
    "venue": "Proceedings of NAACL-HLT 2013, pages 370–379. 2080",
    "year": 2013
  }],
  "id": "SP:6f146e69fcebfb07ec1625b9e5591b0f7f483a42",
  "authors": [{
    "name": "Jun Araki",
    "affiliations": []
  }, {
    "name": "Teruko Mitamura",
    "affiliations": []
  }],
  "abstractText": "Events and their coreference offer useful semantic and discourse resources. We show that the semantic and discourse aspects of events interact with each other. However, traditional approaches addressed event extraction and event coreference resolution either separately or sequentially, which limits their interactions. This paper proposes a document-level structured learning model that simultaneously identifies event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain.",
  "title": "Joint Event Trigger Identification and Event Coreference Resolution with Structured Perceptron"
}