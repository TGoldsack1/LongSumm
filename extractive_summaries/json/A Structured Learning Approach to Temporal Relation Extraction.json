{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention. Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing.\nThe fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction\nand normalization and 2) temporal relation (also known as TLINKs (Pustejovsky et al., 2003a)) extraction. While the first task has now been well handled by the state-of-the-art systems (HeidelTime (Strötgen and Gertz, 2010), SUTime (Chang and Manning, 2012), IllinoisTime (Zhao et al., 2012), NavyTime (Chambers, 2013), UWTime (Lee et al., 2014), etc.) with end-to-end F1 scores being around 80%, the second task has long been a challenging one; even the top systems only achieved F1 scores of around 35% in the TE workshops.\nThe goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them. The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of the temporal relations are missing in human annotation. In this paper, we propose a structured learning approach to temporal relation extraction, where local models are updated based on feedback from global inferences. The structured approach also gives rise to a semisupervised method, making it possible to take advantage of the readily available unlabeled data. As a byproduct, this approach further provides a new, effective perspective on handling those missing relations.\nIn the common formulations, temporal relations are categorized into three types: the E-E TLINKs (those between a pair of events), the T-T TLINKs (those between a pair of timexes), and the E-T TLINKs (those between an event and a timex). While the proposed approach can be generally applied to all three types, this paper focuses on the majority type, i.e., the E-E TLINKs. For example, consider the following snippet taken from the\nar X\niv :1\n90 6.\n04 94\n3v 1\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n19\ntraining set provided in the TE3 workshop. We want to construct a temporal graph as in Fig. 1 for the events in boldface in Ex1.\nEx1 . . . tons of earth cascaded down a hillside, ripping two houses from their foundations. No one was hurt, but firefighters ordered the evacuation of nearby homes and said they’ll monitor the shifting ground.. . .\nAs discussed in existing work (Verhagen, 2004; Bramsen et al., 2006; Mani et al., 2006; Chambers and Jurafsky, 2008), the structure of a temporal graph is constrained by some rather simple rules:\n1. Symmetry. For example, if A is before B, then B must be after A.\n2. Transitivity. For example, if A is before B and B is before C, then A must be before C.\nThis particular structure of a temporal graph (especially the transitivity structure) makes its nodes highly interrelated, as can be seen from Fig. 1. It is thus very challenging to identify the TLINKs between them, even for human annotators: The inter-annotator agreement on TLINKs is usually about 50%-60% (Mani et al., 2006). Fig. 2 shows the actual human annotations provided by TE3. Among all the ten possible pairs of nodes, only three TLINKs were annotated. Even if we only look at main events in consecutive sentences and at events in the same sentence, there are still quite a few missing TLINKs, e.g., the one between hurt and cascaded and the one between monitor and ordered.\nEarly attempts by Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events. State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime\n(Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results. However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph). Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs. Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005). In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline. The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve. This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008).\nAlthough L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning). In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations. Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses. More importantly, they compared structured learning to local baselines, while we find that the comparison between structured learning and L+I is more interesting and important for\nunderstanding the effect of global considerations in the learning phase. In difference from existing methods, we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way. Our solution targets on these issues and, as we show, achieves significant improvements on two commonly used evaluation sets.\nThe rest of this paper is organized as follows. Section 2 clarifies the temporal relation types and the evaluation metric of a temporal graph used in this paper, Section 3 explains the structured learning approach in detail, and Section 4 discusses the practical issue of missing relations. We provide experiments and discussion in Section 5 and conclusion in Section 6."
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 Temporal Relation Types",
    "text": "Existing corpora for temporal processing often follows the interval representation of events proposed in Allen (1984), and makes use of 13 relation types in total. In many systems, vague or none is also included as another relation type when a TLINK is not clear or missing. However, current systems usually use a reduced set of relation types, mainly due to the following reasons.\n1. The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others (see Table 1 in Mani et al. (2006)). For example, relations such as immediately before or immediately after barely exist in a corpus compared to before and after.\n2. Due to the ambiguity in natural language, determining relations like before and immediately before can be a difficult task itself (Chambers et al., 2014).\nIn this work, we follow the reduced set of temporal relation types used in CAEVO (Chambers et al., 2014): before, after, includes, is included, equal, and vague."
  }, {
    "heading": "2.2 Quality of A Temporal Graph",
    "text": "The most recent evaluation metric in TE3, i.e., the temporal awareness (UzZaman and Allen, 2011), is adopted in this work. Specifically, let Gsys and Gtrue be two temporal graphs from the system prediction and the ground truth, respectively. The\nprecision and recall of temporal awareness are defined as follows.\nP = |G−sys ∩G+true| |G−sys| , R = |G−true ∩G+sys| |G−true|\nwhere G+ is the closure of graph G, G− is the reduction of G, “∩” is the intersection between TLINKs in two graphs, and |G| is the number of TLINKs in G. The temporal awareness metric better captures how “useful” a temporal graph is. For example, if system 1 produces ripping is before hurt and hurt is before monitor, and system 2 adds ripping is before monitor on top of system 1. Since system 2 is simply a transitive closure of system 1, they would have the same evaluation scores. Note that vague relations are usually considered as non-existing TLINKs and are not counted during evaluation."
  }, {
    "heading": "3 A Structured Training Approach",
    "text": "As shown in Fig. 1, the learning problem in temporal relation extraction is global in nature. Even the top local method in TE3, UTTime (Laokulrat et al., 2013), only achieved F1=56.5 when presented with a pair of temporal entities (Task C– relation only (UzZaman et al., 2013)). Since the success of an L+I method strongly relies on the quality of the local classifiers, a poor local classifier is obviously a roadblock for L+I methods. Following the insights from Punyakanok et al. (2005), we propose to use a structured learning approach (also called “Inference Based Training” (IBT)).\nUnlike the current L+I approach, where local classifiers are trained independently beforehand without knowledge of the predictions on neighboring pairs, we train local classifiers with feedback that accounts for other relations, by performing global inference in each round of the learning process. In order to introduce the structured learning algorithm, we first explain its most important component, the global inference step."
  }, {
    "heading": "3.1 Inference",
    "text": "In a document with n pairs of events, let φi ∈ X ⊆ Rd be the extracted d-dimensional feature and yi ∈ Y be the temporal relation for the i-th pair of events, i = 1, 2, . . . , n, where Y = {rj}6j=1 is the label set for the six temporal relations we use. Moreover, let x = {φ1, . . . , φn} ∈ X n and y = {y1, . . . , yn} ∈ Yn be more compact representations of all the features and labels in this\ndocument. Given the weight vector wr of a linear classifier trained for relation r ∈ Y (i.e., using the one-vs-all scheme), the global inference step is to solve the following constrained optimization problem:\nŷ = arg max y∈C(Yn) f(x,y), (1)\nwhere C(Yn) ⊆ Yn constrains the temporal graph to be symmetrically and transitively consistent, and f(x,y) is the scoring function:\nf(x,y) = n∑ i=1 fyi(φi) = n∑ i=1 ew T yi φi∑ r∈Y e wTr φi .\nSpecifically, fyi(φi) is the probability of the i-th event pair having relation yi. f(x, y) is simply the sum of these probabilities over all the event pairs in a document, which we think of as the confidence of assigning y = {y1, ..., yn} to this document and therefore, it needs to be maximized in Eq. (1).\nNote that when C(Yn) = Yn, Eq. (1) can be solved for each ŷi independently, which is what the so-called local methods do, but the resulting ŷ may not satisfy global consistency in this way. When C(Yn) 6= Yn, Eq. (1) cannot be decoupled for each ŷi and is usually formulated as an ILP problem (Roth and Yih, 2004; Chambers and Jurafsky, 2008; Do et al., 2012). Specifically, let Ir(ij) ∈ {0, 1} be the indicator function of relation r for event i and event j and fr(ij) ∈ [0, 1] be the corresponding soft-max score. Then the ILP objective for global inference is formulated as follows.\nÎ = argmax I\n∑ ij∈E ∑ r∈Y fr(ij)Ir(ij) (2)\ns.t. ΣrIr(ij) = 1 (uniqueness) , Ir(ij) = Ir̄(ji), (symmetry)\nIr1(ij) + Ir2(jk)− ΣNm=1Irm3 (ik) ≤ 1, (transitivity)\nfor all distinct events i, j, and k, where E = {ij | sentence dist(i, j)≤ 1}, r̄ is the reverse of r, and N is the number of possible relations for r3 when r1 and r2 are true.\nOur formulation in Eq. (2) is different from previous work (Chambers and Jurafsky, 2008; Do et al., 2012) in two aspects: 1) We restrict our event pairs ij to a smaller set E = {ij | sentence dist(i, j)≤ 1} where pairs that are\nmore than one sentence away are deleted for computational efficiency and (usually) for better performance. In fact, to make better use of global constraints, we should have allowed more event pairs in Eq. (2). However, fr(ij) is usually more reliable when i and j are closer in text. Many participating systems in TE3 (UzZaman et al., 2013) have used this pre-filtering strategy to balance the trade-off between confidence in fr(ij) and global constraints. We observe that the strategy fits very well to the existing datasets: As shown in Fig. 3, annotated TLINKs barely exist if two events are two sentences away. 2) Previously, transitivity constraints were formulated as Ir1(ij) + Ir2(jk) − Ir3(ik) ≤ 1, which is a special case when N = 1 and can be understood as “r1 and r2 determine a single r3”. However, it was overlooked that, although some r1 and r2 cannot uniquely determine r3, they can still constrain the set of labels r3 can take. For example, as shown in Fig. 4, when r1=before and r2=is included, r3 is not determined but we know that r3 ∈ {before, is included}1. This information can be easily exploited by allowing N > 1.\nWith these two differences, the optimization problem (2) can still be efficiently solved using off-the-shelf ILP packages such as GUROBI\n1The transitivity table in Allen (1983) shows two more possible relations, overlap and immediately before, which are not in our label set.\n(Gurobi Optimization, Inc., 2012)."
  }, {
    "heading": "3.2 Learning",
    "text": "With the inference solver defined above, we propose to use the structured perceptron (Collins, 2002) as a representative for the inference based training (IBT) algorithm to learn those weight vectors wr. Specifically, let L = {xk,yk}Kk=1 be the labeled training set of K instances (usually documents). The structured perceptron training algorithm for this problem is shown in Algorithm 1. The Illinois-SL package (Chang et al., 2010) was used in our experiments for its structured perceptron component. In terms of the features used in this work, we adopt the same set of features designed for E-E TLINKs in Sec. 3.1 of Do et al. (2012).\nIn Algorithm 1, Line 6 is the inference step as in Eq. (1) or (2), which is augmented with a closure operation on ŷ in the following line. In the case in which there is only one pair of events in each instance (thus no structure to take advantage of), Algorithm 1 reduces to the conventional perceptron algorithm and Line 6 simply chooses the top scoring label. With a structured instance instead, Line 6 becomes slower to solve, but it can provide valuable information so that the perceptron learner is able to look further at other labels rather than an isolated pair. For example in Ex1 and Fig. 1, the fact that (ripping,ordered)=before is established through two other relations: 1) ripping is an adverbial participle and thus included in cascaded and 2) cascaded is before ordered. If (ripping,ordered)=before is presented to a local learning algorithm without knowing its predictions on (ripping,cascaded) and (cascaded,ordered), then the model either cannot support it or overfits it. In IBT, however, if the classifier was correct in deciding (ripping,cascaded) and (cascaded,ordered), then (ripping,ordered) would be correct automatically and would not contribute to updating the classifier."
  }, {
    "heading": "3.3 Semi-supervised Structured Learning",
    "text": "The scarcity of training data and the difficulty in annotation have long been a bottleneck for temporal processing systems. Given the inherent global constraints in temporal graphs, we propose to perform semi-supervised structured learning using the constraint-driven learning (CoDL) algorithm (Chang et al., 2007, 2012), as shown in Algorithm 2, where the function “Learn” in Lines 2 and 9 represents any standard learning algorithm\nAlgorithm 1: Structured perceptron algorithm for temporal relations\nInput: Training set L = {xk,yk}Kk=1, learning rate λ\n1 Perform graph closure on each yk 2 Initialize wr = 0, ∀r ∈ Y 3 while convergence criteria not satisfied do 4 Shuffle the examples in L 5 foreach (x,y) ∈ L do 6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 if ŷ 6= y then 9 wr = wr + λ( ∑ i:yi=r\nφi−∑ i:ŷi=r φi), ∀r ∈ Y\n10 return {wr}r∈Y\n(e.g., perceptron, SVM, or even structured perceptron; here we used the averaged perceptron (Freund and Schapire, 1998)) and subscript “r” means selecting the learned weight vector for relation r ∈ Y . CoDL improves the model learned from a small amount of labeled data by repeatedly generating feedback through labeling unlabeled examples, which is in fact a semi-supervised version of IBT. Experiments show that this scheme is indeed helpful in this problem.\nAlgorithm 2: Constraint-driven learning algorithm Input: Labeled set L, unlabeled set U ,\nweighting coefficient γ 1 Perform closure on each graph in L 2 Initialize wr = Learn(L)r, ∀ r ∈ Y 3 while convergence criteria not satisfied do 4 T = ∅ 5 foreach x ∈ U do 6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 T = T ∪ {(x, ŷ)} 9 wr = γwr + (1− γ)Learn(T )r,∀ r ∈ Y\n10 return {wr}r∈Y"
  }, {
    "heading": "4 Missing Annotations",
    "text": "Since even human annotators find it difficult to annotate temporal graphs, many of the TLINKs are left unspecified by annotators (compare Fig. 2 to Fig. 1). While some of these missing TLINKs can be inferred from existing ones, the vast majority still remain unknown as shown in Table 1. De-\nspite the existence of denser annotation schemes (e.g., Cassidy et al. (2014)), the TLINK annotation task is quadratic in the number of nodes, and it is practically infeasible to annotate complete graphs. Therefore, the problem of identifying these unknown relations in training and test is a major issue that dramatically hurts existing methods.\nWe could simply use these unknown pairs (or some filtered version of them) to design rules or train classifiers to identify whether a TLINK is vague or not. However, we propose to exclude both the unknown pairs and the vague classifier from the training process – by changing the structured loss function to ignore the inference feedback on vague TLINKs (see Line 9 in Algorithm 1 and Line 9 in Algorithm 2). The reasons are discussed below.\nFirst, it is believed that a lot of the unknown pairs are not really vague but rather pairs that the annotators failed to look at (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014). For example, (cascaded, monitor) should be annotated as before but is missing in Fig. 2. It is hard to exclude this noise in the data during training. Second, compared to the overwhelmingly large number of unknown TLINKs (89.5% as shown in Table 1), the scarcity of non-vague TLINKs makes it hard to learn a good vague classifier. Third, vague is fundamentally different from the other relation types. For example, if a before TLINK can be established given a sentence, then it always holds as before regardless of other events around it, but if a TLINK is vague given a sentence, it may still change to other types afterwards if a connection can later be established through other nodes from the context. This distinction emphasizes that vague is a consequence of lack of background/contextual information, rather than a concrete relation type to be trained on. Fourth, without the vague classifier, the predicted temporal graph tends to become more densely connected, thus the global transitivity constraints can be more effective in correcting local mistakes (Chambers\nand Jurafsky, 2008). However, excluding the local classifier for vague TLINKs would undesirably assign nonvague TLINKs to every pair of events. To handle this, we take a closer look at the vague TLINKs. We note that a vague TLINK could arise in two situations if the annotators did not fail to look at it. One is that an annotator looks at this pair of events and decides that multiple relations can exist, and the other one is that two annotators disagree on the relation (similar arguments were also made in Cassidy et al. (2014)). In both situations, the annotators first try to assign all possible relations to a TLINK, and then change the relation to vague if more than one can be assigned. This human annotation process for vague is different from many existing methods, which either identify the existence of a TLINK first (using rules or machinelearned classifiers) and then classify, or directly include vague as a classification label along with other non-vague relations.\nIn this work, however, we propose to mimic this mental process by a post-filtering method2. Specifically, we take each TLINK produced by ILP and determine whether it is vague using its relative entropy (the Kullback-Leibler divergence) to the uniform distribution. Let {rm}Mm=1 be the set of relations that the i-th pair of events can take, we filter the i-th TLINK given by ILP by:\nδi = M∑ m=1 frm(φi) log (Mfrm(φi)),\nwhere frm(φi) is the soft-max score of rm, obtained by the local classifier for rm. We then compare δi to a fixed threshold τ to determine the vagueness of this TLINK; we accept its originally predicted label if δi > τ , or change it to vague otherwise. Using relative entropy here is intuitively appealing and empirically useful as shown in the experiments section; better metrics are of course yet to be designed."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Datasets",
    "text": "The TempEval3 (TE3) workshop (UzZaman et al., 2013) provided the TimeBank (TB) (Pustejovsky et al., 2003b), AQUAINT (AQ) (Graff, 2002), Silver (TE3-SV), and Platinum (TE3-PT) datasets,\n2Some systems (e.g., TARSQI (Verhagen and Pustejovsky, 2008)) employed a similar idea from a different standpoint, by thresholding TLINKs based on confidence scores.\nwhere TB and AQ are usually for training, and TE3-PT is usually for testing. The TE3-SV dataset is a much larger, machine-annotated and automatically-merged dataset based on multiple systems, with the intention to see if these “silver” standard data can help when included in training (although almost all participating systems saw performance drop with TE3-SV included in training).\nTwo popular augmentations on TB are the VerbClause temporal relation dataset (VC) and TimebankDense dataset (TD). The VC dataset has specially annotated event pairs that follow the socalled Verb-Clause structure (Bethard et al., 2007), which is usually beneficial to be included in training (UzZaman et al., 2013). The TD dataset contains 36 documents from TB which were reannotated using the dense event ordering framework proposed in Cassidy et al. (2014). The experiments included in this paper will involve the TE3 datasets as well as these augmentations. Therefore, some statistics on them are shown in Table 2 for the readers’ information."
  }, {
    "heading": "5.2 Baseline Methods",
    "text": "In addition to the state-of-the-art systems, another two baseline methods were also implemented for a better understanding of the proposed ones. The first is the regularized averaged perceptron (AP) (Freund and Schapire, 1998) implemented in the LBJava package (Rizzolo and Roth, 2010) and is a local method. On top of the first baseline, we performed global inference in Eq.(2), referred to as the L+I baseline (AP+ILP). Both of them used the same feature set (i.e., as designed in Do et al. (2012)) as in the proposed structured perceptron (SP) and CoDL for fair comparisons. To clarify,\nSP and CoDL are training algorithms and their immediate outputs are the weight vectors {wr}r∈Y for local classifiers. An ILP inference was performed on top of them to yield the final output, and we refer to it as “S+I” (i.e., structured learning+inference) methods."
  }, {
    "heading": "5.3 Results and Discussion",
    "text": ""
  }, {
    "heading": "5.3.1 TE3 Task C - Relation Only",
    "text": "To show the benefit of using structured learning, we first tested one scenario where the gold pairs of events that have a non-vague TLINK were known priori. This setup was a standard task presented in TE3, so that the difficulty of detecting vague TLINKs was ruled out. This setup also helps circumvent the issue that TE3 penalizes systems which assign extra labels that do not exist in the annotated graph, while these extra labels may be actually correct because the annotation itself might be incomplete. UTTime (Laokulrat et al., 2013) was the top system in this task in TE3. Since UTTime is not available to us, and its performance was reported in TE3 in terms of both E-E and E-T TLINKs together, we locally trained an E-T classifier based on Do et al. (2012) and included its prediction only for fair comparison.\nUTTime is a local method and was trained on TB+AQ and tested on TE3-PT. We used the same datasets for our local baseline and its performance is shown in Table 3 under the name “AP-1”. Note that the reported numbers below are the temporal awareness scores obtained from the official evaluation script provided in TE3. We can see that UTTime is about 3% better than AP-1 in the absolute value of F1, which is expected since UTTime included more advanced features derived from syntactic parse trees. By adding the VC and TD datasets into the training set, we retrained our local baseline and achieved comparable performance to\nUTTime (“AP-2” in Table 3). On top of AP-2, a global inference step enforcing symmetry and transitivity constraints (“AP+ILP”) can further improve the F1 score by 9.3%, which is consistent with previous observations (Chambers and Jurafsky, 2008; Do et al., 2012). SP+ILP further improved the performance in precision, recall, and F1 significantly (per the McNemar’s test (Everitt, 1992; Dietterich, 1998) with p <0.0005), reaching an F1 score of 67.2%. This meets our expectation that structured learning can be better when the local problem is difficult (Punyakanok et al., 2005)."
  }, {
    "heading": "5.3.2 TE3 Task C",
    "text": "In the first scenario, we knew in advance which TLINKs existed or not, so the “pre-filtering” (i.e., ignoring distant pairs as mentioned in Sec. 3.1 and “post-filtering” methods were not used when generating the results in Table 3. We then tested a more practical scenario, where we only knew the events, but did not know which ones are related. This setup was Task C in TE3 and the top system was ClearTK (Bethard, 2013). Again, for fair comparison, we simply added the E-T TLINKs predicted by ClearTK. Moreover, 10% of the training data was held out for development. Corresponding results on the TE3-PT testset are shown in Table 4.\nFrom lines 2-4, all systems see significant drops in performance if compared with the same entries in Table 3. It confirms our assertion that how to handle vague TLINKs is a major issue for this temporal relation extraction problem. The improvement of SP+ILP (line 4) over AP (line 2) was small and AP+ILP (line 3) was even worse than AP, which necessitates the use of a better approach\ntowards vague TLINKs. By applying the postfiltering method proposed in Sec. 4, we were able to achieve better performances using SP+ILP (line 5), which shows the effectiveness of this strategy. Finally, by setting U in Algorithm 2 to be the TE3-SV dataset, CoDL+ILP (line 6) achieved the best F1 score with a relative improvement over ClearTK being 14.8%. Note that when using TE3SV in this paper, we did not use its annotations on TLINKs because of its well-known large noise (UzZaman et al., 2013).\nIn UzZaman et al. (2013), we notice that the best performance of ClearTK was achieved when trained on TB+VC (line 7 is higher than its reported values in TE3 because of later changes in ClearTK), so we retrained the proposed systems on the same training set and results are shown on lines 8-9. In this case, the improvement of S+I over Local was small, which may be due to the lack of training data. Note that line 8 was still significantly different to line 7 per the McNemar’s test, although there was only 0.2% absolute difference in F1, which can be explained from their large differences in precision and recall."
  }, {
    "heading": "5.3.3 Comparison with CAEVO",
    "text": "The proposed structured learning approach was further compared to a recent system, a CAscading EVent Ordering architecture (CAEVO) proposed in Chambers et al. (2014) (lines 10-13). We used the same training set and test set as CAEVO in the S+I systems. Again, we added the E-T TLINKs predicted by CAEVO to both S+I systems. In Chambers et al. (2014), CAEVO was reported on the straightforward evaluation metric including the vague TLINKs, but the temporal awareness scores\nwere used here, which explains the difference between line 11 in Table 4 and what was reported in Chambers et al. (2014).\nClearTK was reported to be outperformed by CAEVO on TD-Test (Chambers et al., 2014), but we observe that ClearTK on line 10 was much worse even than itself on line 7 (trained on TB+VC) and on line 1 (trained on TB+AQ+VC+TD) due to the annotation scheme difference between TD and TB/AQ/VC. ClearTK was designed mainly for TE3, aiming for high precision, which is reflected by its high precision on line 10, but it does not have enough flexibility to cope with two very different annotation schemes. Therefore, we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones. On the TD-Test dataset, all systems other than ClearTK had better F1 scores compared to their performances on TE3-PT. This notable difference (i.e., 48.53 vs 40.3) indicates the better quality of the dense annotation scheme that was used to create TD (Cassidy et al., 2014). SP+ILP outperformed CAEVO and if additional unlabeled dataset TE3-SV was used, CoDL+ILP achieved the best score with a relative improvement in F1 score being 6.3%.\nWe notice that the proposed systems often have higher recall than precision, and that this is less an issue on a densely annotated testset (TD-Test), so their low precision on TE3-PT possibly came from the missing annotations on TE3-PT. It is still under investigation how to control precision and recall in real applications."
  }, {
    "heading": "6 Conclusion",
    "text": "We develop a structured learning approach to identifying temporal relations in natural language text and show that it captures the global nature of this problem better than state-of-the-art systems do. A new perspective towards vague relations is also proved to gain from fully taking advantage of the structured approach. In addition, the global nature of this problem gives rise to a better way of making use of the readily available unlabeled data, which further improves the proposed method. The improved performance on both TE3-PT and TDTest, two differently annotated datasets, clearly shows the advantage of the proposed method over existing methods. We plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank all the reviewers for providing useful comments. This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network; by the US Defense Advanced Research Projects Agency (DARPA) under contract FA8750-13-2-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies of the U.S. Government."
  }],
  "year": 2019,
  "references": [{
    "title": "Maintaining knowledge about temporal intervals",
    "authors": ["James F Allen."],
    "venue": "Communications of the ACM 26(11):832–843.",
    "year": 1983
  }, {
    "title": "Towards a general theory of action and time",
    "authors": ["James F Allen."],
    "venue": "Artificial intelligence 23(2):123–154.",
    "year": 1984
  }, {
    "title": "ClearTK-TimeML: A minimalist approach to TempEval 2013",
    "authors": ["Steven Bethard."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM). volume 2, pages 10–14.",
    "year": 2013
  }, {
    "title": "CU-TMP: Temporal relation classification using syntactic and semantic features",
    "authors": ["Steven Bethard", "James H Martin."],
    "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 129–",
    "year": 2007
  }, {
    "title": "Timelines from text: Identification of syntactic temporal relations",
    "authors": ["Steven Bethard", "James H Martin", "Sara Klingenstein."],
    "venue": "Semantic Computing, 2007. ICSC 2007. International Conference on. IEEE, pages 11–18.",
    "year": 2007
  }, {
    "title": "SemEval-2016 Task 12: Clinical TempEval",
    "authors": ["Steven Bethard", "Guergana Savova", "Wei-Te Chen", "Leon Derczynski", "James Pustejovsky", "Marc Verhagen."],
    "venue": "Proceedings of SemEval pages 1052–1062.",
    "year": 2016
  }, {
    "title": "Inducing temporal graphs",
    "authors": ["P. Bramsen", "P. Deshpande", "Y.K. Lee", "R. Barzilay."],
    "venue": "Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP). Association for Computational Linguistics, Sydney, Aus-",
    "year": 2006
  }, {
    "title": "An annotation framework for dense event ordering",
    "authors": ["Taylor Cassidy", "Bill McDowell", "Nathanel Chambers", "Steven Bethard."],
    "venue": "Technical report, DTIC Document.",
    "year": 2014
  }, {
    "title": "Jointly combining implicit constraints improves temporal ordering",
    "authors": ["N. Chambers", "D. Jurafsky."],
    "venue": "Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).",
    "year": 2008
  }, {
    "title": "NavyTime: Event and time ordering from raw text",
    "authors": ["Nathanael Chambers."],
    "venue": "Technical report, DTIC Document.",
    "year": 2013
  }, {
    "title": "Dense event ordering with a multi-pass architecture",
    "authors": ["Nathanael Chambers", "Taylor Cassidy", "Bill McDowell", "Steven Bethard."],
    "venue": "Transactions of the Association for Computational Linguistics 2:273– 284.",
    "year": 2014
  }, {
    "title": "Classifying temporal relations between events",
    "authors": ["Nathanael Chambers", "Shan Wang", "Dan Jurafsky."],
    "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Lin-",
    "year": 2007
  }, {
    "title": "SUTIME: A library for recognizing and normalizing time expressions",
    "authors": ["Angel X Chang", "Christopher D Manning."],
    "venue": "LREC. volume 2012, pages 3735–3740.",
    "year": 2012
  }, {
    "title": "Guiding semi-supervision with constraint-driven learning",
    "authors": ["M. Chang", "L. Ratinov", "D. Roth."],
    "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, Prague, Czech Republic,",
    "year": 2007
  }, {
    "title": "Structured learning with constrained conditional models",
    "authors": ["M. Chang", "L. Ratinov", "D. Roth."],
    "venue": "Machine Learning 88(3):399–431.",
    "year": 2012
  }, {
    "title": "Structured output learning with indirect supervision",
    "authors": ["M. Chang", "V. Srikumar", "D. Goldwasser", "D. Roth."],
    "venue": "Proc. of the International Conference on Machine Learning (ICML).",
    "year": 2010
  }, {
    "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "Proc. of ACL.",
    "year": 2002
  }, {
    "title": "Approximate statistical tests for comparing supervised classification learning algorithms",
    "authors": ["Thomas G Dietterich."],
    "venue": "Neural computation 10(7):1895– 1923.",
    "year": 1998
  }, {
    "title": "Joint inference for event timeline construction",
    "authors": ["Q. Do", "W. Lu", "D. Roth."],
    "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2012
  }, {
    "title": "The analysis of contingency tables",
    "authors": ["Brian S Everitt."],
    "venue": "CRC Press.",
    "year": 1992
  }, {
    "title": "Large margin classification using the Perceptron algorithm",
    "authors": ["Y. Freund", "R. Schapire."],
    "venue": "Proceedings of the Annual ACM Workshop on Computational Learning Theory (COLT). pages 209–217.",
    "year": 1998
  }, {
    "title": "The AQUAINT corpus of english news text",
    "authors": ["David Graff."],
    "venue": "Linguistic Data Consortium, Philadelphia .",
    "year": 2002
  }, {
    "title": "Tackling representation, annotation and classification challenges for temporal knowledge base population",
    "authors": ["Heng Ji", "Taylor Cassidy", "Qi Li", "Suzanne Tamang."],
    "venue": "Knowledge and Information Systems 41(3):611–646.",
    "year": 2014
  }, {
    "title": "Extraction of events and temporal expressions from clinical narratives",
    "authors": ["P. Jindal", "D. Roth."],
    "venue": "Journal of Biomedical Informatics (JBI) .",
    "year": 2013
  }, {
    "title": "UTTime: Temporal relation classification using deep syntactic features",
    "authors": ["Natsuda Laokulrat", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM). volume 2,",
    "year": 2013
  }, {
    "title": "Context-dependent semantic parsing for time expressions",
    "authors": ["Kenton Lee", "Yoav Artzi", "Jesse Dodge", "Luke Zettlemoyer."],
    "venue": "ACL (1). pages 1437–1447.",
    "year": 2014
  }, {
    "title": "Structured learning for temporal relation extraction from clinical records",
    "authors": ["Tuur Leeuwenberg", "Marie-Francine Moens."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "SemEval-2015 Task 5: QA TEMPEVAL - evaluating temporal information understanding with question answering",
    "authors": ["Hector Llorens", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky."],
    "venue": "Proceed-",
    "year": 2015
  }, {
    "title": "Machine learning of temporal relations",
    "authors": ["Inderjeet Mani", "Marc Verhagen", "Ben Wellner", "Chong Min Lee", "James Pustejovsky."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting",
    "year": 2006
  }, {
    "title": "Three approaches to learning TLINKs in TimeML",
    "authors": ["Inderjeet Mani", "Ben Wellner", "Marc Verhagen", "James Pustejovsky."],
    "venue": "Technical Report CS07–268, Computer Science Department .",
    "year": 2007
  }, {
    "title": "SemEval-2015 Task 4: TimeLine: Cross-document event ordering",
    "authors": ["Anne-Lyse Minard", "Manuela Speranza", "Eneko Agirre", "Itziar Aldabe", "Marieke van Erp", "Bernardo Magnini", "German Rigau", "Ruben Urizar", "Fondazione Bruno Kessler."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Learning and inference over constrained output",
    "authors": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak."],
    "venue": "Proc. of the International Joint Conference on Artificial Intelligence (IJCAI). pages 1124–1129.",
    "year": 2005
  }, {
    "title": "TimeML: Robust specification of event and temporal expressions in text",
    "authors": ["James Pustejovsky", "José M Castano", "Robert Ingria", "Roser Sauri", "Robert J Gaizauskas", "Andrea Setzer", "Graham Katz", "Dragomir R Radev."],
    "venue": "New directions in question",
    "year": 2003
  }, {
    "title": "Learning based java for rapid development of nlp systems",
    "authors": ["N. Rizzolo", "D. Roth."],
    "venue": "Proc. of the International Conference on Language Resources and Evaluation (LREC). Valletta, Malta.",
    "year": 2010
  }, {
    "title": "A linear programming formulation for global inference in natural language tasks",
    "authors": ["D. Roth", "W. Yih."],
    "venue": "Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Conference on Computational Natural Language Learning (CoNLL). Association for Com-",
    "year": 2004
  }, {
    "title": "HeidelTime: High quality rule-based extraction and normalization of temporal expressions",
    "authors": ["Jannik Strötgen", "Michael Gertz."],
    "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, pages",
    "year": 2010
  }, {
    "title": "Overview of the TAC2013 knowledge base population evaluation: English slot filling and temporal slot filling",
    "authors": ["Mihai Surdeanu."],
    "venue": "TAC.",
    "year": 2013
  }, {
    "title": "Temporal evaluation",
    "authors": ["Naushad UzZaman", "James F Allen."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2. Association for Computational",
    "year": 2011
  }, {
    "title": "SemEval-2013 Task 1: TEMPEVAL-3: Evaluating time expressions, events, and temporal relations",
    "authors": ["Naushad UzZaman", "Hector Llorens", "James Allen", "Leon Derczynski", "Marc Verhagen", "James Pustejovsky."],
    "venue": "Second Joint Conference on Lexical and Com-",
    "year": 2013
  }, {
    "title": "Times Between The Lines",
    "authors": ["Marc Verhagen."],
    "venue": "Ph.D. thesis, Brandeis University.",
    "year": 2004
  }, {
    "title": "SemEval-2007 Task 15: TempEval temporal relation identification",
    "authors": ["Marc Verhagen", "Robert Gaizauskas", "Frank Schilder", "Mark Hepple", "Graham Katz", "James Pustejovsky."],
    "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. As-",
    "year": 2007
  }, {
    "title": "Temporal processing with the TARSQI toolkit",
    "authors": ["Marc Verhagen", "James Pustejovsky."],
    "venue": "22nd International Conference on on Computational Linguistics: Demonstration Papers. Association for Computational Linguistics, pages 189–192.",
    "year": 2008
  }, {
    "title": "SemEval-2010 Task 13: TempEval-2",
    "authors": ["Marc Verhagen", "Roser Sauri", "Tommaso Caselli", "James Pustejovsky."],
    "venue": "Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, pages 57–62.",
    "year": 2010
  }, {
    "title": "A robust shallow temporal reasoning system",
    "authors": ["R. Zhao", "Q. Do", "D. Roth."],
    "venue": "Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Demo.",
    "year": 2012
  }],
  "id": "SP:cfeec7fedb9bb52ca0a4f3891420af2cdb646485",
  "authors": [{
    "name": "Qiang Ning",
    "affiliations": []
  }, {
    "name": "Zhili Feng",
    "affiliations": []
  }, {
    "name": "Dan Roth",
    "affiliations": []
  }],
  "abstractText": "Identifying temporal relations between events is an essential step towards natural language understanding. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events. Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods. As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem.",
  "title": "A Structured Learning Approach to Temporal Relation Extraction"
}