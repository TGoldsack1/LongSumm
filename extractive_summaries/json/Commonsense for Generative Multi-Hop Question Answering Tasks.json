{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4220–4230 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4220"
  }, {
    "heading": "1 Introduction",
    "text": "In this paper, we explore the task of machine reading comprehension (MRC) based QA. This task tests a model’s natural language understanding capabilities by asking it to answer a question\n∗ Equal contribution (published at EMNLP 2018). We publicly release all our code, models, and data at:\nhttps://github.com/yicheng-w/CommonSenseMultiHopQA\nbased on a passage of relevant content. Much progress has been made in reasoning-based MRCQA on the bAbI dataset (Weston et al., 2016), which contains questions that require the combination of multiple disjoint pieces of evidence in the context. However, due to its synthetic nature, bAbI evidences have smaller lexicons and simpler passage structures when compared to humangenerated text.\nThere also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop reasoning capabilities. More recent datasets such as QAngaroo (Welbl et al., 2018) have prompted a strong focus on multi-hop reasoning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information.\nWe focus on the recently published NarrativeQA generative dataset (Kočiskỳ et al., 2018) that contains questions requiring multi-hop reasoning for long, complex stories and other narratives, which requires the model to go beyond fact linking and to synthesize non-span answers. Hence, models that perform well on previous reasoning tasks (Dhingra et al., 2018) have had limited success on this dataset. In this paper, we first propose the Multi-Hop Pointer-Generator Model (MHPGM), a strong baseline model that uses multiple hops of bidirectional attention, self-attention, and a pointer-generator decoder to effectively read and reason within a long passage and synthesize a coherent response. Our model achieves 41.49 Rouge-L and 17.33 METEOR on the summary\nsubtask of NarrativeQA, substantially better than the performance of previous generative models.\nNext, to address the issue that understanding human-generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense (background) knowledge, we present an algorithm for selecting useful, grounded multi-hop relational knowledge paths from ConceptNet (Speer and Havasi, 2012) via a pointwise mutual information (PMI) and term-frequency-based scoring function. We then present a novel method of inserting these selected commonsense paths between the hops of document-context reasoning within our model, via the Necessary and Optional Information Cell (NOIC), which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference. With these additions, we further improve performance on the NarrativeQA dataset, achieving 44.16 Rouge-L and 19.03 METEOR (also verified via human evaluation). We also provide manual analysis on the effectiveness of our commonsense selection algorithm.\nFinally, to show the effectiveness and generalizability of our multi-hop reasoning and commonsense methods, we also tested our MHPGM and MHPGM+NOIC models on QAngarooWikiHop (Welbl et al., 2018), which is an extractive dataset for multi-hop reasoning on humangenerated documents. We found that our background commonsense knowledge enhanced model achieved 1.5% higher accuracy than our strong baseline."
  }, {
    "heading": "2 Related Work",
    "text": "Machine Reading Comprehension: MRC has long been a task used to assess a model’s ability to understand and reason about language. Large scale datasets such as CNN/Daily Mail (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) have encouraged the development of many advanced, high performing attention-based neural models (Seo et al., 2017; Dhingra et al., 2017). Concurrently, datasets such as bAbI (Weston et al., 2016) have focused specifically on multi-step reasoning by requiring the model to reason with disjoint pieces of information. On this task, it has been shown that iteratively updating the query representation with information from the context can effectively emulate multi-step reason-\ning (Sukhbaatar et al., 2015).\nMore recently, there has been an increase in multi-paragraph, multi-hop inference QA datasets such as QAngaroo (Welbl et al., 2018) and NarrativeQA (Kočiskỳ et al., 2018). These datasets have much longer contexts than previous datasets, and answering a question often requires the synthesis of multiple discontiguous pieces of evidence. It has been shown that models designed for previous tasks (Seo et al., 2017; Kadlec et al., 2016) have limited success on these new datasets. In our work, we expand upon Gated Attention Network (Dhingra et al., 2017) to create a baseline model better suited for complex MRC datasets such as NarrativeQA by improving its attention and gating mechanisms, expanding its generation capabilities, and allowing access to external commonsense for connecting implicit relations.\nCommonsense/Background Knowledge: Commonsense or background knowledge has been used for several tasks including opinion mining (Cambria et al., 2010), sentiment analysis (Poria et al., 2015, 2016), handwritten text recognition (Wang et al., 2013), and more recently, dialogue (Young et al., 2018; Ghazvininejad et al., 2018). These approaches add commonsense knowledge as relation triples or features from external databases. Recently, largescale graphical commonsense databases such as ConceptNet (Speer and Havasi, 2012) use graphical structure to express intricate relations between concepts, but effective goal-oriented graph traversal has not been extensively used in previous commonsense incorporation efforts. Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs (Bollacker et al., 2008). Knowledge path extraction has been shown to be effective at the task (Bordes et al., 2014; Bao et al., 2016). We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet (Speer and Havasi, 2012).\nIncorporation of External Knowledge: There have been several attempts at using external knowledge to boost model performance on a variety of tasks: Chen et al. (2018) showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into to-\nken representations in dialogue. In MRC, Weissenborn et al. (2017) integrated external background knowledge into an NLU model by using contextually-refined word embeddings which integrated information from ConceptNet (single-hop relations mapped to unstructured text) via a single layer bidirectional LSTM. Concurrently to our work, Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention, where commonsense relations were extracted as triples. This work represented commonsense relations as keyvalue pairs and combined context representation and commonsense via a static gate.\nDiffering from previous works, we employ multi-hop commonsense paths (multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple) to help with our MRC model. Moreover, we use this in tandem with our multi-hop reasoning architecture to incorporate different aspects of the commonsense relationship path at each hop, in order to bridge different inference gaps in the multi-hop QA task. Additionally, our model performs synthesis with its external, background knowledge as it generates, rather than extracts, its answer."
  }, {
    "heading": "3 Methods",
    "text": ""
  }, {
    "heading": "3.1 Multi-Hop Pointer-Generator Baseline",
    "text": "We first rigorously state the problem of generative QA as follows: given two sequences of input tokens: the context, XC = {wC1 , wC2 , . . . , wCn } and the query, XQ = {wQ1 , w Q 2 , . . . , w Q m}, the system should generate a series of answer tokens Xa = {wa1 , wa2 , . . . , wap}. As outlined in previous sections, an effective generative QA model needs to be able to perform several hops of reasoning over long and complex passages. It would also need to be able to generate coherent statements to answer complex questions while having the ability to copy rare words such as specific entities from the reading context. With these in mind, we propose the Multi-Hop Pointer-Generator Model (MHPGM) baseline, a novel combination of previous works with the following major components:\n• Embedding Layer: The tokens are embedded into both learned word embeddings and pretrained context-aware embeddings (ELMo (Peters et al., 2018)).\n• Reasoning Layer: The embedded context is then passed through k reasoning cells, each of which iteratively updates the context representation with information from the query via BiDAF attention (Seo et al., 2017), emulating a single reasoning step within the multi-step reasoning process. • Self-Attention Layer: The context representa-\ntion is passed through a layer of self-attention (Cheng et al., 2016) to resolve long-term dependencies and co-reference within the context. • Pointer-Generator Decoding Layer: A\nattention-pointer-generator decoder (See et al., 2017) that attends on and potentially copies from the context is used to create the answer.\nThe overall model is illustrated in Fig. 1, and the layers are described in further detail below. Embedding layer: We embed each word from the context and question with a learned embedding space of dimension d. We also obtain contextaware embeddings for each word via the pretrained embedding from language models (ELMo) (1024 dimensions). The embedded representation for each word in the context or question, eCi or eQi ∈ Rd+1024, is the concatenation of its learned word embedding and ELMo embedding. Reasoning layer: Our reasoning layer is composed of k reasoning cells (see Fig. 1), where each incrementally updates the context representation. The tth reasoning cell’s inputs are the previous step’s output ({ct−1i }ni=1) and the embedded question ({eQi }mi=1). It first creates step-specific context and query encodings via cell-specific bidirectional LSTMs: ut = BiLSTM(ct−1); vt = BiLSTM(eQ)\nThen, we use bidirectional attention (Seo et al., 2017) to emulate a hop of reasoning by focusing on relevant aspects of the context. Specifically, we first compute context-to-query attention:\nStij =W t 1u t i +W t 2v t j +W t 3(u t i vtj)\nptij = exp(Stij)∑m k=1 exp(S t ik)\n(cq) t i = m∑ j=1 ptijv t j\nwhere W t1 , W t 2 , W t 3 are trainable parameters, and\nis elementwise multiplication. We then compute a query-to-context attention vector:\nmti = max 1≤j≤m Stij\npti = exp(mti)∑n j=1 exp(m t j)\nqc t = n∑ i=1 ptiu t i\nWe then obtain the updated context representation:\ncti = [u t i; (cq) t i;u t i (cq)ti;qct (cq)ti]\nwhere ; is concatenation, ct is the cell’s output. The initial input of the reasoning layer is the embedded context representation, i.e., c0 = eC , and the final output of the reasoning layer is the output of the last cell, ck. Self-Attention Layer: As the final layer before answer generation, we utilize a residual static selfattention mechanism (Clark and Gardner, 2018) to help the model process long contexts with longterm dependencies. The input of this layer is the output of the last reasoning cell, ck. We first pass this representation through a fully-connected layer and then a bi-directional LSTM to obtain another representation of the context cSA. We obtain the self attention representation c′:\nSSAij =W4c SA i +W5c SA j +W6(c SA i cSAj )\npSAij = exp(SSAij )∑n k=1 exp(S SA ik )\nc′i = n∑\nj=1\npSAij c SA j\nwhere W4, W5, and W6 are trainable parameters. The output of the self-attention layer is generated by another layer of bidirectional LSTM.\nc′′ = BiLSTM([c′; cSA; c′ cSA]\nFinally, we add this residually to ck to obtain the encoded context c = ck + c′′. Pointer-Generator Decoding Layer: Similar to the work of See et al. (2017), we use a pointergenerator model attending on (and potentially copying from) the context.\nAt decoding step t, the decoder receives the input xt (embedded representation of last timestep’s output), the last time step’s hidden state st−1 and context vector at−1. The decoder computes the current hidden state st as:\nst = LSTM([xt;at−1], st−1)\nThis hidden state is then used to compute a probability distribution over the generative vocabulary:\nPgen = softmax(Wgenst + bgen)\nWe employ Bahdanau attention mechanism (Bahdanau et al., 2015) to attend over the context (c being the output of self-attention layer):\nαi = v ᵀ tanh(Wcci +Wsst + battn)\nα̂i = exp(αi)∑n j=1 exp(αj)\nat = n∑\ni=1\nα̂ici\nWe utilize a pointer mechanism that allows the decoder to directly copy tokens from the context based on α̂i. We calculate a selection distribution psel ∈ R2, where psel1 is the probability of generating a token from Pgen and psel2 is the probability of copying a word from the context:\no = σ(Waat +Wxxt +Wsst + bptr)\npsel = softmax(o)\nOur final output distribution at timestep t is a weighted sum of the generative distribution and the copy distribution:\nPt(w) = p sel 1 Pgen(w) + p sel 2 ∑ i:wCi =w α̂i"
  }, {
    "heading": "3.2 Commonsense Selection and Representation",
    "text": "In QA tasks that require multiple hops of reasoning, the model often needs knowledge of relations not directly stated in the context to reach the correct conclusion. In the datasets we consider, manual analysis shows that external knowledge is frequently needed for inference (see Table 1).\nEven with a large amount of training data, it is very unlikely that a model is able to learn every nuanced relation between concepts and apply the correct ones (as in Fig. 2) when reasoning\nabout a question. We remedy this issue by introducing grounded commonsense (background) information using relations between concepts from ConceptNet (Speer and Havasi, 2012)1 that help inference by introducing useful connections between concepts in the context and question.\nDue to the size of the semantic network and the large amount of unnecessary information, we need an effective way of selecting relations which provides novel information while being grounded by the context-query pair. Our commonsense selection strategy is twofold: (1) collect potentially relevant concepts via a tree construction method aimed at selecting with high recall candidate reasoning paths, and (2) rank and filter these paths to ensure both the quality and variety of added information via a 3-step scoring strategy (initial node scoring, cumulative node scoring, and path selection). We will refer to Fig. 2 as a running example throughout this section.2"
  }, {
    "heading": "3.2.1 Tree Construction",
    "text": "Given context C and question Q, we want to construct paths grounded in the pair that emulate reasoning steps required to answer the question. In this section, we build ‘prototype’ paths by constructing trees rooted in concepts in the query with the following branching steps3 to emulate multihop reasoning process. For each concept c1 in the question, we do: Direct Interaction: In the first level, we select relations r1 from ConceptNet that directly link c1 to a concept within the context, c2 ∈ C, e.g., in Fig. 2, we have lady → church, lady → mother, lady→ person. Multi-Hop: We then select relations in ConceptNet r2 that link c2 to another concept in the context, c3 ∈ C. This emulates a potential reason-\n1A semantic network where the nodes are individual concepts (words or phrases) and the edges describe directed relations between them (e.g., 〈island, UsedFor, vacation〉).\n2We release all our commonsense extraction code and the extracted commonsense data at: https://github.com/ yicheng-w/CommonSenseMultiHopQA\n3If we are unable to find a relation that satisfies the condition, we keep the steps up to and including the node.\ning hop within the context of the MRC task, e.g., church → house, mother → daughter, person → lover. Outside Knowledge: We then allow an unconstrained hop into c3’s neighbors in ConceptNet, getting to c4 ∈ nbh(c3) via r3 (nbh(v) is the set of nodes that can be reached from v in one hop). This emulates the gathering of useful external information to complete paths within the context, e.g., house→ child, daughter→ child. Context-Grounding: To ensure that the external knowledge is indeed helpful to the task, and also to explicitly link 2nd degree neighbor concepts within the context, we finish the process by grounding it again into context by connecting c4 to c5 ∈ C via r4, e.g., child→ their."
  }, {
    "heading": "3.2.2 Rank and Filter",
    "text": "This tree building process collects a large number of potentially relevant and useful paths. However, this step also introduces a large amount of noise. For example, given the question and full context (not depicted in the figure) in Fig. 2, we obtain the path “between→ hard→ being→ cottage→ country” using our tree building method, which is not relevant to our question. Therefore, to improve the precision of useful concepts, we rank these knowledge paths by their relevance and filter out noise using the following 3-step scoring method: Initial Node Scoring: We want to select paths with nodes that are important to the context, in order to provide the most useful commonsense relations. We approximate importance and saliency for concepts in the context by their termfrequency, under the heuristic that important concepts occur more frequently. Thus we score c ∈ {c2, c3, c5} by: score(c) = count(c)/|C|, where |C| is the context length and count() is the number of times a concept appears in the context. In Fig. 2, this ensures that concepts like daughter are scored highly due to their frequency in the context.\nFor c4, we use a special scoring function as it is an unconstrained hop into ConceptNet. We want c4 to be a logically consistent next step in reasoning following the path of c1 to c3, e.g., in Fig. 2, we see that child is a logically consistent next step after the partial path of mother→ daughter. We approximate this based on the heuristic that logically consistent paths occur more frequently. Therefore, we score this node via Pointwise Mutual Information (PMI) between the partial path c1−3 and c4: PMI(c4, c1−3) = log(P(c4, c1−3)/P(c4)P(c1−3)),\nwhere\nP(c4, c1−3) = # of paths connecting c1, c2, c3, c4\n# of distinct paths of length 4\nP(c4) = # of nodes that can reach c4\n|ConceptNet|\nP(c1−3) = # of paths connecting c1, c2, c3\n# of paths of length 3\nFurther, it is well known that PMI has high sensitivity to low-frequency values, thus we use normalized PMI (NPMI) (Bouma, 2009): score(c4) = PMI(c4, c1−3)/(− logP(c4, c1−3)).\nSince the branching at each juncture represents a hop in the multi-hop reasoning process, and hops at different levels or with different parent nodes do not ‘compete’ with each other, we normalize each node’s score against its siblings:\nn-score(c) = softmaxsiblings(c)(score(c)).\nCumulative Node Scoring: We want to add commonsense paths consisting of multiple hops of relevant information, thus we re-score each node based not only on its relevance and saliency but also that of its tree descendants.\nWe do this by computing a cumulative node score from the bottom up, where at the leaf nodes, we have c-score = n-score, and for cl not a leaf node, we have c-score(cl) = n-score(cl) + f(cl) where f of a node is the average of the c-scores of its top 2 highest scoring children.\nFor example, given the paths lady→ mother→ daughter, lady→ mother→ married, and lady→ mother → book, we start the cumulative scoring at the leaf nodes, which in this case are daughter, married, and book, where daughter and married are scored much higher than book due to their more frequent occurrences. Then, to cumulatively score mother , we would take the average score of its two highest scoring children (in this case married and daughter) and compound that with the score of mother itself. Note that the poor scoring of the irrelevant concept book does not affect the scoring of mother, which is quite high due to the concept’s frequent occurrence and the relevance of its top scoring children. Path Selection: We select paths in a top-down breath-first fashion in order to add information relevant to different parts of the context. Starting at the root, we recursively take two of its children with the highest cumulative scores until we reach a leaf, selecting up to 24 = 16 paths. For example,\nif we were at node mother, this allows us to select the child node daughter and married over the child node book. These selected paths, as well as their partial sub-paths, are what we add as external information to the QA model, i.e., we add the complete path 〈lady, AtLocation, church, RelatedTo, house, RelatedTo, child, RelatedTo, their〉, but also truncated versions of the path, including 〈lady, AtLocation, church, RelatedTo, house, RelatedTo, child〉. We directly give these paths to the model as sequences of tokens.4\nOverall, our sampling strategy provides the knowledge that a lady can be a mother and that mother is connected to daughter. This creates a logical connection between lady and daughter which helps highlight the importance of our second piece of evidence (see Fig. 2). Likewise, the commonsense information we extracted create a similar connection in our third piece of evidence, which states the explicit connection between daughter and Esther. We also successfully extract a more story context-centric connection, in which commonsense provides the knowledge that a lady is at the location church, which directs to another piece of evidence in the context. Additionally, this path also encodes a relation between lady and child, by way of church, which is how lady and Esther are explicitly connected in the story."
  }, {
    "heading": "3.3 Commonsense Model Incorporation",
    "text": "Given the list of commonsense logic paths as sequences of words: XCS = {wCS1 , wCS2 , . . . , wCSl } where wCSi represents the list of tokens that make up a single path, we first embed these commonsense tokens into the learned embedding space used by the model, giving us the embedded commonsense tokens, eCSij ∈ Rd. We want to use these commonsense paths to fill in the gaps of reasoning between hops of inference. Thus, we propose Necessary and Optional Information Cell (NOIC), a variation of our base reasoning cell used in the reasoning layer that is capable of incorporating optional helpful information.\nNOIC This cell is an extension to the base reasoning cell that allows the model to use commonsense information to fill in gaps of reasoning. An example of this is on the bottom left of Fig. 1, where we see that the cell first performs the operations done in the base reasoning cell and then\n4In cases where more than one relation can be used to make a hop, we pick one at random.\nadds optional, commonsense information. At reasoning step t, after obtaining the output of the base reasoning cell, ct, we create a cell-specific representation for commonsense information by concatenating the embedded commonsense paths so that each path has a single vector representation, uCSi . We then project it to the same dimension as cti: v CS i = ReLU(Wu CS i + b) where W and b are trainable parameters. We use an attention layer to model the interaction between commonsense and the context:\nSCSij =W CS 1 c t i +W CS 2 v CS j +W CS 3 (c t i vCSj )\npCSij = exp(SCSij )∑l k=1 exp(S CS ij )\ncCSi = l∑\nj=1\npCSij v CS j\nFinally, we combine this commonsense-aware context representation with the original cti via a sigmoid gate, since commonsense information is often not necessary at every step of inference:\nzi = σ(Wz[c CS i ; c t i] + bz)\n(co) t i = zi cti + (1− zi) cCSi\nWe use cot as the output of the current reasoning step instead of ct. As we replace each base reasoning cell with NOIC, we selectively incorporate commonsense at every step of inference."
  }, {
    "heading": "4 Experimental Setup",
    "text": "Datasets: We report results on two multi-hop reasoning datasets: generative NarrativeQA (Kočiskỳ et al., 2018) (summary subtask) and extractive QAngaroo WikiHop (Welbl et al., 2018). For multiple-choice WikiHop, we rank candidate responses by their generation probability. Similar to previous works (Dhingra et al., 2018), we use the non-oracle, unmasked and not-validated dataset. Evaluation Metrics: We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, Bleu-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and RougeL (Lin, 2004). We also evaluate on CIDEr (Vedantam et al., 2015) which emphasizes annotator consensus. For WikiHop, we evaluate on accuracy.5\nMore dataset, metric, and all other training details are in the supplementary.\n5Due to the 2-week evaluation wait-time on the nonpublic test set, we instead train our model on a sub-section of the training set, pick hyperparameters based on a small"
  }, {
    "heading": "5 Results",
    "text": ""
  }, {
    "heading": "5.1 Main Experiment",
    "text": "The results of our model on both NarrativeQA and WikiHop with and without commonsense incorporation are shown in Table 2 and Table 3. We see empirically that our model outperforms all generative models on NarrativeQA, and is competitive with the top span prediction models. Furthermore, with the NOIC commonsense integration, we were able to further improve performance (p < 0.001 on all metrics6), establishing a new state-of-the-art for the task. We also see that our model performs well on WikiHop,7 and is further improved via the addition of commonsense (p < 0.001), demonstrating the generalizability of both our model and commonsense addition techniques.8"
  }, {
    "heading": "5.2 Model Ablations",
    "text": "We also tested the effectiveness of each component of our architecture as well as the effectiveness of adding commonsense information on the NarrativeQA validation set, with results shown in Table 4. Experiment 1 and 5 are our models pre-\n(500 examples) held-out part of the training set, and test on the original validation set (by treating it as an unseen test set). We will promptly include the non-public test set results in the next version and at: https://github.com/yicheng-w/ CommonSenseMultiHopQA\n6Stat. significance computed using bootstrap test with 100K iterations (Noreen, 1989; Efron and Tibshirani, 1994).\n7Note that we compare our model’s performance to other models’ tuned performance on the development set and ours is still equal or better.\n8All results here are for the standard (non-oracle) unmasked and not-validated dataset. Welbl et al. (2018) has reported higher numbers on different data settings which are not comparable to our results.\nsented in Table 2. Experiment 2 demonstrates the importance of multi-hop attention by showing that if we only allow one hop of attention (even with all other components of the model, including ELMo embeddings) the model’s performance decreases by over 12 Rouge-L points. Experiment 3 and 4 demonstrate the effectiveness of other parts of our model. We see that ELMo embeddings (Peters et al., 2018) were also important for the model’s performance and that self-attention is able to contribute significantly to performance on top of other components of the model. Finally, we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline."
  }, {
    "heading": "5.3 Commonsense Ablations",
    "text": "We also conducted experiments testing the effectiveness of our commonsense selection and incorporation techniques. We first tried to naively add ConceptNet information by initializing the word embeddings with the ConceptNet-trained embeddings, NumberBatch (Speer and Havasi, 2012) (we also change embedding size from 256 to 300). Then, to verify the effectiveness of our commonsense selection and grounding algorithm, we test our best model on in-domain noise by giving each context-query pair a set of random relations grounded in other context-query pairs. This should teach the model about general commonsense relations present in the domain of NarrativeQA but does not provide grounding that fills in specific hops of inference. We also experimented with a simpler commonsense extraction method of using a single hop from the query to the context. The results of these are shown in Table 5, where we see that neither NumberBatch nor random-relationships nor single-hop commonsense offer statistically significant improvements9,\n9The improvement in Rouge-L and METEOR for all three ablation approaches have p ≥ 0.15 with the bootstrap test."
  }, {
    "heading": "1 - 42.3 18.9 18.3 44.9 151.6",
    "text": "whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics. We also present several examples of extracted commonsense and its model attention visualization in the supplementary."
  }, {
    "heading": "6 Human Evaluation Analysis",
    "text": "We also conduct human evaluation analysis on both the quality of the selected commonsense relations, as well as the performance of our final model. Commonsense Selection: We conducted manual analysis on a 50 sample subset of the NarrativeQA test set to check the effectiveness of our commonsense selection algorithm. Specifically, given a context-query pair, as well as the commonsense selected by our algorithm, we conduct two independent evaluations: (1) was any external commonsense knowledge necessary for answering the question?; (2) were the commonsense relations provided by our algorithm relevant to the question? The result for these two evaluations as well as how they overlap with each other are shown in Table 6, where we see that 50% of the cases required external commonsense knowledge, and on a majority (34%) of those cases our algorithm was able to select the correct/relevant commonsense information to fill in gaps of inference. We also see that in general, our algorithm was able to provide useful commonsense 48% of the time. Model Performance: We also conduct human evaluation to verify that our commonsense incorporated model was indeed better than MHPGM. We randomly selected 100 examples from the NarrativeQA test set, along with both models’ predicted answers, and for each datapoint, we asked\n3 external human evaluators (fluent English speakers) to decide (without knowing which model produced each response) if one is strictly better than the other, or that they were similar in quality (bothgood or both-bad). As shown in Table 7, we see that the human evaluation results are in agreement with that of the automatic evaluation metrics: our commonsense incorporation has a reasonable impact on the overall correctness of the model. The inter-annotator agreement had a Fleiss κ = 0.831, indicating ‘almost-perfect’ agreement between the annotators (Landis and Koch, 1977)."
  }, {
    "heading": "7 Conclusion",
    "text": "We present an effective reasoning-generative QA architecture that is a novel combination of previous work, which uses multiple hops of bidirectional attention and a pointer-generator decoder to effectively perform multi-hop reasoning and synthesize a coherent and correct answer. Further, we introduce an algorithm to select grounded, useful paths of commonsense knowledge to fill in the gaps of inference required for QA, as well a Necessary and Optional Information Cell (NOIC) which successfully incorporates this information during multi-hop reasoning to achieve the new state-of-the-art on NarrativeQA."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the reviewers for their helpful comments. This work was supported by DARPA (YFA17-D17AP00022), Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views contained in this article are those of the authors and not of the funding agency."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
    "authors": ["Satanjeev Banerjee", "Alon Lavie."],
    "venue": "Proceedings of the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or",
    "year": 2005
  }, {
    "title": "Constraint-based question answering with knowledge graph",
    "authors": ["Junwei Bao", "Nan Duan", "Zhao Yan", "Ming Zhou", "Tiejun Zhao."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,",
    "year": 2016
  }, {
    "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management",
    "year": 2008
  }, {
    "title": "Question answering with subgraph embeddings",
    "authors": ["Antoine Bordes", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620.",
    "year": 2014
  }, {
    "title": "Normalized (pointwise) mutual information in collocation extraction",
    "authors": ["Gerlof Bouma."],
    "venue": "Proceedings of GSCL, pages 31–40.",
    "year": 2009
  }, {
    "title": "Sentic computing for patient centered applications",
    "authors": ["Erik Cambria", "Amir Hussain", "Tariq Durrani", "Catherine Havasi", "Chris Eckl", "James Munro."],
    "venue": "Signal Processing (ICSP), 2010 IEEE 10th International Conference on, pages 1279–1282. IEEE.",
    "year": 2010
  }, {
    "title": "Neural natural language inference models enhanced with external knowledge",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Diana Inkpen", "Si Wei."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2018
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 551–561.",
    "year": 2016
  }, {
    "title": "Simple and effective multi-paragraph reading comprehension",
    "authors": ["Christopher Clark", "Matt Gardner."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 845–855.",
    "year": 2018
  }, {
    "title": "Neural models for reasoning over multiple mentions using coreference",
    "authors": ["Bhuwan Dhingra", "Qiao Jin", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov."],
    "venue": "Proceedings of the 2018 Conference of",
    "year": 2018
  }, {
    "title": "Gatedattention readers for text comprehension",
    "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
    "year": 2017
  }, {
    "title": "An introduction to the bootstrap",
    "authors": ["Bradley Efron", "Robert J Tibshirani."],
    "venue": "CRC press.",
    "year": 1994
  }, {
    "title": "A knowledge-grounded neural conversation model",
    "authors": ["Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Advances in Neural Information Processing Systems, pages 1693–",
    "year": 2015
  }, {
    "title": "Text understanding with the attention sum reader network",
    "authors": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 908–918.",
    "year": 2016
  }, {
    "title": "The narrativeqa reading comprehension challenge",
    "authors": ["Tomáš Kočiskỳ", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gáabor Melis", "Edward Grefenstette."],
    "venue": "Transactions of the Association of Computational Linguistics,",
    "year": 2018
  }, {
    "title": "The measurement of observer agreement for categorical data",
    "authors": ["J Richard Landis", "Gary G Koch."],
    "venue": "biometrics, pages 159–174.",
    "year": 1977
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out.",
    "year": 2004
  }, {
    "title": "Knowledgeable reader: enhancing cloze-style reading comprehension with external commonsense knowledge",
    "authors": ["Todor Mihaylov", "Anette Frank."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2018
  }, {
    "title": "Computer-intensive methods for testing hypotheses",
    "authors": ["Eric W Noreen."],
    "venue": "Wiley New York.",
    "year": 1989
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "Proceedings of NAACL.",
    "year": 2018
  }, {
    "title": "Sentiment data flow analysis by means of dynamic linguistic patterns",
    "authors": ["Soujanya Poria", "Erik Cambria", "Alexander Gelbukh", "Federica Bisio", "Amir Hussain."],
    "venue": "IEEE Computational Intelligence Magazine, 10(4):26–36.",
    "year": 2015
  }, {
    "title": "Sentic LDA: Improving on LDA with semantic similarity for aspect-based sentiment analysis",
    "authors": ["Soujanya Poria", "Iti Chaturvedi", "Erik Cambria", "Federica Bisio."],
    "venue": "Neural Networks (IJCNN), 2016 International Joint Conference on, pages 4465–4473.",
    "year": 2016
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
    "year": 2016
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J Liu", "Christopher D Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1,",
    "year": 2017
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Representing general relational knowledge in ConceptNet 5",
    "authors": ["Robert Speer", "Catherine Havasi."],
    "venue": "LREC, pages 3679–3686.",
    "year": 2012
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Multi-range reasoning for machine comprehension",
    "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui."],
    "venue": "arXiv preprint arXiv:1803.09074.",
    "year": 2018
  }, {
    "title": "Cider: Consensus-based image description evaluation",
    "authors": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566–4575.",
    "year": 2015
  }, {
    "title": "Common sense knowledge for handwritten chinese text recognition",
    "authors": ["Qiu-Feng Wang", "Erik Cambria", "Cheng-Lin Liu", "Amir Hussain."],
    "venue": "Cognitive Computation, 5(2):234–242.",
    "year": 2013
  }, {
    "title": "Dynamic integration of background knowledge in neural NLU systems",
    "authors": ["Dirk Weissenborn", "Tomáš Kočiskỳ", "Chris Dyer."],
    "venue": "arXiv preprint arXiv:1706.02596.",
    "year": 2017
  }, {
    "title": "Constructing datasets for multi-hop reading comprehension across documents",
    "authors": ["Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel."],
    "venue": "Transactions of the Association of Computational Linguistics, 6:287–302.",
    "year": 2018
  }, {
    "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
    "authors": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov."],
    "venue": "ICLR.",
    "year": 2016
  }, {
    "title": "Incorporating loosestructured knowledge into LSTM with recall gate for conversation modeling",
    "authors": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang."],
    "venue": "International Joint Conference on Neural Networks.",
    "year": 2017
  }, {
    "title": "Augmenting end-to-end dialog systems with commonsense knowledge",
    "authors": ["Tom Young", "Erik Cambria", "Iti Chaturvedi", "Minlie Huang", "Hao Zhou", "Subham Biswas."],
    "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence",
    "year": 2018
  }],
  "id": "SP:0438a7186412e484ec4ad9aad949c7a45ca4c959",
  "authors": [{
    "name": "Lisa Bauer",
    "affiliations": []
  }, {
    "name": "Yicheng Wang",
    "affiliations": []
  }, {
    "name": "Mohit Bansal",
    "affiliations": []
  }],
  "abstractText": "Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multihop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectivelygated attention mechanism. This boosts the model’s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.",
  "title": "Commonsense for Generative Multi-Hop Question Answering Tasks"
}