{
  "sections": [{
    "text": "τ√ γ ) ln(1/ε)), where κl is the con-\ndition number of the local functions and γ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-theart methods for two problems: least-squares regression and classification by logistic regression."
  }, {
    "heading": "1. Introduction",
    "text": "Given the numerous applications of distributed optimization in machine learning, many algorithms have recently emerged, that allow the minimization of objective functions f defined as the average 1n ∑n i=1 fi of functions fi which are respectively accessible by separate nodes in a network (Nedic & Ozdaglar, 2009; Boyd et al., 2011; Duchi et al., 2012; Shi et al., 2015). These algorithms typically alter-\n1MSR-INRIA Joint Center, Palaiseau, France 2INRIA, Ecole Normale Supérieure, Paris, France 3Theory group, Microsoft Research, Redmond, United States. Correspondence to: Kevin Scaman <kevin.scaman@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nnate local incremental improvement steps (such as gradient steps) with communication steps between nodes in the network, and come with a variety of convergence rates (see for example Shi et al. (2014; 2015); Jakovetić et al. (2015); Nedich et al. (2016)).\nTwo main regimes have been looked at: (a) centralized where communications are precisely scheduled and (b) decentralized where communications may not exhibit a precise schedule. In this paper, we consider these two regimes for objective functions which are smooth and stronglyconvex and for which algorithms are linearly (exponentially) convergent. The main contribution of this paper is to propose new and matching upper and lower bounds of complexity for this class of distributed problems.\nThe optimal complexity bounds depend on natural quantities in optimization and network theory. Indeed, (a) for a single machine the optimal number of gradient steps to optimize a function is proportional to the square root of the condition number (Nesterov, 2004), and (b) for mean estimation, the optimal number of communication steps is proportional to the diameter of the network in centralized problems or to the square root of the eigengap of the Laplacian matrix in decentralized problems (Boyd et al., 2006). As shown in Section 3, our lower complexity bounds happen to be combinations of the two contributions above.\nThese lower complexity bounds are attained by two separate algorithms. In the centralized case, the trivial distribution of Nesterov’s accelerated gradient attains this rate, while in the decentralized case, as shown in Section 4, the rate is achieved by a dual algorithm. We compare favorably our new optimal algorithms to existing work in Section 5.\nRelated Work. Decentralized optimization has been extensively studied and early methods such as decentralized gradient descent (Nedic & Ozdaglar, 2009; Jakovetić et al., 2014) or decentralized dual averaging (Duchi et al., 2012) exhibited sublinear convergence rates. More recently, a number of methods with provable linear convergence rates were developed, including EXTRA (Shi et al., 2015; Mokhtari & Ribeiro, 2016), augmented Lagrangians (Jakovetić et al., 2015), and more recent approaches (Nedich et al., 2016). The most popular of\nsuch approaches is the distributed alternating direction method of multipliers (D-ADMM) (Boyd et al., 2011; Wei & Ozdaglar, 2012; Shi et al., 2014) and has led to a large number of variations and extensions. In a different direction, second order methods were also investigated (Mokhtari et al., 2016; Tutunov et al., 2016). However, to the best of our knowledge, the field still lacks a coherent theoretical understanding of the optimal convergence rates and its dependency on the characteristics of the communication network. In several related fields, complexity lower bounds were recently investigated, including the sequential optimization of a sum of functions (Arjevani & Shamir, 2016a;b), distributed optimization in flat (i.e. totally connected) networks (Shamir, 2014; Arjevani & Shamir, 2015), or distributed stochastic optimization (Shamir & Srebro, 2014)."
  }, {
    "heading": "2. Distributed Optimization Setting",
    "text": ""
  }, {
    "heading": "2.1. Optimization Problem",
    "text": "Let G = (V, E) be a connected simple (i.e. undirected) graph of n computing units and diameter ∆, each having access to a function fi(θ) over θ ∈ Rd. We consider minimizing the average of the local functions\nmin θ∈Rd\nf̄(θ) = 1\nn n∑ i=1 fi(θ) (1)\nin a distributed setting. More specifically, we assume that:\n1. Each computing unit can compute first-order characteristics, such as the gradient of its own function or its Fenchel conjugate. By renormalization of the time axis, and without loss of generality, we assume that this computation is performed in one unit of time.\n2. Each computing unit can communicate values (i.e. vectors in Rd) to its neighbors. This communication requires a time τ (which may be smaller or greater than 1).\nThese actions may be performed asynchronously and in parallel, and each node i possesses a local version of the parameter, which we refer to as θi. Moreover, we assume that each function fi is α-strongly convex and βsmooth, and we denote by κl = βα ≥ 1 the local condition number. We also denote by αg , βg and κg , respectively, the strong convexity, smoothness and condition number of the average (global) function f̄ . Note that we always have κg ≤ κl, while the opposite inequality is, in general, not true (take for example f1(θ) = 1{θ < 0}θ2 and f2(θ) = 1{θ > 0}θ2 for which κl = +∞ and κg = 1). However, the two quantities are close (resp. equal) when the local functions are similar (resp. equal) to one another."
  }, {
    "heading": "2.2. Decentralized Communication",
    "text": "A large body of literature considers a decentralized approach to distributed optimization based on the gossip algorithm (Boyd et al., 2006; Nedic & Ozdaglar, 2009; Duchi et al., 2012; Wei & Ozdaglar, 2012). In such a case, communication is represented as a matrix multiplication with a matrix W verifying the following constraints:\n1. W is an n× n symmetric matrix,\n2. W is positive semi-definite,\n3. The kernel of W is the set of constant vectors: Ker(W ) = Span(1), where 1 = (1, ..., 1)>,\n4. W is defined on the edges of the network: Wij 6= 0 only if i = j or (i, j) ∈ E .\nThe third condition will ensure that the gossip step converges to the average of all the vectors shared between the nodes. We will denote the matrix W as the gossip matrix, since each communication step will be represented using it. Note that a simple choice for the gossip matrix is the Laplacian matrix L = D − A, where A is the adjacency matrix of the network and D = diag (∑ iAij ) . However, in the presence of large degree nodes, weighted Laplacian matrices are usually a better choice, and the problem of optimizing these weights is known as the fastest distributed consensus averaging problem and is investigated by Xiao & Boyd (2004); Boyd et al. (2009).\nWe will denote by λ1(W ) ≥ · · · ≥ λn(W ) = 0 the spectrum of the gossip matrixW , and its (normalized) eigengap the ratio γ(W ) = λn−1(W )/λ1(W ) between the second smallest and the largest eigenvalue. Equivalently, this is the inverse of the condition number of W projected on the space orthogonal to the constant vector 1. This quantity will be the main parameter describing the connectivity of the communication network in Section 3.3 and Section 4."
  }, {
    "heading": "3. Optimal Convergence Rates",
    "text": "In this section, we prove oracle complexity lower bounds for distributed optimization in two settings: strongly convex and smooth functions for centralized (i.e. master/slave) and decentralized algorithms based on a gossip matrix W .\nIn the first setting, we show that distributing accelerated gradient descent matches the optimal convergence rate, while, in the second setting, the algorithm proposed in Section 4 is shown to be optimal. Note that we will use the notation g(ε) = Ω(f(ε)) for ∃C > 0 s.t. ∀ε > 0, g(ε) ≥ Cf(ε), and will, for simplicity, omit the additive terms that do not depend on the precision ε in Corollary 1 and Corollary 2."
  }, {
    "heading": "3.1. Black-box Optimization Procedures",
    "text": "The lower bounds provided hereafter depend on a new notion of black-box optimization procedures for the problem in Eq. (1), where we consider distributed algorithms verifying the following constraints:\n1. Local memory: each node i can store past values in a (finite) internal memoryMi,t ⊂ Rd at time t ≥ 0. These values can be accessed and used at time t by the algorithm run by node i, and are updated either by local computation or by communication (defined below), that is, for all i ∈ {1, ..., n},\nMi,t ⊂Mcompi,t ∪M comm i,t . (2)\n2. Local computation: each node i can, at time t, compute the gradient of its local function ∇fi(θ) or its Fenchel conjugate∇f∗i (θ) for a value θ ∈Mi,t in the node’s internal memory, that is, for all i ∈ {1, ..., n},\nMcompi,t = Span ({θ,∇fi(θ),∇f ∗ i (θ) : θ ∈Mi,t−1}) .\n(3) 3. Local communication: each node i can, at time t,\nshare a value to all or part of its neighbors, that is, for all i ∈ {1, ..., n},\nMcommi,t = Span ( ⋃\n(i,j)∈E\nMj,t−τ ) . (4)\n4. Output value: each node imust, at time t, specify one vector in its memory as local output of the algorithm, that is, for all i ∈ {1, ..., n},\nθi,t ∈Mi,t. (5)\nHence, a black-box procedure will return n output values— one for each node of the network—and our analysis will focus on ensuring that all local output values are converging to the optimal parameter of Eq. (1). Moreover, we will say that a black-box procedure uses a gossip matrix W if the local communication is achieved by multiplication of a vector with W . For simplicity, we assume that all nodes start with the simple internal memoryMi,0 = {0}. Note that communications and local computations may be performed in parallel and asynchronously."
  }, {
    "heading": "3.2. Centralized Algorithms",
    "text": "In this section, we show that, for any black-box optimization procedure, at least Ω(√κg ln(1/ε)) gradient steps and Ω(∆ √ κg ln(1/ε)) communication steps are necessary to achieve a precision ε > 0, where κg is the global condition number and ∆ is the diameter of the network. These lower bounds extend the communication complexity lower bounds for totally connected communication networks of\nArjevani & Shamir (2015), and are natural since at least Ω( √ κg ln(1/ε)) steps are necessary to solve a strongly convex and smooth problem up to a fixed precision, and at least ∆ communication steps are required to transmit a message between any given pair of nodes.\nIn order to simplify the proofs of the following theorems, and following the approach of Bubeck (2015), we will consider the limiting situation d → +∞. More specifically, we now assume that we are working in `2 = {θ = (θk)k∈N : ∑ k θ 2 k < +∞} rather than Rd. Theorem 1. Let G be a graph of diameter ∆ > 0 and size n > 0, and βg ≥ αg > 0. There exists n functions fi : `2 → R such that f̄ is αg strongly convex and βg smooth, and for any t ≥ 0 and any black-box procedure one has, for all i ∈ {1, ..., n},\nf̄(θi,t)− f̄(θ∗) ≥ αg 2\n( 1− 4√\nκg\n)1+ t1+∆τ ‖θi,0 − θ∗‖2,\n(6)where κg = βg/αg .\nThe proof of Theorem 1 relies on splitting the function used by Nesterov to prove oracle complexities for strongly convex and smooth optimization (Nesterov, 2004; Bubeck, 2015) on two nodes at distance ∆. One can show that most dimensions of the parameters θi,t will remain zero, and local gradient computations may only increase the number of non-zero dimensions by one. Finally, at least ∆ communication rounds are necessary in-between every gradient computation, in order to share information between the two nodes. The detailed proof is available as supplementary material.\nCorollary 1. For any graph of diameter ∆ and any blackbox procedure, there exists functions fi such that the time to reach a precision ε > 0 is lower bounded by\nΩ ( √ κg ( 1 + ∆τ ) ln ( 1\nε\n)) , (7)\nThis optimal convergence rate is achieved by distributing Nesterov’s accelerated gradient descent on the global function. Computing the gradient of f̄ is performed by sending all the local gradients ∇fi to a single node (denoted as master node) in ∆ communication steps (which may involve several simultaneous messages), and then returning the new parameter θt+1 to every node in the network (which requires another ∆ communication steps). In practice, summing the gradients can be distributed by computing a spanning tree (with the root as master node), and asking for each node to perform the sum of its children’s gradients before sending it to its parent. Standard methods as described by Bertsekas & Tsitsiklis (1989) can be used for performing this parallelization of gradient computations.\nThis algorithm has three limitations: first, the algorithm is not robust to machine failures, and the central role played\nby the master node also means that a failure of this particular machine may completely freeze the procedure. Second, and more generally, the algorithm requires precomputing a spanning tree, and is thus not suited to time-varying graphs, in which the connectivity between the nodes may change through time (e.g. in peer-to-peer networks). Finally, the algorithm requires every node to complete its gradient computation before aggregating them on the master node, and the efficiency of the algorithm thus depends on the slowest of all machines. Hence, in the presence of non-uniform latency of the local computations, or the slow down of a specific machine due to a hardware failure, the algorithm will suffer a significant drop in performance."
  }, {
    "heading": "3.3. Decentralized Algorithms",
    "text": "The gossip algorithm (Boyd et al., 2006) is a standard method for averaging values across a network when its connectivity may vary through time. This approach was shown to be robust against machine failures, non-uniform latencies and asynchronous or time-varying graphs, and a large body of literature extended this algorithm to distributed optimization (Nedic & Ozdaglar, 2009; Duchi et al., 2012; Wei & Ozdaglar, 2012; Shi et al., 2015; Jakovetić et al., 2015; Nedich et al., 2016; Mokhtari et al., 2016).\nThe convergence analysis of decentralized algorithms usually relies on the spectrum of the gossip matrix W used for communicating values in the network, and more specifically on the ratio between the second smallest and the largest eigenvalue of W , denoted γ. In this section, we show that, with respect to this quantity and κl, reaching a precision ε requires at least Ω( √ κl ln(1/ε)) gradient steps\nand Ω (√\nκl γ ln(1/ε)\n) communication steps, by exhibiting\na gossip matrix such that a corresponding lower bound exists.\nTheorem 2. Let α, β > 0 and γ ∈ (0, 1]. There exists a gossip matrix W of eigengap γ(W ) = γ, and α-strongly convex and β-smooth functions fi : `2 → R such that, for any t ≥ 0 and any black-box procedure using W one has, for all i ∈ {1, ..., n},\nf̄(θi,t)− f̄(θ∗) ≥ 3α\n2\n( 1− 16√\nκl\n)1+ t 1+ τ\n5 √ γ ‖θi,0 − θ∗‖2,\n(8) where κl = β/α is the local condition number.\nThe proof of Theorem 2 relies on the same technique as that of Theorem 1, except that we now split the two functions on a subset of a linear graph. These networks have the appreciable property that ∆ ≈ 1/√γ, and we can thus use a slightly extended version of Theorem 1 to derive the desired result. The complete proof is available as supplementary material.\nCorollary 2. For any γ > 0, there exists a gossip matrix W of eigengap γ and α-strongly convex, β-smooth functions such that, with κl = β/α, for any black-box procedure using W the time to reach a precision ε > 0 is lower bounded by\nΩ ( √ κl ( 1 +\nτ √ γ\n) ln ( 1\nε\n)) . (9)\nWe will see in the next section that this lower bound is met for a novel decentralized algorithm called multi-step dual accelerated (MSDA) and based on the dual formulation of the optimization problem. Note that these results provide optimal convergence rates with respect to κl and γ, but do not imply that γ is the right quantity to consider on general graphs. The quantity 1/ √ γ may indeed be very large compared to ∆, for example for star networks, for which ∆ = 2 and 1/ √ γ = √ n. However, on many simple networks, the diameter ∆ and the eigengap of the Laplacian matrix are tightly connected, and ∆ ≈ 1/√γ. For example, for linear graphs, ∆ = n − 1 and 1/√γ ≈ 2n/π, for totally connected networks, ∆ = 1 and 1/ √ γ = 1, and for regular networks, 1/ √ γ ≥ ∆\n2 √ 2 ln2 n (Alon & Milman,\n1985). Finally, note that the case of totally connected networks corresponds to a previous complexity lower bound on communications proven by Arjevani & Shamir (2015), and is equivalent to our result for centralized algorithms with ∆ = 1."
  }, {
    "heading": "4. Optimal Decentralized Algorithms",
    "text": "In this section, we present a simple framework for solving the optimization problem in Eq. (1) in a decentralized setting, from which we will derive several variants, including a synchronized algorithm whose convergence rate matches the lower bound in Corollary 2. Note that the naive approach of distributing each (accelerated) gradient step by gossiping does not lead to a linear convergence rate, as the number of gossip steps has to increase with the number of iterations to ensure the linear rate is preserved. We begin with the simplest form of the algorithm, before extending it to more advanced scenarios."
  }, {
    "heading": "4.1. Single-Step Dual Accelerated Method",
    "text": "A standard approach for solving Eq. (1) (see Boyd et al. (2011); Jakovetić et al. (2015)) consists in rewriting the optimization problem as\nmin θ∈Rd f̄(θ) = min θ1=···=θn\n1\nn n∑ i=1 fi(θi). (10)\nFurthermore, the equality constraint θ1 = · · · = θn is equivalent to Θ √ W = 0, where Θ = (θ1, . . . , θn) and W is a gossip matrix verifying the assumptions described\nAlgorithm 1 Single-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W ∈\nRn×n, η = αλ1(W ) , µ = √ κl− √ γ√ κl+ √ γ\nOutput: θi,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T − 1 do 3: θi,t = ∇f∗i (xi,t), for all i = 1, ..., n 4: yt+1 = xt − ηΘtW 5: xt+1 = (1 + µ)yt+1 − µyt 6: end for\nin Section 2. Note that, since W is positive semi-definite,√ W exists and is defined as √ W = V >Σ1/2V , where W = V >ΣV is the singular value decomposition of W . The equality Θ √ W = 0 implies that each row of Θ is constant (since Ker( √ W ) = Span(1)), and is thus equivalent to θ1 = · · · = θn. This leads to the following primal version of the optimization problem:\nmin Θ∈Rd×n : Θ √ W=0 F (Θ), (11)\nwhere F (Θ) = ∑n i=1 fi(θi). Since Eq. (11) is a convex problem, it is equivalent to its dual optimization problem:\nmax λ∈Rd×n\n−F ∗(λ √ W ), (12)\nwhere F ∗(y) = supx∈Rd×n〈y, x〉 − F (x) is the Fenchel conjugate of F , and 〈y, x〉 = tr(y>x) is the standard scalar product between matrices.\nThe optimization problem in Eq. (12) is unconstrained and convex, and can thus be solved using a variety of convex optimization techniques. The proposed single-step dual accelerated (SSDA) algorithm described in Alg. (1) uses Nesterov’s accelerated gradient descent, and can be thought of as an accelerated version of the distributed augmented Lagrangian method of Jakovetić et al. (2015) for ρ = 0. The algorithm is derived by noting that a gradient step of size η > 0 for Eq. (12) is\nλt+1 = λt − η∇F ∗(λt √ W ) √ W, (13)\nand the change of variable yt = λt √ W leads to\nyt+1 = yt − η∇F ∗(yt)W. (14)\nThis equation can be interpreted as gossiping the gradients of the local conjugate functions ∇f∗i (yi,t), since ∇F ∗(yt)ij = ∇f∗j (yj,t)i. Theorem 3. The iterative scheme in Alg. (1) converges to Θ = θ∗1> where θ∗ is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given precision ε > 0 is\nO ( (1 + τ) √ κl γ ln ( 1 ε )) . (15)\nThis theorem relies on proving that the condition number of the dual objective function is upper bounded by κlγ , and noting that the convergence rate for accelerated gradient descent depends on the square root of the condition number (see, e.g., Bubeck (2015)). A detailed proof is available as supplementary material."
  }, {
    "heading": "4.2. Multi-Step Dual Accelerated Method",
    "text": "The main problem of Alg. (1) is that it always performs the same number of gradient and gossip steps. When communication is cheap compared to local computations (τ 1), it would be preferable to perform more gossip steps than gradient steps in order to propagate the local gradients further than the local neighborhoods of each node. This can be achieved by replacingW by PK(W ) in Alg. (1), where PK is a polynomial of degree at most K. If PK(W ) is itself a gossip matrix, then the analysis of the previous section can be applied and the convergence rate of the resulting algorithm depends on the eigengap of PK(W ). Maximizing this quantity for a fixed K leads to a common acceleration scheme known as Chebyshev acceleration (Auzinger, 2011; Arioli & Scott, 2014) and the choice\nPK(x) = 1− TK(c2(1− x))\nTK(c2) , (16)\nwhere c2 = 1+γ1−γ and TK are the Chebyshev polynomials (Auzinger, 2011) defined as T0(x) = 1, T1(x) = x, and, for all k ≥ 1,\nTk+1(x) = 2xTk(x)− Tk−1(x). (17)\nFinally, verifying that this particular choice of PK(W ) is indeed a gossip matrix, and taking K = b 1√γ c leads to Alg. (2) with an optimal convergence rate with respect to γ and κl. Theorem 4. The iterative scheme in Alg. (2) converges to Θ = θ∗1> where θ∗ is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given precision ε > 0 is\nO ( √ κl ( 1 +\nτ √ γ\n) ln ( 1\nε\n)) . (18)\nThe proof of Theorem 4 relies on standard properties of Chebyshev polynomials that imply that, for the particular choice of K = b 1√γ c, we have\n1√ γ(PK(W )) ≤ 2. Hence, Theorem 3 applied to the gossip matrix W ′ = PK(W ) gives the desired convergence rate. The complete proof is available as supplementary material."
  }, {
    "heading": "4.3. Discussion and Further Developments",
    "text": "We now discuss several extensions to the proposed algorithms.\nAlgorithm 2 Multi-Step Dual Accelerated method Input: number of iterations T > 0, gossip matrix W ∈\nRn×n, c1 = 1−√γ 1+ √ γ , c2 = 1+γ 1−γ , c3 = 2 (1+γ)λ1(W ) , K = ⌊\n1√ γ\n⌋ , η = α(1+c 2K 1 )\n(1+cK1 ) 2 , µ =\n(1+cK1 ) √ κl−1+cK1 (1+cK1 ) √ κl+1−cK1\nOutput: θi,T , for i = 1, ..., n 1: x0 = 0, y0 = 0 2: for t = 0 to T − 1 do 3: θi,t = ∇f∗i (xi,t), for all i = 1, ..., n 4: yt+1 = xt − η ACCELERATEDGOSSIP(Θt,W ,K) 5: xt+1 = (1 + µ)yt+1 − µyt 6: end for\n7: procedure ACCELERATEDGOSSIP(x,W ,K) 8: a0 = 1, a1 = c2 9: x0 = x, x1 = c2x(I − c3W )\n10: for k = 1 to K − 1 do 11: ak+1 = 2c2ak − ak−1 12: xk+1 = 2c2xk(I − c3W )− xk−1 13: end for 14: return x0 − xKaK 15: end procedure\nComputation of ∇f∗i (xi,t). In practice, it may be hard to apply the dual algorithm when conjugate functions are hard to compute. We now provide three potential solutions to this problem: (1) warm starts may be used for the optimization problem ∇f∗i (xi,t) = argminθ fi(θ) − x>i,tθ by starting from the previous iteration θi,t−1. This will drastically reduce the number of steps required for convergence. (2) SSDA and MSDA can be extended to composite functions of the form fi(θ) = gi(Biθ) + c‖θ‖22 for Bi ∈ Rmi×d and gi smooth, and for which we know how to compute the proximal operator. This allows applications in machine learning such as logistic regression. See supplementary material for details. (3) Beyond the composite case, one can also add a small (well-chosen) quadratic term to the dual, and by applying accelerated gradient descent on the corresponding primal, get an algorithm that uses primal gradient computations and achieves almost the same guarantee as SSDA and MSDA (off by a log(κl/γ) factor).\nLocal vs. Global Condition Number. MSDA and SSDA depend on the worst strong convexity of the local functions mini αi, which may be very small. A simple trick can be used to depend on the average strong convexity. Using the proxy functions gi(θ) = fi(θ)−(αi−ᾱ)‖θ‖22 instead of fi, where ᾱ = 1n ∑ i αi is the average strong convexity, will improve the local condition number from κl = maxi βimini αi to\nκ′l = maxi βi − αi\nᾱ − 1. (19)\nSeveral algorithms, including EXTRA (Shi et al., 2015) and DIGing (Nedich et al., 2016), have convergence rates that depend on the strong convexity of the global func-\ntion αg . However, their convergence rates are not optimal, and it is still an open question to know if a rate close to O (√\nκg(1 + τ√ γ ) ln(1/ε)\n) can be achieved with a decen-\ntralized algorithm.\nAsynchronous Setting. Accelerated stochastic gradient descent such as SVRG (Johnson & Zhang, 2013) or SAGA (Defazio et al., 2014) can be used on the dual problem in Eq. (12) instead of accelerated gradient descent, in order to obtain an asynchronous algorithm with a linear convergence rate. The details and exact convergence rate of such an approach are left as future work."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we compare our new algorithms, single-step dual accelerated (SSDA) descent and multi-step dual accelerated (MSDA) descent, to standard distributed optimization algorithms in two settings: least-squares regression and classification by logistic regression. Note that these experiments on simple generated datasets are made to assess the differences between existing state-of-the-art algorithms and the ones provided in Section 4, and do not address the practical implementation details nor the efficiency of the compared algorithms on real-world distributed platforms. The effect of latency, machine failures or variable communication time is thus left for future work."
  }, {
    "heading": "5.1. Competitors and Setup",
    "text": "We compare SSDA and MSDA to four state-of-the-art distributed algorithms that achieve linear convergence rates: distributed ADMM (D-ADMM) (Shi et al., 2014), EXTRA (Shi et al., 2015), a recent approach named DIGing (Nedich et al., 2016), and the distributed version of accelerated gradient descent (DAGD) described in Section 3.2 and shown to be optimal among centralized algorithms.\nWhen available in the literature, we used the optimal parameters for each algorithm (see Theorem 2 by Shi et al. (2014) for D-ADMM and Remark 3 by Shi et al. (2015) for EXTRA). For the DIGing algorithm, the parameters provided by Nedich et al. (2016) are very conservative, and lead to a very slow convergence. We thus manually optimized the parameter for this algorithm. The experiments are simulated using a generated dataset consisting of 10, 000 samples randomly distributed to the nodes of a network of size 100. In order to assess the effect of the connectivity of the network, we ran each experiment on two networks: one 10 × 10 grid and an Erdös-Rényi random network of average degree 6. The quality metric used in this section is the maximum approximation error\net = max i∈V\nf̄(θi,t)− f̄(θ∗), (20)\nwhere θ∗ is the optimal parameter of the optimization prob-\nlem in Eq. (1). The computation time attributes a unit time per local computation and a time τ per communication."
  }, {
    "heading": "5.2. Least-squares Regression",
    "text": "The regularized least-squares regression problem consists in solving the optimization problem\nmin θ∈Rd\n1 m ‖y −X>θ‖22 + c‖θ‖22, (21)\nwhereX ∈ Rd×m is a matrix containing them data points, and y ∈ Rm is a vector containing the m associated values. The task is thus to minimize the empirical quadratic error between a function yi = g(Xi) of d variables and its linear regression ĝ(Xi) = X>i θ on the original dataset (for i = 1, ...,m), while smoothing the resulting approximation by adding a regularizer c‖θ‖22. For our experiments, we fixed c = 0.1, d = 10, and sampled m = 10, 000 Gaussian random variables Xi ∼ N (0, 1) of mean 0 and variance 1. The function to regress is then yi = X>i 1+cos(X > i 1)+ξi where ξi ∼ N (0, 1/4) is an i.i.d. Gaussian noise of variance 1/4. These data points are then distributed randomly and evenly to the n = 100 nodes of the network. Note\nthat the choice of function to regress y does not impact the Hessian of the objective function, and thus the convergence rate of the optimization algorithms.\nFigure 1 and Figure 2 show the performance of the compared algorithms on two networks: a 10 × 10 grid graph and an Erdös-Rényi random graph of average degree 6. All algorithms are linearly convergent, although their convergence rates scale on several orders of magnitude. In all experiments, the centralized optimal algorithm DAGD has the best convergence rate, while MSDA has the best convergence rate among decentralized methods. In all our experiments, MSDA outperforms SSDA, indicating that performing several communication rounds per gradient iteration never degrades the efficiency of the algorithm, while significantly improving it when τ 1."
  }, {
    "heading": "5.3. Logistic Classification",
    "text": "The logistic classification problem consists in solving the optimization problem\nmin θ∈Rd\n1\nm m∑ i=1 ln ( 1 + e−yi·X > i θ ) + c‖θ‖22, (22)\nwhere X ∈ Rd×m is a matrix containing m data points, and y ∈ {−1, 1}m is a vector containing the m class assignments. The task is thus to classify a dataset by learning a linear classifier mapping data pointsXi to their associated class yi ∈ {−1, 1}. For our experiments, we fixed c = 0.1, d = 10, and sampled m = 10, 000 data points, 5, 000 for the first class and 5, 000 for the second. Each data point Xi ∼ N (yi1, 1) is a Gaussian random variable of mean yi1 and variance 1, where yi = 21{i ≤ 5, 000} − 1 is the true class of Xi. These data points are then distributed randomly and evenly to the n = 100 nodes of the network.\nFigure 3 and Figure 4 show the performance of the compared algorithms for logistic classification on two networks: a 10 × 10 grid graph and an Erdös-Rényi random graph of average degree 6. As for least-squares regression, all algorithms are linearly convergent, and their convergence rates scale on several orders of magnitude. In this case, the centralized optimal algorithm DAGD is outperformed by MSDA, although the two convergence rates are relatively similar. Again, when the communication time is smaller than the computation time (τ 1), performing several communication rounds per gradient iteration will\nimprove the efficiency of the algorithm and MSDA substantially outperforms SSDA. Note that, in Figure 4(a), DADMM requires 383 iterations to reach the same error obtained after only 10 iterations of SSDA, demonstrating a substantial improvement over state-of-the-art methods."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we derived optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications in a network. For the decentralized setting, we introduced the multi-step dual accelerated (MSDA) algorithm with a provable optimal linear convergence rate, and showed its high efficiency compared to other state-of-the-art methods, including distributed ADMM and EXTRA. The simplicity of the approach makes the algorithm extremely flexible, and allows for future extensions, including time-varying networks and an analysis for non-strongly-convex functions. Finally, extending our complexity lower bounds to time delays, variable computational speeds of local systems, or machine failures would be a notable addition to this work."
  }],
  "year": 2017,
  "references": [{
    "title": "λ1, isoperimetric inequalities for graphs, and superconcentrators",
    "authors": ["N. Alon", "V.D. Milman"],
    "venue": "Journal of Combinatorial Theory, series B,",
    "year": 1985
  }, {
    "title": "Chebyshev acceleration of iterative refinement",
    "authors": ["M. Arioli", "J. Scott"],
    "venue": "Numerical Algorithms,",
    "year": 2014
  }, {
    "title": "Communication complexity of distributed convex learning and optimization",
    "authors": ["Arjevani", "Yossi", "Shamir", "Ohad"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "On the iteration complexity of oblivious first-order optimization algorithms",
    "authors": ["Arjevani", "Yossi", "Shamir", "Ohad"],
    "venue": "In 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Dimension-free iteration complexity of finite sum optimization problems",
    "authors": ["Arjevani", "Yossi", "Shamir", "Ohad"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Iterative Solution of Large Linear Systems",
    "authors": ["W. Auzinger"],
    "venue": "Lecture notes, TU Wien,",
    "year": 2011
  }, {
    "title": "Parallel and distributed computation : numerical methods",
    "authors": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"],
    "venue": "PrenticeHall International,",
    "year": 1989
  }, {
    "title": "Randomized gossip algorithms",
    "authors": ["Boyd", "Stephen", "Ghosh", "Arpita", "Prabhakar", "Balaji", "Shah", "Devavrat"],
    "venue": "IEEE/ACM Transactions on Networking (TON),",
    "year": 2006
  }, {
    "title": "Fastest mixing markov chain on graphs with symmetries",
    "authors": ["Boyd", "Stephen", "Diaconis", "Persi", "Parrilo", "Pablo", "Xiao", "Lin"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2009
  }, {
    "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
    "authors": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "Convex optimization: Algorithms and complexity",
    "authors": ["Bubeck", "Sébastien"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2015
  }, {
    "title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
    "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Dual averaging for distributed optimization: Convergence analysis and network scaling",
    "authors": ["Duchi", "John C", "Agarwal", "Alekh", "Wainwright", "Martin J"],
    "venue": "IEEE Transactions on Automatic control,",
    "year": 2012
  }, {
    "title": "Fast distributed gradient methods",
    "authors": ["Jakovetić", "Dušan", "Xavier", "Joao", "Moura", "José MF"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2014
  }, {
    "title": "Linear convergence rate of a class of distributed augmented lagrangian algorithms",
    "authors": ["Jakovetić", "Dušan", "Moura", "José MF", "Xavier", "Joao"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2015
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "A decentralized second-order method with exact linear convergence rate for consensus optimization",
    "authors": ["A. Mokhtari", "W. Shi", "Q. Ling", "A. Ribeiro"],
    "venue": "IEEE Transactions on Signal and Information Processing over Networks,",
    "year": 2016
  }, {
    "title": "DSA: Decentralized double stochastic averaging gradient algorithm",
    "authors": ["Mokhtari", "Aryan", "Ribeiro", "Alejandro"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Distributed subgradient methods for multi-agent optimization",
    "authors": ["Nedic", "Angelia", "Ozdaglar", "Asuman"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2009
  }, {
    "title": "Achieving geometric convergence for distributed optimization over timevarying graphs",
    "authors": ["A. Nedich", "A. Olshevsky", "W. Shi"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Introductory lectures on convex optimization : a basic course",
    "authors": ["Nesterov", "Yurii"],
    "venue": "Kluwer Academic Publishers,",
    "year": 2004
  }, {
    "title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation",
    "authors": ["Shamir", "Ohad"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Distributed stochastic optimization and learning",
    "authors": ["Shamir", "Ohad", "Srebro", "Nathan"],
    "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
    "year": 2014
  }, {
    "title": "On the linear convergence of the ADMM in decentralized consensus optimization",
    "authors": ["Shi", "Wei", "Ling", "Qing", "Yuan", "Kun", "Wu", "Gang", "Yin", "Wotao"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2014
  }, {
    "title": "EXTRA: An exact first-order algorithm for decentralized consensus optimization",
    "authors": ["Shi", "Wei", "Ling", "Qing", "Wu", "Gang", "Yin", "Wotao"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "A distributed newton method for large scale consensus optimization",
    "authors": ["R. Tutunov", "H.B. Ammar", "A. Jadbabaie"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Distributed alternating direction method of multipliers",
    "authors": ["Wei", "Ermin", "Ozdaglar", "Asuman"],
    "venue": "In 51st Annual Conference on Decision and Control (CDC),",
    "year": 2012
  }, {
    "title": "Fast linear iterations for distributed averaging",
    "authors": ["Xiao", "Lin", "Boyd", "Stephen"],
    "venue": "Systems & Control Letters,",
    "year": 2004
  }],
  "id": "SP:edf54f3499ea67d45dfb69388838def47a621753",
  "authors": [{
    "name": "Kevin Scaman",
    "affiliations": []
  }, {
    "name": "Francis Bach",
    "affiliations": []
  }, {
    "name": "Sébastien Bubeck",
    "affiliations": []
  }, {
    "name": "Yin Tat Lee",
    "affiliations": []
  }, {
    "name": "Laurent Massoulié",
    "affiliations": []
  }],
  "abstractText": "In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov’s accelerated gradient descent is optimal and achieves a precision ε > 0 in time O(κg(1 + ∆τ) ln(1/ε)), where κg is the condition number of the (global) function to optimize, ∆ is the diameter of the network, and τ (resp. 1) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision ε > 0 in time O( √ κl(1 + τ √ γ ) ln(1/ε)), where κl is the condition number of the local functions and γ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-theart methods for two problems: least-squares regression and classification by logistic regression.",
  "title": "Optimal Algorithms for Smooth and Strongly ConvexDistributed Optimization in Networks"
}