{
  "sections": [{
    "heading": "1. Introduction",
    "text": "It is well-known that sufficiently large multi-layer feedforward networks can approximate any function with desired accuracy (Hornik et al., 1989). An important problem then is to determine the smallest neural network for a given task and accuracy. The standard guideline is the approximation power (variously known as expressiveness) of the network which quantifies the size of the neural network, typically in terms of depth and width, in order to approximate a class of functions within a given error. In particular, several works provided evidence that deeper networks perform better than shallow ones, given a fixed number of hidden units (Bianchini & Scarselli, 2014; Delalleau & Bengio, 2011; Liang & Srikant, 2017; Mhaskar et al., 2016; Pascanu et al., 2014; Telgarsky, 2015; 2016; Yarotsky, 2017).1\nA popular activation function is the rectified linear unit (ReLU), partly because of its low complexity when coupled with backpropagation training (Krizhevsky et al., 2012). It has, therefore, become of interest to determine the power of neural networks with ReLU’s and, more generally, with piecewise linear activation functions.\n1Department of Electrical Engineering, Sharif University of Technology, Iran 2Department of Communications and Electronics, Telecom ParisTech, France. Correspondence to: Mohammad Mehrabi <mohamadmehrabi4@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1For a nice counterexample see (Lu et al., 2017).\nDetermining the capacity of a neural networks with a piecewise linear activation function typically involves two steps. First, evaluate the number of linear pieces (or break points) that the network can produce and, second, tie this number to the approximation error. The works (Montufar et al., 2014; Pascanu et al., 2014) recently showed that a linear increase in depth results in an exponential growth in the number of linear pieces as opposed to width which results only in a polynomial growth. Accordingly, the approximation capacity exhibits a similar tradeoff between depth and width. For related works with respect to classification error see (Telgarsky, 2015; 2016) and with respect to function approximation error see (Liang & Srikant, 2017; Mhaskar et al., 2016; Yarotsky, 2017).\nIn this paper we consider general feedforward neural networks with piecewise linear activation functions and establish bounds on the size of the network in terms of the approximation error, the depth d, the width, and the dimension of the input space to approximate a given function. We first establish an improved upper bound on the number of break points that such a network can produce which is a multiplicative factor dd smaller than the currently best known from (Yarotsky, 2017). This upper bound is obtained by investigating neuron state transitions as introduced in (Raghu et al., 2017). Combining this upper bound with lower bounds in terms of error and dimension, we obtain necessary conditions on the depth, width, error, and dimension for a neural network to approximate a given function. These bounds significantly improve on the corresponding state-of-the-art bounds for certain classes of functions (Theorems 1,2 and Corollaries 1,2,3).\nThe second contribution of the paper (Theorem 3) is an upper bound on the difference of two neural networks with identical weights but different activation functions. This problem is related to “activation function simulation” investigated in (DasGupta & Schnitger, 1993) which leverages network topology to compensate a change in activation function.\nThe paper is organized as follows. In Section 2 we briefly introduce the setup. In Section 3 we present the main results which are then compared with the corresponding ones in the recent literature in Section 4. Finally, Section 5 contains the proofs."
  }, {
    "heading": "2. Preliminaries",
    "text": "Throughout the paper R denotes a compact convex set in Rn, n ≥ 1, and Fσ denotes the set of feedforward neural networks with input R, output R, and activation function σ : R → R. Feedforward here refers to the fact that the neural network contains no cycles; connections are allowed between non-neighbouring layers. It is assumed that σ is a piecewise linear (not necessarily continuous) function with t ≥ 1 linear pieces. The set of all such activation functions is denoted by Σt.\nA neural network f ∈ Fσ consists of a set of input units If , a set of hidden units Hf that operate according to σ, non-zero weights representing connections, and a single output unit which just weight-sums its inputs. To simplify the notation we use f to represent both a neural network and the function that it represents.\nFor instance, in the neural network shown in Fig. 1, we have If = {x1, x2, x3} andHf = {uij , ∀i, j}. Definition 1 (Depth and width). Given a neural network f ∈ Fσ, the depth of a hidden unit h ∈ Hf , denoted as df (h), is the length of the longest path from any i ∈ If to h. The depth of f is\ndf def = max { df (h) ∣∣h ∈ Hf}. The set of hidden units with depth i is\nHif def = { h ∈ Hf ∣∣df (h) = i}. The width of the network is\nωf def = |Hf | df def = ∑df i=1 ωi df\n(1)\nwhere ωi def = |Hif |.\nFor instance, in Fig. 1, the hidden unit u23 can be reached by inputs x1 and x3, by following the paths x1 → u23, x3 → u11 → u23, or x3 → u12 → u23. Therefore, df (u23) = 2. The hidden units of maximum depth are u31, u32, and u33 and hence df = 3,H3f = {u31, u32, u33} and ωf = 8/3.\nThe following simple inequality is frequently used in the paper. Lemma 1. For any t ≥ 1, df ≥ 1, and |Hf | ≥ 1\n((t− 1)ωf + 1)df ≤ t|Hf |.\nProof. Set ωf = |Hf | df\nand observe that( (t− 1) |Hf |\ndf + 1 )df is a non-decreasing function of df and that df ≤ |Hf |.\nDefinition 2 (Affine ε-approximation). Function f ∈ Fσ is an affine ε-approximation of a function g : R → R if\nsup x∈R |f(x)− g(x)| ≤ ε.\nDefinition 3 (Break point). Given (x,y) ∈ R2, function f : R → R admits a break point at α0 ∈ (0, 1) relative to the segment [x,y] if the first order derivative of f((1− α)x + αy) does not exist at α = α0. The total number of break points of f on the (open) segment ]x,y[ is denoted by Bx→y(f). Finally, we let B̄x→y(f) def = Bx→y(f) + 1.\nSince f is piecewise linear B̄x→y(f) simply counts the number of linear pieces that f produces as the input ranges from x to y."
  }, {
    "heading": "3. Main Results",
    "text": "Theorems 1,2 and Corollaries 2,3 provide bounds on the size of a neural network to approximate a given function. These bounds are expressed in terms of the approximation error and width and depth of the network, but hold irrespectively of the weights. Recall that connections are allowed between non-neighboring layers.\nAs a notational convention we use C2(R) to denote the set of functionsR → R whose second order partial derivatives are continuous over R̊ (the interior ofR).\nTheorem 1. Let f ∈ Fσ, σ ∈ Σt, be an ε-approximation of a function g ∈ C2(R) and let x,y ∈ R. Then,\n( (t− 1)ωf + 1 )df ≥ B̄x→y(f) (2)\n≥ ||x− y||2 4 √ ε ·Ψ(g,x,y), (3)\nwhere\nΨ(g,x,y) def = √ inf\n0≤α≤1\n( max { 0, γ(α)δ(α) }) , (4)\nγ(α) def = min { |α1(α)|, |α2(α)| } ,\nδ(α) def = sign ( α1(α)α2(α) ) ,\nand where α1(α) and α2(α) are the largest and smallest eigenvalues of the hessian matrix ∇2g ( (1 − α)x + αy ) , respectively.\nMaximizing the right-hand side of (3) over x,y and using Lemma 1 we obtain:\nCorollary 1. Under the assumptions of Theorem 1 we have\n|Hf | ≥ logt\n( sup\n(x,y)∈R2\n{ ||x− y||2 4 √ ε ·Ψ(g,x,y) }) .\nA function g : R → R that is twice differentiable is said to be strongly convex with parameter µ if ∇2g(x) µI for all x ∈ R̊. Corollary 2. Let f ∈ Fσ, σ ∈ Σt, be an ε-approximation of a function g ∈ C2(R) that is strongly convex with parameter µ > 0. Then,\n|Hf | ≥ 1\n2 logt (µ · (diam(R))2 16ε ) ,\nwhere diam(R) def= sup\n(x,y)∈R ||x− y||2.\nProof. By strong convexity Ψ(g,x,y) ≥ √µ. The result then follows from Theorem 1 and Lemma 1.\nAs an example, consider g(x) = x · x over [0, 1]n. The Hessian matrix is 2In×n and from Corollary 2 we get\n|Hf | ≥ log2 (√ n\n8ε\n) .\nCorollary 3. Let R = [0, 1]n. Let f ∈ Fσ, σ ∈ Σ2,2 be an ε-approximation of a function g ∈ C2(R) such that ∇g(x) 0 for any x ∈ R̊. Then,\n|Hf | ≥ q(g)dfε − 12df (5)\nwhere q(g) > 0 is a constant that only depends on g.\nProof of Corollary 3. From Theorem 1 we get(Hf df + 1 )df ≥ c(g)√ ε ,\n2Recall that Σ2 includes ReLU’s.\nwhere c(g) > 0 is some strictly positive constant, since the Hessian of g is positive definite everywhere over R̊. Since Hf/df ≥ 1 the above inequality implies(\n2 |Hf | df\n)df ≥ c√\nε .\nSince 12c 1 df ≥ q where q = 12 min(c, 1), the above inequality yields the desired result.\nTheorem 2. Let R = [0, 1]n. Let f ∈ Fσ, σ ∈ Σt, be an ε-approximation of a function g : R → R such that |DJ(g)(x)| ≤ δ for any x ∈ [0, 1]n and any multi-index3 J such that |J | = 3. Then,\n( (t−1)ωf+1 )df ≥ √√√√( max x∈[0,1]n ∣∣∆(g)(x)∣∣n−1 − δn 32)+ 16ε , (6) where\n∆(g)(x) = n∑ k=1 d2g dx2k , (7)\nis the Laplacian of g and where a+ = max(a, 0).\nFor instance, approximating\ng(x1, x2) = 10x 2 1 + x 2 1x 2 2 + 10x 2 2\nover [0, 1]2 requires logt ( 0.82√ ε ) hidden units—combine Theorem 2 with Lemma 1.\nWhether it is Theorem 1 or Theorem 2 which provides a better approximation bound depends on g. For instance, for g1(x1, x2) = 20x 2 1 − 2x22 + x21x22 Theorem 1 gives a trivial (zero) lower bound since the two eigenvalues of the Hessian matrix ∇2(g1) have always different signs. Theorem 2 instead gives 0.737√\nε . On the other hand, for g2(x1, x2) =\n10x21 + 10x 2 2 + x 2 1x 2 2 Theorem 1 gives 1.37√ ε as lower bound while Theorem 2 gives 0.82√\nε .\nThe next theorem quantifies the effect of a change of activation function on the output of the neural network. Here, the activation functions need not be piece-wise affine.\nTheorem 3. Let f1 ∈ Fσ1 and f2 ∈ Fσ2 be two neural networks with identical architectures and weights. Suppose that σ1 is a δ-Lipschitz continuous function and suppose that the weights belong to some bounded interval [−A,+A], A > 0. Then,\n||f1−f2||∞ ≤ ||σ1 − σ2||∞\nδ\n(( δ ·A·ωf+1 )df −1 ) . (8)\n3E.g., for J = (2, 1) we have DJ(g(x1, x2)) = ∂ 3g\n∂2x1∂x2 .\nA slightly weaker version of (8) is\n||f1 − f2||∞ ≤ ||σ1 − σ2||∞\nL\n(( L2 · ωf + 1 )df − 1 ) ,\nwhere L = max{A, δ} denotes the Lipschitz-bound defined in (DasGupta & Schnitger, 1993).\nAs an illustration of Theorem 3 consider a feedforward neural network f1 with 100 hidden units, a maximum depth of 5, and the sigmoid as activation function. Suppose the weights belong to interval [−1, 1]. Replacing the sigmoid with a 32-bit quantized function results in an error of at most 0.0001—which can readily be obtained from Theorem 3 with δ = 14 , A = 1, ||σ1 − σ2||∞ = 2 −32."
  }, {
    "heading": "4. Comparison with Previous Works",
    "text": "Consider first the inequality (2). Restricting attention to neural networks with d hidden layers, at most ω units per layer, and where connections are allowed only between neighbouring layers, this inequality gives\nB̄x→y(f) ≤ ( (t− 1)ω + 1 )d . (9)\nThis is to be compared with the previously best known bound (Lemma 3.2 in (Telgarsky, 2016))\n2(2(t− 1)ω)d\nwhich is larger by a multiplicative factor that is exponential in d whenever ω > 1, t ≥ 2. For n = 1, Lemma 2.1 in (Telgarsky, 2015) gives (tω)d which still differs from (9) by a multiplicative factor that is exponential is d for ω > 1, t ≥ 2.\nFor general feedforward neural networks the previously best known bound (see Lemma 4 of (Yarotsky, 2017)) was\nB̄x→y(f) ≤ ( t · ω · df )df which is a multiplicative factor df df larger than (2).\nNow consider the approximation power of neural networks in terms of number of hidden units required to approximate a given function within a given error. Theorem 11 in (Liang & Srikant, 2017) states that to approximate a function [0, 1]n → R, assumed to be differentiable and strongly convex with parameter µ, with a neural network f requires\n|Hf | ≥ 1\n2 log2 ( µ 16ε ) ,\nregardless of the dimension n. Corollary 2 improves this bound to\n1 2 log2 (µ · n 16ε )\nwhich incorporates dimension as well—albeit the dependency on dimension is arguably small.\nCorollary 3 provides a lower bound for ReLU types of networks in terms of the error, the depth, and a constant term which only depends on g. This bound can be compared with the bound of Theorem 6 in (Yarotsky, 2017) which is of order − 12df .4 Hence, Corollary 3 provides a linear (in df ) improvement which is particularly relevant in the deep regime where df = Ω(log(1/ε)). Table 1 summarizes the above discussion.\nTo the best of our knowledge Theorem 3 is the first result to bound the effect of a change in the activation function for given network topology and weights. Noteworthy perhaps, this bound is essentially universal in the weights since it only depends on their range.\nFinally, compared to the cited papers it should perhaps be stressed that the proofs here (see next section) are relatively elementary—e.g., they do not hinge on VC dimension analysis—and hold true for general feedforward networks."
  }, {
    "heading": "5. Analysis",
    "text": "We first establish a few lemmas to prove Proposition 1 which will provide an upper bound on the number of break points. Then we establish Propositions 2 and 3 which will give lower bounds on the number of break points in terms of the approximation error. Combining these propositions will give Theorems 1 and 2. Finally, we prove Theorem 3.\nDefinition 4 (Intermediate set of units). Given f ∈ Fσ and 4Theorem 6 of (Yarotsky, 2017) provides a bound of the form q − 1\n2df where q is a constant that depends on both g and df . However, a close inspection of the proof of this theorem reveals that q depends only on g.\nU ⊆ Hf we define the set of hidden units that lie on a path between the input and U as\nin(U) def= { v ∈ Hf\\U|∃i ∈ If , u ∈ U s.t. v ∈ (i→ u) } where (i→ u) denotes the set of intermediate hidden nodes on the path from i to u.\nFor instance, in Fig. 1 we have\nin({u32}) = {u11, u12, u21, u23}.\nThe following lemma follows from the above definition.\nLemma 2. Given U ⊆ Hf we have\nin(in(U) = ∅\nand in(u) ⊆ (U ∪ in(U))\nfor any u ∈ U . Definition 5 (State). Any σ ∈ Σt partitions the real line (its input) into t intervals I1, I2, ..., It such that on each of these intervals σ is affine. The state of a unit with activation function σ is defined to be s ∈ {1, 2, . . . , t} if its input belongs to Is. By extension, the state of U ⊆ Hf is defined to be the vector of length |U| whose components are the state of each unit in U .\nThe following definition is inspired by the notion of pattern transition introduced in (Raghu et al., 2017):\nDefinition 6 (Transition). Let f ∈ Fσ , U ⊆ Hf and x,y ∈ R. Let zα = (1 − α)x + αy be a parametrization of the line segment [x,y] as α goes from 0 to 1. We say that the state of U experiences a transition at point zα∗ for some α∗ ∈ (0, 1] if the state vector of U changes at zα∗ while the state vector of in(U) does not change at zα∗ . The number of state transitions of U on the segment [x,y], denoted by Nx→y(U), is defined to be the number of state transitions of U as the input changes from x to y on zα. If in(U) = ∅, then Nx→y(U) is defined to be the number of state transitions of U as the input changes from x to y.\nNote that if the state vectors of both U and in(U) change atα, Nx→y(U) does not change at that α. For example, consider the neural network f in Fig. 1. Suppose that U = {u11, u12} and suppose that the state of u11 and u12 changes exactly once along segment zα for some x and y, respectively at α1 and α2. Then Nx→y({u11}) = 1 and Nx→y({u12}) = 1. If α1 = α2, Nx→y(U) = 1, otherwise Nx→y(U) = 2. If U ′ = {u21, u22, u23}, and the state of each of u21, u22 and u23 changes exactly once at either α1 or α2, then Nx→y(U ′) = 0 since the state vector of in(U ′) = U has also changed at both α1 and α2.\nLemma 3. Given f ∈ Fσ and U1,U2 ⊆ Hf such that in(U2) = ∅ and in(U1) ⊆ U2, we have\nNx→y ( U1 ∪ U2 ) ≤ Nx→y ( U1 ) +Nx→y ( U2 ) .\nProof. Suppose Nx→y ( U1 ∪ U2 ) increases by one at α = α∗. If U2 undergoes a state transition at α∗ then, because in(U2) = ∅, we have that Nx→y ( U2 )\nalso increases by one at α∗. Instead, if no state change happens in U2 at α∗ then, due to the state change of U1∪U2 at α∗, the state of U1 must change as well at α∗. Since in(U1) ⊆ U2 and no change in the state of U2 is observed at α∗ we have that Nx→y ( U1 ) necessarily increases by one at α∗.\nLemma 4. Given f ∈ Fσ and U1,U2 ⊆ Hf such that U1 ⊆ U2 and in(U2) = ∅ we have\nNx→y ( U1 ) ≤ Nx→y ( U2 ) .\nProof. Suppose Nx→y ( U1 )\nincreases by one at α∗. Since U1 ⊆ U2 the state of U2 changes as well at α∗. Since in(U2) = ∅ we deduce that Nx→y ( U2 )\nincreases at α∗ by one, thereby concluding the proof.\nLemma 5. Given f ∈ Fσ , for any U ⊆ Hf we have\nNx→y(U) ≤ ∑ u∈U Nx→y(u).\nProof. Suppose that Nx→y(U) increases by one at α∗. Let V ⊆ U be the set of units that experience a transition at α∗. Since we have a transition in the state of U at α∗ we have V 6= ∅. Now, because the neural network is cycle-free,5 there exists some v ∈ V such that in(v) ∩ V = ∅. We claim that the state of in(v) has not changed at α∗. To prove this note that by Lemma 2 we have in(v) ⊆ in(U)∪U and since in(v) ∩ V = ∅ we deduce that in(v) ⊆ (in(U) ∪ U\\V). On the other hand neither U\\V nor in(U) has a transition at α∗. This implies that in(v) has no transition at α∗ and therefore Nx→y(v) increases by one at α∗. This concludes the proof since v ∈ U .\nLemma 6. Given f ∈ Fσ , for any u ∈ Hf we have\nNx→y(u) ≤ (t− 1) ( Nx→y(in(u)) + 1 ) .\nProof. To establish the lemma we show that between transitions of in(u) there are at most t− 1 transitions of u.\n5Recall that throughout the paper neural networks are feedforward.\nSuppose, by way of contradiction, that at least t transitions in the state of u happen while in(u) experiences no change. Then there exists an increasing sequence of real numbers α1, ..., αt+1 from interval [0, 1] and an increasing set of integers k1, k2, ..., kt+1 from S = {1, 2, ..., t}, with ki 6= ki+1, such that for particular w ∈ Rn and b ∈ R we have\nxi def = (1− αi)x + αiy\nw · xi + b ∈ Iki where Ii is defined in Definition 5. Since |S| = t there exists i < j such that ki = kj . Now since ki 6= ki+1 we deduce that j 6= i + 1 and therefore j > i + 1. But w · xi+1 + b lies between w · xi + b and w · xj + b since the sequence α1, α2, ..., αt+1 is increasing. Since w ·xj+b and w ·xi+b belong to Iki , by the connectedness property of the set Ii we deduce that that w · xi+1 + b ∈ Ii. Therefore, we get ki = ki+1 = kj , a contradiction.\nSince a break point of f ∈ Fσ necessarily implies a change in the state of the units we get: Lemma 7. Given (x,y) ∈ R2 and f ∈ Fσ we have\nBx→y(f) ≤ Nx→y(Hf ).\nPropositions 1 and 2 establish inequalities (2) and (3) of Theorem 1. Proposition 1. Given f ∈ Fσ , σ ∈ Σt, we have\nBx→y(f) ≤ (( t− 1 ) ωf + 1 )df − 1. (10)\nProof of Proposition 1. Fix f ∈ Fσ where σ ∈ Σt. Referring to Definition 1, consider the partition\n∪di=1Hif ofHf according to unit depth where d = df .\nFix u ∈ Hi+1f , 0 ≤ i < d. From the definitions of in(u) andHif we get\nin(u) ⊆ i⋃\nj=1\nHjf (11)\nin ( Hi+1f ) ⊆ i⋃ j=1 Hjf\nin ( i⋃ j=1 Hjf ) = ∅.\nApplying Lemma 3 with U1 = Hi+1f and U2 = i⋃\nj=1\nHjf we\nget\nNx→y( i+1⋃ j=1 Hjf ) ≤ Nx→y( i⋃ j=1 Hjf ) +Nx→y(H i+1 f ).\nFrom Lemma 5\nNx→y( i+1⋃ j=1 Hjf ) ≤ Nx→y( i⋃ j=1 Hjf ) + ∑\nu∈Hi+1f\nNx→y(u)\nand applying Lemma 6 to the previous inequality\nNx→y( i+1⋃ j=1 Hjf ) ≤ Nx→y( i⋃ j=1 Hjf )\n+ ∑\nu∈Hi+1f\n( t− 1 )( Nx→y ( in(u) ) + 1 ) .\nThen, using (11) and Lemma 4 we get\nNx→y( i+1⋃ j=1 Hjf )\n≤ Nx→y( i⋃\nj=1\nHjf ) + ∑\nu∈Hi+1f\n( t− 1 )( Nx→y ( i⋃ j=1 Hjf ) + 1 )\n= ( ωi+1(t− 1) + 1 ) Nx→y( i⋃ j=1 Hjf ) + ωi+1(t− 1).\n(12)\nFor u ∈ H1f we have in(u) = ∅ and according to Lemma 6 we deduce that Nx→y(H1f ) ≤ (t− 1)ω1. With this initial condition and the recursive relation in (12) we get\nNx→y( d⋃ j=1 Hjf )\n≤ d∑ j=1 ( ∏ 1≤α1<α2<...<αj≤d ωα1ωα2 · · ·ωαj ( t− 1 )j) ≤ d∑ j=1 ((d j )( ωf (t− 1) )j) = ( ωf (t− 1) + 1 )d − 1\nwith ωf as width of f . Finally, apply Lemma 7 to obtain\nBx→y(f) ≤ (( t− 1 ) ωf + 1 )df − 1.\nProposition 2. Let R be a convex region in Rn. For any affine ε-approximation f : R → R of a function g ∈ C2(R) we have\nBx→y(f) ≥ ||x− y||2\n4 √ ε ·Ψ(g,x,y)− 1 (13)\nwhere Ψ(g,x,y) is defined in (4).\nProof of Proposition 2. We partition R into convex subregionsRi, such that in each subregion f(x) is an affine function. These convex subregions partition a segment [x,y] into sub-segments with end points { x0,x1, ...,xs } , where\nx0 = x,xs = y and s = Bx→y(f)+1. In the sub-segment i ∈ {0, 1, ..., s− 1},\nf(x) = pi.x + qi, x ∈ [xi,xi+1], (14)\nfor some pi and qi. Let xi(r) = (1 − r)xi + rxi+1, r ∈ [0, 1], and define\nfi(r) = (1− r)g(xi) + rg(xi+1), hi(r) = g ( xi(r) ) ,\nli(r) = f(x(r)).\nFrom the definition of ε-approximation, ||hi(r)−li(r)||∞ ≤ ε. Thus\n||fi(r)−hi(r)||∞ ≤ ||fi(r)− li(r)||∞ + ||li(r)− hi(r)||∞ (a) ≤ max { |fi(0)− li(0)|, |fi(1)− li(1)| } + ε\n≤ 2ε, (15)\nwhere ||k(r)||∞ = sup 0≤r≤1 k(r) and step (a) follows because fi(r) and li(r) are both line segments and the maximum distance between them is achieved at end points.\nAs h(r) on (0, 1) is differentiable so there exists r∗i ∈ (0, 1) such that h′i(r ∗ i ) = hi(1) − hi(0). Consider x∗i = (1 − r∗i )xi + r ∗ i xi+1. From (15) we obtain\n|(1− r∗i ) ( g(xi)− g(xi+1) ) − g(x∗i ) + g(xi+1)| ≤ 2ε,\n|r∗i ( g(xi+1)− g(xi) ) + g(xi)− g(x∗i )| ≤ 2ε.\nThen, from the definition of r∗i we have\n|(r∗i − 1)∇g(x∗i ).(xi+1 − xi)− g(x∗i ) + g(xi+1)| ≤ 2ε (16)\n|r∗i∇g(x∗i ).(xi+1 − xi)− g(x∗i ) + g(xi)| ≤ 2ε. (17) Since g ∈ C2(R) a Taylor expansion of g(xi) and g(xi+1) around x∗i gives\ng(xi) = g(x ∗ i )− r∗i∇g ( x∗i ) .(xi+1 − xi)\n+ r∗i 2\n2 (xi+1 − xi)T∇2g\n( xi(αi) ) (xi+1 − xi),\ng(xi+1) = g(x ∗ i ) + (1− r∗i )∇g ( x∗i ) .(xi+1 − xi)\n+ (1− r∗i ) 2\n2 (xi+1 − xi)T∇2g\n( xi(βi) ) (xi+1 − xi),\nwhere 0 ≤ αi ≤ r∗i ≤ βi ≤ 1.\nSubstituting the above relations in inequalities (16) and (17) we get\n|(1− r∗i ) 2 (xi+1 − xi)T∇2g ( xi(βi) ) (xi+1 − xi)| ≤ 4ε,\n(18)\n|r∗i 2(xi+1 − xi)T∇2g ( xi(αi) ) (xi+1 − xi)| ≤ 4ε. (19)\nUse the Rayleigh quotient and the definitions of θ(α), γ(α) to obtain\n| (xi+1 − xi)T∇2g\n( xi(αi) ) (xi+1 − xi)\n(xi+1 − xi)T (xi+1 − xi) |\n≥ inf 0≤α≤1\n( max { 0, θ(α)γ(α) }) .\nCombining the above inequality with (18) and (19) and the fact that r∗i 2 + (1− r∗i )2 ≥ 12 we get\n||xi+1 − xi||22. inf 0≤α≤1\n( max { 0, θ(α)γ(α) }) ≤ 16ε.\nAccordingly, s−1∑ i=0 ( ||xi+1 − xi||2 4 √ ε . √ inf 0≤α≤1 ( max { 0, θ(α)γ(α) })) ≤ s,\nwhich gives\nBx→y(f) ≥ ||x− y||2\n4 √ ε Ψ(g,x,y)− 1.\nProposition 3. Let g : [0, 1]n → R be such that DJ(g)(x) ≤ δ for any x ∈ [0, 1]n and any multi-index J such that |J | = 3. Then, for any affine ε-approximation f\nBx→y(f) ≥\n√√√√( max x∈[0,1]n ∣∣∆(g)(x)∣∣ · n−1 − δ · n 32)+ 16ε − 1\nfor any x,y ∈ [0, 1]n, where ∆ denotes the Laplace operator (7).\nProof of Proposition 3. Define\nz def = arg max x∈R ρ ( ∇2g(x) ) where ρ(·) denotes the spectral radius. Let u be a normalized eigenvector corresponding to an eigenvalue λ where |λ| = ρ ( ∇2g(z) ) , i.e.,\n∇2g(z)u = λu, ||u|| = 1. (20)\nConsider any segment [x,y] in R in the direction of u, i.e., such that x− y = u. The convex subregions of f , defined in the proof of Proposition 2, divide this segment into sub-segments with end points {x0,x1, ...,xs} where x0 = x,xs = y and s = Bx→y(f) + 1. Using the same\nanalysis as in the proof of Proposition 2, from (14)–(19) we obtain (18) and (19). On the other hand, note that\n|(xi+1 − xi)T∇2g ( xi(αi) ) (xi+1 − xi)|\n≥ |(xi+1 − xi)T∇2g ( z ) (xi+1 − xi)|\n− |(xi+1 − xi)T ( ∇2g ( xi(αi) ) −∇2g ( z )) (xi+1 − xi)|\n= |λ| · ||xi+1 − xi||2 − ∣∣tr{(∇2g(xi(αi))−∇2g(z))(xi+1 − xi)(xi+1 − xi)T}∣∣\n(a) ≥ |λ| · ||xi+1 − xi||2 − ∣∣∣∣∇2g(xi(αi))−∇2g(z)∣∣∣∣F∣∣∣∣(xi+1 − xi)(xi+1 − xi)T ∣∣∣∣F = |λ| · ||xi+1 − xi||2\n− ∣∣∣∣∇2g(xi(αi))−∇2g(z)∣∣∣∣F||xi+1 − xi||2\n= ||xi+1 − xi||2 · ( |λ| − nδ · ||z − xi(αi)|| ) ≥ ||xi+1 − xi||2 · ( |λ| − δ · n 32 ) ,\nwhere in step (a) we used the inequality∣∣∣tr(AB)∣∣∣ ≤ ||A||F ||B||F , || · ||F stands for Frobenius norm.\nCombining the above relation with (18), (19) and the fact that r∗i 2 + (1− r∗i )2 ≥ 12 we get\n16ε ≥ ||xi+1 − xi||2 · ( |λ| − δ · n 32 ) ,\nwhich gives\n4 √ ε · ( Bx→y(f) + 1 ) ≥ ||x− y|| · √( |λ| − δ · n 32 )+ .\nFinally, rewriting the above inequality we get\nBx→y(f) ≥ 1 4 √ ε · √( |λ| − δ · n 32 )+ − 1.\nSince |λ| = ρ ( ∇2g(z) ) = max x∈[0,1]n ρ ( ∇2g(x) ) and\n|∆(g)(x)| = |tr(∇2g(x))| ≤ ρ(∇2g(x)) · n,\nwe obtain the desired result.\nProofs of Theorems 1 and 2\nPropositions 1 and 2 give Theorem 1 and Propositions 1 and 3 give Theorem 2.\nProof of Theorem 3\nGiven a neural network f we use o to denote the output unit, w(u, v) to denote the weight of two connected units u and\nv, and b(u) to denote the bias of unit u. Furthermore, given u ∈ Hf and x ∈ R let fu1 (x) denote the output of unit u when the input to f1 is x, and similarly for f2(x). Finally, define the maximum change in hidden layer i as\nεi(x) def = max\nu∈Hif\n{ |fu1 (x)− fu2 (x)| } .\nFix 1 ≤ i ≤ df − 1 and v ∈ Hi+1f . Then,∣∣fv1 (x)− fv2 (x)∣∣ =\n∣∣∣∣∣σ1( ∑ u∈\ni⋃ j=1 Hjf\nw(u, v) · fu1 (x) + b(v) )\n− σ2 ( ∑ u∈\ni⋃ j=1 Hjf\nw(u, v) · fu2 (x) + b(v) )∣∣∣∣∣\n≤ ε+ δ · ( ∑ u∈\ni⋃ j=1 Hjf\n|w(u, v)| · ∣∣fu1 (x)− fu2 (x)∣∣)\n≤ ε+ δA · ( i∑ j=1 ∑ u∈Hjf ∣∣fu1 (x)− fu2 (x)∣∣)\n≤ ε+ δA · ( i∑ j=1 ωjεj(x) )\nwhere the first inequality holds since σ1 is δ-Lipschitz and assuming that ||σ1−σ2||∞ ≤ ε. Hence we get the recursion between εi’s\nεi+1(x) ≤ ε+ δA · ( i∑ j=1 ωjεj(x) )\n(21)\nfor 1 ≤ i ≤ df − 1. Now, since ε1(x) ≤ ∣∣σ1(x)− σ2(x)∣∣ we get ε1(x) ≤ ε. From this initial condition and (21)\nεi+1(x) ≤ ε(1 + δAω1)(1 + δAω2) · · · (1 + δAωi). (22)\nOn the other hand we have |f1(x)− f2(x)| = ∣∣∣ ∑ u∈\ndf⋃ j=1 Hjf\nw(u, o) · ( fu1 (x)− fu2 (x) )∣∣∣ ≤ A ( ε1(x)ω1 + ε2(x)ω2 + ...+ εd(x)ωdf\n) and from (22) we finally get\n|f1(x)− f2(x)|\n≤ ε δ\n( (1 + δAω1)(1 + δAω2)...(1 + δAωdf )− 1 ) ≤ ||σ1 − σ2||∞\nδ\n(( δ ·A · ωf + 1 )df − 1 ) which gives the desired result."
  }],
  "year": 2018,
  "references": [{
    "title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures",
    "authors": ["Bianchini", "Monica", "Scarselli", "Franco"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
    "year": 2014
  }, {
    "title": "The power of approximating: A comparison of activation functions",
    "authors": ["DasGupta", "Bhaskar", "Schnitger", "Georg"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 1993
  }, {
    "title": "Shallow vs. deep sum-product networks",
    "authors": ["Delalleau", "Olivier", "Bengio", "Yoshua"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2011
  }, {
    "title": "Multilayer feedforward networks are universal approximators",
    "authors": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"],
    "venue": "Neural Networks,",
    "year": 1989
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2012
  }, {
    "title": "Why deep neural networks for function approximation",
    "authors": ["Liang", "Shiyu", "R. Srikant"],
    "venue": "In 5th International Conference on Learning Representations (ICLR),",
    "year": 2017
  }, {
    "title": "The expressive power of neural networks: A view from the width",
    "authors": ["Lu", "Zhou", "Pu", "Hongming", "Wang", "Feicheng", "Hu", "Zhiqiang", "Liwei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Learning functions: When is deep better than shallow",
    "authors": ["Mhaskar", "Hrushikesh", "Liao", "Qianli", "Poggio", "Tomaso"],
    "venue": "arXiv preprint,",
    "year": 2016
  }, {
    "title": "On the number of linear regions of deep neural networks",
    "authors": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations",
    "authors": ["Pascanu", "Razvan", "Montufar", "Guido", "Bengio", "Yoshua"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2014
  }, {
    "title": "On the expressive power of deep neural networks",
    "authors": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Representation benefits of deep feedforward networks",
    "authors": ["Telgarsky", "Matus"],
    "venue": "arXiv preprint,",
    "year": 2015
  }, {
    "title": "Benefits of depth in neural networks",
    "authors": ["Telgarsky", "Matus"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2016
  }, {
    "title": "Error bounds for approximations with deep relu networks",
    "authors": ["Yarotsky", "Dmitry"],
    "venue": "Neural Networks,",
    "year": 2017
  }],
  "id": "SP:8ecfa40f518e7a8344dd1b9d958faf41d69e6ad5",
  "authors": [{
    "name": "Mohammad Mehrabi",
    "affiliations": []
  }, {
    "name": "Aslan Tchamkerten",
    "affiliations": []
  }, {
    "name": "Mansoor I. Yousefi",
    "affiliations": []
  }],
  "abstractText": "The approximation power of general feedforward neural networks with piecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the approximation error and network depth and width. These bounds improve upon stateof-the-art bounds for certain classes of functions, such as strongly convex functions. Second, an upper bound is established on the difference of two neural networks with identical weights but different activation functions.",
  "title": "Bounds on the Approximation Power of Feedforward Neural Networks"
}