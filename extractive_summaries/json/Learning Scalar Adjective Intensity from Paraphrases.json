{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Semantically similar adjectives are not fully interchangeable in context. Although hot and scalding are related, the statement “the coffee was hot” does not imply the coffee was scalding. Hot and scalding are scalar adjectives that describe temperature, but they are not interchangeable because they vary in intensity. A native English speaker knows that their relative intensities are given by the ranking hot < scalding. Understanding this distinction is important for language understanding tasks such as sentiment analysis (Pang et al., 2008), question answering (de Marneffe et al., 2010), and textual inference (Dagan et al., 2006).\nExisting lexical resources such as WordNet (Miller, 1995; Fellbaum, 1998) do not include the relative intensities of adjectives. As a result, there have been efforts to automate the process of learning intensity relations (e.g. Sheinman and Tokunaga (2009), de Melo and Bansal (2013), Wilkinson (2017), etc.). Many existing approaches rely\non pattern-based or lexicon-based methods to predict the intensity ranking of adjectives. Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y. As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014). Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3). They can also be precise, but may not cover all adjectives of interest.\nWe propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives. A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great↔ phenomenal. Adjectival paraphrases can be exploited to uncover intensity relationships. A paraphrase pair of the above form, where one phrase is composed of an intensifying adverb and an adjective (really great) and the other is a single-word adjective (phenomenal), provides evidence that great < phenomenal. By drawing this evidence from large, automatically-generated\nparaphrase resources like the Paraphrase Database (PPDB) 1 (Ganitkevitch et al., 2013; Pavlick et al., 2015), it is possible to obtain high-coverage pairwise adjective intensity predictions at reasonably high accuracy.\nWe demonstrate the usefulness of paraphrase evidence for inferring relative adjective intensity in two tasks: ordering sets of adjectives along an intensity scale, and inferring the polarity of indirect answers to yes/no questions. In both cases, we find that combining the relatively noisy, but highcoverage, paraphrase evidence with more precise but low-coverage pattern- or lexicon-based evidence improves overall quality."
  }, {
    "heading": "2 Related Work",
    "text": "Noting that adding adjective intensity relations to WordNet (Miller, 1995; Fellbaum, 1998) would be useful, Sheinman et al. (2013) propose a system for automatically extracting sets of same-attribute adjectives from WordNet ‘dumbbells’ – consisting of two direct antonyms at the poles and satellites of synonymous/related adjectives incident to each antonym (Gross and Miller, 1990) – and ordering them by intensity. The annotations, however, are not in WordNet as of its latest version (3.1).\nWork on adjective intensity generally focuses on two related tasks: clustering adjectives based on the attributes they modify, and ranking sameattribute adjectives by intensity. With respect to the former, common approaches involve clustering adjectives by their contexts (Hatzivassiloglou and McKeown, 1993; Shivade et al., 2015). We do not focus on the clustering task in this paper, but concentrate on the ranking task.\nApproaches to the task of ranking scalar adjectives by their intensity mostly fall under the paradigms of pattern-based or lexicon-based approaches. Pattern-based approaches work by extracting lexical (Sheinman and Tokunaga, 2009; de Melo and Bansal, 2013; Sheinman et al., 2013) or syntactic (Shivade et al., 2015) patterns indicative of an intensity relationship from large corpora. For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y.\nLexicon-based approaches are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993). These methods draw upon a\n1www.paraphrase.org\nlexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity. The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually. In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011).\nOur paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers. A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant than are moderate adverbs like very. Unlike Collex, which requires predetermined sets of ‘end-of-scale’ and ‘normal’ adverbial modifiers, our approach learns the identity and relative importance of intensifying adverbs.\nRelative intensity is just one of several dimensions of gradable adjective semantics. In addition to intensity scales, a comprehensive model of scalar adjective semantics might also incorporate notions of intensity range (Morzycki, 2015), adjective class (Kamp and Partee, 1995), and scale membership according to meaning (Hatzivassiloglou and McKeown, 1993). In this paper we take the position that relative intensity is worth studying on its own because it is an important component of adjective semantics, usable directly for some NLP tasks such as sentiment analysis (Pang et al., 2008), and as part of a more comprehensive model for other tasks like question answering (de Marneffe et al., 2010)."
  }, {
    "heading": "3 Paraphrase-based Intensity Evidence",
    "text": "Adjectival paraphrases provide evidence about the relative intensity of adjectives. A paraphrase of the form RB JJu ↔ JJv – where one phrase is comprised of an adjective modified by an intensifying adverb (RB JJu), and the other is a single-word adjective (JJv) – is evidence that the first adjective is less intense than the second (JJu < JJv). We propose a new method for encoding this evidence and using it to make pairwise adjective intensity predictions. First, a graph (JJGRAPH) is formed to represent over 36k adjectival paraphrases hav-\ning the specified form. Next, data in the graph are used to make pairwise adjective intensity predictions."
  }, {
    "heading": "3.1 Identifying Intensifying Adverbs",
    "text": "In JJGRAPH, nodes are adjectives, and each directed edge (JJu −−→\nRB JJv) corresponds to an adjec-\ntival paraphrase of the form RB JJu ↔ JJv – for example, very tall ↔ large – where one ‘phrase’ (JJv) is an adjective and the other (RB JJu) is an adjectival phrase containing an adverb and adjective (see Figure 1 for examples).\nAdverbs in PPDB can be intensifying or deintensifying. An intensifying adverb (e.g. very, totally) strengthens the adjectives it modifies. In contrast, a de-intensifying adverb (e.g. slightly, somewhat) weakens the adjectives it modifies. Since edges in JJGRAPH ideally point in the direction of increasing intensity, the first step in the process of creating JJGRAPH is to identify a set of adverbs that are likely intensifiers to be included as edges.\nFor this purpose, we generate a set R of likely intensifying adverbs within PPDB using a bootstrapping approach (Figure 2). The process starts with a small seed set of adjective pairs having a known intensity relationship. The seeds are pairs (ju, jv) from PPDB-XXL2 such that ju is a baseform adjective (e.g. hard), and jv is its comparative or superlative form (e.g. harder or hardest). Using the seeds, we identify intensifying ad-\n2PPDB comes in six increasingly large sizes from S to XXXL; larger collections have wider coverage but lower precision. Our work uses XXL.\nverbs by finding adjectival paraphrases in PPDB of the form (riju ↔ jv); because ju < jv, adverb ri is inferred to be intensifying (Round 1). All such ri are added to initial adverb set R1. The process continues by extracting paraphrases (riju′ ↔ jv′) with ri ∈ R1, indicating additional adjective pairs (ju′ , jv′) with intensity direction inferred by ri (Round 2). Finally, the adjective pairs extracted in this second iteration are used to identify additional intensifying adverbs R3, which are added to the final set R = R1 ∪R3 (Round 3).\nIn all, this process generates a set of 610 adverbs. Examination of the set shows that the process does capture many intensifying adverbs like very and abundantly, and excludes many deintensifying adverbs appearing in PPDB like far less and not as. However, due to the noise inherent in PPDB itself and in the bootstrapping process, there are also a few de-intensifying adverbs included in R (e.g. hardly, kind of ) as well as adverbs that are neither intensifying nor deintensifying (e.g. ecologically). It will be important to take this noise into consideration when using JJGRAPH to make pairwise intensity predictions."
  }, {
    "heading": "3.2 Building JJGRAPH",
    "text": "JJGRAPH is built by extracting all 36,756 adjectival paraphrases in PPDB of the specified form RB JJu ↔ JJv, where the adverb belongs to R. The resulting graph has 3,704 unique adjective nodes. JJGRAPH is a multigraph, as there are frequently multiple intensifying relationships between pairs of adjectives. For example, the paraphrases pretty hard ↔ tricky and really hard ↔ tricky are both present in PPDB. There can also be contradictory or cyclic edges in JJGRAPH, as in the example depicted in the JJGRAPH subgraph in Figure 3, where the adverb really connects tasty to lovely and vice versa. Self-edges are allowed (e.g. really hard↔ hard)."
  }, {
    "heading": "3.3 Pairwise Intensity Prediction",
    "text": "Examining the directed adverb edges between two adjectives ju and jv in JJGRAPH provides evidence about the relative intensity relationship between them. However, it has just been noted that JJGRAPH is noisy, containing both contradictory/cyclic edges and adverbs that are not uniformly intensifying. Rather than try to eliminate cycles, or manually annotate each adverb with a weight corresponding to its intensity and polarity\n(Ruppenhofer et al., 2015; Taboada et al., 2011), we aim to learn these weights automatically in the process of predicting pairwise intensity.\nGiven adjective pair (ju, jv), we build a classifier that outputs a score from 0 to 1 indicating the predicted likelihood that ju< jv. Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJGRAPH. The feature space includes only adverbs from R that appear at least 10 times in JJGRAPH, resulting in features for m = 259 unique adverbs in each direction (i.e. from ju to jv and vice versa) for 2m = 518 binary features total. Note that while all adverb features correspond to predicted intensifiers from R, there are some features that are actually de-intensifying due to the noise inherent in the bootstrapping process (Section 3.1).\nWe train the classifier on all 36.7k edges in JJGRAPH, based on a simplifying assumption that all adverbs in R are indeed intensifiers. For each adjective pair (ju, jv) with one or more direct edges from ju to jv, a positive training instance for pair (ju, jv) and a negative training instance for pair (jv, ju) are added to the training set. A logistic regression classifier is trained on the data, using elastic net regularization and 10-fold cross validation to tune parameters.\nThe model parameters output by the training process are in a feature weights vector w ∈ R2m (with no bias term) which can be used to generate a paraphrase-based score for each adjective pair:\nscorepp(ju, jv) = 1\n1 + exp−wxuv − 0.5 (1)\nwhere xuv is the binary feature vector for adjective pair (ju, jv). The decision boundary 0.5 is subtracted from the sigmoid activation function so that pairs predicted to have the directed relation ju< jv will have a positive score, and those predicted to have the opposite directional relation will have a negative score."
  }, {
    "heading": "4 Other Intensity Evidence",
    "text": "Our experiments compare the proposed paraphrase approach with existing pattern- and lexicon-based approaches."
  }, {
    "heading": "4.1 Pattern-based Evidence",
    "text": "We experiment with the pattern-based approach of de Melo and Bansal (2013). Given a pair of adjectives to be ranked by their intensity, de Melo and Bansal (2013) cull intensity patterns from Google n-Grams (Brants and Franz, 2009) as evidence of their intensity order. Specifically, they identify 8 types of weak-strong patterns (e.g. “X, but not Y”) and 7 types of strong-weak patterns (e.g. “not X, but still Y”) that are used as evidence about the directionality of the intensity relationship between adjectives. Given an adjective pair (ju, jv), an overall pattern-based weak-strong score is calculated:\nscorepat(ju, jv) = (Wu − Su)− (Wv − Sv)\ncount(ju) · count(jv) (2)\nwhere Wu and Su quantify the pattern evidence for the weak-strong and strong-weak intensity relations respectively for the pair (ju, jv), and Wv and Sv quantify the pattern evidence for the pair (jv, ju). Wu and Su are calculated as:\nWu = 1\nP1 ∑ p1∈Pws count(p1(ju, jv))\nSu = 1\nP2 ∑ p2∈Psw count(p2(ju, jv)) (3)\nWv and Sv are calculated similarly by swapping the positions of ju and jv. For example, given pair (good, great), Wu might incorporate evidence from patterns “good, but not great” and “not only good but great”, while Sv might incorporate evidence from the pattern “not great, just good”. Pws denotes the set of weak-strong patterns, Psw denotes the set of strong-weak patterns, and P1 and P2 give the total counts of all occurrences of any pattern in Pws and Psw respectively. The score is normalized by the frequencies of ju and jv in order to avoid bias due to high-frequency adjectives. As with the paraphrase-based scoring mechanism (Equation 1), scores output by this method can be positive or negative, with positive scores being indicative of a weak-strong relationship from ju to jv. Note that score(ju, jv) = −score(jv, ju)."
  }, {
    "heading": "4.2 Lexicon-based Evidence",
    "text": "We use the manually-compiled SO-CAL3 lexicon as our third, lexicon-based method for inferring intensity. The SO-CAL lexicon assigns an integer weight in the range [−5, 5] to 2,826 adjectives. The sign of the weight encodes sentiment polarity (positive or negative), and the value encodes intensity (e.g. atrocious, with a weight of -5, is more intense than unlikable, with a weight of -3). SO-CAL is used to derive a pairwise intensity prediction for adjectives (ju,jv) as follows:\nscoresocal(ju, jv) = |L(jv)| − |L(ju)|, iff sign(ju) = sign(jv)\n(4)\nwhere L(jv) gives the lexicon weight for jv. Note that scoresocal is computed only for adjectives having the same polarity direction in the lexicon; otherwise the score is undefined. This is because adjectives belonging to different half scales, such as freezing and steaming, are frequently incomparable in terms of intensity (de Marneffe et al., 2010)."
  }, {
    "heading": "4.3 Combining Evidence",
    "text": "While the pattern-based and lexicon-based pairwise intensity scores are known to be precise but low-coverage (de Melo and Bansal, 2013; Ruppenhofer et al., 2015), we expect that the paraphrase-based score will produce higher coverage at lower accuracy. Thus we also experiment with scoring methods that combine two or three score types. When combining two metrics x and y to generate a score for a pair (ju, jv), we simply use the first metric x if it can be reliably calculated for the pair, and back off to metric y otherwise. More formally, the combined score for metrics x and y is given by:\nscorex+y(ju, jv) = αx · gx(scorex(ju, jv)) + (1− αx) · gy(scorey(ju, jv))\n(5)\nwhere αx ∈ {0, 1} is a binary indicator corresponding to the condition that scorex can be reliably calculated for the adjective pair, and gx(·) is a scaling function (see below). If αx = 1, then scorex is used. Otherwise, if αx = 0, then we default to scorey. When combining three metrics x, y, and z, the combined score is given by:\n3https://github.com/sfu-discourse-lab/ SO-CAL\nscorex+y+z(ju, jv) = αx · gx(scorex(ju, jv)) + (1− αx) · scorey+z(ju, jv)\n(6)\nThe criteria for having αx = 1 varies depending on the metric type. For pattern-based evidence (x=‘pat’), αx = 1 when adjectives ju and jv appear together in any of the intensity patterns culled from Google n-grams (e.g. a pattern like “ju, but not jv” exists). For lexicon-based evidence (x=‘socal’), αx = 1 when both ju and jv are in the SO-CAL vocabulary, and have the same polarity (i.e. are both positive or both negative). For paraphrase-based evidence (x=‘pp’), αx = 1 when ju and jv have one or more edges directly connecting them in JJGRAPH.\nSince the metrics to be combined may have different ranges, we use a scaling function gx(·) to make the scores output by each metric directly comparable:\ngx(w) = sign(w) · (\nlog(|w|)− µx σx + γ\n) (7)\nwhere µx and σx are the estimated population mean and standard deviation of log(scorex) (estimated over all adjective pairs in the dataset), and γ is an offset that ensures positive scores remain positive, and negative scores remain negative. In our experiments we set γ = 5."
  }, {
    "heading": "5 Ranking Adjective Sets by Intensity",
    "text": "The first experimental application for the different paraphrase evidence is an existing model for predicting a global intensity ordering within a set of adjectives. Global ranking models are useful for inferring intensity comparisons between adjectives for which there is no explicit evidence. For example, in ranking three adjectives like warm, hot, and scalding, there may be direct evidence indicating warm < hot and hot < scalding, but no way of directly comparing warm to scalding. Global ranking models infer that warm< scalding based on evidence from the other adjective pairs in the scale."
  }, {
    "heading": "5.1 Global Ranking Model",
    "text": "We adopt the mixed-integer linear programming (MILP) approach of de Melo and Bansal (2013) for generating a global intensity ranking. This model takes a set of adjectives A = {a1, . . . , an}\nand directed, pairwise adjective intensity scores score(ai, aj) as input, and assigns each adjective ai a place along a linear scale xi ∈ [0, 1]. The adjectives’ assigned values define the global ordering. If the predicted weights used as input are inconsistent, containing cycles, the model resolves these by choosing the globally optimal solution.\nRecall that all pairwise scoring metrics produce a positive score for adjective pair (ju, jv) when it is likely that ju< jv, and a negative score otherwise. Consequently, the MILP approach should result in xu < xv when score(ju, jv) is positive, and xu > xv otherwise. This goal is achieved by maximizing the objective function:∑\nu,v\nsign(xv − xu) · score(ju, jv) (8)\nde Melo and Bansal (2013) propose a MILP formulation for maximizing this objective, which we utilize in our experiments. Note that while de Melo and Bansal (2013) incorporate synonymy evidence from WordNet in their ranking method, we do not implement this part of the model."
  }, {
    "heading": "5.2 Experiments",
    "text": "We experiment with using each of the paraphrase-, pattern-, and lexicon-based pairwise scores as input to the global ranking model in isolation. To examine how the scoring methods perform when used in combination, we also test all possible ordered combinations of 2 and 3 scores.\nExperiments are run over three distinct test sets (Table 1). Each dataset contains ordered sets of scalar adjectives belonging to the same scale. In general, scalar adjectives describing the same attribute can be ordered along a full scale (e.g. freezing to sweltering), or a half scale (warm to sweltering); all three test sets group adjectives into half scales. The three datasets are described here, and their characteristics are given in Table 1. deMelo (de Melo and Bansal, 2013)4. 87 adjective\n4http://demelo.org/gdm/intensity/\nsets are extracted from WordNet ‘dumbbell’ structures (Gross and Miller, 1990), and partitioned into half-scale sets based on their pattern-based evidence in the Google N-Grams corpus (Brants and Franz, 2009). Sets are manually annotated for intensity relations (<, >, and =). Wilkinson (Wilkinson and Oates, 2016). Twelve adjective sets are generated by presenting crowd workers with small seed sets (e.g. huge, small, microscopic), and eliciting similar adjectives. Sets are automatically cleaned for consistency, and then annotated for intensity by crowd workers. While the original dataset contains full scales, we manually sub-divide these into 21 half-scales for use in this study. Details on the modification from full- to half-scales are in the Supplemental Material. Crowd. We also crowdsourced a new set of adjective scales with high coverage of the PPDB vocabulary. In a three-step process, we first asked crowd workers whether pairs of adjectives describe the same attribute (e.g. temperature) and therefore should belong along the same scale. Second, sets of same-scale adjectives were refined over multiple rounds. Finally, workers ranked the adjectives in each set by intensity. The final dataset includes 293 adjective pairs along 79 scales.\nWe measure the agreement between the gold standard ranking of adjectives along each scale and the predicted ranking using three commonlyused metrics: Pairwise accuracy. For each pair of adjectives along the same scale, we compare the predicted ordering of the pair after global ranking (<, >, or =) to the gold-standard ordering of the pair, and report overall accuracy of the pairwise predictions. Kendall’s tau (τb). This metric computes the rank correlation between the predicted (rP (J)) and gold-standard (rG(J)) ranking permutations of each adjective scale J , incorporating a correction for ties. Values for τb range from −1 to 1, with extreme values indicating a perfect negative\nor positive correlation, and a value of 0 indicating no correlation between predicted and gold rankings. We report τb as a weighted average over scales in each dataset, where weights correspond to the number of adjective pairs in each scale. Spearman’s rho (ρ). We report the Spearman’s ρ rank correlation coefficient between predicted (rP (J)) and gold-standard (rG(J)) ranking permutations. For each dataset, we calculate this metric just once by treating each adjective in a particular scale as a single data point, and calculating an overall ρ for all adjectives from all scales."
  }, {
    "heading": "5.3 Experimental Results",
    "text": "The results of the global ordering experiment, reported in Table 2, are organized as follows: Score Accuracy pertains to performance of the scoring methods alone – prior to global ranking – while Global Ranking Results pertains to performance of each scoring method as used in the global ranking algorithm. Within Score Accuracy there are two metrics. Coverage gives the percent of unique same-scale adjective pairs from the test set that can be directly scored using the given method. For scorepat, covered pairs are all those that appear together in any recognized pattern;\nfor scorepp, covered pairs are those directly connected in JJGRAPH by one or more direct edges; for scoresocal, covered pairs are all those for which both adjectives are in the SO-CAL lexicon and the metric is defined. Pairwise Accuracy gives the accuracy of the scoring method (before global ranking) on just the covered pairs, meaning that the subset of pairs scored by each method varies. Within Global Ranking Results, we report pairwise accuracy, weighted average τb, and ρ calculated over all pairs after ranking – including both pairs that are covered by the scoring method, and those whose pairwise intensity relationship has been inferred by the ranking algorithm.\nThe results indicate that the pairwise score accuracies (before ranking) for scorepat and scoresocal are higher than those of scorepp for all datasets, but that their coverage is relatively limited. The one exception is the deMelo dataset, where scorepat has high coverage because the dataset was compiled specifically by finding adjective pairs that matched lexical patterns in the corpus. For all datasets, highest coverage is achieved using one of the combined metrics that incorporates paraphrase-based evidence.\nThe impact of these trends is visible on the\nGlobal Ranking Results. When using pairwise intensity scores to compute the global ranking, higher coverage by a metric drives better results, as long as the metric’s accuracy is reasonably high. Thus the paraphrase-based scorepp, with its high coverage, gets better global ranking results than the other single-method scores for two of the three datasets. Further, we find that boosting coverage with a combined metric that incorporates paraphrase evidence produces the highest post-ranking pairwise accuracy scores overall for all three datasets, and the highest average τb and ρ on the Crowd and Wilkinson datasets. We conclude that incorporating paraphrase evidence can improve the quality of this model for ordering adjectives along a scale because it gives high coverage with reasonably high quality.\nThe performance trends on the deMelo dataset differ from those on the Crowd and Wilkinson datasets. In particular, scorepp and scoresocal have substantially lower pre-ranking pairwise accuracy on the pairs they cover in the deMelo dataset than they do for Crowd and Wilkinson: scorepp has an accuracy of just 0.458 on covered pairs in the deMelo dataset, compared with 0.676 and 0.753 on the Crowd and Wilkinson datasets, and score differences for scoresocal are similar. The near-random prediction accuracies of scorepp and scoresocal on deMelo before ranking lead to nearzero correlation values on this dataset after global ranking. To explore possible reasons for these results, we assessed the level of human agreement with each dataset in terms of pairwise accuracy. For each test set, we asked five crowd workers to classify the intensity direction for each adjective pair (ju, jv) in all scales as less than (<), greater than (>), or equal (=). We found that humans agreed with the ‘gold standard’ direction 65% of the time on the Bansal dataset, versus 70% of the time on the Crowd and Wilkinson datasets. It is possible that the more difficult nature of the Bansal dataset, coupled with its method of compilation (i.e. favoring adjective pairs that co-occur with pre-defined intensity patterns), lead to the lower coverage and lower accuracy of scorepp and scoresocal on this dataset."
  }, {
    "heading": "6 Indirect Question Answering",
    "text": "The second task that we address is answering indirect yes or no questions. de Marneffe et al. (2010) observed that answers to such polar questions fre-\nquently omit an explicit yes or no response. In some cases the implied answer depends on the relative intensity of adjective modifiers in the question and answer. For example, in the exchange:\nQ: Was he a successful ruler? A: Oh, a tremendous ruler.\nthe implied answer is yes, which is inferred because successful≤ tremendous in terms of relative intensity. Conversely, in the exchange:\nQ: Does it have a large impact? A: It has a medium-sized impact.\nthe implied answer is no because large>mediumsized.\nde Marneffe et al. (2010) compiled an evaluation set for this task by extracting 123 examples of such indirect question-answer pairs (IQAP) from dialogue corpora. In each exchange, the implied answer (annotated by crowd workers to be yes or no5) depends on the relative intensity relationship between modifiers in the question and answer texts. In their original paper, the authors utilize an automatically-compiled lexicon to make a polarity prediction for each IQAP."
  }, {
    "heading": "6.1 Predicting Answer Polarity",
    "text": "Our goal is to see whether paraphrase-based scores are useful for predicting the polarity of answers in the IQAP dataset. As before, we compare the quality of predictions made using the paraphrase-based evidence with predictions made using pattern-based, lexicon-based, and combined scoring metrics.\nTo use the pairwise scores for inference, we employ a decision procedure nearly identical to that of de Marneffe et al. (2010). If jq and ja are scorable (i.e. have a scorable intensity relationship along the same half-scale), then jq≤ ja implies the answer is yes (first example above), and jq> ja implies the answer is no (second example). If the pair of adjectives is not scorable, then the predicted answer is no, as the pair could be antonyms or completely unrelated. If either jq or ja is missing from the scoring vocabulary, the adjectives are impossible to compare and therefore the prediction is uncertain. The full decision procedure is given in Figure 4.\n5The original dataset contains two additional examples where the answer is annotated as uncertain, but de Marneffe et al. (2010) exclude them from the results and so do we."
  }, {
    "heading": "6.2 Experiments",
    "text": "The decision procedure in Figure 4 is carried out for the 123 IQAP instances in the dataset, varying the score type. We report the accuracy, and macro-averaged precision, recall, and F1-score of the 85 yes and 38 no instances, in Table 3 alongside the percent of instances with adjectives out of vocabulary. Only the combined scores for the two best-scoring combinations, scoresocal+pp and scoresocal+pat+pp, are reported.\nThe simplest baseline of predicting all answers to be “YES” gets highest accuracy in this imbalanced test set, but all score types perform better than the all-“YES” baseline in terms of precision and F1-score. Bouyed by its high precision, the scoresocal – which is derived from a manuallycompiled lexicon – scored higher than scorepp and scorepat. But it mis-predicted 33% of pairs\nas uncertain because of its limited overlap with the IQAP vocabulary. Meanwhile, scorepp had relatively high coverage and a mid-level F-score, while scorepat scored poorly on this dataset due to its sparsity; while all modifiers in the IQAP dataset are in the Google N-grams vocabulary, most do not have observed patterns and therefore return predictions of “NO” (item 2 in Figure 4). As in the global ranking experiments, the paraphrase-based evidence is complementary to the lexicon-based evidence, and thus the combined scoresocal+pp and scoresocal+pat+pp produce significantly better accuracy than any score in isolation (McNemar’s test, p < .01), and also out-perform the original expected ranking method of de Marneffe et al. (2010) (although they do not beat the best-reported score on this dataset, F-score=0.706 (Kim and de Marneffe, 2013))."
  }, {
    "heading": "7 Conclusion",
    "text": "We have proposed adjectival paraphrases as a new source of evidence for predicting intensity relationships between scalar adjectives. While paraphrase-based intensity evidence produces pairwise predictions that are less precise than those produced by pattern- or lexicon-based evidence, the coverage is substantially higher. Thus paraphrases can be successfully used as a complementary source of information for reasoning about adjective intensity."
  }, {
    "heading": "Acknowledgments",
    "text": "This material is based in part on research sponsored by the following organizations: the Allen Institute for Artificial Intelligence (AI2) Key Scientific Challenges program, the Google Ph.D. Fellowship program, the French National Research Agency under project ANR-16-CE33-0013, and DARPA under grant numbers FA8750-13-2-0017 (the DEFT program) and HR0011-15-C-0115 (the LORELEI program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.\nWe are grateful to our anonymous reviewers for their thoughtful and constructive comments."
  }],
  "year": 2019,
  "references": [{
    "title": "A test of goodness of fit",
    "authors": ["Theodore W Anderson", "Donald A Darling"],
    "venue": "Journal of the American statistical association,",
    "year": 1954
  }, {
    "title": "Web 1T 5gram, 10 European languages version 1. Linguistic Data Consortium, Philadelphia",
    "authors": ["Thorsten Brants", "Alex Franz"],
    "year": 2009
  }, {
    "title": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW",
    "authors": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"],
    "year": 2006
  }, {
    "title": "The hitchhiker’s guide to testing statistical significance in natural language processing",
    "authors": ["Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart"],
    "venue": "In Proceedings of the 56th Annual Meeting of the Association",
    "year": 2018
  }, {
    "title": "The design of experiments",
    "authors": ["Ronald Aylmer Fisher"],
    "year": 1935
  }, {
    "title": "PPDB: The paraphrase database",
    "authors": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"],
    "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2013
  }, {
    "title": "Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning",
    "authors": ["Vasileios Hatzivassiloglou", "Kathleen R. McKeown"],
    "venue": "In Proceedings of the 31st Annual Meeting on Association for Computational Linguistics",
    "year": 1993
  }, {
    "title": "Automatic acquisition of hyponyms from large text corpora",
    "authors": ["Marti A Hearst"],
    "venue": "In Proceedings of the 14th conference on Computational linguistics-Volume",
    "year": 1992
  }, {
    "title": "Deriving adjectival scales from continuous space word representations",
    "authors": ["Joo-Kyung Kim", "Marie-Catherine de Marneffe"],
    "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural",
    "year": 2013
  }, {
    "title": "Was it good? It was provocative. Learning the meaning of scalar adjectives",
    "authors": ["Marie-Catherine de Marneffe", "Christopher D. Manning", "Christopher Potts"],
    "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    "year": 2010
  }, {
    "title": "Wordnet: A lexical database for english",
    "authors": ["George A. Miller"],
    "venue": "Commun. ACM,",
    "year": 1995
  }, {
    "title": "Opinion mining and sentiment analysis",
    "authors": ["Bo Pang", "Lillian Lee"],
    "venue": "Foundations and Trends in Information Retrieval,",
    "year": 2008
  }, {
    "title": "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
    "authors": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"],
    "year": 2015
  }, {
    "title": "A generic approach to generate opinion lists of phrases for opinion mining applications",
    "authors": ["Sven Rill", "J. vom Scheidt", "Johannes Drescher", "Oliver Schütz", "Dirk Reinel", "Florian Wogenstein"],
    "venue": "In Proceedings of the First International Workshop on Is-",
    "year": 2012
  }, {
    "title": "Ordering adverbs by their scaling effect on adjective intensity",
    "authors": ["Josef Ruppenhofer", "Jasper Brandes", "Petra Steiner", "Michael Wiegand"],
    "venue": "In Proceedings of the International Conference Recent Advances in Natural Language Processing,",
    "year": 2015
  }, {
    "title": "Comparing methods for deriving intensity scores for adjectives",
    "authors": ["Josef Ruppenhofer", "Michael Wiegand", "Jasper Brandes"],
    "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),",
    "year": 2014
  }, {
    "title": "Adjective intensity and sentiment analysis",
    "authors": ["Raksha Sharma", "Mohit Gupta", "Astha Agarwal", "Pushpak Bhattacharyya"],
    "venue": "In Proceedings of the 2015 Conference on Empirical Methods for Natural Language Processing (EMNLP),",
    "year": 2015
  }, {
    "title": "Large, huge or gigantic? Identifying and encoding intensity relations among adjectives in WordNet",
    "authors": ["Vera Sheinman", "Christiane Fellbaum", "Isaac Julien", "Peter Schulam", "Takenobu Tokunaga"],
    "venue": "Language resources and evaluation,",
    "year": 2013
  }, {
    "title": "Adjscales: Visualizing differences between adjectives for language learners",
    "authors": ["Vera Sheinman", "Takenobu Tokunaga"],
    "venue": "IEICE TRANSACTIONS on Information and Systems,",
    "year": 2009
  }, {
    "title": "Corpus-based discovery of semantic intensity scales",
    "authors": ["Chaitanya P. Shivade", "Marie-Catherine de Marneffe", "Eric Fosler-Lussier", "Albert M. Lai"],
    "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
    "year": 2015
  }, {
    "title": "Lexicon-based methods for sentiment analysis",
    "authors": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede"],
    "venue": "Computational linguistics,",
    "year": 2011
  }, {
    "title": "Identifying and Ordering Scalar Adjectives Using Lexical Substitution",
    "authors": ["Bryan Wilkinson"],
    "venue": "Ph.D. thesis,",
    "year": 2017
  }, {
    "title": "A gold standard for scalar adjectives",
    "authors": ["Bryan Wilkinson", "Tim Oates"],
    "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC),",
    "year": 2016
  }],
  "id": "SP:685fda0e2cc165f40733969c68a7f20d6129164e",
  "authors": [{
    "name": "Anne Cocos",
    "affiliations": []
  }, {
    "name": "Skyler Wharton",
    "affiliations": []
  }, {
    "name": "Ellie Pavlick",
    "affiliations": []
  }, {
    "name": "Marianna Apidianaki",
    "affiliations": []
  }, {
    "name": "Chris Callison-Burch",
    "affiliations": []
  }],
  "abstractText": "Adjectives like warm, hot, and scalding all describe temperature but differ in intensity. Understanding these differences between adjectives is a necessary part of reasoning about natural language. We propose a new paraphrasebased method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives. Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair really hot↔ scalding suggests that hot < scalding. We show that combining this paraphrase evidence with existing, complementary patternand lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to yes/no questions.",
  "title": "Learning Scalar Adjective Intensity from Paraphrases"
}