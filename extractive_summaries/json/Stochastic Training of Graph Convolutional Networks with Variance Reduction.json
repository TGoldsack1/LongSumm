{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Graph convolution networks (GCNs) (Kipf & Welling, 2017) generalize convolutional neural networks (CNNs) (LeCun et al., 1995) to graph structured data. The “graph convolution” operation applies same linear transformation to all the neighbors of a node, followed by mean pooling and nonlinearity. By stacking multiple graph convolution layers, GCNs can learn node representations by utilizing information from distant neighbors. GCNs and their variants (Hamilton et al., 2017a; Veličković et al., 2018) have been applied to semi-supervised node classification (Kipf & Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling, 2016; Berg et al., 2017) and knowledge graphs (Schlichtkrull et al., 2017), outperforming multi-layer perceptron (MLP) models that\n1Dept. of Comp. Sci. & Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., THBI Lab, Tsinghua University, Beijing, 100084, China 2Georgia Institute of Technology 3Ant Financial. Correspondence to: Jun Zhu <dcszj@mail.tsinghua.edu.cn>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndo not use the graph structure, and graph embedding approaches (Perozzi et al., 2014; Tang et al., 2015; Grover & Leskovec, 2016) that do not use node features.\nHowever, the graph convolution operation makes GCNs difficult to be trained efficiently. The representation of a node at layer L is computed recursively by the representations of all its neighbors at layer L − 1. Therefore, the receptive field of a single node grows exponentially with respect to the number of layers, as illustrated in Fig. 1(a), so exactly computing the stochastic gradient is expensive even for a single node. Due to the large receptive field size, Kipf & Welling (2017) propose to train GCN by a batch algorithm, which computes the representations of all the nodes altogether. However, batch algorithms cannot handle largescale datasets because of their slow convergence and the requirement to fit the entire dataset in GPU memory.\nHamilton et al. (2017a) make an initial attempt to develop stochastic training algorithms for GCNs via a scheme of neighbor sampling (NS). Instead of considering all the neighbors, they randomly subsample D(l) neighbors at the l-th layer. Therefore, they reduce the receptive field size to∏ lD\n(l), as shown in Fig. 1(b). They find that for two-layer GCNs, keeping D(1) = 10 and D(2) = 25 neighbors can achieve comparable performance with the original model. However, there is no theoretical guarantee on the convergence of the stochastic training algorithm with NS. Moreover, the time complexity of NS is still D(1)D(2) = 250 times larger than training an MLP, which is unsatisfactory.\nIn this paper, we develop novel control variate-based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate. Our algorithms have new theoretical results on (1) variance reduction from the magnitude of the activation to the magnitude of the difference between current-and-historical activations; (2) exact (zero-variance) predictions at testing time; (3) convergence to a local optimum of GCN during training regardless of the neighbor sampling size D(l), with an asymptotically unbiased stochastic gradient. The theoretical properties allow us to significantly reduce the time complexity of stochastic training by sampling only D(l) = 2 neighbors per node, yet still retain the quality of the model.\nWe empirically test our algorithms on six graph datasets, and the results match with the theory. Comparing with NS, our\nInput\nLayer 1\nLayer 2\n(a) Exact\nInput\nLayer 1\nLayer 2\n(b) Neighbour sampling\nInput\nLayer 1\nLayer 2\n(c) Control variate\nLatest activation Historical activation\nInput\nGraphConv\nDropout\nDropout\nGraphConv\n(1)H\n(2)H\nGraphConv\nGraphConv\n(1)\n(2)\n(d) CVD network\nFigure 1. Two-layer graph convolutional networks, and the receptive field of a single vertex.\nalgorithms significantly reduce the bias and variance of the gradient. Comparing with the exact algorithm which considers all the neighbors, our algorithms with only D(l) = 2 neighbors still get the same accuracy at testing time, and achieve similar predictive performance during training in a comparable number of epochs, with a much lower time complexity, while these results are not achievable by NS. On the largest Reddit dataset, the training time of our algorithm is 7 times shorter than that of the best-performing competitor among exact, neighbor sampling and importance sampling (Chen et al., 2018) algorithms."
  }, {
    "heading": "2. Backgrounds",
    "text": "We briefly review graph convolutional networks (GCNs), stochastic training, neighbor sampling, and importance sampling in this section."
  }, {
    "heading": "2.1. Graph Convolutional Networks",
    "text": "We present our algorithm with a GCN for semi-supervised node classification (Kipf & Welling, 2017). However, the algorithm is neither limited to the task nor the model. Our algorithm is applicable to other models including GraphSAGE-mean (Hamilton et al., 2017a) and graph attention networks (GAT) (Veličković et al., 2018), and other tasks (Kipf & Welling, 2016; Berg et al., 2017; Schlichtkrull et al., 2017; Hamilton et al., 2017b), as long as the model aggregates neighbor activations by averaging.\nIn the node classification task, we have an undirected graph G = (V, E) with V = |V| vertices and E = |E| edges,\nwhere each vertex v consists of a feature vector xv and a label yv. We observe the labels for some vertices VL. The goal is to predict the labels for the rest vertices VU := V\\VL. The edges are represented as a symmetric V × V adjacency matrix A, where Auv is the weight of the edge between u and v, and the propagation matrix P is a normalized version of A: Ã = A+ I , D̃uu = ∑ v Ãuv, and P = D̃ − 12 ÃD̃− 1 2 . A graph convolution layer is defined as Z(l+1) = PH(l)W (l), H(l+1) = σ(Zl+1), (1)\nwhere H(l) is the activation matrix in the l-th layer, whose each row is the activation of a graph node. H(0) = X is the input feature matrix, W (l) is a trainable weight matrix, and σ(·) is an activation function. Denote |·| as the cardinality of a set. The training loss is defined as\nL = 1 |VL| ∑ v∈VL f(yv, z (L) v ), (2)\nwhere f(·, ·) is a loss function. A graph convolution layer propagates information to nodes from their neighbors by computing the neighbor averaging PH(l). Let n(u) be the set of neighbors of node u, and n(u) be its cardinality. The neighbor averaging of node u, (PH(l))u =∑V v=1 Puvh (l) v = ∑ v∈n(u) Puvh (l) v , is a weighted sum of neighbors’ activations. Then, a fully-connected layer is applied on all the nodes, with a shared weight matrix W (l) across all the nodes.\nWe denote the receptive field of a node u as all the activations h(l)v on layer l needed for computing z (L) u . If the layer l is not explicitly mentioned, it is the input layer 0. Intuitively, the receptive field of node u is just all its L-hop neighbors, i.e., nodes that are reachable from u within L hops, as illustrated in Fig. 1(a). When P = I , GCN reduces to a multi-layer perceptron (MLP) model which does not use the graph structure. For MLP, the receptive field of a node u is just the node itself."
  }, {
    "heading": "2.2. Stochastic Training",
    "text": "It is generally expensive to compute the batch gradient ∇L = 1|VL| ∑ v∈VL ∇f(yv, z (L) v ), which involves iterat-\ning over the entire labeled set of nodes. A possible solution is to approximate the batch gradient by a stochastic gradient\n1 |VB| ∑ v∈VB ∇f(yv, z(L)v ), (3)\nwhere VB ⊂ VL is a minibatch of labeled nodes. However, this gradient is still expensive to compute, due to the large receptive field size. For instance, as shown in Table 1, the number of 2-hop neighbors on the NELL dataset is averagely 1,597, which means in a 2-layer GCN, computing the gradient even for a single node needs 1, 597/65, 755 ≈ 2.4% nodes of the entire graph.\nIn subsequent sections, two other stochasticity will be introduced besides the random selection of the minibatch: the random sampling of neighbors (Sec. 2.3) and the random dropout of features (Sec. 5)."
  }, {
    "heading": "2.3. Neighbor Sampling",
    "text": "To reduce the receptive field size, Hamilton et al. (2017a) propose a neighbor sampling (NS) algorithm. NS randomly chooses D(l) neighbors for each node at layer l and develops an estimator NS(l)u of (PH\n(l))u based on Monte-Carlo approximation:\n(PH(l))u ≈ NS(l)u := n(u)\nD(l) ∑ v∈n̂(l)(u) Puvh (l) v ,\nwhere n̂(l)(u) ⊂ n(u) is a subset of D(l) random neighbors. Therefore, NS reduces the receptive field size from all the L-hop neighbors to the number of sampled neighbors,∏L l=1D (l). We refer NS(l)u as the NS estimator of (PH (l))u, and (PH(l))u itself as the exact estimator.\nNeighbor sampling can also be written in a matrix form as Z(l+1) = P̂ (l)H(l)W (l), H(l+1) = σ(Z(l+1)), (4)\nwhere the propagation matrix P is replaced by a sparser unbiased estimator P̂ (l), i.e., EP̂ (l) = P , where P̂ (l)uv = n(u) D(l)\nPuv if v ∈ n̂(l)(u), and P̂ (l)uv = 0 otherwise. Hamilton et al. (2017a) propose to perform an approximate forward propagation as Eq. (4), and do stochastic gradient descent (SGD) with the auto-differentiation gradient. The approximated gradient has two sources of randomness: the random selection of minibatch VB ⊂ VL, and the random selection of neighbors.\nThough P̂ (l) is an unbiased estimator of P , σ(P̂ (l)H(l)W (l)) is not an unbiased estimator of σ(PH(l)W (l)), due to the non-linearity of σ(·). In the sequel, both the prediction Z(L) and gradient ∇f(yv, z(L)v ) obtained by NS are biased, and the convergence of SGD is not guaranteed, unless the sample size D(l) goes to infinity. Because of the biased gradient, the sample size D(l) needs to be large for NS, to keep comparable predictive performance with the exact algorithm. Hamilton\net al. (2017a) choose D(1) = 10 and D(2) = 25, and the receptive field size D(1) ×D(2) = 250 is much larger than one, so the training is still expensive."
  }, {
    "heading": "2.4. Importance Sampling",
    "text": "FastGCN (Chen et al., 2018) is another sampling-based algorithm similar as NS. Instead of sampling neighbors for each node, FastGCN directly subsample the receptive field for each layer altogether. Formally, it approximates (PH(l))u with S samples v1, . . . , vS ∈ V as\n(PH(l))u = V V∑ v=1 1 V Puvh (l) v ≈ V S ∑ vs∼q(v) Puvh (l) vs /q(vs), where they define the importance distribution q(v) ∝∑V u=1 P 2 uv . According to the definition of P in Sec. 2.1, we\nhave q(v) ∝ 1n(v) ∑ (u,v)∈E 1\nn(u) . We refer to this estimator as importance sampling (IS). Chen et al. (2018) show that IS performs better than using a uniform sample distribution q(v) ∝ 1. NS can be viewed as an IS estimator with the importance distribution q(v) ∝ ∑ (u,v)∈E 1 n(u) , because each node u has probability 1n(u) to choose the neighbor v. Though IS may have a smaller variance than NS, it still only guarantees the convergence as the sample size S goes to infinity. Empirically, we find IS to work even worse than NS because sometimes it can select many neighbors for one node, and no neighbor for another, in which case the activation of the latter node is just meaningless zero."
  }, {
    "heading": "3. Control Variate Based Algorithm",
    "text": "We present a novel control variate based algorithm that utilizes historical activations to reduce the estimator variance."
  }, {
    "heading": "3.1. Control Variate Based Estimator",
    "text": "While computing the neighbor average ∑ v∈n(u) Puvh (l) v , we cannot afford to evaluate all the h(l)v terms because they need to be computed recursively, i.e., we again need the activations h(l−1)w of all of v’s neighbors w.\nOur idea is to maintain the history h̄(l)v for each h (l) v as an affordable approximation. Each time when h(l)v is computed, we update h̄(l)v with h (l) v . We expect h̄ (l) v and h (l) v to be similar if the model weights do not change too fast during the training. Formally, let ∆h(l)v = h (l) v − h̄(l)v , we approximate\n(PH(l))u = ∑\nv∈n(u)\nPuv∆h (l) v + ∑ v∈n(u) Puvh̄ (l) v ≈ CV(l)u\n:= n(u)\nD(l) ∑ v∈n̂(l)(u) Puv∆h (l) v + ∑ v∈n(u) Puvh̄ (l) v , (5)\nwhere we represent h(l)v as the sum of ∆h (l) v and h̄ (l) v , and\nwe only apply Monte-Carlo approximation on the ∆h(l)v term. Averaging over all the h̄(l)v terms is still affordable because they do not need to be computed recursively. Since we expect h(l)v and h̄ (l) v to be close, ∆hv will be small and CV(l)u should have a smaller variance than NS (l) u . Particularly, if the model weight is kept fixed, h̄(l)v should eventually equal with h(l)v , so that CV(l)u = 0 + ∑ v∈n(u) Puvh̄\n(l) v =∑\nv∈n(u) Puvh (l) v = (PH(l))u, i.e., the estimator has zero variance. This estimator is referred as CV. We will compare the variance of NS and CV estimators in Sec. 3.2 and show that the variance of CV will be eventually zero during the training in Sec. 4. The term CV(l)u − NS(l)u =∑ v∈n(u) Puvh̄ (l) u − n(u)D(l) ∑ v∈n̂(l)(u) Puvh̄ (l) u is a control variate (Ripley, 2009, Chapter 5) added to the neighbor sampling estimator NS(l)u , to reduce its variance.\nIn matrix form, let H̄(l) be the matrix formed by stacking h̄ (l) v , then CV can be written as\nZ(l+1) = ( P̂ (l)(H(l) − H̄(l)) + PH̄(l) ) W (l). (6)"
  }, {
    "heading": "3.2. Variance Analysis",
    "text": "We analyze the variance of the estimators assuming all the features are 1-dimensional. The analysis can be extended to multiple dimensions by treating each dimension separately. We further assume that n̂(l)(u) is created by sampling D(l) neighbors without replacement from n(u). The following proposition is proven in Appendix A:\nProposition 1. If n̂(l)(u) contains D(l) samples from n(u) without replacement, then Varn̂(l)(u) [ n(u) D(l) ∑ v∈n̂(l)(u) xv ] =\nC(l)u 2D(l) ∑ v1∈n(u) ∑ v2∈n(u) (xv1 − xv2)\n2, where C(l)u = 1− (D(l) − 1)/(n(u)− 1). By Proposition 1, we have Varn̂(l)(u) [ NS (l) u ] =\nC(l)u 2D(l)∑ v1∈n(u) ∑ v2∈n(u) (Puv1h (l) v1 − Puv2h (l) v2 ) 2, in contrast,\nthe variance of the CV estimator is Varn̂(l)(u) [ CV (l) u ] =\nC(l)u 2D(l) ∑ v1∈n(u) ∑ v2∈n(u)(Puv1∆h (l) v1 − Puv2∆h (l) v2 ) 2, which replaces h(l)v by ∆h (l) v . Since ∆h (l) v is usually much smaller than h(l)v , the CV estimator enjoys much smaller variance than the NS estimator. Furthermore, as we will show in Sec. 4.2, ∆h(l)v converges to zero during training, so we achieve not only variance reduction but variance elimination, as the variance vanishes eventually."
  }, {
    "heading": "3.3. Implementation Details",
    "text": "Training with the CV estimator is similar as with the NS estimator (Hamilton et al., 2017a). Particularly, each iteration of the algorithm involves the following steps:\nStochastic GCN with Variance Reduction 1. Randomly select a minibatch VB ⊂ VL of nodes; 2. Build a computation graph that only contains the acti-\nvations h(l)v and h̄ (l) v needed for the current minibatch; 3. Get the predictions by forward propagation as Eq. (6); 4. Get the gradients by backward propagation, and up-\ndate the parameters by SGD; 5. Update the historical activations.\nStep 3 and 4 are handled automatically by frameworks such as TensorFlow (Abadi et al., 2016). The computational graph at Step 2 is defined by the receptive field r(l) and the propagation matrices P̂ (l) at each layer. The receptive field r(l) specifies the activations h(l)v of which nodes should be computed for the current minibatch, according to Eq. (6). We can construct r(l) and P̂ (l) from top to bottom, by randomly adding D(l) neighbors for each node in r(l+1), starting with r(L) = VB. We assume h(l)v is always needed to compute h(l+1)v , i.e., v is always selected as a neighbor of itself. The receptive fields are illustrated in Fig. 1(c), where red nodes are in receptive fields, whose activations h(l)v are needed, and the histories h̄(l)v of blue nodes are also needed. Finally, in Step 5, we update h̄(l)v with h (l) v for each v ∈ r(l). We have the pseudocode for the training in Appendix D."
  }, {
    "heading": "3.4. Time and Space Complexity",
    "text": "GCN has two main types of computation, namely, the sparsedense matrix multiplication (SPMM) such as PH(l), and the dense-dense matrix multiplication (GEMM) such as UW (l). We assume that the input node feature is K-dimensional and the first hidden layer is A-dimensional.\nFor batch GCN, the time complexity is O(EK) for SPMM and O(V KA) for GEMM. For our stochastic training algorithm with control variates, the dominant SPMM computation is the average of neighbor history PH̄(0) for the nodes in r(1), whose size is O(|VB | ∏L l=2D\n(l)), and each node costs O(DK), where D is the average node degree. Therefore, the time complexity of SPMM is approximately O(EK ∏L l=2D\n(l)) per epoch. The dominant GEMM computation is the first fully-connected layer on all the nodes in r(1), whose time complexity is O(V KA ∏L l=2D\n(l)) per epoch. Both time complexities are ∏L l=2D\n(l) times higher than batch GCN, where ∏L l=2D\n(l) = 2 if we sample 2 neighbors per node and there are 2 GCN layers.\nOur algorithm requires an additional O(V LA) space to store historical activations. However, as implemented in our code, the history can be stored in main memory along with the data, which should be larger."
  }, {
    "heading": "4. Theoretical Results",
    "text": "Besides smaller variance, CV also has stronger theoretical guarantees than NS. In this section, we present two theorems. The first states that if the model parameters are fixed, e.g., during testing, CV produces exact predictions after L epochs; and the second establishes the convergence towards a local optimum regardless of the neighbor sampling size.\nIn this section, we assume that the algorithm is run by epochs, where each epoch contains I iterations, and in each iteration we want to compute the stochastic gradient w.r.t. nodes in Vi. We ensure that the activations of all nodes are computed at least one in each epoch, so that the staleness of the history is bounded. We use the subscript i for iteration number and CV to distinguish CV from the exact algorithm, i.e., Z(l)i and H (l) i , Wi, and\ngi(Wi) := 1 |Vi| ∑ v∈Vi ∇f(yv, z (L) i,v ) are the activations, model weights, and stochastic gradients obtained by the exact algorithm; and Z(l)CV,i, H (l) CV,i, and gCV,i(Wi) are their\nCV counterparts. ∇L(Wi) = 1|VL| ∑ v∈VL ∇f(yv, z (L) v ) is the deterministic batch gradient computed by the exact algorithm. The subscript i may be omitted for the exact algorithm ifWi is a constant sequence. We let [L] = {0, . . . , L} and [L]+ = {1, . . . , L}."
  }, {
    "heading": "4.1. Exact Testing",
    "text": "The following theorem reveals the connection between the exact predictions and the approximate predictions by CV. The proof can be found in Appendix B.\nTheorem 1. For a constant sequence of Wi = W and any i > LI (i.e., after L epochs), the activations computed by CV are exact, i.e., Z(l)CV,i = Z\n(l) for each l ∈ [L] and H\n(l) CV,i = H (l) for each l ∈ [L− 1].\nTheorem 1 shows that at testing time, we can run forward propagation with CV for L epoches and get exact prediction. This outperforms NS, which cannot recover the exact prediction unless the neighbor sample size goes to infinity. Comparing with directly making exact predictions by an exact batch algorithm, CV is more scalable because it does not need to load the entire graph into memory."
  }, {
    "heading": "4.2. Convergence Guarantee",
    "text": "The following theorem shows that SGD training with the approximated gradients gCV,i(Wi) still converges to a local optimum, regardless of the neighbor sampling size D(l).\nTheorem 2. Assume that (1) the activation σ(·) is ρLipschitz, (2) the gradient of the cost function ∇zf(y, z) is ρ-Lipschitz and bounded, (3) ‖gCV,V(W )‖∞, ‖g(W )‖∞, and ‖∇L(W )‖∞ are all bounded byG > 0 for all P̂ ,V and W . (4) The lossL(W ) is ρ-smooth, i.e., |L(W2)−L(W1)−\n〈∇L(W1),W2−W1〉| ≤ ρ2 ‖W2 −W1‖ 2 F ∀W1,W2, where 〈A,B〉 = tr(A>B) is the inner product of matrix A and matrix B. (5) The loss L(W ) ≥ L∗ is bounded below. Then, there exists K > 0, s.t., ∀N > LI , if we run SGD for R ≤ N iterations, where R is chosen uniformly from [N ]+, we have\nER ‖∇L(WR)‖2F ≤ 2 L(W1)− L∗ +K + ρK√\nN ,\nfor the updates Wi+1 = Wi−γgCV,i(Wi) and the step size γ = min{ 1ρ , 1√ N }.\nParticularly, limN→∞ ER ‖∇L(WR)‖2 = 0. Therefore, our algorithm converges to a local optimum W where the batch gradient ∇L(W ) = 0. The full proof is in Appendix C. For short, we show that gCV,i(Wi) is unbiased as i→∞, and then show that SGD with such asymptotically unbiased gradients converges to a local optimum.\nTheorem 2 generalizes to graph attention networks (GAT) (Veličković et al., 2018). We leave the variance reduced stochastic estimators for GAT, and discussions on the convergence of GAT and other models in Appendix C.5."
  }, {
    "heading": "5. Handling Dropout of Features",
    "text": "In this section, we consider introducing a third source of randomness, the random dropout of features (Srivastava et al., 2014), which is adopted in various GCN models as a regularization (Kipf & Welling, 2017; Veličković et al., 2018). With dropout, the GCN layer becomesZ(l+1) = M◦ (PH(l))W (l), where Mij ∼ Bern(p) are i.i.d. Bernoulli random variables, and ◦ is the element-wise product. Let EM be the expectation over dropout masks.\nWith dropout, all the activations h(l)v are random variables whose randomness comes from dropout, even in the exact algorithm Eq. (1). We want to design a cheap estimator for the random variable (PH(l))u =∑ v∈n(u) Puvh (l) v , based on a stochastic neighborhood n̂(l)(u). An ideal estimator should have the same distribution with (PH(l))u. However, such an estimator is difficult to design. Instead, we develop an estimator CVD(l)u that eventually has the same mean and variance with (PH(l))u, i.e., En̂(l)(u)EMCVD (l) u = EM (PH(l))u and Varn̂(l)(u)VarMCVD (l) u = VarM (PH (l))u."
  }, {
    "heading": "5.1. Control Variate for Dropout",
    "text": "With dropout, ∆h(l)v = h (l) v − h̄(l)v is not necessarily small even if h̄(l)v and h (l) v have the same distribution. We develop another stochastic approximation algorithm, control variate for dropout (CVD), that works well with dropout.\nOur method is based on the weight scaling procedure (Srivastava et al., 2014) to approximately compute the mean µ (l) v := EM [ h (l) v ] . That is, along with the dropout model,\nwe can run a copy of the model without dropout to obtain the mean µ(l)v , as illustrated in Fig. 1(d). We obtain a stochastic approximation by separating the mean and variance (PH(l))u = ∑\nv∈n(u)\nPuv (̊h (l) v + ∆µ (l) v + µ̄ (l) v ) ≈ CVD(l)u\n:= √ R ∑ v∈n̂ Puvh̊ (l) v +R ∑ v∈n̂ Puv∆µ (l) v + ∑ v∈n(u) Puvµ̄ (l) v ,\nwhere we define n = n̂(l)(u), R = n(u)/D(l) for short, h̊ (l) v = h (l) v − µ(l)v , µ̄(l)v is the historical mean activation, obtained by storing µ(l)v instead of h (l) v , and ∆µ (l) v = µ (l) v − µ̄ (l) v . We separate h (l) v as three terms, the latter two terms on µ(l)v do not have the randomness from dropout, and µ (l) v are treated as if h(l)v for the CV estimator. The first term has zero mean w.r.t. dropout, i.e., EM h̊(l)v = 0. We have En̂(l)(u)EMCVD (l) u = 0 + ∑ v∈n(u) Puv(∆µ (l) v + µ̄ (l) v ) = EM (PH(l))u, i.e., the estimator is unbiased, and we shall see that the estimator eventually has the correct variance if h (l) v ’s are uncorrelated in Sec. 5.2."
  }, {
    "heading": "5.2. Variance Analysis",
    "text": "We analyze the variance under the assumption that the node activations are uncorrelated, i.e., CovM [ h (l) v1 , h (l) v2 ] = 0,∀v1 6= v2. We report the correlation between nodes empirically in Appendix G. To facilitate the analysis of the variance, we introduce two propositions proven in Appendix A . The first helps the derivation of the dropout variance; and the second implies that we can treat the variance introduced by neighbor sampling and by dropout separately.\nProposition 2. If n̂(l)(u) contains D(l) samples from the set n(u) without replacement, x1, . . . , xV are random variables, ∀v,E [xv] = 0 and ∀v1 6= v2,Cov [xv1 , xv2 ] = 0, then VarX,n̂(l)(u) [ n(u) D(l) ∑ v∈n̂(l)(u) xv ] = n(u) D(l) ∑ v∈n(u) Var [xv] . Proposition 3. X and Y are two random variables, and\nf(X,Y ) and g(Y ) are two functions. If EXf(X,Y ) = 0, then VarX,Y [f(X,Y ) + g(Y )] = VarX,Y f(X,Y ) + VarY g(Y ).\nBy Proposition 3, Varn̂VarMCVD(l)u can be written as the sum of Varn̂VarM [√ R ∑ v∈n̂ Puvh̊ (l) v ] and\nVarn̂ [ R ∑ v∈n̂ Puv∆µ (l) v + ∑ v∈n(u) Puvµ̄ (l) v ] . We refer the first term as the variance from dropout (VD) and the second term as the variance from neighbor sampling (VNS). Ideally, VD should equal to the variance of (PH(l))u and VNS should be zero. VNS can be derived by replicating the analysis in Sec. 3.2, and replacing h with µ. Let s(l)v = VarMh (l) v = VarM h̊ (l) v , and S\n(l) u = VarM (PH(l))u =∑\nv∈n(u) P 2 uvs (l) v , By Proposition 2, VD of CVD(l)u is∑\nv∈n(u) P 2 uvVar [̊ h (l) v ] = S (l) u , wich equals with the VD\nof the exact estimator as desired.\nWe summarize the estimators and their variances in Table 2, where the derivations are in Appendix A. As in Sec. 3.2, VNS of CV and CVD depends on ∆µv , which converges to zero as the training progresses, while VNS of NS depends on the non-zero µv. On the other hand, CVD is the only estimator except the exact one that gives correct VD."
  }, {
    "heading": "5.3. Preprocessing Strategy",
    "text": "There are two possible models adopting dropout, Z(l+1) = P (M ◦ H(l))W (l) or Z(l+1) = M ◦ (PH(l))W (l). The difference is whether the dropout layer is before or after neighbor averaging. Kipf & Welling (2017) adopt the former one, and we adopt the latter one, while the two models perform similarly in practice, as we shall see in Sec. 6.1. The advantage of the latter model is that we can preprocess U (0) = PH(0) = PX and takes U (0) as the new input. In this way, the actual number of graph convolution layers is reduced by one — the first layer is merely a fully-connected layer instead of a graph convolution one. Since most GCNs only have two graph convolution layers (Kipf & Welling, 2017; Hamilton et al., 2017a), this gives a significant reduction of the receptive field size and speeds up the computation. We refer this optimization as the preprocessing strategy.\n0 50 100 150 2000.4\n0.6\n0.8\n1.0 citeseer\n0 50 100 150 2000.2\n0.4\n0.6\n0.8\n1.0 cora\n0.8 pubmed\n1.5 nell"
  }, {
    "heading": "6. Experiments",
    "text": "We examine the variance and convergence of our algorithms empirically on six datasets, including Citeseer, Cora, PubMed and NELL from Kipf & Welling (2017) and Reddit, PPI from Hamilton et al. (2017a), with the same train / validation / test splits, as summarized in Table 1. To measure the predictive performance, we report Micro-F1 for the multi-label PPI dataset, and accuracy for all the other multi-class datasets. The model is GCN for the former 4 datasets and GraphSAGE-mean (Hamilton et al., 2017a) for the latter 2 datasets, see Appendix E for the details on the architectures. We repeat the convergence experiments 10 times on Citeseer, Cora, PubMed and NELL, and 5 times on Reddit and PPI. The experiments are done on a Titan X (Maxwell) GPU."
  }, {
    "heading": "6.1. Impact of Preprocessing",
    "text": "We first examine the impact of switching the order of dropout and computing neighbor averaging in Sec. 5.3. Let M0 be the Z(l+1) = P (M ◦H(l))W (l) model by (Kipf & Welling, 2017), and M1 be our Z(l+1) = M ◦ (PH(l))W (l)\n0.72 citeseer\n0.80 cora\nmodel, we compare three settings: M0 and M1 are exact algorithms without any neighbor sampling, and M1+PP samples a large number of D(l) = 20 neighbors and preprocesses PH(0) so that the first neighbor averaging is exact. In Table 3 we can see that all the three settings performs similarly, i.e., switching the order does not affect the predictive performance. Therefore, we use the fastest M1+PP as the exact baseline in following convergence experiments."
  }, {
    "heading": "6.2. Convergence Results",
    "text": "Having the M1+PP algorithm as an exact baseline, the next goal is reducing the time complexity per epoch to make it comparable with the time complexity of MLP, by setting D(l) = 2. We cannot set D(l) = 1 because GraphSAGE explicitly need the activation of a node itself besides the average of its neighbors. Four approximate algorithms are included for comparison: (1) NS, which adopts the NS estimator with no preprocessing. (2) NS+PP, which is same with NS but uses preprocessing. (3) CV+PP, which adopts the CV estimator and preprocessing. (4) CVD+PP, which uses the CVD estimator. All the four algorithms have similar low time complexity per epoch with D(l) = 2, while M1+PP takes D(l) = 20. We study how much convergence speed per epoch and model quality do these approximate algorithms sacrifice comparing with the M1+PP baseline.\nWe set the dropout rate as zero and plot the training loss with respect to number of epochs as Fig. 2. We can see that CV+PP can always reach the same training loss with M1+PP, while NS, NS+PP and IS+PP have higher training losses because of their biased gradients. CVD+PP is not included because it is the same with CV+PP when the dropout rate is zero. The results matches the conclusion of Theorem 2, which states that training with the CV estimator converges\nto a local optimum of Exact, regardless of D(l).\nNext, we turn dropout on and compare the validating accuracy obtained by the model trained with different algorithms at each epoch. Regardless of the training algorithm, the exact algorithm is used for computing predictions on the validating set. The result is shown in Fig. 3. We find that when dropout is present, CVD+PP is the only algorithm that can reach comparable validation accuracy with the exact algorithm on all datasets. Furthermore, its convergence speed with respect to the number of epochs is comparable with M1+PP, implying almost no loss of the convergence speed despite itsD(l) is 10 times smaller. This is already the best we can expect - comparable time complexity with MLP, yet similar model quality with GCN. CVD+PP performs much better than M1+PP on the PubMed dataset, we suspect it finds a better local optimum. Meanwhile, the simpler CV+PP also reaches a comparable accuracy with M1+PP for all datasets except PPI. IS+PP works worse than NS+PP on the Reddit and PPI datasets, perhaps because sometimes nodes can have no neighbor selected, as we mentioned in Sec. 2.4. Our accuracy result for IS+PP can match the result reported by Chen et al. (2018), while their NS baseline, GraphSAGE (Hamilton et al., 2017a), does not implement the preprocessing technique in Sec. 5.3."
  }, {
    "heading": "6.3. Further Analysis on Time Complexity, Testing Accuracy and Variance",
    "text": "Table 4 reports the average number of epochs and time to reach a given 96% validation accuracy on the largest Reddit dataset. Sparse and dense computations are defined in Sec. 3.4. We found that CVD+PP is about 7 times faster than M1+PP due to the significantly reduced receptive field size. Meanwhile, NS and IS+PP does not converge to the given accuracy.\nWe compare the quality of the predictions made by different algorithms, using the same model trained with M1+PP in Fig. 4. As Theorem 1 states, CV reaches the same testing accuracy as the exact algorithm, while NS and NS+PP perform much worse.\nFinally, we compare the average bias and variance of the gradients per dimension for first layer weights relative to the magnitude of the weights in Fig. 5. For models without dropout, the gradient of CV+PP is almost unbiased. For models with dropout, the bias and variance of CV+PP and CVD+PP are usually smaller than NS and NS+PP."
  }, {
    "heading": "7. Conclusions",
    "text": "The large receptive field size of GCN hinders its fast stochastic training. In this paper, we present control variate based algorithms to reduce the receptive field size. Our algorithms can achieve comparable convergence speed with the exact algorithm even the neighbor sampling size D(l) = 2, so that the per-epoch cost of training GCN is comparable with training MLPs. We also present strong theoretical guarantees, including exact prediction and the convergence to a local optimum. Our code is released at https: //github.com/thu-ml/stochastic_gcn."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Shuyu Cheng for his help in proofreading. This work was supported by NSFC Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, Siemens and Intel. L.S. was also supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS1639792 EAGER, NSF CNS-1704701, ONR N00014-15-12340, Intel ISTC, NVIDIA and Amazon AWS."
  }],
  "year": 2018,
  "references": [{
    "title": "Tensorflow: A system for large-scale machine learning",
    "authors": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M Isard"],
    "venue": "In OSDI,",
    "year": 2016
  }, {
    "title": "Graph convolutional matrix completion",
    "authors": ["Berg", "R. v. d", "T.N. Kipf", "M. Welling"],
    "venue": "arXiv preprint arXiv:1706.02263,",
    "year": 2017
  }, {
    "title": "Fastgcn: Fast learning with graph convolutional networks via importance sampling",
    "authors": ["J. Chen", "T. Ma", "C. Xiao"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "node2vec: Scalable feature learning for networks",
    "authors": ["A. Grover", "J. Leskovec"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2016
  }, {
    "title": "Inductive representation learning on large graphs",
    "authors": ["W. Hamilton", "Z. Ying", "J. Leskovec"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Representation learning on graphs: Methods and applications",
    "authors": ["W.L. Hamilton", "R. Ying", "J. Leskovec"],
    "venue": "arXiv preprint arXiv:1709.05584,",
    "year": 2017
  }, {
    "title": "Variational graph auto-encoders",
    "authors": ["T.N. Kipf", "M. Welling"],
    "venue": "arXiv preprint arXiv:1611.07308,",
    "year": 2016
  }, {
    "title": "Semi-supervised classification with graph convolutional networks",
    "authors": ["T.N. Kipf", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Convolutional networks for images, speech, and time series",
    "authors": ["Y. LeCun", "Y Bengio"],
    "venue": "The handbook of brain theory and neural networks,",
    "year": 1995
  }, {
    "title": "Deepwalk: Online learning of social representations",
    "authors": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"],
    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2014
  }, {
    "title": "Stochastic simulation, volume 316",
    "authors": ["B.D. Ripley"],
    "year": 2009
  }, {
    "title": "Modeling relational data with graph convolutional networks",
    "authors": ["M. Schlichtkrull", "T.N. Kipf", "P. Bloem", "Berg", "R. v. d", "I. Titov", "M. Welling"],
    "venue": "arXiv preprint arXiv:1703.06103,",
    "year": 2017
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "Journal of machine learning research,",
    "year": 1929
  }, {
    "title": "Line: Large-scale information network embedding",
    "authors": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"],
    "venue": "In Proceedings of the 24th International Conference on World Wide Web,",
    "year": 2015
  }, {
    "title": "Graph attention networks",
    "authors": ["P. Veličković", "G. Cucurull", "A. Casanova", "A. Romero", "P. Liò", "Y. Bengio"],
    "venue": "In ICLR,",
    "year": 2018
  }],
  "id": "SP:f096c104b8f352b14b7ef1662a34edd085a5f1a0",
  "authors": [{
    "name": "Jianfei Chen",
    "affiliations": []
  }, {
    "name": "Jun Zhu",
    "affiliations": []
  }, {
    "name": "Le Song",
    "affiliations": []
  }],
  "abstractText": "Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms with new theoretical guarantee to converge to a local optimum of GCN regardless of the neighbor sampling size. Empirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors per node. The running time of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.",
  "title": "Stochastic Training of Graph Convolutional Networks with Variance Reduction"
}