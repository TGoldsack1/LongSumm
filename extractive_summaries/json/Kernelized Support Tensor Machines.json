{
  "sections": [{
    "text": "1 Kernelized Support Tensor Train Machines Cong Chen, Kim Batselier, Wenjian Yu, Senior Member, IEEE, Ngai Wong, Senior Member, IEEE\nAbstract—Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrixbased, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TTbased kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.\nI. INTRODUCTION\nMany real-world data appear in matrix or tensor format. For example, a grayscale picture is a 2-way tensor (i.e. a matrix), a color image or a grayscale video is naturally a 3-way tensor, and a color video can be regarded as a 4- way tensor. In such circumstances, extending the vector-based machine learning algorithms to their tensorial format has recently attracted significant interest in the machine learning and data mining communities.For example, neighborhood preserving embedding (NPE) was extended to tensor neighborhood preserving embedding (TNPE) in [1], principal component analysis to multilinear principal component analysis (MPCA) in [2], support vector machines (SVMs) [3] to support tensor machines (STMs) in [4], and restricted Boltzmann machines to their tensorial formats in [5].\nBy reformulating the aforementioned machine learning algorithms into the tensorial framework, a huge performance improvement has been achieved. The main reasons for this improvement can be summarized as follows. Firstly, these tensorized algorithms can naturally utilize the multi-way structure of the original tensor data, which is believed to be useful in\nThis work is partially supported by the Hong Kong Research Grants Council under Project 17246416, the University Research Committee of The University of Hong Kong, Tsinghua University Initiative Scientific Research Program, and NSFC under grant No. 61872206.\nCong Chen is with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong. Email: chencong@eee.hku.hk.\nKim Batselier is with the Delft Center for Systems and Control, Delft University of Technology, Delft, Netherlands. Email: k.batselier@tudelft.nl.\nWenjian Yu is with BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. Email: yuwj@tsinghua.edu.cn.\nNgai Wong is with the Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong. Email: nwong@eee.hku.hk.\nmany machine learning applications such as pattern recognition [6], image completion [7] and anomaly detection [8]. Secondly, vectorizing tensor data leads to high-dimensional vectors, which may cause overfitting especially when the training sample size is relatively small [9]. On the contrary, tensor-based approaches usually derive a more structural and robust model that commonly involves much fewer model parameters, which not only alleviates the overfitting problem, but also saves a lot of storage and computation resources [10], [11].\nIn this paper, we propose a kernelized support tensor train machine (K-STTM) to address few-sample image classification problems due to the fact that collecting labeled pictures is very expensive and time-consuming in many research areas. Specifically, we first employ the tensor train (TT) decomposition [12] to decompose the given tensor data so that a more compact and informative representation of it can be derived. Secondly, we define a TT-based feature mapping strategy to derive a high-dimensional TT in the feature space. This strategy enables us to apply different feature mappings on different data modes, which naturally provides a way to leverage the multi-modal nature of tensor structured data. Thirdly, we propose two ways to build the kernel matrix with the consideration of the consistency with the TT inner product and preservation of information. The constructed kernel matrix is then used by kernel machines to solve the image classification problems.\nThere are two main advantages from the proposed methods. On the one hand, the proposed methods are naturally nonlinear classifiers. It is common that real-life data are not linearly separable. However, most existing supervised tensor learning methods which employ tensor input are often based on a linear model and cannot deal with nonlinear classification problems. In that case, our proposed methods can handle nonlinear learning problems on tensor data better. On the other hand, conventional tensor-based kernel methods focus on flatting tensor data into matrices [13], [14], and thus can only preserve one-mode relationships within the tensor itself. However, our proposed approaches can capture and exploit multi-mode relationships, which commonly leads to more powerful and accurate models.\nThe superiority of our methods is validated through extensive experiments. It is observed that our methods achieve a much better performance than the linear supervised tensor learning methods, which indicates the importance of introducing kernel trick. Furthermore, our methods achieve a better classification performance when the input data are truly highdimensional. Applying different kernel functions on different data modes is also investigated and shows an obvious improvement compared with the baseline.\nThe rest of this paper is organized as follows. In Section II,\nar X\niv :2\n00 1.\n00 36\n0v 1\n[ cs\n.L G\n] 2\nJ an\n2 02\n0\n2 we briefly review some related works in supervised tensor learning. Some useful notations and tensor arithmetic are further introduced in Section III. In Section IV, we formulate the proposed kernelized support tensor train machine (K-\nSTTM). Experiments are shown in Section V to validate the superiority of our methods. Lastly, we draw conclusions and propose some possible extended works in Section VI."
  }, {
    "heading": "II. RELATED WORKS",
    "text": "As one of the most typical supervised learning algorithms, SVM [3] has achieved an enormous success in pattern classification by minimizing the Vapnik-Chervonenkis dimensions and structural risk. However, a standard SVM can not deal with tensorial input directly. The first work that extends SVM to handle tensorial input is [4]. More precisely, a supervised tensor learning (STL) scheme was proposed to train a support tensor machine (STM), where the hyperplane parameters are modeled as a rank-1 tensor instead of a vector. For the parameter training, they employed the alternating projection optimization method.\nAlthough STM is capable to classify tensorial data directly, the expressive power of a rank-1 weight tensor is limited, which often leads to a poor classification accuracy. To increase the model expression capacity, several works were proposed recently based on the STL scheme. Ref. [15] employs a more general tensor structure, i.e., the canonical polyadic (CP) format, to replace the rank-1 weight tensor in STM. However, it is an NP-complete problem to determine the CP-rank. In [16], the STM is generalized to a support Tucker machine (STuM) by representing the weight parameter as a Tucker tensor. Nevertheless, the number of model parameters in STuM is exponentially large, which often leads to a large amount of storage and computation consumption. To overcome this, Ref. [17] proposed a support tensor train machine (STTM), which assumes the potential weight tensor format is a TT. By doing so, the corresponding optimization problem is more scalable and can be solved efficiently. The aforementioned work are all based on the assumption that the given tensorial data are linearly separable. However, this is not the case in most real-world data. It is worth noting that though STTM sounds like the linear case of the proposed K-STTM, they are totally different when the linear kernel is applied on K-STTM. Specifically, K-STTM and STTM use two totally different schemes to train the corresponding model. For KSTTM, it first constructs the kernel matrix with the proposed TT-based kernel function, and then solves the standard SVM problem. However, in STTM, it assumes the parameter in the classification hyperplane can be modeled as a TT, and only updates one TT-core at a time by reformulating the training data.\nTo extend the linear tensorial classifiers to the nonlinear case, the authors in [18] proposed a nonlinear supervised learning scheme called dual structure-preserving kernels (DuSK). Specifically, based on the CP tensor structure, they define a corresponding kernel trick to map the CP format data into a higher-dimensional feature space. Through the introduction of the kernel trick, DuSK can achieve a higher classification\na a A A\nFig. 1: Graphical representation of a scalar a, vector a, matrix A, and third-order tensor A.\nI1\nI2 I4\nI3 I5A B\nFig. 2: Index contraction between two 3-way tensors A and B.\naccuracy. However, since DuSK is based on the CP decomposition, the NP-complete problem on the rank determination still exists. Moreover, through introducing a kernelized CP tensor factorization technique, the same research group in [18] further proposed the Multi-way Multi-level Kernel model [19] and kernelized support tensor machine model [20]. Nevertheless, the CP-rank determination issue still exists since they are all based on the CP decomposition.\nTo avoid the above issues, we propose the K-STTM, which not only introduces the customized kernel function to handle nonlinear classification problems, but also achieves an efficient model training since the scalable TT format is employed."
  }, {
    "heading": "III. PRELIMINARIES",
    "text": "In this Section, we review some basic tensor notations and operations, together with the related tensor train decomposition method."
  }, {
    "heading": "A. Tensor basics",
    "text": "Tensors in this paper are multi-dimensional arrays that generalize vectors (first-order tensors) and matrices (secondorder tensors) to higher orders. A dth-order or d-way tensor is denoted as A ∈ RI1×I2×···×Id and the element of A by A(i1, i2 . . . , id), where 1≤ ik ≤ Ik, k = 1, 2, . . . , d. The numbers I1, I2, . . . , Id are called the dimensions of the tensor A. We use boldface capital calligraphic letters A, B, . . . to denote tensors, boldface capital letters A, B, . . . to denote matrices, boldface letters a, b, . . . to denote vectors, and roman letters a, b, . . . to denote scalars. An intuitive and useful graphical representation of scalars, vectors, matrices and tensors is depicted in Figure 1. The unconnected edges, also called free legs, are the indices of the tensor. Therefore scalars have no free legs, while a matrix has 2 free legs. We will employ these graphical representations to visualize the tensor networks and operations in the following sections whenever possible and refer to [21] for more details. We now briefly introduce some important tensor operations.\nDefinition 1: (Tensor index contraction): A tensor index contraction is the sum over all possible values of the repeated indices in a set of tensors.\n3 A(1) A(2) A(d)... R1 R2 R3 Rd Rd+1\nI1 I2 Id\nFig. 3: Tensor train decomposition of a d-way tensor A into d 3-way tensors A(1),A(2) . . . ,A(d).\nFor example, the following contraction of two 3-way tensors A and B\nC(i1, i2, i4, i5) = I3∑ i3=1 A(i1, i2, i3)B(i3, i4, i5),\nover the i3 index produces a four-way tensor C. We also present the graphical representation of this contraction in Figure 2, where the summation over i3 is indicated by the connected edge. After this contraction, the tensor diagram contains four free legs indexed by i1, i2, i4, i5, respectively.\nDefinition 2: (Tensor inner product): For two tensors A,B ∈ RI1×I2×···×Id , their inner product 〈A,B〉 is defined as\n〈A,B〉 = I1∑ i1=1 I2∑ i2=1 · · · Id∑ id=1 ai1,i2,··· ,idbi1,i2,··· ,id .\nDefinition 3: (Tensor Frobenius norm): The Frobenius norm of a tensor A ∈ RI1×I2×···×Id is defined as ||A||F =√ 〈A,A〉."
  }, {
    "heading": "B. Tensor train decomposition",
    "text": "Here we briefly introduce the tensor train (TT) decomposition that will be utilized in the proposed K-STTM. A TT decomposition [12] represents a d-way tensor A as d 3-way tensors A(1), A(2), . . . , A(d) such that a particular entry of A is written as the matrix product\nA(i1, . . . , id) = A(1)(:, i1, :) · · ·A(d)(:, id, :), (1)\nwhere A(k)(:, ik, :) is naturally a matrix since we fix the second index. Each tensor A(k), k = 1, . . . , d, is called a TT-core and has dimensions Rk × Ik ×Rk+1. Storage of a tensor as a TT therefore reduces from\n∏d i=1 Ri down to∑d\ni=1 RiIiRi+1. In order for the left-hand-side of (1) to be a scalar we require that R1 = Rd+1 = 1. The remaining Rk values are called the TT-ranks. Figure 3 demonstrates how TT-decomposition decomposes a d-way tensor A, where the edges connecting the different circles indicate the matrixmatrix products of (1). To simplify the statement, we define the notation TT (·), which means perform TT decomposition on a d-way tensor. For example, TT (A) is the resulting TT after doing TT decomposition on the full tensor A, namely, A(1), A(2), · · · , A(d) are derived.\nDefinition 4: (TT inner product): The inner product between two tensor trains TT (A) and TT (B) is denoted as 〈TT (A), TT (B)〉. The tensor network diagram of the inner product of two TTs is shown in Figure 4. The lack of unconnected edges in Figure 4 implies that 〈TT (A), TT (B)〉 is a scalar.\nA(1) A(2) A(d)\nB(1) B(2) B(d)\n...\n...\nFig. 4: The inner product between two d-way tensor trains."
  }, {
    "heading": "C. Support vector machines",
    "text": "Since this work is based on traditional SVM, we therefore briefly review the main idea of an SVM. Assume we have a dataset D={xi, yi}Mi=1 of M labeled samples, where xi ∈ Rn are the vectorized data samples with labels yi ∈ {−1, 1}. The goal of an SVM is to find a discriminant hyperplane\nf(x) = wTx + b (2)\nthat maximizes the margin between the two classes where w and b are the weight vector and bias, respectively. However, an SVM is very sensitive to noise since it requires all the training samples to meet the hard margin constraint. In that case, the trained model tends to overfit. To solve this, slack variables ξ1, . . . , ξM are introduced to allow some certain samples to be misclassified, thus enhancing the robustness of the trained model. We can express the learning problem as a quadratic optimization problem\nmin w,b,ξ\n1 2 ||w||2F + C M∑ i=1 ξi\nsubject to yi(wTxi + b) ≥ 1− ξi, ξi ≥ 0, i = 1, . . . ,M. (3)\nThe parameter C controls the trade-off between the size of the weight vector w and the size of the slack variables. It is more common to solve the dual problem of (3), especially when the feature size n is larger than the sample size M . The dual problem format of (3) is\nmin α1,α2,··· ,αM M∑ i=1 αi− 1 2 M∑ i,j=1 αiαjyiyj〈xi,xj〉\nsubject to M∑ i=1 αiyi = 0,\n0 ≤ αi ≤ C, i = 1, . . . ,M, (4)\nwhere 〈xi,xj〉 represents the inner product between vector xi and xj and αi (i = 1, . . . ,M) are the Lagrange multipliers.\nTo solve a nonlinear classification problem with SVM, researchers further introduced the kernel trick that projects the original vectorial data onto a much higher-dimensional feature space through a nonlinear mapping function φ. In the feature space, the data generally become more (linearly) separable. By doing so, the optimization in (4) is transformed into\nmin α1,α2,··· ,αM M∑ i=1 αi− 1 2 M∑ i,j=1 αiαjyiyj〈φ(xi), φ(xj)〉 (5)\n4 with the same constraints as in (4). It turns out that it is possible to make the computation easier by replacing the inner product term 〈φ(xi), φ(xj)〉 with a kernel function k(xi,xj). In that case, the inner product in the high-dimensional feature space can be computed without the need to explicitly compute the mappings φ(xi), φ(xj)."
  }, {
    "heading": "IV. KERNELIZED SUPPORT TENSOR TRAIN MACHINES",
    "text": "In this section, we first demonstrate the tensor-based kernel learning problem and then introduce the proposed K-STTM."
  }, {
    "heading": "A. Problem statement",
    "text": "Given M tensorial training data and their labels, i.e., dataset D = {X i, yi}Mi=1, where X i ∈ RI1×I2×···×Id and yi ∈ {−1, 1}, we want to find a hyperplane\nf(X ) = 〈W ,X 〉+ b (6) that separates the tensorial data into two classes. W is the hyperplane weight tensor with the same dimensions as X i and b is the bias. Similar to the primal problem in SVM, we can derive the corresponding primal optimization problem for (6)\nmin W,b,ξ\n1 2 ||W ||2F + C M∑ i=1 ξi\nsubject to yi(〈W ,X i〉+ b) ≥ 1− ξi, ξi ≥ 0, i = 1, . . . ,M. (7)\nFollowing the scheme of the kernel trick for conventional SVMs, we introduce a nonlinear feature mapping function Φ(·). Then, given a tensor X ∈ RI1×I2×···×Id , we assume it is mapped into the Hilbert space H by\nΦ : X → Φ(X ) ∈ RH1×H2×···×Hd . (8) We need to mention that the dimension of projected tensor Φ(X ) can be infinite depending on the feature mapping function Φ(·). The resulting Hilbert space is then called the tensor feature space and we can further develop the following model\nmin W,b,ξ\n1 2 ||W ||2F + C M∑ i=1 ξi\nsubject to yi(〈W ,Φ(X i)〉+ b) ≥ 1− ξi, ξi ≥ 0, i = 1, . . . ,M, (9)\nwith parameter tensor W ∈ RH1×H2×···×Hd . This model is naturally a linear classifier on the tensor feature space. However, when we map the classifier back to the original data space, it is a nonlinear classifier. To obtain the tensor-based kernel optimization model, we need to transfer model (9) into its dual, namely\nmin α1,α2,··· ,αM M∑ i=1 αi− 1 2 M∑ i,j=1 αiαjyiyj〈Φ(X i),Φ(X j)〉\nsubject to M∑ i=1 αiyi = 0,\n0 ≤ αi ≤ C, i = 1, . . . ,M, (10)\nwhere αi are the Lagrange multipliers. The key task we need to solve is to define a tensorial kernel function K(X i,X j) that computes the inner product 〈Φ(X i),Φ(X j)〉 in the original data space instead of the feature space."
  }, {
    "heading": "B. Customized kernel mapping schemes for TT-based data",
    "text": "Although tensor is a natural structure for representing realworld data, there is no guarantee that such a representation works well for kernel learning. Instead of the full tensor, here we employ a TT for data representation due to the following reasons:\n1) Real-life data often contain redundant information, which is not useful for kernel learning. The TT decomposition has proven to be efficient for removing the redundant information in the original data and provides a more compact data representation. 2) Compared to the Tucker decomposition whose storage scales exponentially with the core tensor, a TT is more scalable (parameter number grows linearly with the tensor order d), which reduces the computation during kernel learning. 3) Unlike the CP decomposition, determining the TT-rank is easily achieved through a series of singular value decompositions (TT-SVD [12] ). This naturally leads to a faster data transformation to the TT format. 4) It is convenient to implement different operations on different tensor modes when data is in the TT format. Since a TT decomposition decomposes the original data into many TT cores, it is possible to apply different kernel functions on different TT cores for a better classification performance. Furthermore, we can emphasize the importance of different tensor modes by putting different weights on those TT cores during the kernel mapping. For example, a color image is a 3-way (pixel-pixel-color) tensor. The color mode can be treated differently with the two pixel modes since they contain different kinds of information, as will be exemplified later.\nIn the following, we demonstrate the proposed TT-based feature mapping approach. Specifically, we map all fibers in each TT-core to the feature space, namely\nΦ : X (i)(ri, :, ri+1)→ Φ(X (i)(ri, :, ri+1)) ∈ RHi\n1 ≤ ri ≤ Ri, i = 1, . . . , d, (11)\nwhere X (i) and Ri are the i-th TT-core and TT-rank of TT (X ), respectively. The fibers of each TT-core are naturally vectors, so the feature mapping works in the same way as for the conventional SVM. We then represent the resulting high-dimensional TT, which is in the tensor feature space, as Φ(TT (X )) ∈ RH1×H2×···×Hd . We stress that Φ(TT (X )) is still in a TT format with the same TT-ranks as TT (X ). In this sense, the TT format data structure is preserved after the feature mapping.\nAfter mapping the TT format data into the TT-based highdimensional feature space, we then demonstrate the two proposed approaches for computing the inner product between two mapped TT format data using kernel function.\n5 1) K-STTM-Prod: The first method is called K-STTM-Prod since we implement consecutive multiplication operations on d fiber inner products, which is consistent with the result of an inner product between two TTs. Assuming Φ(TT (X )) and Φ(TT (Y)) ∈ RH1×H2×···×Hd with TT-ranks Ri and R̂i, i = 1, 2, . . . , d, respectively, their inner product can be computed from\n〈Φ(TT (X )),Φ(TT (Y))〉 = R1∑ r1=1 · · · Rd∑ rd=1 R̂1∑ r̂1=1 · · · R̂d∑ r̂d=1\n( d∏ i=1 〈Φ(X (i)(ri, :, ri+1)),Φ(Y(i)(r̂i, :, r̂i+1))〉). (12)\nWe remark that (12) derives the exact same result as Figure 4 (assuming X = A and Y = B) when an identity feature mapping function Φ(·) is used, namely Φ(TT (X ))=TT (X ). What is more, since each fiber of a mapped TT-core is naturally a vector, we have\n〈Φ(X (i)(ri, :, ri+1)),Φ(Y(i)(r̂i, :, r̂i+1))〉 = K(X (i)(ri, :, ri+1),Y(i)(r̂i, :, r̂i+1)), (13)\nwhere K(·) can be any kernel function used for a standard SVM, such as a Gaussian RBF kernel, polynomial kernel, linear kernel etc. Combining (12) and (13), we obtain the corresponding TT-based kernel function\nK(TT (X ), TT (Y)) = R1∑ r1=1 · · · Rd∑ rd=1 R̂1∑ r̂1=1 · · · R̂d∑ r̂d=1\n( d∏ i=1 K(X (i)(ri, :, ri+1),Y(i)(r̂i, :, r̂i+1))). (14)\nAs mentioned before, different kernel functions can be applied on different tensor modes i = 1, 2, . . . , d. Therefore, the second line in (14) can be generalized to\n( d∏ i=1 Ki(X (i)(ri, :, ri+1),Y(i)(r̂i, :, r̂i+1))).\nOne possible application is in color image classification, where one could apply Gaussian RBF kernels K1 and K2 on its first two spatial modes, while choosing a linear or polynomial kernel K3 for the color mode. This will be investigated in the experiments.\n2) K-STTM-Sum: The second method we propose to construct a TT kernel function is called K-STTM-Sum. Instead of implementing consecutive multiplication operations on d fiber inner products like in K-STTM-Prod, K-STTM-Sum performs consecutive addition operations on them. This idea is inspired by [22] which argues that the product of inner products can lead to the loss/misinterpretation of information. Take the linear kernel as an example, the inner product between two fibers of the same mode could be negative, which indicates a low similarity between those two fibers. However, by implementing consecutive multiplication operations on d fiber inner products, highly negative values could result in a large positive value. In that case, the overall similarity is high which is clearly unwanted. This situation also appears\nwhen employing Gaussian RBF kernels. A nearly zero value would be assigned to two non-similar fibers, which could influence the final result significantly. To this end, we propose the K-STTM-Sum. Similar to K-STTM-Prod, we can obtain the corresponding kernel function as\nK(TT (X ), TT (Y)) = R1∑ r1=1 · · · Rd∑ rd=1 R̂1∑ r̂1=1 · · · R̂d∑ r̂d=1\n( d∑ i=1 Ki(X (i)(ri, :, ri+1),Y(i)(r̂i, :, r̂i+1))). (15)"
  }, {
    "heading": "C. Kernel optimization problem",
    "text": "After defining the TT-based kernel function, we can then replace the term 〈Φ(X i),Φ(X j)〉 in (10) with (14) or (15), and derive our final kernel optimization problem based on the TT structure, namely,\nmin α1,α2,··· ,αM M∑ i=1 αi− 1 2 M∑ i,j=1 αiαjyiyjK(TT (X i), TT (X j))\nsubject to M∑ i=1 αiyi = 0,\n0 ≤ αi ≤ C, i = 1, . . . ,M. (16)\nAfter solving (16), we can get the unknown model parameters α1, α2, . . . , αM and the resulting decision function is then represented as\nf(X ) = sign( M∑ i=1 αiyiK(TT (X i), TT (X )) + b). (17)\nHere we take the Gaussian RBF kernel as an example and summarize the training algorithm of the K-STTM-Prod/Sum as pseudo-code in Algorithm 1. An alternative for doing a grid search to find optimal hyperparameters would be crossvalidation. Generalizing the binary classification to multiclassification can be easily achieved by utilizing a one-vs-one or one-vs-all strategy, namely, we can build several binary classifiers to do multi-class classification."
  }, {
    "heading": "D. Kernel validity",
    "text": "A key property of kernel function in standard SVM is that the resulting kernel matrix is positive semi-definite, which guarantees the mapped high-dimensional feature space is truly an inner product space. Therefore, we provide Theorem 1 to show the validity of K-STTM-Prod and K-STTM-Sum.\nTheorem 1: Kernel functions K-STTM-Prod and K-STTMSum produce positive semi-definite kernel matrices.\nWe provide the proof here. 1) Validity of K-STTM-Prod: We first demonstrate the kernel function validity of K-STTM-Prod. The goal is to show that the final kernel matrix constructed by (14) is positive semidefinite. In the actual implementation, it is extremely inefficient to use TT decomposition to decompose each tensorial sample one by one. The way we did it is by first stacking all\n6 Algorithm 1 K-STTM-Prod/Sum Algorithm\nInput: Training dataset {X i ∈ RI1×···×Id , yi ∈ {−1, 1}}Mi=1; Validation dataset {X j ∈ RI1×···×Id , yj ∈ {−1, 1}}Nj=1; The pre-set TT-ranks R1, R2, . . . , Rd+1; The range of the performance trade-off parameter C and kernel width parameter σ, namely [Cmin, Cmax], and [σmin, σmax]. Output: The Lagrange multipliers α1, α2, . . . , αM ; The bias b.\n1: Compute the TT approximation of training samples {X i}Mi=1 and validation set {X j}Nj=1 with the given TTranks using TT-SVD. 2: for C from Cmin to Cmax do 3: for σ from σmin to σmax do 4: Apply Gaussian RBF kernel on all the tensor modes,\nand construct the kernel matrix according to (14) or (15), which are corresponding to K-STTM-Prod and K-STTM-Sum, respectively.\n5: Solve (16) using the resulting kernel matrix. 6: Compute the classification accuracy on validation set. 7: end for 8: end for 9: Find the best C and σ according to the classification\naccuracy on validation set. 10: Train the K-STTM with the best C and σ by imple-\nmenting step 4 and 5. Thus the the Lagrange multipliers α1, α2, . . . , αM and the bias b are obtained.\nthe d-way samples and then compute a TT decomposition on the resulting (d+ 1)-way tensor directly. By doing so, all TTbased training samples have the same TT-ranks. Also in the case where we compute the TT decomposition separately for each sample, we can still set the TT-ranks of all samples to be identical. That means Ri is equal to R̂i, i = 1, 2, . . . , d for all the TT-based training samples. We can then compute the final kernel matrix by doing R21×. . .×R2d matrix summations, while each matrix in the summation procedure is computed by d times matrix Hadamard product. The matrix factors in the d times Hadamard product are valid kernel matrices since they are computed using the standard kernel function.\nThrough the above analysis, the goal now transforms into proving that the summation or Hadamard product between two positive semi-definite matrices A and B ∈ Rn×n still results in a positive semi-definite matrix.\nFor the summation case, we have{ uTAu ≥ 0, uTBu ≥ 0.\n(18)\nfor every non-zero column vector u ∈ Rn. Obviously we can conclude that\nuT (A+B)u ≥ 0, (19)\nnamely A + B is still positive semi-definite. For the Hadamard product case, we refer to the the Schur product theorem [23] and we can easily obtain\nuT (A B)u ≥ 0, (20)\nfor every non-zero column vector u ∈ Rn, where is the Hadamard product. Thus A B is still positive semi-definite.\nThrough the above analysis, we can conclude that by constraining the TT-based training samples to have identical TT-ranks, we can get a valid kernel matrix using K-STTMProd.\n2) Validity of K-STTM-Sum: The proof for the validity of K-STTM-Sum is similar as it for K-STTM-Prod. The difference between kernel functions (14) and (15) is that (15) only replaces the product with a summation. In that case, for K-STTM-Sum, the final kernel matrix is produced by the summation of a set of valid positive semi-definite matrices. Namely, we can still get a valid kernel matrix using K-STTMSum."
  }, {
    "heading": "E. Convergence and complexity",
    "text": "Here we demonstrate the convergence analysis of our proposed methods and compare the storage and computation complexity with the standard SVM.\nFor the convergence analysis, it is same as it in standard SVM problem. We already show the kernel validity of (14) and (15) in Theorem 1. With a valid kernel matrix, we can solve a quadratic programming problem to get the Lagrange multipliers αi and bias b, which is same as the procedure in standard SVM. Consequently, the convergence analysis is exactly same as it in standard SVM.\nFor the storage complexity analysis, the original tensorial sample storage is O(MId), where I is the maximum value of Ii, i = 1, 2, . . . , d. After representing the original tensorial data as TTs, the data storage becomes to O(MdIR2), where R is the maximum TT-rank. This shows a great reduction especially when the data order d is large.\nFor the computation complexity, the overall result of KSTTM-Prod is the same as the result of K-STTM-Sum if we neglect those low-order polynomial terms. This can be observed from (14) and (15). The main computation costs are similar in those two equations. Therefore we just analyze the K-STTM-Prod method. The computational complexity of constructing the kernel matrix in standard SVM is O(M2Id), where n is the maximum dimension of Ii, i = 1, 2, ...d. When applying the accelerating implementation of K-STTMProd, its kernel matrix computation complexity is O(dI2R4 + M2IdR 2 d), where I and R are the maximum values of Ii and Ri, i = 1, 2, ...d − 1, respectively. Real-world data is commonly low-rank, so the TT-ranks Ri are generally small. Moreover, their dimensions Ii are very high. That indicates our proposed method is more efficient than its vector counterpart since the computation complexity is reduced from exponential to polynomial."
  }, {
    "heading": "V. EXPERIMENTS",
    "text": "We evaluate the effectiveness of the two proposed schemes, K-STTM-Prod and K-STTM-Sum, on real-world tensorial datasets and contrast our methods with the following seven methods as a baseline. • SVM: SVM [3] is one of the most widely used vector-\nbased method for classification. What is more, the proposed K-STTM is a tensorial extension of SVM, so SVM\n7 is selected as a baseline. We employ the widely used convex optimization solver CVX∗ to solve the quadratic programming problem.\n• STM: STM [4] is the first method which extends SVM to the tensorial format, which employs alternating optimization scheme to update the weight tensors and outperforms kernel SVM in some tasks. • STuM: It is a kind of support tensor machine which is based on the Tucker decomposition [16]. The training procedure is similar as the one in STM. • STTM: STTM [17] assumes the weight tensor is a scalable tensor train, which enables STTM to deal with high-dimensional data classification. STM, STuM, and STTM are all tensor-based linear classifiers. In very small sample size problem, sometimes linear classifier are observed to achieve a better classification accuracy than nonlinear classifier [18] since a linear classifier is commonly less complex and more stable and can be better trained than nonlinear classifiers. • DuSK: DuSK is a kernelized support tensor machine using the CP decomposition [18]. Through introducing the kernel trick, it can deal with nonlinear classification tasks. • 3D CNN: CNN is one of the most powerful structure for image classification. The 3D CNN we employ here is an extension of the 2D version in [24]. We replace the 2D convolutional kernels with 3D ones and keep other settings the same. Though 3D CNN is a relatively simple CNN model, it has an advantage in dealing with small sample size problems since it can be trained better than the complicated CNN model. • TT Classifier: As an updated tensor classification method, TT classifier [25] trains a TT as a polynomial classifier and achieves good results on tensorial image classification tasks.\nFor simplicity, all of the kernel based methods, i.e., SVM, DuSK, and K-STTM, employ the Gaussian RBF kernel. The optimal parameters, namely the performance trade-off parameter C, RBF kernel parameter σ, hidden layer size in 3D CNN, plus the corresponding tensor rank in STuM, STTM, DuSK, TT classifier and K-STTM, are determined through a grid search. The detail of the hyperparameter search schemes of all methods in all experiments are demonstrated in Appendix."
  }, {
    "heading": "A. MNIST",
    "text": "First, our proposed methods are compared with the above methods on the well-known MNIST dataset [26], which has a training set of 60k samples and a testing set of 10k samples. Each sample is a 28 × 28 grayscale image of a handwritten digit {0, . . . , 9}. Although there are total 60k training samples, we care more about the small sample size problem. Thus for each class, we randomly choose 50 samples for model training and another 50 for validation. All test samples in each class are used for checking the classification performance of each trained model. Since an SVM is naturally a binary classifier,\n∗http://cvxr.com/cvx/\nwe randomly choose 10 digit pairs out of 45 to check the classification accuracy.\nTable I shows the classification results on different digit pairs. DuSK achieves the lowest accuracy among all the methods , which may be caused by the CP-rank searching: Finding a good CP-rank is an NP-complete problem, so DuSK may perform poorly if it fails to do so. Due to the naturally linearity of STM, STuM and STTM, they also can not achieve a good classification accuracy on real-world data. TT classifier even achieves a very poor classification performance on some digit pairs since it is naturally a polynomial classifier, whose classification power is limited. We notice that the two proposed approaches K-STTM-Prod and K-STTM-Sum only achieve a slightly better accuracy than SVM and 3D CNN on some digit pairs. The main reason that SVM and 3D CNN perform very well also is that MNIST is a relatively small dataset. The data dimension is 784 only, thus conventional SVM and 3D CNN do not encounter the curse of dimensionality and no overfitting occurs. The advantage of tensorial methods are expected to be more apparent when the problem is truly high-dimensional. We therefore consider in the second experiment fMRI image data, whose dimensions are higher than 32k.\nWe also investigate the influence of the TT-rank on the classification accuracy. Figure 5 shows how the classification accuracy of K-STTM-Prod and K-STTM-Sum changes along with increasing TT-rank on two randomly selected digit pairs. We can observe that K-STTM with a small TT-rank can achieve a similar classification performance when it with high TT-rank, and the highest accuracies are all achieved when TTrank is around 5, which means we can select a relatively small TT-rank R to reduce the cost of kernel computation, while at the same time keep the classification performance. This validates the computation complexity analysis in Section IV-E since the R and Rd in O(dI2R4 +M2IdR2d) are often small."
  }, {
    "heading": "B. fMRI datasets",
    "text": "As we mentioned in experiment V-A, tensorial method shows more apparent advantages on high-dimensional dataset. Thus we consider two high-dimensional fMRI datasets, namely the StarPlus fMRI dataset† and the CMU Science 2008 fMRI dataset (CMU2008) [27] to evaluate the classification performance of different models. An fMRI image is essentially a 3-way tensor. Figure 6 from [18] illustrates the tensorial structure of the fMRI image.\n1) StarPlus fMRI dataset: The fMRI images in StarPlus dataset are with dimensions 64 × 64 × 8 that contains 25 to 30 anatomically defined regions (called“Regions of Interest”, or ROIs). To achieve a better classification accuracy, we only consider the following ROIs: ‘CALC’ ‘LIPL’ ‘LT’ ‘LTRIA’ ‘LOPER’ ‘LIPS’ ‘LDLPFC’. After extracting those ROIs, we further normalize the data of each subject. StarPlus fMRI dataset contains the brain images of 6 human subjects. The data of each human subject is partitioned into trials, and each subject has 40 effective trials. Here we only use the first 4 seconds of each trial since the subject was shown one kind of\n†http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-81/www/\n8\nTABLE I: Classification accuracy of different methods for different MNIST digit pairs.\nDigit pair SVM STM STuM STTM DuSK 3D CNN TT classiifer K-STTM-Prod K-STTM-Sum\n{‘1’,‘2’} 98.15% 94.09% 97.96% 97.69% 89.16% 98.20% 73.93% 99.22% 99.27% {‘1’,‘7’} 97.73% 96.62% 97.87% 97.83% 80.95% 98.19% 96.99% 98.34% 98.20% {‘1’,‘8’} 96.49% 93.78% 95.92% 96.30% 87.63% 97.24% 84.48% 97.97% 98.06% {‘2’,‘4’} 98.36% 96.32% 97.46% 97.52% 77.26% 96.27% 93.45% 99.01% 98.61% {‘2’,‘7’} 96.41% 94.46% 95.58% 95.58% 81.26% 94.22% 94.22% 97.09% 96.95% {‘4’,‘6’} 97.16% 97.57% 97.47% 97.42% 78.66% 96.18% 93.71% 98.30% 97.74% {‘4’,‘9’} 89.50% 86.53% 90.65% 90.86% 68.46% 91.91% 59.12% 93.27% 91.77% {‘5’,‘6’} 96.00% 95.29% 95.24% 94.92% 75.78% 92.75% 88.16% 96.49% 96.76% {‘5’,‘8’} 86.92% 78.18% 91.47% 88.10% 70.69% 90.46% 63.56% 94.32% 91.59% {‘7’,‘8’} 94.46% 92.30% 95.85% 95.40% 75.97% 94.30% 94.46% 96.76% 96.16%\nTABLE II: Classification accuracy of different methods for different subjects in StarPlus fMRI datasets.\nSubject SVM STM STuM STTM DuSK 3D CNN TT classifier K-STTM-Prod K-STTM-Sum\n04799 50.00%∗ 36.67% 35.83% 39.61% 47.50% 51.67% 57.50% 68.33% 66.67% 04820 50.00%∗ 43.33% 35.00% 45.83% 46.67% 44.16% 54.17% 70.00% 62.50% 04847 50.00%∗ 38.33% 17.50% 47.50% 53.33% 55.00% 61.67% 65.00% 65.00% 05675 50.00%∗ 37.50% 30.83% 35.00% 55.00% 47.50% 55.00% 60.00% 60.00% 05680 50.00%∗ 38.33% 39.17% 40.00% 64.17% 68.33% 60.83% 73.33% 75.00% 05710 50.00%∗ 40.00% 30.00% 43.33% 54.16% 47.50% 53.33% 59.17% 58.33% ∗ SVM classifies all test samples into one class since no good hyperparameter setting can be found by grid search.\nFig. 5: Classification accuracy of K-STTM-Prod and K-STTM-Sum with different TT-rank on two randomly selected digit pairs. Top figure: digit pair ‘1’,‘2’; bottom figure: digit pair ‘5’,‘6’.\nstimulus (sentence or picture) during the whole period. The fMRI images were collected every 500 msec, thus we can utilize 8 fMRI images in each trial. Overall, we have 320 fMRI images: one half of them were collected when the subject was shown a picture, the other half were collected when the subject was shown a sentence, while we randomly select 140 images for training, 60 for validation and the left for testing.\nThe classification results are listed in Table II. Due to the very high-dimensional and sparse data, SVM fails to find a good hyperparameter setting thus can not do classification. Since fMRI data are very complicated, those linear classifiers, namely STM, STuM and STTM, can not achieve an acceptable performance, and the classification accuracies of them are all\nFig. 6: fMRI images from [18]. (a) An illustration of a 3-way tensor (fMRI image), (b) Visualization of an fMRI image.\nlower than 50%. The classification result of TT classifier is poor on several subjects. DuSK also performs poor on subjects ‘04799’ and ‘04820’. Due to the small number of training samples and high-dimensional data size, the 3D CNN overfits and can not be well trained, while our proposed two methods still achieve the highest classification accuracy on all human subjects.\n2) CMU2008: The second fMRI dataset we consider is CMU2008. It shows the brain activities associated with the meanings of nouns. During the data collection period, the subjects were asked to view 60 different word-picture from 12 semantic categories. There are 5 pictures in each categories and each images is shown to the subject for 6 times. Therefore, we can get 30 fMRI images for each semantic category, and each fMRI image is with dimensions 51 × 61 × 23. In this experiment, we consider all the ROIs thus the classified fMRI images are relatively denser than the images we classified in the StarPlus example. Considering the extremely small number of samples in each category, we therefore follow the experiment settings in [28] , which combines two similar categories into an integrated class. Specifically, we combine categories\n9\nanimal and insect as class Animals, and categories tool and furniture as class Tools. By doing so, we have 60 samples in both Animals and Tools classes. We separate the total 120 fMRI images as training, validation and testing sets, with 50, 20 and 50 images respectively.\nTable III shows the binary classification results of different models. We notice that SVM can perform classification on this dataset since we include all ROIs, which facilitates the hyperparameter searching procedure. However, its classification accuracies on four subjects are lower than 50%. The linear and polynomial model, namely STM, STuM, STTM, and TT classiifer, can only achieve an acceptable performance on a few subjects. Due to the high-dimensional data size, DuSK fails to find a good CP-rank in acceptable time and can not achieve a good classification accuracy. 3D CNN still performs poor due to the very few training samples and high-dimensional feature size. Our proposed two methods still achieve the best classification results on all subjects."
  }, {
    "heading": "C. CIFAR-10",
    "text": "In this experiment, we use the CIFAR-10 dataset [29] to investigate the fourth claim in Section IV-B, namely, we can perform different kernel functions on different tensor modes. Here we demonstrate the effect on K-STTM-Prod only. We also randomly select ten class pairs to do binary classification. Without overlap, 50 samples from the training set of each class are picked randomly for model training and validation respectively, while all the test samples of each class are used for testing. Since each color image is naturally a threeway tensor (pixel-pixel-color), and the first two tensor modes are related to pixel intensity, we therefore utilize the same Gaussian RBF kernel for the first two tensor modes and try a different kernel (linear or polynomial) for the third mode. The parameters c, d in the polynomial kernel k(x,y) = (xTy+c)d\nwere empirically set to c = 1 and d = 2. The baseline case is when the Gaussian RBF kernel is applied to all tensor modes.\nTable IV lists the classification results. We can observe that K-STTM-Prod still achieves the best accuracy on all class pairs. And by applying a linear or polynomial kernel on the color mode, the classification accuracy of K-STTMProd outperforms the baseline case (RBF-RBF-RBF) on nine class pairs, which indicates the potential benefit of employing different kernel functions on different tensor modes when they contain different kind of information. Since the data size of CIFAR-10 is also relatively small, we get a similar observation as the MNIST experiment, namely our method only achieves slightly better classification performance than SVM and 3D CNN on some class pairs. Due to the constrained rank-one model setting, STM can not achieve an acceptable performance. The other two linear classifiers, namely STuM and STTM still perform poor on most of the class pairs. The TT classifier has a similar performance as the STM in this experiment."
  }, {
    "heading": "VI. CONCLUSIONS AND FUTURE WORKS",
    "text": "This paper has proposed a tensor train (TT)-based kernel trick for the first time and devised a kernelized support tensor train machine (K-STTM). Assuming a low-rank TT as the prior structure of multi-dimensional data, we first define a corresponding feature mapping scheme that keeps the TT structure in the feature space. Furthermore, two kernel function construction schemes are proposed with consideration of consistency with the TT inner product and the preservation of information, respectively. The feasibility of applying different kernel mappings on the tensor modes with different characteristics is also investigated. Experiments have demonstrated the superiority of K-STTM over conventional approaches for tensorial data in few-sample size problems.\n10\nWe further envision two future research directions based on the K-STTM framework. Firstly, instead of constructing a kernel matrix in the K-STTM formula, we will consider building a kernel tensor. We believe that the kernel matrix constructed for each mode can contain different information. Simply multiplying or adding this information may not be the best solution. Subsequently, we propose to stack this information into a 3-way kernel tensor and develop a better way to exploit information in each of the modes. Secondly, we will embed the proposed kernel mapping trick into other kernel-based methods such as LSSVM [30], kernel PCA [31] etc., such that these methods can directly deal with tensorial data and achieve potentially better performance."
  }],
  "year": 2020,
  "references": [{
    "title": "Tensor embedding methods",
    "authors": ["G. Dai", "D.-Y. Yeung"],
    "venue": "AAAI, vol. 6, 2006, pp. 330–335.",
    "year": 2006
  }, {
    "title": "Mpca: Multilinear principal component analysis of tensor objects",
    "authors": ["H. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"],
    "venue": "IEEE transactions on Neural Networks, vol. 19, no. 1, pp. 18–39, 2008.",
    "year": 2008
  }, {
    "title": "A training algorithm for optimal margin classifiers",
    "authors": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"],
    "venue": "Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 144–152.",
    "year": 1992
  }, {
    "title": "Supervised tensor learning",
    "authors": ["D. Tao", "X. Li", "W. Hu", "S. Maybank", "X. Wu"],
    "venue": "Fifth IEEE International Conference on Data Mining (ICDM’05). IEEE, 2005, pp. 8–pp.",
    "year": 2005
  }, {
    "title": "Tensor-variate restricted boltzmann machines",
    "authors": ["T.D. Nguyen", "T. Tran", "D. Phung", "S. Venkatesh"],
    "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.",
    "year": 2015
  }, {
    "title": "Tensor decompositions for feature extraction and classification of high dimensional datasets",
    "authors": ["A.H. Phan", "A. Cichocki"],
    "venue": "Nonlinear theory and its applications, IEICE, vol. 1, no. 1, pp. 37–68, 2010.",
    "year": 2010
  }, {
    "title": "Tensor completion for estimating missing values in visual data",
    "authors": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 1, pp. 208–220, 2013.",
    "year": 2013
  }, {
    "title": "Tensor-based anomaly detection: An interdisciplinary survey",
    "authors": ["H. Fanaee-T", "J. Gama"],
    "venue": "Knowledge-Based Systems, vol. 98, pp. 130–147, 2016.",
    "year": 2016
  }, {
    "title": "Tensor learning for regression",
    "authors": ["W. Guo", "I. Kotsia", "I. Patras"],
    "venue": "IEEE Transactions on Image Processing, vol. 21, no. 2, pp. 816–827, 2012.",
    "year": 2012
  }, {
    "title": "Speeding-up convolutional neural networks using fine-tuned cpdecomposition",
    "authors": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"],
    "venue": "arXiv preprint arXiv:1412.6553, 2014.",
    "year": 2014
  }, {
    "title": "Tensorizing neural networks",
    "authors": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"],
    "venue": "Advances in Neural Information Processing Systems, 2015, pp. 442–450.",
    "year": 2015
  }, {
    "title": "Tensor-train decomposition",
    "authors": ["I.V. Oseledets"],
    "venue": "SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295–2317, 2011.",
    "year": 2011
  }, {
    "title": "A kernel-based framework to tensorial data analysis",
    "authors": ["M. Signoretto", "L. De Lathauwer", "J.A. Suykens"],
    "venue": "Neural networks, vol. 24, no. 8, pp. 861–874, 2011.",
    "year": 2011
  }, {
    "title": "Kernel-based tensor partial least squares for reconstruction of limb movements",
    "authors": ["Q. Zhao", "G. Zhou", "T. Adalı", "L. Zhang", "A. Cichocki"],
    "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 3577–3581.",
    "year": 2013
  }, {
    "title": "Higher rank support tensor machines for visual recognition",
    "authors": ["I. Kotsia", "W. Guo", "I. Patras"],
    "venue": "Pattern Recognition, vol. 45, no. 12, pp. 4192– 4203, 2012.",
    "year": 2012
  }, {
    "title": "Support tucker machines",
    "authors": ["I. Kotsia", "I. Patras"],
    "venue": "CVPR 2011. IEEE, 2011, pp. 633–640.",
    "year": 2011
  }, {
    "title": "A support tensor train machine",
    "authors": ["C. Chen", "K. Batselier", "C.Y. Ko", "N. Wong"],
    "venue": "CoRR, vol. abs/1804.06114, 2018. [Online]. Available: http://arxiv.org/abs/1804.06114",
    "year": 1804
  }, {
    "title": "Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages",
    "authors": ["L. He", "X. Kong", "P.S. Yu", "X. Yang", "A.B. Ragin", "Z. Hao"],
    "venue": "Proceedings of the 2014 SIAM International Conference on Data Mining. SIAM, 2014, pp. 127–135.",
    "year": 2014
  }, {
    "title": "Multi-way multi-level kernel modeling for neuroimaging classification",
    "authors": ["L. He", "C.-T. Lu", "H. Ding", "S. Wang", "L. Shen", "P.S. Yu", "A.B. Ragin"],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 356–364.",
    "year": 2017
  }, {
    "title": "Kernelized support tensor machines",
    "authors": ["L. He", "C.-T. Lu", "G. Ma", "S. Wang", "L. Shen", "P.S. Yu", "A.B. Ragin"],
    "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 1442–1451.",
    "year": 2017
  }, {
    "title": "A practical introduction to tensor networks: Matrix product states and projected entangled pair states",
    "authors": ["R. Orús"],
    "venue": "Annals of Physics, vol. 349, pp. 117–158, 2014.",
    "year": 2014
  }, {
    "title": "Tensor Learning in Multi-view Kernel PCA",
    "authors": ["L. Houthuys", "J.A.K. Suykens"],
    "venue": "27th International Conference on Artificial Neural Networks,",
    "year": 2018
  }, {
    "title": "Bemerkungen zur theorie der beschränkten bilinearformen mit unendlich vielen veränderlichen.",
    "authors": ["J. Schur"],
    "venue": "Journal für die reine und Angewandte Mathematik,",
    "year": 1911
  }, {
    "title": "Natural image bases to represent neuroimaging data",
    "authors": ["A. Gupta", "M. Ayhan", "A. Maida"],
    "venue": "International conference on machine learning, 2013, pp. 987–994.",
    "year": 2013
  }, {
    "title": "Parallelized tensor train learning of polynomial classifiers",
    "authors": ["Z. Chen", "K. Batselier", "J.A. Suykens", "N. Wong"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2017.",
    "year": 2017
  }, {
    "title": "MNIST handwritten digit database",
    "authors": ["Y. LeCun", "C. Cortes"],
    "venue": "2010. [Online]. Available: http://yann.lecun.com/exdb/mnist/",
    "year": 2010
  }, {
    "title": "Predicting human brain activity associated with the meanings of nouns",
    "authors": ["T.M. Mitchell", "S.V. Shinkareva", "A. Carlson", "K.-M. Chang", "V.L. Malave", "R.A. Mason", "M.A. Just"],
    "venue": "science, vol. 320, no. 5880, pp. 1191–1195, 2008.",
    "year": 2008
  }, {
    "title": "Sparse optimization in feature selection: application in neuroimaging",
    "authors": ["K. Kampa", "S. Mehta", "C.-A. Chou", "W.A. Chaovalitwongse", "T.J. Grabowski"],
    "venue": "Journal of Global Optimization, vol. 59, no. 2-3, pp. 439–457, 2014.",
    "year": 2014
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "venue": "Citeseer, Tech. Rep., 2009.",
    "year": 2009
  }, {
    "title": "Least squares support vector machine classifiers",
    "authors": ["J.A. Suykens", "J. Vandewalle"],
    "venue": "Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999.",
    "year": 1999
  }, {
    "title": "Nonlinear component analysis as a kernel eigenvalue problem",
    "authors": ["B. Schölkopf", "A. Smola", "K.-R. Müller"],
    "venue": "Neural computation, vol. 10, no. 5, pp. 1299–1319, 1998.",
    "year": 1998
  }],
  "id": "SP:0f8af08e554fdaaf64e3605cd3f50c7b601abd7e",
  "authors": [{
    "name": "Cong Chen",
    "affiliations": []
  }, {
    "name": "Kim Batselier",
    "affiliations": []
  }, {
    "name": "Wenjian Yu",
    "affiliations": []
  }],
  "abstractText": "Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vectoror matrixbased, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TTbased kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs.",
  "title": "Kernelized Support Tensor Train Machines"
}