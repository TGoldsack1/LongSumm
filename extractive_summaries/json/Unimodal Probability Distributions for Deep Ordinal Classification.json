{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Ordinal classification (sometimes called ordinal regression) is a prediction task in which the classes to be predicted are discrete and ordered in some fashion. This is different from discrete classification in which the classes are not ordered, and different from regression in that we typically do not know the distances between the classes (unlike regression, in which we know the distances because the predictions lie on the real number line). Some examples of ordinal classification tasks include predicting the stages of disease for a cancer (Gentry et al., 2015), predicting what star rating a user gave to a movie (Koren & Sill, 2011), or predicting the age of a person (Eidinger et al., 2014).\nTwo of the easiest techniques used to deal with ordinal problems include either treating the problem as a discrete classification and minimising the cross-entropy loss, or treating the problem as a regression and using the squared error loss. The former ignores the inherent ordering between the classes, while the latter takes into account the distances between them (due to the square in the error term) but assumes that the labels are actually real-valued – that is, adjacent classes are equally distant. Furthermore, the cross-entropy loss – under a one-hot target encoding – is formulated such that it only ‘cares’ about the ground truth class, and that probability estimates corresponding to the\n1Montréal Institute of Learning Algorithms, Québec, Canada. Correspondence to: Christopher Beckham <christopher.beckham@polymtl.ca>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nother classes may not necessarily make sense in context. We present an example of this in Figure 1, showing three probability distributions: A, B, and C, all conditioned on some input image. Highlighted in orange is the ground truth (i.e. the image is of an adult), and all probability distributions have identical cross-entropy: this is because the loss only takes into account the ground truth class, − log(p(y|x)c), where c = adult, and all three distributions have the same probability mass for the adult class.\nDespite all distributions having the same cross-entropy loss, some distributions are ‘better’ than others. For example, between A and B, A is preferred, since B puts an unusually high mass on the baby class. However, A and B are both unusual, because the probability mass does not gradually decrease to the left and right of the ground truth. In other words, it seems unusual to place more confidence on ‘schooler’ than ‘teen’ (distribution A) considering that a teenager looks more like an adult than a schooler, and it seems unusual to place more confidence on ’baby’ than ’teen’ considering that again, a teenager looks more like an adult than a baby. Distribution C makes the most sense because the probability mass gradually decreases as we move further away from the most confident class. In this paper, we propose a simple method to enforce this constraint, utilising the probability mass function of either the Poisson or binomial distribution.\nFor the remainder of this paper, we will refer to distributions like C as ‘unimodal’ distributions; that is, distributions where the probability mass gradually decreases on both sides of the class that has the majority of the mass."
  }, {
    "heading": "1.1. Related work",
    "text": "Our work is inspired by the recent work of Hou et al. (2016), who shed light on the issues associated with different probability distributions having the same cross-entropy loss for ordinal problems. In their work, they alleviate this issue by minimising the ‘Earth mover’s distance’, which is defined as the minimum cost needed to transform one probability distribution to another. Because this metric takes into account the distances between classes – moving probability mass to a far-away class incurs a large cost – the metric is appropriate to minimise for an ordinal problem. It turns out that in the case of an ordinal problem, the Earth\ndistribution A\ndistribution B\ndistribution C\nmover’s distance reduces down to Mallow’s distance: emd(ŷ,y) = ( 1 K ) 1 l ||cmf(ŷ)− cmf(y)||l, (1)\nwhere cmf(·) denotes the cumulative mass function for some probability distribution, y denotes the ground truth (one-hot encoded), ŷ the corresponding predicted probability distribution, and K the number of classes. The authors evaluate the EMD loss on two age estimation and one aesthetic estimation dataset and obtain state-of-the-art results. However, the authors do not show comparisons between the probability distributions learned between EMD and crossentropy.\nUnimodality has been explored for ordinal neural networks in da Costa et al. (2008). They explored the use of the binomial and Poisson distributions and a non-parametric way of enforcing unimodal probability distributions (which we do not explore). One key difference between their work and ours here is that we evaluate these unimodal distributions in the context of deep learning, where the datasets are generally much larger and have more variability; however, there are numerous other differences which we will highlight throughout this paper.\nBeckham & Pal (2016) explored a loss function with an intermediate form between a cross-entropy and regression loss. In their work the squared error loss is still used, but a probability distribution over classes is still learned. This is done by adding a regression layer (i.e. a one-unit layer) at the top of what would normally be the classification layer, p(y|x). Instead of learning the weight vector a it is fixed to [0, . . . ,K− 1]T and the squared error loss is minimised. This can be interpreted as drawing the class label from a Gaussian distribution p(c|x) = N(c;E[a]p(y|x), σ2). This technique was evaluated against the diabetic retinopathy dataset and beat most of the baselines employed. Interestingly, since p(c|x) is a Gaussian, this is also unimodal,\nthough it is a somewhat odd formulation as it assumes c is continuous when it is really discrete.\nCheng (2007) proposed the use of binary cross-entropy or squared error on an ordinal encoding scheme rather than the one-hot encoding which is commonly used in discrete classification. For example, if we haveK classes, then we have labels of length K − 1, where the first class is [0, . . . , 0], second class is [1, . . . , 0], third class is [1, 1, . . . , 0] and so forth. With this formulation, we can think of the i’th output unit as computing the cumulative probability p(y > i|x), where i ∈ {0, . . . ,K − 2}. Frank & Hall (2001) also proposed this scheme but in a more general sense by using multiple classifiers (not just neural networks) to model each cumulative probability, and Niu et al. (2016) proposed a similar scheme using CNNs for age estimation. This technique however suffers from the issue that the cumulative probabilities p(y > 0 | x), . . . , p(y > K − 2 | x) are not guaranteed to be monotonically decreasing, which means that if we compute the discrete probabilities p(y = 0 | x), . . . , p(y = K − 1 | x) these are not guaranteed to be strictly positive. To address the monotonicity issue, Schapire et al. (2002) proposed a heuristic solution.\nThere are other ordinal techniques but which do not impose unimodal constraints. The proportional odds model (POM) and its neural network extensions (POMNN, CHNN (Gutiérrez et al., 2014)) do not suffer from the monotonicity issue due to the utilization of monotonically increasing biases in the calculation of probabilities. The stick-breaking approach by Khan et al. (2012), which is a reformulation of the multinomial logit (softmax), could also be used in the ordinal case as it technically imposes an ordering on classes."
  }, {
    "heading": "1.2. Poisson distribution",
    "text": "The Poisson distribution is commonly used to model the probability of the number of events, k ∈ N ∪ 0 occurring in a particular interval of time. The average frequency of these events is denoted by λ ∈ R+. The probability mass function is defined as:\np(k;λ) = λk exp(−λ)\nk! , (2)\nwhere 0 ≤ k ≤ K − 1. While we are not actually using this distribution to model the occurrence of events, we can make use of its probability mass function (PMF) to enforce discrete unimodal probability distributions. For a purely technical reason, we instead deal with the log of the PMF:\nlog [λkexp(−λ)\nk!\n] = log(λkexp(−λ))− log(k!)\n= log(λk) + log(exp(−λ))− log(k!) = k log(λ)− λ− log(k!).\n(3)\nIf we let f(x) denote the scalar output of our deep net (where f(x) > 0 which can be enforced with the softplus nonlinearity), then we denote h(x)j to be:\nj log(f(x))− f(x)− log(j!), (4)\nwhere we have simply replaced the λ in equation (3) with f(x). Then, p(y = j|x) is simply a softmax over h(x):\np(y = j|x) = exp(−h(x)j/τ)∑K i=1 exp(−h(x)i/τ) , (5)\nwhich is required since the support of the Poisson is infinite. We have also introduced a hyperparameter to the softmax, τ , to control the relative magnitudes of each value of p(y = j|x) (i.e., the variance of the distribution). Note that as τ → ∞, the probability distribution becomes more uniform, and as τ → 0, the distribution becomes more ‘one-hot’ like with respect to the class with the largest presoftmax value. We can illustrate this technique in terms of the layers at the end of the deep network, which is shown in Figure 2.\nWe note that the term in equation (4) can be re-arranged and simplified to\nh(x)j = j log(f(x))− f(x)− log(j!) = −f(x) + j log(f(x))− log(j!) = −f(x) + bj(f(x)).\n(6)\nIn this form, we can see that the probability of class j is determined by the scalar term f(x) and a bias term that also depends on f(x). Another technique that uses biases to determine class probabilities is the proportional odds model\n(POM), also called the ordered logit (McCullagh, 1980), where the cumulative probability of a class depends on a learned bias:\np(y ≤ j | x) = sigm(f(x)− bj), (7)\nwhere b1 < · · · < bK . Unlike our technique however, the bias vector b is not a function of x nor f(x), but a fixed vector that is learned during training, which is interesting. Furthermore, probability distributions computed using this technique are not guaranteed to be unimodal.\nFigure 5 shows the resulting probability distributions for values of f(x) ∈ [0.1, 4.85] when τ = 1.0 and τ = 0.3. We can see that all distributions are unimodal and that by gradually increasing f(x) we gradually change which class has the most mass associated with itself. The τ is also an important parameter to tune as it alters the variance of the distribution. For example, in Figure 5(a), we can see that if we are confident in predicting the second class, f(x) should be ∼ 2.6, though in this case the other classes receive almost just as much probability mass. If we set τ = 0.3 however (Figure 5(b)), at f(x) = 2.6 the second class has relatively more mass, which is to say we are even more confident that this is the correct class. An unfortunate side effect of using the Poisson distribution is that the variance is equivalent to the mean, λ. This means that in the case of a large number of classes probability mass will be widely distributed, and this can be seen in the K = 8 case in Figure 6. While careful selection of τ can mitigate this, we also use this problem to motivate the use of the binomial distribution.\nIn the work of da Costa et al. (2008), they address the infinite support problem by using a ‘right-truncated’ Poisson distribution. In this formulation, they simply find the normalization constant such that the probabilities sum to one. This is almost equivalent to what we do, since we use a softmax, although the softmax exponentiates its inputs and we also introduce the temperature parameter τ to control for the variance of the distribution."
  }, {
    "heading": "1.3. Binomial distribution",
    "text": "The binomial distribution is used to model the probability of a given number of ‘successes’ out of a given number of trials and some success probability. The probability mass function for this distribution – for k successes (where 0 ≤ k ≤ K − 1), given K − 1 trials and success probability p – is:\np(k;K − 1, p) = ( K − 1 k ) pk(1− p)K−1−k (8)\nIn the context of applying this to a neural network, k denotes the class we wish to predict, K − 1 denotes the number of classes (minus one since we index from zero), and p = f(x) ∈ [0, 1] is the output of the network that we wish to estimate. While no normalisation is theoretically needed since the binomial distribution’s support is finite, we still had to take the log of the PMF and normalise with a softmax to address numeric stability issues. This means the resulting network is equivalent to that shown in Figure 2, but with the log binomial PMF instead of Poisson. Just like with the Poisson formulation, we can introduce the temperature term τ into the resulting softmax to control for the variance of the resulting distribution.\nFigure 8 shows the resulting distributions achieved by varying p for when K = 4 and K = 8."
  }, {
    "heading": "2. Methods and Results",
    "text": "In this section we go into details of our experiments, including the datasets used and the precise architectures."
  }, {
    "heading": "2.1. Data",
    "text": "We make use of two ordinal datasets appropriate for deep neural networks:\n• Diabetic retinopathy1. This is a dataset consisting of extremely high-resolution fundus image data. The training set consists of 17,563 pairs of images (where a pair consists of a left and right eye image corresponding to a patient). In this dataset, we try and predict from five levels of diabetic retinopathy: no DR (25,810 images), mild DR (2,443 images), moderate DR (5,292 images), severe DR (873 images), or proliferative DR (708 images). A validation set is set aside, consisting of 10% of the patients in the training set. The images are pre-processed using the technique proposed by competition winner Graham (2015) and subsequently resized to 256px width and height.\n• The Adience face dataset2 (Eidinger et al., 2014). This dataset consists of 26,580 faces belonging to 2,284\n1https://www.kaggle.com/c/diabetic-retinopathy-detection/ 2http://www.openu.ac.il/home/hassner/Adience/data.html\nsubjects. We use the form of the dataset where faces have been pre-cropped and aligned. We further preprocess the dataset so that the images are 256px in width and height. The training set consists of merging the first four cross-validation folds together (the last cross-validation fold is the test set), which comprises a total of 15,554 images. From this, 10% of the images are held out as part of a validation set."
  }, {
    "heading": "2.2. Network",
    "text": "We make use of a modest ResNet (He et al., 2015) architecture to conduct our experiments. Table 1 describes the exact architecture. We use the ReLU nonlinearity and HeNormal initialization throughout the network.\nWe conduct the following experiments for both DR and Adience datasets:\n• (Baseline) cross-entropy loss. This simply corresponds to a softmax layer for K classes at the end of the average pooling layer in Table 1. For Adience and DR, this corresponds to a network with 4,309,896 and 4,309,125 learnable parameters, respectively.\n• (Baseline) squared-error loss. Rather than regress f(x) against y, we regress with (K − 1)sigm(f(x)), since we have observed better results with this formulation in the past. For Adience and DR, this corresponds t 4,309,905 and 4,309,131 learnable parameters, respectively.\n• Cross-entropy loss using the Poisson and binomial extensions at the end of the architecture (see Figure 2). For Adience and DR, this corresponds to 4,308,097 learnable parameters for both. Although da Costa\net al. (2008) mention that cross-entropy or squared error can be used, their equations assume a squared error between the (one-hot encoded) ground truth and p(y|x), whereas we use cross-entropy.\n• EMD loss (equation 1) where ` = 2 (i.e. Euclidean norm) and the entire term is squared (to get rid of the square root induced by the norm) using Poisson and binomial extensions at the end of architecture. Again, this corresponds to 4,308,097 learnable parameters for both networks.\nAmongst these experiments, we use τ = 1 and also learn τ as a bias. When we learn τ , we instead learn sigm(τ) since we found this made training more stable. Note that we can also go one step further and learn τ as a function of x, though experiments did not show any significant gain over simply learning it as a bias. However, one advantage of this technique is that the network can quantify its uncertainty on a per-example basis. It is also worth noting that the Poisson and binomial formulations are slightly underparameterised compared to their baselines, but experiments we ran that addressed this (by matching model capacity) did not yield significantly different results.\nIt is also important to note that in the case of ordinal prediction, there are two ways to compute the final prediction: simply taking the argmax of p(y|x) (which is what is simply done in discrete classification), or taking a ‘smoothed’ prediction which is simply the expectation of the integer labels w.r.t. the probability distribution, i.e., E[0, . . . ,K − 1]p(y|x). For the latter, we call this the ‘expectation trick’. A benefit of the latter is that it computes a prediction that considers the probability mass of all classes. One benefit of the former however is that we can use it to easily rank our predictions, which can be important if we are interested in computing top-k accuracy (rather than top1).\nWe also introduce an ordinal evaluation metric – the quadratic weighted kappa (QWK) (Cohen, 1968) – which has seen recent use on ordinal competitions on Kaggle. Intuitively, this is a number between [-1,1], where a kappa κ = 0 denotes the model does no better than random chance, κ < 0 denotes worst than random chance, and κ > 0 better than random chance (with κ = 1 being the best score). The ‘quadratic’ part of the metric imposes a quadratic penalty on misclassifications, making it an appropriate metric to use for ordinal problems.3\nAll experiments utilise an `2 norm of 10−4, ADAM optimiser (Kingma & Ba, 2014) with initial learning rate 10−3, and batch size 128. A ‘manual’ learning rate schedule is\n3The quadratic penalty is arbitrary but somewhat appropriate for ordinal problems. One can plug in any cost matrix into the kappa calculation.\nemployed where we manually divide the learning rate by 10 when either the validation loss or valid set QWK plateaus (whichever plateaus last) down to a minimum of 10−4 for Adience and 10−5 for DR.4"
  }, {
    "heading": "2.3. Experiments",
    "text": "Figure 3 shows the experiments run for the Adience dataset, for when τ = 1.0 (Figure 3(a)) and when τ is learned (Figure 3(b)). We can see that for our methods, careful selection of τ is necessary for the accuracy on the validation set to be on par with that of the cross-entropy baseline. For τ = 1.0, accuracy is poor, but even less so when τ is learned. To some extent, using the smoothed prediction with the expectation trick alleviates this gap. However, because the dataset is ordinal, accuracy can be very misleading, so we should also consider the QWK. For both argmax and expectation, our methods either outperform or are quite competitive with the baselines, with the exception of the QWK argmax plot for when τ = 1, where only our binomial formulations were competitive with the cross-entropy baseline. Overall, considering all plots in Figure 3 it appears the binomial formulation produces better results than Poisson. There also appears to be some benefit gained from using the EMD loss for Poisson, but not for binomial.\nFigure 4 show the experiments run for diabetic retinopathy. We note that unlike Adience, the validation accuracy does not appear to be so affected across all specifications of τ . One potential reason for this is due to Adience having a larger number of classes compared to DR. As we mentioned earlier, the Poisson distribution is somewhat awkward as its variance is equivalent to its mean. Since most of the probability mass sits at the mean, if the mean of the distribution is very high (which is the case for datasets with a large K such as Adience), then the large variance can negatively impact the distribution by taking probability mass away from the correct class. We can see this effect by comparing the distributions in Figure 5 (k = 4) and Figure 6 (k = 8). As with the Adience dataset, the use of the expectation trick brings the accuracy of our methods to be almost on-par with the baselines. In terms of QWK, only our binomial formulations appear to be competitive, but only in the argmax case do one of our methods (the binomial formulation) beat the cross-entropy baseline. At least for accuracy, there appears to be some gain in using the EMD loss for the binomial formulation. Because DR is a much larger dataset compared to Adience, it is possible that the deep net is able to learn reasonable and ‘unimodal-like’ probability distributions without it being enforced in the model architecture.\n4We also re-ran experiments using an automatic heuristic to change the learning rate, and similar experimental results were obtained.\nOverall, across both datasets the QWK for our methods are generally at least competitive with the baselines, especially if we learn τ to control for the variance. In the empirical results of da Costa et al. (2008), they found that the binomial formulation performed better than the Poisson, and when we consider all of our results in Figure 3 and 4 we come to the same conclusion. They justify this result by defining the ‘flexibility’ of a discrete probability distribution and show that the binomial distribution is more ‘flexible’ than Poisson. From our results, we believe that these unimodal methods act as a form of regularization which can be useful in regimes where one is interested in top-k accuracy. For example, in the case of top-k accuracy, we want to know if the ground truth was in the top k predictions, and we may be interested in such metrics if it is difficult to achieve good top-1 accuracy. Assume that our probability distribution p(y|x) has most of its mass on the wrong class, but the correct class is on either side of it. Under a unimodal constraint, it is guaranteed that the two classes on either side of the majority class will receive the next greatest amount of probability mass, and this can result in a correct prediction if we consider top-2 or top-3 accuracy. To illustrate this,\nwe compute the top-k accuracy on the test set of the Adience dataset, shown in Figure 9. We can see that even with the worst-performing model – the Poisson formulation with τ = 1 (orange) – produces a better top-3 accuracy than the cross-entropy baseline (blue)."
  }, {
    "heading": "3. Conclusion",
    "text": "In conclusion, we present a simple technique to enforce unimodal ordinal probabilistic predictions through the use of the binomial and Poisson distributions. This is an important property to consider in ordinal classification because of the inherent ordering between classes. We evaluate our technique on two ordinal image datasets and obtain results competitive or superior to the cross-entropy baseline for both the quadratic weighted kappa (QWK) metric and topk accuracy for both cross-entropy and EMD losses, especially under the binomial distribution. Lastly, the unimodal constraint can makes the probability distributions behave more sensibly in certain settings. However, there may be ordinal problems where a multimodal distribution may be more appropriate. We leave an exploration of this issue for future work. Code will be made available here.5"
  }, {
    "heading": "4. Acknowledgements",
    "text": "We thank Samsung for funding this research. We would like to thank the contributors of Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) (which this project was developed in predominantly), as well as Keras (Chollet et al., 2015) for extra useful code. We thank the ICML reviewers for useful feedback, as well as Eibe Frank.\n5https://github.com/christopher-beckham/deep-unimodalordinal"
  }],
  "year": 2017,
  "references": [{
    "title": "A simple squared-error reformulation for ordinal classification",
    "authors": ["Beckham", "Christopher", "Pal"],
    "venue": "arXiv preprint arXiv:1612.00775,",
    "year": 2016
  }, {
    "title": "A neural network approach to ordinal regression",
    "authors": ["Cheng", "Jianlin"],
    "venue": "CoRR, abs/0704.1028,",
    "year": 2007
  }, {
    "title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
    "authors": ["Cohen", "Jacob"],
    "venue": "Psychological bulletin,",
    "year": 1968
  }, {
    "title": "The unimodal model for the classification of ordinal data",
    "authors": ["da Costa", "Joaquim F Pinto", "Alonso", "Hugo", "Cardoso", "Jaime S"],
    "venue": "Neural Networks,",
    "year": 2008
  }, {
    "title": "Age and gender estimation of unfiltered faces",
    "authors": ["Eidinger", "Eran", "Enbar", "Roee", "Hassner", "Tal"],
    "venue": "IEEE Transactions on Information Forensics and Security,",
    "year": 2014
  }, {
    "title": "A simple approach to ordinal classification",
    "authors": ["Frank", "Eibe", "Hall", "Mark"],
    "venue": "In European Conference on Machine Learning,",
    "year": 2001
  }, {
    "title": "Penalized ordinal regression methods for predicting stage of cancer in highdimensional covariate spaces",
    "authors": ["Gentry", "Amanda Elswick", "Jackson-Cook", "Colleen K", "Lyon", "Debra E", "Archer", "Kellie J"],
    "venue": "Cancer informatics,",
    "year": 2015
  }, {
    "title": "Kaggle diabetic retinopathy detection competition report, 2015. URL https: //kaggle2.blob.core.windows.net/ forum-message-attachments/88655/2795/ competitionreport.pdf",
    "authors": ["Graham", "Ben"],
    "year": 2015
  }, {
    "title": "Ordinal regression neural networks based on concentric hyperspheres",
    "authors": ["Gutiérrez", "Pedro Antonio", "Tiňo", "Peter", "HervásMartı́nez", "César"],
    "venue": "Neural Netw.,",
    "year": 2014
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"],
    "venue": "CoRR, abs/1512.03385,",
    "year": 2015
  }, {
    "title": "Squared earth mover’s distance-based loss for training deep neural networks",
    "authors": ["Hou", "Le", "Yu", "Chen-Ping", "Samaras", "Dimitris"],
    "venue": "CoRR, abs/1611.05916,",
    "year": 2016
  }, {
    "title": "A stick-breaking likelihood for categorical data analysis with latent gaussian models",
    "authors": ["Khan", "Mohammad E", "Mohamed", "Shakir", "Marlin", "Benjamin M", "Murphy", "Kevin P"],
    "venue": "In International conference on Artificial Intelligence and Statistics,",
    "year": 2012
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"],
    "venue": "CoRR, abs/1412.6980,",
    "year": 2014
  }, {
    "title": "Ordrec: an ordinal model for predicting personalized item rating distributions",
    "authors": ["Koren", "Yehuda", "Sill", "Joe"],
    "venue": "In Proceedings of the fifth ACM conference on Recommender systems,",
    "year": 2011
  }, {
    "title": "Regression models for ordinal data",
    "authors": ["McCullagh", "Peter"],
    "venue": "Journal of the royal statistical society. Series B (Methodological), pp",
    "year": 1980
  }, {
    "title": "Ordinal regression with multiple output cnn for age estimation",
    "authors": ["Niu", "Zhenxing", "Zhou", "Mo", "Wang", "Le", "Gao", "Xinbo", "Hua", "Gang"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Modeling auction price uncertainty using boosting-based conditional density estimation",
    "authors": ["Schapire", "Robert E", "Stone", "Peter", "McAllester", "David", "Littman", "Michael L", "Csirik", "János A"],
    "venue": "In ICML, pp",
    "year": 2002
  }],
  "id": "SP:732e8d8f5717f8802426e1b9debc18a8361c1782",
  "authors": [{
    "name": "Christopher Beckham",
    "affiliations": []
  }, {
    "name": "Christopher Pal",
    "affiliations": []
  }],
  "abstractText": "Probability distributions produced by the crossentropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal probability distributions to be unimodal via the use of the Poisson and binomial probability distributions. We evaluate this approach in the context of deep learning on two large ordinal image datasets, obtaining promising results.",
  "title": "Unimodal Probability Distributions for Deep Ordinal Classification"
}