{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The increasing availability of digitized historical corpora, together with newly developed tools of computational analysis, make the quantitative study of language change possible on a larger scale than ever before. Thus, many important questions may now be addressed using a variety of NLP tools that were originally developed to study synchronic similarities between words. This has catalyzed the evolution of an exciting new field\nof historical distributional semantics, which has yielded findings that inform our understanding of the dynamic structure of language (Sagi et al., 2009; Wijaya and Yeniterzi, 2011; Mitra et al., 2014; Hilpert and Perek, 2015; Frermann and Lapata, 2016; Dubossarsky et al., 2016). Recent research has even proposed laws of change that predict the conditions under which the meaning of words is likely to change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016). This is an important development, as traditional historical linguistics has generally been unable to provide predictive models of semantic change.\nHowever, these preliminary results should be addressed with caution. To date, analyses of changes in words’ meanings have relied on the comparison of word representations at different points in time. Thus any proposed change in meaning is contingent on a particular model of word representation and the method used to measure change. Distributional semantic models typically count words and their co-occurrence statistics (explicit models) or predict the embedding contexts of words (implicit models). In this paper, we show that the choice of model may introduce biases into the analysis. We therefore suggest that empirical findings may be used to support laws of semantic change only after a proper control can be shown to eliminate artefactual factors as the underlying cause of the empirical observations.\nRegardless of the specific representation used, a frequent method of measuring the semantic change a word has undergone (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014; Dubossarsky et al., 2015; Kulkarni et al., 2015; Hamilton et al., 2016) is to compare the word’s vector representations between two points in time using the cosine distance:\ncosDist(x, y) = 1− x · y‖x‖2‖y‖2 (1)\n1136\nThis choice naturally assumes that greater distances correspond to greater semantic changes. However, this measure introduces biases that may affect our interpretation of meaning change.\nWe examine various representations of word meaning, in order to identify inherent confounds when meaning change is evaluated using the cosine distance. In addition to the empirical evaluation, in Section 5 we provide an analytical account of the influence of word frequency on cosine distance scores when using these representations.\nIn our empirical investigation, we highlight the critical role of control conditions in the validation of experimental findings. Specifically, we argue that every observation about a change of meaning over time should be subjected to a control test. The control condition described in Section 2.1 is based on the construction of an artificially generated corpus, which resembles the historical corpus in most respects but where no change of meaning over time exists. In order to establish the validity of an observation about meaning change - and even more importantly, the validity of a lawlike generalization about meaning change - the result obtained in a genuine experimental condition should be demonstrated to be lacking (or at least significantly diminished) in the control condition.\nAs we show in Section 4, some recently reported laws of historical meaning change do not survive this proposed test. In other words, similar results are obtained in the genuine and control conditions. These include the correlation of meaning change with word frequency, polysemy (the number of different meanings a word has), and prototypicality (how representative a word is of its category). These factors lie at the basis of the following proposed laws of semantic change:\n• The Law of Conformity, according to which frequency is negatively correlated with semantic change (Hamilton et al., 2016).\n• The Law of Innovation, according to which polysemy is positively correlated with semantic change (Hamilton et al., 2016).\n• The Law of Prototypicality, according to which prototypicality is negatively correlated with semantic change (Dubossarsky et al., 2015).\nOur analysis shows that these laws have only residual effects, suggesting that frequency and\nprototypicality may play a smaller role in semantic change than previously claimed. The main artefact underlying the emergence of the first two laws in both the genuine and control conditions may be due to the SVD step used for the embedding of the PPMI word representation (see Section 2.5)."
  }, {
    "heading": "2 Methods",
    "text": "The historical corpus used here is Google Books 5-grams of English fiction. Equally sized samples of 10 million 5-grams per year were randomly sampled for the period of 1900-1999 (Kim et al., 2014) to prevent the more prolific publication years from biasing the results, and were grouped into ten-year bins. Uncommon words were removed, keeping the 100,000 most frequent words as the vocabulary for subsequent model learning. All words were lowercased and stripped of punctuation.\nThis corpus served as the genuine condition, and was used to replicate and evaluate findings from previous studies. In this corpus, words are expected to change their meaning between decadal bins, as they do in a truly random sample of texts. According to the distributional hypothesis (Firth, 1957), one can extract a word’s meaning from the contexts in which it appears. Therefore, if words’ meanings change over time, as has been argued at least since Reisig (1839), it follows that the words’ contexts should change accordingly, and this change should be detected by our model."
  }, {
    "heading": "2.1 Control condition setup",
    "text": "Complementary to the genuine condition, a control condition was created where no change of meaning is expected. Therefore, any observed change in a word’s meaning in the control condition can only stem from random “noise“, while changes in meaning in the genuine condition are attributed to “real“ semantic change in addition to “noise“. Two methods were used to construct the corpus in the control condition:\nChronologically shuffled corpus (shuffle): 5- grams were randomly shuffled between decadal bins, so that each bin contained 5-grams from all the decades evenly. This was chosen as a control condition for two reasons. First, this condition resembles the genuine condition in size of the vocabulary, size of the corpus, overall variance in words’ usage, and size of the decadal bins. Second and\ncrucially, words are not expected to show any apparent change in their meaning between decades in the control condition, because their various usage contexts are shuffled across decades.\nOne synchronous corpus (subsample): All 5- grams of the year 1999, which amount to 250 million 5-grams, were selected from Google Books English fiction. 10 million 5-grams were randomly subsampled from this selection, and this process was repeated 30 times. This is suggested as an additional control condition since the underlying assumption is always that words in the same year do not change their meaning. Again, unlike in the genuine condition, any changes that are observed based on these 30 subsamples can be attributed only to ”noise” that stems from random sampling, rather than real change in meaning."
  }, {
    "heading": "2.2 Measures of interest",
    "text": "Meaning change: Meaning change was evaluated as the cosine distance between vector representations of the same word in consecutive decades. This was done separately for each processing stage (see Section 2.5). For the subsample condition, this was defined as the average cosine distance between the vectors in all 30 samples.\nFrequency: Words’ frequencies were computed separately for each decadal bin as the number of times a word appeared divided by the total number of words in that decade. For the subsample control condition, it was computed as the number of times a word appeared among the 250 million 5-grams, divided by the total number of words."
  }, {
    "heading": "2.3 Construct validity",
    "text": "To establish the adequacy of our control condition, we compared the meaning change scores (before log-transformation and standardization) between the genuine and the shuffled control conditions. Change scores were obtained by taking the average meaning change over all words in each decade using the representation of the final processing stage (SVD). An adequate control condition will exhibit a lower degree of change compared to the genuine condition, and is expected to show a fixed rate of change across decades (see 3a)."
  }, {
    "heading": "2.4 Statistical analysis",
    "text": "Following common practice (Hamilton et al., 2016), the 10k most frequent words, as measured by their average decadal bin frequencies, were\nused for the analysis of semantic change. Change scores and frequencies were log-transformed, and all variables were subsequently standardized.\nA linear mixed effects model was used to evaluate meaning change in both the genuine and shuffled control conditions. Frequency was set as a fixed effect while random intercepts were set per word. The model attempts to account for semantic change scores using frequency, while controlling for the variability between words by assuming that each word’s behavior is strongly correlated across decades and independent across words as follows:\n∆w(t)i = β0 + βffreq (t) wi + zwi + ε (t) wi (2)\nHere ∆w(t)i is the semantic change score of the i’th word measured between two specific consecutive decades, β0 is the model’s intercept, βf is the fixed-effect predictor coefficient for frequency, zwi ∼ N(0, σ) is a random intercept for the i’th word, and ε(t)wi is an error term associated with the i’th word. We report the predictor coefficient as well as the proportion of variance explained1 by each model. Only statistically significant results (p < .01) are reported. All statistical tests are performed in R (lme4 and MuMln packages)."
  }, {
    "heading": "2.5 Word meaning representation",
    "text": "We used a cascade of processing stages based on the explicit meaning representation of words (i.e., word counts, PPMI, SVD, as explained below) as commonly practiced (Baroni et al., 2014; Levy et al., 2015). For each of these stages, we sought to evaluate the relationship between word frequency and meaning change, by computing the corresponding correlations between these two factors in the subsample control condition.\nCounts: Co-occurrence counts were collected for all the words in the vocabulary per decade.\nPPMI: Sparse square matrices of vocabulary size containing positive pointwise mutual information (PPMI) scores were constructed for each decade based on the co-occurrence counts. We used the context distribution smoothing parameter α = 0.75, as recommended by (Levy et al., 2015), using the following procedure:\nPPMIα(w, c) = max ( log ( P̂ (w, c)\nP̂ (w)P̂α(c)\n) , 0 ) 1R2 for mixed linear models (Nakagawa and Schielzeth,\n2013)\nwhere P̂ (w, c) denotes the probability that word c appears as a context word of w, while P̂ (w) and P̂α(c) =\n#(c)α∑ C #(c)\nα denote the marginal probabilities of the word and its context, respectively.\nSVD: Each PPMI matrix was approximated by a truncated singular value decomposition as described in (Levy et al., 2015). This embedding was shown to improve results on downstream tasks (Baroni et al., 2014; Bullinaria and Levy, 2012; Turney and Pantel, 2010). Specifically, the top 300 elements of the diagonal matrix of singular values Σ, denoted Σd, were retained to represent a new, dense embedding of the word vectors, using the truncated left hand orthonormal matrix Ud:\nWSV Di = (Ud · Σd)i (3) These representations were subsequently aligned with the orthogonal Procrustes method following (Hamilton et al., 2016).\nRelation to other models: (Levy and Goldberg) have shown that the Skip-Gram with Negative Sampling (SGNS) embedding model, e.g. word2vec (Mikolov et al., 2013) - perhaps the most popular model of word meaning representation, implicitly factorizes the values of the wordcontext PMI matrix. Hence, the optimization goal and the sources of information available to SGNS and our model are in fact very similar. We therefore hypothesize that conclusions similar to those reported below can be drawn for SGNS models."
  }, {
    "heading": "3 Results",
    "text": ""
  }, {
    "heading": "3.1 Confound of frequency",
    "text": "There are many factors that may confound the measurement of meaning change. Here we focus\non frequency, and investigate the existence of an artefactual relation between frequency and meaning change. This is done by evaluating this relation in the subsample control condition. Any changes observed in this condition must be the consequence of inherent noise, since this control condition contains random samples from the same year (and the baseline assumption is that no change can be observed within the same year).\nWe first plotted the change scores that use the representation based on word count vs. word frequency. This resulted in a robust correlation (r = −0.915) between the two variables, as shown in Fig. 1a (see the analytical account in Section 5). We repeated the same procedure using the PPMI representation, which showed a much weaker correlation with frequency (r = −0.295), see Fig. 1b.\nFinally, we repeated the same procedure using the final explicit representation after SVD embedding2, see Fig. 1c. Surprisingly, the negative correlation with frequency was reinstated (r = −0.793). To investigate how this came about,\n2Similar results were obtained for the implicit embedding (word2vec-SGNS) described in Section 2.5.\nwe computed the change in the PPMI vectors before and after the low-rank SVD embedding using the cosine-distance. As apparent from Fig. 2, it turns out that the SVD procedure distorts data in an uneven manner - frequent words are distorted less than infrequent words. Thus we demonstrate that this reinstatement of correlation between frequency and change scores is merely an artefactual consequence of the truncated SVD factorization."
  }, {
    "heading": "3.2 Construct validity",
    "text": "Potential confounding factors can be addressed by comparing any experimental finding to a validated control condition. Here we validate the use of the shuffled condition as a proper control. To this end, the average change scores of words per decade in both the genuine and shuffled conditions are compared within each processing stage. In the genuine condition, words appear in different usage contexts between decades, while in the shuffled condition they do not, because the random shuffling creates a homogeneous corpus. Therefore, the validity of the control condition is established if: (a) the change scores are diminished as compared to the genuine condition; (b) change scores are uniform across decades (since decades are shuffled); (c) the variance of change scores is smaller that in the genuine condition. As seen in Fig. 3a, all these requirements are met by the control condition. Note that the change scores in the shuffled condition are all significantly positive, namely, meaning change allegedly exists in this control condition. This supports the claim that any measurement is significantly affected by unrelated noise.\nThus, we have established that the shuffled condition is a suitable control for meaning change.\nWhile validity was established for each of the processing stages, the most robust effect was seen for the PPMI representation, following by SVD and word counts."
  }, {
    "heading": "3.3 Accounting for the frequency confound",
    "text": "In Section 3.1 we used the subsample control condition to establish the confounding effect of frequency on meaning change. We now examine the extent to which this frequency confound exists in a historical corpus. We do so by comparing the frequency confound between the genuine historical corpus and the shuffled historical corpus.\nTo visualize the frequency confound in a manner comparable to the analysis presented in Section 3.1, we again plot change scores vs. frequency, ignoring the time dimension of the data. Fig. 3b presents this plot for the genuine condition. The same analysis is repeated in the shuffled condition, see Fig. 3c.\nBoth plots reveal a highly significant correlation between change scores and frequency. Furthermore, the fact that the correlation coefficients are virtually identical in the genuine and shuffled conditions, with r = −0.748 and r = −0.747 respectively, suggests that they are due to artefactual factors in both conditions and not to true change of meaning over time. In fact, this pattern of results is reminiscent of the spurious pattern we see in Fig. 1c.\nThe relation between frequency and meaning change can also be represented by a linear mixed effect model, with the benefit that this model enables the addition of more explanatory variables to the data. The regression model found frequency to have a negative influence on change scores,\nwith βf=-0.91 and βf=-0.75, for the genuine and shuffled conditions respectively. Importantly, frequency accounted for 67% of the variance in the change scores in the genuine condition, and was only slightly diminished in the shuffled condition, accounting for 56% of the variance. Similar results were obtained for the PPMI representation (see Table 1)."
  }, {
    "heading": "4 Revisiting previous studies",
    "text": "We replicated three recent results that were affected by this frequency effect, since they all define change as the word’s cosine distance relative to itself at two time points. These studies report laws of semantic change that measure the role of frequency in semantic change either directly (Law of Conformity), or indirectly through another linguistic variable that is dependent on frequency (Laws of Innovation and Prototypicality)."
  }, {
    "heading": "4.1 Laws of conformity and innovation",
    "text": "Continuing the work described in Section 3.1, we replicated the model and analysis procedure described in (Hamilton et al., 2016), where two predictors were used together to explain the change scores: frequency and polysemy. Polysemy, which describes the number of different senses a word has, naturally differs among words, where some words are more polysemous than others (compare bank and date to wine). Following (Hamilton et al., 2016), we defined polysemy as the words’ secondary connections patterns - the connections between each word’s co-occurring words (using the entries in the PPMI representation for that word). The more interconnected these secondary connections are, the less polysemic a word is, and vice versa. Polysemy scores were com-\nputed using the authors’ provided code3. We then log-transformed and standardized the polysemy scores. Next, frequency and polysemy were set as two fixed effect predictors in a linear mixed effect model, like the one described in Section 2.4.\nThus we were able to replicate the results in the genuine condition as reported in (Hamilton et al., 2016). Interestingly, the same pattern of results emerged, again, in the shuffled condition (see Table 1). Importantly, the difference in effect size between conditions, as evaluated by the explained variance of frequency and polysemy together, showed a modest effect of 8% over the shuffled condition, pointing to the conclusion that the putative effects may indeed be real, but to a far lesser extent than had been claimed. We conclude that adding polysemy to the analysis contributed very little to the model’s predictive power.\nSince the PPMI representation (the explicit representation without dimensionality reduction with SVD) seems much less affected by spurious effects correlated with frequency (see Fig. 1b), we repeated the analysis of frequency described here and in Section 3.1 while using this representation. The results are listed in Table 1, showing a similar pattern of rather small frequency effect."
  }, {
    "heading": "4.2 Prototypicality",
    "text": "Prototypicality is the degree to which a word is representative of the category of which it is a member (a robin is a more prototypical bird than a parrot). According to the proposed Law of Prototypicality, words with more prototypical meanings will show less semantic change, and vice versa. Following (Dubossarsky et al., 2015), we computed words’ prototypicality scores for each decade as the cos-distance between a word’s vec-\n3https://github.com/williamleif/histwords\ntor and its k-means cluster’s centroid, and extended the analysis to encompass the entire 20th century. The previous regression model assumed independence between words, and therefore assigned words to a random effect variable. However, when modeling prototypicality, this assumption is invalid as relations between words are what inherently define prototypicality. We therefore designed a model in which decades, rather than words, are the random effect variable.\nWith this analysis the prototypicality effect seems to be substantiated in two ways. First, the addition of prototypicality explains an additional 5% of the variance. Second, the effect of prototypicality meets the more stringent requirement of being diminished in the shuffle condition (see Table 1). Nevertheless, here too the effect originally reported was found to be drastically reduced after being compared with the proper control."
  }, {
    "heading": "5 Theoretical analysis",
    "text": "We show in Section 5.1 that the average cosine distance between two vectors representing the same word is equivalent to the variance of the population of vectors representing the same word in independent samples, and is therefore always positive. This is true for any word vector representation.\nIn Sections 5.2-5.3 we prove that the average cosines distance between two count vectors representing the same word is negatively correlated with the frequency of the word, and positively correlated with the polysemy score of the word."
  }, {
    "heading": "5.1 Sampling variability and the cos distance",
    "text": "Lemma 1. Assume two random variables x, y of length ‖x‖2 = ‖y‖2 = 1, distributed iid with expected value µ and covariance matrix Σ. The expected value of the cosine distance between them is equal to the sum of the diagonal elements of Σ.\nProof.\nE(x− y)2 =E(x− µ)2 + E(y − µ)2+ 2E(x− µ)(y − µ)\n=2 ∑ E(xi − µi)2 = 2 ∑ V ar(xi)\nE(x− y)2 =E(x2) + E(y2)− 2E(x · y) =2− 2E (\nx · y ‖x‖2‖y‖2 ) =2E(cosDist(x, y))\nIt follows that E(cosDist(x, y)) = ∑ V ar(xi) (4)\nImplication: The average cosine distance between two samples of the same random variables is directly related to the variance of the variable, or the sampling noise. This variance should be measured empirically whenever cosine distance is used, since only distances that are larger than the empirical variance can be relied upon to support significant observations."
  }, {
    "heading": "5.2 Cos distance of count vectors: frequency",
    "text": "Next, we analyze the cosine distance between 2 iid samples from a normalized multinomial random variable. This distribution models the distribution of the count vector representation. Let ki, 1 ≤ i ≤ m denote the number of times word i appeared in the context of word w, and let m denote the size of the dictionary not including w. Let n = ∑ ki denote the number of words in the count vector of w; n determines the word’s frequency score. Assume that the counts are sampled from the distribution Multinomial(n, ~p), namely\nProb(k1, · · · , km) = (\nn\nk1 · · · , km\n) pk11 · · · pkmm\nLemma 2. The expected value of the cosine distance between two count vectors x, y sampled iid from this distribution is monotonically decreasing with n.\nProof. By definition, 1−E[cosDist(x, y)] equals\nE\n[ x · y\n‖x‖2‖y‖2 ] = ∑ i [ E xi ‖x‖2 ]2 = ∑ i E2i (5)\nWe compute the expected value of Ei directly:\nEi = ∑\n(k1,··· ,km) ki√∑ j k 2 j\n( n\nk1 · · · , km\n) pk11 · · · pkmm\nUsing Taylor expansion:\nki√∑ j k 2 j = ki n√ ( ∑ j kj n ) 2 −∑l 6=j kjkln2 = ki n\n1√ 1−∑l 6=j kjkln2\n= ki n\n( 1 + ε\n2 +O(ε2)\n) (6)\nwhere ε = ∑\nl 6=j kjkl n2\n. The expected value of the 0-order term with respect to ε in (6) equals pi, which is independent of n. We conclude the proof by focusing on the first order term with respect to ε in (6), to be denoted f1, showing that its expected value is monotonically decreasing with n. Specifically:\nf1 = ∑ ~k ∑ l 6=j ki n kj n kl n ( n k1 · · · , km ) pk11 · · · pkmm\nWe switch the summation order and compute each expression in the external sum, considering two cases separately: when l 6= j 6= i∑\n(k1,··· ,km)\nki n kj n kl n\n( n\nk1 · · · , km\n) pk11 · · · pkmm\n= n(n− 1)(n− 2)\nn3 pipjpl\nWhen l 6= j = i w.l.g, we rewrite kikj = ki(ki − 1) + ki, and the sum above becomes n(n−1)(n−2)\nn3 p2i pl + n(n−1) n2 pipl. Thus\nf1 = n− 1 n pi n− 2 n ∑ l,j:l 6=j pjpl + (1− pi) \nand it readily follows that f1 is monotonically increasing with n.\nSince n measures the frequency score of word w, it follows from (5) that the expected value of the cosine distance between two iid samples from the distribution of the count vector of w is monotonically decreasing with the word’s frequency."
  }, {
    "heading": "5.3 Cos distance of count vectors: polysemy",
    "text": "We start our investigation of polysemy by modeling the distribution of the parameters of the multinomial distribution from which count vectors are sampled. A common prior distribution on the vector ~pw in m-simplex, which defines the multinomial distribution generating the context of word w, is the Dirichlet distribution f(~pw; ~αw) = f(p1, · · · , pm;α1, · · · , αm). ~αw is a sparse vector of prior counts on all the words in the dictionary, by which the cooccurrence context of word w is modeled. We divide the set of none-zero indices of ~αw into two subsets: i1, · · · , im0 correspond to the words which always appear in the context of w, while j1, · · · , im1 correspond to the words which appear in the context of w in one given meaning. If w is\npolysemous and has two meanings, then there is a third set of indices k1, · · · , km2 which correspond to the words appearing in the context of w in its second meaning. If w has more then two meanings, they can be modeled with additional sets of disjoint indices.\nLemma 3. Under certain conditions specified in the proof, given two count vectors x, y sampled iid from the above distribution of w, the expected value of the cosine distance between them increases with the number of sets of disjoint indices which represent different meanings of w.\nProof. We will prove that when w has two meanings, the expected value of the cosine distance is larger than in the case of a single meaning. The proof for the general case immediately follows.\nStarting from (6) while keeping only the 0-order term in ε, it follows from the derivations in the proof of Lemma 2 that the expected cosine distance between two count vector samples of w, to be denotedM , is 1−∑ p2i . In our current model ~p is a random variable, and we shall compute the expected value of this random variable under the two conditions, when w has either one or two meanings.\nWe start by observing that, given the definition of the Dirichlet distribution, it follows that\nE(p2i ) =V ar(pi) + E(pi) 2 = αi(1 + αi) α0(1 + α0)\nαo = ∑ αi\n=⇒M = ∑ E(p2i ) = α0 + ∑ α2i\nα0(1 + α0) (7)\nConsidering the different sets of indices in isolation, let ϕo = ∑im0 i=i1 αi, ϕ1 = ∑jm1 i=j1 αi, and\nϕ2 = ∑km2\ni=k1 αi. Let ψo = ∑im0 i=i1\nα2i , ψ1 =∑jm1 i=j1 α2i , and ψ2 = ∑km2 i=k1 α2i .\nWe rewrite (7) for the two conditions:\n1. w has one meaning:\nM (1) = ϕ0 + ϕ1 + ψ0 + ψ1\n(ϕ0 + ϕ1)(1 + ϕ0 + ϕ1)\n2. w has two meanings:\nM (2) = ϕ0 + ϕ1 + ϕ2 + ψ0 + ψ1 + ψ2\n(ϕ0 + ϕ1 + ϕ2)(1 + ϕ0 + ϕ1 + ϕ2)\nWith some algebraic manipulations, it can be shown that M (1) > M (2) if the following holds:\n(ϕ0 + ϕ1)2ϕ2 + (ψ0 + ψ1)ϕ22 (8) +2(ψ0 + ψ1)(ϕ0 + ϕ1)ϕ2 + (ψ0 + ψ1)ϕ2 +(ϕ0 + ϕ1)(ϕ22 − ψ2) > ψ2(ϕ0 + ϕ1)2\nThus when (8) holds, the average cosine distance between two samples of a certain word w gets larger as w acquires more meanings.\n(8) readily holds under reasonable conditions, e.g., when the prior counts for each meaning are similar (as a set) and much bigger than the prior counts of the joint context words (i.e., ϕ0 = ψ0 = ε, ϕ1 = ϕ2, ψ1 = ψ2)."
  }, {
    "heading": "6 Conclusions and discussion",
    "text": "In this article we have shown that some reported laws of semantic change are largely spurious results of the word representation models on which they are based. While identifying such laws is probably within the reach of NLP analyses of massive digital corpora, we argued that a more stringent standard of proof is necessary in order to put them on a firm footing. Specifically, it is necessary to demonstrate that any proposed law of change has to be observable in the genuine condition, but to be diminished or absent in a control condition. We replicated previous studies claiming to establish such laws, which propose that semantic change is negatively correlated with frequency and prototypicality, and positively correlated with polysemy. None of these laws - at least in their strong versions - survived the more stringent standard of proof, since the observed correlations were found in the control conditions.\nIn our analysis, the Law of Conformity, which claims a negative correlation between word frequency and meaning change, was shown to have a much smaller effect size than previously claimed. This indicates that word frequency probably does play a role - but a small one - in semantic change. According to the Law of Innovation, polysemy was claimed to correlate positively with meaning change. However, our analysis showed that polysemy is highly collinear with frequency, and as such, did not demonstrate independent contribution to semantic change. For similar reasons, the alleged role of prototypicality was diminished.\nThese results may be more consonant than previous ones with the findings of historical linguis-\ntics, as it is commonly assumed that the factors leading to semantic change are more diverse than purely distributional factors. For example, sociocultural, political, and technological changes are known to impact semantic change (Bochkarev et al., 2014; Newman, 2015). Furthermore, some regularities of semantic change have been imputed to ‘channel bias‘, inherent biases of utterance production and interpretation on the part of speakers and listeners, e.g., (Moreton, 2008). As such, it would be surprising if word frequency, polysemy, and prototypicality were to capture too high a degree of variance. In other words, since semantic change may result from the interaction of many factors, small effects may be a priori more credible than large ones.\nThe results of our empirical analysis showed that the spurious effects of frequency were much weaker for the explicit PPMI representation unaugmented by SVD dimensionality reduction. We therefore conclude that the artefactual frequency effects reported are inherent to the type of word representations upon which these analyses are based. As the analytical proof in Section 5 demonstrates, it is count vectors that introduce an artefactual dependence on word frequency.\nIntuitively, one might expect that the average value for the cosine distance between a given word’s vector in any two samples would be 0. However, Lemma 1 above shows that this is not the case, and the average distance is the variance of the population of vectors representing the same word. This result is independent of the specific method used to represent words as vectors. Lemma 2 proves that the average cosine distance between two samples of the same word, when using count vector representations, is negatively correlated with the word’s frequency. Thus, the role of frequency cannot be evaluated as an independent predictor in any model based on count vector representations. It remains for future research to establish whether other approaches to word representation, e.g. (Blei et al., 2003; Mikolov et al., 2013), have inherent biases.\nWhile our findings may seem to be mainly negative, since they invalidate proposed laws of semantic change, we would like to point to the positive contribution made by articulating more stringent standards of proof and devising replicable control conditions for future research on language change based on distributional semantics representations."
  }],
  "year": 2017,
  "references": [{
    "title": "Don’t count, predict! A systematic comparison of context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "context-predicting semantic vectors. In Proceedings of ACL, pages 238–247.",
    "year": 2014
  }, {
    "title": "Latent Dirichlet Allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research, 3(4-5):993–1022.",
    "year": 2003
  }, {
    "title": "Universals versus historical contingencies in lexical evolution",
    "authors": ["Vladimir Bochkarev", "Valery Solovyev", "Sören Wichmann."],
    "venue": "Journal of The Royal Society Interface, 11:1–23.",
    "year": 2014
  }, {
    "title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd",
    "authors": ["John A Bullinaria", "Joseph P Levy."],
    "venue": "Behavior research methods, 44(3):890–907.",
    "year": 2012
  }, {
    "title": "A bottom up approach to category mapping and meaning change",
    "authors": ["Haim Dubossarsky", "Yulia Tsvetkov", "Chris Dyer", "Eitan Grossman."],
    "venue": "NetWordS 2015 Word Knowledge and Word Usage, pages 66–70.",
    "year": 2015
  }, {
    "title": "Verbs change more than nouns: A bottom up computational approach to semantic change",
    "authors": ["Haim Dubossarsky", "Daphna Weinshall", "Eitan Grossman."],
    "venue": "Lingue e Linguaggio, 1:5–25.",
    "year": 2016
  }, {
    "title": "Papers in Linguistics 1934– 1951",
    "authors": ["John Rupert Firth."],
    "venue": "Oxford University Press, London.",
    "year": 1957
  }, {
    "title": "A Bayesian model of diachronic meaning change",
    "authors": ["Lea Frermann", "Mirella Lapata."],
    "venue": "TACL, 4:31–",
    "year": 2016
  }, {
    "title": "A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus",
    "authors": ["Kristina Gulordava", "Marco Baroni."],
    "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,",
    "year": 2011
  }, {
    "title": "Diachronic word embeddings reveal statistical laws of semantic change",
    "authors": ["William L Hamilton", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "Proceedings of ACL.",
    "year": 2016
  }, {
    "title": "Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts",
    "authors": ["Martin Hilpert", "Florent Perek."],
    "venue": "Linguistics Vanguard, 1(1):339–350.",
    "year": 2015
  }, {
    "title": "A framework for analyzing semantic change of words across time",
    "authors": ["Adam Jatowt", "Kevin Duh."],
    "venue": "Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries, pages 229–238.",
    "year": 2014
  }, {
    "title": "Temporal analysis of language through neural language models",
    "authors": ["Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov."],
    "venue": "Proceedings of ACL, pages 61–65.",
    "year": 2014
  }, {
    "title": "Statistically significant detection of linguistic change",
    "authors": ["Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."],
    "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 625–635.",
    "year": 2015
  }, {
    "title": "Improving distributional similarity with lessons learned from word embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."],
    "venue": "TACL, 3:211–225.",
    "year": 2015
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3111–3119.",
    "year": 2013
  }, {
    "title": "That’s sick dude!: Automatic identification of word sense change across different timescales",
    "authors": ["Sunny Mitra", "Ritwik Mitra", "Martin Riedl", "Chris Biemann", "Animesh Mukherjee", "Pawan Goyal."],
    "venue": "Proceedings of ACL, pages 1020–1029.",
    "year": 2014
  }, {
    "title": "Analytic bias and phonological typology",
    "authors": ["Elliott Moreton."],
    "venue": "Phonology, 25(1):83–127.",
    "year": 2008
  }, {
    "title": "A general and simple method for obtaining r2 from generalized linear mixed-effects models",
    "authors": ["Shinichi Nakagawa", "Holger Schielzeth."],
    "venue": "Methods in Ecology and Evolution, 4(2):133–142.",
    "year": 2013
  }, {
    "title": "Semantic shift",
    "authors": ["John Newman."],
    "venue": "Nick Rimer, editor, The Routledge Handbook of Semantics, pages 266–280. Routledge, New York.",
    "year": 2015
  }, {
    "title": "Semantic density analysis: Comparing word meaning across time and phonetic space",
    "authors": ["Eyal Sagi", "Stefan Kaufmann", "Brady Clark."],
    "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 104–111. Associa-",
    "year": 2009
  }, {
    "title": "From frequency to meaning: Vector space models of semantics",
    "authors": ["Peter D Turney", "Patrick Pantel."],
    "venue": "Journal of artificial intelligence research, 37:141–188.",
    "year": 2010
  }, {
    "title": "Understanding semantic change of words over centuries",
    "authors": ["Derry Tanti Wijaya", "Reyyan Yeniterzi."],
    "venue": "Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web, pages 35–40. ACM.",
    "year": 2011
  }, {
    "title": "A computational evaluation of two laws of semantic change",
    "authors": ["Yang Xu", "Charles Kemp."],
    "venue": "CogSci.",
    "year": 2015
  }],
  "id": "SP:ab370db3fe0ee0cb272f13ebbb32894de90bec8d",
  "authors": [{
    "name": "Haim Dubossarsky",
    "affiliations": []
  }, {
    "name": "Eitan Grossman",
    "affiliations": []
  }, {
    "name": "Daphna Weinshall",
    "affiliations": []
  }],
  "abstractText": "This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.",
  "title": "Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models"
}