{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1903–1913 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1903"
  }, {
    "heading": "1 Introduction",
    "text": "Attention-based models have become architectures of choice for many NLP tasks. In addition to significant performance gains, these models are attractive, as attention is often used as a proxy for human interpretable rationales. Their success, however, is conditioned on access to large amounts of training data. To make these models applicable in low-resource scenarios, we utilize this connection in the opposite direction. Specifically, we propose an approach to map human rationales to high-performing attention, and use this attention to guide models trained in low-resource scenarios.\nThe notions of rationale and attention are closely related. Both of them highlight word importance for the final prediction. In the case of rationale, the importance is expressed as a hard selection, while attention provides a soft distribution over the words. Figure 1 illustrates this relatedness. One obvious approach to improve low-\n1Our code and data are available at https://github. com/YujiaBao/R2A.\nresource performance is to directly use human rationales as a supervision for attention generation. The implicit assumption behind this method is that machine-generated attention should mimic human rationales. However, rationales on their own are not adequate substitutes for machine attention. Instead of providing a soft distribution, human rationales only provide the binary indication about relevance. Furthermore, rationales are subjectively defined and often vary across annotators. Finally, human rationales are not customized for a given model architecture. In contrast, machine attention is always derived as a part of a specific model architecture.\nTo further understand this connection, we empirically compare models informed by human rationales and those by high-quality attention. To obtain the latter, we derive an “oracle” attention using a large amount of annotations. This “oracle” attention is then used to guide a model that only has access to a small subset of this training data. Not only does this model outperform the oracle-free variant, but it also yields substantial gains over its counterpart trained with human ra-\ntionales — 89.98 % vs 85.22 % average accuracy on three aspects of hotel review (see Section 4 for details). In practice, however, this “oracle” attention is not available. To employ this method, we need to find a way to obtain a substitute for the “oracle” attention.\nIn this paper, we show how to achieve this goal using rationales. Specifically, we learn a mapping from human rationales to high-quality attention (R2A). We hypothesize that this mapping is generalizable across tasks and thus can be transferred from resource-rich tasks.2 Figure 1 illustrates that in both tasks, attention weighs rationale words in a similar fashion: highlighting taskspecific nouns and adjectives, while downplaying functional words. To learn and apply this mapping we need access to rationales in both source and target tasks. In the target task, we assume rationales are provided by humans. In the source task(s), collecting rationales at scale is infeasible. Therefore, we use machine-generated rationales (Lei et al., 2016) as a proxy.\nOur R2A model consists of three components. The first one is an attention-based model for the source task(s) that provides supervision for attention generation. The second component focuses on learning a domain-invariant representation to support transfer. The third component combines this invariant representation and rationales together to generate the attention. These three components are trained jointly to optimize the overall objective. Once the model is trained, we apply it to the target task to generate attention from human rationales. This attention is consequently used to supervise the training of the target classifier.\nWe evaluate our approach on two transfer settings: aspect transfer within single domain and domain transfer across multiple domains. Our experiments demonstrate that our approach delivers significant performance improvements over the baselines. For instance, the average error reduction over the best baseline in domain transfer is over 15%. In addition, both qualitative and quantitative analyses confirm that our R2A model is capable of generating high-quality attention for target tasks.\n2In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains."
  }, {
    "heading": "2 Related Work",
    "text": "Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications.\nRationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work (Zhang et al., 2016) explores the potential of integrating rationales with more complex neural classifiers. In their model, human rationales are directly used to guide the sentence-level attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference between human rationales and machine attention. Moreover, we learn a model to map human rationales into high-quality attention so as to provide a richer supervision for low-resource models.\nTransfer learning When labeled data on the target task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) (Conneau et al., 2017; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios.\nOur work is also related to unsupervised domain\nadaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin et al., 2016; Zhang et al., 2017). In contrast, our model adapts the mapping from rationales to attention; thus after training, it can be applied to different target tasks."
  }, {
    "heading": "3 Method",
    "text": "Problem formulation We assume that we have N source tasks {Si}Ni=1, where each of them has sufficient amounts of labeled examples. Using existing methods (Lei et al., 2016), we can generate rationales for each source example automatically (see Appendix 1 for details). In the target task T , we only have a limited amount of labeled examples with large amounts of unlabeled data. For those labeled examples, we assume access to human-annotated rationales.\nOverview Our goal is to improve classification performance on the target task by learning a mapping from human rationales to high-quality machine attention (R2A). Given the scarcity of our target data, we learn this mapping on resource rich tasks where high-quality attention can be readily obtained during training. Next, the mapping between rationales and attention derived from the source tasks is exported into the target task. To enable this transfer, models have to operate over an invariant representation which we construct via an adversarial objective. Once the mapping is derived, we can translate human rationales in the target task into high-quality attention. This generated attention is then used to provide additional training signal for an attention-based classifier for the target task. The overall pipeline is shown in Figure 2.\nAlternatively, we can view the R2A mapping as a meta model that produces a prior over the attention distribution across different tasks.\nModel architecture Figure 3 illustrates the architecture of our R2A model, which consists of three components.\n• Multi-task learning In order to learn the R2A mapping, we need annotation for the attention. This module generates high-quality attention as an intermediate result by minimizing the prediction error on the source tasks (Section 3.1).\nunlabeled target data\nR2A\nlabeled target data with rationales\nlabeled target data with R2A-generated attention R2A\nStep 1: Training R2A\nStep 2: R2A inference\nStep 3: Training target classifier\nlabeled source data with rationales\n• Domain-invariant encoder This module aims to transform the contextualized representation obtained from the first module into a domain-invariant version. We achieve this goal through domain adversarial training over the source data and the unlabeled target data (Section 3.2).\n• Attention generation This module learns to predict the intermediate attention obtained from the first module based on the domain-invariant representation and the rationales (Section 3.3)."
  }, {
    "heading": "3.1 Multi-task learning",
    "text": "The goal of the multi-task learning module is to learn good attention for each source task. This learned attention will be used later to supervise the attention generation module. This module takes the input text from the source tasks and predicts the labels. To accomplish the previously stated goal, we minimize the prediction error over all labeled source data.\nLet (xt, yt) be a training instance from any source task t ∈ {S1, . . .SN}. We first encode the input sequence xt into hidden states: ht = enc(xt), where enc is a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) that is shared across all source tasks. For each position i, the dense vector hti encodes the content and context information of the word xti. We then pass the\nsequence ht on to a task-specific attention module to produce attention αt = attt(ht) as follows:\nh̃ti = tanh(W t atth t i + b t att), αti = exp(〈h̃ti, qtatt〉)∑ j exp(〈h̃tj , qtatt〉) ,\nwhere 〈·, ·〉 denotes inner product and W tatt, btatt, qtatt are learnable parameters. We predict the label of xt using the weighted sum of its contextualized representation: ŷt = predt( ∑ i α t ih t i), where predt is a task-specific multi-layer perceptron. We train this module to minimize the loss, Llbl, between the prediction and the annotated label for all source tasks. We use cross entropy loss for classification tasks and mean square loss for regression tasks."
  }, {
    "heading": "3.2 Domain-invariant encoder",
    "text": "Supplied with large amounts of source data and unlabeled target data, this module has two goals: 1) learning a general encoder for both source and target corpora, and 2) learning domain-invariant representation. This module enables effective transfer—especially in the presence of significant variance between the source and target domains. We achieve the first goal by optimizing a language modeling objective and the second goal by minimizing the Wasserstein distance between the source and target distribution.\nLet x be an input sequence, and h , [ −→ h ; ←− h ] be its corresponding contextualized representation obtained from enc. Here, −→ h and ←− h denote the output sequence of the forward and backward LSTM, respectively. In order to support transfer, enc should be general enough to effectively represent both source and target corpora. For this reason, we ground the encoder by a language modeling component (Bengio et al., 2003; Mikolov et al., 2011). Specifically, we employ two Softmax classifiers to predict the word xi based on−→ h i−1 and ←− h i+1 respectively. We minimize the cross-entropy lossLlm over all source data and unlabeled target data.\nThe representation h is domain-specific as it is trained to encode useful features for language modeling and the source tasks. To obtain an invariant representation, we employ a transformation layer and propose to align the transformed representation so that it is not distinguishable whether it comes from the source or the target. Specifically, we transform the representation hi at each position i linearly and obtain\nhinvi =Winvhi + binv,\nwhere Winv and binv are learnable parameters. We minimize the Wasserstein distance (Arjovsky et al., 2017) between the distribution of hinv from the source and the one from the target, denoted as PS and PT , respectively. Since hinv is a sequence\nof variable length, L, we summarize it by its first and last element via concatenation, [hinv1 ;h inv L ]. The training objective is defined as:\nLwd = sup ‖f‖L≤K\nEhinv∼PS [ f([hinv1 ;h inv L ]) ]\n− Ehinv∼PT [ f([hinv1 ;h inv L ]) ] ,\nwhere the supremum is over allK-Lipschitz scalar functions f . Following Gulrajani et al. (2017), we approximate f by a multi-layer perceptron, and use gradient penalty to fulfill the Lipschitz constraint."
  }, {
    "heading": "3.3 Attention generation",
    "text": "The goal of this module is to generate high-quality attention for each task. This module combines the domain-invariant representation together with task-specific rationales as its input and predicts task-specific attention scores. We minimize the distance between the predicted attention and the intermediate attention obtained from the multitask learning module.\nFor any source task t ∈ {Si}Ni=1, we denote rt as the task-specific rationales corresponding to the input text xt, and denote hinv,t as the domaininvariant representation of xt. For each position i, we first concatenate rti with the frequency of x t i occurring as a rationale from all training examples of this task. We denote this augmented sequence as r̃t. This frequency term provides the unigram likelihood of each word being a rationale for the task. Then we employ a sequence encoder encr2a and an attention module attr2a to predict the attention scores:\nut = encr2a([hinv,t; r̃t]),\nũti = tanh(W r2a att u t i + b r2a att), α̂ti = exp(〈ũti, qr2aatt 〉)∑ j exp(〈ũtj , qr2aatt 〉) ,\nwhere W r2aatt , b r2a att and q r2a att are learnable parameters, and both encr2a and attr2a are shareable across all tasks. We minimize the distance between α̂t and the αt obtained from the first multitask learning module over all source data:\nLatt = ∑\n(αt,α̂t),t∈{Si}Ni=1\nd(αt, α̂t),\nwhere d(·, ·) can be any distance metric. In this paper, we consider a soft-margin cosine distance:\nd(a, b) , max(0, 1− cos(a, b)− 0.1),\nwhere cos(·, ·) denotes the cosine similarity."
  }, {
    "heading": "3.4 Pipeline",
    "text": "Training R2A We train the three aforementioned modules jointly using both the source data and the unlabeled target data. The overall objective is given by:\nL = Llbl + λattLatt + λlmLlm + λwdLwd. (1)\nThe λs are hyper-parameters that control the importance of each training objective and Ls represent the corresponding loss functions.\nR2A inference Once the R2A model is trained, we can generate attention for each labeled target example based on its human-annotated rationales.\nTraining target classifier When testing the performance on the target task, of course, we are neither provided with labels nor rationales. In order to make predictions for the target task, we train a new classifier under the supervision of both the labels and the R2A-generated attention. Specifically, this target classifier shares the same architecture as the source one in the multi-task learning module. We minimize the prediction loss, LTlbl, on the labeled target data together with the cosine distance, LTatt, between the R2A-generated attention and the attention generated by this target classifier. The joint objective for this target classifier is defined as\nL = LTlbl + λTattLTatt, (2)\nwhere λTatt controls the importance of LTatt. For better transfer, we initialize the encoder in the target classifier as enc from the trained R2A model."
  }, {
    "heading": "4 Experimental Setup",
    "text": ""
  }, {
    "heading": "4.1 Datasets",
    "text": "We evaluate our approach on two transfer settings: transfer among aspects within the same domain and transfer among different domains.\nAspect transfer We first consider the transfer problem between multiple aspects of one domain: beer review. We use a subset of the BeerAdvocate3 review dataset (McAuley et al., 2012) introduced by Lei et al. (2016). This dataset contains reviews with ratings (in the scale of [0, 1]) from three aspects of the beer: look, aroma and palate. We treat\n3https://www.beeradvocate.com\nany two aspects as the source and the other one as the target. We consider a classification setting for each target task. Specifically, reviews with ratings ≤ 0.4 are labeled as negative and those with ≥ 0.6 are labeled as positive. We form our dataset by randomly selecting an equal number of positive and negative examples. Then we randomly select 200 examples and ask human annotators to provide rationales (see Appendix 2 for details). These 200 examples are treated as our labeled training data for the target aspect. Unlabeled target data is not required since both source and target tasks are from the same domain. Table 1 summarizes the statistics of the beer review dataset.\nDomain transfer Our second experiment focuses on domain transfer from beer reviews to different aspects of hotel reviews. We use the TripAdvisor4 hotel review dataset (Wang et al., 2010), with the following three aspects as our transfer target: location, cleanliness, and service. For each aspect, reviews with ratings > 3 are labeled as positive and those with < 3 are labeled as negative. Similarly, we collect human rationales for 200 examples and treat them as our training data (see Appendix 2 for details). Table 2 summarizes the statistics of the hotel review dataset. In this experiment, data from all three aspects of the beer reviews are treated as the source tasks."
  }, {
    "heading": "4.2 Baselines",
    "text": "We compare our approach (OURS) with four types of baselines:\n4https://www.tripadvisor.com\nBasic classifier We train a linear SVM using bag-of-ngrams representation on the labeled target data. We combine uni-gram, bi-grams, and trigrams as features and use tf-idf weighting.\nRationale augmented classifiers We evaluate two previous methods that incorporate human rationales during training: 1) rationale augmented SVM (RA-SVM) (Zaidan et al., 2007), an SVMbased model that utilizes human rationales to regularize the decision boundary of the classifier; 2) rationale augmented CNN (RA-CNN) (Zhang et al., 2016). RA-CNN first trains a CNN-based sentence classifier to estimate the probability that a given sentence contains rationale words. Then RA-CNN scales the contribution of each sentence to the overall representation in proportion to these estimates. The final prediction is made from this overall representation.\nTransfer methods We compare against two variants of our method: 1) TRANS, an attentionbased classifier that does not use human rationales from the target task; 2) RA-TRANS, an attentionbased classifier that directly uses human rationales to supervise the attention. Specifically, TRANS only optimizes the cross-entropy loss LTlbl in the objective (Eq. (2)). For RA-TRANS, the term LTatt in the objective Eq. (2) is replaced by the cosine distance between human rationales and the attention generated by itself. Note that both models still have the ability to transfer, as their encoders are both initialized from enc, which has been trained on source data and unlabeled target data.\nOracle We also report the performance of an ORACLE which shares the same architecture as ours but is supervised by the oracle attention. The oracle attention is derived from a held-out dataset with large-scale annotations for the target task (see Appendix 3 for details). This helps us analyze the contribution of our R2A approach in isolation of the inherent limitations of the target tasks."
  }, {
    "heading": "4.3 Implementation details",
    "text": "We use pre-trained fastText embeddings (Bojanowski et al., 2017), a 200-dimension bidirectional LSTM (Hochreiter and Schmidhuber, 1997) for the language encoder, and a 50- dimension bi-directional LSTM for the R2A encoder. Dropout (Srivastava et al., 2014) is applied with drop rate 0.1 on the word embeddings and the last hidden layers of the classifiers. All\nparameters are optimized using Adam (Kingma and Ba, 2014). We set the initial learning rate to 0.001 and divide it by 10 once the performance on the development set plateaus. For RATRANS, OURS and ORACLE, we tuned λTatt from {102, 101, 100, 10−1, 10−2}. For domain transfer, we set λlm = 0.1, λwd = 0.01 and λatt = 0.01. For aspect transfer, we adapt the same hyperparameters, but set λwd = 0 as both source tasks and the target task come from the same domain. To encourage the R2A-generated attention to be consistent with the provided rationales in aspect transfer, we augment the overall training objective of R2A in Eq. (3.3) by a consistency regularization, which is computed from the cosine distance between the R2A-generated attention and the provided rationales.\nIn addition, computing Llm is both time and memory inefficient because the complexity is linear to the size of the vocabulary, which can be very large. To expedite the training, we adopt a technique proposed by Mikolov et al. (2011), which randomly splits the entire vocabulary into a predefined number of bins and minimizes the loss of the bin prediction instead of the exact token prediction. We set the bin size as 100 for our experiment."
  }, {
    "heading": "5 Results",
    "text": "Aspect transfer Table 3 summarizes the results of aspect transfer on the beer review dataset. Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects. It closely matches the performance of ORACLE with only 0.40% absolute difference.\nSpecifically, all rationale-augmented methods (RA-SVM, RA-TRANS and OURS) outperform their rationale-free counterparts on average. This confirms the value of human rationales in the low-resource settings. We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin. This validates our hypothesis that human rationales and attention are different.\nDomain transfer Table 4 presents the results of domain transfer using 200 training examples. We use the three aspects of the beer review data together as our source tasks while use the three aspects of hotel review data as the target. Our model (OURS) shows marked performance improvement. The error reduction over the best baseline is 15.08% on average.\nWe compare the learning curve in Figure 4. We observe that the performance of our model steadily improves as more annotations are provided, and the improvement over other baselines is significant and consistent.\nAblation study Table 5 presents the results of an ablation study of our model in the setting of domain transfer. As this table indicates, both the language modeling objective and the Wasserstein\ndistance contribute similarly to the task, with the Wasserstein distance having a bigger impact.\nVisualization of representation Figure 5 visualizes the hidden representation of 200 beer reviews and 200 hotel reviews using t-SNE (Maaten and Hinton, 2008). We observe that our model successfully aligns the source and the target feature distribution. This indicates the effectiveness of optimizing the Wasserstein distance objective.\nAnalysis of R2A-generated attention In order to validate that the trained R2A model is able to generate task-specific attention from human rationales, we perform both qualitative and quantitative\n5Since the hidden representation is a sequence of variable length, we applied t-SNE on the concatenation of the first and last element: [hinv1 ;hinvL ].\nanalysis on the R2A-generated attention in the setting of domain transfer. It is worth pointing out that our R2A model has never seen any labeled hotel reviews during training.\nTable 6 presents the average cosine distance between the R2A-generated attention and the oracle attention over the target training set. Compared with human rationales, the R2A-generated attention is much closer to the oracle attention. This explains the large performance boost of our method.\nFigure 6 visualize the R2A-generated attention on the same hotel review with human rationales corresponding to three different aspects. We observe that the trained R2A model is able to produce task-specific attention scores corresponding to the provided human rationale. For example, given the rationale sentence “not the cleanest rooms but bed was clean and so was bathroom”, R2A recognizes that not every token is equally important, and the attention should focus more on “clean”, “cleanest”, “rooms” and “bathroom”.\nAnnotating rationales versus annotating more labeled data Providing rationales for the training data roughly doubles the annotation cost (Zaidan et al., 2007). Given the same annotation budget, a natural question is: shall we collect a few labeled examples with rationales or annotate more labeled examples? To answer this question, we vary the number of training examples in the target task. Figure 7 shows the corresponding learning curve of a classifier that is trained without rationales. The reference line represents the accuracy of our approach trained on 200 examples with rationales. We notice that in order to reach the same level of performance, the rationale-free classifier requires 800, 3100, and 1900 labeled examples on the three target tasks respectively."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we propose a novel approach that utilizes the connection between human rationales and machine attention to improve the performance of low-resource tasks. Specifically, we learn a transferrable mapping from rationales to high-quality attention on resource-rich tasks. The learned mapping is then used to provide an additional supervision for the target task. Experimental results on both aspect and domain transfer validate that the R2A-generated attention serves as a better form of\nsupervision. Our model produces high-quality attention for low-resource tasks."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the MIT NLP group and the reviewers for their helpful discussion and comments. This work is supported by MIT-IBM Watson AI Lab. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations."
  }],
  "year": 2018,
  "references": [{
    "title": "Wasserstein gan",
    "authors": ["Martin Arjovsky", "Soumith Chintala", "Léon Bottou."],
    "venue": "arXiv preprint arXiv:1701.07875.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research, 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics, 5:135–146.",
    "year": 2017
  }, {
    "title": "Reading wikipedia to answer opendomain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1,",
    "year": 2017
  }, {
    "title": "Marginalized denoising autoencoders for domain adaptation",
    "authors": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."],
    "venue": "arXiv preprint arXiv:1206.4683.",
    "year": 2012
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Supervised learning of universal sentence representations from natural language inference data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loic Barrault", "Antoine Bordes."],
    "venue": "arXiv preprint arXiv:1705.02364.",
    "year": 2017
  }, {
    "title": "Domain-adversarial training of neural networks",
    "authors": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky."],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 513–520.",
    "year": 2011
  }, {
    "title": "Improved training of wasserstein gans",
    "authors": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron C Courville."],
    "venue": "Advances in Neural Information Processing Systems, pages 5769–5779.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole."],
    "venue": "arXiv preprint arXiv:1611.01144.",
    "year": 2016
  }, {
    "title": "Supervised attention for sequence-to-sequence constituency parsing",
    "authors": ["Hidetaka Kamigaito", "Katsuhiko Hayashi", "Tsutomu Hirao", "Hiroya Takamura", "Manabu Okumura", "Masaaki Nagata."],
    "venue": "Proceedings of the Eighth International Joint Confer-",
    "year": 2017
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Rationalizing neural predictions",
    "authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107–117.",
    "year": 2016
  }, {
    "title": "A structured self-attentive sentence embedding",
    "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1703.03130.",
    "year": 2017
  }, {
    "title": "Neural machine translation with supervised attention",
    "authors": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita."],
    "venue": "arXiv preprint arXiv:1609.04186.",
    "year": 2016
  }, {
    "title": "Exploiting argument information to improve event detection via supervised attention mechanisms",
    "authors": ["Shulin Liu", "Yubo Chen", "Kang Liu", "Jun Zhao."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2017
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "Visualizing data using t-sne",
    "authors": ["Laurens van der Maaten", "Geoffrey Hinton."],
    "venue": "Journal of machine learning research, 9(Nov):2579–2605.",
    "year": 2008
  }, {
    "title": "Learning attitudes and attributes from multiaspect reviews",
    "authors": ["Julian McAuley", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020– 1025. IEEE.",
    "year": 2012
  }, {
    "title": "Extensions of recurrent neural network language model",
    "authors": ["Tomáš Mikolov", "Stefan Kombrink", "Lukáš Burget", "Jan Černockỳ", "Sanjeev Khudanpur."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,",
    "year": 2011
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1802.05365.",
    "year": 2018
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1509.00685.",
    "year": 2015
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin."],
    "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
    "year": 2017
  }, {
    "title": "Latent aspect rating analysis on review text data: a rating regression approach",
    "authors": ["Hongning Wang", "Yue Lu", "Chengxiang Zhai."],
    "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783–",
    "year": 2010
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Using annotator rationales to improve machine learning for text categorization",
    "authors": ["Omar Zaidan", "Jason Eisner", "Christine Piatko."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for",
    "year": 2007
  }, {
    "title": "Rationale-augmented convolutional neural networks for text classification",
    "authors": ["Ye Zhang", "Iain Marshall", "Byron C Wallace."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in",
    "year": 2016
  }, {
    "title": "Aspect-augmented adversarial networks for domain adaptation",
    "authors": ["Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Transactions of the Association of Computational Linguistics, 5(1):515–528.",
    "year": 2017
  }, {
    "title": "Bi-transferring deep neural networks for domain adaptation",
    "authors": ["Guangyou Zhou", "Zhiwen Xie", "Jimmy Xiangji Huang", "Tingting He."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association",
    "year": 2016
  }],
  "id": "SP:8b37185ad6e25c8e1a6ec622a73d3281e80f4378",
  "authors": [{
    "name": "Yujia Bao",
    "affiliations": []
  }, {
    "name": "Shiyu Chang",
    "affiliations": []
  }, {
    "name": "Mo Yu",
    "affiliations": []
  }, {
    "name": "Regina Barzilay",
    "affiliations": []
  }],
  "abstractText": "Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete humanannotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-ofthe-art baselines, yielding over 15% average error reduction on benchmark datasets.1",
  "title": "Deriving Machine Attention from Human Rationales"
}