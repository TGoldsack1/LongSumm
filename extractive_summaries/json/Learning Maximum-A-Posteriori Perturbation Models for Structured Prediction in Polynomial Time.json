{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Structured prediction can be thought of as a generalization of binary classification to structured outputs, where the goal is to jointly predict several dependent variables. Predicting complex, structured data is of great significance in various application domains including computer vision (e.g., image segmentation, multiple object tracking), natural language processing (e.g., part-of-speech tagging, named entity recognition) and computational biology (e.g. protein structure prediction). However, unlike binary classification, structured prediction presents a set of unique computational and statistical challenges. The chief being that the number of structured outputs is exponential in the input size. For instance, in translation tasks, the number of parse trees of a sentence is exponential in the length of the sentence. Second, it is very common in such domains to have very few training examples as compared to the size of the output space thereby making generalization to unseen inputs difficult.\n1Department of Computer Science, Purdue University, West Lafayette, IN - 47906. Correspondence to: Asish Ghoshal <aghoshal@purdue.edu>, Jean Honorio <jhonorio@purdue.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nThe key computational challenge in structured prediction stems from the inference problem, where a decoder, parameterized by a vector w of weights, predicts (or decodes) the latent structured output y given an observed input x. With the exception of a few special cases, the general inference problem in structured prediction is intractable. For instance in many cases the inference problem reduces to the maximum acyclic subgraph problem which is NP-hard and hard to approximate to within a factor of 1/2 of the optimal solution (Guruswami et al., 2008), or cardinality-constrained submodular maximization, which is also NP-hard and hard to compute a solution better than the (1 1/\")-approximate solution returned by a greedy algorithm (Nemhauser et al., 1978). The learning problem, where the goal is to learn the parameter w of the decoder from a set of labeled training instances, and which involves solving the inference problem as a subroutine, is therefore intractable for all but a few special cases. Hardness of max-margin learning (SVM) was shown by (Sontag et al., 2010).\nHardness results notwithstanding, various methods — which are worst-case exponential-time — have been developed over the last decade for predicting structured data including conditional random fields (Lafferty et al., 2001), and maxmargin approaches (Taskar et al., 2003), to name a few. In these approaches, learning the parameter w of the decoder involves minimizing a loss function L(w, S) over a data set S of m training pairs {(x\ni , y i )}m i=1\n. One could also take a Bayesian approach and learn a posterior distribution Q over decoder parameters w by minimizing the Gibbs loss E w⇠Q [L(w, S)]. McAllester (McAllester, 2007) showed, using the PAC-Bayesian framework, that the commonly used max-margin loss (Taskar et al., 2003) upper bounds the expected Gibbs loss over the data distribution, upto statistical error. Therefore, minimizing the max-margin loss provides a principled way for learning the parameters of a structured decoder. More recently, (Honorio & Jaakkola, 2016) showed that minimizing a surrogate randomized loss, where the max-margin loss is computed over a small number of randomly sampled structured outputs, also bounds the Gibbs loss from above upto statistical error.\nThe above can be thought of as weight based perturbation models. The perturb-and-MAP framework introduced by\n(Papandreou & Yuille, 2011), and henceforth referred to as MAP perturbation, provides an efficient way to generate samples from the Gibbs distribution by injecting random noise (that do not depend on the weights of the decoder w) in the potential or score function of the decoder and then computing the most likely assignment or energy configuration (MAP). MAP perturbation models are an attractive alternative to expensive Markov Chain Monte Carlo simulations for drawing samples from the Gibbs distribution, in that the former facilitates one-shot sampling. Moreover, learning MAP predictors for structured prediction problems is particularly attractive because the predictions are robust to random noise. However, learning the parameters of such MAP predictors involves solving the MAP problem, which in general is intractable. In this paper we obtain a provably polynomial time algorithm for learning the parameters of perturbed MAP predictors with structure based perturbations. In the following paragraph we summarize the main technical contributions of our paper.\nOur contributions. To the best of our knowledge, we are the first to obtain generalization bounds for MAPperturbation models with structure-based (Gumbel) perturbations — for detailed comparison with existing literature see Section 6. While it is well known that Gumbel perturbations induce a conditional random field (CRF) distribution over the structured outputs, we show that the generalization error is upper bounded by a CRF loss up to statistical error. We obtain Rademacher based uniform convergence guarantees for the latter. However, the main contribution of our paper is to obtain a provably polynomial time algorithm for learning MAP-perturbation models for general structured prediction problems. We propose a novel randomized surrogate loss that lower bounds the CRF loss and still upper bounds the expected loss over data distribution, upto approximation and statistical error terms that decay as eO (1/pm) with m being the number of samples. While it is NP-Hard to compute and approximate the CRF loss in general (Barahona, 1982; Chandrasekaran et al., 2008), our surrogate loss can be computed in polynomial time. Our results also imply that one can learn parameters of CRF models for structured prediction in polynomial time under certain conditions. Our work is inspired by the work of (Honorio & Jaakkola, 2016) who also propose a polynomial time algorithm for learning the parameters of a structured decoder in the max-margin framework. In contrast to prior work which consider weight based perturbations, our work is concerned with structure based perturbations. Previous algorithms for learning MAP perturbation models, for instance, the hard-EM algorithm by (Gane et al., 2014) and the moment-matching algorithm by (Papandreou & Yuille, 2011), are in general intractable and have no generalization guarantees. Lastly, the main conceptual contribution of our work is to demonstrate that it is possible to efficiently learn the parameters of a structured\ndecoder with generalization guarantees without solving the inference problem exactly."
  }, {
    "heading": "2. Preliminaries",
    "text": "We begin this section by introducing our notations and formalizing the problem of learning MAP-perturbation models. In structured prediction, we have an input x 2 X and a set of feasible decodings of the input Y(x). Without loss of generality, we assume that |Y(x)|  r for all x 2 X. Input-output pairs (x, y) are represented by a joint feature vector (x, y) 2 Rd. For instance, when x is a sentence and y is a parse tree, the joint feature map (x, y) can be a vector of 0/1-indicator variables representing if a particular word is present in x and a particular edge is present in y. We will assume that min{\nj (x, y) 6= 0 | j 2 [d]} 1 which commonly holds for structured prediction problems, for instance, when using binary features, or features that “count” number of components, edges, parts, etc.\nA decoder f w : X! Y, parameterized by a vector w 2 Rd, returns an output y 2 Y(x) given an input x. We consider linear decoders of the form:\nf w (x) = argmax y2Y(x)\nh (x, y), wi, (1)\nwhich return the highest scoring structured output for a particular input x, where the score is linear in the weights w. As is traditionally the case in high-dimensional statistics, we will assume that the weight vectors are s-sparse, i.e., have at most s non-zero coordinates. We will denote the set of s-sparse d-dimensional vectors by Rd,s.\nIn the perturb and MAP framework, a stochastic decoder first perturbs the linear score by injecting some independent noise for each structured output y, and then returns the structured output that maximizes the perturbed score. Gumbel perturbations are commonly used owing to the max-stability property of the Gumbel distribution. Denoting G( ) as the Gumbel distribution with location and scale parameters 0 and respectively, we have the following stochastic decoder, where ⇠ Gr denotes a collection of r i.i.d. Gumbeldistributed random variables and\ny denotes the Gumbel random variable associated with structured output y:\nf w, (x) = argmax y2Y(x)\nh (x, y), wi+ y . (2)\nFor any weight vector w, and data set S = {(x i , y i )} i.i.d.⇠ Dm, we consider the following expected and empirical zeroone loss:\nL(w,D) = E (x,y)⇠D [E ⇠Gr [1 [y 6= fw, (x)]]] , (3)\nL(w, S) = 1\nm\nmX\ni=1\nE ⇠Gr [1 [yi 6= fw, (xi)]] , (4)\nwhere 1 [ · ] denotes the indicator function and D is the unknown data distribution. We will let the scale parameter depend on the number of samples m and the weight vector w, and write (m,w) > 0. The reason for this will become clear later, but intuitively one would expect that as the number of samples increases, the magnitude of perturbations should decrease in order to control the generalization error. Under Gumbel perturbations, f\nw, (x i ) is distributed according to following conditional random field (CRF) distribution Q(x\ni , w) with pmf q( · ;x i\n, w) (Gumbel, 1954; Papandreou & Yuille, 2011):\nq(y i ;x i , w) = Pr ⇠Gr( ) {fw, (xi) = yi}\n=\nexp( h (xi,yi),wi/ )\nZ(w, x i )\n, (5)\nwhere Z(w, x i ) = P y2Y(x) exp(\nh (xi,y),wi/ ) is the partition function. The empirical loss in (4) can then be computed as:\n(CRF loss) L(w, S) = 1 m\nmX\ni=1\nPr {f w, (x i ) 6= y i } . (6)\nThe ultimate objective of a learning algorithm is to learn a weight vector w that generalizes to unseen data. Therefore, minimizing the expected loss given by (3) is the best strategy towards that end. However, since the data distribution is unknown, one instead minimizes the empirical loss (4) on a finite number of labeled examples S."
  }, {
    "heading": "3. Generalization Bound",
    "text": "As a first step we will show that the empirical loss (6) indeed bounds the expected perturbed loss (3) from above, upto statistical error that decays as eO (1/pm). We have the following generalization bound.\nTheorem 1 (Rademacher based generalization bound). With probability at least 1 over the choice of m samples S:\n(8w 2 Rd,s) L(w,D)  L(w, S) + \"(d, s,m, r, ), where\n\"(d, s,m, r, ) = 2\nr s(ln d+ 2 ln(mr))\nm + 3\nr ln 2/\n2m .\nProof. Let\ng w (x, y) def = Pr ⇠Gr( ) {y 6= fw, (x)} , G def = {g w\n| w 2 Rd,s}. Then by Rademacher based uniform convergence, with probability at least 1 over the choice of m samples, we have\nthat: (8w 2 Rd,s) L(w,D)  L(w, S) + 2bRS(G) + 3 r log 2/\n2m ,\n(7)\nwhere bRS(G) denotes the empirical Rademacher complexity of G. Let = (\ni\n)\nm\ni=1 be independent Rademacher variables. Also define W def= {w/ (w,m) | w 2 Rd,s}. Then, bRS(G)\n= E \" sup\nw2Rd,s\n1\nm\nmX\ni=1\ni g w (x i , y i )\n#\n=\n1 m E\n\" sup\nw2Rd,s\nmX\ni=1\ni\nPr ⇠Gr( ) {yi 6= fw, (xi)} #\n(a)\n=\n1 m E\n\" sup\nw2W\nmX\ni=1\ni\nPr ⇠Gr(1) {yi 6= fw, (xi)} #\n 1 m E ⇠Gr(1)\n\" E \" sup\nw2W\nmX\ni=1\ni 1 [y i 6= f w, (x i )]\n##\n(b)  1 m E ⇠Gr(1)\n\" E \" sup\nw2Rd,s\nmX\ni=1\ni 1 [y i 6= f w, (x i )] ## ,\nwhere step (a) follows from Pr ⇠Gr( ) {yi 6= fw, (xi)}\n= Pr ⇠Gr(1) y i 6= fw / , (xi) , and step (b) follows from W ✓ Rd,s. We will enumerate the structured outputs Y(x\ni ) as y i,1 , . . . , y i,r . For any fixed , the weight vector w induces a linear ordering ⇡\ni ( · ; ) over the structured outputs Y(x\ni ), i.e., h (x i , y i,⇡i(1; ) ), wi + 1\n> h (x\ni , y i,⇡i(2; ) ), wi + 2 > . . . > h (x i , y i,⇡i(r; ) ), wi + r . Let ⇡( ) = {⇡ i\n} be the orderings over all m data points induced by a fixed weight vector w and fixed , and let ⇧( ) be the collection of all orderings ⇡( ) over all w 2 Rd,s for a fixed . Since w is s-sparse we have, from results by (Bennett, 1956; Bennett & Hays, 1960; Cover, 1967), that the number of possible linear orderings is |⇧( )|  d\ns\n(mr)2s  ds(mr)2s . Therefore we have:\nbRS(G)\n 1 m E ⇠Gr( )\n\" E \" sup\n⇡( )2⇧( )\nmX\ni=1\ni 1 ⇥ y i 6= y i,⇡i(1; )\n⇤ ##\n(a)  1 m\np s(log d+ 2 log(mr)) p m\n=\nr s(log d+ 2 log(mr))\nm ,\nwhere the inequality (a) follows from the Massart’s finite class lemma.\nAs a direct consequence of the uniform convergence bound given by Theorem 1, we have that minimizing the CRF loss\n(6) is a consistent procedure for learning MAP-perturbation models."
  }, {
    "heading": "4. Towards an efficient learning algorithm",
    "text": "While Theorem 1 provides theoretical justification for learning MAP-perturbation models by minimizing the CRF loss (6), with the exception of a few special cases, computing the loss function is in general intractable. This is due to the need for computing the partition function Z(w, x) which is an NP-hard problem (Barahona, 1982). Further, even approximating Z(w, x) with high probability and arbitrary precision is also known to be NP-hard (Chandrasekaran et al., 2008).\nTo counter this computational bottleneck, we propose an efficient stochastic decoder that decodes over a randomly sampled set of structured outputs. To elaborate further, given some x 2 X, let R(x,w) be some proposal distribution, parameterized by x and w, over the structured outputs Y(x). We generate a set T0 of n structured outputs sampled independently from the distribution R and define the following efficient stochastic decoder:\nf w, ,T0(x) = argmax y2T0 h (x, y), wi+ y . (8)\nTherefore f w, ,T0(x) is distributed according to the CRF distribution Q(x,w,T0) with pmf q( · ;x,w,T0) and support on T0 as follows:\nq(y;x,w,T0) = Pr ⇠Gn {fw, ,T0(x) = y}\n= 1 [y 2 T0] Z w,x,T0 exp( h (x,y),wi/ ),\nwhere Z w,x,T0 = P y 02T0 exp( h (x,y0),wi/ ). Note that the partition function Z w,x,T0 can be computed in time linear in n, since |T0| = n. Now, let T = {T i | x i\n2 S} be the collection of n structured outputs sampled for each x\ni\nin the data set, from the product distribution R(S, w) def= ⇥m\ni=1\n(R(x i ) n ). Note that the distribution R(S, w) does not depend on the {y\ni }’s in S. We denote the distribution over the collection of sets {T\ni } by R(S, w) to keep the notation light. Additionally, we consider proposal distributions R(x,w) that are equivalent upto linearly inducible orderings of the structured output.\nDefinition 1 (Equivalence of proposal distributions (Honorio & Jaakkola, 2016)). For any x 2 X, two proposal distributions R(x,w) and R(x,w0), with probability mass functions p(·;x,w) and p(·;x,w0), are equivalent if:\n8y, y0 2 Y(x) : h (x, y), wi  h (x, y0), wi and h (x, y), w0i  h (x, y0), w0i\n() 8y 2 Y(x) p(y;x,w) = p(y;x,w0).\nWe then write R(x,w) ⌘ R(x,w0) ⌘ R(x,⇡(x)), where ⇡(x) is the linear ordering over Y(x) induced by w (and w0).\nIntuitively speaking, the above definition requires proposal distributions to depend only on the orderings of the values h (x, y\n1 ), wi, . . . , h (x, y r ), wi and not on the actual value of h (x, y\nj ), wi. To obtain an efficient learning algorithm with generalization guarantees, we will use augmented sets ¯T = {¯T\ni }m i=1\n, where ¯T\ni = T i [{y i }. Then, given a random collection of structured outputs T, we consider the following augmented randomized empirical loss for learning the parameters of the MAP-perturbation model:\nL(w, S, ¯T) = 1\nm\nmX\ni=1\nPr ⇠Gn f w, , ¯Ti(xi) 6= yi . (9)\nAs opposed to the loss function given by (6), the loss in (9) can be computed efficiently for small n. Our next result shows that the randomized augmented loss lower bounds the full CRF loss L(w, S) as long as ¯T\ni is a set, i.e., contains only unique elements. Lemma 1. For all data sets S, T\ni ✓ Y(x i\n), and weight vectors w:\nL(w, S, ¯T) L(w, S) =\n1 m\nmX\ni=1\nPr f w, , ¯Ti(xi) = yi ⇥\nPr f w, (x i ) 2 (Y(x i ) \\ ¯T i )  0 (10)\nProof. For any x 2 X, T ✓ Y(x), y 2 T and weight vector w:\nPr {f w, (x) = y} Pr {f w, ,T(x) = y}\n= eh (x,y),wi ⇢ Z(w, x,T) Z(w, x) Z(w, x)Z(w, x,T)\n=\neh (x,y),wi\nZ(w, x,T)\n1\nZ(w, x)\n8 <\n: X\ny 02Y(x)\\T\neh (x,y 0 ),wi\n9 =\n;\n= Pr {f w, ,T(x) = y}Pr {fw, (x) 2 Y(x) \\ T} .\nSince by construction y i 2 ¯T i , the final claim follows.\nRemark 1. If ¯T i = Y(x i ) then L(w, S) = L(w, S, ¯T i ).\nNext, we will show that an algorithm that learns the parameter w of the MAP-perturbation model, by sampling a small number of structured outputs for each x\ni and minimizing the empirical loss given by (9), generalizes under various choices of the proposal distribution R. Our first step in that direction would be to obtain uniform convergence guarantees for the stochastic loss (9)."
  }, {
    "heading": "4.1. Generalization bound",
    "text": "To obtain our generalization bound, we decompose the difference L(w, S) L(w, S, ¯T) as follows:\nL(w, S) L(w, S, ¯T) = A(w, S) +B(w, S, ¯T), (11) A(w, S) = L(w, S) ET⇠R(S) ⇥ L(w, S, ¯T) ⇤ , (12)\nB(w, S, ¯T) = ET⇠R(S) ⇥ L(w, S, ¯T) ⇤ L(w, S, ¯T), (13) where A(w, S) can be thought of as the approximation error due to using a small number of structured outputs T\ni ’s instead of the full sets Y(x\ni ), while B(w, S, ¯T) be is the statistical error. In what follows, we will bound each of these errors from above.\nFrom Lemma 1 it is clear that the proposal distribution plays a crucial role in determining how far the surrogate loss L(w, S, ¯T) is from the CRF loss L(w, S). To bound the approximation error, we make the following assumption about the proposal distributions R(x,w). Assumption 1. For all (x\ni , y i ) 2 S and weight vectors w 2 Rd,s, the proposal distribution satisfies the following condition with probability at least 1 kwk\n1 /pm, for a constant c 2 [0, 1]:\n(i) T i = {y i } if 8 y 6= y i h (x i , y i ), wi > h (x i , y), wi, (ii) 1\nn P y2Tih (xi, y), wi h (xi, yi), wi+c kwk1 otherwise,\nwhere the probability is taken over the set T i .\nIntuitively, Assumption 1 states that, if y i is not the highest scoring structure under w, then the proposal distribution should return structures T = {y} whose average score is an additive constant factor away from the score of the observed structure y\ni with high probability. Otherwise, the proposal distribution should return the singleton set T = {y\ni } with high probability. Note that Assumption 1 is in comparison much weaker than the low-norm assumption of (Honorio & Jaakkola, 2016), which requires that, in expectation, the norm of the difference between (x, y) and (x, y\ni ) (where y is sampled from the proposal distribution) should decay as 1/pm. The following lemma bounds the approximation error from above.\nLemma 2 (Approximation Error). If the scale parameter of the Gumbel perturbations satisfies:  min( kwk 1\n/logm,wmin/log((r 1)(pm 1))) for all w 6= 0, and n m0.5 c, then under Assumption 1 A(w, S)  \" 1 (m,n,w), where\n\" 1 (m,n,w) def =\nkwk 1p\nm +\n1\n1 + p m ,\nand w min = min{|w j | | |w j | 6= 0, j 2 [d]}.\nProof. Let A i (w, S) def = Pr ⇠G( ) {fw, (xi) 6= yi} ETi ⇥ Pr ⇠G( ) f w, , ¯Ti(xi) 6= yi ⇤\nbe the i-th term of A(w, S). We will consider two cases.\nCase I: y i is strictly the highest scoring structure for x i under w, i.e., 8y 6= y i h (x i , y i ), wi > h (x i\n, y), wi. First note that:\nA i (w, S)  Pr ⇠G( ) {fw, (xi) 6= yi} . (14)\nWe will prove that Pr ⇠G( ) {fw, (xi) 6= yi}  1/pm. Assume instead that Pr ⇠G( ) {fw, (xi) 6= yi} > 1/pm. Then X\ny 6=yi\n( p m 1)eh (xi,y),wi/ > eh (xi,yi),wi/\nLet y0 2 Y(x i ) \\ {y i } be such that h (x i , y0), wi is maximized. Then, (r 1)(pm 1)eh (xi,y0),wi/ upper bounds the left-hand side of the above equation. Taking log on both sides we get:\n> h (x i , y i ) (x i , y0), wi log((r 1)(pm 1))\nSince y i is the unique maximizer of the score h (x i , y i ), wi, (x\ni , y0) and (x i , y i\n) must differ on at least one element in the support set of w. This implies, from above and the assumption that the minimum non-zero element of (x, y) is at least 1:\n> w min\nlog((r 1)(pm 1)) ,\nwhich violates Assumption 1. Therefore from (14) we have that A\ni\n(w, S)  1/pm.\nCase II: 9y 6= y i : h (x i , y), wi h (x i , y i ), wi. Let\ni\n(y) def = (x i , y) (x i , y i ). In this case,\nA i\n(w, S) (a)  ETi ⇥ Pr f w, , ¯Ti(xi) = yi ⇤\n= ETi  exp(h (x i , y i\n), wi/ ) Z(w, x\ni , ¯T i )\n(b) = ETi\n\" 1\n1 + P y2Ti e h i(y),wi/\n#\n(c)  E Si\n 1\n1 + neSi/\n, (15)\nwhere we have defined S i def = 1\nn P y2Tih i(y), wi. In the\nabove, in step (a) we dropped the term Pr {f w, (x i ) = y i } to get an upper bound. Step (b) follows from dividing the numerator and denominator by exp(h (x\ni , y i ), wi) and that y i 2 ¯T i . Step (c) follows from Jensen’s inequality. Now,\nE Si\n 1\n1 + neSi/\n= E Si\n 1\n1 + neSi/ | S i kwk1 2 Pr\n⇢ S i\nkwk1 2\n+ E Si\n 1\n1 + neSi/ | S i\n< kwk 1\n2\nPr ⇢ S i < kwk 1\n2\n(a)  E Si\n 1\n1 + neSi/ | S i kwk1 2 + kwk 1p\nm (b)  E Si  1\n1 + neSi log m/kwk1 | S i kwk1 2 + kwk 1p m\n= E Si\n 1\n1 + nmSi/kwk1 | S i kwk1 2 + kwk 1p m\n 1 1 + n p m +\nkwk 1p\nm , (16)\nwhere inequality (a) follows from Assumption 1 and (b) follows from the fact that  kwk\n1 /logm. Thus from (15) and (16) we have that A\ni\n(w, S)  1/(1+npm) + kwk 1 /pm.\nThe final claim follows from Case I and II.\nNote that for c 0.5 the number of structured outputs needed is n = 1, while in the worst case (c = 0) n = p m. Furthermore, n needs to grow polynomially with respect to m in order to achieve O (1/pm) generalization error. Lemma 3 (Statistical Error). For any fixed data set S, the statistical error B(w, S, ¯T) is bounded, simultaneously for all proposal distributions R(x\ni , w) over {T i }, as follows: PrT (8w 2 Rd,s) B(w, S, ¯T)  \" 2 (d, s, n, r,m, ) | S\n1 , (17) where\n\" 2 (d, s, n, r,m, ) def = 2\nr s(ln d+ 2 ln(nr))\nm +\nr ln 1/\n2m +\nr s(ln d+ 2 ln(mr)) + ln 1/\n2m .\nThe proof of the above lemma is adapted from the proof of Rademacher based uniform convergence, and can be found in Appendix A in the supplementary material.\nNow, we are ready to present our main result proving uniform convergence of the randomized loss L(w, S, ¯T). More specifically, we provide eO (1/pm) generalization error. Theorem 2. With probability at least 1 2 over the choice of the data set S and the set of random structured outputs T, and simultaneously for all w 2 Rd,s and proposal distributions R(x,w):\nL(w,D)  L(w, S, ¯T) + \" 1 + \" 2 , (18)\nwhere \" 1 and \" 2 are defined in Lemma 2 and 3 respectively.\nProof. The claim follows directly from Lemma 2 and Lemma 3 by taking an expectation with respect to S."
  }, {
    "heading": "4.2. Examples of proposal distributions",
    "text": "Having proved uniform convergence of our randomized procedure for learning the parameters of a MAP decoder, we turn our attention to the proposal distribution. We want to construct proposal distributions of the form given by Definition 1 that satisfy Assumption 1 with a large enough constant c. Additionally, for our randomized procedure to run in polynomial time we want the proposal distribution to sample a structured output in constant time. The following algorithm is directly motivated by Assumption 1 where the set neighbors\nk (y) for an input x is defined as: neighbors\nk\n(y) def = {y0 2 Y(x) \\ {y} | H(y, y0)  k}, with H( · , ·) being the Hamming distance.\nAlgorithm 1 An example algorithm implementing a proposal distribution that depends on y\ni 2 S. 1: Input: Weight vector w 2 Rd,s, (x\ni , y i ) 2 S, parameter ↵ 2 [0, 1] and k 1. 2: Output: A structured output y 2 Y(x). 3: With probability ↵ pick y0 uniformly at random from\nY(x i ), and with probability 1 ↵ set y0 to y i\n. 4: y y0. 5: for y0 2 neighbors\nk (y) do 6: if h (x, y0), wi h (x, y), wi then 7: y y0. 8: end if 9: end for\n10: Return y.\nRemark 2. Setting ↵ = kwk 1 /pm, Algorithm 1 satisfies the condition given in Definition 1 as well as Assumption 1. Since, for any w,w0 2 Rd,s that induce the same linear ordering over Y(x), conditioned on the y0 sampled in Step 3, the algorithm returns the same y for both w and w0 with probability 1.\nAlso note that using a larger k ensures that the above algorithm satisfies Assumption 1 with a larger constant c, thereby reducing the number of structured outputs that need to be sampled (n), at the cost of increased computation for sampling a single structured output.\nThe parameter ↵ in Algorithm 1 controls exploration vs exploitation. As ↵ becomes smaller Algorithm 1 returns a proposal from within the neighborhood of y\ni while for larger ↵ it explores high scoring structures in the entire set of candidate structures.\nLastly, note that our results do not violate the hardness results of (Sontag et al., 2010), who essentially show that it is NP-hard to decide if the training data is linearly separable. Depending on whether or not the data is linearly separable, the loss L(w, S) (6) can be large or small (for all or some weight vector). While computing L(w, S) is intractable in\ngeneral, we merely provide an efficiently computable lower bound L(w, S,T) ((9)) that still upper bounds the expected loss L(w,D)."
  }, {
    "heading": "4.3. Minimizing the CRF loss",
    "text": "In this section we discuss strategies for minimizing the (randomized) CRF loss L(w, S, ¯T). Minimizing the randomized CRF loss L(w, S, ¯T) is equivalent to maximizing the randomized CRF gain U(w, S, ¯T) = 1\nm\nP m\ni=1\nPr f w, , ¯Ti(xi) = yi , which in turn is equivalent to maximizing logU(w, S, ¯T). The latter can be accomplished by gradient based methods with the gradient of logU(w, S, ¯T) given by:\nr w logU(w, S, ¯T) =\nP m\ni=1\nq i ( (x i , y i ) E [ (x i , y)])P m\ni=1\nq i\n,\n(19) where q\ni\ndef = Pr f w, , ¯Ti(xi) = yi , and the expectation is taken with respect to y ⇠ Q(x\ni , w, ¯T i ). The exact CRF loss (L(w, S)) can similarly be minimized by using ¯T\ni = Y(x i ), for all x\ni 2 S, in the above. Note that by Jensen’s inequality logU(w, S, ¯T) 1\nm\nP m\ni=1\nlog Pr f w, , ¯Ti(xi) = yi , where the latter can be identified as the log likelihood of the data set S under the CRF distributions {Q(x\ni , w, ¯T i )}. Therefore, L(w, S, ¯T) can be equivalently minimized by minimizing the negative log-likelihood of the data, which in turn gives rise to the well known moment-matching rule known in the literature (Papandreou & Yuille, 2011). Thus, Algorithm 1 can be used with standard moment matching where the expectation is approximated by averaging over y’s drawn from the distribution Q(x\ni , w, ¯T i ). While standard moment matching is in general intractable, moment matching in conjunction with Algorithm 1 is always efficient. Indeed, (19) can be thought of as a “weighted” moment matching rule with weights q\ni\n."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we evaluate our proposed method (CRF_RAND) on synthetic data against three other methods: CRF_ALL, SVM_RAND, and SVM. The CRF_RAND method minimizes the randomized loss L(w, S, ¯T) (9) subject to `\n1 penalty (as prescribed by Lemma 2) by sampling structured outputs from the proposal distribution given by Algorithm 1. The CRF_ALL method minimizes the exact (exponential-time) loss L(w, S) (6). Lastly, SVM is the widely used max-margin method of (Taskar et al., 2003), while SVM_RAND is the randomized SVM method proposed by (Honorio & Jaakkola, 2016).\nWe generate a ground truth parameter w⇤ 2 Rd with random entries sampled independently from a zero mean Gaussian distribution with variance 100. We then randomly set all but s = p d entries to be zero. We then generate\na training set of S of 100 samples. We used the following joint feature map (x, y) for an input output pair. For every pair of possible edges or elements i and j, we set (x, y)\ni,j = 1 [x i,j = 1 ^ i 2 y ^ j 2 y]. For instance, for directed spanning trees of v nodes, we have x 2 {0, 1}(v2) and (x, y) 2 R(v2). We considered directed spanning trees of 6 nodes, directed acyclic graphs of 5 nodes and 2 parents per node, and sets of 4 elements chosen from 15 possible elements. In order to generate each training sample (x, y) 2 S, we generated a random vector x with independent Bernoulli entries with parameter 1/2. After generating x, we set y = f\nw\n⇤ (x), i.e., we solved (1) in order to produce\nthe latent structured output y from the observed input x and the parameter w⇤.\nWe set the ` 1 regularization parameter to be 0.01 for all methods. We used 20 iterations of gradient descent with step size of 1/pt for all algorithms, where t is the iteration, to learn the parameter w for both the exact method and our randomized algorithm. In order to simplify gradient calculations, we simply set = 1/ log((r 1))(pm 1)) during training. For CRF_RAND, we used Algorithm 1 with ↵ = kwk\n1\n/pm and invoke the algorithm p m num-\nber of times to generate the set T i for each i 2 [m] and w. This results in n = |T\ni |  pm. To evaluate the generalization performance of our algorithm we generated a test set S0 = {x0\ni , y0 i }m i=1\nof 100 samples and calculated two losses. The first was the full CRF loss (6) on the test set S0, and the second was the test set hamming loss 1\nm\nP m\ni=1\nˆH(f ŵ (x0 i ), y0 i ), where ˆH( · , ·) is the normalized Hamming distance, and ŵ is the learned parameter. Hamming distance is a popular distortion function used in structured prediction, and provides a more realistic assessment of the performance of a decoder, since in most cases it suffices to recover most of the structure rather than predicting the structure exactly. For DAGs and trees the Hamming distance counts the number of different edges between the structured outputs, while for sets it counts the number of different elements. We normalize the Hamming distance to be between 0 and 1. We computed the mean and 95% confidence intervals of each of these metrics by repeating the above procedure 30 times.\nFigure 1 shows the training and test set errors and the training time of the four different algorithms. CRF_RAND significantly outperformed other algorithms in both the test set loss and test set hamming loss, while being ⇡ 6 times faster than the exact method (CRF_ALL) for DAGs, ⇡ 20 times faster for trees, and⇡ 3 times faster for sets. The exact CRF method (CRF_ALL) was also significantly faster than the exact SVM (SVM) method while achieving similar test set loss and test set hamming loss.\n7rDLn lRVV 7HVt lRVV 7HVt HDmm. lRVV\nDAGV\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n7rDLn lRVV 7HVt lRVV 7HVt HDmm. lRVV\n7rHHV\n7rDLn lRVV 7HVt lRVV 7HVt HDmm. lRVV\n6HtV\nC5F_5A1D C5F_ALL 6V0_5A1D 6V0_ALL\n7rDining time DAGs\n0.0\n50.0\n100.0\n150.0\n200.0\n6 e c o\nn G\ns\n10.04\n58.10\n10.19\n149.91\n7rDining time 7rees\n16.91\n339.76\n15.22\n934.94\n7rDining time 6ets\n21.15\n57.59\n7.32\n147.70\nFigure 1. (Left) Training and test set loss (6), and test set hamming loss of the exact method (CRF_ALL) and our randomized algorithm (CRF_RAND), the randomized SVM method by (Honorio & Jaakkola, 2016) (SVM_RAND), and the exact SVM (SVM_ALL), a.k.a max-margin, method of (Taskar et al., 2003). For the randomized algorithms, i.e., CRF_RAND and SVM_RAND, the training loss is the randomized training loss, i.e., L(w, S, T̄) and L(w, S,T) respectively. (Right) Training time in seconds for the various methods."
  }, {
    "heading": "6. Related Work",
    "text": "Significant body of work exists in computing a single MAP estimate by exploiting problem specific structure, for instance, super-modularity, linear programming relaxations to name a few. However, in this paper we are concerned with the problem of learning the parameters of MAP perturbation models. Among generalization bounds for MAP perturbation models, (Hazan et al., 2013b) prove PAC-Bayesian generalization bounds for weight based perturbations. (Hazan et al., 2013b) additionally propose learning weight based MAP-perturbation models by minimizing the PAC-Bayesian upper bound on the generalization error. However, their method for learning the parameters involves constructing restricted families of posterior distributions over the weights w that lead to smooth, but not necessarily convex, generalization bounds that can be optimized using gradient based methods. For learning MAP-perturbation models with structure based (Gumbel) perturbations, (Gane et al., 2014) propose a hard-EM algorithm which is both worstcase exponential time and has no theoretical guarantees. (Papandreou & Yuille, 2011) on the other hand, propose learning Gumbel MAP-perturbation models by using the moment matching method. However, such an approach is tractable only for energy functions for which the global minimum can be computed efficiently. Lastly, (Hazan et al., 2013a; Orabona et al., 2014) consider the problem of efficiently sampling from MAP perturbation models using low dimensional perturbations. (Hazan & Jaakkola, 2012; Hazan et al., 2013a) additionally propose ways to approximate and bound the partition function. While such bounds on the partition function can be used, in principle, to approximately minimize the CRF loss (6), it is unclear if one can obtain uniform convergence guarantees for the same, given that computing or even approximating the partition function\nis NP-hard (Barahona, 1982; Chandrasekaran et al., 2008)."
  }, {
    "heading": "7. Concluding remarks",
    "text": "We conclude with some directions for future work. While in this work we showed that one can learn with approximate inference, it would be interesting to analyze approximate inference for prediction on an independent test set. Another avenue for future work would be to develop more powerful proposal distributions that allow for more finer-grained control over the parameter c by exploiting problem specific structure like submodularity."
  }, {
    "heading": "Acknowledgements",
    "text": "This material is based upon work supported by the National Science Foundation under Grant No. 1716609-IIS."
  }],
  "year": 2018,
  "references": [{
    "title": "On the computational complexity of ising spin glass models",
    "authors": ["F. Barahona"],
    "venue": "Journal of Physics A: Mathematical and General,",
    "year": 1982
  }, {
    "title": "Determination of the number of independent parameters of a score matrix from the examination of rank",
    "authors": ["J.F. Bennett"],
    "venue": "orders. Psychometrika,",
    "year": 1956
  }, {
    "title": "Multidimensional unfolding: Determining the dimensionality of ranked preference data",
    "authors": ["J.F. Bennett", "W.L. Hays"],
    "year": 1960
  }, {
    "title": "Complexity of inference in graphical models",
    "authors": ["V. Chandrasekaran", "N. Srebro", "P. Harsha"],
    "venue": "In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence,",
    "year": 2008
  }, {
    "title": "The number of linearly inducible orderings of points in d-space",
    "authors": ["T.M. Cover"],
    "venue": "SIAM Journal on Applied Mathematics,",
    "year": 1967
  }, {
    "title": "Learning with maximum a-posteriori perturbation models",
    "authors": ["A. Gane", "T. Hazan", "T. Jaakkola"],
    "venue": "In Artificial Intelligence and Statistics, pp",
    "year": 2014
  }, {
    "title": "Statistical theory of extreme valuse and some practical applications",
    "authors": ["E.J. Gumbel"],
    "venue": "Nat. Bur. Standards Appl. Math. Ser",
    "year": 1954
  }, {
    "title": "Beating the random ordering is hard: Inapproximability of maximum acyclic subgraph",
    "authors": ["V. Guruswami", "R. Manokaran", "P. Raghavendra"],
    "venue": "In Foundations of Computer Science,",
    "year": 2008
  }, {
    "title": "On the partition function and random maximum a-posteriori perturbations",
    "authors": ["T. Hazan", "T. Jaakkola"],
    "venue": "In Proceedings of the 29th International Coference on International Conference on Machine Learning,",
    "year": 2012
  }, {
    "title": "On sampling from the gibbs distribution with random maximum a-posteriori perturbations",
    "authors": ["T. Hazan", "S. Maji", "T. Jaakkola"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Learning efficient random maximum a-posteriori predictors with non-decomposable loss functions",
    "authors": ["T. Hazan", "S. Maji", "J. Keshet", "T. Jaakkola"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Structured prediction: from gaussian perturbations to linear-time principled algorithms",
    "authors": ["J. Honorio", "T. Jaakkola"],
    "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"],
    "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,",
    "year": 2001
  }, {
    "title": "Generalization bounds and consistency",
    "authors": ["D. McAllester"],
    "venue": "Predicting structured data, pp. 247–261,",
    "year": 2007
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions—I",
    "authors": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "On measure concentration of random maximum a-posteriori perturbations",
    "authors": ["F. Orabona", "T. Hazan", "A. Sarwate", "T. Jaakkola"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models",
    "authors": ["G. Papandreou", "A.L. Yuille"],
    "venue": "In Computer Vision (ICCV),",
    "year": 2011
  }, {
    "title": "More data means less inference: A pseudo-max approach to structured learning",
    "authors": ["D. Sontag", "O. Meshi", "A. Globerson", "T.S. Jaakkola"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Max-margin Markov Networks",
    "authors": ["B. Taskar", "C. Guestrin", "D. Koller"],
    "venue": "In Proceedings of the 16th International Conference on Neural Information Processing Systems,",
    "year": 2003
  }],
  "id": "SP:41d94e51dedcac85f14f6c5f848c3ee789f09997",
  "authors": [{
    "name": "Asish Ghoshal",
    "affiliations": []
  }, {
    "name": "Jean Honorio",
    "affiliations": []
  }],
  "abstractText": "MAP perturbation models have emerged as a powerful framework for inference in structured prediction. Such models provide a way to efficiently sample from the Gibbs distribution and facilitate predictions that are robust to random noise. In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP predictors. Our approach is based on minimizing a novel Rademacher-based generalization bound on the expected loss of a perturbed MAP predictor, which can be computed in polynomial time. We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examples.",
  "title": "Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time"
}