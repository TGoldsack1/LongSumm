{
  "sections": [{
    "heading": "1. Introduction",
    "text": "It is well known that the optimization problem for training neural networks can have exponentially many local minima (Auer et al., 1996; Safran & Shamir, 2016) and NP-hardness has been shown in many cases (Blum & Rivest., 1989; Sima, 2002; Livni et al., 2014; Shamir, 2017; Shalev-Shwartz et al., 2017). However, it has been empirically observed (Dauphin et al., 2014; Goodfellow et al., 2015) that the training of state-of-the-art deep CNNs (LeCun et al., 1990; Krizhevsky et al., 2012), which are often overparameterized, is not hampered by suboptimal local minima.\nIn order to explain the apparent gap between hardness results and practical performance, many interesting theoret-\n1Department of Mathematics and Computer Science, Saarland University, Germany 2University of Tübingen, Germany. Correspondence to: Quynh Nguyen <quynh@cs.uni-saarland.de>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nical results have been recently developed (Andoni et al., 2014; Sedghi & Anandkumar, 2015; Janzamin et al., 2016; Haeffele & Vidal, 2017; Gautier et al., 2016; Brutzkus & Globerson, 2017; Soltanolkotabi, 2017; Zhong et al., 2017; Tian, 2017; Du et al., 2018) in order to identify conditions under which one can guarantee that local search algorithms like gradient descent converge to the globally optimal solution. However, it turns out that these approaches are either not practical as they require e.g. knowledge about the data generating measure, or a modification of network structure and objective, or they are for quite restricted network structures, mostly one hidden layer networks, and thus are not able to explain the success of deep networks in general. For deep linear networks one has achieved a quite complete picture of the loss surface as it has been shown that every local minimum is a global minimum (Baldi & Hornik, 1988; Kawaguchi, 2016; Freeman & Bruna, 2017; Hardt & Ma, 2017; Yun et al., 2018). By randomizing the nonlinear part of a feedforward network with ReLU activation function and making some additional simplifying assumptions, (Choromanska et al., 2015a) can relate the loss surface of neural networks to a certain spin glass model. In this model the objective of local minima is close to the global optimum and the number of bad local minima decreases quickly with the distance to the global optimum. This is a very interesting result but is based on a number of unrealistic assumptions (Choromanska et al., 2015b). More recently, (Nguyen & Hein, 2017) have analyzed deep fully connected networks with general activation functions and could show that almost every critical point is a global minimum if one layer has more neurons than the number of training points. While this result holds for networks in practice, it requires a quite extensively overparameterized network.\nIn this paper we overcome the restriction of previous work in several ways. This paper is one of the first ones, which analyzes the loss surface of deep CNNs. CNNs are of high practical interest as they learn very useful representations (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2015; Yosinski et al., 2015) with small number of parameters. We are only aware of (Cohen & Shashua, 2016) who study the expressiveness of CNNs with max-pooling layer and ReLU activation but with rather unrealistic filters (just 1× 1) and no shared weights. In our setting we allow as well max pooling and general activation functions. Moreover, we can have an arbitrary number of filters and we study general convolutions as the filters need not be applied to regular structures like 3×3 but can be patch-based where the only condition is that all the patches have the size of the filter. Convolutional layers, fully connected layers and max-pooling layers can be combined in almost arbitrary order. We study in this paper the expressiveness and loss surface of a CNN where one layer is wide, in the sense that it has more neurons than the number of training points. While this assumption sounds at first quite strong, we want to emphasize that the popular VGG (Simonyan & Zisserman, 2015) and Inception networks (Szegedy et al., 2015b; 2016), see Table 1, fulfill this condition. We show that wide CNNs produce linearly independent feature representations at the wide layer and thus are able to fit the training data exactly (universal finite sample expressivity). This is even true with probability one when all the parameters up to the wide layer are chosen randomly1. We think that this explains partially the results of (Zhang et al., 2017) where they show experimentally for several CNNs that they are able to fit random labels. Moreover, we provide necessary and sufficient conditions for global minima with zero squared loss and show for a particular class of CNNs that almost all critical points are globally optimal, which to some extent explains why wide CNNs can be optimized so efficiently. All proofs are moved to the appendix due to limited space."
  }, {
    "heading": "2. Deep Convolutional Neural Networks",
    "text": "We first introduce our notation and definition of CNNs. Let N be the number of training samples and denote by X = [x1, . . . , xN ]\nT ∈ RN×d, Y = [y1, . . . , yN ]T ∈ RN×m the input resp. output matrix for the training data (xi, yi)Ni=1, where d is the input dimension and m the number of classes.\nLet L be the number of layers of the network, where each layer is either a convolutional, max-pooling or fully connected layer. The layers are indexed from k = 0, 1, . . . , L which corresponds to input layer, 1st hidden layer, . . ., and output layer. Let nk be the width of layer k and fk : Rd → Rnk the function that computes for every input\n1for any probability measure on the parameter space which has a density with respect to the Lebesgue measure\nits feature vector at layer k.\nThe convolutional layer consists of a set of patches of equal size where every patch is a subset of neurons from the same layer. Throughout this paper, we assume that the patches of every layer cover the whole layer, i.e. every neuron belongs to at least one of the patches, and that there are no patches that contain exactly the same subset of neurons. This means that if one patch covers the whole layer then it must be the only patch of the layer. Let Pk and lk be the number of patches resp. the size of each patch at layer k for every 0 ≤ k < L. For every input x ∈ Rd, let{ x1, . . . , xP0 } ∈ Rl0 denote the set of patches at the input\nlayer and { f1k (x), . . . , f Pk k (x) } ∈ Rlk the set of patches at layer k. Each filter of the layer consists of the same set of patches. We denote by Tk the number of convolutional filters and by Wk = [w1k, . . . , w Tk k ] ∈ Rlk−1×Tk the corresponding parameter matrix of the convolutional layer k for every 1 ≤ k < L. Each column of Wk corresponds to one filter. Furthermore, bk ∈ Rnk denotes the bias vector and σk : R → R the activation function for each layer. Note that one can use the same activation function for all layers but we use the general form to highlight the role of different layers. In this paper, all functions are applied componentwise, and we denote by [a] the set of integers {1, 2, . . . , a} and by [a, b] the set of integers from a to b.\nDefinition 2.1 (Convolutional layer) A layer k is called a convolutional layer if its output fk(x) ∈ Rnk is defined for every x ∈ Rd as\nfk(x)h = σk ( 〈 wtk, f p k−1(x) 〉 + (bk)h ) (1)\nfor every p ∈ [Pk−1], t ∈ [Tk], h := (p− 1)Tk + t.\nThe value of each neuron at layer k is computed by first taking the inner product between a filter of layer k and a patch at layer k − 1, adding the bias and then applying the activation function. The number of neurons at layer k is thus nk = TkPk−1, which we denote as the width of layer k. Our definition of a convolutional layer is quite general as every patch can be an arbitrary subset of neurons of the same layer and thus covers most of existing variants in practice.\nDefinition 2.1 includes the fully connected layer as a special case by using Pk−1 = 1, lk−1 = nk−1, f1k−1(x) = fk−1(x) ∈ Rnk−1 , Tk = nk,Wk ∈ Rnk−1×nk , bk ∈ Rnk . Thus we have only one patch which is the whole feature vector at this layer.\nDefinition 2.2 (Fully connected layer) A layer k is called a fully connected layer if its output fk(x) ∈ Rnk is defined for every x ∈ Rd as\nfk(x) = σk ( WTk fk−1(x) + bk ) . (2)\nFor some results we also allow max-pooling layers.\nDefinition 2.3 (Max-pooling layer) A layer k is called a max-pooling layer if its output fk(x) ∈ Rnk is defined for every x ∈ Rd and p ∈ [Pk−1] as\nfk(x)p = max (\n(fpk−1(x))1, . . . , (f p k−1(x))lk−1\n) . (3)\nA max-pooling layer just computes the maximum element of every patch from the previous layer. Since there are Pk−1 patches at layer k−1, the number of output neurons at layer k is nk = Pk−1.\nReformulation of Convolutional Layers: For each convolutional or fully connected layer, we denote by Mk : Rlk−1×Tk → Rnk−1×nk the linear map that returns for every parameter matrix Wk ∈ Rlk−1×Tk the corresponding full weight matrix Uk =Mk(Wk) ∈ Rnk−1×nk . For convolutional layers, Uk can be seen as the counterpart of the weight matrix Wk in fully connected layers. We define Uk =Mk(Wk) = Wk if layer k is fully connected. Note that the mappingMk depends on the patch structure of each convolutional layer k. For example, suppose that layer k has\ntwo filters of length 3, that is, Wk = [w1k, w 2 k] = a db e c f , and nk−1 = 5 and patches given by a 1D-convolution with stride 1 and no padding then:\nUTk =Mk(Wk)T =  a b c 0 0 d e f 0 0 0 a b c 0 0 d e f 0 0 0 a b c 0 0 d e f  .\nThe above ordering of the rows of UTk of a convolutional layer is determined by (1), in particular, the row index h of UTk is calculated as h = (p − 1)Tk + t, which means for every given patch p one has to loop over all the filters t and compute the corresponding value of the output unit by taking the inner product of the h-th row of UTk with the whole feature vector of the previous layer. We assume throughout this paper that that there is no non-linearity at the output layer. By ignoring max-pooling layers for the moment, the feature maps fk : Rd → Rnk can be written as\nfk(x) =  x k = 0 σk ( gk(x) ) 1 ≤ k ≤ L− 1\ngL(x) k = L\nwhere gk : Rd → Rnk is the pre-activation function:\ngk(x) = U T k fk−1(x) + bk, ∀1 ≤ k ≤ L\nBy stacking the feature vectors of layer k of all training samples, before and after applying the activation function, into a matrix, we define:\nFk = [fk(x1), . . . , fk(xN )] T ∈ RN×nk , Gk = [gk(x1), . . . , gk(xN )] T ∈ RN×nk .\nIn this paper, we refer to Fk as the output matrix at layer k. It follows from above that\nFk =  X k = 0\nσk(Gk) 1 ≤ k ≤ L− 1 GL k = L\n(4)\nwhere Gk = Fk−1Uk + 1NbTk for every 1 ≤ k ≤ L.\nIn this paper, we assume the following general condition on the structure of convolutional layers.\nAssumption 2.4 (Convolutional Structure) For every convolutional layer k, there exists at least one parameter matrix Wk ∈ Rlk−1×Tk for which the corresponding weight matrix Uk =Mk(Wk) ∈ Rnk−1×nk has full rank.\nIt is straightforward to see that Assumption 2.4 is satisfied if every neuron belongs to at least one patch and there are no identical patches. As the set of full rank matrices is a dense subset, the following result follows immediately.\nLemma 2.5 If Assumption 2.4 holds, then for every convolutional layer k, the set of Wk ∈ Rlk−1×Tk for which Uk = Mk(Wk) ∈ Rnk−1×nk does not have full rank has Lebesgue measure zero."
  }, {
    "heading": "3. CNN Learn Linearly Independent Features",
    "text": "In this section, we show that a class of standard CNN architectures with convolutional layers, fully connected layers and max-pooling layers plus standard activation functions like ReLU, sigmoid, softplus, etc are able to learn linearly independent features at every wide hidden layer if it has more neurons than the number of training samples. Our assumption on training data is the following.\nAssumption 3.1 (Training data) The patches of different training samples are non-identical, that is, xpi 6= x q j for every p, q ∈ [P0], i, j ∈ [N ], i 6= j.\nAssumption 3.1 is quite weak, especially if the size of the input patches is large. If the assumption does not hold, one can add a small perturbation to the training samples: {x1 + 1, . . . , xN + N} . The set of { i}Ni=1 where Assumption 3.1 is not fulfilled for the new dataset has measure zero. Moreover, { i}Ni=1 can be chosen arbitrarily small so that the influence of the perturbation is negligible. Our main assumptions on the activation function of the hidden layers are the following.\nAssumption 3.2 (Activation function) The activation function σ is continuous, non-constant, and satisfies one of the following conditions:\n• There exist µ+, µ− ∈ R s.t. lim t→−∞ σk(t) = µ− and\nlim t→∞ σk(t) = µ+ and µ+µ− = 0\n• There exist ρ1, ρ2, ρ3, ρ4 ∈ R+ s.t. |σ(t)| ≤ ρ1eρ2t for t < 0 and |σ(t)| ≤ ρ3t+ ρ4 for t ≥ 0\nAssumption 3.2 covers several standard activation functions.\nLemma 3.3 The following activation functions satisfy Assumption 3.2:\n• ReLU: σ(t) = max(0, t)\n• Sigmoid: σ(t) = 11+e−t\n• Softplus: σα(t) = 1α ln(1 + e αt) for some α > 0\nIt is known that the softplus function is a smooth approximation of ReLU. In particular, it holds that:\nlim α→∞ σα(t) = lim α→∞\n1 α ln(1 + eαt) = max(0, t). (5)\nThe first main result of this paper is the following.\nTheorem 3.4 (Linearly Independent Features) Let Assumption 3.1 hold for the training sample. Consider a deep CNN architecture for which there exists some layer 1 ≤ k ≤ L− 1 such that\n1. Layer 1 and layer k are convolutional or fully connected while all the other layers can be convolutional, fully connected or max-pooling\n2. The width of layer k is larger than the number of training samples, nk = TkPk−1 ≥ N\n3. (σ1, . . . , σk) satisfy Assumption 3.2\nThen there exists a set of parameters of the first k layers (Wl, bl)kl=1 such that the set of feature vectors {fk(x1), . . . , fk(xN )} are linearly independent. Moreover, (Wl, bl) k l=1 can be chosen in such a way that all the weight matrices Ul =Ml(Wl) have full rank for every 1 ≤ l ≤ k.\nTheorem 3.4 implies that a large class of CNNs employed in practice with standard activation functions like ReLU, sigmoid or softplus can produce linearly independent features at any hidden layer if its width is larger than the size of training set. Figure 1 shows an example of a CNN architecture that satisfies the conditions of Theorem 3.4 at\nthe first convolutional layer. Note that if a set of vectors is linearly independent then they are also linearly separable. In this sense, Theorem 3.4 suggests that CNNs can produce linearly separable features at every wide hidden layer.\nLinear separability in neural networks has been recently studied by (An et al., 2015), where the authors show that a two-hidden-layer fully connected network with ReLU activation function can transform any training set to be linearly separable while approximately preserving the distances of the training data at the output layer. Compared to (An et al., 2015) our Theorem 3.4 is derived for CNNs with a wider range of activation functions. Moreover, our result shows even linear independence of features which is stronger than linear separability. Recently, (Nguyen & Hein, 2017) have shown a similar result for fully connected networks and analytic activation functions.\nWe want to stress that, in contrast to fully connected networks, for CNNs the condition nk ≥ N of Theorem 3.4 does not imply that the network has a huge number of parameters as the layers k and k + 1 can be chosen to be convolutional. In particular, the condition nk = TkPk−1 ≥ N can be fulfilled by increasing the number of filters Tk or by using a large number of patches Pk−1 (however Pk−1 is upper bounded by nk), which is however only possible if lk−1 is small as otherwise our condition on the patches cannot be fulfilled. In total the CNN has only lk−1Tk + lkTk+1 parameters versus nk(nk−1 +nk+1) for the fully connected network from and to layer k. Interestingly, the VGG-Net (Simonyan & Zisserman, 2015), where in the first layer small 3× 3 filters and stride 1 is used, fulfills for ImageNet the condition nk ≥ N for k = 1, as well as the Inception networks (Szegedy et al., 2015b; 2016), see Table 1.\nOne might ask now how difficult it is to find such parameters which generate linearly independent features at a hidden layer? Our next result shows that once analytic2 activation functions, e.g. sigmoid or softplus, are used at the first k hidden layers of the network, the linear independence of features at layer k holds with probability 1 even if one draws the parameters of the first k layers (Wl, bl)k randomly for any probability measure on the parameter space which has a density with respect to the Lebesgue measure.\nTheorem 3.5 Let Assumption 3.1 hold for the training samples. Consider a deep CNN for which there exists some layer 1 ≤ k ≤ L− 1 such that\n1. Every layer from 1 to k is convolutional or fully connected\n2. The width of layer k is larger than number of training\n2A function σ : R → R is real analytic if its Taylor series about x0 converges to σ(x0) on some neighborhood of x0 for every x0 ∈ R (Krantz & Parks, 2002).\nsamples, that is, nk = TkPk−1 ≥ N\n3. (σ1, . . . , σk) are real analytic functions and satisfy Assumption 3.2.\nThen the set of parameters of the first k layers (Wl, bl)kl=1 for which the set of feature vectors {fk(x1), . . . , fk(xN )} are not linearly independent has Lebesgue measure zero.\nTheorem 3.5 is a much stronger statement than Theorem 3.4, as it shows that for almost all weight configurations one gets linearly independent features at the wide layer. While Theorem 3.5 does not hold for the ReLU activation function as it is not an analytic function, we note again that one can approximate the ReLU function arbitrarily well using the softplus function (see 5), which is analytic function for any α > 0 and thus Theorem 3.5 applies. It is an open question if the result holds also for the ReLU activation function itself. The condition nk ≥ N is not very restrictive as several state-of-the art CNNs , see Table 1, fulfill the condition. Furthermore, we would like to stress that Theorem 3.5 is not true for deep linear networks. The reason is simply that the rank of a product of matrices can at most be the minimal rank among all the matrices. The nonlinearity of the activation function is thus critical (note that the identity activation function, σ(x) = x, does not fulfill Assumption 3.2).\nTo illustrate Theorem 3.5 we plot the rank of the feature matrices of the network in Figure 1. We use the MNIST dataset with N = 60000 training and 10000 test samples. We add small Gaussian noise N (0, 10−5) to every training sample so that Assumption 3.1 is fulfilled. We then vary the number of convolutional filters T1 of the first layer from 10 to 100 and train the corresponding network with squared loss and sigmoid activation function using Adam (Kingma & Ba, 2015) and decaying learning rate for 2000 epochs. In Table 2 we show the smallest singular value of the feature matrices together with the corresponding training loss, training and test error. If number of convolutional filters is large enough (i.e. T1 ≥ 89), one has n1 = 26× 26× T1 ≥ N = 60000, and the second condition of Theorem 3.5 is satisfied for k = 1. Table 2 shows that the feature matrices F1 have full rank in all cases (and F3 in almost all cases), in particular for T1 ≥ 89 as shown in Theorem 3.5. As expected when the feature maps of the training samples are linearly independent after the first layer (F1 has rank 60000 for T ≥ 89) the training error is zero and the training loss is close to zero (the GPU uses single precision). However, as linear independence is stronger than linear separability one can achieve already for T < 89 zero training error.\nIt is interesting to note that Theorem 3.5 explains previous empirical observations. In particular, (Czarnecki et al., 2017) have shown empirically that linear separability is often obtained already in the first few hidden layers of the\ntrained networks. This is done by attaching a linear classifier probe (Alain & Bengio, 2016) to every hidden layer in the network after training the whole network with backpropagation. The fact that Theorem 3.5 holds even if the parameters of the bottom layers up to the wide layer k are chosen randomly is also in line with recent empirical observations for CNN architectures that one has little loss in performance if the weights of the initial layers are chosen randomly without training (Jarrett et al., 2009; Saxe et al., 2011; Yosinski et al., 2014).\nAs a corollary of Theorem 3.4 we get the following universal finite sample expressivity for CNNs. In particular, a deep CNN with scalar output can perfectly fit any scalar-valued function for a finite number of inputs if the width of the last hidden layer is larger than the number of training samples.\nCorollary 3.6 (Universal Finite Sample Expressivity) Let Assumption 3.1 hold for the training samples. Consider a standard CNN with scalar output which satisfies the conditions of Theorem 3.4 at the last hidden layer k = L− 1. Let fL : Rd → R be the output of the network given as\nfL(x) = nL−1∑ j=1 λjf(L−1)j(x) ∀x ∈ Rd\nwhere λ ∈ RnL−1 is the weight vector of the last layer. Then for every target y ∈ RN , there exists { λ, (Wl, bl) L−1 l=1 } so that it holds fL(xi) = yi for every i ∈ [N ].\nThe expressivity of neural networks has been well-studied, in particular in the universal approximation theorems for one hidden layer networks (Cybenko, 1989; Hornik et al., 1989). Recently, many results have shown why deep networks are superior to shallow networks in terms of expressiveness (Delalleau & Bengio, 2011; Telgarsky, 2016; 2015; Eldan & Shamir, 2016; Safran & Shamir, 2017; Yarotsky, 2016; Poggio et al., 2016; Liang & Srikant, 2017; Mhaskar & Poggio, 2016; Montufar et al., 2014; Pascanu et al., 2014; Raghu et al., 2017) While most of these results are derived for fully connected networks, it seems that (Cohen & Shashua, 2016) are the first ones who study expressivity of CNNs. In particular, they show that CNNs with max-pooling and ReLU units are universal in the sense that they can approximate any given function if the size of the networks is unlimited. However, the number of convolutional filters in this result has to grow exponentially with the number of patches and they do not allow shared weights in their result, which is a standard feature of CNNs. Corollary 3.6 shows universal finite sample expressivity, instead of universal function approximation, even for L = 2 and k = 1, that is a single convolutional layer network can perfectly fit the training data as long as the number of hidden units is larger than the number of training samples.\nFor fully connected networks, universal finite sample ex-\npressivity has been studied by (Zhang et al., 2017; Nguyen & Hein, 2017; Hardt & Ma, 2017). It is shown that a single hidden layer fully connected network with N hidden units can express any training set of size N . While the number of training parameters of a single hidden layer CNN with N hidden units and scalar output is just 2N + T1l0, where T1 is the number of convolutional filters and l0 is the size of each filter, it is Nd + 2N for fully connected networks. If we set the width of the hidden layer of the CNN as n1 = T1P0 = N in order to fulfill the condition of Corollary 3.6, then the number of training parameters of the CNN becomes 2N +Nl0/P0, which is less than 3N if l0 ≤ P0 compared to (d+ 2)N for the fully connected case. In practice one almost always has l0 ≤ P0 as l0 is typically a small integer and P0 is on the order of the dimension of the input. Thus, the number of parameters to achieve universal finite sample expressivity is for CNNs significantly smaller than for fully connected networks.\nObviously, in practice it is most important that the network generalizes rather than just fitting the training data. By using shared weights and sparsity structure, CNNs seem to implicitly regularize the model to achieve good generalization performance. Thus even though they can fit also random labels or noise (Zhang et al., 2017) due to the universal finite sample expressivity shown in Corollary 3.6, they seem still to be able to generalize well (Zhang et al., 2017)."
  }, {
    "heading": "4. Optimization Landscape of Deep CNNs",
    "text": "In this section, we restrict our analysis to the use of least squares loss. However, as we show later that the network can produce exactly the target output (i.e. FL = Y ) for some choice of parameters, all our results can also be extended to any other loss function where the global minimum is attained at FL = Y , for instance the squared Hinge-loss analyzed in (Nguyen & Hein, 2017). Let P denote the space\nof all parameters of the network. The final training objective Φ : P → R is given as\nΦ (\n(Wl, bl) L l=1\n) = 1\n2 ‖FL − Y ‖2F (6)\nwhere FL is defined as in (4), which is also the same as\nFL = σL−1(. . . σ1(XU1 + 1Nb T 1 ) . . .)UL + 1Nb T L,\nwhere Ul =Ml(Wl) for every 1 ≤ l ≤ L. We require the following assumptions on the architecture of CNN.\nAssumption 4.1 (CNN Architecture) Every layer in the network is a convolutional layer or fully connected layer and the output layer is fully connected. Moreover, there exists some hidden layer 1 ≤ k ≤ L − 1 such that the following holds:\n• The width of layer k is larger than number of training samples, that is, nk = TkPk−1 ≥ N\n• All the activation functions of the hidden layers (σ1, . . . , σL−1) satisfy Assumption 3.2\n• (σk+1, . . . , σL−1) are strictly increasing or strictly decreasing, and differentiable\n• The network is pyramidal from layer k + 1 till the output layer, that is, nk+1 ≥ . . . ≥ nL\nA typical example that satisfies Assumption 4.1 with k = 1 can be found in Figure 1 where one disregards max-pooling layers and uses e.g. sigmoid or softplus activation function.\nIn the following, let us define for every 1 ≤ k ≤ L− 1 the subset Sk ⊆ P of the parameter space such that\nSk := { (Wl, bl) L l=1 ∣∣ Fk, Uk+2, . . . , ULhave full rank} .\nThe set Sk is the set of parameters where the feature matrix at layer k and all the weight matrices from layer k+2 till the output layer have full rank. In the following, we examine conditions for global optimality in Sk. It is important to note that Sk covers almost the whole parameter space under an additional mild condition on the activation function.\nLemma 4.2 Let Assumption 3.1 hold for the training samples and a deep CNN satisfy Assumption 4.1 for some layer 1 ≤ k ≤ L− 1. If the activation functions of the first k layers (σ1, . . . , σk) are real analytic, then the complementary set P \\ Sk has Lebesgue measure zero.\nIn the next key lemma, we bound the objective function in terms of its gradient magnitude w.r.t. the weight matrix of layer k for which nk ≥ N. For every matrix A ∈ Rm×n, let σmin(A) and σmax(A) denote the smallest and largest\nsingular value of A. Let ‖A‖F = √∑ i,j A 2 ij , ‖A‖min := min i,j |Aij | and ‖A‖max := maxi,j |Aij |. From (4), and (6), it follows that Φ can be seen as a function of (Ul, bl)Ll=1, and thus we can use ∇UkΦ. If layer k is fully connected then Uk = Mk(Wk) = Wk and thus ∇UkΦ = ∇WkΦ. Otherwise, if layer k is convolutional then we note that ∇UkΦ is “not” the true gradient of the training objective because Uk is not the true optimization parameter but Wk. In this case, the true gradient of Φ w.r.t. to the true parameter matrix Wk which consists of convolutional filters can be computed via the chain rule as\n∂Φ ∂(Wk)rs = ∑ i,j ∂Φ ∂(Uk)ij ∂(Uk)ij ∂(Wk)rs\nPlease note that even though we write the partial derivatives with respect to the matrix elements,∇WkΦ resp. ∇UkΦ are the matrices of the same dimension as Wk resp. Uk in the following.\nLemma 4.3 Consider a deep CNN satisfying Assumption\n4.1 for some hidden layer 1 ≤ k ≤ L− 1. Then it holds∥∥∇Uk+1Φ∥∥F ≥σmin(Fk)\n( L−1∏ l=k+1 σmin(Ul+1) ‖σ′l(Gl)‖min ) ‖FL − Y ‖F\nand∥∥∇Uk+1Φ∥∥F ≤σmax(Fk)\n( L−1∏ l=k+1 σmax(Ul+1) ‖σ′l(Gl)‖max ) ‖FL − Y ‖F .\nOur next main result is motivated by the fact that empirically when training over-parameterized neural networks with shared weights and sparsity structure like CNNs, there seem to be no problems with sub-optimal local minima. In many cases, even when training labels are completely random, local search algorithms like stochastic gradient descent can converge to a solution with almost zero training error (Zhang et al., 2017). To understand better this phenomenon, we first characterize in the following Theorem 4.4 the set of points in parameter space with zero loss, and then analyze in Theorem 4.5 the loss surface for a special case of the network. We emphasize that our results hold for standard deep CNNs with convolutional layers with shared weights and fully connected layers.\nTheorem 4.4 (Conditions for Zero Training Error) Let Assumption 3.1 hold for the training sample and suppose that the CNN architecture satisfies Assumption 4.1 for some hidden layer 1 ≤ k ≤ L− 1. Let Φ : P → R be defined as in (6). Given any point (Wl, bl)Ll=1 ∈ Sk. Then it holds that Φ (\n(Wl, bl) L l=1 ) = 0 if and only if∇Uk+1Φ ∣∣∣ (Wl,bl)Ll=1 = 0.\nProof: If Φ (\n(Wl, bl) L l=1\n) = 0 then it follows from\nthe upper bound of Lemma 4.3 that ∇Uk+1Φ = 0. For reverse direction, one has (Wl, bl)Ll=1 ∈ Sk and thus\nrank(Fk) = N and Ul has full rank for every l ∈ [k+2, L]. Thus it holds σmin(Fk) > 0 and σmin(Ul) > 0 for every l ∈ [k + 2, L]. Moreover, (σk+1, . . . , σL−1) have non-zero derivative by Assumption 4.1 and thus ‖σ′l(Gl)‖min > 0 for every l ∈ [k+1, L−1]. This combined with the lower bound in Lemma 4.3 leads to Φ ( Wl, bl) L l=1 ) = ‖FL − Y ‖F = 0.\nLemma 4.2 shows that the set of points which are not covered by Theorem 4.4 has measure zero if the first k layers have analytic activation functions. The necessary and sufficient condition of Theorem 4.4 is rather intuitive as it requires the gradient of the training objective to vanish w.r.t. the full weight matrix of layer k + 1 regardless of the architecture of this layer. It turns out that if layer k + 1 is fully connected, then this condition is always satisfied at a critical point, in which case we obtain that every critical point in Sk is a global minimum with exact zero training error. This is shown in the next Theorem 4.5, where we consider a classification task with m classes, Z ∈ Rm×m is the full rank class encoding matrix e.g. the identity matrix and (X,Y ) the training sample such that Yi: = Zj: whenever the training sample xi belongs to class j for every i ∈ [N ], j ∈ [m].\nTheorem 4.5 (Loss Surface of CNNs) Let (X,Y, Z) be a training set for which Assumption 3.1 holds, the CNN architecture satisfies Assumption 4.1 for some hidden layer 1 ≤ k ≤ L − 1, and layer k + 1 is fully connected. Let Φ : P → R be defined as in (6). Then the following holds\n• Every critical point (Wl, bl)Ll=1 ∈ Sk is a global minimum with Φ ( (Wl, bl) L l=1 ) = 0\n• There exist infinitely many global minima (Wl, bl) L l=1 ∈ Sk with Φ ( (Wl, bl) L l=1 ) = 0\nTheorem 4.5 shows that the loss surface for this type of CNNs has a rather simple structure in the sense that every critical point in Sk must be a global minimum with zero training error. Note that if the activation functions up to layer k are analytic, the complement of Sk has measure zero (see Lemma 4.2). For those critical points lying outside Sk, it must hold that either one of the weight matrices {Uk+2, . . . , UL} has low rank or the set of feature vectors at layer k is not linearly independent (i.e. Fk has low rank). Obviously, some of these critical points can also be global minima, but we conjecture that they cannot be suboptimal local minima due to the following reasons. First, it seems unlikely that a critical point with a low rank weight matrix is a suboptimal local minimum as this would imply that all possible full rank perturbations of the current solution must have larger/equal objective value. However, there is no term\nin the loss function which favors low rank solutions. Even for linear networks, it has been shown by (Baldi & Hornik, 1988) that all the critical points with low rank weight matrices have to be saddle points and thus cannot be suboptimal local minima. Second, a similar argument applies to the case where one has a critical point outside Sk such that the features are not linearly independent. In particular, any neighborhood of such a critical point contains points which have linearly independent features at layer k, from which it is easy to reach zero loss if one fixes the parameters of the first k layers and optimizes the loss w.r.t. the remaining ones. This implies that every small neighborhood of the critical point should contain points from which there exists a descent path that leads to a global minimum with zero loss, which contradicts the fact that the critical point is a suboptimal local minimum. In summary, if there are critical points lying outside the set Sk, then it is very “unlikely” that these are suboptimal local minima but rather also global minima, saddle points or local maxima.\nIt remains an interesting open problem if the result of Theorem 4.5 can be transferred to the case where layer k + 1 is also convolutional. In any case whether layer k + 1 is fully connected or not, one might assume that a solution with zero training error still exists as it is usually the case for practical over-parameterized networks. However, note that Theorem 4.4 shows that at those points where the loss is zero, the gradient of Φ w.r.t. Uk+1 must be zero as well.\nAn interesting special case of Theorem 4.5 is when the network is fully connected in which case all the results of Theorem 4.5 hold without any modifications. This can be seen as a formal proof for the implicit assumption used in the recent work (Nguyen & Hein, 2017) that there exists a global minimum with absolute zero training error for the class of fully connected, deep and wide networks."
  }, {
    "heading": "5. Conclusion",
    "text": "We have analyzed the expressiveness and loss surface of CNNs in realistic and practically relevant settings. As stateof-the-art networks fulfill exactly or approximately the condition to have a sufficiently wide convolutional layer, we think that our results help to understand why current CNNs can be trained so effectively. It would be interesting to discuss the loss surface for cross-entropy loss, which currently does not fit into our analysis as the global minimum does not exist when the data is linearly separable."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors would like to thank the reviewers for their helpful comments on the paper and Maksym Andriushchenko for helping us to set up the experiment on the rank of features."
  }],
  "year": 2018,
  "references": [{
    "title": "Understanding intermediate layers using linear classifier probes",
    "authors": ["G. Alain", "Y. Bengio"],
    "venue": "In ICLR Workshop,",
    "year": 2016
  }, {
    "title": "How can deep rectifier networks achieve linear separability and preserve distances",
    "authors": ["S. An", "F. Boussaid", "M. Bennamoun"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Learning polynomials with neural networks",
    "authors": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Exponentially many local minima for single neurons",
    "authors": ["P. Auer", "M. Herbster", "M.K. Warmuth"],
    "venue": "In NIPS,",
    "year": 1996
  }, {
    "title": "Neural networks and principle component analysis: Learning from examples without local minima",
    "authors": ["P. Baldi", "K. Hornik"],
    "venue": "Neural Networks,",
    "year": 1988
  }, {
    "title": "Training a 3-node neural network is np-complete",
    "authors": ["A. Blum", "R.L. Rivest"],
    "venue": "In NIPS,",
    "year": 1989
  }, {
    "title": "Globally optimal gradient descent for a convnet with gaussian inputs",
    "authors": ["A. Brutzkus", "A. Globerson"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Xception: Deep learning with depthwise separable convolutions, 2016",
    "authors": ["F. Chollet"],
    "year": 2016
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Open problem: The landscape of the loss surfaces of multilayer networks. COLT, 2015b",
    "authors": ["A. Choromanska", "Y. LeCun", "G.B. Arous"],
    "year": 2015
  }, {
    "title": "Convolutional rectifier networks as generalized tensor decompositions",
    "authors": ["N. Cohen", "A. Shashua"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Approximation by superpositions of a sigmoidal function",
    "authors": ["G. Cybenko"],
    "venue": "Mathematics of Control, Signals, and Systems,",
    "year": 1989
  }, {
    "title": "Understanding synthetic gradients and decoupled neural interfaces",
    "authors": ["W.M. Czarnecki", "G. Swirszcz", "M. Jaderberg", "S. Osindero", "O. Vinyals", "K. Kavukcuoglu"],
    "year": 2017
  }, {
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "authors": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Shallow vs. deep sum-product networks",
    "authors": ["O. Delalleau", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "When is a convolutional filter easy to learn",
    "authors": ["S.S. Du", "J.D. Lee", "Y. Tian"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "The power of depth for feedforward neural networks",
    "authors": ["R. Eldan", "O. Shamir"],
    "venue": "In COLT,",
    "year": 2016
  }, {
    "title": "Topology and geometry of half-rectified network optimization",
    "authors": ["C.D. Freeman", "J. Bruna"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods",
    "authors": ["A. Gautier", "Q. Nguyen", "M. Hein"],
    "year": 2016
  }, {
    "title": "Qualitatively characterizing neural network optimization problems",
    "authors": ["I.J. Goodfellow", "O. Vinyals", "A.M. Saxe"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Global optimality in neural network training",
    "authors": ["B.D. Haeffele", "R. Vidal"],
    "venue": "In CVPR,",
    "year": 2017
  }, {
    "title": "Identity matters in deep learning",
    "authors": ["M. Hardt", "T. Ma"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "year": 2016
  }, {
    "title": "Multilayer feedforward networks are universal approximators",
    "authors": ["K. Hornik", "M. Stinchcombe", "H. White"],
    "venue": "Neural Networks,",
    "year": 1989
  }, {
    "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and < 0.5mb model size, 2016",
    "authors": ["F.N. Iandola", "S. Han", "M.W. Moskewicz", "K. Ashraf", "W.J. Dally", "K. Keutzer"],
    "year": 2016
  }, {
    "title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
    "authors": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"],
    "year": 2016
  }, {
    "title": "What is the best multi-stage architecture for object recognition",
    "authors": ["K. Jarrett", "K. Kavukcuoglu", "Y. LeCun"],
    "venue": "In CVPR,",
    "year": 2009
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J.L. Ba"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Handwritten digit recognition with a back-propagation network",
    "authors": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"],
    "venue": "In NIPS,",
    "year": 1990
  }, {
    "title": "Why deep neural networks for function approximation",
    "authors": ["S. Liang", "R. Srikant"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "On the computational efficiency of training neural networks",
    "authors": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Understanding deep image representations by inverting them",
    "authors": ["A. Mahendran", "A. Vedaldi"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "Deep vs. shallow networks : An approximation theory perspective, 2016",
    "authors": ["H. Mhaskar", "T. Poggio"],
    "year": 2016
  }, {
    "title": "On the number of linear regions of deep neural networks",
    "authors": ["G. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "The loss surface of deep and wide neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "On the number of response regions of deep feedforward networks with piecewise linear activations",
    "authors": ["R. Pascanu", "G. Montufar", "Y. Bengio"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Enet: A deep neural network architecture for real-time semantic segmentation, 2016",
    "authors": ["A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello"],
    "year": 2016
  }, {
    "title": "Why and when can deep – but not shallow – networks avoid the curse of dimensionality: a review, 2016",
    "authors": ["T. Poggio", "H. Mhaskar", "L. Rosasco", "B. Miranda", "Q. Liao"],
    "year": 2016
  }, {
    "title": "Numerical Recipes: The art of scientific computing",
    "authors": ["W.H. Press"],
    "year": 2007
  }, {
    "title": "On the expressive power of deep neural networks",
    "authors": ["M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. SohlDickstein"],
    "year": 2017
  }, {
    "title": "On the quality of the initial basin in overspecified networks",
    "authors": ["I. Safran", "O. Shamir"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Depth-width tradeoffs in approximating natural functions with neural networks",
    "authors": ["I. Safran", "O. Shamir"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "On random weights and unsupervised feature learning",
    "authors": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Provable methods for training neural networks with sparse connectivity",
    "authors": ["H. Sedghi", "A. Anandkumar"],
    "venue": "In ICLR Workshop,",
    "year": 2015
  }, {
    "title": "Failures of gradient-based deep learning",
    "authors": ["S. Shalev-Shwartz", "O. Shamir", "S. Shammah"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Distribution-specific hardness of learning neural networks, 2017",
    "authors": ["O. Shamir"],
    "year": 2017
  }, {
    "title": "Training a single sigmoidal neuron is hard",
    "authors": ["J. Sima"],
    "venue": "Neural Computation,",
    "year": 2002
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Learning relus via gradient descent",
    "authors": ["M. Soltanolkotabi"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Rethinking the inception architecture for computer vision, 2015b",
    "authors": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"],
    "year": 2015
  }, {
    "title": "Inception-v4, inception-resnet and the impact of residual connections on learning, 2016",
    "authors": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke", "A. Alemi"],
    "year": 2016
  }, {
    "title": "Representation benefits of deep feedforward networks, 2015. arXiv:1509.08101v2",
    "authors": ["M. Telgarsky"],
    "year": 2015
  }, {
    "title": "Benefits of depth in neural networks",
    "authors": ["M. Telgarsky"],
    "venue": "In COLT,",
    "year": 2016
  }, {
    "title": "An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis",
    "authors": ["Y. Tian"],
    "year": 2017
  }, {
    "title": "Error bounds for approximations with deep relu networks, 2016",
    "authors": ["D. Yarotsky"],
    "year": 2016
  }, {
    "title": "How transferable are features in deep neural networks",
    "authors": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Understanding neural networks through deep visualization",
    "authors": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Global optimality conditions for deep neural networks",
    "authors": ["C. Yun", "S. Sra", "A. Jadbabaie"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "Visualizing and understanding convolutional networks",
    "authors": ["M.D. Zeiler", "R. Fergus"],
    "venue": "In ECCV,",
    "year": 2014
  }, {
    "title": "Understanding deep learning requires re-thinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "Vinyals", "Oriol"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Recovery guarantees for one-hidden-layer neural networks",
    "authors": ["K. Zhong", "Z. Song", "P. Jain", "P. Bartlett", "I. Dhillon"],
    "venue": "In ICML,",
    "year": 2017
  }],
  "id": "SP:e401cfc4797b9364af23e30f3a41e070bf945873",
  "authors": [{
    "name": "Quynh Nguyen",
    "affiliations": []
  }, {
    "name": "Matthias Hein",
    "affiliations": []
  }],
  "abstractText": "We analyze the loss landscape and expressiveness of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a “wide” layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with almost no bad local minima.",
  "title": "Optimization Landscape and Expressivity of Deep CNNs"
}