{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep neural networks (DNNs) have improved performances of many applications, as the non-linearity of DNNs provides expressive modeling capacity, but it also makes DNNs difficult to train and easy to overfit the training data.\nWhitened neural network (WNN) (Desjardins et al., 2015), a recent advanced deep architecture, is ideally to solve the above difficulties. WNN extends batch normalization (BN) (Ioffe & Szegedy, 2015) by normalizing the internal hidden representation using whitening transformation instead of standardization. Whitening helps regularize each diagonal block of the Fisher Information Matrix (FIM) to be an\n1Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China 2Multimedia Laboratory, The Chinese University of Hong Kong, Hong Kong. Correspondence to: Ping Luo <pluo@ie.cuhk.edu.hk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\napproximation of the identity matrix. This is an appealing property, as training WNN using stochastic gradient descent (SGD) mimics the fast convergence of natural gradient descent (NGD) (Amari & Nagaoka, 2000). The whitening transformation also improves generalization. As demonstrated in (Desjardins et al., 2015), WNN exhibited superiority when being applied to various network architectures, such as autoencoder and convolutional neural network, outperforming many previous works including SGD, RMSprop (Tieleman & Hinton, 2012), and BN.\nAlthough WNN is able to reduce the number of training iterations and improve generalization, it comes with a price of increasing training time, because eigen-decomposition occupies large computations. The runtime scales up when the number of hidden layers that require whitening transformation increases. We revisit WNN by breaking down its performance and show that its main runtime comes from two aspects, 1) computing full covariance matrix for whitening and 2) solving singular value decomposition (SVD). Previous work (Desjardins et al., 2015) suggests to overcome these problems by a) using a subset of training data to estimate the full covariance matrix and b) solving the SVD every hundreds or thousands of training iterations. Both of them rely on the assumption that the SVD holds in this period, but it is generally not true. When this period becomes large, WNN degenerates to canonical SGD due to ill conditioning of FIM.\nWe propose generalized WNN (GWNN), which possesses the beneficial properties of WNN, but significantly reduces its runtime and improves its generalization. We introduce two variants of GWNN, including pre-whitening and postwhitening GWNNs. The former one whitens a hidden layer’s input values, whilst the latter one whitens the preactivation values (hidden features). GWNN has three appealing characteristics. First, compared to WNN, GWNN is capable of learning more compact hidden representation, such that the SVD can be approximated by a few top eigenvectors to reduce computation. This compact representation also improves generalization. Second, it enables the whitening transformation to be performed in a short period, maintaining conditioning of FIM. Third, by exploiting knowledge of the distribution of the hidden features, we calculate the covariance matrix in an analytical form to further improve computational efficiency."
  }, {
    "heading": "2. Notation and Background",
    "text": "We begin by defining the basic notation for feed-forward neural network. A neural network transforms an input vector o0 to an output vector o` through a series of ` hidden layers {oi}`i=1. We assume each layer has identical dimension for the simplicity of notation i.e. ∀oi ∈ Rd×1. In this case, all vectors and matrixes in the following should have d rows unless otherwise stated. As shown in Fig.1 (a), each fully-connected (fc) layer consists of a weight matrix, W i, and a set of hidden neurons, hi, each of which receives as input a weighted sum of outputs from the previous layer. We have hi = W ioi−1. In this work, we take fully-connected network as an example. Note that the above computation can be also applied to a convolutional network, where an image patch is vectorized as a column vector and represented by oi−1 and each row of W i represents a filter.\nAs the recent deep architectures typically stack a batch normalization (BN) layer before the pre-activation values, we do not explicitly include a bias term when computing hi, because it is normalized in BN, such that φi = hi−E[hi]√\nVar[hi] , where the expectation and variance are computed over a minibatch of samples. LeCun et al. (2002) showed that such normalization speeds up convergence even when the hidden features are not decorrelated. Furthermore, output of each layer is calculated by a nonlinear activation function. A popular choice is the rectified linear unit, relu(x) = max(0, x). The precise computation for an output is oi = max(0,diag(αi)φi + βi), where diag(x) represents a matrix whose diagonal entries are x. αi and βi are two vectors that scale and shift the normalized features, in order to maintain the network’s representation capacity."
  }, {
    "heading": "2.1. Whitened Neural Networks",
    "text": "This section revisits whitened neural networks (WNN). Any neural architecture can be adapted to a WNN by stacking a whitening transformation layer after the layer’s input. For example, Fig.1 (b) adapts a fc layer as shown in\n(a) into a whitened fc layer. Its information flow becomes\nõi−1 = P i−1(oi−1 − µi−1), ĥi = Ŵ iõi−1, (1)\nφi = ĥ i√\nVar[ĥi] , oi = max(0,diag(αi)φi + βi),\nwhere µi−1 represents a centering variable, µi−1 = E[oi−1]. P i−1 is a whitening matrix whose rows are obtained from eigen-decomposition of Σi−1, which is the covariance matrix of the input, Σi−1 = E[(oi−1 − µi−1)(oi−1 − µi−1)T]. The input is decorrelated by P i−1 in the sense that its covariance matrix becomes an identity matrix, i.e. E[õi−1õi−1T ] = I . To avoid ambiguity, we use ‘ˆ’ to distinguish the variables in WNN and the canonical fc layer whenever necessary. For instance, Ŵ i represents a whitened weight matrix. In Eqn.(1), computation of the BN layer has been simplified because we have E[ĥi] = Ŵ iP i−1(E[oi−1]− µi−1) = 0.\nWe define θ to be a vector consisting of all the whitened weight matrixes concatenated together, θ = {vec(Ŵ 1)T, vec(Ŵ 2)T, ..., vec(Ŵ `)T}, where vec(·) is an operator that vectorizes a matrix by stacking its columns. Let L(o`, y; θ) denote a loss function of WNN, which measures the disagreement between a prediction o` made by the network, and a target y. WNN is trained by minimizing the loss function with respect to the parameter vector θ and two constraints\nmin θ\nL(o`, y; θ) (2)\ns.t. E[õi−1õi−1T ] = I, hi − E[hi] = ĥi, i = 1...`.\nTo satisfy the first constraint, P i−1 is obtained by decomposing the covariance matrix, Σi−1 = U i−1Si−1U i−1T. We choose P i−1 = (Si−1)− 1 2U i−1\nT, where Si−1 is a diagonal matrix whose diagonal elements are eigenvalues and U i−1 is an orthogonal matrix of eigenvectors. The first constraint holds under the construction of eigendecomposition.\nThe second constraint, hi − E[hi] = ĥi, enforces that the centered hidden features are the same, before and after\nadapting a fc layer to WNN, as shown in Fig.1 (a) and (b). In other words, it ensures that their representation powers are identical. By combing the computations in Fig.1 (a) and Eqn.(1), the second constraint implies that ‖(hi−E[hi])− ĥi‖22 = ‖(W ioi−1 − W iµi−1) − Ŵ iõi−1‖22 = 0, which has a closed-form solution, Ŵ i = W i(P i−1)−1. To see this, we have ĥi = W i(P i−1)−1P i−1(oi−1 − µi−1) = W i(oi−1−µi−1) = hi−E[hi]. The representation capacity can be preserved by mapping the whitened weight matrix from the ordinary weight matrix.\nConditioning of the FIM. Here we show that WNN improves training efficiency by conditioning the Fisher information matrix (FIM) (Amari & Nagaoka, 2000). A FIM, denoted as F , consists of `× ` blocks. Each block is indexed by Fij , representing the covariance (co-adaptation) between the whitened weight matrixes of the i-th and j-th layers. We have Fij = E[vec(δŴ i)vec(δŴ j)T], where δŴ i indicates the gradient of the i-th whitened weight matrix. For example, the gradient of Ŵ i is achieved by õi−1(δĥi)T, as illustrated in Eqn.(1). We have vec(δŴ i) = vec(õi−1(δĥi)T) = δĥi ⊗ õi−1, where ⊗ denotes the Kronecker product. In this case, Fij can be rewritten as E[(δĥi ⊗ õi−1)(δĥj ⊗ õj−1)T] = E[δĥi(δĥj)T ⊗ õi−1(õj−1)T]. By assuming δĥ and õ are independent as demonstrated in (Raiko et al., 2012), Fij can be approximated by E[δĥi(δĥj)T] ⊗ E[õi−1(õj−1)T]. As a result, when i = j, each diagonal block of F , Fii, has a block diagonal structure because we have E[õi−1(õi−1)T] = I as shown in Eqn.(2), which improves conditioning of FIM and thus speeds up training. In general, WNN regularizes the diagonal blocks of FIM and achieves stronger conditioning than those methods (LeCun et al., 2002; Tieleman & Hinton, 2012) that regularized the diagonal entries.\nTraining WNN. Alg.1 summarizes training of WNN. At the 1st line, the whitened weight matrix Ŵ i0 is initialized by W i of the ordinary fc layer, which can be pretrained or sampled from a Gaussian distribution. The 4th line shows that Ŵ it is updated in each iteration t using SGD. The first and second constraints are achieved in the 7th and 8th lines respectively. For example, the 8th line ensures that the hidden features are the same before and after updating the whitening matrix. As the distribution of the hidden representation changes after every update of the whitened weight matrix, to maintain good conditioning of FIM, the whitening matrix, P i−1, needs to be reconstructed frequently by performing eigen-decomposition on Σi−1, which is estimated using N samples. N is typically 104 in experiments. However, this raw strategy increases computation time. Desjardins et al. (2015) performed whitening in every τ iterations as shown in the 5th line of Alg.1 to reduce computations, e.g. τ = 103.\nHow good is the conditioning of the FIM by using Al-\nAlgorithm 1 Training WNN 1: Init: initial network parameters θ, αi, βi; whitening matrix P i−1 = I; iteration t = 0; Ŵ it = W i; ∀i ∈ {1...`}.\ng.1? We measure the similarity of the covariance matrix, E[õi−1(õi−1)T], with the identity matrix I . This is called the orthogonality. We employ Pearson’s correlation1 as the similarity between two matrixes. Intuitively, this measure has a value between −1 and +1, representing negative and positive correlations. Larger values indicate higher orthogonality. Fig.2 visualizes four randomly generated covariance matrixes, where (a,b) are sampled from a uniform distribution between 0 and 1. Fig.2 (c,d) are generated by truncating different numbers of columns of a randomly generated orthogonal matrix. For instance, (a,b) have small similarity with respect to the identity matrix. In contrast, when the correlation equals 0.65 as shown in (c), all entries in the diagonal are larger than 0.9 and more than 80% off-diagonal entries have values smaller than 0.1. Furthermore, Pearson’s correlation is insensitive to the size of matrix, such that orthogonality of different layers can be compared together. For example, although matrixes in\n1Given an identity matrix, I , and a covariance matrix, Σ, the Pearson’s correlation between them is defined as corr(Σ, I) = vec(Σ)Tvec(I)√ vec(Σ)Tvec(Σ)·vec(I)Tvec(I) , where vec(Σ) is a normalized vector by subtracting mean of all entries.\n(a) and (b) have different sizes, they have similar value of orthogonality when they are sampled from the same distribution.\nAs shown in Fig.3 (a), we adopt network-in-network (NIN) (Lin et al., 2014) that is trained on CIFAR-10 (Krizhevsky, 2009), and plot the orthogonalities of three different convolutional layers, which are whitened every τ = 103 iterations by using Alg.1. We see that orthogonality values during training have large fluctuations except those of the first convolutional layer, abbreviated as ‘conv1’. This is because the distributions of deeper layers’ inputs change after the whitened weight matrixes have been updated, leading to ill-conditions of the whitening matrixes, which are estimated in a large interval. In fact, large τ will degenerate WNN to canonical SGD. However, ‘conv1’ uses image data as inputs, whose distribution is typically stable during training. Its whitening matrix can be estimated once at the beginning and fixed in the entire training stage.\nIn the section below, we present generalized whitened neural networks to improve conditioning of FIM while reducing computation time."
  }, {
    "heading": "3. Generalized Whitened Neural Networks",
    "text": "We present two types of generalized WNN (GWNN), including pre-whitening and post-whitening GWNNs. Both models share beneficial properties of WNN, but have lower computation time."
  }, {
    "heading": "3.1. Pre-whitening GWNN",
    "text": "This section introduces pre-whitening GWNN, abbreviated as pre-GWNN, which performs whitening transformation before applying the weight matrix (i.e. whiten the input),\nas illustrated in Fig.1 (c). When adapting a fc layer to pre-GWNN, the whitening matrix is truncated by removing those eigenvectors that have small eigenvalues, in order to learn compact representation. This allows the input vector to vary its length, so as to gradually adapt the learned representation to informative patterns with high variations, but not noises. Learning pre-GWNN is formulated analogously to learning WNN in Eqn.(2), but with one additional constraint truncated the rank of the whitening matrix,\nmin θ\nL(o`, y; θ) (3)\ns.t. rank(P i−1) ≤ d′, E[õi−1d′ õ i−1T d′ ] = I,\nhi − E[hi] = ĥi, i = 1...`.\nLet d be the dimension of the original fc layer. By combining Eqn.(2), we have P i−1 = (Si−1)− 1 2U i−1\nT ∈ Rd×d, which is truncated by using P i−1d′ = (S i−1 d′ ) − 12U i−1d′ T ∈ Rd′×d, where Si−1d′ is achieved by keeping rows and columns associated with the first d′ large eigenvalues, whilst U i−1d′ contains the corresponding d\n′ eigenvectors. The value of d′ can be tuned using a validation set. For simplicity, we choose d′ = d2 , which works well throughout our experiments. This is inspired by the finding in (Zhang et al., 2015), who disclosed that the first 50% eigenvectors contribute over 95% energy in a deep model.\nMore specifically, pre-GWNN first projects an input vector to a d′ low-dimensional space, õi−1d′ = P i−1 d′ (o\ni−1 − µi−1) ∈ Rd′×1. The whitened weight matrix then produces a hidden feature vector of d dimensions, which has the same length as the ordinary fc layer, i.e. ĥi = Ŵ iõi−1d′ ∈ Rd×1, where Ŵ i = W i(P i−1 d′ )\n−1 ∈ Rd×d′ . The computations of BN and the nonlinear activation are identical to Eqn.(1).\nTraining pre-GWNN is similar to Alg.1. The main modification is produced at the 7th line in order to reduce runtime. Although Alg.1 decreases number of iterations when training converged, each iteration has additional computation time for eigen-decomposition. For example, in WNN, the required computation of full singular value decomposition (SVD) is typically O(Nd2), where N represents the number of samples employed to estimate the covariance matrix. In particular, when we have ` whitened layers and T is the number of iterations, all whitening transformations occupy O(Nd\n2T` τ ) runtime in\nthe entire training stage. In contrast, pre-GWNN performs the popular online estimation for the top d′ eigenvectors in P i−1d′ such as online SVD (Shamir, 2015; Povey et al., 2015), instead of using full SVD as WNN did. This difference reduces runtime to O( (N+M)d\n′T` τ ′ ), where τ ′\nrepresents the whitening interval in GWNN and M is the number of samples used to estimate the top eigenvectors. We have M = N as employed in previous works.\nFor pre-GWNN, reducing runtime and improving conditioning is a tradeoff, since the former requires to increase τ ′ but the latter requires to decrease it. When M = N and d′ = d2 , we compare the runtime complexity of preGWNN to that of WNN, and obtain a ratio of dτ ′\nτ , which tells us that whitening can be performed in a short interval without increasing runtime. For instance, as shown in Fig.3 (b) when τ ′ = 20, orthogonalities are well preserved and more stable than those in (a). In this case, preGWNN reduces computations of WNN by at least 20× when d > τ , which is a typical choice in recent deep architectures (Krizhevsky et al., 2012; Lin et al., 2014) where d ∈ {1024, 2048, 4096}."
  }, {
    "heading": "3.2. Post-whitening GWNN",
    "text": "Another variant we propose is post-whitening GWNN, abbreviated as post-GWNN. Unlike WNN and pre-GWNN, post-GWNN performs whitening transformation after applying the weight matrix (i.e. whiten the feature), as illustrated in Fig.1 (d). In general, post-GWNN reduces runtime to O( (N\n′+M)d′T` τ ′ ), where N ′ N .\nFig.1 (d) shows how to adapt a fc layer to post-GWNN. Suppose oi−1d′ has been whitened by P i−1 d′ in the previous layer, at the i-th layer we have\nĥi = Ŵ i(oi−1d′ − µ i−1 d′ ), h i d′ = P i d′ ĥ i, (4)\nφid′ = hi d′√\nVar[hi d′ ] , oid′ = max(0,diag(α i d′)φ i d′ + β i d′),\nwhere µi−1d′ = E[o i−1 d′ ]. In Eqn.(4), a feature vector ĥi ∈ Rd×1 is first produced by applying a whitened weight matrix on the input, in order to recover the original feature length as the fc layer. A whitening matrix then projects ĥi to a decorrelated feature vector hid′ ∈ Rd\n′×1. We have Ŵ i = W i(P i−1d′ )\n−1 ∈ Rd×d′ , where P i−1d′ = (Si−1d′ ) − 12U i−1d′ T ∈ Rd′×d, and U i−1 and Si−1 contain eigenvectors and eigenvalues of the hidden features at the i− 1-th layer.\nConditioning. Here we disclose that whitening hidden features also enforces good conditioning of FIM. At this point, we have decorrelated the hidden features by satisfying E[hid′hi T d′ ] = I . Then h i d′ follows a standard multivariate Gaussian distribution, hid′ ∼ N (0, I). As a result, the layer’s output follows a rectified Gaussian distribution, which is uncorrelated as presented in remark 1. In post-GWNN, whitening hidden features of the i− 1- th layer improves conditioning for the i-th layer. To see this, by following the description in Sec.2.1, the diagonal block of FIM associated with the i-th layer can be written as Fii ≈ E[δĥi(δĥi)T]⊗E[(oi−1d′ −µ i−1 d′ )(o i−1 d′ −µ i−1 d′ )\nT], where the parameters have low correlations since Fii has a block diagonal structure.\nAlgorithm 2 Training post-GWNN 1: Init: initial θ, αi, βi; and t = 0, tw = k, λ = twk ; P\ni−1 = I , Ŵ it = W\ni, ∀i ∈ {1...`}. 2: repeat 3: for i = 1 to ` do 4: update Ŵ it , αit, and βit by SGD. 5: if mod(t, τ) = 0 then 6: store old P i−1o = P i−1 d′ . 7: estimate mean and variance of ĥi by a minibatch of N ′ samples or following remark 2 when N ′ = 1. 8: update P i−1d′ by online SVD. 9: transform Ŵ it = Ŵ itP i−1o (P i−1 d′ )\n−1. 10: tw = 1 and λ = twk . 11: end if 12: end for 13: t = t+ 1. 14: if tw < k then tw = tw + 1 end if 15: until convergence\nRemark 1. Let h ∼ N (0, I) and o = max(0, Ah + b). Then E[(oj − E[oj ])(ok − E[ok])] ≈ 0 if A is a diagonal matrix, where j, k index any two entries of o and j 6= k.\nFor remark 1, we have A = diag(αid′) and b = β i d′ . It tells us three things. First, by using whitening and BN, covariance of any two different entries of oid′ approaches zero. Second, at the iteration when we construct P id′ , we can estimate the full covariance matrix of ĥi using the mean and variance of oi−1d′ , E[ĥiĥi T\n] = Ŵ iE[(oi−1d′ − µi−1d′ )(o i−1 d′ − µ i−1 d′ ) T]Ŵ i T\n. The mean and variance can be estimated with a minibatch of samples i.e. N ′ N . Third, to the extreme, when N ′ = 1, these statistics can still be computed in analytical forms leveraging remark 2.\nRemark 2. Let a random variable x ∼ N (0, 1) and y = max(0, ax + b). Then E[y] = a√\n2π e−\nb2\n2a2 + b2Ψ(− b√ 2a )\nand E[y2] = ab√ 2π e−\nb2\n2a2 + a 2+b2\n2 Ψ(− b√ 2a ), where Ψ(x) = 1− erf(x) and erf(x) is the error function.\nThe above remark derives the mean and variance of a rectified output unit that has shift and scale parameters. It generalizes (Arpit et al., 2016) that presented a special case when a = 1 and b = 0. In that case, we have E[y] = 1√\n2π\nand Var[y] = E[y2]−E[y]2 = 12− 1 2π , which are consistent with previous work.\nExtensions. Remark 1 and 2 can be extended to other nonlinear activation functions, such as leaky rectified unit defined as leakyrelu(x) = max(0, x)+amin(0, x), where the slope of the negative part is controlled by the coefficient a, which is fixed in (Maas et al., 2013) and is learned in (He et al., 2015)."
  }, {
    "heading": "3.3. Training post-GWNN",
    "text": "Similar to pre-GWNN, the learning problem can be formulated as\nmin θ\nλL(o`, y; θ) + (1− λ) ∑` i=1 L feat(hi, ĥi; θ) (5)\ns.t. rank(P i) ≤ d′, E[hid′hi T d′ ] = I, i = 1...`.\nEqn.(5) has two loss functions. Different from WNN and pre-GWNN where the feature equality constraint can be satisfied in a closed form, this constraint is treated as an auxiliary loss function in post-GWNN, defined as Lfeat(hi, ĥi) = 12‖(h\ni − E[hi]) − ĥi‖22 and minimized in the training stage. It does not have an analytical solution because there is a nonlinear activation function between the weight matrix and the whitening matrix (i.e. in the previous layer). In Eqn.(5), λ is a coefficient that balances the contribution of two loss functions, and 1− λ is linearly decayed as 1− λ = k−twk , where tw = 1, 2, ..., k. At each time after we update the whitening matrix, we start decay by setting tw = 1 and k indicates the iterations at which we stop annealing.\nAlg.2 summarizes the training procedure. It preforms online update of the top d′ eigenvectors of the whitening matrix similar to pre-GWNN. In comparison, it decreases the runtime of whitening transformation to O( (N\n′+M)d′T` τ ′ ),\nwhich is N+MN ′+M fold reduction with respect to pre-GWNN. For example, when N = M and N ′ = 1, post-GWNN is capable of reducing computations of pre-GWNN and WNN by 2× and (2τ ′)× respectively, while maintaining better conditioning than these alternatives by choosing small τ ′."
  }, {
    "heading": "4. Empirical Studies",
    "text": "We compare WNN, pre-GWNN, and post-GWNN in the following aspects, including a) number of iterations when training converged, b) computation times for training, and c) generalization capacities on various datasets. We also conduct ablation studies with respect to 1) effect of the number of samples N to estimate the covariance matrix for pre-GWNN and 2) effect of N ′ for post-GWNN. Finally, we try to tune the value of d′.\nDatasets. We employ the following datasets. a) MNIST (Lecun et al., 1998) has 60, 000 28× 28 images of 10 handwritten digits (0-9) for training and another 10, 000 test images. 5, 000 images from the training set are randomly selected as a validation set. b) CIFAR-10 (Krizhevsky, 2009) consists of 50, 000 32 × 32 color images for training and 10, 000 images for testing. Each image is categorized into one of the 10 object labels. For CIFAR-10, 5, 000 images are chosen for validation. c) CIFAR-100 (Krizhevsky, 2009) has the same number of\nimages as CIFAR-10, but each image is classified into 100 categories. For CIFAR-100, we select 5, 000 images from training set for validation. d) SVHN (Netzer et al., 2011) consists of color images of house numbers collected by Google Street View. The task is to predict the center digit (0-9) of each image, which is of size 32×32. There are 73, 257 images in the training set, 26, 032 images for test, and 531, 131 additional examples. We follow (Sermanet et al., 2012) to build a validation set by selecting 400 samples per class from the training set and 200 samples per class from the additional set. We didn’t train on validation, which is for tuning hyperparameters.\nExperimental Settings. We have two settings, an unsupervised and a supervised learning settings. First, following (Desjardins et al., 2015), we compare the above three approaches on the task of minimizing reconstruction error of an autoencoder on MNIST. The encoder consists of 4 fc sigmoidal layers, which have 1000, 500, 256, and 30 hidden neurons respectively. The decoder is symmetric and untied with respect to the encoder. Second, for the task of image classification on CIFAR-10, -100, and SVHN, we employ the same network-in-network (NIN) (Lin et al., 2014) architecture, which has 9 convolutional layers and 3 pooling layers defined as2: conv(192, 5)-conv(160, 1)-maxpool(3, 2)-conv(96, 1)- conv(192, 5)-conv(192, 1)-avgpool(3, 2)-conv(192, 1)- conv(192, 5)-conv(192, 1)-conv(l, 1)-avgpool(8, 8), where l = 10 for CIFAR-10 and SVHN and l = 100 for CIFAR-100. For all models, we use SGD with momentum of 0.9."
  }, {
    "heading": "4.1. Comparisons of Convergence and Computations",
    "text": "We record the number of epochs and computation time, when training WNN, pre-, and post-GWNN on MNIST and CIFAR-100, respectively. We employ the first setting above for MNIST and the second setting for CIFAR100. For both settings, hyperparamters are chosen by grid search on the validation sets. The search specifications of minibatch size, learning rate, and whitening interval τ are {64, 128, 256}, {0.1, 0.01, 0.001}, and {20, 50, 100, 103}, respectively. In particular, for WNN and pre-GWNN, the number of samples used to estimate the covariance matrix, N , is picked up from {103, 10 4\n2 , 10 4}. For post-GWNN,\nN ′ is chosen to be the same as the minibatch size and the decay period k = 0.1τ . For a fair comparison, we report the best performance on validation set for each approach, and didn’t employ any data augmentation such as random image cropping and flipping.\n2The ‘conv’, ‘maxpool’, and ‘avgpool’ represent convolution, max pooling, and average pooling respectively. Each convolutional layer is defined as conv(number of filters, filter size). For each pooling layer, we have pool(kernel size, stride). All convolutions have stride 1.\nThe convergence and computation time are reported in Fig.4 (a-d). We have several important observations. First, all three approaches converge much faster than the canonical network trained by SGD. Second, pre- and postGWNN achieve better convergence than WNN on both datasets as shown in (a) and (c). Moreover, post-GWNN outperforms pre-GWNN. Third, post-GWNN significantly reduces computation time compared to all the other methods, as illustrated in (b) and (d). We see that although WNN reduces the number of epochs, it takes long time to train because its whitening transformation occupies large computations."
  }, {
    "heading": "4.2. Performances on various Datasets",
    "text": "We evaluate WNN, pre-, and post-GWNN on CIFAR-10, -100, and SVHN datasets, and compare their classification accuracies to existing state-of-the-art methods. For all the datasets and approaches, we utilize the same network structure as mentioned in the second setting above. For two CIFAR datasets, we adopt minibatch size 64 and initial learning rate 0.1, which is reduced by half after every 25 epochs. We train for 250 epochs. As SVHN is a large dataset, we train for 100 epochs with minibatch size 128 and initial learning rate 0.05, which is reduced by half after every 10 epochs. We train on CIFAR-10 and -100 using both without and with data augmentation, which includes random cropping and horizontal flipping. For SVHN, we didn’t augment data following (Sermanet et al., 2012). For all the methods, we shuffle samples at the beginning of every epoch. We use N = 10 4\n2 for WNN and preGWNN and N ′ = 64 for post-GWNN. For both preand post-GWNN, we have M = N and d′ = d2 . The other experimental settings are similar to Sec.4.1. Table 1 shows the results. We see that pre- and post-GWNN consistently achieve better results than those of WNN, and also outperform previous state-of-the-art approaches."
  }, {
    "heading": "4.3. Ablation Studies",
    "text": "The following experiments are conducted on CIFAR-100 using pre- or post-GWNN. The first two experiments follow the setting as mentioned in Sec.4.1. First, we evaluate the effect of the number of samples, N , used to estimate the covariance matrix in pre-GWNN. We compare performances of using different values of N picked up from {102, 103, 3 × 103, 5 × 103, 104}. Fig.5 (a) plots the results. We see that performance can drop because of ill conditioning when N is small e.g. N = 100. When it is too large e.g. N = 104, we observe slightly overfitting. Second, Fig.5 (b) highlights the effect of N ′ in postGWNN. We see that post-GWNN can work reasonably well when N ′ is small.\nFinally, instead of treating d′ = d2 as a constant in training, we study the effect of tuning its value on the validation set using a simple heuristic strategy. If the validation error reduces more than 2% over 4 consecutive evaluations, we have d′ = d′ − rate × d′.\nIf the error has no reduction over this period, d′ is increased by the same rate as above. We use post-GWNN and follow experimental setting in Sec.4.2. We take two different rates {0.1, 0.2} as examples. Fig.6 plots the variations of dimensions when d = 192 and shows their test errors. We find\nthat keeping d′ as a constant generally produces better result than those obtained by the above strategy, but this strategy yields less runtime because more dimensions are pruned."
  }, {
    "heading": "5. Conclusion",
    "text": "We presented generalized WNN (GWNN) to reduce runtime and improve generalization of WNN. Different from WNN that reduces computation time by whitening with a large period, leading to ill conditioning of FIM, GWNN learns compact internal representation, such that SVD is approximated by the top eigenvectors in an online manner, making GWNN not only reduces computations but also improves generalization. By exploiting the knowledge of the hidden representation’s distribution, we showed that post-GWNN is able to compute the covariance matrix in a closed form, which can be also extended to the other activation function. Extensive experiments demonstrated the effectiveness of GWNN."
  }, {
    "heading": "Acknowledgements",
    "text": "This work is partially supported by the National Natural Science Foundation of China (61503366, 61472410, U1613211), the National Key Research and Development Program of China (No.2016YFC1400700), the External Cooperation Program of BIC, Chinese Academy of Sciences (No.172644KYSB20160033), and the Science and Technology Planning Project of Guangdong Province (2015B010129013, 2014B050505017)."
  }],
  "year": 2017,
  "references": [{
    "title": "Learning activation functions to improve deep neural networks",
    "authors": ["Agostinelli", "Forest", "Hoffman", "Matthew", "Sadowski", "Peter", "Baldi", "Pierre"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Methods of information geometry",
    "authors": ["Amari", "Shun-ichi", "Nagaoka", "Hiroshi"],
    "venue": "In Tanslations of Mathematical Monographs,",
    "year": 2000
  }, {
    "title": "Normalization propagation: A parametric technique for removing internal covariate shift in deep networks",
    "authors": ["Arpit", "Devansh", "Zhou", "Yingbo", "Kota", "Bhargava U", "Govindaraju", "Venu"],
    "year": 2016
  }, {
    "title": "Natural neural networks",
    "authors": ["Desjardins", "Guillaume", "Simonyan", "Karen", "Pascanu", "Razvan", "Kavukcuoglu", "Koray"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
    "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["Krizhevsky", "Alex"],
    "venue": "In Technical Report,",
    "year": 2009
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "In Proceeding of IEEE,",
    "year": 1998
  }, {
    "title": "Efficient backprop",
    "authors": ["LeCun", "Yann", "Bottou", "Leon", "Orr", "Genevieve B", "Mller", "Klaus Robert"],
    "venue": "In Neural Networks: Tricks of the Trade,",
    "year": 2002
  }, {
    "title": "Rectifier nonlinearities improve neural network acoustic models",
    "authors": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Reading digits in natural images with unsupervised feature learning",
    "authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"],
    "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
    "year": 2011
  }, {
    "title": "Parallel training of dnns with natural gradient and parameter averaging",
    "authors": ["Povey", "Daniel", "Zhang", "Xiaohui", "Khudanpur", "Sanjeev"],
    "venue": "In ICLR workshop,",
    "year": 2015
  }, {
    "title": "Deep learning made easier by linear transformations in perceptrons",
    "authors": ["Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"],
    "venue": "In AISTATS,",
    "year": 2012
  }, {
    "title": "Convolutional neural networks applied to house numbers digit classification",
    "authors": ["Sermanet", "Pierre", "Chintala", "Soumith", "LeCun", "Yann"],
    "venue": "In arXiv:1204.3968,",
    "year": 2012
  }, {
    "title": "A stochastic pca and svd algorithm with an exponential convergence rate",
    "authors": ["Shamir", "Ohad"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Accelerating very deep convolutional networks for classification and detection",
    "authors": ["Zhang", "Xiangyu", "Zou", "Jianhua", "He", "Kaiming", "Sun", "Jian"],
    "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }],
  "id": "SP:12f70737d4c61eacfa48d9ea5ae7d2280bb75765",
  "authors": [{
    "name": "Ping Luo",
    "affiliations": []
  }],
  "abstractText": "Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.",
  "title": "Learning Deep Architectures via Generalized Whitened Neural Networks"
}