{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1466–1476, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert.\n1The source code, the dataset and the annotations are publicly available at geometry.allenai.org.\nThe geometry genre has several distinctive characteristics. First, diagrams provide essential information absent from question text. In Figure 1 problem (a), for example, the unstated fact that lines BD and AC intersect at E is necessary to solve the problem. Second, the text often includes difficult references to diagram elements. For example, in the sentence “In the diagram, the longer line is tangent to the circle”, resolving the referent of the phrase “longer line” is challenging. Third, the text often contains implicit relations. For example, in the sentence “AB is 5”, the relations IsLine(AB) and length(AB)=5 are implicit. Fourth, geometric terms can be ambiguous as well. For instance, radius can be a type identifier in “the length of radius AO is 5”, or a predicate in “AO is the radius of circle O”. Fifth, identifying the correct arguments for each relation is challenging. For example, in sentence “Lines AB and CD are perpendicular to EF”, the parser has to determine what is perpendicular to EF—line AB? line\n1466\nCD? Or both AB and CD? Finally, it is hard to obtain large number of SAT-level geometry questions; Learning from a few examples makes this a particularly challenging NLP problem.\nThis paper introduces GEOS, a system that maps geometry word problems into a logical representation that is compatible with both the problem text and the accompanying diagram (Figure 1). We cast the mapping problem as the problem of selecting the subset of relations that is most likely to correspond to each question.\nWe compute the mapping in three main steps (Figure 2). First, GEOS uses text- and diagramparsing to overgenerate a set of relations that potentially correspond to the question text, and associates a score with each. Second, GEOS generates a set of relations (with scores) that corresponds to the diagram. Third, GEOS selects a subset of the relations that maximizes the joint text and diagram scores. We cast this maximization as a submodular optimization problem, which enables GEOS to use a close-to-optimal greedy algorithm. Finally, we feed the derived formal model of the problem to a geometric solver, which computes the answer to the question.\nGEOS is able to solve unseen and unaltered multiple-choice geometry questions. We report on experiments where GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions, providing the first results of this kind. Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work."
  }, {
    "heading": "2 Related Work",
    "text": "Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it\nchallenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work.\nOur work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions.\nCoupling images and the corresponding text has attracted attention in both vision and NLP (Farhadi et al., 2010; Kulkarni et al., 2011; Gupta and Mooney, 2010; Gong et al., 2014; Fang et al., 2014). We build on this powerful paradigm, but instead of generating captions we show how processing multimodal information help improve textual or visual interpretations for solving geometry questions.\nDiagram understanding has been explored since early days in AI (Lin et al., 1985; Hegarty and Just, 1989; Novak, 1995; O’Gorman and Kasturi, 1995; Bulko, 1988; Srihari, 1994; Lovett and Forbus, 2012). Most previous approaches differ from our method because they address the twin problems of diagram understanding and text understanding in isolation. Often, previous work relies on manual identification of visual primitives, or on rule-based system for text analysis. The closest work to ours is the recent work of Seo et al. (2014) that aligns geometric shapes with their textual mentions, but does not identify geometric relations or solve geometry problems."
  }, {
    "heading": "3 Problem Formulation",
    "text": "A geometry question is a tuple (t, d, c) consisting of a text t in natural language, a diagram d\nin raster graphics, and multiple choice answers c = {c1, . . . , cM} (M = 5 in SAT). Answering a geometry question is to find a correct choice ci.\nOur method, GEOS, consists of two steps (Figure 2): (1) interpreting a geometry question by deriving a logical expression that represents the meaning of the text and the diagram, and (2) solving the geometry question by checking the satisfiablity of the derived logical expression. In this paper we mainly focus on interpreting geometry questions and use a standard algebraic solver (see section 7 for a brief description of the solver).\nDefinitions: We formally represent logical expressions in the geometry domain with the language Ω, a subset of typed first-order logic that includes: • constants, corresponding to known numbers\n(e.g., 5 and 2 in Figure 1) or entities with known geometric coordinates. • variables, corresponding to unknown numbers\nor geometrical entities in the question (e.g., O and CE in Figure 1). • predicates, corresponding to geometric or arith-\nmetic relations (e.g., Equals, IsDiameter, IsTangent). • functions, corresponding to properties of geo-\nmetrical entities (e.g., LengthOf, AreaOf) or arithmetic operations (e.g., SumOf, RatioOf).\nEach element in the geometry language has either boolean (e.g., true), numeric (e.g., 4), or entity (e.g., line, circle) type. We refer to all symbols\nin the language Ω as concepts. We use the term literal to refer to the application of a predicate to a sequence of arguments (e.g., IsTriangle(ABC)). Literals are possibly negated atomic formulas in the language Ω. Logical formulas contain constants, variables, functions, existential quantifiers and conjunctions over literals (e.g., ∃x, IsTriangle(x)∧IsIsosceles(x)). Interpretation is the task of mapping a new geometry question with each choice, (t, d, cm), into a logical formula γ in Ω. More formally, the goal is to find γ∗ = arg maxγ∈Γ score(γ; t, d, cm) where Γ is the set of all logical formulas in Ω and score measures the interpretation score of the formula according to both text and diagram. The problem of deriving the best formula γ∗ can be modeled as a combinatorial search in the space of literals L (note that each logical formula γ is represented as a conjunction over literals li).\nGEOS efficiently searches this combinatorial space taking advantage of a submodular set function that scores a subset of literals using both text and diagram. The best subset of literals is the one that has a high affinity with both text and diagram and is coherent i.e., does not suffer from redundancy (see Section 6). More formally,2\nL∗ = arg max L′⊂L λA(L′, t, d)︸ ︷︷ ︸ Affinity +H(L′, t, d)︸ ︷︷ ︸ Coherence , (1) where A(L′, t, d) measures the affinity of the literals in L′ with both the text and the diagram, H(L′, t, d) measures the coverage of the literals in L′ compared to the text and discourages redundancies, and λ is a trade-off parameter between A andH.\nThe affinity A is decomposed into textbased affinity, Atext, and diagram-based affinity, Adiagram. The text-based affinity closely mirrors the linguistic structure of the sentences as well as type matches in the geometry language Ω. For modeling the text score for each literal, we learn a log-linear model. The diagram-based affinity Adiagram grounds literals into the diagram, and scores literals according to the diagram parse. We describe the details on how to compute Atext in section 4 and Adiagram in section 5."
  }, {
    "heading": "4 Text Parser",
    "text": "The text-based scoring function Atext(L, t) computes the affinity score between the set of liter-\n2We omit the argument cm for the ease of notation.\nals L and the question text t. This score is the sum of the affinity scores of individual literals lj ∈ L i.e., Atext(L, t) = ∑ j Atext(lj , t) where Atext(lj , t) 7→ [−∞, 0].3 GEOS learns a discriminative model Atext(lj , t; θ) that scores the affinity of every literal lj ∈ L and the question text t through supervised learning from training data.\nWe represent literals using a hypergraph (Figure 4) (Klein and Manning, 2005; Flanigan et al., 2014). Each node in the graph corresponds to a concept in the geometry language (i.e. constants, variables, functions, or predicates). The edges capture the relations between concepts; concept nodes are connected if one concept is the argument of the other in the geometry language. In order to interpret the question text (Figure 3 step 1), GEOS first identifies concepts evoked by the words or phrases in the input text. Then, it learns the affinity scores which are the weights of edges in the hypergraph. It finally completes relations so that type matches are satistfied in the formal language."
  }, {
    "heading": "4.1 Concept Identification",
    "text": "Concepts are defined as symbols in the geometry language Ω. The concept identification stage maps words or phrases to their corresponding concepts\n3For the ease of notation, we use Atext as a function taking sets of literals or a literal.\nin the geometry language. Note that a phrase can be mapped to several concepts. For instance, in the sentence “ABCD is a square with an area of 1”, the word “square” is a noun referring to some object, so it maps to a variable square. In a similar sentence “square ABCD has an area 1”, the word “square” describes the variable ABCD, so it maps to a predicate IsSquare.\nGEOS builds a lexicon from training data that maps stemmed words and phrases to the concepts in the geometry language Ω. The lexicon is derived from all correspondences between geometry keywords and concepts in the geometry language as well as phrases and concepts from manual annotations in the training data. For instance, the lexicon contains (“square”, {square, IsSquare}) including all possible concepts for the phrase “square”. Note that GEOS does not make any hard decision on which identification is correct in this stage, and defers it to the relation identification stage (Section 4.2). To identify numbers and explicit variables (e.g. “5”, “AB”, “O”), GEOS uses regular expressions. For an input text t, GEOS assigns one node in the graph (Figure 4) for each concept identified by the lexicon."
  }, {
    "heading": "4.2 Relation Identification",
    "text": "A relation is a directed hyperedge between concept nodes. A hyperedge connects two nodes (for unary relations such as the edge between RadiusOf and O in Figure 4) or three nodes (for binary relations such as the hyperedge between Equals and its two arguments RadiusOf and 5 in Figure 4).\nWe use a discriminative model (logistic regression) to predict the probability of a relation ri being correct in text t: Pθ(yi|ri, t) = 1 1+exp (ftext(ri,t)·θ) , where yi ∈ {0, 1} is the label\nfor ri being correct in t, ftext(ri, t) is a feature vector of t and ri, and θ is a vector of parameters to be learned. We define the affinity score of ri by Atext(ri, t; θ) = logPθ(yi|ri, t). The weight of the corresponding hyperedge is the relation’s affinity score. We learn θ using the maximum likelihood estimation of the training data (details in Section 8), with L2 regularization.\nWe train two separate models for learning unary and binary relations. The training data consists of sentence-relation-label tuples (t, r, y); for instance, (“A tangent line is drawn to circle O”, IsTangent(line, O), 1) is a positive training example. All incorrect relations in the sentences of the training data are negative examples (e.g. (“A tangent line is drawn to circle O”, IsCircle(line), 0)).\nThe features for the unary and binary models are shown in Table 1 for the text t and the relation ri. We use two main feature categories. Structural features: these features capture the syntactic cues of the text in the form of text distance, dependency tree labels, and part of speech tags for the words associated with the concepts in the relation. Geometry language features: these features capture the cues available in the geometry language Ω in the form of the types and the truth values of the corresponding concepts in the relation.\nAt inference, GEOS uses the learned models to calculate the affinity scores of all the literals derived from the text t. The affinity score of each literal lj is calculated from the edge (relation) weights in the corresponding subgraph, i.e. Atext(lj , t) = ∑ iAtext(ri, t; θ) for all ri in the literal lj ."
  }, {
    "heading": "4.3 Relation Completion",
    "text": "So far, we have explained how to score the affinities between explicit relations and the question text. Geometry questions usually include implicit concepts. For instance, “Circle O has a radius of 5” implies the Equals relationship between “Radius of circle O” and “5”. In addition, geometry questions include coordinating conjunctions between entities. In “AM and CM bisect BAC and BCA”, “bisect” is shared by two lines and two angles (Figure 5 (b)). Also, consider two sentences: “AB and CD are perpendicular” and “AB is perpendicular to CD”. Both have the same semantic annotation but very different syntactic structures.\nIt is difficult to directly fit the syntactic structure of question sentences into the formal language Ω for implications and coordinating conjunctions, especially due to small training data. We, instead, adopt a two-stage learning inspired by recent work in semantic parsing (Kwiatkowski et al., 2013). Our solution assumes an intermediate representation that is syntactically sound but possibly underspecified. The intermediate representation closely mirrors the linguistic structure of the sentences. In addition, it can easily be transferred to the formal representation in the geometry language Ω.\nFigure 5 shows how implications and coordinating conjunctions are modeled in the intermediate representation. Bridged in Figure 5 (a) indicates that there is a special relation (edge) between the two concepts (e.g., what and PerimeterOf), but the alignment to the geometry language L is not clear. CC in Figure 5 (b) indicates that there is a special relation between two concepts that are connected by “and” in the sentence. GEOS completes\nthe under-specified relations by mapping them to the corresponding well-defined relations in the formal language.\nImplication: We train a log-linear classifier to identify if a Bridged relation (implied concept) exists between two concepts. Intuitively, the classification score indicates the likelihood that certain two concepts (e.g., What and PerimeterOf) are bridged. For training, positive examples are pairs of concepts whose underlying relation is underspecified, and negative examples are all other pairs of concepts that are not bridged. For instance, (what, PerimeterOf) is a positive training example for the bridged relation. We use the same features in Table 1 for the classifier.\nWe then use a deterministic rule to map bridged relations in the intermediate representation to the correct completed relations in the final representation. In particular, we map bridged to Equals if the two children concepts are of type number, and to IsA if the concepts are of type entity (e.g. point, line, circle).\nCoordinating Conjunctions: CC relations model coordinating conjunctions in the intermediate representation. For example, Figure 5 (b) shows the conjunction between the two angles BAC and BCA. We train a log-linear classifier for the CC relations, where the setup of the model is identical to that of the binary relation model in Section 4.2.\nAfter we obtain a list of CC(x,y) in the intermediate representation, we use deterministic rules to coordinate the entities x and y in each CC relation (Figure 5 (b)). First, GEOS forms a set {x, y} for every two concepts x and y that appear in CC(x,y) and transforms every x and y in other literals to {x, y}. Second, GEOS transforms the relations with expansion and distribution rules (Figure 3 Step 1 (iv)). For instance, Perpendicular({x,y}) will be transferred to Perpendicular(x, y) (expansion rule), and LengthOf{x,y}) will be transferred to LengthOf(x) ∧ LengthOf(y) (distribution rule)."
  }, {
    "heading": "5 Diagram Parser",
    "text": "We use the publicly available diagram parser (Seo et al., 2014) to obtain the set of all visual elements (points, lines, circles, etc.), their coordinates, their relationships in the diagram, and their alignment with entity references in the text (e.g. “line AB”, “circle O”). The diagram parser serves two purposes: (a) computing the diagram score as a mea-\nsure of the affinity of each literal with the diagram; (b) obtaining high-confidence visual literals which cannot be obtained from the text.\nDiagram score: For each literal lj from the text parsing, we obtain its diagram score Adiagram(lj , d) 7→ [−∞, 0]. GEOS grounds each literal derived from the text by replacing every variable (entity or numerical variable) in the relation to the corresponding variable from the diagram parse. The score function is the relaxed indicator function of whether a literal is true according to the diagram. For instance, in Figure 1 (a), consider the literal l = Perpendicular(AC, BD). In order to obtain its diagram score, we compute the angle between the lines AC and BD in the diagram and compare it with π/2. The closer the two values, the higher the score (closer to 0), and the farther they are, the lower the score. Note that the variables AC and BD are grounded into the diagram before we obtain the score; that is, they are matched with the actual corresponding lines AC and BD in the diagram.\nThe diagram parser is not able to evaluate the correctness of some literals, in which case their diagram scores are undefined. For instance, Equals(LengthOf(AB), 5) cannot be evaluated in the diagram because the scales in the diagram (pixel) and the text are different. For another example, Equals(what, RadiusOf(circle)) cannot be evaluated because it contains an ungrounded (query) variable, what. When the diagram score of a literal lj is undefined, GEOS lets Adiagram(lj) = Atext(lj).\nIf the diagram score of a literal is very low, then it is highly likely that the literal is false. For example, in Figure 2, Parallel(AC, DB) has a very low diagram score, 0.02, and is apparently false in the diagram. Concretely, if for some literal lj , Adiagram(li) < , then GEOS disregards the text score of li by replacing Atext(lj) with Adiagram(lj). On the other hand, even if the diagram score of a literal is very high, it is still possible that the literal is false, because many diagrams are not drawn to scale. Hence, GEOS adds both text and diagram scores in order to score literals (Section 6).\nHigh-confidence visual literals: Diagrams often contain critical information that is not present in the text. For instance, to solve the question in Figure 1, one has to know that the points A, E, and C are colinear. In addition, diagrams include numer-\nical labels (e.g. one of the labels in Figure 1(b) indicates the measure of the angle ABC = 40 degrees). This kind of information is confidently parsed with the diagram parser by Seo et al. (2014). We denote the set of the high-confidence literals by L∆ that are passed to the solver (Section 7)."
  }, {
    "heading": "6 Optimization",
    "text": "Here, we describe the details of the objective function (Equation 1) and how to efficiently maximize it. The integrated affinity score of a set of literals L′ (the first term in Equation 1) is defined as: A(L′, t, d) = ∑ l′j∈L′\n[Atext(l′j , t) +Adiagram(l′j , d)] where Atext and Adiagram are the text and diagram affinities of l′j , respectively.\nTo encourage GEOS to pick a subset of literals that cover the concepts in the question text and, at the same time, avoid redundancies, we define the coherence function as:\nH(L′, t, d) = Ncovered(L′)−Rredundant(L′)\nwhere Ncovered is the number of the concept nodes used by the literals inL′, andNredundant is the number of redundancies among the concept nodes of the literals. To account for the different scales between A and H, we use the trade-off parameter λ in Equation 1 learned on the validation dataset.\nMaximizing the objective function in Equation 1 is an NP-hard combinatorial optimization problem. However, we show that our objective function is submodular (see Appendix (Section 11) for the proof of submodularity). This means that there exists a greedy method that can provide a reliable approximation. GEOS greedily maximizes Equation 1 by starting from an empty set of literals and adding the next literal lj that maximizes the gain of the objective function until the gain becomes negative (details of the algorithm and the gain function are explained in Figure 3 step 3)."
  }, {
    "heading": "7 Solver",
    "text": "We now have the best set of literals L∗ from the optimization, and the high-confidence visual literals L∆ from the diagram parser. In this step, GEOS determines if an assignment exists to the variables X in L∗ ∪ L∆ that simultaneously satisfies all of the literals. This is known as the problem\nof automated geometry theorem proving in computational geometry (Alvin et al., 2014).\nWe use a numerical method to check the satisfiablity of literals. For each literal lj in L∗ ∪ L∆, we define a relaxed indicator function gj : S 7→ zj ∈ [−∞, 0]. The function zj = gj(S) indicates the relaxed satisfiability of lj given an assignment S to the variables X . The literal lj is completely satisfied if gj(S) = 0. We formulate the problem of satisfiability of literals as the task of finding the assignment S∗ to X such that sum of all indicator functions gj(S∗) is maximized, i.e. S∗ = arg maxS ∑ j gj(S). We use the basing-hopping algorithm (Wales and Doye, 1997) with sequential least squares programming (Kraft, 1988) to globally maximize the sum of the indicator functions. If there exists an assignment such that ∑ j gj(S) = 0, then GEOS finds an assignment to X that satisfies all literals. If such assignment does not exist, then GEOS concludes that the literals are not satisfiable simultaneously. GEOS chooses to answer a geometry question if the literals of exactly one answer choice are simultaneously satisfiable."
  }, {
    "heading": "8 Experimental Setup",
    "text": "Logical Language Ω: Ω consists of 13 types of entities and 94 function and predicates observed in our development set of geometry questions. Implementation details: Sentences in geometry questions often contain in-line mathematical expressions, such as “If AB=x+5, what is x?”. These mathematical expressions cause general purpose parsers to fail. GEOS uses an equation analyzer and pre-processes question text by replacing “=” with “equals”, and replacing mathematical terms (e.g., “x+5”) with a dummy noun so that the dependency parser does not fail.\nGEOS uses Stanford dependency parser (Chen and Manning, 2014) to obtain syntactic information, which is used to compute features for relation identification (Table 1). For diagram parsing, similar to Seo et al. (2014), we assume that GEOS has access to ground truth optical character recognition for labels in the diagrams. For optimization, we tune the parameters λ to 0.5, based on the training examples.4\nDataset: We built a dataset of SAT plane geometry questions where every question has a tex-\n4In our dataset, the number of all possible literals for each sentence is at most 1000.\ntual description in English accompanied by a diagram and multiple choices. Questions and answers are compiled from previous official SAT exams and practice exams offered by the College Board (Board, 2014). In addition, we use a portion of the publicly available high-school plane geometry questions (Seo et al., 2014) as our training set.\nWe annotate ground-truth logical forms for all questions in the dataset. Table 2 shows details of the data and annotation statistics. For evaluating dependency parsing, we annotate 50 questions with the ground truth dependency tree structures of all sentences in the questions. 5\nBaselines: Rule-based text parsing + GEOS diagram solves geometry questions using literals extracted from a manually defined set of rules over the textual dependency parser, and scored by diagram. For this baseline, we manually designed 12 high-precision rules based on the development set. Each rule compares the dependency tree of each sentence to pre-defined templates, and if a template pattern is matched, the rule outputs the relation or function structure corresponding to that template. For example, a rule assigns a relation parent(child-1, child-2) for a triplet of (parent, child-1, child-2) where child-1 is the subject of parent and child-2 is the object of the parent.\nGEOS without text parsing solves geometry questions using a simple heuristic. With simple textual processing, this baseline extracts numerical relations from the question text and then computes the scale between the units in the question and the pixels in the diagram. This baseline rounds the number to the closest choice available in the multiple choices.\nGEOS without diagram parsing solves geometry questions only relying on the literals interpreted from the text. It outputs all literals whose text scores are higher than a tuned threshold, 0.6 on the training set.\nGEOS without relation completion solves ge-\n5The source code, the dataset and the annotations are publicly available at geometry.allenai.org.\nometry questions when text parsing does not use the intermediate representation and does not include the relation completion step."
  }, {
    "heading": "9 Experiments",
    "text": "We evaluate our method on three tasks: solving geometry question, interpreting geometry questions, and dependency parsing. Solving Geometry Questions: Table 3 compares the score of GEOS in solving geometry questions in practice and official SAT questions with that of baselines. SAT’s grading scheme penalizes a wrong answer with a negative score of 0.25. We report the SAT score as the percentage of correctly answered questions penalized by the wrong answers. For official questions, GEOS answers 27 questions correctly, 1 questions incorrectly, and leaves 27 un-answered, which gives it a score of 26.75 out of 55, or 49%. Thus, GEOS’s precision exceeds 96% on the 51% of questions that it chooses to answer. For practice SAT questions, GEOS scores 61%.6\nIn order to understand the effect of individual components of GEOS, we compare the full method with a few ablations. GEOS significantly outperforms the two baselines GEOS without text parsing and GEOS without diagram parsing, demonstrating that GEOS benefits from both text and diagram parsing. In order to understand the text parsing component, we compare GEOS with Rule-based text parsing + GEOS Diagram and GEOS without relation completion. The results show that our method of learning to interpret literals from the text is substantially better than the rule-based baseline. In addition, the relation completion step, which relies on the intermediate representation, helps to improve text interpretation. Error Analysis: In order to understand the errors made by GEOS, we use oracle text parsing and oracle diagram parsing (Table 3). Roughly 38% of the errors are due to failures in text parsing, and about 46% of errors are due to failures in diagram parsing. Among them, about 15% of errors were due to failures in both diagram and text parsing. For an example of text parsing failure, the literals in Figure 6 (a) are not scored accurately due to missing coreference relations (Hajishirzi et al., 2013). The rest of errors are due to problems that require more complex reasoning (Figure 6 (b)).\n6Typically, 50th percentile (penalized) score in SAT math section is 27 out of 54 (50%).\nInterpreting Question Texts: Table 4 details the precision and recall of GEOS in deriving literals for geometry question texts for official SAT questions. The rule-based text parsing baseline achieves a high precision, but at the cost of lower recall. On the other hand, the baseline GEOS without diagram achieves a high recall, but at the cost of lower precision. Nevertheless, GEOS attains substantially higher F1 score compared to both baselines, which is the key factor in solving the questions. Direct application of a generic semantic parser (Berant et al., 2013) with full supervision does not perform well in the geometry domain, mainly due to lack of enough training data. Our initial investigations show the performance of 33% F1 in the official set.\nImproving Dependency Parsing: Table 5 shows the results of different methods in dependency parsing. GEOS returns a dependency parse tree by selecting the dependency tree that maximizes the text score in the objective function from the top 50 trees produced by a generic dependency parser, Stanford parser (Chen and Manning, 2014). Note that Stanford parser cannot handle mathematical symbols and equations. We report the results of a baseline that extends the Stanford dependency parser by adding a pre-processing step to separate the mathematical expressions from the plain sentences (Section 8).\nWe evaluate the performance of GEOS against the best tree returned by Stanford parser by reporting the fraction of the questions whose dependency parse structures match the ground truth annotations. Our results show an improvement of 16% over the Stanford dependency parser when equipped with the equation analyzer. For example, in “AB is perpendicular to CD at E”, the Stan-\nford dependency parser predicts that “E” depends on “CD”, while GEOS predicts the correct parse in which “E” depends on “perpendicular”."
  }, {
    "heading": "10 Conclusion",
    "text": "This paper introduced GEOS, an automated system that combines diagram and text interpretation to solve geometry problems. Solving geometry questions was inspired by two important trends in the current NLP literature. The first is in designing methods for grounded language acquisition to map text to a restricted formalism (instead of a full, domain independent representation). We demonstrate a new algorithm for learning to map text to a geometry language with a small amount of training data. The second is designing methods in coupling language and vision and show how processing multimodal information help improve textual or visual interpretations.\nOur experiments on unseen SAT geometry problems achieve a score of 49% of official questions and a score of 61% on practice questions, providing a baseline for future work. Future work includes expanding the geometry language and the reasoning to address a broader set of geometry questions, reducing the amount of supervision, learning the relevant geometry knowledge, and scaling up the dataset.\nAcknowledgements. The research was supported by the Allen Institute for AI, Allen Distinguished Investigator Award, and NSF (IIS1352249). We thank Dan Weld, Luke Zettlemoyer, Aria Haghighi, Mark Hopkins, Eunsol Choi, and the anonymous reviewers for helpful comments."
  }, {
    "heading": "11 Appendix: Proof of Submodularity of",
    "text": "Equation 1\nWe prove that the objective function in equation (1), λA(L′) + H(L′) is submodular by showing that A(L′) andH(L′) are submodular functions. Submodularity of A. Consider L′ ⊂ L, and a new literal to be added, li ∈ L \\ L′. By the definition of A, it is clear that A(L′ ∪ {lj}) = A(L′) +A({lj}). Hence, for all L′′ ⊂ L′ ⊂ L,\nA(L′′ ∪ {lj})−A(L′′) = A(L′ ∪ {lj})−A(L′)\n. Thus A is submodular. Submodularity of H. We prove that the coverage function, Hcov, and the negation of the redundancy function, −Hred are submodular independently, and thus derive that their sum is submodular. For both, consider we are given L′′ ⊂ L′ ⊂ L, and a new literal lj ∈ L \\ L′. Also, let K ′′ and K ′ denote the the sets of concepts covered by L′′ and L′, respectively, and let Kj denote the set of concepts covered by lj . Coverage: Since K ′′ ⊂ K ′, |K ′′ ∪Kj | − |K ′′| ≥ |K ′ ∪Kj | − |K ′|, which is equivalent to\nHcov(L′′ ∪ {lj})−Hcov(L′′) ≥ Hcov(L′ ∪ {lj})−Hcov(L′)\nRedundancy: Note that Hred(L′′ ∪ {lj}) − Hred(L′′) = |K ′′ ∩Kj |, and similarly, Hred(L′ ∪ {lj}) − Hred(L′) = |K ′ ∩Kj |. Since K ′′ ⊂ K ′, thus |K ′′ ∩Kj | ≤ |K ′ ∩Kj |. Hence,\nHred(L′′ ∪ {lj})−Hred(L′′) ≤ Hred(L′ ∪ {lj})−Hred(L′),\nBy negating both sides, we derive that the negation of the redundancy function is submodular."
  }],
  "year": 2015,
  "references": [{
    "title": "Synthesis of geometry proof problems",
    "authors": ["Chris Alvin", "Sumit Gulwani", "Rupak Majumdar", "Supratik Mukhopadhyay."],
    "venue": "AAAI.",
    "year": 2014
  }, {
    "title": "Naturalli: Natural logic inference for common sense reasoning",
    "authors": ["Gabor Angeli", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
    "authors": ["Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "TACL, 1.",
    "year": 2013
  }, {
    "title": "Semantic parsing via paraphrasing",
    "authors": ["J. Berant", "P. Liang."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Semantic parsing on freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Label ranking under ambiguous supervision for learning semantic correspondences",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Jason Weston."],
    "venue": "ICML.",
    "year": 2010
  }, {
    "title": "Learning high-level planning from text",
    "authors": ["SRK Branavan", "Nate Kushman", "Tao Lei", "Regina Barzilay."],
    "venue": "ACL.",
    "year": 2012
  }, {
    "title": "Understanding text with an accompanying diagram",
    "authors": ["William C. Bulko."],
    "venue": "IEA/AIE.",
    "year": 1988
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher D Manning."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Training a multilingual sportscaster: Using perceptual context to learn language",
    "authors": ["David Chen", "Joohyun Kim", "Raymond Mooney."],
    "venue": "JAIR, 37.",
    "year": 2010
  }, {
    "title": "Information extraction",
    "authors": ["Jim Cowie", "Wendy Lehnert."],
    "venue": "Communications of the ACM, 39(1).",
    "year": 1996
  }, {
    "title": "Dependency tree kernels for relation extraction",
    "authors": ["Aron Culotta", "Jeffrey Sorensen."],
    "venue": "ACL.",
    "year": 2004
  }, {
    "title": "Reading to learn: Constructing features from semantic abstracts",
    "authors": ["Jacob Eisenstein", "James Clarke", "Dan Goldwasser", "Dan Roth."],
    "venue": "EMNLP.",
    "year": 2009
  }, {
    "title": "A heuristic program to solve geometric-analogy problems",
    "authors": ["Thomas G Evans."],
    "venue": "Proceedings of the April 21-23, 1964, spring joint computer conference.",
    "year": 1964
  }, {
    "title": "From captions to visual concepts and back",
    "authors": ["Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt"],
    "year": 2014
  }, {
    "title": "Every picture tells a story: Generating sentences from images",
    "authors": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."],
    "venue": "ECCV.",
    "year": 2010
  }, {
    "title": "A discriminative graph-based parser for the abstract meaning representation",
    "authors": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Discriminative reranking for semantic parsing",
    "authors": ["Ruifang Ge", "Raymond J. Mooney."],
    "venue": "ACL.",
    "year": 2006
  }, {
    "title": "Learning from natural instructions",
    "authors": ["Dan Goldwasser", "Dan Roth."],
    "venue": "IJCAI.",
    "year": 2011
  }, {
    "title": "Improving image-sentence embeddings using large weakly annotated photo collections",
    "authors": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik."],
    "venue": "ECCV.",
    "year": 2014
  }, {
    "title": "Using closed captions as supervision for video activity recognition",
    "authors": ["Sonal Gupta", "Raymond J. Mooney."],
    "venue": "AAAI.",
    "year": 2010
  }, {
    "title": "Reasoning about robocup soccer narratives",
    "authors": ["Hannaneh Hajishirzi", "Julia Hockenmaier", "Erik T. Mueller", "Eyal Amir."],
    "venue": "UAI.",
    "year": 2011
  }, {
    "title": "Joint coreference resolution and named-entity linking with multi-pass sieves",
    "authors": ["Hannaneh Hajishirzi", "Leila Zilles", "Daniel S Weld", "Luke S Zettlemoyer."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "10 understanding machines from text and diagrams",
    "authors": ["Mary Hegarty", "Marcel Adam Just."],
    "venue": "Knowledge acquisition from text and pictures.",
    "year": 1989
  }, {
    "title": "Learning knowledge graphs for question answering through conversational dialog",
    "authors": ["Ben Hixon", "Peter Clark", "Hannaneh Hajishirzi."],
    "venue": "NAACL.",
    "year": 2015
  }, {
    "title": "Learning to solve arithmetic word problems with verb categorization",
    "authors": ["Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Learning language semantics from ambiguous supervision",
    "authors": ["Rohit J. Kate", "Raymond J. Mooney."],
    "venue": "AAAI.",
    "year": 2007
  }, {
    "title": "Adapting discriminative reranking to grounded language learning",
    "authors": ["Joohyun Kim", "Raymond J. Mooney."],
    "venue": "ACL.",
    "year": 2013
  }, {
    "title": "Parsing and hypergraphs",
    "authors": ["Dan Klein", "Christopher D Manning."],
    "venue": "New developments in parsing technology. Springer.",
    "year": 2005
  }, {
    "title": "Multi-resolution language grounding with weak supervision",
    "authors": ["R Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ali Farhadi."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "A software package for sequential quadratic programming",
    "authors": ["Dieter et. al. Kraft."],
    "venue": "DFVLR Obersfaffeuhofen, Germany.",
    "year": 1988
  }, {
    "title": "Baby talk: Understanding and generating image descriptions",
    "authors": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg."],
    "venue": "CVPR.",
    "year": 2011
  }, {
    "title": "Learning to automatically solve algebra word problems",
    "authors": ["Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Scaling semantic parsers with on-the-fly ontology matching",
    "authors": ["T Kwiatkowski", "E Choi", "Y Artzi", "L Zettlemoyer."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Learning semantic correspondences with less supervision",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "ACLAFNLP.",
    "year": 2009
  }, {
    "title": "Efficient diagram understanding with characteristic pattern detection",
    "authors": ["Xinggang Lin", "Shigeyoshi Shimotsuji", "Michihiko Minoh", "Toshiyuki Sakai."],
    "venue": "CVGIP, 30(1).",
    "year": 1985
  }, {
    "title": "Modeling multiple strategies for solving geometric analogy problems",
    "authors": ["A. Lovett", "K. Forbus."],
    "venue": "CCS.",
    "year": 2012
  }, {
    "title": "Diagrams for solving physical problems",
    "authors": ["Gordon Novak."],
    "venue": "Diagrammatic reasoning: Cognitive and computational perspectives.",
    "year": 1995
  }, {
    "title": "Unsupervised semantic parsing",
    "authors": ["Hoifung Poon", "Pedro Domingos."],
    "venue": "EMNLP.",
    "year": 2009
  }, {
    "title": "Large-scale semantic parsing without question-answer pairs",
    "authors": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."],
    "venue": "TACL, 2(Oct).",
    "year": 2014
  }, {
    "title": "Reasoning about quantities in natural language",
    "authors": ["S. Roy", "T. Vieira", "D. Roth"],
    "year": 2015
  }, {
    "title": "Diagram understanding in geometry questions",
    "authors": ["Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni."],
    "venue": "AAAI.",
    "year": 2014
  }, {
    "title": "Computational models for integrating linguistic and visual information: A survey",
    "authors": ["Rohini K Srihari."],
    "venue": "Artificial Intelligence Review, 8(5-6).",
    "year": 1994
  }, {
    "title": "Learning to follow navigational directions",
    "authors": ["Adam Vogel", "Daniel Jurafsky."],
    "venue": "ACL.",
    "year": 2010
  }, {
    "title": "Global optimization by basin-hopping and the lowest energy structures of lennard-jones clusters containing up to 110 atoms",
    "authors": ["David J Wales", "Jonathan PK Doye."],
    "venue": "The Journal of Physical Chemistry A, 101(28).",
    "year": 1997
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "UAI.",
    "year": 2005
  }],
  "id": "SP:c04fe8aa97bee94b285585f4096706894137f0ce",
  "authors": [{
    "name": "Minjoon Seo",
    "affiliations": []
  }, {
    "name": "Hannaneh Hajishirzi",
    "affiliations": []
  }, {
    "name": "Ali Farhadi",
    "affiliations": []
  }, {
    "name": "Oren Etzioni",
    "affiliations": []
  }, {
    "name": "Clint Malcolm",
    "affiliations": []
  }],
  "abstractText": "This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation. We model the problem of understanding geometry questions as submodular optimization, and identify a formal problem description likely to be compatible with both the question text and diagram. GEOS then feeds the description to a geometric solver that attempts to determine the correct answer. In our experiments, GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions.1 Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text.",
  "title": "Solving Geometry Problems: Combining Text and Diagram Interpretation"
}