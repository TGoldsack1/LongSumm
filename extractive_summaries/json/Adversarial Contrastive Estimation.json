{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1021–1032 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1021\nLearning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics."
  }, {
    "heading": "1 Introduction",
    "text": "Many models learn by contrasting losses on observed positive examples with those on some fictitious negative examples, trying to decrease some score on positive ones while increasing it on negative ones. There are multiple reasons why such contrastive learning approach is needed. Computational tractability is one. For instance, instead of using softmax to predict a word for learning word embeddings, noise contrastive estimation (NCE) (Dyer, 2014; Mnih and Teh, 2012) can be used in skip-gram or CBOW word embedding models (Gutmann and Hyvärinen, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Vaswani et al., 2013). Another reason is\n∗authors contributed equally †Work done while author was an intern at Borealis AI\nmodeling need, as certain assumptions are best expressed as some score or energy in margin based or un-normalized probability models (Smith and Eisner, 2005). For example, modeling entity relations as translations or variants thereof in a vector space naturally leads to a distance-based score to be minimized for observed entity-relation-entity triplets (Bordes et al., 2013).\nGiven a scoring function, the gradient of the model’s parameters on observed positive examples can be readily computed, but the negative phase requires a design decision on how to sample data. In noise contrastive estimation for word embeddings, a negative example is formed by replacing a component of a positive pair by randomly selecting a sampled word from the vocabulary, resulting in a fictitious word-context pair which would be unlikely to actually exist in the dataset. This negative sampling by corruption approach is also used in learning knowledge graph embeddings (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015; Wang et al., 2014; Trouillon et al., 2016; Yang et al., 2014; Dettmers et al., 2017), order embeddings (Vendrov et al., 2016), caption generation (Dai and Lin, 2017), etc.\nTypically the corruption distribution is the same for all inputs like in skip-gram or CBOW NCE, rather than being a conditional distribution that takes into account information about the input sample under consideration. Furthermore, the corruption process usually only encodes a human prior as to what constitutes a hard negative sample, rather than being learned from data. For these two reasons, the simple fixed corruption process often yields only easy negative examples. Easy negatives are sub-optimal for learning discriminative representation as they do not force the model to find critical characteristics of observed positive data, which has been independently discovered in applications outside NLP previously (Shrivastava\net al., 2016). Even if hard negatives are occasionally reached, the infrequency means slow convergence. Designing a more sophisticated corruption process could be fruitful, but requires costly trialand-error by a human expert.\nIn this work, we propose to augment the simple corruption noise process in various embedding models with an adversarially learned conditional distribution, forming a mixture negative sampler that adapts to the underlying data and the embedding model training progress. The resulting method is referred to as adversarial contrastive estimation (ACE). The adaptive conditional model engages in a minimax game with the primary embedding model, much like in Generative Adversarial Networks (GANs) (Goodfellow et al., 2014a), where a discriminator net (D), tries to distinguish samples produced by a generator (G) from real data (Goodfellow et al., 2014b). In ACE, the main model learns to distinguish between a real positive example and a negative sample selected by the mixture of a fixed NCE sampler and an adversarial generator. The main model and the generator takes alternating turns to update their parameters. In fact, our method can be viewed as a conditional GAN (Mirza and Osindero, 2014) on discrete inputs, with a mixture generator consisting of a learned and a fixed distribution, with additional techniques introduced to achieve stable and convergent training of embedding models.\nIn our proposed ACE approach, the conditional sampler finds harder negatives than NCE, while being able to gracefully fall back to NCE whenever the generator cannot find hard negatives. We demonstrate the efficacy and generality of the proposed method on three different learning tasks, word embeddings (Mikolov et al., 2013), order embeddings (Vendrov et al., 2016) and knowledge graph embeddings (Ji et al., 2015)."
  }, {
    "heading": "2 Method",
    "text": ""
  }, {
    "heading": "2.1 Background: contrastive learning",
    "text": "In the most general form, our method applies to supervised learning problems with a contrastive objective of the following form:\nL(ω) = Ep(x+,y+,y−) lω(x+, y+, y−) (1)\nwhere lω(x+, y+, y−) captures both the model with parameters ω and the loss that scores a positive tuple (x+, y+) against a negative one (x+, y−). Ep(x+,y+,y−)(.) denotes expectation\nwith respect to some joint distribution over positive and negative samples. Furthermore, by the law of total expectation, and the fact that given x+, the negative sampling is not dependent on the positive label, i.e. p(y+, y−|x+) = p(y+|x+)p(y−|x+), Eq. 1 can be re-written as\nEp(x+)[Ep(y+|x+)p(y−|x+) lω(x+, y+, y−)] (2)\nSeparable loss In the case where the loss decomposes into a sum of scores on positive and negative tuples such as lω(x +, y+, y−) = sω (x +, y+)−s̃ω (x+, y−), then Expression. 2 becomes\nEp+(x)[Ep+(y|x) sω (x, y)− Ep−(y|x) s̃ω (x, y)] (3) where we moved the + and − to p for notational brevity. Learning by stochastic gradient descent aims to adjust ω to pushing down sω (x, y) on samples from p+ while pushing up s̃ω (x, y) on samples from p−. Note that for generality, the scoring function for negative samples, denoted by s̃ω, could be slightly different from sω. For instance, s̃ could contain a margin as in the case of Order Embeddings in Sec. 4.2.\nNon separable loss Eq. 1 is the general form that we would like to consider because for certain problems, the loss function cannot be separated into sums of terms containing only positive (x+, y+) and terms with negatives (x+, y−). An example of such a nonseparable loss is the triplet ranking loss (Schroff et al., 2015): lω = max(0, η + sω (x+, y+) − sω (x\n+, y−)), which does not decompose due to the rectification.\nNoise contrastive estimation The typical NCE approach in tasks such as word embeddings (Mikolov et al., 2013), order embeddings (Vendrov et al., 2016), and knowledge graph embeddings can be viewed as a special case of Eq. 2 by taking p(y−|x+) to be some unconditional pnce(y).\nThis leads to efficient computation during training, however, pnce(y) sacrifices the sampling efficiency of learning as the negatives produced using a fixed distribution are not tailored toward x+, and as a result are not necessarily hard negative examples. Thus, the model is not forced to discover discriminative representation of observed positive\ndata. As training progresses, more and more negative examples are correctly learned, the probability of drawing a hard negative example diminishes further, causing slow convergence."
  }, {
    "heading": "2.2 Adversarial mixture noise",
    "text": "To remedy the above mentioned problem of a fixed unconditional negative sampler, we propose to augment it into a mixture one, λpnce(y) + (1− λ)gθ(y|x), where gθ is a conditional distribution with a learnable parameter θ and λ is a hyperparameter. The objective in Expression. 2 can then be written as (conditioned on x for notational brevity):\nL(ω, θ;x) = λEp(y+|x)pnce(y−) lω(x, y +, y−)\n+ (1− λ)Ep(y+|x)gθ(y−|x) lω(x, y +, y−) (4)\nWe learn (ω, θ) in a GAN-style minimax game:\nmin ω max θ V (ω, θ) = min ω max θ Ep+(x) L(ω, θ;x) (5) The embedding model behind lω(x, y+, y−) is similar to the discriminator in (conditional) GAN (or critic in Wasserstein (Arjovsky et al., 2017) or Energy-based GAN (Zhao et al., 2016), while gθ(y|x) acts as the generator. Henceforth, we will use the term discriminator (D) and embedding model interchangeably, and refer to gθ as the generator."
  }, {
    "heading": "2.3 Learning the generator",
    "text": "There is one important distinction to typical GAN: gθ(y|x) defines a categorical distribution over possible y values, and samples are drawn accordingly; in contrast to typical GAN over continuous data space such as images, where samples are generated by an implicit generative model that warps noise vectors into data points. Due to the discrete sampling step, gθ cannot learn by receiving gradient through the discriminator. One possible solution is to use the Gumbel-softmax reparametrization trick (Jang et al., 2016; Maddison et al., 2016), which gives a differentiable approximation. However, this differentiability comes at the cost of drawing N Gumbel samples per each categorical sample, where N is the number of categories. For word embeddings, N is the vocabulary size, and for knowledge graph embeddings, N is the number of entities, both leading to infeasible computational requirements.\nInstead, we use the REINFORCE (Williams, 1992) gradient estimator for∇θL(θ, x):\n(1−λ)E [ −lω(x, y+, y−)∇θ log(gθ(y−|x)) ] (6)\nwhere the expectation E is with respect to p(y+, y−|x) = p(y+|x)gθ(y−|x), and the discriminator loss lω(x, y+, y−) acts as the reward.\nWith a separable loss, the (conditional) value function of the minimax game is:\nL(ω, θ;x) = Ep+(y|x) sω (x, y) − Epnce(y) s̃ω (x, y)− Egθ(y|x) s̃ω (x, y) (7)\nand only the last term depends on the generator parameter ω. Hence, with a separable loss, the reward is −s̃(x+, y−). This reduction does not happen with a non-separable loss, and we have to use lω(x, y +, y−)."
  }, {
    "heading": "2.4 Entropy and training stability",
    "text": "GAN training can suffer from instability and degeneracy where the generator probability mass collapses to a few modes or points. Much work has been done to stabilize GAN training in the continuous case (Arjovsky et al., 2017; Gulrajani et al., 2017; Cao et al., 2018). In ACE, if the generator gθ probability mass collapses to a few candidates, then after the discriminator successfully learns about these negatives, gθ cannot adapt to select new hard negatives, because the REINFORCE gradient estimator Eq. 6 relies on gθ being able to explore other candidates during sampling. Therefore, if the gθ probability mass collapses, instead of leading to oscillation as in typical GAN, the min-max game in ACE reaches an equilibrium where the discriminator wins and gθ can no longer adapt, then ACE falls back to NCE since the negative sampler has another mixture component from NCE.\nThis behavior of gracefully falling back to NCE is more desirable than the alternative of stalled training if p−(y|x) does not have a simple pnce mixture component. However, we would still like to avoid such collapse, as the adversarial samples provide greater learning signals than NCE samples. To this end, we propose to use a regularizer to encourage the categorical distribution gθ(y|x) to have high entropy. In order to make the the regularizer interpretable and its hyperparameters easy to tune, we design the following form:\nRent(x) = min(0, c−H(gθ(y|x))) (8)\nwhereH(gθ(y|x)) is the entropy of the categorical distribution gθ(y|x), and c = log(k) is the entropy of a uniform distribution over k choices, and k is a hyper-parameter. Intuitively, Rent expresses the prior that the generator should spread its mass over more than k choices for each x."
  }, {
    "heading": "2.5 Handling false negatives",
    "text": "During negative sampling, p−(y|x) could actually produce y that forms a positive pair that exists in the training set, i.e., a false negative. This possibility exists in NCE already, but since pnce is not adaptive, the probability of sampling a false negative is low. Hence in NCE, the score on this false negative (true observation) pair is pushed up less in the negative term than in the positive term.\nHowever, with the adaptive sampler, gω(y|x), false negatives become a much more severe issue. gω(y|x) can learn to concentrate its mass on a few false negatives, significantly canceling the learning of those observations in the positive phase. The entropy regularization reduces this problem as it forces the generator to spread its mass, hence reducing the chance of a false negative.\nTo further alleviate this problem, whenever computationally feasible, we apply an additional two-step technique. First, we maintain a hash map of the training data in memory, and use it to efficiently detect if a negative sample (x+, y−) is an actual observation. If so, its contribution to the loss is given a zero weight in ω learning step. Second, to upate θ in the generator learning step, the reward for false negative samples are replaced by a large penalty, so that the REINFORCE gradient update would steer gθ away from those samples. The second step is needed to prevent null computation where gθ learns to sample false negatives which are subsequently ignored by the discriminator update for ω."
  }, {
    "heading": "2.6 Variance Reduction",
    "text": "The basic REINFORCE gradient estimator is poised with high variance, so in practice one often needs to apply variance reduction techniques. The most basic form of variance reduction is to subtract a baseline from the reward. As long as the baseline is not a function of actions (i.e., samples y− being drawn), the REINFORCE gradient estimator remains unbiased. More advanced gradient estimators exist that also reduce variance (Grathwohl et al., 2017; Tucker et al., 2017; Liu et al., 2018), but for simplicity we use the\nself-critical baseline method (Rennie et al., 2016), where the baseline is b(x) = lω(y+, y?, x), or b(x) = −s̃ω(y?, x) in the separable loss case, and y? = argmaxigθ(yi|x). In other words, the baseline is the reward of the most likely sample according to the generator."
  }, {
    "heading": "2.7 Improving exploration in gθ by leveraging NCE samples",
    "text": "In Sec. 2.4 we touched on the need for sufficient exploration in gθ. It is possible to also leverage negative samples from NCE to help the generator learn. This is essentially off-policy exploration in reinforcement learning since NCE samples are not drawn according to gθ(y|x). The generator learning can use importance re-weighting to leverage those samples. The resulting REINFORCE gradient estimator is basically the same as Eq. 6 except that the rewards are reweighted by gθ(y\n−|x)/pnce(y−), and the expectation is with respect to p(y+|x)pnce(y−). This additional offpolicy learning term provides gradient information for generator learning if gθ(y−|x) is not zero, meaning that for it to be effective in helping exploration, the generator cannot be collapsed at the first place. Hence, in practice, this term is only used to further help on top of the entropy regularization, but it does not replace it."
  }, {
    "heading": "3 Related Work",
    "text": "Smith and Eisner (2005) proposed contrastive estimation as a way for unsupervised learning of log-linear models by taking implicit evidence from user-defined neighborhoods around observed datapoints. Gutmann and Hyvärinen (2010) introduced NCE as an alternative to the hierarchical softmax. In the works of Mnih and Teh (2012) and Mnih and Kavukcuoglu (2013), NCE is applied to log-bilinear models and Vaswani et al. (2013) applied NCE to neural probabilistic language models (Yoshua et al., 2003). Compared to these previous NCE methods that rely on simple fixed sampling heuristics, ACE uses an adaptive sampler that produces harder negatives.\nIn the domain of max-margin estimation for structured prediction (Taskar et al., 2005), loss augmented MAP inference plays the role of finding hard negatives (the hardest). However, this inference is only tractable in a limited class of models such structured SVM (Tsochantaridis et al., 2005). Compared to those models that use exact\nmaximization to find the hardest negative configuration each time, the generator in ACE can be viewed as learning an approximate amortized inference network. Concurrently to this work, Tu and Gimpel (2018) proposes a very similar framework, using a learned inference network for Structured prediction energy networks (SPEN) (Belanger and McCallum, 2016).\nConcurrent with our work, there have been other interests in applying the GAN to NLP problems (Fedus et al., 2018; Wang et al., 2018; Cai and Wang, 2017). Knowledge graph models naturally lend to a GAN setup, and has been the subject of study in Wang et al. (2018) and Cai and Wang (2017). These two concurrent works are most closely related to one of the three tasks on which we study ACE in this work. Besides a more general formulation that applies to problems beyond those considered in Wang et al. (2018) and Cai and Wang (2017), the techniques introduced in our work on handling false negatives and entropy regularization lead to improved experimental results as shown in Sec. 5.4."
  }, {
    "heading": "4 Application of ACE on three tasks",
    "text": ""
  }, {
    "heading": "4.1 Word Embeddings",
    "text": "Word embeddings learn a vector representation of words from co-occurrences in a text corpus. NCE casts this learning problem as a binary classification where the model tries to distinguish positive word and context pairs, from negative noise samples composed of word and false context pairs. The NCE objective in Skip-gram (Mikolov et al., 2013) for word embeddings is a separable loss of the form:\nL = − ∑ wt∈V [log p(y = 1|wt, w+c )\n+ K∑ c=1 log p(y = 0|wt, w−c )] (9)\nHere, w+c is sampled from the set of true contexts and w−c ∼ Q is sampled k times from a fixed noise distribution. Mikolov et al. (2013) introduced a further simplification of NCE, called “Negative Sampling” (Dyer, 2014). With respect to our ACE framework, the difference between NCE and Negative Sampling is inconsequential, so we continue the discussion using NCE. A drawback of this sampling scheme is that it favors more common words as context. Another issue\nis that the negative context words are sampled in the same way, rather than tailored toward the actual target word. To apply ACE to this problem we first define the value function for the minimax game, V (D,G), as follows:\nV (D,G) = Ep+(wc)[logD(wc, wt)] −Epnce(wc)[− log(1−D(wc, wt))] −Egθ(wc|wt)[− log(1−D(wc, wt))]\n(10)\nwith D = p(y = 1|wt, wc) and G = gθ(wc|wt).\nImplementation details For our experiments, we train all our models on a single pass of the May 2017 dump of the English Wikipedia with lowercased unigrams. The vocabulary size is restricted to the top 150k most frequent words when training from scratch while for finetuning we use the same vocabulary as Pennington et al. (2014), which is 400k of the most frequent words. We use 5 NCE samples for each positive sample and 1 adversarial sample in a window size of 10 and the same positive subsampling scheme proposed by Mikolov et al. (2013). Learning for both G and D uses Adam (Kingma and Ba, 2014) optimizer with its default parameters. Our conditional discriminator is modeled using the Skip-Gram architecture, which is a two layer neural network with a linear mapping between the layers. The generator network consists of an embedding layer followed by two small hidden layers, followed by an output softmax layer. The first layer of the generator shares its weights with the second embedding layer in the discriminator network, which we find really speeds up convergence as the generator does not have to relearn its own set of embeddings. The difference between the discriminator and generator is that a sigmoid nonlinearity is used after the second layer in the discriminator, while in the generator, a softmax layer is used to define a categorical distribution over negative word candidates. We find that controlling the generator entropy is critical for finetuning experiments as otherwise the generator collapses to its favorite negative sample. The word embeddings are taken to be the first dense matrix in the discriminator."
  }, {
    "heading": "4.2 Order Embeddings Hypernym Prediction",
    "text": "As introduced in Vendrov et al. (2016), ordered representations over hierarchy can be learned by\norder embeddings. An example task for such ordered representation is hypernym prediction. A hypernym pair is a pair of concepts where the first concept is a specialization or an instance of the second.\nFor completeness, we briefly describe order embeddings, then analyze ACE on the hypernym prediction task. In order embeddings, each entity is represented by a vector in RN , the score for a positive ordered pair of entities (x, y) is defined by sω(x, y) = ||max(0, y − x)||2 and, score for a negative ordered pair (x+, y−) is defined by s̃ω(x\n+, y−) = max{0, η−s(x+, y−)}, where is η is the margin. Let f(u) be the embedding function which takes an entity as input and outputs en embedding vector. We define P as a set of positive pairs and N as negative pairs, the separable loss function for order embedding task is defined by: L= ∑\n(u,v)∈P\nsω(f(u), f(v)))+ ∑\n(u,v)∈N\ns̃(f(u), f(v))\n(11)\nImplementation details Our generator for this task is just a linear fully connected softmax layer, taking an embedding vector from discriminator as input and outputting a categorical distribution over the entity set. For the discriminator, we inherit all model setting from Vendrov et al. (2016): we use 50 dimensions hidden state and bash size 1000, a learning rate of 0.01 and the Adam optimizer. For the generator, we use a batch size of 1000, a learning rate 0.01 and the Adam optimizer. We apply weight decay with rate 0.1 and entropy loss regularization as described in Sec. 2.4. We handle false negative as described in Sec. 2.5. After cross validation, variance reduction and leveraging NCE samples does not greatly affect the order embedding task."
  }, {
    "heading": "4.3 Knowledge Graph Embeddings",
    "text": "Knowledge graphs contain entity and relation data of the form (head entity, relation, tail entity), and the goal is to learn from observed positive entity relations and predict missing links (a.k.a. link prediction). There have been many works on knowledge graph embeddings, e.g. TransE (Bordes et al., 2013), TransR (Lin et al., 2015), TransH (Wang et al., 2014), TransD (Ji et al., 2015), Complex (Trouillon et al., 2016), DistMult (Yang et al., 2014) and ConvE (Dettmers et al., 2017). Many of them use a contrastive learning objective. Here we\ntake TransD as an example, and modify its noise contrastive learning to ACE, and demonstrate significant improvement in sample efficiency and link prediction results.\nImplementation details Let a positive entity-relation-entity triplet be denoted by ξ+ = (h+, r+, t+), and a negative triplet could either have its head or tail be a negative sample, i.e. ξ− = (h−, r+, t+) or ξ− = (h+, r+, t−). In either case, the general formulation in Sec. 2.1 still applies. The non-separable loss function takes on the form:\nl = max(0, η + sω(ξ +)− sω(ξ−)) (12)\nThe scoring rule is:\ns = ‖h⊥ + r− t⊥‖ (13)\nwhere r is the embedding vector for r, and h⊥ is projection of the embedding of h onto the space of r by h⊥ = h + rph>p h, where rp and hp are projection parameters of the model. t⊥ is defined in a similar way through parameters t, tp and rp.\nThe form of the generator gθ(t−|r+, h+) is chosen to be fθ(h⊥,h⊥ + r), where fθ is a feedforward neural net that concatenates its two input arguments, then propagates through two hidden layers, followed by a final softmax output layer. As a function of (r+, h+), gθ shares parameter with the discriminator, as the inputs to fθ are the embedding vectors. During generator learning, only θ is updated and the TransD model embedding parameters are frozen."
  }, {
    "heading": "5 Experiments",
    "text": "We evaluate ACE with experiments on word embeddings, order embeddings, and knowledge graph embeddings tasks. In short, whenever the original learning objective is contrastive (all tasks except Glove fine-tuning) our results consistently show that ACE improves over NCE. In some cases, we include additional comparisons to the state-of-art results on the task to put the significance of such improvements in context: the generic ACE can often make a reasonable baseline competitive with SOTA methods that are optimized for the task.\nFor word embeddings, we evaluate models trained from scratch as well as fine-tuned Glove models (Pennington et al., 2014) on word similarity tasks that consist of computing the similarity\nFigure 3: Left: Rare Word, Right: WS353 similarity scores during the first epoch of training.\nFigure 4: Training from scratch losses on the Discriminator\nbetween word pairs where the ground truth is an average of human scores. We choose the Rare word dataset (Luong et al., 2013) and WordSim353 (Finkelstein et al., 2001) by virtue of our hypothesis that ACE learns better representations for both rare and frequent words. We also qualitatively evaluate ACE word embeddings by inspecting the nearest neighbors of selected words.\nFor the hypernym prediction task, following Vendrov et al. (2016), hypernym pairs are created from the WordNet hierarchy’s transitive closure. We use the released random development split and test split from Vendrov et al. (2016), which both contain 4000 edges.\nFor knowledge graph embeddings, we use TransD (Ji et al., 2015) as our base model, and perform ablation study to analyze the behavior of ACE with various add-on features, and confirm that entropy regularization is crucial for good performance in ACE. We also obtain link prediction results that are competitive or superior to the stateof-arts on the WN18 dataset (Bordes et al., 2014)."
  }, {
    "heading": "5.1 Training Word Embeddings from scratch",
    "text": "In this experiment, we empirically observe that training word embeddings using ACE converges significantly faster than NCE after one epoch. As shown in Fig. 3 both ACE (a mixture of pnce and gθ) and just gθ (denoted by ADV) significantly outperforms the NCE baseline, with an absolute improvement of 73.1% and 58.5% respectively on RW score. We note similar results on WordSim353 dataset where ACE and ADV outperforms\nNCE by 40.4% and 45.7%. We also evaluate our model qualitatively by inspecting the nearest neighbors of selected words in Table. 1. We first present the five nearest neighbors to each word to show that both NCE and ACE models learn sensible embeddings. We then show that ACE embeddings have much better semantic relevance in a larger neighborhood (nearest neighbor 45-50)."
  }, {
    "heading": "5.2 Finetuning Word Embeddings",
    "text": "We take off-the-shelf pre-trained Glove embeddings which were trained using 6 billion tokens (Pennington et al., 2014) and fine-tune them using our algorithm. It is interesting to note that the original Glove objective does not fit into the contrastive learning framework, but nonetheless we find that they benefit from ACE. In fact, we observe that training such that 75% of the words appear as positive contexts is sufficient to beat the largest dimensionality pre-trained Glove model on word similarity tasks. We evaluate our performance on the Rare Word and WordSim353 data. As can be seen from our results in Table 2, ACE on RW is not always better and for the 100d and 300d Glove embeddings is marginally worse. However, on WordSim353 ACE does considerably better across the board to the point where 50d Glove embeddings outperform the 300d baseline Glove model."
  }, {
    "heading": "5.3 Hypernym Prediction",
    "text": "As shown in Table 3, with ACE training, our method achieves a 1.5% improvement on accu-\nracy over Vendrov et al. (2016) without tunning any of the discriminator’s hyperparameters. We further report training curve in Fig. 1, we report loss curve on randomly sampled pairs. We stress that in the ACE model, we train random pairs and generator generated pairs jointly, as shown in Fig. 2, hard negatives help the order embedding model converges faster."
  }, {
    "heading": "5.4 Ablation Study and Improving TransD",
    "text": "To analyze different aspects of ACE, we perform an ablation study on the knowledge graph embedding task. As described in Sec. 4.3, the base\nmodel (discriminator) we apply ACE to is TransD (Ji et al., 2015). Fig. 5 shows validation performance as training progresses. All variants of ACE converges to better results than base NCE. Among ACE variants, all methods that include entropy regularization significantly outperform without entropy regularization. Without the self critical baseline variance reduction, learning could progress faster at the beginning but the final performance suffers slightly. The best performance is obtained without the additional off-policy learning of the generator.\nTable. 4 shows the final test results on WN18 link prediction task. It is interesting to note that ACE improves MRR score more significantly than hit@10. As MRR is a lot more sensitive to the top rankings, i.e., how the correct configuration ranks among the competitive alternatives, this is consistent with the fact that ACE samples hard negatives and forces the base model to learn a more discriminative representation of the positive examples."
  }, {
    "heading": "5.5 Hard Negative Analysis",
    "text": "To better understand the effect of the adversarial samples proposed by the generator we plot the discriminator loss on both pnce and gθ samples. In this context, a harder sample means a higher loss assigned by the discriminator. Fig. 4 shows that discriminator loss for the word embedding task on gθ samples are always higher than on pnce samples, confirming that the generator is indeed sampling harder negatives. For Hypernym Prediction task, Fig.2 shows discriminator loss on negative pairs sampled from NCE and ACE respectively. The higher the loss the harder the negative pair is. As indicated in the left plot, loss on the ACE negative terms collapses\nfaster than on the NCE negatives. After adding entropy regularization and weight decay, the generator works as expected."
  }, {
    "heading": "6 Limitations",
    "text": "When the generator softmax is large, the current implementation of ACE training is computationally expensive. Although ACE converges faster per iteration, it may converge more slowly on wall-clock time depending on the cost of the softmax. However, embeddings are typically used as pre-trained building blocks for subsequent tasks. Thus, their learning is usually the pre-computation step for the more complex downstream models and spending more time is justified, especially with GPU acceleration. We believe that the computational cost could potentially be reduced via some existing techniques such as the “augment and reduce” variational inference of (Ruiz et al., 2018), adaptive softmax (Grave et al., 2016), or the “sparsely-gated” softmax of Shazeer et al. (2017), but leave that to future work.\nAnother limitation is on the theoretical front. As noted in Goodfellow (2014), GAN learning does not implement maximum likelihood estimation (MLE), while NCE has MLE as an asymptotic limit. To the best of our knowledge, more distant connections between GAN and MLE training are not known, and tools for analyzing the equilibrium of a min-max game where players are parametrized by deep neural nets are currently not available to the best of our knowledge."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we propose Adversarial Contrastive Estimation as a general technique for improving supervised learning problems that learn by contrasting observed and fictitious samples. Specifically, we use a generator network in a conditional GAN like setting to propose hard negative examples for our discriminator model. We find that a mixture distribution of randomly sampling negative examples along with an adaptive negative sampler leads to improved performances on a variety of embedding tasks. We validate our hypothesis that hard negative examples are critical to optimal learning and can be proposed via our ACE framework. Finally, we find that controlling the entropy of the generator through a regularization term and properly handling false negatives is crucial for successful training."
  }],
  "year": 2018,
  "references": [{
    "title": "Wasserstein GAN",
    "authors": ["Martin Arjovsky", "Soumith Chintala", "Léon Bottou."],
    "venue": "arXiv preprint arXiv:1701.07875.",
    "year": 2017
  }, {
    "title": "Structured prediction energy networks",
    "authors": ["David Belanger", "Andrew McCallum."],
    "venue": "International Conference on Machine Learning, pages 983–992.",
    "year": 2016
  }, {
    "title": "A semantic matching energy function for learning with multi-relational data",
    "authors": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."],
    "venue": "Machine Learning, 94(2):233–259.",
    "year": 2014
  }, {
    "title": "Translating embeddings for modeling multirelational data",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."],
    "venue": "Advances in neural information processing systems, pages 2787–2795.",
    "year": 2013
  }, {
    "title": "Kbgan: Adversarial learning for knowledge graph embeddings",
    "authors": ["Liwei Cai", "William Yang Wang."],
    "venue": "arXiv preprint arXiv:1711.04071.",
    "year": 2017
  }, {
    "title": "Improving GAN training via binarized representation entropy (BRE) regularization",
    "authors": ["Yanshuai Cao", "Gavin Weiguang Ding", "Kry Yik-Chau Lui", "Ruitong Huang."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Contrastive learning for image captioning",
    "authors": ["Bo Dai", "Dahua Lin."],
    "venue": "Advances in Neural Information Processing Systems, pages 898–907.",
    "year": 2017
  }, {
    "title": "Convolutional 2d knowledge graph embeddings",
    "authors": ["Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel."],
    "venue": "arXiv preprint arXiv:1707.01476.",
    "year": 2017
  }, {
    "title": "Notes on noise contrastive estimation and negative sampling",
    "authors": ["Chris Dyer."],
    "venue": "arXiv preprint arXiv:1410.8251.",
    "year": 2014
  }, {
    "title": "MaskGAN: Better text generation via filling in the",
    "authors": ["William Fedus", "Ian Goodfellow", "Andrew M Dai."],
    "venue": "arXiv preprint arXiv:1801.07736.",
    "year": 2018
  }, {
    "title": "Placing search in context: The concept revisited",
    "authors": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."],
    "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406–",
    "year": 2001
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,",
    "year": 2014
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Advances in neural information processing systems, pages 2672–2680.",
    "year": 2014
  }, {
    "title": "On distinguishability criteria for estimating generative models",
    "authors": ["Ian J Goodfellow."],
    "venue": "arXiv preprint arXiv:1412.6515.",
    "year": 2014
  }, {
    "title": "Backpropagation through the void: Optimizing control variates for black-box gradient estimation",
    "authors": ["Will Grathwohl", "Dami Choi", "Yuhuai Wu", "Geoff Roeder", "David Duvenaud."],
    "venue": "arXiv preprint arXiv:1711.00123.",
    "year": 2017
  }, {
    "title": "Efficient softmax approximation for GPUs",
    "authors": ["Edouard Grave", "Armand Joulin", "Moustapha Cissé", "David Grangier", "Hervé Jégou."],
    "venue": "arXiv preprint arXiv:1609.04309.",
    "year": 2016
  }, {
    "title": "Improved training of wasserstein gans",
    "authors": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron C Courville."],
    "venue": "Advances in Neural Information Processing Systems, pages 5769–5779.",
    "year": 2017
  }, {
    "title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models",
    "authors": ["Michael Gutmann", "Aapo Hyvärinen."],
    "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297–304.",
    "year": 2010
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["Michael U Gutmann", "Aapo Hyvärinen."],
    "venue": "Journal of Machine Learning Research, 13(Feb):307–361.",
    "year": 2012
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole."],
    "venue": "arXiv preprint arXiv:1611.01144.",
    "year": 2016
  }, {
    "title": "Knowledge graph embedding via dynamic mapping matrix",
    "authors": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Learning entity and relation embeddings for knowledge graph completion",
    "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."],
    "venue": "AAAI, volume 15, pages 2181–2187.",
    "year": 2015
  }, {
    "title": "Action-dependent control variates for policy optimization via stein identity",
    "authors": ["Hao Liu", "Yihao Feng", "Yi Mao", "Dengyong Zhou", "Jian Peng", "Qiang Liu."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Better word representations with recursive neural networks for morphology",
    "authors": ["Thang Luong", "Richard Socher", "Christopher D Manning."],
    "venue": "CoNLL, pages 104–113.",
    "year": 2013
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables",
    "authors": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh."],
    "venue": "arXiv preprint arXiv:1611.00712.",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Conditional Generative Adversarial Nets",
    "authors": ["M. Mirza", "S. Osindero."],
    "venue": "ArXiv e-prints.",
    "year": 2014
  }, {
    "title": "Learning word embeddings efficiently with noise-contrastive estimation",
    "authors": ["Andriy Mnih", "Koray Kavukcuoglu."],
    "venue": "Advances in neural information processing systems, pages 2265–2273.",
    "year": 2013
  }, {
    "title": "A fast and simple algorithm for training neural probabilistic language models",
    "authors": ["Andriy Mnih", "Yee Whye Teh."],
    "venue": "arXiv preprint arXiv:1206.6426.",
    "year": 2012
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Self-critical sequence training for image captioning",
    "authors": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."],
    "venue": "arXiv preprint arXiv:1612.00563.",
    "year": 2016
  }, {
    "title": "Augment and reduce: Stochastic inference for large categorical distributions",
    "authors": ["Francisco JR Ruiz", "Michalis K Titsias", "Adji B Dieng", "David M Blei."],
    "venue": "arXiv preprint arXiv:1802.04220.",
    "year": 2018
  }, {
    "title": "Facenet: A unified embedding for face recognition and clustering",
    "authors": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815–823.",
    "year": 2015
  }, {
    "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
    "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean."],
    "venue": "arXiv preprint arXiv:1701.06538.",
    "year": 2017
  }, {
    "title": "Training region-based object detectors with online hard example mining",
    "authors": ["Abhinav Shrivastava", "Abhinav Gupta", "Ross Girshick."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 761–769.",
    "year": 2016
  }, {
    "title": "Contrastive estimation: Training log-linear models on unlabeled data",
    "authors": ["Noah A Smith", "Jason Eisner."],
    "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354–362. Association for Computational Linguis-",
    "year": 2005
  }, {
    "title": "Learning structured prediction models: A large margin approach",
    "authors": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."],
    "venue": "Proceedings of the 22nd international conference on Machine learning, pages 896–903. ACM.",
    "year": 2005
  }, {
    "title": "Complex embeddings for simple link prediction",
    "authors": ["Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard."],
    "venue": "International Conference on Machine Learning, pages 2071–2080.",
    "year": 2016
  }, {
    "title": "Large margin methods for structured and interdependent output variables",
    "authors": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun."],
    "venue": "Journal of machine learning research, 6(Sep):1453–1484.",
    "year": 2005
  }, {
    "title": "Learning approximate inference networks for structured prediction",
    "authors": ["Lifu Tu", "Kevin Gimpel."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models",
    "authors": ["George Tucker", "Andriy Mnih", "Chris J Maddison", "John Lawson", "Jascha Sohl-Dickstein."],
    "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-",
    "year": 2017
  }, {
    "title": "Decoding with largescale neural language models improves translation",
    "authors": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2013
  }, {
    "title": "Order-embeddings of images and language",
    "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."],
    "venue": "International Conference on Learning Representations.",
    "year": 2016
  }, {
    "title": "Incorporating GAN for negative sampling in knowledge representation learning",
    "authors": ["Peifeng Wang", "Shuangyin Li", "Rong Pan."],
    "venue": "The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI18).",
    "year": 2018
  }, {
    "title": "Knowledge graph embedding by translating on hyperplanes",
    "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."],
    "venue": "Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, pages 1112–1119. AAAI Press.",
    "year": 2014
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Machine learning, 8(3-4):229–256.",
    "year": 1992
  }, {
    "title": "Embedding entities and relations for learning and inference in knowledge bases",
    "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."],
    "venue": "arXiv preprint arXiv:1412.6575.",
    "year": 2014
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Bengio Yoshua", "Ducharme Rejean", "Vincent Pascal", "Jauvin Christian."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2003
  }, {
    "title": "Energy-based generative adversarial network",
    "authors": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun."],
    "venue": "arXiv preprint arXiv:1609.03126.",
    "year": 2016
  }],
  "id": "SP:5ed0b69e3401017fda984d18ca8e177ddd4f3218",
  "authors": [{
    "name": "Avishek Joey Bose",
    "affiliations": []
  }, {
    "name": "Huan Ling",
    "affiliations": []
  }, {
    "name": "Yanshuai Cao",
    "affiliations": []
  }],
  "abstractText": "Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.",
  "title": "Adversarial Contrastive Estimation"
}