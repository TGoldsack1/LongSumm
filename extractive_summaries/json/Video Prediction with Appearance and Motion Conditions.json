{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Video prediction is concerned with generating high-fidelity future frames given past observations by learning dynamic visual patterns from videos. It is a promising direction for video representation learning because the model will have to learn to disentangle factors of variation based on complex visual patterns, i.e., how objects move and deform over time, how scenes change as the camera moves, how background changes as the foreground objects move, etc. While the recent advances in deep generative models (Kingma & Welling, 2013; Goodfellow et al., 2014) have brought a rapid progress to image generation (Radford et al., 2016; Isola et al., 2017; Zhu et al., 2017a), relatively little progress has been made in video prediction. We believe this is due in\nThis work was done at Yahoo Research during summer internship. 1University of Michigan, Ann Arbor 2Seoul National University 3Microsoft AI & Research. Correspondence to: Yunseok Jang <yunseokj@umich.edu>, Gunhee Kim <gunhee@snu.ac.kr>, Yale Song <yalesong@microsoft.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\npart to future uncertainty (Walker et al., 2016), making the problem somewhat ill-posed and evaluation difficult.\nPrevious work has addressed the uncertainty issue in several directions. One popular approach is learning to extrapolate multiple past frames into the future (Srivastava et al., 2015; Mathieu et al., 2016). This helps reduce uncertainty because input frames act as conditions that constrain the range of options for the future. However, when input frames are not sufficient statistics of the future, which is often the case with just a few frames (e.g., four in (Mathieu et al., 2016)), these methods suffer from blurry output caused by future uncertainty. Recent methods thus leverage auxiliary information, e.g., motion category labels and human pose, along with multiple input frames (Finn et al., 2016; Villegas et al., 2017b; Walker et al., 2017). Unfortunately, these methods still suffer from motionless and/or blurry output caused by the lack of clear supervision signals or suboptimal solutions found by training algorithms.\nIn this work, we propose an Appearance-Motion Conditional Generative Adversarial Network (AMC-GAN). Unlike most existing methods that learn from multiple input frames (Srivastava et al., 2015; Mathieu et al., 2016; Finn et al., 2016; Villegas et al., 2017b; Liang et al., 2017), which contain both appearance and motion information, we instead disentangle appearance from motion, and learn from a single input frame (appearance) and auxiliary input (motion). This allows our model to learn different factors of variation more precisely. Encoding motion with an auxiliary variable allows our model to manipulate how the future would look like; with a simple change of the auxiliary variable, we can make a neutral face happy or frown, or make a neutral body pose perform different gestures.\nTraining GANs is notoriously difficult (Salimans et al., 2016). We develop a novel conditioning scheme that constructs multiple different combinations of appearance and motion conditions – including even the ones that are not part of the training samples – and specify constraints to the learning objective such that videos generated under different conditions all look plausible. This makes the model generate videos under conditions beyond what is available in the training data and thus work much harder to satisfy the constraints during training, improving the generalization ability. In addition, we incorporate perceptual triplet ranking into\nthe learning objective so that videos with similar conditions look more similar to each other than the ones with different conditions. This mixed-objective learning strategy helps our model find the optimal solution effectively.\nOne useful byproduct of our conditional video prediction setting is that we can design an objective evaluation methodology that checks whether generated videos contain the likely content as specified in the input condition. This is in contrast to the traditional video prediction setting where there is no expected output, other than it being plausibly looking (Vondrick et al., 2016b). We design an evaluation technique where we train a video classifier on real data with motion category labels and test it on generated videos. We also perform qualitative analysis to assess the visual quality of the output, and report favorable results on the MUG facial expression dataset (Aifanti et al., 2010) and the NATOPS human action dataset (Song et al., 2011).\nTo summarize, our contributions include:\n• We propose AMC-GAN that can generate multiple different videos from a single image by manipulating input conditions. The code is available at http://vision.snu.ac.kr/projects/amc-gan. • We develop a novel conditioning scheme that helps the training by varying appearance and motion conditions. • We use perceptual triplet ranking to encourage videos of similar conditions to look similar. To our best knowledge, this has not been explored in video prediction."
  }, {
    "heading": "2. Related Work",
    "text": "Future Prediction: Early work proposed to use the past observation to predict certain representation of the future, e.g., object trajectory (Walker et al., 2014), optical flow (Walker et al., 2015), dense trajectory features (Walker et al., 2016), visual representation (Vondrick et al., 2016a), and human poses (Chao et al., 2017). Our work is distinct from this line of research as we aim to predict future frames rather than certain representation of the future.\nVideo Prediction: Ranzato et al. (2014) proposed a recurrent neural network that predicts a target frame composed of image patches (akin to words in language). Srivastava et al. (2015) used a sequence-to-sequence model to predict future frames. Early observations in video prediction have shown that predicted frames tend to be blurry (Mathieu et al., 2016; Finn et al., 2016). One primary reason for this is future uncertainty (Walker et al., 2016; Xue et al., 2016); there could be multiple correct, equally probable next frames given the previous frames. This observation has motivated two research directions: using adversarial training to make the predicted frames look realistic, and using auxiliary information as conditions to constrain what the future may look like. Our work is closely related to both directions as\nwe perform conditional video prediction with adversarial training. Below we review the most representative work in the two research directions.\nAdversarial Training: Recent methods employ adversarial training to encourage predicted frames to look realistic and less blurry. Most work differ by the design of the discriminator: Villegas et al. (2017b) use an appearance discriminator, Mathieu et al. (2016); Villegas et al. (2017a); Vondrick et al. (2016b); Walker et al. (2017) use a motion discriminator, and Liang et al. (2017); Tulyakov et al. (2017) use both. Vondrick et al. (2016b) use a motion discriminator based on a 3D CNN; Walker et al. (2017) adopt the same motion discriminator. Our motion discriminator is similar to theirs, but differ by the use of conditioning variables. Liang et al. (2017) define two discriminators: an appearance discriminator that inspects each frame, and a motion discriminator that inspects an optical flow image predicted from each consecutive frames. Our work also employs dual discriminators, but we do not require optical flow information.\nConditional Generation: Most approaches in video prediction use multiple frames as input and predict future frames by learning to extrapolate (Ranzato et al., 2014; Srivastava et al., 2015; Mathieu et al., 2016; Villegas et al., 2017a; Liang et al., 2017). We consider these methods related to ours because multiple frames essentially provide appearance and motion conditions. Some of these work, similar to ours, decompose input into appearance and motion pathways and handle them separately (Villegas et al., 2017a; Liang et al., 2017). Our work is, however, distinct from all the previous methods in that we do not “learn to extrapolate”; rather, we learn to predict the future from a single frame so the resulting video faithfully contains motion information provided as an auxiliary variable. This latter aspect makes our work unique because, as we show later in the paper, it allows our model to manipulate the future depending on motion input.\nFor predicting future frames containing human motion, some methods estimate body pose from input frames, and decode input frames (appearance) and poses (motion) into a video (Villegas et al., 2017b; Walker et al., 2017); these methods do video prediction by pose estimation. Pose information is attractive because they are low-dimensional. Our work also uses a motion condition that is of low-dimensional, but is more flexible because we work with generic keypoint statistics (e.g., location and velocity); we show how we encode motion information in Section 4.\nSeveral approaches provide auxiliary information as conditioning variables. Finn et al. (2016) use action and state information of a robotic arm. Oh et al. (2015) use Atari game actions. Reed et al. (2016) propose text-to-image synthesis; Marwah et al. (2017) propose text-to-video prediction. These methods, similar to ours, can manipulate how the output may look like, by changing the auxiliary\ninformation. Thus, we empirically compare our method with Finn et al. (2016), Mathieu et al. (2016) and Villegas et al. (2017a) and report improved performance.\nLastly, different from all above mentioned work, we incorporate a perceptual ranking loss (Wang et al., 2014; Gatys et al., 2015) to encourage videos that share the same appearance/motion conditions to look similar than videos that do not. Our work is, to the best of our knowledge, the first to use this constraint in the video prediction setting."
  }, {
    "heading": "3. Approach",
    "text": "Our goal is to generate a video given an appearance and motion information. We formulate this as learning the conditional distribution p(x|y) where x is a video and y = [ya,ym] is a set of conditions known to occur. We define two conditioning variables, ya and ym, that encode appearance and motion information, respectively.\nWe propose an Appearance-Motion Conditional GAN, shown in Figure 1. The generator G seeks to produce realistic future frames. We denote a generated video by x̂|y = G(z|y), where z is random noise. The two discriminator networks, on the other hand, attempt to distinguish the generated videos from the real ones: Da checks if individual frames look realistic given ya. Dm checks if a video contains realistic motion given ym. Note that either discriminator alone would be insufficient to achieve our goal: without Da a generated video may have inconsistent visual appearance across frames, without Dm a generated video may not depict the motion we intend to hallucinate.\nThe generator and the two discriminators form a conditional GAN (Mirza & Osindero, 2014). This alone would be in sufficient to learn the role of conditioning variables unless a proper care is taken. If we follow the traditional training method (Mirza & Osindero, 2014), the model may treat them as random noise. To ensure that the conditioning variables have intended influence on the data generation process, we employ a ranking network R, which takes as input a triplet (x|y, x̂|y, x̂|y′) and forces x|y and x̂|y to look more similar to each other than x|y and x̂|y′ , because in the latter pair, the conditions do not match (y 6= y′).\nIn addition to the ranking constraint, we propose a novel conditioning scheme to put constraints on the learning objective\nwith respect to the conditioning variables. We explain our learning strategy and the conditioning scheme in Section 3.3, and discuss model training in Section 3.4."
  }, {
    "heading": "3.1. Appearance and Motion Conditions",
    "text": "The appearance condition ya can be any high-level abstraction that encodes visual appearance; we use a single RGB image ya ∈ R64×64×3 (e.g., the first frame of a video).\nThe motion condition ym can also be any high-level abstraction that encodes motion. We define it as ym = [ ylm,y v m ] , where ylm ∈ Rc is a motion category label encoded as a one-hot vector, and yvm ∈ R(T−1)×2k is the velocity of k keypoints in 2D space detected from an image sequence of length T . We explain how we extract keypoints in Section 4. We repeat ylm T−1 times to obtain ym ∈ R(T−1)×q , where q = (c+ 2k). We set T = 32 in all our experiments.\nWe assume ylm is known both during training and inference. However, we assume yvm is known only during training; during inference, we randomly sample it from those training examples that share the same class ylm as the test example."
  }, {
    "heading": "3.2. The Model",
    "text": "We describe the four modules of our model (see Figure 2); implementation details are provided in the supplementary.\nGenerator: This has the encoder-decoder structure with a convLSTM (Shi et al., 2015) in the middle. It takes as input the two conditioning variables ya and ym, and a random noise vector z ∈ Rp sampled from a normal distribution N (0, I). The output is a video x̂|y generated frame-byframe by unrolling the convLSTM for T − 1 times.\nWe use the encoder output to initialize the convLSTM. At each time step t, we provide the t-th slice of ym,t ∈ Rq to the convLSTM, and combine its output with the noise vector z and the encoder output. This becomes input to the image decoder. The noise vector z, sampled once per video, introduces a certain degree of randomness to the decoder, helping the generator probe the distribution better (Goodfellow et al., 2014). We add a skip connection to create a direct path from the encoder to the decoder. This helps the model focus on learning changes in movement rather than full appearance and motion. We empirically found this to be crucial in producing high quality output.\nAppearance Discriminator: This takes as input four images, an appearance condition ya and three frames xt−1:t+1 from either a real or a generated video, and produces a scalar indicating whether the frame is real or fake. Note the conditional formulation with ya: This is crucial to ensure the appearance of generated frames is cohesive across time with the first frame, e.g., it should be facial movements that change over time, not its identity.\nMotion Discriminator: This takes as input a video x = [x1:T ] and the two conditions ya and ylm. It predicts three variables: a scalar indicating whether the video is real or fake, ŷlm ∈ Rc representing motion categories, and ŷvm ∈ R2k representing the velocity of k keypoints. The first is the adversarial discrimination task; we provide the motion category label ylm to perform class-conditional discrimination of the videos. The latter two are auxiliary tasks, similar to InfoGAN (Chen et al., 2016) and BicycleGAN (Zhu et al., 2017b), introduced to make our model more robust. We show the importance of the auxiliary tasks in Section 4.\nPerceptual Ranking: This takes a triplet (x|y, x̂|y, x̂|y′) and outputs a scalar indicating the amount of violation for a constraint d(x|y, x̂|y) < d(x|y, x̂|y′), where d(·, ·) is a function that computes a perceptual distance between two videos, and y′ 6= y; we call y′ a “mismatched” condition.\nTo compute the perceptual distance, we adapt the idea of the perceptual loss used in image style transfer (Gatys et al., 2015; Johnson et al., 2016), in which the distance is measured based on the feature representation at each layer of a pretrained CNN (e.g., VGG-16). In this work, we cannot simply use a pretrained CNN because of the conditioning variables; we instead use our own discriminator networks to compute them. Since we have two discriminators, we choose one based on a mismatched condition y′, i.e., we use Da when y′ = [ya′ ,ym] and Dm when y′ = [ya,ym′ ].\nThere are two ways to compute the perceptual distance: compare filter responses directly or the Gram matrices of the filter responses. The former encourages filter responses of a generated video to replicate, pixel-to-pixel, the ones of a training video. This is too restrictive for our purpose because we want our model to “go beyond” what exists in the training data; we want x̂|y′ to look realistic even if the given (video, condition) pair does not exist in the training set. The latter relaxes this restriction by encouraging filter responses to share similar correlation patterns between two videos. We take this latter approach in our work.\nLet GDj (·) be the Gram matrix computed at the j-th layer of a discriminator network D. We define the distance function\nat the j-th layer of the network as dj(x|y, x̂|y) = ∥∥GDj (x|y)−GDj (x̂|y)∥∥F (1)\nwhere ‖·‖F is the Frobenius norm. To compute the Gram matrix, we reshape the output of the j-th layer from a discriminator network to be the size of Nj ×Mj , where Nj = Tj×Cj (sequence length× number of channels) and Mj = Hj ×Wj (height × width). Denoting this reshaped matrix by ωj(x), the Gram matrix is\nGDj (x) = ωj(x) >ωj(x) / NjMj . (2)\nFinally, we employ triplet ranking (Wang et al., 2014; Schroff et al., 2015) to measure the amount of violation, using x|y as an anchor point and x̂|y and x̂|y′ as positive and negative samples, respectively. Specifically, we use the hinge loss form to quantify the amount of violation:\nR(x|y, x̂|y, x̂|y′) = ∑\nj max\n( 0, ρ− d−j + d + j ) (3)\nwhere ρ determines the margin between positive and negative pairs (we set ρ as 0.01 for Da and 0.001 for Dm), d+j = dj(x|y, x̂|y) and d − j = dj(x|y, x̂|y′), and j = [1, 2]."
  }, {
    "heading": "3.3. Learning Strategy",
    "text": "We specify three constraints on the behavior of our model to help it learn the data distribution effectively:\n• C1: If we take one of the training samples x|y and pair it with a different condition, i.e., (x|y,y′), our discriminators should be able to tell the pair is fake. • C2: Regardless of the input condition, videos produced by the generator should be able to fool the discriminators into believing that x̂|y and x̂|y′ are real. • C3: The pair (x|y, x̂|y) should look more similar to each other than the pair (x|y, x̂|y′ ) because the former shares the same condition (in the latter, y 6= y′).\nConditioning Scheme: We provide three conditions to the generator, listed in Table 1. The first contains the original condition (ya, ym) matched with a training video x|ya,ym . The other two pairs contain mismatched information on\neither variable. We select the mismatched condition by randomly selecting another condition from the training set. Note that we do not feed the pair (ya′ , ym′ ) to the generator as it is equivalent to one of the other three combinations.\nWe provide four conditions to each discriminator, listed in Table 2. The first and the third rows are identical to conditional GAN (Mirza & Osindero, 2014). Training our model with just these two conditions may make our model treat the conditioning variables as random noise in the worst case. This is because there is no constraint on the expected behavior of the conditioning variables on the generation process, other than just having the end results look realistic.\nWe provide (x|y,y′) to the discriminators (the second row in Table 2) and have them identify it as fake; this enforces the constraint C1. Note that there is no gradient flow back to the generator because it has no control over x|y. A similar idea was used by (Reed et al., 2016), where they used a mismatched sentence for the text-to-image synthesis task. We provide (x̂|y′ ,y′) to the discriminators (the fourth row) to enforce the constraint C2. With this, the generator needs to work harder to fool the discriminators because this condition does not exist in the training set. We do not include (x̂|y, y′) and (x̂|y′ , y) because the conditions used in generator do not match with the conditions provided to the discriminator."
  }, {
    "heading": "3.4. Model Training",
    "text": "Our learning objective is to solve the min-max game:\nmin θG max θD Lgan(θG , θD) + Lrank (θG)\n+ LDaux (θD) + LGaux (θG) (4)\nwhere each L{·} has its own loss weight to balance the influence of it (see supplementary). The first term follows the conditional GAN objective (Mirza & Osindero, 2014):\nLgan(θG , θD) = Ex∼pdata(x)[logD(x|y)] + Ez∼pz(z)[log(1−D(G(z|y)|y))] (5)\nwhere we collapsed Da and Dm into D for brevity. We use the cross entropy loss for the real/fake discriminators. The\nAlgorithm 1 AMC-GAN Training Algorithm 1: Input: Dataset {x|y}, conditions y and y′, step size η 2: for each step do 3: z ∼ N (0, I) 4: x̂|y ← G(z|y), x̂|y′ ← G(z|y′) 5: (sr, vr, lr)← D(x|y,y), (sm, vm, lm)← D(x|y,y′)\n(sf , vf , lf )← D(x̂|y,y), (sf ′ , vf ′ , lf ′)← D(x̂|y′ ,y′) 7: LD ← log(sr) + 0.5[log(1− sm) + 0.5(log(1− sf ) + log(1− sf ′))] 9: LDaux ← [ ‖yvm − vr‖22 + ‖y v m − vf‖22 +\n∥∥y′vm − vf ′∥∥22] −∑iylm,i [log y(lr,i) + log y(lf,i) + log y(lf ′,i)]\n11: θD ← θD + η ∂(LD−LDaux )\n∂θD 12: d+j = ∥∥GDj (x|y)−GDj (x̂|y)∥∥F for j = 1, 2\n13: d−j = ∥∥GDj (x|y)−GDj (x̂|y′)∥∥F for j = 1, 2 14: LG ← log(sf ) + log(sf ′) 15: LGaux ← ∑T t=2 ‖xt|y − x̂t|y‖1 + ∑2 j=1d + j\n16: Lrank ← ∑2j=1 max(0, ρ− d−j + d+j ) 17: θG ← θG + η\n∂(LG − LGaux − Lrank ) ∂θG\n18: end for\nsecond term is our perceptual ranking loss (see Eqn. (3))\nLrank (θG) = R(x|y, x̂|y, x̂|y′). (6)\nThe two terms play complementary roles during training: The first encourages the solution to satisfy C1 and C2, while the second encourages the solution to satisfy C3.\nThe third term is introduced to increase the power of our motion discriminator:\nLDaux = LCE (ylm, ŷlm) + LMSE (yvm, ŷvm) (7)\nwhere the first term is the cross entropy loss for predicting motion category labels, and the second is the mean square error loss for predicting the velocity of keypoints.\nThe fourth term is introduced to increase the power of the generator, and is similar to the reconstruction loss widely used in video prediction (Mathieu et al., 2016),\nLGaux = ‖x|y − x̂|y‖1 + ∑\nj dj(x|y, x̂|y). (8)\nAlgorithm 1 summarizes how we train our model. We solve the bi-level optimization problem where we alternate between solving for θD with respect to the optimum of θG and vice versa. We train the discriminator networks based on a mini-batch containing a mix of the four cases listed in Table 2. We put different weights to each of the four cases (Line 7), as suggested by (Reed et al., 2016). The generator is trained on a mini-batch of the three cases listed in Table 1. We use the ADAM optimizer (Kingma & Ba, 2015) with learning rate 2e-4. For the cross entropy losses, we adopt the label smoothing trick (Salimans et al., 2016) with a weight decay of 1e-5 per mini-batch (Arjovsky & Bottou, 2017)."
  }, {
    "heading": "4. Experiments",
    "text": "We evaluate our approach on the MUG facial expression dataset (Aifanti et al., 2010) and the NATOPS human action dataset (Song et al., 2011). The MUG dataset contains 931 video clips performing six basic emotions (Ekman, 1992) (anger, disgust, fear, happy, sad, surprise). We preprocess it so that each video has 32 frames with 64 × 64 pixels (see supplementary for details). We use 11 facial landmark locations (2, 9, 16, 20, 25, 38, 42, 45, 47, 52, 58th) as keypoints for each frame, detected using the OpenFace toolkit (Baltrušaitis et al., 2016). The NATOPS dataset contains 9,600 video clips performing 24 action categories. We crop the video to 180 × 180 pixels with the chest at the center position and rescale it to 64× 64 pixels. We use 9 joint locations (head, chest, naval, L/R-shoulders, L/R-elbows, L/R-wrists) as keypoints for each frame, provided by the dataset."
  }, {
    "heading": "4.1. Quantitative Evaluation",
    "text": "Methodology: We design a c-way motion classifier using a 3D CNN (Tran et al., 2015) that predicts the motion label ylm from a video (see the supplementary for the architecture). To prevent the classifier from predicting the label simply by seeing the input frame(s), we only use the last 28 generated frames as input. We train the classifier on real training data, using roughly 10% for validation, and test it on generated videos from different methods.\nWe compare our method with recent approaches in video prediction: CDNA (Finn et al., 2016), Adv+GDL with `1 loss (Mathieu et al., 2016), and MCnet (Villegas et al., 2017a). For CDNA, we provide ya as input image and ym as the “action & state” variable. We use 10 masks suggested in their work, and disable teacher forcing for fair comparison with other methods. Following the original implementations for Adv+GDL and MCnet, we provide as input the first four consecutive frames, but no ym. We also perform ablative analyses by eliminating various components of our method; we explain various settings as we discuss the results.\nResults: Table 3 shows the results. We notice that the CDNA performs worse than the other methods. This is expected because it predicts future frames by combining multiple frames via masking, each generated by shifting the entire pixels of the previous frame in a certain direction. Our\ndatasets contain complex object deformations that cannot be synthesized simply by shifting pixels. Because our network predicts pixel values directly, we achieve better results on more naturalistic videos. Both Adv+GDL and MCnet outperforms CDNA but not ours. We believe this is because both models learn to extrapolate past observations into the future. Therefore, if the input (four consecutive frames) do not provide enough motion information, as is true in our case (most videos start with “neutral” faces and body poses), extrapolation fails to predict future frames. Lastly, our model outperforms all the baselines by significant margins. It is because our model is optimized to generate videos that fool the motion discriminator with LDaux , which guides our model to preserve well the property of the motion condition ym.\nTo verify whether our model successfully generates videos with the correct motion information provided as input, we run a similar experiment on the MUG dataset with only keypoints extracted from the generated output. For this, we use the OpenFace toolkit (Baltrušaitis et al., 2016) to extract 68 facial landmarks from the predicted output and render them with a Gaussian blur on a 2D grid to produce grayscale images. This is then fed into a c-way 2D CNN classifier (details in the supplementary). The results confirm that our method produces videos with the most accurate keypoint trajectories, with an accuracy of 70.34%, compared to CDNA (23.52%), Adv+GDL (28.81%), MCnet (35.38%).\nFor an ablation study, we remove input conditions and their corresponding discriminators to measure the relative importance of appearance and motion. Not surprisingly, removing either ya or ym signifncantly drops the performance. Similarly, having no y and y′ (i.e., produce videos solely based on random noise z) results in poor performance. Finally, we remove the mismatched conditions y′ from our conditioning scheme, i.e., we use only the first row of Table 1 and the first and third rows of Table 2; this is similar to the standard conditional GAN (Mirza & Osindero, 2014). We can see a performance drop. This is because our model ends up treating the conditioning variables alike to random noise; without contradicting conditions, the discriminators have no chance of learning to discriminate different conditions.\nRemoving Dm shows significant drop in performance, which is expected because without motion constraints the model is incentivized to produce what appears as a static image (repeat ya to make the appearance realistic). Removing Da also decreases the performance, but not as much as removing Dm. This is however deceptive: videos look visu-\nally implausible and appear to be adversarial examples (the faces under “No Da” in Fig. 3). This shows the importance of enforcing constraints on visual appearance: without it, the model “over-optimizes” for the motion constraint.\nFinally, we remove loss terms from our objective Eqn. (4). Removing LDaux significantly deteriorates our model. This shows its effectiveness in enhancing the power of the motion discriminator; without it, similar to removing Dm, the model is less constrained to predict realistic motion. Removing LGaux decreases the performance moderately. Visual inspection of the generated videos revealed that this has a similar effect to removing Da or ya; the model overoptimizes for the motion constraint. This is consistent with the literature (Mathieu et al., 2016; Shrivastava et al., 2017) that shows the effectiveness of the reconstruction constraint. Removing Lrank hurts the performance only marginally; however, we found that the ranking loss improves visual quality and leads to faster model convergence."
  }, {
    "heading": "4.2. Qualitative Results",
    "text": "Methodology: We adapt the evaluation protocol from (Vondrick et al., 2016b) and ask humans to specify their subjective preference when given a pair of videos generated using different methods under the same condition. We randomly chose 100 videos from the test split of each dataset and created 400 video pairs. We recruited 10 participants for this study; each rated all the pairs from each dataset.\nResults: Table 5 shows the results when the participants are provided with motion category information along with videos. This ensures that their decision takes into account both appearance and motion; without the category label, their decision is purely based on appearance. Our participants significantly preferred ours over the baselines. Notably, for the NATOPS dataset, more than 90% of the participants voted for ours. This is because the dataset is more challenging with more categories (24 actions vs. 6 emotions); a model must generate plausibly looking videos (appearance) with distinct movements across categories (motion), which is more challenging with more categories.\nTo evaluate the quality of the generated videos in terms of appearance and motion separately, we designed another experiment with two tasks: We give the participants the same preference task but without motion category information. Subsequently, we ask them to identify which of the 7 facial expressions (neutral and 6 emotions) is depicted in each\ngenerated video. These two tasks focus on appearance and motion, respectively. Our participants preferred ours over CDNA (80.8%), Adv+GDL (86.4%), MCnet (55%), and the ground truth (5%). The MCnet approach was a close match, showing that videos generated by ours and MCnet have a similar quality in terms of appearance. However, results from the second task showed that none of the three baselines successfully produced videos with distinct motion patterns: The human classification accuracy was: Ours 66%, CDNA 7%, Adv+GDL 3%, MCnet 7%, GT: 77%. This suggests that MCnet, while producing visually plausible output, fails to produce videos with intended motion.\nFigure 3 shows generated videos. Our method produces noticeably sharper frames and manifests more distinct/correct motion patterns than the baselines. Most importantly, the results show that we can manipulate the future frames by changing the motion condition; notice how the same input frame ya turns into different videos. The results also show the importance of appearance and motion discriminators. Removing Da deteriorates the visual realism in the output: While the results still manifest the intended motion (“happy” in the first set of examples), the generated frames look visually implausible (the face identity changes over time). Removing Dm produces what appears as a static video.\nThe CDNA produces blurry frames without clear motion, despite the fact that it receives the same ya and ym as our model. MCnet and Adv+GDL receive four-frame input frames, which provide appearance and motion information. While the results are sharper than the CDNA, we see motion patterns are not as distinct/correct as ours (they look almost stationary), due to future uncertainty caused by too little motion information exist in the input. This suggests that the “learning to extrapolate” approaches do not successfully address the ill-conditioning issue in video prediction.\nThe results from our quantitative and qualitative experiments highlight the advantage of our approach: Disentangling appearance from motion in the input space and learning dynamic visual representation using our method produces higher-fidelity videos than the compared methods, which suggests that our method learns video representation more precisely than the baselines."
  }, {
    "heading": "5. Conclusion",
    "text": "We presented an AMC-GAN to address the future uncertainty issue in video prediction. The decomposition of appearance and motion conditions enabled us to design a novel conditioning scheme, which puts constraints on the behavior of videos generated under different conditions. We empirically demonstrated that our method produces sharp videos with the content expected by input conditions better than alternative solutions."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Kang In Kim for helpful comments about building a human evaluation page. We also appreciate Youngjin Kim, Youngjae Yu, Juyoung Kim, Insu Jeon and Jongwook Choi for helpful discussions related to the design of our model. This work is partially supported by Korea-U.K. FP Programme through the National Research Foundation of Korea (NRF-2017K1A3A1A16067245)."
  }],
  "year": 2018,
  "references": [{
    "title": "The MUG Facial Expression Database",
    "authors": ["N. Aifanti", "C. Papachristou", "A. Delopoulos"],
    "venue": "In WIAMIS,",
    "year": 2010
  }, {
    "title": "Towards Principled Methods for Training Generative Adversarial Networks",
    "authors": ["M. Arjovsky", "L. Bottou"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "OpenFace: An Open Source Facial Behavior Analysis Toolkit",
    "authors": ["T. Baltrušaitis", "P. Robinson", "Morency", "L.-P"],
    "venue": "In WACV,",
    "year": 2016
  }, {
    "title": "Forecasting Human Dynamics from Static Images",
    "authors": ["Y. Chao", "J. Yang", "B.L. Price", "S. Cohen", "J. Deng"],
    "year": 2017
  }, {
    "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
    "authors": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"],
    "year": 2016
  }, {
    "title": "An Argument for Basic Emotions",
    "authors": ["P. Ekman"],
    "venue": "Cognition & Emotion,",
    "year": 1992
  }, {
    "title": "Unsupervised Learning for Physical Interaction through Video Prediction",
    "authors": ["C. Finn", "I.J. Goodfellow", "S. Levine"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Texture Synthesis Using Convolutional Neural Networks",
    "authors": ["L. Gatys", "A.S. Ecker", "M. Bethge"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Generative Adversarial Nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Image-to-Image Translation with Conditional Adversarial Networks",
    "authors": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"],
    "year": 2017
  }, {
    "title": "Perceptual Losses for RealTime Style Transfer and Super-Resolution",
    "authors": ["J. Johnson", "A. Alahi", "F. Li"],
    "venue": "In ECCV,",
    "year": 2016
  }, {
    "title": "ADAM: A Method For Stochastic Optimization",
    "authors": ["D.P. Kingma", "J.L. Ba"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Dual Motion GAN for Future-Flow Embedded Video Prediction",
    "authors": ["X. Liang", "L. Lee", "W. Dai", "E.P. Xing"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Attentive Semantic Video Generation using Captions",
    "authors": ["T. Marwah", "G. Mittal", "V.N. Balasubramanian"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Deep Multi-Scale Video Prediction Beyond Mean Square Error",
    "authors": ["M. Mathieu", "C. Couprie", "Y. LeCun"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "ActionConditional Video Prediction using Deep Networks in Atari Games",
    "authors": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Video (Language) Modeling: A Baseline for Generative Models of Natural Videos",
    "authors": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"],
    "year": 2014
  }, {
    "title": "Generative Adversarial Text to Image Synthesis",
    "authors": ["S.E. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"],
    "year": 2016
  }, {
    "title": "Improved Techniques for Training GANs",
    "authors": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Facenet: A Unified Embedding for Face Recognition and Clustering",
    "authors": ["F. Schroff", "D. Kalenichenko", "J. Philbin"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
    "authors": ["X. Shi", "Z. Chen", "H. Wang", "Yeung", "D.-Y", "Wong", "W.-K", "W. chun Woo"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Learning from Simulated and Unsupervised Images through Adversarial Training",
    "authors": ["A. Shrivastava", "T. Pfister", "O. Tuzel", "J. Susskind", "W. Wang", "R. Webb"],
    "year": 2017
  }, {
    "title": "Tracking Body and Hands for Gesture Recognition: Natops Aircraft Handling Signals Database",
    "authors": ["Y. Song", "D. Demirdjian", "R. Davis"],
    "venue": "In FG,",
    "year": 2011
  }, {
    "title": "Unsupervised Learning of Video Representations using LSTMs",
    "authors": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
    "authors": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "MoCoGAN: Decomposing Motion and Content for Video Generation",
    "authors": ["S. Tulyakov", "M. Liu", "X. Yang", "J. Kautz"],
    "year": 2017
  }, {
    "title": "Decomposing Motion and Content for Natural Video Sequence Prediction",
    "authors": ["R. Villegas", "J. Yang", "S. Hong", "X. Lin", "H. Lee"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Learning to Generate Long-term Future via Hierarchical Prediction",
    "authors": ["R. Villegas", "J. Yang", "Y. Zou", "S. Sohn", "X. Lin", "H. Lee"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Anticipating the Future by Watching Unlabeled Video",
    "authors": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"],
    "venue": "In CVPR,",
    "year": 2016
  }, {
    "title": "Generating Videos with Scene Dynamics",
    "authors": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Patch to the Future: Unsupervised Visual Prediction",
    "authors": ["J. Walker", "A. Gupta", "M. Hebert"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Dense Optical Flow Prediction from a Static Image",
    "authors": ["J. Walker", "A. Gupta", "M. Hebert"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "An Uncertain Future: Forecasting from Static Images using Variational Autoencoders",
    "authors": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"],
    "year": 2016
  }, {
    "title": "The Pose Knows: Video Forecasting by Generating Pose Futures",
    "authors": ["J. Walker", "K. Marino", "A. Gupta", "M. Hebert"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Learning Fine-grained Image Similarity with Deep Ranking",
    "authors": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Philbin", "B. Chen", "Y. Wu"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks",
    "authors": ["T. Xue", "J. Wu", "K.L. Bouman", "W.T. Freeman"],
    "year": 2016
  }, {
    "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
    "authors": ["Zhu", "J.-Y", "T. Park", "P. Isola", "A.A. Efros"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Toward Multimodal Imageto-Image Translation",
    "authors": ["Zhu", "J.-Y", "R. Zhang", "D. Pathak", "T. Darrell", "A.A. Efros", "O. Wang", "E. Shechtman"],
    "venue": "In NIPS,",
    "year": 2017
  }],
  "id": "SP:66e2c3d23af8ed76b116121827b9bc5e99cf4acc",
  "authors": [{
    "name": "Yunseok Jang",
    "affiliations": []
  }, {
    "name": "Gunhee Kim",
    "affiliations": []
  }, {
    "name": "Yale Song",
    "affiliations": []
  }],
  "abstractText": "Video prediction aims to generate realistic future frames by learning dynamic visual patterns. One fundamental challenge is to deal with future uncertainty: How should a model behave when there are multiple correct, equally probable future? We propose an Appearance-Motion Conditional GAN to address this challenge. We provide appearance and motion information as conditions that specify how the future may look like, reducing the level of uncertainty. Our model consists of a generator, two discriminators taking charge of appearance and motion pathways, and a perceptual ranking module that encourages videos of similar conditions to look similar. To train our model, we develop a novel conditioning scheme that consists of different combinations of appearance and motion conditions. We evaluate our model using facial expression and human action datasets and report favorable results compared to existing methods.",
  "title": "Video Prediction with Appearance and Motion Conditions"
}