{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 595–606 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Most successful approaches to automated grammatical error correction (GEC) are based on methods from statistical machine translation (SMT), especially the phrase-based variant. For the CoNLL 2014 benchmark on grammatical error correction (Ng et al., 2014), Junczys-Dowmunt and Grundkiewicz (2016) established a set of methods for GEC by SMT that remain state-of-the-art. Systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017) that improve on results by Junczys-Dowmunt and Grundkiewicz (2016) use their set-up as a backbone for more complex systems.\nThe view that GEC can be approached as a machine translation problem by translating from erroneous to correct text originates from Brockett et al. (2006) and resulted in many systems (e.g. Felice et al., 2014; Susanto et al., 2014) that represented the current state-of-the-art at the time.\nIn the field of machine translation proper, the emergence of neural sequence-to-sequence methods and their impressive results have lead to a paradigm shift away from phrase-based SMT towards neural machine translation (NMT). During WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods.\nBased on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2\nIf we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT\n1Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf\n2After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with some overlap with our methods. However, our results stay ahead on all benchmarks while using simpler models.\n595\nstarts low for small corpora, outperforms SMT at a corpus size of about 15 million words, and with increasing size beats SMT with a large in-domain language model.\nTable 1 lists existing training resources for the English as-a-second-language (ESL) grammatical error correction task. Publicly available resources, NUS Corpus of Learner English (NUCLE) by Dahlmeier et al. (2013), Lang-8 NAIST (Mizumoto et al., 2012) and CLC FCE (Yannakoudakis et al., 2011) amount to about 27M tokens. Among these the Lang-8 corpus is quite noisy and of low quality. The Cambridge Learner Corpus (CLC) by Nicholls (2003) — probably the best resource in this list — is non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult.\nContrasting this with Fig. 1, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models. Current state-of-the-art GEC systems based on SMT, however, all include large-scale in-\ndomain language models either following the steps outlined in Junczys-Dowmunt and Grundkiewicz (2016) or directly re-using their domain-adapted Common-Crawl language model.\nIt seems that the current state of neural methods in GEC reflects the behavior for NMT systems trained on smaller data sets. Based on this, we conclude that we can think of GEC as a lowresource, or at most mid-resource, machine translation problem. This means that techniques proposed for low-resource (neural) MT should be applicable to improving neural GEC results.\nIn this work we show that adapting techniques from low-resource (neural) MT and SMT-based GEC methods allows neural GEC systems to catch up to and outperform SMT-based systems. We improve over the previously best-reported neural GEC system (Ji et al., 2017) on the CoNLL 2014 test set by more than 10% M2, over a comparable pure SMT system by Junczys-Dowmunt and Grundkiewicz (2016) by 6%, and outperform the state-of-the-art result of Chollampatt and Ng (2017) by 2%. On the JFLEG data set, we report the currently best results, outperforming the previously best pure neural system (Sakaguchi et al., 2017) by 5.9% GLEU and the best reported results (Chollampatt and Ng, 2017) by 3% GLEU.\nIn Section 2, we describe our NMT-based baseline for GEC, and follow recommendations from the MT community for a trustable neural GEC system. In Section 3, we adapt neural models to make better use of sparse error-annotated data, transferring low-resource MT and GEC-specific SMT methods to neural GEC. This includes a novel training objective for GEC. We investigate how to leverage monolingual data for neural GEC by transfer learning in Section 4 and experiment with language model ensembling in Section 5. Section 6 explores deep NMT architectures. In Section 7, we provide an overview of the experiments and how results relate to the JFLEG benchmark. We also recommend a model-independent toolbox for neural GEC."
  }, {
    "heading": "2 A trustable baseline for neural GEC",
    "text": "In this section, we combine insights from JunczysDowmunt and Grundkiewicz (2016) for grammatical error correction by phrase-based statistical machine translation and from Denkowski and Neubig (2017) for trustable results in neural machine translation to propose a trustable baseline for neural grammatical error correction."
  }, {
    "heading": "2.1 Training and test data",
    "text": "To make our results comparable to state-of-the-art results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well.\nWe choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks.\nFor most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set."
  }, {
    "heading": "2.2 Preprocessing and sub-words",
    "text": "As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets.\nWe follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the\n3https://github.com/AbiWord/enchant\nlarge-vocabulary problem of NMT. This is a well established procedure in neural machine translation and has been demonstrated to be generally superior to UNK-replacement methods. It has been largely ignored in the field of grammatical error correction even when word segmentation issues have been explored (Ji et al., 2017; Schmaltz et al., 2017). To our knowledge, this is the first work to use BPE sub-words for GEC, however, an analysis on advantages of word versus sub-word or character level segmentation is beyond the scope of this paper. A set of 50,000 monolingual BPE units is trained on the error-annotated data and we segment training and test/dev data accordingly. Segmentation is reversed before evaluation."
  }, {
    "heading": "2.3 Model and training procedure",
    "text": "Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description.\nAll embedding vectors consist of 512 units; the RNN states of 1024 units. The number of BPE segments determines the size of the vocabulary of our models, i.e. 50,000 entries. Source and target side use the same vocabulary. To avoid overfitting, we use variational dropout (Gal and Ghahramani, 2016) over GRU steps and input embeddings with probability 0.2. We optimize with Adam (Kingma and Ba, 2014) with an average mini-batch size of ca. 200. All models are trained until convergence (early-stopping with a patience of 10 based on development set cross-entropy cost), saving model checkpoints every 10,000 mini-batches. The best eight model checkpoints w.r.t. the development set M2 score of each training run are averaged elementwise (Junczys-Dowmunt et al., 2016) resulting in a final single model. During decoding we use a beam-size of 24 and normalize model scores by length.6\n4Models, system configurations and outputs are available from https://github.com/grammatical/ neural-naacl2018\n5https://github.com/marian-nmt/marian 6We used a larger beam-size than usual due to experiments with re-ranking of n-best lists not included in the paper. We did not see any differences compared to smaller beams."
  }, {
    "heading": "2.4 Optimizer instability",
    "text": "Junczys-Dowmunt and Grundkiewicz (2016) noticed that discriminative parameter tuning for GEC by phrase-based SMT leads to unstable M2 results between tuning runs. This is a well-known effect for SMT parameter tuning and Clark et al. (2011) recommend reporting results for multiple tuning runs. Junczys-Dowmunt and Grundkiewicz (2016) perform four tuning runs and calculate parameter centroids following Cettolo et al. (2011).\nNeural sequence-to-sequence training is discriminative optimization and as such prone to instability. We already try to alleviate this by averaging over eight best checkpoints, but as seen in Table 3, results for M2 remain unstable for runs with differently initialized weights. An amplitude of 3 points M2 on the CoNLL-2014 test set is larger than most improvements reported in recent papers. None of the recent works on neural GEC account for instability, hence it is unclear if observed outcomes are actual improvements or lucky picks among byproducts of instability. We therefore strongly suggest to provide results for multiple independently trained models. Otherwise improvements of less than 2 or 3 points of M2 remain doubtful. Interestingly, GLEU on the JFLEG data seems to be more stable than M2 on CoNLL data."
  }, {
    "heading": "2.5 Ensembling of independent models",
    "text": "Running multiple experiments to provide averaged results seems prohibitively expensive, but Denkowski and Neubig (2017) and others (e.g. Sutskever et al., 2014; Sennrich et al., 2017a) show that ensembling of independently trained models leads to consistent rewards for MT. For our baseline in Table 3 the opposite seems to be true for M2. This is likely the reason why no other work on neural GEC mentions results for ensembles.\nOn closer inspection, however, we see that the drop in M2 for ensembles is due to a precision bias. M2 being an F-score penalizes increasing distance between precision and recall. The increase in precision for ensembles is to be expected and we see it later consistently for all experiments. Ensembles choose corrections for which all independent models are fairly confident. This leads to fewer but better corrections, hence an increase in precision and a drop in recall. If the models are weak as our baseline, this can result in a lower score. It would, however, be unwise to dismiss ensembles, as we can use their bias towards precision to our advantage whenever they are combined with methods that aim to increase recall. This is true for nearly all remaining experiments."
  }, {
    "heading": "3 Adaptations for GEC",
    "text": "The methods described in this section turn our baseline into a more GEC-specific system. Most have been inspired by techniques from low-resource MT or closely related domain-adaptation techniques for NMT. All modifications are applied incrementally, later models include enhancements from the previous ones."
  }, {
    "heading": "3.1 Source-word dropout as corruption",
    "text": "GEC can be treated as a denoising task where grammatical errors are corruptions that have to be reduced. By introducing more corruption on the source side during training we can teach the model to reduce trust into the source input and to apply corrections more freely. Dropout is one way to introduce noise, but for now we only drop out single units in the embedding or GRU layers, something the model can easily recover from. To make the task harder, we add dropout over source words, setting the full embedding vector for a source word to 1/psrc with a probability of psrc. During our experiments, we found psrc = 0.2 to work best.\nTable 4 show impressive gains for this simple method (+Dropout-Src.). Results for the ensemble match the previously best results on the CoNLL2014 test set for pure neural systems (without the use of an additional monolingual language model) by Ji et al. (2017) and Schmaltz et al. (2017)."
  }, {
    "heading": "3.2 Domain adaptation",
    "text": "The NUCLE corpus matches the domain of the CoNLL benchmarks perfectly. It is however much smaller than the Lang-8 corpus. A setting like this seems to be a good fit for domain-adaptation techniques. Sennrich et al. (2016a) oversample in-domain news data in a larger non-news training corpus. We do the same by adding the NUCLE corpus ten times to the training corpus. This can also be seen as similar to Junczys-Dowmunt and Grundkiewicz (2016) who tune phrase-based SMT parameters on the entire NUCLE corpus. Respectable improvements on both CoNLL test sets (+Domain-Adapt. in Table 4) are achieved."
  }, {
    "heading": "3.3 Error adaptation",
    "text": "Junczys-Dowmunt and Grundkiewicz (2016) noticed that when tuning on the entire NUCLE corpus, even better results can be achieved if the error rate of NUCLE is adapted to the error rate of the original dev set. In NUCLE only 6% of tokens contain errors, while the CoNLL-2013 test set has an error-rate of about 15%. Following JunczysDowmunt and Grundkiewicz (2016), we remove correct sentences from the ten-fold oversampled NUCLE data greedily until an error-rate of 15% is achieved. This can be interpreted as a type of GEC-specific domain adaptation. We mark this method as +Domain-Adapt. in Table 4 and report for the ensemble the so far strongest results for any neural GEC system on the CoNLL benchmark."
  }, {
    "heading": "1 33.5 67.5 20.8 46.6 48.9 53.9",
    "text": ""
  }, {
    "heading": "3 36.8 59.8 28.8 49.2 51.2 56.5",
    "text": ""
  }, {
    "heading": "5 36.2 54.0 30.8 47.0 50.9 55.7",
    "text": ""
  }, {
    "heading": "3.4 Tied embeddings",
    "text": "Press and Wolf (2016) showed that parameter tying between input and output embeddings7 for language models leads to improved perplexity. Similarly, three-way weight-tying between source, target and output embeddings for neural machine translation seems to improve translation quality in terms of BLEU while also significantly decreasing the number of parameters in the model. In monolingual cases like GEC, where source and target vocabularies are (mostly) equal, embedding-tying seems to arise naturally. Output layer, decoder and encoder embeddings all share information which may further enhance the signal from corrective edits. The M2 scores for +Tied-Emb. in Table 4 are inconclusive, but we see improvements in conjunction with later modifications."
  }, {
    "heading": "3.5 Edit-weighted MLE objective",
    "text": "Previously, we applied error-rate adaptation to strengthen the signal from corrective edits in the training data. In this section, we investigate the effects of directly modifying the training loss to incorporate weights for corrective edits.\nAssuming that each target token yj has been generated by a source token xi, we scale the loss for each target token yj by a factor Λ if yj differs from xi, i.e. if yj is part of an edit. Hence, loglikelihood loss takes the following form:\nL(x, y, a) = − Ty∑\nt=1\nλ(xat , yt) logP (yt|x, y<t),\nλ(xat , yt) = { Λ if xat 6= yt 1 otherwise ,\nwhere (x, y) is a training sentence pair and a is a word alignment at ∈ {0, 1, . . . , Tx} such that source token xat generates target token yt. Alignments are computed for each sentence pair with fast-align (Dyer et al., 2013).\n7Output embeddings are encoded in the last output layer of a neural language or translation model.\nThis is comparable to reinforcement learning towards GLEU as introduced by Sakaguchi et al. (2017) or training against diffs by Schmaltz et al. (2017). In combination with previous modifications, edit-weighted Maximum Likelihood Estimation (MLE) weighting seem to outperform both methods. The parameter Λ introduces an additional hyper-parameter that requires tuning for specific tasks and affects the precision/recall trade-off. Table 5 shows Λ = 3 seems to work best among the tested values when chosen to maximize M2 on the CoNLL-2013 dev set.\nFor this setting, we achieve our strongest results of 50.95 M2 on the CoNLL benchmark (system +Edit-MLE) yet. This outperforms the results of a phrase-based SMT system with a large domainadapted language model from Junczys-Dowmunt and Grundkiewicz (2016) by 1% M2 and is the first neural system to beat this strong SMT baseline."
  }, {
    "heading": "4 Transfer learning for GEC",
    "text": "Many ideas in low-resource neural MT are rooted in transfer learning. In general, one first trains a neural model on high-resource data and then uses the resulting parameters to initialize parameters of a new model meant to be trained on lowresource data only. Various settings are possible, e.g. initializing from models trained on large outof-domain data and continuing on in-domain data (Miceli Barone et al., 2017) or using related lan-\nguage pairs (Zoph et al., 2016). Models can also be partially initialized by pre-training monolingual language models (Ramachandran et al., 2017) or only word-embeddings (Gangi and Federico, 2017). In GEC, Yannakoudakis et al. (2017) apply pretrained monolingual word-embeddings as initializations for error-detection models to re-rank SMT n-best lists. Approaches based on pre-training with monolingual data appear to be particularly wellsuited to the GEC task. Junczys-Dowmunt and Grundkiewicz (2016) published 300GB of compressed monolingual data used in their work to create a large domain-adapted Common-Crawl ngram language model.8 We use the first 100M lines. Preprocessing follows section 2.2 including BPE segmentation."
  }, {
    "heading": "4.1 Pre-training embeddings",
    "text": "Similarly to Gangi and Federico (2017) or Yannakoudakis et al. (2017), we use Word2vec (Mikolov et al., 2013) with standard settings to create word vectors. Since weights between source, target and output embeddings are tied, these embeddings are inserted once into the model, but affect computations three-fold, see the blue elements in Figure 2. The remaining parameters of the model are initialized randomly. We refer to this adaptation as +Pretrain-Emb.\n8https://github.com/grammatical/ baselines-emnlp2016"
  }, {
    "heading": "4.2 Pre-training decoder parameters",
    "text": "Following Ramachandran et al. (2017), we first train a GRU-based language model on the monolingual data. The architecture of the language model corresponds as much as possible to the structure of the decoder of the sequence-to-sequence model. All pieces that rely on the attention mechanism or the encoder have been removed. After training for two epochs, all red parameters (including embedding layers) in Figure 2 are copied from the language model to the decoder. Remaining parameters are initialized randomly. This configuration is called +Pretrain-Dec. We pretrain each model separately to make sure that all weights have been initialized randomly."
  }, {
    "heading": "4.3 Results for transfer learning",
    "text": "Table 6 summarizes the results for our transfer learning experiments. We compare the effects of pre-training with and without the edit-weighted MLE objective and can see that pre-training has significantly positive effects in both settings.\nThe last result of 53.3% M2 on the CoNLL-2014 benchmark matches the currently highest reported numbers (53.14% M2) by Chollampatt and Ng\n(2017) for a much more complex system and outperforms the highest neural GEC system (Ji et al., 2017) by 8% M2."
  }, {
    "heading": "5 Ensembling with language models",
    "text": "Phrase-based SMT systems benefit naturally from large monolingual language models, also in the case of GEC as shown by Junczys-Dowmunt and Grundkiewicz (2016). Previous work (Xie et al., 2016; Ji et al., 2017) on neural GEC used n-gram language models to incorporate monolingual data. Xie et al. (2016) built a large 5-gram model and integrated it directly into their beam search algorithm, while Ji et al. (2017) re-use the language model provided by Junczys-Dowmunt and Grundkiewicz (2016) for n-best list re-ranking.\nWe already combined monolingual data with our GEC models via pre-training, but exploiting separate language models is attractive as no additional training is required. Here, we reuse the neural language model created for pre-training.\nSimilarly to Xie et al. (2016), the score s(y|x) for a correction y of sentence x is calculated as\ns(y|x) = 1|y|\n[ 4∑\ni=1\nlogPi(y|x) + α logPLM(y) ] ,\nwhere Pi(y|x) is a translation probability for the i-th model in an ensemble of 4. PLM(y) is the language model probability for y weighted by α. We normalize by sentence length |y|. Using the dev set, we choose α that maximizes this score via linear search in range [0, 2] with step 0.1.\nTable 7 summarizes results for language model ensembling with three of our intermediate configurations. All configurations benefit from the language model in the ensemble, although gains for the pre-trained model are rather small."
  }, {
    "heading": "6 Deeper NMT models",
    "text": "So far we analyzed model-independent9 methods — only training data, hyper-parameters, parameter initialization, and the objective function were modified. In this section we investigate if these techniques can be generalized to deeper or different architectures."
  }, {
    "heading": "6.1 Architectures",
    "text": "We consider two state-of-the-art NMT architectures implemented in Marian:\nDeep RNN A deep RNN-based model (Miceli Barone et al., 2017) proposed by Sennrich et al. (2017a) for their WMT 2017 submissions. This model is based on the shallow model we used until now. It has single layer RNNs in the encoder and decoder, but increases depth by stacking multiple GRU-style blocks inside one RNN cell. A single RNN step passes through all blocks before recursion. The encoder RNN contains 4 stacked GRU blocks, the decoder 8 (1 + 7 due to the conditional GRU). Following Sennrich et al. (2017a), we enable layer-normalization in the RNN-layers. State and embedding dimensions used throughout this work and in Sennrich et al. (2017a) are the same.\nTransformer The self-attention-based model by Vaswani et al. (2017). We base our model on their default architecture of 6 complex attention/selfattention blocks in the encoder and decoder and use the same model dimensions — embeddings vector size is 512 (as before), filter size is 2048."
  }, {
    "heading": "6.2 Training settings",
    "text": "As the deep models are less reliably trained with asynchronous SGD, we change the training algorithm to synchronous SGD and for both models follow the recipe proposed in Vaswani et al. (2017), with an effective base learning rate of 0.0003, learning rate warm-up during the first 16,000 iterations, and an inverse square-root decay after the warmup. As before, we average the best 8 checkpoints. We increase dropout probability over RNN layers to 0.3 for Deep-RNN and similarly set dropout between transformer layers to 0.3. Source-word dropout as a noising technique remains unchanged.\n9The pre-training procedure however needs to be adapted to model architecture if we want to take advantage of every shared parameter, otherwise matching parameter subsets could probably be used successfully."
  }, {
    "heading": "6.3 Pre-training deep models",
    "text": "We reuse all methods included up to +Pretrain-Dec. The pre-training procedure as described in section 4.1 needs to be modified in order to maximize the number of pre-trained parameters for the larger model architectures. Again, we train decoder-only models as typical language models by removing all elements that depend on the encoder, including attention-mechanisms over the source context. We can keep the decoder self-attention layers in the transformer model. We train for two epochs on our monolingual data reusing the hyper-parameters for the parallel case above."
  }, {
    "heading": "6.4 Results",
    "text": "Table 8 summarizes the results for deeper models on the CoNLL dev and test set. Both deep models improve significantly over the shallow model with the transformer model reaching our best result reported on the CoNLL 2014 test set. For that test set it seems that ensembling with language models that were used for pre-training is ineffective when measured with M2; while on the JFLEG data measured with GLEU we see strong improvements (Fig. 3b)."
  }, {
    "heading": "7 A standard tool set for neural GEC",
    "text": "We summarize the results for our experiments in Figure 3 and provide results on the JFLEG test set. Weights for the independent language model in the full ensemble were chosen on the respective dev sets for both tasks. Comparing results according to both benchmarks and evaluation metrics (M2 for CoNLL, GLEU for JFLEG), it seems we can isolate the following set of reliable methods for state-ofthe-art neural grammatical error correction:\n• Ensembling neural GEC models with monolingual language models;\n• Dropping out entire source embeddings;\n• Weighting edits in the training objective during optimization (+Edit-MLE);\n• Pre-training on monolingual data; • Ensembling of independently trained models; • Domain and error adaptation (+Domain-\nAdapt., Error-Adapt.) towards a specific benchmark;\n• Increasing model depth.\nCombinations of these generally10 modelindependent methods helped raising the performance of pure neural GEC systems by more than 10% M2 on the CoNLL 2014 benchmark, also outperforming the previous state-of-the-art (Chollampatt and Ng, 2017), a hybrid phrase-based system with a complex spell-checking system by 2%. We also showed that a pure neural system can easily\n10Increasing depth or changing the architecture to the Transformer model is clearly not model-independent.\noutperform a strong pure phrase-based SMT system (Junczys-Dowmunt and Grundkiewicz, 2016) when similarly adapted to the GEC task.\nOn the JFLEG benchmark we outperform the previously-best pure neural system (Sakaguchi et al., 2017) by 5.9% GLEU (4.5% if no monolingual data is used). Improvements over SMT-based system like Napoles and Callison-Burch (2017)11 and Chollampatt and Ng (2017) are significant and constitute the new state-of-the-art on the JFLEG test set."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was partially funded by Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Facebook.\n11Results based on errata from https://github.com/ cnap/smt-for-gec#errata"
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "The 3rd International Conference on Learning Representations (ICLR2015).",
    "year": 2014
  }, {
    "title": "Natural Language Processing with Python",
    "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."],
    "venue": "O’Reilly Media, Inc., 1st edition.",
    "year": 2009
  }, {
    "title": "Correcting ESL errors using phrasal SMT techniques",
    "authors": ["Chris Brockett", "William B. Dolan", "Michael Gamon."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Compu-",
    "year": 2006
  }, {
    "title": "Methods for smoothing the optimizer instability in SMT",
    "authors": ["Mauro Cettolo", "Nicola Bertoldi", "Marcello Federico."],
    "venue": "MT Summit XIII: the Thirteenth Machine Translation Summit. pages 32–39.",
    "year": 2011
  }, {
    "title": "Connecting the dots: Towards human-level grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications. Association for",
    "year": 2017
  }, {
    "title": "A multilayer convolutional encoder-decoder neural network for grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "The 49th Annual Meeting of the Association for Computational Linguis-",
    "year": 2011
  }, {
    "title": "Better evaluation for grammatical error correction",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "Proceedings of the 2012 Conference of the North American",
    "year": 2012
  }, {
    "title": "Is neural machine translation ready for deployment? a case study on 30 translation directions",
    "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."],
    "venue": "Proceedings of the 9th International Workshop on Spoken",
    "year": 2016
  }, {
    "title": "Phrase-based machine translation is stateof-the-art for automatic grammatical error correction",
    "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR) .",
    "year": 2014
  }, {
    "title": "Six challenges for neural machine translation",
    "authors": ["Philipp Koehn", "Rebecca Knowles."],
    "venue": "Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics, Vancouver, pages 28–39.",
    "year": 2017
  }, {
    "title": "Regularization techniques for fine-tuning in neural machine translation",
    "authors": ["Antonio Valerio Miceli Barone", "Barry Haddow", "Ulrich Germann", "Rico Sennrich."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2017
  }, {
    "title": "Deep architectures for neural machine translation",
    "authors": ["Antonio Valerio Miceli Barone", "Jindřich Helcl", "Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the Second Conference on Machine Translation, Volume 1: Research Papers.",
    "year": 2017
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "CoRR abs/1301.3781.",
    "year": 2013
  }, {
    "title": "The effect of learner corpus size in grammatical error correction of ESL writings",
    "authors": ["Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Masaaki Nagata", "Yu Matsumoto."],
    "venue": "Proceedings of COLING 2012. pages 863–872.",
    "year": 2012
  }, {
    "title": "Systematically adapting machine translation for grammatical error correction",
    "authors": ["Courtney Napoles", "Chris Callison-Burch."],
    "venue": "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications. Association for",
    "year": 2017
  }, {
    "title": "JFLEG: A fluency corpus and benchmark for grammatical error correction",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault."],
    "venue": "Proceedings of the 2017 Conference of the European Chapter of the Association for Computational Lin-",
    "year": 2017
  }, {
    "title": "The CoNLL-2014 shared task on grammatical error correction",
    "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."],
    "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
    "year": 2014
  }, {
    "title": "The CoNLL-2013 shared task on grammatical error correction",
    "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault."],
    "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learn-",
    "year": 2013
  }, {
    "title": "The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT",
    "authors": ["Diane Nicholls."],
    "venue": "Proceedings of the Corpus Linguistics 2003 conference. volume 16, pages 572–581.",
    "year": 2003
  }, {
    "title": "Using the output embedding to improve language models",
    "authors": ["Ofir Press", "Lior Wolf."],
    "venue": "CoRR abs/1608.05859.",
    "year": 2016
  }, {
    "title": "Unsupervised pretraining for sequence to sequence learning",
    "authors": ["Prajit Ramachandran", "Peter Liu", "Quoc Le."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copen-",
    "year": 2017
  }, {
    "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
    "authors": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."],
    "venue": "Transactions of the Association for Computational Linguistics 4:169–182. https:",
    "year": 2016
  }, {
    "title": "Grammatical error correction with neural reinforcement learning",
    "authors": ["Keisuke Sakaguchi", "Matt Post", "Benjamin Van Durme."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short",
    "year": 2017
  }, {
    "title": "Adapting sequence models for sentence correction",
    "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander Rush", "Stuart Shieber."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for",
    "year": 2017
  }, {
    "title": "The University of Edinburgh’s neural MT systems for WMT17",
    "authors": ["Rico Sennrich", "Alexandra Birch", "Anna Currey", "Ulrich Germann", "Barry Haddow", "Kenneth Heafield", "Antonio Valerio Miceli Barone", "Philip Williams."],
    "venue": "Proceedings of",
    "year": 2017
  }, {
    "title": "Nematus: a toolkit for neural machine",
    "authors": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"],
    "year": 2017
  }, {
    "title": "Edinburgh neural machine translation systems for WMT16",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany,",
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-",
    "year": 2016
  }, {
    "title": "Transfer learning for low-resource",
    "authors": ["Knight"],
    "year": 2016
  }],
  "id": "SP:d81aa7db997451046d0f199e158db5da4fc1d766",
  "authors": [{
    "name": "Marcin Junczys-Dowmunt",
    "affiliations": []
  }, {
    "name": "Roman Grundkiewicz",
    "affiliations": []
  }, {
    "name": "Shubha Guha",
    "affiliations": []
  }, {
    "name": "Kenneth Heafield",
    "affiliations": []
  }],
  "abstractText": "Previously, neural methods in grammatical error correction (GEC) did not reach state-ofthe-art results compared to phrase-based statistical machine translation (SMT) baselines. We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC. We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied in most GEC settings. Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models. The combined effects of these methods result in better than state-of-the-art neural GEC models that outperform previously best neural GEC systems by more than 10% M on the CoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural state-of-the-art systems are outperformed by more than 2% on the CoNLL-2014 benchmark and by 4% on JFLEG.",
  "title": "Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task"
}