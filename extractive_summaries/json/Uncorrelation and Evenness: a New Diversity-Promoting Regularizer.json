{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A fundamental task in machine learning (ML) is to discover latent patterns underlying data, for instance, extracting topics from documents and communities from social networks. Latent space models (Bishop, 1998; Knott & Bartholomew, 1999; Blei, 2014) are effective tools to accomplish this task. An LSM contains a collection of learnable components such as hidden units in neural networks and factors in factor analysis (Harman, 1960). Each component is aimed at capturing a hidden pattern. In most LSMs, components are parameterized by vectors.\nAmong the many challenges encountered in latent space modeling, two of them are of particular interest to us.\n1Machine Learning Department, Carnegie Mellon University 2Petuum Inc. Correspondence to: Pengtao Xie <pengtaox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nFirst, under many circumstances, the frequency of patterns is highly imbalanced. Some patterns have very high frequency while others occur less frequently. As a typical example, in a news corpus, politics and economics are frequent topics (patterns) while furniture and gardening are infrequent. Classic LSMs are sensitive to the skewness of pattern frequency and less capable of capturing the infrequent patterns (Wang et al., 2014). Second, when using LSMs, one needs to carefully balance the tradeoff between model size (precisely, the number of components) and modeling power (Xie, 2015). Larger-sized LSMs are more expressive, but incur higher computational complexity. It is desirable but challenging to achieve sufficient modeling power with a small number of components.\nTo address these two challenges, recent studies (Zou & Adams, 2012; Cogswell et al., 2015; Xie et al., 2015; 2016) investigate a “diversification” strategy which encourages the components in LSMs to be mutually different, either through frequentist-style regularization (Zou & Adams, 2012; Cogswell et al., 2015; Xie et al., 2015) or Bayesian learning (Xie et al., 2016). They conjecture that: (1) through “diversification”, some components that are originally aggregated around frequent patterns can be pushed apart to cover infrequent patterns; (2) “diversified” components bear less redundancy and are mutually complementary; a small number of such components are sufficient to model data well.\nAlong this line of research, several diversity-promoting regularizers have been proposed, based upon determinantal point process (Kulesza & Taskar, 2012; Zou & Adams, 2012), cosine similarity (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and covariance (Malkin & Bilmes, 2008; Cogswell et al., 2015). While these regularizers demonstrate notable efficacy, they have certain limitations, such as sensitivity to vector scaling (Zou & Adams, 2012; Malkin & Bilmes, 2008), inability to measure diversity in a global manner (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and computational inefficiency (Cogswell et al., 2015). To address these limitations, we propose a new diversity-promoting regularizer gaining inspiration from principal component analysis (Jolliffe, 2002), biological diversity (Magurran, 2013) and information theory (Cover & Thomas, 2012).\nWe characterize “diversity” by considering two factors: uncorrelation and evenness. Uncorrelation (Cogswell et al., 2015) encourages the components to be uncorrelated, such that each component can independently capture a unique pattern. Evenness is inspired from biological diversity (Magurran, 2013) where an ecosystem is deemed to be more diverse if different species contribute equally to the maintenance of biological balance. Analogously, when measuring component diversity, we assign an “importance” score to each component and encourage these scores to be even. In the context of latent space modeling, evenness ensures each component plays a significant role in pattern discovery rather than being dominated by others.\nWe study uncorrelation and evenness from a statistical perspective. The components are considered as random variables and the eigenvalues of their covariance matrix can be leveraged to characterize these two factors. First, according to Principle Component Analysis (Jolliffe, 2002), the disparity of eigenvalues reflects the correlation among components: the more uniform the eigenvalues, the less correlated the components. Second, eigenvalues represent the variance along principal directions and can be used to measure the “importance” of components. Promoting uniform importance amounts to encouraging evenness among eigenvalues.\nTo promote uniformity among the eigenvalues, we encourage the discrete distribution parametrized by the normalized eigenvalues to have small Kullback-Leibler divergence with the uniform distribution, based on which, we define a uniform eigenvalue regularizer (UER) and make a connection with the von Neumann entropy (Bengtsson & Zyczkowski, 2007) and with the von Neumann divergence (Kulis et al., 2009). We apply UER to two LSMs – distance metric learning (DML) (Xing et al., 2002) and long shortterm memory (LSTM) network (Hochreiter & Schmidhuber, 1997) – to encourage their components to be diverse and develop an efficient optimization algorithm. Experiments on healthcare, image and text data demonstrate that UER (1) greatly improves the performance of LSMs; (2) better captures infrequent patterns; (3) reduces model size without sacrificing modeling power; (4) outperforms other diversity-promoting regularizers.\nThe major contributions of this paper are: • We propose a new diversity-promoting regularizer\nfrom the perspectives of uncorrelation and evenness.\n• We propose to simultaneously promote uncorrelation and evenness by encouraging uniformity among the eigenvalues of the covariance matrix of components.\n• We develop an efficient projected gradient descent algorithm to solve UE regularized LSM problems.\n• In experiments, we demonstrate the effectiveness of this regularizer on two LSMs: DML and LSTM.\nThe rest of the paper is organized as follows. Section 2 reviews related works. Section 3 introduces the uniform eigenvalue regularizer. Section 4 presents experimental results and Section 5 concludes the paper."
  }, {
    "heading": "2. Related Works",
    "text": "Diversity promoting regularization has been widely used in classification (Malkin & Bilmes, 2008), ensemble learning (Yu et al., 2011) and latent space modeling (Zou & Adams, 2012; Xie et al., 2015; 2017). In the sequel, we present a brief review of existing diversity-promoting regularizers. Several regularizers (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015; 2017) are based on pairwise dissimilarity of components: if every two components are dissimilar, then overall the set of components are “diverse”. Given the weight vectors {aj}mj=1 of m components, Yu et al. (2011) define the regularizer as∑\n1≤j<k≤m(1−cjk), where cjk is the cosine similarity between component j and k. In (Bao et al., 2013), the score is defined as − log( 1m(m−1) ∑ 1≤j<k≤m β|cjk|) 1 β where β > 0. In (Xie et al., 2015), the score is defined as mean of {arccos(|cjk|)} minus the variance of {arccos(|cjk|)}, where the variance term is utilized to encourage the dissimilarity scores {arccos(|cjk|)} to be even. Xie et al. (2017) define the regularizer as ∑ 1≤i<j≤m k(ai,aj) where k(·, ·) is a kernel function. These regularizers are applied to classifiers ensemble, neural network and restricted Boltzmann machine. While these regularizers can capture pairwise dissimilarities between components, they are unable to capture higher-order “diversity”.\nDeterminantal Point Process (DPP) (Kulesza & Taskar, 2012) was used by (Zou & Adams, 2012; Mariet & Sra, 2015) to encourage the topic vectors in Latent Dirichlet Allocation (Blei et al., 2003), Gaussian mean vectors in Gaussian Mixture Model and hidden units in neural network to be “diverse”. The DPP regularizer is defined as − log det(L), where L is a m × m kernel matrix and det(·) denotes the determinant of the matrix. Lij equals to k(ai,aj) and k(·, ·) is a kernel function. In geometry, det(L) is the volume of the parallelepiped formed by vectors in the feature space associated with kernel k. Vectors that result in a larger volume are considered to be more “diverse”. Since volume depends on all vectors simultaneously, DPP is able to measure diversity in a global way. The drawback of DPP lies in its sensitivity to the scaling of vectors. The volume increases with the `2 norm of vectors, but “diversity” does not. Malkin & Bilmes (2008) propose to promote diversity by maximizing the determinant of vectors’ covariance matrix. Similar to DPP, this regularizer is sensitive to vector scaling.\nUnlike the aforementioned regularizers which are defined directly on weight vectors, Cogswell et al. (2015) design a\nregularizer on hidden activations in the neural network and influence the parameters indirectly. The number of hidden activations could be much larger than that of weight parameters (like in a convolutional neural network), which may render this regularizer to be computationally inefficient."
  }, {
    "heading": "3. Method",
    "text": "In this section, we develop a uniform eigenvalue regularizer and apply it to promote “diversity” in two LSMs."
  }, {
    "heading": "3.1. Uniform Eigenvalue Regularizer",
    "text": "A latent space model (LSM) is equipped with a set of m components and each component is represented with a vector a ∈ Rd. To achieve broader coverage of infrequent patterns and reduce model size without sacrificing modeling power, previous works (Zou & Adams, 2012; Xie et al., 2015) propose to “diversify” the components by imposing a regularizer over them.\nAs a subjective concept, “diversity” has been defined in various ways as reviewed in Section 2. In this paper, we define a new measure of “diversity” by taking two factors into consideration: uncorrelation and evenness. Uncorrelation is a measure of how uncorrelated the components are. Literally, less correlation is equivalent to more diversity. Evenness is borrowed from biological diversity (Magurran, 2013), which measures how equally important different species are in maintaining the ecological balance within an ecosystem. If no species dominates another, the ecosystem is deemed as more diverse. Likewise, in latent space modeling, we desire the components to play equally important roles and no one dominates another, such that each component contributes significantly to the modeling of data.\nWe characterize the uncorrelation among components from a statistical perspective: treating the components as random variables and measuring their covariance which is proportional to their correlation. Let A ∈ Rd×m denote the component matrix where in the k-th column is the parameter vector ak of component k. Alternatively, we can take a row view (Figure 1(b)) of A: each component is treated as a random variable and each row vector ã>i can be seen as a sample drawn from the random vector formed by the m components. Let µ = 1d ∑d i=1 ãi = 1 dA >1 be the sample mean, where the elements of 1 ∈ Rd are all 1. We compute the empirical covariance matrix of the components as\nG = 1d ∑d i=1(ãi − µ)(ãi − µ)>\n= 1dA >A− ( 1dA >1)( 1dA >1)>\n(1)\nImposing the constraint A>1 = 0, we have G = 1dA >A. Suppose A is a full rank matrix and m < d, then G is a full-rank matrix with rank m.\nFor the next step, we show that the eigenvalues of G play important roles in characterizing the uncorrelation and evenness of components. We start with uncorrelation. Let G = ∑m k=1 λkuku > k be the eigendecomposition where λk is an eigenvalue and uk is the associated eigenvector. As is well known in Principle Component Analysis (Jolliffe, 2002), an eigenvector uk of the covariance matrix G represents a principal direction of the data points and the associated eigenvalue λk tells the variability of points along that direction. As shown in Figure 2(a), the larger λk is, the more spread out the points along the direction uk. When the eigenvectors (principal directions) are not aligned with coordinate axis (as shown in Figure 2), the level of disparity among eigenvalues indicates the level of correlation among the m components (random variables). The more different the eigenvalues are, the higher the correlation is. As shown in Figure 2(a), λ1 is about three times larger than λ2 and there is a high correlation along the direction u1. On the other hand, in Figure 2(b), the two eigenvalues are close to each other and the points evenly spread out in both directions with negligible correlation. In light of this, we would utilize the uniformity among eigenvalues of G to measure how uncorrelated the components are.\nSecondly, we relate the eigenvalues with the other factor of diversity: evenness. When the eigenvectors are aligned with the coordinate axis (as shown in Figure 3(a)), the components are uncorrelated. In this case, we bring in evenness to measure diversity. As stated earlier, we first need to assign each component an importance score. Since the eigenvectors are in parallel to the coordinate axis, the eigenvalues reflect the variance of components. Analogous to PCA which posits that random variables with larger variance are\nmore important, we use variance to measure importance. As shown in Figure 3(a), component 1 has a larger eigenvalue λ1 and accordingly larger variability, hence is more important than component 2. According to the evenness criteria, the components are more diverse if their importance match, which motivates us to encourage the eigenvalues to be uniform. As shown in Figure 3(b), the two eigenvalues are close and the two components have roughly the same variability, hence are similarly important.\nTo sum up, we desire to encourage the eigenvalues to be even in both cases: (1) when the eigenvectors are not aligned with the coordinate axis, they are preferred to be even to reduce the correlation of components; (2) when the eigenvectors are aligned with the coordinate axis, they are encouraged to be even such that different components contribute equally in modeling data. Previously, encouraging evenness among variances (eigenvalues) is investigated in other problems, such as learning compact representations for efficient hashing (Kong & Li, 2012; Ge et al., 2013).\nNext, we discuss how to promote uniformity among eigenvalues. The basic idea is: we normalize the eigenvalues into a probability simplex and encourage the discrete distribution parameterized by the normalized eigenvalues to have small Kullback-Leibler (KL) divergence with the uniform distribution. Given the eigenvalues {λk}mk=1, we first normalize them into a probability simplex λ̂k = λk∑m\nj=1 λj based on which we define a\ndistribution on a discrete random variable X = 1, · · · ,m where p(X = k) = λ̂k. In addition, to guarantee the eigenvalues are strictly positive, we require A>A to be positive definite. To encourage {λ̂k}mk=1 to be uniform, we encourage the distribution p(X) to be “close” to a uniform distribution q(X = k) = 1m , where the “closeness” is measured using KL divergence KL(p||q):∑m k=1 λ̂k log λ̂k 1/m = ∑m k=1 λk log λk∑m j=1 λj −log ∑m j=1 λj+logm.\nIn this equation, ∑m k=1 λk log λk is equivalent to tr(( 1dA >A)log( 1dA\n>A)), where log(·) denotes matrix logarithm. To show this, note that log( 1dA\n>A) =∑m k=1 log(λk)uku > k , according to the property of matrix logarithm. Then we have tr(( 1dA >A) log( 1dA\n>A)) equals to tr(( ∑m k=1 λkuku > k )( ∑m k=1 log(λk)uku > k )) which\nequals to ∑m k=1 λk log λk. According to the property of trace, we have tr( 1dA >A) = ∑m k=1 λk. Then the KL divergence can be turned into a diversity-promoting uniform eigenvalue regularizer (UER):\ntr(( 1dA >A) log( 1dA >A))\ntr( 1dA >A)\n− log tr(1 d A>A) (2)\nsubject to A>A 0 and A>1 = 0. Compared with previous diversity-promoting regularizers, UER has the following benefits: (1) It measures the diversity of all components in a holistic way, rather than reducing to pairwise\ndissimilarities as other regularizers (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) do. This enables UER to capture global relations among components. (2) Unlike determinant-based regularizers (Malkin & Bilmes, 2008; Zou & Adams, 2012) that are sensitive to vector scaling, UER is derived from normalized eigenvalues where the normalization effectively removes scaling. (3) UER is amenable for computation. First, unlike DoCev (Cogswell et al., 2015) that is defined over data-dependent intermediate variables incurring computational inefficiency, UER is directly defined on model parameters independent of data. Second, unlike the regularizers proposed in (Bao et al., 2013; Xie et al., 2015) that are non-smooth, UER is a smooth function. The dominating computation in UER is the matrix logarithm. It does not substantially increase computational overhead as long as the number of components is not too large (e.g., less than 1000).\nWe apply UER to promote diversity in LSMs. Let L(A) denote the objective function of an LSM, then an UEregularized LSM problem can be defined as\nminA L(A) + λ( tr(( 1dA >A) log( 1dA >A))\ntr( 1dA >A)\n− log tr( 1dA >A))\ns.t. A>1 = 0, A>A 0\nwhere λ is the regularization parameter. Similar to other diversity-promoting regularizers, UER is non-convex. Since L(A) in most LSMs is non-convex, adding UER does not substantially increase difficulty for optimization.\nConnection with von Neumann Entropy In this section, we make a connection between UER and von Neumann entropy. A matrix M is referred to as a density matrix (Bengtsson & Zyczkowski, 2007) if its eigenvalues are strictly positive and sum to one, equivalently, M 0 and tr(M) = 1. The von Neumann entropy (Bengtsson & Zyczkowski, 2007) of M is defined as S(M) = −tr(M log M), which is essentially the Shannon entropy of its eigenvalues. If the covariance matrix G of components is a density matrix, then we can use its von Neumann entropy to define a UER. To encourage the eigenvalues {λk}mk=1 of G to be even, we directly encourage the KL divergence between the distribution parameterized by the eigenvalues (without normalization) and the uniform distribution to be small: ∑m k=1 λk log λk 1/m = ∑m k=1 λk log λk+ logm, which is equivalent to encouraging the Shannon entropy of the eigenvalues − ∑m k=1 λk log λk, i.e., the von Neumann entropy of G to be large. Then a new UER can be defined as the negative von Neumann entropy of G: tr(( 1dA >A) log( 1dA >A)), subject to the constraints: (1) A>A 0; (2) tr( 1dA >A) = 1; (3) A>1 = 0. This new UER is a special case of the previous one (Eq.(2)).\nConnection with von Neumann Divergence Next we make a connection between the UER and von Neumann divergence (Kulis et al., 2009). Given two positive defi-\nnite matrices X and Y, their von Neumann divergence is defined as tr(X log X −X log Y −X + Y), which measures the closeness between the two matrices. Given two vectors x,y ∈ Rm, their generalized KL divergence can be defined as ∑m k=1 xk log( xk yk\n)− (xk − yk), which measures the closeness between two vectors. To encourage uniformity among the eigenvalues of the covariance matrix G, we can decrease the generalized KL divergence between these eigenvalues and an all-1 vector:∑m\nk=1 λk log( λk 1 )− (λk − 1)\n= tr(( 1dA >A) log( 1dA >A))− tr( 1dA >A)) +m\n(3)\nwhich is the von Neumann divergence between G and an identity matrix. Hence, encouraging uniformity among eigenvalues can be achieved by making G to be close to an identity matrix based on the von Neumann divergence."
  }, {
    "heading": "3.2. Case Studies",
    "text": "In this section, we apply the uniform eigenvalue regularizer to promote diversity in two latent space models: DML and LSTM. We also applied it to latent Dirichlet allocation (Blei et al., 2003) and classifier ensemble (Yu et al., 2011). Due to space limit, the results of the latter two are deferred to the supplements.\nDistance Metric Learning (DML) Given data pairs either labeled as “similar” or “dissimilar”, DML (Xing et al., 2002; Davis et al., 2007; Guillaumin et al., 2009) aims to learn a distance metric under which similar pairs would be placed close to each other and dissimilar pairs are separated apart. The learned distance can benefit a wide range of tasks, including retrieval, clustering and classification. Following (Weinberger & Saul, 2009), we define the distance metric between x,y ∈ Rd as ‖A>x−A>y‖22 where A ∈ Rd×m is a parameter matrix whose column vectors are components. Built upon the DML formulation in (Xie, 2015), an uniform-eigenvalue regularized DML (DML-UE) problem can be formulated as\nminA ∑\n(x,y)∈S ‖A>x−A>y‖22 + ∑\n(x,y)∈D max(0, 1− ‖A>x−A>y‖22)\n+λ( tr(( 1dA >A) log( 1dA >A))\ntr( 1dA >A)\n− log tr( 1dA >A))\ns.t. A>1 = 0, A>A 0 (4) where S and D are the set of similar and dissimilar pairs respectively. The first and second term in the objective function encourage similar pairs to have small distance and dissimilar pairs to have large distance respectively. The learned metrics are applied for information retrieval.\nLong Short-Term Memory (LSTM) Network LSTM (Hochreiter & Schmidhuber, 1997) is a type of recurrent neural network, that is better at capturing long-term dependency in sequential modeling. At each time step t where\nthe input is xt, there is an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. The transition equations among them are\nit = σ(W (i)xt + U (i)ht−1 + b (i)) ft = σ(W (f)xt + U (f)ht−1 + b (f)) ot = σ(W (o)xt + U (o)ht−1 + b (o)) ct = it tanh(W(c)xt + U(c)ht−1 + b(c)) + ft ct−1 ht = ot tanh(ct)\nwhere W = {W(s)|s ∈ S = {i, f, o, c}} and U = {U(s)|s ∈ S} are gate-specific weight matrices and B = {b(s)|s ∈ S} are bias vectors. The row vectors in W and U are treated as components. Let L(W,U ,B) denote the loss function of an LSTM network and R(·) denote the UER (including constraints), then a UE-regularized LSTM problem can be defined as\nminW,U,B L(W,U ,B) + λ ∑ s∈S(R(W(s)) +R(U(s)))\n(5) The LSTM network is applied for cloze-style reading comprehension (CSRC). The network architecture follows that in (Seo et al., 2017), which achieves the state of the art performance on CSRC."
  }, {
    "heading": "3.3. Algorithm",
    "text": "We develop a projected gradient descent (PGD) algorithm to solve the UE-regularized LSM problem in Eq.(5). The constraint A>A 0 ensures the eigenvalues of A>A are positive, such that log(A>A) is well-defined. However, it makes optimization very nasty. To address this issue, we add a small perturbation I over A>A where is a close-to-zero positive scalar and I is an identity matrix, to ensure log(A>A + I) is always well-defined. Accordingly, the constraint A>A 0 can be eliminated. The PGD algorithm iteratively performs three steps: (1) compute (sub)gradient 4A of the objective function; (2) update A using gradient descent: Ã ← A − η 4 A; (3) project Ã to the constraint set {A|A>1 = 0}. In step (1), the derivative of tr(( 1dA >A + I) log( 1dA >A + I)) is 2dA(log( 1 dA >A + I) + I). To compute the logarithm of 1dA >A + I, we perform an eigen-decomposition of this matrix into UΛU>, transform Λ into another diagonal matrix Λ̃ where Λ̃jj = log(Λjj) and then compute log( 1dA\n>A + I) as UΛ̃U>. The complexity of eigendecomposing this m-by-m matrix is O(m3). In our applications, m is no more than 500, so O(m3) is not a big bottleneck. In addition, this matrix is symmetric and the symmetry can be leveraged for fast eigen-decomposition. In implementation, we use the MAGMA library that supports efficient eigen-decomposition of symmetric matrices on both CPUs and GPUs. In step (3), the projection operation amounts to solving the following problem: minA 12‖A − Ã‖ 2 F subject to A\n>1 = 0. According to KKT conditions (Boyd & Vandenberghe, 2004), we have\nA− Ã + 1λ> = 0 and A>1 = 0. Solving this system of equations, we get A = (I − 1d11\n>)Ã, which centers the row vectors in Ã to have zero mean."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, we present experimental results.\nDataset We used five datasets in the experiments: an electronic health record dataset MIMIC-III (Johnson et al., 2016); two image datasets Stanford-Cars (Krause et al., 2013) and Caltech-UCSD-Birds (Welinder et al., 2010); two question answering (QA) datasets CNN and DailyMail (Hermann et al., 2015). The first three were used for DML and the last two for LSTM. Their statistics are summarized in Table 1. MIMIC-III contains hospital admissions of patients. The class label of each admission is the primarily diagnosed disease. For Stanford-Cars, CNN and DailyMail, we use a single train/test split specified by the data providers; for the other two, five random splits are performed and the results are averaged over the five runs. For the MIMIC-III dataset, we extract 7207-dimensional features: (1) 2 dimensions from demographics, including age and gender; (2) 5300 dimensions from clinical notes, including 5000-dimensional bag-of-words (weighted using tf-idf) and 300-dimensional Word2Vec (Mikolov et al., 2013); (3) 1905-dimensions from lab tests where the zeroorder, first-order and second-order temporal features are extracted for each of the 635 lab items. For bag-of-words, we remove stop words, then select the 5000 words with largest document frequency. For Word2Vec, we train 300- dimensional embeddings for each word; to represent a document, we average the embeddings of all words in this document. For the two image datasets, we use the VGG16 (Simonyan & Zisserman, 2014) convolutional neural network trained on the ImageNet (Deng et al., 2009) dataset to extract features, which are the 4096-dimensional outputs of the second fully-connected layer. In the two QA datasets, each instance consists of a passage, a question and an answer. The question is a cloze-style task where an entity is replaced by a placeholder and the goal is to infer this missing entity (answer) from all the possible entities appearing in the passage.\nExperimental Setup In DML experiments, two samples are labeled as similar if belonging to the same class and dissimilar otherwise. The learned distance metrics are ap-\nplied for retrieval whose performance is evaluated using precision@K. We compare with two sets of regularizers: (1) diversity-promoting regularizers based on determinant of covariance (DC) (Malkin & Bilmes, 2008), cosine similarity (CS) (Yu et al., 2011), determinantal point process (DPP) (Kulesza & Taskar, 2012; Zou & Adams, 2012), InCoherence (IC) (Bao et al., 2013), mutual angles (MA) (Xie et al., 2015), and decorrelation (DeCov) (Cogswell et al., 2015); (2) regularizers that are designed for other purposes, including L2 norm for small norm, L1 norm for sparsity, low-rankness (Recht et al., 2010) and Dropout (Srivastava et al., 2014). All these regularizers are applied to the same DML formulation (Eq.(4) without the regularizer). In addition, we compare with vanilla Euclidean distance (EUC) and other distance learning methods including information theoretic metric learning (ITML) (Davis et al., 2007), logistic discriminant metric learning (LDML) (Guillaumin et al., 2009), and geometric mean metric learning (GMML) (Zadeh et al., 2016). We use 5- fold cross validation to tune the regularization parameter in {10−5, 10−4, · · · , 105} and the number of components in {50, 100, 200, · · · , 500}. The best tuned regularization parameters of UER are: 0.001 for MIMIC, 0.01 for Cars and Birds. The best tuned component numbers are: 200 for MIMIC, 100 for Cars and 200 for Birds. The learning rate of the PGD algorithm is set to 0.001.\nIn LSTM experiments, the model architecture and experimental settings follow the Bidirectional Attention Flow (BIDAF) (Seo et al., 2017) model, which consists of the following layers: character embedding, word embedding, contextual embedding, attention flow, modeling and output. The contextual and modeling layers use long shortterm memory (LSTM) networks (Seo et al., 2017). In char-\nacter embedding based on convolutional neural network, 100 1D filters are used, each with a width of 5. The hidden state size is set to 100. AdaDelta (Zeiler, 2012) is used for optimization with a minibatch size of 48. Dropout (Srivastava et al., 2014) with probability 0.2 is used for all LSTM layers. The model is trained for 8 epochs with early stop when the validation accuracy starts to drop. We compare UER with other diversity-promoting regularizers including DC, CS, DPP, IC, MA and DeCov.\nResults Table 2 shows the retrieval precision (K = 10) on three datasets, where we observe: (1) DML-UE achieves much better precision than DML, proving that UER is an effective regularizer in improving generalization performance; (2) UER outperforms other diversity-promoting regularizers possibly due to its capability to capture global relations among all components and insensitivity to vector scaling; (3) diversity-promoting regularizers perform better than other types of regularizers such as L2, L1, low rank and Dropout, corroborating the efficacy of inducing diversity; (4) DML-UE outperforms other popular distance learning methods such as ITML, LDML and GMML.\nTable 3 shows the number of components that achieves the precision in Table 2. Compared with DML, DMLUE uses much fewer components to achieve better precision. For example, on the Cars dataset, DML-UE achieves 58.2% precision with 100 components. In contrast, with more components (300), DML achieves a much lower precision (53.1%). This demonstrates that by encouraging the components to be diverse, UER is able to reduce model size without sacrificing modeling power. UER encourages equal “importance” among components such that each component plays a significant role in modeling data. As a result, it suffices to use a small number of components to achieve larger modeling power. Compared with other diversity-promoting regularizers, UER achieves better precision with fewer components, demonstrating its ability to better promote diversity.\nNext, we verify whether “diversifying” the components in DML can better capture infrequent patterns. In the MIMICIII dataset, we consider diseases as patterns and consider a disease as “frequent” if more than 1000 hospital admissions are diagnosed with this disease and “infrequent” if otherwise. Table 4 shows the retrieval precision on frequent diseases and infrequent diseases. As can be seen, compared with the baselines, DML-UE achieves more improvement on infrequent diseases than on frequent diseases. This indicates that by encouraging the components to diversely spread out, UER is able to better capture infrequent patterns (diseases in this case) without compromising the performance on frequent patterns. On infrequent diseases, DMLUE outperforms other diversity-promoting methods, showing the advantage of UER over other diversity-promoting regularizers. To further verify this, we select 3 most frequent diseases (hypertension, AFib, CAD) and randomly select 5 infrequent ones (helicobacter pylori, acute cholecystitis, joint pain-shlder, dysarthria, pressure ulcer), and show the precision@10 on each individual disease in Table 5. As can be seen, on the five infrequent diseases, DML-UE achieves higher precision than baselines while on the three frequent diseases, DML-UE achieves comparable precision.\nWe empirically verify whether UER can promote uncorrelation and evenness. Givenm component vectors, we compute the empirical correlation (cosine similarity) of every two vectors, then average these pairwise correlation scores to measure the overall correlation of m vectors. We perform the study by learning distance metrics that have 200 components, on the MIMIC-III dataset. The average correlation under unregularized DML and DML-UE is 0.73 and 0.57 respectively. This shows that UER can reduce corre-\nlation. To measure evenness, we first measure the “importance” of components. For each component with parameter vector a, we project the training examples {xi}Ni=1 onto a: {x>i a}Ni=1, then use the variance of {x>i a}Ni=1 to measure the importance of this component. After that, we map these importance scores into a probabilistic simplex using softmax. Finally, the evenness is measured by the KL divergence between the discrete distribution parameterized by these probabilities and a uniform distribution. A smaller KL divergence indicates larger evenness. On MIMIC-III with 200 components, the KL divergence under unregularized DML and DML-UE is 3.54 and 2.92 respectively. This suggests that our regularizer is able to encourage evenness.\nTable 6 shows the runtime taken by DML methods to reach convergence. Compared with unregularized DML, DMLUE does not increase the training time substantially. The relative increase is 11.2% on MIMIC, 15.4% on Cars and 13.9% on Birds. The runtime of DML-UE is close to DML regularized by other diversity-promoting regularizers.\nIn the LSTM experiments, Table 7 shows state of the art accuracy on the two QA datasets. Compared with the original BIDAF (Seo et al., 2017), our method BIDAF-UE achieves better accuracy, further demonstrating UER’s ability to improve generalization performance. Besides, UER outperforms other regularizers."
  }, {
    "heading": "5. Conclusions",
    "text": "We propose a new diversity-promoting regularizer from the perspectives of uncorrelation which prefers the components in LSMs to be uncorrelated and evenness which encourages the components to contribute equally to the modeling of data. Gaining insight from PCA, promoting uncorrelation and evenness both amount to encouraging the covariance matrix of components to have uniform eigenvalues, which leads to a uniform eigenvalue regularizer (UER). The UER is applied to DML and LSTM. Experimental studies reveal that UER greatly boosts the performance of LSMs, better captures infrequent patterns, reduces model size without compromising modeling power and outperforms other diversity-promoting regularizers."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank the anonymous reviewers for the helpful suggestions and comments. P.X and E.X are supported by National Institutes of Health P30DA035778, R01GM114311, National Science Foundation IIS1617583, DARPA FA872105C0003 and Pennsylvania Department of Health BD4BH4100070287."
  }],
  "year": 2017,
  "references": [{
    "title": "Incoherent training of deep neural networks to decorrelate bottleneck features for speech",
    "authors": ["Bao", "Yebo", "Jiang", "Hui", "Dai", "Lirong", "Liu", "Cong"],
    "year": 2013
  }, {
    "title": "Geometry of quantum states: an introduction to quantum entanglement",
    "authors": ["Bengtsson", "Ingemar", "Zyczkowski", "Karol"],
    "year": 2007
  }, {
    "title": "Latent variable models",
    "authors": ["Bishop", "Christopher M"],
    "venue": "In Learning in graphical models,",
    "year": 1998
  }, {
    "title": "Build, compute, critique, repeat: Data analysis with latent variable models",
    "authors": ["Blei", "David M"],
    "venue": "Annual Review of Statistics and Its Application,",
    "year": 2014
  }, {
    "title": "A thorough examination of the cnn/daily mail reading comprehension",
    "authors": ["Chen", "Danqi", "Bolton", "Jason", "Manning", "Christopher D"],
    "venue": "task. arXiv preprint arXiv:1606.02858,",
    "year": 2016
  }, {
    "title": "Reducing overfitting in deep networks by decorrelating representations",
    "authors": ["Cogswell", "Michael", "Ahmed", "Faruk", "Girshick", "Ross", "Zitnick", "Larry", "Batra", "Dhruv"],
    "venue": "arXiv preprint arXiv:1511.06068,",
    "year": 2015
  }, {
    "title": "Elements of information theory",
    "authors": ["Cover", "Thomas M", "Thomas", "Joy A"],
    "year": 2012
  }, {
    "title": "Attention-over-attention neural networks for reading comprehension",
    "authors": ["Cui", "Yiming", "Chen", "Zhipeng", "Wei", "Si", "Wang", "Shijin", "Liu", "Ting", "Hu", "Guoping"],
    "venue": "arXiv preprint arXiv:1607.04423,",
    "year": 2016
  }, {
    "title": "Information-theoretic metric learning",
    "authors": ["Davis", "Jason V", "Kulis", "Brian", "Jain", "Prateek", "Sra", "Suvrit", "Dhillon", "Inderjit S"],
    "venue": "In Proceedings of the 24th international conference on Machine learning",
    "year": 2007
  }, {
    "title": "Gated-attention readers for text comprehension",
    "authors": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"],
    "venue": "arXiv preprint arXiv:1606.01549,",
    "year": 2016
  }, {
    "title": "Linguistic knowledge as memory for recurrent neural networks",
    "authors": ["Dhingra", "Bhuwan", "Yang", "Zhilin", "Cohen", "William W", "Salakhutdinov", "Ruslan"],
    "venue": "arXiv preprint arXiv:1703.02620,",
    "year": 2017
  }, {
    "title": "Optimized product quantization for approximate nearest neighbor search",
    "authors": ["Ge", "Tiezheng", "He", "Kaiming", "Ke", "Qifa", "Sun", "Jian"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2013
  }, {
    "title": "Is that you? metric learning approaches for face identification",
    "authors": ["Guillaumin", "Matthieu", "Verbeek", "Jakob", "Schmid", "Cordelia"],
    "venue": "In IEEE International Conference on Computer Vision. IEEE,",
    "year": 2009
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Long shortterm memory",
    "authors": ["Hochreiter", "Sepp", "Schmidhuber", "Jürgen"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Mimic-iii, a freely accessible critical care database",
    "authors": ["Johnson", "Alistair EW", "Pollard", "Tom J", "Shen", "Lu", "Lehman", "Li-wei H", "Feng", "Mengling", "Ghassemi", "Mohammad", "Moody", "Benjamin", "Szolovits", "Peter", "Celi", "Leo Anthony", "Mark", "Roger G"],
    "venue": "Scientific data,",
    "year": 2016
  }, {
    "title": "Principal component analysis",
    "authors": ["Jolliffe", "Ian"],
    "venue": "Wiley Online Library,",
    "year": 2002
  }, {
    "title": "Text understanding with the attention sum reader network",
    "authors": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"],
    "year": 2016
  }, {
    "title": "Latent variable models and factor analysis",
    "authors": ["Knott", "Martin", "Bartholomew", "David J"],
    "venue": "Number 7. Edward Arnold,",
    "year": 1999
  }, {
    "title": "Dynamic entity representation with maxpooling improves machine reading",
    "authors": ["Kobayashi", "Sosuke", "Tian", "Ran", "Okazaki", "Naoaki", "Inui", "Kentaro"],
    "venue": "In Proceedings of NAACL-HLT,",
    "year": 2016
  }, {
    "title": "Isotropic hashing",
    "authors": ["Kong", "Weihao", "Li", "Wu-Jun"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "3d object representations for fine-grained categorization",
    "authors": ["Krause", "Jonathan", "Stark", "Michael", "Deng", "Jia", "Fei-Fei", "Li"],
    "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,",
    "year": 2013
  }, {
    "title": "Determinantal point processes for machine learning",
    "authors": ["Kulesza", "Alex", "Taskar", "Ben"],
    "venue": "arXiv preprint arXiv:1207.6083,",
    "year": 2012
  }, {
    "title": "Low-rank kernel learning with bregman matrix divergences",
    "authors": ["Kulis", "Brian", "Sustik", "Mátyás A", "Dhillon", "Inderjit S"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Measuring biological diversity",
    "authors": ["Magurran", "Anne E"],
    "year": 2013
  }, {
    "title": "Ratio semi-definite classifiers",
    "authors": ["Malkin", "Jonathan", "Bilmes", "Jeff"],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
    "year": 2008
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2013
  }, {
    "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
    "authors": ["Recht", "Benjamin", "Fazel", "Maryam", "Parrilo", "Pablo A"],
    "venue": "SIAM review,",
    "year": 2010
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"],
    "year": 2017
  }, {
    "title": "Reasonet: Learning to stop reading in machine comprehension",
    "authors": ["Shen", "Yelong", "Huang", "Po-Sen", "Gao", "Jianfeng", "Chen", "Weizhu"],
    "venue": "arXiv preprint arXiv:1609.05284,",
    "year": 2016
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["Simonyan", "Karen", "Zisserman", "Andrew"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "Iterative alternating neural attention for machine reading",
    "authors": ["Sordoni", "Alessandro", "Bachman", "Philip", "Trischler", "Adam", "Bengio", "Yoshua"],
    "venue": "arXiv preprint arXiv:1606.02245,",
    "year": 2016
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "Natural language comprehension with the epireader",
    "authors": ["Trischler", "Adam", "Ye", "Zheng", "Yuan", "Xingdi", "Suleman", "Kaheer"],
    "venue": "arXiv preprint arXiv:1606.02270,",
    "year": 2016
  }, {
    "title": "Peacock: Learning long-tail topic features for industrial applications",
    "authors": ["Wang", "Yi", "Zhao", "Xuemin", "Sun", "Zhenlong", "Yan", "Hao", "Lifeng", "Jin", "Zhihui", "Liubin", "Gao", "Yang", "Law", "Ching", "Zeng", "Jia"],
    "venue": "ACM Transactions on Intelligent Systems and Technology,",
    "year": 2014
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Caltech-ucsd birds",
    "authors": ["Welinder", "Peter", "Branson", "Steve", "Mita", "Takeshi", "Wah", "Catherine", "Schroff", "Florian", "Belongie", "Serge", "Perona", "Pietro"],
    "year": 2010
  }, {
    "title": "Diversity leads to generalization in neural networks",
    "authors": ["Xie", "Bo", "Liang", "Yingyu", "Song", "Le"],
    "year": 2017
  }, {
    "title": "Learning compact and effective distance metrics with diversity regularization",
    "authors": ["Xie", "Pengtao"],
    "venue": "In European Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Diversifying restricted boltzmann machine for document modeling",
    "authors": ["Xie", "Pengtao", "Deng", "Yuntian", "Xing", "Eric P"],
    "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
    "year": 2015
  }, {
    "title": "Diversitypromoting bayesian learning of latent variable models",
    "authors": ["Xie", "Pengtao", "Zhu", "Jun", "Xing", "Eric"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Distance metric learning with application to clustering with side-information",
    "authors": ["Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2002
  }, {
    "title": "Diversity regularized machine",
    "authors": ["Yu", "Yang", "Li", "Yu-Feng", "Zhou", "Zhi-Hua"],
    "year": 2011
  }, {
    "title": "Geometric mean metric learning",
    "authors": ["Zadeh", "Pourya Habib", "Hosseini", "Reshad", "Sra", "Suvrit"],
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Zeiler", "Matthew D"],
    "venue": "arXiv preprint arXiv:1212.5701,",
    "year": 2012
  }, {
    "title": "Priors for diversity in generative latent variable models",
    "authors": ["Zou", "James Y", "Adams", "Ryan P"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }],
  "id": "SP:8798ea0ee095c5d30e8c5a98af41bf1f7cb0b054",
  "authors": [{
    "name": "Pengtao Xie",
    "affiliations": []
  }, {
    "name": "Aarti Singh",
    "affiliations": []
  }, {
    "name": "Eric P. Xing",
    "affiliations": []
  }],
  "abstractText": "Latent space models (LSMs) provide a principled and effective way to extract hidden patterns from observed data. To cope with two challenges in LSMs: (1) how to capture infrequent patterns when pattern frequency is imbalanced and (2) how to reduce model size without sacrificing their expressiveness, several studies have been proposed to “diversify” LSMs, which design regularizers to encourage the components therein to be “diverse”. In light of the limitations of existing approaches, we design a new diversitypromoting regularizer by considering two factors: uncorrelation and evenness, which encourage the components to be uncorrelated and to play equally important roles in modeling data. Formally, this amounts to encouraging the covariance matrix of the components to have more uniform eigenvalues. We apply the regularizer to two LSMs and develop an efficient optimization algorithm. Experiments on healthcare, image and text data demonstrate the effectiveness of the regularizer.",
  "title": "Uncorrelation and Evenness: a New Diversity-Promoting Regularizer"
}