{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2018 Conference, pages 264–272, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics\n264\ntant for chatbots if we want them to be believable. Typically, many questionanswer pairs are prepared by hand for achieving consistent responses; however, the creation of such pairs is costly. In this study, our goal is to collect a large number of question-answer pairs for a particular character by using role playbased question-answering in which multiple users play the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities."
  }, {
    "heading": "1 Introduction",
    "text": "Having a consistent personality is important for chatbots if we want them to be believable (Li et al., 2016; Gordon et al., 2016; Curry and Rieser, 2016; Sugiyama et al., 2017; Akama et al., 2017). Although neural networkbased methods are emerging for achieving consistent personalities, their quality is not that high (Li et al., 2016). Therefore, in many systems, question-answer pairs are prepared by hand for consistent responses (Takeuchi et al., 2007; Leuski et al., 2009; Traum et al., 2015). However, the creation of such pairs is costly.\nIn this study, our aim is to collect a large number of question-answer pairs for a particular character by using role play-based questionanswering (Higashinaka et al., 2013a) in which\nmultiple users play the roles of certain characters and respond to questions by online users. The concept is shown in Figure 1. The main idea is that role players collectively represent a single character and that a question is broadcast via a character to all role players. In this way, questionanswer pairs can be efficiently collected because there is less burden on people responding, and the entertaining nature of role playing makes people likelier to participate (Ments, 1999). In a smallscale experiment, Higashinaka et al. found that question-answer pairs of a character can be efficiently collected by multiple users and that users are highly motivated to provide questions and answers.\nThere were two limitations to their work. One was that the experiment was conducted using only a small number of people, who were recruited by the authors. It was not clear if the scheme would work with real users (i.e., users who are not recruited nor paid by researchers). The other limitation was that the applicability of the collected data to the creation of chatbots was not verified. In their small-scale experiment, the maximum number of question-answer pairs for a character was only about 80. This was because users were allowed to register any of their favorite characters, resulting in a small amount of data per character. It was difficult to create a chatbot with such little data.\nIn this paper, we tackle these limitations by using role play-based question-answering for collecting question-answer pairs from real users. Regarding the second limitation, we limited the characters to two famous ones so as to collect a large number of question-answer pairs per character and create workable chatbots. We conducted a subjective evaluation of the chatbots by using human participants. Our contributions are as follows:\n• We verified that role play-based question-\nanswering works with real users, collecting a large number of question-answer pairs per character in a short period.\n• We proposed a method to create chatbots from collected question-answer pairs and\nverified that it can lead to good-quality chatbots exhibiting consistent personalities.\nWe first describe our data collection by using role play-based question-answering with real users. Then, we propose our method for creating chatbots using the collected question-answer pairs. Next, we describe the experiment we conducted to evaluate the quality of the chatbots by using human participants. After covering related work, we summarize the paper and mention future work."
  }, {
    "heading": "2 Data collection by real users",
    "text": "To collect a large number of question-answer pairs per character, we focused on two characters: a real person called Max Murai and a fictional character in a novel, Ayase Aragaki. They are popular characters in Japan and have a large number of fans. We created Web sites in their fan communities so that fans could try role play-based questionanswering. We first describe the two characters in more detail and then briefly go over the Web sites. Finally, we present the statistics of the data and look at the results from several aspects."
  }, {
    "heading": "2.1 Characters",
    "text": "Max Murai His real name is Tomotake Murai\n(Max Murai is his stage name). Born in 1981, Murai is a CEO of the IT company AppBank but also a YouTuber who specializes in the live coverage of TV games. He is known to have a frank personality.\nAyase Aragaki A fictional character in the novel\n“Ore no imouto ga konnnai kawaii wakega\nnai” (My Little Sister Can’t Be This Cute), which has sold more than five million copies in Japan in its series. Ayase is not a main character but plays a supporting role. Her character is often referred to as a “Yandere”. According to Wikipedia, Yandere characters are mentally unstable, incredibly deranged, and use extreme violence or brutality as an outlet for their emotions."
  }, {
    "heading": "2.2 Web sites",
    "text": "On the Japanese streaming service NICONICO Douga1, each character has a channel for their fans. The channel is limited to subscribers. Through the generosity of this service, we were allowed to establish our Web sites for role playbased question-answering on their channels. Murai has more than 10,000 subscribers; the number of subscribes for Ayase is not disclosed.\nWe opened the Web sites in March and October 2017 for Murai and Ayase, respectively. Figures 2 and 3 show screenshots of the sites. The appearances of the sites were adjusted to the characters. The users can ask the characters questions by\n1 http://www.nicovideo.jp/\nmeans of a text-field interface, and users who want to play the role of the characters can post answers. To stimulate interaction, the Web sites show the rankings of users by their number of posts. In addition, a “like” button is placed beside each answer so that when a user thinks the answer sounds very much “like” the character in question, this opinion can be reflected in the number of “likes”. The sites were primarily for collecting one-shot questionanswer pairs. It was also possible for the Murai site to collect follow-up question-answer pairs, but this function was rarely utilized by users."
  }, {
    "heading": "2.3 Statistics",
    "text": "The statistics of the postings (at the time of submission) are listed in Table 1. We obtained a total of 12,959 and 15,112 question-answer pairs for Murai and Ayase, respectively. The size of the data is quite large. We want to emphasize that the users were not paid for their participation; they did so voluntarily. This indicates that role play-based question-answering works well with real users. As seen in the table, more than 300 users participated for each character. The questions/answers for Ayase were longer and contained more words and letters."
  }, {
    "heading": "2.4 Efficiency",
    "text": "Table 2 shows the times when the number of question-answer pairs exceeded certain thresholds. We can see how fast we could collect a few thousand question-answer pairs. For both characters, it took just about a couple of days to reach 2,000 question-answer pairs. For Ayase, the pace was much faster than for Murai, reaching 10,000 question-answer pairs in 18 days. After a cer-\ntain period, the pace of the postings slowed. Although role play-based question-answering is certainly entertaining, we may need to consider ways to keep users engaged in the interaction. Enabling more sustainable collection of questionanswer pairs is future work."
  }, {
    "heading": "2.5 Quality of the postings",
    "text": "We also evaluated the answers given by the users through subjective evaluation (see GOLD in Tables 4 and 5). We obtained the average naturalness/character-ness scores of around 3.5– 4.0 on a five-point Likert scale, indicating that the answers collected through role play-based question-answering were good. However, it was surprising that human users also struggled to obtain scores over 4.0, indicating that generating utterances for a particular character is difficult, even for humans."
  }, {
    "heading": "2.6 Satisfaction of users",
    "text": "We asked users of the channels to participate in a survey to determine their user satisfaction. We used the same questionnaire as in (Higashinaka et al., 2013a). It consisted of three questions: (Q1) How do you rate the usability of the Web site?, (Q2) Would you be willing to use the Web site again?, and (Q3) Did you enjoy role playing on the Web site? The users answered based on a five-point Likert scale, with one being the lowest score and five the highest. Twenty-three and 36 participants took part in the survey for Murai and Ayase, respectively.\nTable 3 shows the results of the questionnaire averaged over all participants. Since these results were obtained from volunteers, they may not reflect the view of all site users. However, the results are encouraging: at the very least, they indicate that there are real users who feel very positively about the experience of role play-based questionanswering."
  }, {
    "heading": "3 Creating chatbots from collected question-answer pairs",
    "text": "Now that we have successfully collected a large number of question-answer pairs for our two characters, the next step is to determine if the collected pairs can be useful for creating chatbots that exhibit the personalities of the characters in question; namely, Murai and Ayase. Since the size of the data was not large enough to train neural-generation models (Vinyals and Le, 2015), we opted for a retrieval-based approach in which relevant question-answer pairs are retrieved using an input question as a query and the answer part of the most relevant pair is returned as a chatbot’s response. One of the methods we used is a simple application of an off-the-shelf text search engine, and the other is our proposed method, which is more sophisticated and uses neural-translation models for ranking."
  }, {
    "heading": "3.1 Simple retrieval-based method",
    "text": "This method uses the text search engine LUCENE2 for retrieval. Questions and answers are first indexed with LUCENE. We use a built-in Japanese analyzer for morphological analysis. Given an input question, the BM25 algorithm (Walker et al., 1997) is used to search for a similar question using the content words of the input question. The answers for the retrieved questions are used as the output of this method. Although simple, this method is quite competitive with other methods when there are many question-answer pairs because it is likely that we will be able to find a similar question by word matching."
  }, {
    "heading": "3.2 Proposed method",
    "text": "Only using word-matching may not be sufficient. Therefore, we developed a more elaborate method that re-ranks the results retrieved from LUCENE. Our idea comes from cross-lingual question answering (CLQA) (Leuski et al., 2009) and recent advances in neural conversational models (Vinyals and Le, 2015). We also conducted semantic and intent-level matching between ques-\n2 https://lucene.apache.org/\ntions so that appropriate answer candidates could be ranked higher. Figure 4 shows the flow of this method. Given an input question Q, the method outputs answers in the following steps. The details of some of the key models/modules used in the steps are described later.\n1. Given Q, LUCENE retrieves top-N questionanswer pairs (Q′1, A ′ 1) . . . (Q ′ N , A′ N ), as de-\nscribed in Section 3.1.\n2. The question-type estimation and extended\nnamed entity recognition modules estimate the question types of Q and Q′ and extract extended named entities (Sekine et al., 2002) contained in A′. The question-type match score is calculated by using the match of the question type and the number of extended named entities in A′ requested by Q. See Section 3.3 for details.\n3. The center-word extraction module extracts\ncenter-words (noun phrases (NPs) that represent foci/topics) from both Q and Q′. The center-word score is 1.0 if one of the centerwords of Q is included in those of Q′; otherwise it is 0.0.\n4. The translation model is used to calculate the\nprobability that each A′ is translated from Q, that is, p(A′|Q). We also calculate the probability bi-directionally, that is, p(Q|A′), which has been shown to be effective in CLQA (Leuski et al., 2009). The probabilities are normalized by dividing them by the number of words on the target side. Since the raw probabilities are difficult to integrate with other scores, we sort the question-answer pairs by their probabilities and use their ranks\nto obtain the translation scores. That is, if the rank is r, its score is calculated by\n1.0 − (r − 1)/max rank, (1)\nwhere max rank is the maximum number of elements to be ranked.\n5. The semantic similarity model is used to cal-\nculate the semantic similarity score between Q and Q′. We use Word2vec (Mikolov et al., 2013) to calculate this score. First, we obtain word vectors (trained from Wikipedia) for each word in Q and Q′ and then calculate the cosine similarity between the averaged word vectors.\n6. The score calculation module integrates the\nabove scores to obtain a final score:\nscore(Q, (Q′, A′))\n= w1 ∗ search score\n+ w2 ∗ qtypes match score\n+ w3 ∗ center-word score\n+ w4 ∗ translation score\n+ w5 ∗ rev translation score\n+ w6 ∗ semantic similarity score (2)\nHere, search score indicates the score converted from the rank of the search results from LUCENE. The conversion is done using Eq. (1). rev translation score indicates the translation score derived from p(Q|A′). The w1 . . . w6 denote the weights of the scores.\n7. The question-answer pairs are sorted by their\nscores, and top-M answers are returned as output."
  }, {
    "heading": "3.3 Modules",
    "text": "We describe some of the models/modules used in the above steps.\nQuestion-type estimation and extended named entity recognition We estimated four question types for a question. One is a general question type. We used the taxonomy described in (Higashinaka et al., 2014), which has 16 question subtypes. We trained a logistic-regression based question-type classifier that classifies a question into one of the 16 question types. The other three question types come from an extended named entity taxonomy proposed by Sekine (2002). The taxonomy has three layers ranging from abstract\n(e.g., Product, Location) to more concrete entities (e.g., Car, Spa, City). We trained a logisticregression-based classifier that classifies which of the named entity types is requested in a question. We trained a classifier for each layer; thus, we had three classifiers. Using our in-house data, by two-fold cross-validation, the classification accuracies are 86.0%, 84.9%, 76.9%, and 73.5% for the general question type, layer-1, layer-2, and layer-3 question types, respectively. We also extract extended named entities from an answer candidate (A′) by using our extended named entity recognizer (Higashinaka et al., 2013b) and check whether the extended named entities corresponding to the layer-1, layer-2, and layer-3 question types of a question (Q) are included in A′.\nThe qtypes match score is calculated as follows: if there is a match of the general question type between Q and Q′, the score of one is obtained. Then, the number of extended-namedentity question types covered by the answer candidate is added to this score. Finally, this score is divided by four for normalization.\nCenter-word extraction We define a centerword as an NP that denotes the topic of a conversation. To extract such NPs from an utterance, we used conditional random fields (CRFs) (Lafferty et al., 2001). For the training and testing, we prepared about 20K sentences with centerword annotation. The sentences were those randomly sampled from our in-house open-domain conversation corpus. The feature template uses words, part-of-speech (POS) tags, and semantic categories of current and neighboring words. The extraction accuracy is 76% in F-measure with our in-house test set.\nTranslation model We trained a translation model by using a seq2seq model. We trained the model by using the OpenNMT Toolkit3 with default settings. The translation model learns to translate a question into an answer. By using the trained model, we can obtain the generative probability of an answer given a question; namely p(A′|Q). Since the amount of question-answer pairs was limited, we first trained a model by using our in-house question-answering data comprising 0.5 million pairs. The data were collected using crowd-sourcing. We then adapted the model to our question-answer pairs. The model for p(Q|A′) was trained in the same manner by swapping the\n3 http://opennmt.net/\nsource and target data. To reflect the number of “likes” associated with the answers (see Section 2.2), we augmented the number of samples by their number of “likes”; that is, if a questionanswer pair has n “likes”, n samples of such a question-answer pair are included in the training data."
  }, {
    "heading": "3.4 Extending question-answer pairs",
    "text": "When developing our method, we noticed that, in some cases, top-N search results do not contain good candidates because of the lack of question coverage. When the top-N questions do not semantically match reasonably with the input question, the answers are likely to be inappropriate. To have a wider coverage of questions, we extended our question-answer pairs by using Twitter. Our methodology was simple: for each answer A that occurred twice or more in our questionanswer pairs, we searched for tweets that resemble A with a Levenshtein distance (normalized by the sentence length) below 0.1. Then, if the tweets had an in-reply-to relationship to other tweets, they were retrieved and coupled with A to form extended question-answer pairs. The reason we focused on an answer that occurred twice or more is mainly due to the efficiency of crawling, but such answers that occur multiple times are likely to be characteristics of the characters in question. We obtained 2,607,658 and 1,032,492 extended question-answer pairs for Murai and Ayase, respectively."
  }, {
    "heading": "4 Experiments",
    "text": "We conducted a subjective evaluation to determine the quality of chatbots created from our collected question-answer pairs. We first describe how we prepared the data for evaluation and how we recruited participants. We then describe the evaluation criteria. Next, we describe the methods for comparison, in which we compared the methods presented in the previous section with a rulebased baseline and gold data (human-generated data). Finally, we explain the results and present our analyses."
  }, {
    "heading": "4.1 Data",
    "text": "To create the data for testing, we first randomly split the question-answer pairs into train, development, and test sets with the ratios of 0.8, 0.1, and 0.1, respectively. The splits were made so that the same question would not be included over multiple sets. We used the train and development\nsets to train the translation models. In addition, the question-answer pairs used by LUCENE for retrieval consisted only of train and development data. For each character, 50 questions were randomly sampled from the test set and used as input questions for this experiment."
  }, {
    "heading": "4.2 Procedure",
    "text": "We recruited 26 participants each for Murai and Ayase. The participants were recruited mainly from the subscribers of the channels for the two characters. Before taking part in the experiment, they self-declared their levels of knowledge about the characters. Then, they rated the top-1 output of the five methods (shown below) for the 50 questions; they rated at maximum 250 answers (since some methods output duplicate answers, such answers were only rated once). We compensated for their time by giving Amazon gift cards worth about 20 US dollars."
  }, {
    "heading": "4.3 Evaluation criteria",
    "text": "The participants rated each output answer by their degree of agreement to the following statements on a five-point Likert scale (1: completely disagree, 5: completely agree).\nNaturalness Not knowing who’s speaking, the\nanswer is appropriate to the input question.\nCharacter-ness Knowing that the character in\nquestion is speaking, the answer is appropriate to the input question.\nThe first criterion evaluates the interaction from a general point of view, while the second from the character point of view. Ideally, we want the character-ness to be high, but we want to maintain at least reasonable naturalness when considering the deployment of the chatbots. Note that an utterance can be rated low in terms of naturalness but high in character-ness, or vice-versa: for example, some general utterances, such as greetings, can never be uttered by particular characters."
  }, {
    "heading": "4.4 Methods for comparison",
    "text": "We compared five methods. A rule-based baseline written in Artificial Intelligence Markup Language (AIML) (Wallace, 2009) was used. The aim of having this baseline is to emulate when we do not have any question-answer pairs available. Although this is a simple rule-based baseline, it is a competitive one because it uses one of the largest rule sets in Japanese.\nRule-based baseline (AIML) The typical ap-\nproach to implement a chatbot is by using rules. We used the rules written in AIML created by Higashinaka et al (2015). There are roughly 300K rules. In Japanese, sentence-end expressions are key factors to exhibit personality. Therefore, following the method by Miyazaki et al. (2016), we created sentence-end conversion rules so that the output of this method would have the sentence-end expressions that match the characters in question.\nRetrieval-based method (LUCENE) The\nretrieval-based method described in Section 3.1.\nProposed method 1 (PROP WO EXDB) The\nproposed method described in Section 3.2. This method does not use the extended question-answer pairs from Twitter. The weights w1 . . . w6 are all set to 1.0. We used 10 for N for document retrieval.\nProposed method 2 (PROP) The proposed\nmethod with extended question-answer pairs from Twitter, as described in Section 3.4. We retrieved 10 candidates from collected question-answer pairs and 10 from extended ones. The weights w1 . . . w6 are all set to 1.0.\nUpper bound (GOLD) The gold responses by\nthe online users to the test questions. When multiple answers are given to a question, one is randomly selected."
  }, {
    "heading": "4.5 Results",
    "text": "Tables 4 and 5 list the results for Murai and Ayase, respectively. The topmost row indicates the level of knowledge about the characters. ‘All’ indicates the results of all participants, ‘High’ those who self-declared as being very knowledgeable, and ‘Low’ those who self-declared otherwise. We had 26 High and 6 Low participants for Murai, and 23 High and 3 Low participants for Ayase.\nThe tendencies were the same for the two characters, although the scores for Ayase were generally lower than those of Murai. AIML performed the worst followed by LUCENE. It was surprising that AIML’s score was low; this is probably because of the peculiarities of the input questions for the characters. PROP WO EXDB and PROP performed better than AIML and LUCENE with statistical significance in many cases. GOLD was always the best-performing method. PROP was significantly better than PROP WO EXDB for naturalness but not for character-ness.\nThese results indicate that simple text-based retrieval is not sufficient, and we need more elaborate methods. The effectiveness of the extended question-answer pairs seems to be limited. It can be useful to make the interaction seem natural, but this does not necessarily improve character-ness, although we believe that having the ability to converse naturally is a requirement for chatbots.\nWhen we focus on the results as they relate to the knowledge levels, we see large differences between High and Low. The High participants are likely to differentiate the answers more than Low\nparticipants. For example, for Murai, there were only few cases in which there was statistical significance between the proposed methods when the knowledge level was low. The tendency was the same for Ayase. This highlights the difficulty in evaluating for characters.\nTables 6 and 7 show examples of answers for Murai and Ayase, respectively. Overall, since the proposed methods achieved character-ness scores well over 3 (which is the middle point in the scale), we conclude that we can create chatbots with consistent personalities by means of role play-based question-answering."
  }, {
    "heading": "5 Related Work",
    "text": "Although there have not been any studies involving role play-based question-answering for data collection, there is a large body of research for creating chatbots that show consistent personalities.\nThere have been several studies on characters by generating or rewriting utterances reflecting the underlying personality traits (Mairesse and Walker, 2007; Sugiyama et al., 2014; Miyazaki et al., 2016). In addition, there has been extensive research on extending neural conversational models to reflect personal profiles (Li et al., 2016). Although such neural networkbased methods show promising results, they still suffer from sparsity of data and non-informative utterances (Li et al., 2015). This paper proposed increasing the source data for character building; the data can be useful for neural models."
  }, {
    "heading": "6 Summary and future work",
    "text": "Our goal for this study was to verify the effectiveness of role play-based question-answering for creating chatbots. Focusing on two famous char-\nacters in Japan, we successfully collected a large volume of question-answer pairs for two characters by using real users. We then created chatbots using the question-answer pairs. Subjective evaluation showed that although a simple textretrieval based method does not work well, our proposed method that uses translation models as well as question-type matching and center-word extraction works well, showing reasonable scores in terms of naturalness and character-ness.\nFor future work, we need to consider approaches to improve the quality of the proposed method. For example, we are currently using equal weights for scoring. We believe that they can be optimized using training data. We also want to incorporate other pieces of information that may contribute to the ranking of answers, such as sentence embeddings (Kiros et al., 2015), discourse relations (Lin et al., 2009; Otsuka et al., 2017), and external knowledge about the characters. Although we used two very different characters in this paper, we want to use additional types of characters as targets for role play-based question-answering. We also want to incorporate the chatbots into the Web sites so that the users can feel they are training up the characters."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the developers of DWANGO Co., Ltd. for creating the role play-based questionanswering Web sites. We also thank the subscribers of the Max Murai and Tukasa Fushimi channels on NICONICO Douga for their cooperation. We thank the members of the Service Innovation Department at NTT DOCOMO, especially Yuiko Tsunomori, for helpful discussions and suggestions."
  }],
  "year": 2018,
  "references": [{
    "title": "Generating stylistically consistent dialog responses with transfer learning",
    "authors": ["Reina Akama", "Kazuaki Inada", "Naoya Inoue", "Sosuke Kobayashi", "Kentaro Inui."],
    "venue": "Proc. IJCNLP, volume 2, pages 408–412.",
    "year": 2017
  }, {
    "title": "A subjective evaluation of chatbot engines",
    "authors": ["Amanda Cercas Curry", "Verena Rieser."],
    "venue": "Proc. WOCHAT.",
    "year": 2016
  }, {
    "title": "Wochat chatbot user experience summary",
    "authors": ["Carla Gordon", "Jessica Tin", "Jeremy Brown", "Elisabeth Fritzsch", "Shirley Gabber."],
    "venue": "Proc. WOCHAT.",
    "year": 2016
  }, {
    "title": "Using role play for collecting question-answer pairs for dialogue agents",
    "authors": ["Ryuichiro Higashinaka", "Kohji Dohsaka", "Hideki Isozaki."],
    "venue": "Proc. INTERSPEECH, pages 1097–1100.",
    "year": 2013
  }, {
    "title": "Towards an open-domain conversational system fully based on natural language processing",
    "authors": ["Ryuichiro Higashinaka", "Kenji Imamura", "Toyomi Meguro", "Chiaki Miyazaki", "Nozomi Kobayashi", "Hiroaki Sugiyama", "Toru Hirano", "Toshiro Makino", "Yoshihiro Matsuo."],
    "venue": "Proc. COL-",
    "year": 2014
  }, {
    "title": "On the difficulty of improving hand-crafted rules in chat-oriented dialogue systems",
    "authors": ["Ryuichiro Higashinaka", "Toyomi Meguro", "Hiroaki Sugiyama", "Toshiro Makino", "Yoshihiro Matsuo."],
    "venue": "Proc. APSIPA, pages 1014–1018.",
    "year": 2015
  }, {
    "title": "Question answering technology for pinpointing answers to a wide range of questions",
    "authors": ["Ryuichiro Higashinaka", "Kugatsu Sadamitsu", "Kuniko Saito", "Nozomi Kobayashi."],
    "venue": "NTT Technical Review, 11(7).",
    "year": 2013
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Proc. NIPS, pages 3294–3302.",
    "year": 2015
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"],
    "year": 2001
  }, {
    "title": "Building effective question answering characters",
    "authors": ["Anton Leuski", "Ronakkumar Patel", "David Traum", "Brandon Kennedy."],
    "venue": "Proc. SIGDIAL, pages 18–27.",
    "year": 2009
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "arXiv preprint arXiv:1510.03055.",
    "year": 2015
  }, {
    "title": "A persona-based neural conversation model",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P Spithourakis", "Jianfeng Gao", "Bill Dolan."],
    "venue": "arXiv preprint arXiv:1603.06155.",
    "year": 2016
  }, {
    "title": "Recognizing implicit discourse relations in the penn discourse treebank",
    "authors": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng."],
    "venue": "Proc. EMNLP, pages 343–351.",
    "year": 2009
  }, {
    "title": "PERSONAGE: Personality generation for dialogue",
    "authors": ["François Mairesse", "Marilyn Walker."],
    "venue": "Proc. ACL, pages 496–503.",
    "year": 2007
  }, {
    "title": "The Effective Use of Role Play: Practical Techniques for Improving Learning",
    "authors": ["Morry Van Ments."],
    "venue": "Kogan Page Publishers.",
    "year": 1999
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Proc. NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Towards an entertaining natural language generation system: Linguistic peculiarities of Japanese fictional characters",
    "authors": ["Chiaki Miyazaki", "Toru Hirano", "Ryuichiro Higashinaka", "Yoshihiro Matsuo."],
    "venue": "Proc. SIGDIAL, pages 319–328.",
    "year": 2016
  }, {
    "title": "Utterance selection using discourse relation filter for chat-oriented dialogue systems",
    "authors": ["Atsushi Otsuka", "Toru Hirano", "Chiaki Miyazaki", "Ryuichiro Higashinaka", "Toshiro Makino", "Yoshihiro Matsuo."],
    "venue": "Dialogues with Social Robots, pages 355–365. Springer.",
    "year": 2017
  }, {
    "title": "Extended named entity hierarchy",
    "authors": ["Satoshi Sekine", "Kiyoshi Sudo", "Chikashi Nobata."],
    "venue": "Proc. LREC.",
    "year": 2002
  }, {
    "title": "Evaluation of question-answering system about conversational agent’s personality",
    "authors": ["Hiroaki Sugiyama", "Toyomi Meguro", "Ryuichiro Higashinaka."],
    "venue": "Dialogues with Social Robots, pages 183–194. Springer.",
    "year": 2017
  }, {
    "title": "Large-scale collection and analysis of personal question-answer pairs for conversational agents",
    "authors": ["Hiroaki Sugiyama", "Toyomi Meguro", "Ryuichiro Higashinaka", "Yasuhiro Minami."],
    "venue": "Proc. IVA, pages 420–433.",
    "year": 2014
  }, {
    "title": "Construction and optimization of a question and answer database for a real-environment speech-oriented guidance system",
    "authors": ["Shota Takeuchi", "Tobias Cincarek", "Hiromichi Kawanami", "Hiroshi Saruwatari", "Kiyohiro Shikano."],
    "venue": "Proc. Oriental COCOSDA, pages 149–154.",
    "year": 2007
  }, {
    "title": "Evaluating spoken dialogue processing for time-offset interaction",
    "authors": ["David Traum", "Kallirroi Georgila", "Ron Artstein", "Anton Leuski."],
    "venue": "Proc. SIGDIAL, pages 199– 208.",
    "year": 2015
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "arXiv preprint arXiv:1506.05869.",
    "year": 2015
  }, {
    "title": "Okapi at TREC-6 automatic ad hoc, VLC, routing, filtering and QSDR",
    "authors": ["Steve Walker", "Stephen E Robertson", "Mohand Boughanem", "Gareth JF Jones", "Karen Sparck Jones."],
    "venue": "Proc. TREC, pages 125–136.",
    "year": 1997
  }, {
    "title": "The anatomy of alice",
    "authors": ["Richard S Wallace."],
    "venue": "Parsing the Turing Test, pages 181–210. Springer.",
    "year": 2009
  }],
  "id": "SP:de6717bc9e901d52913f02f5d012091bf6432421",
  "authors": [{
    "name": "Ryuichiro Higashinaka",
    "affiliations": []
  }, {
    "name": "Masahiro Mizukami",
    "affiliations": []
  }, {
    "name": "Hidetoshi Kawabata",
    "affiliations": []
  }, {
    "name": "Emi Yamaguchi",
    "affiliations": []
  }, {
    "name": "Noritake Adachi",
    "affiliations": []
  }, {
    "name": "Junji Tomita",
    "affiliations": []
  }],
  "abstractText": "Having consistent personalities is important for chatbots if we want them to be believable. Typically, many questionanswer pairs are prepared by hand for achieving consistent responses; however, the creation of such pairs is costly. In this study, our goal is to collect a large number of question-answer pairs for a particular character by using role playbased question-answering in which multiple users play the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based questionanswering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities.",
  "title": "Role play-based question-answering by real users for building chatbots with consistent personalities"
}