{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1. The hidden layer, h = (h1, . . . , hd), consists of d binary random variables with an unknown joint distribution Ph : {0, 1}d → [0, 1]. The observed vector x ∈ Rm with m ≥ d features is modeled as\nx = W>h+ σξ, (1)\nwhereW ∈ Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ ≥ 0 is the noise level and ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d. zero mean and unit variance random variables.\n1Dept. of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel. 2Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel. 3Program of Applied Mathematics, Yale University, New Haven, CT 06511, USA. Correspondence to: Ariel Jaffe <ariel.jaffe@weizmann.ac.il>, Roi Weiss <roi.weiss@weizmann.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nFor simplicity we assume it is Gaussian, though our method can be modified to handle other noise distributions.\nThe model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997). A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012). G-RBMs were used, e.g., in modeling human motion (Taylor et al., 2007) and natural image patches (Melchior et al., 2017).\nGiven n i.i.d. samples x1, . . . ,xn from model (1), the goal is to estimate the weight matrixW . A common approach for learning W is by maximum likelihood. As this function is non-convex, common optimization schemes include the EM algorithm and alternating least squares (ALS). In addition, several works developed iterative methods specialized to GRBMs (Hinton, 2010; Cho et al., 2011). All these methods, however, often lack consistency guarantees and may not be well suited for large datasets due to their potential slow convergence. This is not surprising, as learning W under model (1) is believed to be computationally hard; see for example Mossel & Roch (2005).\nOver the past years, several works considered variants and specific instances of model (1) under additional assumptions on the distribution Ph or on the weight matrix W . For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyvärinen et al., 2004). In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyvärinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014). Similarly, when the hidden units are mutually exclusive, namely Ph has support h ∈ {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means. Efficient and consistent algorithms were derived for this case as well (Moitra & Valiant, 2010; Anandkumar et al., 2012a;b; Hsu & Kakade, 2013). Among those, most relevant to this work\nare orthogonal tensor decomposition methods (Anandkumar et al., 2014). Interestingly, these methods can learn some additional latent models, with hidden units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models (Arabshahi & Anandkumar, 2017).\nLearning W given the observed data {xj}nj=1 can also be viewed as a noisy matrix factorization problem. If W is known to be non-negative, then various non-negative matrix factorization methods can be used. Moreover, under appropriate conditions, some of these methods were proven to be computationally efficient and consistent (Donoho & Stodden, 2004; Arora et al., 2012). For general full rank W , the matrix factorization method in Slawski et al. (2013) (SHL) exactly recovers W when σ = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and has no consistency guarantees when σ > 0.\nA tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed data. We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n − 12 ) for any noise level σ ≥ 0.\nThe non-degeneracy conditions we pose are significantly weaker than those required by the previous tensor decomposition methods mentioned above. In particular, their assumptions and resulting methods can be viewed as specific cases of our more general approach.\nSimilarly to the matrix factorization method in Slawski et al. (2013), our algorithm has runtime linear in n, polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime is spent on computing the eigenpairs of a d× d× d tensor. Practically, our method, implemented without any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC. Furthermore, the overall runtime can be significantly reduced, since the step of computing the tensor eigenpairs can be embarrassingly parallelized.\nPaper outline In the next section we give necessary background on tensor eigenpairs. In Section 3 we introduce our\nmethod in the case σ = 0. The case σ ≥ 0 is treated in Section 4. Experiments with our method and comparison to other approaches appear in Section 5. All proofs are deferred to the supplementary material."
  }, {
    "heading": "2. Preliminaries",
    "text": "Notation Denote [d] = {1, . . . , d} and ei as the i-th unit vector. We slightly abuse notation and view a matrixW also as the set of its columns, namely w ∈ W is some column of W and span(W ) is the span of all its columns. The unit sphere is denoted by Sd−1 = {u ∈ Rd : ‖u‖ = 1}.\nA tensor T ∈ Rd×d×d is symmetric if Tijk = Tπ(i,j,k) for all permutations π of i, j, k. Here, we consider only symmetric tensors. T can also be seen as a multi-linear operator: for matrices W 1,W 2,W 3 with W i ∈ Rd×di , the tensor-mode product, denoted T (W 1,W 2,W 3), is a d1 × d2 × d3 tensor whose (i1, i2, i3)-th entry is∑\nj1,j2,j3∈[d]\nW 1j1i1W 2 j2i2W 3 j3i3Tj1j2j3 .\nTensor eigenpairs Several types of eigenpairs of a tensor have been proposed. Here, we consider the following definition, termed Z-eigenpairs by Qi (2005) and l2-eigenpairs by Lim (2005). Henceforth we just call them eigenpairs.\nDefinition 1. (u, λ) ∈ Rd × R is an eigenpair of T if\nT (I,u,u) = λu and ‖u‖ = 1. (2)\nNote that if (u, λ) is an eigenpair then the eigenvalue is simply λ = T (u,u,u). In addition, (−u,−λ) is also an eigenpair. Following common practice we treat these two pairs as one and make the convention that λ ≥ 0.\nIn contrast to the matrix case, the number of eigenvalues {λ} of a tensor T ∈ Rd×d×d can be much larger than d. As shown by Cartwright & Sturmfels (2013), for a d× d× d tensor, there can be at most 2d − 1 of them. With precise definitions appearing in Cartwright & Sturmfels (2013), for a generic tensor, all its eigenvalues have multiplicity one and the number of eigenpairs {(u, λ)} is at most 2d − 1.\nIn principle, enumerating the set of all eigenpairs of a general symmetric tensor is a #P problem (Hillar & Lim, 2013). Nevertheless, several methods have been proposed for computing at least some eigenpairs, including iterative higherorder power methods (Kolda & Mayo, 2011; 2014), homotopy continuation (Chen et al., 2016), semidefinite programming (Cui et al., 2014), and iterative Newton-based methods (Jaffe et al., 2017; Guo et al., 2017). We conclude this section with the definition of Newton-stable eigenpairs (Jaffe et al., 2017) which are most relevant to our work.\nNewton-stable eigenpairs Equivalently to (2), eigenpairs of T can also be characterized by the function g : Rd → Rd,\ng(u) = T (I,u,u)− T (u,u,u) · u. (3)\nIt is easy to verify that a pair (u, λ) with ‖u‖ = 1 is an eigenpair of T if and only if g(u) = 0 and λ = T (u,u,u). The stability of an eigenpair is determined by its Jacobian matrix ∇g(u) ∈ Rd×d, more precisely, by its projection into the d− 1 dimensional subspace orthogonal to u. Formally, let Lu ∈ Rd×(d−1) be a matrix with d− 1 orthonormal columns that span the subspace orthogonal to u and define the (d− 1)× (d− 1) projected Jacobian matrix\nJp(u) = L > u∇g(u)Lu. (4)\nDefinition 2. An eigenpair (u, λ) of T ∈ Rd×d×d is Newton-stable if the matrix Jp(u) has full rank d− 1.\nThe homotopy continuation method in Chen et al. (2016) is guaranteed to compute all the Newton-stable eigenpairs of a tensor. Alternatively, all the Newton-stable eigenpairs can be computed by the iterative orthogonal Newton correction method (O–NCM) in Jaffe et al. (2017) as these are the attracting fixed points for this algorithm. Moreover, O–NCM converges to any Newton-stable eigenpair at a quadratic rate given a sufficiently close initial guess. Finally, for a generic tensor, all its eigenpairs are Newton-stable."
  }, {
    "heading": "3. Learning in the noiseless case",
    "text": "To motivate our approach for estimating the matrix W it is instructive to first consider the ideal noiseless case where σ = 0. In this case, model (1) takes the form x = W>h. Our problem then becomes that of factorizing the observed matrix X = [x1, . . . ,xn] ∈ Rm×n of n samples into a product of real and binary low-rank matrices,1\nFind W ∈ Rd×m, H ∈ {0, 1}d×n s.t. X = W>H. (5)\nTo be able to recover W we first need conditions under which the decomposition of X into W and H is unique. Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore this degeneracy. A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid. Formally, H ∈ {0, 1}d×n is rigid if any non-trivial linear combination of its rows yields a non-binary vector: ∀u 6= 0,\nu>H ∈ {0, 1}n ⇔ u ∈ {ei}di=1. (6)\nCondition (6) is satisfied, for example, when the columns of H include ei and ei + ej for all i 6= j ∈ [d]. If there\n1Note that this is different from the problem known as “Boolean matrix factorization”, where X and W are assumed to be binary as well; see Miettinen & Vreeken (2014) and references therein.\nexists a positive constant p0 > 0 such that Ph(ei) ≥ p0 and Ph(ei + ej) ≥ p0, then for a sample size n > 2 log(d)/p0 the matrix H is rigid with high probability.\nThe following proposition, similar in nature to the (affine constrained) uniqueness guarantee in Slawski et al. (2013), shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary constraints.\nProposition 1. Let X = W>H with H ∈ {0, 1}d×n rigid and W ∈ Rd×m full rank with m ≥ d. Let W † ∈ Rm×d be the unique right pseudo-inverse of W so WW † = Id. Then W and H are unique and for all v ∈ span(X) \\ {0},\nv>X ∈ {0, 1}n ⇔ v ∈W †. (7)\nHence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of finding the unique set W † = {v∗1 , . . . ,v∗d} ⊆ span(X) of d non-zero vectors that satisfy the binary constraints v∗i >X ∈ {0, 1}n. The weight matrix is then W = (W †)†.\nAlgorithm outline We recover W † via a two step procedure. First, a finite set V = {v1,v2, . . . } ⊆ span(X) of candidate vectors is computed with a guarantee that W † ⊆ V . Specifically, V is computed from the set of eigenpairs of a d× d× d tensor, constructed from the low order moments of X . Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v ∈ V that satisfy v>X ∈ {0, 1}n.\nBefore describing the two steps in more detail we first state the additional non-degeneracy conditions we pose. To this end, denote the unknown first, second, and third order moments of the latent binary vector h by\np = E[h] ∈ Rd, C = E[h⊗ h] ∈ Rd×d, C = E[h⊗ h⊗ h] ∈ Rd×d×d. (8)\nNon-degeneracy conditions We assume the following:\n(I) H is rigid.\n(II) rank(2C(I, I, ei)− C) = d for all i ∈ [d].\nCondition (I) implies that both rank(HH>) = d and rank(C) = d. This in turn implies pi = E[hi] > 0 for all i ∈ [d] and that at most one variable hi has pi = 1. Such an “always on” variable can model a fixed bias to x. As far as we know, condition (II) is new and its nature will become clear shortly.\nWe now describe each step of our algorithm in more detail.\nComputing the candidate set To compute a set V that is guaranteed to include the columns of W † we make use of\nthe second and third order moments of x,\nM = E[x⊗ x] ∈ Rm×m, M = E[x⊗ x⊗ x] ∈ Rm×m×m.\n(9)\nGiven a large number of samples n 1, these can be easily and accurately estimated from the sample X . For simplicity, in this section we consider the population setting where n → ∞, so M andM are known exactly. M andM are related to the unknown second and third order moments of h in (8) via (Anandkumar et al., 2014)\nM = W>CW, M = C(W,W,W ). (10)\nSince both C and W are full rank, the number of latent units can be deduced by rank(M) = d. Since C is positive definite, there is a whitening matrix K ∈ Rm×d such that\nK>MK = Id. (11)\nSuch a K can be computed, for example, by an eigendecomposition of M . Although K is not unique, any K ⊆ span(M) that satisfies (11) suffices for our purpose. Define the d× d× d lower dimensional whitened tensor\nW =M(K,K,K). (12)\nDenote the set of eigenpairs ofW by\nU = {(u, λ) ∈ Sd−1 × R+ :W(I,u,u) = λu}. (13)\nOur set of candidates is then\nV = {Ku/λ : (u, λ) ∈ U with λ ≥ 1} ⊆ Rm. (14)\nThe following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W †. Lemma 1. LetW be the tensor in (12) corresponding to model (1) with σ = 0 and let V be as in (14). If condition (I) holds then W † ⊆ V . In particular, each (ui, λi) in the set of d relevant eigenpairs\nU∗ = {(u, λ) ∈ U : Ku/λ ∈W †} (15)\nhas the eigenvalue λi = 1/ √ pi ≥ 1 where pi = E[hi] > 0.\nComputing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W † by first calculating the set U of eigenpairs ofW . Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013). Moreover, besides the d columns of W †, the set V in (14) may contain many spurious candidates, as the number of eigenpairs of W is typically O(2d) d (Cartwright & Sturmfels, 2013).\nNevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs of a tensor under appropriate stability conditions. The following lemma highlights the importance of condition (II) for the stability of the eigenpairs in U∗. Note that conditions (I)-(II) do not depend on W , but only on the distribution of h.\nLemma 2. Let W be the whitened tensor in (12) corresponding to model (1) with σ = 0. If conditions (I)-(II) hold, then all (u, λ) ∈ U∗ are Newton-stable eigenpairs ofW .\nHence, under conditions (I)-(II), the homotopy method in Chen et al. (2016), or alternatively the O–NCM with a sufficiently large number of random initializations (Jaffe et al., 2017), are guaranteed to compute a candidate set V which includes all the columns of W †. The next step is to extract W † out of V .\nFiltering As suggested by Eq. (7) we select the subset of vectors V̄ ⊆ V that satisfy the binary constraints,\nV̄ = {v ∈ V : vTX ∈ {0, 1}n}. (16)\nIndeed, under condition (I), Proposition 1 implies that V̄ = W † and the weight matrix is thus W = V̄ †.\nAlgorithm 2 in Appendix C summarizes our method for the noiseless case and has the following recovery guarantee. Theorem 1. Let X be a matrix of n samples from model (1) with σ = 0. If conditions (I)-(II) hold, then the above method recovers W exactly.\nWe note that when σ = 0 and conditions (I)-(II) hold for the empirical latent moments Ĉ and Ĉ (rather than C and C), the above procedure exactly recovers W when M and M are replaced by their finite sample estimates. The matrix factorization method SHL in Slawski et al. (2013) also exactly recovers W in the case σ = 0. While its runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach. This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d × d × d tensor. However, SHL does not take advantage of the large number of samples n, since only m× d sub-matrices of the m×n sample matrix X are used for constructing its candidate set. Indeed, in the noisy case where σ > 0, SHL has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels of noise. In the next section we derive a modified version of our method that consistently estimates W for any noise level σ ≥ 0."
  }, {
    "heading": "4. Learning in the presence of noise",
    "text": "The method in Section 3 to estimateW is clearly inadequate when σ > 0. However, we now show that by making several adjustments, the two steps of computing the candidate set and its filtering can be both made robust to noise, yielding a consistent estimator of W for any σ ≥ 0.\nComputing the candidate set As in the case σ = 0, our goal in the first step is to compute a finite candidate set\nVσ ⊆ Rm that is guaranteed to contain accurate estimates for the d columns of W †. To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment µ = E[x] and define the following noise corrected moments,\nMσ = M − σ2Im,\nMσ = M− σ2 m∑ i=1 ( µ⊗ ei ⊗ ei\n+ ei ⊗ µ⊗ ei + ei ⊗ ei ⊗ µ ) .\n(17)\nBy assumption, the noise satisfies E[ξ3i ] = 0. Thus, similarly to the moment equations in (10), the modified moments in (17) are related to these ofh by (Anandkumar et al., 2014)\nMσ = W >CW, Mσ = C(W,W,W ). (18)\nHence, if Mσ andMσ were known exactly, a candidate set Vσ that contains W † could be obtained exactly as in the noiseless case, but with M andM replaced with Mσ and Mσ; namely, first calculate the whitening matrix Kσ such that K>σMσKσ = Id and then compute the eigenpairs of the population whitened tensor\nWσ =Mσ(Kσ,Kσ,Kσ). (19)\nIn practice, σ2, d, µ,M andM are all unknown and need to be estimated from the sample matrix X . Assuming m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009). For simplicity, we assume they are known exactly. Similarly, µ, M ,M are consistently estimated by their empirical means, µ̂, M̂ , and M̂. So, after computing the plugin estimates K̂σ such that K̂>σ M̂σK̂σ = Id and Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ), we compute the set Ûσ of eigenpairs of Ŵσ and for some small 0 < τ = O(n− 1 2 ) take our candidate set as\nV̂σ = {K̂σu/λ : (u, λ) ∈ Ûσ with λ ≥ 1−τ}. (20)\nThe following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations. Namely, for perturbations of order δ 1 inWσ and Kσ , the method computes a candidate set V̂σ that contains a subset of d vectors that are O(δ) close to the columns of W †. Furthermore, these d vectors all correspond to Newton-stable eigenpairs of the perturbed tensor and are Ω(1) separated from the other candidates in V̂σ .\nLemma 3. Let Kσ,Wσ be the population quantities in (19) and let K̂σ, Ŵσ be their perturbed versions, inducing the candidate set V̂σ in (20). If conditions (I)-(II) hold, then there are c, δ0, δ1 > 0 such that for all 0 ≤ δ ≤ δ0 the following holds: If the perturbed versions satisfy\nmax{‖Ŵσ −Wσ‖F , ‖K̂σ −Kσ‖F } ≤ δ, (21)\nthen any v∗ ∈W † has a unique v̂ ∈ V̂σ such that\n‖v̂ − v∗‖ ≤ cδ. (22)\nMoreover, v̂ corresponds to a Newton-stable eigenpair of Ŵσ with eigenvalue λ ≥ 1− cδ and for all ṽ ∈ V̂σ \\ {v̂},\n‖ṽ − v∗‖ ≥ δ1 > 2cδ. (23)\nThe proof is based on the implicit function theorem (Hubbard & Hubbard, 2015); small perturbations to a tensor result in small perturbations to its Newton-stable eigenpairs.\nNow, by the delta method, the plugin estimates K̂σ and Ŵσ are both OP (n− 1 2 ) close to their population quantities,\n‖K̂σ −Kσ‖F = OP (n − 12 ),\n‖Ŵσ −Wσ‖F = OP (n − 12 ).\n(24)\nBy (24), we have that (21) holds with δ = OP (n− 1 2 ). Hence, by Lemma 3, the eigenpairs of Ŵσ provide a candidate set V̂σ that contains d vectors that are OP (n− 1 2 ) close to the columns of W †. In addition, any irrelevant candidate is ΩP (1) far away from W †. As we show next, these properties ensure that with high probability the d relevant candidates can be identified in V̂σ .\nFiltering Given the candidate set V̂σ computed in the first step, our goal now is to find a set V̄σ ⊆ V̂σ of d vectors that accurately estimate the d columns of W †. To simplify the theoretical analysis, we assume the filtering step is done using a sample X of size n that is independent of V̂σ. This can be achieved by first splitting a given sample of size 2n into two sets of size n, one for each step.\nRecall that for x from model (1) and any v ∈ Rm,\nv>x = v>W>h+ σv>ξ. (25)\nObviously, when σ > 0, the filtering procedure in (16) for the noiseless case is inadequate, as typically no v∗ ∈ W † will exactly satisfy v∗>X ∈ {0, 1}n. Nevertheless, we expect that for a sufficiently small noise level σ, any v ∈ V̂σ that is close to some v∗ ∈ W † will result in v>X that is close to being binary, while any v sufficiently far from W † will result in v>X that is far from being binary. A natural measure for how v>X is “far from being binary”, similar to the one used for filtering in Slawski et al. (2013), is simply its deviation from its binary rounding,\nmin b∈{0,1}n\n‖vTX − b‖2\nn‖v‖2 . (26)\nEq. (26) works extremely well for small σ, but fails for high noise levels. Here we instead propose a filtering procedure based on the classical Kolmogorov-Smirnov goodness of fit\ntest (Lehmann & Romano, 2006). As we show below, this approach gives consistent estimates of W for any σ > 0.\nBefore describing the test, we first introduce the probabilistic analogue of the rigidity condition (6). For any u ∈ Rd, define its corresponding expected binary rounding error,\nr(u) = Eh∼Ph [\nmin b∈{0,1}\n(u>h− b)2 ] .\nClearly, r(0) = 0 and r(ei) = 0 for all i ∈ [d]. We pose the following expected rigidity condition: for all u 6= 0,\nr(u) = 0 ⇔ u ∈ {ei}di=1. (27)\nAnalogously to the deterministic rigidity condition in (6), condition (27) is satisfied, for example, when Ph(ei) > 0 and Ph(ei + ej) > 0 for all i 6= j ∈ [d].\nTo introduce our filtering test, recall that under model (1), ξ ∼ N (0, Im). Hence, for any fixed v, the random variable v>x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),\nv>x ∼ ∑\nh∈{0,1}d Ph(h) · N (v>W>h, σ2‖v‖2). (28)\nDenote the cumulative distribution function of v>x by Fv . For general v, this mixture may have up to 2d distinct components. However, for v∗ ∈W †, it reduces to a mixture of two components with means at 0 and 1. More precisely, for any candidate v with corresponding eigenvalue λ(v) ≥ 1, define the GMM with two components\n(1− 1λ(v)2 ) · N (0, σ 2‖v‖2) + 1λ(v)2 · N (1, σ 2‖v‖2). (29)\nDenote its cumulative distribution function by Gv. The following lemma shows that under condition (27), Gv fully characterizes the columns of W †.\nLemma 4. Let Kσ,Wσ be the population quantities in (19) and let Vσ be the set of population candidates as computed from the eigenpairs of Wσ. If conditions (I)-(II) and the expected rigidity condition (27) hold, then for any v ∈ Vσ with corresponding eigenvalue λ(v),\nFv = Gv ⇔ v ∈W †.\nGiven the empirical candidate set V̂σ, Lemma 4 suggests ranking all v̂ ∈ V̂σ according to their goodness of fit to Gv̂ and taking the d candidates with the best fit. More precisely, given a sample X = [x1, . . . ,xn] that is independent of V̂σ , for each candidate v̂ ∈ V̂σ we compute the empirical cumulative distribution function, F̂v̂(t) = 1n ∑n j=1 1{v̂>xj ≤ t}, t ∈ R, and calculate its Kolmogorov-Smirnov score\n∆n(v̂) = sup t∈R |F̂v̂(t)−Gv̂(t)|. (30)\nAlgorithm 1 Estimate W when σ > 0 and n <∞ Input: sample matrix X ∈ Rm×n and 0 < τ 1\n1: estimate number of hidden units d and noise level σ2 2: compute empirical moments µ̂, M̂ and M̂ and plugin moments M̂σ and M̂σ of (17) 3: compute K̂σ such that K̂>σ M̂σK̂σ = Id 4: construct Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ) 5: compute the set Ûσ of eigenpairs of Ŵσ 6: compute the candidate set V̂σ in (20) 7: for each v̂ ∈ V̂σ compute its KS score ∆n(v̂) in (30) 8: select V̄σ ⊆ V̂σ of d vectors with smallest ∆n(v̂) 9: return the pseudo-inverse Ŵ = V̄ †σ\nOur estimator V̄σ ⊆ V̂σ for W † is then the set of d vectors with the smallest scores ∆n(v̂). The estimator for W is the pseudo-inverse, Ŵ = V̄ †σ .\nThe following lemma shows that for sufficiently large n, ∆n(v̂) accurately distinguishes between v̂ ∈ V̂σ that are close to the columns of W † from these that are not.\nLemma 5. Let v∗ ∈ W † and v̂(1), v̂(2), . . . a sequence of random vectors such that ‖v̂(n) − v∗‖ = OP (n\n− 12 ). Then, ∆n(v̂(n)) = oP (1). In contrast, if minv∗∈W † ‖v̂(n) − v∗‖ = ΩP (1), then ∆n(v̂(n)) = ΩP (1), provided the expected rigidity condition (27) holds.\nLemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example Lehmann & Romano (2006); Billingsley (2013).\nAlgorithm 1 summarizes our method for estimating W in the general case where σ > 0 and n < ∞. The following theorem establishes its consistency.\nTheorem 2. Let x1, . . . ,xn be n i.i.d. samples from model (1). If conditions (I)-(II) and the expected rigidity condition (27) hold, then the estimator Ŵ computed by Algorithm 1 is consistent, achieving the parametric rate,\nŴ = W +OP (n − 12 ).\nRuntime The runtime of Algorithm 1 is composed of three main parts. First, O(nm3) operations are needed to compute all the relevant moments from the data and to construct the d× d× d whitened tensor Ŵσ . The most time consuming task is computing the eigenpairs of Ŵσ, which can be done by either the homotopy method or O–NCM. Currently, no runtime guarantees are available for either of these methods. In practice, since there are O(2d) eigenpairs, these methods spend O(2d · poly(d)) operations in total. Finally, since there are O(2d) candidates and each KS test takes O(dn) operations (Gonzalez et al., 1977), the filtering procedure runtime is O(d2dn).\nPower-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact that the set UN of Newton-stable eigenpairs ofWσ is typically O(2d). However, in some cases, the set U∗ of d relevant eigenpairs has additional structure so that a smaller candidate set may be computed instead of UN . Consider the subset UP ⊆ UN of power-stable eigenpairs ofWσ: Definition 3. An eigenpair (u, λ) is power-stable if its projected Jacobian Jp(u) is either positive or negative definite.\nTypically, the number of power-stable eigenpairs is significantly smaller than the number of Newton-stable eigenpairs.2 In addition, UP can be computed by the shifted higher-order power method (Kolda & Mayo, 2011; 2014).\nSimilarly to Lemma 2, one can show that UP is guaranteed to contain U∗ whenever the following stronger version of condition (II) holds: for all (ui, λi) ∈ U∗, the matrix\n(WKLui) >(2C(I, I, ei)− C)(WKLui) (31)\nis either positive-definite or negative-definite.\nAs an example, consider the case where Ph has the support h ∈ Id. Then model (1) corresponds to a GMM with d spherical components with linearly independent means. In this case, bothC and C are diagonal with p on their diagonal. Thus, the matrices in (31) take the form −L>ei diag(p)Lei , which are all negative-definite when p > 0. In fact, in this case, Wσ has an orthogonal CP decomposition and the d orthogonal eigenpairs in U∗ are the only negative-definite power-stable eigenpairs ofWσ (Anandkumar et al., 2014). Similarly, when Ph is a product distribution, the same orthogonal structure appears if the centered moments of x are used instead of M andM. As shown in Anandkumar et al. (2014), the power method, accompanied with a deflation procedure, decomposes an orthogonal tensor in polynomial time, thus implying an efficient algorithm in these cases. However, under the much weaker conditions we pose on Ph, the relevant eigenpairs in U∗ are not necessarily powerstable and the CP decomposition ofWσ does not necessarily include U∗."
  }, {
    "heading": "5. Experiments",
    "text": "We demonstrate our method in three scenarios: (I) simulations from the exact binary model (1); (II) learning a common population genetic admixture model; (III) learning the proportion matrix of a cell mixture from DNA methylation levels. Due to lack of space, (III) is deferred to Appendix N. Code to reproduce the simulation results can be found at https://github.com/arJaffe/ BinaryLatentVariables.\n2We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d."
  }, {
    "heading": "5.1. Simulations",
    "text": "We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and Gaussian noise ξ ∼ N (0, Im). The m columns of W were drawn uniformly from the unit sphere Sd−1. Fixing a mean vector a ∈ Rd and a covariance matrix R ∈ Rd×d, each hidden vector h was generated independently by first drawing r ∼ N (a, R) and then taking its binary rounding.\nFigure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and σ (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.\nAs one can see, as opposed to SHL, our method is consistent for σ > 0 and achieves an error rate O(n− 1 2 ) corresponding to a slope of −1 in the upper panel of Fig. 2. In addition, as seen in the lower panel of Fig. 2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate. Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error\n3Code from https://sites.google.com/site/ slawskimartin/code. For each realization X , we made 50 runs of SHL and chose H,W minimizing ‖X −W>H‖F .\nfor high noise levels. A comparison between the runtime of SHL and the spectral method appears in Appendix I."
  }, {
    "heading": "5.2. Population genetic admixture",
    "text": "We present an application of our method to a fundamental problem in population genetics, known as admixture (see Fig. 3). Admixture refers to the mixing of d ≥ 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008). The observed data X is an m× n matrix where m is the number of modern “admixed” individuals and n is the number of relevant locations in their DNA, known as SNPs. Each SNP corresponds to two alleles and individuals may have different alleles. Fixing a reference allele for each location, Xij takes values in {0, 12 , 1} according to the number of reference alleles appearing in the genotype of individual i ∈ [m] at locus j ∈ [n].\nGiven the genotypes X , an important problem in population genetics is to estimate the following two quantities. The allele frequency matrix H ∈ [0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus j ∈ [n] in ancestral population k ∈ [d]; and the admixture proportion matrix W ∈ [0, 1]d×m whose columns sum to 1 and its entry Wki is the proportion of individual i’s genome that was inherited from population k.\nA common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {0, 1, 2} is the sum of two i.i.d. Bernoulli random variables with success probability Fij = ∑d k=1WkiHkj , namely, Xij |H ∼ 1 2 · Binomial(2, Fij). Note that under this model\nE[X|H] = F = W>H. (32)\nAlthough (32) has similar form to model (1), there are two main differences; the noise is not normally distributed and the matrix H is non-binary. Yet, the binary model (1) serves as a good approximation whenever various alleles are rare in some populations but abundant in others. Specifically, for ancestral populations that have been long separated, some alleles may become fixed in one population (i.e., reach frequency of 1) while being totally absent in others.\nSimulating genetic admixture We followed a standard simulation scheme apllied, for example, in Xue et al. (2017); Gravel (2012); Price et al. (2009). First, using SCRM (Staab et al., 2015), we simulated d = 3 ancestral populations separated for 4000 generations and generated the genomes of 40 individuals for each. H was then computed as the frequency of the reference alleles in each population. Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter α ≥ 0. Finally, the genomes of m = 50 admixed individuals were generated as mosaics of genomic segments of individuals from the ancestral populations with proportions W . The mosaic nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs (Xue et al., 2017). A detailed description is in Appendix M.\nWe compare our algorithm to two methods. The first is Admixture (Alexander et al., 2009), one of the most widely used algorithms in population genetics, which aims to maximize the likelihood of X . The second is the recently proposed spectral method ALStructure (Cabreros & Storey, 2017), where an estimation of span(W>) via Chen & Storey (2015) is followed by constrained ALS iterations of W and H . For our method, two modification are needed for Algorithm 1. First, since the distribution ofXij−wTi hj is not Gaussian, the corrected moments M̂σ,M̂σ as calculated by (17) do not satisfy (18). Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details. In addition, the filtering process described in Section 4 is no longer valid. However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.\nFigure 4 compares the results of the 3 methods for α = 0.1, 1, 10. The spectral method outperforms Admixture and ALStructure for α = 1, 10 and performs similarly to Admixture for α = 0.1.\nAcknowledgements This research was funded in part by NIH Grant 1R01HG008383-01A1."
  }],
  "references": [{
    "title": "Fast modelbased estimation of ancestry in unrelated individuals",
    "authors": ["D. Alexander", "J. Novembre", "K. Lange"],
    "venue": "Genome research,",
    "year": 2009
  }, {
    "title": "A spectral algorithm for latent dirichlet allocation",
    "authors": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "A method of moments for mixture models and hidden markov models",
    "authors": ["A. Anandkumar", "D. Hsu", "S. Kakade"],
    "venue": "In COLT,",
    "year": 2012
  }, {
    "title": "Tensor decompositions for learning latent variable models",
    "authors": ["A. Anandkumar", "R. Ge", "D. Hsu", "S. Kakade", "M. Telgarsky"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Spectral methods for correlated topic models",
    "authors": ["F. Arabshahi", "A. Anandkumar"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Computing a nonnegative matrix factorization–provably",
    "authors": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"],
    "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,",
    "year": 2012
  }, {
    "title": "Overlapping clustering: A review",
    "authors": ["S. Baadel", "F. Thabtah", "J. Lu"],
    "venue": "In SAI Computing Conference (SAI),",
    "year": 2016
  }, {
    "title": "Model-based overlapping clustering",
    "authors": ["A. Banerjee", "C. Krumpelman", "J. Ghosh", "S. Basu", "R. Mooney"],
    "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,",
    "year": 2005
  }, {
    "title": "Multifunctional proteins revealed by overlapping clustering in protein interaction",
    "authors": ["E. Becker", "B. Robisson", "C. Chapple", "A. Guénoche", "C. Brun"],
    "venue": "network. Bioinformatics,",
    "year": 2011
  }, {
    "title": "Convergence of probability measures",
    "authors": ["P. Billingsley"],
    "year": 2013
  }, {
    "title": "A nonparametric estimator of population structure unifying admixture models and principal components analysis",
    "authors": ["I. Cabreros", "J. Storey"],
    "venue": "bioRxiv, pp",
    "year": 2017
  }, {
    "title": "The number of eigenvalues of a tensor",
    "authors": ["D. Cartwright", "B. Sturmfels"],
    "venue": "Linear algebra and its applications,",
    "year": 2013
  }, {
    "title": "Computing tensor eigenvalues via homotopy methods",
    "authors": ["L. Chen", "L. Han", "L. Zhou"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2016
  }, {
    "title": "Consistent estimation of lowdimensional latent structure in high-dimensional data",
    "authors": ["X. Chen", "J. Storey"],
    "venue": "arXiv preprint arXiv:1510.03497,",
    "year": 2015
  }, {
    "title": "Improved learning of GaussianBernoulli restricted Boltzmann machines",
    "authors": ["Cho", "Ilin", "Raiko"],
    "venue": "Artificial Neural Networks and Machine Learning–ICANN",
    "year": 2011
  }, {
    "title": "All real eigenvalues of symmetric tensors",
    "authors": ["C. Cui", "Y. Dai", "J. Nie"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2014
  }, {
    "title": "When does non-negative matrix factorization give a correct decomposition into parts? In Advances in neural information processing",
    "authors": ["D. Donoho", "V. Stodden"],
    "year": 2004
  }, {
    "title": "Learning linear transformations",
    "authors": ["A. Frieze", "M. Jerrum", "R. Kannan"],
    "venue": "In Foundations of Computer Science,",
    "year": 1996
  }, {
    "title": "An efficient algorithm for the Kolmogorov-Smirnov and Lilliefors tests",
    "authors": ["T. Gonzalez", "S. Sahni", "W. Franta"],
    "venue": "ACM Transactions on Mathematical Software (TOMS),",
    "year": 1977
  }, {
    "title": "Population genetics models of local ancestry",
    "authors": ["S. Gravel"],
    "venue": "Genetics, 191(2):607–619,",
    "year": 2012
  }, {
    "title": "A modified Newton iteration for finding nonnegative Z-eigenpairs of a nonnegative tensor",
    "authors": ["C. Guo", "W. Lin", "C. Liu"],
    "venue": "arXiv preprint arXiv:1705.07487,",
    "year": 2017
  }, {
    "title": "Most tensor problems are np-hard",
    "authors": ["C. Hillar", "L. Lim"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2013
  }, {
    "title": "A practical guide to training restricted",
    "authors": ["G. Hinton"],
    "venue": "Boltzmann machines. Momentum,",
    "year": 2010
  }, {
    "title": "Reducing the dimensionality of data with neural networks. science",
    "authors": ["G. Hinton", "R. Salakhutdinov"],
    "year": 2006
  }, {
    "title": "Dna methylation arrays as surrogate measures of cell mixture distribution",
    "authors": ["E. Houseman", "P. Accomando", "D. Koestler", "B. Christensen", "C. Marsit", "H. Nelson", "J. Wiencke", "K. Kelsey"],
    "venue": "BMC bioinformatics,",
    "year": 2012
  }, {
    "title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
    "authors": ["D. Hsu", "S. Kakade"],
    "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
    "year": 2013
  }, {
    "title": "Vector calculus, linear algebra, and differential forms: a unified approach",
    "authors": ["J. Hubbard", "B. Hubbard"],
    "venue": "Matrix Editions,",
    "year": 2015
  }, {
    "title": "Independent component analysis, volume 46",
    "authors": ["A. Hyvärinen", "J. Karhunen", "E. Oja"],
    "year": 2004
  }, {
    "title": "Newton correction methods for computing real eigenpairs of symmetric tensors",
    "authors": ["A. Jaffe", "R. Weiss", "B. Nadler"],
    "venue": "arXiv preprint arXiv:1706.02132,",
    "year": 2017
  }, {
    "title": "Learning mixtures of discrete product distributions using spectral decompositions",
    "authors": ["P. Jain", "S. Oh"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2014
  }, {
    "title": "Shifted power method for computing tensor eigenpairs",
    "authors": ["T. Kolda", "J. Mayo"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2011
  }, {
    "title": "An adaptive shifted power method for computing generalized tensor eigenpairs",
    "authors": ["T. Kolda", "J. Mayo"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2014
  }, {
    "title": "Non-parametric detection of the number of signals: Hypothesis testing and random matrix theory",
    "authors": ["S. Kritchman", "B. Nadler"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2009
  }, {
    "title": "Testing statistical hypotheses",
    "authors": ["E. Lehmann", "J. Romano"],
    "venue": "Springer Science & Business Media,",
    "year": 2006
  }, {
    "title": "Worldwide human relationships inferred from genome-wide patterns of variation",
    "authors": ["J. Li", "D. Absher", "H. Tang", "A. Southwick", "A. Casto", "S. Ramachandran", "H. Cann", "G. Barsh", "M. Feldman", "L Cavalli-Sforza"],
    "year": 2008
  }, {
    "title": "Singular values and eigenvalues of tensors: a variational approach",
    "authors": ["L. Lim"],
    "venue": "In Computational Advances in MultiSensor Adaptive Processing,",
    "year": 2005
  }, {
    "title": "Gaussian-binary restricted Boltzmann machines for modeling natural image statistics",
    "authors": ["J. Melchior", "N. Wang", "L. Wiskott"],
    "venue": "PloS one,",
    "year": 2017
  }, {
    "title": "Mdl4bmf: Minimum description length for boolean matrix factorization",
    "authors": ["P. Miettinen", "J. Vreeken"],
    "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
    "year": 2014
  }, {
    "title": "Settling the polynomial learnability of mixtures of gaussians",
    "authors": ["A. Moitra", "G. Valiant"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2010
  }, {
    "title": "Learning nonsingular phylogenies and hidden markov models",
    "authors": ["E. Mossel", "S. Roch"],
    "venue": "In Proceedings of the thirtyseventh annual ACM symposium on Theory of computing,",
    "year": 2005
  }, {
    "title": "Sensitive detection of chromosomal segments of distinct ancestry in admixed populations",
    "authors": ["A. Price", "A. Tandon", "N. Patterson", "K. Barnes", "N. Rafaels", "I. Ruczinski", "T. Beaty", "R. Mathias", "D. Reich", "S. Myers"],
    "venue": "PLoS genetics,",
    "year": 2009
  }, {
    "title": "Inference of population structure using multilocus genotype",
    "authors": ["J. Pritchard", "M. Stephens", "P. Donnelly"],
    "venue": "data. Genetics,",
    "year": 2000
  }, {
    "title": "Eigenvalues of a real supersymmetric tensor",
    "authors": ["L. Qi"],
    "venue": "Journal of Symbolic Computation,",
    "year": 2005
  }, {
    "title": "Monotonic convergence of fixedpoint algorithms for ica",
    "authors": ["P. Regalia", "E. Kofidis"],
    "venue": "IEEE Transactions on Neural Networks,",
    "year": 2003
  }, {
    "title": "Decomposing gene expression into cellular processes",
    "authors": ["E. Segal", "A. Battle", "D. Koller"],
    "venue": "In Proceedings of the Pacific Symposium on Biocomputing,",
    "year": 2002
  }, {
    "title": "Super-exponential methods for blind deconvolution",
    "authors": ["O. Shalvi", "E. Weinstein"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1993
  }, {
    "title": "Matrix factorization with binary components",
    "authors": ["M. Slawski", "M. Hein", "P. Lutsik"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "scrm: efficiently simulating long sequences using the approximated coalescent with recombination",
    "authors": ["Staab", "Zhu", "Metzler", "Lunter"],
    "year": 2015
  }, {
    "title": "Modeling human motion using binary latent variables",
    "authors": ["G. Taylor", "G. Hinton", "S. Roweis"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2007
  }, {
    "title": "Analytical method for blind binary signal separation",
    "authors": ["Van der Veen", "A.-J"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 1997
  }, {
    "title": "An analysis of gaussian-binary restricted Boltzmann machines for natural images",
    "authors": ["N. Wang", "J. Melchior", "L. Wiskott"],
    "venue": "In ESANN,",
    "year": 2012
  }, {
    "title": "The time and place of european admixture in ashkenazi jewish history",
    "authors": ["J. Xue", "T. Lencz", "A. Darvasi", "I. Peer", "S. Carmi"],
    "venue": "PLoS genetics,",
    "year": 2017
  }],
  "id": "SP:6d4426da76fd2fff1a855f491231f446c46bc78a",
  "authors": [{
    "name": "Ariel Jaffe",
    "affiliations": []
  }, {
    "name": "Roi Weiss",
    "affiliations": []
  }, {
    "name": "Shai Carmi",
    "affiliations": []
  }, {
    "name": "Yuval Kluger",
    "affiliations": []
  }, {
    "name": "Boaz Nadler",
    "affiliations": []
  }],
  "abstractText": "Latent variable models with hidden binary units appear in various applications. Learning such models, in particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data. We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate. Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive. We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.",
  "title": "Learning Binary Latent Variable Models: A Tensor Eigenpair Approach"
}