{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1057–1068 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Social power is a difficult concept to define, but is often manifested in how we interact with one another. Understanding these manifestations is important not only to answer fundamental questions in social sciences about power and social interactions, but also to build computational models that can automatically infer social power structures from interactions. The availability and access to large digital repositories of naturally occurring social interactions and the advancements in natural language processing techniques in recent years have enabled researchers to perform large scale studies on linguistic correlates of power, such as words and phrases (Bramsen et al., 2011; Gilbert, 2012), linguistic coordination (DanescuNiculescu-Mizil et al., 2012), agenda control (Tay-\nlor et al., 2012), and dialog structure (Prabhakaran and Rambow, 2014).\nAnother area of research that has recently garnered interest within the NLP community is the modeling of author commitment in text. Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010). More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saurı́ and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015). These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged.\nIn this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions. We use the power analysis framework built by Prabhakaran and Rambow (2014) to perform this study, and measure author commitment using the committed belief tagging framework introduced by (Diab et al., 2009) that distinguishes different types of beliefs expressed in text. Our contributions are two-fold — statistical analysis of author commitment in relation with power, and enrichment of lexical features with commitment labels to aid in computational prediction of power relations. In the first part, we find that au-\n1The BeSt track of the 2017 TAC-KBP evaluation aimed at detecting the “belief and sentiment of an entity toward another entity, relation, or event” (http://www.cs. columbia.edu/˜rambow/best-eval-2017/).\n1057\nthor commitment is significantly correlated with the social power relations between their participants — subordinates use more instances of noncommitment, a finding that is in line with sociolinguistics studies in this area. We also find that subordinates use significantly more reported beliefs (i.e., attributing beliefs to other agents) than superiors. This is a new finding; to our knowledge, there has not been any sociolinguistics studies investigating this aspect of interaction in relation with power. In the second part, we present novel ways of incorporating the author commitment information into lexical features that can capture important distinctions in word meanings conveyed through the belief contexts in which they occur; distinctions that are lost in a model that conflates all occurrences of a word into one unit.\nWe first describe the related work in computational power analysis and computational modeling of cognitive states in Section 2. In Section 3, we describe the power analysis framework we use. Section 4 formally defines the research questions we are investigating, and describes how we obtain the belief information. In Section 5, we present the statistical analysis of author commitment and power. Section 6 presents the utility of enriching lexical features with belief labels in the context of automatic power prediction. Section 7 concludes the paper and summarizes the results."
  }, {
    "heading": "2 Related Work",
    "text": "The notion of belief that we use in this paper (Diab et al., 2009; Prabhakaran et al., 2015) is closely related to the notion of factuality that is captured in FactBank (Saurı́ and Pustejovsky, 2009). They capture three levels of factuality, certain (CT), probable (PB), and possible (PS), as well as the underspecified factuality (Uu). They also record the corresponding polarity values, and the source of the factuality assertions to distinguish between factuality assertions by the author and those by the agents/sources introduced by the author. While FactBank offers a finer granularity, they are annotated on newswire text. Hence, we use the corpus of belief annotations (Prabhakaran et al., 2015) that is obtained on online discussion forums, which is closer to our genre.\nAutomatic hedge/uncertainty detection is a very closely related task to belief detection. The belief tagging framework we use aims to capture the cognitive states of authors, whereas hedges are lin-\nguistic expressions that convey one of those cognitive states — non-committed beliefs. Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text.\nSociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies such as hedging and hesitations in order to adopt an unassertive communication style, which she terms “women’s language”. While many studies have found evidence to support Lakoff’s theory (e.g., (Crosby and Nyquist, 1977; Preisler, 1986; Carli, 1990)), there have also been contradictory findings (e.g., (O’Barr and Atkins, 1980)) that link the difference in the use of hedges to other social factors (e.g., power). O’Barr and Atkins (1980) argue that the use of hedges is linked more to the social positions rather than gender, suggesting to rename “women’s language” to “powerless language”. In later work, O’Barr (1982) formalized the notion of powerless language, which formed the basis of many sociolinguistics studies on social power and communication. O’Barr (1982) analyzed courtroom interactions and identified hedges and hesitations as some of the linguistic markers of “powerless” speech. However, there has not been any computational work which has looked into how power relations relate to the level of commitment expressed in text. In this paper, we use com-\nputational power analysis to perform a large scale data-oriented study on how author commitment in text reveals the underlying power relations.\nThere is a large body of literature in the social sciences that studies power as a social construct (e.g., (French and Raven, 1959; Dahl, 1957; Emerson, 1962; Pfeffer, 1981; Wartenberg, 1990)) and how it relates to the ways people use language in social situations (e.g., (Bales et al., 1951; Bales, 1970; O’Barr, 1982; Van Dijk, 1989; Bourdieu and Thompson, 1991; Ng and Bradac, 1993; Fairclough, 2001; Locher, 2004)). Recent years have seen growing interest in computationally analyzing and detecting power and influence from interactions. Early work in computational power analysis used social network analysis based approaches (Diesner and Carley, 2005; Shetty and Adibi, 2005; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007). Using NLP to deduce social relations from online communication is a relatively new area of active research.\nBramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features. More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013). In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language.\nThere has also been work on analyzing power in other genres of interactions. Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages. Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlated in Wikipedia interactions as well as Supreme Court hearings. Bracewell et al. (2012) and Swayamdipta and Rambow (2012) try to identify pursuit of power in discussion forums. Biran et al. (2012) and Rosenthal (2014) study the problem of predicting influence in Wikipedia talk pages, blogs, and other online forums. Prabhakaran et al. (2013) study manifestations of power of confidence in presidential debates."
  }, {
    "heading": "3 Power in Workplace Email: Data and Analysis Framework",
    "text": "The focus of our study is to investigate whether the level of commitment participants express in their contributions in an interaction is related to the power relations they have with other participants, and how it can help in the problem of predicting social power. In this section, we introduce the power analysis framework as well as the data we use in this study."
  }, {
    "heading": "3.1 Problem",
    "text": "In order to model manifestations of power relations in interactions, we use our interaction analysis framework from (Prabhakaran and Rambow, 2014), where we introduced the problem of predicting organizational power relations between pairs of participants based on single email threads. The problem is formally defined as follows: given an email thread t , and a related interacting participant pair (p1 , p2 ) in the thread, predict whether p1 is the superior or subordinate of p2 . In this formulation, a related interacting participant pair (RIPP) is a pair of participants of the thread such that there is at least one message exchanged within the thread between them (in either direction) and that they are hierarchically related with a superior/subordinate relation."
  }, {
    "heading": "3.2 Data",
    "text": "We use the same dataset we used in (Prabhakaran and Rambow, 2014), which is a version of the Enron email corpus in which the thread structure of email messages is reconstructed (Yeh and Harnly, 2006), and enriched by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts. The corpus captures dominance relations between 13,724 pairs of Enron employees. As in (Prabhakaran and Rambow, 2014), we use these dominance relation tuples to obtain gold labels for the superior or subordinate relationships between pairs of participants. We use the same train-test-dev split as in (Prabhakaran and Rambow, 2014). We summarize the number of threads and related interacting participant pairs in each subset of the data in Table 1."
  }, {
    "heading": "4 Research Hypotheses",
    "text": "Our first objective in this paper is to perform a large scale computational analysis of author com-\nmitment and power relations. Specifically, we want to investigate whether the commitment authors express towards their contributions in organizational interactions is correlated with the power relations they have with other participants. Sociolinguistics studies have found some evidence to suggest that lack of commitment expressed through hedges and hesitations is associated with lower power status (O’Barr, 1982). However, in our study, we go beyond hedge word lists, and analyze different cognitive belief states expressed by authors using a belief tagging framework that takes into account the syntactic contexts within which propositions are expressed."
  }, {
    "heading": "4.1 Obtaining Belief Labels",
    "text": "We use the committed belief analysis framework introduced by (Diab et al., 2009; Prabhakaran et al., 2015) to model different levels of beliefs expressed in text. Specifically, in this paper, we use the 4-way belief distinction — COMMITTEDBELIEF, NONCOMMITTEDBELIEF, REPORTEDBELIEF, and NONAPPLICABLE— introduced in (Prabhakaran et al., 2015).2 (Prabhakaran et al., 2015) presented a corpus of online discussion forums with over 850K words, annotating each propositional head in text with one of the four belief labels. The paper also presented an automatic belief tagger trained on this data, which we use to obtain belief labels in our data. We describe each belief label and our associated hypotheses below.\nCommitted belief (CB): the writer strongly believes that the proposition is true, and wants the reader/hearer to believe that. E.g.:\n(1) a. John will submit the report. b. I know that John is capable.\n2We also performed analysis and experiments using an earlier 3-way belief distinction proposed by (Diab et al., 2009), which also yielded similar findings. We do not report the details of those analyses in this paper.\nAs discussed earlier, lack of commitment in one’s writing/speech is identified as markers of powerless language. We thus hypothesize:\nH. 1. Superiors use more instances of committed belief in their messages than subordinates.\nNon-committed belief (NCB): the writer explicitly identifies the proposition as something which he or she could believe, but he or she happens not to have a strong belief in, for example by using an epistemic modal auxiliary. E.g.:\n(2) a. John may submit the report. b. I guess John is capable.\nThis class captures a more semantic notion of non-commitment than hedges, since the belief annotation attempts to model the underlying meaning rather than language uses, and hence captures other linguistic means of expressing noncommittedness. Following (O’Barr, 1982), we formulate the below hypothesis:\nH. 2. Subordinates use more instances of non committed belief in their messages than superiors.\nReported belief (ROB): the writer attributes belief (either committed or non-committed) to another person or group. E.g.:\n(3) a. Sara says John will submit the report. b. Sara thinks John may be capable.\nNote that this label is only applied when the writer’s own belief in the proposition is unclear. For instance, if the first example above was Sara knows John will submit the report on-time, the writer is expressing commitment toward the proposition that John will submit the report and it will be labeled as committed belief rather than reported belief. Reported belief captures instances where the writer is in effect limiting his/her commitment towards what is stated by attributing the belief to someone else. So, in line with our hypotheses for non-committed beliefs, we formulate the following hypothesis:\nH. 3. Subordinates use more instances of reported beliefs in their messages than superiors.\nNon-belief propositions (NA): – the writer expresses some other cognitive attitude toward the proposition, such as desire or intention (4a), or expressly states that he/she has no belief about the proposition (e.g., asking a question (4b)). E.g.:\n(4) a. I need John to submit the report. b. Will John be capable?\nAs per the above definition, requests for information (i.e., questions) and requests for actions are cases where the author is not expressing a belief about the proposition, but rather expressing the desire that some action be done. In the study correlating power with dialog act tags (Prabhakaran and Rambow, 2014), we found that superiors issue significantly more requests than subordinates. Hence, we expect the superiors to have significantly more non belief expressions in their messages, and formulate the following hypothesis:\nH. 4. Superiors use more instances of non beliefs in their messages than subordinates."
  }, {
    "heading": "4.2 Testing Belief Tagger Bias",
    "text": "NLP tools are imperfect and may produce errors, which poses a problem when using any NLP tool for sociolinguistic analysis. More than the magnitude of error, we believe that whether the error is correlated with the social variable of interest (i.e., power) is more important; e.g., is the belieftagger more likely to find ROB false-positives in subordinates text? To test whether this is the case, we performed manual belief annotation on around 500 propositional heads in our corpus. Logistic regression test revealed that the belief-tagger is equally likely to make errors (both false-positives and false-negatives, for all four belief-labels) in sentences written by subordinates as superiors (the null hypothesis accepted at p > 0.05 for all eight tests)."
  }, {
    "heading": "5 Statistical Analysis",
    "text": "Now that we have set up the analysis framework and research hypotheses, we present the statistical analysis of how superiors and subordinates differ in their relative use of expressions of commitment."
  }, {
    "heading": "5.1 Features",
    "text": "For each participant of each pair of related interacting participants in our corpus, we aggregate each of the four belief tags:\n• CBCount: number of propositional heads tagged as Committed Belief (CB) • NCBCount: number of propositional heads tagged as Non Committed Belief (NCB) • ROBCount: number of propositional heads tagged as Reported Belief (ROB)\n• NACount: number of propositional heads tagged as Non Belief (NA)"
  }, {
    "heading": "5.2 Hypotheses Testing",
    "text": "Our general hypothesis is that power relations do correlate with the level of commitment people express in their messages; i.e., at least one of H.1 - H.4 is true. In this analysis, each participant of the pair (p1 , p2 ) is a data instance. We exclude the instances for which a feature value is undefined.3\nIn order to test whether superiors and subordinates use different types of beliefs, we used a linear regression based analysis. For each feature, we built a linear regression model predicting the feature value using power (i.e., superior vs. subordinate) as the independent variable. Since verbosity of a participant can be highly correlated with each of these feature values (we found it to be highly correlated with subordinates (Prabhakaran and Rambow, 2014)), we added token count as a control variable to the linear regression.\nOur linear regression test revealed significant differences in NCB (b=-.095, t(-8.09), p<.001), ROB (b=-.083, t(-7.162), p<.001) and NA (b=.125, t(4.351), p<.001), and no significant difference in CB (b=.007, t(0.227), p=0.821). Figure 1 pictorially demonstrates these results by plotting the difference between the mean values of each commitment feature (here normalized by token count) of superiors vs. subordinates, as a percentage of mean feature value of the corresponding commitment feature for superiors. Dark bars denote statistically significant differences."
  }, {
    "heading": "5.3 Interpretation of Findings",
    "text": "The results from our statistical analysis validate our original hypothesis that power relations do correlate with the level of commitment people express in their messages. This finding remains statistically significant (p < 0.001) even after applying the Bonferroni correction for multiple testing.\nThe results on NCB confirm our hypothesis that subordinates use more non-committedness in their language. Subordinates’ messages contain 48% more instances of non-committed belief than superiors’ messages, even after normalizing for the length of messages. This is in line with prior sociolinguistics literature suggesting that people with\n3These are instances corresponding to participants who did not send any messages in the thread (some of the pairs in the set of related interacting participant pairs only had oneway communication) or whose messages were empty (e.g., forwarding messages).\n0.05. (RD = (Mean(Subordinates)−Mean(Superiors))∗100Mean(Superiors) ).\nless power tend to use less commitment, previously measured in terms of hedges. However, in our work, we go beyond hedge dictionaries and use expressions of non-committedness that takes into account the syntactic configurations in which the words appear.\nAnother important finding is in terms of reported belief (ROB). Our results strongly verify the hypothesis H.3 that subordinates use significantly more reported beliefs than superiors. In fact, it obtained the largest magnitude of relative difference (65.3% more) of all features we analyzed. To our knowledge, ours is the first study that analyzed the manifestation of power in authors attributing beliefs to others. Our results are in line with the finding in (Agarwal et al., 2014) that “if many more people get mentioned to a person then that person is the boss”, because as subordinates report other people’s beliefs to superiors, they are also likely to mention them.\nThe finding that superiors use more NAs confirms our hypothesis H.4. As discussed earlier, this is expected since superiors issue more requests (as found by (Prabhakaran and Rambow, 2014)), the propositional heads of which would be tagged as NA by the belief tagger. However, our hypothesis H.1 is proven false. Being a superior or subordinate does not affect how often their messages contain CB, which suggests that power differences are manifested only in terms of lack of commitment."
  }, {
    "heading": "6 Commitment in Power Prediction",
    "text": "Our next step is to explore whether we can utilize the hedge and belief labels to improve the performance of an automatic power prediction system. For this purpose, we use our POWERPRE-\nDICTOR system (Prabhakaran and Rambow, 2014) that predicts the direction of power between a pair of related interacting participants in an email thread. It uses a variety of linguistic and dialog structural features consisting of verbosity features (message count, message ratio, token count, token ratio, and tokens per message), positional features (initiator, first message position, last message position), thread structure features (number of all recipients and those in the To and CC fields of the email, reply rate, binary features denoting the adding and removing of other participants), dialog act features (request for action, request for information, providing information, and conventional), and overt displays of power, and lexical features (lemma ngrams, part-of-speech ngrams, and mixed ngrams, a version of lemma ngrams with open class words replaced with their part-ofspeech tags). The feature sets are summarized in Table 2 ((Prabhakaran and Rambow, 2014) has a detailed description of these features).\nNone of the features used in POWERPREDICTOR use information from the parse trees of sentences in the text However, in order to accurately obtain the belief labels, deep dependency parse based features are critical (Prabhakaran et al., 2010). We use the ClearTk wrapper for the Stanford CoreNLP pipeline to obtain the dependency parses of sentences in the email text. To ensure an unified analysis framework, we also use the Stanford CoreNLP for tokenization, part-ofspeech tagging, and lemmatization steps, instead of OpenNLP. This change affects our analysis in two ways. First, the source of part-of-speech tags and word lemmas is different from what was presented in the original system, which might affect the performance of the dialog act tagger and overt display of power tagger (DIA and ODP features). Second, we had to exclude 117 threads (0.3%) from the corpus for which the Stanford CoreNLP failed to parse some sentences, resulting in the removal of 11 data points (0.2%), only one of which\nwas in the test set. On randomly checking, we found that they contained non-parsable text such as dumps of large tables, system logs, or unedited dumps of large legal documents.\nIn order to better interpret how the commitment features help in power prediction, we use a linear kernel SVM in our experiments. Linear kernel SVMs are significantly faster than higher order SVMs, and our preliminary experiments revealed the performance gain by using a higher order SVM to be only marginal. We use the best performing feature set from (Prabhakaran and Rambow, 2014) as a strong baseline for our experiments. This baseline feature set is the combination of thread structure features (THR) and lexical features (LEX). This baseline system obtained an accuracy of 68.8% in the development set."
  }, {
    "heading": "6.1 Belief Label Enriched Lexical Features",
    "text": "Adding the belief label counts into the SVM directly as features will not yield much performance improvements, as signal in the aggregate counts would be minimal given the effect sizes of differences we find in Section 5. In this section, we investigate a more sophisticated way of incorporating the belief tags into the power prediction framework. Lexical features are very useful for the task of power prediction. However, it is often hard to capture deeper syntactic/semantic contexts of words and phrases using ngram features. We hypothesize that incorporating belief tags into the ngrams will enrich the representation and will help disambiguate different usages of same words/phrases. For example, let us consider two sentences: I need the report by tomorrow vs. If I need the report, I will let you know. The former is likely coming from a person who has power, whereas the latter does not give any such indication. Applying the belief tagger to these two sentences will result in I need(CB) the report ... and If I need(NA) the report .... Capturing the difference between need(CB) vs. need(NA) will help the machine learning system to make the distinction between these two usages and in turn improve the power prediction performance.\nIn building the ngram features, whenever we encounter a token that is assigned a belief tag, we append the belief tag to the corresponding lemma or part-of-speech tag in the ngram. We call it the Append version of corresponding ngram feature. We summarize the different versions of each type of\nngram features below: • LN: the original word lemma ngram; e.g.,\ni need the. • LNCBApnd: word lemma ngram with appended\nbelief tags; e.g., i need(CB) the. • PN: the original part-of-speech ngram; e.g.,\nPRP VB DT. • PNCBApnd: part-of-speech ngram with ap-\npended belief tags; e.g., PRP VB(CB) DT. • MN: the original mixed ngram; e.g., i VB the. • MNCBApnd: mixed ngram with appended belief\ntags; e.g., i VB(CB) the. In Table 3, we show the results obtained by incorporating the belief tags in this manner to the LEXICAL features of the original baseline feature set. The first row indicates the baseline results and the following rows show the impact of incorporating belief tags using the Append method. While the Append version of both lemma ngrams and mixed ngrams improved the results, the Append version of part of speech ngrams reduced the results. The combination of best performing version of each type of ngram obtained slightly lower result than using the Append version of word ngram alone, which posted the overall best performance of 69.3%, a significant improvement (p<0.05) over not using any belief information. We use the approximate randomization test (Yeh, 2000) for testing statistical significance of the improvement.\nFinally, we verified that our best performing feature sets obtain similar improvements in the unseen test set. The baseline system obtained 70.2% accuracy in the test set. The best performing configuration from Table 3 significantly improved this accuracy to 70.8%. The second best performing configuration of using the Append version of both word and mixed ngrams obtained only a small improvement upon the baseline in the test set."
  }, {
    "heading": "6.2 Word NGram Feature Analysis",
    "text": "We inspect the feature weights assigned to the LNCBApnd version of lemma ngrams in our best performing model. Each lemma ngram that contains a propositional head (e.g., need) has four possible LNCBApnd ngram versions: need(CB), need(NCB), need(ROB), and need(NA). For each lemma ngram, we calculate the standard deviation of weights assigned to different LNCBApnd versions in the learned model as a measure of variation captured by incorporating belief tags into that ngram.4\nFigure 2 shows the feature weights of different LNCBApnd versions of twenty five propositional heads whose lemma unigrams had the highest standard deviation. The y-axis lists propositional heads arranged in the decreasing order of standard deviation from bottom to top, while the x-axis denotes the feature weights. The markers distinguish the different LNCBApnd versions of each propositional head — square denotes COMMITTEDBE-\n4Not all lemma ngrams have all four versions; we calculated standard deviation using the versions present.\nLIEF, circle denotes NONCOMMITTEDBELIEF, triangle denotes REPORTEDBELIEF, and diamond denotes NONAPPLICABLE. The feature versions with negative weights are associated more with subordinates’ messages, whereas those with positive weights are associated more with superiors’ messages. Since NCB and ROB versions are rare, they rarely get high weights in the model.\nWe find that by incorporating belief labels into lexical features, we capture important distinctions in social meanings expressed through words that are lost in the regular lemma ngram formulation. For example, propositional heads such as know, need, hold, mean and want are indicators of power when they occur in CB contexts (e.g., i need ...), whereas their usages in NA contexts (e.g., do you need?, if i need..., etc.) are indicators of lack of power. In contrast, the CB version of attend, let, plan, could, check, discuss, and feel (e.g., i will attend/check/plan ...) are strongly associated with lack of power, while their NA versions (e.g., can you attend/check/plan?) are indicators of power."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we made two major contributions. First, we presented a large-scale data oriented analysis of how social power relations between participants of an interaction correlate with different types of author commitment in terms of their relative usage of hedges and different levels of beliefs — committed belief, non-committed belief, reported belief, and non-belief. We found evidence that subordinates use significantly more propositional hedges than superiors, and that superiors and subordinates use significantly different proportions of different types of beliefs in their messages. In particular, subordinates use significantly more non-committed beliefs than superiors. They also report others’ beliefs more often than superiors. Second, we investigated different ways of incorporating the belief tag information into the machine learning system that automatically detects the direction of power between pairs of participants in an interaction. We devised a sophisticated way of incorporating this information into the machine learning framework by appending the heads of propositions in lexical features with corresponding belief tags, demonstrating its utility in distinguishing social meanings expressed through the different belief contexts.\nThis study is based on emails from a single corporation, at the beginning of the 21st century. Our findings on the correlation between author commitment and power may be reflective of the work culture that prevailed in that organization at the time when the emails were exchanged. It is important to replicate this study on emails from multiple organizations in order to assess whether these results generalize across board. It is likely that behavior patterns are affected by factors such as ethnic culture (Cox et al., 1991) of the organization, and the kinds of conversations interactants engage in (for instance, co-operative vs. competitive behavior (Hill et al., 1992)). We intend to explore this line of inquiry in future work."
  }, {
    "heading": "Acknowledgments",
    "text": "This paper is partially based upon work supported by the DARPA DEFT program under a grant to Columbia University; all three co-authors were at Columbia University when portions of this work were performed. The views expressed here are those of the author(s) and do not reflect the official policy or position of the Department of De-\nfense or the U.S. Government. We thank Dan Jurafsky and the anonymous reviewers for their helpful feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "A comprehensive gold standard for the Enron organizational hierarchy",
    "authors": ["Apoorv Agarwal", "Adinoyi Omuya", "Aaron Harnly", "Owen Rambow."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
    "year": 2012
  }, {
    "title": "Enron corporation: You’re the boss if people get mentioned to you",
    "authors": ["Apoorv Agarwal", "Adinoyi Omuya", "Jingwei Zhang", "Owen Rambow."],
    "venue": "Proceedings of the 2014 International Conference on Social Computing. ACM, New York, NY, USA, Social-",
    "year": 2014
  }, {
    "title": "Detecting hedge cues and their scope in biomedical text with conditional random fields",
    "authors": ["Shashank Agarwal", "Hong Yu."],
    "venue": "Journal of biomedical informatics 43(6):953–961.",
    "year": 2010
  }, {
    "title": "Personality and interpersonal behavior",
    "authors": ["Robert F. Bales."],
    "venue": "Holt, Rinehart, and Winston (New York).",
    "year": 1970
  }, {
    "title": "Channels of communication in small groups",
    "authors": ["Robert F. Bales", "Fred L. Strodtbeck", "Theodore M. Mills", "Mary E. Roseborough."],
    "venue": "American Sociological Review pages 16(4), 461–468.",
    "year": 1951
  }, {
    "title": "Detecting influencers in written online conversations",
    "authors": ["Or Biran", "Sara Rosenthal", "Jacob Andreas", "Kathleen McKeown", "Owen Rambow."],
    "venue": "Proceedings of the Second Workshop on Language in Social Media. Association for Compu-",
    "year": 2012
  }, {
    "title": "Language and symbolic power",
    "authors": ["Pierre Bourdieu", "John B. Thompson."],
    "venue": "Harvard University Press.",
    "year": 1991
  }, {
    "title": "A motif approach for identifying pursuits of power in social discourse",
    "authors": ["David B. Bracewell", "Marc Tomlinson", "Hui Wang."],
    "venue": "ICSC. IEEE Computer Society, pages 1–8.",
    "year": 2012
  }, {
    "title": "Extracting Social Power Relationships from Natural Language",
    "authors": ["Philip Bramsen", "Martha Escobar-Molano", "Ami Patel", "Rafael Alonso."],
    "venue": "ACL. The Association for Computational Linguistics, pages 773–782.",
    "year": 2011
  }, {
    "title": "Gender, language, and influence",
    "authors": ["Linda L. Carli."],
    "venue": "Journal of Personality and Social Psychology 59(5):941.",
    "year": 1990
  }, {
    "title": "The concept of power",
    "authors": ["Robert A. Dahl"],
    "year": 1957
  }, {
    "title": "Using hedges to classify",
    "authors": ["Robert E. Mercer"],
    "year": 2006
  }, {
    "title": "Language and power",
    "authors": ["Norman Fairclough."],
    "venue": "Pearson Education.",
    "year": 2001
  }, {
    "title": "The conll-2010 shared task: Learning to detect hedges and their scope in natural language text",
    "authors": ["Richárd Farkas", "Veronika Vincze", "György Móra", "János Csirik", "György Szarvas."],
    "venue": "Proceedings of the Fourteenth Conference on Computational Nat-",
    "year": 2010
  }, {
    "title": "The Bases of Social Power",
    "authors": ["John R. French", "Bertram Raven."],
    "venue": "Dorwin Cartwright, editor, Studies in Social Power, University of Michigan Press, pages 150–167+.",
    "year": 1959
  }, {
    "title": "Phrases that Signal Workplace Hierarchy",
    "authors": ["Eric Gilbert."],
    "venue": "Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work. ACM, New York, NY, USA, CSCW ’12, pages 1037–1046.",
    "year": 2012
  }, {
    "title": "Cooperative versus competitive structures in related and unrelated diversified firms",
    "authors": ["Charles W.L. Hill", "Michael A. Hitt", "Robert E. Hoskisson."],
    "venue": "Organization Science 3(4):501–521.",
    "year": 1992
  }, {
    "title": "Hedging in scientific research articles, volume 54",
    "authors": ["Ken Hyland."],
    "venue": "John Benjamins Publishing.",
    "year": 1998
  }, {
    "title": "Recognizing speculative language in biomedical research articles: a linguistically motivated perspective",
    "authors": ["Halil Kilicoglu", "Sabine Bergler."],
    "venue": "BMC bioinformatics 9(Suppl 11):S10.",
    "year": 2008
  }, {
    "title": "Language and Woman’s Place",
    "authors": ["Robin Lakoff."],
    "venue": "Language in society 2(01):45–79.",
    "year": 1973
  }, {
    "title": "Power and politeness in action: disagreements in oral communication",
    "authors": ["Miriam A. Locher."],
    "venue": "Language, power, and social process. M. de Gruyter. http://books.google.com/ books?id=Aa32A4gWb8sC.",
    "year": 2004
  }, {
    "title": "The frequency of hedging cues in citation contexts in scientific writing",
    "authors": ["Robert E. Mercer", "Chrysanne Di Marco", "Frederick W. Kroon."],
    "venue": "Advances in artificial intelligence, Springer, pages 75–88.",
    "year": 2004
  }, {
    "title": "Memory-based resolution of insentence scopes of hedge cues",
    "authors": ["Roser Morante", "Vincent Van Asch", "Walter Daelemans."],
    "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning—Shared Task. Association",
    "year": 2010
  }, {
    "title": "Inferring organizational titles in online communication",
    "authors": ["Jr. Galileo Mark S. Namata", "Lise Getoor", "Christopher P. Diehl."],
    "venue": "Proceedings of the 2006 conference on Statistical network analysis. SpringerVerlag, Berlin, Heidelberg, ICML’06, pages 179–",
    "year": 2007
  }, {
    "title": "Power in language: Verbal communication and social influence",
    "authors": ["Sik Hung Ng", "James J. Bradac"],
    "year": 1993
  }, {
    "title": "Linguistic evidence: language, power, and strategy in the courtroom. Studies on law and social control",
    "authors": ["M. William"],
    "venue": "O’Barr",
    "year": 1982
  }, {
    "title": "women’s language” or ”powerless language”? Women and Language in Literature and Society",
    "authors": ["William M. O’Barr", "Bowman K. Atkins"],
    "year": 1980
  }, {
    "title": "Power in organizations",
    "authors": ["Jeffrey Pfeffer."],
    "venue": "Pitman, Marshfield, MA.",
    "year": 1981
  }, {
    "title": "A New Dataset and Evaluation for Belief/Factuality",
    "authors": ["Strassel", "Gregory Werner", "Janyce Wiebe", "Yorick Wilks."],
    "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015). Association for Computational",
    "year": 2015
  }, {
    "title": "Who Had the Upper Hand? Ranking Participants of Interactions Based on Their Relative Power",
    "authors": ["Vinodkumar Prabhakaran", "Ajita John", "Dorée D. Seligmann."],
    "venue": "Proceedings of the IJCNLP. Asian Federation of Natural Language Processing,",
    "year": 2013
  }, {
    "title": "Written Dialog and Social Power: Manifestations of Different Types of Power in Dialog Behavior",
    "authors": ["Vinodkumar Prabhakaran", "Owen Rambow."],
    "venue": "Proceedings of the IJCNLP. Asian Federation of Natural Language Processing, Nagoya, Japan,",
    "year": 2013
  }, {
    "title": "Predicting power relations between participants in written dialog from a single thread",
    "authors": ["Vinodkumar Prabhakaran", "Owen Rambow."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
    "year": 2014
  }, {
    "title": "Automatic Committed Belief Tagging",
    "authors": ["Vinodkumar Prabhakaran", "Owen Rambow", "Mona Diab."],
    "venue": "Coling 2010: Posters. Coling 2010 Organizing Committee, Beijing, China, pages 1014–1022. http://www.aclweb.org/anthology/",
    "year": 2010
  }, {
    "title": "Who’s (Really) the Boss? Perception of Situational Power in Written Interactions",
    "authors": ["Vinodkumar Prabhakaran", "Owen Rambow", "Mona Diab."],
    "venue": "24th International Conference on Computational",
    "year": 2012
  }, {
    "title": "Linguistic sex roles in conversation",
    "authors": ["Bent Preisler."],
    "venue": "Berlin: Mouton de Gruyter .",
    "year": 1986
  }, {
    "title": "Hedging and speaker commitment",
    "authors": ["Anna Prokofieva", "Julia Hirschberg."],
    "venue": "5th International Workshop on Emotion, Social Signals, Sentiment and Linked Open Data. LREC.",
    "year": 2014
  }, {
    "title": "Detecting Influencers in Social Media Discussions",
    "authors": ["Sara Rosenthal."],
    "venue": "XRDS: Crossroads, The ACM Magazine for Students 21(1):40–45.",
    "year": 2014
  }, {
    "title": "A hedging annotation scheme focused on epistemic phrases for informal language",
    "authors": ["Liliana Mamani Sanchez", "Carl Vogel."],
    "venue": "Proceedings of the IWCS Workshop on Models for Modality Annotation. Association for Computational Linguistics,",
    "year": 2015
  }, {
    "title": "FactBank: a corpus annotated with event factuality. Language Resources and Evaluation 43:227–268",
    "authors": ["Roser Saurı", "James Pustejovsky"],
    "year": 2009
  }, {
    "title": "Discovering important nodes through graph entropy the case of Enron email database",
    "authors": ["Jitesh Shetty", "Jafar Adibi."],
    "venue": "Proceedings of the 3rd international workshop on Link discovery. ACM, New York, NY, USA, LinkKDD ’05, pages 74–",
    "year": 2005
  }, {
    "title": "Modeling sociocultural phenomena in discourse",
    "authors": ["Tomek Strzalkowski", "George Aaron Broadwell", "Jennifer Stromer-Galley", "Samira Shaikh", "Sarah Taylor", "Nick Webb."],
    "venue": "Proceedings of the 23rd International Conference on COL-",
    "year": 2010
  }, {
    "title": "Coling 2010 Organizing Committee, Beijing, China",
    "authors": ["ING"],
    "venue": "http://www.aclweb.org/ anthology/C10-1117.",
    "year": 2010
  }, {
    "title": "The Pursuit of Power and Its Manifestation in Written Dialog",
    "authors": ["Swabha Swayamdipta", "Owen Rambow."],
    "venue": "2012 IEEE Sixth International Conference on Semantic Computing 0:22–29. https://doi.org/http:",
    "year": 2012
  }, {
    "title": "Chinese and American Leadership Characteristics: Discovery and Compar",
    "authors": ["Sarah M. Taylor", "Ting Liu", "Samira Shaikh", "Tomek Strzalkowski", "George Aaron Broadwell", "Jennifer Stromer-Galley", "Umit Boz", "Xiaoai Ren", "Jingsi Wu", "Feifei Zhang"],
    "year": 2012
  }, {
    "title": "Structures of discourse and structures of power",
    "authors": ["Teun A Van Dijk."],
    "venue": "Annals of the International Communication Association 12(1):18–59.",
    "year": 1989
  }, {
    "title": "More accurate tests",
    "authors": [],
    "year": 2000
  }],
  "id": "SP:1420aa3a7722e9fdad5fb2ea68a326a794ccf0fa",
  "authors": [{
    "name": "Vinodkumar Prabhakaran",
    "affiliations": []
  }, {
    "name": "Premkumar Ganeshkumar",
    "affiliations": []
  }, {
    "name": "Owen Rambow",
    "affiliations": []
  }],
  "abstractText": "Understanding how social power structures affect the way we interact with one another is of great interest to social scientists who want to answer fundamental questions about human behavior, as well as to computer scientists who want to build automatic methods to infer the social contexts of interactions. In this paper, we employ advancements in extrapropositional semantics extraction within NLP to study how author commitment reflects the social context of an interactions. Specifically, we investigate whether the level of commitment expressed by individuals in an organizational interaction reflects the hierarchical power structures they are part of. We find that subordinates use significantly more instances of non-commitment than superiors. More importantly, we also find that subordinates attribute propositions to other agents more often than superiors do — an aspect that has not been studied before. Finally, we show that enriching lexical features with commitment labels captures important distinctions in social meanings.",
  "title": "Author Commitment and Social Power: Automatic Belief Tagging to Infer the Social Context of Interactions"
}