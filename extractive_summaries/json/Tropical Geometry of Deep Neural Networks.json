{
  "sections": [{
    "text": "Index Terms—Tropical geometry, decision boundaries, lottery ticket hypothesis, pruning, adversarial attacks.\nF"
  }, {
    "heading": "1 INTRODUCTION",
    "text": "D EEP Neural Networks (DNNs) have demonstratedoutstanding performance across a variety of research domains, including computer vision [1], speech recognition [2], natural language processing [3], [4], quantum chemistry [5], and healthcare [6], [7] to name a few [8]. Nevertheless, a rigorous interpretation of their success remains elusive [9]. For instance, in an attempt to uncover the expressive power of DNNs, the work of [10] studied the complexity of functions computable by DNNs that have piecewise linear activations. They derived a lower bound on the maximum number of linear regions. Several other works have followed to improve such estimates under certain assumptions [11]. In addition, and in attempt to understand some of the subtle behaviours DNNs exhibit, e.g. the sensitive reaction of DNNs to small input perturbations, several works directly investigated the decision boundaries induced by a DNN for classification. The work of [12] showed that the smoothness of these decision boundaries and their curvature can play a vital role in network robustness. Moreover, the expressiveness of these decision boundaries at perturbed inputs was studied in [13], where it was shown that these boundaries do not resemble the boundaries around benign inputs. The work of [14] showed that under certain assumptions, the decision boundaries of the last fully connected layer of DNNs will converge to a linear SVM. Also, [15] showed that the decision regions of DNNs with width smaller than\nPart of the work was done while MG was at itemis AG. §. Equal contribution\nthe input dimension are unbounded. More recently, and due to the popularity of the piecewise linear ReLU as an activation function, there has been a surge in the number of works that study this class of DNNs in particular. As a result, this has incited significant interest in new mathematical tools that help analyze piecewise linear functions, such as tropical geometry. While tropical geometry has shown its potential in many applications such as dynamic programming [16], linear programming [17], multi-objective discrete optimization [18], enumerative geometry [19], and economics [20], [21], it has only been recently used to analyze DNNs. For instance, the work of [22] showed an equivalency between the family of DNNs with piecewise linear activations and integer weight matrices and the family of tropical rational maps, i.e. ratio between two multi-variate polynomials in tropical algebra. This study was mostly concerned about characterizing the complexity of a DNN by counting the number of linear regions, into which the function represented by the DNN can divide the input space. This was done by counting the number of vertices of a polytope representation recovering the results of [10] with a simpler analysis. More recently, [23] leveraged this equivalency to propose a heuristic for neural network minimization through approximating the tropical rational map.\nContributions. In this paper, we take the results of [22] several steps further and present a novel perspective on the decision boundaries of DNNs using tropical geometry. To that end, our contributions are three-fold. (i) We de-\nar X\niv :2\n00 2.\n08 83\n8v 2\n[ cs\n.L G\n] 1\n0 Fe\nb 20\n21\n2 rive a geometric representation (convex hull between two zonotopes) for a super set to the decision boundaries of a DNN in the form (Affine, ReLU, Affine). (ii) We demonstrate a support for the lottery ticket hypothesis [24] from a geometric perspective. (iii) We leverage the geometric representation of the decision boundaries, referred to as the decision boundaries polytope, in two interesting applications: network pruning and adversarial attacks. For tropical pruning, we design a geometrically inspired optimization to prune the parameters of a given network such that the decision boundaries polytope of the pruned network does not deviate too much from its original network counterpart. We conduct extensive experiments with AlexNet [1] and VGG16 [25] on SVHN [26], CIFAR10, and CIFAR 100 [27] datasets, in which 90% pruning rate is achieved with a marginal drop in testing accuracy. For tropical adversarial attacks, we show that one can construct input adversaries that can change network predictions by perturbing the decision boundaries polytope."
  }, {
    "heading": "2 PRELIMINARIES TO TROPICAL GEOMETRY",
    "text": "For completeness, we first provide preliminaries to tropical geometry and refer the interested readers to [28], [29] for more details.\nDefinition 1. (Tropical Semiring§) The tropical semiring T is the triplet {R ∪ {−∞},⊕, }, where ⊕ and define tropical addition and tropical multiplication, respectively. They are denoted as:\nx⊕ y = max{x, y}, x y = x+ y, ∀x, y ∈ T."
  }, {
    "heading": "It can be readily shown that −∞ is the additive identity and 0 is the multiplicative identity.",
    "text": "Given the previous definition, a tropical power can be formulated as x a = x x · · · x = a.x, for x ∈ T, a ∈ N, where a.x is standard multiplication. Moreover, a tropical quotient can be defined as: x y = x − y, where x − y is standard subtraction. For ease of notation, we write x a as xa.\nDefinition 2. (Tropical Polynomials) For x ∈ Td, ci ∈ R and ai ∈ Nd, a d-variable tropical polynomial with n monomials f : Td → Td can be expressed as:\nf(x) = (c1 xa1)⊕ (c2 xa2)⊕ · · · ⊕ (cn xan), ∀ ai 6= aj when i 6= j.\nWe use the more compact vector notation xa = xa11 x a2 2 · · · xadd . Moreover and for ease of notation, we will denote ci xai as cixai throughout the paper.\nDefinition 3. (Tropical Rational Functions) A tropical rational is a standard difference or a tropical quotient of two tropical polynomials: f(x)− g(x) = f(x) g(x). Algebraic curves or hypersurfaces in algebraic geometry, which are the solution sets to polynomials, can be analogously extended to tropical polynomials too.\nDefinition 4. (Tropical Hypersurfaces) A tropical hypersurface of a tropical polynomial f(x) = c1xa1 ⊕ · · · ⊕ cnxan is the set\n§. A semiring is a ring that lacks an additive inverse.\nof points x where f is attained by two or more monomials in f , i.e.\nT (f) := {x ∈ Rd : cixai = cjxaj = f(x), for some ai 6= aj}.\nTropical hypersurfaces divide the domain of f into convex regions, where f is linear in each region. Also, every tropical polynomial can be associated with a Newton polytope.\nDefinition 5. (Newton Polytopes) The Newton polytope of a tropical polynomial f(x) = c1xa1 ⊕ · · · ⊕ cnxan is the convex hull of the exponents ai ∈ Nd regarded as points in Rd, i.e.\n∆(f) := ConvHull{ai ∈ Rd : i = 1, . . . , n and ci 6= −∞}.\nA tropical polynomial determines a dual subdivision, which can be constructed by projecting the collection of upper faces (UF) in P(f) := ConvHull{(ai, ci) ∈ Rd × R : i = 1, . . . , n} onto Rd. That is to say, the dual subdivision determined by f is given as δ(f) := {π(p) ⊂ Rd : p ∈ UF(P(f))}, where π : Rd × R → Rd is the projection that drops the last coordinate. It has been shown by [29] that the tropical hypersurface T (f) is the (d-1)-skeleton of the polyhedral complex dual to δ(f). This implies that each node of the dual subdivision δ(f) corresponds to one region in Rd where f is linear. This is exemplified in Figure 1 with three tropical polynomials, and to see this clearly, we will elaborate on the first tropical polynomial example f(x, y) = x ⊕ y ⊕ 0. Note that as per Definition 4, the tropical hypersurface is the set of points (x, y) where x = y, y = 0, and x = 0. This indeed gives rise to the three solid red lines indicating the tropical hypersurfaces. As for the dual subdivision δ(f), we observe that x⊕ y ⊕ 0 can be written as (x1 y0) ⊕ (x0 y1) ⊕ (x0 y0). Thus, and since the monomials are bias free (ci = 0), then P(f) = ConvHull{(1, 0, 0), (0, 1, 0), (0, 0, 0)}. It is then easy to see that δ(f) = ConvHull{(1, 0), (0, 1), (0, 0)}, since UP(P(f)) = P(f), which is the black triangle in solid lines in Figure 1. One key observation in all three examples in Figure 1 is that the number of regions where f is linear (that is 3, 6 and 10, respectively) is equal to the number of nodes in the corresponding dual subdivisions. Second, the tropical hypersurfaces are parallel to the normals to the edges of the dual subdivision polytope. This observation will be essential for the remaining part of the paper. Several other observations are summarized by [30]. Moreover, [22] showed an equivalency between tropical rational maps and a family of neural network f : Rn → Rk with piecewise linear activations through the following theorem.\nTheorem 1. (Tropical Characterization of Neural Networks, [22]). A feedforward neural network with integer weights and real biases with piecewise linear activation functions is a function f : Rn → Rk, whose coordinates are tropical rational functions of the input, i.e., f(x) = H(x) Q(x) = H(x)−Q(x), where H and Q are tropical polynomials.\nWhile this is new in the context of tropical geometry, it is not surprising, since any piecewise linear function can be written as a difference of two max functions over a set of hyperplanes [31].\nBefore any further discussion, we first recap the definition of zonotopes.\n3\nFig. 1. Tropical Hypersurfaces and their Corresponding Dual Subdivisions. We show three tropical polynomials, where the solid red and black lines are the tropical hypersurfaces T (f) and dual subdivisions δ(f) to the corresponding tropical polynomials, respectively. T (f) divides The domain of f into convex regions where f is linear. Moreover, each region is in one-to-one correspondence with each node of δ(f). Lastly, the tropical hypersurfaces are parallel to the normals of the edges of δ(f) shown by dashed red lines.\nDefinition 6. Let u1, . . . ,uL ∈ Rn. The zonotope formed by u1, . . . ,uL is defined as Z(u1, . . . ,uL) := { ∑L i=1 xiu\ni : 0 ≤ xi ≤ 1}. Equivalently, Z can be expressed with respect to the generator matrix U ∈ RL×n, where U(i, :) = ui> as ZU := {U>x : ∀x ∈ [0, 1]L}.\nAnother common definition for a zonotope is the Minkowski sum of the set of line segments {u1, . . . ,uL} (refer to appendix), where a line segment of the vector ui in Rn is defined as {αui : ∀α ∈ [0, 1]}. It is well-known that the number of vertices of a zonotope is polynomial in the number of line segments, i.e. |vert (ZU) | ≤ 2 ∑n−1 i=0 (L−1 i ) =\nO ( Ln−1 ) [32]."
  }, {
    "heading": "3 DECISION BOUNDARIES OF NEURAL NETWORKS AS POLYTOPES",
    "text": "In this section, we analyze the decision boundaries of a network in the form (Affine, ReLU, Affine) using tropical geometry. For ease, we use ReLUs as the non-linear activation, but any other piecewise linear function can also be used. The functional form of this network is: f(x) = Bmax (Ax + c1,0) + c2, where max(.) is an element-wise operator. The outputs of the network f are the logit scores. Throughout this section, we assume§ that A ∈ Zp×n, B ∈ Z2×p, c1 ∈ Rp and c2 ∈ R2. For ease of notation, we only consider networks with two outputs, i.e. B2×p, where the extension to a multi-class output follows naturally and is discussed in the appendix. Now, since f is a piecewise linear function, each output can be expressed as a tropical rational as per Theorem 1. If f1 and f2 refer to the first and second outputs respectively, we have f1(x) = H1(x) Q1(x) and f2(x) = H2(x) Q2(x), where H1, H2, Q1 and Q2 are tropical polynomials. In what follows and for ease of presentation, we present our main results where the network f has no biases, i.e. c1 = 0 and c2 = 0, and we leave the generalization to the appendix.\nTheorem 2. For a bias-free neural network in the form f(x) : Rn → R2, where A ∈ Zp×n and B ∈ Z2×p, let R(x) = H1(x) Q2(x) ⊕ H2(x) Q1(x) be a tropical polynomial. Then: • Let B = {x ∈ Rn : f1(x) = f2(x)} define the decision boundaries of f , then B ⊆ T (R(x)).\n§. Without loss of generality, as one can very well approximate real weights as fractions and multiply by the least common multiple of the denominators as discussed in [22].\n• δ (R(x)) = ConvHull (ZG1 ,ZG2). ZG1 is a zonotope in Rn with line segments {(B+(1, j) + B−(2, j))[A+(j, :),A−(j, :)]}pj=1 and shift (B−(1, :) + B+(2, :))A−, where A+ = max(A, 0) and A− = max(−A, 0). ZG2 is a zonotope in Rn with line segments {(B−(1, j) + B+(2, j))[A+(j, :),A−(j, :)]}pj=1 and shift (B+(1, :) + B−(2, :))A−.\nDigesting Theorem 2. This theorem aims at characterizing the decision boundaries (where f1(x) = f2(x)) of a bias-free neural network of the form (Affine, ReLU, Affine) through the lens of tropical geometry. In particular, the first result of Theorem 2 states that the tropical hypersurface T (R(x)) of the tropical polynomial R(x) is a superset to the set of points forming the decision boundaries, i.e. B. Just as discussed earlier and exemplified in Figure 1, tropical hypersurfaces are associated with a corresponding dual subdivision polytope δ(R(x)). Based on this, the second result of Theorem 2 states that this dual subdivision is precisely the convex hull of two zonotopes denoted as ZG1 and ZG2 , where each zonotope is only a function of the network parameters A and B.\nTheorem 2 bridges the gap between the behaviour of the decision boundaries B, through the superset T (R(x)), and the polytope δ (R(x)), which is the convex hull of two zonotopes. It is worthwhile to mention that [22] discussed a special case of the first part of Theorem 2 for a neural network with a single output and a score function s(x) to classify the output. To the best of our knowledge, this work is the first to propose a tropical geometric formulation of a superset containing the decision boundaries of a multi-class classification neural network. In particular, the first result of Theorem 2 states that one can perhaps study the decision boundaries, B, directly by studying their superset T (R(x)). While studying T (R(x)) can be equally difficult, the second result of Theorem 2 comes in handy. First, note that, since the network is bias-free, π becomes an identity mapping with δ(R(x)) = ∆(R(x)), and thus the dual subdivision δ(R(x)), which is the Newton polytope ∆(R(x)) in this case, becomes a well-structured geometric object that can be exploited to preserve decision boundaries as per the second part of Theorem 2. Now, based on the results of [29] (Proposition 3.1.6) and as discussed in Figure 1, the normals to the edges of the polytope δ(R(x)) (convex hull of two zonotopes) are in one-to-one correspondence with the tropical hypersurface T (R(x)). Therefore, one can study the\n4\nFig. 2. Decision Boundaries as Geometric Structures. The decision boundaries B (in red) comprise two linear pieces separating classes C1 and C2. As per Theorem 2, the dual subdivision of this single hidden neural network is the convex hull between the zonotopes ZG1 and ZG2 . The normals to the dual subdivison δ(R(x)) are in one-to-one correspondence to the tropical hypersurface T (R(x)), which is a superset to the decision boundaries B. Note that some of the normals to δ(R(x)) (in red) are parallel to the decision boundaries.\ndecision boundaries, or at least their superset T (R(x)), by studying the orientation of the dual subdivision δ(R(x)).\nWhile Theorem 2 presents a strong relation between a polytope (convex hull of two zonotopes) and the decision boundaries, it remains unclear how such a polytope can be efficiently constructed. Although the number of vertices of a zonotope is polynomial in the number of its generating line segments, fast algorithms for enumerating these vertices are still restricted to zonotopes with line segments starting at the origin [33]. Since the line segments generating the zonotopes in Theorem 2 have arbitrary end points, we present the next result that transforms these line segments into a generator matrix of line segments starting from the origin as in Definition 6. This result is essential for an efficient computation of the zonotopes in Theorem 2.\nProposition 1. The zonotope formed by p line segments in Rn with arbitrary end points {[ui1,ui2]} p i=1 is equivalent to the zonotope formed by the line segments {[ui1 − ui2,0]} p i=1 with a\nshift of ∑p i=1 u i 2.\nWe can now represent with the following corollary the arbitrary end point line segments forming the zonotopes in Theorem 2 with generator matrices, which allow us to leverage existing algorithms that enumerate zonotope vertices [33].\nCorollary 1. The generators of ZG1 ,ZG2 in Theorem 2 can be defined as G1 = Diag[(B+(1, :)) + (B−(2, :))]A and G2 = Diag[(B+(2, :)) + (B−(1, :))]A, both with shift (B−(1, :) + B+(2, :) + B+(1, :) + B−(2, :))A−, where Diag(v) arranges v in a diagonal matrix.\nNext, we show several applications for Theorem 2 by leveraging the tropical geometric structure."
  }, {
    "heading": "4 TROPICAL PERSPECTIVE TO THE LOTTERY TICKET HYPOTHESIS",
    "text": "The lottery ticket hypothesis was recently proposed by [24], in which the authors surmise the existence of sparse trainable sub-networks of dense, randomly-initialized, feedforward networks that when trained in isolation perform as well as the original network in a similar number of iterations. To find such sub-networks, [24] propose the following simple algorithm: perform standard network pruning, initialize the pruned network with the same initialization\nthat was used in the original training setting, and train with the same number of epochs. They hypothesize that this results in a smaller network with a similar accuracy. In other words, a sub-network can have decision boundaries similar to those of the original larger network. While we do not provide a theoretical reason why this pruning algorithm performs favorably, we utilize the geometric structure that arises from Theorem 2 to reaffirm such behaviour. In particular, we show that the orientation of the dual subdivision δ(R(x)) (referred to as decision boundaries polytope), where the normals to its edges are parallel to T (R(x)) that is a superset to the decision boundaries, is preserved after pruning with the proposed initialization algorithm of [24]. Conversely, pruning with a different initialization at each iteration results in a significant variation in the orientation of the decision boundaries polytope and ultimately in reduced accuracy.\nTo this end, we train a neural network with 2 inputs (n = 2), 2 outputs, and a single hidden layer with 40 nodes (p = 40). We then prune the network by removing the smallest x% of the weights. The pruned network is then trained using different initializations: (i) the same initialization as the original network [24], (ii) Xavier [34], (iii) standard Gaussian, and (iv) zero mean Gaussian with variance 0.1. Figure 3 shows the decision boundaries polytope, i.e. δ(R(x)), as we perform more pruning (increasing the x%) with different initializations. First, we show the decision boundaries by sampling and classifying points in a grid with the trained network (first subfigure). We then plot the decision boundaries polytope δ(R(x)) as per the second part of Theorem 2 denoted as original polytope (second subfigure). While there are many overlapping vertices in the original polytope, the normals to some of the edges (the major visible edges) are parallel to the decision boundaries shown in the first subfigure of Figure 3. We later show the decision boundaries polytope for the same network under different levels of pruning. One can observe that the orientation of δ(R(x)) for all different initialization schemes deviates much more from the original polytope as compared to the lottery ticket initialization. This gives an indication that lottery ticket initialization indeed preserves the decision boundaries, since it preserves the orientation of the decision boundaries polytope throughout the evolution of pruning. An alternative means to study the lottery ticket could be to directly observe the polytopes representing\n5\nFig. 3. Effect of Different Initializations on the Decision Boundaries Polytope. From left to right: training dataset, decision boundaries polytope of original network (before pruning), followed by the decision boundaries polytope for networks pruned at different pruning percentages using different initializations. Note that in the original polytope there are many more vertices than just 4, but they are very close to each other forming many small edges that are not visible in the figure.\nFig. 4. Tropical Pruning Pipeline. Pruning the 4th node, or equivalently removing the two yellow vertices of zonotope ZG2 does not affect the decision boundaries polytope, which will lead to no change in accuracy.\nthe functional form of the network, i.e. δ(H{1,2}(x)) and δ(Q{1,2}(x)), in lieu of the decision boundaries polytopes. However, this strategy may fail to provide a conclusive analysis of the lottery ticket, since there can exist multiple polytopes δ(H{1,2}(x)) and δ(Q{1,2}(x)) for networks with the same decision boundaries. This highlights the importance of studying the decision boundaries directly. Additional discussions and experiments are left for the appendix."
  }, {
    "heading": "5 TROPICAL NETWORK PRUNING",
    "text": "Network pruning has been identified as an effective approach to reduce the computational cost and memory usage during network inference. While it dates back to the work of [35] and [36], network pruning has recently gained more attention. This is due to the fact that most neural networks over-parameterize commonly used datasets. In network pruning, the task is to find a smaller subset of the network parameters, such that the resulting smaller network has\nsimilar decision boundaries (and thus supposedly similar accuracy) to the original over-parameterized network. In this section, we show a new geometric approach towards network pruning. In particular and as indicated by Theorem 2, preserving the polytope δ(R(x)) preserves a superset to the decision boundaries, T (R(x)), and thus the decision boundaries themselves.\nMotivational Insight. For a single hidden layer neural network, the dual subdivision to the decision boundaries is the polytope that is the convex hull of two zonotopes, where each is formed by taking the Minkowski sum of line segments (Theorem 2). Figure 4 shows an example, where pruning a neuron in the network has no effect on the dual subdivision polytope and hence no effect on performance. This occurs, since the tropical hypersurface T (R(x)) before and after pruning is preserved, thus, keeping the decision boundaries the same.\nProblem Formulation. In light of the motivational insight, a natural question arises: Given an over-parameterized\n6\nFig. 5. Pruning Ressults on Toy Networks. We apply tropical pruning on the toy network that is in the form of Affine followed by a ReLU followed by another Affine. From left to right: (a) dataset used for training (b) pruning networks with 100 hidden nodes (c) 200 hidden nodes (d) 300 hidden nodes.\nFig. 6. Results of Tropical Pruning. Pruning-accuracy plots for AlexNet (top) and VGG16 (bottom) trained on SVHN, CIFAR10, and CIFAR100, pruned with our tropical method and three other pruning methods.\nbinary output neural network f(x) = Bmax (Ax,0), can one construct a new neural network, parameterized by sparser weight matrices Ã and B̃, such that this smaller network has a dual subdivision δ(R̃(x)) that preserves the decision boundaries of the original network?\nTo address this question, we propose the following optimization problem to compute Ã and B̃:\nmin Ã,B̃\nd ( δ(R̃(x)), δ(R(x)) ) = min\nÃ,B̃ d ( ConvHull ( ZG̃1 ,ZG̃2 ) ,ConvHull (ZG1 ,ZG2) ) .\n(1) The function d(.) defines a distance between two geometric objects. Since the generators G̃1 and G̃2 are functions of Ã and B̃ (as per Theorem 2), this optimization problem can be challenging to solve. However, for pruning purposes, one can observe from Theorem 2 that if the generators G̃1 and G̃2 had fewer number of line segments (rows), this corresponds to a fewer number of rows in the weight matrix Ã (sparser weights). So, we observe that if G̃1 ≈ G1\nand G̃2 ≈ G2, then ˜δ(R(x)) ≈ δ(R(x)), and thus the decision boundaries tend to be preserved as a consequence. Therefore, we propose the following optimization problem as a surrogate to the one in Problem (1):\nmin Ã,B̃\n1\n2 ( ∥∥∥G̃1 −G1∥∥∥2 F + ∥∥∥G̃2 −G2∥∥∥2 F ) + λ1 ∥∥∥G̃1∥∥∥ 2,1 + λ2 ∥∥∥G̃2∥∥∥ 2,1 .\n(2)\nThe matrix mixed norm for C ∈ Rn×k is defined as ‖C‖2,1 = ∑n i=1 ‖C(i, :)‖2, which encourages the matrix C to be row sparse, i.e. complete rows of C are zero. The first two terms in Problem (2) aim at approximating the original dual subdivision δ(R(x)) by approximating the underlying generator matrices, G1 and G2. This aims to preserve the orientation of the decision boundaries of the newly constructed network. On the other hand, the second two terms in Problem (2) act as regularizers to control the sparsity of the constructed network by controlling the sparsity in the number of line segments. We observe that Problem (2) is not quadratic in its variables, since as per\n7 Corollary, 1 G̃1 = Diag[ReLU(B̃(1, :)) + ReLU(−B̃(2, :))]Ã and G̃2 = Diag[ReLU(B̃(2, :)) + ReLU(−B̃(1, :))]Ã. However, since Problem (2) is separable in the rows of Ã and B̃, we solve Problem (2) via alternating optimization over these rows, where each sub-problem can be shown to be convex and exhibits a closed-form solution leading to a very efficient solver. For ease of notation, we refer to ReLU(B̃(i, :)) and ReLU(−B̃(i, :)) as B̃+(i, :) and B̃−(i, :), respectively. As such, the per row update for Ã (first linear layer) is given as follows:\nÃ(i, :)\n= max 1− λ1 √ ci1 + λ2 √ ci2\n(ci1 + c i 2)\n1∥∥∥ci1G1(i,:)+ci2G2(i,:)1 2 (c i 1+c i 2) ∥∥∥ 2 , 0  . ( ci1G1(i, :) + c i 2G2(i, :)\n1 2 (c i 1 + c i 2)\n) ,\nwhere ci1 is the i th element of c1 = ReLU(B(1, :)) + ReLU(−B(2, :)) and c2 = ReLU(B(2, :)) + ReLU(−B(1, :)). Similarly, the closed form update for the jth element of the second linear layer is as follows:\nB̃+(1, j) = max ( 0,\nÃ(j, :)>G̃1+(j, :)− λ‖Ã(j, :)‖2 ‖Ã(j, :)‖22\n) ,\nwhere G1+ = Diag(B+(1, :))A. A similar argument can be used to update the variables B̃+(2, :), B̃−(1, :), and B̃−(2, :). The details of deriving the aforementioned update steps and the extension to the multi-class case are left to the appendix. Note that all updates are cheap, as they are expressed in a closed form single step. In all subsequent experiments, we find that running the alternating optimization for a single iteration is sufficient to converge to a reasonable solution, thus, leading to a very efficient overall solver. Extension to Deeper Networks. While the theoretical results in Theorem 2 and Corollary 1 only hold for a shallow network in the form of (Affine, ReLU, Affine), we propose a greedy heuristic to prune much deeper networks by applying the aforementioned optimization for consecutive blocks of (Affine, ReLU, Affine) starting from the input and ending at the output of the network. This extension from a theoretical study of 2 layer network was observed in several works such as [37].\nExperiments on Tropical Pruning. Here, we evaluate the performance of the proposed pruning approach as compared to several classical approaches on several architectures and datasets. In particular, we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU) and Class Distribution (CD) [38], [39]. In Class Blind, all the parameters of a layer are sorted by magnitude where the x% with smallest magnitude are pruned. In contrast, Class Uniform prunes the parameters with smallest x% magnitudes per node in a layer. Lastly, Class Distribution performs pruning of all parameters for each node in the layer, just as in Class Uniform, but the parameters are pruned based on the standard deviation σc of the magnitude of the parameters per node.\nToy Networks. To verify our theoretical work, we first start by pruning small networks that are in the form of\nAffine followed by a ReLU nonlinearity followed by another Affine layer. We train the aforementioned network on two 2D datasets with a varying number of hidden nodes (100, 200, 300). In this setup, we observe from Figure 5 that when Theorem 2 assumptions hold, our proposed tropical pruning is indeed competitive, and in many cases outperforms, the other non decision boundaries aware pruning schemes.\nReal Networks. Since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers. We train AlexNet and VGG16 on SVHN, CIFAR10, and CIFAR100 datasets. We observe that we can prune more than 90% of the classifier parameters for both networks without affecting the accuracy. Since pruning is often a single block within a larger compression scheme that in many cases involves inexpensive fast fine tuning, we demonstrate experimentally that our approach can is competitive and sometimes outperforms other methods even when all parameters or when only the biases are fine-tuned after pruning. These experiments in addition to many others are left for the appendix.\nSetup. To account for the discrepancy in input resolution, we adapt the architectures of AlexNet and VGG16, since they were originally trained on ImageNet [40]. The fully connected layers of AlexNet and VGG16 have sizes (256,512,10) and (512,512,10) for SVHN and CIFAR10, respectively, and with the last dimension increased to 100 for CIFAR100. All networks were trained to baseline test accuracy of (92%,74%,43%) for AlexNet on SVHN, CIFAR10, and CIFAR100, respectively and (92%,92%,70%) for VGG16. To evaluate the performance of pruning and following previous work [38], we report the area under the curve (AUC) of the pruning-accuracy plot. The higher the AUC is, the better the trade-off is between pruning rate and accuracy. For efficiency purposes, we run the optimization in Problem 2 for a single alternating iteration to identify the rows in Ã and elements of B̃ that will be pruned.\nResults. Figure 5 shows the comparison between our tropical approach and the three popular pruning schemes on both AlexNet and VGG16 over the different datasets. Our proposed approach can indeed prune out as much as 90% of the parameters of the classifier without sacrificing much of the accuracy. For AlexNet, we achieve much better performance in pruning as compared to other methods. In particular, we are better in AUC by 3%, 3%, and 2% over other pruning methods on SVHN, CIFAR10 and CIFAR100, respectively. This indicates that the decision boundaries can indeed be preserved by preserving the dual subdivision polytope. For VGG16, we perform similarly well on both SVHN and CIFAR10 and slightly worse on CIFAR100. While the performance achieved here is comparable to the other pruning schemes, if not better, we emphasize that our contribution does not lie in outperforming state-of-the-art pruning methods, but in giving a new geometry-based perspective to network pruning. More experiments were conducted where only network biases or only the classifier are fine-tuned after pruning. Retraining only biases can be sufficient, as they do not contribute to the orientation of the decision boundaries polytope (and effectively the decision boundaries), but only to its translation. Discussions on biases and more results are left for the appendix.\n8\nFig. 7. Dual View of Tropical Adversarial Attacks. We show the effects of tropical adversarial attacks on a synthetic binary dataset at four different input points (black dots in black). From left to right: the decision regions of the original model, perturbed model, and the corresponding decision boundaries polytopes (green for original and blue for perturbed).\nComparison Against Tropical Geometry Approaches. A recent tropical geometry inspired approach was proposed to address the problem of network pruning. In particular, [23], [41] (SM) proposed an interesting yet heuristic algorithm to directly approximate the tropical rational by approximating the Newton polytope. For fair comparison and following the setup of SM, we train LeNet on MNIST and monitor the test accuracy as we prune its neurons. We report (neurons kept, SM, ours) triplets in (%) as follows: (100, 98.60, 98.84), (90, 95.71, 98.82), (75, 95.05, 98.8), (50, 95.52, 98.71), (25, 91.04, 98.36), (10, 92.79, 97.99), and (5, 92.93, 94.91). It is clear that tropical pruning outperforms SM by a margin that reaches 7%. This demonstrates that our theoretically motivated approach is still superior to more recent pruning approaches."
  }, {
    "heading": "6 TROPICAL ADVERSARIAL ATTACKS",
    "text": "DNNs are notorious for being susceptible to adversarial attacks. In fact, adding small imperceptible noise, referred to as adversarial attacks, to the input of these networks can hinder their performance. Several works investigated the decision boundaries of neural networks in the presence of adversarial attacks. For instance, [42] analyzed the high dimensional geometry of adversarial examples by means of manifold reconstruction. Also, [13] crafted adversarial attacks by estimating the distance to the decision boundaries using random search directions. In this work, we provide a tropical geometric view to this task, where we show how Theorem 2 can be leveraged to construct a tropical geometry-based targeted adversarial attack.\nDual View to Adversarial Attacks. For a classifier f : Rn → Rk and input x0 classified as c, a standard formulation for targeted adversarial attacks flips the prediction to a particular class t and is usually defined as\nmin η D(η) s.t. argmaxi fi(x0 + η) = t 6= c (3)\nThis objective aims to compute the lowest energy input noise η (measured by D) such that the the new sample (x0 + η) crosses the decision boundaries of f to a new classification region. Here, we present a dual view to adversarial attacks. Instead of designing a sample noise η such that (x0 + η) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region. In particular, let A1 be the first linear\nlayer of f , such that f(x0) = g(A1x0). One can now perturb A1 to alter the decision boundaries and relate this parameter perturbation to the input perturbation as follows:\ng((A1 + ξA1)x0) = g (A1x0 + ξA1x0)\n= g(A1x0 + A1η) = f(x0 + η). (4)\nFrom this dual view, we observe that traditional adversarial attacks are intimately related to perturbing the parameters of the first linear layer through the linear system: A1η = ξA1x0. The two views and formulations are identical under such condition. With this analysis, Theorem 2 provides explicit means to geometrically construct adversarial attacks by perturbing the decision boundaries. In particular, since the normals to the dual subdivision polytope δ(R(x)) of a given neural network represent the tropical hypersurface T (R(x)), which is a superset to the decision boundaries set B, ξA1 can be designed to result in a minimal perturbation to the dual subdivision that is sufficient to change the network prediction of x0 to the targeted class t. Based on this observation, we formulate the problem as follows:\nmin η,ξA1\nD1(η) +D2(ξA1)\ns.t. − loss(g(A1(x0 + η)), t) ≤ −1; − loss(g(A1 + ξA1)x0, t) ≤ −1; (x0 + η) ∈ [0, 1]n, ‖η‖∞ ≤ 1; ‖ξA1‖∞,∞ ≤ 2, A1η = ξA1x0.\n(5)\nThe loss is the standard cross-entropy loss. The first row of constraints ensures that the network prediction is the desired target class t when the input x0 is perturbed by η, and equivalently by perturbing the first linear layer A1 by ξA1 . This is identical to f1 as proposed by [43]. Moreover, the third and fourth constraints guarantee that the perturbed input is feasible and that the perturbation is bounded, respectively. The fifth constraint is to limit the maximum perturbation on the first linear layer, while the last constraint enforces the dual equivalence between input perturbation and parameter perturbation. The function D2 captures the perturbation of the dual subdivision polytope upon perturbing the first linear layer by ξA1 . For a single hidden layer neural network parameterized as (A1 + ξA1) ∈ Rp×n and B ∈ R2×p for the first and second layers respectively, D2 can capture the perturbations in each of the two zonotopes discussed in Theorem 2 and we define it as:\n9\nFig. 8. Effect of Tropical Adversarial Attacks on MNIST Images. First row from the left: Clean image, perturbed images classified as [7,3,2,1,0] respectively. Second row from left: Clean image, perturbed images classified as [9,8,7,3,2] respectively. Third row from left: Clean image, perturbed images classified as [9,8,7,5,3] respectively. Fourth row from left: Clean image, perturbed images classified as [9,4,3,2,1] respectively. Fifth row from left: Clean image, perturbed images classified as [8,4,3,2,1] respectively.\nD2(ξA1) = 1\n2 2∑ j=1 ∥∥Diag(B+(j, :))ξA1∥∥2F + ∥∥Diag(B−(j, :))ξA1∥∥2F .\n(6)\nThe derivation, discussion, and extension of (6) to multiclass neural networks is left for the appendix. We solve Problem (5) with a penalty method on the linear equality constraints, where each penalty step is solved with ADMM [44] in a similar fashion to the work of [45]. The details of the algorithm are left for the appendix.\nMotivational Insight to the Dual View. Here, we train a single hidden layer neural network, where the size of the input is 2 with 50 hidden nodes and 2 outputs on a simple dataset as shown in Figure 7. We then solve Problem 5 for a given x0 shown in black. We show the decision boundaries\nfor the network with and without the perturbation at the first linear layer ξA1 . Figure 7 shows that perturbing an edge of the dual subdivision polytope, by perturbing the first linear layer, indeed corresponds to perturbing the decision boundaries and results in the misclassification of x0. Interestingly and as expected, perturbing different decision boundaries corresponds to perturbing different edges of the dual subdivision.\nMNIST Experiment. Here, we design perturbations to misclassify MNIST images. Figure 8 shows several adversarial examples that change the network prediction for digits {5, 4, 1, 8, 9} to other targets. Namely our tropical reformulation of adversarial attack successfully causes the network to misclassifying digit 9 as 8,4,3,2, and 1, respectively. In some cases, the perturbation η is as small as = 0.1, where x0 ∈ [0, 1]n. We again emphasize that our approach is not meant to be compared with (or beat) state of the art adver-\n10\nsarial attacks, but rather to provide a novel geometrically inspired perspective that can shed new light in this field."
  }, {
    "heading": "7 CONCLUSION",
    "text": "We leverage tropical geometry to characterize the decision boundaries of neural networks in the form (Affine, ReLU, Affine) and relate it to well-studied geometric objects such as zonotopes and polytopes. We leaverage this representation in providing a tropical perspective to support the lottery ticket hypothesis, network pruning and designing adversarial attacks. One natural extension for this work is a compact derivation for the characterization of the decision boundaries of convolutional neural networks (CNNs) and graphical convolutional networks (GCNs)."
  }, {
    "heading": "APPENDIX PRELIMINARIES AND DEFINITIONS.",
    "text": "Fact 1. P +̃Q = {p+ q,∀p ∈ P and q ∈ Q} is the Minkowski sum between two sets P and Q.\nFact 2. Let f be a tropical polynomial and let a ∈ N. Then P(fa) = aP(f).\nLet both f and g be tropical polynomials. Then\nFact 3. P(f g) = P(f)+̃P(g). (7)\nFact 4. P(f ⊕ g) = ConvexHull ( V (P(g)) ∪ V (P(g)) ) . (8)\nNote that V(P(f)) is the set of vertices of the polytope P(f). Fact 5. Let {Si}ni=1 be the set of n line segments. Then we have that\nS = S1+̃ . . . +̃Sn = P +̃V\nwhere the sets P = +̃j∈C1Sj and V = +̃j∈C2Sj where C1 and C2 are any complementary partitions of the set {Si}ni=1. Definition 7. Upper Face of a Polytope P : UF(P ) is an upper face of polytope P in Rn if x + ten /∈ P for any x ∈ UF(P ), t > 0 where en is a canonical vector. Formally,\nUF(P ) = {x : x ∈ P, x + ten /∈ P ∀t > 0}"
  }, {
    "heading": "EXAMPLES",
    "text": "We revise the second example in Figure 1. Note that the two dimensional tropical polynomial f(x, y) can be written as follows:\nf(x, y)\n= (x⊕ y ⊕ 0) ((x 1)⊕ (y 1)⊕ 0) = (x x 1)⊕ (x y 1)⊕ (x 0)⊕ (y x 1) ⊕ (y y 1)⊕ (y 0)⊕ (0 x 1)⊕ (0 y 1)⊕ (0 0) = (x2 1)⊕ (x y 1)⊕ (x) ⊕ (y x 1)⊕ (y2 1)⊕ (y)⊕ (x 1)⊕ (y 1)⊕ (0) = (x2 1)⊕ (x y 1)⊕ (x)⊕ (y2 1)⊕ (y 1)⊕ (0) = (x2 y0 1)⊕ (x y 1)⊕ (x y0 0) ⊕ (x0 y2 1)⊕ (x0 y 1)⊕ (x0 y0 0)\nFirst equality follows since multiplication is distributive in rings and semi rings. The second equality follows since 0 is the multiplication identity. The penultimate equality follows since y 1 ≥ y, x y 1 ≥ x y 1 and x ≥ x 1 ∀ x, y. Therefore, the tropical hypersurface T (f) is defined as the of (x, y) where f achieves its maximum at least twice in its monomials. That is to say,\nT (f) = {f(x, y) = (x2 1) = (x y 1)}∪ {f(x, y) = x2 1 = x}∪ {(f(x, y) = x = 0} ∪ {f(x, y) = x = x y 1}∪ {f(x, y) = y 1 = 0} ∪ {f(x, y) = y 1 = x y 1}∪ {f(x, y) = y 1 = y2 1} ∪ {f(x, y) = y2 1 = x y 1}.\nThis set T (f) is shown by the red lines in the second example in Figure 1. As for constructing the dual subdivision δ(f), we project the upperfaces in the newton polygon P(f) to R2. Note that P(f) with biases as per the definition in Section 2 is given as P(f) = ConvHull{(ai, ci) ∈ R2 × R ∀i = 1, . . . , 6} where (ai, ci) are the exponents and biases in the monomials of f , respectively. Therefore,P(f) is the ConvexHull of the points {(2, 0,−1), (1, 1, 1), (1, 0, 0), (0, 2, 1), (0, 1, 1), (0, 0, 0)} as shown in Figure 9(a). As per Definition 7, the set of upper faces of P is:\nUP(P(f)) = ConvHull{(0, 2, 1), (1, 1, 1), (0, 1, 1)}∪ ConvHull{(0, 1, 1), (1, 1, 1), (1, 0, 0)}∪ ConvHull{(0, 1, 1), (1, 0, 0), (0, 0, 0)}∪ ConvHull{(1, 1, 1), (2, 0,−1), (1, 0, 0)}.\nThis set UP(P(f)) is then projected, through π, to R2 shown in the yellow dashed lines in Figure 9(a) to construct the dual subdivision δ(f) in Figure 9(b). For example, note that the point (0, 2, 1) ∈ UF(f) and thereafter, π(0, 2, 1) = (0, 2, 0) ∈ δ(f)."
  }, {
    "heading": "PROOF OF THEOREM 2",
    "text": "Theorem 2. For a bias-free neural network in the form of f(x) : Rn → R2 where A ∈ Zp×n and B ∈ Z2×p, let R(x) = H1(x) Q2(x) ⊕ H2(x) Q1(x) be a tropical polynomial. Then:\n• Let B = {x ∈ Rn : f1(x) = f2(x)} define the decision boundaries of f , then B ⊆ T (R(x)). • δ (R(x)) = ConvHull (ZG1 ,ZG2). ZG1 is a zonotope in Rn with line segments {(B+(1, j) + B−(2, j))[A+(j, :),A−(j, :)]}pj=1 and shift (B−(1, :) + B+(2, :))A−. ZG2 is a zonotope in Rn with line segments {(B−(1, j) + B+(2, j))[A+(j, :),A−(j, :)]}pj=1 and shift (B+(1, :) + B−(2, :))A−. The line segment (B+(1, j) + B−(2, j))[A+(j, :),A−(j, :)] has end points A+(j, :) and A−(j, :) in Rn and scaled by (B+(1, j) + B−(2, j)).\nNote that A+ = max(A, 0) and A− = max(−A, 0) where the max(.) is element-wise. The line segment (B(1, j)+ + B(2, j)−)[A(j, :)+,A(j, :)−] is one that has the end points\n11\n(a) (b)\nFig. 9. The Newton Polygon and the Corresponding Dual subdivision. The left Figure shows the newton polygon P(f) for the tropical polynomial defined in the second example in Figure 1. The dual subdivision δ(f) is constructed by shadowing/projecting the upper faces of P(f) on R2.\nA(j, :)+ and A(j, :)− in Rn and scaled by the constant B(1, j)+ + B(2, j)−.\nProof. For the first part, recall from Theorem1 that both f1 and f2 are tropical rationals and hence,\nf1(x) = H1(x)−Q1(x) f2(x) = H2(x)−Q2(x)\nThus;\nB = {x ∈ Rn : f1(x) = f2(x)} = {x ∈ Rn : H1(x)−Q1(x) = H2(x)−Q2(x)} = {x ∈ Rn : H1(x) +Q2(x) = H2(x) +Q1(x)} = {x ∈ Rn : H1(x) Q2(x) = H2(x) Q1(x)}\nRecall that the tropical hypersurface is defined as the set of x where the maximum is attained by two or more monomials. Therefore, the tropical hypersurface of R(x) is the set of x where the maximum is attained by two or more monomials in (H1(x) Q2(x)), or attained by two or more monomials in (H2(x) Q1(x)), or attained by monomials in both of them in the same time, which is the decision boundaries. Hence, we can rewrite that as\nT (R(x)) = T (H1(x) Q2(x)) ∪ T (H2(x) Q1(x)) ∪ B.\nTherefore B ⊆ T (R(x)). For the second part of the Theorem, we first use the decomposition proposed by [22], [46] to show that for a network f(x) = Bmax (Ax,0), it can be decomposed as tropical rational as follows\nf(x) = ( B+ −B− ) ( max(A+x,A−x)−A−x ) = [ B+ max(A+x,A−x) + B−A−x\n] − [ B−max(A+x,A−x) + B+A−x ] .\nTherefore, we have that H1(x) +Q2(x) = ( B+(1, :) + B−(2, :) ) max(A+x,A−x)\n+ ( B−(1, :) + B+(2, :) ) A−x,\nH2(x) +Q1(x) = ( B−(1, :) + B+(2, :) ) max(A+x,A−x)\n+ ( B+(1, :) + B−(2, :) ) A−x.\nTherefore, note that:\nδ(R(x)) = δ (( H1(x) Q2(x) ) ⊕ ( H2(x) Q1(x) )) 8 = ConvexHull ( δ ( H1(x) Q2(x) ) ,\nδ ( H2(x) Q1(x) )) 7 = ConvexHull ( δ ( H1(x) ) +̃δ ( Q2(x) ) ,\nδ ( H2(x) ) +̃δ ( Q1(x) )) .\nNow observe that H1(x) = ∑p j=1 ( B+(1, j) +\nB−(2, j) ) max ( A+(j, :),A−(j, :)x ) tropically is given as\nfollows H1(x) = pj=1 [ xA +(j,:) ⊕ xA−(j,:) ]B+(1,j) B−(2,j)\n, thus we have that :\nδ(H1(x))\n= +̃ p j=1\n[( B+(1, p) + B−(2, p) )( δ(xA +(p,:) ⊕ xA −(p,:)) )]\n= +̃ p j=1\n[( B+(1, p) + B−(2, p) ) ConvexHull ( A+(p, :),A−(p, :) )] .\nThe operator +̃pj=1 indicates a Minkowski sum over the index j between sets. Note that ConvexHull ( A+(i, :\n),A−(i, :) ) is the convexhull between two points which\n12\nFig. 10. Comparison between the decision boundaries polytope and the polytopes representing the functional representation of the network. First column: decision boundaries polytope δ(R(x)) while the remainder of the columns are the zonotopes δ(H1(x)), δ(Q1(x)), δ(H2(x)) and δ(Q2(x)) respectively. Under varying pruning rate across the rows, it is to be observed that the changes that affected the dual subdivisions of the functional representations are far smaller compared to the decision boundaries polytope.\nis a line segment in Zn with end points that are {A+(i, :),A−(i, :)} scaled with B+(1, i) + B−(2, i). Observe that δ(F1(x)) is a Minkowski sum of line segments which is is a zonotope. Moreover, note that Q2(x) = (B −(1, :) + B+(2, :))A−x tropically is given as follows Q2(x) = pj=1xA −(j,:)(B +(1,j) B−(2,j)) . One can see that δ(Q2(x)) is the Minkowski sum of the points {(B−(1, j) − B+(2, j))A−(j, :)}∀j in Rn (which is a standard sum) resulting in a point. Lastly, δ(H1(x))+̃δ(Q2(x)) is a Minkowski sum between a zonotope and a single point which corresponds to a shifted zonotope. A similar symmetric argument can be applied for the second part δ(H2(x))+̃δ(Q1(x)).\nIt is also worthy to mention that the extension to network with multi class output is trivial. In that case all of the analysis can be exactly applied studying the decision boundary between any two classes (i, j) where B = {x ∈ Rn : fi(x) = fj(x)} and the rest of the proof will be exactly the same."
  }, {
    "heading": "PROOF OF PROPOSITION 1",
    "text": "Proposition 1. The zonotope formed by p line segments in Rn with arbitrary end points {[ui1,ui2]} p i=1 is equivalent to the zonotope formed by the line segments {[ui1 − ui2,0]} p i=1 with a\nshift of ∑p i=1 u i 2.\nProof. Let Uj be a matrix with Uj(:, i) = uij , i = 1, . . . , p, w be a column-vector with w(i) = wi, i = 1, . . . , p and 1p is a column-vector of ones of length p. Then, the zonotope Z formed by the Minkowski sum of line segments with arbitrary end points can be defined as:\nZ = { p∑ i=1 wiu i 1 + (1− wi)ui2;wi ∈ [0, 1], ∀ i } = { U1w −U2w + U21p, w ∈ [0, 1]p\n} = { (U1 −U2)w + U21p, w ∈ [0, 1]p }\n= { (U1 −U2)w, w ∈ [0, 1]p } +̃ { U21p } .\n13\nSince the Minkowski sum of between a polytope and a point is a translation; thereafter, the proposition follows directly from Definition 6.\nCorollary 2. The generators of ZG1 ,ZG2 in Theorem 2 can be defined as G1 = Diag[(B+(1, :)) + (B−(2, :))]A and G2 = Diag[(B+(2, :)) + (B−(1, :))]A, both with shift (B−(1, :) + B+(2, :) + B+(1, :) + B−(2, :))A−, where Diag(v) arranges v in a diagonal matrix.\nProof. This follows directly by applying Proposition 1 to the second bullet point of Theorem 2."
  }, {
    "heading": "TROPICAL LOTTERY TICKET HYPOTHESIS: SUPPLEMENTAL EXPERIMENTS",
    "text": "A natural question is whether it is necessary to visualize the dual subdivision polytope of the decision boundaries, i.e. δ(R(x)), where R(x) = H1(x) Q2(x) ⊕ H2(x) Q1(x) as opposed to visualizing the tropical polynomials δ(H{1,2}(x)) and δ(Q{1,2}(x)) directly for the tropical reaffirmation of the lottery ticket hypothesis. That is similar to asking whether it is necessary to visualize and study the decision boundaries polytope δ(R(x)) as compared to the the dual subdivision polytope of the functional form of the network since for the 2-output neural network described in Theorem 2 we have that f1(x) = H1(x) Q1(x) and f2(x) = H2(x) Q2(x). We demonstrate this with an experiment that demonstrates the differences between these two views. For this purpose, we train a single hidden layer neural network on the same dataset shown in Figure 3. We perform several iterations of pruning in a similar fashion to Section 5 and visualise at each iteration both the decision boundaries polytope and all the dual subdivisions of the aforementioned tropical polynomials representing the functional form of the network, i.e. δ(H{1,2}(x)) and δ(Q{1,2}(x)). It is to be observed from Figure 10 that despite that the decision boundaries were barely affected with the lottery ticket pruning, the zonotopes representing the functional form of the network endure large variations. That is to say, investigating the dual subdivisions describing the functional form of the networks through the four zonotopes δ(H{1,2}(x)) and δ(Q{1,2}(x)) is not indicative enough to the behaviour of the decision boundaries."
  }, {
    "heading": "TROPICAL PRUNING: OPTIMIZATION OF OBJECTIVE",
    "text": ""
  }, {
    "heading": "2 OF THE BINARY CLASSIFIER",
    "text": "min Ã,B̃\n1\n2 ∥∥∥G̃1 −G1∥∥∥2 F + 1 2 ∥∥∥G̃2 −G2∥∥∥2 F\n+ λ1 ∥∥∥G̃1∥∥∥ 2,1 + λ2 ∥∥∥G̃2∥∥∥ 2,1 .\n(9)\nNote that G̃1 = Diag [ ReLU(B̃(1, :)) + ReLU(−B̃(2, :)) ] Ã,\nG̃2 = Diag [ ReLU(B̃(2, :)) + ReLU(−B̃(1, :)) ] Ã. Note that\nG1 = Diag [ ReLU(B(1, :)) + ReLU(−B(2, :)) ] A and G2 =\nDiag [ ReLU(B(2, :)) + ReLU(−B(1, :)) ] A. For ease of no-\ntation, we refer to ReLU(B̃(i, :)) and ReLU(−B̃(i, :)) as B̃+(i, :) and B̃−(i, :), respectively. We solve the problem with co-ordinate descent an alternate over variables."
  }, {
    "heading": "Updating Ã:",
    "text": "Ã =argminÃ 1\n2 ∥∥∥Diag (c1) Ã−G1∥∥∥2 F + 1 2 ∥∥∥Diag(c2)Ã−G2∥∥∥2 F\n+ λ1 ∥∥∥Diag(c1)Ã∥∥∥ 2,1 + λ2 ∥∥∥Diag(c2)Ã∥∥∥ 2,1 ,\nwhere c1 = ReLU(B(1, :)) + ReLU(−B(2, :)) and c2 = ReLU(B(2, :)) + ReLU(−B(1, :)). Note that the problem is separable per-row of Ã. Therefore, the problem reduces to updating rows of Ã independently and the problem exhibits a closed form solution.\nÃ(i, :) = argminÃ(i,:) 1\n2 ∥∥∥ci1Ã(i, :)−G1(i, :)∥∥∥2 2\n+ 1\n2 ∥∥∥ci2Ã(i, :)−G2(i, :)∥∥∥2 2\n+ (λ1 √ ci1 + λ2 √ ci2) ∥∥∥Ã(i, :)∥∥∥ 2\n= argminÃ(i,:) 1\n2 ∥∥∥∥∥Ã(i, :)− ci1G1(i, :) + ci2G2(i, :)1 2 (c i 1 + c i 2) ∥∥∥∥∥ 2\n2\n+ 1\n2\nλ1 √ ci1 + λ2 √ ci2\n1 2 (c i 1 + c i 2)\n∥∥∥Ã(i, :)∥∥∥ 2\n= max 1− 1 2 λ1 √ ci1 + λ2 √ ci2\n1 2 (c i 1 + c i 2)\n1∥∥∥ci1G1(i,:)+ci2G2(i,:)1 2 (c i 1+c i 2) ∥∥∥ 2 , 0  . ( ci1G1(i, :) + c i 2G2(i, :)\n1 2 (c i 1 + c i 2)\n) .\nUpdating B̃+(1, :):\nB̃+(1, :) = argminB̃+(1,:) 1\n2 ∥∥∥Diag(B̃+(1, :)) Ã−C1∥∥∥2 F\n+ λ1 ∥∥∥Diag(B̃+(1, :)) Ã + C2∥∥∥ 2,1 ,\ns.t. B̃+(1, :) ≥ 0. Note that C1 = G1 − Diag ( B̃−(2, :) ) Ã and where\nDiag ( B̃−(2, :) ) Ã. Note the problem is separable in the\ncoordinates of B̃+(1, :) and a projected gradient descent can be used to solve the problem in such a way as:\nB̃+(1, j) = argminB̃+(1,j) 1\n2 ∥∥∥B̃+(1, j)Ã(j, :)−C1(j, :)∥∥∥2 2\n+ λ1 ∥∥∥B̃+(1, j)Ã(j, :) + C2(j, :)∥∥∥ 2 ,\ns.t. B̃+(1, j) ≥ 0.\nA similar symmetric argument can be used to update the variables B̃+(2, :), B̃+(1, :) and B̃−(2, :)."
  }, {
    "heading": "TROPICAL PRUNING: ADAPTING OPTIMIZATION 2 FOR MULTI-CLASS CLASSIFIER",
    "text": "Note that Theorem 2 describes a superset to the decision boundaries of a binary classifier through the dual subdivision R(x), i.e. δ(R(x)). For a neural network f with k classes, a natural extension for it is to analyze the pair-wise\n14\ndecision boundaries of of all k-classes. Thus, let T (Rij(x)) be the superset to the decision boundaries separating classes i and j. Therefore, a natural extension to the geometric loss in Equation 1 is to preserve the polytopes among all pairwise follows:\nmin Ã,B̃ ∑ ∀[i,j]∈S d ( ConvexHull ( ZG̃(i+,j−) ,ZG̃(j+,i−) ) ,\nConvexHull ( ZG(i+,j−) ,ZG(j+,i−) )) . (10)\nThe set S is all possible pairwise combinations of the k classes such that S = {{i, j},∀i 6= j, i = 1, . . . , k, j = 1, . . . , k}. The generator Z(G̃(i,j)) is the zonotope with the generator matrix G̃(i+,j−) =\nDiag [ ReLU(B̃(i, :)) + ReLU(−B̃(j, :)) ] Ã. However, such\nan approach is generally computationally expensive, particularly, when k is very large. To this end, we make the following observation that G̃(i+,j−) can be equivalently written as a Minkowski sum between two sets zonotopes with the generators Gi+ = Diag [ ReLU(B̃(i, :) ] Ã and\nGj− = Diag [ ReLU(B̃j−) ] Ã. That is to say, ZG̃(i+,j−) = ZG̃i++̃ZG̃j− . This follows from the associative property of Minkowski sums given as follows:\nHence, G̃(i+,j−) can be seen a concatenation between G̃i+ and G̃j− . Thus, the objective in 10 can be expanded as follows:\nmin Ã,B̃ ∑ ∀{i,j}∈S d ( ConvexHull ( ZG̃(i+,j−) ,ZG̃(j+,i−) ) ,\nConvexHull ( ZG(i+,j−) ,ZG(j+,i−) )) = min\nÃ,B̃ ∑ ∀{i,j}∈S d ( ConvexHull ( ZG̃i+ +̃ZG̃j− ,ZG̃+j +̃ZG̃i− ) ,\nConvexHull ( ZGi+ +̃ZGj− ,ZG+j +̃ZGi− )) ≈ min\nÃ,B̃ ∑ ∀[i,j]∈S ∥∥∥(G̃i+ G̃j− ) − ( Gi+ Gj− )∥∥∥2 F + ∥∥∥(G̃i− G̃j+ ) − ( Gi− Gj+ )∥∥∥2 F\n= min Ã,B̃ ∑ ∀{i,j}∈S 1 2 ∥∥∥G̃i+ −Gi+∥∥∥2 F + 1 2 ∥∥∥G̃i− −Gi−∥∥∥2 F\n+ 1\n2 ∥∥∥G̃j+ −Gj+∥∥∥2 F + 1 2 ∥∥∥G̃j− −Gj−∥∥∥2 F\n= min Ã,B̃ k − 1 2 k∑ i=1 ∥∥∥G̃i+ −Gi+∥∥∥2 F + ∥∥∥G̃i− −Gi−∥∥∥2 F .\nThe approximation follows in a similar argument to the binary classifier case. The last equality follows from a counting argument. We solve the objective for all multi-class networks in the experiments with alternating optimization in a similar fashion to the binary classifier case. Similarly to the binary classification approach, we introduce the ‖.‖2,1 to enforce sparsity constraints for pruning purposes. Therefore\nthe overall objective has the form:\nmin Ã,B̃\n1\n2 k∑ i=1 ∥∥∥G̃i+ −Gi+∥∥∥2 F + ∥∥∥G̃i− −Gi−∥∥∥2 F\n+ λ (∥∥∥G̃i+∥∥∥ 2,1 + ∥∥∥G̃i−∥∥∥ 2,1 ) .\nFor completion, we derive the updates for Ã and B̃."
  }, {
    "heading": "Updating Ã:",
    "text": "Ã = argminÃ k∑ i=1 1 2 (∥∥∥Diag(B̃+(i, :)) Ã−Gi+∥∥∥2 F\n+ ∥∥∥Diag(B̃−(i, :)) Ã−Gi−∥∥∥2\nF ) + λ (∥∥∥Diag(B̃+(i, :)) Ã∥∥∥ 2,1\n+ ∥∥∥Diag(B̃−(i, :)) Ã∥∥∥\n2,1\n) .\nSimilar to the binary classification, the problem is separable in the rows of Ã. A closed form solution in terms of the proximal operator of `2 norm follows naturally for each Ã(i, :). Updating B̃+(i, :):\nB̃+(i, :) = argminB̃+(i,:) 1\n2 ∥∥∥Diag(B̃+(i, :)) Ã− G̃i+∥∥∥2 F\n+ λ ∥∥∥Diag(B̃+(i, :)) Ã∥∥∥\n2,1 ,\ns.t. B̃+(i, :) ≥ 0.\nNote that the problem is separable per coordinates of B+(i, :) and each subproblem is updated as:\nB̃+(i, j) = argminB̃+(i,j) 1\n2 ∥∥∥B̃+(i, j)Ã(j, :)− G̃i+(j, :)∥∥∥2 2\n+ λ ∥∥∥B̃+(i, j)Ã(j, :)∥∥∥\n2 , s.t. B̃+(i, j) ≥ 0\n= argminB̃+(i,j) 1\n2 ∥∥∥B̃+(i, j)Ã(j, :)− G̃i+(j, :)∥∥∥2 2\n+ λ ∣∣∣B̃(i, j)∣∣∣ ∥∥∥Ã(j, :)∥∥∥\n2 , s.t. B̃+(i, j) ≥ 0\n= max ( 0,\nÃ(j, :)>G̃i+(j, :)− λ‖Ã(j, :)‖2 ‖Ã(j, :)‖22\n) .\nA similar argument can be used to update B̃−(i, :) ∀i. Finally, the parameters of the pruned network will be constructed A← Ã and B← B̃+ − B̃−."
  }, {
    "heading": "TROPICAL PRUNING: SUPPLEMENTAL EXPERIMENTS",
    "text": "Experimental Setup. In all experiments of the tropical pruning section, all algorithms are run for only a single iteration where λ increases linearly from 0.02 with a factor of 0.01. Increasing λ corresponds to increasing weight sparsity and we keep doing until sparsification is 100%. Experiments. We conduct more experimental results on AlexNet and VGG16 on SVHN, CIFAR10 and CIFAR100 datasets. We examine the performance for when the networks have only the biases of the classifier fine tuned after tuning as shown in Figure 11. Moreover, a similar\n15\nFig. 11. Results of Tropical Pruning with Fine Tuning the Biases of the Classifier. Tropical pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different pruning methods with fine tuning the biases of the classifier only.\nFig. 12. Results of Tropical Pruning with Fine Tuning the Biases of the Network. Tropical pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different pruning methods with fine tuning the biases of the network.\nexperiments is reported for the same networks but for when the biases for the complete networks are fine tuned as in Figure 12."
  }, {
    "heading": "TROPICAL ADVERSARIAL ATTACKS: ALGORITHM FOR SOLVING (5)",
    "text": "In this section, we are going to derive an algorithm for solving the following problem:\nmin η,ξA1\nD1(η) +D2(ξA1)\ns.t. − loss(g(A1(x0 + η)), t) ≤ −1 − loss(g(A1 + ξA1)x0, t) ≤ −1, (x0 + η) ∈ [0, 1]n, ‖η‖∞ ≤ 1, ‖ξA1‖∞,∞ ≤ 2, A1η − ξA1x0 = 0. (11) The function D2(ξA) captures the perturbdation in the dual subdivision polytope such that the dual subdivion of the network with the first linear layer A1 is similar to the dual subdivion of the network with the first linear layer A1 + ξA1 . This can be generally formulated as an approximation to the following distance function\nd ( ConvHull ( ZG̃1 ,ZG̃2 ) ,ConvHull (ZG1 ,ZG2) ) , where\nG̃1 = Diag [ ReLU(B̃(1, :)) + ReLU(−B̃(2, :)) ] ( Ã + ξA1 ) ,\nG̃2 = Diag [ ReLU(B̃(2, :)) + ReLU(−B̃(1, :)) ] ( Ã + ξA1 ) ,\nG1 = Diag [ ReLU(B̃(1, :)) + ReLU(−B̃(2, :)) ] Ã and G2 =\nDiag [ ReLU(B̃(2, :)) + ReLU(−B̃(1, :)) ] Ã. In particular, to\napproximate the function d, one can use a similar argument as in used in network pruning 5 such that D2 approximates the generators of the zonotopes directly as follows:\nD2(ξA1) = 1\n2 ∥∥∥G̃1 −G1∥∥∥2 F + 1 2 ∥∥∥G̃2 −G2∥∥∥2 F\n= 1\n2 ∥∥∥Diag(B+(1, :))ξA1∥∥∥2 F + 1 2 ∥∥∥Diag(B−(1, :))ξA1∥∥∥2 F\n+ 1\n2 ∥∥∥Diag(B+(2, :))ξA1∥∥∥2 F + 1 2 ∥∥∥Diag(B−(2, :))ξA1∥∥∥2 F .\nThis can thereafter be extended to multi-class network with k classes as follows D2(ξA1) = 1 2 ∑k j=1 ∥∥∥Diag(B+(j, :))ξA1∥∥∥2 F + ∥∥∥Diag(B−(j, :))ξA1∥∥∥2 F . Following [45], we take D1(η) = 12 ‖η‖ 2 2. Therefore, we can write 11 as follows:\n16\nAlgorithm 1 Solving Problem (5)\nInput: A1 ∈ Rp×n,B ∈ Rk×p,x0 ∈ Rn, t, λ > 0, γ > 1,K > 0, ξA1 = 0p×n, η1 = z1 = w1 = z1 = u1 = w1 = 0n. Output: η, ξA1 Initialize: ρ = ρ0 while not converged do\nfor k ≤ K do η update: ηk+1 = (2λA>1 A1 + (2 + ρ)I) −1(2λA>1 ξ k A1 x0 + ρz k − uk)\nw update: wk+1 =  min(1− x0, 1) : zk − 1/ρvk > min(1− x0, 1) max(−x0,− 1) : zk − 1/ρvk < max(−x0,− 1) zk − 1/ρvk : otherwise z update: zk+1 = 1 ηk+1+2ρ\n(ηk+1zk + ρ(ηk+1 + 1/ρuk + wk + 1/ρvk)−∇L(zk + x0)) ξA1 update: ξ k+1 A1 = argminξA‖ξA1‖ 2 F + λ‖ξA1x0 −A1ηk+1‖22 + L̄(A1) s.t. ‖ξA1‖∞,∞ ≤ 2 u update: uk+1 = uk + ρ(ηk+1 − zk+1) v update: vk+1 = vk + ρ(wk+1 − zk+1) ρ← γρ\nend λ← γλ ρ← ρ0\nend\nmin η,ξA D1(η) + k∑ j=1 ∥∥∥Diag(B+(j, :))ξA∥∥∥2 F\n+ ∥∥∥Diag(B−(j, :))ξA∥∥∥2\nF .\ns.t. − loss(g(A1(x0 + η)), t) ≤ −1, − loss(g((A1 + ξA1)x0), t) ≤ −1, (x0 + η) ∈ [0, 1]n, ‖η‖∞ ≤ 1, ‖ξA1‖∞,∞ ≤ 2, A1η − ξA1x0 = 0.\nTo enforce the linear equality constraints A1η − ξA1x0 = 0, we use a penalty method, where each iteration of the penalty method we solve the sub-problem with ADMM updates. That is, we solve the following optimization problem with ADMM with increasing λ such that λ → ∞. For ease of notation, lets denote L(x0 + η) = −loss(g(A1(x0 + η)), t), and L̄(A1) = −loss(g((A1 + ξA1)x0), t).\nmin η,z,w,ξA1 ‖η‖22 + k∑ j=1 ∥∥∥Diag(ReLU(B(j, :))ξA1∥∥∥2 F\n+ ∥∥∥Diag(ReLU(−B(j, :)))ξA1∥∥∥2\nF + L(x0 + z)\n+ h1(w) + h2(ξA1) + λ‖A1η − ξA1 x0‖22 + L̄(A1). s.t. η = z z = w.\nwhere\nh1(η) = { 0, if (x0 + η) ∈ [0, 1]n, ‖η‖∞ ≤ 1 ∞, else\nh2(ξA1) = { 0, if ‖ξA1‖∞,∞ ≤ 2 ∞, else .\nThe augmented Lagrangian is given as follows:\nL(η,w, z, ξA1 ,u,v) :=\n‖η‖22 + L(x0 + z) + h1(w) + k∑ j=1 ∥∥Diag(B+(j, :))ξA1∥∥2F + ∥∥Diag(B−(j, :))ξA1∥∥2F + L̄(A1) + h2(ξA1)\n+ λ‖A1η − ξA1 x0‖22 + u>(η − z) + v>(w − z) + ρ\n2 (‖η − z‖22 + ‖w − z‖22).\nThereafter, ADMM updates are given as follows:\n{ηk+1,wk+1} = argminη,wL(η,w, z k, ξkA1 ,u k,vk),\nzk+1 = argminzL(η k+1,wk+1, z, ξkA1 ,u k,vk), ξk+1A1 = argminξA1L(η k+1,wk+1, zk+1, ξA1 ,u k,vk).\nuk+1 = uk + ρ(ηk+1 − zk+1), vk+1 = vk + ρ(wk+1 − zk+1).\nUpdating η:\nηk+1 = argminη‖η‖ 2 2 + λ‖A1η − ξA1 x0‖22\n+ u>η + ρ\n2 ‖η − z‖22 = ( 2λA>1 A1 + (2 + ρ)I )−1\n( 2λA>1 ξ k A1x0 + ρz k − uk ) .\nUpdating w:\nwk+1 = argminwv k>w + h1(w) +\nρ 2 ‖w − zk‖22\n= argminw 1\n2 ∥∥∥∥∥w − ( zk − v k ρ )∥∥∥∥∥ 2\n2\n+ 1\nρ h1(w).\nThe update w is separable in coordinates as follows:\n17\nwk+1 =  min(1− x0, 1) : zk − 1/ρvk > min(1− x0, 1) max(−x0,− 1) : zk − 1/ρvk < max(−x0,− 1) zk − 1/ρvk : otherwise"
  }, {
    "heading": "Updating z:",
    "text": "zk+1 =argminzL(x0 + z)− u k>z− vk>z\n+ ρ\n2\n( ‖ηk+1 − z‖22 + ‖wk+1 − z‖22 ) .\nLiu et al. [47] showed that the linearized ADMM converges for some non-convex problems. Therefore, by linearizing L and adding Bergman divergence term ηk/2‖z−zk‖22, we can then update z as follows:\nzk+1 = 1\nηk + 2ρ\n( ηkzk + ρ ( ηk+1\n+ 1\nρ uk + wk+1 +\n1 ρ vk ) −∇L(zk + x0) ) .\nIt is worthy to mention that the analysis until this step is inspired by [45] with modifications to adapt our new formulation. Updating ξA:\nξk+1A =argminξA‖ξA1‖ 2 F + λ‖ξA1x0 −A1η‖22 + L̄(A1)\ns.t. ‖ξA1‖∞,∞ ≤ 2.\nThe previous problem can be solved with proximal gradient methods."
  }, {
    "heading": "TROPICAL ADVERSARIAL ATTACKS: EXPERIMENTAL SETUP",
    "text": "Experimental Setup. For the tropical adversarial attacks experiments, there are five different hyper parameters which are\n1 : The upper bound on the input perturbation. 2 : The upper bound on the perturbation in the first layer. λ : The trade off between input and first layer perturbations. η : Bergman divergence constant. ρ : ADMM constant.\nFor all of the experiments, we set the values of 2, λ, η and ρ to 1, 10−3, 2.5 and 1, respectively. As for 1 it is set to 0.1 upon attacking MNIST images of digit 4 set to 0.2 for all other MNIST images."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "This work was supported by King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research."
  }],
  "year": 2021,
  "references": [{
    "title": "ImageNet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "Advances in Neural Information Processing Systems (NeurIPS), 2012.",
    "year": 2012
  }, {
    "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
    "authors": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"],
    "venue": "IEEE Signal Processing Magazine, 2012.",
    "year": 2012
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "venue": "International Conference on Learning Representations, 2015.",
    "year": 2015
  }, {
    "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
    "authors": ["J. Devlin", "M.-W. Chang", "K. Lee", "K. Toutanova"],
    "venue": "arXiv preprint arXiv:1810.04805, 2018.",
    "year": 1810
  }, {
    "title": "Quantum-chemical insights from deep tensor neural networks",
    "authors": ["K. Schütt", "F. Arbabzadah", "S. Chmiela", "K.-R. Müller", "A. Tkatchenko"],
    "venue": "Nature Communications, 2017.",
    "year": 2017
  }, {
    "title": "End-to-end lung cancer screening with threedimensional deep learning on low-dose chest computed tomography",
    "authors": ["D. Ardila", "A.P. Kiraly", "S. Bharadwaj", "B. Choi", "J.J. Reicher", "L. Peng", "D. Tse", "M. Etemadi", "W. Ye", "G. Corrado", "D.P. Naidich", "S. Shetty"],
    "venue": "Nature Medicine, 2019.",
    "year": 2019
  }, {
    "title": "Whole-genome deep-learning analysis identifies contribution of noncoding mutations to autism risk",
    "authors": ["J. Zhou", "C.Y. Park", "C.L. Theesfeld", "A. Wong", "Y. Yuan", "C. Scheckel", "J. Fak", "J. Funk", "K. Yao", "Y. Tajima", "A. Packer", "R. Darnell", "O.G. Troyanskaya"],
    "venue": "Nature Genetics, 2019.",
    "year": 2019
  }, {
    "title": "Deep learning",
    "authors": ["Y. LeCun", "Y. Bengio", "G. Hinton"],
    "venue": "Nature, 2015.",
    "year": 2015
  }, {
    "title": "Understanding machine learning: From theory to algorithms",
    "authors": ["S. Shalev-Shwartz", "S. Ben-David"],
    "venue": "Cambridge university press,",
    "year": 2014
  }, {
    "title": "On the number of linear regions of deep neural networks",
    "authors": ["G. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"],
    "venue": "Advances in Neural Information Processing Systems (NeurIPS), 2014.",
    "year": 2014
  }, {
    "title": "Understanding deep neural networks with rectified linear units",
    "authors": ["R. Arora", "A. Basu", "P. Mianjy", "A. Mukherjee"],
    "venue": "International Conference on Learning Representations, 2018.",
    "year": 2018
  }, {
    "title": "Robustness via curvature regularization, and vice versa",
    "authors": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "J. Uesato", "P. Frossard"],
    "venue": "CVPR, 2019.",
    "year": 2019
  }, {
    "title": "Decision boundary analysis of adversarial examples",
    "authors": ["W. He", "B. Li", "D. Song"],
    "venue": "International Conference on Learning Representations (ICLR), 2018.",
    "year": 2018
  }, {
    "title": "On the decision boundary of deep neural networks",
    "authors": ["Y. Li", "P. Richtarik", "L. Ding", "X. Gao"],
    "venue": "arXiv preprint arXiv:1808.05385, 2018.",
    "year": 1808
  }, {
    "title": "On decision regions of narrow deep neural networks",
    "authors": ["H.-P. Beise", "S.D. Da Cruz", "U. Schröder"],
    "venue": "arXiv preprint arXiv:1807.01194, 2018.",
    "year": 1807
  }, {
    "title": "The tropical geometry of shortest paths",
    "authors": ["M. Joswig", "B. Schröter"],
    "venue": "arXiv preprint arXiv:1904.01082, 2019.",
    "year": 1904
  }, {
    "title": "Tropicalizing the simplex algorithm",
    "authors": ["X. Allamigeon", "P. Benchimol", "S. Gaubert", "M. Joswig"],
    "venue": "SIAM J. Discrete Math. 29:2, 2015.",
    "year": 2015
  }, {
    "title": "Monomial tropical cones for multicriteria optimization",
    "authors": ["M. Joswig", "G. Loho"],
    "venue": "AIP Conference Proceedings, 2019.",
    "year": 2019
  }, {
    "title": "Enumerative tropical algebraic geometry in r2",
    "authors": ["G. Mikhalkin"],
    "venue": "Journal of the American Mathematical Society, 2004.",
    "year": 2004
  }, {
    "title": "Tropical polyhedra are equivalent to mean payoff games",
    "authors": ["M. Akian", "S. Gaubert", "A. Guterman"],
    "venue": "International Journal of Algebra and Computation, 2009.",
    "year": 2009
  }, {
    "title": "Product-mix auctions and tropical geometry",
    "authors": ["N. Mai Tran", "J. Yu"],
    "venue": "Mathematics of Operations Research, 2015.",
    "year": 2015
  }, {
    "title": "Tropical geometry of deep neural networks",
    "authors": ["L. Zhang", "G. Naitzat", "L.-H. Lim"],
    "venue": "International Conference on Machine Learning, ICML 2018, 2018.",
    "year": 2018
  }, {
    "title": "Tropical polynomial division and neural networks",
    "authors": ["G. Smyrnis", "P. Maragos"],
    "venue": "arXiv preprint arXiv:1911.12922, 2019.",
    "year": 1911
  }, {
    "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks.",
    "authors": ["J. Frankle", "M. Carbin"],
    "venue": "in ICLR. OpenReview.net,",
    "year": 2019
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556, 2014.",
    "year": 2014
  }, {
    "title": "Reading digits in natural images with unsupervised feature learning",
    "authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"],
    "venue": "Advances in Neural Information Processing Systems (NeurIPS), 2011.  18",
    "year": 2011
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "venue": "Citeseer, Tech. Rep., 2009.",
    "year": 2009
  }, {
    "title": "Introduction to Tropical Geometry, ser",
    "authors": ["D. Maclagan", "B. Sturmfels"],
    "venue": "Graduate Studies in Mathematics. American Mathematical Society,",
    "year": 2015
  }, {
    "title": "A bit of tropical geometry",
    "authors": ["E. Brugallé", "K. Shaw"],
    "venue": "The American Mathematical Monthly, 2014.",
    "year": 2014
  }, {
    "title": "On the expressibility of piecewise-linear continuous functions as the difference of two piecewise-linear convex functions",
    "authors": ["D. Melzer"],
    "venue": "Quasidifferential Calculus. Springer, 1986.",
    "year": 1986
  }, {
    "title": "Minkowski addition of polytopes: computational complexity and applications to gröbner bases",
    "authors": ["P. Gritzmann", "B. Sturmfels"],
    "venue": "SIAM Journal on Discrete Mathematics, 1993.",
    "year": 1993
  }, {
    "title": "A randomized algorithm for enumerating zonotope vertices",
    "authors": ["K. Stinson", "D.F. Gleich", "P.G. Constantine"],
    "venue": "arXiv preprint arXiv:1602.06620, 2016.",
    "year": 2016
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["X. Glorot", "Y. Bengio"],
    "venue": "Proceedings of 13th International Conference on Artificial Intelligence and Statistics, 2010.",
    "year": 2010
  }, {
    "title": "Optimal brain damage",
    "authors": ["Y. LeCun", "J.S. Denker", "S.A. Solla"],
    "venue": "Advances in neural information processing systems, 1990.",
    "year": 1990
  }, {
    "title": "Second order derivatives for network pruning: Optimal brain surgeon",
    "authors": ["B. Hassibi", "D.G. Stork"],
    "venue": "Advances in neural information processing systems, 1993.",
    "year": 1993
  }, {
    "title": "Analytic expressions for probabilistic moments of pl-dnn with gaussian input",
    "authors": ["A. Bibi", "M. Alfadly", "B. Ghanem"],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 9099–9107.",
    "year": 2018
  }, {
    "title": "Learning both weights and connections for efficient neural networks",
    "authors": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"],
    "venue": "CoRR, 2015.",
    "year": 2015
  }, {
    "title": "Compression of neural machine translation models via pruning",
    "authors": ["A. See", "M. Luong", "C.D. Manning"],
    "venue": "CoRR, 2016.",
    "year": 2016
  }, {
    "title": "Imagenet: A large-scale hierarchical image database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"],
    "venue": "Computer Vision and Patter Recognition Conference (CVPR), 2009.",
    "year": 2009
  }, {
    "title": "Multiclass neural network minimization via tropical newton polytope approximation",
    "authors": ["G. Smyrnis", "P. Maragos"],
    "venue": "International Conference on Machine Learning, ICML 2020, 2020.",
    "year": 2020
  }, {
    "title": "On the geometry of adversarial examples",
    "authors": ["M. Khoury", "D. Hadfield-Menell"],
    "venue": "arXiv preprint arXiv:1811.00525, 2018.",
    "year": 1811
  }, {
    "title": "Towards evaluating the robustness of neural networks",
    "authors": ["N. Carlini", "D.A. Wagner"],
    "venue": "CoRR, 2016.",
    "year": 2016
  }, {
    "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
    "authors": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"],
    "venue": "Foundations and Trends® in Machine learning, 2011.",
    "year": 2011
  }, {
    "title": "Structured adversarial attack: Towards general implementation and better interpretability",
    "authors": ["K. Xu", "S. Liu", "P. Zhao", "P. Chen", "H. Zhang", "D. Erdogmus", "Y. Wang", "X. Lin"],
    "venue": "arXiv preprint arXiv:1808.01664, 2018.",
    "year": 1808
  }, {
    "title": "Trusting svm for piecewise linear cnns",
    "authors": ["L. Berrada", "A. Zisserman", "M.P. Kumar"],
    "venue": "arXiv preprint arXiv:1611.02185, 2016.",
    "year": 2016
  }],
  "id": "SP:0a92498962c2d419e21ecae625457402f155ba79",
  "authors": [{
    "name": "Motasem Alfarra",
    "affiliations": []
  }, {
    "name": "Adel Bibi",
    "affiliations": []
  }, {
    "name": "Hasan Hammoud",
    "affiliations": []
  }, {
    "name": "Mohamed Gaafar",
    "affiliations": []
  }, {
    "name": "Bernard Ghanem",
    "affiliations": []
  }],
  "abstractText": "This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piecewise linear non-linearity activations. We use tropical geometry, a new development in the area of algebraic geometry, to characterize the decision boundaries of a simple network of the form (Affine, ReLU, Affine). Our main finding is that the decision boundaries are a subset of a tropical hypersurface, which is intimately related to a polytope formed by the convex hull of two zonotopes. The generators of these zonotopes are functions of the network parameters. This geometric characterization provides new perspectives to three tasks. (i) We propose a new tropical perspective to the lottery ticket hypothesis, where we view the effect of different initializations on the tropical geometric representation of a network’s decision boundaries. (ii) Moreover, we propose new tropical based optimization reformulations that directly influence the decision boundaries of the network for the task of network pruning. (iii) At last, we discuss the reformulation of the generation of adversarial attacks in a tropical sense. We demonstrate that one can construct adversaries in a new tropical setting by perturbing a specific set of decision boundaries by perturbing a set of parameters in the network.",
  "title": "On the Decision Boundaries of Neural Networks: A Tropical Geometry Perspective"
}