{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Matrix decomposition methods such as factor analysis are a widely used class of models for unsupervised learning. Typically, they operate on two-way matrices with rows thought of as objects and columns thought of as properties. The decomposition amounts to computing two low-rank matrices, one that contains prototypes of properties (loadings) and one that denotes a compressed representation of each object as a combination of the prototypes (factors). However, these methods are ill-suited for data with higher arity\n1Department of Statistics, University of Oxford, UK 2The Alan Turing Institute, London, UK 3Centre for Computational Biology, Institute of Cancer and Genomic Sciences, University of Birmingham, UK. Correspondence to: Tammo Rukat <tammo.rukat@stats.ox.ac.uk>.\n2 https://github.com/TammoR/LogicalFactorisationMachines\nFigure 1: Graphical intuition for 3-way Boolean tensor decomposition. Logical conjunction (∧) of Boolean vectors (columns of factor matrices) generates Rank-1 tensors. The full tensor is the logical disjunction (∨) of rank-1 tensors.\nsuch as ternary interactions. Examples include network interactions at different time-points, gene-gene interactions for different individuals or any two-way data under different experimental conditions. Such data requires methods that specifically account for the higher-order relationship and that are commonly referred to as tensor decomposition.\nBoolean tensor decomposition factorises a K-way binary tensor X ∈ {0, 1}N1×...×NK , into K binary factor matrices Fk ∈ {0, 1}Nk×L, using the Boolean algebra such that\nx[n] ≈ L∨ l=1  ∧ n∈[n] fnl  . (1) Here, x[n] is a single tensor entry and [n] denotes a tuple of indices [n1, . . . , nK ]. We call k = 1 . . .K the modes of the tensor, ∨ and ∧ denote the logical OR and the logical AND operation, respectively. In plain English, eq. (1) says that an observation is 1, if and only if there exist one or more latent dimensions in which all corresponding factors are 1. This leads to easily interpretable factor matrices, Fk, where each latent dimension denotes a subset of entries along mode k, for instance subsets of objects, properties and conditions that occur jointly in the data. This can be thought of as a particular type of canonical polyadic (CP) decomposition, as illustrated in Fig. 1. A different graphical intuition starts from the factor rows and considers the 3-way Boolean tensor product as a three-stage template assignment procedure. Rows of a factor matrix F1 are one-dimensional binary templates of size of the first tensor mode. Rows of the second factor matrix F2 indicates possible patterns of appearance of these templates along the second tensor dimensions. In the same manner, the third factor matrix, F3, ar X iv :1 80 5. 04 58\n2v 1\n[ st\nat .M\nL ]\n1 1\nM ay\n2 01\n8\nindicates which disjunction of these 2D patterns appears in each slice of the tensor, and so forth.\nWe present the first probabilistic approach to Boolean tensor decomposition, the TensOrMachine, featuring distinctly improved accuracy compared to the previous state-of-the-art methods. We develop scalable, sampling-based posterior inference and infer distributions over factor matrices which enables full quantification of uncertainty. In contrast to previous approaches, the probabilistic framework readily treats missing data, allows for tensor completion and integration of prior information. Importantly, the latent representations are amenable to informative and intuitive interpretation."
  }, {
    "heading": "2. Related Work",
    "text": "Tensor decomposition methods are widely used in many domains of application as reviewed by Kolda & Bader (2009). Boolean Tensor decomposition was first introduced in the Psychometric literature by Leenen et al. (1999) and has received lasting attention, following the formal study by Miettinen (2011). The author shows that computing the optimal decomposition is NP-hard and proposes an alternating least squares heuristics as approximate strategy. A different approach based on a random-walk procedure is described by Erdos & Miettinen (2013b) and trades accuracy in the factors for computational scalability. More recently a distributed Apache Spark implementation of the alternating least squares approach has been brought forward (Park et al., 2017). It demonstrates that alternating leasts squares is the state-of-the-art for computing accurate decompositions and serves as baseline method for our experiments. Boolean Matrix Factorisation (Miettinen et al., 2006) shares the logical structure of Boolean tensor decomposition. It has many real-world applications such as collaborative filtering (Su & Khoshgoftaar, 2009) and computer vision (Lázaro-Gredilla et al., 2016) and has previously been approached under a probabilistic perspective (Ravanbakhsh et al., 2016; Rukat et al., 2017). Despite the sustained theoretical interest, there have been only few practical applications of Boolean tensor decomposition, as for instance for information extraction (Erdos & Miettinen, 2013a) and clustering (Metzler & Miettinen, 2015). One of the contributions of this work is the presentation of Boolean Tensor decomposition as an interpretable, versatile and scalable analysis method."
  }, {
    "heading": "3. The TensOrMachine",
    "text": ""
  }, {
    "heading": "3.1. Model Description",
    "text": "We propose a probabilistic generative process for the model described in eq. (1). Each tensor entry, x[n] = x[n1,...,nK ], is a Bernoulli random variable that equals 1 with a probability\ngreater 12 if the following holds true\n∃ l : fnl = 1 ∀ n ∈ [n1, . . . , nK ]. (2)\nThe logistic sigmoid σ(x) = (1+e−x)−1 has the convenient property σ(−x) = 1− σ(x) and lets us readily parametrise the corresponding likelihood\np(x[n]|.) = σ λx̃[n] 1− 2∏\nl 1− ∏ n∈[n] fnl  . (3)\nThe term inside parenthesis evaluates to 1 if the condition in eq. (2) is met and to -1, otherwise. Throughout this work, we use a tilde to denote the mapping from {0, 1} to {−1, 1} such that x̃ = 2x−1. The noise is parametrised by λ ∈ R+, such that for λ→ 0 the likelihood is constant, independent of the factors and for λ→∞ all probability mass is put on the deterministic Boolean tensor product following eq. (1). We can specify Bernoulli priors on the observations x[n] or choose more structured prior distributions. A beta-prior on σ(λ) is a computationally convenient choice as we discuss in the following section. However, in the relevant regime of thousand and more data-points such priors are easily outweighed by the observed data. We therefore assume constant prior distributions in the following derivations and experiments."
  }, {
    "heading": "3.2. Relation to Noisy-OR Models",
    "text": "Boolean decomposition models have an interesting relation to the noisy-OR, a canonical model for multi-causal interactions, which makes the assumption of independence between the inhibition of latent causes (Pearl, 1988). TensOrM violates this assumption with inhibition occurring on the event-level rather than on the cause-level. As an example, in analogy to the data presented in Section. 5.2, consider the event of a students attending a lecture. We assume that this has two possible causes. They may (i) use the opportunity to meet fellow students (ii) have a particular question that they wish to clarify in class. An inhibition on the level of causes may occur if they found a satisfying answer to their question in a textbook. Then, the desire to meet their friends still contributes to the likelihood of attending class. This naturally modelled by the noisy-OR. In contrast, inhibition on the event-level may occur if a traffic disruption makes it impossible to get to campus. This is naturally described by a Boolean factor model, where any additional causes to attend the lecture will not contribute to the likelihood. We argue that for many practical purposes the latter is an interesting alternative. Even if we were to believe in the independent inhibition of latent causes, usually only one latent cause triggers an event. Compared to the noisy-OR, TensOrM acts as an Occam’s razor and enforces sparse and simple explanations, aiming to associate exactly one hidden cause with every event."
  }, {
    "heading": "3.3. Posterior Inference",
    "text": "We condition any entry of a factor matrix, fnkl, on the state of all other parameters which yields the full conditional probability for Gibbs sampling:\np(fnkl|.) = σ ( λf̃nkl ∑ [n]\nnkfixed\nx̃[n]M(nk,l)→[n]\n) . (4)\nWe give a formal derivation in the Supplementary Information A and provide an intuitive explanation in the following. The sigmoid maps from the real line to the interval of [0, 1]. The sum in its argument is taken over all observations x[n] whose likelihood may depend on fnkl. These observations are given by the (K−1)–way sub-tensor of X with dimension nk fixed and correspond to all children of fnkl in a graphical model representation. For each of them, the term inside the sigmoid contributes values in {−λ, 0, λ}. This depends on whether or not f̃ and x̃ are aligned and an another indicator variable M(nk,l)→[n] that take values in {0, 1} and denotes whether the state of fnkl has any relevance for the likelihood of x[n]. It takes the form\nM(nk,l)→[n] = ( ∏ n∈[n]/nk fnl )∏ l′ 6=l ( 1− ∏ n∈[n] fnl′ ) . (5)\nThis variable is again composed of two indicators. The first term is a product over the state of all co-parents of fnkl to observation x[n] in the same latent dimension l. Following the rules of the Boolean product, all these co-parents need to be one in order for fnkl to contribute to the likelihood of x[n]. This corresponds to the AND-operation in eq. (1) that evaluates to zero if any its arguments are zero. The second term is a product of the co-parents in all other latent dimensions and evaluates to zero if any of them explains away observation x[n]. Following the rules of the Boolean product, a single latent dimension is sufficient to explain an observation and makes its likelihood independent of the state of fnkl. This corresponds to the OR-operation in eq. (1) that evaluates to one if any of its arguments is one. It is crucial for the speed of our implementation that eq. (5), and thus eq. (4), can be rapidly evaluated. In particular, discovering a zero in any of the two terms of eq. (5) suffices for the whole expression to be zero and avoids the necessity of considering the remainder of the Markov blanket. Pseudocode for the computational procedure is given in Algorithm 1. Note that updates of fnkl can trivially be computed in parallel across nk for a fixed tensor-mode k. The computation time scales linearly in each tensor dimension and sub-linear in the latent dimension.\nAfter a sampling all factor entries from their full conditional, we set the noise-parameter, λ, to its conditional MAP estimate and repeat until convergence before drawing posterior samples. The conditional MLE of λ is available in\nAlgorithm 1 Computation of the full conditional of fnkl m = 0 // initialise integer count for the sum in eq. (4) for [n] in all tensor indices with slice nk fixed do\n// check relevance of fnkl for x[n]. for n in [n] do\nif fnl = 0 then // fnkl has no relevance for x[n]. continue with next [n]\nend if end for // check for explaining away. for l′ in 1, . . . , L except l do\nfor n in [n] do if fnl′ = 0 then\ncontinue with next l’ end if // x[n] is explained away. continue (next [n])\nend for end for m = m + x̃nd\nend for p(fnkl|.) = ( 1 + exp ( −λ · f̃nkl ·m ))−1\nclosed form and given by the logit of the fraction of datapoints that are correctly reconstructed by the deterministic Boolean product of the current state of the factors. As a prior for σ(λ), it is natural to employ a Beta-distribution, p(σ(λ)) = Beta(σ(λ)|α, β). This intuitively affects the MAP estimate, adding α correctly predicted data points and β incorrectly predicted data points, such that\nσ(λ)MAP =\nα+ ∑ [n] I ( x[n] = ∨ l [ ∧ n∈[n] fnl ]) α+ β +\nK∏ k=1 Nk\n. (6)\nHere, the indicator I evaluates to 1 if its arguments is true and to 0 otherwise. We see that a uniform prior, p(σ(λ)) = Beta(α=1, β=1), corresponds to applying Laplace’s rule of succession to the maximum likelihood estimate."
  }, {
    "heading": "3.4. Tensor Reconstruction Methods and Missing Data",
    "text": "An important application of latent variable models is the prediction of missing observations, e.g. in collaborative filtering or data imputation problems (Su & Khoshgoftaar, 2009). Our probabilistic construction allows us to deal with missing data in a unified way. When computing updates for the latent factors, we can marginalise over any missing observations. Practically, this is equivalent to setting any missing entries x̃[n] to 0, such that they do not contribute to the conditional probability in eq. (4) and contribute a\nfactor of 12 to the likelihood in eq. (9). Thus, encoding observed data as {−1, 1} and unobserved data as 0 is the most convenient choice for any practical implementation. For MAP updates of λ, following eq. (6), unobserved datapoints points are simply excluded.\nFor a direct comparison of predictions to previous methods that only provide a point estimate of the factors, we can reconstruct the data based on the Boolean product of the factor MAP estimates. We use the marginal MAP of each factor entry, fnkl ∈ [0, 1] and find no relevant difference to using the joint MAP. If instead we want to use the full posterior, we can determine the reconstruction of X by rounding the posterior predictive,\n1\nS M∑ s=1 p(x[n]|F (s) 1 , . . . , F (s) K ) , (7)\nto the closest binary value. Here, (F (s)1 , . . . , F (s) K ) is a posterior sample. Yet another alternative is to compute the estimate from the factor matrix mean, thus taking posterior uncertainty but no higher-order correlations into account. Denoting the posterior mean of a factor entry as f̂ , we have p(x[n] = 1|.) ≈ 1 − ∏ l(1 − ∏ n∈[n] f̂nl). The predictive accuracy of this computationally cheap approximation is on par with the more expensive posterior predictive estimate as we show empirically in the next section and in Fig. 2(b)."
  }, {
    "heading": "4. Experiments on Simulated Data",
    "text": ""
  }, {
    "heading": "4.1. Random Tensor Decomposition",
    "text": "We first demonstrate the capabilities of TensOrM on simulated data and compare to the state-of-the-art method distributed boolean tensor factorisation (dbtf) (Park et al., 2017). We simulate random 3-way tensors, X , of size 20×20×20 and vary rank L, expected density E(X) and noise-level. To this end, we take the Boolean product between binary i.i.d. random matrices, Fk, of size 20×L, such that the expected tensor density is given by E(X) = 1− [ 1− (E(Fk))3 ]L . We introduce noise by flipping each entry in the tensor with a given probability. Posterior samples of the factors are drawn, following the procedure described in Section 3.3 with λ initialised to 0.5 and the initial factors drawn i.i.d. Bern(0.5). The reconstruction of X is determined based on the posterior predictive following eq. (7). The reconstruction accuracy is computed as the fraction of correctly reconstructed entries in the noise-free tensor and is shown across a variety of conditions in Fig. 2(a). Our method achieves distinctly higher accuracies throughout all conditions. The margin becomes bigger for very noisy data, as well as for higher tensor ranks and for particularly dense or particularly sparse data.\nCan the superior reconstruction performance be explained\nby implicit model averaging when computing the posterior predictive following eq. (7)? In order to address this conjecture, Fig. 2(b) compares the reconstruction accuracies of the posterior predictive compared to the reconstruction based solely on the MAP estimates of each factor. We can see, that posterior averaging plays an important role only in scenarios with high noise levels. The main performance gain, however, is simply due to a more accurate decomposition. In the practically relevant regime of moderate noise levels below 30%, posterior averaging has virtually no impact. We further note in Fig. 2(a), that dbtf features a similar or higher reconstruction accuracy with respect to the noisy training data. This indicates over-fitting and becomes particularly apparent for large noise levels and large tensor ranks. What distinguishes the decompositions of dbtf and TensOrM in this regime? In Fig. 2(c) we show the training performance under random perturbations of the inferred factors for a noise level of 40% and a expected density of 10%. Our method has a lower training accuracy but is more stable towards random perturbations of the parameters whereas the training accuracy of dbtf decreases rapidly. This shows that TensOrM converges to solutions of larger point-wise density in the parameter space that have better generalisation properties. Recently, this phenomenon has been studied in order to improve the generalisation of deep neural networks (Chaudhari et al., 2016). Estimating posterior distributions, Bayesian techniques naturally assign more probability to such solutions."
  }, {
    "heading": "4.2. Model Selection",
    "text": "A notorious challenge for latent variable models is the choice of dimensionality. Previously, Erdos & Miettinen (2013b) have used the Minimum description length (MDL) principle for this task in Boolean Tensor Decomposition. We follow their derivation and compare to two approaches that our Bayesian treatment offers readily. For the Bayesian Occam’s razor, we start our inference procedure with a large latent dimensionality. After convergence of the Markov chain, we remove all latent dimensions that do not contribute to the likelihood. In these dimensions one of the factors is usually all zeros and the other factors are uniformly random. After removal we restart the burn-in procedure and repeat until only contributing dimensions remain. In the second approach, cross validation, we treat 20% of the data as unobserved during training and choose the model dimensionality that achieves the highest posterior predictive accuracy on the held-out data. Results on random matrices for different noise levels are shown in Fig.3, following the previously described simulation procedure. We find that both approaches clearly outperform MDL. In the case of noise-free observations the Bayesian Occam’s razor features virtually perfect accuracy. For the more realistic scenario of moderate noise levels cross validation is superior.\n(a) Random tensor reconstruction accuracy under variation of the noise level (top), the expected tensor density (centre) and the underlying tensor rank (bottom). Averages are taken across all shown combinations of the other two parameters and across ten random tensors for each such configuration. Test accuracy on noise-free data is shown in solid colours, training accuracy on noisy data in faint colours. Compare to Fig. 8 by Park et al. (2017).\n(b) Comparison of reconstruction methods – We compare predictions based on TensOrM posterior predictive, the factor matrix MAP, and mean as described in Section 3.4 to the the dbtf baseline. M1 indicates the performance margin that is due to finding more accurate factor matrices, M2 denotes the margin that is due to posterior averaging. The underlying tensors are of rank 10. The results are averaged across tensor densities and 10 random repetitions.\n(c) Robustness of training performance under random perturbations – We show reconstruction accuracy on the noisy training data based on the factor matrix MAP estimate under random flips of the factor entries. The underlying tensor has a noise level of 40% and a expected density of 10%. We perform 10 random repetitions but standard deviations are too small to be visible. TensOrM recovers exactly the expected reconstruction accuracy of 60%, while dbtf overfits to the training data. Its reconstruction accuracy deteriorates rapidly under random perturbations, indicating that it has converged to a comparatively narrow mode of the posterior.\nFigure 2: The TensOrMachine is compared to the previous state-of-the-art method, distributed Boolean tensor decompositions (dbtf), and properties of its solutions are investigated. All examples are based on 20×20×20 tensors generated from the Boolean product of i.i.d Bernoulli random matrices. Noise is introduced by flipping every entry in the tensor with a given probability. The reconstruction accuracy indicates the fraction of correctly reconstructed entries in the noise-free test data or in the noisy training data. Reconstructions are based on rounding the posterior predictive to the closest value if not mentioned otherwise. All plots indicate means and standard deviations (some are too small to be visible)."
  }, {
    "heading": "5. Real-world Applications",
    "text": "Here we demonstrate the ability of TensOrM to infer meaningful, interpretable representations from real-world 3-way datasets of temporal interaction networks and temporal object×property–relations. With these moderately sized datasets of less than 100,000 data-points, sampling until convergence and drawing 50 samples takes only few seconds on a single core. Eventually, we turn to a large-scale biological example, analysing networks of relative geneexpression in cancer patients with more than 10 billion data points. Here, the inference procedure takes around 10 hours."
  }, {
    "heading": "5.1. Hospital Ward Interaction Networks",
    "text": "Records of contact between pairs of individuals in a university hospital have originally been acquired to investigate transmission routes of infectious diseases (Vanhems et al., 2013). Proximity sensor measurements were taken for 75 individuals in 20 second time windows across 5 days. In order to examine daily patterns, we group the time-points into 13 time-of-day windows. This leaves us with a binary 13×75×75 time-adjacency tensor. We compute the decomposition, choosing 8 latent dimensions based on crossvalidation as described in Section 3.3. The predictive accuracy on the held-out test data is approximately 91%. Fig. 4(a) shows posterior means of the time-specific and (one of the two equivalent) individual-specific factor matrices. Individuals are labelled as either administrative staff (ADM), medical doctors (MED), nurses (NUR) or patients (PAT). Using this proximity data alone, we were able to recapitulate information about the work patterns of staff groups. For example, 10am-12pm and 3pm-5pm represent the main morning and afternoon clinic hours when all administrative, medical and nursing staff came into interaction, shown in latent dimensions 1 and 3. Whilst medical doctors are closely interacting throughout typical work hours, 9am-7pm, their network does not include any nurses or patients as found in dimension 0. This can be explained by frequent interaction in the doctor’s room but also hints to a lack of interaction with the nursing staff, a phenomenon frequently discussed\nin the literature (see e.g. Manias & Street, 2001). 2pm-3pm indicates a period in which nurses and administrative staff are heavily interacting without participation of medical doctors, presumably indicating an afternoon break or daily joint meeting, see latent dimension 3. Nurses, however, were active throughout the day including being the main staffing body at night as dimension 7 shows. In comparison, dbtf finds only 2 latent dimensions (0 and 5 in our analysis)."
  }, {
    "heading": "5.2. Student Seating Position",
    "text": "Our second example is part of the student-life dataset introduced by Harari et al. (2017) and given by records of the seating positions of students throughout a 9-week Android programming course. We partition the time-points into weeks of the course and the seating positions into six regions front/back - left/centre/right. An additional seating category is labelled with a question mark, where the seating coordinates were not provided but the students appeared in class. This yields a 9×7×42 week-seat-student tensor. We choose seven latent dimensions, again, by optimising for the test-set likelihood with a reconstruction accuracy of approximately 92% on the held out data. The decomposition is shown in Fig. 4(b) and, once more, open to straightforward interpretation. The majority of students shows up to class in the first week of the course and sits scattered throughout the lecture hall, as can be seen in dimensions 0 and 1. The group of students that attends lectures most consistently, dimension 4, sits in the front-centre. The remaining groups describe subsets of students, each corresponding to exactly one of the four front/back left/right seating regions. The formation of some of these groups starts only in week 2 of the course and they all eventually stop to appear in class after 6-7 weeks. The input tensor confirms that only two students attend class in weeks 8 and 9. Similar solutions are found by dbtf and shown in the SI (E) but lack the ability to characterise uncertainty around the inferred mode.\n(a) Dynamic hospital interaction networks – Decomposition on the 13×75×75 time-adjacency-tensor of interactions in a hospital ward. We only show one of the two equivalent individual-specific factors. (b) Student dynamic seating throughout course – Decomposition of the 9×7×42 week-seat-student tensor indicating seating positions for a 9-week university course on Android programming.\n(c) Representations of cancer patients – Each column corresponds to one out of approximately 8,000 cancer patients and indicates which of the latent properties of relative expression among approximately 2,000 genes they exhibit. Patients are ordered by type of cancer. See Figure 1 in the Supplementary Material for the corresponding representation from PCA on the continuous expression data, as well as for a legend of the disease types.\nFigure 4: Real-world example of factors of TensOrMachine decomposition. Colours indicate posterior means."
  }, {
    "heading": "9 (+) LGG, PCPG, LAML,GBM, SKCM, HNSC RAP1 SIGNALLING",
    "text": ""
  }, {
    "heading": "5.3. Networks of Relative Gene Expression in Cancer",
    "text": "Gene expression profiling has been used extensively in cancer research, e.g. for the characterisation of cancer subtypes, for the stratification of patients or for the identification of therapeutic targets. Correlations in gene expression are at the basis of protein interaction in cellular pathways and latent variable models are frequently applied to extract biologically meaningful information from such data. We propose a novel way of analysing gene expression data that can be applied to any continuous measurement with object×attribute structure (here patient×gene). The publicly available TCGA dataset (Weinstein et al., 2013) contains gene expression measurements of a large variety of cancer patients across different types of cancer. After preprocessing, we are left with approximately 8,000 patients and 1,100 genes. We normalise the expression values for each gene across all patients to allow for comparison between genes. Then the data is transformed into a 3-way tensor using the relational encoding\n(patient, genei, genej) =  1; if expri > exprj 0; if expri < exprj unobserved; else .\n(8)\nThis provides an entirely new view of the data in terms of networks of relative expression. For every latent dimension, the two gene specific factors indicate a subset of genes, where genes in the first subset are likely to be more expressed than genes in the second subset. The patient factor indicates which of these relational expression properties\neach patient exhibits. Importantly, the factors are amenable to distinct and intuitive interpretation with immediate biological relevance. We show the patient-specific factor in Fig. 4(c) with patients ordered by cancer-type. We observe a remarkable disease specificity with many latent expression networks being exclusively present in virtually all cancers of certain types. In addition, some latent properties are scattered throughout cancers of various types, highlighting the heterogeneity of the disease. Application of a traditional method, principal component analysis (PCA) on the continuous patient×gene array, is shown in the Supplementary Material C. It lacks both, the interpretability and the degree of disease-type specificity. We use the representations of both methods as features for random forest classifiers. The predictive accuracy of the disease type is approximately 91% for the binary TensOrM features, 86% for continuous features from PCA and 81% for binary dbtf features, shown in see SI (C, D).\nThe corresponding gene-specific factors characterise latent gene-networks but are more difficult to analyse for a nonspecialist. In particular, the richness of the relational latent encoding can only partially be captured by traditional pathway analyses. Nevertheless, we investigate the biological plausibility of the inferred gene sets by running a gene set enrichment analysis of the genes in each factor dimension against the KEGG pathway database (Kanehisa et al., 2017). Results are indicated in Tab. 1 and highlight the biological plausibility and significance of our analysis. We underline a few examples in the following. Firstly, Hippo signalling has been shown to be active in LUAD, LUSC, COAD, OV and LIHC (Harvey et al., 2013), all recovered by Dims. 4, 5. Secondly, aberrant Wnt signalling is observed in many cancers but most prominently in COAD (Polakis, 2012) which is found in Dim. 5. Next, PRAD progression is known to be controlled by focal adhesion (Figel & Gelman, 2011) as found in Dim. 15 as well as by the AGE-RAGE signalling pathway recovered in Dim. 7 (Bao et al., 2015). Finally, Ras signalling is aberrant in most tumours, but most significantly in PAAD (Downward, 2003) as confirmed in dims. 18, 24."
  }, {
    "heading": "6. Conclusion",
    "text": "We have introduced the first probabilistic approach to Boolean tensor decomposition. It reaches state-of-the-art performance and can readily deal with missing data which is ubiquitous in large datasets. Due to the particular combinatorial structure of the factor matrix posterior, sampling based inference scales to large datasets and enables the inference of posterior distributions over factor matrices, providing full uncertainty quantification. We further show that Boolean tensor decomposition leads to insightful latent representations and provides a novel view on the molecular basis of cancer by relationally encoding continuous expression data."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. CY is supported by MRC grant MR/P02646X/1. TR is funded by EPSRC grant EP/G037280/1. TR thanks Taha Ceritli for helpful comments."
  }, {
    "heading": "A. Derivation of the conditionals",
    "text": "Here we derive the full conditional distribution for a factor entry fnkl as given in eq. (4) in the main paper. For constant priors, p(fnkl) = const., the conditional is given by normalising the likelihood for fnkl ∈ {0, 1}. The likelihood has the form\np(x[n]|.) = σ λx̃[n] 1− 2∏\nl 1− ∏ n∈[n] fnl  (9)\nand is factorial in the data-points x[n]. Terms that do not depend on fnkl cancel in the conditional and thus we take the product over all [n] with nk fixed. Then normalising gives\np(fnkl = 1|.) =\n∏ [n], nkfixed\np(x[n]|fnkl = 1, rest)∏ [n], nkfixed p(x[n]|fnkl = 1, rest) + ∏ [n], nkfixed p(x[n]|fnkl = 0, rest)\n(10)\n=σ  ∑ [n]\nnkfixed\nlog p(x[n]|fnkl = 1, rest) p(x[n]|fnkl = 0, rest)\n . (11)\nConsidering the term inside the logarithm in eq. (11) and using eq. (9) we find\np(x[n]|fnkl = 1, rest) p(x[n]|fnkl = 0, rest) =  1; if ( ∏ n∈[n]/nk fnl ) ∏ l′ 6=l ( 1− ∏ n∈[n] fnl′ ) = 0 1+exp(λx̃[n])\n1+exp(−λx̃[n]) = eλx̃[n] ; otherwise .\n(12)\nThe first equality describes all cases where the term inside the parenthesis in eq. (9) takes a value that is independent of the value of fnkl. The second term follows in all other cases and by expanding the logistic sigmoid. Hence, we can write eq. (11) as\np(fnkl = 1|.) = σ λ ∑ [n]\nnkfixed\nx̃[n] M(nk,l)→[n]︷ ︸︸ ︷( ∏ n∈[n]/nk fnl )∏ l′ 6=l ( 1− ∏ n∈[n] fnl′ ) . (13)"
  }, {
    "heading": "B. Cancer-type legend",
    "text": "• BLCA: Bladder Urothelial Carcinoma • BRCA: Breast invasive carcinoma • CESC: Cervical squamous cell carcinoma and endocer-\nvical adenocarcinoma • COAD: Colon adenocarcinoma • ESCA: Esophageal carcinoma • GBM: Glioblastoma multiforme • HNSC: Head and Neck squamous cell carcinoma • KIRC: Kidney renal clear cell carcinoma • KIRP: Kidney renal papillary cell carcinoma • LAML: Acute Myeloid Leukemia • LGG: Brain Lower Grade Glioma\n• LIHC: Liver hepatocellular carcinoma • LUAD: Lung adenocarcinoma • LUSC: Lung squamous cell carcinoma • OV: Ovarian serous cystadenocarcinoma • PAAD: Pancreatic adenocarcinoma • PCPG: Pheochromocytoma and Paraganglioma • PRAD: Prostate adenocarcinoma • SARC: Sarcoma • SKCM: Skin Cutaneous Melanoma • STAD: Stomach adenocarcinoma • TGCT: Testicular Germ Cell Tumours • THCA: Thyroid carcinoma"
  }, {
    "heading": "C. Principal component analysis of gene expression data",
    "text": ""
  }, {
    "heading": "D. DBTF on Gene Expression Data",
    "text": "Fig. 6 shows patient representation for relative gene expression networks computed by dbtf. The analysis is limited to 350 genes, since our 32 core, 128 GB machine runs out of memory for larger analyses using the implementation provided by Park et al. [2017]. Data that was treated as missing in our original analysis is treated as 0 here. Comparing to Fig. 4(c) we observe similar but noisier disease-specific patterns."
  }, {
    "heading": "E. Real World Data Analysed with dbtf",
    "text": "Fig. 7 shows the results of dbtf, corresponding to the analysis in Fig. 4(b) of the main paper. While this solution looks similar to a possible point estimate of the TensOrM analysis, dbtf lack the ability to characterise posterior uncertainty which is crucial in this example. For the hospital data, dbtf infers only 3 latent dimensions. These correspond to dimensions 2, 5, and 7 of the TensOrM analysis. Note, that we can not assess the predictive performance since dbtf is unable to deal with held-out data"
  }],
  "year": 2018,
  "references": [{
    "title": "AGE/RAGE/Akt pathway contributes to prostate cancer cell proliferation by promoting Rb phosphorylation and degradation",
    "authors": ["J.M. Bao", "M.Y. He", "Y.W. Liu", "Y.J. Lu", "Y.Q. Hong", "H.H. Luo", "Z.L. Ren", "S.C. Zhao", "Y. Jiang"],
    "venue": "Am J Cancer Res,",
    "year": 2015
  }, {
    "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
    "authors": ["Chaudhari", "Pratik", "Choromanska", "Anna", "Soatto", "Stefano", "LeCun", "Yann"],
    "venue": "arXiv preprint arXiv:1611.01838,",
    "year": 2016
  }, {
    "title": "Targeting ras signalling pathways in cancer therapy",
    "authors": ["Downward", "Julian"],
    "venue": "Nature Reviews Cancer,",
    "year": 2003
  }, {
    "title": "Discovering facts with boolean tensor tucker decomposition",
    "authors": ["Erdos", "Dóra", "Miettinen", "Pauli"],
    "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,",
    "year": 2013
  }, {
    "title": "Walk’n’merge: A scalable algorithm for boolean tensor factorization",
    "authors": ["Erdos", "Dóra", "Miettinen", "Pauli"],
    "venue": "In Data Mining (ICDM),",
    "year": 2013
  }, {
    "title": "Focal adhesion kinase controls prostate cancer progression via intrinsic kinase and scaffolding functions",
    "authors": ["S. Figel", "I.H. Gelman"],
    "venue": "Anticancer Agents Med Chem,",
    "year": 2011
  }, {
    "title": "The hippo pathway and human cancer",
    "authors": ["Harvey", "Kieran F", "Zhang", "Xiaomeng", "Thomas", "David M"],
    "venue": "Nature reviews Cancer,",
    "year": 2013
  }, {
    "title": "KEGG: new perspectives on genomes, pathways, diseases and drugs",
    "authors": ["M. Kanehisa", "M. Furumichi", "M. Tanabe", "Y. Sato", "K. Morishima"],
    "venue": "Nucleic Acids Res.,",
    "year": 2017
  }, {
    "title": "Tensor decompositions and applications",
    "authors": ["Kolda", "Tamara G", "Bader", "Brett W"],
    "venue": "SIAM REVIEW,",
    "year": 2009
  }, {
    "title": "Hierarchical Compositional Feature Learning",
    "authors": ["Lázaro-Gredilla", "Miguel", "Liu", "Yi", "Phoenix", "D Scott", "George", "Dileep"],
    "venue": "arXiv preprint arXiv:1611.02252,",
    "year": 2016
  }, {
    "title": "Indclas: A three-way hierarchical classes model",
    "authors": ["Leenen", "Iwin", "Van Mechelen", "Iven", "De Boeck", "Paul", "Rosenberg", "Seymour"],
    "year": 1999
  }, {
    "title": "Nurse-doctor interactions during critical care ward rounds",
    "authors": ["E. Manias", "A. Street"],
    "venue": "J Clin Nurs,",
    "year": 2001
  }, {
    "title": "Clustering boolean tensors",
    "authors": ["Metzler", "Saskia", "Miettinen", "Pauli"],
    "venue": "Data Mining and Knowledge Discovery,",
    "year": 2015
  }, {
    "title": "Boolean tensor factorizations",
    "authors": ["Miettinen", "Pauli"],
    "venue": "In Data Mining (ICDM),",
    "year": 2011
  }, {
    "title": "The Discrete Basis Problem, pp. 335–346",
    "authors": ["Miettinen", "Pauli", "Mielikäinen", "Taneli", "Gionis", "Aristides", "Das", "Gautam", "Mannila", "Heikki"],
    "venue": "ISBN 978-3-540-46048-0",
    "year": 2006
  }, {
    "title": "Fast and scalable distributed boolean tensor factorization",
    "authors": ["Park", "Namyong", "Oh", "Sejoon", "U. Kang"],
    "year": 2017
  }, {
    "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
    "authors": ["Pearl", "Judea"],
    "year": 1988
  }, {
    "title": "Wnt signaling in cancer",
    "authors": ["Polakis", "Paul"],
    "venue": "Cold Spring Harbor perspectives in biology,",
    "year": 2012
  }, {
    "title": "Boolean Matrix Factorization and Noisy Completion via Message Passing",
    "authors": ["Ravanbakhsh", "Siamak", "Póczos", "Barnabás", "Greiner", "Russell"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Bayesian Boolean Matrix Factorisation",
    "authors": ["Rukat", "Tammo", "Holmes", "Chris C", "Titsias", "Michalis K", "Yau", "Christopher"],
    "venue": "Proceedings of the 34th Annual International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "A Survey of Collaborative Filtering Techniques",
    "authors": ["Su", "Xiaoyuan", "Khoshgoftaar", "Taghi M"],
    "venue": "Adv. in Artif. Intell.,",
    "year": 2009
  }, {
    "title": "Estimating potential infection transmission routes in hospital wards using wearable proximity sensors",
    "authors": ["©gis", "Corinne", "Kim", "Byeul-a", "Comte", "Brigitte", "Voirin", "Nicolas"],
    "venue": "PLoS ONE, 8 (9):e73970,",
    "year": 2013
  }, {
    "title": "DBTF on Gene Expression Data Fig. 6 shows patient representation for relative gene expression networks computed by dbtf. The analysis is limited to 350 genes, since our 32 core, 128 GB machine runs out of memory for larger analyses using the implementation provided by Park et al. [2017",
    "authors": ["D. TensOrMachine"],
    "year": 2017
  }],
  "id": "SP:8ebafcfba95b79bb25c2e0a467bb9ad1947ec9aa",
  "authors": [{
    "name": "Tammo Rukat",
    "affiliations": []
  }, {
    "name": "Chris C. Holmes",
    "affiliations": []
  }, {
    "name": "Christopher Yau",
    "affiliations": []
  }],
  "abstractText": "Boolean tensor decomposition approximates data of multi-way binary relationships as product of interpretable low-rank binary factors, following the rules of Boolean algebra. Here, we present its first probabilistic treatment. We facilitate scalable sampling-based posterior inference by exploitation of the combinatorial structure of the factor conditionals. Maximum a posteriori decompositions feature higher accuracies than existing techniques throughout a wide range of simulated conditions. Moreover, the probabilistic approach facilitates the treatment of missing data and enables model selection with much greater accuracy. We investigate three real-world data-sets. First, temporal interaction networks in a hospital ward and behavioural data of university students demonstrate the inference of instructive latent patterns. Next, we decompose a tensor with more than 10 billion data points, indicating relations of gene expression in cancer patients. Not only does this demonstrate scalability, it also provides an entirely novel perspective on relational properties of continuous data and, in the present example, on the molecular heterogeneity of cancer. Our implementation is available on GitHub2.",
  "title": "TensOrMachine: Probabilistic Boolean Tensor Decomposition"
}