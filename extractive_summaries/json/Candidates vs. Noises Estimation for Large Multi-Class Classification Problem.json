{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In practice one often encounters multi-class classification problem with a large number of classes. For example, applications in image classification (Russakovsky et al., 2015) and language modeling (Mikolov et al., 2010) usually have tens to hundreds of thousands of classes. Under such cases, training the standard softmax logistic or one-against-all models becomes impractical.\nOne promising way to handle the large class size is to use sampling. In language models, a commonly adopted technique is Noise-Contrastive Estimation (NCE) (Gutmann & Hyvärinen, 2012). This method is originally proposed\n1Tencent AI Lab, Shenzhen, China. Correspondence to: Lei Han <lxhan@tencent.com>, Yiheng Huang <arnoldhuang@tencent.com>, Tong Zhang <tongzhang@tongzhangml.org>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfor estimating probability densities and has been applied to various language modeling situations, such as learning word embeddings, context generation and neural machine translation (Mnih & Teh, 2012; Mnih & Kavukcuoglu, 2013; Vaswani et al., 2013; Sordoni et al., 2015). NCE reduces the problem of multi-class classification to binary classification problem, which discriminates between a target class distribution and a noise distribution and a few noise classes are sampled as a representation of the entire noise space. In general, the noise distribution is given a priori. For example, a power-raised unigram distribution has been shown to be effective in language models (Mikolov et al., 2013; Ji et al., 2015; Mnih & Teh, 2012). Recently, some variants of NCE have been proposed. The Negative Sampling (Mikolov et al., 2013) is a simplified version of NCE that ignores the numerical probabilities in the distributions and discriminates between only the target class and noise samples; the One vs. Each (Titsias, 2016) solves a very similar problem motivated by bounding the softmax logistic log-likelihood. Two other variants, BlackOut (Ji et al., 2015) and complementary sum sampling (Botev et al., 2017), employ parametric forms of the noise distribution and use sampled noises to approximate the normalization factor. In summary, NCE and its variants use (only) the observed class versus the noises; by sampling the noises, these methods avoid the costly computation of the normalization factor to achieve fast training speed. In this paper, we will generalize the idea by using a subset of classes (which can be automatically learned), called candidate classes, against the remaining noise classes. Compared to NCE, this approach can significantly improve the statistical efficiency when the true class belongs to the candidate classes with high probability.\nAnother type of popular methods for large class space is the tree structured classifier (Beygelzimer et al., 2009; Bengio et al., 2010; Deng et al., 2011; Choromanska & Langford, 2015; Daume III et al., 2017; Jernite et al., 2017). In these methods, a tree structure is defined over the classes which are treated as leaves. Each internal node of the tree is assigned with a local classifier, routing the examples to one of its descendants. Decisions are made from the root until reaching a leaf. Then, the multi-class classification problem is reduced to solving a number of small local models defined by a tree, which typically admits a logarithmic complexity on the total number of classes. Generally, tree classifiers\ngain training and prediction speed while suffering a loss of accuracy. The performance of tree classifier may rely heavily on the quality of the tree (Mnih & Hinton, 2009). Earlier approaches use fixed tree, such as the Filter Tree (Beygelzimer et al., 2009) and the Hierarchical Softmax (HSM) (Morin & Bengio, 2005). Recent methods are able to adjust the tree and learn the local classifiers simultaneously, such as the LOMTree (Choromanska & Langford, 2015) and Recall Tree (Daume III et al., 2017). Our approach is complementary to these tree classifiers, because we study the orthogonal issue of consistent class sampling, which in principle can be combined with many of these tree methods. In fact, a tree structure will be used in our approach to select a small subset of candidate classes. Since we focus on the class sampling aspect, we do not necessarily employ the best tree construction method in our experiments.\nIn this paper, we propose a method to efficiently deal with the large class problem by paying attention to a small subset of candidate classes instead of the entire class space. Given a data point x (without observing y), we select a small number of competitive candidates as a set Cx. Then, we sample the remaining classes, which are treated as noises, to represent the entire noise space in the large normalization factor. The estimation is referred to as Candidates vs. Noises Estimation (CANE). We show that CANE is consistent and its computation using stochastic gradient method is independent of the class size K. Moreover, the statistical variance of the CANE estimator can approach that of the maximum likelihood estimator (MLE) of the softmax logistic regression when Cx can cover the target class y with high probability. This statistical efficiency is a key advantage of CANE over NCE, and its effect can be observed in practice.\nWe then describe two concrete algorithms: the first one is a generic stochastic optimization procedure for CANE; the second one employs a tree structure with leaves as classes to enable fast beam search for candidate selection. We also apply CANE to solve the word probability estimation problem in neural language modeling. Experimental results conducted on both classification and neural language modeling problems show that CANE achieves significant speedup compared to the standard softmax logistic regression. Moreover, it achieves superior performance over NCE, its variants, and a number of the state-of-the-art tree classifiers."
  }, {
    "heading": "2. Candidates vs. Noises Estimation",
    "text": "Consider a K-class classification problem (K is large) with n training examples (xi, yi)|ni=1, where xi is from an input space X and yi ∈ {1, · · · ,K}. The softmax logistic regression solves\nmax θ\n1\nn n∑ i=1 K∑ k=1 I(yi = k) log esk(xi,θ)∑K k′=1 e sk′ (xi,θ) , (1)\nwhere sk(x,θ) for k = 1, · · · ,K is a model parameterized by θ. Solving Eq. (1) requires computing a score for every class and the summation in the normalization factor, which is very expensive when K is large.\nGenerally speaking, given x, only a small number of classes in the entire class space might be competitive to the true class. Therefore, we propose to find a small subset of classes as a candidate set Cx ⊂ {1, · · · ,K} and treat the classes outside Cx as noises, so that we can focus on the small set Cx instead of the entire K classes. We will discuss one way to choose Cx in Section 4. Denote the remaining K − |Cx| noises as a set Nx, so Nx is the complementary set of Cx. We propose to sample some noise class j ∈ Nx to represent the entire Nx. That is, we replace the partial summation ∑ j∈Nx e\nsj(x,θ) in the denominator of Eq. (1) by esj(x,θ)/qx(j) using some sampled class j with an arbitrary sampling probability qx(j), where qx(j) ∈ (0, 1) and∑ j∈Nx qx(j) = 1. Thus, the denominator ∑K k′=1 e sk′ (x,θ)\nwill be approximated as ∑ k′∈Cx e\nsk′ (x,θ) + esj(x,θ)/qx(j). Given example (x, y) and its candidate set Cx, if y ∈ Cx, then for some sampled noise class j, we will focus on maximizing the approximated probability\nesy(x,θ)∑ k′∈Cx e sk′ (x,θ) + esj(x,θ)/qx(j) ; (2)\notherwise, if y 6∈ Cx, we maximize\nesy(x,θ)∑ k′∈Cx e sk′ (x,θ) + esy(x,θ)/qx(y) (3)\nalternatively, where y is treated as the sampled noise in place. Now, with Eqs. (2) and (3), in expectation, we will need to solve the following objective:\nmaximize R(θ) =\nEx [ ∑ k∈Cx p(y = k|x) ∑ j∈Nx qx(j) log esk(x,θ)∑\nk′∈Cx e sk′ (x,θ)+ e\nsj(x,θ) qx(j)\n+ ∑\nk∈Nx\np(y = k|x) log esk(x,θ)∑\nk′∈Cx e sk′ (x,θ) + e\nsk(x,θ) qx(k)\n] , (4)\nand empirically, we will need to solve\nmaximize R̂n(θ) =\n1\nn n∑ i=1 [ I(yi ∈ Cxi ) ∑ j∈Nxi qxi (j) log esyi (xi,θ)∑ k′∈Cxi esk′ (xi,θ)+ e sj(xi,θ) qxi (j)\n+ I(yi /∈ Cxi ) log esyi (xi,θ)∑\nk′∈Cxi esk′ (xi,θ) + e\nsyi (xi,θ) qxi (yi)\n] . (5)\nEq. (5) consists of two summations over both the data points and the classes in the noise set Nx. Therefore, we can employ a ‘doubly’ stochastic gradient optimization method by\nsampling both data points i ∈ {1, . . . , n} and noise classes j ∈ Nxi . It is not difficult to check that each stochastic gradient is bounded under reasonable conditions, which means that the computational cost for solving (5) using stochastic gradient is independent of the class number K. Since we only choose a small number of candidates in Cx, the computation for each stochastic gradient in Eq. (5) is efficient. The above method is referred to as Candidates vs. Noises Estimation (CANE)."
  }, {
    "heading": "3. Properties",
    "text": "In this section, we investigate the statistical properties of CANE. The parameter space of the softmax logistic model in Eq. (1) has redundancy, observing that adding any function h(x) to sk(x,θ) for k = 1, · · · ,K will not change the objective. Similar situation happens for Eqs. (4) and (5). To avoid this redundancy, one can add some constraints on the K scores or simply fix one of them as zero, e.g., let sK(x,θ) = 0. To facilitate the analysis, we will fix sK(x,θ) = 0 and consider Cx ∪ Nx = {1, · · · ,K − 1} within this section. First, we have the following result.\nTheorem 1 (Infinity-Sample Consistency). By viewing the objective R as a function of {s1, · · · , sK−1}, R achieves its maximum if and only if sk = log p(y=k|x) p(y=K|x) for k = 1, · · · ,K − 1.\nIn Theorem 1, the global optima is exactly the log-odds function with class K as the reference class. Now, considering the parametric form sk(x,θ), there exists a true parameter θ∗ so that sk(x,θ∗) = log p(y=k|x) p(y=K|x) if the model sk(x,θ) is correctly specified. The following theorem shows that the CANE estimator θ̂ = arg maxθ R̂n(θ) is consistent with the true parameter θ∗.\nTheorem 2 (Finite-Sample Asymptotic Consistency). Given x, denote Cx as {i1, · · · , i|Cx|} andNx as {j1, · · · , j|Nx|}. Suppose that the parameter space is compact and ∀θ 6= θ∗ such that PX (sk(x,θ) 6= sk(x,θ∗)) > 0 for x ∼ X , k 6= K. Assume ‖∇θsk(x,θ)‖, ‖∇2θsk(x,θ)‖ and ‖∇3θsk(x,θ)‖ for k 6= K are bounded under some norm ‖ · ‖ defined on the parameter space of θ. Then, as n→∞, the estimator θ̂ converges to θ∗.\nThe above theorem shows that similar to the maximum likelihood estimator of Eq. (1), the CANE estimator in Eq. (5) is also consistent. Next, we have the asymptotic normality for θ̂ as follows.\nTheorem 3 (Asymptotic Normality). Under the same assumption used in Theorem 2, as n → ∞, √ n(θ̂ − θ∗) follows the asymptotic normal distribution:\n√ n(θ̂ − θ∗) d−→ N(0, [Ex∇M∇>]−1), (6)\nwhere\nM = ∑\nj∈Nx\nqx(j) [ diag (uj)−\n1 p(K,x) + ∑\nk∈Cx p(k,x) + p(j,x) qx(j)\nuju > j\n] ,\nuj = ( p(i1,x), · · · , p(i|Cx|,x)︸ ︷︷ ︸\nThe candidate part\n, 0, · · · , p(j,x)/qx(j), · · · , 0︸ ︷︷ ︸ The noise part\n)> ,\nfor j = j1, · · · , j|Nx|, ∇ = diag ([ ∇θsi1 (x,θ), · · · ,∇θsi|Cx| (x,θ),∇θsj1 (x,θ),\n· · · ,∇θsj|Nx| (x,θ) ]>) .\nTheorem 3 shows that the CANE method has a statistical variance of [Ex∇M∇>]−1. As we will see in the next corollary, if one can successfully choose the candidate set Cx so that it covers the observed label y with high probability, then the difference between the statistical variance of CANE and that of Eq. (1) is small. Therefore, choosing a good candidate set can be important for practical applications. Moreover, under standard conditions, the computation of CANE using stochastic gradient is independent of the class size K because the variance of stochastic gradient is bounded. Corollary 1 (Low Statistical Variance). The variance of the maximum likelihood estimator for the softmax logistic regression in Eq. (1) has the form [Ex∇Mmle∇>]−1. If∑ k∈Cx∪{K} p(k,x) → 1, i.e., the probability that Cx ∪ {K} covers the observed class label y approaches 1, then\n[Ex∇M∇>]−1 → [Ex∇Mmle∇>]−1."
  }, {
    "heading": "4. Algorithm",
    "text": "In this section, we propose two algorithms. The first one is a general optimization procedure for CANE. The second implementation provides an efficient way to select a competitive set Cx using a tree structure defined on the classes."
  }, {
    "heading": "4.1. A General Optimization Algorithm",
    "text": "Eq. (5) suggests an efficient algorithm using a ‘doubly’ stochastic gradient descend (SGD) method by sampling both the data points and classes. That is, by sampling a data point (x, y), we find the candidate set Cx ⊂ {1, · · · ,K}. If y ∈ Cx, we sample Nn noises from Nx according to qx and denote the selected noises as a set Tx (|Tx| = Nn). We then optimize\n1 |Tx| ∑ j∈Tx log esy(x,θ)∑ k′∈Cx e sk′ (x,θ) + esj(x,θ)/qx(j) ,\nwith gradient∇θR̂ given by Eq. (7). Otherwise, if y 6∈ Cx, we optimize\nlog esy(x,θ)∑\nk′∈Cx e sk′ (x,θ) + esy(x,θ)/qx(y)\n,\nAlgorithm 1 A general optimization procedure for CANE. 1: Input: K, (xi, yi)|ni=1, number of candidates Nc = |Cx|,\nnumber of sampled noises Nn = |Tx|, sampling strategy q and learning rate η.\n2: Output: θ̂.\n3: Initialize θ; 4: for every sampled example do 5: Receive example (x, y); 6: Find the candidate set Cx; 7: if y ∈ Cx then 8: Sample Nn noises outside Cx according to q and denote the selected noise set as Tx; 9: θ ← θ + η∇θR̂ with∇θR̂ given by\n∇θsy(x,θ)− 1 |Tx| ∑ j∈Tx\n(7)\n∑k′∈Cx esk′ (x,θ)∇θsk′(x,θ) + esj(x,θ)qx(j) ∇θsj(x,θ)∑ k′∈Cx e sk′ (x,θ) + e sj(x,θ) qx(j)  ; 10: else 11: θ ← θ + η∇θR̂ with∇θR̂ given by\n∇θsy(x,θ)−∑ k′∈Cx e sk′ (x,θ)∇θsk′(x,θ) + e sy(x,θ)\nqx(y) ∇θsy(x,θ)∑\nk′∈Cx e sk′ (x,θ) + e\nsy(x,θ) qx(y)\n; (8)\n12: end if 13: end for\nwith gradient∇θR̂ given by Eq. (8). This general procedure is provided in Algorithm 1. Algorithm 1 has a complexity of O(Nc +Nn) (where Nc = |Cx|), which is independent of the class size K. In step 6, any method can be used to select Cx."
  }, {
    "heading": "4.2. Beam Tree Algorithm",
    "text": "In the second algorithm, we provide an efficient way to find a competitive Cx. An attractive strategy is to use a tree defined on the classes, because one can perform fast heuristic search algorithms based on a tree structure to prune the uncompetitive classes. Indeed, any structure, e.g., graph or groups, can be used alternatively as long as the structure allows to efficiently prune uncompetitive classes. We will use tree structure for candidate selection in this paper.\nGiven a tree structure defined on the K classes, the model sk(x,θ) is interpreted as a tree model illustrated in Fig. 1. For simplicity, Fig. 1 uses a binary tree over K = 8 labels as example while any tree structure can be used for selecting Cx. In the example, circles denote internal nodes and squares indicate classes. The parameters are kept in the edges and denoted as θ(o,c), where o indicates an internal node and c is the index of the c-th child of node o. Therefore, a pair (o, c) represents an edge from node o to its c-th child. The dashed circles indicate that we do not keep any\nparameters in the internal nodes. Now, define sk(x,θ) as sk(x,θ) = gψ(x) · ∑\n(o,c)∈Pk\nθ(o,c), (9)\nwhere gψ(x) is a function parameterized by ψ and it maps the input x ∼ X to a representation gψ(x) ∈ Rdr for some dr. For example, in image classification, a good choice of the representation gψ(x) of the raw pixels x is usually a deep neural network. Pk denotes the path from the root to the class k. Eq. (9) implies that the score of an example belonging to a class is calculated by summing up the scores along the corresponding path. Now, in Fig. 1, suppose that we are given an example (x, y) with class y = 2 (blue). Using beam search, we find two candidates with high scores, i.e., class 1 (green) and class 2. Then, we let Cx = {1, 2}. In this case, we have y ∈ Cx, so we need to sample noises. Suppose we sample one class 6 (orange). According to Eq. (7), the parameters along the corresponding paths (red) will be updated.\nFormally, given example (x, y), if y ∈ Cx, we sample noises as a set Tx. Then for (o, c) ∈ PCx∪Tx , where PCx∪Tx = ∪k∈Cx∪TxPk, the gradient with respect to θ(o,c) is\n∂R̂\n∂θ(o,c) =\n1 |Tx| ∑ j∈Tx\n[ I ((o, c) ∈ Py)− (10)\n∑ k′∈Cx I((o, c) ∈ Pk′ )e sk′ (x,θ) + I((o, c) ∈ Pj) e sj(x,θ)\nqx(j)∑ k′∈Cx e sk′ (x,θ) + e sj(x,θ) qx(j)\n] gψ(x).\nNote that an edge may be included in multiple selected paths. For example, P1 and P2 share edges (1, 1) and (2, 1) in Fig. 1. The case of y 6∈ Cx can be illustrated similarly. The gradient with respect to θ(o,c) when y 6∈ Cx is\n∂R̂\n∂θ(o,c) =\n[ I ((o, c) ∈ Py)− (11)\n∑ k′∈Cx I((o, c) ∈ Pk′ )e sk′ (x,θ) + I((o, c) ∈ Py) e sy(x,θ)\nqx(y)∑ k′∈Cx e sk′ (x,θ) + e sy(x,θ) qx(y)\n] gψ(x).\nAlgorithm 2 The Beam Tree Algorithm. 1: Input: K, (xi, yi)|ni=1, representation function gψ(x), num-\nber of candidates Nc = |Cx|, number of sampled noises Nn = |Tx|, sampling strategy q and learning rate η.\n2: Output: θ̂.\n3: Construct a tree on the K classes; 4: Initialize θ; 5: for every sampled example do 6: Receive example (x, y); 7: Given x, use beam search to find the Nc classes with high scores to compose Cx; 8: if y ∈ Cx then 9: Sample Nn noises outside Cx according to q and denote\nthe selected noise set as Tx; 10: Find the paths with respect to the classes in Cx ∪ Tx; 11: else 12: Find the paths with respect to the classes in Cx ∪ {y}; 13: end if 14: Sum up the scores along each selected path for the corresponding class; 15: θ(o,c) ← θ(o,c) + η ∂R̂∂θ(o,c) for each (o, c) included in the selected paths according to Eqs. (10) and (11); 16: ψ ← ψ + η ∂R̂\n∂g ∂g ∂ψ ; // if g is parameterized. 17: end for\nThe gradients in Eqs. (10) and (11) enjoy the following property.\nProposition 1. At each iteration of Algorithm 2, if an edge (o, c) is included in every selected path, then θ(o,c) does not need to be updated.\nThe proof of Proposition 1 is straightforward that if (o, c) belongs to every selected path, then the gradients in Eqs. (10) and (11) are 0. The above property allows a fast detection of those parameters which do not need to be updated in SGD and hence can save computations. In practice, the number of shared edges is related to the tree structure.\nSince we use beam search to choose the candidates in a tree structure, the proposed algorithm is referred to as Beam Tree, which is depicted in Algorithm 2. 1 For the tree construction method in step 3, we can use some hierarchical clustering based methods which will be detailed in the experiments and supplementary material. In the algorithm, the beam search needs O (Nc logbK) operations, where b is a constant related to the tree structure, e.g., binary tree for b = 2. The parameter updating needsO((Nc+Nn) logbK) operations. Therefore, Algorithm 2 has a complexity of O((2Nc +Nn) logbK) which is logarithmic with respect to K. The term logbK is from the tree structure used in this specific candidate selection method, so it does not conflict with the complexity of the general Algorithm 1, which is independent of K. Another advantage of the Beam Tree\n1The beam search procedure in step 7 is provided in the supplementary material.\nalgorithm is that it allows fast predictions and can naturally output the top-J predictions using beam search. The prediction time has an order of O (J logbK) for the top-J predictions."
  }, {
    "heading": "5. Application to Neural Language Modeling",
    "text": "In this section, we apply the CANE method to neural language modeling which solves a probability density estimation problem. In neural language models, the conditional probability distribution of the target word w given context h is defined as\nPh(w) = esw(h,θ)∑\nw′∈V e sw′ (h,θ)\n,\nwhere sw(h,θ) is the scoring function with parameter θ. A wordw in the context hwill be represented by an embedding vector uw ∈ Rd with embedding size d. Given context h, the model computes the score for the target word w as\nsw(h,θ) = gϕ(uh)vw,\nwhere θ = {u,v,ϕ}, gϕ(·) is a representation function (parameterized by ϕ) of the embeddings in the context h, e.g., a LSTM modular (Hochreiter & Schmidhuber, 1997), and vw is the weight parameter for the target word w. Both the word embedding u and weight parameter v need to be estimated. In language models, the vocabulary size |V| is usually very large and the computation of the normalization factor is expensive. Therefore, instead of estimating the exact probability distribution Ph(w), sampling methods such as NCE and its variants (Mnih & Kavukcuoglu, 2013; Ji et al., 2015) are typically adopted to approximate Ph(w).\nIn order to apply the CANE method, we need to select the candidates given any context h. For multi-class classification problem, we have devised a Beam Tree algorithm in Algorithm 2 that uses a tree structure to select candidates, and the tree can be obtained by some hierarchical clustering methods over x before learning. However, different from the classification problem, the word embeddings in the language model are not known before training, and thus obtaining a hierarchical structure based on the word embeddings is not practical. In this paper, we construct a simple tree with only one layer under the root, where the layer contains N subsets formed by splitting the words according to their frequencies. At each iteration of Algorithm 2, we route the example by selecting the subset with the largest score (in place of beam search) and then sample the candidates from the subset according to some distribution. For the noises in CANE, we directly sample words out of the candidate set according to q. Other methods can be used to select the candidates alternatively, for example, one can choose candidates conditioned on the context h using a lightly pre-trained N-gram model."
  }, {
    "heading": "6. Related Algorithms",
    "text": "We provide a discussion comparing CANE with the existing techniques for solving the large class space problem. Given (x, y), NCE and its variants (Gutmann & Hyvärinen, 2012; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013; Ji et al., 2015; Titsias, 2016; Botev et al., 2017) use the observed class y as the only ‘candidate’, while CANE chooses a subset of candidates Cx according to x. NCE assumes the entire noise distribution Pnoise(y) is known (e.g., a power-raised unigram distribution). However, in general multi-class classification problems, when the knowledge of the noise distribution is absent, NCE may have unstable estimations using an inaccurate noise distribution. CANE is developed for general multi-class classification problems and does not rely on a known noise distribution. Instead, CANE focuses on a small candidate set Cx. Once the true class label is contained in Cx with high probability, CANE will have low statistical variance. The variants of NCE (Mikolov et al., 2013; Ji et al., 2015; Titsias, 2016; Botev et al., 2017) also sample one or multiple noises to replace the normalization factor while according theoretical guarantees on the consistency and variance are rarely discussed. NCE and its variants can not speed up prediction while the Beam Tree algorithm can reduce the prediction complexity to O(logK).\nThe Beam Tree algorithm is related to some tree classifiers, while CANE is a general procedure and we only use tree structure to select candidates. The Beam Tree method itself is also different from existing tree classifiers. Most of the state-of-the-art tree classifiers, e.g., LOMTree (Choromanska & Langford, 2015) and Recall Tree (Daume III et al., 2017), store local classifiers in their internal nodes, and route examples through the root until reaching the leaf. Differently, the Beam Tree algorithm shown in Fig. 1 does not maintain local classifiers, and it only uses the tree structure to perform global heuristic search for candidate selection. We will compare our approach to some state-of-the-art tree classifiers in the experiments."
  }, {
    "heading": "7. Experiments",
    "text": "We evaluate the CANE method in various applications in this section, including both multi-class classification problems and neural language modeling. We compare CANE with NCE, its variants and some state-of-the-art tree classifiers that have been used for large class space problems. The competitors include the standard softmax, the NCE (Mnih & Kavukcuoglu, 2013; Mnih & Teh, 2012), the BlackOut (Ji et al., 2015), the hierarchical softmax (HSM) (Morin & Bengio, 2005), the Filter Tree (Beygelzimer et al., 2009) implemented in Vowpal-Wabbit (VW, a learning platform)2,\n2https://github.com/JohnLangford/vowpal_ wabbit/wiki\nthe LOMTree (Choromanska & Langford, 2015) in VW and the Recall Tree (Daume III et al., 2017) in VW."
  }, {
    "heading": "7.1. Classification Problems",
    "text": "In this section, we consider four multi-class classification problems, including the Sector3 dataset with 105 classes (Chang & Lin, 2011), the ALOI4 dataset with 1000 classes (Geusebroek et al., 2005), the ImageNet-20105 dataset with 1000 classes, and the ImageNet-10K5 dataset with 10K classes (ImageNet Fall 2009 release). The data from Sector and ALOI is split into 90% training and 10% testing. In ImageNet-2010, the training set contains 1.3M images and we use the validation set containing 50K images as the test set. The ImageNet-10K data contains 9M images and we randomly split the data into two halves for training and testing by following the protocols in (Deng et al., 2010; Sánchez & Perronnin, 2011; Le, 2013). For ImageNet2010 and ImageNet-10K datasets, similar to (Oquab et al., 2014), we transfer the mid-level representations from the pre-trained VGG-16 net (Simonyan & Zisserman, 2014) on ImageNet 2012 data (Russakovsky et al., 2015) to our case. Then, we concatenate CANE or other compared methods above the partial VGG-16 net as the top layer. The parameters of the partial VGG-16 net are pre-trained6 and kept fixed. Only the parameters in the top layer are trained on the target datasets, i.e., ImageNet-2010 and ImageNet-10K.\nWe use b-nary tree for CANE and set b = 10 for all classification problems. We trade off |Cx| and |Tx| to see how these parameters affect the learning performance. Different configurations will be referred to as ‘CANE-(|Cx| vs. |Tx|)’. We always let |Cx|+ |Tx| equal the number of noises used by NCE and BlackOut, so that these methods will have the same number of considered classes. We use ‘NCE-k’ and ‘BlackOut-k’ to denote the corresponding method with k noises. Generally, a large |Cx|+ |Tx| and k will lower the variance of CANE, NCE and BlackOut and improve their performance, but this also increases the computation. We set k = 10 for Sector and ALOI and k = 20 for ImageNet-2010 and ImageNet-10K. We uniformly sample noises in CANE. For NCE and BlackOut, by following (Mnih & Teh, 2012; Mnih & Kavukcuoglu, 2013; Ji et al., 2015; Botev et al., 2017), we use the power-raised unigram distribution with the power factor selected from {0, 0.1, 0.3, 0.5, 0.75, 1} to sample the noises. However, when the classes are balanced as in many cases of the classification datasets, this distribution reduces to the uniform distribution. For the com-\n3http://www.cs.utexas.edu/˜xrhuang/ PDSparse/\n4http://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/multiclass.html\n5http://image-net.org 6http://www.robots.ox.ac.uk/˜vgg/\nresearch/very_deep/\npared tree classifiers, the HSM adopts the same tree used by CANE, the Filter Tree generates a fixed tree itself in VW, the LOMTree and Recall Tree use binary trees and they are able to adjust the tree structure automatically.\nAll the methods use SGD with learning rate selected from {0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0}. The Beam Tree algorithm requires a tree structure and we use some tree generated by a simple hierarchical clustering method on the centers of the individual classes.7 We run all the methods 50 epochs on Sector, ALOI and ImageNet-2010 datasets and 20 epochs on ImageNet-10K to report the accuracy vs. epoch curves. All the methods are implemented using a standard CPU machine with quad-core Intel Core i5 processor.\nFig. 2 and Table 1 show the accuracy vs. epoch plots and\n7The method is provided in the supplementary material.\nthe training / testing time for NCE, BlackOut, CANE and Softmax. The tree classifiers in the VW platform require the number of training epochs as input and do not take evaluation directly after each epoch, so we report the final results of the tree classifiers in Table 2. For ImageNet-10K data, the Softmax method is very time consuming (even with multithread implementation) and we do not report this result. As we can observe, by fixing |Cx| + |Tx|, using more candidates than noises in CANE will achieve better performance, because a larger Cx will increase the chance to cover the target class y. The probability that the target class is included in the selected candidate set on the test data is reported in Table 3. On all the datasets, CANE with larger candidate set achieves considerable improvement compared to other methods in terms of accuracy. The speed of processing each example of CANE is slightly slower than that of NCE and BlackOut because of beam search, however, CANE shows faster convergence to reach higher accuracy. Moreover, the prediction time of CANE is much faster than those of NCE and BlackOut. It is worth mentioning that CANE exceeds some state-of-the-art results on the ImageNet-10K data, e.g., 19.2% top-1 accuracy reported in (Le, 2013) and 21.9% top1 accuracy reported in (Mensink et al., 2013) which are conducted fromO(K) methods; but it underperforms the recent result 28.4% in (Huang et al., 2016). This is probably because the VGG-16 net works better than the neural network structure used in (Le, 2013) and the distance-based method in (Mensink et al., 2013), while the method in (Huang et al., 2016) adopts a better feature embedding, which leads to superior prediction performance on this dataset."
  }, {
    "heading": "1 68.89% 44.84% 5 76.59% 39.59%",
    "text": ""
  }, {
    "heading": "5 96.57% 86.47% 10 87.29% 53.28%",
    "text": ""
  }, {
    "heading": "9 97.92% 93.59% 15 91.17% 60.22%",
    "text": ""
  }, {
    "heading": "7.2. Neural Language Modeling",
    "text": "In this experiment, we apply the CANE method to neural language modeling. We test the methods on two benchmark corpora: the Penn TreeBank (PTB) (Mikolov et al., 2010) and Gutenberg8 corpora. The Penn TreeBank dataset contains 1M tokens and we choose the most frequent 12K words appearing at least 5 times as the vocabulary. The Gutenberg dataset contains 50M tokens and the most frequent 116K words appearing at least 10 times are chosen as the vocabulary. We set the embedding size as 256 and use a LSTM model with 512 hidden states and 256 projection size. The sequence length is fixed as 20 and the learning rate is selected from {0.025, 0.05, 0.1, 0.2}.\nThe tree classifiers evaluated in multi-class classification problems can not be directly applied to solve the language modeling problem, so we omit their comparison and focus on the evaluation of the sampling methods. We sample 40, 60 and 80 noises for NCE and Blackout respectively and use power-raised unigram distribution with the power factor selected from {0, 0.25, 0.5, 0.75, 1}. For CANE, we adopt the one-layer tree structure discussed in Section 5 with N = 6 subsets, split by averaging over the word frequencies. We uniformly sample the candidates when reaching any subset. For efficiency consideration, we respectively sample 40, 60 and 80 candidates plus one more uniform noise for CANE. The experiments in this section are implemented on a machine with NVIDIA Tesla M40 GPUs.\nThe test perplexities are shown in Fig. 3. As we can observe, the CANE method always achieves faster convergence and lower perplexities (approaching that of Softmax) compared to NCE and Blackout under various settings. Generally, when the number of selected candidates / noises decrease, the test perplexities of all the methods increase on both datasets, while the performance degradation of CANE is not obvious. By using GPUs, all the methods can finish training within a few minutes on the PTB dataset; for the Gutenberg corpus, CANE and BlackOut have similar training time that is around 5 hours on all the three settings, while NCE spends around 6-8 hours on these tasks and Softmax uses 35 hours to finish the training.\n8www.gutenberg.org"
  }, {
    "heading": "8. Conclusion",
    "text": "We proposed Candidates vs. Noises Estimation (CANE) for fast learning in multi-class classification problems with many labels and applied this method to the word probability estimation problem in neural language models. We showed that CANE is consistent and the computation using SGD is always efficient (that is, independent of the class size K). Moreover, the new estimator has low statistical variance approaching that of the softmax logistic regression, if the observed class label belongs to the candidate set with high probability. Empirical results demonstrated that CANE is effective for speeding up both training and prediction in multi-class classification problems and CANE is effective in neural language modeling. We note that this work employs a fixed distribution (i.e., the uniform distribution) to sample noises in CANE. However it can be very useful in practice to estimate the noise distribution, i.e., q, during training, and select noise classes according to this distribution."
  }],
  "year": 2018,
  "references": [{
    "title": "Label embedding trees for large multi-class tasks",
    "authors": ["S. Bengio", "J. Weston", "D. Grangier"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2010
  }, {
    "title": "Complementary sum sampling for likelihood approximation in large scale classification",
    "authors": ["A. Botev", "B. Zheng", "D. Barber"],
    "venue": "In Artificial Intelligence and Statistics (AISTATS),",
    "year": 2017
  }, {
    "title": "Libsvm: a library for support vector machines",
    "authors": ["Chang", "C.-C", "Lin", "C.-J"],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
    "year": 2011
  }, {
    "title": "Logarithmic time online multiclass prediction",
    "authors": ["A.E. Choromanska", "J. Langford"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Logarithmic time one-against-some",
    "authors": ["H. Daume III", "N. Karampatziakis", "J. Langford", "P. Mineiro"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "What does classifying more than 10,000 image categories tell us",
    "authors": ["J. Deng", "A.C. Berg", "K. Li", "L. Fei-Fei"],
    "venue": "In European Conference on Computer Vision (ECCV),",
    "year": 2010
  }, {
    "title": "Fast and balanced: Efficient label tree learning for large scale object recognition",
    "authors": ["J. Deng", "S. Satheesh", "A.C. Berg", "F. Li"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2011
  }, {
    "title": "The amsterdam library of object images",
    "authors": ["Geusebroek", "J.-M", "G.J. Burghouts", "A.W. Smeulders"],
    "venue": "International Journal of Computer Vision (IJCV),",
    "year": 2005
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2012
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural Computation,",
    "year": 1997
  }, {
    "title": "Local similarity-aware deep feature embedding",
    "authors": ["C. Huang", "C.C. Loy", "X. Tang"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Simultaneous learning of trees and representations for extreme classification and density estimation",
    "authors": ["Y. Jernite", "A. Choromanska", "D. Sontag"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies",
    "authors": ["S. Ji", "S. Vishwanathan", "N. Satish", "M.J. Anderson", "P. Dubey"],
    "venue": "arXiv preprint arXiv:1511.06909,",
    "year": 2015
  }, {
    "title": "Building high-level features using large scale unsupervised learning",
    "authors": ["Q.V. Le"],
    "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2013
  }, {
    "title": "Distance-based image classification: Generalizing to new classes at near-zero cost",
    "authors": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),",
    "year": 2013
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["T. Mikolov", "M. Karafiát", "L. Burget", "J. Černockỳ", "S. Khudanpur"],
    "venue": "In Eleventh Annual Conference of the International Speech Communication Association,",
    "year": 2010
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "A scalable hierarchical distributed language model",
    "authors": ["A. Mnih", "G.E. Hinton"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2009
  }, {
    "title": "Learning word embeddings efficiently with noise-contrastive estimation",
    "authors": ["A. Mnih", "K. Kavukcuoglu"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "A fast and simple algorithm for training neural probabilistic language models",
    "authors": ["A. Mnih", "Y.W. Teh"],
    "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),",
    "year": 2012
  }, {
    "title": "Hierarchical probabilistic neural network language model",
    "authors": ["F. Morin", "Y. Bengio"],
    "venue": "In The International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2005
  }, {
    "title": "Learning and transferring mid-level image representations using convolutional neural networks",
    "authors": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"],
    "venue": "In Proceedings of The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2014
  }, {
    "title": "Imagenet large scale visual recognition challenge",
    "authors": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"],
    "venue": "International Journal of Computer Vision (IJCV),",
    "year": 2015
  }, {
    "title": "High-dimensional signature compression for large-scale image classification",
    "authors": ["J. Sánchez", "F. Perronnin"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2011
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "Nie", "J.-Y", "J. Gao", "B. Dolan"],
    "venue": "arXiv preprint arXiv:1506.06714,",
    "year": 2015
  }, {
    "title": "One-vs-each approximation to softmax for scalable estimation of probabilities",
    "authors": ["M.K. Titsias"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Decoding with large-scale neural language models improves translation",
    "authors": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"],
    "venue": "In The Conference on Empirical Methods on Natural Language Processing (EMNLP),",
    "year": 2013
  }],
  "id": "SP:133e4dc1ebaab927f76d4b71b0b76b82c2fe054b",
  "authors": [{
    "name": "Lei Han",
    "affiliations": []
  }, {
    "name": "Yiheng Huang",
    "affiliations": []
  }, {
    "name": "Tong Zhang",
    "affiliations": []
  }],
  "abstractText": "This paper proposes a method for multi-class classification problems, where the number of classes K is large. The method, referred to as Candidates vs. Noises Estimation (CANE), selects a small subset of candidate classes and samples the remaining classes. We show that CANE is always consistent and computationally efficient. Moreover, the resulting estimator has low statistical variance approaching that of the maximum likelihood estimator, when the observed label belongs to the selected candidates with high probability. In practice, we use a tree structure with leaves as classes to promote fast beam search for candidate selection. We further apply the CANE method to estimate word probabilities in learning large neural language models. Extensive experimental results show that CANE achieves better prediction accuracy over the Noise-Contrastive Estimation (NCE), its variants and a number of the state-ofthe-art tree classifiers, while it gains significant speedup compared to standard O(K) methods.",
  "title": "Candidates vs. Noises Estimation for Large Multi-Class Classification Problem"
}