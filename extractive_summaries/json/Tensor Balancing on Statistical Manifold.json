{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Matrix balancing is the problem of rescaling a given square nonnegative matrix A ∈ Rn×n≥0 to a doubly stochastic matrix RAS, where every row and column sums to one, by multiplying two diagonal matrices R and S. This is a fundamental process for analyzing and comparing matrices in a wide range of applications, including input-output analysis in economics, called the RAS approach (Parikh, 1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004), seat assignments in elections (Balinski, 2008; Akartunalı &\n1National Institute of Informatics 2JST PRESTO 3RIKEN Brain Science Institute 4Graduate School of Frontier Sciences, The University of Tokyo 5RIKEN AIP 6NIMS. Correspondence to: Mahito Sugiyama <mahito@nii.ac.jp>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nEvery ber sums to 1\nGiven tensor A\nMultistochastic tensor A’ Submanifold (β)\nProbability distribution P\nStatistical manifold (dually at Riemannian manifold)\nProjectionTensor balancing\nProjected distribution Pβ\nFigure 1. Overview of our approach.\nKnight, 2016), Hi-C data analysis (Rao et al., 2014; Wu & Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and the optimal transportation problem (Cuturi, 2013; Frogner et al., 2015; Solomon et al., 2015). An excellent review of this theory and its applications is given by Idel (2016).\nThe standard matrix balancing algorithm is the SinkhornKnopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp, 1967; Marshall & Olkin, 1968; Knight, 2008), a special case of Bregman’s balancing method (Lamond & Stewart, 1981) that iterates rescaling of each row and column until convergence. The algorithm is widely used in the above applications due to its simple implementation and theoretically guaranteed convergence. However, the algorithm converges linearly (Soules, 1991), which is prohibitively slow for recently emerging large and sparse matrices. Although Livne & Golub (2004) and Knight & Ruiz (2013) tried to achieve faster convergence by approximating each step of Newton’s method, the exact Newton’s method with quadratic convergence has not been intensively studied yet.\nAnother open problem is tensor balancing, which is a generalization of balancing from matrices to higher-order multidimentional arrays, or tensors. The task is to rescale an N th order nonnegative tensor to a multistochastic tensor, in which every fiber sums to one, by multiplying (N −1)th order N tensors. There are some results about mathematical properties of multistochastic tensors (Cui et al., 2014; Chang et al., 2016; Ahmed et al., 2003). However, there is no result for tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now.\nHere we show that Newton’s method with quadratic convergence can be applied to tensor balancing while avoiding solving a linear system on the full tensor. Our strategy is to realize matrix and tensor balancing as projection onto a dually flat Riemmanian submanifold (Figure 1), which is a statistical manifold and known to be the essential structure for probability distributions in information geometry (Amari, 2016). Using a partially ordered outcome space, we generalize the log-linear model (Agresti, 2012) used to model the higher-order combinations of binary variables (Amari, 2001; Ganmor et al., 2011; Nakahara & Amari, 2002; Nakahara et al., 2003), which allows us to model tensors as probability distributions in the statistical manifold. The remarkable property of our model is that the gradient of the manifold can be analytically computed using the Möbius inversion formula (Rota, 1964), the heart of combinatorial mathematics (Ito, 1993), which enables us to directly obtain the Jacobian matrix in Newton’s method. Moreover, we show that (n − 1)N entries for the size nN of a tensor are invariant with respect to one of the two coordinate systems of the statistical manifold. Thus the number of equations in Newton’s method is O(nN−1).\nThe remainder of this paper is organized as follows: We begin with a low-level description of our matrix balancing algorithm in Section 2 and demonstrate its efficiency in numerical experiments in Section 3. To guarantee the correctness of the algorithm and extend it to tensor balancing, we provide theoretical analysis in Section 4. In Section 4.1, we introduce a generalized log-linear model associated with a partial order structured outcome space, followed by introducing the dually flat Riemannian structure in Section 4.2. In Section 4.3, we show how to use Newton’s method to compute projection of a probability distribution onto a submanifold. Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6."
  }, {
    "heading": "2. The Matrix Balancing Algorithm",
    "text": "Given a nonnegative square matrix A = (aij) ∈ Rn×n≥0 , the task of matrix balancing is to find r, s ∈ Rn that satisfy\n(RAS)1 = 1, (RAS)T1 = 1, (1)\nwhere R = diag(r) and S = diag(s). The balanced matrix A′ = RAS is called doubly stochastic, in which each entry a′ij = aijrisj and all the rows and columns sum to one. The most popular algorithm is the Sinkhorn-Knopp algorithm, which repeats updating r and s as r = 1/(As) and s = 1/(ATr). We denote by [n] = {1, 2, . . . , n} hereafter.\nIn our algorithm, instead of directly updating r and s, we update two parameters θ and η defined as\nlog pij = ∑ i′≤i ∑ j′≤j θi′j′ , ηij = ∑ i′≥i ∑ j′≥j pi′j′ (2)\nMatrix Constraints for balancing\nfor each i, j ∈ [n], where we normalized entries as pij = aij/ ∑ ij aij so that ∑ ij pij = 1. We assume for simplicity that each entry is strictly larger than zero. The assumption will be removed in Section 5.\nThe key to our approach is that we update θ(t)ij with i = 1 or j = 1 by Newton’s method at each iteration t = 1, 2, . . . while fixing θij with i, j ̸= 1 so that η(t)ij satisfies the following condition (Figure 2):\nη (t) i1 = (n− i+ 1)/n, η (t) 1j = (n− j + 1)/n.\nNote that the rows and columns sum not to 1 but to 1/n due to the normalization. The update formula is described as θ (t+1) 11 ... θ (t+1) 1n θ (t+1) 21\n... θ (t+1) n1\n =  θ (t) 11 ... θ (t) 1n\nθ (t) 21 ... θ (t) n1\n − J−1  η (t) 11 − (n− 1 + 1)/n ... η (t) 1n − (n− n+ 1)/n η (t) 21 − (n− 2 + 1)/n\n... η (t) n1 − (n− n+ 1)/n\n , (3)\nwhere J is the Jacobian matrix given as\nJ(ij)(i′j′)= ∂η\n(t) ij ∂θ (t) i′j′ = ηmax{i,i′}max{j,j′}−n2ηijηi′j′ , (4)\nwhich is derived from our theoretical result in Theorem 3. Since J is a (2n−1)×(2n−1) matrix, the time complexity of each update is O(n3), which is needed to compute the inverse of J .\nAfter updating to θ(t+1)ij , we can compute p (t+1) ij and η (t+1) ij by Equation (2). Since this update does not ensure the condition ∑ ij p (t+1) ij = 1, we again update θ (t+1) 11 as θ (t+1) 11 = θ (t+1) 11 − log ∑ ij p (t+1) ij and recompute p (t+1) ij and η(t+1)ij for each i, j ∈ [n].\nBy iterating the above update process in Equation (3) until convergence, A = (aij) with aij = npij becomes doubly stochastic."
  }, {
    "heading": "3. Numerical Experiments",
    "text": "We evaluate the efficiency of our algorithm compared to the two prominent balancing methods, the standard SinkhornKnopp algorithm (Sinkhorn, 1964) and the state-of-the-art\nalgorithm BNEWT (Knight & Ruiz, 2013), which uses Newton’s method-like iterations with conjugate gradients. All experiments were conducted on Amazon Linux AMI release 2016.09 with a single core of 2.3 GHz Intel Xeon CPU E5-2686 v4 and 256 GB of memory. All methods were implemented in C++ with the Eigen library and compiled with gcc 4.8.31. We have carefully implemented BNEWT by directly translating the MATLAB code provided in (Knight & Ruiz, 2013) into C++ with the Eigen library for fair comparison, and used the default parameters. We measured the residual of a matrix A′ = (a′ij) by the squared norm ∥(A′1−1, A′T1−1)∥2, where each entry a′ij is obtained as npij in our algorithm, and ran each of three algorithms until the residual is below the tolerance threshold 10−6.\nHessenberg Matrix. The first set of experiments used a Hessenberg matrix, which has been a standard benchmark for matrix balancing (Parlett & Landis, 1982; Knight & Ruiz, 2013). Each entry of an n × n Hessenberg matrix Hn = (hij) is given as hij = 0 if j < i − 1 and hij = 1 otherwise. We varied the size n from 10 to 5, 000, and measured running time (in seconds) and the number of iterations of each method.\nResults are plotted in Figure 3. Our balancing algorithm with the Newton’s method (plotted in blue in the figures)\n1An implementation of algorithms for matrices and third order tensors is available at: https://github.com/ mahito-sugiyama/newton-balancing\nis clearly the fastest: It is three to five orders of magnitude faster than the standard Sinkhorn-Knopp algorithm (plotted in red). Although the BNEWT algorithm (plotted in green) is competitive if n is small, it suddenly fails to converge whenever n ≥ 200, which is consistent with results in the original paper (Knight & Ruiz, 2013) where there is no result for the setting n ≥ 200 on the same matrix. Moreover, our method converges around 10 to 20 steps, which is about three and seven orders of magnitude smaller than BNEWT and Sinkhorn-Knopp, respectively, at n = 100.\nTo see the behavior of the rate of convergence in detail, we plot the convergence graph in Figure 4 for n = 20, where we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT algorithm, which contrasts with our quick convergence.\nTrefethen Matrix. Next, we collected a set of Trefethen matrices from a collection website2, which are nonnegative diagonal matrices with primes. Results are plotted in Figure 5, where we observe the same trend as before: Our algorithm is the fastest and about four orders of magnitude faster than the Sinkhorn-Knopp algorithm. Note that larger matrices with n > 300 do not have total support, which is the necessary condition for matrix balancing (Knight & Ruiz, 2013), while the BNEWT algorithm fails to converge if n = 200 or n = 300."
  }, {
    "heading": "4. Theoretical Analysis",
    "text": "In the following, we provide theoretical support to our algorithm by formulating the problem as a projection within a statistical manifold, in which a matrix corresponds to an element, that is, a probability distribution, in the manifold.\nWe show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold, where the Jacobian matrix in Equation (4) is derived from the gradient of the manifold.\n2http://www.cise.ufl.edu/research/sparse/ matrices/"
  }, {
    "heading": "4.1. Formulation",
    "text": "We introduce our log-linear probabilistic model, where the outcome space is a partially ordered set, or a poset (Gierz et al., 2003). We prepare basic notations and the key mathematical tool for posets, the Möbius inversion formula, followed by formulating the log-linear model."
  }, {
    "heading": "4.1.1. MÖBIUS INVERSION",
    "text": "A poset (S,≤), the set of elements S and a partial order ≤ on S, is a fundamental structured space in computer science. A partial order “≤” is a relation between elements in S that satisfies the following three properties: For all x, y, z ∈ S, (1) x ≤ x (reflexivity), (2) x ≤ y, y ≤ x ⇒ x = y (antisymmetry), and (3) x ≤ y, y ≤ z ⇒ x ≤ z (transitivity). In what follows, S is always finite and includes the least element (bottom) ⊥ ∈ S; that is, ⊥ ≤ x for all x ∈ S. We denote S \\ {⊥} by S+.\nRota (1964) introduced the Möbius inversion formula on posets by generalizing the inclusion-exclusion principle. Let ζ :S × S → {0, 1} be the zeta function defined as\nζ(s, x) = { 1 if s ≤ x, 0 otherwise.\nThe Möbius function µ :S×S → Z satisfies ζµ = I , which is inductively defined for all x, y with x ≤ y as\nµ(x, y) =  1 if x = y, − ∑\nx≤s<y µ(x, s) if x < y, 0 otherwise.\nFrom the definition, it follows that∑ s∈S ζ(s, y)µ(x, s) = ∑ x≤s≤y\nµ(x, s) = δxy,∑ s∈S ζ(x, s)µ(s, y) = ∑ x≤s≤y µ(s, y) = δxy (5)\nwith the Kronecker delta δ such that δxy = 1 if x = y and δxy = 0 otherwise. Then for any functions f , g, and h with the domain S such that\ng(x) = ∑ s∈S ζ(s, x)f(s) = ∑ s≤x f(s),\nh(x) = ∑ s∈S ζ(x, s)f(s) = ∑ s≥x f(s),\nf is uniquely recovered with the Möbius function: f(x) = ∑ s∈S µ(s, x)g(s), f(x) = ∑ s∈S µ(x, s)h(s).\nThis is called the Möbius inversion formula and is at the heart of enumerative combinatorics (Ito, 1993)."
  }, {
    "heading": "4.1.2. LOG-LINEAR MODEL ON POSETS",
    "text": "We consider a probability vector p on (S,≤) that gives a discrete probability distribution with the outcome space S.\nA probability vector is treated as a mapping p :S → (0, 1) such that ∑ x∈S p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.\nUsing the zeta and the Möbius functions, let us introduce two mappings θ :S → R and η :S → R as\nθ(x) = ∑ s∈S µ(s, x) log p(s), (6)\nη(x) = ∑ s∈S ζ(x, s)p(s) = ∑ s≥x p(s). (7)\nFrom the Möbius inversion formula, we have log p(x) = ∑ s∈S ζ(s, x)θ(s) = ∑ s≤x θ(s), (8)\np(x) = ∑ s∈S µ(x, s)η(s). (9)\nThey are generalization of the log-linear model (Agresti, 2012) that gives the probability p(x) of an n-dimensional binary vector x = (x1, . . . , xn) ∈ {0, 1}n as\nlog p(x) = ∑ i θixi + ∑ i<j θijxixj + ∑ i<j<k θijkxixjxk\n+ · · ·+ θ1...nx1x2 . . . xn − ψ,\nwhere θ = (θ1, . . . , θ12...n) is a parameter vector, ψ is a normalizer, and η = (η1, . . . , η12...n) represents the expectation of variable combinations such that\nηi = E[xi] = Pr(xi = 1),\nηij = E[xixj ] = Pr(xi = xj = 1), i < j, . . .\nη1...n = E[x1 . . . xn] = Pr(x1 = · · · = xn = 1).\nThey coincide with Equations (8) and (7) when we let S = 2V with V = {1, 2, . . . , n}, each x ∈ S as the set of indices of “1” of x, and the order ≤ as the inclusion relationship, that is, x ≤ y if and only if x ⊆ y. Nakahara et al. (2006) have pointed out that θ can be computed from p using the inclusion-exclusion principle in the log-linear model. We exploit this combinatorial property of the loglinear model using the Möbius inversion formula on posets and extend the log-linear model from the power set 2V to any kind of posets (S,≤). Sugiyama et al. (2016) studied a relevant log-linear model, but the relationship with Möbius inversion formula has not been analyzed yet."
  }, {
    "heading": "4.2. Dually Flat Riemannian Manifold",
    "text": "We theoretically analyze our log-linear model introduced in Equations (6), (7) and show that they form dual coordinate systems on a dually flat manifold, which has been mainly studied in the area of information geometry (Amari, 2001; Nakahara & Amari, 2002; Amari, 2014; 2016). Moreover, we show that the Riemannian metric and connection of our model can be analytically computed in closed forms.\nIn the following, we denote by ξ the function θ or η and by ∇ the gradient operator with respect to S+ = S \\{⊥}, i.e., (∇f(ξ))(x) = ∂f/∂ξ(x) for x ∈ S+, and denote by S the set of probability distributions specified by probability vectors, which forms a statistical manifold. We use uppercase letters P,Q,R, . . . for points (distributions) in S and their lowercase letters p, q, r, . . . for the corresponding probability vectors treated as mappings. We write θP and ηP if they are connected with p by Equations (6) and (7), respectively, and abbreviate subscripts if there is no ambiguity."
  }, {
    "heading": "4.2.1. DUALLY FLAT STRUCTURE",
    "text": "We show that S has the dually flat Riemannian structure induced by two functions θ and η in Equation (6) and (7). We define ψ(θ) as\nψ(θ) = −θ(⊥) = − log p(⊥), (10)\nwhich corresponds to the normalizer of p. It is a convex function since we have\nψ(θ) = log ∑ x∈S exp  ∑ ⊥<s≤x θ(s)  from log p(x) = ∑ ⊥<s≤x θ(s)−ψ(θ). We apply the Legendre transformation to ψ(θ) given as\nφ(η) = max θ′\n( θ′η − ψ(θ′) ) , θ′η = ∑ x∈S+ θ′(x)η(x). (11)\nThen φ(η) coincides with the negative entropy.\nTheorem 1 (Legendre dual). φ(η) = ∑ x∈S p(x) log p(x).\nProof. From Equation (5), we have\nθ′η = ∑ x∈S+  ∑ ⊥<s≤x µ(s, x) log p′(s) ∑ s≥x p(s)  = ∑ x∈S+ p(x) ( log p′(x)− log p′(⊥) ) .\nThus it holds that θ′η − ψ(θ′) = ∑ x∈S p(x) log p′(x). (12)\nHence it is maximized with p(x) = p′(x).\nSince they are connected with each other by the Legendre transformation, they form a dual coordinate system ∇ψ(θ) and ∇φ(η) of S (Amari, 2016, Section 1.5), which coincides with θ and η as follows.\nTheorem 2 (dual coordinate system).\n∇ψ(θ) = η, ∇φ(η) = θ. (13)\nProof. They can be directly derived from our definitions (Equations (6) and (11)) as\n∂ψ(θ) ∂θ(x) =\n∑ y≥x exp (∑ ⊥<s≤y θ(s) ) ∑\ny∈S exp (∑ ⊥<s≤y θ(s) ) =∑ s≥x p(s) = η(x),\n∂φ(η) ∂η(x) = ∂ ∂η(x)\n( θη − ψ(θ) ) = θ(x).\nMoreover, we can confirm the orthogonality of θ and η as\nE\n[ ∂ log p(s)\n∂θ(x)\n∂ log p(s)\n∂η(y) ] = ∑ s∈S ζ(x, s)µ(s, y) = δxy.\nThe last equation holds from Equation (5), hence the Möbius inversion directly leads to the orthogonality.\nThe Bregman divergence is known to be the canonical divergence (Amari, 2016, Section 6.6) to measure the difference between two distributions P and Q on a dually flat manifold, which is defined as\nD [P,Q] = ψ(θP ) + φ(ηQ)− θP ηQ. In our case, since we have φ(ηQ) = ∑\nx∈S q(x) log q(x) and θP ηQ−ψ(θP ) = ∑ x∈S q(x) log p(x) from Theorem 1 and Equation (12), it is given as\nD [P,Q] = ∑ x∈S q(x) log q(x) p(x) ,\nwhich coincides with the Kullback–Leibler divergence (KL divergence) from Q to P : D [P,Q] = DKL [Q,P ]."
  }, {
    "heading": "4.2.2. RIEMANNIAN STRUCTURE",
    "text": "Next we analyze the Riemannian structure on S and show that the Möbius inversion formula enables us to compute the Riemannian metric of S. Theorem 3 (Riemannian metric). The manifold (S, g(ξ)) is a Riemannian manifold with the Riemannian metric g(ξ) such that for all x, y ∈ S+\ngxy(ξ) =  ∑ s∈S [ ζ(x, s)ζ(y, s)p(s)− η(x)η(y) ] if ξ = θ,\n∑ s∈S µ(s, x)µ(s, y)p(s)−1 if ξ = η.\nProof. Since the Riemannian metric is defined as\ng(θ) = ∇∇ψ(θ), g(η) = ∇∇φ(η),\nwhen ξ = θ we have\ngxy(θ) = ∂2\n∂θ(x)∂θ(y) ψ(θ) =\n∂\n∂θ(x) η(y)\n= ∂\n∂θ(x) ∑ s∈S ζ(y, s) exp  ∑ ⊥<u≤s θ(u)− ψ(θ) \n= ∑ s∈S ζ(x, s)ζ(y, s)p(s)− |S|η(x)η(y).\nWhen ξ = η, it follows that\ngxy(η) = ∂2\n∂η(x)∂η(y) φ(η) =\n∂\n∂η(x) θ(y)\n= ∂\n∂η(x) ∑ s≤y µ(s, y) log ∑ u≥s µ(s, u)η(u)  = ∑ s∈S µ(s, x)µ(s, y)p(s)−1.\nSince g(ξ) coincides with the Fisher information matrix,\nE\n[ ∂\n∂θ(x) log p(s)\n∂\n∂θ(y) log p(s)\n] = gxy(θ),\nE\n[ ∂\n∂η(x) log p(s)\n∂\n∂η(y) log p(s)\n] = gxy(η).\nThen the Riemannian (Levi–Chivita) connection Γ(ξ) with respect to ξ, which is defined as\nΓxyz(ξ) = 1\n2\n( ∂gyz(ξ)\n∂ξ(x) + ∂gxz(ξ) ∂ξ(y) − ∂gxy(ξ) ∂ξ(z) ) for all x, y, z ∈ S+, can be analytically obtained. Theorem 4 (Riemannian connection). The Riemannian connection Γ(ξ) on the manifold (S, g(ξ)) is given in the following for all x, y, z ∈ S+,\nΓxyz(ξ) =  1 2 ∑ s∈S ( ζ(x, s)− η(x) )( ζ(y, s)− η(y) ) ( ζ(z, s)− η(z) ) p(s) if ξ = θ, −1 2 ∑ s∈S µ(s, x)µ(s, y)µ(s, z)p(s)−2 if ξ = η.\nProof. Connections Γxyz(θ) and Γxyz(η) can be obtained by directly computing ∂gyz(θ)/∂θ(x) and ∂gyz(η)/∂η(x), respectively."
  }, {
    "heading": "4.3. The Projection Algorithm",
    "text": "Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model (Amari, 2016). Here we define projection of distributions on posets and show that Newton’s method can be applied to perform projection as the Jacobian matrix can be analytically computed."
  }, {
    "heading": "4.3.1. DEFINITION",
    "text": "Let S(β) be a submanifold of S such that\nS(β) = {P ∈ S | θP (x) = β(x), ∀x ∈ dom(β)} (14)\nspecified by a function β with dom(β) ⊆ S+. Projection of P ∈ S onto S(β), calledm-projection, which is defined as the distribution Pβ ∈ S(β) such that{\nθPβ (x) = β(x) if x ∈ dom(β), ηPβ (x) = ηP (x) if x ∈ S+ \\ dom(β),\nis the minimizer of the KL divergence from P to S(β):\nPβ = argmin Q∈S(β) DKL[P,Q].\nThe dually flat structure with the coordinate systems θ and η guarantees that the projected distribution Pβ always exists and is unique (Amari, 2009, Theorem 3). Moreover, the Pythagorean theorem holds in the dually flat manifold, that is, for any Q ∈ S(β) we have\nDKL[P,Q] = DKL[P, Pβ ] +DKL[Pβ , Q].\nWe can switch η and θ in the submanifold S(β) by changing DKL[P,Q] to DKL[Q,P ], where the projected distribution Pβ of P is given as{\nθPβ (x) = θP (x) if x ∈ S+ \\ dom(β), ηPβ (x) = β(x) if x ∈ dom(β),\nThis projection is called e-projection.\nExample 1 (Boltzmann machine). Given a Boltzmann machine represented as an undirected graph G = (V,E) with a vertex set V and an edge set E ⊆ {{i, j} | i, j ∈ V }. The set of probability distributions that can be modeled by a Boltzmann machine G coincides with the submanifold\nSB = {P ∈ S | θP (x) = 0 if |x| > 2 or x ̸∈ E},\nwith S = 2V . Let P̂ be an empirical distribution estimated from a given dataset. The learned model is the mprojection of the empirical distribution P̂ onto SB, where the resulting distribution Pβ is given as{\nθPβ (x) = 0 if |x| > 2 or x ̸∈ E, ηPβ (x) = ηP̂ (x) if |x| = 1 or x ∈ E."
  }, {
    "heading": "4.3.2. COMPUTATION",
    "text": "Here we show how to compute projection of a given probability distribution. We show that Newton’s method can be used to efficiently compute the projected distribution Pβ by iteratively updating P (0)β = P as P (0) β , P (1) β , P (2) β , . . . until converging to Pβ .\nLet us start with the m-projection with initializing P (0)β = P . In each iteration t, we update θ(t)Pβ (x) for all x ∈ domβ while fixing η(t)Pβ (x) = ηP (x) for all x ∈ S\n+ \\ dom(β), which is possible from the orthogonality of θ and η. Using Newton’s method, η(t+1)Pβ (x) should satisfy( θ (t) Pβ (x)− β(x) ) + ∑\ny∈dom(β)\nJxy ( η (t+1) Pβ (y)− η(t)Pβ (y) ) = 0,\nfor every x ∈ dom(β), where Jxy is an entry of the |dom(β)| × |dom(β)| Jacobian matrix J and given as\nJxy = ∂θ\n(t) Pβ (x)\n∂η (t) Pβ (y) = ∑ s∈S µ(s, x)µ(s, y)p (t) β (s) −1\nfrom Theorem 3. Therefore, we have the update formula for all x ∈ dom(β) as\nη (t+1) Pβ (x) = η (t) Pβ\n(x)− ∑\ny∈dom(β)\nJ−1xy ( θ (t) Pβ (y)− β(y) ) .\nIn e-projection, update η(t)Pβ (x) for x ∈ dom(β) while fixing θ(t)Pβ (x) = θP (x) for all x ∈ S\n+ \\ dom(β). To ensure η (t) Pβ\n(⊥) = 1, we add ⊥ to dom(β) and β(⊥) = 1. We update θ(t)Pβ (x) at each step t as\nθ (t+1) Pβ (x) = θ (t) Pβ\n(x)− ∑\ny∈dom(β)\nJ ′ −1 xy ( η (t) Pβ (y)− β(y) ) ,\nJ ′xy = ∂η\n(t) Pβ (x)\n∂θ (t) Pβ (y) = ∑ s∈S ζ(x, s)ζ(y, s)p (t) β (s)\n− |S|η(t)Pβ (x)η (t) Pβ (y).\nIn this case, we also need to update θ(t)Pβ (⊥) as it is not guaranteed to be fixed. Let us define\np ′(t+1) β (x) = p (t) β (x) ∏ s∈dom(β)\nexp ( θ (t+1) Pβ (s) )\nexp ( θ (t) Pβ (s) ) ζ(s, x).\nSince we have\np (t+1) β (x) =\nexp ( θ (t+1) Pβ (⊥) )\nexp ( θ (t) Pβ (⊥) ) p′(t+1)β (x),\nit follows that\nθ (t+1) Pβ (⊥)− θ(t)Pβ (⊥)\n= − log ( exp ( θ (t) Pβ (⊥) ) + ∑ x∈S+ p ′(t+1) β (x) ) ,\nThe time complexity of each iteration is O(|dom(β)|3), which is required to compute the inverse of the Jacobian matrix.\nGlobal convergence of the projection algorithm is always guaranteed by the convexity of a submanifold S(β) defined in Equation (14). Since S(β) is always convex with respect to the θ- and η-coordinates, it is straightforward to see that our e-projection is an instance of the Bregman algorithm onto a convex region, which is well known to always converge to the global solution (Censor & Lent, 1981)."
  }, {
    "heading": "5. Balancing Matrices and Tensors",
    "text": "Now we are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold."
  }, {
    "heading": "5.1. Matrix Balancing",
    "text": "Recall that the task of matrix balancing is to find r, s ∈ Rn that satisfy (RAS)1 = 1 and (RAS)T1 = 1 with R = diag(r) and S = diag(s) for a given nonnegative square matrix A = (aij) ∈ Rn×n≥0 .\nLet us define S as S = {(i, j) | i, j ∈ [n] and aij ̸= 0}, (15)\nwhere we remove zero entries from the outcome space S as our formulation cannot treat zero probability, and give each probability as p((i, j)) = aij/ ∑ ij aij . The partial order ≤ of S is naturally introduced as x = (i, j) ≤ y = (k, l) ⇔ i ≤ j and k ≤ l, (16)\nresulting in ⊥ = (1, 1). In addition, we define ιk,m for each k ∈ [n] and m ∈ {1, 2} such that\nιk,m = min{x = (i1, i2) ∈ S | im = k }, where the minimum is with respect to the order ≤. If ιk,m does not exist, we just remove the entire kth row if m = 1 or kth column if m = 2 from A. Then we switch rows and columns of A so that the condition\nι1,m ≤ ι2,m ≤ · · · ≤ ιn,m (17) is satisfied for each m ∈ {1, 2}, which is possible for any matrices. Since we have\nη(ιk,m)− η(ιk+1,m) = { ∑n j=1 p((k, j)) if m = 1,∑n i=1 p((i, k)) if m = 2\nif the condition (17) is satisfied, the probability distribution is balanced if for all k ∈ [n] and m ∈ {1, 2}\nη(ιk,m) = n−k+1\nn .\nTherefore, we obtain the following result.\nMatrix balancing as e-projection: Given a matrix A ∈ Rn×n with its normalized probability distribution P ∈ S such that p((i, j)) = aij/ ∑ ij aij . Define the poset (S,≤) by Equations (15) and (16) and let S(β) be the submanifold of S such that S(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},\nwhere the function β is given as dom(β) = {ιk,m ∈ S | k ∈ [n],m ∈ {1, 2}},\nβ(ιk,m) = n−k+1\nn .\nMatrix balancing is the e-projection of P onto the submanifold S(β), that is, the balanced matrix (RAS)/n is the distribution Pβ such that{\nθPβ (x) = θP (x) if x ∈ S+ \\ dom(β), ηPβ (x) = β(x) if x ∈ dom(β),\nwhich is unique and always exists in S, thanks to its dually flat structure. Moreover, two balancing vectors r and s are\nexp\n( i∑\nk=1\nθPβ (ιk,m)− θP (ιk,m)\n) = { ri if m = 1, ai if m = 2,\nfor every i ∈ [n] and r = rn/ ∑\nij aij . ■"
  }, {
    "heading": "5.2. Tensor Balancing",
    "text": "Next, we generalize our approach from matrices to tensors. For anN th order tensorA = (ai1i2...iN ) ∈ Rn1×n2×···×nN and a vector b ∈ Rnm , the m-mode product of A and b is defined as\n(A×m b)i1...im−1im+1...iN = nm∑\nim=1\nai1i2...iN bim .\nWe define tensor balancing as follows: Given a tensor A ∈ Rn1×n2×···×nN with n1 = · · · = nN = n, find (N − 1) order tensors R1, R2, . . . , RN such that\nA′ ×m 1 = 1 (∈ Rn1×···×nm−1×nm+1×···×nN ) (18) for all m ∈ [N ], i.e., ∑n\nim=1 a′i1i2...iN = 1, where each\nentry a′i1i2...iN of the balanced tensor A ′ is given as a′i1i2...iN = ai1i2...iN ∏\nm∈[N ]\nRmi1...im−1im+1...iN .\nA tensor A′ that satisfies Equation (18) is called multistochastic (Cui et al., 2014). Note that this is exactly the same as the matrix balancing problem if N = 2.\nIt is straightforward to extend matrix balancing to tensor balancing as e-projection onto a submanifold. Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P such that\np(x) = ai1i2...iN / ∑ j1j2...jN aj1j2...jN (19)\nfor all x = (i1, i2, . . . , iN ). The objective is to obtain Pβ such that ∑n im=1 pβ((i1, . . . , iN )) = 1/(n N−1) for all m ∈ [N ] and i1, . . . , iN ∈ [n]. In the same way as matrix balancing, we define S as\nS = { (i1, i2, . . . , iN ) ∈ [n]N ∣∣ ai1i2...iN ̸= 0 } with removing zero entries and the partial order ≤ as\nx = (i1 . . . iN ) ≤ y = (j1 . . . jN ) ⇔ ∀m ∈ [N ], im ≤ jm.\nIn addition, we introduce ιk,m as\nιk,m = min{x = (i1, i2, . . . , iN ) ∈ S | im = k }.\nand require the condition in Equation (17).\nTensor balancing as e-projection: Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P ∈ S given in Equation (19). The submanifold S(β) of multistochastic tensors is given as\nS(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},\nwhere the domain of the function β is given as\ndom(β) = { ιk,m | k ∈ [n],m ∈ [N ] }\nand each value is described using the zeta function as β(ιk,m) = ∑ l∈[n] ζ(ιk,m, ιl,m) 1 nN−1 .\nTensor balancing is the e-projection of P onto the submanifold S(β), that is, the multistochastic tensor is the distribution Pβ such that{\nθPβ (x) = θP (x) if x ∈ S+ \\ dom(β), ηPβ (x) = β(x) if x ∈ dom(β),\nwhich is unique and always exists in S, thanks to its dually flat structure. Moreover, each balancing tensor Rm is\nRmi1...im−1im+1...iN\n= exp  ∑ m′ ̸=m im′∑ k=1 θPβ (ιk,m′)− θP (ιk,m′)  for every m ∈ [N ] and R1 = R1nN−1/ ∑ j1...jN\naj1...jN to recover a multistochastic tensor. ■ Our result means that the e-projection algorithm based on Newton’s method proposed in Section 4.3 converges to the unique balanced tensor whenever S(β) ̸= ∅ holds."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using Newton’s method. Our algorithm quadratically converges, while the popular Sinkhorn-Knopp algorithm linearly converges. We have examined the efficiency of our algorithm in numerical experiments on matrix balancing and showed that the proposed algorithm is several orders of magnitude faster than the existing approaches.\nWe have analyzed theories behind the algorithm, and proved that balancing is e-projection in a special type of a statistical manifold, in particular, a dually flat Riemannian manifold studied in information geometry. Our key finding is that the gradient of the manifold, equivalent to Riemannian metric or the Fisher information matrix, can be analytically obtained using the Möbius inversion formula.\nOur information geometric formulation can model several machine learning applications such as statistical analysis on a DAG structure. Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study in future work."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors sincerely thank Marco Cuturi for his valuable comments. This work was supported by JSPS KAKENHI Grant Numbers JP16K16115, JP16H02870 (MS), JP26120732 and JP16H06570 (HN). The research of K.T. was supported by JST CREST JPMJCR1502, RIKEN PostK, KAKENHI Nanostructure and KAKENHI JP15H05711."
  }],
  "year": 2017,
  "references": [{
    "title": "Categorical data analysis",
    "authors": ["A. Agresti"],
    "venue": "Wiley, 3 edition,",
    "year": 2012
  }, {
    "title": "Polyhedral Cones of Magic Cubes and Squares, volume 25 of Algorithms and Combinatorics",
    "authors": ["M. Ahmed", "J. De Loera", "R. Hemmecke"],
    "year": 2003
  }, {
    "title": "Network models and biproportional rounding for fair seat allocations in the UK elections",
    "authors": ["K. Akartunalı", "P.A. Knight"],
    "venue": "Annals of Operations Research,",
    "year": 2016
  }, {
    "title": "Information geometry on hierarchy of probability distributions",
    "authors": ["S. Amari"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2001
  }, {
    "title": "Information geometry and its applications: Convex function and dually flat manifold",
    "authors": ["S. Amari"],
    "venue": "ETVC",
    "year": 2008
  }, {
    "title": "Information geometry of positive measures and positive-definite matrices: Decomposable dually flat structure. Entropy",
    "authors": ["S. Amari"],
    "year": 2014
  }, {
    "title": "Information Geometry and Its Applications. Springer, 2016",
    "authors": ["S. Amari"],
    "year": 2016
  }, {
    "title": "Fair majority voting (or how to eliminate gerrymandering)",
    "authors": ["M. Balinski"],
    "venue": "American Mathematical Monthly,",
    "year": 2008
  }, {
    "title": "An iterative row-action method for interval convex programming",
    "authors": ["Y. Censor", "A. Lent"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 1981
  }, {
    "title": "Polytopes of stochastic tensors",
    "authors": ["H. Chang", "V.E. Paksoy", "F. Zhang"],
    "venue": "Annals of Functional Analysis,",
    "year": 2016
  }, {
    "title": "Birkhoff–von Neumann theorem for multistochastic tensors",
    "authors": ["Cui", "L.-B", "W. Li", "M.K. Ng"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2014
  }, {
    "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
    "authors": ["M. Cuturi"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Learning with a Wasserstein loss",
    "authors": ["C. Frogner", "C. Zhang", "H. Mobahi", "M. Araya", "T.A. Poggio"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "Continuous Lattices and Domains",
    "authors": ["G. Gierz", "K.H. Hofmann", "K. Keimel", "J.D. Lawson", "M. Mislove", "D.S. Scott"],
    "year": 2003
  }, {
    "title": "A review of matrix scaling and sinkhorn’s normal form for matrices and positive maps",
    "authors": ["M. Idel"],
    "year": 2016
  }, {
    "title": "ed.). Encyclopedic Dictionary of Mathematics",
    "authors": ["K. Ito"],
    "venue": "The MIT Press,",
    "year": 1993
  }, {
    "title": "The Sinkhorn–Knopp algorithm: Convergence and applications",
    "authors": ["P.A. Knight"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2008
  }, {
    "title": "A fast algorithm for matrix balancing",
    "authors": ["P.A. Knight", "D. Ruiz"],
    "venue": "IMA Journal of Numerical Analysis,",
    "year": 2013
  }, {
    "title": "Biproportional techniques in input-output analysis: Table updating and structural analysis",
    "authors": ["M. Lahr", "L. de Mesnard"],
    "venue": "Economic Systems Research,",
    "year": 2004
  }, {
    "title": "Bregman’s balancing method",
    "authors": ["B. Lamond", "N.F. Stewart"],
    "venue": "Transportation Research Part B: Methodological,",
    "year": 1981
  }, {
    "title": "Scaling by binormalization",
    "authors": ["O.E. Livne", "G.H. Golub"],
    "venue": "Numerical Algorithms,",
    "year": 2004
  }, {
    "title": "Scaling of matrices to achieve specified row and column sums",
    "authors": ["A.W. Marshall", "I. Olkin"],
    "venue": "Numerische Mathematik,",
    "year": 1968
  }, {
    "title": "Input-Output Analysis: Foundations and Extensions",
    "authors": ["R.E. Miller", "P.D. Blair"],
    "year": 2009
  }, {
    "title": "Sinkhorn solves sudoku",
    "authors": ["T.K. Moon", "J.H. Gunther", "J.J. Kupin"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2009
  }, {
    "title": "Information-geometric measure for neural spikes",
    "authors": ["H. Nakahara", "S. Amari"],
    "venue": "Neural Computation,",
    "year": 2002
  }, {
    "title": "Gene interaction in DNA microarray data is decomposed by information geometric measure",
    "authors": ["H. Nakahara", "S. Nishimura", "M. Inoue", "G. Hori", "S. Amari"],
    "year": 2003
  }, {
    "title": "A comparison of descriptive models of a single spike train by information-geometric measure",
    "authors": ["H. Nakahara", "S. Amari", "B.J. Richmond"],
    "venue": "Neural computation,",
    "year": 2006
  }, {
    "title": "Forecasts of input-output matrices using the R.A.S",
    "authors": ["A. Parikh"],
    "venue": "method. The Review of Economics and Statistics,",
    "year": 1979
  }, {
    "title": "Methods for scaling to doubly stochastic form",
    "authors": ["B.N. Parlett", "T.L. Landis"],
    "venue": "Linear Algebra and its Applications,",
    "year": 1982
  }, {
    "title": "On the foundations of combinatorial theory I: Theory of Möbius functions",
    "authors": ["Rota", "G.-C"],
    "venue": "Z. Wahrseheinlichkeitstheorie,",
    "year": 1964
  }, {
    "title": "A relationship between arbitrary positive matrices and doubly stochastic matrices",
    "authors": ["R. Sinkhorn"],
    "venue": "The Annals of Mathematical Statistics, 35(2):876–879,",
    "year": 1964
  }, {
    "title": "Concerning nonnegative matrices and doubly stochastic matrices",
    "authors": ["R. Sinkhorn", "P. Knopp"],
    "venue": "Pacific Journal of Mathematics,",
    "year": 1967
  }, {
    "title": "Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains",
    "authors": ["J. Solomon", "F. de Goes", "G. Peyré", "M. Cuturi", "A. Butscher", "A. Nguyen", "T. Du", "L. Guibas"],
    "venue": "ACM Transactions on Graphics,",
    "year": 2015
  }, {
    "title": "The rate of convergence of sinkhorn balancing",
    "authors": ["G.W. Soules"],
    "venue": "Linear Algebra and its Applications,",
    "year": 1991
  }, {
    "title": "Information decomposition on structured space",
    "authors": ["M. Sugiyama", "H. Nakahara", "K. Tsuda"],
    "venue": "IEEE International Symposium on Information Theory,",
    "year": 2016
  }, {
    "title": "A computational strategy to adjust for copy number in tumor",
    "authors": ["Wu", "H.-J", "F. Michor"],
    "venue": "Hi-C data. Bioinformatics,",
    "year": 2016
  }],
  "id": "SP:35703d3ec93f008ae1ba0da7dc044e4e208d0945",
  "authors": [{
    "name": "Mahito Sugiyama",
    "affiliations": []
  }, {
    "name": "Hiroyuki Nakahara",
    "affiliations": []
  }, {
    "name": "Koji Tsuda",
    "affiliations": []
  }],
  "abstractText": "We solve tensor balancing, rescaling an N th order nonnegative tensor by multiplying N tensors of order N −1 so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton’s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton’s method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.",
  "title": "Tensor Balancing on Statistical Manifold"
}