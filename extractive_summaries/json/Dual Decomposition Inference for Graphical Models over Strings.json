{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.\nThis is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation."
  }, {
    "heading": "1 Introduction",
    "text": "Graphical models allow expert modeling of complex relations and interactions between random variables. Since a graphical model with given parameters defines a probability distribution, it can be used to reconstruct values for unobserved variables. The marginal inference problem is to compute the posterior marginal distributions of these variables. The MAP inference (or MPE) problem is to compute the single highest-probability joint assignment to all the unobserved variables.\nInference in general graphical models is NPhard even when the variables’ values are finite discrete values such as categories, tags or domains. In this paper, we address the more challenging setting\n∗This material is based upon work supported by the National Science Foundation under Grant No. 1423276.\nwhere the variables in the graphical models range over strings. Thus, the domain of the variables is an infinite space of discrete structures.\nIn NLP, such graphical models can deal with large, incompletely observed lexicons. They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis.\nIn this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges. We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015). We show that the method generally converges and that it achieves better results than alternatives.\nThe rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion."
  }, {
    "heading": "2 Graphical Models Over Strings",
    "text": ""
  }, {
    "heading": "2.1 Factor Graphs and MAP Inference",
    "text": "To perform inference on a graphical model (directed or undirected), one first converts the model to a factor graph representation (Kschischang et al., 2001). A factor graph is a finite bipartite\n1In some task settings, it is also necessary to discover the model topology along with the model parameters. In this paper we do not treat that structure learning problem. However, both structure learning and parameter learning need to call inference—such as the method presented here—in order to evaluate proposed topologies or improve their parameters.\n917\ngraph over a set X = {X1, X2, . . .} of variables and a set F of factors. An assignment to the variables is a vector of values x = (x1, x2, . . .). Each factor F ∈ F is a real-valued function of x, but it depends on a given xi only if F is connected to Xi in the graph. Thus, a degree d-factor scores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors:\nscore(x) def= ∑ F∈F F (x). (1)\nWe seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def= (1/Z) exp score(x)."
  }, {
    "heading": "2.2 The String Case",
    "text": "Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical\n2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007). Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment.\nmodels have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015).\nThe variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3\nPast work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Côté et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015)."
  }, {
    "heading": "2.3 Finite-State Belief Propagation",
    "text": "BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable.\nMurphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari-\n3Finite-state software libraries often support only these cases. Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d > 2.\nables. In the string case, they have infinitely many rows and columns, indexed by possible strings.\nDreyer and Eisner (2009) represented these infinite vectors and matrices by WFSAs and WFSTs, respectively. They observed that the simple linear-algebra operations used by BP can be implemented by finite-state constructions. The pointwise product of two vectors is the intersection of their WFSAs; the marginalization of a matrix is the projection of its WFST; a vector-matrix product is computed by composing the WFSA with the WFST and then projecting onto the output tape. For degree > 2, BP’s rank-d tensors become dtape WFSMs, and these constructions generalize.\nUnfortunately, except in small acyclic models, the BP messages—which are WFSAs—usually become impractically large. Each intersection or composition involves a cross-product construction. For example, when finding the marginal distribution at a degree-d variable, intersecting d WFSA messages having m states each may yield a WFSA with up to md states. (Our models in section 6 include variables with d up to 156.) Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles (Dreyer and Eisner, 2009), as ours do.\nThe usual solution is to prune or otherwise approximate the messages at each step. In particular, Cotterell and Eisner (2015) gave a principled way to approximate the messages using variablelength n-gram models, using an adaptive variant of Expectation Propagation (Minka, 2001)."
  }, {
    "heading": "2.4 Dual Decomposition Inference",
    "text": "In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes. BP can solve each subproblem without approximation.4\nThe subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution. This information is encoded in WFSAs that score possible values of a string variable. DD incrementally adjusts the WFSAs so as to encourage values that agree with\n4Such small BP problems commonly arise in NLP. In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology.\nthe variable’s average value across subproblems. Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to Cotterell and Eisner (2015). They may still grow over time; but DD often halts while the WFSAs are still small. It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP."
  }, {
    "heading": "2.5 Switching Between Semirings",
    "text": "Our factors may be nondeterministic WFSMs. So when F ∈ F scores a given d-tuple of string values, it may accept that d-tuple along multiple different WFSM paths with different scores, corresponding to different alignments of the strings.\nFor purposes of MAP inference, we define F to return the maximum of these path scores. That is, we take the WFSMs to be defined with weights in the (max,+) semiring (Mohri et al., 2002). Equivalently, we are seeking the “best global solution” in the sense of choosing not only the strings xi but also the alignments of the d-tuples.5\nTo do so, we must solve each DD subproblem in the same sense. We use max-product BP. This still applies the Dreyer-Eisner method of section 2.3. Since these WFSMs are defined in the (max,+) semiring, the method’s finite-state operations will combine weights using max and +.\nMAP inference in our setting is in general computationally undecidable.6 However, if DD converges (as in our experiments), then its solution is guaranteed to be the true MAP assignment.\nIn section 6, we will compare DD with (loopy) max-product BP and (loopy) sum-product BP. These respectively approximate MAP inference and marginal inference over the entire factor graph. Marginal inference computes marginal string probabilities that sum (rather than maximize) over the choices of other strings and the choices of paths. Thus, for sum-product BP, we re-interpret the factor WFSMs as defined over the (logadd,+) semiring. This means that the exponentiated score assigned by a WFSM is the sum of the exponentiated scores of the accepting paths.\n5This problem is more specifically called MPE inference. 6The trouble is that we cannot bound the length of the latent strings. If we could, then we could encode them using a finite set of boolean variables, and solve as an ILP problem. But that would allow us to determine whether there exists a MAP assignment with score ≥ 0. That is impossible in general, because it would solve Post’s Correspondence Problem as a simple special case (see Dreyer and Eisner (2009))."
  }, {
    "heading": "3 A Sample Task: Generative Phonology",
    "text": "Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphical model for morphophonology. Cotterell et al. (2015) defined a Bayesian network to describe the generative process of phonological words. Our Figure 1 shows a conversion of their model to a factor graph and explains what the variables and factors mean.\nInference on this graph performs unsupervised discovery of latent strings. Given observed surface representations of words (SRs), inference aims to recover the underlying representations (URs) of the words and their shared constituent morphemes. The latter can then be used to predict held-out SRs.\nNotice that the 8 edges in the first layer of Figure 1 form a cycle; such cycles make BP inexact. Moreover, the figure shows only a schematic fragment of the graphical model. In the actual experiments, the graphical models have up to 829 variables, and the variables representing morpheme URs are connected to up to 156 factors (because many words share the same affix).\nTo handle the above challenges without approximation, we want to decompose the original problem into subproblems where each subproblem can be solved efficiently. In particular, we want the subproblems to be free of cycles and highdegree nodes. In our phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem. This decomposition is shown in Figure 2.\nWhile the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2. DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables."
  }, {
    "heading": "4 Dual Decomposition",
    "text": "Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗."
  }, {
    "heading": "4.1 Review of Dual Decomposition",
    "text": "To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel). For example, our experiments partition Figure 1 as shown in Figure 2.\nSpecifically, we partition the factors into K sets F1, . . . ,FK . Each factor F ∈ F appears in exactly one of these sets. This lets us rewrite the score (1) as ∑ k ∑ F∈Fk F (x). Instead of simply seeking its maximizer x, we equivalently seek\nargmax x1,...,xK K∑ k=1 ( ∑ F∈Fk F (xk) ) s.t. x1 = · · · = xK (2)\nIf we dropped the equality constraint, (2) could be solved by separately maximizing∑\nF∈Fk F (x k) for each k. This “subproblem” is itself a MAP problem which considers only the factors Fk and the variables X k adjacent to them in the original factor graph. The subproblem objective does not depend on the other variables.\nWe now attempt to enforce the equality constraint indirectly, by adding Lagrange multipliers that encourage agreement among the subproblems. Assume for the moment that the variables in the factor graph are real-valued (each xki is in R). Then consider the Lagrangian relaxation of (2),\nmax x1,...,xK K∑ k=1 ( ∑ F∈Fk F (xk) + ∑ i λki · xki ) (3)\nThis can still be solved by separate maximizations. For any choices of λki ∈ R having (∀i) ∑ k λ k i = 0, it upper-bounds the objective of (2). Why? The solution to (2) achieves the same value in (3), yet (3) may do even better by considering solutions that do not satisfy the constraint. Our goal is to find λki values that tighten this upper bound as much as possible. If we can find λki values so that\nthe optimum of (3) satisfies the equality constraint, then we have a tight bound and a solution to (2).\nTo improve the method, recall that subproblem k considers only variables X k. It is indifferent to the value ofXi ifXi /∈ X k, so we just leave xki undefined in the subproblem’s solution. We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality. Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi.7"
  }, {
    "heading": "4.2 Substring Count Features",
    "text": "But what do we do if the variables are strings? The Lagrangian term λki ·xki in (3) is now ill-typed. We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers.\nThis corresponds to changing the constraint in (2). Instead of requiring x1i = · · · = xKi for each i, we are now requiring γ(x1i ) = · · · = γ(xKi ), i.e., these strings must agree in their features.\nWe want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow Paul and Eisner (2012) and use a substring count feature for each w ∈ Σ∗. In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8\nComputing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors.\nWe will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variable Xi maintains an active set of features, Wi. Only these features may have nonzero Lagrange multipliers. While the active set can grow over time, it will be finite at any given step.\nGiven the Lagrange multipliers, subproblem k of (3) is simply MAP inference on the factor graph consisting of the variables X k and factors Fk as well as an extra unary factor Gki at each Xi ∈ X k:\n7Without this optimization, the Lagrangian term λki · xki would have driven xki to match that value anyway.\n8More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary symbols. We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features.\nGki (x k) def= λki · γ(xki ) (4)\nThese unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only onWi, while its weights come from λki ."
  }, {
    "heading": "4.3 Projected Subgradient Method",
    "text": "We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition.\nThe projected subgradient step is\nλki := λ k i + η · ( µi − γ(xki ) ) (5)\nwhere η > 0 is the current step size, and µi is the mean of γ(xk\n′ i ) over all subproblems k ′ that consider Xi. This update modifies (3) to encourage solutions xk such that γ(xki ) comes closer to µi.\nFor each i, we update all λki at once to preserve the property that (∀i)∑k λki = 0. However, we are only allowed to update components of the λki that correspond to features in the active setWi. To ensure that we continue to make progress even after we agree on these features, we first expandWi by adding the minimal strings (if any) on which the xki do not yet all agree. For example, we will add the abc feature only when the xki already agree on their counts of its substrings ab and bc.9\nAlgorithm 1 summarizes the whole method. Table 1 illustrates how one active setWi (section 4.3) evolves, in our experiments, as it tries to enforce agreement on a particular string xi."
  }, {
    "heading": "4.4 Past Work: Implicit Intersection",
    "text": "Our DD algorithm is an extension of one that Paul and Eisner (2012) developed for the simpler implicit intersection problem. Given many WFSAs F1, . . . , FK , they were able to find the string x with maximum total score ∑K k=1 Fk(x). (They applied this to solve instances of the NP-hard Steiner 9In principle, we should check that they also (still) agree on a, b, and c, but we skip this check. Our active set heuristic is almost identical to that of Paul and Eisner (2012).\nAlgorithm 1 DD for graphical models over strings 1: initialize the active setWi for each variable Xi ∈ X 2: initialize λki = 0 for each Xi and each subproblem k 3: for t = 1 to T do . max number of iterations 4: for k = 1 to K do . solve all primal subproblems 5: if any of the λki have changed then 6: run max-product BP on the acyclic graph de-\nfined by variablesX k and factorsFk andGki 7: extract MAP strings: ∀i with Xi ∈ X k, xki\nis the label of the max-scoring accepting path in the WFSA that represents the belief at Xi\n8: for each Xi ∈ X do . improve dual bound 9: if the defined strings xki are not all equal then 10: Expand active feature setWi . section 4.3 11: Update each λki . equation (5) 12: Update each Gki from Θi,λ k i . see (4) 13: if none of the Xi required updates then 14: return any defined xki (all are equal) for each i 15: return {x1i , . . . , xKi } for each i . failed to converge\nstring problem, i.e., finding the string x of minimum total edit distance to a collection ofK ≈ 100 given strings.) The naive solution to this problem would be to find the highest-weighted path in the intersection F1 ∩ · · · ∩ FK . Unfortunately, the intersection of WFSAs takes the Cartesian product of their state sets. Thus materializing this intersection would have taken time exponential in K.\nTo put this another way, inference is NP-hard even on a “trivial” factor graph: a single variable X1 attached to K factors. Recall from section 2.3 that BP would solve this via the expensive intersection above. Paul and Eisner (2012) instead applied DD with one subproblem per factor. We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles."
  }, {
    "heading": "4.5 Block Coordinate Update",
    "text": "We also explored a possible speedup for our algorithm. We used a block coordinate update variant of the algorithm when performing inference on the phonology problem and observed an empirical speedup. Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition.\nIn general, block algorithms minimize the objective by holding some variables fixed while updating others. Sontag et al. (2011) proposed a sophisticated block method called MPLP that considers all values of variable Xi instead of the ones obtained from the best assignments for the subproblems. However, it is not clear how to apply their technique to string-valued variables. Instead, the algorithm we propose here is much simpler—it\ndivides the primal variables into groups and updates each group’s associated dual variables in turn, using a single subgradient step (5). Note that this way of partitioning the dual variables has the nice property that we can still use the projected subgradient update we gave in (5) and preserve the property that (∀i)∑k λki = 0.\nIn the graphical model for generative phonology, there are two types of underlying morphemes in the first layer: word stems and word affixes. Our block coordinate update algorithm thus alternates between subgradient updates to the dual variables for the stems and the dual variables for the affixes. Note that when performing block coordinate update on the dual variables, the primal variables are not held constant, but rather are chosen by optimizing the corresponding subproblem."
  }, {
    "heading": "5 Experimental Setup",
    "text": ""
  }, {
    "heading": "5.1 Datasets",
    "text": "We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3. Inference in this model aims to reconstruct underlying morphemes. Since our focus is inference, we will evaluate these reconstructions directly (whereas Cotterell et al. (2015) evaluated their ability to predict novel surface forms using the reconstructions).\nOur factor graphs have a similar topology to the pedagogical fragment shown in Figure 1. How-\never, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs. jhu.edu/tacl2015/. Briefly:\nEXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995). Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes."
  }, {
    "heading": "5.2 Evaluation Scheme",
    "text": "We compared three types of inference:\nDD Use DD to perform exact MAP inference. SP Perform approximate marginal inference by\nsum-product loopy BP with pruning (Cotterell et al., 2015).\nMP Perform approximate MAP inference by max-product loopy BP with pruning. DD and SP improve this baseline in different ways.\nDD predicts a string value for each variable. For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable.\nWe report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human (Cotterell et al., 2015). We also compare these predicted URs to one another, to see how well the methods agree."
  }, {
    "heading": "5.3 Parameterization",
    "text": "The model of Cotterell et al. (2015) has two factor types whose parameters must be chosen.10 The first is a unary factor Mφ. Each underlyingmorpheme variable (layer 1 of Figure 1) is connected to a copy of Mφ, which gives the prior distribution over its values. The second is a binary factor Sθ. For each surface word (layer 3), a copy of Sθ gives its conditional distribution given the corresponding underlying word (layer 2). Mφ and Sθ respectively model the lexicon and the phonology of the specific language; both are encoded as WFSMs.\n10The model also has a three-way factor, connecting layers 1 and 2 of Figure 1. This represents deterministic concatenation (appropriate for these languages) and has no parameters.\nMφ is a 0-gram generative model: at each step it emits a character chosen uniformly from the alphabet Σ with probability φ, or halts with probability 1−φ. It favors shorter strings in general, but φ determines how weak this preference is. Sθ is a sequential edit model that produces a word’s SR by stochastically copying, inserting, substituting, and deleting the phonemes of its UR. We explore two ways of parameterizing it.\nModel 1 is a simple model in which θ is a scalar, specifying the probability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes.\nModel 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena."
  }, {
    "heading": "5.4 Training",
    "text": "When evaluating an inference method from section 5.2, we use the same inference method both for prediction and within training.\nWe train Model 1 by grid search. Specifically, we choose φ ∈ [0.65, 1) and θ ∈ [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (max,+) semiring).\nFor Model 2, we compared two methods for training the φ and θ parameters (θ is a vector):\nModel 2S Supervised training, which observes the “true” (hand-constructed) values of the URs. This idealized setting uses the best possible parameters (trained on the test data). Model 2E Expectation maximization (EM), whose E step imputes the unobserved URs.\nEM’s E step calls for exact marginal inference, which is intractable for our model. So we substitute the same inference method that we are test-\n11That is, probability mass of (1− θ)/3 is divided equally among the |Σ| possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1− θ)/3 is allocated to deletion.\ning. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD.\nAs initialization, our first E step uses the trained version of Model 1 for the same inference method."
  }, {
    "heading": "5.5 Inference Details",
    "text": "We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually< 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many parts of the graph settle down early because discrete values can converge in finite time.12\nWe follow Paul and Eisner (2012, section 5.1) fairly closely. In particular: Our stepsize in (5) is η = α/(t + 500), where t is the iteration number; α = 1 for Model 2S and α = 10 otherwise. We proactively include all 1-gram and 2-gram substring features in the active sets Wi at initialization, rather than adding them only as needed. At iterations 200, 400, and 600, we proactively add all 3-, 4-, and 5-gram features (respectively) on which the counts still disagree; this accelerates convergence on the few variables that have not already converged. We handle negative-weight cycles as Paul and Eisner do. If we had ever failed to converge within 2000 iterations, we would have used their heuristic to extract a prediction anyway.\nModel 1 suffers from a symmetry-breaking problem. Many edits have identical probability, and when we run inference, many assignments will tie for highest scoring configuration. This can prevent DD from converging and makes performance hard to measure. To break these ties, we add “jitter” separately to each copy of Mφ in Figure 1. Specifically, if Fi is the unary factor attached to Xi, we expand our 0-gram model Fi(x) = log((p/|Σ|)|x| · (1 − p)) to become Fi(x) = log( ∏ c∈Σ p |x|c c,i · (1 − p)), where |x|c denotes the count of character c in string x, and pc,i ∝ (p/|Σ|) · exp εc,i where εc,i ∼ N(0, 0.01) and we preserve ∑ c∈Σ pc,i = p.\n12A variable need not update λ if its strings agree; a subproblem is not re-solved if none of its variables updated λ."
  }, {
    "heading": "6 Experimental Results",
    "text": ""
  }, {
    "heading": "6.1 Convergence and Speed of DD",
    "text": "As linguists know, reconstructing an underlying stem or suffix can be difficult. We may face insufficient evidence or linguistic irregularity—or regularity that goes unrecognized because the phonological model is impoverished (Model 1) or poorly trained (early EM iterations on Model 2). DD may then require extensive negotiation to resolve disagreements among subproblems. Furthermore, DD must renegotiate as conditions change elsewhere in the factor graph (Table 1).\nDD converged in all of our experiments. Note that DD (section 4.3) has converged when all the equality constraints in (2) are satisfied. In this case, we have found the true MAP configuration.\nIn section 4.5, we discussed a block coordinate update variation (BCDD) of our DD algorithm. Figure 3 shows the convergence behavior of BCDD against the naive projected subgradient algorithm (NVDD) on the four EXERCISE languages under Model 1. The dual objective (3) always upper-bounds the primal score (i.e., the score (1) of an assignment derived heuristically from the current subproblem solutions). The dual decreases as the algorithm progresses. When the two objectives meet, we have found an optimal solution to the primal problem. We can see in Figure 3 that our DD algorithm converges quickly on the four EXERCISE languages and BCDD converges consistently faster than NVDD. We use BCDD in the remaining experiments.\nWhen DD runs fast, it is competitive with the\nother methods. It is typically faster on the EXERCISE data, and a few times slower on the CELEX data. But we stop the other methods after 20 iterations, whereas DD runs until it gets an exact answer. We find that this runtime is unpredictable and sometimes quite long. In the grid search for training Model 1, we observed that changes in the parameters (φ, θ) could cause the runtime of DD inference to vary by 2 orders of magnitude. Similarly, on the CELEX data, the runtime on Model 1 (over 10 different N = 600 subsets of English) varied from about 1 hour to nearly 2 days.13"
  }, {
    "heading": "6.2 Comparison of Inference",
    "text": "For each language, we constructed several different unsupervised prediction problems. In each problem, we observe some size-N subset of the words in our dataset, and we attempt to predict the URs of the morphemes in those words. For each CELEX language, we took N = 600, and used three of the size-N training sets from (Cotterell et al., 2015). For each EXERCISE language, we took N to be one less than the dataset size, and used all N + 1 subsets of size N , again similar to (Cotterell et al., 2015). We report the unweighted macro-average of all these accuracy numbers.\n13Note that our implementation is not optimized; e.g., it uses Python (not Cython).\nWe compare DD, SP, and MP inference on each language under different settings. Table 2 shows aggregate results, as an unweighted average over multiple languages and training sets. We present various additional results at http://cs. jhu.edu/˜npeng/emnlp2015/, including a perlanguage breakdown of the results, runtime numbers, and significance tests.\nThe results for Model 1 are shown in Tables 2a and 2b. As we can see, in both datasets, dual decomposition performed the best at recovering the URs, while MP performed the worst. Both DD and MP are doing MAP inference, so the differences reflect the search error in MP. Interestingly, DD agrees more with SP than with MP, even though SP uses marginal inference.\nAlthough the aggregate results on the EXERCISE dataset show a large improvement of DD over both of the BP algorithms, the gain all comes from the English language. SP actually does better than DD on Catalan and Maori, and MP also gets better results than DD on Maori, tying with SP.\nFor Model 2S, all inference methods achieved 100% accuracy on the EXERCISE dataset, so we do not show a table. The results on the CELEX dataset are shown in Table 2c. Here both DD and MP performed equally well, and outperformed BP—a result like (Spitkovsky et al., 2010). This trend is consistent over all three languages: DD and MP always achieve similar results and both outperform SP. Of course, one advantage of DD in the setting is that it actually finds the true MAP prediction of the model; the errors are known to be due to the model, not the search procedure.\nFor Model 2E, we show results on the EXERCISE dataset in Table 2d. Here the results resemble the pattern of Model 1."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "We presented a general dual decomposition algorithm for MAP inference on graphical models over strings, and applied it to an unsupervised learning task in phonology. The experiments show that our DD algorithm converges and gets better results than both max-product and sum-product BP.\nTechniques should be explored to speed up the DD method. Adapting the MPLP algorithm (Sontag et al., 2011) to the string-valued case would be a nontrivial extension. We could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update."
  }],
  "year": 2015,
  "references": [{
    "title": "Generalized algorithms for constructing statistical language models",
    "authors": ["Cyril Allauzen", "Mehryar Mohri", "Brian Roark."],
    "venue": "Proceedings of ACL, pages 40–47.",
    "year": 2003
  }, {
    "title": "The CELEX lexical database on CDROM",
    "authors": ["R. Harald Baayen", "Richard Piepenbrock", "Leon Gulikers"],
    "year": 1995
  }, {
    "title": "A probabilistic approach to diachronic phonology",
    "authors": ["Alexandre Bouchard-Côté", "Percy Liang", "Thomas L Griffiths", "Dan Klein."],
    "venue": "Proceedings of EMNLP-CoNLL, pages 887–896.",
    "year": 2007
  }, {
    "title": "A probabilistic approach to language change",
    "authors": ["Alexandre Bouchard-Côté", "Percy Liang", "Thomas Griffiths", "Dan Klein."],
    "venue": "Proceedings of NIPS.",
    "year": 2008
  }, {
    "title": "Penalized expectation propagation for graphical models over strings",
    "authors": ["Ryan Cotterell", "Jason Eisner."],
    "venue": "Proceedings of NAACL-HLT, pages 932– 942, Denver, June. Supplementary material (11 pages) also available.",
    "year": 2015
  }, {
    "title": "Stochastic contextual edit distance and probabilistic FSTs",
    "authors": ["Ryan Cotterell", "Nanyun Peng", "Jason Eisner."],
    "venue": "Proceedings of ACL, Baltimore, June. 6 pages.",
    "year": 2014
  }, {
    "title": "Modeling word forms using latent underlying morphs and phonology",
    "authors": ["Ryan Cotterell", "Nanyun Peng", "Jason Eisner."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:433–447.",
    "year": 2015
  }, {
    "title": "Graphical models over multiple strings",
    "authors": ["Markus Dreyer", "Jason Eisner."],
    "venue": "Proceedings of EMNLP, pages 101–110, Singapore, August.",
    "year": 2009
  }, {
    "title": "Discovering morphological paradigms from plain text using a Dirichlet process mixture model",
    "authors": ["Markus Dreyer", "Jason Eisner."],
    "venue": "Proceedings of EMNLP, pages 616–627, Edinburgh, July.",
    "year": 2011
  }, {
    "title": "Parameter learning for loopy markov random fields with structural support vector machines",
    "authors": ["Thomas Finley", "Thorsten Joachims."],
    "venue": "ICML Workshop on Constrained Optimization and Structured Output Spaces.",
    "year": 2007
  }, {
    "title": "Finding cognate groups using phylogenies",
    "authors": ["David Hall", "Dan Klein."],
    "venue": "Proceedings of ACL.",
    "year": 2010
  }, {
    "title": "Large-scale cognate recovery",
    "authors": ["David Hall", "Dan Klein."],
    "venue": "Proceedings of EMNLP.",
    "year": 2011
  }, {
    "title": "A note on join and auto-intersection of n-ary rational relations",
    "authors": ["André Kempe", "Jean-Marc Champarnaud", "Jason Eisner."],
    "venue": "Loek Cleophas and Bruce Watson, editors, Proceedings of the Eindhoven FASTAR Days (Computer Science Technical",
    "year": 2004
  }, {
    "title": "Machine transliteration",
    "authors": ["Kevin Knight", "Jonathan Graehl."],
    "venue": "Computational Linguistics, 24(4).",
    "year": 1998
  }, {
    "title": "Beyond pairwise energies: Efficient optimization for higherorder MRFs",
    "authors": ["Nikos Komodakis", "Nikos Paragios."],
    "venue": "Proceedings of CVPR, pages 2985– 2992. IEEE.",
    "year": 2009
  }, {
    "title": "MRF optimization via dual decomposition: Message-passing revisited",
    "authors": ["Nikos Komodakis", "Nikos Paragios", "Georgios Tziritas."],
    "venue": "Proceedings of ICCV, pages 1–8. IEEE.",
    "year": 2007
  }, {
    "title": "Dual decomposition for parsing with non-projective head automata",
    "authors": ["Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."],
    "venue": "Proceedings of EMNLP, pages 1288– 1298.",
    "year": 2010
  }, {
    "title": "Factor graphs and the sum-product algorithm",
    "authors": ["F.R. Kschischang", "B.J. Frey", "H.A. Loeliger."],
    "venue": "IEEE Transactions on Information Theory, 47(2):498–519, February.",
    "year": 2001
  }, {
    "title": "An augmented lagrangian approach to constrained map inference",
    "authors": ["André Martins", "Mário Figueiredo", "Pedro Aguiar", "Eric P. Xing", "Noah A. Smith."],
    "venue": "Proceedings of ICML, pages 169–176.",
    "year": 2011
  }, {
    "title": "Expectation propagation for approximate Bayesian inference",
    "authors": ["Thomas P. Minka."],
    "venue": "Proceedings of UAI, pages 362–369.",
    "year": 2001
  }, {
    "title": "Weighted finite-state transducers in speech recognition",
    "authors": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley."],
    "venue": "Computer Speech & Language, 16(1):69–88.",
    "year": 2002
  }, {
    "title": "Loopy belief propagation for approximate inference: An empirical study",
    "authors": ["Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan."],
    "venue": "Proceedings of UAI, pages 467–475.",
    "year": 1999
  }, {
    "title": "Implicitly intersecting weighted automata using dual decomposition",
    "authors": ["Michael J. Paul", "Jason Eisner."],
    "venue": "Proceedings of NAACL, pages 232–242.",
    "year": 2012
  }, {
    "title": "Speech recognition by composition of weighted finite automata",
    "authors": ["Fernando C.N. Pereira", "Michael Riley."],
    "venue": "Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press, Cambridge, MA.",
    "year": 1997
  }, {
    "title": "A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing",
    "authors": ["Alexander M. Rush", "Michael Collins."],
    "venue": "Technical report available from arXiv.org as arXiv:1405.5208.",
    "year": 2014
  }, {
    "title": "Introduction to dual decomposition for inference",
    "authors": ["David Sontag", "Amir Globerson", "Tommi Jaakkola."],
    "venue": "Optimization for Machine Learning, 1:219– 254.",
    "year": 2011
  }, {
    "title": "Viterbi training improves unsupervised dependency parsing",
    "authors": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D. Manning."],
    "venue": "Proceedings of CoNLL, page 917, Uppsala, Sweden, July.",
    "year": 2010
  }, {
    "title": "Large margin methods for structured and interdependent output variables",
    "authors": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun."],
    "venue": "Journal of Machine Learning Research, 6:1453–1484, September. 927",
    "year": 2005
  }],
  "id": "SP:d2470fe737cfbae4729915db9e25f3b5a58e6176",
  "authors": [{
    "name": "Nanyun Peng",
    "affiliations": []
  }, {
    "name": "Ryan Cotterell",
    "affiliations": []
  }, {
    "name": "Jason Eisner",
    "affiliations": []
  }],
  "abstractText": "We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features. This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.",
  "title": "Dual Decomposition Inference for Graphical Models over Strings"
}