{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In this work, we study a basic question that arises in the study of high dimensional vector representations: given a dataset D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Such density estimates are important building blocks in non-parametric clustering, determining the popularity of topics, search and recommendation systems, the analysis of the neighborhoods of nodes in social networks, and in outlier detection, where geometric representations of data are frequently used. Yet for high dimensional datasets, we still lack simple, practical, experimentally verified and theoretically justified solutions to tackle this question.\nOur questions have been studied in the context of spherical range counting. One class of solution methods arising in the computational geometry literature, such as hierarchical splitting via trees, (Arya et al., 2010) have performance guarantees that depend exponentially on dimension. These are unsuitable for the higher dimensional models that ma-\n1Stanford University, USA 2Laserlike Inc, USA. Correspondence to: Xian Wu <xwu20@stanford.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nchine learning methods are increasingly shifting towards e.g. word embeddings (Pennington et al., 2014; Mikolov et al., 2013) and graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover & Leskovec, 2016; Yang et al., 2016; Wang et al., 2017; Hamilton et al., 2017). Over-parameterized models are oftentimes easier to train (Livni et al., 2014), and perform just as well, if not better (Zhang et al., 2016). Word embeddings is one example where rigorous evaluation has shown increased performance with higher dimensionality (Melamud et al., 2016) (Lai et al., 2016).\nIn this paper, we develop an estimation scheme for high dimensional datasets to count the number of elements around a query that are in a given radius of cosine similarity. Angular distance, which corresponds to Euclidean distance for data points on the unit sphere is commonly used in applications related to word and document embeddings, and image and video search (Jegou et al., 2011) (Huang et al., 2012). Brute force search requires a linear scan over the entire dataset, which is prohibitively expensive. Our approach uses indexing and search via locality sensitive hashing (LSH) functions in order to estimate the size of the neighborhood in a more efficient manner than retrieving the neighbors within the given radius of similarity.\nRecent work has also explored LSH techniques for spherical range counting and related questions around density estimation for high-dimensional models. For example (Aumüller et al., 2017) generalizes nearest neighbor LSH hash functions to be sensitive to custom distance ranges. (Ahle et al., 2017) builds many different parameterized versions of the prototypical LSH hash tables and adaptively probes them for spherical range reporting. The closest works to ours in terms of solution method that we are aware of is that of (Spring & Shrivastava, 2017), giving an LSH based estimator to compute the partition function of a log-linear model, and (Charikar & Siminelakis, 2017), adapting LSH to solve a class of kernel density estimation problems. Both works produce an unbiased estimator, using LSH to implement a biased sampling scheme that lowers the variance of this estimator. However their technique leverages only one hash bucket per table, and hence requires a large number of tables for an accurate estimate. The biggest drawback to these works is the very high storage (hash tables) and query complexities – their techniques, as presented, are impractical for\nadoption.\nOur approach improves upon the storage and sample complexities of previous methods using a combination of extracting information from multiple buckets per table (hence reducing table complexity) and importance sampling (hence reducing sample complexity). As we show in our experimental study on GLOVE embeddings, our estimate of the number of elements that are 60 degrees from a query q (which corresponds to synonyms and/or related words to q in the English vocabulary), achieves multiple orders of magnitude improved accuracy over competing methods, subject to reasonable and practical resource constraints. Our theoretical analysis develops a rigorous understanding of our technique and offers practitioners further insight on optimizing our solution method for their particular datasets."
  }, {
    "heading": "2. Problem Formulation and Approach",
    "text": "Given a dataset D of vectors v1, . . . vn ∈ Rd on the unit sphere, a query q ∈ Rd also on the unit sphere, and a range of angles of interestA, for example 0-60 degrees, how many elements v in D are such that the angle between q and v, denoted θqv, are within range A? We use Aq to denote the set of data vectors v that are within angle A to q (that have angular distance to query q that is in the range of interest A). Our goal is to preprocess D in order to estimate the cardinality of this set, denoted |Aq|, efficiently for any given q.\nOne final note is that our scheme is conceptualized using bit-wise LSH functions; functions that hash vectors to 0-1 bits, and where the hamming distance between the hash sequences of two data points captures information about their angular distance. For their simplicity, easy implementation, and high performance in practice, bit hashes such as hyperplane LSH (Charikar, 2002) are the standard hash functions used in practice for angular distance (Andoni et al., 2015). Our technique and results can be extended for other hash functions; however, we will use hamming distance and other implementation details specific to bit-wise LSH functions in this work."
  }, {
    "heading": "2.1. Approach Overview",
    "text": "Our overall estimation scheme is an implementation of importance sampling. It consists of two steps, a preprocessing step that applies locality sensitive hash functions to our dataset to produce hash tables. After this preprocessing step, we sample from our hash tables to produce our final estimate.\nTo help guide the reader through the technical details of our implementation, we first offer an intuitive explanation of our approach. Our importance sampling scheme achieves 2 main objectives: we concentrate the elements of interest\nin our overall dataset into a few buckets that we can easily sample from, and we sample from these buckets to produce our estimate. In order to compensate for the concentrated sampling, we adjust the value of each sample by the inverse of the probability that the sample lands in the target buckets.\nOur technique relies on the key insight that LSH functions can effectively implement both of these objectives. Using LSH functions to index our dataset ensures that for a given query q, elements that are close to q in angular distance have a comparative higher probability of hashing to q’s bucket and to buckets that are of small hamming distance to q’s bucket, thereby concentrating the elements of interest into certain buckets that we can selectively sample from.\nAdditionally, the hamming distance collision probabilities for bit-wise LSH functions are well expressed in terms of angular distance. Consider random hyperplane LSH (Charikar, 2002), where each hash vector is chosen uniformly at random from the d-dimensional unit sphere. Each hash vector r contributes one bit to the hash sequence of a data point v, based on the rule:\nhr(v) = { 0 if r · v ≤ 0 1 otherwise.\nIt is well-known that for any particular hamming distance i, and any data point x,\nP(dqx = i|θqx) = ( t\ni\n)( 1− θqx\nπ )t−i( θqx π )i where dqx is the hamming distance between the hash for query q and the hash for data vector x, θqx denotes the angle between the 2 vectors, and t is the total number of bits in the hash sequence.\nThus, the choice of t affects the sensitivity of the LSH scheme – the correlation between the hamming distances of two hash sequences and the angle between the two underlying data points. Moreover, depending on the design choice for t, the set of hamming distances I that contains most of the probability mass for collision with elements of angular distance in range A is different. This is also a consideration in our sampling scheme; we want to sample from buckets of hamming distances I that have a high probability of containing elements that are within angle A of q.\nOur sampling scheme picks elements over K hash tables from buckets that are at hamming distance I to the query, where I is tuned to A. Given a sample, x, we compute the angular distance θqx = cos−1(q · x). Let p(x) = P(dqx ∈ I|θqx), the collision probability that x lands in a bucket that is hamming distance I from q over the random choice of hash functions.\nWe define a random variable Z as a function of sample x as follows:\nZ =\n{∑K k=1 C k q (I)\nK·p(x) if θqx ∈ A 0 otherwise.\n(1)\nwhere Ckq (I) is the total number of elements in buckets of hamming distance I from q’s bucket in table k.\nWe take S samples and construct Z1, Z2, . . . ZS . We report∑S i=1 Zi S as our estimate for |Aq|.\nComparison to Related Work: Note that our problem can be viewed as kernel density estimation problem for a specific kernel function that has value 1 for pairs of points within the required angle range of interest and 0 outside. However the analysis of (Charikar & Siminelakis, 2017) does not apply to our setting because they need a scale free hash function (with collision probabilities related to the kernel value) and there is no such function for our 0-1 kernel. The work of (Spring & Shrivastava, 2017) does not make such an assumption on the hash function, but they do not give an analysis that gives meaningful bounds in our setting. As noted previously, both works only look at a single hash bucket in each hash table, leading to a high storage overhead."
  }, {
    "heading": "2.2. Main Result",
    "text": "We establish the following theoretical bounds on the storage and sample complexity of our estimator in order to achieve a (1±ε)-approximation to the true count with high probability. Theorem 2.1 (Main Result). For a given angular distance range of interest A and a given query q, with probability 1 − δ, our estimator returns a (1 ± ε)approximation to |Aq|, the true number of elements within\nangle A to q using O\n( 1\nε2 min x∈Aq\np(x) log( 1 δ )\n) tables and\nO\n( E(Cq(I))\nε2|Aq|· min x∈Aq\np(x) log( 1 δ )\n) samples.\nTo help the reader digest this result, we briefly compare this statement to the sample complexity of naive random sampling. It can be shown through a standard BernoulliChernoff argument that the sample complexity for random sampling is O( n|Aq|ε2 ln ( 1 δ ) ), where n|Aq| is the inverse proportion of elements of interest in the overall population. Intuitively this says that you need to take more random samples if |Aq| is very small compared to n.\nOur sample complexity replaces the n|Aq| term with E(Cq(I)) |Aq|· min x∈Aq p(x) , where |Aq| · minx∈Aq p(x) is a measure of the expected number of elements from the set of interest Aq that will land in hamming distance I to q, and E(Cq(I)) is\nthe expected size of the overall sampling pool of elements in hamming distance I. This ratio of expectations seems intuitive – one would expect to get such an expression if our scheme took one sample per table. Surprisingly, we achieve this same type of sample complexity bound while sampling from relatively few hash tables.\nJust like random sampling, our sample complexity bound is also based on the proportion of elements of interest in hamming distance I to the total number of elements in hamming distance I. However, it is easy to see that applying LSH to our dataset will increase this proportion to yield a smaller sample complexity. We choose I so that min\nx∈Aq p(x) is high\n(this probability can be high even for a small set of hamming distances I, since p(x) is the cumulative probability mass of I successes in t trials, and binomial distributions in t concentrate in an O( √ t) sized interval around the mean), and E(Cq(I)) to be small (to filter out elements that are not interesting).\nThere are certain tradeoffs to choosing I . If more hamming distances are included in I, then min\nx∈Aq p(x) is higher, how-\never, E(Cq(I)) is also larger. The optimal choice for I is to choose the hamming distances that substantially increase min x∈Aq p(x) yet do not substantially increase E(Cq(I)) (so not too many uninteresting elements are infiltrating those buckets).\nIn the following sections, we explain our scheme further and present our experimental results."
  }, {
    "heading": "3. Preprocessing",
    "text": "The preprocessing step contributes 3 key ingredients to the overall estimation scheme:\nHash Tables: Given a family of bit-wise hash functions H, define a function family G = {g : D → {0, 1}t} such that g(v) = (h1(v), . . . ht(v)), where hj ∈ H. To construct K tables, we choose K functions g1, g2, . . . gK from G independently and uniformly at random. We store each v ∈ D in bucket gk(v) for k = 1, 2 . . .K. This step sets up the hash tables that we will sample from in our scheme.\nCounts Vector: We create a counts vector, denoted Cki ∈ Rt+1 for each hash address ik for each table k ∈ {1, . . . ,K}, whereCki (d) is the count of the total number of items in buckets that are at hamming distance d = 0, 1, . . . t away from ik in table k.\nSampler: We create a sampler that given a separate hash address ik for each table k ∈ {1, . . . ,K} and set of hamming distances I, returns a data point uniformly at random from the union of elements that were hashed to buckets of hamming distance I from ik across the K tables.\nWe describe in greater detail the 3 contributions of the preprocessing step. For the rest of this paper, all omitted proofs appear in Appendix C."
  }, {
    "heading": "3.1. Hash Tables",
    "text": "Setting up quality hash tables to enable accurate and efficient importance sampling is vital to our scheme. Since we are importance sampling from buckets of hamming distance I acrossK tables, we need to make enough tables to guarantee unbiasedness or near-unbiasedness for our sampling-based estimator; due to the variance of the randomly generated hash functions, if we make too few tables we may not find enough elements of interest contained in those tables within hamming distance I. We want to characterize the bias of our importance sampling scheme in relation to the contents of the buckets of our hash tables.\nWe let Bkq (I) denote the set of hash buckets that are at hamming distance I from the hash address of query q for table k. Next, we introduce an intermediate random variable:\nW = 1\nK K∑ k=1 ∑ x∈Aq 1(x ∈ Bkq (I)) p(x) .\nwhere p(x) = P(dqx ∈ I|θqx).\nW is a random variable that represents the sum of the elements of interest |Aq| that are hashed to the buckets of sampling focus Bkq (I), weighted by their probabilities p(x). It is clear that once the set of hash functions is fixed, W becomes deterministic.\nWe first show that the random variable Z, as defined in Equation (1), is an unbiased estimator. Lemma 3.1 (Expectation of Z). The expectation of Z over the random choice of hash functions is |Aq|, i.e. E(Z) = |Aq|. The expectation of Z given a specific realization of hash functions, or equivalently, given W , is E(Z|W ) =W .\nAs a consequence, it is immediately clear that E(W ) = |Aq|. It is important to understand the implications of this lemma. In particular, the expression for E(Z|W ) says that in a specific realization of a choice of hash functions (or a set of tables), the estimator Z is biased if W 6= |Aq|. Therefore K is essential for helping concentrate the realized value of W around its mean.\nSince in expectation, our estimator Z gives W , we want to understand how many tables K are required to ensure that W concentrates around its mean, |Aq|. This is related to the variance of W .\nWe also introduce a new quantity p(x, y) = P(dqx ∈ I ∩ dqy ∈ I|θqx, θqy), the collision probability that x and y both land in buckets that are hamming distance I from q over the random choice of hash functions.\nLemma 3.2 (Variance of W ). σ2(W ) = 1 K ∑ x,y∈Aq ( p(x,y) p(x)p(y) − 1 ) We want to put these pieces together to make a statement about the number of tables K we should create to guarantee low inherent bias in our estimator. We use Chebyshev’s Inequality to bound W ’s deviation from its mean as a function of K with a constant failure probability 18 . For simplicity, we fix a constant failure probability that we will boost later by average over several sets of estimators. This analysis is without loss of generality, as the bounds can be adjusted for any desired failure probability δ. We will use this piece again when we analyze our overall estimator. Lemma 3.3 (Bound on Number of Tables). It suffices to make K ≥ 8ε2 min\nx∈Aq p(x) tables to guarantee that W is within\nε of |Aq| (relatively) with probability 78 .\nProof. Chebyshev’s inequality states: P(|W − |Aq|| ≥ ε|Aq|) ≤ σ 2(W ) ε2|Aq|2 .\nTherefore, to achieve a constant failure probability δ = 18 , it suffices to create enough tables so that\nσ2(W ) = 1\nK ∑ x,y∈Aq ( p(x, y) p(x)p(y) − 1 ) ≤ ε 2|Aq|2 8\nHence K needs to be large enough so that: K ≥ 8 ∑ x,y∈Aq ( p(x,y) p(x)p(y) − 1 ) ε2|Aq|2\nSince p(x, y) ≤ min{p(x), p(y)}, we see that it is sufficient for K to satisfy\nK ≥ 8|Aq|2\n( minx∈Aq 1 p(x) − 1 ) ε2|Aq|2\nTherefore we conclude with the following bound on K:\nK ≥ 8 ε2 min\nx∈Aq p(x)\n(2)\nWe emphasize that the joint probability p(x, y) ≤ min{p(x), p(y)} is a very loose worst-case bound assuming high correlation between data points. The final bound for K, Equation (2), is also a worst-case bound in the sense that it is possible that a very minuscule fraction of x ∈ Aq have small values for p(x). In the experimental section of the paper, we do an empirical analysis of the inherent bias for different values of K and demonstrate that for real datasets the number of tables needed can be far fewer than what is theoretically required in the worst case scenario."
  }, {
    "heading": "3.2. Counts Vector",
    "text": "Query q maps to a bucket ik for each table k = 1, 2 . . .K. The preprocessing step produces an average counts vector corresponding to bucket ik, denoted Ckq , where C k q (i) is the count of the total number of items in buckets that are at hamming distance i = 0, 1, . . . t away from the hash address for q in table k. For the hamming distances of interest I , we let Ckq (I) = ∑ d∈I C k q (d).\nCkq (I) is an integral part of our weighted importance sampling scheme. In Appendix A, we show how to compute these vectors efficiently.\nTheorem 3.1 (Aggregate-Counts). Given a set of K hash tables, each with 2t hash buckets with addresses in {0, 1}t, Aggregate-Counts (Algorithm 1) computes, for each hash address i, the number of elements in buckets that are hamming distance 0, 1, . . . t away from i, in each of the K tables, in time O(Kt22t).\nNote that the t in our hashing scheme is the length of the hash sequence; as a general rule of thumb, for bit-wise hash functions, implementers choose t ≈ log(n), so as to average out to one element per hash bucket. Therefore, the preprocessing runtime of a reasonable hashing implementation for Aggregate-Counts (Algorithm 1) is approximately O(nK log2(n)).\nThe key benefit of Aggregate-Counts is that it computes via a message-passing or dynamic programming strategy that is much more efficient than a naive brute-force approach that would take time O(K22t), or O(Kn2) if t ≈ log(n)."
  }, {
    "heading": "3.3. Sampler",
    "text": "We create a sampler that, given a hash address ik for each table, and a set of hamming distances I that we want to sample from, generates a sample uniformly at random from the union of elements that were hashed to hamming distance I across the K tables. For an implementation and analysis, please consult Appendix B.\nTheorem 3.2 (Sampler). Given a set of K hash tables, each with 2t hash buckets with addresses in {0, 1}t, a sampling scheme consisting of a data structure and a sampling algorithm can generate a sample uniformly at random from any fixed hash table k, an element at hamming distance d to hash address i. The data structure is a counts matrix that can be precomputed in preprocessing time O(Kt32t), and the sampling algorithm Hamming-Distance-Sampler (Algorithm 2) generates a sample in time O(t).\nAgain, if we follow t ≈ log(n), the preprocessing time comes out to roughly O(nK log3(n)). Also we expect the O(t) online sample generation cost to be negligible compared to, say, the inner product computation cost for q · x,\nwhich our method and all competing methods use. We describe the importance sampling scheme in the next section."
  }, {
    "heading": "4. Sampling",
    "text": "We now analyze our sampling algorithm. Recall that our sampling scheme works in the following way. Given query q, we generate the hash for q in each of our K tables, by solving for ik = gk(q) for k = 1, . . .K. Given the hash for q in each of our K tables and the set of hamming distances I that we want to sample from, we invoke our sampler to generate a sample from across the K tables.\nGiven this sample, x, we compute the angular distance θqx = cos\n−1(q · x). Let p(x) = P(dqx ∈ I|θqx), the collision probability that x lands in a bucket that is hamming distance I from q over the random choice of hash functions; p(x) is an endogenous property of an LSH function.\nWe score each sample as in Equation (1).\nWe take S samples and construct Z1, Z2, . . . ZS . We report∑S i=1 Zi S as our estimate for |Aq|.\nAs an immediate consequence of Lemma 3.1, it is clear that\nE [∑S i=1 Zi S ] = |Aq| .\nNow we analyze the variance of our estimator:\nLemma 4.1 (Variance of Estimator).\nE (∑Si=1 Zi S − |Aq| )2 ≤ E[Z2] S + σ2(W )\nThis decomposition of the variance into the two terms indicates that the variance is coming from two sources. The first source is the variance of the samples, E[Z\n2] S . If we don’t take\nenough samples, we do not get a good estimate. The second source is the variance from the random variable W , σ2(W ), which corresponds to the contents in the tables. As we have shown, it is crucial to create enough tables so that W is concentrated around its expectation, |Aq|. Therefore, this second source of variance of the overall estimator comes from the variance of the hash functions that underlie table creation and composition.\nThe σ2(W ) term has already been analyzed in Section 3.1, see Lemma 3.2. Now we analyze the second moment of Z.\nLemma 4.2 (Variance of Z).\nE[Z2] = ∑ x∈Aq ∑ y∈D [ p(x, y) K · p(x)2 + ( 1− 1 K ) p(y) p(x) ]\nNow that we have all the components, we are ready to put together the final sample and storage complexities for our estimator. We want a final estimate that concentrates with at most error around its mean, |Aq| with probability 1− δ. To do this, we make several sets 1, 2, . . .M of our estimator (one estimator consists of a set of K tables and S samples). We choose K and S so that the failure probability of our estimator is a constant, say 14 . Each estimator produces an estimate, call it Em, for m ∈ {1, . . .M}. We report our final estimate as the median of these estimates. This is the classic Median-of-Means technique.\nLet Fm be the indicator variable indicating if the estimator Em fails to concentrate. Clearly E(Fm) ≤ 14 . Moreover, E(F = ∑M m=1 Fm) ≤ M 4 . The probability that the median estimate is bad, P(median of Emfails) ≤ P(half of Em fails) = P(F ≥ M2 ). By a simple Chernoff bound, we see that: P(F ≥ M2 ) ≤ e\n−(2 ln 2−1)M4 ≤ e−M11 . So to satisfy a desired failure probability δ, it suffices to have e\n−M 11 ≤ δ, therefore M ∈ O(log( 1δ )).\nIn the rest of the section, we establish bounds on K and S so that one estimator fails with probability at most 14 . We appeal again to Chebyshev’s Inequality:\nP (∣∣∣∣∣ ∑S i=1 Zi S − |Aq| ∣∣∣∣∣ ≥ ε|Aq| ) ≤ σ2( ∑S i=1 Zi S ) ε2|Aq|2\nIn Lemma 4.1, we analyze the variance of our estimator, and show that σ2( ∑S i=1 Zi S ) ≤ E[Z2] S + σ\n2(W ). Therefore, in order so that the failure probability is less than 14 , it suffices\nto have σ2( ∑S\ni=1 Zi S ) ≤ ε2|Aq|2 4 , which can be obtained by\nletting E[Z 2] S ≤ ε2|Aq|2 8 and σ 2(W ) ≤ ε 2|Aq|2 8 .\nFocusing on the σ2(W ) term, which depends on the number of tables K created, we show in Lemma 3.3 from Section 3.1 that it suffices to take K ≥ 8ε2 min\nx∈Aq p(x) .\nNow that we have our table complexity, we can analyze our sampling complexity S to bound E[Z\n2] S .\nLemma 4.1. Suppose K ≥ 8ε2 min x∈Aq p(x) . Then S ∈\nO\n( E(Cq(I))\nε2|Aq|· min x∈Aq p(x)\n) suffices to achieve E[Z\n2] S ≤ ε2|Aq|2 8 .\nProof. By Lemma 4.2 we have:\nE[Z2] S = 1 S ∑ x∈Aq ∑ y∈D [ p(x, y) K · p(x)2 + ( 1− 1 K ) p(y) p(x) ]\nSubstituting for K ≥ 8ε2 min x∈Aq p(x) gives:\nE[Z2] S ≤ 1 S ∑ x∈Aq ∑ y∈D ε2p(x, y) minx∈Aq p(x) 8p(x)2 + p(y) p(x)  ≤ 1 S ∑ x∈Aq ∑ y∈D [ ε2p(x, y) 8p(x) + p(y) p(x) ]\n≤ 1 S ∑ x∈Aq ∑ y∈D [ (1 + ε2) p(y) p(x) ]\nIn order to guarantee E[Z 2] S ≤ ε2|Aq|2 8 , we need:\nS ≥\n∑ x∈Aq ∑ y∈D [ (1 + ε2) p(y)p(x) ] ε2|Aq|2\n= (1 + ε2)\n∑ x∈Aq 1 p(x) ∑ y∈D p(y)\nε2|Aq|2\n= (1 + 1\nε2 )\n∑ x∈Aq\n1 p(x) E(Cq(I)) |Aq|2\nTherefore, we conclude that\nS ∈ O  E(Cq(I)) ε2|Aq| · min\nx∈Aq p(x)  is sufficient.\nPutting together Lemmas 3.3 and 4.1 with the median of means strategy yields our main result, Theorem 2.1.\nIn the rest of this paper we discuss the results of our experiments on real datasets."
  }, {
    "heading": "5. Experiments",
    "text": "We describe our experiments using the GLOVE dataset. We use the set of 400,000 pre-trained 50-dimensional word embedding vectors trained from Wikipedia 2014 + Gigaword 5, provided by (Pennington et al., 2014). We normalize the embeddings, as is standard in many word embedding applications (Sugawara et al., 2016) We choose 3 query words with different neighborhood profiles: “venice”, “cake”, “book”. Venice has the smallest neighborhood, with 206 elements with angular distance less than 60 degrees, cake has a medium sized neighborhood with about 698 elements, book has the largest neighborhood with 1275 elements. The histogram for these 3 queries are shown in Figure 1.\nWe also choose our angle range of interest, A, to be 0- 60 degrees. A search through our dataset gave “florence”, “cannes”, “rome” as representative elements that are 40-50\ndegrees from “venice”, and “renaissance”, “milan”, “tuscany”, “italy” in the 50-60 degree range. Terms such as “cheesecake”, “desserts”, “ganache”, and “bakes” appear in the 40-50 degree annulus around “cake”, while terms such as “fruitcake”, “cupcake”, “confections”, “poundcake”, and “eggs” appear in the 50-60 degree histogram. For “book”: “character”, “chronicles”, “paperback”, “authors”, and “text” are in the 40-50 degree range while “bestseller”, “protagonist”, “publishers”, “booklet”, “publishes”, “editing”, “monograph”, and “chapter” are in the 50-60 degree range. This particular experiment shows that while elements in the 40-50 degree range are extremely related, words in the 50-60 degree range are also relevant, and so we fix A to be 0-60 degrees in all of our experiments. We also fix t = 20 in all of our experiments, since we have 400,000 embeddings in total and 20 ≈ log2(400, 000).\nAs Table 1 illustrates, the biggest challenge for this estimation problem is the fact that the count of the number of elements within 0-60 degrees is dwarfed by the number of elements 60-120 degrees away from the queries. This issue makes locality sensitive techniques necessary for efficient search and retrieval in high dimensions.\nTable 1: Statistics of Queries\nQUERY # WITHIN 60 DEGREES % OF POPULATION\nVENICE 206 .0515 CAKE 698 .1745 BOOK 1275 .31875\nAs we have previously mentioned in section 3.1, the number of tables K theoretically required for (near) unbiased estimation relies on a worst-case variance bound; real-world data do not necessarily exhibit worst-case behavior. In our studies of our 3 queries see Figures 2, the inherent bias of our estimator decreases as we increase the sampling ham-\nming threshold. This is as expected, using a larger range of hamming distances helps concentrate the count of the elements of interest Aq that fall into the specified range of hamming distances around the mean, which means that a smaller K is required to achieve small bias.\nMoreover, the empirical bias of our estimator at hamming threshold 5 is around 5% for 20 hash tables, with very little improvement with 40 hash tables. This is consistent with our 3 queries. With this in mind, we compare our estimator against the benchmark estimator introduced by (Spring & Shrivastava, 2017). Though their work originally intended to solve a different problem, their technique can solve our problem by adapting the weight function appropriately. The key differences between their work and ours is that they only probe the 0 hamming distance bucket in each table, similar to the classic LSH literature, and instead of sampling, they simply enumerate the elements in the hamming distance 0 bucket for each table. For higher values of K, which our experiments demonstrate that their estimator needs in order to get good results, enumeration might not be so efficient.\nIn Figure 3, we compare (Spring & Shrivastava, 2017)’s technique of enumerating and importance-weighting hamming distance 0 elements to our technique of importance sampling from different hamming thresholds. Our experiments use random hyperplane LSH and we report relative error averaged over 25 trials, where in each trial we generate a new set of K tables. Panel (b) experiments with (Spring & Shrivastava, 2017)’s technique for the 3 queries, with different choices of K, the number of tables. Our results show that even for K = 40 tables, the relative error of their technique can still be higher than 50%, particularly for queries with small neighborhoods such as “venice”. For “venice” the increase in table allocation from 20 to 40 made a very small difference to the overall estimation error. “book” and “cake” fared better at 40 tables, however, the error was still around 25 %, while our estimator (panel a) estimated to within about 10% error using only 20 tables.\nPanel (a) of Figure 3 shows that utilizing any hamming threshold greater than 0 gives superior estimation performance to staying only within the 0 hamming distance bucket. In this experiment, we fix our sampling budget to 1000 samples and the table budget to 20 tables. The hamming distance 0 error reported in this figure uses enumeration; all other hamming thresholds use the 1000 sampling budget. In our experiments for the 3 queries, one can expect about 80 points in total in the hamming distance 0 buckets across 20 tables. In this experiment, our technique uses 1000 samples vs 80 points, however, this (somewhat negligible in today’s computing infrastructure) sample complexity trades off against a large improvement in precision, as well as a much lower storage cost in the number of tables K.\nFinally, we note that panel (a) of Figure 3 shows the smallest\nerror for “venice” at hamming threshold 3. This is related to the characteristics of this query and the sampling budget. We see in this example that for “venice”, which is a fairly isolated data point compared to the other 2 queries, going to further hamming distances actually hurts the quality of the estimate because we actually dilute the proportion of interesting elements. Using higher thresholds typically requires more samples, as shown in Figure 4. However, higher thresholds typically lowers the inherent bias in the importance sampling scheme, as demonstrated in Figure 2. Implementers should consider this tradeoff in their algorithmic design choices."
  }, {
    "heading": "6. Discussion",
    "text": "Given the case study of our estimator achieving the smallest estimation error for “venice” at hamming threshold 3, whereas for the more popular queries “cake” and “book” performance improves steadily at higher hamming thresholds, it would be interesting to, from the practitioner’s point of view, understand what is the best hamming threshold to sample from, and given a hamming threshold, how many samples should be taken for a quality estimate. The optimal\nsample complexity is data-dependent, and cannot be known without a sense of |Aq|, the very quantity we aim to estimate. But instead of fixing the sample complexity up-front, is there a way we can iteratively, in an on-line fashion, determine whether we should keep sampling or stop, based on a current belief of |Aq|?"
  }, {
    "heading": "Acknowledgements",
    "text": "This work was initiated while the authors were visiting Laserlike, Inc. Xian Wu was supported by a Harold Thomas Hahn Jr. Fellowship from the Department of Management Science and Engineering at Stanford University. Moses Charikar was supported by NSF grant CCF-1617577 and a Simons Investigator Award."
  }],
  "year": 2018,
  "references": [{
    "title": "Parameter-free locality sensitive hashing for spherical range reporting",
    "authors": ["T.D. Ahle", "M. Aumüller", "R. Pagh"],
    "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2017
  }, {
    "title": "Practical and optimal lsh for angular distance",
    "authors": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "A unified approach to approximate proximity searching",
    "authors": ["S. Arya", "G.D. da Fonseca", "D.M. Mount"],
    "venue": "European Symposium on Algorithms,",
    "year": 2010
  }, {
    "title": "Grarep: Learning graph representations with global structural information",
    "authors": ["S. Cao", "W. Lu", "Q. Xu"],
    "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
    "year": 2015
  }, {
    "title": "Hashing-based-estimators for kernel density in high dimensions",
    "authors": ["M. Charikar", "P. Siminelakis"],
    "venue": "IEEE Symposium on Foundations of Computer Science,",
    "year": 2017
  }, {
    "title": "Similarity estimation techniques from rounding algorithms",
    "authors": ["M.S. Charikar"],
    "venue": "In Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing,",
    "year": 2002
  }, {
    "title": "node2vec: Scalable feature learning for networks",
    "authors": ["A. Grover", "J. Leskovec"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2016
  }, {
    "title": "Inductive representation learning on large graphs",
    "authors": ["W. Hamilton", "Z. Ying", "J. Leskovec"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Product quantization for nearest neighbor search",
    "authors": ["H. Jegou", "M. Douze", "C. Schmid"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2011
  }, {
    "title": "How to generate a good word embedding",
    "authors": ["S. Lai", "K. Liu", "S. He", "J. Zhao"],
    "venue": "IEEE Intelligent Systems,",
    "year": 2016
  }, {
    "title": "The role of context types and dimensionality in learning word embeddings",
    "authors": ["O. Melamud", "D. McClosky", "S. Patwardhan", "M. Bansal"],
    "venue": "URL https://arxiv. org/abs/1601.00893",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"],
    "year": 2013
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C.D. Manning"],
    "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2014
  }, {
    "title": "Deepwalk: Online learning of social representations",
    "authors": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"],
    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2014
  }, {
    "title": "A new unbiased and efficient class of lsh-based samplers and estimators for partition function computation in log-linear models",
    "authors": ["R. Spring", "A. Shrivastava"],
    "year": 2017
  }, {
    "title": "On approximately searching for similar word embeddings",
    "authors": ["K. Sugawara", "H. Kobayashi", "M. Iwasaki"],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
    "year": 2016
  }, {
    "title": "Line: Large-scale information network embedding",
    "authors": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"],
    "venue": "In Proceedings of the 24th International Conference on World Wide Web,",
    "year": 2015
  }, {
    "title": "Community preserving network embedding",
    "authors": ["X. Wang", "P. Cui", "J. Wang", "J. Pei", "W. Zhu", "S. Yang"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "Revisiting semi-supervised learning with graph embeddings",
    "authors": ["Z. Yang", "W. Cohen", "R. Salakhudinov"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "year": 2016
  }],
  "id": "SP:3817b42757e113525011af9e9ad5469a993702c0",
  "authors": [{
    "name": "Xian Wu",
    "affiliations": []
  }, {
    "name": "Moses Charikar",
    "affiliations": []
  }, {
    "name": "Vishnu Natchu",
    "affiliations": []
  }],
  "abstractText": "An important question that arises in the study of high dimensional vector representations learned from data is: given a set D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Our algorithm uses locality sensitive hashing to preprocess the data to accurately and efficiently estimate the answers to such questions via an unbiased estimator that uses importance sampling. A key innovation is the ability to maintain a small number of hash tables via preprocessing data structures and algorithms that sample from multiple buckets in each hash table. We give bounds on the space requirements and query complexity of our scheme, and demonstrate the effectiveness of our algorithm by experiments on a standard word embedding dataset.",
  "title": "Local Density Estimation in High Dimensions"
}