{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Covariance matrices play a fundamental role in machine learning and statistics owing to their capability to retain the second-order information of data samples (Feller, 1966). For example, Principal Component Analysis (PCA) along\n1Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China. 2Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong. Correspondence to: Xixian Chen <xxchen@cse.cuhk.edu.hk>, Michael R. Lyu <lyu@cse.cuhk.edu.hk>, Irwin King <king@cse.cuhk.edu.hk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwith its extensions (Zou et al., 2006), Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis (QDA) (Anzai, 2012) are powerful for dimension reduction and denoising, which require the estimation of a covariance matrix from a given collection of data points. Other prominent examples include Generalized Least Squares (GLS) regression that requires the estimation of the noise covariance matrix (Kariya & Kurata, 2004), Independent Component Analysis (ICA) that relies on pre-whitening based on the covariance matrix (Hyvärinen et al., 2004), and Generalized Method of Moments (GMM) (Hansen, 1982) that improves the effectiveness by a precise covariance matrix.\nMany practical applications also rely on covariance matrix directly (Bartz, 2016). In biology, gene relevance networks and gene association networks are straightforwardly inferred from the covariance matrix (Butte et al., 2000; Schäfer & Strimmer, 2005). In modern wireless communications, protocols optimize the bandwidth based on covariance estimates (Tulino & Verdú, 2004). In array signal processing, the capon beamformer linearly combines the sensors to minimize the noise in the signal, which is closely related to the portfolio optimization on covariance matrices (Abrahamsson et al., 2007). For policy learning in the field of robotics, it requires reliable estimates of the covariance matrix between policy parameters (Deisenroth et al., 2013).\nCalculation of a covariance matrix usually requires enormous computational resources in the form of communication and storage because large and high-dimensional data are now routinely gathered at an exploding rate from many distributed remote sites, such as sensor networks, surveillance, and distributed databases (Haupt et al., 2008; Shi et al., 2014; Ha & Barber, 2015). In particular, high communication cost of transmitting the distributed data from the remote sites to the fusion center (i.e., a destination to conduct complex data analysis tasks) will require tremendous bandwidth and power consumption (Srisooksai et al., 2012; Abbasi-Daresari & Abouei, 2016). Formally, given a data matrix X ∈ Rd×n with d features and n instances collected from the remote sites, the covariance matrix is computed in the fusion center by C , 1nXX\nT − x̄x̄T , where x̄ = 1n ∑n i=1 xi ∈ Rd (Feller, 1966). For simplicity\nof discussion, we temporarily assume the empirical mean is zero, i.e., x̄ = 0. The covariance matrix can be written as C = 1nXX\nT consequently (Azizyan et al., 2015). Then, it takes O(nd) communication burden to transmit data from numerous remote sites to the fusion center to form the full data set X, O(nd) storage in total to store X in remote sites, andO(nd+d2) storage withO(nd2) time to calculate C in the fusion center. When n, d 1, the overall cost is prohibitively expensive for practical scenarios like wireless sensors which have narrow transmission bandwidth, limited storage, and low power supply.\nTo tackle such computational challenges, compressed data can be leveraged to estimate the covariance matrix, which essentially has roots in compressed sensing. One solution is to process each data point by multiplying it with a single projection matrix S ∈ Rd×m whose entry follows the Gaussian distribution N (0, 1m ) (Mahoney, 2011). Thus, storing STX and the estimated covariance matrix requires O(mn+d2) space in total, sending STX to the fusion center incurs a O(mn) communication cost, and calculating STX and the covariance matrix estimator 1nSS TXXTSST takes O(mdn + m2n + m2d + md2) time. This method substantially reduces all computational costs if m n, d. Note that synchronizing only a seed between remote sites and the fusion center allows pseudo-random number generators to reconstruct an identical S, which avoids sending S directly and imposes a negligible computational burden.\nHowever, the example solution has two critical drawbacks. The first is that the operations on the Gaussian matrix is inefficient. One could use a sparse projection matrix (Li et al., 2006), structured matrix (Ailon & Chazelle, 2009) or sampling matrix (Drineas et al., 2006b) to achieve a better tradeoff between computational cost and estimation precision. The second problem is that applying a single projection matrix to all data points cannot consistently estimate the covariance matrix, i.e., the estimator cannot converge to the actual covariance matrix even if the sample size n grows to infinity with d fixed. This issue is demonstrated both theoretically and empirically in (Azizyan et al., 2015) and also briefly described in (Gleichman & Eldar, 2011; Anaraki & Hughes, 2014; Anaraki & Becker, 2017).\nIn this paper, we thus adopt n distinct projection matrices for n data vectors (Azizyan et al., 2015; Anaraki & Hughes, 2014; Anaraki & Becker, 2017; Anaraki, 2016) to achieve consistent covariance matrix estimation, and construct a specific sampling matrix to increase both its efficiency and accuracy. On the whole, we do not make statistical assumptions on the distributed data X ∈ Rd×n with n, d 1, nor do we impose structural assumptions on the covariance matrix C such as being low-rank or sparse. Our goal is to compress data and recover C efficiently and accurately, and the contributions in our work are summarized as follows:\n• First, in contrast to all existing methods (Azizyan et al., 2015; Anaraki & Hughes, 2014; Anaraki & Becker, 2017; Anaraki, 2016) that are based on dataoblivious projection matrices, we propose to estimate the covariance matrix based on the data compressed by a weighted sampling scheme. This strategy is dataaware with a capacity to explore the most important entries. Hence, we require considerably fewer entries to achieve an equal estimation accuracy.\n• Second, we provide error analysis for the derived unbiased covariance estimator, which rigorously demonstrates that our method can compress data to a much smaller volume than other methods. The proofs also indicate our probability distribution is specifically designed to render a covariance matrix estimation based on the compressed data as accurate as possible.\n• Third, we specify our method by an efficient algorithm whose computational complexity is superior to other methods. By additionally considering the best tradeoff between the estimation accuracy and the compression ratio, our algorithm ultimately incurs a significantly lower computational cost than the other methods.\n• Finally, we validate our method on both synthetic and real-world datasets, which demonstrates a better performance than the other methods.\nThe remainder of this paper is organized as follows. In Section 2, we review the prior work. In Section 3, we present our method along with theoretical analysis and emphasize its achievements. In Section 4, we provide extensive empirical results, and in Section 5 we conclude the whole work."
  }, {
    "heading": "2. Related Work",
    "text": "There have been several investigations of ways to achieve accurate covariance matrix estimation from the lowdimensional compressed observations constructed by applying a distinct projection matrix {Si}ni=1 ∈ Rd×m to each data vector {xi}ni=1 ∈ Rd. The work of (Qi & Hughes, 2012) adopts a Gaussian matrix to compress data via STi xi, and recovers them by Si(S T i Si) −1(STi xi). Because Si(S T i Si)\n−1STi is a strictly m-dimensional orthogonal projection drawn uniformly at random, it can capture the information of all entries in each data vector uniformly and substantively. Then, 1n ∑n i=1 Si(S T i Si) −1STi xix T i Si(S T i Si)\n−1STi up to a known scaling factor is expected to constitute accurate and consistent covariance matrix estimation. This estimator can be modified to an unbiased one, and its error analysis is thoroughly provided in (Azizyan et al., 2015). However, a Gaussian matrix is dense and unstructured, which imposes an extra computational burden. Also, many matrix inversions take a considerable amount of time, and\nthe whole square matrix has to be loaded into the memory. Biased estimator 1n ∑n i=1 SiS T i xix T i SiS T i is thus proposed in (Anaraki & Hughes, 2014) to improve the efficiency by avoiding matrix inversions and assigning Si to be a sparse matrix. This method is less accurate because SiSTi approximates only an m-dimensional random orthogonal projection. Its another disadvantage is that the result only holds for data samples under statistical assumptions. Based on (Anaraki & Hughes, 2014), another study proposes an unbiased estimator (Anaraki, 2016), but it still adopts an unstructured sparse matrix that is insufficiently computation-efficient and fails to provide the error bounds to characterize the estimation error versus the compression ratio. Recently, sampling matrices Si ∈ Rd×m constructed via uniform sampling without replacement have been employed (Anaraki & Becker, 2017). This approach is efficient, but it only results in poor accuracy if data are compressed directly by STi xi ∈ Rd because SiSTi is an mdimensional orthogonal projection drawn only from d deterministic orthogonal spaces/coordinates, and the d − m entries of each vector are removed. To avoid sacrificing much accuracy, use of the computationally efficient Hadamard matrix (Tropp, 2011) before sampling has also been proposed in (Anaraki & Becker, 2017). It flattens out whole entries, particularly those with large magnitudes, to all coordinates to ensure that poor uniform sampling with a small sampling size still obtains some information among all entries. However, the Hadamard matrix involves deterministic orthogonal projection and is unable to capture the information uniformly in all coordinates of each vector, which results in the need for numerous samples to achieve sufficient accuracy. (Anaraki & Becker, 2017) constitutes the current state of the art in the tradeoff between the estimation accuracy and computational efficiency. Throughout the paper, we group the foregoing representative methods into Gauss-Inverse (Azizyan et al., 2015; Qi & Hughes, 2012), Sparse (Anaraki & Hughes, 2014; Anaraki, 2016), and UniSample-HD (Anaraki & Becker, 2017), and the unbiased estimators produced by these methods are adopted in the subsequent theoretical and empirical comparisons.\nA number of other methods have been proposed to recover covariance matrix from compressed data (Chen et al., 2013; Bioucas-Dias et al., 2014; Dasarathy et al., 2015; Cai et al., 2015). These methods are only applicable to low-rank, sparse, or statistically-assumed covariance matrices.\nInteresting work has also been done in the area of low-rank matrix approximation via randomized techniques. In addition to simply embedding the data X into space spanned by a single random projection matrix S, a representative study (Halko et al., 2011) improves approximation accuracy by replacing the random projection matrix S with a low-dimensional data-aware matrix XS′, where S′ is a random projection matrix. However, X has to be low-rank,\nand computing XS′ requires one extra pass through all entries in X. It is not suitable for our settings, where we do not impose structural assumptions on the covariance matrix, nor do we fully observe all data. Moreover, (Azizyan et al., 2015) demonstrates both theoretically and empirically that a single projection matrix for all data points cannot consistently and accurately estimate the covariance matrix. The problem also exist in (Wu et al., 2016; Mroueh et al., 2016) aiming for a fast approximation of matrix products in a single pass, which only results in an inconsistent covariance matrix estimation and suits the low-rank case.\nAmong randomized techniques, it is also worth briefly discussing sampling approaches in matrix approximation. Literature in (Drineas et al., 2006a; Papailiopoulos et al., 2014; Woodruff, 2014; Holodnak & Ipsen, 2015) proposes to leverage column sampling in which the sampling probabilities in the sampling matrix are either the column norms or leverage scores. Other work (Woodruff, 2014; Achlioptas & Mcsherry, 2007; Achlioptas et al., 2013) performs element-wise sampling on the entire matrix based on the relative magnitudes over all data entries. These researches employ different sampling distributions to sample entries in a matrix. However, they have to observe all data fully to calculate the sampling distributions, which also requires one or more extra passes. In addition, their sampling probabilities are designed for matrix approximation, which cannot be trivially extended to covariance matrix estimation because the exact covariance matrix in our setting cannot be calculated in advance. Note that although the uniform sampling in matrix approximation is a simple one-pass algorithm, it performs poorly on many problems because usually there exists structural non-uniformity in the data which has been verified in (Anaraki & Becker, 2017)."
  }, {
    "heading": "3. Our Approach",
    "text": "In this section, we first introduce the definition and background to our overall work. We then justify and present our method of data compression and covariance matrix estimation, followed by the primary results and analysis."
  }, {
    "heading": "3.1. Preliminaries",
    "text": "Let [k] denote a set of integers {1, 2, . . . , k}. Given a matrix X ∈ Rd×n, for j ∈ [d], i ∈ [n], we let xi ∈ Rd denote the i-th column of X, and xji denote the (j, i)-th element of X or j-th element of xi. Let {Xt}kt=1 denote the set of matrices {X1,X2, . . . ,Xk}, and xji,t denote the (j, i)-th element of Xt. Let XT denote the transpose of X, and Tr(X) denote its trace. Let |x| denote the absolute value of x. Let ‖X‖2 and ‖X‖F denote the spectral norm and Frobenius norm of X, respectively. Let ‖x‖q = ( ∑d j=1 |xj |q)1/q for q ≥ 1 be the `q-norm of x ∈ Rd. Let D(x) be a square\ndiagonal matrix with the elements of vector x on the main diagonal, and D(X) also be a square diagonal matrix whose main diagonal has only the main diagonal elements of X."
  }, {
    "heading": "3.2. Method and Algorithm",
    "text": "As discussed previously, Gauss-Inverse (Azizyan et al., 2015; Qi & Hughes, 2012) and Sparse (Anaraki & Hughes, 2014; Anaraki, 2016) suffer from deficiencies in either computational efficiency or estimation accuracy, whereas UniSample-HD (Anaraki & Becker, 2017) is less accurate but offers a good tradeoff between estimation accuracy and computational efficiency. We thus propose the adoption of weighted sampling matrices {Si}ni=1 ∈ Rd×m to compress data via STi xi and then back-project the compressed data into the original space via SiSTi xi. The recovered data is then used for covariance matrix estimation as shown in Eq. (1). Hence, a high computational efficiency is maintained. Although Si removes at least d−m entries from the i-th vector, the remainders can be the most informative and are retained. With the carefully designed sampling probabilities, our unbiased estimator Ce performs as accurately as or more accurately than its counterparts asymptotically in terms of matrix spectral norm ‖Ce − C‖2. Note we have not quantified the error in any other entry-wise norm (e.g., the Frobenius norm) that could be uninformative on the quality of the approximate invariant subspace and unstable regarding the additive random error (Anaraki, 2016; Achlioptas et al., 2013; Gittens, 2011).\nAlgorithm 1 The proposed algorithm. Input:\nData X ∈ Rd×n, sampling size m, and 0 < α < 1. Output:\nEstimated covariance matrix Ce ∈ Rd×d. 1: Initialize Y ∈ Rm×n, T ∈ Rm×n, v ∈ Rn, and w ∈\nRn with 0. 2: for all i ∈ [n] do 3: Load xi into memory, let vi = ‖xi‖1 = ∑d k=1 |xki|\nand wi = ‖xi‖22 = ∑d k=1 x 2 ki\n4: for all j ∈ [m] do 5: Pick tji ∈ [d] with pki ≡ P(tji = k) = α |xki|vi +\n(1− α)x 2 ki\nwi , and let yji = xtjii\n6: Pass the compressed data Y, sampling indices T, v, w, and α to the fusion center. 7: for all i ∈ [n] do 8: Initialize Si ∈ Rd×m and P ∈ Rd×n with 0 9: for all j ∈ [m] do\n10: Let ptjii = α |yji| vi + (1 − α)y 2 ji wi , and stjij,i =\n1√ mptjii\n11: Compute Ce as defined in Eq. (1) by using {Si}ni=1, T, P, and Y.\nWe here summarize our method in Algorithm 1. In a nutshell, we employ a weighted sampling that is able to explore the most important entries to reduce estimation error ‖Ce − C‖2. Steps 1 to 5 in our proposed algorithm show how to compress distributed data in many remote sites. In step 5, each entry is retained with probability proportional to the combination of its relative absolute value and square value, and such sampling probability is designed to make ‖Ce − C‖2 as small as possible. Step 6 shows the communication procedure, and steps 7 to 11 reveal how to construct an unbiased covariance matrix estimator in the fusion center from compressed data. In many computing cases, it is possible to manipulate vectors of length O(d) in memory, and thus when compressing data via weighted sampling, only one pass is required to move data from the external space to memory. Hence, our algorithm is also applicable to streaming data. For a covariance matrix defined as C = 1nXX\nT − x̄x̄T , we can exactly calculate x̄ = 1n ∑n i=1 xi in the fusion center via x̄ = 1 n ∑g j=1 uj , where {xi}ni=1 are from g n remote sites, and uj ∈ Rd is the summation of all data vectors in the j-th remote site before being compressed. Doing so makes no deviation on the following error analysis and imposes only a negligible computational burden."
  }, {
    "heading": "3.3. Primary Provable Results",
    "text": "In this part, we introduce the proposed covariance matrix estimator. In Algorithm 1, we employ Y, T, v, and w to calculate {Si}ni=1. It can be verified that using only {Si}ni=1 and Y is able to obtain {STi xi}ni=1. Thus, we describe our estimator via {Si}ni=1 and {STi xi}ni=1 in the following theorem, which shows our estimator is unbiased. Theorem 1. Assume X ∈ Rd×n and the sampling size 2 ≤ m < d. Sample m entries from each xi ∈ Rd with replacement by running Algorithm 1. Let {pki}dk=1 and Si ∈ Rd×m denote the sampling probabilities and sampling matrix, respectively. Then, the unbiased estimator for the target covariance matrix C = 1n ∑n i=1 xix T i = 1 nXX\nT can be recovered as\nCe = Ĉ1 − Ĉ2, (1) where E [Ce] = C, Ĉ1 = mnm−n ∑n i=1 SiS T i xix T i SiS T i ,\nand Ĉ2 = mnm−n ∑n i=1 D(SiSTi xixTi SiSTi )D(bi) with bki = 1\n1+(m−1)pki .\nNote that at mostm entries in each bi have to be calculated because each SiSTi xix T i SiS T i has at most m non-zero entries in its diagonal. Now, having achieved the above unbiased estimator Ce, we analyze its properties. We precisely upper bound the estimation error for the original estimator C in the matrix spectral norm. Theorem 2. Given X ∈ Rd×n and the sampling size 2 ≤ m < d, let C and Ce be defined as in Theorem 1. If the\nsampling probabilities satisfy pki = α |xki| ‖xi‖1 +(1−α) x2ki ‖xi‖22 with 0 < α < 1 for all k ∈ [d] and i ∈ [n], then with probability at least 1− η − δ,\n‖Ce −C‖2 ≤ log( 2d δ ) 2R 3 +\n√ 2σ2 log( 2d\nδ ), (2)\nwhere R = maxi∈[n] [ 7‖xi‖22 n + log 2( 2ndη ) 14‖xi‖21 nmα2 ] , and\nσ2 = ∑n i=1 [ 8‖xi‖42 n2m2(1−α)2 + 4‖xi‖21‖xi‖ 2 2 n2m3α2(1−α) + 9‖xi‖42 n2m(1−α) + 2‖xi‖22‖xi‖ 2 1\nn2m2α(1−α)\n] + ‖ ∑n i=1 ‖xi‖21xix 2 i n2mα ‖2.\nA large R and σ2 work against the accuracy of Ce. Accordingly, our sampling probabilities are designed to make R and σ2 as small as possible to improve the accuracy. In the proof of Theorem 2, we also show that the selection of q = 1, 2 in |xki|\nq∑d k=1 |xki|q used for constructing the sampling\nprobability pki = α |xki| ‖xi‖1 + (1− α) x2ki ‖xi‖22\nis necessary and sufficient to make the error bound considerably tight.\nFurthermore, α balances the performance by `1-norm based sampling |xki|‖xi‖1 and `2-norm based sampling x2ki ‖xi‖22\n. `2 sampling penalizes small entries more than `1 sampling. Hence `2 sampling is more likely to select larger entries to decrease error. However, as seen from the proof in the appendix, different from `1 sampling, `2 sampling is unstable and sensitive to small entries, and it can make estimation error incredibly high if extremely small entries are picked. Hence, if α varies from 1 to 0, the estimation error will decrease and then increase, which is also empirically verified in the appendix.\nThe error bound in Theorem 2 involves many datadependent quantities, whereas our primary interest lies in studying the tradeoff between the computational efficiency and estimation accuracy by employing weighted sampling to compress data and estimate covariance matrix. To clarify, we modify Theorem 2 and make the bound explicitly dependent on n, d, and m with the constraint 2 ≤ m < d. Corollary 1. Given X ∈ Rd×n and the sampling size 2 ≤ m < d, let C and Ce be created by Algorithm 1. Define ‖xi‖1 ‖xi‖2 ≤ ϕ with 1 ≤ ϕ ≤ √ d, and ‖xi‖2 ≤ τ for all i ∈ [n]. Then, with probability at least 1− η − δ we have\n‖Ce −C‖2 ≤ min{Õ ( f + τ2ϕ\nm\n√ 1\nn + τ2\n√ 1\nnm\n) ,\nÕ ( f + τϕ\nm √ d‖C‖2 n + τ √ d‖C‖2 nm ) }, (3)\nwhere f = τ 2 n + τ2ϕ2 nm + τϕ √ ‖C‖2 nm , and Õ(·) hides the logarithmic factors on η, δ, m, n, d, and α.\nThe formulation above explores the fact that 1 ≤ ‖xi‖1/‖xi‖2 ≤ √ d by the Cauchy-Schwarz inequality.\nBefore proceeding, we make several remarks to make a comparison with the following representative work: GaussInverse, UniSample-HD, and Sparse. The first two methods provide error analysis without assuming data distribution, which is shown in (Azizyan et al., 2015; Anaraki & Becker, 2017) and illustrated in our appendix. In the following remarks, only our method is sensitive to ϕ, and we also employ the fact that 1nd‖X‖ 2 F ≤ ‖C‖2 ≤ maxi∈[n] ‖xi‖22 = τ2 to simplify all asymptotic bounds. Remark 1. Eq. (3) with ϕ = √ d indicates the error bound for our estimator Ce in the worst case, where the magnitudes of each entry in all of the input data vectors are the same (i.e., highly uniformly distributed). Even in this case, our error bound has a leading term of order min{Õ ( τ2d nm +\nτ √\nd‖C‖2 nm + τ2 m √ d n ) , Õ ( τ2d nm + τd m √ ‖C‖2 n ) }, which is the same as Gauss-Inverse ignoring logarithmic factors. In contrast, as the magnitudes of the entries in each data vector become uneven, ϕ gets smaller, leading to a tighter error bound than that in Gauss-Inverse. Furthermore, when most of the entries in each vector xi have very low magnitudes, the summation of these magnitudes will be comparable to a particular constant. This situation is typical because in practice only a limited number of features in each input data dominate the learning performance. Hence, ϕ turns to O(1), and Eq. (3) becomes\nmin{Õ ( τ2 n + τ 2 √ 1 nm ) , Õ ( τ2 n + τ √ d‖C‖2 nm ) }, which is tighter than the leading term of Gauss-Inverse by a factor of at least √ d/m. As explained in the next section, GaussInverse also lacks computational efficiency.\nRemark 2. As our target is to compress data to a smaller m that is not comparable to d in practice, O(d − m) can be approximately regarded as O(d). Then, the error\nof UniSample-HD is Õ ( τ2d nm + τ √ d‖C‖2 nm + τ2d m √ 1 nm ) , which is asymptotically worse than our bound. When n is sufficiently large, the leading term of its error becomes\nÕ ( τ √\nd‖C‖2 nm + τ2d m √ 1 nm ) , which can be weaker than the\nleading term in our method by a factor of 1 to √ d/m when ϕ = √ d, and at least d/m when ϕ = O(1).\nHowever, if m is sufficiently close to d, which is not meaningful for practical usage, O(d−m) = O(1) will hold and the error of UniSample-HD becomes Õ ( τ2d nm +τ √ d‖C‖2 nm +\nτ2 m √ d nm ) . This bound may slightly outperform ours by a\nfactor of √ d/m = O(1) when ϕ = √ d, but is still worse than ours when ϕ = O(1). These results also coincide with the fact that UniSample-HD adopts uniform sampling without replacement combined with the Hadamard matrix, but we employ weighted sampling with replacement.\nRemark 3. The Sparse method, which employs a sparse\nmatrix for each Si, is not sufficiently accurate as demonstrated in our experiments. Moreover, there is no error analysis available for its unbiased estimator to characterize the estimation error versus the compression ratio.\nThus far, we have not made statistical nor structural assumptions concerning the input data or covariance matrix to derive our provable results. However, motivated by (Azizyan et al., 2015), it is also straightforward to extend our results to the statistical data and a low-rank covariance matrix estimation. The derived results below are polynomially equivalent to those in Gauss-Inverse (Azizyan et al., 2015). Corollary 2 shows the (low-rank) covariance matrix estimation on Gaussian data, and Corollary 3 indicates the derived covariance estimator also guarantees the accuracy of the principal components regarding the subspace learning.\nCorollary 2. Given X ∈ Rd×n (2 ≤ d) and an unknown population covariance matrix Cp ∈ Rd×d with each column vector xi ∈ Rd i.i.d. generated from the Gaussian distribution N (0,Cp). Let Ce be constructed by Algorithm 1 with the sampling size 2 ≤ m < d. Then, with probability at least 1− η − δ − ζ,\n‖Ce −Cp‖2 ‖Cp‖2 ≤ Õ ( d2 nm + d m √ d n ) ; (4)\nAdditionally, assuming rank(Cp)≤ r, with probability at least 1− η − δ − ζ we have\n‖[Ce]r −Cp‖2 ‖Cp‖2 ≤Õ ( rd nm + r m √ d n + √ rd nm ) , (5)\nwhere [Ce]r is the solution to minrank(A)≤r ‖A − Ce‖2, and Õ(·) hides the logarithmic factors on η, δ, ζ, m, n, d, and α.\nCorollary 3. Given X, d, m, Cp and Ce as defined in Corollary 2. Let ∏ k = ∑k i=1 uiu T i and ∏̂ k =∑k\ni=1 ûiû T i with {ui}ki=1 and {ûi}ki=1 being the leading k eigenvectors of Cp and Ce, respectively. Denote by λk the k-th largest eigenvalue of Cp. Then, with probability at least 1− η − δ − ζ,\n‖ ∏̂ k − ∏ k ‖2\n‖Cp‖2 ≤ 1 λk − λk+1 Õ ( d2 nm + d m √ d n ) , (6)\nwhere the eigengap λk − λk+1 > 0 and Õ(·) hides the logarithmic factors on η, δ, ζ, m, n, d, and α.\nThe proof details of all our theoretical results are relegated to the appendix. We leverage the Matrix Bernstein inequality (Tropp, 2015) and establish the error bound of our proposed estimator on an arbitrary sampling probability in order to determine which sampling probability brings the best estimation accuracy. The employment of the Matrix Bernstein inequality involves controlling the range and\nvariance of all zero-mean random matrices, whose derivations differ from those in (Azizyan et al., 2015; Anaraki & Becker, 2017) because of different data compression schemes. Moreover, to obtain the desired tight bound for the range and variance, we precisely provide a group of closed-form equalities or concentration inequalities for various quantities (see our proposed Lemma 1 and Lemma 2 along with their proofs in the appendix)."
  }, {
    "heading": "3.4. Computational Complexity",
    "text": "Recall that we have n data samples in the d-dimensional space, and let m be the target compressed dimension. Regarding estimating C = 1nXX\nT , the computational comparisons between our method and the representative baseline methods are presented in Table 1, in which Standard method means that we compute C directly without data compression. For the definition of covariance matrix C = 1nXX\nT −x̄x̄T , extra computational costs (i.e.,O(gd) storage,O(gd) communication cost, andO(nd) time) must be added to the last four compression methods in the table, where g n is the number of the entire remote sites. All detailed analysis is relegated to the appendix.\nTG and TS in Table 1 represent the time taken to generate the Gaussian matrices and sparse matrices by fast pseudo-random number generators like Mersenne Twister (Matsumoto & Nishimura, 1998), which can be enormous (Anaraki & Becker, 2017) and proportional to nmd and nd2, respectively, up to certain small constants. Hence, our method can be regarded as the most efficient when d is large. Furthermore, by using the smallest m to obtain the same estimation accuracy as the other methods, our approach incurs the least computational burden."
  }, {
    "heading": "4. Empirical Studies",
    "text": "In this section, we empirically verify the properties of the proposed method and demonstrate its superiority. We compare its estimation accuracy with that of Gauss-Inverse, Sparse, and UniSample-HD. We also report the time comparisons.\nWe run all algorithms on both synthetic and real-world datasets whose largest dimension is around and below 105. Such dimension is not very high in modern data analysis, but this limitation is due to that reporting the estimation error by calculating the spectral norm of a covariance matrix with its size larger than 105 × 105 will take intolerable amount of memory and time. The parameter selection on α is deferred to the appendix, and we empirically set α = 0.9. To allow a fair comparison of the time consumptions measured by FLOPS, we implement all algorithms in C++ and run them in a single thread mode on a standard workstation with Intel CPU@2.90GHz and 128GB RAM."
  }, {
    "heading": "4.1. Experiments on Synthetic Datasets",
    "text": "To clearly examine the performance, we compare all methods on six synthetic datasets: {Xi}3i=1 ∈ R1024×20000, X4 ∈ R1024×200000, X5 ∈ R2048×200000, and X6 ∈ R65536×200000, which are generated based on the generative model (Liberty, 2013). Specifically, given a matrix X ∈ Rd×n from such model, it is formally defined as X = UFG, where U ∈ Rd×k defines the signal column space with UTU = Ik (k ≤ d), the square diagonal matrix F ∈ Rk×k contains the diagonal entries fii = 1−(i−1)/k that gives linearly diminishing signal singular values, and G ∈ Rk×n is the signal coefficient with gij ∼ N (0, 1) that is the Gaussian distribution. We let k ≈ 0.005d, then setting d = 1024 and n = 20000 completes the creation of data X1. For X2, it is defined as DX, where each entry in the square diagonal matrix D is defined by dii = 1/βi, and βi is randomly sampled from the integer set [15]. Regarding X3, it is constructed in the same way as X1 except that F now becomes an identity matrix. Next, {Xi}6i=4 follow the same generation strategy of X2 except for the n and d.\nIn Figure 1, we plot the relative estimation error averaged over ten runs with its standard deviation versus the naive compression ratio cf = m/d. Note that a large cf is not necessary for practical usage, and our aim is to compress data to a smaller volume. In Figure 2, we report the running time taken in both the compressing and recovering stages, which preliminarily depicts the efficiency of the different methods and indicates how much power should be spent in the practical computation.\nGenerally, our method displays the least error and deviation for all datasets and its error decreases dramatically with an increase at a small cf . This observation indicates that our method can achieve sufficient estimation accuracy by using substantially fewer data entries than the other methods. For X1 (ϕ = 0.81 √ d), the magnitudes of the data entries are highly uniformly distributed, and thus our method can be regarded as uniform sampling with replacement, which may perform slightly worse than UniSampleHD and Gauss-Inverse if cf becomes large enough. After allowing the magnitudes to vary within a moderately larger range in X2 (ϕ = 0.55 √ d), our method considerably outperforms the other three methods. Its improvement comes from that only our method is sensitive to ϕ and a smaller ϕ produces a tighter result, as demonstrated by Remarks 1 and 2. However, the error of each method in X3 (τ/ √ ‖C‖2 = 5.5, ϕ = 0.81 √ d) is larger than that in X1\n(τ/ √ ‖C‖2 = 4.3, ϕ = 0.81 √ d), respectively. It is be-\ncause of that almost all methods are sensitive to τ/ √ ‖C‖2,\nand the error ‖Ce−C‖2/‖C‖2 increases when τ/ √ ‖C‖2 rises. Such phenomenon is demonstrated via dividing numerous error bounds in Remarks 1 and 2 by ‖C‖2. Our method also achieves the best performance in X4. Although the ϕ and τ/ √ ‖C‖2 in X4 are approximately equal with those in X2, yet the proved error bounds with Remarks 1 and 2 reveal that a larger n in X4 will lead to smaller estimation errors given the same cf . Finally, our method also achieves the best accuracy when the dimension d increases in both X5 and X6. Besides, taking more data (i.e., enlarging n) as suggested by X4 can be considered to reduce the error in X5 and X6. Note that Gauss-Inverse has not been run on X6 since it costs enormous time.\nTurning to Gauss-Inverse, it becomes highly accurate when\ncf increases but requires much more time than Standard (see Figure 2) so that its usage might be ruled out in practice. However, Gauss-Inverse remains a good choice when we are in urgent need of reducing the storage and communication burden. Sparse, which has no error analysis of its unbiased estimator, generally performs less accurately than the others but requires less time than Standard. UniSampleHD is efficient while it still consumes more time than our method. Also, its accuracy is inferior to our method especially when cf is small. In conclusion, our method is capable of compressing data to a very small size while guaranteeing both estimation accuracy and computational efficiency.\nAs confirmed in Figure 1, a large n benefits the estimation accuracy. Thus, we study its effect more quantitatively. We conduct experiments following the settings as defined in Corollaries 2 and 3, and their results in Eqs. (4)-(6) clearly show that the errors decay in 1/ √ n convergence rate if d n. We run our method on another two synthetic datasets {Xt}8t=7 ∈ Rd×n that follow the d-dimensional multivariate normal distribution N (0,Cpt), where the (i, j)-th element of Cp7 ∈ Rd×d is 0.5|i−j|/50, and Cp8 ∈ Rd×d is a low-rank matrix that satisfies minrank(A)≤r ‖A−Cp7‖2. We take d = 1000, r = 5, m/d = {0.02, 0.05, 0.15}, k = {5, 10, 15}, and vary n from 1000 to 100000. In Figure 3, the top three plots report the errors as defined in the LHS of Eqs. (4)-(6), respectively. Then, dividing such errors by 1/ √ n obtains the bottom three plots accordingly.\nThe observation that the curves in plots (d)-(f) are roughly flat validates that the error bounds induced by our method decay rapidly with n in the 1/ √ n convergence rate, which coincides with Eqs. (4)-(6). In addition to the fast error convergence for the low-rank matrix Cp8, our method can also obtain an increasingly better estimation accuracy for a high-rank covariance matrix Cp7 if we enlarge n, which is displayed in plot (a). Besides, considering the omitted plot where the eigengap λk−λk+1 of Cp7 decreases with k, the fact that the errors in plot (c) increase with k also coheres with Eq. (6). To conclude, our method also adapts well to the specific settings in Corollaries 2 and 3, and all induced error bounds indeed satisfy a 1/ √ n convergence rate."
  }, {
    "heading": "4.2. Experiments on Real-world Datasets",
    "text": "In the second set of experiments, we use nine publicly available real-world datasets (Chang & Lin, 2011; Blake & Merz, 1998; Amsaleg, 2010), some of which are gathered from many distributed sensors. Their statistics are displayed in Figure 4. We again compare the estimation accuracy of the proposed method against the other three approaches. As can be seen from the figure, our method consistently exhibits superior accuracy over all cf = m/d, and its error decreases dramatically when cf grows. The error of the other three methods also decreases with cf but is still large at a small cf . Besides, our method enjoys the least deviation. In summary, these results confirm that our method can compress data to the lowest volume with the best accuracy, thereby substantially reducing storage, communication, and processing time cost in practice."
  }, {
    "heading": "5. Conclusion",
    "text": "In this paper, we describe a weighted sampling method for accurate and efficient calculation of an unbiased covariance matrix estimator. The analysis demonstrates that our method can employ a smaller data volume than the other approaches to achieve an equal accuracy, and is highly efficient regarding the communication, storage, and processing time. The empirical results of the algorithm’s application to both synthetic and real-world datasets further support our analysis and demonstrate its significant improvements over other state-of-the-art methods.\nCompared with the sampling-with-replacement scheme in this paper, we seek to make more achievements via a sampling-without-replacement scheme in the future work. Analyzing the corresponding unbiased estimator will pose significant technical challenges in this research direction."
  }, {
    "heading": "Acknowledgments",
    "text": "We truly thank Akshay Krishnamurthy for the fruitful discussions and interpretations on (Azizyan et al., 2015). We also thank Yuxin Su for the help on the experiments. The work described in this paper was fully supported by the National Natural Science Foundation of China (Project No. 61332010), the Research Grants Council of the Hong Kong Special Administrative Region, China ((No. CUHK 14208815 and No. CUHK 14234416 of the General Research Fund), and 2015 Microsoft Research Asia Collaborative Research Program (Project No. FY16-RESTHEME-005)."
  }],
  "year": 2017,
  "references": [{
    "title": "Toward cluster-based weighted compressive data aggregation in wireless sensor networks",
    "authors": ["S. Abbasi-Daresari", "J. Abouei"],
    "venue": "Ad Hoc Networks,",
    "year": 2016
  }, {
    "title": "Enhanced covariance matrix estimators in adaptive beamforming",
    "authors": ["R. Abrahamsson", "Y. Selen", "P. Stoica"],
    "venue": "In Acoustics, Speech and Signal Processing,",
    "year": 2007
  }, {
    "title": "Fast computation of lowrank matrix approximations",
    "authors": ["D. Achlioptas", "F. Mcsherry"],
    "venue": "Proceedings of the annual ACM symposium on Theory of computing,",
    "year": 2007
  }, {
    "title": "Near-optimal entrywise sampling for data matrices",
    "authors": ["D. Achlioptas", "Z.S. Karnin", "E. Liberty"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "The fast johnson-lindenstrauss transform and approximate nearest neighbors",
    "authors": ["N. Ailon", "B. Chazelle"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Datasets for approximate nearest neighbor search",
    "authors": ["L. Amsaleg"],
    "year": 2010
  }, {
    "title": "Estimation of the sample covariance matrix from compressive measurements",
    "authors": ["F. Anaraki"],
    "venue": "IET Signal Processing,",
    "year": 2016
  }, {
    "title": "Preconditioned data sparsification for big data with applications to pca and k-means",
    "authors": ["F. Anaraki", "S. Becker"],
    "venue": "IEEE Transactions on Information",
    "year": 2017
  }, {
    "title": "Memory and computation efficient pca via very sparse random projections",
    "authors": ["F. Anaraki", "S. Hughes"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Pattern Recognition ",
    "authors": ["Y. Anzai"],
    "venue": "Machine Learning. Elsevier,",
    "year": 2012
  }, {
    "title": "Extreme compressive sampling for covariance estimation",
    "authors": ["M. Azizyan", "A. Krishnamurthy", "A. Singh"],
    "venue": "arXiv preprint arXiv:1506.00898,",
    "year": 2015
  }, {
    "title": "Advances in high-dimensional covariance matrix estimation",
    "authors": ["D. Bartz"],
    "year": 2016
  }, {
    "title": "Covalsa: Covariance estimation from compressive measurements using alternating minimization",
    "authors": ["J. Bioucas-Dias", "D. Cohen", "Y.C. Eldar"],
    "venue": "In Signal Processing Conference (EUSIPCO),",
    "year": 2014
  }, {
    "title": "Discovering functional relationships between rna expression and chemotherapeutic susceptibility using relevance networks",
    "authors": ["A.J. Butte", "P. Tamayo", "D. Slonim", "T.R. Golub", "I.S. Kohane"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2000
  }, {
    "title": "Rop: Matrix recovery via rankone projections",
    "authors": ["T.T. Cai", "A Zhang"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Libsvm: a library for support vector machines",
    "authors": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
    "year": 2011
  }, {
    "title": "Exact and stable covariance estimation from quadratic sampling via convex programming",
    "authors": ["Y. Chen", "Y. Chi", "A. Goldsmith"],
    "year": 2013
  }, {
    "title": "Sketching sparse matrices, covariances, and graphs via tensor products",
    "authors": ["G. Dasarathy", "P. Shah", "B.N. Bhaskar", "R.D. Nowak"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2015
  }, {
    "title": "A survey on policy search for robotics",
    "authors": ["M.P. Deisenroth", "G. Neumann", "J Peters"],
    "venue": "Foundations and Trends in Robotics,",
    "year": 2013
  }, {
    "title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication",
    "authors": ["P. Drineas", "R. Kannan", "M.W. Mahoney"],
    "venue": "SIAM Journal on Computing,",
    "year": 2006
  }, {
    "title": "Subspace sampling and relative-error matrix approximation",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"],
    "venue": "In Approximation, Randomization, and Combinatorial Optimization",
    "year": 2006
  }, {
    "title": "introduction to probability theory and its applications",
    "authors": ["W. Feller"],
    "venue": "vol. ii.[an]",
    "year": 1966
  }, {
    "title": "The spectral norm error of the naive nystrom extension",
    "authors": ["A. Gittens"],
    "venue": "arXiv preprint arXiv:1110.5305,",
    "year": 2011
  }, {
    "title": "Blind compressed sensing",
    "authors": ["S. Gleichman", "Y.C. Eldar"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2011
  }, {
    "title": "Robust pca with compressed data",
    "authors": ["W. Ha", "R.F. Barber"],
    "venue": "In Advances in Neural Information Processing Systems, pp. 1936–1944,",
    "year": 2015
  }, {
    "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
    "authors": ["N. Halko", "P. Martinsson", "J.A. Tropp"],
    "venue": "SIAM review,",
    "year": 2011
  }, {
    "title": "Large sample properties of generalized method of moments estimators",
    "authors": ["L.P. Hansen"],
    "venue": "Econometrica: Journal of the Econometric Society,",
    "year": 1982
  }, {
    "title": "Compressed sensing for networked data",
    "authors": ["J. Haupt", "W.U. Bajwa", "M. Rabbat", "R. Nowak"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2008
  }, {
    "title": "Randomized approximation of the gram matrix: Exact computation and probabilistic bounds",
    "authors": ["J.T. Holodnak", "I.C. Ipsen"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2015
  }, {
    "title": "Independent component analysis, volume 46",
    "authors": ["A. Hyvärinen", "J. Karhunen", "E. Oja"],
    "year": 2004
  }, {
    "title": "Generalized least squares",
    "authors": ["T. Kariya", "H. Kurata"],
    "year": 2004
  }, {
    "title": "Very sparse random projections",
    "authors": ["P. Li", "T.J. Hastie", "K.W. Church"],
    "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,",
    "year": 2006
  }, {
    "title": "Simple and deterministic matrix sketching",
    "authors": ["E. Liberty"],
    "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2013
  }, {
    "title": "Randomized algorithms for matrices and data",
    "authors": ["M. Mahoney"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "Mersenne twister: a 623-dimensionally equidistributed uniform pseudorandom number generator",
    "authors": ["M. Matsumoto", "T. Nishimura"],
    "venue": "ACM Transactions on Modeling and Computer Simulation,",
    "year": 1998
  }, {
    "title": "Co-occuring directions sketching for approximate matrix multiply",
    "authors": ["Y. Mroueh", "E. Marcheret", "V. Goel"],
    "venue": "arXiv preprint arXiv:1610.07686,",
    "year": 2016
  }, {
    "title": "Provable deterministic leverage score sampling",
    "authors": ["D. Papailiopoulos", "A. Kyrillidis", "C. Boutsidis"],
    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2014
  }, {
    "title": "Invariance of principal components under low-dimensional random projection of the data",
    "authors": ["H. Qi", "S.M. Hughes"],
    "venue": "In Image Processing",
    "year": 2012
  }, {
    "title": "An empirical bayes approach to inferring large-scale gene association",
    "authors": ["J. Schäfer", "K. Strimmer"],
    "venue": "networks. Bioinformatics,",
    "year": 2005
  }, {
    "title": "Correlated compressive sensing for networked data",
    "authors": ["T. Shi", "D. Tang", "L. Xu", "T. Moscibroda"],
    "venue": "In UAI, pp",
    "year": 2014
  }, {
    "title": "Practical data compression in wireless sensor networks: A survey",
    "authors": ["T. Srisooksai", "K. Keamarungsi", "P. Lamsrichan", "K. Araki"],
    "venue": "Journal of Network and Computer Applications,",
    "year": 2012
  }, {
    "title": "Improved analysis of the subsampled randomized hadamard transform",
    "authors": ["J.A. Tropp"],
    "venue": "Advances in Adaptive Data Analysis,",
    "year": 2011
  }, {
    "title": "An introduction to matrix concentration inequalities",
    "authors": ["J.A. Tropp"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2015
  }, {
    "title": "Random matrix theory and wireless communications, volume 1",
    "authors": ["A.M. Tulino", "S. Verdú"],
    "venue": "Now Publishers Inc,",
    "year": 2004
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["D.P. Woodruff"],
    "venue": "arXiv preprint arXiv:1411.4357,",
    "year": 2014
  }, {
    "title": "Single pass pca of matrix products",
    "authors": ["S. Wu", "S. Bhojanapalli", "S. Sanghavi", "A. Dimakis"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Sparse principal component analysis",
    "authors": ["H. Zou", "T. Hastie", "R. Tibshirani"],
    "venue": "Journal of computational and graphical statistics,",
    "year": 2006
  }],
  "id": "SP:8b81c961925c91c250848f94551b60a704c9d8b1",
  "authors": [{
    "name": "Xixian Chen",
    "affiliations": []
  }, {
    "name": "Michael R. Lyu",
    "affiliations": []
  }, {
    "name": "Irwin King",
    "affiliations": []
  }],
  "abstractText": "Estimating covariance matrices is a fundamental technique in various domains, most notably in machine learning and signal processing. To tackle the challenges of extensive communication costs, large storage capacity requirements, and high processing time complexity when handling massive high-dimensional and distributed data, we propose an efficient and accurate covariance matrix estimation method via data compression. In contrast to previous data-oblivious compression schemes, we leverage a data-aware weighted sampling method to construct lowdimensional data for such estimation. We rigorously prove that our proposed estimator is unbiased and requires smaller data to achieve the same accuracy with specially designed sampling distributions. Besides, we depict that the computational procedures in our algorithm are efficient. All achievements imply an improved tradeoff between the estimation accuracy and computational costs. Finally, the extensive experiments on synthetic and real-world datasets validate the superior property of our method and illustrate that it significantly outperforms the state-of-the-art algorithms.",
  "title": "Toward Efficient and Accurate Covariance Matrix  Estimation on Compressed Data"
}