{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Learning from time-series data is of paramount importance for prediction, anomaly detection, classification, and other critical tasks that appear in business and society. Various models of time-series have been studied in the literature to better learn from time-series data. These include vector autoregressive (VAR) models (Lütkepohl, 2005), hidden Markov models (HMM) (Baum & Petrie, 1966), and recurrent neural networks (RNN) (Rumelhart et al., 1986), including long short term memory (LSTM) (Hochreiter & Schmidhuber, 1997) and echo state networks (ESN) (Jaeger & Haas, 2004). With these models of time-series, one seeks to learn the relation between past values and future values.\nIn some of these models of time-series, hidden units (or latent variables) play essential roles in taking into account long term dependency or non-linearity in time-series. Hid-\n1IBM Research - Tokyo, Tokyo, Japan. Correspondence to: Takayuki Osogami <osogami@jp.ibm.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nden units, however, make it difficult to learn the parameters of those models. For example, the Baum-Welch algorithm (Baum & Petrie, 1966) learns the parameters of an HMM by iteration of expectation and maximization (i.e., an EM algorithm). An RNN, including LSTM, is trained via back propagation through time (Rumelhart et al., 1986). These algorithms are time-consuming and do not necessarily find optimal values of the parameters.\nThis difficulty in learning a model with hidden units partly stems from the fact that the values of hidden units can only be reliably estimated after observing future values of target time-series. It then requires iteration or back propagation to learn the relation between the hidden values and preceding values. An ESN, on the other hand, gives up learning the hard-to-learn parameters between hidden values and preceding (visible or hidden) values and set those parameters randomly. The ESN only learns the relation between visible values and preceding (hidden) values (Jaeger & Haas, 2004).\nWe study a model of time-series whose parameters can be represented as a matrix M or a set of such matrices. An element Mi,j of the matrix may, for example, represent the weight between a past value of a unit i and a future value of a unit j. In a VAR model, each matrix corresponds to the coefficients for a particular lag. In an RNN, a matrix corresponds to the weight between hidden units. We consider a situation where it is hard to estimate an appropriate value of Mi,j for some of the units j ∈ H (in particular, H may denote the set of hidden units).\nWe propose a method of training a time-series model with hidden units in a bidirectional manner, one from the past and the other from the future. From a time-series model with parameter M, we construct a backward model, whose parameters are represented by the transposed matrix M> or a set of such transposed matrices. The (i, j)-th element of M> is Mj,i, which represents the weight between a preceding value of a unit j and a succeeding value of a unit i. Our key idea is to let the preceding value represent a future value and the succeeding value represent a past value in the backward model. Then an intuitive meaning of Mj,i in the backward model matches that in the original (forward) model. We use a common matrix M both in the forward model and in the backward model. Namely, the parameters are shared between the two models.\nThe two models have an identical structure and trained in an identical manner with a stochastic gradient method (Bottou, 2009) except that we train the forward model using timeseries in a standard (forward) manner and the backward model using the time-series from the future to the past. We alternately train the forward model to learn M and the backward model to learn M>. An advantage of our bidirectional training is that the elements of M that are hard to estimate differ between when we train the forward model and when we train the backward model. For the forward model, it is hard to estimate Mi,j for j ∈ H (i.e., M:,H). For the backward model, it is hard to estimate Mj,i for j ∈ H (i.e., MH,: = (M\n>):,H). Although MH,H is hard to estimate for both models, the other elements of M can be reliably estimated in either of the two models. This idea of bidirectional training for learning time-series models with hidden units constitutes our first contribution.\nHere, we extend a Dynamic Boltzmann Machine (DyBM) to incorporate hidden units and apply bidirectional training to learn its parameters. The DyBM has been proposed by Osogami & Otsuka (2015a;b) and subsequently studied by Dasgupta et al. (2016); Dasgupta & Osogami (2017); Kajino (2017). The prior work on the DyBM does not consider hidden units but instead uses various features of past values to capture the long term dependency in time-series. For example, Dasgupta & Osogami (2017) use an ESN to create nonlinear features of past values. In our context, these features are fed into visible units of a DyBM. In particular, what features of past values are used is determined randomly without learning. Our analysis of the DyBM with hidden units illuminates the difficulty of learning some of its parameters. With bidirectional learning, we seek to learn the weight from the past visible values to the future hidden values, which corresponds to learning what features of past values are effective for prediction. The DyBM with hidden units and its analysis constitute our second contribution.\nWe validate the effectiveness of bidirectional training and the hidden units in DyBMs through numerical experiments using synthetic and real time-series. We will show that the DyBM with hidden units can be trained more effectively with bidirectional training and reduces the predictive error by up to 90 %."
  }, {
    "heading": "1.1. Related Work",
    "text": "Our bidirectional training is related to but different from bidirectional recurrent neural networks (BRNN) (Schuster & Paliwal, 1997; Baldi et al., 1999), including bidirectional LSTM (Graves & Schmidhuber, 2005; Chen & Chaudhari, 2005). Similar to our bidirectional training, a BRNN trains both a forward model and a backward model. These two models, however, do not share parameters, contrary to our bidirectional training. In fact, motivation and purpose of the\nBRNN are quite different from ours. The BRNN uses both the sequence from the past and the sequence from the future to better estimate missing values. The BRNN thus needs both of the two sequences for learning and for prediction. On the other hand, our bidirectional training uses both a forward sequence and a backward sequence at the time of learning, but the trained model uses only the sequence from the past to predict future unseen values. Namely, our bidirectional training is used to learn a model for predicting future values from past values, while the BRNN is used for estimating missing values from past and future values.\nOur bidirectional training is also related to but different from Forward Backward Lasso Granger (FBLG) by Cheng et al. (2014). In FBLG, forward and backward VAR models are estimated with Lasso, and an averaged model is used to infer the Granger causality. The VAR models in FBLG do not have hidden units, while our bidirectional training is motivated by the need for training time-series models with hidden units. Winkler et al. (2016) also study a backward VAR in the context of the Granger causality but do not consider hidden units.\nAnother related work is structure learning of a VAR model (Bahadori et al., 2013) or a simpler linear dynamical system (Jalali & Sanghavi, 2012) that takes into account the existence of unobserved (or latent) variables. However, their goal is to reliably estimate the structure of the relation between observable variables by taking into account the unobserved variables. This is in contrast to the purpose of our bidirectional training, which aims at learning the relation between visible units and hidden units."
  }, {
    "heading": "2. DyBM with Hidden Units",
    "text": "We study a particularly structured Boltzmann machine for time-series (see Figure 1). Corresponding to a segment of a time-series of length T + 1, the Boltzmann machine in Figure 1 has T + 1 layers in the horizontal (temporal) direction. Each layer corresponds to a time t−δ for 0 ≤ δ ≤ T . Each layer has two parts, hidden and visible. The visible part x[t−δ] at the δ-th layer represents the values of the timeseries at time t − δ. The hidden part h[t−δ] represents the values of hidden units at time t − δ. Here, units within each layer do not have connections to each other. We let x[<t] ≡ (x[s])t−T≤s<t and define h[<t] analogously.\nThe Boltzmann machine in Figure 1 has bias parameter b and weight parameter (U, V, W, Z). Let θ ≡ (V,W,b) be the parameters connected to visible units x[t] (from the units in the past, x[s] or h[s] for s < t) and φ ≡ (U,Z). The energy of this Boltzmann machine is given as follows:\nEθ,φ(x [t],h[t]|x[<t],h[<t])\n= Eθ(x [t]|x[<t],h[<t]) + Eφ(h[t]|x[<t],h[<t]), (1)\nwhere we define\nEθ(x [t]|x[<t],h[<t]) (2)\n= −b>x[t] − T∑ δ=1 (x[t−δ])>W[δ] x[t] − T∑ δ=1 (h[t−δ])>V[δ] x[t]\nand define Eφ(h[t]|x[<t],h[<t]) from (2) by letting W ← U, V← Z, b← 0, and x[t] ← h[h].\nSimilar to the DyBM in Osogami & Otsuka (2015a), we study the case where W ≡ (W[δ])1≤δ≤T and other matrices have the following parametric forms for δ ≥ d:\nW[δ] = λδ−dW[d], V[δ] = λδ−dV[d], (3)\nZ[δ] = λδ−d Z[d], U[δ] = λδ−dU[d], (4)\nwhere λ is a decay rate satisfying 0 ≤ λ < 1. Then, in the limit of T → ∞, the energy in (2) can be represented as follows (and Eφ(h[t]|x[<t],h[<t]) has an analogous limit shown in (37) of the supplementary material):\nEθ(x [t]|x[<t],h[<t])\n= −b>x[t] − d−1∑ δ=1 (x[t−δ])>W[δ] x[t] − d−1∑ δ=1 (h[t−δ])>V[δ] x[t]\n− (α[t−1])>W[d] x[t] − (β[t−1])>V[d] x[t], (5)\nwhere α[t−1] is referred to as an eligibility trace in Osogami & Otsuka (2015a) and defined as follows (here, we define an eligibility trace β[t−1] for the hidden part analogously): α[t−1] ≡ ∞∑ δ=d λδ−d x[t−δ], β[t−1] ≡ ∞∑ δ=d λδ−d h[t−δ]. (6)\nThe energy in (5) gives the conditional probability distribution over x[t] given x[<t] and h[<t]. For binary-valued time-series, we have\npθ(x [t]|x[<t],h[<t]) = 1\nZ exp(−Eθ(x[t]|x[<t],h[<t])) (7)\nfor any binary vector x[t], where Z is the normalization factor for the probabilities to sum up to one. Due to the structure in Figure 1, the values in x[t] = (x[t]i )i=1,2,... are conditionally independent of each other given x[<t] and h[<t], so that we can represent\npθ(x [t]|x[<t],h[<t]) = ∏ i=1,2,... pθ,i(x [t] i |x [<t],h[<t]), (8)\nwhere the conditional probability pi(x [t] i |x[<t],h[<t]) is defined with the energy associated with unit i (see Osogami & Otsuka (2015a)). For real-valued time-series, one can define the conditional density pi(x [t] i |x[<t],h[<t]) with a Gaussian distribution whose mean is given from the energy associated with unit i (Dasgupta & Osogami, 2017; Osogami, 2016). Conditional distributions can be defined analogously for h[t] (see (40)–(42) and (51) in the supplementary material)."
  }, {
    "heading": "3. Training a DyBM with Hidden Units",
    "text": "Here, we derive a learning rule for θ. We will also see that φ cannot be trained in an analogous manner.\nOur DyBM with binary hidden units gives the probability of a time-series, x ≡ (x[t])t=`,...,u, by\npθ,φ(x) = ∑ h̃ pφ(h̃|x) u∏ t=` pθ(x [t]|x[<t], h̃[<t]) (9)\nwhere ∑\nh̃ denotes the summation over all of the possible values of hidden units from time t = ` to t = u, and\npφ(h̃|x) ≡ u∏ s=` pφ(h̃ [s]|x[<s], h̃[<s]), (10)\nwhere pφ(h̃[s]|x[<s], h̃[<s]) is defined analogously to (7)– (8) and provided in (36) of the supplementary material, and we arbitrarily define x[s] = 0 and h̃[s] = 0 for s < `.\nWe seek to maximize the log likelihood of a given x by maximizing a lower bound given by Jensen’s inequality:\nlog pθ,φ(x) = log (∑\nh̃\npφ(h̃|x) u∏ t=` pθ(x [t]|x[<t], h̃[<t]) ) (11)\n≥ ∑ h̃ pφ(h̃|x) log ( u∏ t=` pθ(x [t]|x[<t], h̃[<t]) ) (12)\n= ∑ h̃ pφ(h̃|x) u∑ t=` log pθ(x [t]|x[<t], h̃[<t]) (13)\n= u∑ t=` ∑ h̃[<t] pφ(h̃ [<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t])\n≡ Lθ,φ(x), (14)\nwhere the summation with respect to h̃[<t] is over all of the possible values of h̃[s] for s ≤ t− 1, and\npφ(h̃ [<t]|x[<t−1]) ≡ t−1∏ s=` pφ(h̃ [s]|x[<s], h̃[<s]). (15)\nThe gradient of the lower bound with respect to θ is:\n∇θLθ,φ(x) (16)\n= u∑ t=` ∑ h̃[<t] pφ(h̃ [<t]|x[<t−1])∇θ log pθ(x[t]|x[<t], h̃[<t]).\nThe right-hand side of (16) is a summation of expected gradients, which suggests a method of stochastic gradient. Namely, at each step t, we sample h[t−1] according to pφ(h [t−1]|x[<t−1],h[<t−1]) and update θ on the basis of\n∇θ log pθ(x[t]|x[<t],h[<t]). (17)\nThis learning rule is equivalent to the one for the model where all of the units are visible, except that the values for the hidden units are given by sampled values.\nTherefore, the learning rule for θ follows directly from Osogami & Otsuka (2015a):\nb← b+ η (x[t] − 〈X[t]〉θ) (18) W[d] ←W[d] + ηα[t−1] (x[t] − 〈X[t]〉θ)> (19) V[d] ← V[d] + η β[t−1] (x[t] − 〈X[t]〉θ)> (20) W[δ] ←W[δ] + η x[t−δ] (x[t] − 〈X[t]〉θ)> (21) V[δ] ← V[δ] + η h[t−δ] (x[t] − 〈X[t]〉θ)> (22)\nfor 1 ≤ δ < d, where 〈X[t]〉θ denotes the expected values of x[t] with respect to pθ in (7).\nNow we take the gradient of Lθ,φ(x) with respect to φ:\n∇φLθ,φ(x) (23)\n= u∑ t=` ∑ h̃[<t] ∇φpφ(h̃[<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t]),\nwhere\n∇φpφ(h̃[<t]|x[<t−1])\n= ∇φ t−1∏ s=` pφ(h̃ [s]|x[<s], h̃[<s]) (24)\n= t−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s]) t−1∏ s′=` pφ(h̃ [s′]|x[<s ′], h̃[<s ′])\n= pφ(h̃ [<t]|x[<t−1]) t−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s]). (25)\nPlugging (25) into the right-hand side of (23), we obtain\n∇φLθ,φ(x)\n= u∑ t=` ∑ h̃[<t] pφ(h̃ [<t]|x[<t−1]) log pθ(x[t]|x[<t], h̃[<t])\nt−1∑ s=` ∇φ log pφ(h̃[s]|x[<s], h̃[<s]). (26)\nSimilar to (16), the expression of (26) suggests a method of stochastic gradient: at each time t, we sample h[t−1] according to pφ(h[t−1]|x[<t−1],h[<t−1]) and update φ on the basis of the following stochastic gradient:\nlog pθ(x [t]|x[<t],h[<t])Gt−1, (27)\nwhere\nGt−1 ≡ t−1∑ s=` ∇φ log pφ(h[s]|x[<s],h[<s]). (28)\nComputation of (26) involves mainly two interrelated inefficiencies. First, although (26) can be approximately computed using sampled hidden values h̃[<t] in the same way as (16), the samples cannot be reused after updating φ because it was sampled from the distribution with the previous parameter. Second, since each summand of Gt−1 is dependent on φ, Gt−1 also has to be recomputed after each update. Thus, the computational complexity of (27) grows linearly with respect to the length of the time-series (i.e., t− `), in contrast to (17), whose complexity is independent of that length. One could approximately compute (28) recursively:\nGt ← γ Gt−1 + (1− γ)∇φ log pφ(h[t]|x[<t],h[<t]), (29)\nwhere γ ∈ [0, 1) is a discount factor. The recursive update rule with γ < 1 puts exponentially small weight γt−s on ∇φ log pφ(h[s]|x[<s],h[<s]) computed with an old value of φ (i.e., s t). This recursively computed Gt is related to the momentum in gradient descent (Qian, 1999). See the supplementary material for specific learning rules suggested by (27)–(29).\nObserve in (26) that ∇φLθ,φ(x) consists of the products of log pθ(x[t]|x[<t],h[<t]) and ∇φ log pφ(h[s]|x[<s],h[<s]) for s < t. Without the dependency on log pθ(x\n[t]|x[<t],h[<t]), the parameter φ is updated in a way that h[s] is more likely to be generated (i.e., the learning rule would be equivalent to that for visible units). Such an update rule is undesirable, because h[s] has been sampled and is not necessarily what we want to sample again. The dependency on log pθ(x[t]|x[<t],h[<t]) suggests that φ is updated by a large amount if the sampled h[s] happens to make the future values, x[t] for t > s, likely. Intuitively, weighting ∇φ log pφ(h[s]|x[<s],h[<s]) by log pθ(x[t]|x[<t],h[<t]) for t > s is inevitable, because whether the particular values of hidden units are good for the purpose of predicting future values will only be known after seeing future values."
  }, {
    "heading": "4. Learning with Reversed Time-series",
    "text": "Because the stochastic gradient for φ requires approximations that are not needed for θ, we might not be able to learn appropriate values of φ as effectively as θ. This motivates us to consider a backward DyBM in Figure 2, which has a common set of parameters, (θ, φ), as the forward DyBM in Figure 1 but defines the conditional distribution for time-series from the future. Specifically, the energy of the backward DyBM is represented analogously to (1) with the superscript [< t] replaced by [> t], where we define\nEθ(x [t]|x[>t],h[>t]) = −b>x[t] − T∑ δ=1 (x[t+δ])>(W[δ])>x[t]\n− T∑ δ=1 (x[t+δ])>(U[δ])>h[t] (30)\nand Eφ(h[t]|x[>t],h[>t]) is defined from (30) by letting W ← V, U ← Z, b ← 0, and x[t] ← h[t]. Similar to the forward DyBM, we assume that the weight has the parametric form of (4) and let T →∞.\nNamely, the backward DyBM is obtained from the forward DyBM by the following changes: W ← W>, Z ← Z>, U← V>, and V← U>. The other difference between (2) and (30) is the sign of δ, but this is because the backward DyBM deals with time-series from the future.\nBecause the backward DyBM has the structure that is equivalent to that of the forward DyBM, it can be trained in\nthe same manner as the forward DyBM but using timeseries from the future. Specifically, θ′ ≡ (U>,W>,b) in the backward DyBM is optimized analogously to θ in the forward DyBM. Likewise, φ′ ≡ (V>,Z>) is optimized analogously to φ. Recall that φ and φ′ are relatively hard to optimize, and θ and θ′ are relatively easy to optimize.\nOur key observation is that the parameter U, which is in φ and is relatively hard to optimize in the forward DyBM, is in θ′ and is relatively easy to optimize in the backward DyBM. By training both the forward DyBM and the backward DyBM, we expect to effectively find appropriate values of θ and θ′.\nConsider a stochastic process X(t) whose distribution is given by a forward DyBM. We remark that the distribution of the stochastic process X(−t) that is defined by reversing X(t) is generally different from what the corresponding backward DyBM gives unless the DyBMs have no hidden units. The exact distribution of X(−t) needs to be given by marginalizing out the past values (i.e., succeeding values for the backward process). Despite this discrepancy, we expect that bidirectional training is effective because of intuitive correspondence between the forward DyBM and the backward DyBM. In particular, W [δ]i,j in both DyBMs represent the strength of the correlation between the past value of unit i and the future value of unit j, where the time is separated by δ. A recommendation is, however, to perform backward training more moderately than forward training. We will show an example of a specific procedure in the next section."
  }, {
    "heading": "5. Numerical Experiments",
    "text": "We now demonstrate the effectiveness of bidirectional training through numerical experiments in two settings. The purpose of the first setting is to study whether bidirectional learning of the DyBM with hidden units can indeed learn to predict what cannot be done without bidirectional learning or hidden units. We use a synthetic dataset that is designed specifically for this purpose. In the second setting, we study the effectiveness of bidirectional learning and hidden units on real datasets. We use the two datasets that have been used in Dasgupta & Osogami (2017) as well as a 391 dimensional time-series, which is substantially larger than the eight or lower dimensional time-series that are used in the other experiments. The experiments are carried out with a Python implementation on workstations having 48-64 GB memory and 2.6-4.0 GHz CPU."
  }, {
    "heading": "5.1. Specific Learning Algorithms to Evaluate",
    "text": "For each dataset, we train a DyBM with or without hidden units. Because all of the datasets are real valued, visible units are Gaussian and give predictions by (5) with x[t] omit-\nAlgorithm 1 Specific steps of bidirectional learning evaluated in experiments T : The total number of iterations T0: The number of iterations of bidirectional learning F : The relative frequency of forward learning for t = 1 to T do\nif t < T0 and t mod F + 1 = 0 then Backward learning to update (U,W,b) else Forward learning to update (V,W,b)\nend if end for\nted (see (52) in the supplementary material). To reduce the variability in the experiments, we use the expected value for the output of a hidden unit instead of sampling a binary value. By the law of large numbers, the use of expected value corresponds to having an infinitely many binary hidden units that are conditionally independent and identically distributed (i.i.d.) given the internal state of the DyBM. See also (Sutskever et al., 2008; Sutskever & Hinton, 2007) for the related use of the expected values for hidden units.\nA DyBM with hidden unit is trained bidirectionally or only with forward learning. A DyBM without hidden units is trained only with forward learning. While bidirectional learning has several design choices, here we evaluate the specific algorithm shown in Algorithm 1. In particular, we perform bidirectional learning for the first T0 iterations, where the backward training is apply every F +1 steps. For the rest of T − T0 iterations, we perform forward learning only. Throughout we set F = 2. Note that Z is fixed with its initial values throughout learning. Here we do not update U in forward learning and V in backward learning. This is partly because the learning rule of (27)-(28) has no effect when we use the expected values in hidden units.\nBefore applying Algorithm 1, the bias b is initialized to zero, and the weight (U, V, W, Z) is initialized with i.i.d. normal random variable with mean 0 and standard deviation of 0.01 (Hinton, 2012) except the following two changes. First, we set the mean of W[1] as an identity matrix for real datasets, because using the previous value for prediction is clearly beneficial for these datasets. Second, we use the small standard deviation of 0.001 for the large dataset of 391 dimensions for faster convergence. The learning rate is adjusted according to AdaGrad (Duchi et al., 2011), where the the initial learning rate is optimized as we discuss in the following.\nThroughout the experiments, we set the decay rate of the eligibility traces in (6) to zero: α[t−1] = x[t−d] and β[t−1] = h[t−d]. The delay d and the number of hidden units are varied for each dataset."
  }, {
    "heading": "5.2. Synthetic Data",
    "text": "We first demonstrate the effectiveness of our bidirectional training in a synthetic setting of learning a one-dimensional noisy sawtooth wave, which is generated according to\nx[t] = t C − ⌊ t C ⌋ + εt, for t = 0, 1, . . . (31)\nwhere C is the period of the noisy sawtooth wave, and εt is an i.i.d. normal random variable, whose mean is fixed at 0 and standard deviation at 0.01. The noisy sawtooth wave has large discontinuity at the end of each period, which makes hidden units essential for learning.\nHere we train a DyBM with one hidden unit or no hidden units. Throughout, the delay is set d = 4, and the learning rate is initialized to η = 1.0. Bidirectional learning is continued for T0 = T/2 iterations, where T is varied depending on C. In each iteration, we use one period of the noisy sawtooth wave and update the parameters with stochastic gradients.\nFigure 3(a) and (c) show how learning progresses over iterations. The period of the noisy sawtooth wave is C = 6 in (a) and C = 8 in (c). Training is continued for T = 1, 000 iterations for C = 6 and T = 30, 000 iterations for C = 8. In every F + 1 iterations, we let the DyBM predict the one-step-ahead value for each of the C steps of one period during forward learning. We evaluate the root mean squared error (RMSE) of one-step-ahead predictions against true values. For clarity, the RMSE curves are smoothed with a Gaussian filter with window size of 50.\nIn the figure, the solid curves show the results with the bidirectionally trained DyBMs (Bidirectional). As a baseline, we also train the DyBM only with forward training and show the results with dashed curves (Baseline). The dotted curves show the results with the DyBM with no hidden units (No hidden). The comparison suggests that Bidirectional can substantially (by a factor of 10) improve the predictive accuracy over Baseline. Notice also that the hidden unit can hurt the predictive accuracy without bidirectional training. No hidden often exhibits lower RMSE than Baseline.\nIn Figure 3(c), the reduction of the RMSE accelerates at T0 iterations, after which bidirectional learning is no longer performed. This suggests that, after learning appropriate values of U (namely, what features should be used for prediction) via bidirectional learning, it is better to optimally learn (V,W,b) given the learned values of U. Namely, although bidirectional learning help learn appropriate values of U, it does not necessarily optimize all of the parameters of the DyBM. Recall also the discussion at the end of Section 4 that the backward DyBM is not exactly the same as the time-reversed DyBM.\nFigure 3(b) and (d) show the values predicted by bidirection-\nally trained DyBMs (black curves) and the corresponding target values (red curves). For clarity, we use a noiseless sawtooth wave as the target by letting εt = 0 in (31). Observe that the bidirectionally trained DyBM well predicts the next value of the sawtooth wave, substantially better than the baseline (or the DyBM with no hidden unit). In particular, the bidirectionally trained DyBM can well predict the sharp drop at the end of each period. This is in contrast to the baseline, whose prediction is rather smoothed out over the period."
  }, {
    "heading": "5.3. Real Data",
    "text": "Next, we demonstrate the effectiveness of bidirectional training on three real datasets, including the two datasets used in Dasgupta & Osogami (2017). The first dataset is the monthly sunspot number1, which we will refer to as Sunspot. This time-series has one dimension and 2,820 steps (corresponding to January 1749 to December 1983). The second dataset is the weekly retail gasoline and diesel prices2, which we will refer to as Price. This time-series has eight dimensions (corresponding to eight locations in the US) and 1,223 steps (corresponding to April 5th, 1993 to September 5th, 2016). Following Dasgupta & Osogami (2017), the first 67 % of each time-series is used for training, and the remaining 33 % is used for test. See Dasgupta & Osogami (2017) for further details about the first two datasets. The third dataset is the NOAA Global Surface Temperature3, which consists of a real valued time-series of 1,635 steps (t = 1, . . . , 1635) with 391 dimensions. We use the first 80 % of the time-series for training and the remaining 20 % for test.\nWe normalize the values of each dataset in a way that the\n1https://datamarket.com/data/set/22t4/ 2https://www.eia.gov/dnav/pet/pet_pri_\ngnd_a_epm0_pte_dpgal_w.htm 3V4.00 of Air Temperature (air.mon.anom.nc) from https://www.esrl.noaa.gov/psd/data/gridded/ data.noaaglobaltemp.html\nvalues in a training data are in [0,1] for each dimension. Notice that this normalization differs from that in (Dasgupta & Osogami, 2017), where the values in training or test data are in [0,1] for each dimension. However, the use of test data for normalization is less appropriate. This difference in normalization has no effect on the Price dataset, but the results on the Sunspot dataset need to be renormalized to be compared against those in (Dasgupta & Osogami, 2017).\nHere we train a DyBM with four hidden units or no hidden units. In each iteration, we use the whole training data (except the first d steps for forward learning and the last d steps for backward learning, which are used only to update the internal state of the DyBM) once and update the parameters with stochastic gradients.\nWe find that the speed of convergence is sensitive to the initial learning rate. Here, we choose the initial learning rate from {20, 2−1, 2−2, . . .}. Specifically, we choose 2−k with the smallest k such that the training RMSE after the initial T/100 iterations is smaller than that with 2−(k+1). Because the training RMSE tends to decrease with k up to a point and then increases with k, we usually choose the initial learning rate that minimizes the training RMSE after those initial iterations.\nFigure 4 shows the RMSE of one-step-ahead prediction with respect to the test data after every F + 1 iterations of training. We show the results where the delay is set d = 30 for Sunspot, d = 3 for Price, and d = 2 for Temperature. However, we have also run experiments with d ∈ {20, 40} for Sunspot and d ∈ {2, 4} for Price and have found that these do not improve the accuracy for any of the three methods. For the large dataset of Temperature, we have been able to perform limited experiments due to its relatively heavy computational requirements. Again, we compare Bidirectional against Baseline and No hidden. However, we now vary T0 ∈ {T/4, T/2, T}, so that each figure has three solid curves. The range of the vertical axis is chosen in a way that the upper limit corresponds to the RMSE with a naı̈ve\nprediction of using the preceding values as prediction.\nFor the Sunspot dataset (a), we find that Bidirectional does not improve upon Baseline, although having hidden units (Bidirectional and Baseline) can make the RMSE lower than No hidden. This means that the randomly set values of U is effective, and bidirectional learning does not find better values of U. After 1,000 iterations, however, Bidirectional with T0 = T/4 or T0 = T/2 achieves the RMSE that is essentially indistinguishable from that with Baseline. The best RMSE achieved by the Baseline is 0.0698, which corresponds to 0.0657 when the dataset is normalized in the way of (Dasgupta & Osogami, 2017) and is lower than 0.0734 reported in (Dasgupta & Osogami, 2017).\nFor Price (b) and Temperature (c), Bidirectional improves upon Baseline particularly when the bidirectional learning is stopped after T0 < T iterations. Bidirectional reduces the RMSE more slowly than Baseline or No hidden but eventually outperforms the others, and the reduction of the RMSE can be accelerated by stopping the bidirectional training after T0 < T iterations. In particular, the best RMSE achieved by Bidirectional with T0 = T/4 is 0.0399, which is lower than 0.0564 reported in (Dasgupta & Osogami, 2017). In addition, while Baseline and No hidden (namely, forward learning only) starts overfitting to training data and increases the RMSE with respect to the test data after some iterations, bidirectional training appears to avoid such overfit."
  }, {
    "heading": "6. Conclusion",
    "text": "We have proposed bidirectional training for time-series models with hidden units. Namely, we consider two models that have a common set of parameters, where one model is trained with forward time-series and the other with backward time-series. Our key idea is that some of the parameters that are difficult to learn in one model can be effectively learned in the other model. Numerical experiments suggest that bidirectional training has the additional effect of\navoiding overfit to training data.\nThe DyBM with hidden units analyzed in Sections 2–4 is new, and its analysis has two highlights, which have led to proposing bidirectional training. The first highlight is that the learning rule for V (hidden-to-visible weight) in (20)–(22) becomes equivalent to those for W (visible-tovisible weight) in (20)–(22) when the lower bound (14) is maximized. The second highlight is that we cannot learn the weight to hidden units (φ = (U,Z)) in the same way as the weight to visible units (θ = (V,W,b)) due to the form of the gradient in (27).\nAlthough we have demonstrated the effectiveness of bidirectional training in specific cases, its capabilities are not fully explored. Bidirectional training has many design choices that need further study. For example, one might want to use the gradients in (27)-(28), possibly with the approximation in (29), to update (U,Z) in forward learning and (V,Z) in backward learning. It would also be interesting to apply bidirectional training to other time-series models having parameters that represent the dependency between hidden values at one time and visible values at another time. In addition to DyBM and VAR with hidden units, these include Spiking Boltzmann Machine (Hinton & Brown, 1999) and Conditional Restricted Boltzmann Machine (Taylor et al., 2007). Because bidirectional training is largely complementary to other techniques for learning time-series, it would be interesting to investigate how bidirectional training improves performance when it is combined with these other techniques. We expect that this work opens up a line of research on more effective methods of bidirectional training."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by JST CREST Grant Number JPMJCR1304, Japan."
  }],
  "year": 2017,
  "references": [{
    "title": "Fast structure learning in generalized stochastic processes with latent factors",
    "authors": ["M.T. Bahadori", "Y. Liu", "E.P. Xing"],
    "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2013
  }, {
    "title": "Exploiting the past and the future in protein secondary structure",
    "authors": ["P. Baldi", "S. Brunak", "P. Frasconi", "G. Soda", "G. Pollastri"],
    "venue": "prediction. Bioinformatics,",
    "year": 1999
  }, {
    "title": "Statistical inference for probabilistic functions of finite state Markov chains",
    "authors": ["L.E. Baum", "T. Petrie"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1966
  }, {
    "title": "Online learning and stochastic approximations",
    "authors": ["L. Bottou"],
    "venue": "On-Line Learning in Neural Networks,",
    "year": 2009
  }, {
    "title": "FBLG: A simple and effective approach for temporal dependence discovery from time series data",
    "authors": ["D. Cheng", "M.T. Bahadori", "Y. Liu"],
    "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2014
  }, {
    "title": "Nonlinear dynamic Boltzmann machines for time-series prediction",
    "authors": ["S. Dasgupta", "T. Osogami"],
    "venue": "In The 31st AAAI Conference on Artificial Intelligence",
    "year": 2017
  }, {
    "title": "Regularized dynamic Boltzmann machine with delay pruning for unsupervised learning of temporal sequences",
    "authors": ["S. Dasgupta", "T. Yoshizumi", "T. Osogami"],
    "venue": "In Proceedings of the 23rd International Conference on Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Framewise phoneme classification with bidirectional LSTM networks and other neural network architectures",
    "authors": ["A. Graves", "J. Schmidhuber"],
    "venue": "Neural Networks,",
    "year": 2005
  }, {
    "title": "Spiking Boltzmann machines",
    "authors": ["G.E. Hinton", "A.D. Brown"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 1999
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural Computation,",
    "year": 1997
  }, {
    "title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
    "authors": ["H. Jaeger", "H. Haas"],
    "year": 2004
  }, {
    "title": "Learning the dependence graph of time series with latent factors",
    "authors": ["A. Jalali", "S. Sanghavi"],
    "venue": "In Proceedings of the 29th International Conference on Machine Learning,",
    "year": 2012
  }, {
    "title": "A functional dynamic Boltzmann machine",
    "authors": ["H. Kajino"],
    "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "New Introduction to Multiple Time Series Analysis",
    "authors": ["H. Lütkepohl"],
    "year": 2005
  }, {
    "title": "Learning binary or real-valued timeseries via spike-timing dependent plasticity",
    "authors": ["T. Osogami"],
    "venue": "CoRR, abs/1612.04897,",
    "year": 2016
  }, {
    "title": "Seven neurons memorizing sequences of alphabetical images via spike-timing dependent plasticity",
    "authors": ["T. Osogami", "M. Otsuka"],
    "venue": "Scientific Reports,",
    "year": 2015
  }, {
    "title": "Learning dynamic Boltzmann machines with spike-timing dependent plasticity",
    "authors": ["T. Osogami", "M. Otsuka"],
    "venue": "Technical Report RT0967, IBM Research,",
    "year": 2015
  }, {
    "title": "On the momentum term in gradient descent learning algorithms",
    "authors": ["N. Qian"],
    "venue": "Neural Networks: The Official Journal of the International Neural Network Society,",
    "year": 1999
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["M. Schuster", "K.K. Paliwal"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 1997
  }, {
    "title": "The recurrent temporal restricted Boltzmann machine",
    "authors": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2008
  }, {
    "title": "Modeling human motion using binary latent variables",
    "authors": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2007
  }, {
    "title": "Validity of time reversal for testing granger causality",
    "authors": ["I. Winkler", "D. Panknin", "D. Bartz", "Müller", "K.-R", "S. Haufe"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2016
  }],
  "id": "SP:7d424fad26cf7ef419bfa509f36abae09e6a8089",
  "authors": [{
    "name": "Takayuki Osogami",
    "affiliations": []
  }, {
    "name": "Hiroshi Kajino",
    "affiliations": []
  }, {
    "name": "Taro Sekiyama",
    "affiliations": []
  }],
  "abstractText": "Hidden units can play essential roles in modeling time-series having long-term dependency or nonlinearity but make it difficult to learn associated parameters. Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of parameters as the original (forward) model. Our key observation is that only a subset of the parameters is hard to learn, and that subset is complementary between the forward model and the backward model. By training both of the two models, we can effectively learn the values of the parameters that are hard to learn if only either of the two models is trained. We apply bidirectional learning to a dynamic Boltzmann machine extended with hidden units. Numerical experiments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.",
  "title": "Bidirectional Learning for Time-series Models with Hidden Units"
}