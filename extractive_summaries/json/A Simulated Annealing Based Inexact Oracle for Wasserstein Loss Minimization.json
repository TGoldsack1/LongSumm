{
  "sections": [{
    "heading": "1. Introduction",
    "text": "An oracle is a computational module in an optimization procedure that is applied iteratively to obtain certain characteristics of the function being optimized. Typically, it calculates the value and gradient of loss function l(x,y). In the vast majority of machine learning models, where those loss functions are decomposable along each dimension (e.g., Lp norm, KL divergence, or hinge loss), rxl(·,y) or\n1College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA. 2Department of Statistics, The Pennsylvania State University, University Park, PA.. Correspondence to: Jianbo Ye <jxy198@ist.psu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nr y l(x, ·) is computed in O(m) time, m being the complexity of outcome variables x or y. This part of calculation is often negligible compared with the calculation of full gradient with respect to the model parameters. But this is no longer the case in learning problems based on Wasserstein distance due to the intrinsic complexity of the distance. We will call such problems Wasserstein loss minimization (WLM). Examples of WLMs include Wasserstein barycenters (Li & Wang, 2008; Agueh & Carlier, 2011; Cuturi & Doucet, 2014; Benamou et al., 2015; Ye & Li, 2014; Ye et al., 2017b), principal geodesics (Seguy & Cuturi, 2015), nonnegative matrix factorization (Rolet et al., 2016; Sandler & Lindenbaum, 2009), barycentric coordinate (Bonneel et al., 2016), and multi-label classification (Frogner et al., 2015).\nWasserstein distance is defined as the cost of matching two probability measures, originated from the literature of optimal transport (OT) (Monge, 1781). It takes into account the cross-term similarity between different support points of the distributions, a level of complexity beyond the usual vector data treatment, i.e., to convert the distribution into a vector of frequencies. It has been promoted for comparing sets of vectors (e.g. bag-of-words models) by researchers in computer vision, multimedia and more recently natural language processing (Kusner et al., 2015; Ye et al., 2017a). However, its potential as a powerful loss function for machine learning has been underexplored. The major obstacle is a lack of standardized and robust numerical methods to solve WLMs. Even to empirically better understand the advantages of the distance is of interest.\nAs a long-standing consensus, solving WLMs is challenging (Cuturi & Doucet, 2014). Unlike the usual optimization in machine learning where the loss and the (partial) gradient can be calculated in linear time, these quantities are non-smooth and hard to obtain in WLMs, requiring solution of a costly network transportation problem (a.k.a. OT). The time complexity, O(m3 logm), is prohibitively high (Orlin, 1993). In contrast to the Lp or KL counterparts, this step of calculation elevates from a negligible fraction of the overall learning problem to a dominant portion, preventing the scaling of WLMs to large data. Recently, iterative approximation techniques have been developed to compute the loss and the (partial) gradient at com-\nplexity O(m2/\") (Cuturi, 2013; Wang & Banerjee, 2014). However, nontrivial algorithmic efforts are needed to incorporate these methods into WLMs because WLMs often require multi-level loops (Cuturi & Doucet, 2014; Frogner et al., 2015). Specifically, one must re-calculate through many iterations the loss and its partial gradient in order to update other model dependent parameters.\nWe are thus motivated to seek for a fast inexact oracle that (i) runs at lower time complexity per iteration, and (ii) accommodates warm starts and meaningful early stops. These two properties are equally important for efficiently obtaining adequate approximation to the solutions of a sequence of slowly changing OTs. The second property ensures that the subsequent OTs can effectively leverage the solutions of the earlier OTs so that the total computational time is low. Approximation techniques with low complexity per iteration already exist for solving a single OT, but they do not possess the second property. In this paper, we introduce a method that uses a time-inhomogeneous Gibbs sampler as an inexact oracle for Wasserstein losses. The Markov chain Monte Carlo (MCMC) based method naturally satisfies the second property, as reflected by the intuition of physicists that MCMC samples can efficiently “remix from a previous equilibrium.”\nWe propose a new optimization approach based on Simulated Annealing (SA) (Kirkpatrick et al., 1983; Corana et al., 1987) for WLMs where the outcome variables are treated as probability measures. SA is especially suitable for the dual OT problem, where the usual Metropolis sampler can be simplified to a Gibbs sampler. To our knowledge, existing optimization techniques used on WLMs are different from MCMC. In practice, MCMC is known to easily accommodate warm start, which is particularly useful in the context of WLMs. We name this approach GibbsOT for short. The algorithm of Gibbs-OT is as simple and efficient as the Sinkhorn’s algorithm — a widely accepted method to approximately solve OT (Cuturi, 2013). We show that Gibbs-OT enjoys improved numerical stability and several algorithmic characteristics valuable for general WLMs. By experiments, we demonstrate the effectiveness of Gibbs-OT for solving optimal transport with Coulomb cost (Benamou et al., 2016) and the Wasserstein non-negative matrix factorization (NMF) problem (Sandler & Lindenbaum, 2009; Rolet et al., 2016)."
  }, {
    "heading": "2. Related Work",
    "text": "Recently, several methods have been proposed to overcome the aforementioned difficulties in solving WLMs. Representatives include entropic regularization (Cuturi, 2013; Cuturi & Doucet, 2014; Benamou et al., 2015) and Bregman ADMM (Wang & Banerjee, 2014; Ye et al., 2017b). The main idea is to augment the original optimization prob-\nlem with a strongly convex term such that the regularized objective becomes a smooth function of all its coordinating parameters. Neither the Sinkhorn’s algorithm nor Bregman ADMM can be readily integrated into a general WLM. Based on the entropic regularization of primal OT, Cuturi & Peyré (2016) recently showed that the Legendre transform of the entropy regularized Wasserstein loss and its gradient can be computed in closed form, which appear in the first-order condition of some complex WLM problems. Using this technique, the regularized primal problem can be converted to an equivalent Fenchel-type dual problem that has a faster numerical solver in the Euclidean space (Rolet et al., 2016). But this methodology can only be applied to a certain class of WLM problems of which the Fencheltype dual has closed forms of objective and full gradient. In contrast, the proposed SA-based approach directly deals with the dual OT problem without assuming any particular mathematical structure of the WLM problem, and hence is more flexible to apply.\nMore recent approaches base on solving the dual OT problems have been proposed to calculate and optimize the Wasserstein distance between a single pair of distributions with very large support sets — often as large as the size of an entire machine learning dataset (Montavon et al., 2016; Genevay et al., 2016; Arjovsky et al., 2017). For these methods, scalability is achieved in terms of the support size. Our proposed method has a different focus on calculating and optimizing Wasserstein distances between many pairs all together in WLMs, with each distribution having a moderate support size (e.g., dozens or hundreds). We aim at scalability for the scenarios when a large set of distributions have to be handled simultaneously, that is, the optimization cannot be decoupled on the distributions. In addition, existing methods have no on-the-fly mechanism to control the approximation quality at a limited number of iterations."
  }, {
    "heading": "3. Preliminaries of Optimal Transport",
    "text": "In this section, we present notations, mathematical backgrounds, and set up the problem of interest.\nDefinition 3.1 (Optimal Transportation, OT). Let p 2 m1 ,q 2 m2 , where m is the set of m-dimensional simplex: m def. = {q 2 Rm\n+ : hq, i = 1}. The set of transportation plans between p and q is defined as ⇧(p,q) def.= {Z 2 Rm1⇥m2 : Z · m2 = p;ZT · m1 = q; }. Let M 2 Rm1⇥m2\n+ be the matrix of costs. The optimal transport cost between p and q with respect to M is\nW (p,q) def. = min Z2⇧(p,q) hZ,Mi . (1)\nIn particular, ⇧(·, ·) is often called the coupling set.\nNow we relate primal version of (discrete) OT to a variant of its dual version. One may refer to Villani (2003) for the general background of the Kantorovich-Rubenstein duality. In particular, our formulation introduces an auxiliary parameter CM for the sake of mathematical soundness in defining Boltzmann distributions.\nDefinition 3.2 (Dual Formulation of OT). Let CM > 0, denote vector [g\n1 , . . . , gm1 ] T by g, and vector [h 1 , . . . , hm2 ] T\nby h. We define the dual domain of OT by\n⌦(M) def. =\nn\nf = [g;h] 2 Rm1+m2\nCM < gi hj  Mi,j , 1  i  m1, 1  j  m2 o .\n(2)\nInformally, for a sufficiently large CM (subject to p,q,M ), the LP problem Eq. (1) can be reformulated as 1\nW (p,q) = max f2⌦(M)\nhp,gi hq,hi . (3)\nLet the optimum set be ⌦⇤(M). Then any optimal point f ⇤ = (g\n⇤,h⇤) 2 ⌦⇤(M) constructs a (projected) subgradient such that g⇤ 2 @W/@p and h⇤ 2 @W/@q . The main computational difficulty of WLMs comes from the fact that (projected) subgradient f⇤ is not efficiently solvable.\nNote that ⌦(M) is an unbound set in Rm1+m2 . In order to constrain the feasible region to be bounded, we alternatively define\n⌦\n0 (M)={f = [g;h] 2 ⌦(M) | g 1 = 0}. (4)\nOne can show that the maximization in ⌦(M) as Eq. (3) is equivalent to the maximization in ⌦\n0 (M) because hp, m1i = hq, m2i."
  }, {
    "heading": "4. Simulated Annealing for Optimal Transport via Gibbs Sampling",
    "text": "Following the basic strategy outlined in the seminal paper of simulated annealing (Kirkpatrick et al., 1983), we present the definition of Boltzmann distribution supported on ⌦\n0 (M) below which, as we will elaborate, links the dual formulation of OT to a Gibbs sampling scheme (Algorithm 1 below).\nDefinition 4.1 (Boltzmann Distribution of OT). Given a temperature parameter T > 0, the Boltzmann distribution\n1However, for any proper M and strictly positive p,q, there exists CM such that the optimal value of primal problem is equal to the optimal value of the dual problem. This modification is solely for an ad-hoc treatment of a single OT problem. In general cases of (p,q,M), when CM is pre-fixed, the solution of Eq. (3) may be suboptimal.\nof OT is a probability measure on ⌦ 0 (M) ✓ Rm1+m2 1 such that\np(f ;p,q) / exp  1\nT (hp,gi hq,hi) . (5)\nIt is a well-defined probability measure for an arbitrary finite CM > 0.\nThe basic concept behind SA states that the samples from the Boltzmann distribution will eventually concentrate at the optimum set of its deriving problem (e.g. W (p,q)) as T ! 0. However, since the Boltzmann distribution is often difficult to sample, a practical convergence rate remains mostly unsettled for specific MCMC methods.\nBecause ⌦(M) defined by Eq. (2) (also ⌦ 0 ) has a conditional independence structure among variables, a Gibbs sampler can be naturally applied to the Boltzmann distribution defined by Eq. (5). We summarize this result below.\nProposition 4.1. Given any f = (g;h) 2 ⌦ 0\n(M) and any CM > 0, we have for any i and j,\ngi  Ui(h) def.= min 1jm2 (Mi,j + hj) , (6)\nhj Lj(g) def.= max 1im1 (gi Mi,j) . (7)\nand\ngi > bLi(h) def. = max\n1jm2 ( CM + hj) , (8)\nhj < bUj(g) def. = max\n1im1 (CM + gi) . (9)\nHere Ui = Ui(h) and Lj = Lj(g) are auxiliary variables. Suppose f follows the Boltzmann distribution by Eq. (5), gi’s are conditionally independent given h, and likewise hj’s are also conditionally independent given g. Furthermore, it is immediate from Eq. (5) that each of their conditional probabilities within its feasible region (subject to CM ) satisfies\np(gi|h) / exp ⇣gipi\nT\n⌘\n, bLi(h) < gi  Ui(h), (10)\np(hj |g) / exp ✓ hjqj T ◆ , Lj(g)  hj < bUj(g), (11)\nwhere 2  i  m 1 and 1  j  m 2 .\nRemark 1. As CM ! +1, bUj(g) ! +1 and bLi(h) ! 1. For 2  i  m\n1 and 1  j  m 2\n, one can approximate the conditional probability p(gi|h) and p(hj |g) by exponential distributions.\nBy Proposition. 4.1, our proposed time-inhomogeneous Gibbs sampler is given in Algorithm 1. Specifically in Algorithm 1, the variable g\n1\nis fixed to zero by the definition\nAlgorithm 1 Gibbs Sampling for Optimal Transport Given f (0) 2 ⌦\n0 (M), p 2 m1 and q 2 m2 , and T (1), . . . , T (2N) > 0, for t = 1, . . . , N , we define the following Markov chain\n1. Randomly sample\n✓ 1 , . . . , ✓m2 i.i.d.⇠ Exponential(1).\nFor j = 1, 2, . . . ,m 2 , let (\nL(t)j := max1im1\n⇣ g(t 1)i Mi,j ⌘\nh(t)j := L (t) j + ✓j · T (2t 1)/qj\n(12)\n2. Randomly sample\n✓ 2 , . . . , ✓m1 i.i.d.⇠ Exponential(1).\nFor i = (1), 2, . . . ,m 1 , let (\nU (t)i := min1jm2\n⇣\nMi,j + h (t) j\n⌘\ng(t)i := U (t) i ✓i · T (2t)/pi\n(13)\nof ⌦ 0 (M). But we have found in experiments that by calculating U (t)\n1 and sampling g(t) 1\nin Algorithm 1 according to Eq. (13), one can still generate MCMC samples from ⌦(M) such that the energy quantity hp,gi hq,hi converges to the same distribution as that of MCMC samples from ⌦\n0 (M). Therefore, we will not assume g 1 = 0 from now on and develop analysis solely for the unconstrained version of Gibbs-OT.\nFigure 1 illustrates the behavior of the proposed Gibbs sampler with a cooling schedule at different temperatures. As T decreases along iterations, the 95% percentile band for sample f becomes thinner and thinner. Remark 2. Algorithm 1 does not specify the actual cooling schedule, nor does the analysis of the proposed Gibbs sampler in Theorem A.2. We have been agnostic here for a reason. In the SA literature, cooling schedules with guaranteed optimality are often too slow to be useful in practice. To our knowledge, the guaranteed rate of SA approach is worse than the combinatorial solver for OT. As a result, a well-accepted practice of SA for many complicated optimization problems is to empirically adjust cooling schedules, a strategy we take for our experiments. Remark 3. Although the exact cooling schedule is not specified, we still provide a quantitative upper bound of the chosen temperature T at different iterations in Appendix A Eq. (24). One can calculate such bound at the cost of m logm at certain iterations to check whether the current temperature is too high for the used Gibbs-OT to accurately\napproximate the Wasserstein gradient. In practice, we find this bound helps one quickly select the beginning temperature of Gibbs-OT algorithm. Definition 4.2 (Notations for Auxiliary Statistics). Besides the Gibbs coordinates g and h, the Gibbs-OT sampler naturally introduces two auxiliary variables, U and L. Let\nL (t) =\nh\nL(t) 1 , . . . , L(t)m2 iT and U(t) = h U (t) 1 , . . . , U (t)m1 iT .\nLikewise, denote the collection of g(t)i and h (t) j by vectors\ng (t) and h(t) respectively. The following sequence of auxiliary statistics\n[. . . , z2t 1, z2t, z2t+1, . . . , ] def. =\n\n. . . ,\n\nL\n(t)\nU\n(t 1)\n,\n\nL\n(t)\nU\n(t)\n,\n\nL\n(t+1)\nU\n(t)\n, . . .\n(14)\nfor t = 1, . . . , N is also a Markov chain. They can be redefined equivalently by specifying the transition probabilities p(zn+1|zn) for n = 1, . . . , 2N , a.k.a., the conditional p.d.f. p(U(t)|L(t)) for t = 1, . . . , N and p(L(t+1)|U(t)) for t = 1, . . . , N 1. One may notice that the alternative representation converts the Gibbs sampler to one whose structure is similar to a hidden Markov model, where the g,h chain is conditional independent given the U,L chain and has (factored) exponential emission distributions. We will use this equivalent representation in Appendix A and develop analysis based on the U,L chain accordingly. Remark 4. We now consider the function\nV (x,y) def. = hp,xi hq,yi ,\nand define a few additional notations. Let V (Ut 0 ,Lt) be denoted by V (zt+t 0 ), where t0 = t or t 1. If g,h are independently resampled according to Eq. (12) and (13), we will have the inequalities that\nE [V (g,h)|zn]  V (zn) . Both V (z) and V (g,h) converges to the exact loss W (p,q) at the equilibrium of Boltzmann distribution p(f ;p,q) as T ! 0. 2"
  }, {
    "heading": "5. Gibbs-OT: An Inexact Oracle for WLMs",
    "text": "In this section, we introduce a non-standard SA approach for the general WLM problems. The main idea is to replace the standard Boltzmann energy with an asymptotic consistent upper bound, outlined in our previous section. Let\nR(✓) :=\n|D| X\ni=1\nW (pi(✓),qi(✓))\n2The conditional quantity V (zn) V (g,h)|zn is the sum of two Gamma random variables: Gamma(m1, 1/T (2t)) + Gamma(m2, 1/T (2t 0+1)) where t0 = t or t0 = t 1.\nbe our prototyped objective function, where D represents a dataset, pi,qi are prototyped probability densities for representing the i-th instance. We now discuss how to solve min✓2⇥ R(✓).\nTo minimize the Wasserstein losses W (p,q) approximately in such WLMs, we propose to instead optimize its asymptotic consistent upper bound E[V (z)] at equilibrium of Boltzmann distribution p(f ;p,q) using its stochastic gradients: U 2 @V (z)/@p and L 2 @V (z)/@q . Therefore, one can calculate the gradient approximately:\nr✓R ⇡ |D| X\ni=1\n[J✓(pi(✓))Ui J✓(qi(✓))Li]\nwhere J✓(·) is the Jacobian, Ui, Li are computed from Algorithm 1 for the problem W (pi,qi) respectively. Together with the iterative updates of model parameters ✓, one gradually anneals the temperature T . The equilibrium of p(f ;p,q) becomes more and more concentrated. We assume the inexact oracle at a relatively higher temperature is adequate for early updates of the model parameters, but sooner or later it becomes necessary to set T smaller to better approximate the exact loss.\nIt is well known that the variance of stochastic gradient usually affects the rate of convergence. The reason to replace V (g,h) with V (z) as the inexact oracle (for some T > 0) is motivated by the same intuition. The variances of MCMC samples g(t)i , h (t) j of Algorithm 1 can be very large if pi/T and qj/T are small, making the embedded first-order method inaccurate unavoidably. But we find the variances of max/min statistics U (t)i , L (t) j are much smaller. Fig. 1 shows an example. The bias introduced in the replacement is also well controlled by decreasing the temperature parameter T . For the sake of efficiency, we use a very simple convergence diagnostics in the practice of Gibbs-OT. We check the values of V (z(2t)) such that the\nMarkov chain is roughly considered mixed if every ⌧ iteration the quantity V (z(2t)) (almost) stops increasing (⌧=5 by default), say, for some t,\nV (z(2t)) V (z(2(t ⌧))) < 0.01⌧T · V (z(2t)), we terminate the Gibbs iterations."
  }, {
    "heading": "6. Applications of Gibbs-OT",
    "text": ""
  }, {
    "heading": "6.1. Toy OT Examples",
    "text": "1D Case with Euclidean Cost. We first illustrate the differences between the approximate primal solutions computed by different methods by replicating a toy example in (Benamou et al., 2015). The toy example calculates the OT between two 1D two-mode distributions. We visualize their solved coupling as a 2D image in Fig. 2 at the budgets in terms of different number of iterations. Given their different convergence behaviors, when one wants to compromise with using pre-converged primal solutions in WLMs, he or she has to account for the different results computed by different numerical methods, even though they all aim at the Wasserstein loss.\nAs a note, Sinkhorn, B-ADMM and Gibbs-OT share the same computational complexity per iteration. The difference in their actual CPU time comes from the different arithmetic operations used. B-ADMM may be the slowest because it requires log() and exp() operations. When memory efficiency is of concern, both the implementations of Sinkhorn and Gibbs-OT can be modified to take only O(m\n1 +m 2 ) additional memory besides the space for caching the cost matrix M .\nTwo Electrons with Coulomb Cost in DFT. In quantum mechanics, Coulomb cost (or electron-electron Coulomb repulsion) is an important energy functional in Density Functional Theory (DFT). Numerical methods that solve\nthe multi-marginal OT problem with unbounded costs remains an open challenge in DFT (Benamou et al., 2016). We consider two uniform densities on 1D domain [0, 1] with Coulomb cost c(x, y) = 1/|x y| which has analytic solutions. Coulumb cost is different from the usual metric cost in the OT literature, which is unbounded and singular at x = y. As observed in (Benamou et al., 2016), the entropic regularized primal solution becomes more concentrated at boundaries, which is not physically plausible. This effect is not observed in the Gibbs-OT solution as shown in Appendix Fig. 3. As shown by Fig 1, the variables U,V in computation are always in bounded range (with an overwhelming probability), thus the algorithm does not endure any numerical difficulties.\nFor entropic regularization (Benamou et al., 2015; 2016), we empirically select the minimal \" which does not cause numerical overflow before 5000 iterations (in which \" =\n0.5/N ). For Gibbs-OT, we use a geometric temperature scheme such that T = 2.0(1/l4)n/l/N at the n-th iteration, where l is the max iteration number. For the unbounded Coulomb cost, Bregman ADMM (Wang & Banerjee, 2014) does not converge to a solution close to the true optimum."
  }, {
    "heading": "6.2. Wasserstein NMF",
    "text": "We now illustrate how the proposed Gibbs-OT can be used as a ready-to-plugin inexact oracle for a typical WLM — Wasserstein NMF (Sandler & Lindenbaum, 2009; Rolet et al., 2016). The data parallelization of this framework is natural because the Gibbs-OT samplers subject to different instances are independent.\nProblem Formulation. Given a set of discrete probability measures { i}ni=1 (data) over Rd, we want to estimate a model ⇥ = { k}Kk=1, such that for each i, there exists a membership vector (i) 2 K : i ⇡ PK\nk=1 (i) k k, where each k is again a discrete probability measure to be estimated. Therefore, Wasserstein NMF reads min ⇥,⌅ Pn i=1 W ⇣ i, PK k=1 (i) k k ⌘ , where ⌅ = ( (1), . . . , (n)) is the collection of membership vectors, and W is the Wasserstein distance. One can write the problem by plugging Eq. (3) in the dual formulation:\nmin ⇥,⌅ max\nF={fi}ni=1\nXn\ni=1\nh hbw(i),gii hw(i),hii i (15)\ns.t. k = Xm\ni=1 v(k)i xi , (16)\nb (i) =\nXK\nk=1 (i)k k , (17)\nfi 2 ⌦ ⇣ M(b (i), i) ⌘ , (18)\nwhere bw(i) 2 m is the weight vector of discrete probability measure b (i) and w(i) 2 mi is the weight vector of (i). M(·, ·) denotes the transportation cost matrix between the supports of two measures. The global optimization solves all three sets of variables (⇥,⌅, F ). In the sequel, we assume support points of { k}mk=1 — {xi}mi=1 are shared and pre-fixed.\nAlgorithm. At every epoch, one updates variables either sequentially (indexed by i) or all together. It is done by first executing the Gibbs-OT oracle subject to the i-th instance and then updating v(k) and the membership vector (i) accordingly at a chosen step size > 0. At the end of each epoch, the temperature parameter T is adjusted T := T ⇣ 1 q 1\nm+m̄\n⌘\n, where m̄ = 1n Pn i=1 mi . For each instance i, the algorithm proceeds with the following steps iteratively:\n1. Initiate from the last computed U/V sample subject to instance i, execute the Gibbs-OT Gibbs sampler at\nconstant temperature T until a mixing criterion is met, and get Ui.\n2. For k = 1, . . . ,K, update v(k) 2 m based on gradient (i)k Ui using the iterates of online mirror descent (MD) subject to the step-size (Beck & Teboulle, 2003).\n3. Also update the membership vector (i) 2 K based on gradient (hv(1),Uii, . . . , hv(K),Uii)T using the iterates of accelerated mirror descent (AMD) with restarts subject to the same step-size (Krichene et al., 2015).\nWe note that the practical speed-ups we achieved via the above procedure is the warm-start feature in Step 1. If one uses a black-box OT solver, this dimension of speed-ups is not viable.\nResults. We investigate the empirical convergence of the proposed Wasserstein NMF method by two datasets: one is a subset of MNIST handwritten digit images which contains 200 digits of “5”, and the other is the ORL 400-face dataset. Our results are based on a C/C++ implementation with vectorization. In particular, we set K = 40, = 2.0 for both datasets. The learned components are visualized together with alternative approaches (smoothed WNMF (Rolet et al., 2016) and regular NMF) in Appendix Figs. 4 and 5. From these figures, we observe that our learned components using Gibbs-OT are shaper than the smoothed W-NMF. This can be explained by the fact that Gibbs-OT can potentially push for higher quality of approximation by gradually annealing the temperature. We also observe that the learned components might possess some salt-and-pepper noise. This is because the Wasserstein distance by definition is not very sensitive to the subpixel displacements. On a single-core of a 3.3 GHz Intel\nCore i5 CPU, the average time spent for each epoch for these two datasets are 0.84 seconds and 16.8 seconds, respectively. It is about two magnitude faster than fully solving all OTs via a commercial LP solver 3."
  }, {
    "heading": "7. Discussions",
    "text": "The solution of primal OT (Monge-Kantorovich problem) have many direct interpretations, where the solved transport is a coupling between two measures. Hence, it could be well motivated to consider regularizing the solution on the primal domain in those problems (Cuturi, 2013). Meanwhile, the solution of dual OT can be meaningful in its own right. For instance, in finance, the dual solution is directly interpreted as the vanilla prices implementing robust static super-hedging strategies. The entropy regularized OT, under the Fenchel-type dual, provides a smoothed unconstrained dual problem as shown in (Cuturi & Peyré, 2016). In this paper, we develop Gibbs-OT, whose solutions respect the dual feasibility of OT and are subject to a different regularization effect as explained by (Abernethy & Hazan, 2015). It is a numerical stable and computational suitable oracle to handle WLM.\nAcknowledgement. This material is based upon work supported by the National Science Foundation under Grant Nos. ECCS-1462230 and DMS-1521092. The authors would also like to thank anonymous reviewers for their valuable comments.\n3We use the specialized network flow solver in Mosek (https://www.mosek.com) for the computation, which is found faster than general simplex or IPM solver at moderate problem scale."
  }],
  "year": 2017,
  "references": [{
    "title": "Faster convex optimization: Simulated annealing with an efficient universal barrier",
    "authors": ["Abernethy", "Jacob", "Hazan", "Elad"],
    "venue": "arXiv preprint arXiv:1507.02528,",
    "year": 2015
  }, {
    "title": "Barycenters in the Wasserstein space",
    "authors": ["Agueh", "Martial", "Carlier", "Guillaume"],
    "venue": "SIAM J. Math. Analysis,",
    "year": 2011
  }, {
    "title": "Iterative Bregman projections for regularized transportation problems",
    "authors": ["Benamou", "Jean-David", "Carlier", "Guillaume", "Cuturi", "Marco", "Nenna", "Luca", "Peyré", "Gabriel"],
    "venue": "SIAM J. on Scientific Computing,",
    "year": 2015
  }, {
    "title": "A numerical method to solve multi-marginal optimal transport problems with Coulomb cost",
    "authors": ["Benamou", "Jean-David", "Carlier", "Guillaume", "Nenna", "Luca"],
    "venue": "In Splitting Methods in Communication,",
    "year": 2016
  }, {
    "title": "Wasserstein barycentric coordinates: Histogram regression using optimal transport",
    "authors": ["Bonneel", "Nicolas", "Peyré", "Gabriel", "Cuturi", "Marco"],
    "venue": "ACM Trans. on Graphics,",
    "year": 2016
  }, {
    "title": "Minimizing multimodal functions of continuous variables with the simulated annealing algorithm corrigenda for this article is available here",
    "authors": ["Corana", "Angelo", "Marchesi", "Michele", "Martini", "Claudio", "Ridella", "Sandro"],
    "venue": "ACM Trans. on Mathematical Software,",
    "year": 1987
  }, {
    "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
    "authors": ["Cuturi", "Marco"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Fast computation of Wasserstein barycenters",
    "authors": ["Cuturi", "Marco", "Doucet", "Arnaud"],
    "venue": "In Proc. Int. Conf. Machine Learning,",
    "year": 2014
  }, {
    "title": "A smoothed dual approach for variational Wasserstein problems",
    "authors": ["Cuturi", "Marco", "Peyré", "Gabriel"],
    "venue": "SIAM J. on Imaging Sciences,",
    "year": 2016
  }, {
    "title": "Learning with a Wasserstein loss",
    "authors": ["Frogner", "Charlie", "Zhang", "Chiyuan", "Mobahi", "Hossein", "Araya", "Mauricio", "Poggio", "Tomaso A"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Stochastic optimization for large-scale optimal transport",
    "authors": ["Genevay", "Aude", "Cuturi", "Marco", "Peyré", "Gabriel", "Bach", "Francis"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Accelerated mirror descent in continuous and discrete time",
    "authors": ["Krichene", "Walid", "Bayen", "Alexandre", "Bartlett", "Peter L"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "From word embeddings to document distances",
    "authors": ["Kusner", "Matt", "Sun", "Yu", "Kolkin", "Nicholas", "Weinberger", "Kilian"],
    "venue": "In Proc. of the Int. Conf. on Machine Learning,",
    "year": 2015
  }, {
    "title": "Real-time computerized annotation of pictures",
    "authors": ["Li", "Jia", "Wang", "James Z"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2008
  }, {
    "title": "Wasserstein training of restricted boltzmann machines",
    "authors": ["Montavon", "Grégoire", "Müller", "Klaus-Robert", "Cuturi", "Marco"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "A faster strongly polynomial minimum cost flow algorithm",
    "authors": ["Orlin", "James B"],
    "venue": "Operations Research,",
    "year": 1993
  }, {
    "title": "Fast dictionary learning with a smoothed Wasserstein loss",
    "authors": ["Rolet", "Antoine", "Cuturi", "Marco", "Peyré", "Gabriel"],
    "venue": "In AISTAT,",
    "year": 2016
  }, {
    "title": "Nonnegative matrix factorization with earth mover’s distance metric",
    "authors": ["Sandler", "Roman", "Lindenbaum", "Michael"],
    "venue": "In Proc. of the Conf. on Computer Vision and Pattern Recognition,",
    "year": 2009
  }, {
    "title": "Principal geodesic analysis for probability measures under the optimal transport metric",
    "authors": ["Seguy", "Vivien", "Cuturi", "Marco"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Topics in Optimal Transportation",
    "authors": ["Villani", "Cédric"],
    "venue": "Number 58. American Mathematical Soc.,",
    "year": 2003
  }, {
    "title": "Bregman alternating direction method of multipliers",
    "authors": ["Wang", "Huahua", "Banerjee", "Arindam"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Scaling up discrete distribution clustering using admm",
    "authors": ["Ye", "Jianbo", "Li", "Jia"],
    "venue": "In Proc. of the Int. Conf. on Image Processing,",
    "year": 2014
  }, {
    "title": "Determining gains acquired from word embedding quantitatively using discrete distribution clustering",
    "authors": ["Ye", "Jianbo", "Li", "Yanran", "Wu", "Zhaohui", "Wang", "James Z", "Wenjie", "Jia"],
    "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics,",
    "year": 2017
  }, {
    "title": "Fast discrete distribution clustering using Wasserstein barycenter with sparse support",
    "authors": ["Ye", "Jianbo", "Wu", "Panruo", "Wang", "James Z", "Li", "Jia"],
    "venue": "IEEE Trans. on Signal Processing,",
    "year": 2017
  }],
  "id": "SP:8ebac183869c0a4ecb8ae4c6604460205d11bb13",
  "authors": [{
    "name": "Jianbo Ye",
    "affiliations": []
  }, {
    "name": "James Z. Wang",
    "affiliations": []
  }, {
    "name": "Jia Li",
    "affiliations": []
  }],
  "abstractText": "Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.",
  "title": "A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization"
}