{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Distributed machine learning is crucial for many settings where the data is possessed by multiple parties or when the quantity of data prohibits processing at a central location. It helps to reduce the computational complexity, improve both the robustness and the scalability of data processing. In a distributed setting, multiple entities/nodes collaboratively work toward a common optimization objective through an\n1Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, Michigan, USA. Correspondence to: Xueru Zhang <xueru@umich.edu>, Mohammad Mahdi Khalili <khalili@umich.edu>, Mingyan Liu <mingyan@umich.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ninteractive process of local computation and message passing, which ideally should result in all nodes converging to a global optimum. Existing approaches to decentralizing an optimization problem primarily consist of subgradientbased algorithms (Nedic et al., 2008; Nedic & Ozdaglar, 2009; Lobel & Ozdaglar, 2011), ADMM-based algorithms (Wei & Ozdaglar, 2012; Ling & Ribeiro, 2014; Shi et al., 2014; Zhang & Kwok, 2014; Ling et al., 2016), and composite of subgradient and ADMM (Bianchi et al., 2014). It has been shown that ADMM-based algorithms can converge at the rate of O( 1k ) while subgradient-based algorithms typically converge at the rate of O( 1√\nk ), where k is the number\nof iterations (Wei & Ozdaglar, 2012). In this study, we will solely focus on ADMM-based algorithms.\nThe information exchanged over the iterative process gives rise to privacy concerns if the local training data is proprietary to each node, especially when it contains sensitive information such as medical or financial records, web search history, and so on. It is therefore highly desirable to ensure such iterative processes are privacy-preserving.\nA widely used notion of privacy is the ε-differential privacy; it is generally achieved by perturbing the algorithm such that the probability distribution of its output is relatively insensitive to any change to a single record in the input (Dwork, 2006). Several differentially private distributed algorithms have been proposed, including (Hale & Egerstedty, 2015; Huang et al., 2015; Han et al., 2017; Zhang & Zhu, 2017; Bellet et al., 2017). While a number of such studies have been done for (sub)gradient-based algorithms, the same is much harder for ADMM-based algorithms due to its computational complexity stemming from the fact that each node is required to solve an optimization problem in each iteration. To the best of our knowledge, only (Zhang & Zhu, 2017) applies differential privacy to ADMM, where the noise is either added to the dual variable (dual variable perturbation) or the primal variable (primal variable perturbation) in ADMM updates. However, (Zhang & Zhu, 2017) could only bound the privacy loss of a single iteration. Since an attacker can potentially use all intermediate results to perform inference, the privacy loss accumulates over time through the iterative process. It turns out that the tradeoff between the utility of the algorithm and its privacy preservation over the entire computational process becomes hard using the existing method.\nIn this study we propose a perturbation method that could simultaneously improve the accuracy and privacy for ADMM. We start with a modified version of ADMM whereby each node independently decides its own penalty parameter in each iteration; it may also differ from the dual updating step size. For this modified ADMM we establish conditions for convergence and quantify the lower bound of the convergence rate. We then present a penalty perturbation method to provide differential privacy. Our numerical results show that under this method, by increasing the penalty parameter over iterations, we can achieve stronger privacy guarantee as well as better algorithmic performance, i.e., more stable convergence and higher accuracy.\nThe remainder of the paper is organized as follows. We present problem formulation and definition of differential privacy and ADMM in Section 2 and a modified ADMM algorithm along with its convergence analysis in Section 3. A private version of this ADMM algorithm is then introduced in Section 4 and numerical results in Section 5. Discussions are given in Section 6 and Section 7 concludes the paper."
  }, {
    "heading": "2. Preliminaries",
    "text": ""
  }, {
    "heading": "2.1. Problem Formulation",
    "text": "Consider a connected network1 given by an undirected graph G(N ,E ), which consists of a set of nodes N = {1, 2, · · · , N} and a set of edges E = {1, 2, · · · , E}. Two nodes can exchange information if and only if they are connected by an edge. Let Vi denote node i’s set of neighbors, excluding itself. A node i contains a dataset Di = {(xni , yni )|n = 1, 2, · · · , Bi}, where xni ∈ Rd is the feature vector representing the n-th sample belonging to i, yni ∈ {−1, 1} the corresponding label, and Bi the size of Di.\nConsider the regularized empirical risk minimization (ERM) problems for binary classification defined as follows:\nmin fc OERM (fc, Dall) = N∑ i=1 C Bi Bi∑ n=1 L (yni f T c x n i )+ρR(fc)\n(1) where C ≤ Bi and ρ > 0 are constant parameters of the algorithm, the loss function L (·) measures the accuracy of classifier, and the regularizer R(·) helps to prevent overfitting. The goal is to train a (centralized) classifier fc ∈ Rd over the union of all local datasets Dall = ∪i∈N Di in a distributed manner using ADMM, while providing privacy guarantee for each data sample 2.\n1A connected network is one in which every node is reachable (via a path) from every other node.\n2The proposed penalty perturbation method is not limited to classification problems. It can be applied to general ADMM-based distributed algorithms since the convergence and privacy analysis"
  }, {
    "heading": "2.2. Conventional ADMM",
    "text": "To decentralize (1), let fi be the local classifier of each node i. To achieve consensus, i.e., f1 = f2 = · · · = fN , a set of auxiliary variables {wij |i ∈ N , j ∈ Vi} are introduced for every pair of connected nodes. As a result, (1) is reformulated equivalently as:\nmin {fi},{wij} ÕERM ({fi}Ni=1, Dall) = N∑ i=1 O(fi, Di)\ns.t. fi = wij , wij = fj , i ∈ N , j ∈ Vi\n(2)\nwhere O(fi, Di) = C\nBi\n∑Bi n=1 L (y n i f T i x n i ) + ρ\nN R(fi).\nThe objective in (2) can be solved using ADMM. Let {fi} be the shorthand for {fi}i∈N ; let {wij , λkij} be the shorthand for {wij , λkij}i∈N ,j∈Vi,k∈{a,b}, where λaij , λbij are dual variables corresponding to equality constraints fi = wij and wij = fj respectively. Then the augmented Lagrangian is as follows:\nLη({fi}, {wij , λkij}) = N∑ i=1 O(fi, Di)\n+ N∑ i=1 ∑ j∈Vi (λaij) T (fi − wij) + N∑ i=1 ∑ j∈Vi (λbij) T (wij − fj) (3)\n+ N∑ i=1 ∑ j∈Vi η 2 (||fi − wij ||22 + ||wij − fj ||22) .\nIn the (t + 1)-th iteration, the ADMM updates consist of the following:\nfi(t+ 1) = argmin fi Lη({fi}, {wij(t), λkij(t)}) ; (4)\nwij(t+ 1) = argmin wij Lη({fi(t+ 1)}, {wij , λkij(t)}) ; (5)\nλaij(t+ 1) = λ a ij(t) + η(fi(t+ 1)− wij(t+ 1)) ; (6)\nλbij(t+ 1) = λ b ij(t) + η(wij(t+ 1)− fj(t+ 1)) . (7)\nUsing Lemma 3 in (Forero et al., 2010), if dual variables λaij(t) and λ b ij(t) are initialized to zero for all node pairs (i, j), then λaij(t) = λ b ij(t) and λ k ij(t) = −λkji(t) will hold for all iterations with k ∈ {a, b}, i ∈ N , j ∈ Vi.\nLet λi(t) = ∑ j∈Vi λ a ij(t) = ∑ j∈Vi λ b ij(t), then the ADMM iterations (4)-(7) can be simplified as:\nfi(t+ 1) = argmin fi {O(fi, Di) + 2λi(t)T fi\n+η ∑ j∈Vi ||1 2 (fi(t) + fj(t))− fi||22 } ; (8)\nλi(t+ 1) = λi(t) + η\n2 ∑ j∈Vi (fi(t+ 1)− fj(t+ 1)) . (9)\nin Section 3 & 4 remain valid."
  }, {
    "heading": "2.3. Differential Privacy",
    "text": "Differential privacy (Dwork, 2006) can be used to measure the privacy risk of each individual sample in the dataset quantitatively. Mathematically, a randomized algorithm A (·) taking a dataset as input satisfies ε-differential privacy if for any two datasets D, D̂ differing in at most one data point, and for any set of possible outputs S ⊆ range(A ), Pr(A (D) ∈ S) ≤ exp(ε)Pr(A (D̂) ∈ S) holds. We call two datasets differing in at most one data point as neighboring datasets. The above definition suggests that for a sufficiently small ε, an adversary will observe almost the same output regardless of the presence (or value change) of any one individual in the dataset; this is what provides privacy protection for that individual."
  }, {
    "heading": "2.4. Private ADMM proposed in (Zhang & Zhu, 2017)",
    "text": "Two randomizations were proposed in (Zhang & Zhu, 2017): (i) dual variable perturbation, where each node i adds a random noise to its dual variable λi(t) before updating its primal variable fi(t) using (8) in each iteration; and (ii) primal variable perturbation, where after updating primal variable fi(t), each node adds a random noise to it before broadcasting to its neighbors. Both were evaluated for a single iteration for a fixed privacy constraint. As we will see later in numerical experiments, the privacy loss accumulates significantly when inspected over multiple iterations.\nIn contrast, in this study we will explore the use of the penalty parameter η to provide privacy. In particular, we will allow this to be private information to every node, i.e., each decides its own η in every iteration and it is not exchanged among the nodes. Below we will begin by modifying the ADMM to accommodate private penalty terms."
  }, {
    "heading": "3. Modified ADMM (M-ADMM)",
    "text": ""
  }, {
    "heading": "3.1. Making η a node’s private information",
    "text": "Conventional ADMM (Boyd et al., 2011) requires that the penalty parameter η be fixed and equal to the dual updating step size for all nodes in all iterations. Varying the penalty parameter to accelerate convergence in ADMM has been proposed in the literature. For instance, (He et al., 2002; Magnússon et al., 2014; Aybat & Iyengar, 2015; Xu et al., 2016) vary this penalty parameter in every iteration but keep it the same for different equality constraints in (2). In (Song et al., 2016; Zhang & Wang, 2017) this parameter varies in each iteration and is allowed to differ for different equality constraints. However, all of these modifications are based on the original ADMM (Eqn. (4)-(7)) and not on the simplified version (Eqn. (8)-(9)); the significance of this difference is discussed below in the context of privacy requirement. Moreover, we will decouple ηi(t+1) from the dual updating step size, denoted as θ below. For simplicity, θ is fixed for\nall nodes in our analysis, but can also be private information as we show in numerical experiments.\nFirst consider replacing η with ηij(t+ 1) in Eqn. (4)-(5) of the original ADMM (as is done in (Song et al., 2016; Zhang & Wang, 2017)) and replacing η with θ in Eqn. (6)-(7); we obtain the following:\nfi(t+ 1) = argmin fi {O(fi, Di) + 2λi(t)T fi\n+ ∑ j∈Vi ηij(t+ 1) + ηji(t+ 1) 2 ||1 2 (fi(t) + fj(t))− fi||22} ;\nλi(t+ 1) = λi(t) + θ\n2 ∑ j∈Vi (fi(t+ 1)− fj(t+ 1)) .\nThis however violates our requirement that ηji(t) be node j’s private information since this is needed by node i to perform the above computation. To resolve this, we instead start from the simplified ADMM, modifying Eqn. (8)-(9):\nfi(t+ 1) = argmin fi {O(fi, Di) + 2λi(t)T fi\n+ηi(t+ 1) ∑ j∈Vi ||fi − 1 2 (fi(t) + fj(t))||22 } ; (10)\nλi(t+ 1) = λi(t) + θ\n2 ∑ j∈Vi (fi(t+ 1)− fj(t+ 1)) , (11)\nwhere ηi(t+ 1) is now node i’s private information. Indeed ηi(t+ 1) is no longer purely a penalty parameter related to any equality constraint in the original sense. We will however refer to it as the private penalty parameter for simplicity. The above constitutes the M-ADMM algorithm."
  }, {
    "heading": "3.2. Convergence Analysis",
    "text": "We next show that the M-ADMM (Eqn. (10)-(11)) converges to the optimal solution under a set of common technical assumptions. Our proof is based on the method given in (Ling et al., 2016).\nAssumption 1: Function O(fi, Di) is convex and continuously differentiable in fi, ∀i.\nAssumption 2: The solution set to the original ERM problem (1) is nonempty and there exists at least one bounded element.\nThe KKT optimality condition of the primal update (10) is:\n0 = ∇O(fi(t+ 1), Di) + 2λi(t) +ηi(t+ 1) ∑ j∈Vi (2fi(t+ 1)− (fi(t) + fj(t))) . (12)\nWe next rewrite (11)-(12) in matrix form. Define the adjacency matrix of the network A ∈ RN×N as\naij = { 1, if node i and node j are connected 0, otherwise .\nStack the variables fi(t), λi(t) and ∇O(fi(t), Di) for i ∈ N into matrices, i.e.,\nf̂(t) =  f1(t) T f2(t) T\n... fN (t) T\n ∈ RN×d , Λ(t) =  λ1(t) T λ2(t) T\n... λN (t) T\n ∈ RN×d\n∇Ô(f̂(t), Dall) =  ∇O(f1(t), D1)T ∇O(f2(t), D2)T\n... ∇O(fN (t), DN )T  ∈ RN×d Let Vi = |Vi| be the number of neighbors of node i, and define the degree matrix D = diag([V1;V2; · · · ;VN ]) ∈ RN×N . Define for the t-th iteration a penalty-weighted matrix W (t) = diag([η1(t); η2(t); · · · ; ηN (t)]) ∈ RN×N . Then the matrix form of (11)-(12) are:\n∇Ô(f̂(t+ 1), Dall) + 2Λ(t) + 2W (t+ 1)Df̂(t+ 1) −W (t+ 1)(D +A)f̂(t) = 0N×d ; (13)\n2Λ(t+ 1) = 2Λ(t) + θ(D −A)f̂(t+ 1) . (14)\nNote that D −A is the Laplacian matrix and D +A is the signless Laplacian matrix of the network, with the following properties if the network is connected: (i) D ± A 0 is positive semi-definite; (ii) Null(D − A) = c1, i.e., every member in the null space of D −A is a scalar multiple of 1 with 1 being the vector of all 1’s (Kelner, 2007). Let √ X denote the square root of a symmetric positive semi-definite (PSD) matrix X that is also symmetric PSD, i.e., √ X √ X = X . Define matrix Y (t) such that 2Λ(t) =√\nD −AY (t). Since Λ(0) = zeros(N, d), which is in the column space of D −A, this together with (14) imply that Λ(t) is in the column space of D − A and √ D −A. This guarantees the existence of Y (t). This allows us to rewrite (13)-(14) as:\n∇Ô(f̂(t+ 1), Dall) + √ D −AY (t+ 1)\n+(W (t+ 1)− θI)(D −A)f̂(t+ 1) +W (t+ 1)(D +A)(f̂(t+ 1)− f̂(t)) = 0N×d ; (15)\nY (t+ 1) = Y (t) + θ √ D −Af̂(t+ 1) . (16)\nLemma 3.1 [First-order Optimality Condition (Ling et al., 2016)] Under Assumptions 1 and 2, the following two statements are equivalent:\n• f̂∗ = [(f∗1 )T ; (f∗2 )T ; · · · ; (f∗N )T ] ∈ RN×d is consensual, i.e., f∗1 = f ∗ 2 = · · · = f∗N = f∗c where f∗c is the\noptimal solution to (1).\n• There exists a pair (f̂∗, Y ∗) with Y ∗ = √ D −AX\nfor some X ∈ RN×d such that\n∇Ô(f̂∗, Dall) + √ D −AY ∗ = 0N×d ; (17)√ D −Af̂∗ = 0N×d . (18)\nLemma 3.1 shows that a pair (Y ∗, f̂∗) satisfying (17)(18) is equivalent to the optimal solution of our problem, hence the convergence of M-ADMM is proved by showing that (Y (t), f̂(t)) converges to a pair (Y ∗, f̂∗) satisfying (17)(18).\nTheorem 3.1 Consider the modified ADMM defined by (10)-(11). Let {Y (t), f̂(t)} be outputs in each iteration and (Y ∗, f̂∗) a pair satisfying (17)-(18). Denote\nZ(t) =\n[ Y (t)\nf̂(t)\n] ∈ R2N×d, Z∗ = [ Y ∗\nf̂∗\n] ∈ R2N×d\nJ(t) = [ IN×N θ 0 0 W (t)(D +A) ] ∈ R2N×2N\nLet 〈·, ·〉F be the Frobenius inner product of two matrices. We have\n〈Z(t+ 1)−Z∗, J(t+ 1)(Z(t+ 1)−Z(t))〉F ≤ 0 . (19)"
  }, {
    "heading": "If ηi(t + 1) ≥ ηi(t) ≥ θ > 0 and ηi(t) < +∞, ∀t, i, then",
    "text": "(Y (t), f̂(t)) converges to (Y ∗, f̂∗)."
  }, {
    "heading": "3.3. Convergence Rate Analysis",
    "text": "To further establish the convergence rate of modified ADMM, an additional assumption is used:\nAssumption 3: For all i ∈ N , O(fi, Di) is strongly convex in fi and has Lipschitz continues gradients, i.e., for any f1i and f 2 i , we have:\n(f1i −f2i )T (∇O(f1i , Di)−∇O(f2i , Di)) ≥ mi||f1i −f2i ||22\n||∇O(f1i , Di)−∇O(f2i , Di)||2 ≤Mi||f1i − f2i ||2 (20) where mi > 0 is the strong convexity constant and 0 < Mi < +∞ is the Lipschitz constant.\nTheorem 3.2 Define Dm = diag([m1;m2; · · · ;mN ]) ∈ RN×N and DM = diag([M21 ;M22 ; · · · ;M2N ]) ∈ RN×N with mi > 0 and 0 < Mi < +∞ as given in Assumption 3. Denote by ||X||2J = 〈X, JX〉F the Frobenius inner product of any matrix X and JX; denote by σmin(·) and σmax(·) the smallest nonzero, and the largest, singular values of a matrix, respectively.\nLet σ̃max(t) = σmax(W (t)(D +A)), σ̄max/min(t) = σmax/min((W (t)− θI)(D −A)) and µ > 1 be an arbitrary constant. Consider any δ(t) that satisfies (21)(22):\nδ(t)µ2σ̃max(t) θσmin(D −A) ≤ 1 (21)\nand\nδ(t)( µσ̄max(t) 2IN + µ2DM θσmin(D −A)(µ− 1) +W (t)(D +A))\n2(W (t)− θI)(D −A) + 2Dm . (22)"
  }, {
    "heading": "If ηi(t + 1) ≥ ηi(t) ≥ θ > 0 and ηi(t) < +∞, ∀t, i, then",
    "text": "(Y (t), f̂(t)) converges to (Y ∗, f̂∗) in the following sense:\n(1 + δ(t))||Z(t)− Z∗||2J(t) ≤ ||Z(t− 1)− Z ∗||2J(t) .\nFurthermore, a lower bound on δ(t) is:\nmin{θσmin(D −A) µ2σ̃max(t) , 2mo + 2σ̄min(t) µ2M2O+µσ̄max(t) 2\nθσmin(D−A)(µ−1) + σ̃max(t) } (23)\nwhere mo = mini∈N {mi} and MO = maxi∈N {Mi}.\nAlthough Theorem 3.2 only gives a lower bound on the convergence rate (1 + δ(t)) of the M-ADMM, it reflects the impact of penalty {ηi(t)}Ni=1 on the convergence. Since σ̄max(t) = σmax((W (t)− θI)(D −A)) and σ̃max(t) = σmax(W (t)(D +A)), larger penalty results in larger σ̄max(t) and σ̃max(t). By (23), the first term,\nθσmin(D−A) µ2σ̃max(t)\nis smaller when σ̃max(t) is larger. The second term is bounded by θσmin(D−A)(µ−1)(2mo+2σ̄min(t))µσ̄max(t)2 , which is smaller when σ̄max(t) is larger. Therefore, the convergence rate 1 + δ(t) decreases as {ηi(t)}Ni=1 increase."
  }, {
    "heading": "4. Private M-ADMM",
    "text": "In this section we present a privacy preserving version of MADMM. To begin, a random noise i(t+1) with probability density proportional to exp{−αi(t + 1)|| i(t + 1)||2} is added to penalty term in the objective function of (10):\nLprivi (t+ 1) = O(fi, Di) + 2λi(t) T fi +ηi(t+ 1) ∑ j∈Vi ||fi + i(t+ 1)− 1 2 (fi(t) + fj(t))||22\n(24)\nTo generate this noisy vector, choose the norm from the gamma distribution with shape d and scale 1αi(t+1) and the direction uniformly, where d is the dimension of the feature space. Then node i’s local result is obtained by finding the optimal solution to the private objective function:\nfi(t+ 1) = argmin fi\nLprivi (t+ 1), i ∈ N . (25)\nIt is equivalent to (26) below when noise ηi(t+1)Vi i(t+1)\nAlgorithm 1 Penalty perturbation (PP) method Parameter: Determine θ such that 2c1 < BiC ( ρ N + 2θVi)\nholds for all i. Initialize: Generate fi(0) randomly and λi(0) = 0d×1 for every node i ∈ N , t = 0 Input: {Di}Ni=1, {αi(1), · · · , αi(T )}Ni=1 for t = 0 to T − 1 do\nfor i = 1 to N do Generate noise i(t+ 1) ∼ exp(−αi(t+ 1)|| ||2) Perturb the penalty term according to (24) Update primal variable via (25) end for for i = 1 to N do\nBroadcast fi(t+ 1) to all neighbors j ∈ Vi end for for i = 1 to N do\nUpdate dual variable according to (11) end for\nend for Output: upper bound of the total privacy loss β\nis added to the dual variable λi(t):\nargmin fi\nL̃privi (t+ 1) = C\nBi Bi∑ n=1 L (yni f T i x n i ) + ρ N R(fi)\n+2(λi(t) + ηi(t+ 1)Vi i(t+ 1)) T fi +ηi(t+ 1) ∑ j∈Vi ||fi − 1 2 (fi(t) + fj(t))||22 .\nFurther, if ηi(t+1) = η = θ,∀i, t, then the above is reduced to the dual variable perturbation in (Zhang & Zhu, 2017)3.\nThe complete procedure is shown in Algorithm 1, where the condition used to generate θ helps bound the worst-case privacy loss but is not necessary in guaranteeing convergence.\nIn a distributed and iterative setting, the “output” of the algorithm is not merely the end result, but includes all intermediate results generated and exchanged during the iterative process. For this reason, we formally state the differential privacy definition in this setting below.\nDefinition 4.1 Consider a connected network G(N ,E ) with a set of nodes N = {1, 2, · · · , N}. Let f(t) = {fi(t)}Ni=1 denote the information exchange of all nodes in the t-th iteration. A distributed algorithm is said to satisfy β-differential privacy during T iterations if for any two datasets Dall = ∪iDi and D̂all = ∪iD̂i, differing in at\n3Only a single iteration is considered in (Zhang & Zhu, 2017) while imposing a privacy constraint. Since we consider the entire iterative process, we don’t impose per-iteration privacy constraint but calculate the total privacy loss.\nmost one data point, and for any set of possible outputs S during T iterations, the following holds:\nPr({f(t)}Tt=0 ∈ S|Dall) Pr({f(t)}Tt=0 ∈ S|D̂all) ≤ exp(β)\nWe now state our main result on the privacy property of the penalty perturbation algorithm using the above definition. Additional assumptions on L (·) and R(·) are used.\nAssumption 4: The loss function L is strictly convex and twice differentiable. |L ′| ≤ 1 and 0 < L ′′ ≤ c1 with c1 being a constant.\nAssumption 5: The regularizer R is 1-strongly convex and twice continuously differentiable.\nTheorem 4.1 Normalize feature vectors in the training set such that ||xni ||2 ≤ 1 for all i ∈ N and n. Then the private M-ADMM algorithm (PP) satisfies the β-differential privacy with\nβ ≥ max i∈N { T∑ t=1 C(1.4c1 + αi(t)) ηi(t)ViBi } . (26)"
  }, {
    "heading": "5. Numerical Experiments",
    "text": "We use the same dataset as (Zhang & Zhu, 2017), i.e., the Adult dataset from the UCI Machine Learning Repository (Lichman, 2013). It consists of personal information of around 48,842 individuals, including age, sex, race, education, occupation, income, etc. The goal is to predict whether the annual income of an individual is above $50,000.\nTo preprocess the data, we (1) remove all individuals with missing values; (2) convert each categorical attribute (with m categories) to a binary vector of length m; (3) normalize columns (features) such that the maximum value of each column is 1; (4) normalize rows (individuals) such that its l2 norm is at most 1; and (5) convert labels {≥ 50k,≤ 50k} to {+1,−1}. After this preprocessing, the final data includes 45,223 individuals, each represented as a 105-dimensional vector of norm at most 1.\nWe will use as loss function the logistic loss L (z) = log(1 + exp(−z)), with |L ′| ≤ 1 and L ′′ ≤ c1 = 14 . The regularizer is R(fi) = 12 ||fi|| 2 2. We will measure the accuracy of the algorithm by the average loss L(t) := 1 N ∑N i=1 1 Bi ∑Bi n=1 L (y n i fi(t)\nTxni ) over the training set. We will measure the privacy of the algorithm by the upper bound P (t) := max i∈N { ∑t r=1 C(1.4c1+αi(r)) ηi(r)ViBi\n}. The smaller L(t) and P (t), the higher accuracy and stronger privacy guarantee."
  }, {
    "heading": "5.1. Convergence of M-ADMM",
    "text": "We consider a five-node network and assign each node the following private penalty parameters: ηi(t) = ηi(1)q t−1 i for node i, where [η1(1), · · · , η5(1)] = [0.55, 0.65, 0.6, 0.55, 0.6] and [q1, · · · , q5] = [1.01, 1.03, 1.1, 1.2, 1.02].\nFigure 1(a) shows the convergence of M-ADMM under these parameters while using a fixed dual updating step size θ = 0.5 across all nodes (blue curve). This is consistent with Theorem 3.1. As mentioned earlier, this step size can also be non-fixed (black) and different (red) for different nodes. In\nFigure 1(b) we let each node use the same penalty ηi(t) = η(t) = 0.5qt−11 and compare the results by increasing q1, q1 ≥ 1. We see that increasing penalty slows down the convergence, and larger increase in q1 slows it down even more, which is consistent with Theorem 3.2."
  }, {
    "heading": "5.2. Private M-ADMM",
    "text": "We next inspect the accuracy and privacy of the penalty perturbation (PP) based private M-ADMM (Algorithm 1) and compare it with the dual variable perturbation (DVP) method proposed in (Zhang & Zhu, 2017). In this set of experiments, for simplicity of presentation we shall fix θ = 0.5, let ηi(t) = η(t) = θqt−11 , and noise αi(t) = α(t) = α(1)qt−12 for all nodes. We observe similar results when ηi(t) and αi(t) vary from node to node.\nFor each parameter setting, we perform 10 independent runs of the algorithm, and record both the mean and the range of their accuracy. Specifically, Ll(t) denotes the average loss over the training dataset in the t-th iteration of the l-th experiment (1 ≤ l ≤ 10). The mean of average loss is then given by Lmean(t) = 110 ∑10 l=1 L\nl(t), and the range Lrange(t) = max\n1≤l≤10 Ll(t) − min 1≤l≤10 Ll(t). The larger the\nrange Lrange(t) the less stable the algorithm, i.e., under the same parameter setting, the difference in performances (convergence curves) of every two experiments is larger. Each parameter setting also has a corresponding upper bound on the privacy loss denoted by P (t). Figures 2(a)2(b) show both Lmean(t) and Lrange(t) as vertical bars centered at Lmean(t). Their corresponding privacy upper bound is given in Figures 2(c)2(d). The pair 2(a)-2(c) (resp. 2(b)2(d)) is for the same parameter setting.\nFigure 2 compares PP (blue & red, with ηi(t) increasing geometrically) with DVP (black & magenta, with ηi(t) = θ, ∀i, t). We see that in both cases improved accuracy comes at the expense of higher privacy loss (from magenta to black under DVP, from red to blue under PP). However, we also see that with suitable choices of q1, q2, PP can outperform DVP significantly both in accuracy and in privacy (e.g., red outperforms magenta in both accuracy and privacy, and blue outperforms black in both accuracy and privacy).\nWe also performed experiments with the same dataset on larger networks with tens and hundreds of nodes and with samples evenly and unevenly spread across nodes. In both cases, convergence is attained and our algorithm continues to outperform (Zhang & Zhu, 2017) in a large network (see Figures 3 & 4). Since the privacy loss of the network is dominated by the node with the largest privacy loss and it increases as the number of samples in a node decreases (Theorem 4.1), the loss of privacy in a network with uneven sample size distributions is higher; note that this is a common issue with this type of analysis."
  }, {
    "heading": "6. Discussion",
    "text": "Our numerical results show that increasing the penalty {ηi(t)}Ni=1 over iterations can improve the algorithm’s accuracy and privacy simultaneously. Below we provide some insight on why this is the case and discuss possible generalizations of our method."
  }, {
    "heading": "6.1. Higher accuracy",
    "text": "When the algorithm is perturbed by random noise, which is necessary to achieve privacy, increasing the penalty parameters over iterations makes the algorithm more noise resistant. In particular, for the minimization in (25), larger ηi(t+ 1) results in smaller updates of variables, i.e., smaller distance between fi(t + 1) and fi(t). In the non-private case, since fi(t) always moves toward the optimum, smaller update slows down the process. In the private case, on the other hand, since a random noise is added to each update, fi(t) does not always move toward the optimum in each step. When the overall perturbation has a larger variance, it is more likely that fi(t) could move further away from the optimum in some iterations. Because larger ηi(t) leads to smaller update, it helps prevent fi(t) from moving too far away from the optimum, thus stabilizing the algorithm (smaller Lrange(t))."
  }, {
    "heading": "6.2. Stronger privacy",
    "text": "First of all, more added noise means stronger privacy guarantee. Increasing ηi(t) and αi(t) in such a way that the overall perturbation 2ηi(t)Vi i(t)T fi(t) in (26) is increasing leads to less privacy loss, as shown in Figure 2. The noise resistance provided by an increasing ηi(t) indeed allows larger noises to be added under PP without jeopardizing convergence as observed in Section 6.1.\nMore interestingly, keeping ηi(t) private further strengthens privacy protection. Consider the following threat model: An attacker knows {(xni , yni )} Bi n=2 and {fj(t)}j∈Vi∪i for all t, i.e., all data points except for the first data point of node i, as well as all intermediate results of node i and its neighbors. If the attacker also knows the dual updating step size θ and penalty parameter {ηi(t)}Tt=1 of node i, it can then infer the unknown data point (x1i , y 1 i ) with high confidence by combining the KKT optimality conditions from all iterations (see supplementary material for details). However, if the penalty parameters {ηi(t)}Tt=1 are private to each node, then it is impossible for the attacker to infer the unknown data. Even if the attacker knows the participation of an individual, it remains hard to infer its features."
  }, {
    "heading": "6.3. Generalization & comparison",
    "text": "The main contribution of this paper is the finding that increasing {ηi}Ni=1 improves the algorithm’s ability to resist\nnoise: even though we increase noise in each iteration to improve privacy, the accuracy does not degrade significantly due to this increasing robustness, which improves the privacy-utility tradeoff. This property holds regardless of the noise distribution. While the present privacy analysis uses a similar framework as in (Chaudhuri et al., 2011; Zhang & Zhu, 2017) (objective perturbation with added Gamma noise), we can also use methods from other existing (centralized) ERM differentially private algorithms to every iteration in ADMM. For example, if we allow some probability (δ > 0) of violating -differential privacy and adopt a weaker variant ( , δ)-differential privacy, we can adopt methods from works such as (Kifer et al., 2012; Jain & Thakurta, 2014; Bassily et al., 2014), by adding Gaussian noise to achieve tighter bounds on privacy loss. However, as noted above, the robustness is improved as {ηi}Ni=1 increases; thus the same conclusion can be reached that both privacy and accuracy can be improved.\nThis idea can also be generalized to other differentially private iterative algorithms. A key observation of our algorithm is that the overall perturbation (2ηi(t)Vi i(t)T fi(t)) is related to the parameter that controls the updating step size (ηi(t)). In general, if the algorithm is perturbed in each iteration with a quantity φ( , ξ), which is a function of added noise and some parameter ξ that controls the step size, such that the resulting step size and φ( , ξ) move in opposite directions (i.e., decreasing step size increases the φ( , ξ)), then it is possible to simultaneously improve both accuracy and privacy by varying ξ to decrease the step size over time.\nInterestingly, in a differentially private (sub)gradient-based distributed algorithm (Huang et al., 2015), the step size\nand the overall perturbation move in the same direction (i.e., decreasing step size decreases perturbation). The reason for this difference is that under this subgradient-based algorithm, the sensitivity of the algorithm decreases with decreasing step size, which in turn leads to privacy constraint being satisfied with smaller perturbation. In contrast, for ADMM the sensitivity of the algorithm is independent of the step size, and the perturbation actually needs to increase to improve privacy guarantee; the decreasing step size acts to compensate for this increase in noise to maintain accuracy, as discussed in Section 6.1.\nThis issue of step size never arises in the study of (Zhang & Zhu, 2017) because the analysis is only for a single iteration; however, as we have seen doing so leads to significant total privacy loss over many iterations."
  }, {
    "heading": "7. Conclusions",
    "text": "This paper presents a penalty-perturbation idea to introduce privacy preservation in iterative algorithms. We showed how to modify an ADMM-based distributed algorithm to improve privacy without compromising accuracy. The key idea is to add a perturbation correlated to the step size so that they change in opposite directions. Applying this idea to other iterative algorithms can be part of the future work."
  }, {
    "heading": "Acknowledgements",
    "text": "This work is supported by the NSF under grants CNS1422211, CNS-1646019, CNS-1739517."
  }],
  "year": 2018,
  "references": [{
    "title": "An alternating direction method with increasing penalty for stable principal component pursuit",
    "authors": ["N.S. Aybat", "G. Iyengar"],
    "venue": "Computational Optimization and Applications,",
    "year": 2015
  }, {
    "title": "Differentially private empirical risk minimization: Efficient algorithms and tight error bounds",
    "authors": ["R. Bassily", "A. Smith", "A. Thakurta"],
    "venue": "arXiv preprint arXiv:1405.7085,",
    "year": 2014
  }, {
    "title": "Fast and Differentially Private Algorithms for Decentralized Collaborative Machine Learning",
    "authors": ["A. Bellet", "R. Guerraoui", "M. Taziki", "M. Tommasi"],
    "venue": "PhD thesis,",
    "year": 2017
  }, {
    "title": "A stochastic primal-dual algorithm for distributed asynchronous composite optimization",
    "authors": ["P. Bianchi", "W. Hachem", "F. Iutzeler"],
    "venue": "In Signal and Information Processing (GlobalSIP),",
    "year": 2014
  }, {
    "title": "Differentially private empirical risk minimization",
    "authors": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Consensusbased distributed support vector machines",
    "authors": ["P.A. Forero", "A. Cano", "G.B. Giannakis"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Differentially private cloudbased multi-agent optimization with constraints",
    "authors": ["M. Hale", "M. Egerstedty"],
    "venue": "In American Control Conference (ACC),",
    "year": 2015
  }, {
    "title": "Differentially private distributed constrained optimization",
    "authors": ["S. Han", "U. Topcu", "G.J. Pappas"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2017
  }, {
    "title": "A new inexact alternating directions method for monotone variational inequalities",
    "authors": ["B. He", "Liao", "L.-Z", "D. Han", "H. Yang"],
    "venue": "Mathematical Programming,",
    "year": 2002
  }, {
    "title": "Differentially private distributed optimization",
    "authors": ["Z. Huang", "S. Mitra", "N. Vaidya"],
    "venue": "In Proceedings of the 2015 International Conference on Distributed Computing and Networking,",
    "year": 2015
  }, {
    "title": "near) dimension independent risk bounds for differentially private learning",
    "authors": ["P. Jain", "A.G. Thakurta"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Private convex empirical risk minimization and high-dimensional regression",
    "authors": ["D. Kifer", "A. Smith", "A. Thakurta"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2012
  }, {
    "title": "Decentralized linearized alternating direction method of multipliers",
    "authors": ["Q. Ling", "A. Ribeiro"],
    "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2014
  }, {
    "title": "Weighted admm for fast decentralized network optimization",
    "authors": ["Q. Ling", "Y. Liu", "W. Shi", "Z. Tian"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2016
  }, {
    "title": "Distributed subgradient methods for convex optimization over random networks",
    "authors": ["I. Lobel", "A. Ozdaglar"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2011
  }, {
    "title": "On the convergence of an alternating direction penalty method for nonconvex problems",
    "authors": ["S. Magnússon", "P.C. Weeraddana", "M.G. Rabbat", "C. Fischione"],
    "venue": "In Signals, Systems and Computers,",
    "year": 2014
  }, {
    "title": "Distributed subgradient methods for multi-agent optimization",
    "authors": ["A. Nedic", "A. Ozdaglar"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2009
  }, {
    "title": "Distributed subgradient methods and quantization effects",
    "authors": ["A. Nedic", "A. Olshevsky", "A. Ozdaglar", "J.N. Tsitsiklis"],
    "venue": "In Decision and Control,",
    "year": 2008
  }, {
    "title": "On the linear convergence of the admm in decentralized consensus optimization",
    "authors": ["W. Shi", "Q. Ling", "K. Yuan", "G. Wu", "W. Yin"],
    "venue": "IEEE Trans. Signal Processing,",
    "year": 2014
  }, {
    "title": "Fast admm algorithm for distributed optimization with adaptive penalty",
    "authors": ["C. Song", "S. Yoon", "V. Pavlovic"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Distributed alternating direction method of multipliers",
    "authors": ["E. Wei", "A. Ozdaglar"],
    "venue": "In Decision and Control (CDC),",
    "year": 2012
  }, {
    "title": "Adaptive admm with spectral penalty parameter selection",
    "authors": ["Z. Xu", "M.A. Figueiredo", "T. Goldstein"],
    "venue": "arXiv preprint arXiv:1605.07246,",
    "year": 2016
  }, {
    "title": "Privacy-preserving decentralized optimization based on admm",
    "authors": ["C. Zhang", "Y. Wang"],
    "venue": "arXiv preprint arXiv:1707.04338,",
    "year": 2017
  }, {
    "title": "Asynchronous distributed admm for consensus optimization",
    "authors": ["R. Zhang", "J. Kwok"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Dynamic differential privacy for admm-based distributed classification learning",
    "authors": ["T. Zhang", "Q. Zhu"],
    "venue": "IEEE Transactions on Information Forensics and Security,",
    "year": 2017
  }],
  "id": "SP:4958d799a9b4685ee4dd731fe20916776c5da203",
  "authors": [{
    "name": "Xueru Zhang",
    "affiliations": []
  }, {
    "name": "Mohammad Mahdi Khalili",
    "affiliations": []
  }, {
    "name": "Mingyan Liu",
    "affiliations": []
  }, {
    "name": "Mahdi Khalili",
    "affiliations": []
  }],
  "abstractText": "Alternating direction method of multiplier (ADMM) is a popular method used to design distributed versions of a machine learning algorithm, whereby local computations are performed on local data with the output exchanged among neighbors in an iterative fashion. During this iterative process the leakage of data privacy arises. A differentially private ADMM was proposed in prior work (Zhang & Zhu, 2017) where only the privacy loss of a single node during one iteration was bounded, a method that makes it difficult to balance the tradeoff between the utility attained through distributed computation and privacy guarantees when considering the total privacy loss of all nodes over the entire iterative process. We propose a perturbation method for ADMM where the perturbed term is correlated with the penalty parameters; this is shown to improve the utility and privacy simultaneously. The method is based on a modified ADMM where each node independently determines its own penalty parameter in every iteration and decouples it from the dual updating step size. The condition for convergence of the modified ADMM and the lower bound on the convergence rate are also derived.",
  "title": "Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms "
}