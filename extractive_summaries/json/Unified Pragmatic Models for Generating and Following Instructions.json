{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1951–1963 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "How should speakers and listeners reason about each other when they communicate? A core insight of computational pragmatics is that speaker and listener agents operate within a cooperative game-theoretic context, and that each agent benefits from reasoning about others’ intents and actions within that context. Pragmatic inference has been studied by a long line of work in linguistics, natural language processing, and cognitive science. In this paper, we present a technique for layering explicit pragmatic inference on top of models for complex, sequential instruction-following and instruction-generation tasks. We investigate a range of current data sets for both tasks, showing that pragmatic behavior arises naturally from this inference procedure, and gives rise to state-of-theart results in a variety of domains.\nConsider the example shown in Figure 1a, in which a speaker agent must describe a route to\na target position in a hallway. A conventional learned instruction-generating model produces a truthful description of the route (walk forward four times). But the pragmatic speaker in this paper, which is capable of reasoning about the listener, chooses to also include additional information (the intersection with the bare concrete hall), to reduce potential ambiguity and increase the odds that the listener reaches the correct destination.\nThis same reasoning procedure also allows a listener agent to overcome ambiguity in instructions by reasoning counterfactually about the speaker (Figure 1b). Given the command walk along the blue carpet and you pass two objects, a conven-\n1951\ntional learned instruction-following model is willing to consider all paths that pass two objects, and ultimately arrives at an unintended final position. But a pragmatic listener that reasons about the speaker can infer that the long path would have been more easily described as go to the sofa, and thus that the shorter path is probably intended. In these two examples, which are produced by the system we describe in this paper, a unified reasoning process (choose the output sequence which is most preferred by an embedded model of the other agent) produces pragmatic behavior for both speakers and listeners.\nThe application of models with explicit pragmatic reasoning abilities has so far been largely restricted to simple reference games, in which the listener’s only task is to select the right item from among a small set of candidate referents given a single short utterance from the speaker. But as the example shows, there are real-world instruction following and generation tasks with rich action spaces that might also benefit from pragmatic modeling. Moreover, approaches that learn to map directly between human-annotated instructions and action sequences are ultimately limited by the effectiveness of the humans themselves. The promise of pragmatic modeling is that we can use these same annotations to build a model with a different (and perhaps even better) mechanism for interpreting and generating instructions.\nThe primary contribution of this work is to show how existing models of pragmatic reasoning can be extended to support instruction following and generation for challenging, multi-step, interactive tasks. Our experimental evaluation focuses on four instruction-following domains which have been studied using both semantic parsers and attentional neural models. We investigate the interrelated tasks of instruction following and instruction generation, and show that incorporating an explicit model of pragmatics helps in both cases. Reasoning about the human listener allows a speaker model to produce instructions that are easier for humans to interpret correctly in all domains (with absolute gains in accuracy ranging from 12% to 46%). Similarly, reasoning about the human speaker improves the accuracy of the listener models in interpreting instructions in most domains (with gains in accuracy of up to 10%). In all cases, the resulting systems are competitive with, and in many cases exceed, results from past\nstate-of-the-art systems for these tasks.1"
  }, {
    "heading": "2 Problem Formulation",
    "text": "Consider the instruction following and instruction generation tasks shown in Figure 1, where an agent must produce or interpret instructions about a structured world context (e.g. walk along the blue carpet and you pass two objects).\nIn the instruction following task, a listener agent begins in a world state (in Figure 1 an initial map location and orientation). The agent is then tasked with following a sequence of direction sentences d1 . . . dK produced by humans. At each time t the agent receives a percept yt, which is a feature-based representation of the current world state, and chooses an action at (e.g. move forward, or turn). The agent succeeds if it is able to reach the correct final state described by the directions.\nIn the instruction generation task, the agent receives a sequence of actions a1, · · · aT along with the world state y1, · · · yT at each action, and must generate a sequence of direction sentences d1, . . . dK describing the actions. The agent succeeds if a human listener is able to correctly follow those directions to the intended final state.\nWe evaluate models for both tasks in four domains. The first domain is the SAIL corpus of virtual environments and navigational directions (MacMahon et al., 2006; Chen and Mooney, 2011), where an agent navigates through a twodimensional grid of hallways with patterned walls and floors and a discrete set of objects (Figure 1 shows a portion of one of these hallways).\nIn the three SCONE domains (Long et al., 2016), the world contains a number of objects with various properties, such as colored beakers which an agent can combine, drain, and mix. Instructions describe how these objects should be manipulated. These domains were designed to elicit instructions with a variety of context-dependent language phenomena, including ellipsis and coreference (Long et al., 2016) which we might expect a model of pragmatics to help resolve (Potts, 2011)."
  }, {
    "heading": "3 Related Work",
    "text": "The approach in this paper builds upon long lines of work in pragmatic modeling, instruction following, and instruction generation.\n1Source code is available at http://github.com/ dpfried/pragmatic-instructions\nPragmatics Our approach to pragmatics (Grice, 1975) belongs to a general category of rational speech acts models (Frank and Goodman, 2012), in which the interaction between speakers and listeners is modeled as a probabilistic process with Bayesian actors (Goodman and Stuhlmüller, 2013). Alternative formulations (e.g. with bestresponse rather than probabilistic dynamics) are also possible (Golland et al., 2010). Inference in these models is challenging even when the space of listener actions is extremely simple (Smith et al., 2013), and one of our goals in the present work is to show how this inference problem can be solved even in much richer action spaces than previously considered in computational pragmatics. This family of pragmatic models captures a number of important linguistic phenomena, especially those involving conversational implicature (Monroe and Potts, 2015); we note that many other topics studied under the broad heading of “pragmatics,” including presupposition and indexicality, require different machinery.\nWilliams et al. (2015) use pragmatic reasoning with weighted inference rules to resolve ambiguity and generate clarification requests in a humanrobot dialog task. Other recent work on pragmatic models focuses on the referring expression generation or “contrastive captioning” task introduced by Kazemzadeh et al. (2014). In this family are approaches that model the listener at training time (Mao et al., 2016), at evaluation time (Andreas and Klein, 2016; Monroe et al., 2017; Vedantam et al., 2017; Su et al., 2017) or both (Yu et al., 2017b; Luo and Shakhnarovich, 2017).\nOther conditional sequence rescoring models that are structurally similar but motivated by concerns other than pragmatics include Li et al. (2016) and Yu et al. (2017a). Lewis et al. (2017) perform a similar inference procedure for a competitive negotiation task. The language learning model of Wang et al. (2016) also features a structured output space and uses pragmatics to improve online predictions for a semantic parsing model. Our approach in this paper performs both generation and interpretation, and investigates both structured and unstructured output representations.\nInstruction following Work on instruction following tasks includes models that parse commands into structured representations processed by a rich execution model (Tellex et al., 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Guu\net al., 2017), and models that map directly from instructions to a policy over primitive actions (Branavan et al., 2009), possibly mediated by an intermediate alignment or attention variable (Andreas and Klein, 2015; Mei et al., 2016). We use a model similar to Mei et al. (2016) as our base listener in this paper, evaluating on the SAIL navigation task (MacMahon et al., 2006) as they did, as well as the SCONE context-dependent execution domains (Long et al., 2016).\nInstruction generation Previous work has also investigated the instruction generation task, in particular for navigational directions. The GIVE shared tasks (Byron et al., 2009; Koller et al., 2010; Striegnitz et al., 2011) have produced a large number of interactive direction-giving systems, both rule-based and learned. The work most immediately related to the generation task in this paper is that of Daniele et al. (2017), which also focuses on the SAIL dataset but requires substantial additional structured annotation for training, while both our base and pragmatic speaker models learn directly from strings and action sequences.\nOlder work has studied the properties of effective human strategies for generating navigational directions (Anderson et al., 1991). Instructions of this kind can be used to extract templates for generation (Look, 2008; Dale et al., 2005), while here we focus on the more challenging problem of learning to generate new instructions from scratch. Like our pragmatic speaker model, Goeddel and Olson (2012) also reason about listener behavior when generating navigational instructions, but rely on rule-based models for interpretation."
  }, {
    "heading": "4 Pragmatic inference procedure",
    "text": "As a foundation for pragmatic inference, we assume that we have base listener and speaker models to map directions to actions and vice-versa. (Our notation for referring to models is adapted from Bergen et al. (2016).) The base listener, L0, produces a probability distribution over sequences of actions, conditioned on a representation of the directions and environment as seen before each action: PL0(a1:T |d1:K , y1:T ). Similarly, the base speaker, S0, defines a distribution over possible descriptions conditioned on a representation of the actions and environment: PS0(d1:K |a1:T , y1:T ).\nOur pragmatic inference procedure requires these base models to produce candidate outputs from a given input (actions from descriptions, for\nthe listener; descriptions from actions, for the speaker), and calculate the probability of a fixed output given an input, but is otherwise agnostic to the form of the models.\nWe use standard sequence-to-sequence models with attention for both the base listener and speaker (described in Section 5). Our models use segmented action sequences, with one segment (sub-sequence of actions) aligned with each description sentence dj , for all j ∈ {1 . . .K}. This segmentation is either given as part of the training and testing data (in the instruction following task for the SAIL domain, and in both tasks for the SCONE domain, where each sentence corresponds to a single action), or is predicted by a separate segmentation model (in the generation task for the SAIL domain), see Section 5."
  }, {
    "heading": "4.1 Models",
    "text": "Using these base models as self-contained modules, we derive a rational speaker and rational listener that perform inference using embedded instances of these base models (Figure 2a). When describing an action sequence, a rational speaker S1 chooses a description that has a high chance of causing the listener modeled by L0 to follow the given actions:\nS1(a1:T ) = argmax d1:K\nPL0(a1:T |d1:K , y1:T ) (1)\n(noting that, in all settings we explore here, the percepts y1:T are completely determined by the actions a1:T ). Conversely, a rational listener L1 follows a description by choosing an action sequence which has high probability of having caused the\nspeaker, modeled by S0, to produce the description:\nL1(d1:K) = argmax a1:T\nPS0(d1:K |a1:T , y1:T ) (2)\nThese optimization problems are intractable to solve for general base listener and speaker agents, including the sequence-to-sequence models we use, as they involve choosing an input (from a combinatorially large space of possible sequences) to maximize the probability of a fixed output sequence. We instead follow a simple approximate inference procedure, detailed in Section 4.2.\nWe consider also incorporating the scores of the base model used to produce the candidates. For the case of the speaker, we define a combined rational speaker, denoted S0 · S1, that selects the candidate that maximizes a weighted product of probabilities under both the base listener and the base speaker:\nargmax d1:K\nPL0(a1:T |d1:K , y1:T )λ\n× PS0(d1:K |a1:T , y1:T )1−λ (3)\nfor a fixed interpolation hyperparameter λ ∈ [0, 1]. There are several motivations for this combination with the base speaker score. First, as argued by Monroe et al. (2017), we would expect varying degrees of base and reasoned interpretation in human speech acts. Second, we want the descriptions produced by the model to be fluent descriptions of the actions. Since the base models are trained discriminatively, maximizing the probability of an output sequence for a fixed input sequence, their scoring behaviors for fixed outputs paired with inputs dissimilar to those seen in the training set may be\npoorly calibrated (for example when conditioning on ungrammatical descriptions). Incorporating the scores of the base model used to produce the candidates aims to prevent this behavior.\nTo define rational listeners, we use the symmetric formulation: first, draw candidate action sequences from L0. For L1, choose the actions that achieve the highest probability under S0; and for the combination model L0 · L1 choose the actions with the highest weighted combination of S0 and L0 (paralleling equation 3)."
  }, {
    "heading": "4.2 Inference",
    "text": "As in past work (Smith et al., 2013; Andreas and Klein, 2016; Monroe et al., 2017), we approximate the optimization problems in equations 1, 2, and 3: use the base models to generate candidates, and rescore them to find ones that are likely to produce the desired behavior.\nIn the case of the rational speaker S1, we use the base speaker S0 to produce a set of n candidate descriptions w(1)1:K1 . . . w (n) 1:Kn\nfor the sequences a1:T , y1:T , using beam search. We then find the score of each description under PL0 (using it as the input sequence for the observed output actions we want the rational speaker to describe), or a weighted combination of PL0 and the original candidate score PS0 , and choose the description w(j)1:Kj with the largest score, approximately solving the maximizations in equations 1 or 3, respectively. We perform a symmetric procedure for the rational listener: produce action sequence candidates from the base listener, and rescore them using the base speaker.2\nAs the rational speaker must produce long output sequences (with multiple sentences), we interleave the speaker and listener in inference, determining each output sentence sequentially. From a list of candidate direction sentences from the base speaker for the current subsequence of actions, we choose the top-scoring direction under the listener model (which may also condition on the directions which have been output previously), and then\n2We use ensembles of models for the base listener and speaker (subsection 5.3), and to obtain candidates that are high-scoring under the combination of models in the ensemble, we perform standard beam search using all models in lock-step. At every timestep of the beam search, each possible extension of an output sequence is scored using the product of the extension’s conditional probabilities across all models in the ensemble.\nmove on to the next subsequence of actions.3"
  }, {
    "heading": "5 Base model details",
    "text": "Given this framework, all that remains is to describe the base models L0 and S0. We implement these as sequence-to-sequence models that map directions to actions (for the listener) or actions to directions (for the speaker), additionally conditioning on the world state at each timestep."
  }, {
    "heading": "5.1 Base listener",
    "text": "Our base listener model, L0, predicts action sequences conditioned on an encoded representation of the directions and the current world state. In the SAIL domain, this is the model of Mei et al. (2016) (illustrated in green in Figure 2b for a single sentence and its associated actions), see “domain specifics” below.\nEncoder Each direction sentence is encoded separately with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997); the LSTM’s hidden states are reset for each sentence. We obtain a representation hek for the kth word in the current sentence by concatenating an embedding for the word with its forward and backward LSTM outputs.\nDecoder We generate actions incrementally using an LSTM decoder with monotonic alignment between the direction sentences and subsequences of actions; at each timestep the decoder predicts the next action for the current sentence w1:M (including choosing to shift to the next sentence). The decoder takes as input at timestep t the current world state, yt and a representation zt of the current sentence, updates the decoder state hd, and outputs a distribution over possible actions:\nhdt = LSTMd(h d t−1, [Wyyt, zt])\nqt =Wo(Wyyt +Whh d t +Wzzt)\np(at | a1:t−1, y1:t, w1:M ) ∝ exp(qt)\nwhere all weight matrices W are learned parameters. The sentence representation zt is produced using an attention mechanism (Bahdanau et al., 2015) over the representation vectors he1 . . . h e M\n3We also experimented with sampling from the base models to produce these candidate lists, as was done in previous work (Andreas and Klein, 2016; Monroe et al., 2017). In early experiments, however, we found better performance with beam search in the rational models for all tasks.\nfor words in the current sentence:\nαt,k ∝ exp(v · tanh(Wdhdt−1 +Wehek))\nzt =\nM∑\nk=1\nαt,kh e k\nwhere the attention weights αt,k are normalized to sum to one across positions k in the input, and weight matrices W and vector v are learned.\nDomain specifics For SAIL, we use the alignments between sentences and route segments annotated by Chen and Mooney (2011), which were also used in previous work (Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Mei et al., 2016). Following Mei et al. (2016), we reset the decoder’s hidden state for each sentence.\nIn the SCONE domains, which have a larger space of possible outputs than SAIL, we extend the decoder by: (i) decomposing each action into an action type and arguments for it, (ii) using separate attention mechanisms for types and arguments and (iii) using state-dependent action embeddings. See Appendix A in the supplemental material for details. The SCONE domains are constructed so that each sentence corresponds to a single (nondecomposed) action; this provides our segmentation of the action sequence."
  }, {
    "heading": "5.2 Base speaker",
    "text": "While previous work (Daniele et al., 2017) has relied on more structured approaches, we construct our base speaker model S0 using largely the same sequence-to-sequence machinery as above. S0 (illustrated in orange in Figure 2b) encodes a sequence of actions and world states, and then uses a decoder to output a description.\nEncoder We encode the sequence of vector embeddings for the actions at and world states yt using a bidirectional LSTM. Similar to the base listener’s encoder, we then obtain a representation het for timestep t by concatenating at and yt with the LSTM outputs at that position.\nDecoder As in the listener, we use an LSTM decoder with monotonic alignment between direction sentences and subsequences of actions, and attention over the subsequences of actions. The decoder takes as input at position k an embedding for the previously generated word wk−1 and a representation zk of the current subsequence of\nactions and world states, and produces a distribution over words (including ending the description for the current subsequence and advancing to the next). The decoder’s output distribution is produced by:\nhdk = LSTMd(h d k−1, [wk−1, zk]) qk =Whh d k +Wzzk\np(wk | w1:k−1, a1:T , y1:T ) ∝ exp(qk)\nwhere all weight matrices W are learned parameters.4 As in the base listener, the input representation zk is produced by attending to the vectors he1 . . . h e T encoding the input sequence (here, encoding the subsequence of actions and world states to be described):\nαk,t ∝ exp(v · tanh(Wdhdk−1 +Wehet ))\nzk = T∑\nt=1\nαk,t h e t\nThe decoder’s LSTM state is reset at the beginning of each sentence.\nDomain specifics In SAIL, for comparison to the generation system of Daniele et al. (2017) which did not use segmented routes, we train a route segmenter for use at test time. We also represent routes using a collapsed representation of action sequences. In the SCONE domains, we (i) use the same context-dependent action embeddings used in the listener, and (ii) don’t require an attention mechanism, since only a single action is used to produce a given sentence within the sequence of direction sentences. See Appendix A for more details."
  }, {
    "heading": "5.3 Training",
    "text": "The base listener and speaker models are trained independently to maximize the conditional likelihoods of the actions–directions pairs in the training sets. See Appendix A for details on the optimization, LSTM variant, and hyperparameters.\nWe use ensembles for the base listener L0 and base speaker S0, where each ensemble consists of 10 models trained from separate random parameter initializations. This follows the experimental setup of Mei et al. (2016) for the SAIL base listener.\n4All parameters are distinct from those used in the base listener; the listener and speaker are trained separately."
  }, {
    "heading": "6 Experiments",
    "text": "We evaluate speaker and listener agents on both the instruction following and instruction generation tasks in the SAIL domain and three SCONE domains (Section 2). For all domains, we compare the rational listener and speaker against the base listener and speaker, as well as against past state-of-the-art results for each task and domain. Finally, we examine pragmatic inference from a model combination perspective, comparing the pragmatic reranking procedure to ensembles of a larger number of base speakers or listeners.\nFor all experiments, we use beam search both to generate candidate lists for the rational systems (section 4.2) and to generate the base model’s output. We fix the beam size n to be the same in both the base and rational systems, using n = 20 for the speakers and n = 40 for the listeners. We tune the weight λ in the combined rational agents (L0 · L1 or S0 · S1) to maximize accuracy (for listener models) or BLEU (for speaker models) on each domain’s development data."
  }, {
    "heading": "6.1 Instruction following",
    "text": "We evaluate our listener models by their accuracy in carrying out human instructions: whether the systems were able to reach the final world state which the human was tasked with guiding them to.\nSAIL We follow standard cross-validation evaluation for the instruction following task on the SAIL dataset (Artzi and Zettlemoyer, 2013; Artzi\na red guy appears on the far left then to orange’s other side\net al., 2014; Mei et al., 2016).5 Table 1 shows improvements over the base listener L0 when using the rational listener L0 · L1 in the single- and multi-sentence settings. We also report the best accuracies from past work. We see that the largest relative gains come in the multi-sentence setting, where handling ambiguity is potentially more important to avoid compounding errors. The rational model improves on the published results of Mei et al. (2016), and while it is still below the systems of Artzi and Zettlemoyer (2013) and Artzi et al. (2014), which use additional supervision in the form of hand-annotated seed lexicons and logical domain representations, it approaches their results in the single-sentence setting.\nSCONE In the SCONE domains, past work has trained listener models with weak supervision\n5Past work has differed in the handling of undetermined orientations in the routes, which occur in the first state for multi-sentence routes and the first segment of their corresponding single-sentence routes. For comparison to both types of past work, we train and evaluate listeners in two settings: Abs, which sets these undetermined starting orientations to be a fixed absolute orientation, and Rel, where an undetermined starting orientation is set to be a 90 degree rotation from the next state in the true route.\n(with no intermediate actions between start and end world states) on a subset of the full SCONE training data. We use the full training set, and to use a model and training procedure consistent with the SAIL setting, train listener and speaker models using the intermediate actions as supervision as well.6 The evaluation method and test data are the same as in past work on SCONE: models are provided with an initial world state and a sequence of 5 instructions to carry out, and are evaluated on their accuracy in reaching the intended final world state.\nResults are reported in Table 2. We see gains from the rational system L0 · L1 in both the Alchemy and Scene domains. The pragmatic inference procedure allows correcting errors or overly-literal interpretations from the base listener. An example is shown in Figure 3. The base listener (left) interprets then to orange’s other side incorrectly, while the rational listener discounts this interpretation (it could, for example, be better described by to the left of blue) and produces the action the descriptions were meant to describe (right). To the extent that human annotators already account for pragmatic effects when generating instructions, examples like these suggest that our model’s explicit reasoning is able to capture interpretation behavior that the base sequence-tosequence listener model is unable to model."
  }, {
    "heading": "6.2 Instruction generation",
    "text": "As our primary evaluation for the instruction generation task, we had Mechanical Turk workers carry out directions produced by the speaker mod-\n6Since the pragmatic inference procedure we use is agnostic to the models’ training method, it could also be applied to the models of Guu et al. (2017); however we find that pragmatic inference can improve even upon our stronger base listener models.\nels (and by other humans) in a simulated version of each domain. For SAIL, we use the simulator released by Daniele et al. (2017) which was used in their human evaluation results, and we construct simulators for the three SCONE domains. In all settings, we take a sample of 50 action sequences from the domain’s test set (using the same sample as Daniele et al. (2017) for SAIL), and have three separate Turk workers attempt to follow the systems’ directions for the action sequence.\nTable 3 gives the average accuracy of subjects in reaching the intended final world state across all sampled test instances, for each domain. The “human-generated” row reports subjects’ accuracy at following the datasets’ reference directions. The directions produced by the base speaker S0 are often much harder to follow than those produced by humans (e.g. 29.3% of S0’s directions are correctly interpretable for Alchemy, vs. 83.3% of human directions). However, we see substantial gains from the rational speaker S0 ·S1 over S0 in all cases (with absolute gains in accuracy ranging from 12.4% to 46.0%), and the average accuracy of humans at following the rational speaker’s directions is substantially higher than for humanproduced directions in the Tangrams domain. In the SAIL evaluation, we also include the directions produced by the system of Daniele et al. (2017) (DBW), and find that the rational speaker’s directions are followable to comparable accuracy.\nWe also compare the directions produced by the systems to the reference instructions given by humans in the dataset, using 4-gram BLEU7 (Pap-\n7See Appendix A for details on evaluating BLEU in the SAIL setting, where there may be a different number of reference and predicted sentences for a given example.\nineni et al., 2002) in Table 4. Consistent with past work (Krahmer and Theune, 2010), we find that BLEU score is a poor indicator of whether the directions can be correctly followed.\nQualitatively, the rational inference procedure is most successful in fixing ambiguities in the base speaker model’s descriptions. Figure 4 gives a typical example of this for the last few timesteps from a Tangrams instance. The base speaker correctly describes that the shape should be added back, but does not specify where to add it, which could lead a listener to add it in the same position it was deleted. The human speaker also makes this mistake in their description. This speaks to the difficulty of describing complex actions pragmatically even for humans in the Tangrams domain. The ability of the pragmatic speaker to produce directions that are easier to follow than humans’ in this domain (Table 3) shows that the pragmatic model can generate something different (and in some cases better) than the training data."
  }, {
    "heading": "6.3 Pragmatics as model combination",
    "text": "Finally, our rational models can be viewed as pragmatically-motivated model combinations, producing candidates using base listener or speaker models and reranking using a combination of scores from both. We want to verify that a rational listener using n ensembled base listeners and n base speakers outperforms a simple ensemble of 2n base listeners (and similarly for the rational speaker).\nFixing the total number of models to 20 in each\nlistener experiment, we find that the rational listener (using an ensemble of 10 base listener models and 10 base speaker models) still substantially outperforms the ensembled base listener (using 20 base listener models): accuracy gains are 68.5→ 71.6%, 70.1 → 72.0%, 71.9 → 72.7%, and 69.1 → 69.6% for SAIL single-sentence Rel, Alchemy, Scene, and Tangrams, respectively.\nFor the speaker experiments, fixing the total number of models to 10 (since inference in the speaker models is more expensive than in the follower models), we find similar gains as well: the rational speaker improves human accuracy at following the generated instructions from 61.9 → 73.4%, 30.7 → 74.7%, 32.0 → 66.0%, 58.7 → 92.7%, for SAIL, Alchemy, Scene, and Tangrams, respectively.8"
  }, {
    "heading": "7 Conclusion",
    "text": "We have demonstrated that a simple procedure for pragmatic inference, with a unified treatment for speakers and listeners, obtains improvements for instruction following as well as instruction generation in multiple settings. The inference procedure is capable of reasoning about sequential, interdependent actions in non-trivial world contexts. We find that pragmatics improves upon the performance of the base models for both tasks, in most cases substantially. While this is perhaps unsurprising for the generation task, which has been discussed from a pragmatic perspective in a variety of recent work in NLP, it is encouraging that pragmatic reasoning can also improve performance for a grounded listening task with sequential, structured output spaces."
  }, {
    "heading": "Acknowledgments",
    "text": "We are grateful to Andrea Daniele for sharing the SAIL simulator and their system’s outputs, to Hongyuan Mei for help with the dataset, and to Tom Griffiths and Chris Potts for helpful comments and discussion. This work was supported by DARPA through the Explainable Artificial Intelligence (XAI) program. DF is supported by a Huawei / Berkeley AI fellowship. JA is supported by a Facebook graduate fellowship.\n8The accuracies for the base speakers are slightly different than in Table 3, despite being produced by the same systems, since we reran experiments to control as much as possible for time variation in the pool of Mechanical Turk workers."
  }, {
    "heading": "A Supplemental Material",
    "text": "A.1 SCONE listener details We factor action production in each of the three SCONE domains, separately predicting the action type and the arguments specific to that action type. Action types and arguments are listed in the first two columns of Table 5. For example, Alchemy’s actions involve predicting the action type, a potential source beaker index i and target beaker index j, and potential amount to drain a. All factors of the action (the type and options for each argument) are predicted using separate attention mechanisms, which produce a vector qf giving unnormalized scores for factor f (e.g. scoring each possible type, or each possible choice for the argument).\nWe also obtain state-specific embeddings of actions, to make it easier for the model to learn relevant features from the state embeddings (e.g. rather than needing to learn to select the region of the state vector corresponding to the 5th beaker in the action MIX(5) in Alchemy, this action’s contextual embedding encodes the current content of the 5th beaker). We incorporate these statespecific embeddings into computation of the action probabilities using a bilinear bonus score:\nb(a) = q>Wqaa+ w>a a\nwhere q is the concatenation of all qf factor scoring vectors, and Wqa and wa are a learned parameter matrix and vector, respectively. This bonus score b(a) for each action is added to the un-\nnormalized score for the corresponding action a (computed by summing the entries of the qf vectors which correspond to the factored action components), and the normalized output distribution is then produced using a softmax over all valid actions.\nA.2 SAIL speaker details Since our speaker model operates on segmented action sequences, we train a route segmenter on the training data and then predict segmentations for the test data. This provides a closer comparison to the generation system of Daniele et al. (2017) which did not use segmented routes. The route segmenter runs a bidirectional LSTM over the concatenated state and action embeddings (as in the speaker encoder), then uses a logistic output layer to classify whether the route should be split at each possible timestep. We also collapse consecutive sequences of forward movement actions into single actions (e.g. MOVE4 representing four consecutive forward movements), which we found helped prevent counting errors (such as outputting move forward three when the correct route moved forward four steps).\nA.3 SCONE speaker details We use a one-hot representation of the arguments (see Table 5) and contextual embedding (as described in A.1) for each action at as input to the SCONE speaker encoder at time t (along with the representation et of the world state, as in SAIL). Since SCONE uses a monotonic, one-toone alignment between actions and direction sentences, the decoder does not use a learned attention mechanism but fixes the contextual representation zk to be the encoded vector at the action corresponding to the sentence currently being generated.\nA.4 Training details We optimize model parameters using ADAM (Kingma and Ba, 2015) with default hyperparameters and the initialization scheme of Glorot and Bengio (2010). All LSTMs have one layer. The LSTM cell in both the listener and the follower use coupled input and forget gates, and peephole connections to the cell state (Greff et al., 2016). We also apply the LSTM variational dropout scheme of Gal and Ghahramani (2016), using the same dropout rate for inputs, outputs, and recurrent connections. See Table 6 for hyperparameters. We\nperform early stopping using the evaluation metric (accuracy for the listener and BLEU score for the speaker) on the development set.\nA.5 Computing BLEU for SAIL To compute BLEU in the SAIL experiments, as the speaker models may choose produce a different number of sentences for each route than in the true description, we obtain a single sequence of words from a multi-sentence description produced for a route by concatenating the sentences, separated by end-of-sentence tokens. We then calculate corpus-level 4-gram BLEU between all these sequences in the test set and the true multisentence descriptions (concatenated in the same way)."
  }],
  "year": 2018,
  "references": [{
    "title": "The HCRC map task corpus. Language and speech 34(4):351–366",
    "authors": ["Anne H. Anderson", "Miles Bader", "Ellen Gurman Bard", "Elizabeth Boyle", "Gwyneth Doherty", "Simon Garrod", "Stephen Isard", "Jacqueline Kowtko", "Jan McAllister", "Jim Miller"],
    "year": 1991
  }, {
    "title": "Alignmentbased compositional semantics for instruction following",
    "authors": ["Jacob Andreas", "Dan Klein."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Reasoning about pragmatics with neural listeners and speakers",
    "authors": ["Jacob Andreas", "Dan Klein."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Learning compact lexicons for CCG semantic parsing",
    "authors": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
    "authors": ["Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Transactions of the Association for Computational Linguistics 1(1):49–62.",
    "year": 2013
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "International Conference on Learning Representations .",
    "year": 2015
  }, {
    "title": "Pragmatic reasoning through semantic inference",
    "authors": ["Leon Bergen", "Roger Levy", "Noah Goodman."],
    "venue": "Semantics and Pragmatics 9.",
    "year": 2016
  }, {
    "title": "Reinforcement learning for mapping instructions to actions",
    "authors": ["S.R.K. Branavan", "Harr Chen", "Luke S. Zettlemoyer", "Regina Barzilay."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational",
    "year": 2009
  }, {
    "title": "Report on the first NLG challenge on generating instructions in virtual environments (GIVE)",
    "authors": ["Donna Byron", "Alexander Koller", "Kristina Striegnitz", "Justine Cassell", "Robert Dale", "Johanna Moore", "Jon Oberlander."],
    "venue": "Proceedings of the 12th european",
    "year": 2009
  }, {
    "title": "Fast online lexicon learning for grounded language acquisition",
    "authors": ["David L Chen."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 430–439.",
    "year": 2012
  }, {
    "title": "Learning to interpret natural language navigation instructions from observations",
    "authors": ["David L. Chen", "Raymond J. Mooney."],
    "venue": "Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence. volume 2, pages 1–2.",
    "year": 2011
  }, {
    "title": "Using natural language generation in automatic route",
    "authors": ["Robert Dale", "Sabine Geldof", "Jean-Philippe Prost."],
    "venue": "Journal of Research and practice in Information Technology 37(1):89.",
    "year": 2005
  }, {
    "title": "Navigational instruction generation as inverse reinforcement learning with neural machine translation",
    "authors": ["Andrea F. Daniele", "Mohit Bansal", "Matthew R. Walter."],
    "venue": "Proceedings of Human-Robot Interaction .",
    "year": 2017
  }, {
    "title": "Predicting pragmatic reasoning in language games",
    "authors": ["Michael C Frank", "Noah D Goodman."],
    "venue": "Science 336(6084):998–998.",
    "year": 2012
  }, {
    "title": "A theoretically grounded application of dropout in recurrent neural networks",
    "authors": ["Yarin Gal", "Zoubin Ghahramani."],
    "venue": "Advances in Neural Information Processing Systems 29 (NIPS).",
    "year": 2016
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "AISTATS. volume 9, pages 249–256.",
    "year": 2010
  }, {
    "title": "Dart: A particle-based method for generating easy-to-follow directions",
    "authors": ["Robert Goeddel", "Edwin Olson."],
    "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, pages 1213–1219.",
    "year": 2012
  }, {
    "title": "A game-theoretic approach to generating spatial descriptions",
    "authors": ["Dave Golland", "Percy Liang", "Dan Klein."],
    "venue": "Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
    "year": 2010
  }, {
    "title": "Knowledge and implicature: Modeling language understanding as social cognition",
    "authors": ["Noah D Goodman", "Andreas Stuhlmüller."],
    "venue": "Topics in cognitive science 5(1):173–184.",
    "year": 2013
  }, {
    "title": "Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems",
    "authors": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutnı́k", "Bas R Steunebrink", "Jürgen Schmidhuber"],
    "year": 2016
  }, {
    "title": "Logic and conversation",
    "authors": ["H.P. Grice."],
    "venue": "P. Cole and J. L. Morgan, editors, Syntax and Semantics: Vol. 3: Speech Acts, Academic Press, San Diego, CA, pages 41–58.",
    "year": 1975
  }, {
    "title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
    "authors": ["Kelvin Guu", "Panupong Pasupat", "Evan Zheran Liu", "Percy Liang."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "ReferItGame: Referring to objects in photographs of natural scenes",
    "authors": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 787–798.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "International Conference on Learning Representations .",
    "year": 2015
  }, {
    "title": "Report on the second NLG challenge on generating instructions in virtual environments (GIVE-2)",
    "authors": ["Alexander Koller", "Kristina Striegnitz", "Andrew Gargett", "Donna Byron", "Justine Cassell", "Robert Dale", "Johanna Moore", "Jon Oberlander."],
    "venue": "Proceedings of",
    "year": 2010
  }, {
    "title": "Deal or no deal? end-to-end learning for negotiation dialogues",
    "authors": ["Mike Lewis", "Denis Yarats", "Yann N Dauphin", "Devi Parikh", "Dhruv Batra."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2017
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational",
    "year": 2016
  }, {
    "title": "Simpler context-dependent logical forms via model projections",
    "authors": ["Reginald Long", "Panupong Pasupat", "Percy Liang."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2016
  }, {
    "title": "Cognitively-inspired direction giving",
    "authors": ["Gary Wai Keung Look."],
    "venue": "Ph.D. thesis, Massachusetts Institute of Technology.",
    "year": 2008
  }, {
    "title": "Comprehension-guided referring expressions",
    "authors": ["Ruotian Luo", "Gregory Shakhnarovich."],
    "venue": "Computer Vision and Pattern Recognition.",
    "year": 2017
  }, {
    "title": "Walk the talk: Connecting language, knowledge, and action in route instructions",
    "authors": ["Matt MacMahon", "Brian Stankiewicz", "Benjamin Kuipers."],
    "venue": "Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence 2(6):4.",
    "year": 2006
  }, {
    "title": "Generation and comprehension of unambiguous object descriptions",
    "authors": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan Yuille", "Kevin Murphy."],
    "venue": "Computer Vision and Pattern Recognition.",
    "year": 2016
  }, {
    "title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew Walter."],
    "venue": "Proceedings of the Meeting of the Association for the Advancement of Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "Colors in context: A pragmatic neural model for grounded language understanding",
    "authors": ["Will Monroe", "Robert X.D. Hawkins", "Noah D. Goodman", "Christopher Potts."],
    "venue": "Transactions of the Association for Computational Linguistics .",
    "year": 2017
  }, {
    "title": "Learning in the Rational Speech Acts model",
    "authors": ["Will Monroe", "Christopher Potts."],
    "venue": "Proceedings of 20th Amsterdam Colloquium. ILLC, Amsterdam.",
    "year": 2015
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational",
    "year": 2002
  }, {
    "title": "Learning and using language via recursive pragmatic reasoning about other agents",
    "authors": ["Nathaniel J Smith", "Noah Goodman", "Michael Frank."],
    "venue": "Advances in Neural Information Processing Systems. pages 3039–3047.",
    "year": 2013
  }, {
    "title": "Report on the second second challenge on generating instructions in virtual environments GIVE-2.5",
    "authors": ["Kristina Striegnitz", "Alexandre Denis", "Andrew Gargett", "Konstantina Garoufi", "Alexander Koller", "Mariët Theune"],
    "venue": "In Proceedings of the 13th Euro-",
    "year": 2011
  }, {
    "title": "Reasoning about finegrained attribute phrases using reference games",
    "authors": ["Jong-Chyi Su", "Chenyun Wu", "Huaizu Jiang", "Subhransu Maji."],
    "venue": "International Conference on Computer Vision.",
    "year": 2017
  }, {
    "title": "Understanding natural language commands for robotic navigation and mobile manipulation",
    "authors": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R. Walter", "Ashis Gopal Banerjee", "Seth Teller", "Nicholas Roy."],
    "venue": "In Proceedings of the Na-",
    "year": 2011
  }, {
    "title": "Context-aware captions from context-agnostic supervision",
    "authors": ["Ramakrishna Vedantam", "Samy Bengio", "Kevin Murphy", "Devi Parikh", "Gal Chechik."],
    "venue": "Computer Vision and Pattern Recognition (CVPR). volume 3.",
    "year": 2017
  }, {
    "title": "Learning language games through interaction",
    "authors": ["Sida I. Wang", "Percy Liang", "Christopher D. Manning."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2016
  }, {
    "title": "Going beyond literal command-based instructions: Extending robotic natural language interaction capabilities",
    "authors": ["Tom Williams", "Gordon Briggs", "Bradley Oosterveld", "Matthias Scheutz."],
    "venue": "AAAI. pages 1387–1393.",
    "year": 2015
  }, {
    "title": "The neural noisy channel",
    "authors": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomas Kocisky."],
    "venue": "International Conference on Learning Representations .",
    "year": 2017
  }, {
    "title": "A joint speaker-listener-reinforcer model for referring expressions",
    "authors": ["Licheng Yu", "Hao Tan", "Mohit Bansal", "Tamara L. Berg."],
    "venue": "Computer Vision and Pattern Recognition.",
    "year": 2017
  }, {
    "title": "All LSTMs have one layer. The LSTM cell in both the listener and the follower use coupled input and forget gates, and peephole connections to the cell state (Greff et al., 2016)",
    "authors": ["Glorot", "Bengio"],
    "year": 2010
  }],
  "id": "SP:56cbbbf6a41a127fd3a9899c20a02707e5e2ffee",
  "authors": [{
    "name": "Daniel Fried Jacob",
    "affiliations": []
  }, {
    "name": "Andreas Dan Klein",
    "affiliations": []
  }],
  "abstractText": "We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.",
  "title": "Unified Pragmatic Models for Generating and Following Instructions"
}