{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 939–949 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1087"
  }, {
    "heading": "1 Introduction",
    "text": "In distributional vector representations, opposite relations are not fully captured. Take, for example, words such as “great” and “awful” that can appear with similar frequency in the same sentence structure: “John had a great meeting” and “John had an awful day.” Word embeddings, which are successful in a wide array of NLP tasks (Turney et al., 2010; Dhillon et al., 2015), fail to capture this antonymy because they follow the distributional hypothesis that similar words are used in similar\ncontexts (Harris, 1954), thus assigning small cosine or euclidean distances between the vector representations of “great” and “awful”.\nWhile vector space models (Turney et al., 2010) such as word2vec (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), or Eigenwords (Dhillon et al., 2015) capture relatedness, they do not adequately encode synonymy and semantic similarity (Mohammad et al., 2013; Scheible et al., 2013). Our goal is to create clusters of synonyms or semantically equivalent words and linguistically motivated unified constructs. Signed graphs, which are graphs with negative edge weights, were first introduced by Cartwright and Harary (1956). However, signed graph clustering for multiclass normalized cuts (K-clusters) has been largely unexplored until recently. We present a novel theory and method that extends multiclass normalized cuts (K-cluster) of Yu and Shi (2003) to signed graphs (Gallier, 2016)1 and the work of Kunegis et al. (2010) to K-clustering. This extension allows the incorporation of knowledge base information, positive and negatively weighted links (see figure 2.1). Negative edges serve as repellent or opposite relationships between nodes.\nOur signed spectral normalized graph cut algorithm (henceforth, signed clustering) builds negative edge relations into graph embeddings using similarity structure in vector spaces. It takes as input an initial set of vectors and edge relations, and hence is easy to combine with any word embedding method. This paper formally improves on the discrete optimization problem of Yu and Shi (2003).\nSigned clustering gives better clusters than spectral clustering (Shi and Malik, 2000) of word embeddings, and it has better coverage and is more robust than thesaurus look-up. This is because the-\n1Gallier (2016) is a full theoretical exposition of our methods with proofs on arXiv.\n939\nsauri erroneously give equal weight to rare senses of a word – for example, “rich” as a rarely used synonym of “absurd”. Also, the overlap between thesauri is small, due to their manual creation. Lin (1998) found 17.8397% overlap between synonym sets from Roget’s Thesaurus and WordNet 1.5. We find similarly small overlap between all three thesauri tested.\nWe evaluate our clusters using SimLex-999 (Hill et al., 2014) and SimVerb-3500 (Gerz et al., 2016) as a ground truth for our cluster evaluation. Finally, we test our method on the sentiment analysis task. Overall, signed spectral clustering can augment methods using signed information and has broad application for many fields.\nOur main contributions are: the novel extension of signed clustering to the multiclass (K-cluster), and the application of this method to create semantic word clusters that are agnostic to vector space representations and thesauri."
  }, {
    "heading": "1.1 Related Work",
    "text": "Semantic word cluster and distributional thesauri have been well studied in the NLP literature (Lin, 1998; Curran, 2004). Recently there has been a line of research on incorporating synonyms and antonyms into word embeddings. Our approach is very much in the line of Vlachos et al. (2009). However, they explicitly made verb clusters using Dirichlet Process Mixture Models and must-link / cannot-link clustering. Furthermore, they note that cannot-link clustering does not improve performance whereas our signed clustering antonyms are key.\nMost recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner. One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrkšić et al., 2016).\nAlternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015).\nOur approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links. Unlike\nthe previous approaches using semi-supervised methods, we incorporated the thesauri as a knowledge base. Similar to word vector retrofitting and counter-fitting methods described in Faruqui et al. (2015) and Mrkšić et al. (2016), our signed clustering method uses existing vector representations to create word clusters.\nTo our knowledge, this work is the first theoretical foundation of multiclass signed normalized cuts.2 Zass and Shashua (2005) solved multiclass cluster from another approach, by relaxing the orthogonality assumption and focusing instead on the non-negativity constraint. This led to a doubly stochastic optimization problem. Negative edges are handled by a constrained hyperparameter. Hou (2005) used positive degrees of nodes in the degree matrix of a signed graph with weights (-1, 0, 1), which was advanced by Kolluri et al. (2004) and Kunegis et al. (2010) using absolute values of weights in the degree matrix. Interestingly, Chiang et al. (2014) presented a theoretical foundation for edge sign prediction and a recursive clustering approach. Mercado et al. (2016) found that using the geometric mean of the graph Laplacian improves performance.\nWang et al. (2016) used semi-supervised polarity induction (Rao and Ravichandran, 2009) to create clusters of words with similar valence and arousal. Must-link and cannot-link soft spectral clustering (Rangapuram and Hein, 2012) share similarities with our method, particularly in the limit where there are no must-link edges present. Both must-link and cannot-link clustering as well as polarity induction differ in optimization method. Our method is significantly faster due to the use of randomized SVD (Halko et al., 2011) and can thus be applied to large scale NLP problems.\nWe developed a novel theory and algorithm that extends the clustering of Shi and Malik (2000) and Yu and Shi (2003) to the multiclass signed graph case."
  }, {
    "heading": "2 Signed Graph Cluster Estimation",
    "text": ""
  }, {
    "heading": "2.1 Signed Normalized Cut",
    "text": "Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs.\n2The full exposition by Gallier (2016) is available on arXiv.\nSuch graphs (with weights (−1, 0,+1)) were introduced as early as 1953 by (Harary, 1953), to model social relations involving disliking, indifference, and liking. The problem of clustering the nodes of a signed graph arises naturally as a generalization of the clustering problem for weighted graphs. Figure 1 shows a signed graph of word similarities with a thesaurus overlay. Gallier\n(2016) extends normalized cuts to signed graphs in order to incorporate antonym information into word clusters.\nDefinition 2.1. A weighted graph is a pair G = (V,W ), where V = {v1, . . . , vm} is a set of nodes or vertices, and W is a symmetric matrix called the weight matrix, such that wi j ≥ 0 for all i, j ∈ {1, . . . ,m}, and wi i = 0 for i = 1, . . . ,m. We say that a set {vi, vj} is an edge iff wi j > 0. The corresponding (undirected) graph (V,E) with E = {{vi, vj} | wi j > 0}, is called the underlying graph of G.\nGiven a signed graph G = (V,W ) (where W is a symmetric matrix with zero diagonal entries), the underlying graph of G is the graph with node set V and set of (undirected) edges E = {{vi, vj} | wij 6= 0}.\nIf (V,W ) is a signed graph, where W is an m × m symmetric matrix with zero diagonal entries and with the other entries wij ∈ R arbitrary, for any node vi ∈ V , the signed degree of vi is defined as\ndi = d(vi) = m∑\nj=1\n|wij |,\nand the signed degree matrix D as\nD = diag(d(v1), . . . , d(vm)).\nFor any subset A of the set of nodes V , let\nvol(A) = ∑\nvi∈A di =\n∑\nvi∈A\nm∑\nj=1\n|wij |.\nFor any two subsets A and B of V and AC which is the complement of A, define links+(A,B), links−(A,B), and cut(A,AC) by\nlinks+(A,B) = ∑\nvi∈A,vj∈B wij>0\nwij\nlinks−(A,B) = ∑\nvi∈A,vj∈B wij<0\n−wij\ncut(A,AC) = ∑\nvi∈A,vj∈AC wij 6=0\n|wij |.\nThen, the signed Laplacian L is defined by\nL = D −W,\nand its normalized version Lsym by\nLsym = D −1/2 LD −1/2 = I −D−1/2WD−1/2.\nKunegis et al. (2010) showed that L is positive semidefinite. For a graph without isolated vertices, we have d(vi) > 0 for i = 1, . . . ,m, so D\n−1/2 is well defined.\nGiven a partition of V into K clusters (A1, . . . , AK), if we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi ∈ Aj 0 if vi /∈ Aj ,\nfor some aj 6= 0. For illustration, suppose m = 5 and A1 = {v1, v3} then (X1)> = [a1, 0, a1, 0, 0]. Definition 2.2. The signed normalized cut sNcut(A1, . . . , AK) of the partition (A1, ..., AK) is defined as\nsNcut(A1, . . . ,AK) =\nK∑\nj=1\ncut(Aj , A C j ) + 2links −(Aj , Aj)\nvol(Aj) .\nIt should be noted that this formulation differs significantly from Kunegis et al. (2010) and even more so from must-link / cannot-link clustering.\nObserve that minimizing sNcut(A1, . . . , AK) minimizes the number of positive and negative edges between clusters and also the number of negative edges within clusters. Removing the term links−(Aj , Aj) reduces sNcut to normalized cuts.\nA linear algebraic formulation is\nsNcut(A1, . . . , AK) = K∑\nj=1\n(Xj)>LXj (Xj)>DXj .\nwhere X is the N ×K matrix whose jth column is Xj ."
  }, {
    "heading": "2.2 Optimization Problem",
    "text": "We now formulate K-way clustering of a graph using normalized cuts.\nIf we let X = {\n[X1 . . . XK ] | Xj = aj(xj1, . . . , xjN ),\nxji ∈ {1, 0}, aj ∈ R, Xj 6= 0 }\nour solution set is\nK = { X ∈ X | (Xi)>DXj = 0,\n1 ≤ i, j ≤ K, i 6= j } .\nThe resulting optimization problem is\nminimize\nK∑\nj=1\n(Xj)>LXj (Xj)>DXj\nsubject to (Xi)>DXj = 0,\n1 ≤ i, j ≤ K, i 6= j, X ∈ X . The problem can be reformulated to an equiva-\nlent optimization problem:\nminimize tr(X>LX)\nsubject to X>DX = I, X ∈ X . We then form a relaxation of the above problem,\ndropping the condition that X ∈ X , giving Relaxed Problem\nminimize tr(Y >D −1/2 LD −1/2 Y ) subject to Y >Y = I.\nThe minimum of the relaxed problem is achieved by the K unit eigenvectors associated with the smallest eigenvalues of Lsym."
  }, {
    "heading": "2.3 Finding an Approximate Discrete Solution",
    "text": "Given a solution Z of the relaxed problem, we look for pairs (X,Q) with X ∈ X and where Q is aK×K matrix with nonzero and pairwise orthogonal columns, with ‖X‖F = ‖Z‖F , that minimize\nϕ(X,Q) = ‖X − ZQ‖F .\nHere, ‖A‖F is the Frobenius norm of A. This nonlinear optimization problem involves two unknown matrices X and Q. To solve the relaxed problem, we proceed by alternating between minimizing ϕ(X,Q) = ‖X − ZQ‖F with respect to X holding Q fixed (step 5 in algorithm 1), and minimizing ϕ(X,Q) with respect to Q holding X fixed (steps 6 and 7 in algorithm 1).\nThis second stage in which X is held fixed has been studied, but it is still a hard problem for which no closed-form solution is known. Hence we divide the problem into steps 6 and 7 for which the solution is known. Since Q is of the form Q = RΛ whereR ∈ O(K) and Λ is a diagonal invertible matrix, we minimize ‖X − ZRΛ‖F . The matrix RΛ is not a minimizer of ‖X − ZRΛ‖F in general, but it is an improvement on R alone, and both stages can be solved quite easily. In step 6 the problem reduces to minimizing −2tr(Q>Z>X); that is, maximizing tr(Q>Z>X).\nAlgorithm 1 Signed Clustering 1: Input: W the weight matrix (without isolated nodes),\nK the number of clusters, and termination threshold . 2: Using theD the degree matrix, and the signed Laplacian\nL, compute Lsym the signed normalized Laplacian.\n3: Initialize Λ = I , X = D − 1\n2U where U is the matrix of the eigenvectors corresponding to the K smallest eigenvalues of Lsym. 3 4: while ‖X − ZRΛ‖F > do 5: Minimize ‖X − ZRΛ‖F with respect to X holding Q fixed. 6: Fix X , Z, and Λ, find R ∈ O(K) that minimizes ‖X − ZRΛ‖F . 7: Fix X , Z, and R, find a diagonal invertible matrix Λ that minimizes ‖X − ZRΛ‖F . 8: end while 9: Find the discrete solution X∗ by choosing the\nlargest entry xij on row i set xij = 1 and all other xij = 0 for row i.\n10: Output: X∗.\nSteps 3 through 10 may be replaced by standard Kmeans clustering. It should also be noted that by\nremoving the solution requirement that Xj 6= 0, the algorithm can find k ≤ K clusters."
  }, {
    "heading": "3 Similarity Calculation",
    "text": "The main input to the spectral signed clustering algorithm is the similarity matrixW , which overlays both the distributional properties and thesaurus information. Following Belkin and Niyogi (2003), we chose the heat kernel based on the Euclidean distance between word vector representations as our similarity metric, such that\nWij =    0 if e− ‖wi−wj‖2 σ <\ne− ‖wi−wj‖2 σ otherwise .\nwhere σ and are hyperparameters found using grid search (see Supplemental material for more detail).\nWe represented the thesaurus as two matrices where\nT synij = { 1 if words i and j are synonyms 0 otherwise .\nand\nT antij = { −1 if words i and j are antonyms 0 otherwise .\nT syn is the synonym graph and T ant is the antonym graph. The signed graph can then be written in matrix form as Ŵ = γW +βantT ant W+βsynT syn W , where computes Hadamard product (element-wise multiplication).\nThe parameters γ, βsyn, and βant are tuned to the data target dataset using cross validation. The reader should note that σ and are not found using a target dataset, but instead using cross validation and grid search to minimize the number of negative edges within clusters and the number of disconnected components in the cluster."
  }, {
    "heading": "4 Evaluation Metrics",
    "text": "We evaluated the clusters using both intrinsic and extrinsic methods. For intrinsic evaluation, we used thesaurus information for two novel metrics: 1) the number of negative edges (NNE) within the clusters, which in our semantic clusters is the number of antonyms in the same cluster, and 2) the number of disconnected components (NDC) in the synonym graph, so the number of groups of words\nthat are not connected by a synonym relation in the thesaurus. The NDC thus has the disadvantage that it is a function of the thesaurus coverage. Our third intrinsic measure uses a gold standard designed to measure how well we capture word similarity: Semantically similar words should be in the same cluster and semantically dissimilar words should not. For extrinsic evaluation, as descibed below, we measure how much our clusters help to identify text polarity. We also compare multiple word embeddings and thesauri to demonstrate the stability of our method."
  }, {
    "heading": "5 Experiments with Synthetic Data",
    "text": "In order to evaluate our signed graph clustering method, we first focused on intrinsic measures of cluster quality in synthetic data. To do so, we created random signed graphs with the same proportion of positive and negative edges as in our real dataset. Figure 2 demonstrates that the number of\nnegative edges within a cluster is minimized using our clustering algorithm on simulated data. As the number of clusters becomes large, the number of disconnected components, which includes clusters of size one, consistently increases. Determining the optimal cluster size and similarity parameters requires making a trade off between NDC and NNE. For example, in figure 2 the optimal cluster size is 20. One can see that as the number of clusters increases NNE goes to zero, but the number of disconnected components becomes the number of vertices. In the extreme case all clusters contain one vertex. K-means, also shown in figure 2, does not optimize NNE."
  }, {
    "heading": "6 Experimental Setup",
    "text": ""
  }, {
    "heading": "6.1 Word Embeddings",
    "text": "We used four different word embedding methods for evaluation: Skip-gram vectors (word2vec) (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), Eigenwords (Dhillon et al., 2015), and Global Context (GloCon) (Huang et al., 2012); however, we only report the results for word2vec, which is the most popular word embedding (see the supplemental material for other embeddings). We used word2vec 300 dimensional embeddings which were trained on several billion words of English: the Gigaword and the English discussion forum data gathered as part of BOLT. Tokenization was performed using CMU’s Twokenize.4"
  }, {
    "heading": "6.2 Thesauri",
    "text": "Several thesauri were used in order to test the robustness including Roget’s Thesaurus (Roget, 1852), the Microsoft Word English (MS Word) thesaurus from Samsonovic et al. (2010) and WordNet 3.0 (Miller, 1995).\nWe chose a subset of 5108 words for the training dataset, which had high overlap between various sources. Changes to the training dataset had minimal effects on the optimal parameters. Within the training dataset, each of the thesauri had roughly 3700 antonym pairs; combined they had 6680. However, the number of distinct connected components varied, with Roget’s Thesaurus having the fewest (629), and MS Word Thesaurus (1162) and WordNet (2449) having the most. These ratios were consistent across the full dataset."
  }, {
    "heading": "6.3 Gold Standard SimLex-999 And",
    "text": "SimVerb-3500\nFollowing the analysis of Vlachos et al. (2009), we threshold the semantically similar datasets to find word pairs which should or should not belong to the same cluster. As ground truth, we extracted 120 semantically similar words from SimLex-999 with a similarity score greater than 8 out of 10. SimLex-999 is a gold standard resource for semantic similarity, not relatedness, based on ratings by human annotators.\nOur 120 pair subset of SimLex-999 has multiple parts-of-speech including Noun-Noun pairs, VerbVerb pairs and Adjective-Adjective pairs. Within\n4https://github.com/brendano/ ark-tweet-nlp\nSimVerb-3500, we used a subset of 318 semantically similar verb pairs.\nThe community is attempting to define better gold standards; however, currently these are the best datasets that we are aware of. We tried to use WordNet, Roget, and the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) as a gold standard, but manual inspection as well as empirical results showed that none of the automatically generated datasets were a sufficient gold standard. Possibly the symmetric pattern of (Schwartz et al., 2015) would have been sufficient; we did not have time to validate this."
  }, {
    "heading": "6.4 Stanford Sentiment Treebank",
    "text": "We also evaluated our clusters by using them as features for predicting sentiment, using sentiment treebank 5 (Socher et al., 2013) with coarsegrained labels on phrases and sentences from movie review excerpts. This dataset is widely used for the evaluation of sentiment analysis. We used the standard partition of the treebank into training (6920), development (872), and test (1821) sets."
  }, {
    "heading": "7 Cluster Evaluation",
    "text": "Table 1 shows the four most-associated words with “accept” using different methods.\nWe now turn to quantitative measures of word similarity and synonym cluster quality."
  }, {
    "heading": "7.1 Comparison with K-means and Normalized Cuts",
    "text": "In order to assess the model we tested (1) Kmeans, (2) normalized cuts without thesaurus, and (3) signed normalized cuts. As a baseline, we created clusters using K-means on the original word2vec vector representations where the number of K clusters was set to 750.\nTable 2 shows the relative ratios of the different clustering methods of with respect to antonym pair inclusion and the number of disconnected components within the clusters. For both methods, over twenty percent of the clusters contain antonym pairs even though the median cluster size is six. Signed clustering radically reduced the number of antonyms within clusters compared to the other methods.\n5http://nlp.stanford.edu/sentiment/ treebank.html"
  }, {
    "heading": "8 Empirical Results",
    "text": "Tables 3 and 5 present our main result. When using our signed clustering method with similar words, as labeled by SimLex-999 and SimVerb3500, our clustering accuracy increased by 5% on both SimLex-999 and SimVerb-3000. Furthermore, by combining the thesauri lookup with our clustering, we achieved almost perfect accuracy (96%). Table 5 shows the sentiment analysis task performance. Our method outperforms all methods with similar complexity; however, we did not reach state-of-the-art results when compared to much more complex models which also use a richer dataset."
  }, {
    "heading": "8.1 Evaluation Using Word Similarity Datasets",
    "text": "In a perfect setting, all word pairs rated highly similar by human annotators would be in the same cluster, and all words which were rated dissimilar would be in different clusters. Since our clustering algorithm produced sets of words, we used this evaluation instead of the more commonly reported correlations.\nIn table 3 we show the results of the evaluation with SimLex-999. Combining thesaurus lookup and word2vec+CombThes clusters, labeled as Lookup + SC(W2V), yielded an accuracy of 0.96 (5 errors). Note that clusters using word2vec with normalized cuts does not improve accuracy. The MSW thesaurus has much lower coverage, but 100 % accuracy, which is why when\ncombined with the signed clustering the performance is 0.95. In table 3 we state the proportion of clusters containing dissimilar words as a sanity check for cluster size. (See supplemental material for full cluster size optimization information.) Another important result is that the verb accuracy yielded the largest accuracy gains, consistent with the results of Schwartz et al. (2015).\nTable 4 clearly shows that the overall performance of all methods is lower for verb similarity. However, the improvement using both signed clustering as well as thesaurus look is also larger."
  }, {
    "heading": "8.2 Sentiment Analysis",
    "text": "We trained an l2-norm regularized logistic regression (Friedman et al., 2001) and simultaneously γ, βsyn, and βant using our word clusters in order to predict the coarse-grained sentiment at the sentence level. The γ and β parameters were found using a portion of the data where we iteratively switch between the logistic regression and the parameters, holding each fixed. However, hyperparameters σ and , and the number of clusters\nK were optimized minimizing error using grid search. We compared our model against existing models: Naive Bayes with bag of words (NB) (Socher et al., 2013), sentence word embedding averages (VecAvg), retrofitted sentence word embeddings (RVecAvg) (Faruqui et al., 2015) that incorporate thesaurus information, simple recurrent neural networks (RNN), and two baselines of normalized cuts and signed normalized cuts using only thesaurus information.\nWhile the state-of-the art Convolutional Neural Network (CNN) (Kim, 2014) is at 0.881, our model performs quite well with much less information and complexity. Table 5 shows that signed clustering outperforms the baselines of Naive Bayes, normalized cuts, and signed cuts using just thesaurus information. Furthermore, we outperform comparable models, including retrofitting, which has thesaurus information, and the recurrent neural network, which has access to domain specific context information.\nSigned clustering using only thesaurus information (SC(Thes)) performed significantly worse than all other methods. This was largely due to low coverage; rare words such as “WOW” and “???” are not covered. As expected, because normalized cut clusters include antonyms, the method performs worse than others. Nonetheless the improvement from 0.79 to 0.836 is quite drastic."
  }, {
    "heading": "9 Conclusion",
    "text": "We developed a novel theory for signed normalized cuts and an algorithm for finding their discrete solution. We showed that we can find su-\nperior semantically similar clusters which do not require new word embeddings but simply overlay thesaurus information on preexisting ones. The clusters are general and can be used with many out-of-the-box word embeddings. By accounting for antonym relationships, our algorithm greatly outperforms simple normalized cuts. Finally, we examined our clustering method on the sentiment analysis task from Socher et al. (2013) sentiment treebank dataset and showed that it improved performance versus comparable models.\nOur automatically generated clusters give better coverage than manually constructed thesauri. Our signed spectral clustering method allows us to incorporate the knowledge contained in these thesauri without modifying the word embeddings themselves. We further showed that use of the thesauri can be tuned to the task at hand.\nOur signed spectral clustering method could be applied to a broad range of NLP tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content. It could also be used to explore multi-view relationships, such as aligning synonym clusters across multiple languages. Another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words. Furthermore, signed spectral clustering has broader applications such as cellular biology, social networking, and electricity networks. Finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
  }],
  "year": 2017,
  "references": [{
    "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
    "authors": ["Mikhail Belkin", "Partha Niyogi."],
    "venue": "Neural computation 15(6):1373– 1396.",
    "year": 2003
  }, {
    "title": "Structural balance: a generalization of heider’s theory",
    "authors": ["Dorwin Cartwright", "Frank Harary."],
    "venue": "Psychological review 63(5):277.",
    "year": 1956
  }, {
    "title": "Multi-relational latent semantic analysis",
    "authors": ["Kai-Wei Chang", "Wen-tau Yih", "Christopher Meek."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1602–",
    "year": 2013
  }, {
    "title": "Prediction and clustering in signed networks: A local to global perspective",
    "authors": ["Kai-Yang Chiang", "Cho-Jui Hsieh", "Nagarajan Natarajan", "Inderjit S. Dhillon", "Ambuj Tewari."],
    "venue": "Journal of Machine Learning Research 15:1177–1213.",
    "year": 2014
  }, {
    "title": "From distributional to semantic similarity",
    "authors": ["James Richard Curran"],
    "year": 2004
  }, {
    "title": "Eigenwords: Spectral word embeddings",
    "authors": ["Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar."],
    "venue": "Journal of Machine Learning Research 16:3035– 3078. http://jmlr.org/papers/v16/dhillon15a.html.",
    "year": 2015
  }, {
    "title": "Retrofitting word vectors to semantic lexicons",
    "authors": ["Manaal Faruqui", "Jesse Dodge", "Kumar Sujay Jauhar", "Chris Dyer", "Eduard Hovy", "A. Noah Smith."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
    "year": 2015
  }, {
    "title": "The elements of statistical learning, volume 1",
    "authors": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."],
    "venue": "Springer series in statistics Springer, Berlin.",
    "year": 2001
  }, {
    "title": "Spectral theory of unsigned and signed graphs applications to graph clustering: a survey",
    "authors": ["Jean Gallier."],
    "venue": "arXiv preprint arXiv:1601.04692 .",
    "year": 2016
  }, {
    "title": "Ppdb: The paraphrase database",
    "authors": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
    "year": 2013
  }, {
    "title": "Simverb-3500: A largescale evaluation set of verb similarity",
    "authors": ["Daniela Gerz", "Ivan Vulić", "Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "arXiv preprint arXiv:1608.00869 .",
    "year": 2016
  }, {
    "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
    "authors": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp."],
    "venue": "SIAM review 53(2):217–288.",
    "year": 2011
  }, {
    "title": "On the notion of balance of a signed graph",
    "authors": ["Frank Harary."],
    "venue": "The Michigan Mathematical Journal 2(2):143–146.",
    "year": 1953
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig S Harris."],
    "venue": "Word .",
    "year": 1954
  }, {
    "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "arXiv preprint arXiv:1408.3456 .",
    "year": 2014
  }, {
    "title": "Bounds for the least laplacian eigenvalue of a signed graph",
    "authors": ["Yao Ping Hou."],
    "venue": "Acta Mathematica Sinica 21(4):955–960.",
    "year": 2005
  }, {
    "title": "Improving word representations via global context and multiple word prototypes",
    "authors": ["Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational",
    "year": 2012
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 1746–1751.",
    "year": 2014
  }, {
    "title": "Spectral surface reconstruction from noisy point clouds",
    "authors": ["Ravikrishna Kolluri", "Jonathan Richard Shewchuk", "James F O’Brien"],
    "venue": "In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing",
    "year": 2004
  }, {
    "title": "Spectral analysis of signed graphs for clustering, prediction and visualization",
    "authors": ["Jérôme Kunegis", "Stephan Schmidt", "Andreas Lommatzsch", "Jürgen Lerner", "Ernesto William De Luca", "Sahin Albayrak."],
    "venue": "SDM. SIAM, volume 10, pages 559–559.",
    "year": 2010
  }, {
    "title": "Automatic retrieval and clustering of similar words",
    "authors": ["Dekang Lin."],
    "venue": "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2. http://aclweb.org/anthology/P98-",
    "year": 1998
  }, {
    "title": "Clustering signed networks with the geometric mean of laplacians",
    "authors": ["Pedro Mercado", "Francesco Tudisco", "Matthias Hein."],
    "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information",
    "year": 2016
  }, {
    "title": "Efficient estimation of word",
    "authors": ["frey Dean"],
    "year": 2013
  }, {
    "title": "Principal semantic components of language and the measurement of meaning",
    "authors": ["Alexei V Samsonovic", "Giorgio A Ascoli", "Jeffrey Krichmar."],
    "venue": "PloS one 5(6):e10921.",
    "year": 2010
  }, {
    "title": "Uncovering distributional differences between synonyms and antonyms in a word space model",
    "authors": ["Silke Scheible", "Sabine Schulte im Walde", "Sylvia Springorum."],
    "venue": "Proceedings of the Sixth International Joint Conference on Nat-",
    "year": 2013
  }, {
    "title": "Symmetric pattern based word embeddings for improved word similarity prediction",
    "authors": ["Roy Schwartz", "Roi Reichart", "Ari Rappoport."],
    "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning. Associa-",
    "year": 2015
  }, {
    "title": "Normalized cuts and image segmentation",
    "authors": ["Jianbo Shi", "Jitendra Malik."],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 22(8):888–905.",
    "year": 2000
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "D. Christopher Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceed-",
    "year": 2013
  }, {
    "title": "Learning sentimentspecific word embedding for twitter sentiment classification",
    "authors": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-",
    "year": 2014
  }, {
    "title": "From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research 37(1):141–188",
    "authors": ["Peter D Turney", "Patrick Pantel"],
    "year": 2010
  }, {
    "title": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, Association for Computational Linguistics, chapter Unsupervised and Constrained Dirichlet",
    "authors": ["Andreas Vlachos", "Anna Korhonen", "Zoubin Ghahramani"],
    "year": 2009
  }, {
    "title": "Community-based weighted graph model for valence-arousal prediction of affective words",
    "authors": ["Jin Wang", "Liang-Chih Yu", "K Robert Lai", "Xuejie Zhang."],
    "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 24(11):1957–1968.",
    "year": 2016
  }, {
    "title": "Polarity inducing latent semantic analysis",
    "authors": ["Wen-tau Yih", "Geoffrey Zweig", "John Platt."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Asso-",
    "year": 2012
  }, {
    "title": "Multiclass spectral clustering",
    "authors": ["Stella X Yu", "Jianbo Shi."],
    "venue": "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on. IEEE, pages 313–319.",
    "year": 2003
  }, {
    "title": "A unifying approach to hard and probabilistic clustering",
    "authors": ["Ron Zass", "Amnon Shashua."],
    "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on. IEEE, volume 1, pages 294– 301.",
    "year": 2005
  }, {
    "title": "Word semantic representations using bayesian probabilistic tensor factorization",
    "authors": ["Jingwei Zhang", "Jeremy Salwen", "Michael Glass", "Alfio Gliozzo."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural",
    "year": 2014
  }],
  "id": "SP:d04357e044276ad5fee741d2c3e89df238b95538",
  "authors": [{
    "name": "João Sedoc",
    "affiliations": []
  }, {
    "name": "Jean Gallier",
    "affiliations": []
  }, {
    "name": "Lyle Ungar",
    "affiliations": []
  }, {
    "name": "Dean Foster",
    "affiliations": []
  }],
  "abstractText": "Vector space representations of words capture many aspects of word similarity, but such methods tend to produce vector spaces in which antonyms (as well as synonyms) are close to each other. For spectral clustering using such word embeddings, words are points in a vector space where synonyms are linked with positive weights, while antonyms are linked with negative weights. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations. By using randomized spectral decomposition (Halko et al., 2011) and sparse matrices, our method is both fast and scalable. We validate our clusters using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for sentiment prediction.",
  "title": "Semantic Word Clusters Using Signed Spectral Clustering"
}