{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 125–135 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences\nAll the calculated electronic band structures are metallic .\n0.42 0.34 0.20\n0.44\nalong with the target task (Socher et al., 2011; Yogatama et al., 2017).\nMotivated by the promising results of recent joint learning approaches, we present a novel NMT model that can learn a task-specific latent graph structure for each source-side sentence. The graph structure is similar to the dependency structure of the sentence, but it can have cycles and is learned specifically for the translation task. Unlike the aforementioned approach of learning single syntactic trees, our latent graphs are composed of “soft” connections, i.e., the edges have realvalued weights (Figure 1). Our model consists of two parts: one is a task-independent parsing component, which we call a latent graph parser, and the other is an attention-based NMT model. The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task.\nIn experiments, we demonstrate that our model can be effectively pre-trained by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset."
  }, {
    "heading": "2 Latent Graph Parser",
    "text": "We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and\n125\na unique root node (ROOT) is added. Given a sentence of length N , the parent node Hwi ∈ {w1, . . . , wN , ROOT} (Hwi 6= wi) of each word wi (1 ≤ i ≤ N) is called its head. The sentence is thus represented as a set of tuples (wi, Hwi , `wi), where `wi is a dependency label.\nIn this paper, we remove the constraint of using the tree structure and represent a sentence as a set of tuples (wi, p(Hwi |wi), p(`wi |wi)), where p(Hwi |wi) is the probability distribution of wi’s parent nodes, and p(`wi |wi) is the probability distribution of the dependency labels. For example, p(Hwi = wj |wi) is the probability that wj is the parent node of wi. Here, we assume that a special token 〈EOS〉 is appended to the end of the sentence, and we treat the 〈EOS〉 token as ROOT. This approach is similar to that of graph-based dependency parsing (McDonald et al., 2005) in that a sentence is represented with a set of weighted arcs between the words. To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al. (2017)."
  }, {
    "heading": "2.1 Word Representation",
    "text": "The i-th input word wi is represented with the concatenation of its d1-dimensional word embedding vdp(wi) ∈ Rd1 and its character n-gram embedding c(wi) ∈ Rd1 : x(wi) = [vdp(wi); c(wi)]. c(wi) is computed as the average of the embeddings of the character n-grams in wi."
  }, {
    "heading": "2.2 POS Tagging Layer",
    "text": "Our latent graph parser builds upon multilayer bi-directional Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units (Graves and Schmidhuber, 2005). In the first layer, POS tagging is handled by computing a hidden state h(1)i = [ −→ h (1) i ; ←− h (1) i ] ∈ R2d1 for wi,\nwhere −→ h\n(1) i = LSTM(\n−→ h\n(1) i−1, x(wi)) ∈ Rd1 and←−\nh (1) i = LSTM(\n←− h\n(1) i+1, x(wi)) ∈ Rd1 are hidden\nstates of the forward and backward LSTMs, respectively. h(1)i is then fed into a softmax classifier to predict a probability distribution p(1)i ∈ RC (1) for word-level tags, where C(1) is the number of POS classes. The model parameters of this layer can be learned not only by human-annotated data, but also by backpropagation from higher layers, which are described in the next section."
  }, {
    "heading": "2.3 Dependency Parsing Layer",
    "text": "Dependency parsing is performed in the second layer. A hidden state h(2)i ∈ R2d1 is computed by −→ h\n(2) i = LSTM(\n−→ h\n(2) i−1, [x(wi); y(wi);\n−→ h\n(1) i ])\nand ←− h\n(2) i = LSTM(\n←− h\n(2) i+1, [x(wi); y(wi);\n←− h\n(1) i ]),\nwhere y(wi) = W (1) ` p (1) i ∈ Rd2 is the POS information output from the first layer, and W (1)` ∈ Rd2×C(1) is a weight matrix.\nThen, (soft) edges of our latent graph representation are obtained by computing the probabilities\np(Hwi = wj |wi) = exp (m(i, j))∑\nk 6=i exp (m(i, k)) , (1)\nwhere m(i, k) = h(2)Tk Wdph (2) i (1 ≤ k ≤ N + 1, k 6= i) is a scoring function with a weight matrix Wdp ∈ R2d1×2d1 . While the models of Hashimoto et al. (2017), Zhang et al. (2017), and Dozat and Manning (2017) learn the model parameters of their parsing models only by humanannotated data, we allow the model parameters to be learned by the translation task.\nNext, [h(2)i ; z(Hwi)] is fed into a softmax classifier to predict the probability distribution p(`wi |wi), where z(Hwi) ∈ R2d1 is the weighted average of the hidden states of the parent nodes: ∑ j 6=i p(Hwi = wj |wi)h(2)j . This results in the latent graph representation (wi, p(Hwi |wi), p(`wi |wi)) of the input sentence."
  }, {
    "heading": "3 NMT with Latent Graph Parser",
    "text": "The latent graph representation described in Section 2 can be used for any sentence-level tasks, and here we apply it to an Attention-based NMT (ANMT) model (Luong et al., 2015). We modify the encoder and the decoder in the ANMT model to learn the latent graph representation."
  }, {
    "heading": "3.1 Encoder with Dependency Composition",
    "text": "The ANMT model first encodes the information about the input sentence and then generates a sentence in another language. The encoder represents the word wi with a word embedding venc(wi) ∈ Rd3 . It should be noted that venc(wi) is different from vdp(wi) because each component is separately modeled. The encoder then takes the word embedding venc(wi) and the hidden state h (2) i as the input to a uni-directional LSMT:\nh (enc) i = LSTM(h (enc) i−1 , [venc(wi); h (2) i ]), (2)\nwhere h(enc)i ∈ Rd3 is the hidden state corresponding to wi. That is, the encoder of our model is a three-layer LSTM network, where the first two layers are bi-directional.\nIn the sequential LSTMs, relationships between words in distant positions are not explicitly considered. In our model, we explicitly incorporate such relationships into the encoder by defining a dependency composition function:\ndep(wi) = tanh(Wdep[henci ; h(Hwi); p(`wi |wi)]), (3)\nwhere h(Hwi) = ∑\nj 6=i p(Hwi = wj |wi)h(enc)j is the weighted average of the hidden states of the parent nodes.\nNote on character n-gram embeddings In NMT models, sub-word units are widely used to address rare or unknown word problems (Sennrich et al., 2016). In our model, the character n-gram embeddings are fed through the latent graph parsing component. To the best of our knowledge, the character n-gram embeddings have never been used in NMT models. Wieting et al. (2016), Bojanowski et al. (2017), and Hashimoto et al. (2017) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words."
  }, {
    "heading": "3.2 Decoder with Attention Mechanism",
    "text": "The decoder of our model is a single-layer LSTM network, and the initial state is set with h(enc)N+1 and its corresponding memory cell. Given the t-th hidden state h(dec)t ∈ Rd3 , the decoder predicts the t-th word in the target language using an attention mechanism. The attention mechanism in Luong et al. (2015) computes the weighted average of the hidden states h(enc)i of the encoder:\ns(i, t) = exp (h (dec) t ·h(enc)i )∑N+1\nj=1 exp (h (dec) t ·h(enc)j )\n, (4)\nat = ∑N+1 i=1 s(i, t)h (enc) i , (5)\nwhere s(i, t) is a scoring function which specifies how much each source-side hidden state contributes to the word prediction.\nIn addition, like the attention mechanism over constituency tree nodes (Eriguchi et al., 2016b), our model uses attention to the dependency composition vectors:\ns′(i, t) = exp (h (dec) t ·dep(wi))∑N\nj=1 exp (h (dec) t ·dep(wj))\n, (6)\na′t = ∑N i=1 s ′(i, t)dep(wi), (7)\nTo predict the target word, a hidden state h̃(dec)t ∈ Rd3 is then computed as follows:\nh̃ (dec) t = tanh(W̃ [h (dec) t ; at; a ′ t]), (8)\nwhere W̃ ∈ Rd3×3d3 is a weight matrix. h̃(dec)t is fed into a softmax classifier to predict a target word distribution. h̃(dec)t is also used in the transition of the decoder LSTMs along with a word embedding vdec(wt) ∈ Rd3 of the target word wt:\nh (dec) t+1 = LSTM(h (dec) t , [vdec(wt); h̃ (dec) t ]), (9)\nwhere the use of h̃(dec)t is called input feeding proposed by Luong et al. (2015).\nThe overall model parameters, including those of the latent graph parser, are jointly learned by minimizing the negative log-likelihood of the prediction probabilities of the target words in the training data. To speed up the training, we use BlackOut sampling (Ji et al., 2016). By this joint learning using Equation (3) and (7), the latent graph representations are automatically learned according to the target task.\nImplementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences. This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model. We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016; Press and Wolf, 2017). Also, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy.\nTranslation At test time, we use a novel beam search algorithm which combines statistics of sentence lengths (Eriguchi et al., 2016b) and length normalization (Cho et al., 2014). During the beam search step, we use the following scoring function for a generated word sequence y = (y1, y2, . . . , yLy) given a source word sequence x = (x1, x2, . . . , xLx):\n1 Ly  Ly∑ i=1 log p(yi|x, y1:i−1) + log p(Ly|Lx)  , (10)\nwhere p(Ly|Lx) is the probability that sentences of length Ly are generated given source-side sentences of length Lx. The statistics are taken by using the training data in advance. In our experiments, we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short."
  }, {
    "heading": "4 Experimental Settings",
    "text": ""
  }, {
    "heading": "4.1 Data",
    "text": "We used an English-to-Japanese translation task of the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b) used in the Workshop on Asian Translation (WAT), since it has been shown that syntactic information is useful in English-to-Japanese translation (Eriguchi et al., 2016b; Neubig et al., 2015). We followed the data preprocessing instruction for the English-toJapanese task in Eriguchi et al. (2016b). The English sentences were tokenized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea tool1. Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs where the maximum sentence length is 50. In what follows, we call this dataset the large training dataset. We further selected the first 20,000 and 100,000 pairs to construct the small and medium training datasets, respectively. The development data include 1,790 pairs, and the test data 1,812 pairs.\nFor the small and medium datasets, we built the vocabulary with words whose minimum frequency is two, and for the large dataset, we used words whose minimum frequency is three for English and five for Japanese. As a result, the vocabulary of the target language was 8,593 for the small dataset, 23,532 for the medium dataset, and 65,680 for the large dataset. A special token 〈UNK〉 was used to replace words which were not included in the vocabularies. The character ngrams (n = 2, 3, 4) were also constructed from each training dataset with the same frequency settings."
  }, {
    "heading": "4.2 Parameter Optimization and Translation",
    "text": "We turned hyper-parameters of the model using development data. We set (d1, d2) = (100, 50) for the latent graph parser. The word and character n-gram embeddings of the latent graph parser\n1http://www.phontron.com/kytea/.\nwere initialized with the pre-trained embeddings in Hashimoto et al. (2017).2 The weight matrices in the latent graph parser were initialized with uniform random values in [− √ 6√\nrow+col , +\n√ 6√\nrow+col ],\nwhere row and col are the number of rows and columns of the matrices, respectively. All the bias vectors and the weight matrices in the softmax layers were initialized with zeros, and the bias vectors of the forget gates in the LSTMs were initialized by ones (Jozefowicz et al., 2015).\nWe set d3 = 128 for the small training dataset, d3 = 256 for the medium training dataset, and d3 = 512 for the large training dataset. The word embeddings and the weight matrices of the NMT model were initialized with uniform random values in [−0.1, +0.1]. The training was performed by mini-batch stochastic gradient descent with momentum. For the BlackOut objective (Ji et al., 2016), the number of the negative samples was set to 2,000 for the small and medium training datasets, and 2,500 for the large training dataset. The mini-batch size was set to 128, and the momentum rate was set to 0.75 for the small and medium training datasets and 0.70 for the large training dataset. A gradient clipping technique was used with a clipping value of 1.0. The initial learning rate was set to 1.0, and the learning rate was halved when translation accuracy decreased. We used the BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training. We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique. For regularization, we used L2-norm regularization with a coefficient of 10−6 and applied dropout (Hinton et al., 2012) to Equation (8) with a dropout rate of 0.2.\nThe beam size for the beam search algorithm was 12 for the small and medium training datasets, and 50 for the large training dataset. We used BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and perplexity scores as our evaluation metrics. Note that lower perplexity scores indicate better accuracy."
  }, {
    "heading": "4.3 Pre-Training of Latent Graph Parser",
    "text": "The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing. In this paper we used\n2The pre-trained embeddings can be found at https: //github.com/hassyGo/charNgram2vec.\nthe widely-used Wall Street Journal (WSJ) training data to jointly train the POS tagging and dependency parsing components. We used the standard training split (Section 0-18) for POS tagging. We followed Chen and Manning (2014) to generate the training data (Section 2-21) for dependency parsing. From each training dataset, we selected the first K sentences to pre-train our model. The training dataset for POS tagging includes 38,219 sentences, and that for dependency parsing includes 39,832 sentences.\nThe parser including the POS tagger was first trained for 10 epochs in advance according to the multi-task learning procedure of Hashimoto et al. (2017), and then the overall NMT model was trained. When pre-training the POS tagging and dependency parsing components, we did not apply dropout to the model and did not fine-tune the word and character n-gram embeddings to avoid strong overfitting."
  }, {
    "heading": "4.4 Model Configurations",
    "text": "LGP-NMT is our proposed model that learns the Latent Graph Parsing for NMT.\nLGP-NMT+ is constructed by pre-training the latent parser in LGP-NMT as described in Section 4.3.\nSEQ is constructed by removing the dependency composition in Equation (3), forming a sequential NMT model with the multi-layer encoder.\nDEP is constructed by using pre-trained dependency relations rather than learning them. That is, p(Hwi = wj |wi) is fixed to 1.0 such that wj is the head of wi. The dependency labels are also given by the parser which was trained by using all the training samples for parsing and tagging.\nUNI is constructed by fixing p(Hwi = wj |wi) to 1 N for all the words in the same sentence. That is, the uniform probability distributions are used for equally connecting all the words."
  }, {
    "heading": "5 Results on Small and Medium Datasets",
    "text": "We first show our translation results using the small and medium training datasets. We report averaged scores with standard deviations across five different runs of the model training."
  }, {
    "heading": "5.1 Small Training Dataset",
    "text": "Table 1 shows the results of using the small training dataset. LGP-NMT performs worse than SEQ\nand UNI, which shows that the small training dataset is not enough to learn useful latent graph structures from scratch. However, LGP-NMT+ (K = 10,000) outperforms SEQ and UNI, and the standard deviations are the smallest. Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model. We can also see that DEP performs the worst. This is not surprising because previous studies, e.g., Li et al. (2015), have reported that using syntactic structures do not always outperform competitive sequential models in several NLP tasks.\nNow that we have observed the effectiveness of pre-training our model, one question arises naturally:\nhow many training samples for parsing and tagging are necessary for improving the translation accuracy?\nTable 2 shows the results of using different numbers of training samples for parsing and tagging. The results of K= 0 and K= 10,000 correspond to those of LGP-NMT and LGP-NMT+ in Table 1, respectively. We can see that using the small amount of the training samples performs better than using all the training samples.3 One possible reason is that the domains of the translation dataset and the parsing (tagging) dataset are considerably different. The parsing and tagging datasets come from WSJ, whereas the translation dataset comes from abstract text of scientific papers in a wide range of domains, such as\n3We did not observe such significant difference when using the larger datasets, and we used all the training samples in the remaining part of this paper.\nbiomedicine and computer science. These results suggest that our model can be improved by a small amount of parsing and tagging datasets in different domains. Considering the recent universal dependency project4 which covers more than 50 languages, our model has the potential of being applied to a variety of language pairs."
  }, {
    "heading": "5.2 Medium Training Dataset",
    "text": "Table 3 shows the results of using the medium training dataset. In contrast with using the small training dataset, LGP-NMT is slightly better than SEQ. LGP-NMT significantly outperforms UNI, which shows that our adaptive learning is more effective than using the uniform graph weights. By pre-training our model, LGP-NMT+ significantly outperforms SEQ in terms of the BLEU score. Again, DEP performs the worst among all the models.\nBy using our beam search strategy, the Brevity Penalty (BP) values of our translation results are equal to or close to 1.0, which is important when evaluating the translation results using the BLEU scores. A BP value ranges from 0.0 to 1.0, and larger values mean that the translated sentences have relevant lengths compared with the reference translations. As a result, our BLEU evaluation results are affected only by the word n-gram precision scores. BLEU scores are sensitive to the BP values, and thus our beam search strategy leads to more solid evaluation for NMT models."
  }, {
    "heading": "6 Results on Large Dataset",
    "text": "Table 4 shows the BLEU and RIBES scores on the development data achieved with the large training dataset. Here we focus on our models and SEQ because UNI and DEP consistently perform worse than the other models as shown in Table 1 and 3. The averaging technique and attentionbased unknown word replacement (Jean et al., 2015; Hashimoto et al., 2016) improve the scores.\n4http://universaldependencies.org/.\nAgain, we see that the translation scores of our model can be further improved by pre-training the model.\nTable 5 shows our results on the test data, and the previous best results summarized in Nakazawa et al. (2016a) and the WAT website5 are also shown. Our proposed models, LGP-NMT and LGP-NMT+, outperform not only SEQ but also all of the previous best results. Notice also that our implementation of the sequential model (SEQ) provides a very strong baseline, the performance of which is already comparable to the previous state of the art, even without using ensemble techniques. The confidence interval (p ≤ 0.05) of the RIBES score of LGP-NMT+ estimated by bootstrap resampling (Noreen, 1989) is (82.27, 83.37), and thus the RIBES score of LGP-NMT+ is significantly better than that of SEQ, which shows that our latent parser can be effectively pre-trained with the human-annotated treebank.\nThe sequential NMT model in Cromieres et al. (2016) and the tree-to-sequence NMT model in Eriguchi et al. (2016b) rely on ensemble techniques while our results mentioned above are obtained using single models. Moreover, our model is more compact6 than the previous best NMT model in Cromieres et al. (2016). By applying the ensemble technique to LGP-NMT, LGP-NMT+,\n5http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/list.php?t=1&o=1.\n6Our training time is within five days on a c4.8xlarge machine of Amazon Web Service by our CPU-based C++ code, while it is reported that the training time is more than two weeks in Cromieres et al. (2016) by their GPU code.\nand SEQ, the BLEU and RIBES scores are further improved, and both of the scores are significantly better than the previous best scores."
  }, {
    "heading": "6.1 Analysis on Translation Examples",
    "text": "Figure 2 shows two translation examples7 to see how the proposed model works and what is missing in the state-of-the-art sequential NMT model, SEQ. Besides the reference translation, the outputs of our models with and without pre-training, SEQ, and Google Translation8 are shown.\nSelectional Preference In the translation example (1) in Figure 2, we see that the adverb “obliquely” is interpreted differently across the systems. As in the reference translation, “obliquely” is a modifier of the verb “crosses”. Our models correctly capture the relationship between the two words, whereas Google Translation and SEQ treat “obliquely” as a modifier of the verb “existed”. This error is not a surprise since the verb “existed” is located closer to “obliquely” than the verb “crosses”. A possible reason for the correct interpretation by our models is that they can better capture long-distance dependencies and are less susceptible to surface word distances. This is an indication of our models’ ability of capturing domain-specific selectional preference that cannot be captured by purely sequential\n7These English sentences were created by manual simplification of sentences in the development data.\n8The translations were obtained at https: //translate.google.com in Feb. and Mar. 2017.\nmodels. It should be noted that simply using standard treebank-based parsers does not necessarily address this error, because our pre-trained dependency parser interprets that “obliquely” is a modifier of the verb “existed”.\nAdverb or Adjective The translation example (2) in Figure 2 shows another example where the adverb “negatively” is interpreted as an adverb or an adjective. As in the reference translation, “negatively” is a modifier of the verb “controls”. Only LGP-NMT+ correctly captures the adverb-verb relationship, whereas “negatively” is interpreted as the adjective “negative” to modify the noun “ImRNA” in the translation results from Google Translation and LGP-NMT. SEQ interprets “negatively” as both an adverb and an adjective, which leads to the repeated translations. This error suggests that the state-of-the-art NMT models are strongly affected by the word order. By contrast, the pre-training strategy effectively embeds the information about the POS tags and the dependency relations into our model."
  }, {
    "heading": "6.2 Analysis on Learned Latent Graphs",
    "text": "Without Pre-Training We inspected the latent graphs learned by LGP-NMT. Figure 1 shows an example of the learned latent graph obtained for a sentence taken from the development data of the translation task. It has long-range dependencies and cycles as well as ordinary left-to-right dependencies. We have observed that the punctuation mark “.” is often pointed to by other words with large weights. This is primarily because the hidden state corresponding to the mark in each sentence has rich information about the sentence.\nTo measure the correlation between the latent graphs and human-defined dependencies, we parsed the sentences on the development data of the WSJ corpus and converted the graphs into dependency trees by Eisner’s algorithm (Eisner, 1996). For evaluation, we followed Chen and Manning (2014) and measured Unlabeled Attachment Score (UAS). The UAS is 24.52%, which shows that the implicitly-learned latent graphs are partially consistent with the human-defined syntactic structures. Similar trends have been reported by Yogatama et al. (2017) in the case of binary constituency parsing. We checked the most dominant gold dependency labels which were assigned for the dependencies detected by LGPNMT. The labels whose ratio is more than 3% are\nAll the calculated electronic band structures are metallic . 0.86 0.97 0.99\n1.0\n0.85 1.0\nAll the calculated electronic band structures are metallic .\n0.26 0.60 0.99\n0.86 1.0\nROOT 1.0\n0.23\n0.95 0.82\n(a)\n(b)\nnn, amod, prep, pobj, dobj, nsubj, num, det, advmod, and poss. We see that dependencies between words in distant positions, such as subject-verb-object relations, can be captured.\nWith Pre-Training We also inspected the pretrained latent graphs. Figure 3-(a) shows the dependency structure output by the pre-trained latent parser for the same sentence in Figure 1. This is an ordinary dependency tree, and the head selection is almost deterministic; that is, for each word, the largest weight of the head selection is close to 1.0. By contrast, the weight values are more evenly distributed in the case of LGP-NMT as shown in Figure 1. After the overall NMT model training, the latent parser is adapted to the translation task, and Figure 3-(b) shows the adapted latent graph. Again, we can see that the adapted weight values are also distributed and different from the original pre-trained weight values, which suggests that human-defined syntax is not always optimal for the target task.\nThe UAS of the pre-trained dependency trees is 92.52%9, and that of the adapted latent graphs is 18.94%. Surprisingly, the resulting UAS (18.94%) is lower than the UAS of our model without pretraining (24.52%). However, in terms of the translation accuracy, our model with pre-training is better than that without pre-training. These results suggest that human-annotated treebanks can provide useful prior knowledge to guide the overall model training by pre-training, but the resulting sentence structures adapted to the target task do not need to highly correlate with the treebanks.\n9The UAS is significantly lower than the reported score in Hashimoto et al. (2017). The reason is described in Section 4.3."
  }, {
    "heading": "7 Related Work",
    "text": "While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation. They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems. By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional preference and PP attachments, in a task-oriented fashion.\nOur model learns latent graph structures in a source-side language. Eriguchi et al. (2017) have proposed a model which learns to parse and translate by using automatically-parsed data. Thus, it is also an interesting direction to learn latent structures in a target-side language.\nAs for the learning of latent syntactic structure, there are several studies on learning task-oriented syntactic structures. Yogatama et al. (2017) used a reinforcement learning method on shift-reduce action sequences to learn task-oriented binary constituency trees. They have shown that the learned trees do not necessarily highly correlate with the human-annotated treebanks, which is consistent with our experimental results. Socher et al. (2011) used a recursive autoencoder model to greedily construct a binary constituency tree for each sentence. The autoencoder objective works as a regularization term for sentiment classification tasks. Prior to these deep learning approaches,\nWu (1997) presented a method for bilingual parsing. One of the characteristics of our model is directly using the soft connections of the graph edges with the real-valued weights, whereas all of the above-mentioned methods use one best structure for each sentence. Our model is based on dependency structures, and it is a promising future direction to jointly learn dependency and constituency structures in a task-oriented fashion.\nFinally, more related to our model, Kim et al. (2017) applied their structured attention networks to a Natural Language Inference (NLI) task for learning dependency-like structures. They showed that pre-training their model by a parsing dataset did not improve accuracy on the NLI task. By contrast, our experiments show that such a parsing dataset can be effectively used to improve translation accuracy by varying the size of the dataset and by avoiding strong overfitting. Moreover, our translation examples show the concrete benefit of learning task-oriented latent graph structures."
  }, {
    "heading": "8 Conclusion and Future Work",
    "text": "We have presented an end-to-end NMT model by jointly learning translation and source-side latent graph representations. By pre-training our model using treebank annotations, our model significantly outperforms both a pipelined syntax-based model and a state-of-the-art sequential model. On English-to-Japanese translation, our model outperforms the previous best models by a large margin. In future work, we investigate the effectiveness of our approach in different types of target tasks."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers and Akiko Eriguchi for their helpful comments and suggestions. We also thank Yuchen Qiao and Kenjiro Taura for their help in speeding up our training code. This work was supported by CREST, JST, and JSPS KAKENHI Grant Number 17J09620."
  }],
  "year": 2017,
  "references": [{
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Enriching Word Vectors with Subword Information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics, 5:135–146.",
    "year": 2017
  }, {
    "title": "A Fast and Accurate Dependency Parser using Neural Networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740–750.",
    "year": 2014
  }, {
    "title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder",
    "authors": ["Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2017
  }, {
    "title": "On the Properties of Neural Machine Translation: Encoder– Decoder Approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Struc-",
    "year": 2014
  }, {
    "title": "Kyoto University Participation to WAT 2016",
    "authors": ["Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi."],
    "venue": "Proceedings of the 3rd Workshop on Asian Translation, pages 166–174.",
    "year": 2016
  }, {
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Efficient Normal-Form Parsing for Combinatory Categorial Grammar",
    "authors": ["Jason Eisner."],
    "venue": "Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 79–86.",
    "year": 1996
  }, {
    "title": "Character-based Decoding in Tree-to-Sequence Attention-based Neural Machine Translation",
    "authors": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."],
    "venue": "Proceedings of the 3rd Workshop on Asian Translation, pages 175–183.",
    "year": 2016
  }, {
    "title": "Tree-to-Sequence Attentional Neural Machine Translation",
    "authors": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2016
  }, {
    "title": "Learning to Parse and Translate Improves Neural Machine Translation",
    "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
    "year": 2017
  }, {
    "title": "Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures",
    "authors": ["Alex Graves", "Jurgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "Domain Adaptation and AttentionBased Unknown Word Replacement in Chinese-toJapanese Neural Machine Translation",
    "authors": ["Kazuma Hashimoto", "Akiko Eriguchi", "Yoshimasa Tsuruoka."],
    "venue": "Proceedings of the 3rd Workshop on Asian Translation,",
    "year": 2016
  }, {
    "title": "Simple Customization of Recursive Neural Networks for Semantic Relation Classification",
    "authors": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Nat-",
    "year": 2013
  }, {
    "title": "A Joint ManyTask Model: Growing a Neural Network for Multiple NLP Tasks",
    "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
    "year": 2017
  }, {
    "title": "Improving neural networks by preventing co-adaptation of feature detectors",
    "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "CoRR, abs/1207.0580.",
    "year": 2012
  }, {
    "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
    "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."],
    "venue": "arXiv, cs.CL 1611.01462.",
    "year": 2016
  }, {
    "title": "Automatic Evaluation of Translation Quality for Distant Language Pairs",
    "authors": ["Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-",
    "year": 2010
  }, {
    "title": "Montreal Neural Machine Translation Systems for WMTf15",
    "authors": ["Sébastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 134–140.",
    "year": 2015
  }, {
    "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies",
    "authors": ["Shihao Ji", "S.V.N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey."],
    "venue": "Proceedings of the 4th International Conference on",
    "year": 2016
  }, {
    "title": "An Empirical Exploration of Recurrent Network Architectures",
    "authors": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 2342–2350.",
    "year": 2015
  }, {
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "NAVER Machine Translation System for WAT 2015",
    "authors": ["Hyoung-Gyu Lee", "JaeSong Lee", "Jun-Seok Kim", "Chang-Ki Lee."],
    "venue": "Proceedings of the 2nd Workshop on Asian Translation, pages 69–73.",
    "year": 2015
  }, {
    "title": "When Are Tree Structures Necessary for Deep Learning of Representations",
    "authors": ["Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy"],
    "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2015
  }, {
    "title": "Modeling Source Syntax for Neural Machine Translation",
    "authors": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
    "year": 2017
  }, {
    "title": "Effective Approaches to Attentionbased Neural Machine Translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "Online Large-Margin Training of Dependency Parsers",
    "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 91–98.",
    "year": 2005
  }, {
    "title": "Feature Forest Models for Probabilistic HPSG Parsing",
    "authors": ["Yusuke Miyao", "Jun’ichi Tsujii"],
    "venue": "Computational Linguistics,",
    "year": 2008
  }, {
    "title": "Overview of the 3rd Workshop on Asian Translation",
    "authors": ["Toshiaki Nakazawa", "Hideya Mino", "Chenchen Ding", "Isao Goto", "Graham Neubig", "Sadao Kurohashi", "Eiichiro Sumita."],
    "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016).",
    "year": 2016
  }, {
    "title": "ASPEC: Asian Scientific Paper Excerpt Corpus",
    "authors": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."],
    "venue": "Proceedings of the 10th Conference on International Lan-",
    "year": 2016
  }, {
    "title": "On the Elements of an Accurate Tree-to-String Machine Translation System",
    "authors": ["Graham Neubig", "Kevin Duh."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 143–149.",
    "year": 2014
  }, {
    "title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015",
    "authors": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."],
    "venue": "Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 35–41.",
    "year": 2015
  }, {
    "title": "Computer-Intensive Methods for Testing Hypotheses: An Introduction",
    "authors": ["Eric W. Noreen."],
    "venue": "WileyInterscience.",
    "year": 1989
  }, {
    "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings",
    "year": 2002
  }, {
    "title": "Using the Output Embedding to Improve Language Models",
    "authors": ["Ofir Press", "Lior Wolf."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 157–163.",
    "year": 2017
  }, {
    "title": "Linguistic Input Features Improve Neural Machine Translation",
    "authors": ["Rico Sennrich", "Barry Haddow."],
    "venue": "Proceedings of the First Conference on Machine Translation, pages 83–91.",
    "year": 2016
  }, {
    "title": "Neural Machine Translation of Rare Words with Subword Units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
    "year": 2016
  }, {
    "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Nat-",
    "year": 2011
  }, {
    "title": "Syntactically Guided Neural Machine Translation",
    "authors": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 299–305.",
    "year": 2016
  }, {
    "title": "Sequence to Sequence Learning with Neural Networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in Neural Information Processing Systems 27, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Charagram: Embedding Words and Sentences via Character n-grams",
    "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515.",
    "year": 2016
  }, {
    "title": "Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora",
    "authors": ["Dekai Wu."],
    "venue": "Computational Linguistics, 23(3):377–404.",
    "year": 1997
  }, {
    "title": "A Syntaxbased Statistical Translation Model",
    "authors": ["Kenji Yamada", "Kevin Knight."],
    "venue": "Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530.",
    "year": 2001
  }, {
    "title": "Towards Bidirectional Hierarchical Representations for AttentionBased Neural Machine Translation",
    "authors": ["Baosong Yang", "Derek F. Wong", "Tong Xiao", "Lidia S. Chao", "Jingbo Zhu."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in",
    "year": 2017
  }, {
    "title": "Learning to Compose Words into Sentences with Reinforcement Learning",
    "authors": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Dependency Parsing as Head Selection",
    "authors": ["Xingxing Zhang", "Jianpeng Cheng", "Mirella Lapata."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 665–676.",
    "year": 2017
  }, {
    "title": "Evaluating Neural Machine Translation in English-Japanese Task",
    "authors": ["Zhongyuan Zhu."],
    "venue": "Proceedings of the 2nd Workshop on Asian Translation, pages 61–68.",
    "year": 2015
  }, {
    "title": "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies",
    "authors": ["Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
    "year": 2016
  }],
  "id": "SP:eec15bc21ddc668e75115972e6716dff0f1944df",
  "authors": [{
    "name": "Kazuma Hashimoto",
    "affiliations": []
  }, {
    "name": "Yoshimasa Tsuruoka",
    "affiliations": []
  }],
  "abstractText": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective. In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our model can be further improved by pretraining it with a small amount of treebank annotations. Our final ensemble model significantly outperforms the previous best models on the standard Englishto-Japanese translation dataset.",
  "title": "Neural Machine Translation with Source-Side Latent Graph Parsing"
}