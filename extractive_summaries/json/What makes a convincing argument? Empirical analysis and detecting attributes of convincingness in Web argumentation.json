{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1214–1223, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "People engage in argumentation in various contexts, both online and in the real life. Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose which is to convince or to persuade (O’Keefe,\n2011; van Eemeren et al., 2014; Blair, 2011). Assessing the quality and strength of perceived arguments therefore plays an inherent role in argumentative discourse. Despite strong theoretical foundations and plethora of normative theories, such as Walton’s schemes and their critical questions (Walton, 1989), an ideal model of critical discussion in the pragma-dialectic view (Van Eemeren and Grootendorst, 1987), or research into fallacies (Boudry et al., 2015), assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners (Weltzer-Ward et al., 2009; Swanson et al., 2015; Rosenfeld and Kraus, 2015).\nAddressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation. Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation. However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing.\nWe asked the following research questions. First, can we assess what makes an argument convincing in a purely empirical fashion as opposite to theoretical normative approaches? Second, to what extent can the problem be tackled by computational models? To address these questions, we exploit our recently introduced UKPConvArg1 corpus (Habernal and Gurevych, 2016). This data set consists of 11,650 argument pairs – two arguments with the\n1214\nPrompt: Should physical education be mandatory in schools? Stance: Yes!\nArgument 1 Argument 2 PE should be compulsory because it keeps us constantly fit and healthy. If you really dislike sports, then you can quit it when you’re an adult. But when you’re a kid, the best thing for you to do is study, play and exercise. If you prefer to be lazy and lie on the couch all day then you are most likely to get sick and unfit. Besides, PE helps kids be better at teamwork. physical education should be mandatory cuhz 112,000 people have died in the year 2011 so far and it’s because of the lack of physical activity and people are becoming obese!!!!\nA1 is more convincing than A2, because: • “A1 is more intelligently written and makes\nsame standpoint to the given topic, annotated with a binary relation describing which argument from the pair is more convincing. Each pair also contains several reasons written in natural language explaining which properties of the arguments influence their convincingness. An example of such an argument pair is shown in Figure 1.\nWe use these natural language reasons as a proxy to assess qualitative properties of the arguments in each argument pair. Our main contributions are: (1) We propose empirically inspired labels of quality properties of Web arguments and design a hierarchical annotation scheme. (2) We create a new large crowd-sourced benchmark data set containing 9,111 argument pairs multi-labeled with 17 categories which is improved by local and global filtering techniques. (3) We experiment with several computational models, both traditional and neu-\nral network-based, and evaluate their performance quantitatively and qualitatively.\nThe newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1"
  }, {
    "heading": "2 Related Work",
    "text": "The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work.\nThere are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse. Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability. Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions.\nRecently, research in persuasive essay scoring has started combining holistic approaches based on rubrics for several dimensions typical to this genre with explicit argument detection. Persing and Ng (2015) manually labeled 1,000 student persuasive essays with a single score on the 1–4 scale and trained a regression predictor with a rich feature set using LIBSVM. Among traditional features (such as POS or semantic frames), an argument structure parser by Stab and Gurevych (2014) was employed. Farra et al. (2015) also deal with essay scoring but rather then tackling the argument structure, they focus on methods for detecting opinion expressions. Persuasive essays however represent a genre with a rather strict qualitative and formal requirements (as taught in curricula) and substantially differ from online argumentation.\nArgument evaluation belongs to the central research topics among argumentation scholars (Toul-\n1https://github.com/UKPLab/ emnlp2016-empirical-convincingness\nmin, 2003; Walton et al., 2008; Van Eemeren and Grootendorst, 1987). Yet treatment of assessing argumentation quality, persuasiveness, or convincingness is traditionally based on evaluating relevance, sufficiency or acceptability of premises (Govier, 2010; Johnson and Blair, 2006) or categorizing fallacies (Hamblin, 1970; Tindale, 2007). However, the nature of these normative approaches causes a gap between the ‘ideal’ models and empirically encountered real-world arguments, such as those on the Web (van Eemeren et al., 2014; Walton, 2012).\nRegarding the methodology utilized later in this paper, deep (recursive) neural networks have gained extreme popularity in NLP in recent years. Long Short-Term Memory networks (LSTM) with Attention mechanism have been applied on textual entailment (Rocktäschel et al., 2016), QuestionAnswering (Golub and He, 2016), or source-code summarization (Allamanis et al., 2016)."
  }, {
    "heading": "3 Data",
    "text": "As our source data set, we took the publicly available UKPConvArg1 corpus.2 It is based on arguments originated from 16 debates from Web debate platforms createdebate.com and convinceme.net, each debate has two sides (usually pro and con). Arguments from each of the 32 debate sides are connected into a set of argument pairs, and each argument pair is annotated with a binary relation (argument A is more/less convincing than argument B), resulting in total into 11,650 argument pairs. Annotations performed by Habernal and Gurevych (2016) also contain several reasons written by crowd-workers that explain why a particular argument is more or less convincing; see an example in Figure 1.\nAs these reasons were written in an uncontrolled setting, they naturally reflect the main properties of argument quality in a downstream task, which is to decide which argument from a pair is more convincing. It differs from scoring arguments in isolation, which is inherently harder not only due to subjectivity in argument “strength” decision but also because of possible annotator’s prior bias (Habernal and Gurevych, 2016). Assessing an argument\n2https://github.com/UKPLab/ acl2016-convincing-arguments\nin context helps to emphasize its main flaws or strengths. This approach is also known as knowledge elicitation – acquiring appropriate information from experts by asking ”why?” (Reed and Rowe, 2004).\nWe therefore used the reasons as a proxy for developing a scheme for labeling argument quality attributes. This was done in a purely bottom-up empirical manner, as opposed to using ‘standard’ evaluation criteria known from argumentation literature (Johnson and Blair, 2006; Schiappa and Nordin, 2013). In particular, we split all reasons into several reason units by simple preprocessing (splitting using Stanford CoreNLP (Manning et al., 2014), segmentation into Elementary Discourse Units by RST tools (Surdeanu et al., 2015)) and identified the referenced arguments (A1 or A2) by pattern matching and dependency parsing. For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus."
  }, {
    "heading": "3.1 Annotation scheme",
    "text": "In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units). Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers. The workers were split into two groups; we then estimated gold labels for each group using MACE (Hovy et al., 2013) and compared both groups’ results in order to find systematic discrepancies. Finally, we ended up with a set of 19 distinct labels (classes). As the number of classes is too big for non-expert crowd workers, we developed a hierarchical annotation process guided by questions that narrow down the final class decision. The scheme is depicted in Figure 2.4 Workers were shown only the reason units without seeing the original arguments.\n3We picked this example for its simplicity, in reality the texts are much more fuzzy.\n4It might seem that some labels are missing, such as C8-2 and C8-3; these belong to those removed during the pilot studies."
  }, {
    "heading": "3.2 Annotation",
    "text": "We sampled 26,000 unique reason units ordered by the original author competence provided as part of the UKPConvArg corpus. We expected that workers with higher competence tend to write better reasons for their explanations. Using the previously introduced scheme, 776 AMT workers annotated the batch during two weeks; we required assignments from 5 workers for a single item. We employed MACE (Hovy et al., 2013) for gold label and worker competence estimation with 95% threshold to ignore the less confident labels. Several workers were rejected based on their low computed competence and other criteria, such as too short submission times."
  }, {
    "heading": "3.3 Data cleaning",
    "text": "We performed several cleaning procedures to increase quality and consistency of the annotated data (apart from initial MACE filtering already explained above).\nLocal cleaning First, we removed 3,859 reason units annotated either with C1-2 (”not an explanation”) and C8-6 (”too topic-specific”, which usually paraphrases some details from the related argument and is not general enough). In the next step, we removed reason units with wrong polarity. In particular, all reason units labeled with C8-* or C9-* should refer to the more convincing argument in the argument pair (as they describe positive properties), whereas all reasons with labels C5-*, C6-*, and C7-* should refer to the less convincing argument. The target arguments for reason units were known from the heuristic preprocessing (see above); in this step 2,455 units were removed.\nGlobal cleaning Since the argument pairs from one debate can be projected into an argument graph (Habernal and Gurevych, 2016), we utilized this ‘global’ context for further consistency cleaning.\nSuppose we have two argument pairs, P1(A → B) and P2(B → C) (where→ means “is more convincing than”). Let P1(RB) be reason unit targeting\nB in argument pair P1 and similarly P2(RB) reason unit targeting B in argument pair P2. In other words, two reason units target the same argument in two different argument pairs (in one of them the argument is more convincing while in the other pair it is less convincing). There might then exist contradicting combination of classes for P1(RB) and P2(RB). For example classes C9-2 and C7-3 are contradicting, as the same argument cannot be both ”on the topic” and ”off-topic” at the same time.\nWhen such a conflict between two reason units occurred, we selected the reason with a higher score using the following formula:\nwW ∗ σ   ∑\nA=G\nwA − λ ∑\nA 6=G wA\n  (1)\nwhere wW is the competence of the original author of the reason unit (originated from the UKPConvArg corpus), A = G are crowdsourced assignments for a single reason unit that match the final predicted gold label, A 6= G are assignments that differ from the final predicted gold label, wA is the competence of worker for assignment A, λ is a penalty for non-gold labels, and σ is the sigmoid function to squeeze the score between 0 and 1.\nWe found 25 types of global contradictions between labels for reason units and used them for cleaning the data; in total 3,790 reason units were removed in this step. After all cleaning procedures, annotations from reason units were mapped back to argument pairs, resulting into a multi-label annotation of one or both arguments from the given pair. In total 9,111 pairs from the UKPConvArg corpus were annotated.\nFor example, the final annotations of argument pair shown in Figure 1 contain four labels – C8-1 (as the more convincing argument “has more details, information, facts, or examples / more reasons / better reasoning / goes deeper / is more specific”), C9-3 (as the more convincing argument “has provoking question / makes you think”), C5-2 (as the less convincing argument “has language issues / bad grammar /...”), and C6-1 (as the less convincing argument “provides not enough support / ...” ). Only four of six reason units for this argument pair were annotated because of the competence score of their authors.\nTable 1 shows number of labels per argument pairs; about a half of the argument pairs have only one label. Figure 3 shows distribution of label in the entire data set which is heavily skewed towards C8-1 label. This is not surprising, as this label was used for reason units pointing out that the more convincing argument provided more reasons, details, information or better reasoning – a feature inherent to argumentation seen as giving reasons (Freeley and Steinberg, 2008)."
  }, {
    "heading": "3.4 Data validation",
    "text": "Since the qualitative attributes of arguments were annotated indirectly by labeling their corresponding reason units without seeing the original arguments, we wanted to validate correctness of this approach. We designed a validation study, in which workers were shown the original argument pair and two sets of labels. The first set contained the true labels as annotated previously, while we randomly replaced few labels in the second set. The goal was then to decide which set of labels better explains that argument A is\nmore convincing than argument B. For example, for the argument pair from Figure 1, one set of shown labels would be {C8-1, C9-3, C5-2, C6-1} (the correct set) while the other ‘distracting’ set would be {C8-1, C9-3, C5-1, C7-3} .\nWe randomly sampled 500 argument pairs and collected 9 assignments per pair on AMT; we again used MACE with 95% threshold. Accuracy of workers on 235 argument pairs achieved 82%. We can thus conclude that workers tend to prefer explanations based on labels from the reason units and using the annotation process presented in this section is reliable. Total costs of the annotations including pilot studies, bonuses, and data validation were USD 3,300."
  }, {
    "heading": "4 Experiments",
    "text": "We propose two experiments, both performed in 16- fold cross-domain validation. In each fold, argument pairs from 15 debates are used and the remaining one is used for testing. In both experiments, it is assumed that the more convincing argument in a pair is known and we concatenate (using a particular delimiter) both arguments such that the more convincing argument comes first."
  }, {
    "heading": "4.1 Predicting full multi-label distribution",
    "text": "This experiment is a multi-label classification. Given an argument pair annotated with several labels, the goal is to predict all these labels.\nWe use two deep learning models. Our first model, Bidirectional Long Short-Term Memory (BLSTM) network contains two LSTM blocks (forward and backward), each with 64 hidden units on the output. The output is concatenated into a single vector and pushed through sigmoid layer with 17 output units (corresponding to 17 labels). We use cross entropy loss function in order to minimize distance of label distributions in training and test data (Nam et al., 2014). In the input layer, we rely on pre-trained word embeddings from Glove (Pennington et al., 2014) whose weights are updated during training the network.\nThe second models is BLSTM extended with an attention mechanism (Rocktäschel et al., 2016; Golub and He, 2016) combined with convolution layers over the input. In particular, the input em-\nbedding layer is convoluted using 4 different convolution sizes (2, 3, 5, 7), each with 1,000 randomly initialized weight vectors. Then we perform maxover-time pooling and concatenate the output into a single vector. This vector is used as the attention module in BLSTM.\nWe evaluate the system using two widely used metrics in multi-label classification. First, Hamming loss is the average per-item per-class total error; the smaller the better (Zhang and Zhou, 2007). Second, we report One-error (Sokolova and Lapalme, 2009) which corresponds to the error of the predicted label with highest probability; the smaller the better. We do not report other metrics (such as Area Under PRC-curves, MAP, or cover) as they require tuning a threshold parameter, see a survey by Zhang and Zhou (2014).\nResults from Table 2 do not show significant differences between the two models. Putting the oneerror numbers into human performance context can be done only indirectly, as the data validation pre-\nsented in Section 3.4 had a different set-up. Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder.\nError analysis and discussion We examined outputs from the label distribution prediction for BLSTM/ATT/CNN. It turns out that the output layer leans toward predicting the dominant label C8-1, while prediction of other labels is seldom. We suspect two causes, first, the highly skewed distribution of labels (see Figure 3) and, second, insufficient training data sizes where 13 classes have less than 1k training examples (while Goodfellow et al. (2016) recommend at least 5k instances per class).\nAlthough multi-label classification may be viewed as a set of binary classification tasks that decides for each label independently (and thus allows for employing other ‘standard’ classifiers such as SVM), this so-called binary relevance approach ignores dependencies between the labels. That is why we focused directly on deep-learning methods, as they are capable of learning and predicting a full label distribution (Nam et al., 2014)."
  }, {
    "heading": "4.2 Predicting flaws in less convincing arguments",
    "text": "In the second experiment, we focus on predicting flaws in arguments using coarse-grained labels. While this task makes several simplifications in the labeling, it still provides meaningful insights into argument quality assessment. For this purpose, we use only argument pairs where the less convincing argument is labeled with a single label (no multi-label classification). Second, we merged all labels from categories C5-* C6-* C7-* into three classes corresponding to their parent nodes in the annotation decision schema from Figure 2. Table 3 shows distribution of the gold data for this task with explanation of the labels. It is worth noting that predicting flaws in the less convincing argument is still contextdependent and requires the entire argument pair because some of the quality labels are relative to the more convincing argument (such as “less reasoning” or “not enough support”).\nFor this experiment, we modified the output layer\nof the neural models from the previous experiment. The non-linear output function is softmax and we train the networks using categorical cross-entropy loss. We also add another baseline model that employs SVM with RBF kernel5 and a rich set of linguistically motivated features, similarly to (Habernal and Gurevych, 2016). The feature set includes uni- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spell-checking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc.6 It results into a sparse 60k-dimensional feature vector space.\nResults in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail). Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35. The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not significantly better.\n5We used LISBVM (Chang and Lin, 2011) with the default hyper-parameters. As Fernández-Delgado et al. (2014) show, SVM with gaussian kernels is a reasonable best choice on average.\n6Detailed explanation of the features can be found directly in the attached source codes.\nError analysis We manually examined several dozens of predictions where the BLSTM model failed but the BLSTM/ATT/CNN model was correct in order to reveal some phenomena that the system is capable to cope with. First, the BLSTM/ATT/CNN model started catching some purely abusive, sarcastic, and attacking arguments. Also, the language/grammar issues were revealed in many cases, as well as using slang in arguments.\nExamining predictions in which both systems failed reveal some fundamental limitations of the current purely data-driven computational approach. While the problem of not catching off-topic arguments can be probably modeled by incorporating the debate description or some sort of debate topic model into the attention vector, the more common issue of non-sense arguments or fallacious arguments (which seem like actual arguments on the first view) needs much deeper understanding of realworld knowledge, logic, and reasoning."
  }, {
    "heading": "5 Conclusion",
    "text": "This paper presented a novel task in the field of computational argumentation, namely empirical assessment of reasons for argument convincingness. We created a new large benchmark data set by utilizing a new annotation scheme and several filtering strategies for crowdsourced data. Then we tackled two challenging tasks, namely multi-label classification of argument pairs in order to reveal qualitative properties of the arguments, and predicting flaws in the less convincing argument from the given argument pair. We performed all evaluations in a cross-domain scenario and experimented with feature-rich SVM and two state-of-the-art neural network models. The results are promising but show that the task is inherently complex as it requires deep reasoning about the presented arguments that goes beyond capabilities of the current computational models. By releasing the\nUKPConvArg2 data and code to the community, we believe more progress can be made in this direction in the near future."
  }, {
    "heading": "Acknowledgments",
    "text": "This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No I/82806, by the German Institute for Educational Research (DIPF), by the German Research Foundation (DFG) via the GermanIsraeli Project Cooperation (DIP, grant DA 1600/1- 1), by the GRK 1994/1 AIPHES (DFG), by the ArguAna Project GU 798/20-1 (DFG), and by Amazon Web Services in Education Grant award. Lastly, we would like to thank the anonymous reviewers for their valuable feedback."
  }],
  "year": 2016,
  "references": [{
    "title": "A convolutional attention network for extreme summarization of source code",
    "authors": ["Miltiadis Allamanis", "Hao Peng", "Charles Sutton."],
    "venue": "Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learn-",
    "year": 2016
  }, {
    "title": "Argumentation as rational persuasion",
    "authors": ["J. Anthony Blair."],
    "venue": "Argumentation, 26(1):71–81.",
    "year": 2011
  }, {
    "title": "The Fake, the Flimsy, and the Fallacious: Demarcating Arguments in Real Life",
    "authors": ["Maarten Boudry", "Fabio Paglieri", "Massimo Pigliucci."],
    "venue": "Argumentation, 29(4):431–456.",
    "year": 2015
  }, {
    "title": "LIBSVM: A Library for Support Vector Machines",
    "authors": ["Chih-Chung Chang", "Chih-Jen Lin."],
    "venue": "ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.",
    "year": 2011
  }, {
    "title": "A computer readability formula designed for machine scoring",
    "authors": ["Meri Coleman", "T.L. Liau."],
    "venue": "Journal of Applied Psychology, 60:283–284.",
    "year": 1975
  }, {
    "title": "Gender-preferential text mining of e-mail discourse",
    "authors": ["Malcolm Corney", "Olivier de Vel", "Alison Anderson", "George Mohay."],
    "venue": "Proceedings of the 18th An-",
    "year": 2002
  }, {
    "title": "Scoring persuasive essays using opinions and their targets",
    "authors": ["Noura Farra", "Swapna Somasundaran", "Jill Burstein."],
    "venue": "Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 64–74, Denver, Colorado, June. As-",
    "year": 2015
  }, {
    "title": "Do we Need Hundreds of Classifiers to Solve Real World Classification Problems",
    "authors": ["Manuel Fernández-Delgado", "Eva Cernadas", "Senén Barro", "Dinani Amorim"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "A new readability yardstick",
    "authors": ["Rudolf Flesch."],
    "venue": "Journal of Applied Psychology, 32:221–233.",
    "year": 1948
  }, {
    "title": "Argumentation and Debate",
    "authors": ["Austin J. Freeley", "David L. Steinberg."],
    "venue": "Cengage Learning, Stamford, CT, USA, 12th edition.",
    "year": 2008
  }, {
    "title": "CharacterLevel Question Answering with Attention",
    "authors": ["David Golub", "Xiaodong He."],
    "venue": "arXiv preprint. http://arxiv.org/abs/1604.00727.",
    "year": 2016
  }, {
    "title": "Deep learning",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."],
    "venue": "Book in preparation for MIT Press.",
    "year": 2016
  }, {
    "title": "A Practical Study of Argument",
    "authors": ["Trudy Govier."],
    "venue": "Wadsworth, Cengage Learning, 7th edition.",
    "year": 2010
  }, {
    "title": "Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse",
    "authors": ["Ivan Habernal", "Iryna Gurevych."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2127–2137, Lisbon, Por-",
    "year": 2015
  }, {
    "title": "Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM",
    "authors": ["Ivan Habernal", "Iryna Gurevych."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2016
  }, {
    "title": "Argumentation Mining in User-Generated Web Discourse",
    "authors": ["Ivan Habernal", "Iryna Gurevych."],
    "venue": "Computational Linguistics, 43(1). In press. Preprint: http://arxiv.org/abs/1601.02403.",
    "year": 2017
  }, {
    "title": "Fallacies",
    "authors": ["Charles L. Hamblin."],
    "venue": "Methuen, London, UK.",
    "year": 1970
  }, {
    "title": "Variation in the contextuality of language: An empirical measure",
    "authors": ["Francis Heylighen", "Jean-Marc Dewaele."],
    "venue": "Foundations of Science, 7(3):293–340.",
    "year": 2002
  }, {
    "title": "Learning Whom to Trust with MACE",
    "authors": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."],
    "venue": "Proceedings of NAACL-HLT 2013, pages 1120–1130, Atlanta, Georgia. Association for Computational Linguistics.",
    "year": 2013
  }, {
    "title": "Logical Self-Defense",
    "authors": ["Ralph H. Johnson", "Anthony J. Blair."],
    "venue": "International Debate Education Association.",
    "year": 2006
  }, {
    "title": "Combining argument mining techniques",
    "authors": ["John Lawrence", "Chris Reed."],
    "venue": "Proceedings of the 2nd Workshop on Argumentation Mining, pages 127–136, Denver, CO, June. Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55–60.",
    "year": 2014
  }, {
    "title": "LargeScale Multi-label Text Classification – Revisiting Neural Networks",
    "authors": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Mencı́a", "Iryna Gurevych", "Johannes Fürnkranz"],
    "year": 2014
  }, {
    "title": "Conviction, persuasion, and argumentation: Untangling the ends and means of influence",
    "authors": ["Daniel J. O’Keefe"],
    "year": 2011
  }, {
    "title": "Identifying appropriate support for propositions in online user comments",
    "authors": ["Joonsuk Park", "Claire Cardie."],
    "venue": "Proceedings of the First Workshop on Argumentation Mining, pages 29–38, Baltimore, Maryland, June. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Joint prediction in mst-style discourse parsing for argumentation mining",
    "authors": ["Andreas Peldszus", "Manfred Stede."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 938–948, Lisbon, Portugal, September. As-",
    "year": 2015
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, Octo-",
    "year": 2014
  }, {
    "title": "Modeling argument strength in student essays",
    "authors": ["Isaac Persing", "Vincent Ng."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Pa-",
    "year": 2015
  }, {
    "title": "Araucaria: software for argument analysis, diagramming and representation",
    "authors": ["Chris Reed", "Glenn Rowe."],
    "venue": "International Journal on Artificial Intelligence Tools, 13(04):961–979, dec. 1222",
    "year": 2004
  }, {
    "title": "Reasoning about entailment with neural attention",
    "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom."],
    "venue": "Proceedings of the 2016 International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "Providing arguments in discussions based on the prediction of human argumentative behavior",
    "authors": ["Ariel Rosenfeld", "Sarit Kraus."],
    "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 1320–1327.",
    "year": 2015
  }, {
    "title": "Argumentation: Keeping Faith with Reason",
    "authors": ["Edward Schiappa", "John P. Nordin."],
    "venue": "Pearson UK, 1st edition.",
    "year": 2013
  }, {
    "title": "Automated readability index",
    "authors": ["J.R. Senter", "E.A. Smith."],
    "venue": "Technical report AMRL-TR-66-220, Aerospace Medical Research Laboratories, Ohio.",
    "year": 1967
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 Conference on Empirical Meth-",
    "year": 2013
  }, {
    "title": "A systematic analysis of performance measures for classification tasks",
    "authors": ["Marina Sokolova", "Guy Lapalme."],
    "venue": "Information Processing & Management, 45(4):427–437.",
    "year": 2009
  }, {
    "title": "Identifying argumentative discourse structures in persuasive essays",
    "authors": ["Christian Stab", "Iryna Gurevych."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, Doha, Qatar, October. Association for",
    "year": 2014
  }, {
    "title": "Two practical rhetorical structure theory parsers",
    "authors": ["Mihai Surdeanu", "Tom Hicks", "Marco Antonio Valenzuela-Escarcega."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstra-",
    "year": 2015
  }, {
    "title": "Argument Mining: Extracting Arguments from Online Dialogue",
    "authors": ["Reid Swanson", "Brian Ecker", "Marilyn Walker."],
    "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217–226, Prague, Czech Republic. Asso-",
    "year": 2015
  }, {
    "title": "Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-faith Online Discussions",
    "authors": ["Chenhao Tan", "Vlad Niculae", "Cristian DanescuNiculescu-Mizil", "Lillian Lee."],
    "venue": "Proceedings of the 25th International Conference",
    "year": 2016
  }, {
    "title": "Fallacies and Argument Appraisal",
    "authors": ["Christopher W. Tindale."],
    "venue": "Cambridge University Press, New York, NY, USA, critical reasoning and argumentation edition.",
    "year": 2007
  }, {
    "title": "The Uses of Argument, Updated Edition",
    "authors": ["Stephen E. Toulmin."],
    "venue": "Cambridge University Press, New York.",
    "year": 2003
  }, {
    "title": "Fallacies in pragma-dialectical perspective",
    "authors": ["Frans H. Van Eemeren", "Rob Grootendorst."],
    "venue": "Argumentation, 1(3):283–301.",
    "year": 1987
  }, {
    "title": "Handbook of Argumentation Theory",
    "authors": ["Frans H. van Eemeren", "Bart Garssen", "Erik C.W. Krabbe", "A. Francisca Snoeck Henkemans", "Bart Verheij", "Jean H.M. Wagemans."],
    "venue": "Springer, Berlin/Heidelberg.",
    "year": 2014
  }, {
    "title": "Argumentation Schemes",
    "authors": ["Douglas Walton", "Christopher Reed", "Fabrizio Macagno."],
    "venue": "Cambridge University Press.",
    "year": 2008
  }, {
    "title": "Informal Logic: A Handbook for Critical Argument",
    "authors": ["Douglas N. Walton."],
    "venue": "Cambridge University Press.",
    "year": 1989
  }, {
    "title": "Using argumentation schemes for argument extraction: A bottom-up method",
    "authors": ["Douglas Walton."],
    "venue": "International Journal of Cognitive Informatics and Natural Intelligence, 6(3):33–61.",
    "year": 2012
  }, {
    "title": "Assessing quality of critical thought in online discussion",
    "authors": ["Lisa Weltzer-Ward", "Beate Baltes", "Laura Knight Lynn."],
    "venue": "Campus-Wide Information Systems, 26(3):168–177.",
    "year": 2009
  }, {
    "title": "ML-KNN: A lazy learning approach to multi-label learning",
    "authors": ["Min Ling Zhang", "Zhi Hua Zhou."],
    "venue": "Pattern Recognition, 40(7):2038–2048.",
    "year": 2007
  }, {
    "title": "A Review on Multi-Label Learning Algorithms",
    "authors": ["Min-Ling Zhang", "Zhi-Hua Zhou."],
    "venue": "IEEE Transactions on Knowledge and Data Engineering, 26(8):1819– 1837.",
    "year": 2014
  }],
  "id": "SP:4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a",
  "authors": [{
    "name": "Ivan Habernal",
    "affiliations": []
  }, {
    "name": "Iryna Gurevych",
    "affiliations": []
  }],
  "abstractText": "This article tackles a new challenging task in computational argumentation. Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one. We approach this task in a fully empirical manner by annotating 26k explanations written in natural language. These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws. We create a new crowd-sourced corpus containing 9,111 argument pairs, multilabeled with 17 classes, which was cleaned and curated by employing several strict quality measures. We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments. Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task. We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community.",
  "title": "What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation"
}