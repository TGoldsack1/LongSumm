{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1379–1390, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Translation coverage is a major concern in statistical machine translation (SMT) which relies on large amounts of parallel, sentence-aligned text. In (Callison-Burch et al., 2006), even with a training data size of 10 million word tokens, source vocabulary coverage in unseen data does not go above 90%. The problem is worse with multi-word OOV phrases. Copying OOVs to the output is the most common solution. However, even noisy translations of OOVs can improve reordering and language model scores (Zhang et al., 2012). Transliteration is useful but not a panacea for the OOV problem (Irvine and Callison-Burch, 2014b). We find and remove the named entities, dates, etc. in\nthe source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores.\nIn this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daumé and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each).\nTo our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked\n1379\nwith phrases from parallel training data. 2) Graph propagation that uses PPDB paraphrases. 3) An SMT model that incorporates new translation candidates. Sec. 3 explains these three stages in detail.\nUsing PPDB has several advantages: 1) Resources such as PPDB can be built and used for many different tasks including but not limited to SMT. 2) PPDB contains many features that are useful to rank the strength of a paraphrase connection and with more information than distributional profiles. 3) Paraphrases in PPDB are often better than paraphrases extracted from monolingual or comparable corpora because a large-scale multilingual paraphrase database such as PPDB can pivot through a large amount of data in many different languages. It is not limited to using the source language data for finding paraphrases which distinguishes it from previous uses of paraphrases for SMT.\nPPDB is a natural resource for paraphrases. However, PPDB was not built with the specific application to SMT in mind. Other applications such as text-to-text generation have used PPDB (Ganitkevitch et al., 2011) but SMT brings along a specific set of concerns when using paraphrases: translation candidates should be transferred suitably across paraphrases. There are many cases, e.g. when faced with different word senses where transfer of a translation is not appropriate. Our proposed methods of using PPDB use graph propagation to transfer translation candidates in a way that is sensitive to SMT concerns.\nIn our experiments (Sec. 5) we compare our approach with the state-of-the-art in three different settings in SMT: 1) when faced with limited amount of parallel training data; 2) a domain shift between training and test data; and 3) handling a morphologically complex source language. In each case, we show that our PPDB-based approach outperforms the distributional profile approach."
  }, {
    "heading": "2 Paraphrase Extraction",
    "text": "Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014)."
  }, {
    "heading": "2.1 Paraphrases from Distributional Profiles",
    "text": "A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f , its distributional profile is:\nDP (f) = {〈A(f, wi)〉 | wi ∈ V }\nV is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI). For each potential context word wi:\nA(f, wi) = log2 P (f, wi) P (f)P (wi)\n(1)\nTo evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is:\nS(f1, f2) = cos(DP (f1), DP (f2)) =∑ wi∈V A(f1, wi)A(f2, wi)√∑\nwi∈V A(f1, wi) 2 √∑ wi∈V A(f2, wi) 2 (2)\nwhere V is the vocabulary. Note that in Eqn. (2) wi’s are the words that appear in the context of f1 or f2, otherwise the PMI values would be zero.\nConsidering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previous works (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) to reduce the search space. For each phrase we keep candidate paraphrases which appear in one of the surrounding context (e.g. Left Right) among all occurrences of the phrase."
  }, {
    "heading": "2.2 Paraphrases from bilingual pivoting",
    "text": "Bilingual pivoting uses parallel corpora between the source language, F , and a pivot language T . If two phrases, f1 and f2, in a same language are paraphrases, then they share a translation in other languages with p(f1|f2) as a paraphrase score:\nS(f1, f2) = p(f1|f2) = ∑\nt\np(f1|t)p(t|f2) (3)\nwhere t is a phrase in language T . p(f1|t) and p(t|f2) are taken from the phrase table extracted from parallel data for languages F and T . In Fig. 1 from (Bannard and Callison-Burch, 2005) we see that paraphrase pairs like (in check, under control) can be extracted by pivoting over the German phrase unter kontrolle.\nThe multilingual Paraphrase Database (PPDB) (Ganitkevitch and Callison-Burch, 2014) is a published resource for paraphrases extracted using bilingual pivoting. It leverages syntactic information and other resources to filters and scores each paraphrase pair using a large set of features. These features can be used by a log linear model to score paraphrases (Zhao et al., 2008). We used a linear combination of these features using the equation in Sec. 3 of (Ganitkevitch and Callison-Burch, 2014) to score paraphrase pairs. PPDB version 1 is broken into different levels of coverage. The smaller sizes contain only better-scoring, high-precision paraphrases, while larger sizes aim for high coverage.\nAlgorithm 1 PPDB Graph Propagation for SMT PhrTable = PhraseTableGeneration(); ParaDB = ParaphraseExtraction(); (Sec. 2) InitGraph = GraphConstruct(PhrTable, ParaDB); (Sec. 3.1) PropGraph = GraphPropagation(InitGraph); (Sec. 3.2) for phrase ∈ {OOVs} do\nnewTrans = TranslationFinder(PropGraph, phrase); Augment(PhrTable, newTrans); (Sec. 3.3)\nTuneMT(PhrTable);"
  }, {
    "heading": "3 Methodology",
    "text": "After paraphrase extraction we have paraphrase pairs, (f1, f2) and a score S(f1, f2) we can induce new translation rules for OOV phrases using the steps in Algo. (1): 1) A graph of source phrases is constructed as in (Razmara et al., 2013); 2) translations are propagated as labels through the graph as explained in Fig. 2; and 3) new translation rules obtained from graph-propagation are integrated with the original phrase table."
  }, {
    "heading": "3.1 Graph Construction",
    "text": "We construct a graph G(V,E,W ) over all source phrases in the paraphrase database and the source language phrases from the SMT phrase table extracted from the available parallel data. V corresponds to the set of vertices (source phrases), E is the set of edges between phrases and W is weight of each using the score function S defined in Sec. 2. V has two types of nodes: seed (labeled) nodes, Vs, from the SMT phrase table, and regular nodes, Vr. Note that in this step OOVs are part of these regular nodes, and we try to find translation in the propagation step for all of these regular nodes. In graph construction and propagation,\nwe do not know which phrasal nodes correspond to OOVs in the dev and test set. Fig. 2 shows a small slice of the actual graph used in one of our experiments; This graph is constructed using the paraphrase database on the right side of the figure. Filled nodes have a distribution over translations (the possible “labels” for that node). In our setting, we consider the translation e to be the “label” and so we propagate the labeling distribution p(e|f) which is taken from the feature function for the SMT log-linear model that is taken from the SMT phrase table and we propagate this distribution to unlabeled nodes in the graph."
  }, {
    "heading": "3.2 Graph Propagation",
    "text": "Considering the translation candidates of known phrases in the SMT phrase table as the “labels” we apply a soft label propagation algorithm in order to assign translation candidates to “unlabeled” nodes in the graph, which include our OOV phrases. As described by the example in Fig. 2 we wish two outcomes: 1) transfer of translations (or “labels”) to unlabeled nodes (OOV phrases) from labeled nodes, and 2) smoothing the label distribution at each node. We use the Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009) for graph propagation. Suppose we have m different possible labels plus one dummy label, a soft label Ŷ ∈ ∆m+1 is a m + 1 dimension probability vector. The dummy label is used when there is low confidence on correct labels. Based on MAD, we want to find soft label vectors for each node by optimizing the objective function below:\nmin Ŷ µ1 ∑ v∈Vs P1,v||Yv − Ŷ ||22 +\nµ2 ∑\nv∈V,u∈N(v) P2,vWv,u||Ŷv − Ŷu||22 +\nµ3 ∑ v∈V P3,v||Ŷv −Rv||22\n(4)\nIn this objective function, µi and Pi,v are hyperparameters (∀v : ΣiPi,v = 1). Rv ∈ ∆m+1 is our prior belief about labeling. First component of the function tries to minimize the difference of new distribution to the original distribution for the seed nodes. The second component insures that nearby neighbours have similar distributions, and the final component is to make sure that the distribution does not stray from a prior distribution. At the end of propagation, we wish to find a label distribution for our OOV phrases. We describe\nin Sec. 4.2.2 the reasons for choosing MAD over other graph propagation algorithms. The MAD graph propagation generalizes the approach used in (Razmara et al., 2013). The Structured Label Propagation algorithm (SLP) was used in (Saluja et al., 2014; Zhao et al., 2015) which uses a graph structure on the target side phrases as well. However, we have found that in our diverse experimental settings (see Sec. 5) MAD had two properties we needed compared to SLP: one was the use of graph random walks which allowed us to control translation candidates and MAD also has the ability to penalize nodes with a large number of edges (also see Sec. 4.2.2)."
  }, {
    "heading": "3.3 Phrase Table Integration",
    "text": "After propagation, for each potential OOV phrase we have a list of possible translations with corresponding probabilities. A potential OOV is any phrase which does not appear in training, but could appear in unseen data. We do not look at the dev or test data to produce the augmented phrase table. The original phrase table is now augmented with new entries providing translation candidates for potential OOVs; Last column in Table 2 shows how many entries have been added to the phrase table for each experimental settings. A new feature is added to the standard SMT log-linear discriminative model and introduced into the phrase table. This new feature is set to either 1.0 for the phrase table entries that already existed; or `i which is the log probability (from graph propagation) for the translation candidate i for potential OOVs. In case the dummy label exists with high probability or the label distribution is uniform, an identity rule is added to the phrase table (copy over source to target)."
  }, {
    "heading": "4 Analysis of the Framework",
    "text": ""
  }, {
    "heading": "4.1 Propagation of poor translations",
    "text": "Automatic paraphrase extraction generates many possible paraphrase candidates and many of them are likely to be false positives for finding translation candidates for OOVs. Distributional profiles rely on context information which is not sufficient to derive accurate paraphrases for many phrases and this results in many low quality paraphrase candidates. Bilingual pivoting uses word alignments which can also introduce errors depending on the size and quality of the bilingual data used. Alignment errors also introduce poor translations.\nIn graph propagation, these errors may be propagated and result in poor translations for OOVs.\nWe could address this issue by aggressively pruning the potential paraphrase candidates to improve the precision. However, this results in a dramatic drop in coverage and many OOV phrases do not obtain any translation candidates. We use a combination of the following three steps to augment our graph propagation framework."
  }, {
    "heading": "4.1.1 Graph pruning and PPDB sizes",
    "text": "Pruning the graph avoids error propagation by removing unreliable edges. Pruning removes edges with an edge weight lower than a minimum threshold or by limiting the number of neighbours to the top-K edges (Talukdar, 2009). PPDB has different sizes with different levels of accuracy and coverage. We can do graph pruning simply by choosing to use different sizes of PPDB. As we can see in Fig. 3 results vary from language to language depending on the pruning used. For instance, the L size results in the best score for French-English. We choose the best size of PPDB for each language based on a separate held-out set and independently from each of the SMT-based tasks in our experimental results. Our conclusion from our experiments with the different sizes of PPDB is that removing phrases (or nodes in our graph) is not desirable. However, removing unreliable edges is useful. As seen in Table 1, increasing the size of PPDB leads to a rapid increase in nodes followed by a larger number of edges in the very large PPDB sizes."
  }, {
    "heading": "4.1.2 Pruning the translation candidates",
    "text": "Another solution to the error propagation issue is to propagate all translation candidates but when providing translations to OOVs in the final phrase\ntable to eliminate all but the top L translations for each phrase (which is the usual ttable limit in phrase-based SMT (Koehn et al., 2003)). Based on a development set, separate from the test sets we used, we found that the best value of L was 10."
  }, {
    "heading": "4.1.3 External Resources for Filtering",
    "text": "Applying more informative filters can be also used to improve paraphrase quality. This can be done through additional features for paraphrase pairs. For example, edit distance can be used to capture misspelled paraphrases. We use a Named Entity Recognizer to exclude names, numbers and dates from the paraphrase candidates. Even after removing these tokens, 3.32% of tokens of test set are still OOVs . In addition, we use a list of stop words to remove nodes which have too many connections. These two filters improve our results (more in Sec. 5)."
  }, {
    "heading": "4.2 Path sensitivity",
    "text": "Graph propagation has been used in many NLP tasks like POS tagging, parsing, etc. but propagating translations in a graph as labels is much more challenging. Due to huge number of possible labels (translations) and many low quality edges, it is very likely that many wrong translations are rapidly propagated in few steps. Razmara et al. (2013) show that unlabeled nodes inside the graph, called bridge nodes, are useful for the transfer of translations when there is no other connection between an OOV phrase and a node with known translation candidates. However, they show that using the full graph with long paths of bridge nodes hurts performance. Thus the propagation has to be constrained using path sensitivity. Fig. 4 shows this issue in a part of an English para-\nphrase graph. After three iterations, German translation “Lager” reaches “majority” which is totally irrelevant as a translation candidate. Transfer of translation candidates should prefer close neighbours and only with a very low probability to other nodes in the graph."
  }, {
    "heading": "4.2.1 Pre-structuring the graph",
    "text": "Razmara et al. (2013) avoid a fully connected graph structure. They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to Razmara et al. (2013). In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach."
  }, {
    "heading": "4.2.2 Graph random walks",
    "text": "Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights. Optimization for the Modified Adsorption (MAD) objective function in Sec. 3.2 can be viewed as a controlled random walk (Talukdar et al., 2008; Talukdar and Crammer, 2009). This is formalized as three actions: inject, continue and abandon with corresponding pre-defined probabilities Pinj , Pcont and Pabnd respectively as in (Talukdar and Crammer, 2009). A random walk through the graph will transfer labels from one node to another node, and probabilities Pcont and Pabnd control exploration of the graph. By reducing the values of Pcont and increasing Pabnd we can control\nthe label propagation process to optimize the quality of translations for OOV phrases. Again, this is done on a held-out development set and not on the test data. The optimal values in our experiments for these probabilities are Pinj = 0.9, Pcont = 0.001, Pabnd = 0.01."
  }, {
    "heading": "4.2.3 Early stopping of propagation",
    "text": "In Modified Adsorption (MAD) (see Sec. 3.2) nodes in the graph that are closely linked will tend to similar label distributions as the number of iterations increase (even when the path lengths increase). In our setting, smoothing the label distribution helps in the first few iterations, but is harmful as the number of iterations increase due to the factors shown in Fig. 4. We use early stopping which limits the number of iterations. We varied the number of iterations from 1 to 10 on a held-out dev set and found that 5 iterations was optimal."
  }, {
    "heading": "5 Evaluation",
    "text": "We first show the effect of OOVs on translation quality, then evaluate our approach in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages. In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process."
  }, {
    "heading": "5.1 Experimental Setup",
    "text": "We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2. fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3.\n1http://www.cdec-decoder.org 2EgivenFCoherent, SampleCountF, CountEF, MaxLexF-\ngivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF\nKenLM (Heafield, 2011) is used to train a 5- gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework3. We use maximum phrase length 10. For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM). Each graph propagation iteration takes about 3 minutes.\nFor French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results, this works very well in our data. For Arabic, AQMAR is used to exclude named-entities (Mohit et al., 2012). For each of the experimental settings below we show the OOV statistics in Table 2."
  }, {
    "heading": "5.2 Impact of OOVs: Oracle experiment",
    "text": "This oracle experiment shows that translation of OOVs beyond named entities, dates, etc. is potentially very useful in improving output translation. We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7) (Koehn, 2005). WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors)."
  }, {
    "heading": "5.3 Case 1: Limited Parallel Data",
    "text": "In this experiment we use a setup similar to (Razmara et al., 2013). To have fair comparison,\n3Junto : https://github.com/parthatalukdar/junto\nwe use 10K French-English parallel sentences, randomly chosen from Europarl to train translation system, as reported in (Razmara et al., 2013). ACL/WMT 20054 is used for dev and test data. We re-implement their paraphrase extraction method (DP) to extract paraphrases from French side of Europarl (2M sentences). We use unigram nodes to construct graphs for both DP and PPDB. In bipartite graphs, each node is connected to at most 20 nodes. For tripartite graphs, each node is connected to 15 labeled and 5 unlabeled nodes.\nFor intrinsic evaluation, we use MeanReciprocal-Rank (MRR) and Recall. MRR is the mean of reciprocal rank of the candidate list compared to the gold list (Eqn. 5). Recall shows percentage of gold list covered by the candidate list (Eqn. 6). Gold translations for OOVs are given by concatenating the test data to training and running a word aligner.\nMRR = 1 |O| |O|∑ i=1 1 ranki for O = {OOVs} (5)\nRecall = |{gold list} ∩ {candidate list}|\n|{gold list}| (6)\nTable 5 compares DP and PPDB in terms of BLEU, MRR and Recall. It indicates that PPDB (large size) outperforms DP in both intrinsic and extrinsic evaluation measures. Although tripartite graph did not improve the results for DP, it results in statistically significantly better BLEU score for PPDB in comparison to DP (evaluated by MultEval (Clark et al., 2011)). Thus we use tripartite graph in the rest of experiments. The last row in the table shows the result of combining DP and PPDB by multiplying the normalized scores of both paraphrase lists.\nThis setting is included for three reasons: 1) we exploit the small data size to explore different choices in our approach such as, e.g. choosing bipartite versus tripartite graph structures; 2)\n4http://www.statmt.org/wpt05/mt-shared-task/\nto show how well our PPDB approach does compared to the DP approach in terms of MRR and recall; and 3) to show applicability of our approach for a low-resource language. However we used French instead of a language which is truly resource-poor due to the lack of available paraphrases for a true resource poor language, e.g. Malagasy."
  }, {
    "heading": "5.4 Case 2: Domain Adaptation",
    "text": "Domain adaptation is another case that suffers from massive number of OOVs. We compare our approach with Marginal Matching (Irvine et al., 2013), a state of the art approach in SMT domain adaptation. We use their setup and data and compare our results to their reported results (Irvine et al., 2013). 250K lines of Hansard parliamentary proceeding are used for training MT. Dev and test sets are available for two different domains: Medical and Science domains. For medical domain random subset of EMEA corpus (Tiedemann, 2009) and for the science domain a corpus of scientific articles (Carpuat et al., 2012) has been used. Unigram paraphrases using DP are extracted from French side of Europarl.\nTable 6 compares the results in terms of BLEU score. In both medical and science domains, graph-propagation approach using PPDB (large) performs significantly better than DP (p < 0.02), and has comparable results to Marginal Matching.\nMarginal Matching performs better in science domain but graph-propagation approach with PPDB outperforms it in medical domain getting a +1.79 BLEU score improvement over the baseline."
  }, {
    "heading": "5.5 Case 3: Morphologically Rich Languages",
    "text": "Both Distribution Profiling and Bilingual Pivoting propose morphological variants of a word as paraphrase pairs. Even more so in PPDB due to pivoting over English. We choose Arabic-English task for this experiment. We train the SMT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline."
  }, {
    "heading": "6 Related Work",
    "text": "Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para-\nphrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010).\nDaumé and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised.\nAlexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table.\nUsing comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here.\nBilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). How-\never, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b)."
  }, {
    "heading": "7 Conclusion and Future work",
    "text": "In future work, we would like to include translations for infrequent phrases which are not OOVs. We would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity. We also would like to expand this work for morphologically rich languages by exploiting other resources like morphological analyzer and campare our approach to the current state of art approaches which are using these types of resources. In conclusion, we have shown significant improvements to the quality of statistical machine translation in three different cases: low resource SMT, domain shift, and morphologically complex languages. Through the use of semi-supervised graph propagation, a large scale multilingual paraphrase database can be used to improve the quality of statistical machine translation."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank Chris CallisonBurch and Juri Ganitkevitch for providing us the latest version of PPDB, the anonymous reviewers for their comments. The research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC RGPIN 262313 and RGPAS 446348) to the last author."
  }],
  "year": 2015,
  "references": [{
    "title": "Graph-based learning for statistical machine translation",
    "authors": ["Andrei Alexandrescu", "Katrin Kirchhoff."],
    "venue": "NAACL 2009.",
    "year": 2009
  }, {
    "title": "Vector space models for phrase-based machine translation",
    "authors": ["Tamer Alkhouli", "Andreas Guta", "Hermann Ney."],
    "venue": "EMNLP 2014: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.",
    "year": 2014
  }, {
    "title": "Paraphrasing with bilingual parallel corpora",
    "authors": ["Colin Bannard", "Chris Callison-Burch."],
    "venue": "ACL 2005.",
    "year": 2005
  }, {
    "title": "Improving statistical machine translation by paraphrasing the training data",
    "authors": ["Francis Bond", "Eric Nichols", "Darren Scott Appling", "Michael Paul."],
    "venue": "IWSLT 2008.",
    "year": 2008
  }, {
    "title": "Improved statistical machine translation using paraphrases",
    "authors": ["Chris Callison-Burch", "Philipp Koehn", "Miles Osborne."],
    "venue": "NAACL 2006.",
    "year": 2006
  }, {
    "title": "Syntactic constraints on paraphrases extracted from parallel corpora",
    "authors": ["Chris Callison-Burch."],
    "venue": "EMNLP 2008.",
    "year": 2008
  }, {
    "title": "Domain adaptation in machine",
    "authors": ["Marine Carpuat", "H Daumé III", "Alexander Fraser", "Chris Quirk", "Fabienne Braune", "Ann Clifton", "Ann Irvine", "Jagadeesh Jagarlamudi", "John Morgan", "Majid Razmara", "Aleš Tamchyna", "Katharine Henry", "Rachel Rudinger"],
    "year": 2012
  }, {
    "title": "Better hypothesis testing for statistical machine translation: controlling for optimizer instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "ACL 2011.",
    "year": 2011
  }, {
    "title": "Ultraconservative online algorithms for multiclass problems",
    "authors": ["Koby Crammer", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research.",
    "year": 2003
  }, {
    "title": "Domain adaptation for machine translation by mining unseen words",
    "authors": ["III Hal Daumé", "Jagadeesh Jagarlamudi."],
    "venue": "ACL 2011.",
    "year": 2011
  }, {
    "title": "Facilitating translation using source language paraphrase lattices",
    "authors": ["Jinhua Du", "Jie Jiang", "Andy Way."],
    "venue": "EMNLP 2010.",
    "year": 2010
  }, {
    "title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models",
    "authors": ["Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Johnathan Weese", "Ferhan Ture", "Phil Blunsom", "Hendra Setiawan", "Vladimir Eidelman", "Philip Resnik"],
    "year": 2010
  }, {
    "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2",
    "authors": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."],
    "venue": "NAACL HLT 2013.",
    "year": 2013
  }, {
    "title": "The multilingual paraphrase database",
    "authors": ["Juri Ganitkevitch", "Chris Callison-Burch."],
    "venue": "LREC 2014.",
    "year": 2014
  }, {
    "title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation",
    "authors": ["Juri Ganitkevitch", "Chris Callison-Burch", "Courtney Napoles", "Benjamin Van Durme."],
    "venue": "NAACL HLT 2011.",
    "year": 2011
  }, {
    "title": "Learning continuous phrase representations for translation modeling",
    "authors": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng."],
    "venue": "ACL 2014.",
    "year": 2014
  }, {
    "title": "Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences",
    "authors": ["Nikesh Garera", "Chris Callison-Burch", "David Yarowsky."],
    "venue": "CoNLL 2009.",
    "year": 2009
  }, {
    "title": "Four Techniques for Online Handling of Out-of-Vocabulary Words in ArabicEnglish Statistical Machine Translation",
    "authors": ["Nizar Habash."],
    "venue": "ACL 2008.",
    "year": 2008
  }, {
    "title": "Learning bilingual lexicons from monolingual corpora",
    "authors": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "ACL 2008.",
    "year": 2008
  }, {
    "title": "KenLM: faster and smaller language model queries",
    "authors": ["Kenneth Heafield."],
    "venue": "WMT 2011.",
    "year": 2011
  }, {
    "title": "Supervised bilingual lexicon induction with multiple monolingual signals",
    "authors": ["Ann Irvine", "Chris Callison-Burch."],
    "venue": "NAACL 2013.",
    "year": 2013
  }, {
    "title": "Hallucinating phrase translations for low resource MT",
    "authors": ["Ann Irvine", "Chris Callison-Burch."],
    "venue": "CoNLL-2014.",
    "year": 2014
  }, {
    "title": "Using comparable corpora to adapt mt models to new domains",
    "authors": ["Ann Irvine", "Chris Callison-Burch."],
    "venue": "ACL 2014.",
    "year": 2014
  }, {
    "title": "Monolingual marginal matching for translation model adaptation",
    "authors": ["Ann Irvine", "Chris Quirk", "Hal Daumé III."],
    "venue": "EMNLP 2013.",
    "year": 2013
  }, {
    "title": "Paraphrasing for automatic evaluation",
    "authors": ["David Kauchak", "Regina Barzilay."],
    "venue": "NAACL 2008.",
    "year": 2006
  }, {
    "title": "Learning a translation lexicon from monolingual corpora",
    "authors": ["Philipp Koehn", "Kevin Knight."],
    "venue": "ACL 2002 workshop on unsupervised lexical acquisition.",
    "year": 2002
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "NAACL 2003.",
    "year": 2003
  }, {
    "title": "Moses: open source toolkit for statistical machine translation",
    "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch"],
    "year": 2007
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "MT summit 2005, volume 5.",
    "year": 2005
  }, {
    "title": "Automatic retrieval and clustering of similar words",
    "authors": ["Dekang Lin."],
    "venue": "ACL 1998.",
    "year": 1998
  }, {
    "title": "Learning translation consensus with structured label propagation",
    "authors": ["Shujie Liu", "Chi-Ho Li", "Mu Li", "Ming Zhou."],
    "venue": "ACL 2012.",
    "year": 2012
  }, {
    "title": "Using paraphrases for parameter tuning in statistical machine translation",
    "authors": ["Nitin Madnani", "Necip Fazil Ayan", "Philip Resnik", "Bonnie J Dorr."],
    "venue": "WMT 2007.",
    "year": 2007
  }, {
    "title": "Multipath translation lexicon induction via bridge languages",
    "authors": ["Gideon S. Mann", "David Yarowsky."],
    "venue": "NAACL 2001.",
    "year": 2001
  }, {
    "title": "Improved statistical machine translation using monolingually-derived paraphrases",
    "authors": ["Yuval Marton", "Chris Callison-Burch", "Philip Resnik."],
    "venue": "EMNLP 2009.",
    "year": 2009
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."],
    "venue": "CoRR.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."],
    "venue": "NIPS 2013.",
    "year": 2013
  }, {
    "title": "Source-language entailment modeling for translating unknown terms",
    "authors": ["Shachar Mirkin", "Lucia Specia", "Nicola Cancedda", "Ido Dagan", "Marc Dymetman", "Idan Szpektor."],
    "venue": "ACL-IJCNLP 2009.",
    "year": 2009
  }, {
    "title": "Recall-oriented learning of named entities in arabic wikipedia",
    "authors": ["Behrang Mohit", "Nathan Schneider", "Rishav Bhowmick", "Kemal Oflazer", "Noah A Smith."],
    "venue": "EACL 2012, pages 162–173.",
    "year": 2012
  }, {
    "title": "Extracting parallel sub-sentential fragments from nonparallel corpora",
    "authors": ["Dragos Stefan Munteanu", "Daniel Marcu."],
    "venue": "ACL 2006.",
    "year": 2006
  }, {
    "title": "Improved statistical machine translation using monolingual paraphrases",
    "authors": ["Preslav Nakov."],
    "venue": "ECAI 2008: 18th European Conference on Artificial Intelligence. IOS Press.",
    "year": 2008
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Comput. Linguist.",
    "year": 2003
  }, {
    "title": "Minimum error rate training for statistical machine translation",
    "authors": ["Franz Josef Och."],
    "venue": "ACL 2003.",
    "year": 2003
  }, {
    "title": "Paraphrase lattice for statistical machine translation",
    "authors": ["Takashi Onishi", "Masao Utiyama", "Eiichiro Sumita."],
    "venue": "ACL 2010.",
    "year": 2010
  }, {
    "title": "Identifying word translations in non-parallel texts",
    "authors": ["Reinhard Rapp."],
    "venue": "ACL 1995.",
    "year": 1995
  }, {
    "title": "Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation",
    "authors": ["Majid Razmara", "Maryam Siahbani", "Reza Haffari", "Anoop Sarkar."],
    "venue": "ACL 2013.",
    "year": 2013
  }, {
    "title": "Improving translation via targeted paraphrasing",
    "authors": ["Philip Resnik", "Olivia Buzek", "Chang Hu", "Yakov Kronrod", "Alex Quinn", "Benjamin B Bederson."],
    "venue": "EMNLP 2010.",
    "year": 2010
  }, {
    "title": "Graph-based semi-supervised learning of translation models from monolingual data",
    "authors": ["Avneesh Saluja", "Hany Hassan", "Kristina Toutanova", "Chris Quirk."],
    "venue": "ACL 2014.",
    "year": 2014
  }, {
    "title": "Inducing translation lexicons via diverse similarity measures and bridge languages",
    "authors": ["Charles Schafer", "David Yarowsky."],
    "venue": "CoNLL 2002.",
    "year": 2002
  }, {
    "title": "Extracting parallel sentences from comparable corpora using document level alignment",
    "authors": ["Jason R. Smith", "Chris Quirk", "Kristina Toutanova."],
    "venue": "NAACL 2010.",
    "year": 2010
  }, {
    "title": "New Regularized Algorithms for Transductive Learning",
    "authors": ["Partha Pratim Talukdar", "Koby Crammer."],
    "venue": "European Conference on Machine Learning.",
    "year": 2009
  }, {
    "title": "Weakly-supervised acquisition of labeled class instances using graph random walks",
    "authors": ["Partha Pratim Talukdar", "Joseph Reisinger", "Marius Paşca", "Deepak Ravichandran", "Rahul Bhagat", "Fernando Pereira."],
    "venue": "EMNLP 2008.",
    "year": 2008
  }, {
    "title": "Topics in graph construction for semi-supervised learning",
    "authors": ["Partha Pratim Talukdar."],
    "venue": "Technical Report MS-CIS-09-13, University of Pennsylvania, Dept of Computer and Info. Sci.",
    "year": 2009
  }, {
    "title": "Bilingual lexicon extraction from comparable corpora using label propagation",
    "authors": ["Akihiro Tamura", "Taro Watanabe", "Eiichiro Sumita."],
    "venue": "EMNLP-CoNLL 2012.",
    "year": 2012
  }, {
    "title": "News from OPUS-A collection of multilingual parallel corpora with tools and interfaces",
    "authors": ["Jörg Tiedemann."],
    "venue": "Recent advances in natural language processing.",
    "year": 2009
  }, {
    "title": "Learning a phrase-based translation model from monolingual data with application to domain adaptation",
    "authors": ["Jiajun Zhang", "Chengqing Zong."],
    "venue": "ACL 2013.",
    "year": 2013
  }, {
    "title": "Handling unknown words in statistical machine translation from a new perspective",
    "authors": ["Jiajun Zhang", "Feifei Zhai", "Chengqing Zong."],
    "venue": "Natural Language Processing and Chinese Computing. Springer.",
    "year": 2012
  }, {
    "title": "Bilingually-constrained phrase embeddings for machine translation",
    "authors": ["Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong."],
    "venue": "ACL 2014.",
    "year": 2014
  }, {
    "title": "Pivot approach for extracting paraphrase patterns from bilingual corpora",
    "authors": ["Shiqi Zhao", "Haifeng Wang", "Ting Liu", "Sheng Li."],
    "venue": "ACL 2008.",
    "year": 2008
  }, {
    "title": "Learning translation models from monolingual continuous representations",
    "authors": ["Kai Zhao", "Hany Hassan", "Michael Auli."],
    "venue": "NAACL 2015.",
    "year": 2015
  }, {
    "title": "Bilingual word embeddings for phrase-based machine translation",
    "authors": ["Will Zou", "Richard Socher", "Daniel Cer", "Christopher Manning."],
    "venue": "EMNLP 2013. 1390",
    "year": 2013
  }],
  "id": "SP:0d254da10fdae7f1a474b69381c3e668e0db94fd",
  "authors": [{
    "name": "Ramtin Mehdizadeh Seraj",
    "affiliations": []
  }, {
    "name": "Maryam Siahbani",
    "affiliations": []
  }, {
    "name": "Anoop Sarkar",
    "affiliations": []
  }],
  "abstractText": "The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages. In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases. In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality. We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional profiles from monolingual source data.",
  "title": "Improving Statistical Machine Translation with a Multilingual Paraphrase Database"
}