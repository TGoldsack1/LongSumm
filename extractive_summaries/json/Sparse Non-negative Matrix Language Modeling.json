{
  "sections": [{
    "text": "A second contribution is a comparison between SNM and the related class of Maximum Entropy language models. While much cheaper computationally, we show that SNM achieves slightly better perplexity results for the same feature set and same speech recognition accuracy on voice search and short message dictation. Index Terms: language modeling, maximum entropy, speech recognition, machine learning"
  }, {
    "heading": "1. Introduction",
    "text": "A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size is in the tens, hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that W is such a segment, or sentence.\nSince the parameter space of P (wk|w1, w2, . . . , wk−1) is too large, the language model (LM) is forced to put the context Wk−1 = w1, w2, . . . , wk−1 into an equivalence class determined by a function Φ(Wk−1). As a result,\nP (W ) ∼= n∏ k=1 P (wk|Φ(Wk−1)) (1)\nResearch in language modeling consists of finding appropriate equivalence classifiers Φ and methods to estimate P (wk|Φ(Wk−1)). Once the form Φ(Wk−1) is specified, only the problem of estimating P (wk|Φ(Wk−1)) from training data remains.\nThe contribution of this paper is two-fold: • Sections 2 and 3 present a new way of estimating the\nsparse non-negative model (SNM) by using a small amount of held-out data and the multinomial loss that is natural for LM; we validate it against the previous estimation method in [1]-[2]\n• Section 4 contrasts the SNM LM with the related class of maximum entropy (MaxEnt) LMs; experiments on the One Billion Words corpus [3] show that SNM achieves slightly better perplexity results for the same feature set, and same speech recognition accuracy on voice search and short message dictation."
  }, {
    "heading": "2. Notation and Modeling Assumptions",
    "text": "We denote with e an event in the training/development/test data corresponding to each prediction (wk|Φ(Wk−1)) in Eq. (1); each event consists of:\n• a set of features F(e) = {f1, . . . , fk, . . . , fF (e)} ⊂ F , where F denotes the set of features in the model, collected on the training data: F = ∪e∈T F(e);\n• a predicted (target) word w = t(e) from the LM vocabulary V; we denote with V = |V| the size of the vocabulary.\nThe set of features F(e) is obtained by applying the equivalence classification function Φ(Wk−1) to the context of the prediction. The n-gram model is a particular case extracting all n-gram features of length 0, . . . , n−1 from theWk−1 context1, respectively.\n2.1. Skip-n-gram Language Modeling\nA simple variant on the n-gram model is the skip-n-gram model; a skip-n-gram feature extracted from the context Wk−1 is characterized by the tuple (r, s, a) where:\n• r denotes number of remote context words\n• s denotes the number of skipped words\n• a denotes the number of adjacent context words\nrelative to the target word wk being predicted. For example, in the sentence, <S> The quick brown fox jumps over the lazy dog </S> a (1, 2, 3) skip-gram feature for the target word dog is: [brown skip-2 over the lazy]\nTo control the size of F(e) it is recommended to limit the skip length s and also either (r+a) or both r and s. We configure the skip-n-gram feature extractor to produce all features f defined by the equivalence class Φ(Wk−1) that meet constraints on minimum and maximum values for:\n• the number of context words used r + a;\n• the number of remote words r;\n1The empty feature is considered to have length 0, it is present in every event e, and it produces the unigram distribution on the language model vocabulary.\nCopyright © 2017 ISCA\nhttp://dx.doi.org/10.21437/Interspeech.2017-4932725\n• the number of adjacent words a;\n• the skip length s.\nWe also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features. Tied skip-n-gram features look like: [curiosity skip-* the cat]\nA simple extension that leverages context beyond the current sentence, as well as other categorical features such as geolocation is presented and evaluated in [4].\nIn order to build a good probability estimate for the target word wk in a context Wk−1, or an event e in our notation, we need a way of combining arbitrary features which do not fall into a simple hierarchy like regular n-gram features. The following section describes a simple yet novel approach for combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view."
  }, {
    "heading": "3. Multinomial Loss for the Sparse Non-negative Matrix Language Model",
    "text": "The sparse non-negative matrix (SNM) language model (LM) [1]-[2] assigns probability to a word by applying the equivalence classification function Φ(W ) to the context of the prediction, as explained in the previous section, and then using a matrix M, where Mfw is indexed by feature f ∈ F and word w ∈ V . We further assume that the model is parameterized as a slight variation on conditional relative frequencies for words w given features f , denoted as c(w|f):\nP (w|Φ(W )) ∝ ∑\nf∈Φ(W ) c(w|f) · exp(A(f, w;θ))︸ ︷︷ ︸ Mfw (2)\nThe adjustment function A(f, w;θ) is a real-valued function whose task is to estimate the relative importance of each input feature f for the prediction of the given target word w. It is computed by a linear model on meta-features h extracted from each link (f, w):\nA(f, w;θ) = ∑ k θkhk(f, w) (3)\nThe meta-features are either strings identifying the feature type, feature, link etc., or bucketed feature and link counts. We also allow all possible conjunctions of elementary meta-features, and estimate a weight θk for each (elementary or conjoined) meta-feature hk. In order to control the model size we use the hashing technique in [5],[6]. The meta-feature extraction is explained in more detail in [7].\nAssuming we have a sparse matrix M of adjusted relative frequencies, the probability of an event e = (w|Φ(W )) predicting word w in context Φ(W ) is computed as follows:\nP (e) = yt(e)/y(e) yt(e) = ∑ f∈F ∑ w∈V 1f (e) · 1w(e)Mfw\ny(e) = ∑ f∈F 1f (e)Mf∗\nwhere Mf∗ ensures that the model is properly normalized over the LM vocabulary:\nMf∗ = ∑ w∈V Mfw\nand the indicator functions 1f (e) and 1w(e) select a given feature and target word in the event e, respectively.\nWith this notation and using the shorthand Afw = A(f, w;θ), the derivative of the log-probability for event ewith respect to the adjustment function Afw for a given link (f, w) is:\n∂ logP (e)\n∂Afw =\n∂ log yt(e) ∂Afw − ∂ log y(e) ∂Afw\n= 1\nyt(e)\n∂yt(e) ∂Afw − 1 y(e) ∂y(e) ∂Afw\n= 1f (e)Mfw\n[ 1w(e)\nyt(e) − 1 y(e)\n] (4)\nmaking use of the fact that ∂Mfw ∂Afw = ∂c(w|f)exp(Afw) ∂Afw = c(w|f)exp(Afw) = Mfw Propagating the gradient ∂ logP (e)\n∂Afw to the θ parameters of\nthe adjustment functionA(f, w;θ) is done using mini-batch estimation for efficiency reasons:\n∂ logP (e)\n∂θk = ∑ (f,w):hk∈meta−feat(f,w) ∂ logP (e) ∂Afw\nθk,B+1 ← θk,B − η ∑ e∈B ∂ logP (e) ∂θk (5)\nRather than using a single fixed learning rate η, we use AdaGrad [8] which uses a separate adaptive learning rate ηk,B for each weight θk,B :\nηk,B = γ√\n∆0 + ∑B b=1 [∑ e∈b ∂ logP (e) ∂θk ]2 (6) whereB is the current batch index, γ is a constant scaling factor for all learning rates and ∆0 is an initial accumulator constant. Basing the learning rate on historical information tempers the effect of frequently occurring features which keeps the weights small and as such acts as a form of regularization.\nWe refer the reader to Sections 3.1-2 of [7] for further details on the implementation and a description of the metafeatures used by the adjustment model."
  }, {
    "heading": "4. Maximum Entropy Language Model",
    "text": "Maximum entropy (MaxEnt) is a well studied modeling technique that similarly to SNM allows the use of arbitrary features for estimating P (w|Φ(W )). It differs from the SNM parameterization of P (w|Φ(W )) for an event e = (w|Φ(W )), see Eq. 2 for a direct comparison:\nP (w|Φ(W )) = exp[\n∑ f,w 1w(e) · 1f (e) · λf,w]\nZΦ(W ) (7)\nwhere λf,w is the parameter associated with the joint MaxEnt feature (f, w) and ZΦ(W ) is a normalization factor or partition function.\nWe train our MaxEnt models using stochastic gradient descent. The computational complexity of each update is O(|V|), similar to multinomial SNM updates. In practice one SNM update on a given held-out event is more expensive due to the large number of meta-feature being updated; on the other hand MaxEnt training requires multiple iterations though the full training data set, making it orders of magnitude more expensive to train.\nWe distribute the training of MaxEnt model using hundreds of machines using the Iterative Parameter Mixture (IPM) method [9] as described in [10]."
  }, {
    "heading": "4.1. Hierarchical Modeling",
    "text": "Even using the IPM method, MaxEnt model training is still too expensive for very large corpora. We partition the vocabulary in clusters c(w) and use a hierarchical model:\nP (w|Φ(W )) ∝ P (c(w)|Φ(W )) · P (w|Φ(W ), c(w)) (8)\nwhere P (c(w)|Φ(W )) is a model over the cluster vocabulary and P (w|Φ(W ), c(w)) is a model similar to Equation 7, but normalized over the words in cluster c(w). Now, instead of one update with complexity O(|V|), training requires two updates with complexity as low as O(| √ (V)|. Hierarchical modeling can improve model quality [11]. The quality of the hierarchical model depends crucially on the clustering method. In this paper we use the algorithm described in [12]. SNM can also benefit from hierarchical modeling in which case P (c(w)|Φ(W )) and P (w|Φ(W ), c(w)) are independent SNM models."
  }, {
    "heading": "4.2. Contrasting SNM versus MaxEnt Model Estimation",
    "text": "SNM and MaxEnt differ in the estimation procedure. Most SNM parameters are stored in the matrix M which is initialized with counts from the training data. These parameters are fixed during the adjustment procedure, which only needs to estimate a relatively small set of parameters whose size is controlled by the types of meta-features used in the model. This can be done reliably and efficiently with a small held-out set of high quality in-domain data. MaxEnt does not have a known comparable initialization procedure; training requires updating all parameters from scratch by iterating through the training data until each feature is seen enough times; it is often beneficial to have a second training stage that adapts the model to in-domain transcribed data."
  }, {
    "heading": "5. Experiments",
    "text": ""
  }, {
    "heading": "5.1. Experiments on the One Billion Words Language Modeling Benchmark",
    "text": "Our first experimental setup used the One Billion Word Benchmark (OBWB) corpus2 made available by [3]. For complete-\n2http://www.statmt.org/lm-benchmark\nness, here is a short description of the corpus, containing only monolingual English data:\n• total number of training tokens is about 0.8 billion\n• the vocabulary provided consists of 793471 words including sentence boundary markers <S>, </S>, and was constructed by discarding all words with count below 3\n• words outside of the vocabulary were mapped to an <UNK> token, also part of the vocabulary\n• sentence order was randomized\n• the test data consisted of 159658 words (without counting the sentence beginning marker <S> which is never predicted by the language model)\n• the out-of-vocabulary (OOV) rate on the test set was 0.28%.\nThe foremost concern when using held-out data for estimating the adjustment model is the limited amount of data available in a practical setup, so we used a small development set consisting of 33 thousand words.\nWe conducted experiments using two feature extraction configurations identical to those used in [1]: 5-gram and skip-10-gram, see Appendix in [7]. The AdaGrad parameters in Eq. (6) are set to: γ = 0.1, ∆0 = 1.0, and the mini-batch size is 2048 samples. We also experimented with various adjustment model sizes (200M, 20M, and 200k hashed parameters) and non-lexicalized meta-features. The results are presented in Table 1; since there was little dependence on the adjustment model size, we only list results for the 20M hashed parameters setting.\nA first conclusion is that training the adjustment model on held-out data using multinomial loss matches the previous results reported in [1].\nA second conclusion is that we can indeed get away with very small amounts of development data. This is excellent news, because development data is typically in short supply."
  }, {
    "heading": "5.2. Comparison with Equivalent Maximum Entropy Model",
    "text": "Table 2 shows the perplexity of various hierarchical models on the OBWB, all experiments use the same 1000 cluster vocabulary partition. We observe that hierarchical modeling slightly improves SNM modeling relative to the best 5-gram model in Table 1. We also see that the SNM model has significantly lower perplexity than the MaxEnt model. The number of parameters is higher for SNM because it stores an extra parameter per context."
  }, {
    "heading": "5.3. ASR Experiments",
    "text": "We conducted automatic speech recognition (ASR) experiments to compare SNM and MaxEnt hierarchical models. All experiments were based on Google’s cloud based mobile ASR system for Italian. This is a state of the art system with an LSTM acoustic model and a 15 million n-gram LM for the first pass, estimated from various data sources using Bayesian interpolated [13]. In this work the proposed second pass SNM and\nMaxEnt models, respectively, are used to rescore n-best lists generated by the first pass system. The score of the second pass LM is log linearly interpolated with the first pass LM score and with the acoustic model score.\nThe corpus used to train the second pass models consists of 35 billion words of mobile written sentences and a small subset of 15 million transcribed words. All data were anonymized and stripped of personally identifying items.\nThe vocabulary contains 3.9 million words. We rank the vocabulary according to the distribution in automatically recognized ASR logs. The most frequent million words are clustered in 1000 clusters. The remaining words are assigned to a special cluster <TAIL>. For efficiency its cluster conditional submodel P (w|Φ(W ), c(w) = <TAIL>) is estimated using unigram relative frequencies instead of an SNM or MaxEnt model.\nThe types of features used fall into the following feature templates:\n• word n-grams, y, wi−1, · · · , wi−k up to 5-gram. • word cluster n-grams, y, c(wi−1), · · · , c(wi−k) from 3\nto 5-gram. • skip 2-grams, y, ∗, wi−k up to 5 word gap. • left and right skip 3-grams, y, ∗, wi−k+1, wi−k, and y, wi−1, ∗, wi−k up to 3 word gap.\ny is the token being predicted c(wi) in the case of the cluster sub-model and wi in the case of the cluster conditioned submodels.\nWe trained MaxEnt models with and without PrefixBackoff 0 features as described in [10]. These features are shared between contexts Wk−1 in the same feature template and trigger when a regular feature is missing. We selected approximately 5 billion features per model using counts cut offs. The MaxEnt models were first trained for 5 epochs on the training data and then adapted by continuing training on the transcribed data subset for 3 additional epochs using a small learning rate.\nCounts for the unadjusted SNM model were collected during initialization. The model was then adjusted using a 500k word held-out subset of the transcribed data. We used 20 million hashed parameters and trained for 3 epochs using AdaGrad. We observed no improvements by increasing the size of the held-out set or the number of epochs.\nWe shared as many components as possible between the MaxEnt and SNM implementations, including: all data processing, clustering, feature extraction, ASR rescorer, etc. The differences between the two systems are mostly limited to the probability and gradient computation functions.\nTable 3 shows a breakdown of the training time for the MaxEnt and SNM LMs, respectively. Our MaxEnt training pipeline is highly optimized but it is clear that multinomial SNM training requires a very small fraction of the computational resources of MaxEnt: MaxEnt training and adapting requires about 2,442 machine hours whereas SNM adjustment requires only 2/3 machine hours. While still very significant, the end to end training\ntime reduction is not as impressive due to MaxEnt’s distributed implementation and to lengthy sibling stages such as word clustering.\nTable 4 presents the word error rate (WER) of various models measured on voice search and short message held-out test sets. We observe that SNM matches the accuracy of standard MaxEnt. When augmented with back-off features MaxEnt improves slightly; experiments adding similar back-off features to SNM were unproductive."
  }, {
    "heading": "6. Conclusions and Future Work",
    "text": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [1]: as observed in [14], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model. Correcting the deficiency in [1] induced by using a Poisson model for each binary random variable does not seem to make a difference in the quality of the estimated model.\nBeing able to train on held-out data is very important in practical situations where the training data is mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training, or taking into account the data source for a given skip/n-gram feature and combining them for best performance on held-out/test data (similar to the Bayesian interpolation algorithm [13] used for estimating the first-pass model in our ASR system).\nWe find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model.\nJust like MaxEnt, SNM can be extended to hierarchical modeling and we observe significant perplexity improvements: multinomial adjusted SNM achieves 14% relative lower perplexity than similar MaxEnt models on the One Billion Words Benchmark. In terms of ASR quality SNM matches the WER of standard MaxEnt while being orders of magnitude cheaper to train. However, MaxEnt augmented with back-off features is slightly better. In future work we plan to research techniques for adding similar features to SNM models."
  }, {
    "heading": "7. Acknowledgments",
    "text": "Thanks go to Yoram Singer for clarifying the correct mini-batch variant of AdaGrad, Noam Shazeer for assistance on understanding his implementation of the adjustment function estimation, Kunal Talwar, Amir Globerson for useful discussions and former summer intern Joris Pelemans for suggestions while preparing the paper."
  }, {
    "heading": "8. References",
    "text": "[1] N. Shazeer, J. Pelemans, and C. Chelba, “Skip-gram language\nmodeling using sparse non-negative matrix probability estimation,” CoRR, vol. abs/1412.1454, 2014. [Online]. Available: http://arxiv.org/abs/1412.1454\n[2] N. M. Shazeer, J. Pelemans, and C. Chelba, “Sparse non-negative matrix language modeling for skip-grams,” in Proceedings of Interspeech 2015, 2015, pp. 1428–1432.\n[3] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, and P. Koehn, “One billion word benchmark for measuring progress in statistical language modeling,” CoRR, vol. abs/1312.3005, 2013. [Online]. Available: http://arxiv.org/abs/1312.3005\n[4] C. Chelba and N. Shazeer, “Sparse non-negative matrix language modeling for geo-annotated query session data,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec 2015, pp. 8–14.\n[5] K. Ganchev and M. Dredze, “Small statistical models by random feature mixing,” in Proceedings of the ACL-2008 Workshop on Mobile Language Processing, Association for Computational Linguistics, 2008, pp. 604–613.\n[6] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg, “Feature hashing for large scale multitask learning,” in Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML ’09. New York, NY, USA: ACM, 2009, pp. 1113–1120. [Online]. Available: http://doi.acm.org/10.1145/1553374.1553516\n[7] C. Chelba and F. Pereira, “Multinomial loss on heldout data for the sparse non-negative matrix language model,” CoRR, vol. abs/1511.01574, 2015. [Online]. Available: http://arxiv.org/abs/1511.01574\n[8] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimization,” J. Mach. Learn. Res., vol. 12, pp. 2121–2159, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=1953048.2021068\n[9] K. Hall, S. Gilpin, and G. Mann, “Mapreduce/bigtable for distributed optimization,” in Neural Information Processing Systems Workshop on Leaning on Cores, Clusters, and Clouds, 2010.\n[10] F. Biadsy, K. Hall, P. Moreno, and B. Roark, “Backoff inspired features for maximum entropy language models,” in Proceedings of Interspeech, 2014.\n[11] S. F. Chen, “Shrinking exponential language models,” in Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, ser. NAACL ’09. Stroudsburg, PA, USA: Association for Computational Linguistics, 2009, pp. 468–476.\n[12] J. Uszkoreit and T. Brants, “Distributed word clustering for large scale class-based language modeling in machine translation,” in ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, June 15-20, 2008, Columbus, Ohio, USA, 2008, pp. 755–762.\n[13] C. Allauzen and M. Riley, “Bayesian language model interpolation for mobile speech input,” in Interspeech 2011, 2011, pp. 1429–1432.\n[14] P. Xu, A. Gunawardana, and S. Khudanpur, “Efficient subsampling for training complex language models,” in Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP ’11. Stroudsburg, PA, USA: Association for Computational Linguistics, 2011, pp. 1128–1136. [Online]. Available: http://dl.acm.org/citation.cfm?id=2145432.2145552"
  }],
  "year": 2017,
  "references": [{
    "title": "Skip-gram language modeling using sparse non-negative matrix probability estimation",
    "authors": ["N. Shazeer", "J. Pelemans", "C. Chelba"],
    "venue": "CoRR, vol. abs/1412.1454, 2014. [Online]. Available: http://arxiv.org/abs/1412.1454",
    "year": 2014
  }, {
    "title": "Sparse non-negative matrix language modeling for skip-grams",
    "authors": ["N.M. Shazeer", "J. Pelemans", "C. Chelba"],
    "venue": "Proceedings of Interspeech 2015, 2015, pp. 1428–1432.",
    "year": 2015
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn"],
    "venue": "CoRR, vol. abs/1312.3005, 2013. [Online]. Available: http://arxiv.org/abs/1312.3005",
    "year": 2013
  }, {
    "title": "Sparse non-negative matrix language modeling for geo-annotated query session data",
    "authors": ["C. Chelba", "N. Shazeer"],
    "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Dec 2015, pp. 8–14.",
    "year": 2015
  }, {
    "title": "Small statistical models by random feature mixing",
    "authors": ["K. Ganchev", "M. Dredze"],
    "venue": "Proceedings of the ACL-2008 Workshop on Mobile Language Processing, Association for Computational Linguistics, 2008, pp. 604–613.",
    "year": 2008
  }, {
    "title": "Feature hashing for large scale multitask learning",
    "authors": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"],
    "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML ’09. New York, NY, USA: ACM, 2009, pp. 1113–1120. [Online]. Available: http://doi.acm.org/10.1145/1553374.1553516",
    "year": 2009
  }, {
    "title": "Multinomial loss on heldout data for the sparse non-negative matrix language model",
    "authors": ["C. Chelba", "F. Pereira"],
    "venue": "CoRR, vol. abs/1511.01574, 2015. [Online]. Available: http://arxiv.org/abs/1511.01574",
    "year": 2015
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "J. Mach. Learn. Res., vol. 12, pp. 2121–2159, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=1953048.2021068",
    "year": 2011
  }, {
    "title": "Mapreduce/bigtable for distributed optimization",
    "authors": ["K. Hall", "S. Gilpin", "G. Mann"],
    "venue": "Neural Information Processing Systems Workshop on Leaning on Cores, Clusters, and Clouds, 2010.",
    "year": 2010
  }, {
    "title": "Backoff inspired features for maximum entropy language models",
    "authors": ["F. Biadsy", "K. Hall", "P. Moreno", "B. Roark"],
    "venue": "Proceedings of Interspeech, 2014.",
    "year": 2014
  }, {
    "title": "Shrinking exponential language models",
    "authors": ["S.F. Chen"],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, ser. NAACL ’09. Stroudsburg, PA, USA: Association for Computational Linguistics, 2009, pp. 468–476.",
    "year": 2009
  }, {
    "title": "Distributed word clustering for large scale class-based language modeling in machine translation",
    "authors": ["J. Uszkoreit", "T. Brants"],
    "venue": "ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, June 15-20, 2008, Columbus, Ohio, USA, 2008, pp. 755–762.",
    "year": 2008
  }, {
    "title": "Bayesian language model interpolation for mobile speech input",
    "authors": ["C. Allauzen", "M. Riley"],
    "venue": "Interspeech 2011, 2011, pp. 1429–1432.",
    "year": 2011
  }, {
    "title": "Efficient subsampling for training complex language models",
    "authors": ["P. Xu", "A. Gunawardana", "S. Khudanpur"],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP ’11. Stroudsburg, PA, USA: Association for Computational Linguistics, 2011, pp. 1128–1136. [Online]. Available: http://dl.acm.org/citation.cfm?id=2145432.2145552 2729",
    "year": 2011
  }],
  "id": "SP:734d2616a900fbbede56230958cf11ba39f2cbe2",
  "authors": [{
    "name": "Ciprian Chelba",
    "affiliations": []
  }, {
    "name": "Diamantino Caseiro",
    "affiliations": []
  }, {
    "name": "Fadi Biadsy",
    "affiliations": []
  }],
  "abstractText": "We present a new method for estimating the sparse non-negative model (SNM) by using a small amount of held-out data and the multinomial loss that is natural for language modeling; we validate it experimentally against the previous estimation method which uses leave-one-out on training data and a binary loss function and show that it performs equally well. Being able to train on held-out data is very important in practical situations where training data is mismatched from held-out/test data. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model, which is the only model component estimated using gradient descent; the bulk of model parameters are relative frequencies counted on training data. A second contribution is a comparison between SNM and the related class of Maximum Entropy language models. While much cheaper computationally, we show that SNM achieves slightly better perplexity results for the same feature set and same speech recognition accuracy on voice search and short message dictation.",
  "title": "Sparse Non-negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap"
}