{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3120–3131 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3120"
  }, {
    "heading": "1 Introduction",
    "text": "Short texts have become an important form for individuals to voice opinions and share information on online platforms. A large body of dailygenerated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017).\nAlthough many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang\n∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab.\n† Jing Li is the corresponding author.\net al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To alleviate the data sparsity problem, some approaches exploit knowledge from external resources like Wikipedia (Jin et al., 2011) and knowledge bases (Lucia and Ferrari, 2014; Wang et al., 2017a). These approaches, however, rely on a large volume of high-quality external data, which may be unavailable to some specific domains or languages (Li et al., 2016a).\nTo illustrate the difficulties in classifying short texts, we take the tweet classification in Table 1 as an example. In the test instance S, only given the 11 words it contains, it is difficult to understand why its label is New.Music.Live. Without richer context, classifiers are likely to classify S into the same category as the training instance R1, which happens to share many words with S, in spite of the different categories they belong to,1 rather than R2, which only shares the word “wristbands” with S. Under this circumstance, how might we enrich the context of these short texts? If looking at R2, we can observe that the semantic meaning of “wristbands” can be extended from its co-\n1R1 is about SuperBowl, the annual championship game of the National Football League. R2 and S are both about New.Music.Live, the flagship live music show.\noccurrence with “Bieber”, which is highly indicative of New.Music.Live.2 Such relation can further help in recognizing the word “wristbands” to be important when classifying the test instance S.\nMotivated by the above-mentioned observations, we present a novel neural framework, named as topic memory networks (TMN), for short text classification that does not rely on external knowledge. Our model can identify the indicative words for classification, e.g., “wristbands” in S, via jointly exploiting the document-level word co-occurrence patterns, e.g., “wristbands” and “Bieber” in R2. To be more specific, built upon the success of neural topic models (Srivastava and Sutton, 2017; Miao et al., 2017), our model is capable of discovering latent topics3, which can capture the co-occurrence of words in document level. To employ the latent topics for short text classification, we propose a novel topic memory mechanism, which is inspired by memory networks (Weston et al., 2014; Graves et al., 2014), that allows the model to put attention upon the indicative latent topics useful to classification. With such corpus-level latent topic representations, each short text instance is enriched, which thus helps alleviate the data sparsity issues.\nIn prior research, though the effects of topic models for short text classification have been explored (Phan et al., 2008; Ren et al., 2016), existing methods tend to use pre-trained topics as features. To the best of our knowledge, our model is the first to encode latent topic representations via memory networks for short text classification, which allows joint inference of latent topics.\nTo evaluate our model, we experiment and compare it with existing methods on four benchmark datasets. Experimental results indicate that our model outperforms state-of-the-art counterparts on short text classification. The quantitative and qualitative analysis illustrate the capability of our model in generating topic representations that are meaningful and indicative of different categories."
  }, {
    "heading": "2 Topic Memory Networks",
    "text": "In this section, we describe our topic memory networks (TMN), whose overall architecture is shown\n2Justine Bieber was on New.Music.Live in 2011. There was a business activity for this event that gave free wristbands to fans if they supported Bieber on Twitter.\n3 Latent topics are the distributional clusters of words that frequently co-occur in some of the instances instead of widely appearing throughout the corpus (Blei et al., 2003).\nSoftmax\nlog\nEmbedding\nClassifier\nEm bedding\nNeural Topic Model Topic Memory Mechanism Classifier\nN TM\nEncoder Decoder ,\nΣ y\nFigure 1: The overall framework of our topic memory networks. The dotted boxes from left to right show the neural topic model, the topic memory mechanism, and the classifier. Here the classifier allows multiple options and the details are left out.\nin Figure 1. There are three major components: (1) a neural topic model (NTM) to induce latent topics (described in Section 2.1), (2) a topic memory mechanism that maps the inferred latent topics to classification features (described in Section 2.2), and (3) a text classifier, which produces the final classification labels for instances. These three components can be updated simultaneously via a joint learning process, which is introduced in Section 2.3. In particular, for the classifier, our TMN framework allows the combination of multiple options, e.g., CNN and RNN, which can be determined by the specific application scenario.\nFormally, given X = {x1,x2, . . . ,xM} as the input with M short text instances, each instance x is processed into two representations: bag-ofwords (BoW) term vector xBoW ∈ RV and word index sequence vector xSeq ∈ RL, where V is the vocabulary size and L is the sequence length. xBoW is fed into the neural topic model to induce latent topics. Such topics are further matched with the embedded xSeq to learn classification features in the topic memory mechanism. Then, the classifier concatenates the representations produced by the topic memory mechanism and the embedded xSeq to predict the classification label y for x."
  }, {
    "heading": "2.1 Neural Topic Model",
    "text": "Our topic model is inspired by neural topic model (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017) that induces latent topics in neural networks. NTM is based on variational auto-encoder (VAE) (Kingma and Welling, 2013), involved with a continuous latent variable z as an intermediate representation. Here in NTM, the latent variable\nΣ Σ\nz ∈ RK , where K denotes the number of topics. In the following, we describe the generation and the inference of the model in turn.\nNTM Generation. Similar to LDA-style topic models, we assume x having a topic mixture θ represented as a K-dimensional distribution, which is generated via Gaussian softmax construction (Miao et al., 2017). Each topic k is represented by a word distribution φk over the vocabulary. Specifically, the generation story for x is: • Draw latent variable z ∼ N (µ,σ2) • θ = softmax(fθ(z)) • For the n-th word in x: – Draw word wn ∼ softmax(fφ(θ)) where f∗(·) is a neural perceptron that linearly transforms inputs, activated by a non-linear transformation. Here we use rectified linear units (ReLUs) (Nair and Hinton, 2010) as activate functions. The prior parameters of z, µ and σ, are estimated from the input data and defined as:\nµ = fµ(fe(xBoW )), logσ = fσ(fe(xBoW )) (1) Note that NTM is based on VAE, where an encoder estimates the prior parameters and a decoder describes the generation story. Compared with the basic VAE, NTM includes the additional distributional vectors θ and φ, which can yield latent topic representations and thus ensuring their better interpretability in learning process (Miao et al., 2017).\nNTM Inference. In NTM, we use variational inference (Blei et al., 2016) to approximate a posterior distribution over z given all the instances. The loss function of NTM is defined as\nLNTM = DKL(q(z) || p(z |x))− Eq(z)[p(x | z)] (2)\nthe negative of variational lower bound, where q(z) is a standard Normal prior N (0, I). p(z |x) and p(x | z) are probabilities to describe encoding and decoding processes, respectively.4 Due to the\n4In implementation, to smooth the gradients, we apply reparameterization on z following previous work (Kingma and Welling, 2013; Rezende et al., 2014).\nspace limitation, we leave out the derivation details and refer the readers to Miao et al. (2017)."
  }, {
    "heading": "2.2 Topic Memory Mechanism",
    "text": "We exploit a topic memory mechanism to map the latent topics produced by NTM (described in Section 2.1) to the features for classification. Inspired by memory networks (Weston et al., 2014; Sukhbaatar et al., 2015), we design two memory matrices, a source memory S and a target memory T , both of which are in K×E size (K for the number of topics and E for the pre-defined size of word embeddings). S and T are produced by two ReLU-actived neural perceptrons, both taking the topic-word weight matrixW φ ∈ RK×V as inputs. Recall that in NTM, we use fφ(·) to compute the word distributions given θ. W φ is the kernel weight matrix of fφ(·), whereW φk,v represents the importance of the v-th word in reflecting the k-th topic. Assuming U as the embedded xSeq (word sequence form of x), in source memory, we compute the match between the k-th topic and the embedding of the l-th word in xSeq by\nP k,l = sigmoid(W s[Sk;U l] + b s) (3)\nwhere [x;y] denotes the merge of x and y, and we use concatenation operation here (Dou, 2017; Chen et al., 2017). Ws and bs are parameters to be learned. To further combine the instance-topic mixture θ with P , we define the integrated memory weights as\nξk = θk + γ ∑ l P k,l (4)\nwhere γ is the pre-defined coefficient. Then, in target memory, via weighting target memory matrix T with ξ, we obtain the output representation R of the topic memory mechanism:\nRk = ξkT k (5)\nThe concatenation of R and U (embedded xSeq) further serves as the features for classification.\nIn particular, similar to the memory networks in prior research (Sukhbaatar et al., 2015; Chen et al., 2017), our model can be extended to handle multiple computation layers (hops). As shown in Figure 2, each hop contains a source matrix and a target matrix, and different hops are stacked following the way presented in Sukhbaatar et al. (2015)."
  }, {
    "heading": "2.3 Joint Learning",
    "text": "The entire TMN model integrates the three modules in Figure 1, i.e., the neural topic model, the topic memory mechanism, and the classifier, which can be updated simultaneously in one framework. In doing so, we jointly tackle topic modeling and classification, and define the loss function of the overall framework to combine the two effects as following:\nL = LNTM + λLCLS (6)\nwhere LNTM represents the loss of NTM and LCLS is the cross entropy reflecting classification loss. λ is the trade-off parameter controlling the balance between topic model and classification."
  }, {
    "heading": "3 Experiment Setup",
    "text": ""
  }, {
    "heading": "3.1 Datasets",
    "text": "We conduct experiments on four short text datasets, namely, Snippets, TagMyNews, Twitter, and Weibo. Their details are described as follows.\nSnippets. This dataset contains Google search snippets released by Phan et al. (2008). There are eight ground-truth labels, e.g., health and sport.\nTagMyNews. We use the news titles as instances from the benchmark classification dataset released by Vitale et al. (2012).5 This dataset contains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is annotated with one from seven labels, e.g., sci-tech.\nTwitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track.6 Following previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic labels in each tweet such as “#Trump” and “#SuperBowl”, serve as our ground-truth class labels.\n5http://acube.di.unipi.it/tmn-dataset/ 6http://trec.nist.gov/data/tweets\nSpecifically, we construct the dataset with the following steps. First, we remove the tweets without hashtags. Second, we rank hashtags by their frequencies. Third, we manually remove the hashtags that cannot mark topics, such as “#fb” for indicating the source of tweets from Facebook, and combine the hashtags referring to the same topic, such as “#DonaldTrump” and “#Trump”. Finally, we select the top 50 frequent hashtags, and all tweets containing these hashtags.\nWeibo. To evaluate our model on a different language other than English, we employ a Chinese dataset with short segments of text for topic classification. This dataset is released by Li et al. (2016b) with a collection of messages posted in June 2014 on Weibo, a popular Twitter alike platform in China.7 Similar to Twitter, Weibo allows up to 140 Chinese characters in its messages. In this Weibo dataset, each Weibo message is labeled with a hashtag as its category, and there are 50 distinct hashtag labels in total, following the same procedure performed for the Twitter dataset.\nTable 2 shows the statistic information of the four datasets. Each dataset is randomly split into 80% for training and 20% for test. 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twitter, with gensim tokenizer8 for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013)9 for word segmentation. In addition, for each dataset, we maintain a vocabulary built based on the training set with removal of stop words10 and words occurring less than 3 times. The inputs of topic models xBoW are constructed based on this vocabulary following common topic model settings (Blei et al., 2003; Miao et al., 2016). Differently, we use the raw word sequence (without words removal) for the inputs of classification xSeq as is done in previous work of text classification (Kim, 2014; Liu et al., 2017)."
  }, {
    "heading": "3.2 Model Settings",
    "text": "We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews\n7The original dataset contains conversations to enrich the context of Weibo posts, which are not considered here.\n8https://radimrehurek.com/gensim/ utils.html\n9https://github.com/FudanNLP/fnlp 10https://radimrehurek.com/gensim/\nparsing/preprocessing.html\ndatasets, we use pre-trained GloVe embeddings (Pennington et al., 2014)11. For Twitter and Weibo datasets, we pre-train embeddings on largescale external data with 99M tweets and 467M Weibo messages, respectively. For the number of topics, we follow previous settings (Yan et al., 2013; Das et al., 2015; Dieng et al., 2016) to set K = 50. For all the other hyperparameters, we tune them on the development set by grid search. For our classifier, we employ CNN in experiment because of its better performance in short text classification than its counterparts such as RNN (Wang et al., 2017a). The hidden size of CNN is set as 500. The dimension of word embedding E = 200. γ = 0.8 for trading off θ and P , and λ = 1.0 for controlling the effects of topic model and classification. In the learning process, we run our model for 800 epochs with early-stop strategy applied (Caruana et al., 2000)."
  }, {
    "heading": "3.3 Comparison Models",
    "text": "For comparison, we consider a weak baseline of majority vote, which assigns the major class labels in training set to all test instances. We further compare with the widely-used baseline SVM+BOW, SVM with unigram features (Wang and Manning, 2012). We also consider other SVM-based baselines: SVM+LDA, SVM+BTM, SVM+NTM, whose features are topic distributions for instances learned by LDA (Blei et al., 2003), BTM (Yan et al., 2013), and NTM (Miao et al., 2017), respectively. In particular, BTM is one of the state-ofthe-art topic models for short texts. To compare with neural classifiers, we test bidirectional long\n11http://nlp.stanford.edu/data/glove. 6B.zip (200d)\nshort-term memory with attention (AttBiLSTM) (Zhang et al., 2015a) and convolutional neural network (CNN) classifiers (Kim, 2014). No topic representation is encoded in these two classifiers. We also compare with the state-of-the-art shorttext classifier CNN+TEWE (Ren et al., 2016), i.e., CNN classifier with topic-enriched word embeddings (TEWE), where the word embeddings are enriched by pre-trained NTM-inferred topic models. Moreover, to investigate the effectiveness of our proposed topic memory mechanism, we compare with CNN+NTM, which concatenates the representations learned by CNN and topics induced by NTM as classification features. In addition, we compare with our variant, TMN (Separate TM Inference), where topics are induced separately before classification, and only used for initializing the topic memory. To be consistent, our model with a joint learning process for topic modeling and classification, described in Section 2.3, is named as TMN (Joint TM Inference). Note that the comparison CNN-based models share the same settings as our model, and the hidden size for each direction of BiLSTM is set to 100."
  }, {
    "heading": "4 Experimental Results",
    "text": ""
  }, {
    "heading": "4.1 Classification Comparison",
    "text": "Table 3 shows the comparison on classification results, where the accuracy and average F1 scores on different classes labels are reported. We have the following observations.\n• Topic representations are indicative features. On all four datasets, simply by combining topic representations into features, SVM models produce better results than the models without ex-\nploiting topic features (i.e., SVM+BOW). This observation indicates that latent topic representations captured at corpus level are helpful to alleviate the data sparsity problem in short text classification.\n•Neural network models are effective. It is seen that neural models based on either CNN or AttBiLSTM yield better results than SVM. This observation shows the effectiveness of representation learning in neural networks for short texts.\n• CNN serves as a better classifier for short texts than AttBiLSTM. In comparison of CNN and AttBiLSTM without taking topic features, we observe that CNN yields generally better results on all the four datasets. This is consistent with the discovery in Wang et al. (2017a), where CNN can better encode short texts than sequential models.\n• Topic memory is useful to classification. By exploring topic representations in memory mechanisms, our TMN model, inferring topic models either separately or jointly with classification, significantly outperform the best comparison models on each of the four datasets. Particularly, when compared with CNN+TEWE and CNN+NTM, both concatenating topics as part of the features, the results yielded by TMN are better. This demonstrates the effectiveness of topic memory to learn indicative topic representations for short text classification.\n• Jointly inferring latent topics is effective to text classification. In comparison between two TMN variants, TMN (Joint TM Inference) produces better classification results, though large margin improvements are not observed on the three English datasets, i.e., TagMyNews, Snippets, and Twitter. This may be because the classifiers do not rely too much on high-quality latent topics, since other features may be sufficient to indicate the labels, e.g., word positions in the instance. As a result, better topic models, learned via jointly induced with classification, may not provide richer information for classification. Nevertheless, we notice that on\nNon-topic :::::: words are wave-underlined and in\nblue, and off-topic words are underlined and in red.\nChinese Weibo dataset, the jointly trained topic model improves the accuracy and average F1 by 2.3% and 2.0%, respectively. It may result from the prevalence of word order misuse in informal Weibo messages. This mis-order phenomenon is common in Chinese and generally does not affect understanding. The rich information conveyed by Chinese characters are capable of indicating semantic meanings of words even without correct orders (Qin et al., 2016; Wang et al., 2017b). As a result, the CNN classifier, which encodes orders of words, may also bring such mis-order noise to classification. For these instances with misordered words, a better topic model that learns text instances as unordered words, provides useful representations that compensate the loss of information in word orders and in turn improves the performance of text classification."
  }, {
    "heading": "4.2 Topic Coherence Comparison",
    "text": "In Section 4.1, we find that TMN can significantly outperform comparison models on short text classification. In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful topics. We use the CV metric (Röder et al., 2015) computed by Palmetto toolkit12 to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI (Bouma, 2009). Table 4 shows the comparison results of LDA, BTM, NTM, and TMN on the three English datasets.13 Note that we do not report CV scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics.\n12https://github.com/dice-group/ Palmetto\n13In the rest of this paper, without otherwise indicated, TMN is used as a short form for TMN (Joint TM Inference).\nAs can be seen, TMN yields higher CV scores by large margins than all others in comparison. This indicates that jointly exploring classification would be effective in producing coherent topics. The reason is that the supervision from classification labels can guide unsupervised topic models in discovering meaningful and interpretable topics. We also observe that NTM produces better results than LDA and BTM, which implies the effectiveness of inducing topic models by neural networks.\nTo further analyze the quality of yielded topics, Table 5 shows the top 10 words of the sample latent topics reflecting “Egyptian revolution of 2011” discovered by various models. We find that LDA yields off-topic word “bowl”. For the results of BTM and NTM, though we do not find off-topic words, non-topic words like “need” and “stay” are included.14 The topic generated by TMN appears to be the best, which presents indicative words like “tahrir” and “cairo”, for the event."
  }, {
    "heading": "4.3 Results with Varying Hyperparameters",
    "text": "We further study the impact of two important hyperparameters in TMN, i.e., the hop number and the topic number, which will be discussed in turn.\nImpact of Hop Numbers. Recall that Figure 2 shows the capacity of TMN in combining multiple hops. Here we analyze the effects of hop numbers on the accuracy of TMN. Table 6 reports the results, where NH refers to using N hops (N = 1, 2, ..., 6). As can be seen, generally, TMN with 5 hops achieves the best accuracy on most datasets except for Snippets dataset. We also observe that, although within a particular range, more hops can produce better accuracy, the increasing trends are not always monotonic. For example, TMN-6H always exhibits lower accuracy than TMN-5H. This observation implies that the overall representation ability of TMN is enhanced as the increasing complexity of the model via combining more hops.\n14Off-topic words are more likely to be interpreted to reflect other topics. Non-topic words cannot clearly indicate the corresponding topic.\nHowever, this enhancement will reach saturation when the hop number exceeds a threshold, which is 5 hops for most datasets in our experiment.\nImpact of Topic Numbers. Figure 3 shows the accuracy of TMN and CNN+TEWE (the best comparison model in Table 3) given varying K, the number of topics on TagMyNews and Twitter datasets.15 As we can see, the curves of all the models are not monotonic and the best accuracy is achieved given a particular number of topics, e.g., K=50 for TMN on TagMyNews dataset. When comparing different curves, we observe that TMN yields consistently better accuracy than CNN+TEWE, a comparison model shown in Table 3, which demonstrates the robust performance of TMN over varying number of topics."
  }, {
    "heading": "4.4 A Case Study on Topic Memory",
    "text": "Section 4.1 demonstrates the effectiveness of using topic memory on short text classification. To further understand why, in this section, we use the test instance S in Table 1 to analyze what the information captured by topic memory is indicative of class labels. Recall that the label of S, which should be New.Music.Live, can be indicated by containing word “wristbands” and the collocation of “wristbands” and “Bieber” in training instance R2 labeled New.Music.Live. Figure 4 shows the heatmaps of the weight matrix P in topic memory and the topic mixture θ captured by NTM for instance S. As can be seen, the top 3 words for the latent topic with the largest value in θ are “bieber”, “justine”, and “tuesday”, which can effectively indicate the class label of S to be New.Music.Live because Justine Bieber was there on Tuesday. Interestingly, S contains none of the top three words. The latent semantic relations of S and these words are purely uncovered by the cooccurrence of words in S with other instances in\n15We observe similar distributions on Snippets and Weibo.\nthe corpus, which further shows the benefit of using latent topics for alleviating the sparsity in short texts. We also observe that topic memory learns different representations for topical word “wristband”, highly indicating instance label, and background words, such as “i” and “for”. This explains why topic memory is effective to classification."
  }, {
    "heading": "4.5 Error Analysis",
    "text": "In this section, we take our classification results on TagMyNews dataset as an example to analyze our errors. We observe that one major type of incorrect prediction should be ascribed to the polysemy phenomenon. For example, the instance “NBC gives ‘the voice’ post super bowl slot” should be categorized as entertainment. However, failing to understand the particular meaning of “the voice” here as the name of a television singing competition, our model mistakenly categorizes this instance as sport because of the occurrence “super bowl”. In future work, we would exploit context-sensitive topical word embeddings (Witt et al., 2016), which is able to distinguish the meanings of the same word in different contexts. Another main error type comes from the failure to capture phrase-level semantics. Taking “On the merits of face time and living small” as an example, without understanding “face time” as a phrase, our model wrongly predicts its category as business instead of its correct label as sci tech. Such errors can be reduced by enhancing our NTM to phrase discovery topic models (Lindsey et al., 2012; He, 2016), which is worthy exploring in future work."
  }, {
    "heading": "5 Related Work",
    "text": "Our work mainly builds on two streams of prior work: short text classification and topic models.\nShort Text Classification. In the line of short text classification, most work focuses on alleviating the severe sparsity issues in short texts (Yan et al., 2013). Some previous efforts encode knowledge from external resource (Jin et al., 2011; Lucia and Ferrari, 2014; Wang et al., 2017a; Ma et al., 2018). Instead, our work learns effective representations only from internal data. For some specific classification tasks, such as sentiment analysis, manually-crafted features are designed to fit the target task (Pak and Paroubek, 2010; Jiang et al., 2011). Distinguished from them, we employ deep learning framework for representation learning, which requires no feature engineering process and thus ensures its general applicability to diverse classification scenarios. In comparison with the established classifiers applying deep learning methods (dos Santos and Gatti, 2014; Lee and Dernoncourt, 2016), our work differs from them in the leverage of corpus-level latent topic representations for alleviating data sparsity issues. In existing classification models using topic features, pre-trained topic mixtures are leveraged as part of features (Phan et al., 2008; Ren et al., 2016; Chen et al., 2017). Differently, our model encodes topic representations in a memory mechanism where topics are induced jointly with text classification in an end-to-end manner.\nTopic Models. Well-known topic models, e.g., probabilistic latent semantic analysis (pLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003), have shown advantages in capturing effective semantic representations, and proven beneficial to varying downstream applications, such as summarization (Haghighi and Vanderwende, 2009) and recommendation (Zeng et al., 2018; Bai et al., 2018). For short text data, topic model variants have been proposed to reduce the effects of sparsity issues on topic modeling, such as biterm topic model (BTM) (Yan et al., 2013) and LeadLDA (Li et al., 2016b). Recently, owing to the popularity of variational auto-encoder (VAE) (Kingma and Welling, 2013), it is able to induce latent topics in neural networks, namely, neural topic models (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017). Although the concept of NTM has been mentioned earlier in Cao et al. (2015), their model is based on matrix factorization. Differently, VAE-style NTM (Srivastava and Sutton, 2017; Miao et al., 2017) follows the LDA fashion as probabilistic generative models, which is easy to interpret and extend. The NTM in our framework is in VAE-style, whose effects on short text classification serve as the key focus of our work."
  }, {
    "heading": "6 Conclusion",
    "text": "We have presented topic memory networks that exploit corpus-level topic representations with a topic memory mechanism for short text classification. The model alleviates data sparsity issues via jointly learning latent topics and text categories. Empirical comparisons with state-of-the-art models on four benchmark datasets have demonstrated the validity and effectiveness of our model, where better results have been achieved on both short text classification and topic coherence evaluation."
  }, {
    "heading": "Acknowledgements",
    "text": "This work is partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 and No. CUHK 14234416 of the General Research Fund), and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award). We thank Shuming Shi, Dong Yu, Tong Zhang, Baolin Peng, Haoli Bai, and the three anonymous reviewers for the insightful suggestions on various aspects of this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural Relational Topic Model for Scientific Article Analysis",
    "authors": ["Haoli Bai", "Zhuangbin Chen", "Michael R. Lyu", "Irwin King", "Zenglin Xu."],
    "venue": "Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM",
    "year": 2018
  }, {
    "title": "Variational Inference: A Review for Statisticians",
    "authors": ["David M. Blei", "Alp Kucukelbir", "Jon D. McAuliffe."],
    "venue": "CoRR, abs/1601.00670.",
    "year": 2016
  }, {
    "title": "Latent Dirichlet Allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Normalized (Pointwise) Mutual Information in Collocation Extraction",
    "authors": ["Gerlof Bouma."],
    "venue": "Proceedings of GSCL, pages 31–40.",
    "year": 2009
  }, {
    "title": "A Novel Neural Topic Model and Its Supervised Extension",
    "authors": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji."],
    "venue": "Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence, Austin, Texas, USA, pages 2210–2216.",
    "year": 2015
  }, {
    "title": "Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, an”d Early Stopping",
    "authors": ["Rich Caruana", "Steve Lawrence", "C. Lee Giles."],
    "venue": "Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems,",
    "year": 2000
  }, {
    "title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis",
    "authors": ["Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,",
    "year": 2017
  }, {
    "title": "Gaussian LDA for Topic Models with Word Embeddings",
    "authors": ["Rajarshi Das", "Manzil Zaheer", "Chris Dyer."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-",
    "year": 2015
  }, {
    "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency",
    "authors": ["Adji B. Dieng", "Chong Wang", "Jianfeng Gao", "John William Paisley."],
    "venue": "CoRR, abs/1611.01702.",
    "year": 2016
  }, {
    "title": "Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network",
    "authors": ["Zi-Yi Dou."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen,",
    "year": 2017
  }, {
    "title": "Neural Turing Machines",
    "authors": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."],
    "venue": "CoRR, abs/1410.5401.",
    "year": 2014
  }, {
    "title": "Exploring content models for multi-document summarization",
    "authors": ["Aria Haghighi", "Lucy Vanderwende."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computa-",
    "year": 2009
  }, {
    "title": "Extracting Topical Phrases from Clinical Documents",
    "authors": ["Yulan He."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, Phoenix, Arizona, USA, pages 2957–2963.",
    "year": 2016
  }, {
    "title": "Probabilistic Latent Semantic Indexing",
    "authors": ["Thomas Hofmann."],
    "venue": "Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’99, Berkeley, CA, USA, pages 50–57.",
    "year": 1999
  }, {
    "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset",
    "authors": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, pages",
    "year": 2015
  }, {
    "title": "Target-dependent Twitter Sentiment Classification",
    "authors": ["Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao."],
    "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings",
    "year": 2011
  }, {
    "title": "Transferring Topical Knowledge from Auxiliary Long Texts for Short Text Clustering",
    "authors": ["Ou Jin", "Nathan Nan Liu", "Kai Zhao", "Yong Yu", "Qiang Yang."],
    "venue": "Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011,",
    "year": 2011
  }, {
    "title": "Bag of Tricks for Efficient Text Classification",
    "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa-",
    "year": 2017
  }, {
    "title": "Convolutional Neural Networks for Sentence Classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, A meeting of SIGDAT, a Special Interest Group of the ACL, Doha,",
    "year": 2014
  }, {
    "title": "AutoEncoding Variational Bayes",
    "authors": ["Diederik P. Kingma", "Max Welling."],
    "venue": "CoRR, abs/1312.6114.",
    "year": 2013
  }, {
    "title": "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks",
    "authors": ["Ji Young Lee", "Franck Dernoncourt."],
    "venue": "The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2016
  }, {
    "title": "Topic Modeling for Short Texts with Auxiliary Word Embeddings",
    "authors": ["Chenliang Li", "Haoran Wang", "Zhiqian Zhang", "Aixin Sun", "Zongyang Ma."],
    "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in In-",
    "year": 2016
  }, {
    "title": "Topic Extraction from Microblog Posts Using Conversation Structures",
    "authors": ["Jing Li", "Ming Liao", "Wei Gao", "Yulan He", "KamFai Wong."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Volume",
    "year": 2016
  }, {
    "title": "A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes",
    "authors": ["Robert V. Lindsey", "William Headden", "Michael Stipicevic."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-",
    "year": 2012
  }, {
    "title": "Adversarial Multi-task Learning for Text Classification",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Volume 1: Long Papers, Vancouver, Canada,",
    "year": 2017
  }, {
    "title": "EgoCentric: Ego Networks for Knowledge-based Short Text Classification",
    "authors": ["William Lucia", "Elena Ferrari."],
    "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014,",
    "year": 2014
  }, {
    "title": "Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM",
    "authors": ["Yukun Ma", "Haiyun Peng", "Erik Cambria."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans,",
    "year": 2018
  }, {
    "title": "Discovering Discrete Latent Topics with Neural Variational Inference",
    "authors": ["Yishu Miao", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, pages",
    "year": 2017
  }, {
    "title": "Neural Variational Inference for Text Processing",
    "authors": ["Yishu Miao", "Lei Yu", "Phil Blunsom."],
    "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, pages 1727–1736.",
    "year": 2016
  }, {
    "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
    "authors": ["Vinod Nair", "Geoffrey E. Hinton."],
    "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), Haifa, Israel, pages 807–814.",
    "year": 2010
  }, {
    "title": "Glove: Global Vectors",
    "authors": ["pher D. Manning"],
    "year": 2014
  }, {
    "title": "Improving Twitter Senti",
    "authors": ["Donghong Ji"],
    "year": 2016
  }, {
    "title": "End-To-End Memory Networks",
    "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, NIPS 2015, Montreal,",
    "year": 2015
  }, {
    "title": "Classification of Short Texts by Deploying Topical Annotations",
    "authors": ["Daniele Vitale", "Paolo Ferragina", "Ugo Scaiella."],
    "venue": "Advances in Information Retrieval - 34th European Conference on IR Research, ECIR 2012, Barcelona, Spain, pages 376–",
    "year": 2012
  }, {
    "title": "Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification",
    "authors": ["Jin Wang", "Zhongyuan Wang", "Dawei Zhang", "Jun Yan."],
    "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJ-",
    "year": 2017
  }, {
    "title": "Exploiting Word Internal Structures for Generic Chinese Sentence Representation",
    "authors": ["Shaonan Wang", "Jiajun Zhang", "Chengqing Zong."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,",
    "year": 2017
  }, {
    "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification",
    "authors": ["Sida I. Wang", "Christopher D. Manning."],
    "venue": "The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, ACL 2012, Volume 2:",
    "year": 2012
  }, {
    "title": "Memory Networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "CoRR, abs/1410.3916.",
    "year": 2014
  }, {
    "title": "Explaining Topical Distances Using Word Embeddings",
    "authors": ["Nils Witt", "Christin Seifert", "Michael Granitzer."],
    "venue": "27th International Workshop on Database and Expert Systems Applications, DEXA 2016 Workshops, Porto, Portugal, pages 212–217.",
    "year": 2016
  }, {
    "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers",
    "authors": ["Yijun Xiao", "Kyunghyun Cho."],
    "venue": "CoRR, abs/1602.00367.",
    "year": 2016
  }, {
    "title": "A Biterm Topic Model for Short Sexts",
    "authors": ["Xiaohui Yan", "Jiafeng Guo", "Yanyan Lan", "Xueqi Cheng."],
    "venue": "22nd International World Wide Web Conference, WWW 2013, Rio de Janeiro, Brazil, pages 1445–1456.",
    "year": 2013
  }, {
    "title": "Microblog conversation recommendation via joint modeling of topics and discourse",
    "authors": ["Xingshan Zeng", "Jing Li", "Lu Wang", "Nicholas Beauchamp", "Sarah Shugars", "Kam-Fai Wong."],
    "venue": "Proceedings of the 2018 Conference of the North American",
    "year": 2018
  }, {
    "title": "Relation Classification via Recurrent Neural Network",
    "authors": ["Dongxu Zhang", "Dong Wang."],
    "venue": "CoRR, abs/1508.01006.",
    "year": 2015
  }, {
    "title": "Bidirectional Long Short-Term Memory Networks for Relation Classification",
    "authors": ["Shu Zhang", "Dequan Zheng", "Xinchen Hu", "Ming Yang."],
    "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, PACLIC 29,",
    "year": 2015
  }, {
    "title": "Advertising Keywords Recommendation for Short-Text Web Pages Using Wikipedia",
    "authors": ["Weinan Zhang", "Dingquan Wang", "Gui-Rong Xue", "Hongyuan Zha."],
    "venue": "ACM TIST, 3(2):36:1–36:25.",
    "year": 2012
  }, {
    "title": "Character-level Convolutional Networks for Text Classification",
    "authors": ["Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, NIPS 2015,",
    "year": 2015
  }, {
    "title": "Encoding conversation context for neural keyphrase extraction from microblog posts",
    "authors": ["Yingyi Zhang", "Jing Li", "Yan Song", "Chengzhi Zhang."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
    "year": 2018
  }],
  "id": "SP:c0507e8d32d408c99af4f2bb0e658d06efa0fd48",
  "authors": [{
    "name": "Jichuan Zeng",
    "affiliations": []
  }, {
    "name": "Jing Li",
    "affiliations": []
  }, {
    "name": "Yan Song",
    "affiliations": []
  }, {
    "name": "Cuiyun Gao",
    "affiliations": []
  }, {
    "name": "Michael R. Lyu",
    "affiliations": []
  }, {
    "name": "Irwin King",
    "affiliations": []
  }],
  "abstractText": "Many classification models work poorly on short texts due to data sparsity. To address this issue, we propose topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels. Different from most prior work that focuses on extending features with external knowledge or pre-trained topics, our model jointly explores topic inference and text classification with memory networks in an end-to-end manner. Experimental results on four benchmark datasets show that our model outperforms state-of-the-art models on short text classification, meanwhile generates coherent topics.",
  "title": "Topic Memory Networks for Short Text Classification"
}