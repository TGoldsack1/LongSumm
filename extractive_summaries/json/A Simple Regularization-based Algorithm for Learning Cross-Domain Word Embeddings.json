{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Recently, the learning of distributed representations for natural language words (or word embeddings) has received a significant amount of attention (Mnih and Hinton, 2007; Turian et al., 2010; Mikolov et al., 2013a,b,c; Pennington et al., 2014). Such representations were shown to be able to capture syntactic and semantic level information associated with words (Mikolov et al., 2013a). Word embeddings were shown effective in tasks such as named entity recognition (Sienčnik, 2015), sentiment analysis (Li and Lu, 2017) and syntactic parsing (Durrett and Klein, 2015). One common assumption made by most of the embedding methods is that, the text corpus is from one single domain; e.g., articles from bioinformatics. However, in practice, there are often text corpora from multiple domains; e.g., we may have text collections from broadcast news or Web blogs, whose words are not necessarily limited to bioinformatics. Can these corpora from different domains help\nlearn better word embeddings, so as to improve the downstream NLP applications in a target domain like bioinformatics? Our answer is yes, because despite the domain differences, these additional domains do introduce more text data converying useful information (i.e., more words, more word co-occurrences), which can be helpful for consolidating the word embeddings in the target bioinformatics domain.\nIn this paper, we propose a simple and easyto-implement approach for learning cross-domain word embeddings. Our model can be seen as a regularized skip-gram model (Mikolov et al., 2013a,b), where the source domain information is selectively incorporated for learning the target domain word embeddings in a principled manner."
  }, {
    "heading": "2 Related Work",
    "text": "Learning a continuous representation for words has been studied for quite a while (Hinton et al., 1986). Many earlier word embedding methods employed the computationally expensive neural network architectures (Collobert and Weston, 2008; Mikolov et al., 2013c). Recently, an efficient method for learning word representations, namely the skip-gram model (Mikolov et al., 2013a,b) was proposed and implemented in the widely used word2vec toolkit. It tries to use the current word to predict the surrounding context words, where the prediction is defined over the embeddings of these words. As a result, it learns the word embeddings by maximizing the likelihood of predictions.\nDomain adaptation is an important research topic (Pan et al., 2013), and it has been considered in many NLP tasks. For example, domain adaptation is studied for sentiment classification (Glorot et al., 2011) and parsing (McClosky et al., 2010), just to name a few. However, there is very\nar X\niv :1\n90 2.\n00 18\n4v 1\n[ cs\n.C L\n] 1\nF eb\n2 01\n9\nlittle work on domain adaptation for word embedding learning. One major reason preventing people from using text corpora from different domains for word embedding learning is the lack of guidance on which kind of information is worth learning from the source domain(s) for the target domain. In order to address this problem, some pioneering work has looked into this problem. For example, Bollegala et al. (2015) considered those frequent words in the source domain and the target domain as the “pivots”. Then it tried to use the pivots to predict the surrounding “non-pivots”, meanwhile ensuring the pivots to have the same embedding across two domains. Embeddings learned from such an approach were shown to be able to improve the performance on a cross-domain sentiment classification task. However, this model fails to learn embeddings for many words which are neither pivots nor non-pivots, which could be crucial for some downstream tasks such as named entity recognition."
  }, {
    "heading": "3 Our Approach",
    "text": "Let us first state the objective of the skip-gram model (Mikolov et al., 2013a) as follows:\nLD = ∑\n(w,c)∈D\n#(w, c) ( log σ(w · c)\n+ k∑ i=1 Ec′i∼P (w)[log σ(−w · c ′ i)] ) (1)\nwhere D refers to the complete text corpus from which we learn the word embeddings. The word w is the current word, c is the context word, and #(w, c) is the number of times they co-occur in D. We use w and c to denote the vector representations forw and c, respectively. The function σ(·) is the sigmoid function. The word c′i is a “negative sample” sampled from the distribution P (w) – typically chosen as the unigram distribution U(w) raised to the 3/4rd power (Mikolov et al., 2013b).\nIn our approach, we first learn for each word w an embedding ws from the source domain Ds. Next we learn the target domain embeddings as follows:\nL′Dt = LDt − ∑\nw∈Dt∩Ds\nαw · ||wt −ws||2 (2)\nwhereDt refers to the target domain, and wt is the target domain representation for w. Such an regu-\nlarized objective can still be optimized using standard stochastic gradient descent. Note that in the above formula, the regularization term only considers words that appear in both source and target domain, ignoring words that only appear in either the source or the target domain only.\nOur approach is inspired by the recent regularization-based domain adaptation framework (Lu et al., 2016). Here, αw measures the amount of transfer across the two domains when learning the representation for word w. If it is large, it means we require the embeddings of word w in the two domains to be similar. We define αw as follows:\nαw = σ(λ · φ(w)) (3)\nwhere λ is a hyper-parameter to decide the scaling factor of the significance function φ(·), which allows the user to control the degree of “knowledge transfer” from source domain to target domain.\nHow do we define the significance function φ(w) that controls the amount of transfer for the word w? We first define the frequency of the word w in the dataset D as fD(w), the number of times the word w appears in the domain D. Based on this we can define the normalized frequency for the word w as follows:\nFD(w) = fD(w)\nmaxw′∈Dk fD(w ′)\n(4)\nwhere Dk ⊂ D consists of all except for the top k most frequent words from D1.\nWe define the function φ(·) based on the following metric that is motivated by the well-known Sørensen-Dice coefficient (Sørensen, 1948; Dice, 1945) commonly used for measuring similarities:\nφ(w) = 2 · FDs(w) · FDt(w) FDs(w) + FDt(w)\n(5)\nWhy does such a definition make sense? We note that the value of φ(w) would be high only if both both FDs(w) and FDt(w) are high – in this case the word w is a frequent word across different domains. Intuitively, these are likely those words whose semantics do not change across the two domains, and we should be confident about making their embeddings similar in the two domains. On the other hand, domain-specific words\n1In all our experiments, we empirically set k to 20.\ntend to be more frequent in one domain than the other. In this case, the resulting φ(w) will also have a lower score, indicating a smaller amount of transfer across the two domains. While other userdefined significance functions are also possible, in this work we simply adopt such a function based on the above simple observations. We will validate our assumptions with experiments in the next section."
  }, {
    "heading": "4 Experiments",
    "text": "We present extensive evaluations to assess the effectiveness of our approach. Following recent advice by Nayak et al. (2016) and Faruqui et al. (2016), to assess the quality of the learned word embeddings, we considered employing the learned word embeddings as continuous features in several down-stream NLP tasks, including entity recognition, sentiment classification, and targeted sentiment analysis.\nWe have used various datasets from different domains for learning cross-domain word embeddings under different tasks. We list the data statistics in Table 1."
  }, {
    "heading": "4.1 Baseline Methods",
    "text": "We consider the following baseline methods when assessing the effectiveness of our approach.\n• DISCRETE: only discrete features (such as bag of words, POS tags, word n-grams and POS tag n-grams, depending on the actual down-stream task) were considered. All following systems include both these base features and the respective additional features.\n• SOURCE: we train word embeddings from the source domain as additional features.\n• TARGET: we train word embeddings from the target domain as additional features.\n• ALL: we combined the data from two domains to form a single dataset for learning word embeddings as additional features.\n• CONCAT: we simply concatenate the learned embeddings from both source and target domains as additional features.\n• DARep: we use the previous approach of Bollegala et al. (2015) for learning crosslingual word representations as additional features."
  }, {
    "heading": "4.2 Entity Recognition",
    "text": "Our first experiment was conducted on entity recognition (Tjong Kim Sang and De Meulder, 2003; Florian et al., 2004), where the task is to extract semantically meaning entities and their mentions from the text.\nFor this task, we built a standard entity recognition model using conditional random fields (Lafferty et al., 2001). We used the standard features which are commonly used for different methods, including word unigrams and bigrams, bagof-words features, POS tag window features, POS tag unigrams and bigram features. We conducted two sets of experiments on two different datasets. The first dataset is the GENIA dataset (Ohta et al., 2002), a popular dataset used in bioinformatics, and the second is the ACE-2005 dataset (Walker et al., 2006), which is a standard dataset used for various information extraction tasks.\nFor the GENIA dataset which consists of 10,946 sentences, we used Enwik9 as the source domain and PubMed as the target domain for learning word embeddings. We set the dimension of word representations as 50.\nFor the experiments on ACE, we selected the BN subset of ACE2005, which consists of 4,460 CNN headline news and share a similar domain with Gigaword. We used Enwik9 as the source domain and Gigaword as the target domain. We followed a procedure similar to GENIA for experiments.\nTo tune our hyperparameter λ, we first split the last 10% of the training set as the development\nportion. We then trained a model using the remaining 90% as the training portion and used the development portion for development of the hyperparameter λ. After development, we re-trained the models using the original training set2.\nWe report the results in Table 2. From the results we can observe that the embeddings learned using our algorithm can lead to improved performance when used in this particular down-stream NLP task. We note that in such a task, many entities consist of domain-specific terms, therefore learning good representations for such words can be crucial. As we have discussed earlier, our regularization method enables our model to differentiate domain-specific words from words which are more general in the learning process. We believe this mechanism can lead to improved learning of representations for both types of words."
  }, {
    "heading": "4.3 Sentiment Classification",
    "text": "The second task we consider is sentiment classification, which is essentially a text classification task, where the goal is to assign each text document a class label indicating its sentiment polarity (Pang et al., 2002; Liu, 2012).\nThis is also the only task presented in the previous DARep work by Bollegala et al. (2015). As such, we largely followed Bollegala et al. (2015) for experiments. Instead of using the dataset they used which only consists of 2,000 reviews, we considered two much larger datasets – IMDB and Yelp 2014 – for such a task, which was previously used in a sentiment classification task (Tang et al., 2015). IMDB dataset (Diao et al., 2014) is crawled from the movie review site IMDB3 which consists of 84,919 reviews. Yelp 2014 dataset consists\n2We selected the optimal value for the hyper-parameter λ from the set λ ∈ {0.1, 1, 5, 10, 20, 30, 50} for all experiments in this paper.\n3http://www.imdb.com\nof 231,163 online reviews provided by the Yelp Dataset Challenge4.\nFollowing Bollegala et al. (2015), for this task we simply learned the word embeddings from the training portion of the review datasets themselves only. No external data was used for learning word embeddings. As Bollegala et al. (2015) only evaluated on a small dataset in their paper for such a task, to understand the effect of varying the amount of training data, we also tried to train our model on datasets with different sizes. We conducted two sets of experiments: we first used the Yelp dataset as the source domain and IMDB as the target domain, and then we switched these two datasets in our second set of experiments. Figure 1 shows the F1 measures for different word embeddings when different amounts of training data were used. We also compared with the previous approach for domain adaptation (Lu et al., 2016) which only employs discrete features. We can observe that when the dataset becomes large, our learned word embeddings are shown to be more effective than all other approaches. When the complete training set is used, our model significantly outperforms DARep (p < 0.05 for both directions with bootstrap resampling test (Koehn, 2004)). DARep appears to be effective when the training dataset is small. However, as the training set size increases, there is no significant improvement for such an approach. As we can also observe from the figure, our approach consistently gives better results than baseline approaches (except for the second experiment when 20% of the data was used). Furthermore, when the amount of training data increases, the differences between our approach and other approaches generally become larger.\nSuch experiments show that our model works\n4https://www.yelp.com/dataset challenge\nwell when different amounts of data are available, and our approach appears to be more competitive when a large amount of data is available."
  }, {
    "heading": "4.4 Targeted Sentiment Analysis",
    "text": "We also conducted experiments on targeted sentiment analysis (Mitchell et al., 2013) – the task of jointly recognizing entities and their sentiment information. We used the state-of-the-art system for targeted sentiment analysis by Li and Lu (2017) whose code is publicly available 5, and used the data from (Mitchell et al., 2013) which consists of 7,105 Spanish tweets and 2,350 English tweets, with named entities and their sentiment information annotated. Note that the model of Li and Lu (2017) is a structured prediction model that involves latent variables. The experiments here therefore allow us to assess the effectiveness of our approach on such a setup involving latent variables. We follow Li and Lu (2017) and report precision (P.), recall (R.) and F1-measure (F1) for such a targeted sentiment analysis task, where the prediction is regarded as correct if and only if both the entity’s boundary and its sentiment information are correct. Also, unlike previous experiments, which are conducted on English only, these experiments additionally allow us to assess our approach’s effectiveness when a different language other than English is considered.\nFor the English task, we used Enwik9 as the source domain for learning word embeddings, and our crawled English tweets as the target domain. For the Spanish task, we used Eswiki as the source domain, and we also crawled Spanish tweets as the target domain. See Table 1 for the statistics. Similar to the experiments conducted for entity recognition, we split the first 80% of the data for training, the next 10% for development and the last 10% for evaluation. We tuned the hyper-parameter λ using the development set and re-trained the embeddings on the dataset combining the training and\n5Available at http://statnlp.org/research/st/.\nthe development set, which are then used in final evaluations. Results are reported in Table 3, which show our approach is able to achieve the best results across two datasets in such a task, and outperforms DARep (p < 0.05). Interestingly, the concatenation approach appears to be competitive in this task, especially for the Spanish dataset, which appears to be better than the DARep approach. However, we note such an approach does not capture any information transfer across different domains in the learning process. In contrast, our approach learns embeddings for the target domain by capturing useful cross-domain information and therefore can lead to improved modeling of embeddings that are shown more helpful for this specific down-stream task."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "In this paper, we presented a simple yet effective algorithm for learning cross-domain word embeddings. Motivated by the recent regularizationbased domain adaptation framework (Lu et al., 2016), the algorithm performs learning by augmenting the skip-gram objective with a simple regularization term. Our work can be easily extended to multi-domain scenarios. The method is also flexible, allowing different user-defined metrics to be incorporated for defining the function controlling the amount of domain transfer.\nFuture work includes performing further investigations to better understand and to visualize what types of information has been transferred across domains and how such information influence different types of down-stream NLP tasks. It is also important to understand how such an approach will work on other types of models such as neural networks based NLP models. Our code is available at http://statnlp.org/research/lr/."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank all the reviewers for their useful feedback to the earlier draft of this paper. This work was done when the first author was visiting Singapore University of Technology and Design. We thank the support of Human-centered Cyber-physical Systems Programme at Advanced Digital Sciences Center from Singapores Agency for Science, Technology and Research (A*STAR). This work is supported by MOE Tier 1 grant SUTDT12015008."
  }],
  "year": 2019,
  "references": [{
    "title": "Unsupervised cross-domain word representation learning",
    "authors": ["Danushka Bollegala", "Takanori Maehara", "Ken ichi Kawarabayashi."],
    "venue": "Proc. of ACLIJCNLP, pages 730–740.",
    "year": 2015
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proc. of ICML, pages 160–167. ACM.",
    "year": 2008
  }, {
    "title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)",
    "authors": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."],
    "venue": "Proc. of KDD, pages 193–202.",
    "year": 2014
  }, {
    "title": "Measures of the amount of ecologic association between species",
    "authors": ["Lee R Dice."],
    "venue": "Ecology, 26(3):297– 302.",
    "year": 1945
  }, {
    "title": "Neural crf parsing",
    "authors": ["Greg Durrett", "Dan Klein."],
    "venue": "arXiv preprint arXiv:1507.03641.",
    "year": 2015
  }, {
    "title": "Problems with evaluation of word embeddings using word similarity tasks",
    "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer."],
    "venue": "arXiv preprint arXiv:1605.02276.",
    "year": 2016
  }, {
    "title": "A statistical model for multilingual entity detection and tracking",
    "authors": ["R. Florian", "H. Hassan", "A. Ittycheriah", "H. Jing", "N. Kambhatla", "X. Luo", "N. Nicolov", "S. Roukos."],
    "venue": "Proc. of NAACL/HLT, pages 1–8.",
    "year": 2004
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proc. of ICML, pages 513–520.",
    "year": 2011
  }, {
    "title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol",
    "authors": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart."],
    "venue": "1. chapter Distributed Representations, pages 77–109. MIT Press, Cambridge, MA, USA.",
    "year": 1986
  }, {
    "title": "Statistical significance tests for machine translation evaluation",
    "authors": ["Philipp Koehn."],
    "venue": "Proc. of EMNLP, pages 388–395.",
    "year": 2004
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"],
    "venue": "In Proc. of ICML,",
    "year": 2001
  }, {
    "title": "Learning latent sentiment scopes for entity-level sentiment analysis",
    "authors": ["Hao Li", "Wei Lu."],
    "venue": "Proc. of AAAI, pages 3482–3489.",
    "year": 2017
  }, {
    "title": "Sentiment analysis and opinion mining",
    "authors": ["Bing Liu."],
    "venue": "Synthesis lectures on human language technologies, 5(1).",
    "year": 2012
  }, {
    "title": "A general regularization framework for domain adaptation",
    "authors": ["Wei Lu", "Hai Leong Chieu", "Jonathan Löfgren."],
    "venue": "Proc. of EMNLP, pages 950– 954.",
    "year": 2016
  }, {
    "title": "Automatic domain adaptation for parsing",
    "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson."],
    "venue": "Proc. of NAACL-HLT, pages 28–36.",
    "year": 2010
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Proc. of NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."],
    "venue": "Proc. of NAACL-HLT, pages 746–751.",
    "year": 2013
  }, {
    "title": "Open domain targeted sentiment",
    "authors": ["Margaret Mitchell", "Jacqueline Aguilar", "Theresa Wilson", "Benjamin Van Durme."],
    "venue": "Proc. of EMNLP, pages 1643–1654.",
    "year": 2013
  }, {
    "title": "Three new graphical models for statistical language modelling",
    "authors": ["Andriy Mnih", "Geoffrey Hinton."],
    "venue": "Proc. of ICML, pages 641–648.",
    "year": 2007
  }, {
    "title": "Evaluating word embeddings using a representative suite of practical tasks",
    "authors": ["Neha Nayak", "Gabor Angeli", "Christopher D Manning."],
    "venue": "ACL 2016, page 19.",
    "year": 2016
  }, {
    "title": "The genia corpus: An annotated research abstract corpus in molecular biology domain",
    "authors": ["Tomoko Ohta", "Yuka Tateisi", "Jin-Dong Kim."],
    "venue": "Proc. of the second international conference on Human Language Technology Research, pages 82–86.",
    "year": 2002
  }, {
    "title": "Transfer joint embedding for cross-domain named entity recognition",
    "authors": ["Sinno Jialin Pan", "Zhiqiang Toh", "Jian Su."],
    "venue": "ACM Transactions on Information Systems (TOIS), 31(2):7.",
    "year": 2013
  }, {
    "title": "Thumbs up?: sentiment classification using machine learning techniques",
    "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."],
    "venue": "Proc. of EMNLP, pages 79–86.",
    "year": 2002
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "Proc. of EMNLP, volume 14, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Adapting word2vec to named entity recognition",
    "authors": ["Scharolta Katharina Sienčnik."],
    "venue": "Proc. of NODALIDA, 109, pages 239–243. Linköping University Electronic Press.",
    "year": 2015
  }, {
    "title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on danish commons",
    "authors": ["Thorvald Sørensen."],
    "venue": "Biol. Skr., 5:1–34.",
    "year": 1948
  }, {
    "title": "Learning semantic representations of users and products for document level sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proc. of ACL-IJCNLP, pages 1014–1023.",
    "year": 2015
  }, {
    "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F Tjong Kim Sang", "Fien De Meulder."],
    "venue": "Proc. of HLT-NAACL, pages 142–147.",
    "year": 2003
  }, {
    "title": "Ace 2005 multilingual training corpus",
    "authors": ["Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda."],
    "venue": "Linguistic Data Consortium,",
    "year": 2006
  }],
  "id": "SP:d70df38462a8effa02fd8c8cfb94da8f82fcbcad",
  "authors": [{
    "name": "Wei Yang",
    "affiliations": []
  }, {
    "name": "Wei Lu",
    "affiliations": []
  }, {
    "name": "Vincent W. Zheng",
    "affiliations": []
  }],
  "abstractText": "Learning word embeddings has received a significant amount of attention recently. Often, word embeddings are learned in an unsupervised manner from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting embeddings. How to effectively train word embedding models using data from different domains remains a problem that is underexplored. In this paper, we present a simple yet effective method for learning word embeddings based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.",
  "title": "A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings"
}