{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 962–967 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n962"
  }, {
    "heading": "1 Introduction",
    "text": "Discourse segmentation, which divides text into proper discourse units, is one of the fundamental tasks in natural language processing. According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a complex text is composed of non-overlapping Elementary Discourse Units (EDUs), as shown in Table 1. Segmenting text into such discourse units is a key step in discourse analysis (Marcu, 2000) and can benefit many downstream tasks, such as sentence compression (Sporleder and Lapata, 2005) or document summarization (Li et al., 2016).\nSince EDUs are initially designed to be determined with lexical and syntactic clues (Carlson et al., 2001), existing methods for discourse segmentation usually design hand-crafted features to capture these clues (Feng and Hirst, 2014). Especially, nearly all previous methods rely on syntactic parse trees to achieve good performance.\n1Our code is available at https://github.com/ PKU-TANGENT/NeuralEDUSeg\nBut extracting such features usually takes a long time, which contradicts the fundamental role of discourse segmentation and hinders its actual use. Considering the great success of deep learning on many NLP tasks (Lu and Li, 2016), it’s a natural idea for us to design an end-to-end neural model that can segment texts fast and accurately.\nThe first challenge of applying neural methods to discourse segmentation is data insufficiency. Due to the limited size of labeled data in existing corpus (Carlson et al., 2001), it’s quite hard to train a data-hungry neural model without any prior knowledge. In fact, some traditional features, such as the POS tags or parse trees, naturally provide strong signals for identifying EDUs. Removing them definitely increases the difficulty of learning an accurate model. Secondly, many EDU boundaries are actually not determined locally. For example, to recognize the boundary between e3 and e4 in Table 1, our model has to be aware that e3 is an embedded clauses starting from “overlooking”, otherwise it could regard “San Fernando Valley” as the subject of e4. Such kind of long-distance dependency can be precisely extracted from parse trees but is difficult for neural models to capture.\nTo address these challenges, in this paper, we propose a neural discourse segmenter based on the BiLSTM-CRF (Huang et al., 2015) framework and further improve it from two aspects. Firstly, since the discourse segmentation corpus is too small to learn precise word representations, we transfer a word representation model trained on a large corpus into our task, and show that this trans-\nferred model can provide very useful information for our task. Secondly, in order to model longdistance dependency, we employ the self-attention mechanism (Vaswani et al., 2017) when encoding the text. Different from previous self-attention, we restrict the attention area to a neighborhood of fixed size. The motivation is that effective information for determining the boundaries is usually collected from adjacent EDUs, while the whole text may contain many disturbing words, which could mislead the model into incorrect decisions. In summary, the contributions of this work are as follows:\n• Our neural discourse segmentation model doesn’t rely on any syntactic features, while it can outperform other state-of-the-art systems and achieve significant speedup.\n• To our knowledge, we are the first to transfer word representations learned from large corpus into discourse segmentation task and show that they can significantly alleviate the data insufficiency problem.\n• Based on the nature of discourse segmentation, we propose a restricted attention mechanism , which enables the model to capture useful information within a neighborhood but ignore unnecessary faraway noises."
  }, {
    "heading": "2 Neural Discourse Segmentation Model",
    "text": "We model discourse segmentation as a sequence labeling task, where the start word of each EDU (except the first EDU) is supposed to be labeled as 1 and other words are labeled as 0. Figure 1 gives an overview of our segmentation model. We will introduce the BiLSTM-CRF framework in Section 2.1, and describe the two key components of our model in Section 2.2, 2.3."
  }, {
    "heading": "2.1 BiLSTM-CRF for Sequence Labeling",
    "text": "Conditional Random Fields (CRF) (Lafferty et al., 2001) is an effective method to sequence labeling problem and has been widely used in many NLP tasks (Sutton and McCallum, 2012). To approach our discourse segmentation task in a neural way, we adopt the BiLSTM-CRF model (Huang et al., 2015) as the framework of our system. Formally, given an input sentence x = {xt}nt=1, we first embed each word into a vector et. Then these word embeddings are fed into a bi-directional LSTM\nlayer to model the sequential information:\nht = BiLSTM(ht−1, et) (1)\nwhere ht is the concatenation of the hidden states from both forward and backward LSTMs. After encoding this sentence, we make labeling decisions for each word. Instead of modeling the decisions independently, the CRF layer computes the conditional probability p(y|h;W,b) over all possible label sequences y given h as follows:\np(y|h;W,b) = ∏n\ni=1 ψi(yi−1, yi,h)∑ y′∈Y ∏n i=1 ψi(y ′ i−1, y ′ i,h) (2)\nwhere ψi(yi−1, yi,h) = exp(wThi + b) is the potential function and Y is the set of possible label sequences. The training objective is to maximize the conditional likelihood of the golden label sequence. During testing, we search for the label sequence with the highest conditional probability."
  }, {
    "heading": "2.2 Transferring Representations Learned from Large Corpus",
    "text": "Due to the large parameter space, neural models usually require much more training data in order to achieve good performance. However, to the best of our knowledge, nearly all existing discourse segmentation corpora are limited in size. After we remove all the syntactic features, which has been proven useful in many previous work (Bach et al., 2012; Feng and Hirst, 2014; Joty et al., 2015), it’s expected that our neural model will not achieve very satisfying results.\nTo tackle this issue, we propose to leverage model learned from other large datasets, aiming that this transferred model has been well trained\nto encode text and capture useful signals. Instead of training the transferred model by ourselves, in this paper, we adopt the ELMo word representations (Peters et al., 2018), which are derived from a bidirectional language model (BiLM) trained on one billion word benchmark corpus (Chelba et al., 2014). Specifically, this BiLM has one character convolution layer and two biLSTM layers, and correspondingly there are three internal representations for each word xt, which are denoted as {hLMt,l }3l=1. Following (Peters et al., 2018), we compute the ELMo representation rt for word xt as follows:\nrt = γ LM ∑3 l=0 sLMl h LM t,l (3)\nwhere sLM are normalized weights and γLM controls the scaling of the entire ELMo vector. Then we concatenate rt with the word embedding et, and take them as the input of Equation (1)."
  }, {
    "heading": "2.3 Restricted Self-Attention",
    "text": "As we have introduced in Section 1, some EDU boundaries rely on relatively long-distance signals to recognize, while normal LSTM model is still weak at this. Recently, self-attention mechanism, which relates different positions of a single sequence, has been successfully applied to many NLP tasks (Vaswani et al., 2017; Wang et al., 2017) and shows its superiority in capturing long dependency. However, we found that most boundaries are actually only influenced by nearby EDUs, thereby forcing the model to attend to the whole sequence will bring in unnecessary noises. Therefore, we propose a restricted self-attention mechanism, which only collects information from a fixed neighborhood. To do this, we first compute the similarity between current word xi and each nearby word xj within a window:\nsi,j = w T attn[hi,hj ,hi hj ] (4)\nThen the attention vector ai is computed as a weighted sum of nearby words:\nαi,j = esi,j∑K\nk=−K e si,i+k\n(5)\nai = ∑K\nj=−K αi,i+khi+k (6)\nwhere hyper-parameter K is the window size. This attention vector ai is then put into another\nBiLSTM layer together with hi in order to fuse the information:\nh̃t = BiLSTM(h̃t−1, [ht,at]) (7)\nWe use h̃t as the new input to the CRF layer."
  }, {
    "heading": "3 Experiments and Results",
    "text": ""
  }, {
    "heading": "3.1 Dataset and Metrics",
    "text": "We conduct experiments on the RST Discourse Treebank (RST-DT) (Carlson et al., 2001). The original corpus contains 385 Wall Street Journal articles from the Penn Treebank, which are divided in to training set (347 articles, 6132 sentences) and test set (38 articles, 991 sentences). We randomly sample 34 (10%) articles from the train set as validation set in order to tune the hyperparameters and only train our model on the remained train set. We follow mainstream studies (Soricut and Marcu, 2003; Joty et al., 2015) to measure segmentation accuracy only with respect to the intra-sentential segment boundaries, and we report Precision (P), Recall (R) and F1-score (F1) for segmentation performance."
  }, {
    "heading": "3.2 Implementation Details",
    "text": "We tune all the hyper-parameters according to the model performance on the separated validation set. The 300-D Glove embeddings (Pennington et al., 2014) are employed and kept fixed during training. We use the AllenNLP toolkit (Gardner et al., 2018) to compute the ELMo word representations. The hidden size of our model is set to be 200 and the batch size is 32. L2 regularization is applied to trainable variables with its weight as 0.0001 and we use dropout between every two layers, where the dropout rate is 0.1. For model training, we employ the Adam algorithm (Kingma and Ba, 2014) with its initial learning rate as 0.0001 and we clip the gradients to a maximal norm 5.0. Exponential moving average is applied to all trainable variables with a decay rate 0.9999. The window size K for restricted attention is set to be 5."
  }, {
    "heading": "3.3 Performance",
    "text": "The results of our model and other competing systems on the test set of RST-DT are shown in Table 2. We compare our results against the following systems: (1) SPADE (Soricut and Marcu, 2003) is an early system using simple lexical and syntactic features; (2) NNDS (Subba and Di Eugenio, 2007) uses a neural network classifier to do the\nsegmentation after extracting features; (3) CRFSeg (Hernault et al., 2010) is the first discourse segmenter using CRF model; (4) CODRA (Joty et al., 2015) uses fewer features and a simple logistic regression model to achieve impressive results; (5) Reranking (Bach et al., 2012) reranks the N-best outputs of a base CRF segmenter; (6) Two-Pass (Feng and Hirst, 2014) conducts a second segmentation after extracting global features from the first segmentation result. All these methods rely on tree features and we list their performance given different parse trees, where Gold are the trees extracted from the Penn Treebank (Prasad et al., 2005), Stanford represents trees from the Stanford parser (Klein and Manning, 2003) and BLLIP represents those from the BLLIP parser (Charniak and Johnson, 2005). It should be noted that the results of SPADE and CRFSeg are taken from Bach et al. (2012) since the original papers adopt different evaluation metrics. All the other results are taken from the corresponding original papers.\nFrom Table 2, we can see that our model achieves state-of-the-art performance without extra parse trees. Especially, if no gold parse trees are provided, our system outperforms other methods by more than 1.7 points in F1 score. Since the gold parse trees are not available when processing new sentences, this improvement becomes more valuable when the system is put into use.\n3In parallel with our work, Li et al. (2018) proposes another neural model with its performance as: P-91.6, R-92.8, F1-92.2. We didn’t see their paper at the time of submission, but it’s worth mentioning here for the readers’ reference.\nTo further explore the influence of different components in our model, we also report the results of ablation experiments in Table 2. We can see that the transferred ELMo representations provide the most significant improvement. This accords with our assumption that the RST-DT corpus itself is not large enough to train an expressive neural model sufficiently. With the help of the transferred representations, we are capable of capturing more semantic and syntactic signals. Also, comparing the models with and without the restricted self-attention, we find that this attention mechanism can further boost the performance. Especially, if there are no ELMo vectors, the improvement provided by the attention mechanism is more noticeable."
  }, {
    "heading": "3.4 Speed Comparison",
    "text": "We also measure the speedup of our model against traditional systems in Table 3. The Two-Pass system has the best performance among all existing methods, while SPADE is much simpler with less features. We test these systems on the same machine (CPU: Intel Xeon E5-2690, GPU: NVIDIA Tesla P100). The results show that our system is 2.4-6.5 times faster than the compared systems if the batch size is 1. Moreover, if we process the test sentences in parallel, we can achieve 20.2- 54.8 times speedup with the batch size as 32. This makes our system more practical in actually use."
  }, {
    "heading": "3.5 Effect of Restricted Self-Attention",
    "text": "We propose to restrict the self-attention within a neighborhood instead of the whole sequence. Table 4 demonstrates the performance of our model over different window size K. We can see that all these results is better than the performance our model without attention mechanism. However, a proper restriction window is helpful for the attention mechanism to take better effect."
  }, {
    "heading": "4 Conclusion",
    "text": "In this paper, we propose a neural discourse segmenter that can segment text fast and accurately. Different from previous methods, our segmenter doesn’t rely on any hand-crafted features, especially the syntactic parse tree. To achieve our goal, we propose to leverage the word representations learned from large corpus and we also propose a restricted self-attention mechanism. Experimental results on RST-DT show that our system can achieve state-of-the-art performance together with significant speedup."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their insightful comments on this paper. This work was partially supported by National Natural Science Foundation of China (61572049 and 6187022165). The corresponding author of this paper is Sujian Li."
  }],
  "year": 2018,
  "references": [{
    "title": "A reranking model for discourse segmentation using subtree features",
    "authors": ["Ngo Xuan Bach", "Minh Le Nguyen", "Akira Shimazu."],
    "venue": "Proceedings of the SIGDIAL 2012 Conference, The 13th Annual Meeting of the Special Interest Group on Discourse and",
    "year": 2012
  }, {
    "title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory",
    "authors": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurovsky."],
    "venue": "Proceedings of the SIGDIAL 2001 Workshop, The 2nd Annual Meeting of the Special Interest Group",
    "year": 2001
  }, {
    "title": "Coarseto-fine n-best parsing and maxent discriminative reranking",
    "authors": ["Eugene Charniak", "Mark Johnson."],
    "venue": "ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, Uni-",
    "year": 2005
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."],
    "venue": "IN-",
    "year": 2014
  }, {
    "title": "Two-pass discourse segmentation with pairing and global features",
    "authors": ["Vanessa Wei Feng", "Graeme Hirst."],
    "venue": "CoRR, abs/1407.8215.",
    "year": 2014
  }, {
    "title": "Allennlp: A deep semantic natural language processing platform",
    "authors": ["Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1803.07640.",
    "year": 2018
  }, {
    "title": "Hilda: a discourse parser using support vector machine classification",
    "authors": ["Hugo Hernault", "Helmut Prendinger", "David A DuVerle", "Mitsuru Ishizuka", "Tim Paek."],
    "venue": "Dialogue and Discourse, 1(3):1–33.",
    "year": 2010
  }, {
    "title": "Bidirectional LSTM-CRF models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "CoRR, abs/1508.01991.",
    "year": 2015
  }, {
    "title": "CODRA: A novel discriminative framework for rhetorical analysis",
    "authors": ["Shafiq R. Joty", "Giuseppe Carenini", "Raymond T. Ng."],
    "venue": "Computational Linguistics, 41(3):385–435.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Accurate unlexicalized parsing",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, 7-12 July 2003, Sapporo Convention Center, Sapporo, Japan., pages 423–430.",
    "year": 2003
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
    "year": 2001
  }, {
    "title": "Segbot: A generic neural text segmentation model with pointer network",
    "authors": ["Jing Li", "Aixin Sun", "Shafiq Joty."],
    "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm,",
    "year": 2018
  }, {
    "title": "The role of discourse units in near-extractive summarization",
    "authors": ["Junyi Jessy Li", "Kapil Thadani", "Amanda Stent."],
    "venue": "Proceedings of the SIGDIAL 2016 Conference, The 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
    "year": 2016
  }, {
    "title": "Recent progress in deep learning for NLP",
    "authors": ["Zhengdong Lu", "Hang Li."],
    "venue": "Tutorial Abstracts, NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for",
    "year": 2016
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C Mann", "Sandra A Thompson."],
    "venue": "Text-Interdisciplinary Journal for the Study of Discourse, 8(3):243–281.",
    "year": 1988
  }, {
    "title": "The theory and practice of discourse parsing and summarization",
    "authors": ["Daniel Marcu."],
    "venue": "MIT press.",
    "year": 2000
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "CoRR, abs/1802.05365.",
    "year": 2018
  }, {
    "title": "The penn discourse treebank as a resource for natural language generation",
    "authors": ["Rashmi Prasad", "Aravind Joshi", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Bonnie Webber."],
    "venue": "Proceedings of the Corpus Linguistics Workshop on Using Corpora for",
    "year": 2005
  }, {
    "title": "Sentence level discourse parsing using syntactic and lexical information",
    "authors": ["Radu Soricut", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language",
    "year": 2003
  }, {
    "title": "Discourse chunking and its application to sentence compression",
    "authors": ["Caroline Sporleder", "Mirella Lapata."],
    "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 257–264.",
    "year": 2005
  }, {
    "title": "Automatic discourse segmentation using neural networks",
    "authors": ["Rajen Subba", "Barbara Di Eugenio."],
    "venue": "Proceedings of the 11th Workshop on the Semantics and Pragmatics of Dialogue, pages 189– 190.",
    "year": 2007
  }, {
    "title": "An introduction to conditional random fields",
    "authors": ["Charles A. Sutton", "Andrew McCallum."],
    "venue": "Foundations and Trends in Machine Learning, 4(4):267– 373.",
    "year": 2012
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
    "year": 2017
  }, {
    "title": "Gated self-matching networks for reading comprehension and question answering",
    "authors": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-",
    "year": 2017
  }],
  "id": "SP:4d7d955826d1f75269f2dec84d7a714ad83c6009",
  "authors": [{
    "name": "Yizhong Wang",
    "affiliations": []
  }, {
    "name": "Sujian Li",
    "affiliations": []
  }, {
    "name": "Jingfeng Yang",
    "affiliations": []
  }],
  "abstractText": "Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an endto-end neural segmenter based on BiLSTMCRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new stateof-the-art performance. 1",
  "title": "Toward Fast and Accurate Neural Discourse Segmentation"
}