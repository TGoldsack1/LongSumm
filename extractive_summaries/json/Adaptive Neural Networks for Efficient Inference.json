{
  "sections": [{
    "heading": "1 Introduction",
    "text": "In recent years, more and more methods are proposed for CNN inference optimization, such as model quantization [Gong et al., 2014; Han et al., 2015b], tensor decomposition (TD) [Kim et al., 2015], weight pruning [Wen et al., 2016; Liu et al., 2017], knowledge distillation [Hinton et al., 2015; Yim et al., 2017] and new network architecture designs [Iandola et al., 2016; Howard et al., 2017], etc. Most existing methods can achieve promising model compression and floating-point operations (FLOPs) reduction with marginal accuracy degradation. However, smaller model size and FLOPs cannot guarantee the practical speed-up for inference. In this work, we propose the GAP method to prune\nthe weight in order to achieve better practical inference optimization. The pruned network could be supported by any off-the-shelf deep learning libraries, thus practical acceleration can be achieved without any effort to build additional compute libraries. Our method falls into the type of weight pruning based on structural sparsity constraint.\nMost existing structural sparsity-induced weight pruning methods prune the network at either channel-level or kernellevel. [Wen et al., 2016] uses group sparsity regularization to help select the removable kernels or channels, and network slimming (NS) [Liu et al., 2017] alters to use `1-norm on the scaling factors of batch normalization (BN) layers for channel selection. However, recent models are of much more complex structures, such as cross-connection, including 1-ton connection and n-to-1 connection. For such structure, pruning a channel cannot result in pruning of the preceding kernel since output of the filter may still be used by other layers. As a result, post-processing may be necessary after channellevel pruning to maintain the network topology. For example, DenseNet [Huang et al., 2016] has 1-to-n connections, as the output of one layer will be reused by all the following convolution layers, and each re-usage has its own BN layer. After channel-level pruning, the NS method chooses to remain all the kernels before the 1-to-n connection and insert a selection layer before each connected subsequent layers to determine which subset of the received channels should be selected. The selection layer will involve memory copy and increase the inference time. Therefore, to get an efficient network practically, a more general method is needed, which could take the network topology into consideration during pruning, so as to avoid additional post-processing.\nTackling this issue, we propose GAP for network topologyadaptive pruning. In the method, the network is viewed as a computational graph G = {V,E}, with the vertices {V } denoting the computation operations, and the edges {E} describing the information flow. In GAP, we conduct network pruning by removal of certain vertices or edges based on graph topology analysis. According to graph theory, the vertices can be divided into articulation points and nonarticulation points, where an articulation point of graph G is a vertex, of which removal disconnects G [Cormen et al., 2009]. To guarantee the information flow from the input to output, only the non-articulation points can be pruned. Similarly, the edges can be classified into bridges and non-bridges,\nar X\niv :1\n81 1.\n08 58\n9v 1\n[ cs\n.C V\n] 2\n1 N\nov 2\n01 8\nwhile a bridge of G is an edge, of which removal disconnects G. Only the non-bridges can be removed, otherwise the information flow will be broken off.\nThe whole procedure of GAP follows the framework of sparsity-induced weight pruning methods: 1) conduct structural sparsity constraint on the model parameters during training, 2) prune the vertices or edges with minor significance, 3) finetune the pruned graph. In GAP, pruning can be conducted at either vertex-level or edge-level. At vertex-level, the graph topology is considered in order to avoid postprocessing which may affect inference efficiency. For the vertices on the same cross-connection, regularization is conducted collaboratively using group sparsity to prune them all or keep them all. At edge-level, we mainly focus on the slimming of the multiple paths. Thus the graph is analyzed at coarser level. Although a coaser-level pruning may suffer from more serious performance degradation [Mao et al., 2017], edge-level pruning is still considered in our method, since it not only reduces the computation cost but also the memory access times, which can further accelarate the inference in realtime.\nFor finetuning step, we introduce a self-taught KD procedure. In traditional KD methods, more complex networks are used as teachers to guide the student network training. In the proposed method, we choose to use the original model as the teacher. So it is actually a self-taught mechanism.\nThe contribution of this work can be summarized as: 1. We propose the GAP method for topology-adaptive CNN inference optimization, which does not need any post-processing even when the network contains crossconnections.\n2. In GAP, a CNN model can be pruned at vertex-level as well as edge-level for the networks with multi-path data flow.\n3. A self-taught KD mechanism for finetuning is proposed to further improve performance of the pruned network."
  }, {
    "heading": "2 Related work",
    "text": "The inference optimization methods can be categorized into two classes: 1) reducing the model representation precision and 2) reducing the number of model parameters.\n1) Reducing the model representation precision. This category includes network quantization and binarization. Network quantization compresses the bitwidth of the weights, activations or both [Gong et al., 2014; Han et al., 2015a]. Extreme quantization is to binarize the network [Hubara et al., 2016], using 1-bit to represent a value. Such kind of works using fix-point or binary representation need specially designed compute acceleration library or hardware. Additionally, the binarization methods always suffer from significant accuracy loss.\n2) Reducing the number of model parameters. Such kind of methods can be categorized into new network architecture designs, TD, weight pruning and KD.\nNew network architecture designs. Some researches explore to get the inference-efficiency at the beginning of network design, such as SqueezeNet [Iandola et al., 2016] and MobileNet [Howard et al., 2017] etc. The main technique\nis to replace the large convolution filters by a stack of small filters and train the network end-to-end.\nTensor decomposition aims to reduce the FLOPs by decomposing a large 4-D filter into several small tensors by Canonical Polyadic (CP) decomposition [Lebedev et al., 2014] or Tucker decomposition [Kim et al., 2015]. The TDbased methods will introduce more 1× 1 convolution layers, which is less cache-efficient. As a result, the practical speedup ratio (SR) is not as ideal as the theoretical value.\nWeight pruning can reduce the model size by removing some redundant parameters. The work in [Han et al., 2015b] employs the magnitude of the weights to evaluate the weight importance to determine which parameters should be removed. This kind of pruning (fine-grained pruning) needs dedicated compute libraries or/and hardware design, such as EIE [Han et al., 2016]. More works explore to find structural pruning, which can get practical speed-up over existing compute libraries. The structural pruning methods [He et al., 2017; Wen et al., 2016] remove part of the filters/channels offline based on certain importance measurement or online using sparsity constraint while training. A finetuning procedure is conducted to compensate the performance loss. However, such methods usually ignore the network topology. As a result, additional post-processing layer may be needed to deal with complex network structure, such as cross-connections.\nKnowledge Distillation is proposed by Hinton et al. to guide the student network training by a pretrained teacher model using soft target [Hinton et al., 2015]. The method aims to transfer the knowledge from a complex teacher model to the student network. FitNet [Romero et al., 2014] extends the method by distilling the knowledge not only in the output but also the intermediate representations.\nIn this paper, our method falls into the type of weight pruning based on structural sparsity constraint, and the strategy of KD is adopted to distill the knowledge from original model to maintain the model performance."
  }, {
    "heading": "3 Proposed method",
    "text": "Given a pretrained model, graph pruning can be conducted using the following steps: 1) Re-train with sparsity regularization. The sparsity is conducted on some parameters with certain structural pattern to make some vertices or edges removable; 2) Sort all the weights and determine the pruning threshold; 3) Remove the correpsonding vertices or edges according to the threshold; 4) Finetune the pruned graph with or without self-taught KD."
  }, {
    "heading": "3.1 Notations",
    "text": "In this section, we use two kinds of description to represent a CNN network: mathematics and graph.\nMathematics. For convolution, we use X, W, Z to denote the input feature maps, convolution kernels and output feature maps, respectively. Each channel Zi in the output feature maps corresponds to a filter Wi, and the batch normalized result is represented by Ẑi,\nZi = X ∗Wi, Ẑi = Zi − µi√ σ2i + · γi + βi, (1)\nwhere µi and σ2i are mean and variance of the channel, γi and βi are the scaling factor and bias factor, respectively.\nWe use a symbol W to represent all the parameters in CNN, including {W}, {γ}, {β} and also the other parameters, such as those in the FC layer. W can be learned by the following optimization,\nmin W\nf (I,W ) +R(W ), (2)\nwhere, I denotes the input pairs including data and labels, f (·) is the loss function,R (·) is the regularization used in the training process, such as Frobenious-norm for weight decay.\nGraph. We use a graph G = {V,E} to represent a network, where vertices {V } denote the computation operations and edges {E} show the data flow. Subset module of DenseNet and ResNet [He et al., 2016] are shown in Figures 1 and 2. In CNN, the computation operations include convolution, BN, activation, concat, add and FC, etc. Since convolution accounts for the majority of computational load, we focus on the pruning of convolution vertices in our method."
  }, {
    "heading": "3.2 Vertex-level pruning",
    "text": "There are several rules to represent a graph for vertex-level pruning. 1) A convolution vertex represents a single filter rather than a set of filters in a convolution layer. Otherwise, we cannot prune the network at the fine filter-level when pruning graph at vertex level. 2) Similarly, a BN vertex represents the operation on a single channel. 3) Because the activation functions are always placed after BN and are conducted element-wise, we fuse the activation function into the BN layer to simplify the graph illustration. In the following we will show 1) how the original channel-level pruning is performed [Liu et al., 2017] and 2) how we perform the structural vertex-level pruning with consideration of the graph topology.\nFor CNN with BN layers, the scaling factors in BN layers can play a role of measuring the importance of each channel, and thus can be directly used for channel selection with\nsparsity regularization. The channel-level pruning can be obtained by modifying the optimization in Eq.(2) as\nmin W\nf (I,W ) +R(W ) + λsRs ({γ}) , (3)\nwhere Rs (·) is the sparsity regularization, which is typical realized using `1-norm, λs is the balance parameter which can trade-off between the sparsity loss and the original loss.\nIn a graph, channel-level pruning is targeted to remove the insignificant BN vertices. However, in DensNet as shown in Figure 1(a), removal of a BN vertex cannot result in removal of the preceding convolution vertex, because the convolution vertex still has outgoing edges connecting with other BN vertices. Similarly in residual module as shown in Figure 2(a), there is n-to-1 connection due to the add-operation. The addvertex cannot be removed if one of its incoming is remained, and once an add-vertex is remained, all its incoming edges should be remained to guarantee the validity of the data flow.\nTherefore, to remove a certain convolution vertex, the graph topology should be taken into consideration, especially for the networks with cross-connections. Based on the conception, we propose to adaptively prune network at vertexlevel by a more structural way.\nFirstly, the BN vertices are classified into articulation points {Va} and non-articulation points {V̄a};\nSecondly, {V̄a} is further split into to 1-to-1 connection {V̄ (1 1)a }, 1-to-n connection {V̄ (1 n)a } and n-to-1 connection {V̄ (n 1)a } BN vertices. If we use Pv and Cv to represent the set of the parent and child vertices of v, then the definitions are as follows,\n{V̄ (1 n)a } = {v ∈ V̄a | ∃w ∈ V̄a, w 6= v,Pv ∩ Pw 6= ∅}, {V̄ (n 1)a } = {v ∈ V̄a | ∃w ∈ V̄a, w 6= v, Cv ∩ Cw 6= ∅},\n{V̄ (1 1)a } = {v ∈ V̄a | ∀w ∈ V̄a, w 6= v, Cv ∩ Cw = ∅,Pv ∩ Pw = ∅},\n(4)\nIn GAP, we ignore cross-connections when the shared child vertex is “concat”. When different feature maps are combined through “concat”, there is still no correlation among them. As a result, for the parent vertices of concat-vertex, whether they can be pruned still depends on themselves.\nFinally, different constraints are conducted on different subsets,\nmin W\nf (I,W ) +R(W ) + λsRs ({γs}) + λgsRgs ({ γgs })\ns.t. γs ∈ {V̄ (1 1)a }, γgs ∈ {V̄ (1 n)a } ∪ {V̄ (n 1)a } (5)\nThe vertices in {V̄ (1 1)a } are regularized by `1-norm Rs (·), and those in {V̄ (1 n)a } and {V̄ (n 1)a } are constrained by group sparsity, using `2,1-norm Rgs (·), while each group denotes the vertices that share the same parent or child vertex."
  }, {
    "heading": "3.3 Edge-level pruning",
    "text": "In this section, we introduce the edge-level pruning. Recently, many networks are proposed using multi-path data flow, such as the inception module in GoogleNet, the fire module in\nSqueezeNet and the “dense” connection in DenseNet. Specifically, group convolution is a special case for multi-path design, with all paths identical. Figure 3(a) shows the original group convolution module, while (b) shows an equivalent structure, which is easier for topology analysis. As the network structure are usually designed with redundancy to help solving the highly non-convex optimization [Luo et al., 2017], not all paths are essential for the network performance. Thus certain paths can be pruned to reduce the model size and FLOPs of inference. In a graph, this can be realized by edge-level pruning. Different from Section 3.2, here we can treat a network as graph at a coarser level: a set of filters in a convolution layer is regarded as a single vertex. Similarly, a BN vertex represents a whole BN layer in edge-level pruning. When there are multiple paths for data flow, the edges on such paths become non-bridge. Thus the multi-path pruning is equivalent to removing part of the non-bridge edges. And the sparsity regularization to make non-bridge edges pruning is conducted as steps below:\nFirstly, the non-bridge edges are selected as candidates to be pruned. As shown in Figures 3(c) and 4(b), if one edge is removed, the whole path will be disabled. Thus there is no need to regulate all the non-bridge edges on a certain path. We only choose the last edge in each path to conduct pruning. Furthermore, in CNNs, multiple paths are always combined together using a “concat” operation. Therefore, we use concat-vertex to detect the edges to be pruned. The set of selected edges is denoted as {Es}. Secondly, each selected edge is scaled by an additional parameter γe, acting as a measurement of the edge’s importance. The edge scaling factors are therefore constrained using sparsity regularization,\nmin W\nf (I,W ) +R(W ) + λesRes ({γe}) , (6)\nwhere, Res (·) denotes `1-norm on the scaling factors {γe}, λes is the balance parameter."
  }, {
    "heading": "3.4 Self-taught KD",
    "text": "After training with sparsity constraints, all the scaling factors can be sorted. The vertices or edges with scaling factor smaller than a certain threshold are pruned, while the threshold can be obtained by the pruning ratio. The pruned graph may suffer from certain performance degradation, and it can be compensated by finetuning. In addition to naive finetuning, we propose to finetune the network using a self-taught KD strategy in this paper.\nFor the pruned network, the original model is apparently a more complex model with better performance, and it can act as the teacher in KD for finetuning. In addition, the pretrained model is already provided, which is rather important in practice, as there is always limited resource and time to train a more complex teacher model for a specific task. As the knowledge is distilled from the original model to the pruned network, we denote it as self-taught KD. In the following sections, experimental results will show that the performance of the pruned network is sufficiently improved when compared with the naive finetuning strategy."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Implementations",
    "text": "We evaluated the effectiveness of the proposed pruning method using two widely used datasets: CIFAR10 [Krizhevsky and Hinton, 2009] and ImageNet LSVRC 2012 [Russakovsky et al., 2015]. Considering the “topology-adaptive” attribute of GAP, ResNet, DenseNet and ResNeXt [Xie et al., 2017] were chosen for the evaluation.\nTo evaluate the inference efficiency, we used three criteria: model compression ratio (CR), theoretical SR and practical SR. Model size and FLOPs before and after pruning were used to compute the model CR and theoretical SR. We used the practical SR as an additional indicator of inference efficiency, since the memory access and movement time are not considered in FLOPs. TensorFlow is used as the basic framework. The practical SR was evaluated with the library of CUDA8, cuDNN5 on a GPU (GTX1080Ti). As for all the network trainings, SGD with a Nesterov momentum of 0.9 was used as the optimizer. Weight decay for the networks was set to 10−4. For the sparsity regularization, the balance parameters for vertex-level were set to λgs = 10∗λs for simplicity, as the structural group sparsity is harder to be sparsified. λs was selected in {10−2, 10−3, 10−4}, based on sparseness of the targeted weights. For edge-level, λes was searched in {10−1, 10−2, 10−3}. All the layers were pruned simultaneously based on an adaptive threshold, which was determined by the pruning proportion."
  }, {
    "heading": "4.2 Experiments on CIFAR10",
    "text": "Data augmentation of CIFAR10 for training were conducted using random cropping and mirroring. Images were normalized channel-wise based on statistical values. For experiments on CIFAR10, ResNet-164, DenseNet-40 (k=12) and ResNeXt-29 (8×64d) were adopted. The original pre-trained models were implemented by ourselves based on TensorFlow,\nwith the same settings as the authors’. In the first step, the pre-trained models were retrained with sparsity constraint using mini-batch size 128 for 10 epochs, with a learning rate of 0.01. All layers in the graph were pruned together and then finetuned for 90 epochs. Initial learning rate for finetuning was set to 0.01 and divided by 10 at 2/3 of the total epochs. For self-taught KD, a temperature of T = 5 was used and the relative weight for soft target was set to 1.\nPruning results are shown in Table 1. In DenseNet and ResNeXt, the networks are pruned with the same percentage for channel-level, vertex-level and edge-level. The results suggest that the strategy of finetuning with self-taught KD performs better than naive finetuning in restoring the degradation of classification accuracy caused by pruning. By comparison, we can see that structurally pruning at vertex-level can get higher model CR and theoretical SR, while vertexlevel pruning can still get better performance in classification error rate. In ResNeXt, with approximately no loss of accuracy, pruning 60% off at vertex-level can get 2.68× practical SR while only 1.69× at channel-level. In DenseNet, channellevel pruning achieves almost no speed-up as it introduces additional selection operations, which increases the memory access time. Compared with ResNeXt, DenseNet is already a quite compact network, which is more difficult to be pruned. However, we still achieve 2.04× model CR and 1.27× practical SR through vertex-level pruning with marginal loss of performance. In ResNet, only channel-level and vertex-level pruning were conducted. The model was pruned to comparable model size and FLOPs at the two levels. At vertex-level, we can get 1.48× practical SR and 2.39× model CR with minor loss of accuracy.\nEdge-level pruning leads to the largest remaining model size and FLOPs, because edge-level pruning can only prune part of the graph. In ResNeXt, only the edges contained in\nthe group convolution can be pruned. Similarly, only the dense connections can be removed in DenseNet. Additionally, edge-level gets the worst error rate. This is naturally because it prunes the network at a coarse-grained level, which will do more harm on the network [Mao et al., 2017]. However, the benefit of edge-level pruning is that it has little gap between practical SR and theoretical SR: edge-level pruning reduces the number of computation operations, as a result it can reduce the FLOPs as well as the memory access times, while the theoretical SR ignores the issue of memory access. Specifically, in ResNeXt, the practical SR actually exceeds the theoretical one.\nFurthermore, we use ResNeXt-29 to quantitatively analyze the network performance with respect to different model CRs, theoretical and practical SRs. Results are shown in Figure 5. Vertex-level pruning achieves lower error rate than channellevel with the same model CR, theoretical or practical SR, especially in the practical SR measurement. Through vertexlevel pruning with self-taught KD finetuning, we can get approximately 12× model CR, 15× theoretical SR and 4.3× practical SR with nearly no loss of accuracy. Although, edgelevel suffers more in accuracy loss, it achieves larger practical SR at the same level of theoretical SR. As shown in Figure 5(c), at the same error rate, edge-level pruning gets higher actual speed-up than channel-level pruning. Finally, we can see that the performance is sufficiently improved by finetuning with the self-taught KD compared with naive finetuning."
  }, {
    "heading": "4.3 Experiments on ImageNet",
    "text": "We adopted the same data augmentation scheme as in [Huang et al., 2016] for ImageNet. Top-1 error rate of a single center crop was used as the performance measurement. In the first step, the pretrained models were retrained with sparsity constraint using a mini-batch size of 256 on 4 GPUs for 1\nepoch, with the learning rate being 0.01. For finetuning, the model was trained for 40 epochs, with an initial learning rate of 0.01, which was decreased by a factor of 10 at 15th and 30th epochs, respectively. For KD, T = 5 was used and the relative weight for soft target was set to 100 making the loss magnitude of soft target and hard target comparable.\nOn ImageNet, ResNeXt-50 (32 × 4d) and DenseNet-BC121 were validated, while the model settings were the same as in [Xie et al., 2017] and [Huang et al., 2016]. Table 2 shows the pruning results. To better evaluate the performance, the models at channel-level and vertex-level are pruned to comparable model size and FLOPs. At vertex-level, the pruned networks can achieve quite similar error rates after finetuning with self-taught KD. At the same time, we get a model CR of 1.67× and practical SR of 1.51× in ResNeXt while 1.32× CR and 1.14× practical SR in DenseNet.\nFor edge-level pruning, Figure 6 shows the distribution of edge scaling parameters {γe} after training with sparsity in DenseNet-121. {γe} are all initialized to be 1.0, and we can see that after training, non-zero γe becomes sparse so that we can prune the edges with low scaling values. Furthermore, it can be observed that the information flow between blocks are critical, as indicated in the first row and last column of each block, while the layer connections within a block may have high redundancy. For ResNeXt-50, Figure 7 shows the number of remained edges in ResNeXt-50 with different pruning percentages. We use a global threshold to adaptively prune each layer, as the redundency may vary with different layers.\nBecause of the bottleneck structure in DenseNet-BC, edgelevel pruning can only remove the convolutional layers with kernel size 1 × 1. Therefore, it can only achieve a quite low model CR and FLOPs reduction. As for ResNeXt, only edges involved in the group convolutions can be pruned, thus the model CR and theoretical SR are also not high enough. However, as described in Section 4.2, the benefit of edge-level pruning is that it can also reduce the memory access times\nand thus further accelerate the inference. In ResNeXt, the practical SR exceeds the theoretical value, and in DenseNet it has quite little gap with the theoretical SR."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we propose the GAP for CNN model compression and inference acceleration. By adaptive analysis of the graph, the method can directly remove certain vertices or edges to achieve a compact and efficient graph for inference by maintaining the original graph topology. The pruned network can achieve practical speed-up without any post-processing to deal with complex structures. For finetuning, we adopt a self-taught KD strategy to improve the network performance. The strategy can sufficiently improve the model performance and it does not introduce extra workload, which is quite applicable for practical tasks. Experimental results show it can make the inference more efficient, with high model CR and practical SR, while keeping the network performance very close to original model. As the future work, we will develop an auto-tuning mechanism to search optimal hyper-parameters involved in the framework, and we are going to investigate the scheme to combine the vertex-level and edge-level pruning, so that a more rational mixed-level pruning can be conducted for a network given computation resource or latency limitation."
  }],
  "year": 2018,
  "references": [{
    "title": "The MIT Press",
    "authors": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein. Introduction to Algorithms", "Third Edition"],
    "venue": "3rd edition,",
    "year": 2009
  }, {
    "title": "Compressing deep convolutional networks using vector quantization",
    "authors": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"],
    "venue": "arXiv preprint arXiv:1412.6115,",
    "year": 2014
  }, {
    "title": "Deep compression: Compressing deep neural networks with pruning",
    "authors": ["Song Han", "Huizi Mao", "William J Dally"],
    "venue": "trained quantization and huffman coding. arXiv preprint arXiv:1510.00149,",
    "year": 2015
  }, {
    "title": "In Advances in Neural Information Processing Systems",
    "authors": ["Song Han", "Jeff Pool", "John Tran", "William Dally. Learning both weights", "connections for efficient neural network"],
    "venue": "pages 1135–1143,",
    "year": 2015
  }, {
    "title": "Eie: efficient inference engine on compressed deep neural network",
    "authors": ["Han et al", "2016] Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A Horowitz", "William J Dally"],
    "venue": "In Proceedings of the 43rd International Symposium on Computer Architecture,",
    "year": 2016
  }, {
    "title": "In IEEE conference on computer vision and pattern recognition (CVPR)",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun. Deep residual learning for image recognition"],
    "venue": "pages 770–778,",
    "year": 2016
  }, {
    "title": "In The IEEE International Conference on Computer Vision (ICCV)",
    "authors": ["Yihui He", "Xiangyu Zhang", "Jian Sun. Channel pruning for accelerating very deep neural networks"],
    "venue": "Oct",
    "year": 2017
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"],
    "venue": "arXiv preprint arXiv:1503.02531,",
    "year": 2015
  }, {
    "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
    "authors": ["Howard et al", "2017] Andrew Howard", "Menglong Zhu", "Bo Chen", "Dmitry Kalenichenko", "Weijun Wang", "Tobias Weyand", "Marco Andreetto", "Hartwig Adam"],
    "venue": "arXiv preprint arXiv:1704.04861,",
    "year": 2017
  }, {
    "title": "Densely connected convolutional networks",
    "authors": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger", "Laurens van der Maaten"],
    "venue": "arXiv preprint arXiv:1608.06993,",
    "year": 2016
  }, {
    "title": "In Advances in Neural Information Processing Systems",
    "authors": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio. Binarized neural networks"],
    "venue": "pages 4107–4115.",
    "year": 2016
  }, {
    "title": "and Kurt Keutzer",
    "authors": ["Forrest N Iandola", "Song Han", "Matthew W Moskewicz", "Khalid Ashraf", "William J Dally"],
    "venue": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360,",
    "year": 2016
  }, {
    "title": "Compression of deep convolutional neural networks for fast and low power mobile applications",
    "authors": ["Yong-Deok Kim", "Eunhyeok Park", "Sungjoo Yoo", "Taelim Choi", "Lu Yang", "Dongjun Shin"],
    "venue": "arXiv preprint arXiv:1511.06530,",
    "year": 2015
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["Alex Krizhevsky", "Geoffrey Hinton"],
    "venue": "Technical report,",
    "year": 2009
  }, {
    "title": "Speeding-up convolutional neural networks using finetuned cp-decomposition",
    "authors": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"],
    "venue": "arXiv preprint arXiv:1412.6553,",
    "year": 2014
  }, {
    "title": "In IEEE International Conference on Computer Vision (ICCV)",
    "authors": ["Zhuang Liu", "Jianguo Li", "Zhiqiang Shen", "Gao Huang", "Shoumeng Yan", "Changshui Zhang. Learning efficient convolutional networks through network slimming"],
    "venue": "pages 2755–2763. IEEE,",
    "year": 2017
  }, {
    "title": "Thinet: A filter level pruning method for deep neural network compression",
    "authors": ["Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin"],
    "venue": "arXiv preprint arXiv:1707.06342,",
    "year": 2017
  }, {
    "title": "Exploring the regularity of sparse structure in convolutional neural networks",
    "authors": ["Huizi Mao", "Song Han", "Jeff Pool", "Wenshuo Li", "Xingyu Liu", "Yu Wang", "William J Dally"],
    "venue": "arXiv preprint arXiv:1705.08922,",
    "year": 2017
  }, {
    "title": "Fitnets: Hints for thin deep nets",
    "authors": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"],
    "venue": "arXiv preprint arXiv:1412.6550,",
    "year": 2014
  }, {
    "title": "Imagenet large scale visual recognition challenge",
    "authors": ["Russakovsky et al", "2015] Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"],
    "year": 2015
  }, {
    "title": "In Advances in Neural Information Processing Systems",
    "authors": ["Wei Wen", "Chunpeng Wu", "Yandan Wang", "Yiran Chen", "Hai Li. Learning structured sparsity in deep neural networks"],
    "venue": "pages 2074–2082,",
    "year": 2016
  }, {
    "title": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
    "authors": ["Saining Xie", "Ross Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He. Aggregated residual transformations for deep neural networks"],
    "venue": "pages 5987–5995. IEEE,",
    "year": 2017
  }, {
    "title": "A gift from knowledge distillation: Fast optimization",
    "authors": ["Junho Yim", "Donggyu Joo", "Jihoon Bae", "Junmo Kim"],
    "venue": "network minimization and transfer learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2017
  }],
  "id": "SP:cc93a14d98c7252b87552dc6fbf1b90c5c396d1f",
  "authors": [{
    "name": "Mengdi Wang",
    "affiliations": []
  }, {
    "name": "Qing Zhang",
    "affiliations": []
  }, {
    "name": "Jun Yang",
    "affiliations": []
  }, {
    "name": "Xiaoyuan Cui",
    "affiliations": []
  }, {
    "name": "Wei Lin",
    "affiliations": []
  }],
  "abstractText": "In this work, we propose a graph-adaptive pruning (GAP) method for efficient inference of convolutional neural networks (CNNs). In this method, the network is viewed as a computational graph, in which the vertices denote the computation nodes and edges represent the information flow. Through topology analysis, GAP is capable of adapting to different network structures, especially the widely used cross connections and multi-path data flow in recent novel convolutional models. The models can be adaptively pruned at vertex-level as well as edge-level without any post-processing, thus GAP can directly get practical model compression and inference speed-up. Moreover, it does not need any customized computation library or hardware support. Finetuning is conducted after pruning to restore the model performance. In the finetuning step, we adopt a self-taught knowledge distillation (KD) strategy by utilizing information from the original model, through which, the performance of the optimized model can be sufficiently improved, without introduction of any other teacher model. Experimental results show the proposed GAP can achieve promising result to make inference more efficient, e.g., for ResNeXt-29 on CIFAR10, it can get 13× model compression and 4.3× practical speed-up with marginal loss of accuracy.",
  "title": "Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks"
}