{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4737–4742 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4737"
  }, {
    "heading": "1 Introduction",
    "text": "Targeted sentiment analysis (TSA) aims to extract targets in a text and simultaneously predict their sentiment classes (Hu and Liu, 2004; Jin et al., 2009; Li et al., 2010; Yang and Cardie, 2013). For example, given a sentence “ESPN poll says Michael Jordan is the greatest basketball athlete”, the targets are ESPN and Michael Jordan and their sentiment classes are Neutral and Positive respectively.\nTargeted sentiment analysis can be seen as two tasks: target extraction and sentiment classification. Some researchers have tackled two tasks separately, e.g., target extraction (Liu et al., 2013; Wang et al., 2016a; Yin et al., 2016) and sentiment classification (Tang et al., 2016; Wang et al., 2016b; Ruder et al., 2016). Recently, some researches have attempted to conduct the two tasks jointly and generally see them as sequence labeling problems, where the B/I/O labels indicate target boundaries and the Positive/Neutral/Negative labels denote sentiment classes (Klinger and Cimiano, 2013; Yang and Cardie, 2013). Mitchell et al.\n(2013) explore labeling targets and their sentiment classes simultaneously by using the Conditional Random Fields (CRF) approach with traditional manual discrete features, and present three models: pipeline, joint and collapsed, according to different labeling processes of the two tasks. They find that the pipeline method outperforms the joint model on tweet dataset. Further, Zhang et al. (2015) introduce word embedding representations into the CRF framework and find that it is beneficial to integrate word embeddings into handcraft features in TSA regardless of pipeline, joint or collapsed methods.\nWith the success of deep learning techniques, neural networks have demonstrated their capability of sequence labeling (Collobert et al., 2011; Pei et al., 2014; Chen et al., 2015). However, Zhang et al. (2015) only use word embeddings to enrich features without taking full advantages of neural networks’ potential in automatically capturing important sequence labeling features like long distance dependencies and character-level features.\nTo make better use of neural networks to explore appropriate character-level features and high-level semantic features for the two tasks, we design a hierarchical multi-layer bidirectional gated recurrent units networks (HMBiGRU) which uses a multi-layer Bi-GRU to automatically learn character features (e.g. capitalization, noun suffix, etc) on letter sequence and model long distance dependencies between words on the concatenation of word embedding and its character features. The learned character features can also address out-of-vocabulary word problems.\nIn above example, the target label and sentiment label for Michael Jordon are “B-Person, IPerson” and “B-Positive, I-Positive”, we can see that the boundary information (B, I) of target label and sentiment label is consistent. From the\nview of human, we should first predict the target label and give corresponding sentiment label afterwards. Therefore, we introduce target label information into predicting sentiment label. In this way, our model can know about the target boundary information when predicting the sentiment label. Meanwhile, we also introduce transition matrix (Collobert et al., 2011) to model the dependencies between labels.\nWe conduct experiments on two datasets, and the performances show that our models outperform other baselines. This verifies the effectiveness of neural networks in TSA. In the experiments, we find that the target label information is important for predicting sentiment label. We also analyze the performance of multi-layer Bi-GRU and hierarchical architecture in learning character features and dependencies between words."
  }, {
    "heading": "2 Model",
    "text": "We will detailedly introduce our model in this section, and our model is shown in Figure 1. Supposing that a sentence is composed of n words [w1, w2, ..., wn]. For each word wi consists of li characters [c1, c2, ..., cli ] and li is the length of wi. We embed all words and characters into low-dimensional real-value vectors which can be learned by language model (Bengio et al., 2003; Mikolov et al., 2013). We represent sentence as a matrix of word embeddings W = [E1, E2, ..., En] ∈ Rn×dw . Similarly, word wi is denoted as a matrix of character embeddings Ci ∈ Rli×dc , and dw and dc are the size of word embedding and character embedding respectively.\nFirst, we design a hierarchical two-layer architecture where each layer includes a multi-layer bidirectional Gated Recurrent Units (MBi-GRU). GRU is good at modeling a sequence with the benefits of avoiding the gradient vanishing and exploding problems. For a MBi-GRU, supposing that it has M layers of Bi-GRU, the hidden state on layerm ∈ {1, 2, ...,m} at time t ∈ {1, 2, ..., n} is recursively computed by:\nhmt = BiGRU(h m−1 t , h m t−1). (1)\nwhere the superscript of h denotes the corresponding layer of a MBi-GRU, and h0 means the original inputs. BiGRU is bidirectional GRU which is\ndefined as:\nBiGRU(xt, ht−1) = −→ ht ⊕ ←− ht ; (2)\n−→ ht = GRU(xt, −−→ ht−1); (3) ←− ht = GRU(xt, ←−− ht−1). (4)\nwhere xt is inputs which can be word embeddings or the hidden states of other BiGRU. ⊕ indicates the operation of concatenating two vectors.\nWith the matrix of character embeddings Ci as inputs, we utilize a MBi-GRU to learn characterlevel abstract features for word wi based on its character embeddings. Through MBi-GRU, we can obtain the hidden states [hM1 , h M 2 , ..., h M li ] on which a max-pooling operation is applied to output the character-level features ri ∈ R2dc for word wi. The character features of all words in a sentence form a new matrix C ∈ Rn×2dc . Next, We concatenate C with the matrix of word embeddings W and denote the concatenation as F ∈ Rn×(dw+2dc). With F as input, We utilize another MBi-GRU to learn the hidden states H = [h′M1 , h ′M 2 , ..., h ′M n ] as the final representations of the sentence. Therefore, the hierarchical two-layer MBi-GRU architecture can learn highlevel abstract features with consideration of both character-level and word-level information.\nAfter learning the final representations for sentence, we first project the features: tfi = h′ M i of each word into target label space by:\nyit = f(tfi ·W tp + btp) (5)\nwhere W tp and b t p are weight matrix and bias.\nAs we know, the boundary of a target should be the same as that of its sentiment in sequence label. As the example in Section 1, the target label and sentiment label of Michael Jordan are “BPerson, I-Person” and “B-Positive, I-Positive” respectively. To learn this kind of consistency, we introduce the target label information into predicting sentiment label by:\nyis = f(sfi ·W sp + bsp) (6)\nwhere sfi = h′ M i ⊕ yit, W ts and bts are weight matrix and bias respectively. This makes our model know the target label information when predicting their sentiment.\nFor sequence labeling, there usually exist dependencies between labels. Take the target labeling task for example, label I will never follow label\nB-Positive I-Positive\nO. To consider the influence of label dependencies, we introduce the transition matrix Ai,j proposed by Collobert et al. (2011) which measures the probability of jumping from label i to label j.\nGiven the sentence x = [w1, w2, ..., wn] and the scores yt = [y1t , y 2 t , ..., y n t ] and ys = [y1s , y 2 s , ..., y n s ] computed by Eq. 5 and Eq. 6, we get the target labeling scores by summing up transition scores and the scores yit:\ns(yt, x, θt) = ∑n\ni=1 (Ati−1,i + y i t); (7)\nwhereAt is label transition matrix for target labeling. θt = θ ∪ {Ati,j}, and θ denotes parameters of HMBi-GRUs.\nNext, we normalize the target label scores over all possible labeling paths of target (i.e., Yt) by a softmax function:\npt(yt|x) = es(yt,x,θt)∑\nŷt∈Yt e s(ŷt,x,θt)\n; (8)\nWe can also use Eq. 7 and Eq. 8 to get the normalized sentiment label scores ps(ys|x). To train our model, we define the loss function by:\nloss = − log(pt(yt|x))− log(ps(ys|x)). (9)\nFinally, we obtain targets label sequence y∗t and their sentiment label sequence y∗s which have maximal score y∗t = argmaxŷ∈Yt(s(x, ŷ, θt)) y ∗ s = argmaxŷ∈Ys(s(x, ŷ, θs)). y ∗ t and y ∗ s can be computed by Viterbi algorithm."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Setup",
    "text": "To validate the effectiveness of our model, we conduct experiments on two datasets, consisting of\nEnglish tweets and Spanish tweets, which are constructed by Mitchell et al. (2013)1. Table 2 depicts the statistics of data, which contains sentence number, target number and the number of positive target, negative target and neutral target. To evaluate the system performance, we adopt Precision, Recall and F-measure. In our experiments, we evaluate the performance of detecting targets (DT) and targeted sentiment analysis (TSA) which a target is taken as correct only when the boundary and the sentiment are both correctly recognized. We also adopt Precision, Recall and F-measure used in Zhang et al. (2015) to evaluate our model. The reason why we don’t compare with Mitchell et al. (2013) is that they only evaluate the beginning of targets along with the sentiment expressed towards it.\nIn our experiments, we use embeddings from Pennington et al. (2014)2 and Cieliebak et al. (2017)3 for English words and Spanish words respectively. The character embeddings are initialized by Xavier (Glorot and Bengio, 2010) and their dimension is 50. In our model, all unknown words, weight matrices and biases are initialized by Xavier Glorot and Bengio (2010). The dimensions of the character-level and word-level hidden states in MBi-GRU are set to 300 and 600 respectively. The layer number of multi-layer bidirectional GRU is set to 2. To avoid overfitting, we adopt dropout on embeddings, sfi and tfi, and the dropout rate is set to 0.5. The word embeddings and character embeddings will be tuned during training. Finally, we utilize Adam (Kingma and Ba, 2014) to optimize all parameters of our model."
  }, {
    "heading": "3.2 Baselines",
    "text": "To investigate the performance of our joint model, we compare it with several baselines as follows: • Discrete uses traditional discrete features as 1http://www.m-mitchell.com/code/index. html 2https://nlp.stanford.edu/projects/ glove/ 3https://spinningbytes.com/resources/ embeddings/\ninputs and multi-label CRF which contains two separate output clique potentials and two separate edge clique potentials for target extraction and sentiment classification respectively. There also exist links between target labels and sentiment labels for each word (Zhang et al., 2015).\n• Neural uses word embeddings transformed with non-linear function as inputs, and others are the same as Discrete model (Zhang et al., 2015).\n• Integrated integrates both discrete features and word embeddings into the same CRF framework and other settings are the same as Discrete (Zhang et al., 2015).\n• Bi-GRU only uses word embeddings as inputs, and Bi-GRU is employed to learn representations for sentence.\n•MBi-GRU also uses word embeddings as inputs, but MBi-GRU is utilized to model sentence.\n• HBi-GRU first uses Bi-GRU to learn character level features for each word. Then, character level features and word embeddings are concatenated as inputs for another Bi-GRU to learn final representations for sentence.\n• No-Target uses HMBi-GRU to learn representations for sentence, but h′Mi (depicted in Section 2) are used to predict target label and sentiment label separately. No-Target doesn’t let target label information to affect sentiment label. This is the biggest difference between No-Target and ours.\nIt is noticed that all of Bi-GRU, MBi-GRU and HBi-GRU use transition matrix to model the dependencies between labels and introduce target label information into predicting sentiment label."
  }, {
    "heading": "3.3 Analysis",
    "text": "Table 2 displays the performance comparison of our models with the baselines. We can see that Discrete gets the worst results on English dataset, and Neural gets the worst results on Spanish dataset. The Integrate greatly improves the performances on both datasets because discrete features and word embeddings can complement each other.\nBi-GRU greatly improves the performance compared with Discrete and Neural but gets worse performance than Integrate. This verifies the effectiveness of neural networks in TAS. However, simple neural networks are not enough to acquire better results. MBi-GRU learns high-level features via multi-layer bidirectional GRU and achieves comparable results compared with Integrate.\nNevertheless, Bi-GRU and MBi-GRU do not make full use of character-level features. HBiGRU incorporates character-level features by BiGRU on letter sequence of word. We can see that HBi-GRU improves about 1.85% and 1.16% in TSA on both datasets compared with Integrate. The performance of HBi-GRU demonstrates the importance of character-level features in TSA, and the hierarchical architecture is good at leaning multi-level (character-level, word-level) features.\nOur model improves 3.20%, 2.59% in TSA and 2.39%, 0.27% in DT on both datasets compared with the existing best system: Integrate. Compared with No-Target, our model introduces target label information into predicting sentiment label and improves about 0.66%, 1.44% in TSA and 0.59%, 0.91% in DT on both datasets. The improvements demonstrate that target label information plays important roles in predicting sentiment label. It is noticed that the results of our model in\nDT are also improved compared with No-Target. The reason may be that the gradients from sentiment loss have positive effects on detecting targets.\nIn a word, our model achieves state-of-the-art in DT and TSA on both datasets. Character-level features play great roles in DT and TSA, and HMBiGRU is good at learning multi-level features. It is useful to learn boundary consistence by introducing target label information into predicting sentiment label."
  }, {
    "heading": "3.4 Case Study",
    "text": "Here, we use a tweet from English Dataset as a case study, and the tweet is “Congratulations to our Champ Roger Federer ...”. We apply NoTarget and our model on the tweet. No-Target and our model get the same target labels: [O,O,O,O,BPerson,I-Person,...], and we can see that both models correctly extract the target: Roger Federer, and this results show the effectiveness of both models in detecting targets. Our model successfully obtains the correct sentiment labels: [O,O,O,O,B-Positive,I-Positive,...]. However, NoTarget predicts a wrong sentiment label sequence: [O,O,O,B-Positive,I-Positive,O,...]. We can see that No-Target wrongly regard Champ as the beginning position and ignore Federer. The reasons are that the first letter of Champ is capitalized, which may mislead No-Target and there is no correlation between target and sentiment label. In our model, we incorporate target label information into predicting sentiment label. Therefore, our model tends to force target and sentiment label to have same boundary information.\nThis case study shows that the target label information plays important roles in predicting sentiment label because they share the same boundary information."
  }, {
    "heading": "4 Related Work",
    "text": "Early works on target sentiment analysis were based on subjects and features. For example, Yi et al. (2003) extracted all references to the given subject and determined the sentiment of each reference. Hu and Liu (2004) first proposed several techniques to mine the product features that customers have expressed their opinions and determined their sentiment, and Popescu and Etzioni (2007) utilized unsupervised methods to identify opinions with respect to features and determine the\npolarity of opinions. Jin et al. (2009) proposed a novel lexicalized HMMs model to mine customer reviews of a product and extract highly specific product related entities which reviewers expressed their opinion, and they also identified the sentiment of opinion entities. The works of (Yang and Cardie, 2013) and (Li et al., 2010) are similar to (Jin et al., 2009). However, these works only take pre-defined features into account and can not find new features. To automatically extract targets and predict their sentiment, Mitchell et al. (2013) first proposed a conditional random fields (CRF) framework to jointly detect entities and identify their sentiment. Based on the work of (Mitchell et al., 2013), Zhang et al. (2015) explored the effect of word embeddings and automatic feature combinations by extending a CRF baseline using neural networks.\nWe propose a neural networks based joint model which extracts targets and their sentiments simultaneously. Our model takes full advantages of neural networks’ potential in capturing sequence labeling features such as long distance dependencies and character-level features. Furthermore, Our model allows the target label to have positive effects on their sentiment label because target label shares boundary information with sentiment label."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we propose a HMBi-GRU based joint model for targeted sentiment analysis. Our model will simultaneously extract targets and predict their sentiment. Furthermore, our model introduces target information into predicting corresponding sentiment label. Experiments show that the well-designed neural networks can greatly improve the result for targeted sentiment analysis, and target label information plays great roles in predicting sentiment label."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank anonymous reviewers for their insightful suggestions. Our work is supported by National Natural Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China under Grant No.2017YFB1002101. The corresponding author of this paper is Houfeng Wang."
  }],
  "year": 2018,
  "references": [{
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "JMLR, 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Long short-term memory neural networks for chinese word segmentation",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."],
    "venue": "EMNLP, pages 1197–1206.",
    "year": 2015
  }, {
    "title": "A twitter corpus and benchmark resources for german sentiment analysis",
    "authors": ["Mark Cieliebak", "Jan Deriu", "Dominic Egger", "Fatih Uzdilli."],
    "venue": "SocialNLP, page 45.",
    "year": 2017
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "JMLR, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "AISTATS, pages 249–256.",
    "year": 2010
  }, {
    "title": "Mining and summarizing customer reviews",
    "authors": ["Minqing Hu", "Bing Liu."],
    "venue": "SIGKDD, pages 168– 177. ACM.",
    "year": 2004
  }, {
    "title": "A novel lexicalized hmm-based learning framework for web opinion mining",
    "authors": ["Wei Jin", "Hung Hay Ho", "Rohini K Srihari."],
    "venue": "ICML, pages 465–472.",
    "year": 2009
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Bidirectional inter-dependencies of subjective expressions and targets and their value for a joint model",
    "authors": ["Roman Klinger", "Philipp Cimiano."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume",
    "year": 2013
  }, {
    "title": "Structure-aware review mining and summarization",
    "authors": ["Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Ying-Ju Xia", "Shu Zhang", "Hao Yu."],
    "venue": "ACL, pages 653–661.",
    "year": 2010
  }, {
    "title": "Opinion target extraction using partially-supervised word alignment model",
    "authors": ["Kang Liu", "Heng Li Xu", "Yang Liu", "Jun Zhao."],
    "venue": "IJCAI, volume 13, pages 2134–2140.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Open domain targeted sentiment",
    "authors": ["Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme."],
    "venue": "ENMLP, pages 1643–1654.",
    "year": 2013
  }, {
    "title": "Maxmargin tensor neural network for chinese word segmentation",
    "authors": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."],
    "venue": "ACL, pages 293–303.",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Extracting product features and opinions from reviews",
    "authors": ["Ana-Maria Popescu", "Orena Etzioni."],
    "venue": "Natural language processing and text mining, pages 9–28. Springer.",
    "year": 2007
  }, {
    "title": "A hierarchical model of reviews for aspect-based sentiment analysis",
    "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G Breslin."],
    "venue": "arXiv preprint arXiv:1609.02745.",
    "year": 2016
  }, {
    "title": "Effective lstms for target-dependent sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu."],
    "venue": "International Conference on Computational Linguistics, pages 3298–3307.",
    "year": 2016
  }, {
    "title": "Recursive neural conditional random fields for aspect-based sentiment analysis",
    "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao."],
    "venue": "arXiv preprint arXiv:1603.06679.",
    "year": 2016
  }, {
    "title": "Attention-based lstm for aspect-level sentiment classification",
    "authors": ["Yequan Wang", "Minlie Huang", "Li Zhao", "Xiaoyan Zhu."],
    "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 606–615.",
    "year": 2016
  }, {
    "title": "Joint inference for fine-grained opinion extraction",
    "authors": ["Bishan Yang", "Claire Cardie."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1640–1649.",
    "year": 2013
  }, {
    "title": "Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques",
    "authors": ["Jeonghee Yi", "Tetsuya Nasukawa", "Razvan Bunescu", "Wayne Niblack."],
    "venue": "Data Mining, 2003. ICDM 2003. Third IEEE International Conference",
    "year": 2003
  }, {
    "title": "Unsupervised word and dependency path embeddings for aspect term extraction",
    "authors": ["Yichun Yin", "Furu Wei", "Li Dong", "Kaimeng Xu", "Ming Zhang", "Ming Zhou."],
    "venue": "arXiv preprint arXiv:1605.07843.",
    "year": 2016
  }, {
    "title": "Neural networks for open domain targeted sentiment",
    "authors": ["Meishan Zhang", "Yue Zhang", "Duy-Tin Vo."],
    "venue": "EMNLP, pages 612–621.",
    "year": 2015
  }],
  "id": "SP:216ab8c08a1048d582df917945a9458afde1be66",
  "authors": [{
    "name": "Dehong Ma",
    "affiliations": []
  }, {
    "name": "Sujian Li",
    "affiliations": []
  }, {
    "name": "Houfeng Wang",
    "affiliations": []
  }],
  "abstractText": "Targeted sentiment analysis (TSA) aims at extracting targets and classifying their sentiment classes. Previous works only exploit word embeddings as features and do not explore more potentials of neural networks when jointly learning the two tasks. In this paper, we carefully design the hierarchical multi-layer bidirectional gated recurrent units (HMBi-GRU) model to learn abstract features for both tasks, and we propose a HMBi-GRU based joint model which allows the target label of word to have influence on its sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HMBi-GRU in learning abstract features.",
  "title": "Joint Learning for Targeted Sentiment Analysis"
}