{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Most machine learning systems implicitly or explicitly assume that their training experience is representative of their test experience. This assumption is rarely true in real-world deployments of machine learning, where “unknown unknowns”, or “alien” data, can arise without warning. Ig-\n*Equal contribution 1Department of Statistics, Oregon State University, Oregon, USA 2School of EECS, Oregon State University, Oregon, USA 3University of California, Berkeley, California, USA. Correspondence to: Si Liu <lius2@oregonstate.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nnoring the potential for such aliens can lead to serious safety concerns in many applications and significantly degrade the accuracy of test set predictions in others. For example, consider a scientific application where a classifier is trained to recognize specific categories of insects in freshwater samples in order to detect important environmental changes (Lytle et al., 2010). Test samples will typically contain some fraction of specimens belonging to species not represented in the training data. A classifier that is unaware of these new species will misclassify the specimens as belonging to existing species. This will produce incorrect scientific conclusions.\nThe problem of open category detection is to detect such alien examples at test time. An ideal algorithm for this problem would guarantee a user-specified alien-detection rate (e.g., 95%), while attempting to minimize the false alarm rate. Unfortunately, no existing algorithm provides such guarantees under general conditions. In addition, empirical evaluations of existing algorithms for open category detection typically do not directly evaluate alien detection rates, which are perhaps the most relevant for safety-critical applications. Overall, our current theoretical and practical understanding of open category detection is lacking from a safety and accuracy perspective.\nIs it possible to achieve open category detection with guarantees? In this paper, we take a step toward answering this question by studying a simplified, but practically relevant, problem setting. To motivate our setting, consider the above insect identification problem. At training time it is reasonable to expect that a clean training set is available that contains only the insect categories of interest. At test time, a new sample will include insects from the training categories along with some percentage of insects from new alien categories. Further, scientists may have reasonable estimates for this percentage based on their scientific knowledge and practical experience. We would like to guarantee that the system is able to raise an alarm for, say, 95% of the insects from alien classes, with each alarm being examined by a scientist. At the same time, we would like to avoid as many “false alarms” as possible, since each alarm requires scientist effort.\nTo formalize the example, our setting assumes two training sets: a clean training dataset involving a finite set of cate-\ngories and a contaminated dataset that contains a fraction α of aliens. Our first contribution is to show that, in this setting, theoretical guarantees are possible given knowledge of an upper bound on α. In particular, we give an algorithm that uses this knowledge to provide Probably Approximately Correct (PAC) guarantees for achieving a user-specified alien detection rate. While knowledge of a non-trivial upper bound on α may not always be possible, in many situations it will be possible to select a reasonable value based on domain knowledge, prior data, or by inspecting a sample of the test data.\nThe key idea behind our algorithm is to leverage modern anomaly detectors, which are trained on the clean data. Our algorithm combines the anomaly-score distributions over the clean and contaminated training data in order to derive an alarm threshold that achieves the desired guarantee on the alien detection rate on new test queries. In theory the detection rate guarantee will be met regardless of the quality of the anomaly detector. The quality of the detector, however, has a significant impact on the false alarm rate, with better detectors leading to fewer false alarms.\nWe carry out experiments1 on synthetic and benchmark datasets using a state-of-the-art anomaly detector, the Isolation Forest (Liu et al., 2008). We vary the amount of training data, the fraction α of alien data points, along with the accuracy of the upper bound on α provided to our algorithm. The results indicate that our algorithm can achieve the guaranteed performance when enough data is available, as predicted by the theory. The results also show that for the considered benchmarks, the Isolation Forest anomaly detector is able to support non-trivial false positive rates given enough data. The results also illustrate the inherent difficulty of the problem for small datasets and/or small values of α. Overall, our results provide a useful baseline for driving future work on open category detection with guarantees."
  }, {
    "heading": "2. Related Work",
    "text": "Open category detection is related to the problem of oneclass classification, which aims to detect outliers relative to a single training class. One-class SVMs (OCSVMs) (Schölkopf et al., 2001) are popular for this problem. However, they have been found to perform poorly for open category detection due to poor generalization (Zhou & Huang, 2003), which has been partly addressed by later work (Manevitz & Yousef, 2002; Wu & Ye, 2009; Jin et al., 2004; Cevikalp & Triggs, 2012). OCSVMs have been employed in a multi-class setting similar to open category detection (Heflin et al., 2012; Pritsos & Stamatatos, 2013).\n1Code for reproducing our experiments can be found at https://github.com/liusi2019/ocd.\nHowever, there are no direct mechanisms to control the alien detection rate of these methods, which is a key requirement for our problem setting.\nWork on classification with rejection/abstaining options (Chow, 1970; Wegkamp, 2007; Tax & Duin, 2008; Pietraszek, 2005; Geifman & El-Yaniv, 2017) allows classifiers to abstain from making predictions when they are not confident. While loosely related to open category detection, these approaches do not directly consider the possibility of novel categories, but rather focus on assessing confidence with respect to the known categories. Due to their closedworld discriminative nature, it is easy to construct scenarios where such methods are incorrectly confident about the class of an alien and do not abstain.\nA variety of prior work has addressed variants of open category detection. This includes work on formalizing the concept of “open space” to characterize the region of the feature space outside of the support of the training set (Scheirer et al., 2013). Variants of SVMs have also been developed, such as the One-vs-Set Machine (Scheirer et al., 2013) and the Weibull-calibrated SVM (Scheirer et al., 2014). Additional work has addressed open category detection by tuning the decision boundary based on unlabeled data which contains data from novel categories (Da et al., 2014). Approaches based on nearest neighbor methods have also been proposed (Mendes Júnior et al., 2017). None of these methods, however, allow for the direct control of alien detection rates, nor do they provide theoretical guarantees.\nThere is also recent interest in open category detection for deep neural networks applied to vision and text classification (Bendale & Boult, 2016; Shu et al., 2017). These methods usually train a neural network in a standard closed-world setting, but then analyze various activations in the network in order to detect aliens. Another related line of work is detection of out-of-distribution instances, which is similar to open category detection but assumes that the test data come from a completely different distribution compared to the training distribution (Hendrycks & Gimpel, 2017; Liang et al., 2018). All of this work is quite specialized to deep neural networks and does not provide direct control of alien detection rates or theoretical guarantees."
  }, {
    "heading": "3. Problem Setup",
    "text": "We consider open category detection where there is an unknown nominal data distribution D0 over labeled examples from a known set of category labels. We receive as input a “clean” nominal training set S0 containing k i.i.d. draws from D0. In practice, S0 will correspond to some curated labeled data that contains only known categories of interest.\nWe also receive as input an unlabeled “mixture” dataset Sm that contains n points drawn i.i.d. from a mixture dis-\ntribution Dm. Specifically, the mixture distribution Dm is a combination of the nominal distribution D0 and an unknown alien distribution Da, which is a distribution over novel categories (alien data points). We assume that Da is stationary, so that all alien points that appear as future test queries will also be drawn from Da.\nAt training time, we assume that Dm is a mixture distribution, with probability α of generating an alien data point from Da and probability of 1− α of generating a nominal point. Our results hold even if the test queries come from a mixture with a different value of α as long as the alien test points are drawn from Da.\nGiven these datasets, our problem is to label test instances from Dm as either “alien” or “nominal”. In particular, we wish to achieve a specified alien detection rate, which is the fraction of alien data points in Dm that are classified as “alien” (e.g., 95%). At the same time we would like the false positive rate to be small, which is the fraction of nominal data points incorrectly classified as aliens.\nOur approach to this problem assumes the availability of an anomaly detector that is trained on S0 and assigns anomaly scores to all data points in both S0 and Sm. Intuitively, the anomaly scores order the test examples according to how anomalous they appear relative to the nominal data (higher scores being more anomalous). An ideal detector would rank all alien data points higher than all nominals, though in practice, the ordering will not be so clean. Our approach labels data in Sm by selecting a threshold on the anomaly scores and labeling all data points with scores above the threshold as aliens and the remaining points as nominals. Our key challenge is to select a threshold that provides a guarantee on the alien detection rate."
  }, {
    "heading": "4. Algorithms for Open Category Detection",
    "text": "In order to obtain theoretical guarantees, our algorithm assumes knowledge of the alien mixture probability α that generates the mixture data Sm. Later, we will show that knowing an upper bound on α is sufficient to obtain a guarantee.\nOur approach is based on considering the cumulative distribution functions (CDFs) over anomaly scores of a fixed anomaly detector. Let F0, Fa, and Fm be the CDFs of anomaly scores for the nominal data distribution D0, alien distribution Da, and mixture distribution Dm respectively. Since Dm is a simple mixture of D0 and Da, we can write Fm as\nFm(x) = (1− α)F0(x) + αFa(x).\nFrom this we can derive the CDF for Fa in terms of Fm and F0:\nFa(x) = Fm(x)− (1− α)F0(x)\nα .\nGiven the ability to derive Fa, it is straightforward to achieve an alien detection rate of 1− q (e.g. 95%) by selecting an anomaly score threshold τq that is the q quantile of Fa and raising an alarm on all test queries whose anomaly score is greater than τq .\nIn reality, we do not have access to Fm or F0 and hence cannot exactly determine Fa. Rather, we have samples Sm and S0. Thus, our algorithm works with the empirical CDFs F̂0 and F̂m, which are simple step-wise constant approximations, and estimates an empirical CDF over aliens:\nF̂a(x) = F̂m(x)− (1− α)F̂0(x)\nα . (1)\nOur algorithm computes the above estimate of F̂a and uses it to select a threshold τ̂q to be the largest threshold such that F̂a(τ̂q) ≤ q, where 1 − q is the target alien detection rate. This choice will minimize the number of false alarms. The steps of this algorithm are as follows.\nAlgorithm 1 1: Get anomaly scores for all points in S0 and Sm, denoted x1, x2, . . . , xk and y1, y2, . . . , yn respectively.\n2: Compute empirical CDFs F̂0 and F̂m. 3: Calculate F̂a using equation 1. 4: Output detection threshold\nτ̂q = max{u ∈ S : F̂a(u) ≤ q},\nwhere S = {x1, x2, . . . , xk, y1, y2, . . . , yn}.\nAlthough F̂m and F̂0 are both legal CDFs, the estimate for F̂a from step 3 may not be a legal CDF, because it is the difference of two noisy estimates—it may not increase monotonically and it may even be negative. A good technique for dealing with this problem is to employ isotonization (Barlow & Brunk, 1972) and clipping. Isotonization finds the monotonically increasing function F̂ ∗a closest to F̂a in squared error. To convert F̂a into a legal CDF, define F̌a = min{max{F̂ ∗a ,0},1}, where the min and max operators are applied pointwise to their arguments. We performed experiments (shown in the supplementary materials) to test whether using F̌a in Step 4 would improve the performance of the overall algorithm. We found that it did not."
  }, {
    "heading": "5. Finite Sample Guarantee",
    "text": "In the limit of infinite data (both nominal and mixture) and perfect knowledge of α, F̂a will converge to the true alien CDF, and our algorithm will achieve the desired alien detection rate. In this section, we consider the finite data case where |S0| = |Sm| = n. We derive a value for the sample size n that guarantees with high probability over random\ndraws of S0 and Sm, that fraction 1 − q − of the alien test points will be detected, where is an additional error incurred because of the finite sample size n.\nOur key theoretical tool is a finite sample result on the uniform convergence of empirical CDF functions (Massart, 1990). To use this result, we make the reasonable technical assumption that the nominal and alien CDFs, F0 and Fa, are continuous. In the following, let η be the target alien detection rate, q be the input to Algorithm 1, τ̂q be the estimated q-quantile of the alien CDF (step 4 of Alg. 1), and be an error parameter. The following theorem gives the sample complexity for guaranteeing that 1 − η of the alien examples will be detected using threshold τ̂q .\nTheorem 1. Let S0 and Sm be nominal and mixture datasets containing n i.i.d. samples from the nominal and mixture data distributions respectively. For any ∈ (0, 1−q) and δ ∈ (0, 1), if\nn > 1\n2 ln\n2\n1− √ 1− δ\n( 1 )2 ( 2− α α )2 ,\nthen with probability at least 1− δ, Algorithm 1 will return a threshold τ̂q that achieves an alien detection rate of at least 1− η, where η = q + .\nThe proof is in the Appendix. Note that n grows as O( 1 2α2 log 1 δ ). Hence, this guarantee is polynomial in all relevant parameters, which we believe is the first such guarantee for open category detection. The result can be generalized to the case where n0 < nm; in practice, the larger the mixture sample Sm is, the easier it is to estimate τq, because this provides more alien points for estimating the q-th quantile of Fa.\nThe theorem gives us flexibility in setting and q (the algorithm input) to achieve a guarantee of 1−η. The parameter controls a trade-off between sample size and false alarm rate. To minimize the false alarm rate, we want to make q large (to obtain a larger threshold), so we want to set q close to η. But, as q → η, → 0, and n→∞. To minimize the sample size n, we want to make q as small as possible, because that allows to be larger and hence n becomes smaller. The optimal setting of depends on how the false alarm rate grows with τq, which in turn depends on the relative shape of F0 and Fa. In a real safety application, we can estimate these from S0 and Sm and choose an appropriate q value.\nWhat if we don’t know the exact value of α? If our algorithm uses an upper bound α′ on the true α to compute F̂a, we can still provide a guarantee. In this case, in addition to the assumptions in Theorem 1, we need a concept of an anomaly detector being admissible. We say that an anomaly detector is admissible for a problem, if the anomaly score CDFs satisfy F0(x) ≥ Fm(x) for all x ∈ R. Most reasonable anomaly detectors will be admissible in this sense, since\nthe alien CDF will typically concentrate more mass toward larger anomaly score values compared to F0. Indeed, if this is not the case, there is little hope since there is effectively no signal to distinguish between aliens and nominals.\nCorollary 1. Consider running Algorithm 1 using an upper bound α′ on the true α. Under the same assumptions as Theorem 1, if the anomaly detector is admissible and\nn > 1\n2 ln\n2\n1− √ 1− δ\n( 1 )2 ( 2− α′\nα′\n)2 ,\nthen with probability at least 1− δ, Algorithm 1 will return a threshold τ̂q that achieves an alien detection rate of at least 1− η, where η = q + .\nThe proof is in the Appendix. While we can achieve a guarantee using an upper bound on α′, the returned threshold will be more conservative (smaller) than if we had used the true α. This will result in higher false alarm rates, since more nominal points will be above the threshold. Thus it is desirable to use a value of α′ that is as close to α as possible."
  }, {
    "heading": "6. Experiments",
    "text": "We performed experiments to answer four questions. Question Q1: how accurate is our estimate of τ̂q as a function of n and α? Question Q2: how loose are the bounds from Theorem 1? Question Q3: what are typical values of the false alarm rates for various settings of n and α on real datasets? Question Q4: how do these observed values change if we employ an overestimate α′ > α?\nAll of our experiments employ the Isolation Forest anomaly detector (Liu et al., 2008), which has been demonstrated to be a state-of-the-art detector in recent empirical studies (Emmott et al., 2013). In the Supplementary Materials we show similar results with the LODA anomaly detector (Pevný, 2015).\nTo address Q1 and Q2, we run controlled experiments on synthetic data. The data points are generated from 9- dimensional normal distributions. The dimensions of the nominal distribution D0 are independently distributed as N(0, 1). The alien distribution is similar, but with probability 0.4, 3 of the 9 dimensions (chosen uniformly at random) are distributed as N(3, 1) and with probability 0.6, 4 of the 9 dimensions (chosen uniformly at random) follow N(3, 1). This ensures that the anomalies are not highly similar to each other and models the situation in which there are many different kinds of alien objects, not just a single alien class forming a tight cluster.\nIn each experiment, the nominal dataset and the mixture dataset are of the same size n, and the mixture dataset contains a proportion α of anomaly points. We fixed the target quantile to be q = 0.05. The experiments are\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95 1 Re ca ll\n5! = 0.01\n100⋯10000 100⋯10000\n! = 0.05\n100⋯10000\n! = 0.10\n100⋯10000\n! = 0.20 ! = 0.50\n) = 100⋯10000\nFigure 1. Comparison of recall achieved by τ̂q compared to oracle recall of 0.95. Error bars are 95% confidence intervals. Settings of n and α increase from left to right starting with α = 0.01 and n ∈ {100, 500, 1K, 5K, 10K} up to α = 0.5 and n = 10K.\ncarried out for n ∈ {100, 500, 1K, 5K, 10K} and α ∈ {0.01, 0.05, 0.10, 0.20, 0.50}. For testing, we create two large datasets G0 and Ga, with G0 being a pure nominal dataset, Ga being a pure alien dataset, and |G0| = |Ga| = 20K. The Isolation Forest algorithm computes 1000 full depth isolation trees on the nominal data. Each tree is grown on a randomly-selected 20% subsample of the clean data points. We compute anomaly scores for the nominal points via out-of-bag estimates and anomaly scores for the mixture points, G0, and Ga using the full isolation forest. For each combination of n and α, we repeat the experiment 100 times. We measure the fraction of aliens detected (the “recall”) and the fraction of nominal points declared to be alien (the “false positive rate”) by applying the τ̂q estimate to threshold the anomaly scores in G0 and Ga.\nTo assess the accuracy of our τ̂q estimates (Q1), we could compare them to the true values. However, this comparison is hard to interpret, because τ is expressed on the scale of anomaly scores, which are somewhat arbitrary. Instead, Figure 1 plots the recall achieved by τ̂q. If τ̂q had been estimated perfectly, the recall would always be 1−q = 0.95. However, we see that the recall is often less than 0.95, which indicates that τ̂q is over-estimated, especially when n and α are small. This behavior is predicted by our theory, where we see that the sample size requirements grow inversely with α2. For larger α and n, the recall guarantee is generally achieved. Figure 2 compares the false positive rate of the true oracle τq to the false positive rate of the estimate τ̂q . For each combination of α and n, we have 100 replications of the experiment and therefore 100 estimates τ̂a and 100 FPR rates. For each of these, the true FPR is computed using G0.\n2! = 0.01\n100⋯10000 100⋯10000\n! = 0.05\n100⋯10000\n! = 0.10\n100⋯10000\n! = 0.20 ! = 0.50\n) = 100⋯100000\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nFP R\nFigure 2. Comparison of oracle FPR to the FPR achieved by τ̂q . Error bars span from the 25th to 75th percentile with the blue dot marking the median of the 100 trials. Orange markers indicate the oracle FPR. Settings of n and α increase from left to right starting with α = 0.01 and n ∈ {100, 500, 1K, 5K, 10K} up to α = 0.5 and n = 10K.\nThe error bars summarize the resulting 100 FPR values by the median and inter-quartile range. We see that for small n and α, the FPR can be quite different from the oracle rate, but for larger n and α, the estimates are very good.\nTo assess the looseness of the bounds (Q2), for each combination of n and α, we fix δ = 0.05 and compute the value of η such that 95 of the 100 runs achieved a recall of at least 1− η (thus η empirially achieves the 1− δ guarantee). We then compute = η − q and the corresponding required sample size n∗ according to Theorem 1. Figure 3 shows a\nplot of n∗ versus the actual n. The distance of these points from the n∗ = n diagonal line show that the theory is fairly loose, although it becomes tighter as n gets large.\nBenchmark Data Experiments. To address our third and fourth questions, we performed experiments on six UCI multiclass datasets: Landsat, Opt.digits, pageb, Shuttle, Covertype and MNIST. In addition to these, we provide results for the Tiny ImageNet dataset. In each multiclass dataset, we split the classes into two groups: nominal and alien. For Tiny ImageNet, we train a deep neural network classifier on 200 nominal classes and treat the remaining 800 as aliens. The nominal classes for UCI datasets are MNIST(1,3,7), Landsat(1,7), OCR(1,3,4,5,7), pageb(1,5), Letter recognition(1,3), and Shuttle(1,4). We generated\nnominal and mixture datasets for various values of α. The value of n for each dataset is 1532 for Landsat,788 for Letter recognition, 568 for OCR, 4912 for pageb, 5000 for Shuttle, 13,624 for Covertype, 11,154 for MNIST, and 10,000 for Tiny ImageNet. Because we cannot create datasets with large n, we cannot measure the true value of τq .\nAfter computing the anomaly scores for both nominal and mixture datasets, we applied Algorithm 1 within a 10-fold cross validation. We divide the mixture data points at random into 10 groups. For each fold, we estimate F̂a and τ̂a from 9 of the 10 groups and then score the mixture points in the held-out fold according to τ̂a. In all other respects, the experimental protocol is the same as for the synthetic data. For Tiny ImageNet, the anomaly scores are obtained by applying a baseline method (Hendrycks & Gimpel, 2017).\nTo answer Q3, Figures 4 and 6 plot the false positive rate as a function of α for the UCI and vision datasets, respectively. We see that the FPR ranges from 3.6% to 26.9% on UCI depending on the dataset and the level of α. The vision datasets have higher FPR, especially MNIST, which has a large number of alien classes that are not distinguished well by the anomaly detector. The FPR depends primarily on the domain, because the key issue is how well the anomaly detector distinguishes between nominal and alien examples. The false alarm rate generally improves as α increases. In some applications, it may be possible to enrich Sm so that α is larger on the training set to take advantage of this phenomenon. It is interesting to note that once τ̂a has been computed, it can be applied to test datasets having different (or unknown) values of α.\nFigures 5 and 7 plot the recall rate as a function of α for the UCI and vision datasets. We set q = 0.05 in these experiments. Theorem 1 only guarantees a recall of 1−q− ,\nwhere depends on n. Hence, it is nice to see that for three of the domains (Shuttle, Covertype, and Landsat) in UCI and for both vision datasets, the recall is very close to 1− q = 0.95. These are the domains with the largest values of n. The value of α has a bigger impact on recall than it does on FPR. This is because the effective number of alien training examples is αn, which can be very small for some datasets when α = 0.1. This shows that in applications such as fraud detection, where α may be very small, the mixture dataset Sm needs to be very large.\nTo answer Q4 regarding the impact of using an incorrect value α′ > α, we repeated these experiments with α′ = α+ξ, for ξ ∈ {0.002, 0.004, 0.006, 0.008, 0.010}. Figure 8 plots the change in false positive rate and recall as a function\nof α′ − α. Two points are plotted for each combination of α′ and dataset, the change in Recall and the change in FPR. We observe that the recall increases slightly (in the range from 0.01 to 0.05). However, the false positive rate increases by much larger amounts (from 0.01 to 0.336). This demonstrates that it is very important to determine the value of α accurately."
  }, {
    "heading": "7. Summary",
    "text": "We have taken a step toward open category detection with guarantees by providing a PAC-style guarantee on the probability of detecting 1− η of the aliens on the test data. This is the first such guarantee under any similarly general conditions. We have shown that this guarantee is satisfied in our experiments, although the guarantee is somewhat loose, especially on small training sets. Obtaining a guarantee requires more data than standard PAC guarantees on expected prediction accuracy. This is because we must estimate the q quantile of the alien anomaly score distribution, where q is typically quite small. Nonetheless, our experiments show that our algorithm gives good recall performance and non-trivial false alarm rates on datasets of reasonable size.\nIt is important to note that the very formulation of a PACstyle guarantee on the probability of detecting aliens requires assuming that the aliens are drawn from a welldefined distribution Da. While this is appropriate in some applications, such as the insect survey application described in the introduction, it is not appropriate for adversarial settings. In such settings, a PAC-style guarantee does not make sense, and some other form of safety guarantee needs to be formulated.\nTo obtain the guarantee, we employ two training datasets: a clean dataset that contains no aliens and an (unlabeled) contaminated dataset that contains a known fraction α of aliens. An important theoretical problem for future research is to develop a method that can estimate a tight upper bound on α̂ > α. We believe this is possible, but we have not yet found a method that guarantees that α̂ > α.\nOur guarantee requires more data as α becomes small. Fortunately, when α is small, it may be possible in some applications to afford lower recall rates, since the frequency of aliens will be smaller. However, in safety-critical applications where a single undetected alien poses a serious threat, there is little recourse other than to collect more data or allow for higher false positive rates."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by a gift from Huawei, Inc., and grants from the Future of Life Institute and the NSF Grant 1514550. Any opinions, findings, and conclusions\nor recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors."
  }, {
    "heading": "A. Proof for Theorem 1",
    "text": "Suppose there are n random variables which are i.i.d. from the distribution with CDF F and let F̂n be the empirical CDF calculated from this sample. Then Massart (1990) shows that\nP ( √ n sup\nx |F̂n(x)− F (x)| > λ) ≤ 2 exp(−2λ2) (2)\nholds without any restriction on λ. Making use of this, and assuming we use the same sample size n for both the mixture dataset and the clean data set, for any ∈ (0, 1−q), we seek to determine how large n needs to be in order to guarantee that with probability at least 1 − δ our quantile estimate τ̂q satisfies Fa(τ̂q) ≤ q + . To achieve this, we want to have\nP (sup x |F̂a(x)− Fa(x)| > ) ≤ δ.\nWe have\nP (sup x |F̂a(x)− Fa(x)| > )\n= P (sup x | F̂m(x)− (1− α)F̂0(x) α −\nFm(x)− (1− α)F0(x) α | > )\n= P (sup x | 1 α (F̂m(x)− Fm(x))−\n1− α α (F̂0(x)− F0(x))| > )\n≤ P (( 1 α sup x |F̂m(x)− Fm(x)|+\n1− α α sup x |F̂0(x)− F0(x)|) > )\n≤ P ({ 1 α sup x |F̂m(x)− Fm(x)| > 1 2− α }\n∪ {1− α α sup x |F̂0(x)− F0(x)| > 1− α 2− α })\n= P ({sup x |F̂m(x)− Fm(x)| >\nα\n2− α }\n∪ {sup x |F̂0(x)− F0(x)| >\nα\n2− α }).\nMaking use of (2), when\nn > 1\n2 ln\n2\n1− √ 1− δ ( 1 )2( 2− α α )2,\nwe will have\nP (sup x |F̂m(x)− Fm(x)| >\nα\n2− α ) ≤ 1−\n√ 1− δ,\nP (sup x |F̂0(x)− F0(x)| >\nα\n2− α ) ≤ 1−\n√ 1− δ.\nIn this case we will have\nP (sup x |F̂a(x)− Fa(x)| > )\n≤ 1− P ({sup x |F̂m(x)− Fm(x)| ≤\nα\n2− α }\n∩ {sup x |F̂0(x)− F0(x)| ≤\nα\n2− α })\n≤ 1− (1− 1 + √ 1− δ)2\n= δ.\nNow we have with probability at least 1− δ,\n|F̂a(x)− Fa(x)| ≤ , ∀x ∈ R.\nIf this inequality holds, then for any value τ̂q such that F̂a(τ̂q) ≤ q, we have\nFa(τ̂q) ≤ F̂a(τ̂q) + ≤ q + .\nSo we have with probability at least 1− δ, any τ̂q satisfying F̂a(τ̂q) ≤ q will satisfy Fa(τ̂q) ≤ q + ."
  }, {
    "heading": "B. Proof for Corollary 1",
    "text": "If α′ ≥ α, and if we write\nF ′a(x) = Fm(x)− (1− α′)F0(x)\nα′ ,\nthen F ′a is still a legal CDF, because\nF ′a(−∞) = 0, F ′a(∞) = 1,\nand it is easy to show that F ′a is monotonically nondecreasing.\nBut\nF ′a(x)−Fa(x) = (α− α′)(Fm(x)− F0(x))\nαα′ ≥ 0,∀x ∈ R,\nand because of this, if we let τ̂ ′q denote the threshold we get from using α′, we will have Fa(τ̂ ′q) ≤ F ′a(τ̂ ′q). By the proof of previous theorem, we know that when n > 1 2 ln 2 1− √ 1−δ ( 1 ) 2( 2−α ′ α′ ) 2, we have with probability at least 1− δ, F ′a(τ̂ ′q) ≤ q + , and thus we have Fa(τ̂ ′q) ≤ q + ."
  }],
  "year": 2018,
  "references": [{
    "title": "The isotonic regression problem and its dual",
    "authors": ["RE Barlow", "Brunk", "HD"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1972
  }, {
    "title": "Towards open set deep networks",
    "authors": ["A. Bendale", "T.E. Boult"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2016
  }, {
    "title": "Efficient object detection using cascades of nearest convex model classifiers",
    "authors": ["H. Cevikalp", "B. Triggs"],
    "venue": "In 2012 IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2012
  }, {
    "title": "On optimum recognition error and reject tradeoff",
    "authors": ["C. Chow"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1970
  }, {
    "title": "Learning with augmented class by exploiting unlabeled data",
    "authors": ["Da", "Qing", "Yu", "Yang", "Zhou", "Zhi-Hua"],
    "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,",
    "year": 2014
  }, {
    "title": "Systematic construction of anomaly detection benchmarks from real data",
    "authors": ["Emmott", "Andrew F", "Das", "Shubhomoy", "Dietterich", "Thomas", "Fern", "Alan", "Wong", "Weng-Keen"],
    "venue": "In Proceedings of the ACM SIGKDD workshop on outlier detection and description,",
    "year": 2013
  }, {
    "title": "Detecting and classifying scars, marks, and tattoos found in the wild",
    "authors": ["B. Heflin", "W. Scheirer", "T.E. Boult"],
    "venue": "In 2012 IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS),",
    "year": 2012
  }, {
    "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
    "authors": ["Hendrycks", "Dan", "Gimpel", "Kevin"],
    "venue": "In Proceedings of International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Face detection using one-class-based support vectors",
    "authors": ["Jin", "Hongliang", "Liu", "Qingshan", "Lu", "Hanqing"],
    "venue": "In Sixth IEEE International Conference on Automatic Face and Gesture Recognition,",
    "year": 2004
  }, {
    "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
    "authors": ["Liang", "Shiyu", "Li", "Yixuan", "R. Srikant"],
    "venue": "International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "Isolation forest",
    "authors": ["Liu", "Fei Tony", "Ting", "Kai Ming", "Zhou", "Zhi-Hua"],
    "venue": "In Data Mining,",
    "year": 2008
  }, {
    "title": "One-class svms for document classification",
    "authors": ["Manevitz", "Larry M", "Yousef", "Malik"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2002
  }, {
    "title": "The tight constant in the dvoretzky-kieferwolfowitz inequality",
    "authors": ["P. Massart"],
    "venue": "The Annals of Probability,",
    "year": 1990
  }, {
    "title": "Loda: Lightweight on-line detector of anomalies",
    "authors": ["Pevný", "Tomáš"],
    "venue": "Machine Learning,",
    "year": 2014
  }, {
    "title": "Optimizing abstaining classifiers using roc analysis",
    "authors": ["Pietraszek", "Tadeusz"],
    "venue": "In Proceedings of the 22Nd International Conference on Machine Learning,",
    "year": 2005
  }, {
    "title": "OpenSet Classification for Automated Genre Identification, pp. 207–217",
    "authors": ["Pritsos", "Dimitrios A", "Stamatatos", "Efstathios"],
    "year": 2013
  }, {
    "title": "Toward open set recognition",
    "authors": ["W.J. Scheirer", "A. de Rezende Rocha", "A. Sapkota", "T.E. Boult"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2013
  }, {
    "title": "Probability models for open set recognition",
    "authors": ["W.J. Scheirer", "L.P. Jain", "T.E. Boult"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2014
  }, {
    "title": "Estimating the support of a high-dimensional distribution",
    "authors": ["Schölkopf", "Bernhard", "Platt", "John C", "Shawe-Taylor", "Smola", "Alex J", "Williamson", "Robert C"],
    "venue": "Neural Comput.,",
    "year": 2001
  }, {
    "title": "DOC: deep open classification of text",
    "authors": ["Shu", "Lei", "Xu", "Hu", "Liu", "Bing"],
    "venue": "documents. CoRR,",
    "year": 2017
  }, {
    "title": "Growing a multi-class classifier with a reject option",
    "authors": ["D.M.J. Tax", "R.P.W. Duin"],
    "venue": "Pattern Recognition Letters,",
    "year": 2008
  }, {
    "title": "Lasso type classifiers with a reject option",
    "authors": ["Wegkamp", "Marten H"],
    "year": 2007
  }, {
    "title": "A small sphere and large margin approach for novelty detection using training data with outliers",
    "authors": ["M. Wu", "J. Ye"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2009
  }, {
    "title": "Relevance feedback in image retrieval: A comprehensive review",
    "authors": ["Zhou", "Xiang Sean", "Huang", "Thomas S"],
    "venue": "Multimedia Systems,",
    "year": 2003
  }],
  "id": "SP:123df44051ac8170a5cd60abd17c0e404e23955f",
  "authors": [{
    "name": "Si Liu",
    "affiliations": []
  }, {
    "name": "Risheek Garrepalli",
    "affiliations": []
  }, {
    "name": "Thomas G. Dietterich",
    "affiliations": []
  }, {
    "name": "Alan Fern",
    "affiliations": []
  }, {
    "name": "Dan Hendrycks",
    "affiliations": []
  }],
  "abstractText": "Open category detection is the problem of detecting “alien” test instances that belong to categories or classes that were not present in the training data. In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a “clean” training set that contains only the target categories of interest and an unlabeled “contaminated” training set that contains a fraction α of alien examples. Under the assumption that we know an upper bound on α, we develop an algorithm with PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Empirical results on synthetic and standard benchmark datasets demonstrate the regimes in which the algorithm can be effective and provide a baseline for further advancements.",
  "title": "Open Category Detection with PAC Guarantees"
}