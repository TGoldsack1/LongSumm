{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Graphical Models (GMs) express the factorization of the joint multivariate probability distribution over subsets of variables via graphical relations among them. They have played an important role in many fields, including computer vision (Freeman et al., 2000), speech recognition (Bilmes, 2004), social science (Scott, 2017) and deep learning (Hinton & Salakhutdinov, 2006). Given a GM, computing the partition function Z (the normalizing constant) is the essence of other statistical inference tasks such as marginalization and sampling. The partition function can be calculated efficiently in tree-structured GMs through an\n1School of Electrical Engineering, KAIST, Daejeon, South Korea 2Theoretical Division, T-4 & Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, NM 87545, USA 3Skolkovo Institute of Science and Technology, 143026 Moscow, Russia 4University of Cambridge, UK 5The Alan Turing Institute, UK 6AITrics, Seoul, South Korea. Correspondence to: Jinwoo Shin <jinwoos@kaist.ac.kr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\niterative (dynamic programming) algorithm eliminating, i.e. summing up, variables sequentially. In principle, the elimination strategy extends to arbitrary loopy graphs, but the computational complexity is exponential in the tree-width, e.g., the junction-tree method (Shafer & Shenoy, 1990). Formally, the computation task is #P-hard even to approximate (Jerrum & Sinclair, 1993).\nVariational approaches are often the most popular practical choice for approximate computation of the partition function. They map the counting problem into an approximate optimization problem stated over a polynomial (in the graph size) number of variables. The optimization is typically solved iteratively via a message-passing algorithm, e.g., mean-field (Parisi, 1988), belief propagation (Pearl, 1982), tree-reweighted (Wainwright et al., 2005), or gauges and/or re-parametrizations (Ahn et al., 2017; 2018 (accepted to appear). Lack of accuracy control and difficulty in forcing convergence in an acceptable number of steps are, unfortunately, typical for hard GM instances. Markov chain Monte Carlo methods (e.g., see Alpaydin, 2014) are also popular to approximate the partition function, but typically suffer, even more than variational methods, from slow convergence/mixing.\nApproximate elimination is a sequential method to estimate the partition function. Each step consists of summation over variables followed by (or combined with) approximation of the resulting complex factors. Notable flavors of this method include truncation of the Fourier coefficients (Xue et al., 2016), approximation by random mixtures of rank-1 tensors (Wrigley et al., 2017), and arguably the most popular, elimination over mini-buckets (Dechter & Rish, 2003; Liu & Ihler, 2011). One advantage of the mini-bucket elimination approach is the ability to control the trade-off between computational complexity and approximation quality by adjusting an induced-width parameter. Note that analogous control in variational methods, such as varying region sizes in generalized belief propagation (Yedidia et al., 2001), typically results in much more complicated optimization formulations to solve. Another important advantage of mini-bucket elimination is that it is always guaranteed to terminate and, usually, it does so quickly. This is in contrast to iterative message-passing implementations of variational methods which can be notoriously slow on difficult instances.\nContribution. We improve the approximation quality of mini-bucket methods using tensor network and renormalization group approaches from statistical physics. In this regard, our method extends a series of recent papers exploring multi-linear tensor network transformations/contractions (Novikov et al., 2014; Wrigley et al., 2017; Ahn et al., 2017; 2018 (accepted to appear). More generally, tensor network renormalization algorithms (Levin & Nave, 2007; Evenbly & Vidal, 2015) have been proposed in the quantum and statistical physics literature for estimating partition functions. The algorithms consist of coarse-graining the graph/network by contracting sub-graphs/networks using a low-rank projection as a subroutine. However, the existing renormalization methods in the physics literature have focused primarily on a restricted class of tensor-factorized models over regular grids/lattices,1 while factor-graph models (Clifford, 1990) over generic graphical structures are needed in most machine learning applications.\nFor generalizing them to factor-graph models, one would face at two challenges: (a) coarse-graining of the tensor network relies on the periodic structure of grid/lattices and (b) its low-rank projections are only defined on “edge variables” that allows only two adjacent factors. To overcome them, we first replace the coarse-graining step by sequential elimination of the mini-bucket algorithms, and then use the strategy of “variable splitting” in order to generate auxiliary edge variables. Namely, we combine ideas from tensor network renormalization and the mini-bucket schemes where one is benefical to the other. We propose two algorithms, which we call MBR and GBR:\n• Mini-bucket renormalization (MBR) consists of sequentially splitting summation over the current (remaining) set of variables into subsets – multiple minibuckets which are then “renormalized”. We show that this process is, in fact, equivalent to applying low-rank projections on the mini-buckets to approximate the variable-elimination process, thus resulting in better approximation than the original mini-bucket methods. In particular, we show how to resolve approximate renormalization locally and efficiently through application of truncated singular value decomposition (SVD) over small matrices.\n• While MBR is based on a sequence of local low-rank approximations applied to the mini-buckets, globalbucket renormalization (GBR) extends MBR by approximating mini-buckets globally. This is achieved by first applying MBR to mini-buckets, then calibrating the choice of low rank projections by minimizing the partition function approximation error with respect to renormalization of the “global-bucket”. Hence, GBR\n1 The special models are related to what may be called Forneystyle grids/lattices (Forney, 2001) in the GM community.\ntakes additional time to run but may be expected to yield better accuracy.\nBoth algorithms are easily applicable to arbitrary GMs with interactions (factors) of high orders, hyper-graphs and large alphabets. We perform extensive experiments on synthetic (Ising models on complete and grid graphs) and real-world models from the UAI dataset. In our experiments, both MBR and GBR show performance superior to other stateof-the-art elimination and variational algorithms."
  }, {
    "heading": "2. Preliminaries",
    "text": "Graphical model. Consider a hyper-graph G = (V, E) with vertices V = {1, · · · , n} and hyper-edges E ⊂ 2V . A graphical model (GM)M = (G,F) associates a collection of n discrete random variables x = [xi : i ∈ V] ∈ XV =∏ i∈V Xi with the following joint probability distribution:\nPr(x) = 1\nZ ∏ α∈E fα(xα), Z = ∑ x ∏ α∈E fα(xα),\nwhere Xi = {1, 2, · · · di}, xα = [xi : i ∈ α], F = {fα}α∈E is a set of non-negative functions called factors, and Z is the normalizing constant called the partition function that is computationally intractable.\nAlgorithm 1 Bucket Elimination (BE)\n1: Input: GMM† = (G†,F†) and elimination order o.\n2: F ← F† 3: for i in o do 4: Bi ← {fα|fα ∈ F , i ∈ α} 5: Generate new factor fBi\\{i} by (1). 6: F ← F ∪ {fBi\\{i}} \\ Bi 7: end for 8: Output: Z = ∏ fα∈F fα\nMini-bucket elimination. Bucket (or variable) elimination (BE, Dechter, 1999; Koller & Friedman, 2009) is a procedure for computing the partition function exactly based on sequential elimination of variables. Without loss of generality, we assume through out the paper that the elimination order is fixed o = [1, · · · , n]. BE groups factors by placing each factor fα in the “bucket” Bi ⊂ F of its earliest argument i ∈ α appearing in the elimination order o. Next, BE eliminates the variable by introducing a new factor marginalizing the product of factors in it, i.e.,\nfBi\\{i}(xBi\\{i}) = ∑ xi ∏ fα∈Bi fα(xα). (1)\nHere, xBi\\{i} abbreviates xV(Bi)\\{i}, where V(Bi) indicates the set of variables associated with the bucket Bi. The\nsubscript in fBi\\{i} represents a similar abbreviation. Finally, the new function fBi\\{i} is added to another bucket corresponding to its earliest argument in the elimination order. Formal description of BE is given in Algorithm 1.\nOne can easily check that BE applies a distributive property for computing Z exactly: groups of factors corresponding to buckets are summed out sequentially, and then the newly generated factor (without the eliminated variable) is added to another bucket. The computational cost of BE is exponential with respect to the number of uneliminated variables in the bucket, i.e., its complexity is O ( dmaxi |V(Bi)||V| ) . Here, maxi∈V |V(Bi)| is called the induced width of G given the elimination order o, and the minimum possible induced width across all possible o is called the tree-width. Furthermore, we remark that summation of GM over variables defined on the subset of vertices α, i.e., ∑ xα ∏ β∈E fβ , can also be computed via BE in O(dmaxi |V(Bi)|+|V\\α||V|) time by summing out variables in elimination order oα on α.\nMini-bucket elimination (MBE, Dechter & Rish, 2003) achieves lower complexity by approximating each step of BE by splitting the computation of each bucket into several smaller “mini-buckets”. Formally, for each variable i in the elimination order o, the bucket Bi is partitioned into mi mini-buckets {B`i} mi `=1 such that Bi = ⋃mi `=1 B`i and B`1i ∩ B `2 i = ∅ for any `1, `2. Next, MBE generates new factors differently from BE as follows:\nfB`i\\{i}(xB`i\\{i}) = maxxi ∏ fα∈B`i fα(xα), (2)\nfor all ` = 1, · · · ,mi − 1 and\nfBmii \\{i} (xBmii \\{i} ) = ∑ xi ∏ fα∈B mi i fα(xα). (3)\nOther steps are equivalent to that of BE. Observe that MBE replaces the exact marginalization of the bucket in (1) by its upper bound, i.e., ∑ xi ∏ fα∈Bi fα ≤ ∏m `=1 fB`i\\{i}, and hence yields an upper bound of Z. We remark that one could instead obtain a lower bound for Z by replacing max by min in (2).\nNote that one has to choose mini-buckets for MBE carefully as their sizes typically determine complexity and accuracy: smaller mini-buckets may be better for speed, but worse in accuracy. Accordingly, MBE has an additional induced width bound parameter ibound as the maximal size of a mini-bucket, i.e., |V(B`i )| ≤ ibound + 1. The time complexity of MBE is O ( dibound+1|E| ·maxα∈E |α| ) , since the maximum number of mini-buckets is bounded by |E|maxα∈E |α|."
  }, {
    "heading": "3. Mini-Bucket Renormalization",
    "text": "We propose a new scheme, named mini-bucket renormalization (MBR). Our approach approximates BE by splitting each bucket into several smaller mini-buckets to be “renormalized”. Inspired by tensor renormalization groups (TRG, Levin & Nave, 2007, see also references therein) in the physics literature, MBR utilizes low-rank approximations to the mini-buckets instead of simply applying max (or min) operations as in MBE. Intuitively, MBR is similar to MBE but promises better accuracy."
  }, {
    "heading": "3.1. Algorithm Description",
    "text": "For each variable i in the elimination order o, MBR partitions a bucket Bi into mi distinct mini-buckets {B`i} mi `=1 with maximal size bounded by ibound. Then for ` = 1, · · · ,mi − 1, mini-bucket B`i is “renormalized” through replacing vertex i by its replicate i` and then introducing local factors r`i , ri` for error compensation, i.e.,\nB̃`i ← {fα\\{i}∪{i`}|fα ∈ B`i} ∪ {r`i , ri`}.\nHere, r`i , ri` are local “compensating/renormalizing factors”, chosen to approximate the factor fB`i = ∏ fα∈B`i\nfα well, where MBE approximates it using (2) and (3). See Figure 1 for illustration of the renormalization process. Specifically, the local compensating/renormalizing factors are chosen by solving the following optimization:\nmin r`i ,ri`\n∑ xB`\ni\n( fB`i (xB`i )− f̃B`i (xB`i ) )2 , (4)\nwhere f̃B`i is the factor induced on xB`i from the renormalized mini-bucket B̃`i :\nf̃B`i (xB`i ) = ∑ x`i ∏ fα∈B̃`i fα(xα)\n= r`i (xi) ∑ x i` ri`(xi`) ∏ fα∈B`i fα(xi` ,xα\\{i}∪{i`}).\nWe show that (4) can be solved efficiently in Section 3.2. After all mini-buckets are processed, factors can be summed over the variables xi1 , · · · , ximi−1 and xi separately, i.e.,\nintroduce new factors as follows: fB`i\\{i}(xB`i\\{i}) = ∑ x i` ∏ fα∈B̃`i\\{r`i} fα(xα), (5)\n= ∑ x i` r`i (xi`) ∏ fα∈B`i fα(xi` ,xα\\{i}∪{i`}),\nfor ` = 1, · · · ,mi − 1 and\nfBmii \\{i} (xBmii \\{i} ) = ∑ xi mi−1∏ `=1 r`i (xi`) ∏\nfα∈B mi i\nfα(xα).\n(6) Resulting factors are then added to its corresponding minibucket and repeat until all buckets are processed, like BE and MBE. Here, one can check that mi∏ `=1 fB`i\\{i}(xB`i\\{i}) = ∑ xi fBmii (xBmii ) mi−1∏ `=1 f̃B`i (xB`i ),\n≈ ∑ xi mi∏ `=1 fB`i (xB`i ),\nfrom (4) and MBR indeed approximates BE. The formal description of MBR is given in Algorithm 2.\nAlgorithm 2 Mini-bucket renormalization (MBR)\n1: Input: GMM† = (G†,F†), elimination order o and induced width bound ibound.\n2: F ← F† 3: for i in o do 4: Bi ← {fα|fα ∈ F , i ∈ α} 5: Divide Bi into mi subgroups {B`i} mi `=1 such that |V(B`i )| ≤ ibound+ 1 for ` = 1, · · · ,mi. 6: for ` = 1, · · · ,mi − 1 do 7: Generate compensating factors r`i , ri` by (4). 8: Generate new factor fB`i\\{i} by (5). 9: end for\n10: Generate new factor fBmii \\{i} by (6). 11: for ` = 1, · · · ,mi do 12: F ← F ∪ {fB`i\\{i}} \\ B ` i 13: end for 14: end for 15: Output: Z = ∏ fα∈F fα"
  }, {
    "heading": "3.2. Complexity",
    "text": "The optimization (4) is related to the rank-1 approximation on fB`i , which can be solved efficiently via (truncated) singular value decomposition (SVD). Specifically, let M be a d× d|V(B`i )|−1 matrix representing fB`i as follows:\nM xi, ∑ j∈V(B`i ),j 6=i xj ∏ k∈V(B`i ),k>j d  = fB`i (xB`i ). (7)\nThen rank-1 truncated SVD for M solves the following optimization:\nmin r1,r2 ‖M− r1r>2 M‖F ,\nwhere optimization is over d-dimensional vectors r1, r2 and ‖·‖F denotes the Frobenious norm. Namely, the solution r1 = r2 becomes the most significant (left) singular vector of M, associated with the largest singular value.2 Especially, since M is a non-negative matrix, its most significant singular vector is always non-negative due to the Perron-Frobenius theorem (Perron, 1907). By letting ri`(x) = r1(x) and r`i (x) = r2(x), one can check that this optimization is equivalent to (4), where in fact, ri`(x) = r ` i (x), i.e., they share the same values. Due to the above observations, the complexity of (4) is NSVD(M) that denotes the complexity of SVD for matrix M. Therefore, the overall complexity becomes\nO (NSV D(M) · T ) = O ( NSVD(M) · |E| ·max α∈E |α| ) ,\nwhere NSVD(M) = O(dibound+2) in general, but typically much faster in the existing SVD solver."
  }, {
    "heading": "4. Global-Bucket Renormalization",
    "text": "In the previous section, MBR only considers the local neighborhood for renormalizing mini-buckets to approximate a single marginalization process of BE. Here we extend the approach and propose global-bucket renormalization (GBR), which incorporates a global perspective. The new scheme re-updates the choice of compensating local factors obtained in MBR by considering factors that were ignored during the original process. In particular, GBR directly minimizes the error in the partition function from each renormalization, aiming for improved accuracy compared to MBR."
  }, {
    "heading": "4.1. Intuition and Key-Optimization",
    "text": "Renormalized GMs in MBR. For providing the underlying design principles of GBR, we first track an intermediate estimation of the partition function made during MBR by characterizing the corresponding sequence of renormalized GMs. Specifically, we aim for constructing a sequence of T + 1 GMs M(1), · · · ,M(T+1) with T = ∑n i=1(mi − 1) by breaking each i-th iteration of MBR into mi − 1 steps of GM renormalizations, M(1) is the original GM, and each transition fromM(t) toM(t+1) corresponds to renormalization of some mini-bucket B`i to B̃`i . Then, the intermediate estimation for the original partition function Z at the t-th step is partition function Z(t) ofM(t) where the last one Z(T+1) is the output of MBR.\n2 r1 = r2 holds without forcing it.\nTo this end, we introduce “scope” for each factor fα appearing in MBR to indicate which parts of GM are renormalized at each step. In particular, the scope Sfα = (Gfα ,Ffα) consists of a graph Gfα = (Vfα , Efα) and set of factors Ffα that are associated to fα as follows:\nfα(xα) = ∑\nxVfα\\α ∏ fβ∈Ffα fβ(xβ).\nInitially, scopes associated with initial factors are defined by themselves, i.e.,\nSfα ← ((α, {α}), {fα}), (8)\nfor each factor fα of M(1), and others of M(t) = ((V(t), E(t)),F (t)) with t ≥ 2 are to be defined iteratively under the MBR process, as we describe in what follows.\nConsider the t-th iteration of MBR, where mini-bucket B`i is being renormalized into B̃`i . Then scope Sf for all f ∈ B`i goes through renormalization by replacing every i in the scope by i` as follows:\nS̃f ← (Vf \\ {i}∪{i`}, {ᾱ|α ∈ Ef}, {fᾱ|fα ∈ Ff}), (9)\nwhere ᾱ =\n{ α \\ {i} ∪ {i`} if i ∈ α\nα otherwise . Then, the re-\nspective GM is renormalized accordingly for the change of scopes, in addition to compensating factors r`i , ri` :\nV(t+1) ← V(t) ∪ {i`},\nE(t+1) ← E(t) \\ EB`i ∪ ẼB`i ∪ {{i}, {i `}}, (10)\nF (t+1) ← F (t) \\ FB`i ∪ F̃B`i ∪ {r ` i , ri`},\nwhere EB`i = ∪f∈B`iEf , and other union of scope components ẼB`i ,VB`i , ṼB`i ,FB`i , F̃B`i are defined similarly. Finally, scope SfB`\ni \\{i}\nfor newly generated factors fB`i\\{i} is\nSfB` i \\{i} ← ((ṼB`i , ẼB`i ∪ {{i `}}), F̃B`i ∪ {ri`}). (11)\nFurthermore, if ` = mi − 1, we have\nSfBmi i \\{i} ← ((VBmii , EBmii ∪ {{i}}),FBmii ∪ {r ` i} mi−1 `=1 ).\n(12)\nThis is repeated until the MBR process terminates, as formally described in Algorithm 3. By construction, the output of MBR is equal to the partition function of the last GM M(T+1), which is computable via BE with induced width smaller than ibound+ 1 given elimination order\nõ = [11, · · · , 1m1−1, 1, · · · , n1, · · · , nmn−1, n]. (13)\nSee Algorithm 3 for the formal description of this process, and Figure 2 for an example.\nOptimizing intermediate approximations. Finally, we provide an explicit optimization formulation for minimizing the change of intermediate partition functions in terms of induced factors. Specifically, for each t-th renormalization, i.e., fromM(t) toM(t+1), we consider change of the following factor fi induced from global-bucketF (t) to variable xi in a “skewed” manner as follows:\nfi(x (1) i , x (2) i ) := ∑ xV(t)\\{i} ∏ fα∈FB`\ni\nfα(x (1) i ,xα\\{i})\n· ∏\nfα∈F(t)\\FB` i\nfα(x (2) i ,xα\\{i}),\nwhere x(1)i , x (2) i are the newly introduced “split variables” that are associated with the same vertex i, but allowed to have different values for our purpose. Next, the bucket F (t) is renormalized into F (t+1), leading to the induced factor of f̃i defined as follows:\nf̃i(x (1) i , x (2) i ) := ∑ xV(t+1)\\{i} ∏ fα∈F̃B`\ni ∪{r`i ,ri`}\nfα(x (1) i ,xα\\{i})\n· ∏\nfα∈F(t+1)\\F̃B` i \\{r`i ,ri`}\nfα(x (2) i ,xα\\{i})\n=r`i (x (1) i ) ∑ x i` ri`(xi`)fi(xi` , x (2) i ).\nThen change in fi is directly related with change in partition function since Z(t−1) and Z(t) can be described as follows:\nZ(t−1) = ∑ xi fi(xi, xi), Z (t) = ∑ xi f̃i(xi, xi).\nConsequently, GBR chooses to minimize the change in fi by re-updating r`i , ri` , i.e., it solves\nmin r`i ,ri` ∑ x (1) i ,x (2) i ( fi(x (1) i , x (2) i )− f̃i(x (1) i , x (2) i ) )2 . (14)\nHowever, we remark that (14) is intractable since its objective is “global”, and requires summation over all variables except one, i.e., xV(t)\\{i}, and this is the key difference from (4) which seeks to minimize the error described by the local mini-bucket. GBR avoids this issue by substituting fi by its tractable approximation gi, which is to be described in the following section.\nAlgorithm 3 GM renormalization\n1: Input: GMM† = (G†,F†), elimination order o and induced width bound ibound.\n2: M(1) ←M† 3: Run Algorithm 2 with inputM(1), o, ibound to obtain\nmini-buckets B`i and compensating factors r`i , ri` for i = 1, · · · , n and ` = 1, · · · ,mi.\n4: for f ∈ F (1) do 5: Assign scope Sf for f by (8). 6: end for 7: for i in o do 8: for ` = 1, · · · ,mi − 1 do 9: for f ∈ B`i do\n10: Renormalize scope Sf for f into S̃f by (9). 11: end for 12: Set t = ∑i−1 j=1(mi − 1) + `. 13: Renormalize GMM(t) intoM(t+1) by (10). 14: Assign scope SfB`\ni \\{i} for factor fB`i\\{i} by (11). 15: end for 16: Assign scope SfBmi\ni \\{i} for factor fBmii \\{i} by (12). 17: end for\n18: Output: Final GMM(T+1) with T = ∑n i=1(mi − 1)."
  }, {
    "heading": "4.2. Algorithm Description",
    "text": "In this section, we provide a formal description of GBR. First, consider the sequence of GMs M(1), · · · ,M(T+1) from interpreting MBR as GM renormalizations. This corresponds to T choices of compensating factors made at each renormalization, i.e., r(1), · · · , r(T ) where r(t)(x) = r`i (x) = ri`(x) for the associated replicate vertex i\n`. GBR modifies this sequence iteratively by replacing intermediate choice of compensation r(t) by another choice s(t)(x) = s`i(x) = si`(x) in reversed order, approximately solving (14) until all compensating factors are updated. Then, GBR outputs partition function Z(T+1) forM(T+1) as an approximation of the original partition function Z.\nNow we describe the process of choosing new compensating factors s`i , si` at t\n′-th iteration of GBR by approximately solving (14). To this end, the t′-th choice of compensating factors are expressed as follows:\nr(1), · · · , r(t), s(t+1), · · · , s(T ), (15)\nwith t = T − t′ + 1 and s(t+1), · · · , s(T ) already chosen in the previous iteration of GBR. Next, consider sequence of GMs M̂(1), · · · ,M̂(T+1) that were generated similarly with GM renormalization corresponding to MBR, but with compensating factors chosen by (15). Observe that the first t renormalizations of GM correspond to those of MBR since the updates are done in reverse order, i.e., M̂(t′) =M(t′) for t′ < t + 1. Next, fi in (4) is expressed as summation over x`\nV̂(t+1)\\{i,i`} in M̂(t+1), defined as follows:\nfi(xi` , xi) = ∑\nxV̂(t+1)\\{i,i`} ∏ fα∈F̂(t+1)\\{r`i ,ri`} fα(xα).\nSince fi resembles the partition function in a way that it is also a summation over GM with small change in a set of factors, i.e., excluding local factors r`i , ri` , we expect fi to be approximated well by a summation gi over xV̂(T+1)\\{i,i`} in M̂(T+1):\ngi(xi` , xi) := ∑\nxV̂(T+1)\\{i,i`} ∏ fα∈F̂(T+1)\\{r`i ,ri`} fα(xα),\nwhich can be computed in O(|E|dibound+2) complexity via appying BE in M̂(T+1) with elimination order õ \\ {i, i`} as in (13). Then the following optimization is obtained as an approximation of (14):\nmin s`i ,si` ∑ x (1) i ,x (2) i ( gi(x (1) i , x (2) i )− g̃i(x (1) i , x (2) i ) )2 , (16)\nwhere g̃i corresponds to renormalized factor f̃i:\ng̃i(xi` , xi) := s ` i(xi`) ∑ x i` si`(xi`)gi(xi` , xi).\nAs a result, one can expect choosing compensating factors from (16) to improve over that of (4) as long as MBR provides reasonable approximation for fi. The optimization is again solvable via rank-1 truncated SVD and the overall complexity of GBR is\nO ( dibound+2NSVD(M\nglobal) · |E|2 ·max α∈E |α|2\n) ,\nwhere NSVD(Mglobal) = O(d3) is the complexity for performing SVD on d× d matrix Mglobal representing function g as in (7). While the formal description of GBR is conceptually a bit complicated, one can implement it efficiently.\nSpecifically, during the GBR process, it suffices to keep only the description of renormalized GMM(T+1) with an ongoing set of compensating factors, e.g., (15), in order to update compensating factors iteratively by (16). The formal description of the GBR scheme is provided in Algorithm 4.\nAlgorithm 4 Global-Bucket Renormalization (GBR)\n1: Input: GMM† = (G†,F†), elimination order o and induced width bound ibound.\n2: Run Algorithm 3 with inputM†, o, ibound to obtain renormalized GMM and compensating factors r`i for i = 1, · · · , n and ` = 1, · · · ,mi. 3: Set the renormalized elimination order as follows:\nõ = [11, · · · , 1m1−1, 1, · · · , n1, · · · , nmn−1, n].\n4: for i` = nmn−1, · · · , n1, · · · , 1m1−1, · · · , 11 do 5: Generate s`i , s ` i by solving\nmin s`i ,si` ∑ x (1) i ,x (2) i ( gi(x (1) i , x (2) i )− g̃i(x (1) i , x (2) i ) )2 ,\n6: where gi, g̃i is defined as follows: gi(xi` , xi) = ∑\nxV\\{i,i`} ∏ fα∈F\\{r`i ,ri`} fα(xα),\ng̃i(xi` , xi) = s ` i(xi`) ∑ x`i si`(x ` i)gi(x ` i , xi),\nwith its computation done by BE with elimination order of õ \\ {i, i`}.\n7: Update GMM by F ← F \\ {r`i , ri`} ∪ {s`i , si`}. 8: end for 9: Get Z = ∑ x ∏ fα∈F fα(xα) via BE with elimination\norder õ. 10: Output: Z"
  }, {
    "heading": "5. Experimental Results",
    "text": "In this section, we report experimental results on performance of our algorithms for approximating the partition function Z. Experiments were conducted for Ising models defined on grid-structured and complete graphs as well as two real-world datasets from the UAI 2014 Inference Competition (Gogate, 2014). We compare our mini-bucket renormalization (MBR) and global-bucket renormalization (GBR) scheme with other mini-bucket algorithms, i.e., minibucket elimination (MBE) by Dechter & Rish (2003) and weighted mini-bucket elimination (WMBE) by Liu & Ihler (2011). Further, we also run the popular variational inference algorithms: mean-field (MF), loopy belief propagation\n(BP) and generalized belief propagation (GBP) by Yedidia et al. (2001). For all mini-bucket algorithms, we unified the choice of elimination order for each instance of GM by applying min-fill heuristics (Koller & Friedman, 2009). Further, WMBE used additional fixed point reparameterization updates for improving its approximation, as proposed by Liu & Ihler (2011) and GBP was implemented to use the same order of memory as the mini-bucket algorithms for given ibound. See the supplementary material for more details on implementations of the algorithms. For performance measure, we use the log-partition function error, i.e., | log10 Z − log10 Zapprox| where Zapprox is the approximated partition function from a respective algorithm.\nIsing models. We first consider the most popular binary pairwise GMs, called Ising models (Onsager, 1944): p(x) = 1Z exp (∑ i∈V φixi + ∑ (i,j)∈E φijxixj ) , where xi ∈ {−1, 1}. In our experiments, we draw φij and φi uniformly from intervals of [−∆,∆] and [−0.1, 0.1] respectively, where ∆ is a parameter controlling the ‘interaction strength’ between variables. As ∆ grows, the inference task is typically harder. Experiments were conducted in two settings: complete graphs with 15 variables (105 pairwise factors), and 15 × 15 non-toroidal grid graphs (225 variables, 420 pairwise factors). Both settings have moderate tree-width, enabling exact computation of the partition function using BE with induced widths of 15 and 16, respectively. We vary the interaction strength ∆ and the induced width bound ibound (for mini-bucket algorithms and GBP), where ibound = 10 and ∆ = 1.0 are the default choices. For each choice of parameters, results are obtained by averaging over 100 random model instances.\nAs shown in Figure 3a-d, both MBR and GBR perform impressively compared to MF and BP. Somewhat surprisingly, GBR outperforms GBP and even MBR is not worse than GBP although GBP (using the same order of memory) is more expensive to run due to its iterative nature. Next, the relative benefit of our methods compared to the earlier approaches increases with ∆ (more difficult instances), and as the bound of induced width gets smaller. This suggests that our methods scale well with the size and difficulty of GM.\nIn Figure 3c, where ibound is varied for complete graphs, we observe that GBR does not improve over MBR when ibound is small, but does so after ibound grows large. This is consistent with our expectation that in order for GBR to improve over MBR, the initial quality of MBR should be acceptable. Our experimental setup on Ising grid GMs with varying interaction strength is identical to that of Xue et al. (2016), where they approximate variable elimination in the Fourier domain. Comparing results, one can observe that our methods significantly outperform their prior algorithm.\nFigure 3e reports the trade-off between accuracy and elapsed\ntime with varying ibound. Here, we observe that both MBR and GBR are faster than WMBE and any of the variational inference algorithms. Further, we also note that increasing ibound does not necessarily lead to slower running time, while the accuracy is improved. This is because smaller ibound increases the number of mini-buckets and the corresponding updates.\nUAI datasets. We further show results of real-world models from the UAI 2014 Inference Competition, namely the Promedus (medical diagnosis) and Linkage (genetic linkage) datasets, consisting of 28 and 17 instances of GMs respectively. More details of the datasets are provided in the supplementary material. Again, induced width bounds are set to ibound = 10. The experimental results are summarized in Figure 3f and 3g. Results for MF were omitted since MF was not able to run on these instances by its construction. First, in Promedus dataset, i.e., Figure 3f, MBR and GBR clearly dominates over all other algorithms. Even when GBR fails to improve MBR, it still outperforms other algorithms. Next, in Linkage dataset, i.e., 3g, MBR and GBR are often outperformed by GBP where the latter is significantly (often 100×) more expensive to run than the formers. Typically, MBR and GBR are nearly as good as GBP. They outperform all mini-bucket variants and BP.\nGuide for implementation. Based on the experiments, we\nprovide useful recommendations for application of MBR and GBR. First, we emphasize that using the min-fill heuristics for choosing the appropriate elimination order can improve the performance of MBR and GBR (see the supplementary material). Whenever memory is available, running MBR with increased ibound typically leads to better tradeoff between complexity and performance than running GBR. When memory is limited, GBR is recommended for improving the approximation quality without additional memory."
  }, {
    "heading": "6. Conclusion and Future Work",
    "text": "We developed a new family of mini-bucket algorithms, MBR and GBR, inspired by the tensor network renormalization framework in statistical physics. The proposed schemes approximate the variable elimination process efficiently by repeating low-rank projections of mini-buckets. Extensions to higher-order low-rank projections (Xie et al., 2012; Evenbly, 2017) might improve performance. GBR calibrates MBR via minimization of renormalization error for the partition function explicitly. A similar optimization was considered in the so-called second-order renormalization groups (Xie et al., 2009; 2012). Hence, there is scope to explore potential variants of GBR. Finally, another direction to generalize MBR is to consider larger sizes of buckets to renormalize, e.g., see (Evenbly & Vidal, 2015; Hauru et al., 2018)."
  }, {
    "heading": "Acknowledgements",
    "text": "AW acknowledges support from the David MacKay Newton research fellowship at Darwin College, The Alan Turing Institute under EPSRC grant EP/N510129/1 & TU/B/000074, and the Leverhulme Trust via the CFI. This work was partly supported by Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (No.2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework) and ICT R&D program of MSIP/IITP [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion]."
  }],
  "year": 2018,
  "references": [{
    "title": "Gauging variational inference",
    "authors": ["Ahn", "Sungsoo", "Chertkov", "Michael", "Shin", "Jinwoo"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2017
  }, {
    "title": "Gauged mini-bucket elimination for approximate inference",
    "authors": ["Ahn", "Sungsoo", "Chertkov", "Michael", "Shin", "Jinwoo", "Weller", "Adrian"],
    "venue": "Artificial Intelligence and Statistics (AISTATS),",
    "year": 2018
  }, {
    "title": "Introduction to machine learning",
    "authors": ["Alpaydin", "Ethem"],
    "venue": "MIT press,",
    "year": 2014
  }, {
    "title": "Graphical models and automatic speech recognition",
    "authors": ["Bilmes", "Jeffrey A"],
    "venue": "In Mathematical foundations of speech and language processing,",
    "year": 2004
  }, {
    "title": "Markov random fields in statistics. Disorder in physical systems: A volume",
    "authors": ["Clifford", "Peter"],
    "venue": "in honour of John M. Hammersley,",
    "year": 1990
  }, {
    "title": "Bucket elimination: A unifying framework for reasoning",
    "authors": ["Dechter", "Rina"],
    "venue": "Artificial Intelligence,",
    "year": 1999
  }, {
    "title": "Mini-buckets: A general scheme for bounded inference",
    "authors": ["Dechter", "Rina", "Rish", "Irina"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2003
  }, {
    "title": "Algorithms for tensor network renormalization",
    "authors": ["Evenbly", "Glen"],
    "venue": "Physical Review B,",
    "year": 2017
  }, {
    "title": "Tensor network renormalization",
    "authors": ["Evenbly", "Glen", "Vidal", "Guifre"],
    "venue": "Physical review letters,",
    "year": 2015
  }, {
    "title": "Codes on graphs: Normal realizations",
    "authors": ["Forney", "G David"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2001
  }, {
    "title": "Learning low-level vision",
    "authors": ["Freeman", "William T", "Pasztor", "Egon C", "Carmichael", "Owen T"],
    "venue": "International journal of computer vision,",
    "year": 2000
  }, {
    "title": "Renormalization of tensor networks using graphindependent local truncations",
    "authors": ["Hauru", "Markus", "Delcamp", "Clement", "Mizera", "Sebastian"],
    "venue": "Physical Review B,",
    "year": 2018
  }, {
    "title": "Reducing the dimensionality of data with neural networks",
    "authors": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"],
    "year": 2006
  }, {
    "title": "Polynomial-time approximation algorithms for the Ising model",
    "authors": ["Jerrum", "Mark", "Sinclair", "Alistair"],
    "venue": "SIAM Journal on computing,",
    "year": 1993
  }, {
    "title": "Probabilistic graphical models: principles and techniques",
    "authors": ["Koller", "Daphne", "Friedman", "Nir"],
    "venue": "MIT press,",
    "year": 2009
  }, {
    "title": "Tensor renormalization group approach to two-dimensional classical lattice models",
    "authors": ["Levin", "Michael", "Nave", "Cody P"],
    "venue": "Physical review letters,",
    "year": 2007
  }, {
    "title": "Bounding the partition function using Hölder’s inequality",
    "authors": ["Liu", "Qiang", "Ihler", "Alexander T"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning",
    "year": 2011
  }, {
    "title": "Putting mrfs on a tensor train",
    "authors": ["Novikov", "Alexander", "Rodomanov", "Anton", "Osokin", "Vetrov", "Dmitry"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Crystal statistics. i. a two-dimensional model with an order-disorder transition",
    "authors": ["Onsager", "Lars"],
    "venue": "Physical Review,",
    "year": 1944
  }, {
    "title": "Reverend Bayes on inference engines: A distributed hierarchical approach",
    "authors": ["Pearl", "Judea"],
    "venue": "Cognitive Systems Laboratory, School of Engineering and Applied Science,",
    "year": 1982
  }, {
    "title": "Social network analysis. Sage, 2017",
    "authors": ["Scott", "John"],
    "year": 2017
  }, {
    "title": "A new class of upper bounds on the log partition function",
    "authors": ["Wainwright", "Martin J", "Jaakkola", "Tommi S", "Willsky", "Alan S"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2005
  }, {
    "title": "Tensor belief propagation",
    "authors": ["Wrigley", "Andrew", "Lee", "Wee Sun", "Ye", "Nan"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Second renormalization of tensor-network states",
    "authors": ["ZY Xie", "HC Jiang", "QN Chen", "ZY Weng", "T. Xiang"],
    "venue": "Physical review letters,",
    "year": 2009
  }, {
    "title": "Coarse-graining renormalization by higherorder singular value decomposition",
    "authors": ["ZY Xie", "Chen", "Jing", "MP Qin", "JW Zhu", "LP Yang", "Xiang", "Tao"],
    "venue": "Physical Review B,",
    "year": 2012
  }, {
    "title": "Variable elimination in the fourier domain",
    "authors": ["Xue", "Yexiang", "Ermon", "Stefano", "Le Bras", "Ronan", "Selman", "Bart"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Generalized belief propagation",
    "authors": ["Yedidia", "Jonathan S", "Freeman", "William T", "Weiss", "Yair"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2001
  }],
  "id": "SP:650ebda78d718a2c5e6dd01018e447021d4b8ef1",
  "authors": [{
    "name": "Sungsoo Ahn",
    "affiliations": []
  }, {
    "name": "Michael Chertkov",
    "affiliations": []
  }, {
    "name": "Adrian Weller",
    "affiliations": []
  }, {
    "name": "Jinwoo Shin",
    "affiliations": []
  }],
  "abstractText": "Probabilistic graphical models are a key tool in machine learning applications. Computing the partition function, i.e., normalizing constant, is a fundamental task of statistical inference but it is generally computationally intractable, leading to extensive study of approximation methods. Iterative variational methods are a popular and successful family of approaches. However, even state of the art variational methods can return poor results or fail to converge on difficult instances. In this paper, we instead consider computing the partition function via sequential summation over variables. We develop robust approximate algorithms by combining ideas from mini-bucket elimination with tensor network and renormalization group methods from statistical physics. The resulting “convergence-free” methods show good empirical performance on both synthetic and real-world benchmark models, even for difficult instances.",
  "title": "Bucket Renormalization for Approximate Inference"
}