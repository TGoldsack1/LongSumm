{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Regularized empirical risk minimization with linear predictors is a key workhorse in machine learning. It has the following general form:\nmin x2Rd\n( P (x) def = 1\nn\nnX\ni=1\ni\n(a > i x) + g(x)\n) (1)\nwhere a i 2 Rd is one of the n data samples with d features. i\n: R! R is a convex loss function of the linear predictor a\n> i x, for i = 1, · · · , n, and g : Rd ! R is a convex regularization function for the coefficient vector x 2 Rd. The loss function\ni assigns a cost to the difference between the linear predictor a>\ni\nx and the associated label b i .\nWith continuous and discrete b i , (1) captures regression and classification problems respectively. As a popular instance,\n1Department of ICES, University of Texas, Austin 2Department of CS, Carnegie Mellon University, Pittsburgh 3Department of CS, University of Texas, Austin 4Amazon/A9, Palo Alto. Correspondence to: Qi Lei <leiqi@ices.utexas.edu>, Ian E.H. Yen <eyan@cs.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhen i (z) = max{0, 1 b i z} and g(x) = µ/2kxk22, (1) reduces to the linear SVM (support vector machine) classification problem. While setting\ni (z) = log(1+exp( b i z)), we obtain the logistic regression problem.\nWe are interested in developing efficient algorithms for solving this general problem (1) for the setting where the coefficient vector x is assumed to be sparse. Applications where such a sparsity is natural include large-scale multiclass/multi-label classification, low-degree polynomial data mapping (Chang et al., 2010), n-gram feature mapping (Sonnenburg & Franc, 2010), and random feature kernel machines (Rahimi & Recht, 2007), specifically with a sparsity constraint on the random features (Yen et al., 2014).\nOur paper is organized as follows: In Section 2 we review existing algorithms to solve the primal, dual as well as primal-dual formulations of the problem (1). In Section 3, we present our Doubly Greedy Primal-Dual Coordinate Descent method for the convex-concave saddle point formulation of the problem (1). We propose an alternative method that is more efficient in practice with the use of incrementally increased active sets in both primal and dual variables. In Section 4 we show linear convergence for our proposed algorithm, and demonstrate the advantages of greedy methods with sparse variables. Finally in Section 5 we compare the performance of our method with other state-of-the-art methods on some real-world datasets, both with respect to time and iterations."
  }, {
    "heading": "2. Formulation and related work",
    "text": "Notations: We use A to denote the data matrix, with rows A\ni\n= a\ni corresponding to samples, and the column Aj corresponding to features. We use [n] to compactly denote {1, 2, · · ·n}. Throughout the paper, k · k denotes l2-norm unless otherwise specified.\nAssumptions: In order to establish equivalence of the primal, dual problem and the convex-concave saddle point formulation, we make the following assumptions.\n• g, the regularization for primal variable, is assumed to be µ-strongly convex, formally,\ng(y) g(x) + hrg(x),y xi+ µ 2 ky xk2, for any sub-gradient rg(x) 2 @g(x),x,y 2 Rd. We\nalso assume that g has decomposable structure, i.e., g(x) = P i g i (x i ).\n• i is 1 -smooth, for i 2 [n]: i (y)  i (x)+ 0 i\n(x)(y x)+ 2 (y x)2, x, y 2 R or equivalently, 0\ni is Lipschitz continuous, i.e., | 0\ni (x) 0 i (y)|  1 |x y|."
  }, {
    "heading": "2.1. Primal, dual and primal-dual formulations",
    "text": "Under the assumption of strongly convex regularization g and smooth loss function\ni , minimizing (1) is equivalent to maximizing its dual formulation:\nmax y2Rn\n( D(y) ⌘ g⇤( A > y\nn ) 1 n\nnX\ni=1\n⇤ i (y i )\n) (2)\nor the unique solution for the following convex-concave saddle point problem:\nmax y2Rn min x2Rd\n( L(x,y) = g(x) + 1\nn y >Ax 1 n\nnX\ni=1\n⇤ i (y i )\n)\n(3)\nNote that i (a > i x) in (1) is also smooth with respect to x, since r\nx i\n(a > i x) = 0 i (a > i x)a i , therefore i (a > i x)\nis R2/ -smooth with respect to x, where R is defined as R = max\ni ka i k2. (Zhang & Xiao, 2014) thus defined the condition number for the primal-dual form as:\n def =\nR2 µ .\nWe share this definition in this paper. The commonly used condition number for the gradient descent of the primal form is simply (R2/ + µ)/µ = 1 + , see (Nesterov, 2004)."
  }, {
    "heading": "2.2. Related work",
    "text": "There has been a long line of work over the years to derive fast solvers for the generic optimization problem (1). In Table 1, we review the time complexity to achieve ✏ error with respect to either primal, dual or primal-dual optimality for existing methods.\nPrimal (accelerated) gradient descent (Nesterov, 2004; 2005) require O((1+) log(1/✏)) (or O((1+p) log(1/✏)) if accelerated) iterations to achieve primal error less than ✏. Note that 1+ is the condition number of (1). Since each iteration takes O(nd) operations, the overall time complexity is O(nd (1 + ) log(1/✏)) (or O(nd (1 + p) log(1/✏)) if accelerated). Due to the large per iteration cost for large n, stochastic variants that separately optimize each i\nhave proved more popular in big data settings. Examples include SGD (Bottou, 2010), SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), SAGA (Defazio et al., 2014), MISO (Mairal, 2015) and their accel-\nerated versions (Xiao & Zhang, 2014). The stochastic scheme of optimizing individual\ni is similar to updating each dual coordinate individually. Their time complexity thus matches that of dual coordinate descent methods (Hsieh et al., 2008; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu et al., 2014), which enjoy a time complexity of O(nd (1+/n) log(1/✏)), and a further acceleration step (Shalev-Shwartz & Zhang, 2016; 2013a) will improve the complexity to O(nd (1 +p/n) log(1/✏)). These stochastic methods have a lower per iteration cost of O(d), but each step optimizes terms that are much less well-conditioned, and consequently have a larger iteration complexity, for instance of O(n (1 + p /n) log(1/✏)) in the accelerated case.\nWith the primal-dual formulation, (Zhang & Xiao, 2014) introduce a novel stochastic primal-dual coordinate method (SPDC), which with acceleration achieves a time complexity of O(nd (1 +p/n) log(1/✏)), matching that of accelerated stochastic dual coordinate descent methods.\nHowever, in practice, SPDC could lead to more expensive computations for sparse data matrices due to dense updates. For some special choices of the model, (Zhang & Xiao, 2014) provided efficient implementation for sparse feature structures, but the average update time for each coordinate is still much longer than that of dual coordinate descent. Moreover, they cannot exploit intermediate sparse iterates by methods such as shrinking technique (Hsieh et al., 2008). We note moreover that acceleration is not always practically useful in many real-world settings, other than in extremely ill-conditioned situations. In particular,  is typically of the order of p n or n as shown in (Bousquet & Elisseeff, 2002; Zhang & Xiao, 2014), and therefore the conditioning of O(1+p/n) is not necessarily much better than O(1+ /n). Our experiments also corroborate this, showing that vanilla dual coordinate descent under reasonable precision or condition number is not improved upon by SDPC.\nTherefore we raise the following question: Does the primaldual formulation have other good properties that could be leveraged to improve optimization performance?\nFor instance, some recent work with the primal-dual formulation updates stochastically sampled coordinates (Yu et al., 2015), which has a reduced cost per iteration, provided the data admits a low-rank factorization or when the proximal mapping for primal and dual variables are relatively computational expensive, which however may not hold in practice, so that the the noise caused by this preprocessing could hurt test performance. Moreover, even when their assumptions hold, their low-rank matrix factorization step itself may dominate the total computation time."
  }, {
    "heading": "2.3. Our Contribution",
    "text": "In this paper, we try to address the key question above in the setting of empirical risk minimization problems with very large n and d, and where the set of primal (and/or dual) variables are assumed to be sparse. We then show that the primal-dual formulation of the problem allows for naturally leveraging available primal and/or dual sparsity.\n2 [\n,].\nIn Table 1, we review the total time complexity to achieve ✏ accuracy. We can see that all algorithms that achieve a linear convergence rate require running time that has a factor nd, and in particular, none of their convergence rates involve the sparsity of the primal or dual variables.\nThere have been some attempts to modify existing primal or dual coordinate approaches in order to exploit sparsity of either primal or dual variables, but these do not perform very well in practice. One popular approach uses a shrinking heuristic in updating dual coordinates (Hsieh et al., 2008), which however still requires complexity linear to the number of coordinates d and does not guarantee rate of convergence. (Nutini et al., 2015) consider the idea of searching more important active coordinates to update in each iteration. Their greedy updates yield an iteration complexity linear in 1/µ1 instead of d/µ, where µ and µ1 are the parameters of strong convexity with respect to L2 and L1 norms respectively. However, with the commonly used L2 regularization term µk · k2 that is used to ensure µ-strong convexity, the term is exactly µ1 = µ\nd l1-strongly convex. Moreover, in practice, searching active coordinates causes considerable overhead. While there have been some strategies proposed to address this such as (Dhillon et al., 2011) that leverages nearest neighbor search to reduce the searching time, these have further requirements on the data structure used to store the data. Overall, it thus remains to more carefully study the optimization problem in order to facilitate the use of greedy\napproaches to exploit primal or dual sparsity.\nIn this paper, we propose a Doubly Greedy Primal-Dual (DGPD) Coordinate method that greedily selects and updates both primal and dual variables. This method enjoys an overall low time complexity under a sparsity assumption on the primal variables:\nTheorem 2.1. Main result: (informal) For the empirical risk minimization problem (1) with l1 + l2 regularization, there exists an algorithm (DGPD) that achieves ✏ error in O(s(n+d)(1+ \nn\n) log 1 ✏ )) time, where s is an upper bound of the sparsity of the primal variables."
  }, {
    "heading": "3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method",
    "text": "Coordinate-wise updates are most natural when g is separable, as is assumed for instance in the Stochastic Primal-Dual Coordinate method of (Zhang & Xiao, 2014). In this paper, to exploit sparsity in primal variables, we additionally focus on the case where g(x) = µ2 kxk2 + kxk1. With respect to the loss function , it is assumed to be 1\n-smooth and convex. For instance, setting\ni as the smooth hinge loss(Shalev-Shwartz & Zhang, 2013b):\ni\n(z) =\n8 <\n:\n0 if b i z 1 1 2 biz if biz  0\n( 1 2 biz)2 otherwise,\nthe smoothness parameter = 12 . For the logit function i (z) = log(1 + exp( b i\nz), the smoothness parameter = 4.\nWhen iterates are sparse, it is more efficient to perform greedy coordinate descent. We will provide a brief theoretical vignette of this phenomenon in Section 4.1. With this motivation, our proposed method Doubly Greedy PrimalDual Coordinate Descent (DGPD) greedily selects and updates both the primal and dual variables, one coordinate a time. Our overall method is detailed in Algorithm 1.\nIn Algorithm 1, we start from all zero vectors x(0), z(0) 2 Rn, and y(0),w(0) 2 Rd, where x(0), and y(0) are the iterates for primal and dual variables, and w(0) and z(0) are two auxiliary vectors, maintained as w ⌘ Ax and z ⌘ A>y to cache and reduce computations.\nPrimal Updates. In each iteration, we first compute the optimal primal variable x̄(t 1) for the current y(t 1), i.e.,\n¯ x (t 1) = argmin\nx L(x,y(t 1))) Eqn.(4) Then, we only update the coordinate j(t) that will decrease L(x,y) the most, i.e., j(t) = argmin\nk2[d] L(x(t)+(x̄(t 1) k x(t) k )e k ,y(t 1))) Eqn.(5) Both two processes cost O(d) operations. Afterwards, we update the value of w with Eqn. (6) such that w(t) = Ax(t)\nAlgorithm 1 Doubly Greedy Primal-Dual Coordinate method 1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0. 2: Initialize: x(0) 0 2 Rd, y(0) 0 2 Rn,w(0) ⌘ Ax = 0 2 Rn, z(0) ⌘ A>y = 0 2 Rd 3: for t = 1, 2, · · · , T do 4: Choose greedily the primal coordinate to update:\nx̄\n(t)\nk\nargmin ↵\n1\nn\nz (t 1) k ↵+ g k (↵)\n, 8k 2 [d] (4)\nj (t) argmin k2[d]\n1\nn\nz (t 1) k (x̄ (t) k x(t 1) k ) + g k (x̄ (t) k ) g k (x (t 1) k )\n(5)\nx\n(t)\nk\n(\nx̄\n(t)\nk if k = j(t), x\n(t 1) k otherwise. 5: Update w to maintain the value of Ax:\nw (t) w(t 1) + (x(t) j x(t 1) j )A j (6) 6: Choose greedily the dual coordinate to update:\ni (t) argmax k2[n]\n|w(t 1) k\n1 n (\n⇤ k ) 0 (y (t 1) k )| (7)\ny\n(t)\nk\n(\nargmax\n1\nn\nw\n(t)\nk\n⇤ k ( ) 1 2⌘ ( y(t 1) k ) 2\nif k = i(t)\ny (t 1) k\notherwise. (8)\n7: Update z to maintain the value of A>y z\n(t) z(t 1) + (y(t) i (t) y(t 1) i (t) )A i (t) (9) 8: end for 9: Output: x(T ),y(T )\nin O(d) or O(nnz(Aj)) operations. This greedy choice of j(t) and aggressive update induces a sufficient primal progress, as shown in Lemma A.1.\nDual Updates. We note that the updates are not exactly symmetric in the primal x and dual y variables. The updates for the dual variables y do follow along similar lines as x, except that we use the Gauss-Southwell rule to select variables, and introduce a step size ⌘. This is motivated by our convergence analysis, which shows that each primal update step yields a large descent in the objective, while each dual update only ascends the dual objective modulo an error term. This required a subtle analysis to show that the error terms were canceled out in the end by the progress made in the primal updates. But to proceed with such an analysis required the use of a step size in the dual updates, to balance the progress made in the dual updates, and the error term it introduced. Note moreover, that we are using the Gauss-Southwell rule to choose the variable to optimize in the dual variables y, while we simply use the coordinate that causes the most function descent in the primal variables x. This is because our choice of step size in the dual updates required computations that are shared with our current approach of selecting the optimal primal variable. This does incur more overhead when compared to the Gauss Southwell rule however, so that we simply use the latter for optimizing y.\nThe most significant feature in our method is that we select\nand update one coordinate in both the primal and dual coordinates greedily. With a simple trick that maintains the value of w ⌘ Ax and z ⌘ A>y (Lei et al., 2016), we are able to select and update primal and dual coordinates in O(n) and O(d) operations respectively. This happens when computing the value of Ax and A>y, which are the bottleneck in computing the gradient or updating the variables. An extension to choose and update a batch of primal and dual coordinate is straightforward. We provide further discussions on the designing of Algorithm 1 in Section 4.\nIn this paper, we have not incorporated an extrapolation/acceleration scheme to our algorithm. As noted earlier, in practice the condition number  is usually comparable to n, thus adding an extrapolation term that reduces the conditioning from /n to p /n is not necessarily materially advantageous in real applications. Meanwhile, an extrapolation step usually worsens the stability of the algorithm, and is not easily combined with incorporating greedy updates, which is crucial to the leveraging the primal or dual sparsity structure in this paper. We thus defer an accelerated extension of our algorithm incorporating extrapolation term to future work.\nFor Algorithm 1, each iteration can be seen to have a cost of O(n + d), while in Section 4 we show that the iteration complexity for our method is O((1 + \nn )s log(1/✏)) assuming that the primal variables are s-sparse. Therefore the overall time complexity for our algorithm is O (1 + \nn\n)s(n + d) log(1/✏) , which is cheaper than the\ntime complexity of even the accelerated SPDC algorithm O (1 +p\nn\n)nd log(1/✏) except for extremely ill conditioned cases."
  }, {
    "heading": "3.1. A Practical Extension of DGPD",
    "text": "In real application settings, Algorithm 1 has some drawbacks. When data is sparse, we still require O(n) and O(d) operations to update primal and dual variables. Even when the data is dense, to find the greedy coordinate and to update it requires comparable time complexity, which suggests we should find some ways to eliminate overhead in practice.\nTo resolve these issues, we introduce the Doubly Greedy Primal-Dual Coordinate method with Active Sets in Algorithm 2. We make use of what we call active sets, that contains the newly selected coordinates as well as the current non-zero variables. We construct these active sets A\nx\nand A y\nfor both primal and dual variables. Initially, they are set as empty sets. In each iteration, we recurrently select coordinates outside the active sets with the Gauss-Southwell rule, and add them to A\nx and A y\n. We then optimize all the variables within the active sets. Once a primal/dual variable gets set to 0, we can drop it from the corresponding active sets. This practice keeps the active sets A\nx and A y\nas the support of primal and dual variables. Notice g0\nk\n(x k ) is 0 when x\nk is zero, so that the variable selection step for primal variables can be simplified as stated in (10).\nNow the time complexity per iteration becomes |A x |n + |A\ny |d. The sparsity in primal variables is encouraged by the choice of `1 + `2 regularization. Meanwhile, as shown by (Yen et al., 2016), a sparse set of primal variables usually induces a sparse set of dual variables. Therefore |A\nx |⌧ d and |A\ny | ⌧ n in practice, and the cost per iteration is sub-linear to nd. We present further details in Section 3.2."
  }, {
    "heading": "3.2. Efficient Implementation for Sparse Data Matrix",
    "text": "Suppose we are given a sparse data matrix A with number of non-zero elements of each column and each row bounded by nnz\ny and nnz x respectively, one can further reduce the cost for computing (10) and (12) from O(d|A\ny | + n|A x |) to O(nnz\nx |A y | + nnz y |A x |) by storing both {A i }n i=1 and\n{Aj}d j=1 as sparse vectors and computing A>y and Ax as\nA>y = X\ni2A y\nA> i y i\n, Ax = X\nj2A x\nAjx j . (14)\nIn our implementation, whenever the active sets A y , A x are expanded, we further maintain a submatrix [A]A which contains only rows in A\ny and columns in A x\n, so the primal and dual updates (11), (13) only cost P i2A\ny\nnnz([A i ]A x ). This results in each update costing less than the search steps, and therefore, in practice, one can conduct multiple rounds of updates (11), (13) before conducting the search (10), (12), which in our experiment speeds up convergence\nsignificantly."
  }, {
    "heading": "4. Convergence Analysis",
    "text": "In this section, we introduce the primal gap\np\nand dual gap\nd and analyze the convergence rate in terms of their sum, which we call primal and dual sub-optimality =\np\n+\nd\n.\nDefinition 4.1. For the following convex-concave function L(x,y) def= g(x) + 1\nn\ny >Ax 1 n\nP n\ni=1 ⇤ i (y i ), with\nits primal form P (x) def= min y L(x,y), and dual form D(y) def = max\nx L(x,y), we define the primal gap at iteration t as\n(t) p\ndef = L(x(t+1),y(t)) D(y(t))\n, the dual gap at iteration t as\n(t) d def = D⇤ D(y(t))\nand sub-optimality as\n(t) def =\n(t) p + (t) d .\nTheorem 4.2. Suppose in (1), g is µ-strongly convex (`1 + `2) regularization, and i is 1\n-smooth. Let R = max i2[n] kaik2. Then DGPD achieves\n(t+1)  2n 2n+ ⌘\n(t), (15)\nif step size ⌘(t) satisfies that\n⌘(t)  2n 2µ\nkx(t) ¯x(t)k0(5R2 + n µ) (16)\nSuppose kx(t) ¯x(t)k0  s, if we choose step size ⌘ = 2n2µ (5R2+n µ)s , then it requires\nO(s( n + 1) log 1 ✏ )\niterations for achieving ✏ primal and dual sub-optimality.1\nProof sketch: The proof analysis is straightforward with the introduction of primal and dual sub-optimality . We divide the proof into primal-dual progress, primal progress, and dual progress.\n• Primal-Dual Progress (Lemma A.2).\n(t) d + (t) p ( (t 1) d + (t 1) p )\n L(x(t+1),yt) L(xt,yt) +⌘( 1\nn hA i (t) ,x(t) ¯x(t)i)2\n⌘( 1 n hA i (t) , ¯x(t)i 1 n ( ⇤ i (t)) 0 (y(t) i (t))) 2(17)\n1This result can be easily connected to traditional convergence analysis in primal or dual form. Notice  ✏ is sufficient requirement that dual gap\nd\n= D ⇤ D(y)  ✏, therefore the dual variable y(t) converges to optimal y⇤ with the same convergence rate.\nAlgorithm 2 Doubly Greedy Primal-Dual Coordinate method with Active Sets\n1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0. 2: Initialize: x(0) 0 2 Rd, y(0) 0 2 Rn, A(0)\nx\n?,A(0) y ? 3: for t 1, 2, · · · , T do 4: Update the active set A(t)\nx\ngreedily based on the optimal primal variable ¯x(t 1) and update x in its active set.\nx̄(t) k\nargmin ↵\n1 n hAk,y(t 1)i↵+ g k (↵) , 8k 2 [d]\nj(t) argmax k2[d] |x̄(t 1) k | (10)\nA(t) x\nA(t 1) x [ {j(t)} x(t) j\n(\nx̄(t 1) j , if j 2 A(t) x x(t 1) j , if j /2 A(t) x\n(11)\n5: Update the active set A(t) y greedily based on the value of r y L(x(t),y(t 1)) and update y in its active set. i(t) argmax\nk2[n] A(t 1) y\n|hA k ,x(t)i 1 n ( ⇤ k ) 0 (y(t 1) k )|. (12)\nA(t) y\nA(t 1) y [ {i(t)}\ny(t) i\n(\nargmax 1 n hA i ,x(t)i 1 n ⇤ i ( ) 12⌘ ( y(t 1)k ) , if i 2 A(t) y y(t 1) i , if i /2 A(t) y\n(13)\n6: Kick out 0 variables from active sets. A(t)\ny\nA(t) y\n[\ni,y (t) i =0\n{i}, A(t) x\nA(t) x\n[\nj,x (t) j =0\n{j}\n7: end for 8: Output: x(T ),y(T )\nThis lemma connects the descent in PD sub-optimality with primal progress and dual progress. The third term and the second terms respectively represent the potential dual progress if we used the optimal ¯x(t), and the irrelevant part generated from the difference between ¯x(t) and x(t).\n• Primal Progress (Lemma A.1). L(x(t),y(t)) L(x(t+1),y(t)) 1kx(t) ¯x(t)k0 1 (t) p\n(18)\nThis inequality simply demonstrates function loss from primal update is at least a ratio of primal gap.\n• Dual Progress (Lemma A.3). ( 1\nn hA i (t) ,x(t) ¯x(t)i)2\n( 1 n hA i (t) , ¯x(t)i 1 n ( ⇤ i (t)) 0 (y(t) i (t))) 2\n 2n (t) d +\n5R2 2n2 kx(t) ¯x(t)k2 (19)\nFinally, we establish the relation between the dual progress in our algorithm with dual gap and difference between ¯x(t) and x(t). Now we can prove our main theorem 4.2.\nFor cleaner notation, write a = ⌘ 2n , b = 5⌘R2 2n2 . k¯x(t) x (t)k0  s. By combining (18) and (19) to (17), we get:\n(t) d\n(t 1) d + (t) p\n(t 1) p\n L(x(t+1),yt) L(xt,yt) a (t) d\n+\n2\nbkx(t) ¯x(t)k2\n L(x(t+1),yt) L(xt,yt) a (t) d\n+b L(x(t),y(t)) L(¯x(t),y(t))\n= (1 b) L(x(t+1),yt) L(xt,yt) a (t) d\n+b L(x(t+1),yt) L(¯x(t),yt)\n 1 b s 1 (t) p a (t) d\n+b L(x(t+1),yt) L(¯x(t),yt)\n= 1 b s 1 b (t) p a (t) d\nHere the second inequality comes from strong convexity of L(·,y(t)). The fourth inequality comes from Lemma A.1. Therefore when a  1 b\ns 1 b, or sufficiently a  (s(1 + 5/n)) 1, we get (t)  11+a (t 1). Since a < 1, (a + 1) 1/a  1/2, therefore (t)  (1 + a) t (0) \n2\nat (0). Therefore when T O(s(1 + /n) log2 (0)\n✏\n),\n(T )  ✏."
  }, {
    "heading": "4.1. Analysis on greedy methods for sparse iterates",
    "text": "In this section, we give a simple analysis of the greedy variable selection rule showing that when iterate and minimizer of a generic optimization problem are sparse, its convergence rate is faster than choosing random coordinates. We define the optimization problem in the space of Rn:\nmin x2Rn f(x)\n, where f is µ-strongly convex L-smooth: |r\ni f(x+ ↵e i ) r i f(x)|  L|↵|, 8x 2 Rn Under this setting, a random coordinate descent method with step size 1\nL , achieves E[f(x+) f⇤]  (1 µ nL ) f(x)\nf⇤ , where x+ is the next iterate of x.\nUnder the assumption that the current iterate x and the optimal x⇤ are both k-sparse, we thereby conduct greedy coordinate descent rule, i.e., x+ = x + ⌘e\ni ⇤ , where ⌘, i⇤ satisfies f(x+⌘e\ni\n⇤ ) = min\ni, f(x+ e i ). With L-Lipchitz continuity, we have:\nf(x+ ⌘e i ⇤ ) f(x)\n min ,i hrf(x), e i i+ L 2 2\n= min\n,i\nhrf(x), e i i+ L 2 k e i k21\n= min\nx hrf(x), xi+ L 2 k xk21\n min x\nf(x+ x) f(x) + L\n2\nk xk21\n min 0 1\nf(x+ (x⇤ x)) f(x) + L\n2\n2kx⇤ xk21\n min 0 1\n(f⇤ f(x)) + L\n2\n2kx⇤ xk21\nThe last two inequalities are obtained by constraining x to be of the form (x⇤ x) and by the convexity of f . For the k-sparse x, and x⇤, x x⇤ is at most 2k-sparse, and for any 2k-sparse vector a, kak21  2kkak22. Hereby we obtain:\nmin\n0 1\n(f⇤ f(x)) + L\n2\n2kx⇤ xk21\n min 0 1 (f⇤ f(x)) + Lk 2kx⇤ xk22\n min 0 1\n(f⇤ f(x)) 2kL\nµ 2(f⇤ f(x))\n=\nµ\n8kL (f⇤ f(x))\nTherefore f(x+) f⇤  (1 µ8kL )(f(x) f⇤), and when k ⌧ n, this convergence rate could be much better than randomized coordinate descent."
  }, {
    "heading": "5. Experiment",
    "text": "In this section, we implement the Doubly-Greedy PrimalDual Coordinate Descent algorithm with Active Sets, and compare its performance with other state-of-the-art methods for `1+`2-regularized Empirical Risk minimization, including Primal Randomized Coordinate Descent (PrimalRCD) (Richtárik & Takác, 2014), Dual Randomized Coordinate Descent (DualRCD, i.e., SDCA) (Shalev-Shwartz & Zhang, 2013b) and the Stochastic Primal-Dual Coordinate Method (SPDC) (Zhang & Xiao, 2014).\nWe conduct experiments on large-scale multi-class data sets with linear and non-linear feature mappings, as shown in Table 2. For Mnist and Aloi we use Random Fourier (RF) and Random Binning (RB) feature proposed in (Rahimi & Recht, 2007) to approximate effect of RBF Gaussian kernel and Laplacian Kernel respectively. The features generated by Random Fourier are dense, while Random Binning gives highly sparse data.\nWe give results for 2 {0.1, 0.01} and µ 2 {1, 0.1, 0.01}, where Figure 1 shows results for = 0.1, µ = 0.01 and others can be found in Appendix B. In the above six figures, we compare the running time with objective function. While in the below figures, the x-axis is number of iterations. For the baseline methods, one iteration is one pass over all the variables, and for our method, it is several (5) passes over the active sets. From the figures, we can see that in all cases, DGPD has better performance than other methods. Notice for clear presentation purposes we use log-scale for Mnist-RB-time, Aloi-RB-time and RCV-time, where our algorithm achieves improvements over others of orders of magnitude.\nThe result shows that, by exploiting sparsity in both the primal and dual, DGPD has much less cost per iteration and thus is considerably faster in terms of training time, while by maintaining an active set it does not sacrifice much in terms of convergence rate. Note since in practice we perform multiple updates after each search, the convergence rate (measured in outer iterations) can be sometimes even better than DualRCD."
  }, {
    "heading": "6. Acknowledgements",
    "text": "I.D. acknowledges the support of NSF via CCF-1320746, IIS-1546452, and CCF-1564000. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS1149803, IIS-1320894, IIS-1447574, and DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences."
  }],
  "year": 2017,
  "references": [{
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["Bottou", "Léon"],
    "venue": "In Proceedings of COMPSTAT’2010,",
    "year": 2010
  }, {
    "title": "Stability and generalization",
    "authors": ["Bousquet", "Olivier", "Elisseeff", "André"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "Training and testing low-degree polynomial data mappings via linear svm",
    "authors": ["Chang", "Yin-Wen", "Hsieh", "Cho-Jui", "Kai-Wei", "Ringgaard", "Michael", "Lin", "Chih-Jen"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Efficient one-vs-one kernel ridge regression for speech recognition",
    "authors": ["Chen", "Jie", "Wu", "Lingfei", "Audhkhasi", "Kartik", "Kingsbury", "Brian", "Ramabhadrari", "Bhuvana"],
    "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2016
  }, {
    "title": "Saga: A fast incremental gradient method with support for nonstrongly convex composite objectives",
    "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Nearest neighbor based greedy coordinate descent",
    "authors": ["Dhillon", "Inderjit S", "Ravikumar", "Pradeep K", "Tewari", "Ambuj"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "A dual coordinate descent method for large-scale linear svm",
    "authors": ["Hsieh", "Cho-Jui", "Chang", "Kai-Wei", "Lin", "Chih-Jen", "Keerthi", "S Sathiya", "Sundararajan", "Sellamanickam"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Coordinate-wise power method",
    "authors": ["Lei", "Qi", "Zhong", "Kai", "Dhillon", "Inderjit S"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Incremental majorization-minimization optimization with application to large-scale machine learning",
    "authors": ["Mairal", "Julien"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "Introductory Lectures on Convex Optimization: A Basic Course",
    "authors": ["Y. Nesterov"],
    "venue": "Springer Science & Business Media,",
    "year": 2004
  }, {
    "title": "Smooth minimization of non-smooth functions",
    "authors": ["Nesterov", "Yu"],
    "venue": "Mathematical programming,",
    "year": 2005
  }, {
    "title": "Coordinate descent converges faster with the gauss-southwell rule than random selection",
    "authors": ["Nutini", "Julie", "Schmidt", "Mark", "Laradji", "Issam H", "Friedlander", "Michael", "Koepke", "Hoyt"],
    "venue": "arXiv preprint arXiv:1506.00552,",
    "year": 2015
  }, {
    "title": "Randomized dual coordinate ascent with arbitrary sampling",
    "authors": ["Qu", "Zheng", "Richtárik", "Peter", "Zhang", "Tong"],
    "venue": "arXiv preprint arXiv:1411.5873,",
    "year": 2014
  }, {
    "title": "Random features for large-scale kernel machines",
    "authors": ["Rahimi", "Ali", "Recht", "Benjamin"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2007
  }, {
    "title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
    "authors": ["Richtárik", "Peter", "Takác", "Martin"],
    "venue": "Mathematical Programming,",
    "year": 2014
  }, {
    "title": "Minimizing finite sums with the stochastic average gradient",
    "authors": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"],
    "venue": "Mathematical Programming,",
    "year": 2013
  }, {
    "title": "Accelerated mini-batch stochastic dual coordinate ascent",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2013
  }, {
    "title": "Stochastic dual coordinate ascent methods for regularized loss",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "Mathematical Programming,",
    "year": 2016
  }, {
    "title": "Coffin: A computational framework for linear svms",
    "authors": ["Sonnenburg", "Sören", "Franc", "Vojtech"],
    "venue": "In Proceedings of the 27th International Conference on Machine Learning",
    "year": 2010
  }, {
    "title": "Revisiting random binning features: Fast convergence and strong parallelizability",
    "authors": ["Wu", "Lingfei", "Yen", "Ian EH", "Chen", "Jie", "Yan", "Rui"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "A proximal stochastic gradient method with progressive variance reduction",
    "authors": ["Xiao", "Lin", "Zhang", "Tong"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "Trading computation for communication: Distributed stochastic dual coordinate ascent",
    "authors": ["Yang", "Tianbao"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2013
  }, {
    "title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification",
    "authors": ["Yen", "Ian EH", "Huang", "Xiangru", "Zhong", "Kai", "Ravikumar", "Pradeep", "Dhillon", "Inderjit S"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Sparse random feature algorithm as coordinate descent in hilbert space",
    "authors": ["Yen", "Ian En-Hsu", "Lin", "Ting-Wei", "Shou-De", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem",
    "authors": ["Yu", "Adams Wei", "Lin", "Qihang", "Yang", "Tianbao"],
    "venue": "arXiv preprint arXiv:1508.03390,",
    "year": 2015
  }, {
    "title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization",
    "authors": ["Zhang", "Yuchen", "Xiao", "Lin"],
    "venue": "arXiv preprint arXiv:1409.3257,",
    "year": 2014
  }],
  "id": "SP:8e4ca34e61d72c60fc908f76524cf66cf8aed7df",
  "authors": [{
    "name": "Qi Lei",
    "affiliations": []
  }, {
    "name": "Ian E.H. Yen",
    "affiliations": []
  }, {
    "name": "Chao-yuan Wu",
    "affiliations": []
  }, {
    "name": "Inderjit S. Dhillon",
    "affiliations": []
  }, {
    "name": "Pradeep Ravikumar",
    "affiliations": []
  }],
  "abstractText": "We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy PrimalDual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables. It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.",
  "title": "Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization"
}