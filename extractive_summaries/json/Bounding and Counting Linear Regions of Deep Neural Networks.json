{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We have seen an unprecedented success of DNNs in computer vision, speech, and other domains (Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). While the popular networks such as AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2015), and residual networks (He et al., 2016) have shown record beating performance on various image recognition tasks, empirical results still govern the design of network architecture in terms of depth and activation functions. Two important considerations that are part of most successful architectures are greater depth and the use of PWL activation functions such as rectified linear units (ReLUs). This large gap between practice and theory has driven\n*Equal contribution 1Carnegie Mellon University, Pittsburgh, USA 2The University of Utah, Salt Lake City, USA. Correspondence to: Thiago Serra <tserraaz@alumni.cmu.edu>, Christian Tjandraatmadja <ctjandra@alumni.cmu.edu>, Srikumar Ramalingam <srikumar@cs.utah.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nresearchers toward mathematical modeling of the expressive power of DNNs (Cybenko, 1989; Anthony & Bartlett, 1999; Pascanu et al., 2014; Montúfar et al., 2014; Bianchini & Scarselli, 2014; Eldan & Shamir, 2016; Telgarsky, 2015; Mhaskar et al., 2016; Raghu et al., 2017; Montúfar, 2017).\nThe expressiveness of DNNs can be studied by transforming one network to another with different number of layers or activation functions. While any continuous function can be modeled using a single hidden layer of sigmoid activation functions (Cybenko, 1989), shallow networks require exponentially more neurons to model functions that can be modeled using much smaller deeper networks (Delalleau & Bengio, 2011). There are a wide variety of activation functions, which come with different modeling capabilities, such as threshold (f(z) = (z > 0)), logistic (f(z) = 1/(1 + exp(−e))), ReLU (f(z) = max{0, z}), and maxout (f(z1, z2, . . . , zk) = max{z1, z2, . . . , zk}). It has been shown that sigmoid networks are more expressive than similar-sized threshold networks (Maass et al., 1994), and ReLU networks are more expressive than similar-sized\nthreshold networks (Pan & Srikumar, 2016).\nThe complexity or expressiveness of neural networks belonging to the family of PWL functions can also be analyzed by looking at how the network can partition the input space to an exponential number of linear response regions (Pascanu et al., 2014; Montúfar et al., 2014). The basic idea of a PWL function is simple: we can divide the input space into several regions and we have individual linear functions for each of these regions. Functions partitioning the input space to a larger number of linear regions are considered to be more complex, or in other words, possess better representational power. In the case of ReLUs, it has been shown that some deep networks separate their input space into exponentially more linear response regions than their shallow counterparts despite using the same number of activation functions (Pascanu et al., 2014). The results were later extended and improved. In particular, Montúfar et al. (2014) show upper and lower bounds on the maximal number of linear regions for a ReLU DNN and a single layer maxout network, and a lower bound for a maxout DNN. Furthermore, Raghu et al. (2017) and Montúfar (2017) improve the upper bound for a ReLU DNN. This upper bound asymptotically matches the lower bound from Montúfar et al. (2014) when the number of layers and input dimension are constant and all layers have the same width. Finally, Arora et al. (2018) improve the lower bound by providing a family of ReLU DNNs with an exponential number of regions for fixed size and depth.\nMain Contributions\nThis paper directly improves on the results of Montúfar et al. (Pascanu et al., 2014; Montúfar et al., 2014; Montúfar, 2017), Raghu et al. (2017), and Arora et al. (2018). Fig. 1 highlights the main contributions, and the following list summarizes all the contributions:\n• We achieve tighter upper and lower bounds on the maximal number of linear regions of the PWL function corresponding to a DNN that employs ReLUs as shown in Fig. 1. As a special case, we present the exact maximal number of regions when the input dimension is one. We additionally provide the first upper bound for multi-layer maxout networks. (See Sections 3 and 4).\n• We show for ReLUs that the exact maximal number of linear regions of shallow networks is larger than that of deep networks if the input dimension exceeds the number of neurons, a result that could not be inferred from bounds in prior work. (See Figure 5).\n• We use a mixed-integer linear formulation to show that counting linear regions is indeed possible. For\nthe first time, we show the exact number of linear regions for a sample of DNNs as shown in Fig. 1. This new capability can be used to evaluate the tightness of the bounds and potentially to analyze the correlation between accuracy and the number of linear regions. (See Sections 5 and 6)."
  }, {
    "heading": "2. Notations and Background",
    "text": "Let us assume that a feedforward neural network, which is studied in this paper, has n0 input variables given by x = {x1, x2, . . . , xn0}, and m output variables given by y = {y1, y2, . . . , ym}. Each hidden layer l = {1, 2, . . . , L} has nl hidden neurons whose activations are given by hl = {hl1, hl2, . . . , hlnl}. Let W\nl be the nl × nl−1 matrix where each row corresponds to the weights of a neuron of layer l. Let bl be the bias vector used to obtain the activation functions of neurons in layer l. Based on the ReLU(x) = max{0, x} activation function, the activations of the hidden neurons and the outputs are given below:\nh1 = max{0,W 1x + b1} hl = max{0,W lhl−1 + bl} y = WL+1hL\nAs considered in Pascanu et al. (2014), the output layer is a linear layer that computes the linear combination of the activations from the previous layer without any ReLUs.\nWe can treat the DNN as a piecewise linear (PWL) function F : Rn0 → Rm that maps the input x in Rn0 to y in Rm. We define linear regions based on activation patterns.\nActivation Pattern: Let us consider an input vector x = {x1, . . . , xn0}. For every layer l we define an activation set Sl ⊆ {1, . . . , nl} such that e ∈ Sl if and only if the ReLU e is active, i.e. hle > 0. We aggregate these activation sets into S = (S1, . . . , Sl), which we call an activation pattern. In this work, we may consider activation patterns up to a layer l ≤ L. Activation patterns were previously defined in terms of strings (Raghu et al., 2017; Montúfar, 2017). We say that an input x corresponds to an activation pattern S if feeding x to the DNN results in the activations in S.\nWe define linear regions as follows.1\nDefinition 1. Given a PWL function F : Rn0 → Rm represented by a DNN, a linear region is the set of inputs that correspond to a same activation pattern in the DNN.\n1There is a subtly different definition of linear region in the literature (Pascanu et al., 2014; Montúfar et al., 2014), as follows. Given a PWL function F , a linear region is a maximal connected subset of the input space on which F is linear. The two definitions are essentially the same, except in a few degenerate cases. There could be scenarios where two different activation patterns may correspond to two adjacent regions with the same linear function, in which case this definition considers them as a single one. However, the bounds derived in this paper are valid for both definitions.\nIn this paper, we interchangeably refer to S as an activation pattern or a region for convenience.\nIn Figure 2 we show a simple ReLU DNN with two inputs {x1, x2} and 3 layers. The activation units {a, b, c, d, e, f} on these layers can be thought of as hyperplanes that each divide the space in two. On one side of the hyperplane, the unit outputs a positive value. For all points on the other side of the hyperplane including itself, the unit outputs 0.\nOne may wonder: into how many linear regions do n hyperplanes split a space? Zaslavsky (1975) shows that an arrangement of n hyperplanes divides a d-dimensional space into at most ∑d s=0 ( n s ) regions, a bound that is attained when they are in general position. The term general position basically means that a small perturbation of the hyperplanes does not change the number of regions. This corresponds to the exact maximal number of regions of a single layer DNN with n ReLUs and input dimension d.\nIn Figures 2(b)–(c), we provide a visualization of how ReLUs partition the input space. Figure 2(c) shows the hyperplanes corresponding to the ReLUs at layers l = 1, 2, and 3, from top to bottom. Figure 2(b) considers these same hyperplanes in the input space x. If we consider only the first-layer hyperplanes, the 2D input space is partitioned into 4 regions, as per Zaslavsky (1975) (( 2 0 ) + ( 2 1 ) + ( 2 2 ) = 4 ) . The regions are further partitioned as we consider additional layers. Subsequent hyperplanes are affected by the transformations applied in the earlier layers.\nFigure 2 also highlights that activation boundaries behave like hyperplanes when inside a region and may bend whenever they intersect with a boundary from a previous layer. This has also been pointed out by Raghu et al. (2017). In particular, they cannot appear twice in the same region as they are defined by a single hyperplane if we fix the region. Moreover, these boundaries do not need to be connected, as illustrated in Figure 3."
  }, {
    "heading": "3. Tighter Bounds for Rectifier Networks",
    "text": "Montúfar et al. (2014) derive an upper bound of 2N for N units, which can be obtained by mapping linear regions to activation patterns. Raghu et al. (2017) improve this result by deriving an asymptotic upper bound of O(nLn0) to the maximal number of regions, assuming nl = n for all layers l and n0 = O(1). Montúfar (2017) further tightens the upper bound to ∏L l=1 ∑dl j=0 ( nl j ) , where dl = min{n0, n1, . . . , nl}. Moreover, Montúfar et al. (2014) prove a lower bound of(∏L−1 l=1 bnl/n0cn0 )∑n0 j=0 ( nL j ) when n ≥ n0, or asymptotically Ω((n/n0)(L−1)n0nn0). Arora et al. (2018) present a lower bound of 2 ∑n0−1 j=0 ( m−1 j ) wL−1 where 2m = n1 and w = nl for all l = 2, . . . , L. We derive both upper and lower bounds that improve upon these previous results."
  }, {
    "heading": "3.1. An Upper Bound on the Number of Linear Regions",
    "text": "In this section, we prove the following upper bound on the number of regions.\nTheorem 1. Consider a deep rectifier network with L layers, nl rectified linear units at each layer l, and an input of dimension n0. The maximal number of regions of this\nneural network is at most\n∑ (j1,...,jL)∈J L∏ l=1 ( nl jl )\nwhere J = {(j1, . . . , jL) ∈ ZL : 0 ≤ jl ≤ min{n0, n1 − j1, . . . , nl−1 − jl−1, nl} ∀l = 1, . . . , L}. This bound is tight when L = 1.\nNote that this is a stronger upper bound than the one that appeared in Montúfar (2017), which can be derived from this bound by relaxing the terms nl − jl to nl and factoring the expression. When n0 = O(1) and all layers have the same width n, we have the same best known asymptotic bound O(nLn0) first presented in Raghu et al. (2017).\nTwo insights can be extracted from the above expression:\n1. Bottleneck effect. The bound is sensitive to the positioning of layers that are small relative to the others, a property we call the bottleneck effect. If we subtract a neuron from one of two layers with the same width, choosing the one closer to the input layer will lead to a larger (or equal) decrease in the bound. This occurs because each index jl is essentially limited by the widths of the current and previous layers, n0, n1, . . . , nl. In other words, smaller widths in the first few layers of the network imply a bottleneck on the bound.\nThe following proposition illustrates this bottleneck effect of the bound for the 2-layer case (see Appendix A for the proof).\nProposition 2. Consider a 2-layer network with widths n1, n2 and input dimension n0 > max{n1, n2}. Then moving a neuron from the first layer to the second layer strictly decreases the bound.\nFigure 4 illustrates this behavior. For the solid line, we keep the total size of the network the same but shift from a small-to-large network (i.e., smaller width near the input layer and larger width near the output layer) to a large-to-small network in terms of width. We see that the bound monotonically increases as we reduce the bottleneck. If we add a layer of constant width at the end, represented by the dashed line, the bound decreases when the layers before the last become too small and create a bottleneck for the last layer.\nWhile this is a property of the upper bound rather than of the exact maximal number of regions, we observe in Section 6 that empirical results for the number of regions of a trained network exhibit a behavior that resembles the bound as the width of the layers vary. Moreover, this bottleneck effect appears at a more fundamental level. For example, having a first layer of size\none forces all hyperplanes corresponding to subsequent layers to be parallel to each other in the input space, reflecting the fact that we have compressed all information into a single value. More generally, the smaller a layer is, the more linearly dependent the hyperplanes from subsequent layers will be, which results in fewer regions. Further in this section, we formalize this in terms of dimension and show that the dimension of the image of a region is limited by the widths of earlier layers, which is used to prove Theorem 1.\n2. Deep vs shallow for large input dimensions. In several applications such as imaging, the input dimension can be very large. Montúfar et al. (2014) show that if the input dimension n0 is constant, then the number of regions of deep networks is asymptotically larger than that of shallow (single-layer) networks. We complement this picture by establishing that if the input dimension is large, then shallow networks can attain more regions than deep networks.\nMore precisely, we compare a deep network with L layers of equal width n and a shallow network with one layer of width Ln. Denote the exact maximal number of regions byR(n0, n1, . . . , nL), where n0 is the input dimension and n1, . . . , nL are the widths of layers 1 through L of the network.\nCorollary 3. Let L ≥ 2, n ≥ 1, and n0 ≥ Ln. Then\nR(n0, n, . . . , n︸ ︷︷ ︸ L times ) < R(n0, Ln)\nMoreover, limL→∞ R(n0,n,...,n) R(n0,Ln) = 0.\nThis is a consequence of Theorem 1 (see Appendix A for the proof).\nFigure 5 (a) illustrates this behavior. As we increase the number of layers while keeping the total size of the network constant, the bound plateaus at a value lower than the exact maximal number of regions for shallow networks. Moreover, the number of layers that yields the highest bound decreases as we increase the input dimension n0. Hence, for a given number of units and n0, there is a particular depth maximizing the bound.\nThis property cannot be inferred from upper bounds derived in prior work: previous bounds for a network with L layers of size n are no smaller than R(n0, Ln) for a sufficiently large n0. Figure 5 (b) shows the behavior of the previous best known bound.\nWe remark that asymptotically both deep and shallow networks can attain exponentially many regions when the input dimension is sufficiently large. More precisely, for a DNN with the same width n per layer, the maximal number of linear regions is Ω(2 2 3Ln) when n0 ≥ n/3 (see Appendix B).\nWe now build towards the proof of Theorem 1. For a given activation set Sl and a matrix W with nl rows, let σSl(W ) be the operation that zeroes out the rows of W that are inactive according to Sl. This represents the effect of the ReLUs. We say that a region is at layer l if it corresponds to an activation pattern (S1, . . . , Sl) up to layer l. For a region S at layer l − 1, define W̄ lS := W l σSl−1(W l−1) · · ·σS1(W 1).\nEach region S at layer l − 1 may be partitioned by a set of hyperplanes defined by the neurons of layer l. When viewed in the input space, these hyperplanes are the rows of W̄ lSx+ b = 0 for some b. To verify this, note that, if we recursively substitute out the hidden variables hl−1, . . . , h1 from the original hyperplane W lhl−1 + bl = 0 following S , the resulting weight matrix applied to x is W̄ lS .\nA central element in the proof of Theorem 1 is the dimension of the image of a linear region S under the DNN function hl up to a layer l, which we denote by dim(hl(S)). For a fixed region S at layer l, that function is linear, and thus the dimension equals to the rank of the coefficient matrix. Therefore, dim(hl(S)) = rank(σSl(W l) · · ·σS1(W 1)). This can be interpreted as the dimension of the space corresponding to S that the hyperplanes defined by W l+1hl + bl+1 = 0 effectively partitions. A key observation is that, once this dimension falls to a certain value, the regions contained in S at subsequent layers cannot recover to a higher dimension.\nZaslavsky (1975) showed that the maximal number of regions in Rd induced by an arrangement of m hyperplanes is at most ∑d j=0 ( m j ) . Moreover, this value is attained if\nand only if the hyperplanes are in general position. The lemma below tightens this bound for a special case where the hyperplanes may not be in general position.\nLemma 4. Consider m hyperplanes in Rd defined by the rows of Wx+ b = 0. Then the number of regions induced by the hyperplanes is at most ∑rank(W ) j=0 ( m j ) .\nThe proof is given in Appendix C. Its key idea is that it suffices to count regions within the row space of W . The next lemma brings Lemma 4 into our context.\nLemma 5. The number of regions induced by the nl neurons at layer l within a certain region S is at most∑min{nl,dim(hl−1(S))}\nj=0\n( nl j ) .\nProof. The hyperplanes in a region S of the input space are given by the rows of W̄ lSx + b = 0 for some b. By definition, the rank of W̄ lS is upper bounded by min{rank(W l), rank(σSl−1(W l−1) · · ·σS1(W 1))} = min{rank(W l),dim(hl−1(S))}. That is, rank(W̄ lS) ≤ min{nl,dim(hl−1(S))}, and we apply Lemma 4.\nIn the next lemma, we show that the dimension of the image of a region S can be bounded recursively in terms of the dimension of the image of the region containing S and the number of activated neurons defining S. Lemma 6. Let S be a region at layer l and S ′ be the region at layer l − 1 that contains it. Then dim(hl(S)) ≤ min{|Sl|,dim(hl−1(S ′))}.\nProof. dim(hl(S)) = rank(σSl(W l) · · ·σS1(W 1)) ≤ min{rank(σSl(W l)), rank(σSl−1(W l−1) · · ·σS1(W 1)) ≤ min{|Sl|,dim(hl−1(S ′))}. The last inequality comes from zeroed out rows not counting towards the matrix rank.\nWe now have the ingredients to prove Theorem 1.\nProof of Theorem 1. As illustrated in Figure 2, the partitioning can be regarded as a sequential process: at each layer, we partition the regions obtained from the previous layer. When viewed in the input space, each region S obtained at layer l−1 is potentially partitioned by nl hyperplanes given by the rows of W̄ lSx+ b = 0 for some bias b. Some hyperplanes may fall outside the interior of S and have no effect.\nWith this process in mind, we recursively bound the number of subregions within a region. More precisely, we construct a recurrence R(l, d) to upper bound the maximal number of regions attainable from partitioning a region with image of dimension d using units from layers l, l+1, . . . , L. The base case is given by Lemma 5: R(L, d) = ∑min{nL,d} j=0 ( nL j ) . Based on Lemma 6, the recurrence groups together regions with same activation set size |Sl|, as follows: R(l, d) =∑nl\nj=0Nnl,d,jR(l + 1,min{j, d}) for all l = 1, . . . , L− 1.\nHere, Nnl,d,j represents the maximum number of regions with |Sl| = j from partitioning a space of dimension d with nl hyperplanes. We bound this value next.\nFor each j, there are at most ( nl j ) regions with |Sl| = j, as they can be viewed as subsets of nl neurons of size j. In total, Lemma 5 states that there are at most ∑min{nl,d} j=0 ( nl j\n) regions. If we allow these regions to have the highest |Sl| possible, for each j from 0 to min{nl, d} we have at most(\nnl nl−j ) = ( nl j ) regions with |Sl| = nl − j.\nTherefore, we can write the recurrence as\nR(l, d) =  min{nl,d}∑ j=0 ( nl j ) R(l + 1,min{nl − j, d}) if 1 ≤ l ≤ L− 1, min{nL,d}∑\nj=0\n( nL j ) if l = L.\nThe recurrence R(1, n0) can be unpacked to\nmin{n1,d1}∑ j1=0 ( n1 j1 )min{n2,d2}∑ j2=0 ( n2 j2 ) · · · min{nL,dL}∑ jL=0 ( nL jL )\nwhere dl = min{n0, n1 − j1, . . . , nl−1 − jl−1}. This can be made more compact, resulting in the final expression. The bound is tight when L = 1 since it becomes∑min{n0,n1} j=0 ( n1 j ) , which is the maximal number of regions of a single-layer network.\nAs a side note, Theorem 1 can be further tightened if the weight matrices are known to have small rank. The bound from Lemma 5 can be rewritten as ∑min{rank(W l),dim(hl−1(S))}\nj=0\n( nl j ) if we do not relax\nrank(W l) to nl in the proof. The term rank(W l) follows through the proof of Theorem 1 and the index set J in the theorem becomes {(j1, . . . , jL) ∈ ZL : 0 ≤ jl ≤ min{n0, n1 − j1, . . . , nl−1 − jl−1, rank(W l)} ∀l ≥ 1}.\nA key insight from Lemmas 5 and 6 is that the dimensions of the images of the regions are non-increasing as we move through the layers partitioning them. In other words, if at any layer the dimension of the image of a region becomes small, then that region will not be able to be further partitioned into a large number of regions. For instance, if the dimension of the image of a region falls to zero, then that region will never be further partitioned. This suggests that if we want to have many regions, we need to keep dimensions high. We use this idea in the next section to construct a DNN with many regions."
  }, {
    "heading": "3.2. The Case of Dimension One",
    "text": "If the input dimension n0 is equal to 1 and nl = n for all layers l, the upper bound presented in the previous section reduces to (n + 1)L. On the other hand, the lower bound given by Montúfar et al. (2014) becomes nL−1(n+ 1). It is then natural to ask: are either of these bounds tight? The answer is that the upper bound is tight in the case of n0 = 1, assuming there are sufficiently many neurons.\nTheorem 7. Consider a deep rectifier network with L layers, nl ≥ 3 rectified linear units at each layer l, and an input of dimension 1. The maximal number of regions of this neural network is exactly ∏L l=1(nl + 1).\nThe expression above is a simplified form of the upper bound from Theorem 1 in the case n0 = 1.\nThe proof of this theorem in Appendix D has a construction with n+1 regions that replicate themselves as we add layers, instead of n as in Montúfar et al. (2014). That is motivated by an insight from the previous section: in order to obtain more regions, we want the dimension of the image of every region to be as large as possible. When n0 = 1, we want all regions to have images with dimension one. This intuition leads to a new construction with one additional region that can be replicated with other strategies."
  }, {
    "heading": "3.3. Lower Bounds on the Maximal Number of Linear Regions",
    "text": "Both the lower bounds from Montúfar et al. (2014) and from Arora et al. (2018) can be slightly improved, since their approaches are based on extending a 1-dimensional construction similar to the one in Section 3.2. We do both since they are not directly comparable: the former bound is in terms of the number of neurons in each layer and the latter is in terms of the total size of the network.\nTheorem 8. The maximal number of linear regions induced by a rectifier network with n0 input units and L hidden layers with nl ≥ 3n0 for all l is lower bounded by (∏L−1\nl=1 (⌊ nl n0 ⌋ + 1 )n0)∑n0 j=0 ( nL j ) .\nThe proof of this theorem is in Appendix E. For comparison, the differences between the lower bound theorem (Theorem 5) from Montúfar et al. (2014) and the above theorem is the replacement of the condition nl ≥ n0 by the more restrictive nl ≥ 3n0, and of bnl/n0c by bnl/n0c+ 1. Theorem 9. For any m ≥ 1 and w ≥ 2, there exists a rectifier network with n0 input units and L hidden layers of size 2m + w(L − 1) that has 2 ∑n0−1 j=0 ( m−1 j ) (w + 1)L−1 linear regions.\nThe proof of this theorem is in Appendix F. The differences between Theorem 2.11(i) from Arora et al. (2018) and the above theorem is the replacement of w by w + 1. They\nconstruct a 2m-width layer with many regions and use a one-dimensional construction for the remaining layers."
  }, {
    "heading": "4. An Upper Bound on the Number of Linear Regions for Maxout Networks",
    "text": "We now consider a deep neural network composed of maxout units. Given weights W lj for j = 1, . . . , k, the output of a rank-k maxout layer l is given by\nhl = max{W l1hl−1 + bl1, . . . ,W lkhl−1 + blk}\nIn terms of bounding number of regions, a major difference between the next result for maxout units and the previous one for ReLUs is that dimensionality plays less of a role, since neurons may no longer have an inactive state with zero output. Nevertheless, using techniques similar to the ones from Section 3.1, the following theorem can be shown (see Appendix G for the proof).\nTheorem 10. Consider a deep neural network withL layers, nl rank-k maxout units at each layer l, and an input of dimension n0. The maximal number of regions is at most\nL∏ l=1 dl∑ j=0 (k(k−1) 2 nl j )\nwhere dl = min{n0, n1, . . . , nl}.\nAsymptotically, if nl = n for all l = 1, . . . , L, n ≥ n0, and n0 = O(1), then the maximal number of regions is at most O((k2n)Ln0)."
  }, {
    "heading": "5. Exact Counting of Linear Regions",
    "text": "If the input space x ∈ Rn0 is bounded by minimum and maximum values along each dimension, or else if x corresponds to a polytope more generally, then we can define a mixed-integer linear formulation mapping polyhedral regions of x to the output space y. The assumption that x is bounded and polyhedral is natural in most applications, where each value xi has known lower and upper bounds (e.g., the value can vary from 0 to 1 for image pixels). Among other things, we can use this formulation to count the number of linear regions.\nIn the formulation that follows, we use continuous variables to represent the input x, which we can also denote as h0, the output of each neuron i in layer l as hli, and the output y as hL+1. To simplify the representation, we lift this formulation to a space that also contains the output of a complementary set of neurons, each of which is active when the corresponding neuron is not. Namely, for each neuron i in layer l we also have a variable h l\ni := max{0,−W lihl−1− bli}. We use binary variables of the form zli to denote if each\nneuron i in layer l is active or else if the complement is. Finally, we assume M to be a sufficiently large constant.\nFor a given neuron i in layer l, the following set of constraints maps the input to the output:\nW lih l−1 + bli = h l i − h\nl i (1)\nhli ≤Mzli (2)\nh l\ni ≤M(1− zli) (3) hli ≥ 0 (4)\nh l\ni ≥ 0 (5) zli ∈ {0, 1} (6)\nTheorem 11. Provided that |W lihl−1 + bli| ≤ M for any possible value of hl−1, a formulation with the set of constraints (1)–(6) for each neuron of a rectifier network is such that a feasible solution with a fixed value for x yields the output y of the neural network.\nProof. It suffices to prove that the constraints for each neuron map the input to the output in the same way that the neural network would. If W lih\nl−1 + bli > 0, it follows that hli − h l\ni > 0 according to (1). Since both variables are non-negative due to (4) and (5) whereas one is non-positive due to (2), (3), and (6), then zli = 1 and hli = max { 0,W lih l−1 + bli } . If W lih l−1 + bli < 0, then it similarly follows that hli − h l i < 0, z l i = 0, and thus h l i = min { 0,W lih l−1 + bli } . If W lih l−1 + bli = 0, then either hli = 0 or h l\ni = 0 due to constraints (4) to (6) whereas (1) implies that h l\ni = 0 or h l i = 0, respectively. In this case,\nthe value of zli is arbitrary but irrelevant.\nA systematic method to count integer solutions is the onetree approach (Danna et al., 2007), which resumes the search after an optimal solution has been found using the same branch-and-bound tree. This method also allows for counting feasible solutions within a threshold of the optimal value. Note that in constraints (1)–(6), the variables zli can be either 0 or 1 when they lie on the activation boundary, whereas we want to consider a neuron active only when its output is strictly positive. This discrepancy may cause doublecounting when activation boundaries overlap. We can address that by defining an objective function that maximizes the minimum output f of an active neuron, which is positive in the non-degenerate cases that we want to count. We state this formulation for rectifier networks as follows:\nmax f\ns.t. (1)–(6) ∀ neuron i in layer l (P) f ≤ hli + (1− zli)M ∀ neuron i in layer l x ∈ X\nA discusssion on the choice for the M constants can be found in Appendix H. In addition, we discuss the mixedrepresentability of DNNs in Appendix I, the theory for unrestricted inputs in Appendix J, and a mixed-integer formulation for maxout networks in Appendix K, respectively."
  }, {
    "heading": "6. Experiments",
    "text": "We perform an experiment to count linear regions of smallsized networks with ReLU activation units on the MNIST benchmark dataset (LeCun et al., 1998). In this experiment, we train rectifier networks with two hidden layers summing up to 22 neurons. We train 10 networks for each configuration for 20 epochs or training steps, and we count all linear regions within 0 ≤ x ≤ 1. The counting code is written in C++ (gcc 4.8.4) using CPLEX Studio 12.8 as a solver and ran in Ubuntu 14.04.4 on a machine with 40 Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz processors and 132 GB of RAM. The runtimes for counting different configuration can be found in Appendix L.\nFigure 1 shows the average, minimum, and maximum number of linear regions for each configuration of these networks. The plot also contains the corresponding upper bounds for each configuration from Theorem 1 and those from Montúfar et al. (2014) and Montúfar (2017), which are the first and the tightest bounds from prior work respectively. Note that upper bound from Theorem 1 is tighter.\nFigure 6 shows how the number of regions for each trained DNN compares with Cross-Entropy (CE) error on training set, and Misclassification Rate (MR) on testing set. If an intermediate layer in a network has only 1 or 2 neurons, it is natural to expect very high training and test errors. We discard such DNNs with a layer of small width in Figure 7."
  }, {
    "heading": "7. Discussion",
    "text": "Our ReLU upper bound indicates that small widths in early layers cause a bottleneck effect on the number of regions. If\nwe reduce the width of an early layer, the dimension of the image of the linear regions become irrecoverably smaller throughout the network and the regions will not be able to be partitioned as much. This intuition allows us to develop a 1-dimensional construction with the maximal number of regions by eliminating a zero-dimensional bottleneck. In our experiment, we validate the bottleneck effect by observing that the actual number of linear regions is asymmetric when one layer is increased and another decreased in size.\nAn unexpected consequence of one of our results is that shallow networks can attain more linear regions when the input dimensions exceed the number of neurons. This complements prior work, which has not considered large input dimensions, a common case in practice.\nWe also observe in Figure 5 that the depth that maximizes the upper bound from Theorem 1 increases with the number of units and decreases with the size of the input. It would be interesting to investigate if this also happens with respect to the actual number of regions, in which case the depth could be chosen according to those parameters.\nHowever, it would be possible that DNNs configurations with large number of linear regions do not generalize well if there are so many regions that each training point can be singled out in a different region, in particular if regions with similar labels are unlikely to be compositionally related.\nNevertheless, we have initial evidence that training and classification accuracies relate to the number of regions for configurations where no layer is too small, hence suggesting that the number of regions can be a metric for comparing similar DNN configurations. However, in the cases where one layer is too small, we have also observed that the number of regions can be large and do not reflect the capacity of the DNN. We hypothesize that the presence of low dimensionality negatively affects the accuracy of the network, even when the number of regions is large because of other layers. This indicates that potentially more insights could be gained from investigating the shape of linear regions."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the anonymous reviewers for useful suggestions."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural network learning: Theoretical foundations",
    "authors": ["M. Anthony", "P. Bartlett"],
    "year": 1999
  }, {
    "title": "Understanding deep neural networks with rectified linear units",
    "authors": ["R. Arora", "A. Basu", "P. Mianjy", "A. Mukherjee"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "Disjunctive programming",
    "authors": ["E. Balas"],
    "venue": "Annals of Discrete Mathematics,",
    "year": 1979
  }, {
    "title": "A lift-and-project cutting plane algorithm for mixed 0–1 programs",
    "authors": ["E. Balas", "S. Ceria", "G. Cornuéjols"],
    "venue": "Mathematical Programming,",
    "year": 1993
  }, {
    "title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures",
    "authors": ["M. Bianchini", "F. Scarselli"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
    "year": 2014
  }, {
    "title": "Maximum resilience of artificial neural networks",
    "authors": ["Cheng", "C.-H", "G. Nührenberg", "H. Ruess"],
    "venue": "In ATVA,",
    "year": 2017
  }, {
    "title": "Multi column deep neural network for traffic sign classification",
    "authors": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"],
    "venue": "Neural Networks,",
    "year": 2012
  }, {
    "title": "Approximation by superpositions of a sigmoidal function",
    "authors": ["G. Cybenko"],
    "venue": "Mathematics of Control, Signals and Systems,",
    "year": 1989
  }, {
    "title": "Generating multiple solutions for mixed integer programming problems",
    "authors": ["E. Danna", "M. Fenelon", "Z. Gu", "R. Wunderling"],
    "venue": "In IPCO, pp",
    "year": 2007
  }, {
    "title": "Shallow vs. deep sum-product networks",
    "authors": ["O. Delalleau", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "The power of depth for feedforward neural networks",
    "authors": ["R. Eldan", "O. Shamir"],
    "venue": "In Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "year": 2016
  }, {
    "title": "Deep neural networks for acoustic modeling in speech recognition",
    "authors": ["G. Hinton", "L. Deng", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2012
  }, {
    "title": "Representability in mixed integer programming, I: Characterization results",
    "authors": ["R. Jeroslow"],
    "venue": "Discrete Applied Mathematics,",
    "year": 1987
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "A comparison of the computational power of sigmoid and boolean threshold circuits",
    "authors": ["W. Maass", "G. Schnitger", "E. Sontag"],
    "venue": "Theoretical Advances in Neural Computation and Learning,",
    "year": 1994
  }, {
    "title": "Learning real and boolean functions: When is deep better than shallow",
    "authors": ["H. Mhaskar", "Q. Liao", "T.A. Poggio"],
    "year": 2016
  }, {
    "title": "Notes on the number of linear regions of deep neural networks",
    "authors": ["G. Montúfar"],
    "venue": "In SampTA,",
    "year": 2017
  }, {
    "title": "On the number of linear regions of deep neural networks",
    "authors": ["G. Montúfar", "R. Pascanu", "K. Cho", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Expressiveness of rectifier networks",
    "authors": ["X. Pan", "V. Srikumar"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "On the number of response regions of deep feedforward networks with piecewise linear activations",
    "authors": ["R. Pascanu", "G. Montúfar", "Y. Bengio"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "On the expressive power of deep neural networks",
    "authors": ["M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. SohlDickstein"],
    "year": 2017
  }, {
    "title": "Representation benefits of deep feedforward networks",
    "authors": ["M. Telgarsky"],
    "venue": "CoRR, abs/1509.08101,",
    "year": 2015
  }, {
    "title": "Facing up to arrangements: face-count formulas for partitions of space by hyperplanes",
    "authors": ["T. Zaslavsky"],
    "venue": "American Mathematical Society,",
    "year": 1975
  }],
  "id": "SP:9232ef1d77d6ed997966ca7dda08c7c36274402c",
  "authors": [{
    "name": "Thiago Serra",
    "affiliations": []
  }, {
    "name": "Christian Tjandraatmadja",
    "affiliations": []
  }, {
    "name": "Srikumar Ramalingam",
    "affiliations": []
  }],
  "abstractText": "We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation. These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.",
  "title": "Bounding and Counting Linear Regions of Deep Neural Networks"
}