{
  "sections": [{
    "heading": "1. Introduction",
    "text": ""
  }, {
    "heading": "1.1. Background and motivation",
    "text": "Maximum selection (maxing) and sorting (ranking) are fundamental problems in Computer Science with numerous important applications. Deterministic versions of these problems are well studied.\nIn practical applications, comparisons are rarely deterministic. For example in soccer, when Real Madrid plays Barcelona the outcome is not always the same. Similarly, individual preferences in restaurants vary a lot. Other practical applications are in areas such as social choice (Caplin & Nalebuff, 1991; Soufiani et al., 2013), web search and information retrieval (Radlinski & Joachims, 2007; Radlinski et al., 2008), crowdsourcing (Chen et al., 2013; gif), recommender systems (Baltrunas et al., 2010) and several others.\nThese practical applications and the intrinsic theoretical interest, has led to significant work on the probabilistic version of maxing and ranking. Yet the most general model for which maxing can be done using near-linear comparisons is not known. We consider the most general transitive model\n1University of California, San Diego. Correspondence to: Venkatadheeraj Pichapati <dheerajpv7@ucsd.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nthat guarantees the existence of maximum and show that under this model any maxing algorithm requires quadratic many comparisons. We also consider a slightly more restrictive transitive model and propose a linear complexity maxing algorithm, making it the most general model known for which linear complexity maxing is possible. Also, for the most general known model with sub-quadratic complexity for ranking, we improve the complexity, making it orderwise optimal. We also propose an optimal algorithm that can simulate all pairwise comparisons."
  }, {
    "heading": "1.2. Notation and problem formulation",
    "text": "Without loss of generality, let [n] def= {1, 2, ..., n} be the set of n elements. We consider probabilistic noisy comparisons i.e., whenever two elements i and j are compared, i is returned with an unknown probability pi,j . There are no “ties” i.e., pj,i = 1 − pi,j . Let p̃i,j def = pi,j − 12 be the centered preference probability.\nA maximal is an element i that is preferable to every other element i.e., p̃i,j ≥ 0 ∀j. A ranking is a permutation σ1, σ2, ..., σn of [n] such that p̃σi,σj ≥ 0 whenever i > j.\nBut sometimes maximal and ranking might not even exist. For example, consider the popular Rock-Paper-Scissor game i.e., p1,2 = p2,3 = p3,1 = 1. Notice that under this model there is neither a maximal nor a ranking. Hence we need additional constraints on pairwise probabilities pi,j .\nNotice that for ranking to exist, there must exist an ordering ( ) among elements s.t. whenever i j, p̃i,j ≥ 0. The models that have such an ordering are said to satisfy Weak Stochastic Transitivity (WST). Observe that WST is sufficient for existence of both maximal and ranking.\nMore restrictive notions of transitivity are motivated and used in different contexts. Strong Stochastic Transitivity (SST) which assumes that whenever i j k, p̃i,k ≥ max(p̃i,j , p̃j,k), as its name suggests is a stronger notion of transitivity that confines the model more than WST, hence less general. Medium Stochastic Transitivity (MST) (Skorepa, 2010) sitting in between WST and SST, assumes that whenever i j k, p̃i,k ≥ min(p̃i,j , p̃j,k). From WST to MST to SST, the model becomes more restrictive.\nAnother model restriction used in some of the previous\nworks Stochastic Triangle Inequality (STI), assumes that whenever i j k, p̃i,k ≤ p̃i,j + p̃j,k. In this paper we propose maxing and ranking algorithms for models under various set of constraints.\nThere is also a concern with finding an exact maximal and ranking. Consider the case of n = 2 and p̃1,2 ≈ 0. Notice that in this case where n is just 2, finding maximal and ranking could take arbitrarily many comparisons. Easy fix to alleviate this problem is to consider Probably Approximately Correct (PAC) formulation which we also adopt.\nAn element i is said to be -preferable to j if p̃i,j ≥ − . For ∈ (0, 1/2), an -maximal is an element i that is - preferable to all elements i.e., p̃i,j ≥ − ∀j. Given 0 < < 1/2, 0 < δ ≤ 1/2, a PAC maxing algorithm must output an -maximal with probability≥ 1−δ. Similarly, an -ranking is a permutation σ1, σ2, ..., σn of [n] such that σi is -preferable to σj whenever i > j. Given 0 < < 1/2, 0 < δ ≤ 1/2, a PAC ranking algorithm must output an -ranking with probability ≥ 1− δ."
  }, {
    "heading": "1.3. Related work",
    "text": "Researchers initially considered more restrictive models. (Feige et al., 1994) considered constant noise model i.e., p̃i,j = α > 0 if i j and presented a maxing algorithm that usesO ( n α2 log 1 δ ) comparisons and outputs maximal with probability ≥ 1 − δ. It also presented a ranking algorithm that uses O ( n logn α2 ) comparisons and outputs ranking with probability ≥ 1− 1/n.\nAnother set of widely-studied restrictive models are parametric ones. (Szörényi et al., 2015) considered one of the most popular parametric models, Plackett-Luce (Plackett, 1975; Luce, 2005) and presented PAC maxing and ranking algorithms that useO ( n 2 log n δ ) andO ( n logn 2 log n δ\n) comparisons respectively.\nResearchers also considered models that are more general than parametric models, yet still more restrictive than WST. (Yue & Joachims, 2011) considered models that satisfy both SST and STI and derived a PAC maxing algorithm that uses O ( n 2 log n δ ) comparisons. Later (Falahatgar et al., 2017b) considered same model and proposed an optimal PAC maxing algorithm that uses O ( n 2 log 1 δ\n) comparisons. It also proposed a PAC ranking algorithm that with probability ≥ 1 − 1/n, outputs an -ranking using O ( n logn(log logn)3\n2\n) comparisons, (log log n)3 times\nthe known lower bound. Until now, it was not known if the additional (log log n)3 factor is necessary for PAC ranking.\n(Falahatgar et al., 2017a) considered models that satisfy only SST but not necessarily STI and proposed an optimal PAC maxing algorithm that uses O ( n 2 log 1 δ ) compar-\nisons. They also showed that there exists a model which satisfies SST and yet no algorithm can find an -ranking for this model using o(n2) comparisons, establishing a lower bound of Ω(n2) comparisons once STI property is dropped.\nAmong other related works we can point out (BusaFekete et al., 2014b; Lee et al., 2014; Dudı́k et al., 2015; Hüllermeier et al., 2008), who considered models more general than WST under different definitions of maximum and ranking. More discussion about these models can be found in Appendix G. (Busa-Fekete et al., 2014a; Mohajer et al., 2017) considered the non-PAC version and (Rajkumar & Agarwal, 2014; Negahban et al., 2012; 2016; Jang et al., 2016) considered the non-adaptive version of this problem. Also (Acharya et al., 2016; Ajtai et al., 2015) considered the deterministic adversarial version of maxing and ranking. (Shah et al., 2016b; Chatterjee et al., 2015; Shah et al., 2016a) studied the problem of estimating pairwise probabilities in non-adaptive setting."
  }, {
    "heading": "2. New results and Outline",
    "text": "Maxing Linear-complexity maxing algorithm under SST by (Falahatgar et al., 2017a) encourages the search for a linear-complexity maxing algorithm for models with only WST properties. Two questions then arise: 1a) Is a linear complexity PAC maxing algorithm possible for models with only WST property? 1b) If not, does there exist a model more general than SST and less general than WST for which a linear complexity PAC maxing is possible?\nWe resolve both questions in this paper: 1a) No. Theorem 1 in Section 3 shows that there are WST models for which any PAC maxing algorithm requires Ω(n2) comparisons. 1b) Yes. In Theorem 8 in Section 4, we derive a PAC maxing algorithm for MST model that uses O ( n 2 log 1 δ\n) comparisons for δ ≥ min(1/n, e−n1/4).\nRanking Motivated by the previous results of ranking under SST + STI, three questions arise: 2a) For models with SST + STI, is the additional (log log n)3 factor necessary for PAC ranking algorithms? 2b) Since the nearlinear complexity of ranking under SST + STI changes to quadratic complexity by dropping STI (Falahatgar et al., 2017a), is there a sub-quadratic algorithm for ranking under MST + STI? 2c) For models with SST + STI, since PAC ranking is possible with near linear complexity, is it also possible to approximate all pairwise probabilities to accuracy of using near linear number of comparisons?\nWe essentially resolve all three questions. 2a) No. In Theorem 9 in Section 5, we improve the PAC ranking algorithm for models with SST + STI removing additional (log log n)3 factor and hence making it optimal. 2b) No. Theorem 10 in Section 6 shows that there is a model with MST+STI, for which any PAC ranking algorithm requires\nΩ(n2) comparisons. 2c) Yes. For models with SST + STI, in Theorems 11 and 12 in Sections 7, we present an optimal algorithm that uses O ( nmin(n,1/ ) logn\n2\n) comparisons\nand approximates all pairwise probabilities to accuracy of with probability ≥ 1− 1/n.\nWe present experiments over simulated data in Section 8 and end with our conclusions in Section 9.\nInterpretation Table 1 summarizes all known results for problems of maxing, ranking, and finding pairwise probabilities under different transitive properties. Notice that under the most general model WST, all these problems require quadratic many comparisons and under the most restrictive model SST + STI, all problems have optimal algorithms with near-linear complexity. For MST and WST models adding STI property does not influence complexity for any problem. But for SST model adding STI property facilitates near-linear complexity algorithms for PAC ranking and approximating pairwise probabilities.\nIt is easy to see that once all pairwise probabilities are approximated to accuracy of /2, one can find an -maximum and an -ranking. Hence approximating pairwise probabilities is harder than PAC ranking and lower bound for PAC ranking implies a lower bound for problem of approximating pairwise probabilities. Therefore in Table 1 lower bounds for finding pij follow from lower bounds for ranking. Further in Appendix B.1, under WST model, we present a trivial algorithm that with probability ≥ 1 − δ, estimates all pairwise probabilities to accuracy of using O ( n2\n2 log n δ\n) comparisons. Hence upper bound of\nO ( n2\n2 log n δ\n) follows for all problems."
  }, {
    "heading": "3. PAC maxing for WST",
    "text": "We show the lower bound of Ω(n2) for maxing under WST by presenting an example for which any algorithm requires\nΩ(n2) comparisons to output a 1/4-maximum for δ ≤ 1/8.\nTo establish the lower bound, we reduce the problem of finding a 1/4-maximum to finding the left most piece of a linear jigsaw puzzle. We consider the following model with n elements S = {a1, a2, . . . , an} : p̃ai,ai+1 = 12∀i < n, and p̃ai,aj = µ(0 < µ < 1/n\n10),∀j > i + 1. This model satisfies WST since there exists an underlying order , ai aj if i < j (because p̃ai,aj > 0) and a1 is the only 1/4-maximum under this model.\nObserve that ai is always preferred to ai+1, but for every non consecutive pair, comparison output is almost a fair coin flip. We make the problem simpler by giving the extra information of whether two non consecutive elements are being compared. Notice that this only makes the problem easier, namely, complexity for modified problem is smaller than that of original problem.\nThe modified problem is similar to a linear jigsaw puzzle where if we compare two pieces we will know if pieces are adjacent or not and if adjacent, which piece is on the left, the goal is to find the left most piece. We show that w.h.p., any algorithm neither finds more than n/32 connections (a set of neighbors) nor asks Ω(n) comparisons for the left most piece. We use this to show the lower bound. The proof is in Appendix A.\nTheorem 1. There exists a model that satisfies WST for which any algorithm requires Ω(n2) comparisons to find a 1/4-maximum with probability ≥ 7/8."
  }, {
    "heading": "4. PAC maxing for MST",
    "text": "Outline In this section, we propose OPT-MAX, a linear complexity maxing algorithm for MST. In the process, we present two other suboptimal maxing algorithms SOFTSEQ-ELIM, NEAR-OPT-MAX and use them as building blocks in OPT-MAX. SOFT-SEQ-ELIM finds an - maximum with quadratic complexity. Its performance depends on the starting element (anchor). NEAR-OPT-MAX\nfirst finds a good anchor and then uses SOFT-SEQ-ELIM, guaranteeing near linear comparison complexity. OPTMAX builds on NEAR-OPT-MAX and finds an -maximum in linear-complexity for δ ≥ min(1/n, e−n1/4)."
  }, {
    "heading": "4.1. SOFT-SEQ-ELIM",
    "text": "Before presenting SOFT-SEQ-ELIM, we first present the subroutine COMPARE we use to compare two elements.\nCOMPARE COMPARE takes 5 parameters : two elements i, j that need to be compared, lower bias l, upper bias u, confidence δ and deems if p̃i,j < l or p̃i,j > u. It compares i and j for 8( u− l)2 log 2 δ times. Let p̂i,j be the fraction of times i won and ˆ̃pi,j = p̂i,j − 1/2. If ˆ̃pi,j < 3 l 4 + u 4 , then COMPARE deems p̃i,j < l (returns 1), if ˆ̃pi,j > l 4 + 3 u 4 , then COMPARE deems p̃i,j > u (returns 3) and for other ranges of ˆ̃pi,j , COMPARE not able to take a decision, returns 2.\nLemma 2 bounds comparisons used by COMPARE and proves its correctness. COMPARE and its analysis is presented in Appendix C.2.\nLemma 2. For u > l, COMPARE(i, j, l, u, δ) uses ≤ 8 ( u− l)2 log 2 δ comparisons and if p̃i,j < l, then w.p.≥ 1− δ, it returns 1, else if p̃i,j > u, w.p.≥ 1− δ, it returns 3. Further if p̃i,j ≤ ( l + u)/2, w.p.≥ 1 − δ, it does not return 3 and if p̃i,j > ( l + u)/2, w.p.≥ 1− δ, it does not return 1.\nSOFT-SEQ-ELIM SOFT-SEQ-ELIM takes 5 parameters: input set S, starting anchor element r, lower bias l, upper bias u and confidence δ. SOFT-SEQ-ELIM happens in rounds. In each round, it compares the current anchor a with remaining elements one by one using COMPARE. Due to probabilistic nature, we cannot exactly compare if p̃e,a > u vs p̃e,a ≤ u. Hence we compare if p̃e,a > u vs p̃e,a < l. For an element e, if COMPARE deems p̃e,a < l, then SOFT-SEQ-ELIM eliminates that element and if COMPARE deems p̃e,a > u, then SOFT-SEQ-ELIM updates current anchor element to e and eliminates a. This process is continued until the current anchor element is not updated after comparing with all remaining elements and then SOFT-SEQ-ELIM outputs final anchor element.\nIf p̃e,a < l or p̃e,a > u, COMPARE deems correctly. If l ≤ p̃e,a ≤ u, then COMPARE can sometimes fail to output any decision and in that case, SOFT-SEQ-ELIM neither eliminates that element nor updates the anchor element, it just moves to next remaining element in S.\nTheoretically, performance of SOFT-SEQ-ELIM strongly depends on the starting anchor element r. To define a good anchor element, similar to (Falahatgar et al., 2017a), an element a is called an ( ,m)-good anchor if a is\nAlgorithm 1 SOFT-SEQ-ELIM 1: inputs 2: Set S, element r, lower bias l, upper bias u, confi-\ndence δ 3: Q = S \\ {r} 4: while Q 6= ∅ do 5: r′ = r, Q′ = ∅ 6: for c ∈ Q do 7: k = COMPARE(c, r, l, u, 2δ|S|2 ) 8: if k == 1 then 9: Q′ = Q′ ⋃ {c}.\n10: else if k == 3 then 11: r ← c 12: Q′ = Q′ ⋃ {c} 13: break 14: end if 15: end for 16: if r == r′ then 17: break 18: end if 19: Q = Q \\Q′ 20: end while 21: return r\nnot -preferable to at most m elements, i.e., |{e : e ∈ S and p̃e,a > }| ≤ m. We show that every element for which initial anchor r is l-preferable is deemed bad and gets eliminated after its first comparison round and hence comparisons spent on all such elements is O(|S|). Since initial anchor r is an ( l,m)-good anchor element, there are only m elements for which r is not l-preferable. We later show that only these elements can become anchors, leading to at most m changes of anchors. Therefore each such element gets compared in at mostm rounds and hence we can bound total comparison rounds by O(|S| + m2). Lemma 3 bounds comparisons used by SOFT-SEQ-ELIM and proves its correctness. Proof is in Appendix C.3.\nLemma 3. If r is an ( l,m)-good anchor element, w.p.≥ 1 − δ, SOFT-SEQ-ELIM(S, r, l, u, δ) uses O ( |S|+m2 ( u− l)2 log |S| δ ) comparisons and outputs r̂, an u maximum of S, such that either r̂ = r or p̃r̂,r > l+ u2 .\nCorollary 4 bounds comparisons used by SOFT-SEQ-ELIM for any starting anchor. Proof follows from Lemma 3\nCorollary 4. For any r, w.p.≥ 1 − δ, SOFT-SEQ-ELIM(S, r, l, u, δ) uses O\n( |S|2\n( u− l)2 log |S| δ ) comparisons and outputs r̂, an u maximum of S, such that either r̂ = r or p̃r̂,r > l+ u2 .\nNow we build on SOFT-SEQ-ELIM and propose a near linear algorithm NEAR-OPT-MAX."
  }, {
    "heading": "4.2. NEAR-OPT-MAX",
    "text": "NEAR-OPT-MAX(S, , δ) w.p.≥ 1 − δ, uses O ( |S| 2 ( log |S|δ )2) comparisons and outputs an -\nmaximum of S.\nSince complexity of SOFT-SEQ-ELIM depends on the initial anchor element, if we can pick a good initial anchor element, then we can reduce the number of comparisons. One way to pick a good initial anchor element is to find an /2-maximum of a randomly picked subset.\nLemma 5 shows that an -maximum of a randomly picked subset is a good anchor element. Proof in Appendix C.4.\nLemma 5. If r is an -maximum of a set Q, formed by picking m elements randomly from S, then w.p.≥ 1 − δ, r is an ( , |S|m log |S| δ ) -good anchor element of S.\nNEAR-OPT-MAX(S, , δ) first picks a random subset Q of size √ |S| log 4|S|δ and uses SOFT-SEQ-ELIM to find an\n/2-maximum of Q.\nBy Lemma 5, w.p.≥ 1 − δ/4, an /2-maximum of Q will be an ( /2, √ |S| log 4|S|δ )-good anchor element. NEAROPT-MAX then uses SOFT-SEQ-ELIM with /2-maximum of Q as initial anchor to find an -maximum of S. Since the initial anchor is provably good, we are able to bound the comparisons.\nAlgorithm 2 NEAR-OPT-MAX 1: inputs 2: Set S, bias , confidence δ\n3: Form a set Q by selecting √ |S| log 4|S|δ random ele-\nments from S without replacement. 4: a← random element from Q, Q = Q \\ {a} 5: r ← SOFT-SEQ-ELIM ( Q, a, 0, 2 , δ 4 ) , S = S \\ {r} 6: return SOFT-SEQ-ELIM(S, r, /2, , δ/2)\nLemma 6 bounds the comparisons used by NEAR-OPT-MAX and proves its correctness.\nLemma 6. With probability ≥ 1 − δ, NEAR-OPT-MAX(S, , δ) uses O ( |S| 2 ( log |S|δ )2) comparisons and outputs an -maximum of S.\nWe build on NEAR-OPT-MAX and derive an optimal algorithm for δ ≥ min(1/|S|, e−|S|1/4)."
  }, {
    "heading": "4.3. Optimal linear Algorithm",
    "text": "We first present an algorithm that is optimal for low ranges of δ i.e., min(e−|S| 1/4\n, 1/|S|) ≤ δ ≤ 1|S|1/3 .\n4.3.1. LOW RANGES OF δ\nWe first find a good anchor, this time using NEAR-OPTMAX and then use SOFT-SEQ-ELIM with NEAR-OPTMAX output as initial anchor.\nOPT-MAX-LOW picks a random subset of size |S|3/4 and finds an /2-maximum of this set using NEAR-OPT-MAX. We later show that output is an ( /2,O( √ |S|))-good anchor element of S. OPT-MAX-LOW then uses SOFT-SEQELIM with the previous output as initial anchor to find an -maximum of S. Since initial anchor is good, we are able to bound comparisons used by OPT-MAX-LOW.\nObserve that in OPT-MAX-LOW, we call SOFT-SEQ-ELIM three times in total: two times during NEAR-OPT-MAX and once to produce the final output. Each successive call of SOFT-SEQ-ELIM acts on higher size, namely first we find /4-maximum in a small set and using this element as anchor, then we find /2-maximum in a larger set and finally using this new element as anchor, we find an - maximum of the whole set S.\nAlgorithm 3 OPT-MAX-LOW 1: inputs 2: Set S, bias , confidence δ 3: Form a set Q by selecting |S|3/4 random elements\nfrom S without replacement 4: r ← NEAR-OPT-MAX(Q, 2 , δ 3 ) 5: return SOFT-SEQ-ELIM(S, r, 2 , , δ 3 )\nLemma 7 bounds comparisons used by OPT-MAX-LOW and proves its correctness. Proof is in Appendix C.6.\nLemma 7. For 1|S|1/3 ≥ δ ≥ min(1/|S|, e −|S|1/4), w.p.≥ 1− δ, OPT-MAX-LOW(S, , δ) uses O( |S| 2 log 1 δ ) comparisons and outputs r, an -maximum\n4.3.2. HIGHER RANGES OF CONFIDENCE δ For low ranges of confidence δ ( δ ≤ 1|S|1/3 ) , notice that log 1δ and log |S| δ are of same order and hence if we use SOFT-SEQ-ELIM with a good anchor, we can guarantee complexity of O ( |S| 2 log |S| δ ) = O ( |S| 2 log 1 δ ) .\nHowever, for high values of δ, this is not the case. We solve this problem by pruning S to a smaller set of size |S|/ log |S| such that it contains all good elements and then use SOFT-SEQ-ELIM. Due to space constraint, we present PRUNE, the pruning algorithm, OPT-MAX-MEDIUM, and OPT-MAX-HIGH, linear complexity maxing algorithms for higher ranges of confidence in Appendix C.8."
  }, {
    "heading": "4.4. Full Algorithm",
    "text": "In Theorem 8 we bound comparisons used by OPT-MAX and prove its correctness. Proof follows from Lemmas 7\nAlgorithm 4 OPT-MAX inputs\nSet S, bias , confidence δ if δ ≤ 1|S|1/3 then\nreturn OPT-MAX-LOW(S, , δ) end if if δ ≤ 1log |S| then\nreturn OPT-MAX-MEDIUM(S, , δ) end if return OPT-MAX-HIGH(S, , δ)\nand corresponding Lemmas 19 and 20 for OPT-MAXMEDIUM and OPT-MAX-HIGH given in Appendix C.8.\nTheorem 8. For δ ≥ min(1/|S|, e−|S|1/4), w.p.≥ 1 − δ, OPT-MAX(S, , δ) uses O ( |S| 2 log 1 δ ) comparisons and outputs an -maximum of S."
  }, {
    "heading": "5. Ranking for SST+STI",
    "text": "(Falahatgar et al., 2017b) provides a ranking algorithm that w.p.≥ 1 − 1/|S|, uses O ( |S| 2 log |S|(log log |S|) 3 )\ncomparisons and outputs an -ranking of input set S.\nWe build on their algorithm BINARY-SEARCH-RANKING, improving two components which lead to additional (log log |S|)3 factor, thereby proposing an optimal - ranking algorithm that uses O ( |S| 2 log |S| ) comparisons.\nIn Appendix 5, we outline the algorithm proposed in (Falahatgar et al., 2017b), pointing out the two components that lead to additional factor, and present ideas that improve over these components. For detailed explanation of BINARY-SEARCH-RANKING we refer readers to (Falahatgar et al., 2017b). Now we explain the high-level idea of how we improve over these components.\nThe two components that we improve upon share the property that each is being called for Ω(|S|/(log |S|)3) times and at each time finds a correct output w.p.≥ 1− 1/|S|5.\nInstead of finding a correct output w.p.≥ 1− 1/|S|5 in one shot, and incurring high complexity, we propose the following. First use the component to find a correct output w.p.≥ 1 − 1/ log |S|, then check if the output is correct or not. If the output is deemed to be not correct, run the component again, finding a correct output w.p.≥ 1− 1/|S|6.\nThus to show the potency of this idea, it suffices to show: One, the second run is only invoked a few times and two, the complexity of checking whether an output is correct is not high. Our main contribution is RANK-CHECK algorithm that checks if an ordered set is -ranked or not 3 - ranked. We present RANK-CHECK in Appendix D.3\nTheorem 9. BINARY-SEARCH-RANKING(S, ) (Falahatgar et al., 2017b) with new improved components presented here, w.p.≥ 1−1/|S|, usesO ( |S| log |S|\n2\n) comparisons and\noutputs an -ranking of S."
  }, {
    "heading": "6. Lower bound for ranking for MST+STI",
    "text": "In this section we show that there exists a model with both MST and STI properties under which any PAC ranking algorithm requires quadratic many comparisons. Consider the model S = {a1, a2, ..., an} s.t. a1 is preferable to a2 i.e., p̃a1,a2 = 1/2 and comparison between any other pair is almost a fair coin flip i.e., p̃ai,aj = µ ∀i < j and {i, j} 6= {1, 2} for some µ < 1/n10. This model satisfies both MST and STI. Any permutation which has a1 coming after a2 is a 1/4-ranking. But since comparison between any pair other than (a1, a2) is essentially a fair coin toss, any strategy that does not compare a1 and a2 will not have them in correct order in the output w.p.≈ 1/2 and hence won’t be a 1/4-ranking. Therefore this problem is similar to finding a single biased coin among ( n 2 ) coins which needs Ω(n2) comparisons.\nTheorem 10 bounds the complexity required for -ranking of models with MST and STI. Proof is in Appendix E.\nTheorem 10. There exists a model with MST and STI properties for which any algorithm requires Ω(n2) comparisons to output a 1/4-ranking w.p.≥ 7/8."
  }, {
    "heading": "7. Finding pairwise probabilities for SST+STI",
    "text": "Theorem 9 shows that for a model satisfying both SST and STI, an -ranking can be found using O ( |S| log |S|\n2\n) com-\nparisons. In this section we answer the question whether under same model we can approximate all pairwise probabilities to accuracy of using almost same complexity.\nWe first show a lower bound of Ω ( |S|min(|S|,1/ ) 2 log |S| )\nutilizing a model for which Ω(|S|min(|S|, 1/ )) pairwise probabilities need to be approximated using comparisons. Later we present APPROX-PROB that uses comparisons only for O(|S|min(|S|, 1/ )) pairs and hence obtain orderwise same upper bound as lower bound."
  }, {
    "heading": "7.1. Lower Bound",
    "text": "We show that any algorithm requires Ω ( |S|min(|S|,1/ ) log |S|\n2\n) comparisons to approximate\nall pairwise probabilities to accuracy.\nWe prove the lower bound by using the model: (4k+4) ≤ p̃ai+k,ai ≤ (4k + 8) for 1 ≤ k ≤ min(n − i, b 116 − 2c) and p̃ai+k,ai = 1/4 for k > min(n− i, b 116 − 2c).\nIt can be shown that this model satisfies both SST and STI.\nUnder this model, the only way to approximate unfixed pairwise probabilities is by comparing those pairs. Since pairwise probabilities are not fixed for Ω(nmin(n, 1/ )) pairs, any algorithm needs to approximate those many probabilities to accuracy of , hence the lower bound.\nTheorem 11 bounds the required complexity to approximate all pairwise probabilities. Proof is in Appendix F.1 Theorem 11. For < 1/48, there exists a model that satisfies both SST and STI for which any algorithm requires Ω ( |S|min(|S|,1/ ) 2 log |S| ) comparisons to approximate all\npairwise probabilities to accuracy w.p. ≥ 3/4."
  }, {
    "heading": "7.2. Upper Bound",
    "text": "Here we propose an algorithm to approximate all pairwise probabilities to an accuracy of .\nThe proposed algorithm, first finds an /8-ranking of the input set S and then approximates pairwise probabilities. By Theorem 9, w.p.≥ 1− 1|S|2 we can find an /8-ranking\nof the input set S using O ( |S| log |S|\n2\n) comparisons. We\npresent APPROX-PROB that given an /8-ranked set, approximates all pairwise probabilities to an accuracy of .\nAPPROX-PROB APPROX-PROB takes an /8-ranked ordered set S i.e., p̃S(i),S(j) ≤ /8 ∀i < j and bias and approximates all pairwise probabilities to an accuracy of .\nNote that it is enough to approximate p̃S(j),S(i) for j ≥ i since p̃S(i),S(j) = −p̃S(j),S(i). For all i > 1, APPROXPROB compares S(i) and S(1), 16 log |S| 4\n2 times and approximates p̃S(i),S(1) by ˆ̃pS(i),S(1), the fraction of times S(i) won rounded off to the nearest multiple of . Since for perfectly ranked ordered set p̃S(i+1),S(1) ≥ p̃S(i),S(1), if ˆ̃pS(i+1),S(1) < ˆ̃pS(i),S(1), then APPROX-PROB corrects ˆ̃pS(i+1),S(1), setting it equal to ˆ̃pS(i),S(1). It can be shown that p̃S(i),S(1) is approximated to an accuracy of 7 8 .\nAPPROX-PROB continues this process by approximating p̃S(i),S(2) for i ≥ 2 by increasing i one at a time. For a perfectly ranked set, p̃S(i−1),S(2) ≤ p̃S(i),S(2) ≤ p̃S(i),S(1) and hence if ˆ̃pS(i−1),S(2) = p̃S(i),S(1), APPROXPROB does not use comparisons to approximate p̃S(i),S(2), instead assigns ˆ̃pS(i),S(2) = ˆ̃pS(i−1),S(2). Whenever ˆ̃pS(i−1),S(2) 6= p̃S(i),S(1), APPROX-PROB approximates p̃S(i),S(2) by comparing S(i) and S(2). It can be shown that p̃S(i),S(2) is approximated to accuracy of .\nAPPROX-PROB continues this process for S(3), then S(4) and so on until S(n). Notice that whenever ˆ̃pS(i−1),S(j) = ˆ̃pS(i),S(j−1), APPROX-PROB does not use comparisons to approximate p̃S(i),S(j) but simply assigns ˆ̃pS(i),S(j) = ˆ̃pS(i−1),S(j). We show this in fact happens at many places\nand only O(|S|min(|S|, 1/ )) pairwise probabilities are approximated using comparisons. This enables obtaining orderwise same upper bound as the lower bound.\nAlgorithm 5 APPROX-PROB 1: inputs 2: Ordered Set S, bias 3: ˆ̃pS(1),S(1) = 0 4: for i from 2 to |S| do 5: Compare S(1) and S(i) for 16 2 log |S| 4 times\n6: ˆ̃pS(i),S(1) = [ fraction of times S(i) won − 1 2 ] 7: if ˆ̃pS(i),S(1) < ˆ̃pS(i−1),S(1) then 8: ˆ̃pS(i),S(1) = ˆ̃pS(i−1),S(1) 9: end if\n10: end for 11: for j from 2 to |S| do 12: ˆ̃pS(j),S(j) = 0 13: for k from j + 1 to |S| do 14: if ˆ̃pS(k−1),S(j) = ˆ̃pS(k),S(j−1) then 15: ˆ̃pS(k),S(j) = ˆ̃pS(k−1),S(j) 16: else 17: Compare S(j) and S(k) for 16 2 log |S| 4 times\n18: ˆ̃pS(k),S(j) = [ fraction of times S(k) won − 1 2 ] 19: end if 20: end for 21: end for\nTheorem 12 shows the correctness of APPROX-PROB and bounds its comparisons. Proof is in Appendix F.3\nTheorem 12. Given an /8-ranked ordered set S i.e., p̃S(i),S(j) ≤ /8 ∀i < j, APPROX-PROB(S, ) uses O( |S|min(|S|,1/ ) 2 log |S|) comparisons and w.p.≥ 1− 1 |S|2 approximates all pairwise probabilities to accuracy of ."
  }, {
    "heading": "8. Experiments",
    "text": "In this section, we compare the performance of our maxing algorithms with previous work on synthetic data. All results presented here are averaged over 1000 runs.\nWe compare our maxing algorithms SOFT-SEQELIM, NEAR-OPT-MAX, and OPT-MAX with SEQELIMINATE (Falahatgar et al., 2017a), KNOCKOUT (Falahatgar et al., 2017b), MallowsMPI (BusaFekete et al., 2014a), AR (Heckel et al., 2016) and BTM-PAC (Yue & Joachims, 2011). KNOCKOUT and BTM-PAC are PAC maxing algorithms for models with both SST and STI properties. SEQ-ELIMINATE is a PAC maxing algorithm for SST model. MallowsMPI, originally designed for Mallows model, finds a condorcet winner which exists under WST. AR is a maxing algorithm that finds Borda winner that is same as condorcet winner\nunder WST. In all experiments, we use maxing algorithms to find a 0.05-maximum with δ = 0.1.\nWe first consider the model pi,j = 0.6 ∀i < j same as in (Yue & Joachims, 2011; Falahatgar et al., 2017b;a) that satisfies both SST and STI properties. Note that i = 1 is the only 0.05-maximum under this model. Figure 1 presents number of comparisons used by each maxing algorithm. Observe that compared to other algorithms, BTMPAC uses too many comparisons even for n = 15. The reason might be BTM-PAC is mainly intended for reducing regret in the conventional bandits setting. The bar for BTM-PAC complexity for n = 100 is not fully shown in the figure to better scale the other complexity bars. Comparison complexity of AR is high for n = 100 mainly because AR eliminates elements based on Borda scores and Borda scores are very close to each other for large n. We drop BTM-PAC and AR henceforth.\nNow we consider a model that satisfies MST but not SST, i.e., p5i+l,5i+k = 0.6 ∀i < n/5 − 1, 1 ≤ l < k ≤ 5 and p5i+l,5j+k = 0.52 ∀i < j < n/5 − 1, 0 < l, k ≤ 5. Notice that under this model elements are divided into groups of five where within each group |p̃i,j | = 0.1 and for elements in two different groups |p̃i,j | = 0.02, hence there is a 0.05-maximum in each group. Figure 2 demonstrates comparison complexity of algorithms under this model. SEQELIMINATE uses fewer comparisons, but it fails to output a 0.05-maximum with probability 0.21 for n = 25 and 0.19 for n = 100. Hence SEQ-ELIMINATE fails once SST is not satisfied. This is because when you compare a 0.05-maximum of a group with an element in other group, 0.05-maximum can get eliminated with probability ≈ 0.5. Hence with lots of groups SEQ-ELIMINATE fails. Other algorithms find a 0.05-maximum in all runs. We drop SEQELIMINATE henceforth.\nNow we consider a model that does not satisfy STI but satisfies MST i.e., n = 10 and p1,j = 1/2 + q̃ ∀j ≤ n/2, p1,j = 1 ∀j > n/2 and pi,j = 1/2 + q̃ ∀1 < i < j, q̃ < 0.05. Under this model any i ≤ 5 is a 0.05-maximum. Figure 3 shows the average comparison complexity of algorithms under this model. KNOCKOUT uses fewer com-\nparisons, but fails to output a 0.05-maximum with probability 0.12 for q̃ = 0.001 and 0.25 for q̃ = 0.0001, hence fails to meet the confidence requirement once STI is dropped. Other algorithms find a 0.05-maximum in all runs.\nIt is interesting to note that MallowsMPI uses more comparisons as q̃ decreases, whereas the complexity of other algorithms remains almost same. This is because MallowsMPI tries to find absolute maximum which is not always practical. Further note that the performance of SOFTSEQ-ELIM is better than NEAR-OPT-MAX, and NEAROPT-MAX is better than OPT-MAX. This is because the bias gap for SOFT-SEQ-ELIM, NEAR-OPT-MAX and OPTMAX is , /2 and /4 respectively, resulting in higher constants for NEAR-OPT-MAX and OPT-MAX. While the theoretical order complexity is higher for SOFT-SEQ-ELIM, in practice it can find a good anchor quickly and seems to have near-linear order complexity."
  }, {
    "heading": "9. Conclusion",
    "text": "We studied the problem of maxing, ranking, and estimating comparison probabilities under different stochastic transitivity constraints. We showed that under WST, maxing needs quadratic comparisons. We also presented a linearcomplexity algorithm for maxing under MST. We also proposed an optimal ranking algorithm for SST models with Stochastic Triangle Inequality, closing (log log n)3 gap. For the same model, we proposed an optimal algorithm for estimating the comparison probabilities."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "We thank NSF for supporting this work through grants CIF1564355 and CIF-1619448.\nReferences http://www.gif.gf/.\nAcharya, J., Falahatgar, M., Jafarpour, A., Orlitsky, A., and Suresh, A. T. Maximum selection and sorting with adversarial comparators and an application to density estimation. arXiv preprint arXiv:1606.02786, 2016.\nAjtai, M., Feldman, V., Hassidim, A., and Nelson, J. Sorting and selection with imprecise comparisons. ACM Transactions on Algorithms (TALG), 12(2):19, 2015.\nBaltrunas, L., Makcinskas, T., and Ricci, F. Group recommendations with rank aggregation and collaborative filtering. In Proceedings of the fourth ACM conference on Recommender systems, pp. 119–126. ACM, 2010.\nBusa-Fekete, R., Hüllermeier, E., and Szörényi, B. Preference-based rank elicitation using statistical models: The case of mallows. In Proc. of the ICML, pp. 1071–1079, 2014a.\nBusa-Fekete, R., Szörényi, B., and Hüllermeier, E. Pac rank elicitation through adaptive sampling of stochastic pairwise preferences. In AAAI, 2014b.\nCaplin, A. and Nalebuff, B. Aggregation and social choice: a mean voter theorem. Econometrica: Journal of the Econometric Society, pp. 1–23, 1991.\nChatterjee, S. et al. Matrix estimation by universal singular value thresholding. The Annals of Statistics, 43(1):177– 214, 2015.\nChen, X., Bennett, P. N., Collins-Thompson, K., and Horvitz, E. Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of the sixth ACM international conference on Web search and data mining, pp. 193–202. ACM, 2013.\nDudı́k, M., Hofmann, K., Schapire, R. E., Slivkins, A., and Zoghi, M. Contextual dueling bandits. arXiv preprint arXiv:1502.06362, 2015.\nFalahatgar, M., Hao, Y., Orlitsky, A., Pichapati, V., and Ravindrakumar, V. Maxing and ranking with few assumptions. In Advances in Neural Information Processing Systems, pp. 7063–7073, 2017a.\nFalahatgar, M., Orlitsky, A., Pichapati, V., and Suresh, A. T. Maximum selection and ranking under noisy comparisons. In International Conference on Machine Learning, pp. 1088–1096, 2017b.\nFeige, U., Raghavan, P., Peleg, D., and Upfal, E. Computing with noisy information. SIAM Journal on Computing, 23(5):1001–1018, 1994.\nHeckel, R., Shah, N. B., Ramchandran, K., and Wainwright, M. J. Active ranking from pairwise comparisons and when parametric assumptions don’t help. arXiv preprint arXiv:1606.08842, 2016.\nHüllermeier, E., Fürnkranz, J., Cheng, W., and Brinker, K. Label ranking by learning pairwise preferences. Artificial Intelligence, 172(16-17):1897–1916, 2008.\nJang, M., Kim, S., Suh, C., and Oh, S. Top-k ranking from pairwise comparisons: When spectral ranking is optimal. arXiv preprint arXiv:1603.04153, 2016.\nLee, D. T., Goel, A., Aitamurto, T., and Landemore, H. Crowdsourcing for participatory democracies: Efficient elicitation of social choice functions. In Second AAAI Conference on Human Computation and Crowdsourcing, 2014.\nLuce, R. D. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005.\nMohajer, S., Suh, C., and Elmahdy, A. Active learning for top-k rank aggregation from noisy comparisons. In International Conference on Machine Learning, pp. 2488– 2497, 2017.\nNegahban, S., Oh, S., and Shah, D. Iterative ranking from pair-wise comparisons. In NIPS, pp. 2474–2482, 2012.\nNegahban, S., Oh, S., and Shah, D. Rank centrality: Ranking from pairwise comparisons. Operations Research, 2016.\nPlackett, R. L. The analysis of permutations. Applied Statistics, pp. 193–202, 1975.\nRadlinski, F. and Joachims, T. Active exploration for learning rankings from clickthrough data. In Proceedings of the 13th ACM SIGKDD, pp. 570–579. ACM, 2007.\nRadlinski, F., Kurup, M., and Joachims, T. How does clickthrough data reflect retrieval quality? In Proceedings of the 17th ACM conference on Information and knowledge management, pp. 43–52. ACM, 2008.\nRajkumar, A. and Agarwal, S. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In Proc. of the ICML, pp. 118–126, 2014.\nShah, N., Balakrishnan, S., Guntuboyina, A., and Wainwright, M. Stochastically transitive models for pairwise comparisons: Statistical and computational issues. In International Conference on Machine Learning, pp. 11– 20, 2016a.\nShah, N. B., Balakrishnan, S., and Wainwright, M. J. Feeling the bern: Adaptive estimators for bernoulli probabilities of pairwise comparisons. arXiv preprint arXiv:1603.06881, 2016b.\nSkorepa, M. Decision making: a behavioral economic approach. Palgrave Macmillan, 2010.\nSoufiani, H. A., Chen, W., Parkes, D. C., and Xia, L. Generalized method-of-moments for rank aggregation. In Advances in Neural Information Processing Systems, pp. 2706–2714, 2013.\nSzörényi, B., Busa-Fekete, R., Paul, A., and Hüllermeier, E. Online rank elicitation for plackett-luce: A dueling bandits approach. In NIPS, pp. 604–612, 2015.\nYue, Y. and Joachims, T. Beat the mean bandit. In Proc. of the ICML, pp. 241–248, 2011."
  }],
  "year": 2018,
  "references": [{
    "title": "Maximum selection and sorting with adversarial comparators and an application to density estimation",
    "authors": ["J. Acharya", "M. Falahatgar", "A. Jafarpour", "A. Orlitsky", "A.T. Suresh"],
    "venue": "arXiv preprint arXiv:1606.02786,",
    "year": 2016
  }, {
    "title": "Sorting and selection with imprecise comparisons",
    "authors": ["M. Ajtai", "V. Feldman", "A. Hassidim", "J. Nelson"],
    "venue": "ACM Transactions on Algorithms (TALG),",
    "year": 2015
  }, {
    "title": "Group recommendations with rank aggregation and collaborative filtering",
    "authors": ["L. Baltrunas", "T. Makcinskas", "F. Ricci"],
    "venue": "In Proceedings of the fourth ACM conference on Recommender systems,",
    "year": 2010
  }, {
    "title": "Preference-based rank elicitation using statistical models: The case of mallows",
    "authors": ["R. Busa-Fekete", "E. Hüllermeier", "B. Szörényi"],
    "venue": "In Proc. of the ICML,",
    "year": 2014
  }, {
    "title": "Pac rank elicitation through adaptive sampling of stochastic pairwise preferences",
    "authors": ["R. Busa-Fekete", "B. Szörényi", "E. Hüllermeier"],
    "venue": "In AAAI,",
    "year": 2014
  }, {
    "title": "Aggregation and social choice: a mean voter theorem",
    "authors": ["A. Caplin", "B. Nalebuff"],
    "venue": "Econometrica: Journal of the Econometric Society,",
    "year": 1991
  }, {
    "title": "Matrix estimation by universal singular value thresholding",
    "authors": ["S Chatterjee"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Pairwise ranking aggregation in a crowdsourced setting",
    "authors": ["X. Chen", "P.N. Bennett", "K. Collins-Thompson", "E. Horvitz"],
    "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,",
    "year": 2013
  }, {
    "title": "Maxing and ranking with few assumptions",
    "authors": ["M. Falahatgar", "Y. Hao", "A. Orlitsky", "V. Pichapati", "V. Ravindrakumar"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Maximum selection and ranking under noisy comparisons",
    "authors": ["M. Falahatgar", "A. Orlitsky", "V. Pichapati", "A.T. Suresh"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Computing with noisy information",
    "authors": ["U. Feige", "P. Raghavan", "D. Peleg", "E. Upfal"],
    "venue": "SIAM Journal on Computing,",
    "year": 1994
  }, {
    "title": "Active ranking from pairwise comparisons and when parametric assumptions don’t help",
    "authors": ["R. Heckel", "N.B. Shah", "K. Ramchandran", "M.J. Wainwright"],
    "venue": "arXiv preprint arXiv:1606.08842,",
    "year": 2016
  }, {
    "title": "Label ranking by learning pairwise preferences",
    "authors": ["E. Hüllermeier", "J. Fürnkranz", "W. Cheng", "K. Brinker"],
    "venue": "Artificial Intelligence,",
    "year": 1897
  }, {
    "title": "Top-k ranking from pairwise comparisons: When spectral ranking is optimal",
    "authors": ["M. Jang", "S. Kim", "C. Suh", "S. Oh"],
    "venue": "arXiv preprint arXiv:1603.04153,",
    "year": 2016
  }, {
    "title": "Crowdsourcing for participatory democracies: Efficient elicitation of social choice functions",
    "authors": ["D.T. Lee", "A. Goel", "T. Aitamurto", "H. Landemore"],
    "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing,",
    "year": 2014
  }, {
    "title": "Individual choice behavior: A theoretical analysis",
    "authors": ["R.D. Luce"],
    "venue": "Courier Corporation,",
    "year": 2005
  }, {
    "title": "Active learning for top-k rank aggregation from noisy comparisons",
    "authors": ["S. Mohajer", "C. Suh", "A. Elmahdy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Iterative ranking from pair-wise comparisons",
    "authors": ["S. Negahban", "S. Oh", "D. Shah"],
    "venue": "In NIPS, pp",
    "year": 2012
  }, {
    "title": "Rank centrality: Ranking from pairwise comparisons",
    "authors": ["S. Negahban", "S. Oh", "D. Shah"],
    "venue": "Operations Research,",
    "year": 2016
  }, {
    "title": "The analysis of permutations",
    "authors": ["R.L. Plackett"],
    "venue": "Applied Statistics, pp",
    "year": 1975
  }, {
    "title": "Active exploration for learning rankings from clickthrough data",
    "authors": ["F. Radlinski", "T. Joachims"],
    "venue": "In Proceedings of the 13th ACM SIGKDD,",
    "year": 2007
  }, {
    "title": "How does clickthrough data reflect retrieval quality",
    "authors": ["F. Radlinski", "M. Kurup", "T. Joachims"],
    "venue": "In Proceedings of the 17th ACM conference on Information and knowledge management,",
    "year": 2008
  }, {
    "title": "A statistical convergence perspective of algorithms for rank aggregation from pairwise data",
    "authors": ["A. Rajkumar", "S. Agarwal"],
    "venue": "In Proc. of the ICML,",
    "year": 2014
  }, {
    "title": "Stochastically transitive models for pairwise comparisons: Statistical and computational issues",
    "authors": ["N. Shah", "S. Balakrishnan", "A. Guntuboyina", "M. Wainwright"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Feeling the bern: Adaptive estimators for bernoulli probabilities of pairwise comparisons",
    "authors": ["N.B. Shah", "S. Balakrishnan", "M.J. Wainwright"],
    "venue": "arXiv preprint arXiv:1603.06881,",
    "year": 2016
  }, {
    "title": "Decision making: a behavioral economic approach",
    "authors": ["M. Skorepa"],
    "venue": "Palgrave Macmillan,",
    "year": 2010
  }, {
    "title": "Generalized method-of-moments for rank aggregation",
    "authors": ["H.A. Soufiani", "W. Chen", "D.C. Parkes", "L. Xia"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Online rank elicitation for plackett-luce: A dueling bandits approach",
    "authors": ["B. Szörényi", "R. Busa-Fekete", "A. Paul", "E. Hüllermeier"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Beat the mean bandit",
    "authors": ["Y. Yue", "T. Joachims"],
    "venue": "In Proc. of the ICML, pp",
    "year": 2011
  }],
  "id": "SP:5b3e250399f40f51c8af6bbc3a970a26c01924f3",
  "authors": [{
    "name": "Moein Falahatgar",
    "affiliations": []
  }, {
    "name": "Ayush Jain",
    "affiliations": []
  }, {
    "name": "Alon Orlitsky",
    "affiliations": []
  }, {
    "name": "Venkatadheeraj Pichapati",
    "affiliations": []
  }, {
    "name": "Vaishakh Ravindrakumar",
    "affiliations": []
  }],
  "abstractText": "We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating all pairwise preference probabilities, in the adaptive setting. With just Weak Stochastic Transitivity, we show that maxing requires Ω(n) comparisons and with slightly more restrictive Medium Stochastic Transitivity, we present a linear complexity maxing algorithm. With Strong Stochastic Transitivity and Stochastic Triangle Inequality, we derive a ranking algorithm with optimalO(n log n) complexity and an optimal algorithm that estimates all pairwise preference probabilities.",
  "title": "The Limits of Maxing, Ranking, and Preference Learning"
}