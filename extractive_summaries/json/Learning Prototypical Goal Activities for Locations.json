{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1297–1307 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1297"
  }, {
    "heading": "1 Introduction",
    "text": "Every day, people go to different places to accomplish goals. People go to stores to buy clothing, go to restaurants to eat, and go to the doctor for medical services. People travel to specific destinations to enjoy the beach, go skiing, or see historical sites. For most places, people typically go there for a common set of reasons, which we will refer to as prototypical goal activities (goal-acts) for a location. For example, a prototypical goal-act for restaurants would be “eat food” and for IKEA would be “buy furniture”.\nPrevious research has established that recognizing people’s goals is essential for narrative text understanding and story comprehension (Schank and Abelson, 1977; Wilensky, 1978; Lehnert, 1981; Elson and McKeown, 2010; Goyal et al., 2013).\nGoals and plans are essential to understand people’s behavior and we use our knowledge of prototypical goals to make inferences when reading. For example, consider the following pair of sentences: “Mary went to the supermarket. She needed milk.” Most people will infer that Mary purchased milk, unless told otherwise. But a purchase event is not explicitly mentioned. In contrast, a similar sentence pair “Mary went to the theatre. She needed milk.” feels incongruent and does not produce that inference. Recognizing goals is also critical for conversational dialogue systems. For example, if a friend tells you that they went to a restaurant, you might reply “What did you eat?”, but if a friend says that they went to Yosemite, a more appropriate response might be “Did you hike?” or “Did you see the waterfalls?”.\nOur knowledge of prototypical goal activities also helps us resolve semantic ambiguity. For example, consider the following sentences:\n(a) She went to the kitchen and got chicken. (b) She went to the supermarket and got chicken. (c) She went to the restaurant and got chicken.\nIn sentence (a), we infer that she retrieved chicken (e.g., from the refrigerator) but did not pay for it. In (b), we infer that she paid for the chicken but probably did not eat it at the supermarket. In (c), we infer that she ate the chicken at the restaurant. Note how the verb “got” maps to different presumed events depending on the location.\nOur research aims to learn the prototypical goalacts for locations using a text corpus. First, we extract activities that co-occur with locations in goaloriented syntactic patterns. Next, we construct an activity profile matrix that consists of an activity vector (profile) for each of the locations. We then apply a semi-supervised label propagation algorithm to iteratively revise the activity profile strengths based on a small set of labeled locations.\nWe also incorporate external resources to measure similarity between different activity expressions. Our results show that this semi-supervised learning approach outperforms several baseline methods in identifying the prototypical goal activities for locations."
  }, {
    "heading": "2 Related Work",
    "text": "Recognizing plans and goals is fundamental to narrative story understanding (Schank and Abelson, 1977; Bower, 1982). Conceptual knowledge structures developed in prior work have shown the importance of this type of knowledge, including plans (Wilensky, 1978), goal trees (Carbonell, 1979), and plot units (Lehnert, 1981). Wilensky’s research aimed to understand the actions of characters in stories by analyzing their goals, and their plans to accomplish those goals. For example, someone’s goal might be to obtain food with a plan to go to a restaurant. Our work aims to learn prototypical goals associated with a location, to support similar inference capabilities during story understanding.\nGoals and plans can also function to trigger scripts (Cullingford, 1978), such as the $RESTAURANT script. There has been growing interest in learning narrative event chains and script knowledge from large text corpora (e.g., (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Pichotta and Mooney, 2014, 2016)). In addition, Goyal et al. (2010; 2013) developed a system to automatically produce plot unit representations for short stories. A manual analysis of their stories revealed that 61% of Positive/Negative Affect States originated from completed plans and goals, and 46% of Mental Affect States originated from explicitly stated or inferred plans and goals.\nElson & McKeown (2010) included plans and goals in their work on creating extensive story bank annotations that capture the knowledge needed to understand narrative structure. Researchers have also begun to explore NLP methods for recognizing the goals, desires, and plans\nof characters in stories. Recent work has explored techniques to detect wishes (desires) in natural language text (Goldberg et al., 2009) and identify desire fulfillment (Chaturvedi et al., 2016; Rahimtoroghi et al., 2017).\nGraph-based semi-supervised learning has been successfully used for many tasks, including sentiment analysis (Rao and Ravichandran, 2009; Feng et al., 2013), affective event recognition (Ding and Riloff, 2016) and class-instance extraction (Talukdar and Pereira, 2010). The semi-supervised learning algorithm used in this paper is modeled after a framework developed by Zhu et al. (2003) based on harmonic energy minimization and a label propagation algorithm described in (Zhu and Ghahramani, 2002)."
  }, {
    "heading": "3 Learning Prototypical Goal Activities",
    "text": "Our aim is to learn the most prototypical goal-acts for locations. To tackle this problem, we first extract locations and related activities from a large text corpus. Then we use a semi-supervised learning method to identify the goal activities for individual locations. In the following sections we describe these processes in detail."
  }, {
    "heading": "3.1 Location and Activity Extraction",
    "text": "To collect information about locations and activities, we use the 2011 Spinn3r dataset (Burton et al., 2011). Since our interest is learning about the activities of ordinary people in their daily lives, we use the Weblog subset of the Spinn3r corpus, which contains over 133 million blog posts.\nWe use the text data to identify activities that are potential goal-acts for a location. However we also need to identify locations and want to include both proper names (e.g., Disneyland) as well as nominals (e.g., store, beach), so Named Entity Recognition will not suffice. Consequently, we extract (Loc,Act) pairs using syntactic patterns.\nFirst, we apply the Stanford dependency parser (Manning et al., 2014). We then extract sentences that match the pattern “go to X to Y ” with the\nfollowing conditions: (1) there exists a subject connecting to “go”, (2) X has an nmod (nominal modifier) relation to “go” (lemma), (3)X is a noun or noun compound, (4) Y has an xcomp relation (open clausal complement) with “go”, and (5) Y is a verb. Figure 1 depicts the intended syntactic structure, which we will informally call the “go to” pattern. For sentences that match this pattern, we extract X as a location and Y as an activity. If the verb is followed by a particle and/or noun phrase (NP), then we also include the particle and head noun of the NP. For example, we extract activities such as “pray”, “clean up”, and “buy sweater”.\nThis syntactic structure was chosen to identify activities that are described as being the reason why someone went to the location. However it is not perfect. In some cases, X is not a location (e.g., “go to great lengths to ...” yields “lengths” as a location), or Y is not a goal-act for X (e.g., “go to the office to retrieve my briefcase ...” yields “retrieve briefcase” which is not a prototypical goal for “office”). Interestingly, the pattern extracts some nominals that are not locations in a strict sense, but behave as locations. For example, “go to the doctor” extracts “doctor” as a location. Literally a doctor is a person, but in this context it really refers to the doctor’s office, which is a location. The pattern also extracts entities such as “roof”, which are not generally thought of as locations but do have a fixed physical location. Other extracted entities are virtual but function as locations, such as “Internet”. For the purposes of this work, we use the term location in a general sense to include any place or object that has a physical, virtual or implied location.\nThe “go to” pattern worked quite well at extracting (Loc,Act) pairs, but in relatively small quantities due to the very specific nature of the syntactic structure. So we tried to find additional activities for those locations. Initially, we tried harvesting activities that occurred in close proximity (within 5 words) to a known location, but the results were\ntoo noisy. Instead, we used the pattern “Y in/at X” with the same syntactic constraints for Y (the extracted activity) and X (a location extracted by the “go to” pattern).\nWe discovered many sentences in the corpus that were exactly or nearly the same, differing only by a few words, which resulted in artificially high frequency counts for some (Loc,Act) pairs. So we filtered duplicate or near-duplicate sentences by computing the longest common substring of sentence pairs that extracted the same (Loc,Act). If the shared substring had length ≥ 5, then we discarded the “duplicate” sentence.\nFinally, we applied three filters. To keep the size of the data manageable, we discarded locations and activities that were each extracted with frequency < 30 by our patterns. And we manually filtered locations that are Named Entities corresponding to cities or larger geo-political regions (e.g., provinces or countries). Large regions defined by government boundaries fall outside the scope of our task because the set of activities that typically occur in (say) a city or country is so broad. Finally, we added a filter to try to remove extremely general activities that can occur almost anywhere (e.g., visit). If an activity co-occurred with > 20% of the extracted (distinct) locations, then we discarded it.\nAfter these filters, we extracted 451 distinct locations, 5143 distinct activities, roughly 200, 000 distinct (Loc,Act) pairs, and roughly 500, 000 instances of (Loc,Act) pairs."
  }, {
    "heading": "3.2 Activity Profiles for Locations",
    "text": "We define an activity profile matrix Y of size n×m, where n is the number of distinct locations andm is the number of distinct activities. Yi,j represents the strength of the jth activity aj being a goal-act for li. We use yi ∈ Rm to denote the ith row of Y . Table 1 shows an illustration of (partial) activity profiles for four locations.1 Our goal is\n1Not actual values, for illustration only.\nto learn the Yi,j values so that activities with high strength are truly goal-acts for location li.\nWe could build the activity profile for location li using the co-occurrence data extracted from the blog corpus. For example, we could estimate P (aj | li) directly from the frequency counts of the activities extracted for li. However, a high co-occurrence frequency doesn’t necessarily mean that the activity represents a prototypical goal. For example, the activity “have appointment” frequently co-occurs with “clinic” but doesn’t reveal the underlying reason for going to the clinic (e.g., probably to see a doctor or undergo a medical test). To appreciate the distinction, imagine that you asked a friend why she went to a health clinic, and she responded with “because I had an appointment”. You would likely view her response as being snarky or evasive (i.e., she didn’t want to tell you the reason). In Section 4, we will evaluate this approach as a baseline and show that it does not perform well."
  }, {
    "heading": "3.3 Semi-Supervised Learning of Goal-Act Probabilities",
    "text": "Our aim is to learn the activity profiles for locations using a small amount of labeled data, so we frame this problem as a semi-supervised learning task. Given a small number of “seed” locations coupled with predefined goal-acts, we want to learn the goal-acts for new locations."
  }, {
    "heading": "3.3.1 Location Similarity Graph",
    "text": "We use li ∈ L to represent location li, where |L| = n. We define an undirected graph G = (V,E) with vertices representing locations (|V | = n) and edgesE = V ×V , such that each pair of vertices vi and vk is connected with an edge eik whose weight represents the similarity between li and lk.\nWe can then represent the edge weights as an n × n symmetric weight matrix W indicating the similarity between locations. There could be many ways to define the weights, but for now we use the following definition from (Zhu et al., 2003), where σ2 is a hyper-parameter2:\nWi,k = exp ( − 1 σ2 (1− sim (li, lk)) )\n(1)\nTo assess the similarity between locations, we measure the cosine similarity between vectors of their co-occurrence frequencies with activities. Specifically, let matrix Fn×m = [f1, ..., fn]T\n2We use the same value σ2 = 0.03 as (Zhu et al., 2003).\nwhere fi is a vector of length m capturing the co-occurrence frequencies between location li and each activity aj in the extracted data (i.e., Fi,j is the number of times that activity aj occurred with location li). We then define location similarity as:\nsim(li, lk) = fTi fk ‖fi‖‖fk‖\n(2)"
  }, {
    "heading": "3.3.2 Initializing Activity Profiles",
    "text": "We use semi-supervised learning with a set of “seed” locations from human annotations, and another set of locations that are unlabeled. So we subdivide the set of locations into S = {l1, ..., ls}, which are the seed locations, and U = {ls+1, ..., ls+u}, which are the unlabeled locations, such that s + u = n. For an unlabeled location li ∈ U , the initial activity profile is the normalized co-occurrence frequency vector f i.\nFor each seed location li ∈ S, we first automatically construct an activity profile vector hi based on the gold goal-acts which were obtained from human annotators as described in Section 4.1. All activities not in the gold set are assigned a value of zero. Each activity aj in the gold set is assigned a probability P (aj | li) based on the gold answers. However, the gold goal-acts may not match the activity phrases found in the corpus (see discussion in Section 4.3), so we smooth the vector created with the gold goal-acts by averaging it with the normalized co-occurrence frequency vector f i extracted from the corpus.\nThe activity profiles of seed locations stay constant through the learning process. We use y0i to denote the initial activity profiles. So when li ∈ S, y0i = (f i + hi)/2."
  }, {
    "heading": "3.3.3 Learning Goal-Act Strengths",
    "text": "We apply a learning framework developed by (Zhu et al., 2003) based on harmonic energy minimization and extend it to multiple labels. Intuitively, we assume that similar locations should share similar activity profiles,3 which motivates the following objective function over matrix Y :\nargmin Y ∑ i,k Wi,k‖yi − yk‖2,\ns.t. yi = y0i for each li ∈ S (3)\nLet D = (di) denote an n × n diagonal matrix where di = ∑n k=1Wi,k. Let’s split Y by the sth\n3This is a heuristic but is not always true.\nrow: Y = [ Ys Yu ] , then split W (similarly for D) into four blocks by the sth row and column:\nW = [ Wss Wsu Wus Wuu ] (4)\nFrom (Zhu et al., 2003), Eq (3) is given by:\nYu = (Duu −Wuu)−1WusYs (5)\nWe then use the label propagation algorithm described in (Zhu and Ghahramani, 2002) to compute Y :\nAlgorithm 1 repeat Y ← D−1WY Clamp yi = y0i for each li ∈ S\nuntil convergence"
  }, {
    "heading": "3.3.4 Activity Similarity",
    "text": "One problem with the above algorithm is that it only takes advantage of relations between vertices (i.e., locations). If there are intrinsic relations between activities, they could be exploited as a complementary source of information to benefit the learning. Intuitively, different pairs of activities share different similarities, e.g., “eat burgers” should be more similar to “have lunch” than “read books”.\nUnder this idea, similar to the previous location similarity weight matrixW , we want to define an activity similarity weight matrix Am×m where Ai,k indicates the similarity weight between activity ai and ak:\nAi,k = exp ( − 1 σ2 (1− sim (ai, ak)) )\n(6)\nwhere σ2 is the same as in Eq (1). We explore 3 different similarity functions sim(ai, ak) based on co-occurrence with locations, word matching, and embedding similarities.\nFirst, similar to Eq (2), we can use each activity’s co-occurrence frequency with all locations as its location profile and define a similarity score based on cosine values of location profile vectors:\nsimL(ai, ak) = gTi gk ‖gi‖‖gk‖\n(7)\nwhere the predefined co-occurrence frequency matrix F = [f1, ..., fn]T = [g1, ...,gm].\nAs a second option, the similarity between activities can often be implied by their lexical overlap, e.g., two activities sharing the same verb or noun might be related. For each word belonging to any of our activities, we use WordNet (Miller, 1995) to find its synonyms. We also include the word itself in the synonym set. If the synonym sets of two words overlap, we call these two words “match”. Then we define the lexical overlap similarity function between ai and ak:\nsimO(ai, ak) =  1 if verb and noun match 0.5 if verb or noun match 0 otherwise\n(8) As a third option, we can use 300-dimension word embedding vectors (Pennington et al., 2014) trained on 840 billion tokens of web data to compute semantic similarity. We compute an activity’s embedding as the average of its words’ embeddings. Let simE(ai, ak) be the cosine value between the embedding vectors of ai and ak:\nsimE(ai, ak) = cos〈Embed(ai),Embed(ak)〉 (9)\nFinally, we can plug these similarity functions into Eq (6). We use AL, AO, AE to denote the corresponding matrix. We can also plug in multiple similarity metrics such as (simL + simE)/2 and use combination symbols AL+E to denote the matrix."
  }, {
    "heading": "3.3.5 Injecting Activity Similarity",
    "text": "Once we have a similarity matrix for activities, the next question is how will it help with the activity profile computation? Recall from Eq (5), we know that the activity profile of an unlabeled location can be represented by a linear combination of other locations’ activity profiles. The activity profile matrix Y is an n ×m matrix where each row is the activity profile for a location. We can also view Y as a matrix whose each column is the location profile for an activity. Using the same idea, we can make each column approximate a linear combination of its highly related columns (i.e., the location profile of an activity will become more similar to the location profiles of its similar activities). Our expectation is that this approximation will help improve the quality of Y .\nBy being right multiplied by matrix A, Y gets updated from manipulating its columns (activities) as well. We modify the algorithm accordingly as below:\nAlgorithm 2 repeat Y ← D−1WYA Clamp yi = y0i for each li ∈ S\nuntil convergence"
  }, {
    "heading": "4 Evaluation",
    "text": ""
  }, {
    "heading": "4.1 Gold Standard Data",
    "text": "Since this is a new task and there is no existing dataset for evaluation, we use crowd-sourcing via Amazon Mechanical Turk (AMT) to acquire gold standard data. First, we released a qualification test containing 15 locations along with detailed annotation guidelines. 25 AMT workers finished our assignment, and we chose 15 of them who did the best job following our guidelines to continue. We gave the 15 qualified workers 200 new locations, consisting of 152 nominals and 48 proper names,4 randomly selected from our extracted data and set aside as test data. For each location, we asked the AMT workers to complete the following sentence:\nPeople go to LOC to VERB NOUN\nLOC was replaced by one of the 200 locations. Annotators were asked to provide an activity that is the primary reason why a person would go to that location, in the form of just a VERB or a VERB NOUN pair. Annotators also had the option to label a location as an “ERROR” if they felt that the provided term is not a location, since our location extraction was not perfect.\n4Same distribution as in the whole location set.\nOnly 10 annotators finished labeling our test cases, so we used their answers as the gold standard. We discarded 12 locations that were labeled as an “ERROR” by ≥ 3 workers.5 This resulted in a test set of 188 locations paired with 10 manually defined goal-acts for each one.\nA key question that we wanted to investigate through this manual annotation effort is to know whether people truly do associate the same prototypical goal activities with locations. To what extent do people agree when asked to list goalacts? Also, some places clearly have a smaller set of goal-acts than others. For example, the primary reason to go to an airport is to catch a flight, but there’s a larger set of common reasons why people go to Yosemite (e.g.,“hiking camping”, “rock climbing”, “see waterfalls”, etc.).\nComplicating matters, the AMT workers often described the same activity with different words (e.g., “buy book” vs. “purchase book”). Automatically recognizing synonymous event phrases is a difficult NLP problem in its own right.6 So solely for the purpose of analysis, we manually merged activities that have a nearly identical meaning. We were extremely conservative and did not merge similar or related phrases that were not synonymous because the granularity of terms may matter for this task (e.g., we did not merge “eat burger” and “eat lunch” because one may apply to a specific location while the other does not).\nFigure 2 shows the results of our analysis. Only 1 location was assigned exactly the same goal-act by all 10 annotators. But at least half (5) of the annotators listed the same goal-act for 40% of the locations. And nearly 80% of locations had one or more goal-acts listed by ≥ 3 people. These results show that people often do share the same associations between prototypical goal-acts and locations. These results are also very conservative because many different answers were also similar (e.g. “eat burger”, “eat meal”).\nIn Table 2 we show examples of locations and the goal-acts listed for them by the human annotators. If multiple people gave the same answer, we show the number in parentheses. For example, given the location “Toys R Us”, 9 people listed “buy toys” as a goal-act and 1 person listed “browse gifts”. We see from Table 2 that\n5We found that the workers rarely used the “ERROR” label, so setting this threshold to be 3 was a strong signal.\n6We tried using WordNet synsets to conflate phrases, but it didn’t help much.\nsome locations yield very similar sets of goal-acts (e.g., sink, airport, bookstore), while other locations show more diversity (e.g., lake, chiropractor, Chinatown)."
  }, {
    "heading": "4.2 Baselines",
    "text": "To assess the difficulty of this NLP task, we created 3 baseline systems for comparison with our learning approach. All of these methods take the list of activities that co-occurred with a location li in our extracted data and rank them.\nThe first baseline, FREQ, ranks the activities based on the co-occurrence frequency Fi,j between li and aj in our patterns. The second baseline, PMI, ranks the activities using point-wise mutual information. The third baseline, EMBED, ranks the activities based on the cosine similarity of the semantic embedding vectors for li and aj . We use GloVe (Pennington et al., 2014) 300- dimension embedding vectors pre-trained on 840 billion tokens of web data. For locations and activities with multiple words, we create an embedding by averaging the vectors of their constituent words."
  }, {
    "heading": "4.3 Matching Activities",
    "text": "The gold standard contains a set of goal-acts for each location. Since the same activity can be expressed with many different phrases, the only way to truly know whether two phrases refer to the same activity is manual evaluation, which is expensive. Furthermore, many activities are very similar or highly related, but not exactly the same. For example, “eat burger” and “eat food” both describe eating activities, but the latter is more general than the former. Considering them to be the same is not always warranted (e.g., “eat\nburger” is a logical goal-act for McDonald’s but not for Baskin-Robbins which primarily sells ice cream). As another example, “buy chicken” and “eat chicken” refer to different events (buying and eating) so they are clearly not the same semantically. But at a place like KFC, buying chicken implies eating chicken, and vice versa, so they seem like equally good answers as goal-acts for KFC. Due to the complexities of determining which gold standard answers belong in equivalence classes, we considered all of the goal-acts provided by the human annotators to be acceptable answers.\nTo determine whether an activity aj produced by our system matches any of the gold goal-acts for a location li, we report results using two types of matching criteria. Exact Match judges aj to be a correct answer for li if (1) it exactly matches (after lemmatization) any activity in li’s gold set, or (2) aj’s verb and noun both appear in li’s gold set, though possibly in different phrases. For example, if a gold set contains “buy novels” and “browse books”, then “buy books” will be a match.\nSince Exact Match is very conservative, we also define a Partial Match criterion to give 50% credit for answers that partially overlap with a gold answer. An activity aj is a partial match for li if either its verb or noun matches any of the activities in li’s gold set of goal-acts. For example, “buy burger” would be a partial match with “buy food” because their verbs match."
  }, {
    "heading": "4.4 Evaluation Metrics",
    "text": "All of our methods produce a ranked list of hypothesized goal-acts for a location. So we use Mean Reciprocal Rank (MRR) to judge the quality of the top 10 activities in each ranked list. We report two types of MRR scores.\nMRR based on the Exact Match criteria (MRRE) is computed as follows, where n is the\nnumber of locations in the test set:\nMRRE = 1\nn n∑ i=1\n1\nrank of 1st Exact Match (10)\nWe also compute MRR using both the Exact Match and Partial Match criteria. First, we need to identify the “best” answer among the 10 activities in the ranked list, which depends both on each activity’s ranking and its matching score. The matching score for activity aj is defined as:\nscore(aj) =  1 if aj is an Exact Match 0.5 if aj is a Partial Match 0 otherwise\nGiven 10 ranked activities a1 ... a10 for li, we then compute:\nbest score(li) = max j=1..10 score(aj) rank(aj)\nAnd then finally define MRRP as follows:\nMRRP = 1\nn n∑ i=1 best score(li) (11)"
  }, {
    "heading": "4.5 Experimental Results",
    "text": "Unless otherwise noted, all of our experiments report results using 4-fold cross-validation on the 200 locations in our test set. We used 4 folds to ensure 50 seed locations for each run (i.e., 1 fold for training and 3 folds for testing).\nThe first two columns of Table 3 show the MRR results under Exact Match and Partial Match conditions. The first 3 rows show the results for the baseline systems, and the remaining rows show results for our Activity Profile (AP) semi-supervised learning method. We show results for 5 variations of the algorithm: AP uses Algorithm 1, and the others use Algorithm 2 with different Activity Similarity measures: AP+AL (location profile similarity), AP+AO (overlap similarity), AP+AE (embedding similarity), and AP+AL+E (location profiles plus embeddings).\nTable 3 shows that our AP algorithm outperforms all 3 baseline methods. When adding Activity Similarity into the algorithm, we find that AL slightly improves performance, butAO andAE do not. However, we also tried combining them and obtained improved results by usingAL andAE together, yielding an MRRP score of 0.42.\nTo gain more insight about the behavior of the models, Table 3 also shows results for the topranked 1, 2, and 3 answers. For these experiments, the system gets full credit if any of its top k answers exactly matches the gold standard, or 50% credit if a partial match is among its top k answers. These results show that our AP method produces more correct answers at the top of the list than the baseline methods.\nTable 4 shows six locations with their gold answers and the Top 3 goal-acts hypothesized by our best AP system and the PMI and FREQ baselines. The activities in boldface were deemed correct (including Partial Match). For “bookstore” and “pharmacy”, all of the methods perform well. Note the challenge of recognizing that different phrases mean essentially the same thing (e.g., “fill prescription”, “pick up prescription”, “find medicine”). For “university” and “Meijer”, the AP method produces more appropriate answers than the baseline methods. For “market” and “phone”, all three methods struggle to produce good answers. Since “market” is polysemous, we see activities related to both stores and financial markets. And “phone” arguably is not a location at all, but most human annotators treated it as a virtual location, listing goal-acts related to telephones. However our algorithm considered phones to be similar to computers, which makes sense for today’s smartphones. In general, we also observed that Internet sites behave as virtual locations in language (e.g., “I went to YouTube...”)."
  }, {
    "heading": "4.6 Discussion",
    "text": "The goal-acts learned by our system were extracted from the Spinn3r dataset, while the gold standard answers were provided by human annotators, so the same (or very similar) activities are often expressed in different ways (see Section 4.3). This raises the question: what is the upper bound on system performance when evaluating against human-provided goal-acts? To answer this, we compared all of the activities that co-occurred with each location in the corpus against its gold goalacts. Only 36% of locations had at least one gold goal-act among its extracted activities when matching identical strings (after lemmatization). Because of this issue, our Exact Match criteria also allowed for combining verbs and nouns from different gold answers. Under this Exact Match criteria, 73% of locations had at least one gold goal-act\namong the extracted activities, so this represents an upper bound on performance using this metric. Under the Partial Match criteria, 98% of locations had at least one gold goal-act among the extracted activities, but only 50% credit was awarded for these cases so the maximum score possible would be ∼86%.\nWe also manually inspected 200 gold locations to analyze their types. We discovered some related groups, but substantial diversity overall. The largest group contains ∼20% of the locations, which are many kinds of stores (e.g., Ikea, WalMart, Apple store, shoe store). Even within a group, different locations often have quite different sets of co-occurring activities. In fact, we discovered some spelling variants (e.g., “WalMart” and “wal mart”), but they also have substantially different activity vectors (e.g., because one spelling is much more frequent), so the model learns about them independently.8 Other groups include restaurants (∼5%), home-related (e.g., bathroom) (∼5%), education (∼5%), virtual (e.g., Wikipedia) (∼3%), medical (∼3%) and landscape (e.g., hill) (∼3%). It is worth noting that our locations were extracted by two syntactic patterns and it remains to be seen if this has brought in any bias — detecting location nouns (especially nominals)\n7A lemmatization error for the verb “enrolled”. 8Of course normalizing location names beforehand may\nbe beneficial in future work.\nis a challenging problem in its own right."
  }, {
    "heading": "5 Conclusions and Future Work",
    "text": "We introduced the problem of learning prototypical goal activities for locations. We obtained human annotations and showed that people do associate prototypical goal-acts with locations. We then created an activity profile framework and applied a semi-supervised label propagation algorithm to iteratively update the activity strengths for locations. We demonstrated that our learning algorithm identifies goal-acts for locations more accurately than several baseline methods.\nHowever, this problem is far from solved. Challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora. Nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research. In future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts."
  }, {
    "heading": "Acknowledgments",
    "text": "We are grateful to Haibo Ding for valuable comments on preliminary versions of this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Plans and Goals in Understanding Episodes",
    "authors": ["Gordon H Bower."],
    "venue": "Advances in Psychology, 8:2–",
    "year": 1982
  }, {
    "title": "The ICWSM 2011 Spinn3r Dataset",
    "authors": ["K. Burton", "N. Kasch", "I. Soboroff."],
    "venue": "Proceedings of the Fifth Annual Conference on Weblogs and Social Media (ICWSM-2011).",
    "year": 2011
  }, {
    "title": "Subjective Understanding: Computer Models of Belief Systems",
    "authors": ["J.G. Carbonell."],
    "venue": "Ph.D. thesis, Yale University.",
    "year": 1979
  }, {
    "title": "Unsupervised Learning of Narrative Event Chains",
    "authors": ["Nathanael Chambers", "Dan Jurafsky."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT-2008).",
    "year": 2008
  }, {
    "title": "Unsupervised Learning of Narrative Schemas and Their Participants",
    "authors": ["Nathanael Chambers", "Dan Jurafsky."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-",
    "year": 2009
  }, {
    "title": "Ask, and Shall You Receive? Understanding Desire Fulfillment in Natural Language Text",
    "authors": ["Snigdha Chaturvedi", "Dan Goldwasser", "Hal Daumé III."],
    "venue": "Processings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016).",
    "year": 2016
  }, {
    "title": "Script Application: Computer Understanding of Newspaper Stories",
    "authors": ["Richard Edward Cullingford."],
    "venue": "Ph.D. thesis, Yale University.",
    "year": 1978
  }, {
    "title": "Acquiring Knowledge of Affective Events from Blogs using Label Propagation",
    "authors": ["Haibo Ding", "Ellen Riloff."],
    "venue": "Processings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016).",
    "year": 2016
  }, {
    "title": "Building a Bank of Semantically Encoded Narratives",
    "authors": ["David Elson", "Kathleen McKeown."],
    "venue": "Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC-2010).",
    "year": 2010
  }, {
    "title": "Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning",
    "authors": ["Song Feng", "Jun Seok Kang", "Polina Kuznetsova", "Yejin Choi."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-2013).",
    "year": 2013
  }, {
    "title": "May all your wishes come true: A study of wishes and how to recognize them",
    "authors": ["Andrew B Goldberg", "Nathanael Fillmore", "David Andrzejewski", "Zhiting Xu", "Bryan Gibson", "Xiaojin Zhu."],
    "venue": "Proceedings of Human Language Technologies: The 2009",
    "year": 2009
  }, {
    "title": "Automatically producing plot unit representations",
    "authors": ["Amit Goyal", "Ellen Riloff", "Hal Daumé III"],
    "year": 2010
  }, {
    "title": "A Computational Model for Plot Units",
    "authors": ["Amit Goyal", "Ellen Riloff", "Hal Daumé III."],
    "venue": "Computational Intelligence, 29(3):466–488.",
    "year": 2013
  }, {
    "title": "Skip n-grams and ranking functions for predicting script events",
    "authors": ["Bram Jans", "Steven Bethard", "Ivan Vulić", "Marie Francine Moens."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational",
    "year": 2012
  }, {
    "title": "Plot Units and Narrative Summarization",
    "authors": ["Wendy G Lehnert."],
    "venue": "Cognitive Science, 5(4):293–331.",
    "year": 1981
  }, {
    "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Com-",
    "year": 2014
  }, {
    "title": "WordNet: A Lexical Database for English",
    "authors": ["George A Miller."],
    "venue": "Communications of the ACM, 38(11):39–41.",
    "year": 1995
  }, {
    "title": "GloVe: Global Vectors for Word Representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP-2014).",
    "year": 2014
  }, {
    "title": "Statistical script learning with multi-argument events",
    "authors": ["Karl Pichotta", "Raymond Mooney."],
    "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2014).",
    "year": 2014
  }, {
    "title": "Learning Statistical Scripts with LSTM Recurrent Neural Networks",
    "authors": ["Karl Pichotta", "Raymond J Mooney."],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016).",
    "year": 2016
  }, {
    "title": "Modelling Protagonist Goals and Desires in First-Person Narrative",
    "authors": ["Elahe Rahimtoroghi", "Jiaqi Wu", "Ruimin Wang", "Pranav Anand", "Marilyn Walker."],
    "venue": "Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    "year": 2017
  }, {
    "title": "Semisupervised Polarity Lexicon Induction",
    "authors": ["Delip Rao", "Deepak Ravichandran."],
    "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2009).",
    "year": 2009
  }, {
    "title": "Scripts, Plans, Goals and Understanding",
    "authors": ["Roger C Schank", "Robert Abelson."],
    "venue": "Lawrence Erlbaum.",
    "year": 1977
  }, {
    "title": "Experiments in Graph-based Semi-supervised Learning Methods for Class-instance Acquisition",
    "authors": ["Partha Pratim Talukdar", "Fernando Pereira."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    "year": 2010
  }, {
    "title": "Understanding Goal-based Stories",
    "authors": ["Robert Wilensky."],
    "venue": "Ph.D. thesis, Yale University.",
    "year": 1978
  }, {
    "title": "Learning from Labeled and Unlabeled Data with Label Propagation",
    "authors": ["Xiaojin Zhu", "Zoubin Ghahramani."],
    "venue": "Technical report, Carnegie Mellon University.",
    "year": 2002
  }, {
    "title": "Semi-supervised Learning Using Gaussian Fields and Harmonic Functions",
    "authors": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D Lafferty."],
    "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML-2003).",
    "year": 2003
  }],
  "id": "SP:c1c81b49d56f837afdb15098f1cc722a8b0d1655",
  "authors": [{
    "name": "Tianyu Jiang",
    "affiliations": []
  }, {
    "name": "Ellen Riloff",
    "affiliations": []
  }],
  "abstractText": "People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.",
  "title": "Learning Prototypical Goal Activities for Locations"
}