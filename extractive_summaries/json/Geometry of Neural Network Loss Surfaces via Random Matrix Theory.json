{
  "sections": [{
    "text": "2."
  }, {
    "heading": "1. Introduction",
    "text": "Neural networks have witnessed a resurgence in recent years, with a smorgasbord of architectures and configurations designed to accomplish ever more impressive tasks. Yet for all the successes won with these methods, we have managed only a rudimentary understanding of why and in what contexts they work well. One difficulty in extending our understanding stems from the fact that the neural network objectives are generically non-convex functions in high-dimensional parameter spaces, and understanding their loss surfaces is a challenging task. Nevertheless, an improved understanding of the loss surface could have a large impact on optimization (Saxe et al.; Dauphin et al.,\n1Google Brain. Correspondence to: Jeffrey Pennington<jpennin@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n2014; Choromanska et al., 2015; Neyshabur et al., 2015), architecture design, and generalization (Keskar et al.)."
  }, {
    "heading": "1.1. Related work",
    "text": "There is no shortage of prior work focused on the loss surfaces of neural networks. Choromanska et al. (2015) and Dauphin et al. (2014) highlighted the prevalence of saddle points as dominant critical points that plague optimization, as well as the existence of many local minima at low loss values. Dauphin et al. (2014) studied the distribution of critical points as a function of the loss value empirically and found a trend which is qualitatively similar to predictions for random Gaussian landscapes (Bray & Dean, 2007). Choromanska et al. (2015) argued that the loss function is well-approximated by a spin-glass model studied in statistical physics, thereby predicting the existence of local minima at low loss values and saddle points at high loss values as the network increases in size. Goodfellow et al. observed that loss surfaces arising in practice tend to be smooth and seemingly convex along lowdimensional slices. Subsequent works (Kawaguchi, 2016; Safran & Shamir, 2016; Freeman & Bruna, 2016) have furthered these and related ideas empirically or analytically, but it remains safe to say that we have a long way to go before a full understanding is achieved."
  }, {
    "heading": "1.2. Our contributions",
    "text": "One shortcoming of prior theoretical results is that they are often derived in contexts far removed from practical neural network settings – for example, some work relies on results for generic random landscapes unrelated to neural networks, and other work draws on rather tenuous connections to spin-glass models. While there is a lot to be gained from this type of analysis, it leaves open the possibility that characteristics of loss surfaces specific to neural networks may be lost in the more general setting. In this paper, we focus narrowly on the setting of neural network loss surfaces and propose an analytical framework for studying the spectral density of the Hessian matrix in this context.\nOur bottom-up construction assembles an approximation of the Hessian in terms of blocks derived from the weights, the data, and the error signals, all of which we assume to\nbe random variables1. From this viewpoint, the Hessian may be understood as a structured random matrix and we study its eigenvalues in the context of random matrix theory, using tools from free probability. We focus on singlehidden-layer networks, but in principle the framework can accommodate any network architecture. After establishing our methodology, we compute approximations to the Hessian at several levels of refinement. One result is a prediction that for critical points of small index, the index scales like the energy to the 3/2 power."
  }, {
    "heading": "2. Preliminaries",
    "text": "Let W (1) ∈ Rn1×n0 and W (2) ∈ Rn2×n1 be weight matrices of a single-hidden-layer network without biases. Denote by x ∈ Rn0×m the input data and by y ∈ Rn2×m the targets. We will write z(1) =W (1)x for the pre-activations. We use [·]+ = max(·, 0) to denote the ReLU activation, which will be our primary focus. The network output is\nŷiµ = n1∑ k=1 W (2) ik [z (1) kµ ]+ , (1)\nand the residuals are eiµ = ŷiµ−yiµ. We use Latin indices for features, hidden units, and outputs, and µ to index examples. We consider mean squared error, so that the loss (or energy) can be written as,\nL = n2 = 1\n2m n2,m∑ i,µ=1 e2iµ . (2)\nThe Hessian matrix is the matrix of second derivatives of the loss with respect to the parameters, i.e. Hαβ = ∂\n2L ∂θα∂θβ ,\nwhere θα ∈ {W (1),W (2)}. It decomposes into two pieces, H = H0 + H1, where H0 is positive semi-definite and where H1 comes from second derivatives and contains all of the explicit dependence on the residuals. Specifically,\n[H0]αβ ≡ 1\nm n2,m∑ i,µ=1 ∂ŷiµ ∂θα ∂ŷiµ ∂θβ ≡ 1 m [JJT ]αβ , (3)\nwhere we have introduced the Jacobian matrix, J , and,\n[H1]αβ ≡ 1\nm n2,m∑ i,µ=1 eiµ ( ∂2ŷiµ ∂θα∂θβ ) . (4)\nWe will almost exclusively consider square networks with n ≡ n0 = n1 = n2. We are interested in the limit of large networks and datasets, and in practice they are typically of the same order of magnitude. A useful charac-\n1Although we additionally assume the random variables are independent, our framework does not explicitly require this assumption, and in principle it could be relaxed in exchange for more technical computations.\nterization of the network capacity is the ratio of the number of parameters to the effective number of examples2, φ ≡ 2n2/mn = 2n/m. As we will see, φ is a critical parameter which governs the shape of the distribution. For instance, eqn. (3) shows that H0 has the form of a covariance matrix computed from the Jacobian, with φ governing the rank of the result.\nWe will be making a variety of assumptions throughout this work. We distinguish primary assumptions, which we use to establish the foundations of our analytical framework, from secondary assumptions, which we invoke to simplify computations. To begin with, we establish our primary assumptions:\n1. The matrices H0 and H1 are freely independent, a property we discuss in sec. 3.\n2. The residuals are i.i.d. normal random variables with tunable variance governed by , eiµ ∼ N (0, 2 ). This assumption allows the gradient to vanish in the large m limit, specifying our analysis to critical points.\n3. The data features are i.i.d. normal random variables.\n4. The weights are i.i.d. normal random variables.\nWe note that normality may not be strictly necessary. We will discuss these assumptions and their validity in sec. 5."
  }, {
    "heading": "3. Primer on Random Matrix Theory",
    "text": "In this section we highlight a selection of results from the theory of random matrices that will be useful for our analysis. We only aim to convey the main ideas and do not attempt a rigorous exposition. For a more thorough introduction, see, e.g., (Tao, 2012)."
  }, {
    "heading": "3.1. Random matrix ensembles",
    "text": "The theory of random matrices is concerned with properties of matricesM whose entriesMij are random variables. The degree of independence and the manner in which the Mij are distributed determine the type of random matrix ensemble to which M belongs. Here we are interested primarily in two ensembles: the real Wigner ensemble for which M is symmetric but otherwise the Mij are i.i.d.; and the real Wishart ensemble for which M = XXT where Xij are i.i.d.. Moreover, we will restrict our attention to studying the limiting spectral density of M. For a random matrix Mn ∈ Rn×n, the empirical spectral density is defined as,\nρMn(λ) = 1\nn n∑ j=1 δ(λ− λj(Mn)) , (5)\n2In our context of squared error, each of the n2 targets may be considered an effective example.\nwhere the λj(Mn), j = 1, . . . , n, denote the n eigenvalues of Mn, including multiplicity, and δ(z) is the Dirac delta function centered at z. The limiting spectral density is defined as the limit of eqn. (5) as n→∞, if it exists.\nFor a matrix M of the real Wigner matrix ensemble whose entries Mij ∼ N (0, σ2), Wigner (1955) computed its limiting spectral density and found the semi-circle law,\nρSC(λ;σ, φ) =\n{ 1\n2πσ2\n√ 4σ2 − λ2 if |λ| ≤ 2σ\n0 otherwise . (6)\nSimilarly, if M = XXT is a real Wishart matrix with X ∈ Rn×p and Xij ∼ N (0, σ2/p), then the limiting spectral density can be shown to be the Marchenko-Pastur distribution (Marčenko & Pastur, 1967),\nρMP(λ;σ, φ) = { ρ(λ) if φ < 1 (1− φ−1)δ(λ) + ρ(λ) otherwise ,\n(7) where φ = n/p and,\nρ(λ) = 1\n2πλσφ\n√ (λ− λ−)(λ+ − λ)\nλ± = σ(1± √ φ)2 .\n(8)\nThe Wishart matrixM is low rank if φ > 1, which explains the delta function density at 0 in that case. Notice that there is an eigenvalue gap equal to λ−, which depends on φ."
  }, {
    "heading": "3.2. Free probability theory",
    "text": "Suppose A and B are two random matrices. Under what conditions can we use information about their individual spectra to deduce the spectrum of A + B? One way of analyzing this question is with free probability theory, and the answer turns out to be exactly when A and B are freely independent. Two matrices are freely independent if\nEf1(A)g1(B) · · · fk(A)gk(B) = 0 (9) whenever fi and gi are such that\nEfi(A) = 0 , Egi(B) = 0 . (10)\nNotice that when k = 1, this is equivalent to the definition of classical independence. Intuitively, the eigenspaces of two freely independent matrices are in “generic position” (Speicher, 2009), i.e. they are not aligned in any special way. Before we can describe how to compute the spectrum of A+ B, we must first introduce two new concepts, the Stieltjes transform and theR transform."
  }, {
    "heading": "3.2.1. THE STIELTJES TRANSFORM",
    "text": "For z ∈ C \\ R the Stieltjes transform G of a probability distribution ρ is defined as,\nG(z) = ∫ R ρ(t) z − t dt , (11)\nfrom which ρ can be recovered using the inversion formula,\nρ(λ) = − 1 π lim →0+ ImG(λ+ i ) . (12)"
  }, {
    "heading": "3.2.2. THE R TRANSFORM",
    "text": "Given the Stieltjes transform G of a probability distribution ρ, the R transform is defined as the solution to the functional equation,\nR ( G(z) ) + 1\nG(z) = z . (13)\nThe benefit of theR transform is that it linearizes free convolution, in the sense that,\nRA+B = RA +RB , (14) if A and B are freely independent. It plays a role in free probability analogous to that of the log of the Fourier transform in commutative probability theory.\nThe prescription for computing the spectrum of A + B is as follows: 1) Compute the Stieltjes transforms of ρA and ρB ; 2) From the Stieltjes transforms, deduce the R transforms RA and RB ; 3) From RA+B = RA + RB , compute the Stieltjes transform GA+B ; and 4) Invert the Stieltjes transform to obtain ρA+B ."
  }, {
    "heading": "4. Warm Up: Wishart plus Wigner",
    "text": "Having established some basic tools from random matrix theory, let us now turn to applying them to computing the limiting spectral density of the Hessian of a neural network at critical points. Recall from above that we can decompose the Hessian into two parts, H = H0 + H1 and that H0 = JJ\nT /m. Let us make the secondary assumption that at critical points, the elements of J and H1 are i.i.d. normal random variables. In this case, we may take H0 to be a real Wishart matrix andH1 to be a real Wigner matrix. We acknowledge that these are strong requirements, and we will discuss the validity of these and other assumptions in sec. 5."
  }, {
    "heading": "4.1. Spectral distribution",
    "text": "We now turn our attention to the spectral distribution of H and how that distribution evolves as the energy changes. For this purpose, only the relative scaling between H0 and H1 is relevant and we may for simplicity take σH0 = 1 and σH1 = √ 2 . Then we have,\nρH0(λ) = ρMP(λ; 1, φ), ρH1(λ) = ρSC(λ; √ 2 , φ) ,\n(15) which can be integrated using eqn. (11) to obtain GH0 and GH1 . Solving eqn. (13) for theR transforms then gives,\nRH0(z) = 1\n1− zφ , RH1(z) = 2 z . (16)\nWe proceed by computing theR transform of H ,\nRH = RH0 +RH1 = 1\n1− zφ + 2 z , (17)\nso that, using eqn. (13), we find that its Stieltjes transform GH solves the following cubic equation,\n2 φG3H − (2 + zφ)G2H +(z+φ− 1)GH − 1 = 0 . (18)\nThe correct root of this equation is determined by the asymptotic behavior GH ∼ 1/z as z → ∞ (Tao, 2012). From this root, the spectral density can be derived through the Stieltjes inversion formula, eqn. (12). The result is illustrated in fig. 1. For small , the spectral density resembles the Marchenko-Pastur distribution, and for small enough φ there is an eigenvalue gap. As increases past a critical value c, the eigenvalue gap disappears and negative eigenvalues begin to appear. As gets large, the spectrum becomes semicircular."
  }, {
    "heading": "4.2. Normalized index",
    "text": "Because it measures the number of descent directions3, one quantity that is of importance in optimization and in the analysis of critical points is the fraction of negative eigenvalues of the Hessian, or the index of a critical point, α. Prior work (Dauphin et al., 2014; Choromanska et al., 2015) has observed that the index of critical points typically grows rapidly with energy, so that critical points with many descent directions have large loss values. The precise form of this relationship is important for characterizing the geometry of loss surfaces, and in our framework it can be readily computed from the spectral density via integration,\nα( , φ) ≡ ∫ 0 −∞ ρ(λ; , φ) dλ = 1− ∫ ∞ 0 ρ(λ; , φ) dλ .\n(19) Given that the spectral density is defined implicitly through equation eqn. (18), performing this integration analytically\n3Here we ignore degenerate critical points for simplicity.\nis nontrivial. We discuss a method for doing so in the supplementary material. The full result is too long to present here, but we find that for small α,\nα( , φ) ≈ α0(φ) ∣∣∣∣ − c c ∣∣∣∣3/2 , (20) where the critical value of ,\nc = 1\n16 (1− 20φ− 8φ2 + (1 + 8φ)3/2) , (21)\nis the value of the energy below which all critical points are minimizers. We note that c can be found in a simpler way: it is the value of below which eqn. (18) has only real roots at z = 0, i.e. it is the value of for which the discriminant of the polynomial in eqn. (18) vanishes at z = 0. Also, we observe that c vanishes cubically as φ approaches 1,\nc ≈ 2\n27 (1− φ)3 +O(1− φ)4 . (22)\nThe 3/2 scaling in eqn. (20) is the same behavior that was found in (Bray & Dean, 2007) in the context of the field theory of Gaussian random functions. As we will see later, the 3/2 scaling persists in a more refined version of this calculation and in numerical simulations."
  }, {
    "heading": "5. Analysis of Assumptions",
    "text": ""
  }, {
    "heading": "5.1. Universality",
    "text": "There is a wealth of literature establishing that many properties of large random matrices do not depend on the details of how their entries are distributed, i.e. many results are universal. For instance, Tao & Vu (2012) show that the spectrum of Wishart matrices asymptotically approaches the Marcenko-Pastur law regardless of the distribution of the individual entries, so long as they are independent, have mean zero, unit variance, and finite k-th moment for k > 2. Analogous results can be found for Wigner matrices (Aggarwal). Although we are unaware of any existing analy-\nses relevant for the specific matrices studied here, we believe that our conclusions are likely to exhibit some universal properties – for example, as in the Wishart and Wigner cases, normality is probably not necessary.\nOn the other hand, most universality results and the tools we are using from random matrix theory are only exact in the limit of infinite matrix size. Finite-size corrections are actually largest for small matrices, which (counterintuitively) means that the most conservative setting in which to test our results is for small networks. So we will investigate our assumptions in this regime. We expect agreement with theory only to improve for larger systems."
  }, {
    "heading": "5.2. Primary assumptions",
    "text": "In sec. 2 we introduced a number of assumptions we dubbed primary and we now discuss their validity."
  }, {
    "heading": "5.2.1. FREE INDEPENDENCE OF H0 AND H1",
    "text": "Our use of free probability relies on the free independence of H0 and H1. Generically we may expect some alignment between the eigenspaces of H0 and H1 so that free independence is violated; however, we find that this violation is often quite small in practice. To perform this analysis, it suffices to examine the discrepancy between the distribution of H0 +H1 and that of H0 +QH1QT , where Q is an orthogonal random matrix of Haar measure (Chen et al., 2016). Fig. 2 show minimal discrepancy for a network trained on autoencoding task; see the supplementary material for further details and additional experiments. More precise methods for quantifying partial freeness exist (Chen et al., 2016), but we leave this analysis for future work."
  }, {
    "heading": "5.2.2. RESIDUALS ARE I.I.D. RANDOM NORMAL",
    "text": "First, we note that eiµ ∼ N (0, 2 ) is consistent with the definition of the energy in eqn. (2). Furthermore, because the gradient of the loss is proportional to the residuals, it vanishes in expectation (i.e. asm→∞), which specializes our analysis to critical points. So this assumptions seems necessary for our analysis. It is also consistent with the priors leading to the choice of the squared error loss function. Altogether we believe this assumption is fairly mild."
  }, {
    "heading": "5.2.3. DATA ARE I.I.D. RANDOM NORMAL",
    "text": "This assumption is almost never strictly satisfied, but it is approximately enforced by common preprocessing methods, such as whitening and random projections."
  }, {
    "heading": "5.2.4. WEIGHTS ARE I.I.D. RANDOM NORMAL",
    "text": "Although the i.i.d. assumption is clearly violated for a network that has learned any useful information, the weight distributions of trained networks often appear random, and sometimes appear normal (see, e.g., fig. S1 of the supplementary material)."
  }, {
    "heading": "5.3. Secondary assumptions",
    "text": "In sec. 4 we introduced two assumptions we dubbed secondary, and we discuss their validity here."
  }, {
    "heading": "5.3.1. J AND H1 ARE I.I.D. RANDOM NORMAL",
    "text": "Given that J andH1 are constructed from the residuals, the data, and the weights – variables that we assume are i.i.d. random normal – it is not unreasonable to suppose that their entries satisfy the universality conditions mentioned above. In fact, if this were the case, it would go a long way to validate the approximations made in sec. 4. As it turns out, the situation is more complicated: both J and H1 have substructure which violates the independence requirement for the universality results to apply. We must understand and take into account this substructure if we wish to improve our approximation further. We examine one way of doing so in the next section."
  }, {
    "heading": "6. Beyond the Simple Case",
    "text": "In sec. 4 we illustrated our techniques by examining the simple case of Wishart plus Wigner matrices. This analysis shed light on several qualitative properties of the spectrum of the Hessian matrix, but, as discussed in the previous section, some of the assumptions were unrealistic. We believe that it is possible to relax many of these assumptions to produce results more representative of practical networks. In this section, we take one step in that direction and focus on the specific case of a single-hidden-layer ReLU network."
  }, {
    "heading": "6.1. Product Wishart distribution and H1",
    "text": "In the single-hidden-layer ReLU case, we can write H1 as,\nH1 =\n( 0 H12\nH12 T 0\n) ,\nwhere its off-diagonal blocks are given by the matrix H12,\n[H12]ab;cd = 1\nm ∑ iµ eiµ ∂ ∂W (2) cd ∂ŷiµ ∂W (1) ab\n= 1\nm m∑ µ=1 ecµδadθ(z (1) aµ )xbµ .\n(23)\nHere it is understood that the matrix H12 is obtained from the tensor [H12]ab;cd by independently vectorizing the first two and last two dimensions, δad is the Kronecker delta function, and θ is the Heaviside theta function. As discussed above, we take the error to be normally distributed,\necµ = (∑\nj\nW (2) cj [z (1) jµ ]+ − ycµ(x)\n) ∼ N (0, 2 ) . (24)\nWe are interested in the situation in which the layer width n and the number of examples m are large, in which case θ(z (1) aµ ) can be interpreted as a mask that eliminates half of the terms in the sum over µ. If we reorder the indices so that the surviving ones appear first, we can write,\nH12 ≈ 1\nm δad m/2∑ µ̂=1 ecµ̂xbµ̂ = 1 m In ⊗ êx̂T ,\nwhere we have written ê and x̂ to denote the n×m2 matrices derived from e and x by removing the vanishing half of their columns.\nOwing to the block off-diagonal structure of H1, the squares of its eigenvalues are determined by the eigenvalues of the product of the blocks,\nH12H12 T =\n1\nm2 In ⊗ (êx̂T )(êx̂T )T . (25)\nThe Kronecker product gives an n-fold degeneracy to each eigenvalue, but the spectral density is the same as that of\nM ≡ 1 m2 (êx̂T )(êx̂T )T . (26)\nIt follows that the spectral density of H1 is related to that of M ,\nρH1(λ) = |λ|ρM (λ2) . (27)\nNotice thatM is a Wishart matrix where each factor is itself a product of real Gaussian random matrices. The spectral density for matrices of this type has been computed using\nthe cavity method (Dupic & Castillo, 2014). As the specific form of the result is not particularly enlightening, we defer its presentation to the supplementary material. The result may also be derived using free probability theory (Muller, 2002; Burda et al., 2010). From either perspective it is possible to derive the theR transform, which reads,\nRH1(z) = φz\n2− φ2z2 . (28)\nSee fig. 3 for a comparison of our theoretical prediction for the spectral density to numerical simulations4."
  }, {
    "heading": "6.2. H0 and the Wishart assumption",
    "text": "Unlike the situation for H1, to the best of our knowledge the limiting spectral density of H0 cannot easily be deduced from known results in the literature. In principle it can be computed using diagrammatics and the method of moments (Feinberg & Zee, 1997; Tao, 2012), but this approach is complicated by serious technical difficulties – for example, the matrix has both block structure and Kronecker structure, and there are nonlinear functions applied to the elements. Nevertheless, we may make some progress by focusing our attention on the smallest eigenvalues ofH0, which we expect to be the most relevant for computing the smallest eigenvalues of H .\nThe empirical spectral density of H0 for an autoencoding network is shown in fig. 4. At first glance, this distribution does not closely resemble the MarchenkoPastur distribution (see, e.g., the = 0 curve in fig. 1) owing to its heavy tail and small eigenvalue gap. On the other hand, we are not concerned with the large eigenvalues, and even though the gap is small, its scaling with φ appears to be well-approximated by λ− from eqn. (8)\n4As the derivation requires that the error signals are random, in the simulations we manually overwrite the network’s error signals with random noise.\nfor appropriate σ. See fig. 5. This observation suggests that as a first approximation, it is sensible to continue to represent the limiting spectral distribution of H0 with the Marchenko-Pastur distribution."
  }, {
    "heading": "6.3. Improved approximation to the Hessian",
    "text": "The above analysis motivates us to propose an improved approximation toRH(z),\nRH(z) = σ\n1− σzφ +\nφz\n2− φ2z2 , (29)\nwhere the first term is the R transform of the MarchenkoPastur distribution with the σ parameter restored. As before, an algebraic equation defining the Stieltjes transform of ρH can be deduced from this expression,\n2 = 2 ( z − σ(1− φ) ) G− φ ( 2σz + (1− φ) ) G2H\n− φ2 ( z + σ(φ− 2) ) G3H + z σφ 3G4H . (30)\nUsing the same techniques as in sec. 4, we can use this equation to obtain the function α( , φ). We again find the same 3/2 scaling, i.e.,\nα( , φ) ≈ α̃0(φ) ∣∣∣∣ − c c ∣∣∣∣3/2 . (31) Compared with eqn. (20), the coefficient function α̃0(φ) differs from α0(φ) and,\nc = σ2 ( 27− 18χ− χ2 + 8χ3/2 ) 32φ(1− φ)3 , χ = 1+16φ−8φ2 .\n(32)\nDespite the apparent pole at φ = 1, c actually vanishes there,\nc ≈ 8\n27 σ2(1− φ)3 +O(1− φ)4 . (33)\nCuriously, for σ = 1/2, this is precisely the same behavior we found for the behavior of c near 1 in the Wishart plus Wigner approximation in sec. 4. This observation, combined with the fact that the 3/2 scaling in eqn. (31) is also what we found in sec. 4, suggest that it is H0, rather than H1, that is governing the behavior near c."
  }, {
    "heading": "6.4. Empirical distribution of critical points",
    "text": "We conduct large-scale experiments to examine the distribution of critical points and compare with our theoretical predictions. Uniformly sampling critical points of varying energy is a difficult problem. Instead, we take more of a brute force approach: for each possible value of the index, we aim to collect many measurements of the energy and compute the mean value. Because we cannot control the index of the obtained critical point, we run a very large number of experiments (∼50k) in order to obtain sufficient data for each value of α. This procedure appears to be a fairly robust method for inferring the α( ) curve.\nWe adopt the following heuristic for finding critical points. First we optimize the network with standard gradient descent until the loss reaches a random value between 0 and the initial loss. From that point on, we switch to minimizing a new objective, Jg = |∇θL|2, which, unlike the primary objective, is attracted to saddle points. Gradient descent on Jg only requires the computation of Hessian-vector products and can be executed efficiently.\nWe discard any run for which the final Jg > 10−6; otherwise we record the final energy and index.\nWe consider relatively small networks and datasets in order to run a large number of experiments. We train single-hidden-layer tanh networks of size n = 16, which also equals the input and output dimensionality. For each training run, the data and targets are randomly sampled from standard normal distributions, which makes this a kind of memorization task. The results are summarized in fig. 6. We observe that for small α, the scaling α ≈ | − c|3/2 is a good approximation, especially for smaller φ. This agreement with our theoretical predictions provides support for our analytical framework and for the validity of our assumptions.\nAs a byproduct of our experiments, we observe that the energy of minimizers is well described by a simple function, c = 12 (1− φ)\n2. Curiously, a similar functional form was derived for linear networks (Advani & Ganguli, 2016), c = 1 2 (1 − φ). In both cases, the value at φ = 0 and φ = 1 is understood simply: at φ = 0, the network has zero effective capacity and the variance of the target distribution determines the loss; at φ = 1, the matrix of hidden units is no longer rank constrained and can store the entire input-output map. For intermediate values of φ, the fact that the exponent of (1 − φ) is larger for tanh networks than for linear networks is the mathematical manifestation of the nonlinear network’s better performance for the same number of parameters. Inspired by these observations and by the analysis of Zhang et al. (2016), we speculate that this result may have a simple information-theoretic explanation, but we leave a quantitative analysis to future work."
  }, {
    "heading": "7. Conclusions",
    "text": "We introduced a new analytical framework for studying the Hessian matrix of neural networks based on free probability and random matrix theory. By decomposing the Hessian into two pieces H = H0 + H1 one can systematically study the behavior of the spectrum and associated quantities as a function of the energy of a critical point. The approximations invoked are on H0 and H1 separately, which enables the analysis to move beyond the simple representation of the Hessian as a member of the Gaussian Orthogonal Ensemble of random matrices.\nWe derived explicit predictions for the spectrum and the index under a set of simplifying assumptions. We found empirical evidence in support of our prediction that that small α ≈ | − c|3/2, raising the question of how universal the 3/2 scaling may be, especially given the results of (Bray & Dean, 2007). We also showed how some of our assumptions can be relaxed at the expense of reduced generality of network architecture and increased technical calculations. An interesting result of our numerical simulations of a memorization task is that the energy of minimizers appears to be well-approximated by a simple function of φ. We leave the explanation of this observation as an open problem for future work."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Dylan J. Foster, Justin Gilmer, Daniel HoltmannRice, Ben Poole, Maithra Raghu, Samuel S. Schoenholz, Jascha Sohl-Dickstein, and Felix X. Yu for useful comments and discussions."
  }],
  "year": 2017,
  "references": [{
    "title": "Statistical mechanics of optimal convex inference in high dimensions",
    "authors": ["Advani", "Madhu", "Ganguli", "Surya"],
    "venue": "Physical Review X,",
    "year": 2016
  }, {
    "title": "Statistics of critical points of gaussian fields on large-dimensional spaces",
    "authors": ["Bray", "Alan J", "Dean", "David S"],
    "venue": "Phys. Rev. Lett.,",
    "year": 2007
  }, {
    "title": "Eigenvalues and singular values of products of rectangular gaussian random matrices",
    "authors": ["Z Burda", "A Jarosz", "G Livan", "MA Nowak", "A. Swiech"],
    "venue": "Physical Review E,",
    "year": 2010
  }, {
    "title": "Partial freeness of random matrices",
    "authors": ["Chen", "Jiahao", "Van Voorhis", "Troy", "Edelman", "Alan"],
    "venue": "arXiv preprint arXiv:1204.2257,",
    "year": 2016
  }, {
    "title": "The loss surface of multilayer",
    "authors": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michaël", "Arous", "Gérard Ben", "LeCun", "Yann"],
    "venue": "networks. JMLR,",
    "year": 2015
  }, {
    "title": "Spectral density of products of wishart dilute random matrices. part i: the dense case",
    "authors": ["Dupic", "Thomas", "Castillo", "Isaac Pérez"],
    "venue": "arXiv preprint arXiv:1401.7802,",
    "year": 2014
  }, {
    "title": "Renormalizing rectangles and other topics in random matrix theory",
    "authors": ["Feinberg", "Joshua", "Zee", "Anthony"],
    "venue": "Journal of statistical physics,",
    "year": 1997
  }, {
    "title": "Topology and geometry of half-rectified network optimization. 2016",
    "authors": ["Freeman", "C. Daniel", "Bruna", "Joan"],
    "venue": "URL http://arxiv.org/abs/1611.01540",
    "year": 2016
  }, {
    "title": "Qualitatively characterizing neural network optimization problems",
    "authors": ["Goodfellow", "Ian J", "Vinyals", "Oriol", "Saxe", "Andrew M"],
    "venue": "ICLR",
    "year": 2015
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["Kawaguchi", "Kenji"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
    "authors": ["Keskar", "Nitish Shirish", "Mudigere", "Dheevatsa", "Nocedal", "Jorge", "Smelyanskiy", "Mikhail", "Tang", "Ping Tak Peter"],
    "year": 2017
  }, {
    "title": "Intégration des fonctions inverses",
    "authors": ["Laisant", "C-A"],
    "venue": "Nouvelles annales de mathématiques, journal des candidats aux écoles polytechnique et normale,",
    "year": 1905
  }, {
    "title": "Distribution of eigenvalues for some sets of random matrices",
    "authors": ["Marčenko", "Vladimir A", "Pastur", "Leonid Andreevich"],
    "venue": "Mathematics of the USSR-Sbornik,",
    "year": 1967
  }, {
    "title": "On the asymptotic eigenvalue distribution of concatenated vector-valued fading channels",
    "authors": ["Muller", "Ralf R"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2002
  }, {
    "title": "Path-sgd: Path-normalized optimization in deep neural networks",
    "authors": ["Neyshabur", "Behnam", "Salakhutdinov", "Ruslan", "Srebro", "Nathan"],
    "year": 2015
  }, {
    "title": "On the quality of the initial basin in overspecified",
    "authors": ["Safran", "Itay", "Shamir", "Ohad"],
    "venue": "neural networks. JMLR,",
    "year": 2016
  }, {
    "title": "Free probability theory",
    "authors": ["Speicher", "Roland"],
    "venue": "arXiv preprint arXiv:0911.0087,",
    "year": 2009
  }, {
    "title": "Random covariance matrices: universality of local statistics of eigenvalues",
    "authors": ["T. Tao", "V. Vu"],
    "venue": "Annals of Probability,",
    "year": 2012
  }, {
    "title": "Topics in random matrix theory, volume 132",
    "authors": ["Tao", "Terence"],
    "venue": "American Mathematical Society Providence,",
    "year": 2012
  }, {
    "title": "Characteristic vectors of bordered matrices with infinite dimensions",
    "authors": ["Wigner", "Eugene P"],
    "venue": "Annals of Mathematics, pp",
    "year": 1955
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"],
    "venue": "arXiv preprint arXiv:1611.03530,",
    "year": 2016
  }],
  "id": "SP:a1fdb0f3b3cd2d9b01dd829295d7d9113c782d15",
  "authors": [{
    "name": "Jeffrey Pennington",
    "affiliations": []
  }, {
    "name": "Yasaman Bahri",
    "affiliations": []
  }],
  "abstractText": "Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, φ, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1 2 (1− φ) .",
  "title": "Geometry of Neural Network Loss Surfaces via Random Matrix Theory"
}