{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The choice of problem formulation for regression has a large impact on prediction performance on new data— generalization performance. There is an extensive literature on problem formulations to promote generalization, including robust losses (Huber, 2011; Ghosh et al., 2017; Barron, 2017); proxy losses and reductions between problems (Langford et al., 2006); the addition of regularization to impose constraints or preferences on the solution; the addition of label noise (Szegedy et al., 2016); and even ensuring multiple tasks are learned simultaneously, rather than separately, as in multi-task learning (Caruana, 1998). There is typically a goal in mind—such as classification accuracy\n1Department of Computing Science, University of Alberta, Edmonton. Correspondence to: Martha White <whitem@ualberta.ca>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nor absolute error for regression—but those losses are not necessarily directly minimized.\nIn recent years, there has been a particular focus on learning representations with neural networks that generalize better. With fixed representations, the loss or problem formulation can only have so much impact, because the learned function is a linear function of inputs. With (deep) neural networks, however, the performance can vary widely, based even on simple modifications such as the initialization (Glorot & Bengio, 2010). Particularly in classification, modifying the outputs can significantly improve performance. An extensive empirical study on classification and age prediction (Gao et al., 2017), under label ambiguity, showed that data augmentation on the label side—putting a distribution over an ambiguous label—significantly improved test accuracy, validated also by other work on age estimation (Rothe et al., 2018). Work on model compression (Ba & Caruana, 2013; Urban et al., 2016) and distillation (Hinton et al., 2015) highlight that a smaller student model can be trained to capture the generalization ability of a larger teacher model. In general, there is a growing literature on data augmentation and label smoothing, that advocates for reduced overfitting and improved generalization from modifying the outputs (Norouzi et al., 2016; Szegedy et al., 2016; Xie et al., 2016; Miyato et al., 2016; Pereyra et al., 2017) and in reinforcement learning where learning distributional outputs, rather than means, improves performance (Bellemare et al., 2017).\nThere has been some work—though considerably less— towards understanding the impact of the properties of the loss that promote effective optimization. There is a recent insight that minimizing training time increases generalization performance (Hardt et al., 2015), motivating the design of losses that can be more easily optimized. Though not the focus in data augmentation, there have been some insights about loss properties. Gao et al. (2017) showed that their data augmentation approach provided a faster convergence rate (see their Figure 8). Pereyra et al. (2017) showed that label smoothing and their regularizer penalizing confident predictions for classification provided smoother gradient norms than without regularization. Bellemare et al. (2017) hypothesized that the properties of the KL-divergence could have improved learning performance, in a reinforcement learning setting. These papers hint at something deeper occurring with the loss, and motivate investigation into not just the conversion of the problem but into the loss itself.\nIn this work, we show that the properties of the loss have a significant effect, and better explain the resulting increase in performance than preventing overfitting. We first propose a new loss for regression, called a Histogram Loss (HL). The targets are converted to a target distribution, and the KL-divergence taken between a histogram density and this target distribution. The choice of histogram density provides a relatively flexible prediction distribution, that nonetheless enables the KL-divergence to be computed efficiently. The prediction is then the expected value of this histogram density. This modification could be seen as converting the problem to a more difficult (multi-task) problem—from one output, to multiple values to represent the distribution—that promotes generalization in the learner and reduces overfitting. We show that instead of this hypothesis, the (optimization) properties of the HL seem to be the key factor in the resulting improved accuracy. We provide a series of empirical results to support this hypothesis. We also characterize the norm of the gradient of the HL which directly relates to sample complexity (Hardt et al., 2015). The bounds on the variability of the gradient help explain the positive empirical performance of the HL, and further motivate the use of this loss as an alternative for the standard loss for regression."
  }, {
    "heading": "2. Distributional Losses for Regression",
    "text": "In this section, we introduce the Histogram Loss (HL), which generalizes beyond special cases of soft-target losses used in recent work (Norouzi et al., 2016; Szegedy et al., 2016; Gao et al., 2017). We first introduce the loss and how it can be used for regression. We then relate it to other objectives, including maximum likelihood for regression and other methods that learn distributions."
  }, {
    "heading": "2.1. Learning means and distributions",
    "text": "In regression, it is common to use the squared-error loss, or `2 loss. This corresponds to assuming that the continuous target variable Y is Gaussian distributed, conditioned on inputs x 2 Rd: Y ⇠ N (µ = f(x), 2) for a fixed variance 2 > 0 and some function f : Rd ! R on the inputs, such as a linear function f(x) = hx,wi for weights w 2 Rd. The maximum likelihood function f for n samples {xi, yi}, corresponds to minimizing the `2 loss\nmin f2F\nnX\nj=1\n(f(xj) yj)2 (1)\nwith prediction f(x) ⇡ E[Y |x].\nAlternatively, one could consider learning a distribution over Y directly, and then taking the mean of that distribution—or other statistics—to provide a prediction. This additional difficulty seems hardly worth the effort, considering only the mean is required for prediction. However, as motivated above, the increased difficulty could beneficially prevent\noverfitting and promote generalization.\nThere are many options for learning conditional distributions, p(y|x) even when only considering those that use neural networks (Bishop, 1994; Tang & Salakhutdinov, 2013; Rothe et al., 2015; Bellemare et al., 2017). The goal of this work, however, is not to provide another method to learn distributions. Rather, the goal is to benefit from inducing a distribution over Y , even if that distribution will subsequently not be used, other than for computing a mean prediction. In our experiments, we will compare to an approach that learns distributions, but only to evaluate regression performance."
  }, {
    "heading": "2.2. The Histogram Loss",
    "text": "Consider predicting a continuous target Y with event space Y , given inputs x. Instead of directly predicting Y , we select a target distribution on Y |x. This target distribution is selected upfront, by us, rather than being learned. Suppose the target distribution has support [a, b], pdf p, and cdf F . We would like to learn a parameterized prediction distribution qx : Y ! [0, 1], conditioned on x, by minimizing a KL-divergence to p. For any p, however, this may be expensive. Further, depending on the parameterization of the prediction distribution, this may also be potentially non-convex in those parameters.\nWe propose to restrict the prediction distribution qx to be a histogram density. Assume [a, b] has been uniformly partitioned into k bins, of width wi, and let function f : X ! [0, 1]k provide k-dimensional vector f(x) of the coefficients indicating the probability the target is in that bin, given x. The density qx corresponds to a (normalized) histogram, and has density values fi(x)/wi per bin. The KL-divergence between p and qx is\nDKL(p||qx) = h(p, qx) h(p).\nThe second term is the differential entropy—the extension of entropy to continuous random variables. Because the second term only depends on p, the aim is to minimize the first term: the cross-entropy between p and qx. This loss simplifies, due to the form on qx:\nh(p, qx) = Z b\na p(y) log qx(y)dy\n= kX\ni=1\nZ li+wi\nli\np(y) log fi(x)\nwi dy\n= kX\ni=1\nlog fi(x)\nwi (F (li + wi) F (li))| {z }\npi\n.\nIn the minimization, the width itself can be ignored, because log fi(x)wi = log fi(x) logwi, giving the Histogram Loss\nHL(p, qx) = kX\ni=1\npi log fi(x). (2)\nThis loss has several useful properties. One important property is that it is convex in fi(x); even if the loss is not convex in all network parameters, it is at least convex on the last layer. The other three benefits are due to restricting the form of the predicted distribution qx to be a histogram density. First, the divergence to the full distribution p can be efficiently computed. This contrasts previous work, which samples the KL for a subset of y values (Norouzi et al., 2016; Szegedy et al., 2016). Second, the choice of p is flexible, as long as its CDF can be evaluated for each bin. The weighting pi = F (li + wi) F (li) can be computed offline once for each sample, making it inexpensive to query repeatedly for each sample during training. Third, different distributional choices simply result in different weightings in the cross-entropy. This simplicity facilitates interpreting the impact of changing the distributional assumptions on Y ."
  }, {
    "heading": "2.3. Target distributions and related objectives",
    "text": "Below, we consider some special cases for p that are of interest and highlight connections to previous work.\nTruncated Gaussian on Y |x and HL-Gaussian. Consider a truncated Gaussian distribution, on support [a, b], as the target distribution. The mean µ for this Gaussian is the datapoint yj itself, with fixed variance 2. The pdf p is\np(y) = 1\nZ p 2⇡\ne (y µ)\n2\n2 2\nwhere Z = 12 (erf ⇣ b µp 2 ⌘ erf ⇣ a µp 2 ⌘ ), and the HL has\npi = 12Z\n✓ erf ✓ li + wi µp\n2\n◆ erf ✓ li µp\n2\n◆◆ .\nThis distribution enables significant smoothing over Y , through the variance parameter 2. We call this loss HLGaussian, defined by number of bins k and variance 2. Based on positive empirical performance, it will be the main HL loss that we advocate for and analyze.\nSoft Targets and a Histogram Density on Y |x. In classification, such as multinomial logistic regression, it is typical to assume Y |x is a categorical distribution, where Y is discrete. The goal is still to estimate E[Y |x] and when training, hard 0-1 values for Y are used in the cross-entropy. Soft labels, instead of 0-1 labels, can be used by adding label noise (Norouzi et al., 2016; Szegedy et al., 2016; Pereyra et al., 2017). This can be seen as an instance of HL, but for discrete Y , where a categorical distribution is selected for the target distribution. Minimizing the cross-entropy to these soft-labels corresponds to trying to match such a smoothed target distribution, rather than the original 0-1 categorical distribution.\nSuch soft targets have also been considered for ordinal regression, again motivated as label smoothing, for age prediction (Gao et al., 2017; Rothe et al., 2018). The outputs\nare smoothed using radial basis function similarities to a set of bin centers. This procedure can be seen as selecting a histogram density for the target distribution, where the coefficients for each bin are determined by these radial basis function similarities. The resulting loss is similar to HLGaussian, with slightly different pi, though introduced as data augmentation to smooth (ordinal) targets.\nDirac delta on Y |x. Finally, we consider the relationship to maximum likelihood. For classification, Norouzi et al. (2016) and Szegedy et al. (2016) used a combination of maximum likelihood and a KL-divergence to a (uniform) distribution. Szegedy et al. (2016) add uniform noise to the labels and Norouzi et al. (2016) sample from an exponentiated reward distribution, with a temperature parameter, for structured prediction. Both consider only a finite set for Y , because they both address classification problems.\nThe relationship between KL-Divergence and maximum likelihood can be extended to continuous Y . The connection is typically in terms of statistical consistency: the maximum likelihood estimator approaches the minimum of the KLdivergence to the true distribution, if the distributions are of the same parametric form (Wasserman, 2004, Theorem 9.13). They can, however, be connected for finite samples with different distributions. Consider Gaussians centered around datapoints yj , with arbitrarily small variances 12a 2:\na,j(y) = 1\na2 p ⇡ exp\n⇣ (y yj) 2\na2\n⌘ .\nLet the target distribution have p(y) = a,j(y) for each sample. Define function pi,j : [0,1) ! [0, 1] as pi,j(a) =R li+wi li\na,j(y)dy . For each yj , as a ! 0, pi,j(a) ! 1 if yj 2 [li, li + wi] and pi,j(a) ! 0 otherwise. So, for ij s.t. yj 2 [lij , lij+wi],\nlim a!0 HL( a,j , qxj ) = log fij (xj).\nThe sum over samples for the HL to the Dirac delta on Y |x, then, corresponds to the negative log-likelihood for qx\nargmin f1,...,fk\nnX\nj=1\nlog fij (xj) = argmin f1,...,fk\nnX\nj=1\nlog qxj (yj).\nSuch a delta distribution on Y |x results in one coefficient pi being 1, reflecting the distributional assumption that Y is certainly in a bin. In the experiments, we compare to this loss, which we call HL-OneBin.\nUsing a similar analysis to above, p(y) can be considered as a mixture between a,j(y) and a uniform distribution. For a weighting of ✏ on the uniform distribution, the resulting loss HL-Uniform has pi = ✏ for i 6= ij , and pij = 1 k✏."
  }, {
    "heading": "3. Optimization properties of the HL",
    "text": "There are at least two motivations for this loss, in terms of promoting the search for effective solutions. The first is the stability of gradients, promoting stable gradient descent. The second is a connection to learning optimal policies in reinforcement learning. Both provide some insight that the properties of the HL, during optimization, promote better generalization performance.\nStable gradients for HL. Hardt et al. (2015) have shown that the generalization performance for stochastic gradient descent is bounded by the number of steps that stochastic gradient descent takes during training, even for non-convex losses. The bound is also dependent on the properties of the loss. In particular, it is beneficial to have a loss function with small Lipschitz constant L, which bounds the norm of the gradient. Below, we discuss how the HL with a Gaussian distribution (HL-Gaussian) in fact promotes an improved bound on this norm, over both the `2 loss and the HL with all weight in one bin (HL-OneBin).\nIn the proposition bounding the HL-Gaussian gradient, we assume\nfi(x) = exp( ✓(x) >wi)Pk j=1 exp( ✓(x) >wj) (3)\nfor some function ✓ : X ! Rk parameterized by a vector of parameters ✓. For example, ✓(x) could be the last hidden layer in a neural network, with parameters ✓ for the entire network up to that layer. The proposition provides a bound on the gradient norm in terms of the current network parameters. Our goal is to understand how the gradients might vary locally for the parameters, as opposed to globally bounding the norm and characterizing the Lipschitz constant only in terms of the properties of the function class and loss. Proposition 1 (Local Lipschitz constant for HL-Gaussian). Assume x, y are fixed, giving fixed coefficients pi in HLGaussian. Let fi(x) be as in (3), defined by the parameters w = {w1, . . . ,wk} and ✓, providing the predicted distribution qx. Assume for all i that w>i ✓(x) is locally l-Lipschitz continuous w.r.t ✓\nkr✓(w>i ✓(x))k  l (4)\nThen the norm of the gradient for HL-Gaussian, w.r.t. to all the parameters in the network {✓,w}, is bounded by\nkr✓,wHL(p, qx)k (l+k ✓(x)k) kX\ni=1\n|pi fi(x)| (5)\nProof. First consider the gradient of the HL, with explicit details on these computations in Appendix A\n@\n@wi\nkX\nj=1\npj log fj(x) = (pi fi(x)) ✓(x)\nThe norm of the gradient of HL in Equation (2), w.r.t. w which is composed of all the weights wi 2 Rk is\n@\n@w\nkX\nj=1\npj log fj(x) \nkX\ni=1\n@\n@wi\nkX\nj=1\npj log fj(x)\n= kX\ni=1\nk(pi fi(x)) ✓(x)k\n kX\ni=1\n|pi fi(x)|k ✓(x)k\nSimilarly, the norm of the gradient w.r.t. ✓ is\n@\n@✓\nkX\nj=1\npj log fj(x) =\nkX\ni=1\n(pi fi(x))r✓w>i ✓(x)\n kX\ni=1\n(pi fi(x))r✓w>i ✓(x)\n kX\ni=1\n|pi fi(x)|l\nTogether, these bound the norm kr✓,wHL(p, qx)k.\nThe results by Hardt et al. (2015) suggest it is beneficial for the local Lipschitz constant—or the norm of the gradient— to be small on each step. HL-Gaussian provides exactly this property. Besides the network architecture—which we are here assuming is chosen outside of our control—the HL-Gaussian gradient norm is proportional to |pi fi(x)|. This number is guaranteed to be less than 1, but generally is likely to be even smaller, especially if fi(x) reasonably accurately predicts pi. Further, the gradients should push the weights to stay within a range specified by pi, rather than preferring to push some to be very small—close to 0—and others to be close to 1. For example, if fi(x) starts relatively uniform, then the objective does not encourage predictions fi(x) to get smaller than pi. If pi are non-negligible, this keeps fi(x) away from zero and the loss in a smaller range.\nThis contrasts both the norm of the gradient for the `2 loss and HL-OneBin. For the `2 loss, (f(x) y) h r✓w> ✓(x)\n✓(x)\ni is the gradient, giving gradient norm bound\n(l+k ✓(x)k)|f(x) y|. The constant |f(x) y|, as opposed to Pk i=1 |pi fi(x)|, can be much larger, even if y is normalized between [0, 1], and can vary significantly more. HL-OneBin, on the other hand, shares the same constant as HL-Gaussian, but suffers from another problem. The Lipschitz constant ` in Equation (4) will likely be larger, because pi is frequently zero and so pushes fi(x) towards zero. This results in larger objective values and pushes w>i ✓(x) to get larger, to enable fi(x) to get close to 1.\nConnection to reinforcement learning. The HL can also be motivated through a connection to maximum entropy reinforcement learning. In reinforcement learning, an agent iteratively selects actions and transitions between states, to maximize (long-term) reward. The agent’s goal is to find an optimal policy, in as few interactions as possible. To do so, the agent begins by exploring more, to then enable more efficient convergence to optimal. Supervised learning can be expressed as a reinforcement learning problem (Norouzi et al., 2016), where action selection conditioned on a state corresponds to making a prediction conditioned on a feature vector. An alternative view to minimizing prediction error is to search for a policy to make accurate predictions.\nOne strategy to efficiently find an optimal policy is through a maximum entropy objective. The policy balances between selecting the action it believes to be optimal—make its current best prediction—and acting more randomly—with high-entropy. For continuous action set Y , the goal is to minimize the following objective\nZ\nX ps(x)\nh ⌧h(qx)\nZ\nY qx(y)r(y, yi)dy\ni dx (6)\nwhere ⌧ > 0; ps is a distribution over states x; qx is the policy or distribution over actions for a given x; and r(y, yi) is the reward function, such as the negative of the objective r(y, yi) = 12 (y yi)\n2. Minimizing (6) corresponds to minimizing the KL-divergence across x between qx and the exponentiated payoff distribution p(y) = 1Z exp(r(y, yi)/⌧) where Z = R exp(r(y, yi)/⌧), because DKL(qx||p) = h(qx) Z qx(y) log p(y)dy\n= h(qx) ⌧ 1 Z qx(y)r(y, yi)dy + logZ.\nThe connection between the HL and maximum-entropy reinforcement learning is that both are minimizing a divergence to this exponentiated distribution p. The HL, however, is minimizing DKL(p||qx) instead of DKL(qx||p). For example, Gaussian target distribution with variance 2 corresponds to minimizing DKL(p||qx) with r(y, yi) =\n12 (y yi) 2 and ⌧ = 2. These two KL-divergences are not the same, but a similar argument to Norouzi et al. (2016) could be extended for continuous y, showing DKL(qx||p) is upper-bounded by DKL(p||qx) plus variance terms. The intuition, then, is that minimizing the HL is promoting an efficient search for an optimal (prediction) policy."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, we investigate the utility of the HL-Gaussian for regression, compared to using an `2 loss. We particularly investigate why the modification to this distributional loss improves performance, designing experiments to test if it is due to (a) the utility of learning distributions or smoothed\ntargets, (b) a bias-variance trade-off from bin size or variance in the HL-Gaussian, (c) an improved representation, (d) nonlinearity introduced by the HL and (e) improved optimization properties of the loss.\nDatasets and pre-processing. All features are transformed to have zero mean and unit variance. We randomly split the data into train and test sets in each run. The CT Position dataset is from CT images of patients (Graf et al., 2011), with 385 features and the target set to the relative location of the image. The Song Year dataset is a subset of The Million Song Dataset (Bertin-Mahieux et al., 2011), with 90 audio features for a song and target corresponding to the release year. The Bike Sharing dataset (Fanaee-T & Gama, 2014), about hourly bike rentals for two years, has 16 features and target set to the number of rented bikes.\nRoot mean squared error (RMSE) and mean absolute error (MAE) are reported over 5 runs, with standard errors. We include both errors and objective values, on train and test, to provide a more complete picture of the causes of differences between the losses. For space, we only include in-depth results on CT Position in the main body. We summarize the overall conclusions on all three datasets below, and include the tables for Song Year and Bike Sharing in Appendix C and more dataset information in Appendix B.\nAlgorithms. We compared several regression strategies, distribution learning approaches and several variants of HL. All the approaches—except for Linear Regression—use the same neural network, with differences only in the output layer. The architecture for Song Year is 90-45-45-45-45-1 (4 hidden layers of size 45), for Bike Sharing is 16-64-64- 64-64 and for CT Position is 385-192-192-192-192-1. All units employ ReLU activation, except the last layer with linear activations. Unless specified otherwise, all networks using HL have 100 bins. Meta-parameters for comparison algorithms are chosen according to best Test MAE. Network architectures were chosen according to best Test MAE for `2, with depth and width varied across 7 different values with final choices being neither biggest nor smallest.\nLinear Regression is included as a baseline, using ordinary least squares with the inputs. Squared-error `2 is the neural network trained using the `2 loss. The targets are normalized to range [0, 1], which was needed to improve stability and accuracy. Absolute-error `1 is the neural network using the `1 loss. `2+Noise is the same as `2, except Gaussian noise is added to the targets as a form of augmentation. The standard deviation of the noise is selected from {10 5, 10 4, 10 3, 10 2, 10 1}. `2+Clipping is the same as `2, but with gradient norm clipping during training. The threshold for clipping is selected from {0.01, 0.1, 1, 10}.\nHL-OneBin is the HL, with Dirac delta target distribution. HL-Uniform is the HL, with a target distribution that mixes between a delta distribution and the uniform distribution, with a weighting of ✏ on the uniform and 1 ✏ on the delta, where ✏ 2 {10 5, 10 4, 10 3, 10 2, 10 1}. HL-Gaussian is the HL, with a truncated Gaussian distribution as the target distribution. The variance 2 is set to the radius of the bins. MDN is a Mixture Density Network (Bishop, 1994) that models the target distribution as a mixture of Gaussian distributions. The original model uses an exponential activation to model the standard deviations. However, inspired by (Lakshminarayanan et al., 2017), we used softplus activation plus a small constant (10 2) to avoid numerical instability. We selected the number of components from {2, 4, 8, 16, 32}. Predictions are made by taking the mean of the mixture model given by the MDN. `2+Softmax use a softmax-layer with `2 loss,Pk\ni=1(fi(xj)ci yj)2 for bin centers ci, with otherwise the same settings as HL-Gaussian.\nWe used Scikit-learn (Pedregosa et al., 2011) for the implementations of Linear Regression, and Keras (Chollet et al., 2015) for the neural network models. All neural network models are trained with mini-batch size 256 using the Adam optimizer (Kingma & Ba, 2014) with a learning rate 1e-3 and the parameters are initialized according to the method suggested by LeCun et al. (1998). Dropout (Srivastava et al., 2014) with rate 0.05 is added to the input layer of all neural networks to avoid overfitting. We trained the networks for 1000 epochs on CT Position, 150 epochs on Song Year and 500 epochs on Bike Sharing.\nOverall performance and conclusions (Tables 1, 4, 6). We first report the relative performance of all these models, on the CT Position dataset (Table 1) and, in Appendix C, the Song Year dataset (Table 4) and Bike Sharing dataset (Table 6). The overall conclusions are that the HL-Gaussian never harms performance—slightly improving performance on the Song Year dataset—and otherwise can significantly improve performance over alternatives—on both the CT Position and Bike Sharing datasets. We only report the\nfull set of algorithms for CT Position, and more in-depth experiments understanding the result on that domain.\nLearning other distributions is not effective (Table 1). HL-Gaussian improves performance, but the other distribution-learning approaches appear to have little advantages, as shown in Table 1. HL-OneBin and HL-Uniform can actually do worse than Regression. MDN provides only minor gains over Regression. Interestingly, it has been shown MDN suffers from numerical instabilities, making training difficult (Oord et al., 2016; Rupprecht et al., 2016).\nA related idea to learning the distribution explicitly is to use data augmentation, through label smoothing. We therefore also compared to directly modifying the labels and gradients, with `2-Noise and `2-Clipping. These models do perform slightly better than Regression for some settings, but do not achieve the same gains as HL-Gaussian.\nThe bias-variance trade-off in the loss definition is not significantly impacting performance (Figure 1). If one fixes the possible range of the output variable, the distribution becomes more and more expressive as the number of bins increases. The model could have a higher chance of overfitting in this situation. Reducing the number of bins, on the other hand, introduces discretization error and increases the bias. Further, the entropy parameter 2 introduces a bias-variance trade-off, making the target distribution more uniform as entropy increases—likely resulting in lower variance—but also washing out the signal—incurring high bias. The selection of these parameters, therefore, may provide a opportunity to influence this bias-variance tradeoff, and improve performance by essentially optimizing the loss for a problem. The ability for the user to select these parameters could explain some of the performance gains in recent results (Gao et al., 2017; Bellemare et al., 2017), compared to standard losses that cannot be tuned.\nWe tested the impact of varying the number of bins, and the entropy 2 for HL-Gaussian. We found that these parameters, especially the entropy, can have an impact on performance, but that the results were much more robust to changing these parameters than might be expected (reported\nin more depth in Figure 1). It does not seem to be the case, therefore, that the tuning of these hyperparameters is the primary explanation for the improved performance.\nThe learned representation is not better (Table 2). Learning a distribution, as opposed to a single statistic, provides a more difficult target—one that could require a better representation. The hypothesis is that amongst the functions f in your function class F , there is a set of functions that can predict the targets almost equally well. To distinguish amongst these functions, a wider range of tasks can make it more likely to select the true function, or at least one that generalizes better.\nWe conducted three experiments to test the hypothesis than an improved representation is learned. We first trained with HL-Gaussian and `2, to obtain their representations. We tested (a) swapping the representations and re-learning only the last layer, (b) initializing with the other’s representation, (c) and using the same fixed random representation for both. For (a) and (c), the optimizations for both are convex, since the representation is fixed. The results in Table 2, are surprisingly conclusive: using the representation from HL-Gaussian does not improve performance of `2, and even under a random representation, HL-Gaussian performs significantly better than `2. This suggests that HL-Gaussian is not causing a more useful or more general representation to be learned, as otherwise `2 should be able to take advantage of that representation.\nThe softmax nonlinearity is not the main cause (Table 1). The HL-Gaussian can be seen as a generalized linear model, where a small amount of non-linearity is introduced from the transfer. The level of nonlinearity is similar to that in the cross-entropy loss, and the effect should be small because each transformed output w>i (x) has to predict a probability value. This contrasts with an alternative way to\nuse a softmax layer—which we call `2+Softmax—which gets to tune the softmax layer to directly predict y given x. Such a layer has additional parameters to predict one target (100 additional parameters, for 100 bins). This contrast the HL-Gaussian, which has also 100 bins but has to predict 100 targets instead of just one target.1\nDespite the differences between the role of the softmax in HL-Gaussian and `2+Softmax, we provide this comparison to provide some insight into potential nonlinearities introduced by the loss. The result in Table 1 shows that this softmax layer can improve performance (to 12.720), but not as significant as HL-Gaussian (8.992). This is particularly intriguing, because as mentioned above, `2+Softmax can much more flexibly tune the nonlinear softmax layer. The ability to outperform `2+softmax-layer emphasizes that there are properties of the HL causing improvements beyond the use of the softmax.\nHL-Gaussian trains fast (Figure 4). We trained `2, HL-OneBin, and HL-Gaussian on the CT Position dataset with no dropout to find the role of the loss function on the rate of convergence. We also computed the norm the gradient w.r.t. the parameters of the last layer after each epoch, and normalized the gradient norms of each model by their median to compare their variability. As shown in Figure 4, HL-Gaussian has significantly better behaved gradients, than `2. Correspondingly, it converges significantly faster and more smoothly. The other two methods that more carefully controlled gradients—`2-Noise and `2-Clip—provided the next best gains to HL-Gaussian.\n1It is possible that having 100 extra parameters in the last layer makes it possible to benefit from randomness, over the `2. We ran experiments enabling the `2 to have 100 outputs, each predicting the target but with different initial weights. Even selecting the best of the 100 outputs on the test data only slightly improved performance, with a test MAE of 18.421."
  }, {
    "heading": "5. Conclusion",
    "text": "We introduced a novel loss for regression, called the Histogram Loss (HL), that explicitly constructs a distribution over targets to predict, rather than directly estimating the mean of the target conditioned on inputs. The loss involves minimizing the KL-divergence between a predicted distribution and this target distribution. To make this loss efficient to compute, without significantly reducing modeling power, we restrict the class of approximation densities to histogram densities. We highlight that for a particular setting of the HL—with a target Gaussian distribution—the norm of the gradient does not grow large or vary widely. Combined with recent results that show reducing training steps for stochastic gradient results in improved generalization provide some theoretical justification for why we observe such strong performance of HL-Gaussian in practice. We conduct a series of experiments to identify this gain, with evidence that the main role is not due to overfitting or an improved representation, but rather due to the fact that the HL can be optimized in a smaller number of steps, with smoother gradients.\nThe introduction of the HL provides several avenues to improve our choice of loss function. One direction is to more explicitly take advantage of the specification of the target distribution. In this work, we considered this loss only for a fixed set of bins, widths and variance parameter for the target distribution. To be more agnostic to these choices, we demonstrated performance across possible parameter settings. However, these parameters could be determined using meta-parameter optimization strategies, such as cross validation, or even learning strategies with particular objectives for these parameters. The key property to make the HL easy to specify and optimize was the use of a histogram to predict the target; the derivation does not prevent also optimizing the bins centers, widths and variances.\nOverall, this work provides some unification of recent results using soft targets, through the introduction of the HL. We hope for it to facilitate discussion and development on the design of losses that promote learning, and direct further investigation into the importance of the optimization properties of these losses."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Alberta Innovates for funding AMII (the Alberta Machine Intelligence Institute) and this research."
  }],
  "year": 2018,
  "references": [{
    "title": "Do Deep Nets Really Need to be Deep",
    "authors": ["L.J. Ba", "R. Caruana"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A distributional perspective on reinforcement learning",
    "authors": ["M.G. Bellemare", "W. Dabney", "R. Munos"],
    "venue": "arXiv preprint arXiv:1707.06887,",
    "year": 2017
  }, {
    "title": "The million song dataset",
    "authors": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"],
    "venue": "In Ismir,",
    "year": 2011
  }, {
    "title": "Mixture density networks",
    "authors": ["C.M. Bishop"],
    "venue": "Technical Report,",
    "year": 1994
  }, {
    "title": "Multitask learning. In Learning to learn, pp. 95–133",
    "authors": ["R. Caruana"],
    "year": 1998
  }, {
    "title": "Event labeling combining ensemble detectors and background knowledge",
    "authors": ["H. Fanaee-T", "J. Gama"],
    "venue": "Progress in Artificial Intelligence,",
    "year": 2014
  }, {
    "title": "Deep label distribution learning with label ambiguity",
    "authors": ["Gao", "B.-B", "C. Xing", "Xie", "C.-W", "J. Wu", "X. Geng"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2017
  }, {
    "title": "Robust loss functions under label noise for deep neural networks",
    "authors": ["A. Ghosh", "H. Kumar", "P. Sastry"],
    "venue": "In AAAI Conference on Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "2d image registration in ct images using radial image descriptors",
    "authors": ["F. Graf", "Kriegel", "H.-P", "M. Schubert", "S. Pölsterl", "A. Cavallaro"],
    "venue": "Medical Image Computing and Computer-Assisted Intervention,",
    "year": 2011
  }, {
    "title": "Train faster, generalize better: Stability of stochastic gradient descent",
    "authors": ["M. Hardt", "B. Recht", "Y. Singer"],
    "venue": "arXiv preprint arXiv:1509.01240,",
    "year": 2015
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["G. Hinton", "O. Vinyals", "J. Dean"],
    "venue": "arXiv preprint arXiv:1503.02531,",
    "year": 2015
  }, {
    "title": "Robust statistics",
    "authors": ["P.J. Huber"],
    "venue": "In International Encyclopedia of Statistical Science,",
    "year": 2011
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
    "authors": ["B. Lakshminarayanan", "A. Pritzel", "C. Blundell"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Predicting Conditional Quantiles via Reduction to Classification",
    "authors": ["J. Langford", "R. Oliveira", "B. Zadrozny"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence,",
    "year": 2006
  }, {
    "title": "Efficient backprop",
    "authors": ["Y. LeCun", "L. Bottou", "G.B. Orr", "Müller", "K.-R"],
    "venue": "In Neural networks: Tricks of the trade,",
    "year": 1998
  }, {
    "title": "Distributional smoothing by virtual adversarial examples",
    "authors": ["T. Miyato", "Maeda", "S.-i", "M. Koyama", "K. Nakae", "S. Ishii"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "Reward augmented maximum likelihood for neural structured prediction",
    "authors": ["M. Norouzi", "S. Bengio", "N. Jaitly", "M. Schuster", "Y. Wu", "D Schuurmans"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Pixel recurrent neural networks",
    "authors": ["Oord", "A. v. d", "N. Kalchbrenner", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1601.06759,",
    "year": 2016
  }, {
    "title": "Regularizing neural networks by penalizing confident output distributions",
    "authors": ["G. Pereyra", "G. Tucker", "J. Chorowski", "Ł. Kaiser", "G. Hinton"],
    "venue": "arXiv preprint arXiv:1701.06548,",
    "year": 2017
  }, {
    "title": "Learning in an uncertain world: Representing ambiguity through multiple hypotheses",
    "authors": ["C. Rupprecht", "I. Laina", "M. Baust", "F. Tombari", "G.D. Hager", "N. Navab"],
    "venue": "arXiv preprint arXiv:1612.00197,",
    "year": 2016
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "Rethinking the inception architecture for computer vision",
    "authors": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Learning stochastic feedforward neural networks",
    "authors": ["Y. Tang", "R.R. Salakhutdinov"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional",
    "authors": ["G. Urban", "K.J. Geras", "S.E. Kahou", "Ö. Aslan", "S. Wang", "R. Caruana", "A. Mohamed", "M. Philipose", "M. Richardson"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "All of Statistics: A Concise Course in Statistical Inference",
    "authors": ["L. Wasserman"],
    "year": 2004
  }],
  "id": "SP:bb2944569a2b3d3b8340b36d4903c8cddf20047f",
  "authors": [{
    "name": "Ehsan Imani",
    "affiliations": []
  }, {
    "name": "Martha White",
    "affiliations": []
  }],
  "abstractText": "There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels—such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.",
  "title": "Improving Regression Performance with Distributional Losses"
}