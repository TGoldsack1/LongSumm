{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 894–899 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n894"
  }, {
    "heading": "1 Introduction",
    "text": "Due to recent advances in deep learning techniques, intelligent personal digital assistants (IPDAs) such as Amazon Alexa, Google Assistant, Microsoft Cortana, and Apple Siri have been widely used as real-life applications of natural language understanding (Sarikaya et al., 2016; Sarikaya, 2017).\nIn natural language understanding, domain classification is a task that finds the most relevant domain given an input utterance (Tur and de Mori, 2011). For example, “make a lion sound” and “find me an apple pie recipe” should be classified as ZooKeeper and AllRecipe, respectively. Recent IPDAs cover more than several thousands of diverse domains by including third-party developed domains such as Alexa Skills (Kumar et al., 2017; Kim et al., 2018a; Kim and Kim, 2018),\nGoogle Actions, and Cortana Skills, which makes domain classification to be a more challenging task.\nGiven a large number of domains, leveraging user’s enabled domain information1 has been shown to improve the domain classification performance since enabled domains reflect the user’s context in terms of domain usage (Kim et al., 2018b). For an input utterance, Kim et al. (2018b) use attention mechanism so that a weighted sum of the enabled domain vectors are used as an input signal as well as the utterance vector. The enabled domain vectors and the attention weights are automatically trained in an end-to-end fashion to be helpful for the domain classification.\nIn this paper, we propose a supervised enablement attention mechanism for more effective attention on the enabled domains. First, we use logistic sigmoid instead of softmax as the attention activation function to relax the constraint that the weight sum over all the enabled domains is 1 to the constraint that each attention weight is between 0 and 1 regardless of the other weights (Martins and Astudillo, 2016; Kim et al., 2017). Therefore, all the attention weights can be very low if there are no enabled domains relevant to a groundtruth so that we can disregard the irrelevant enabled domains, and multiple attention weights can have high values when multiple enabled domains are helpful for disambiguating an input utterance. Second, we encourage each attention weight to be high if the corresponding enabled domain is a ground-truth domain and if otherwise, to be low, by a supervised attention method (Mi et al., 2016) so that the attention weights can be directly tuned for the downstream classification task. Third, we\n1Enabled domain information specifically refers to preferred or authenticated domains in Amazon Alexa, but it can be extended to other information such as the list of recently used domains.\napply self-distillation (Furlanello et al., 2018) on top of the enablement attention weights so that we can better utilize the enabled domains that are not ground-truth domains but still relevant.\nEvaluating on datasets obtained from real usage in a large-scale IPDA, we show that our approach significantly improves domain classification performance by utilizing the domain enablement information effectively."
  }, {
    "heading": "2 Model",
    "text": "Figure 1 shows the overall architecture of the proposed model.\nGiven an input utterance, each word of the utterance is represented as a dense vector through word embedding followed by bidirectional long shortterm memory (BiLSTM) (Graves and Schmidhuber, 2005). Then, an utterance vector is composed by concatenating the last outputs of the forward LSTM and the backward LSTM.2\nTo represent the domain enablement information, we obtain a weighted sum of domain enablement vector where the weights are calculated by logistic sigmoid function on top of the multiplicative attention (Luong et al., 2015) for the utterance vector and the domain enablement vectors. The attention weight of an enabled domain e is formu-\n2We have also evaluated word vector summation, CNN (Kim, 2014), BiLSTM mean-pooling, and BiLSTM maxpooling (Conneau et al., 2017) as alternative utterance representation methods, but they did not show better performance on our task.\nlated as follows:\nae = σ (u · ve) ,\nwhere u is the utterance vector, ve is the enablement vector of enabled domain e, and σ is sigmoid function. Compared to conventional attention mechanism using softmax function, which constraints the sum of the attention weights to be 1, sigmoid attention has more expressive power, where each attention weight can be between 0 and 1 regardless of the other weights. We show that using sigmoid attention is actually more effective for improving prediction performance in Section 3.\nThe utterance vector and the weighted sum of the domain enablement vectors are concatenated to represent the utterance and the domain enablement as a single vector. Given the concatenated vector, a feed-forward neural network with a single hidden layer3 is used to predict the confidence score by logistic sigmoid function for each domain.\nOne issue of the proposed architecture is that the domain enablement can be trained to be a very strong signal, where one of the enabled domains would be the predicted domains regardless of the relevancy of the utterances to the predicted domains in many cases. To reduce this prediction bias, we use randomly sampled enabled domains\n3We utilize scaled exponential linear units (SeLU) as the activation function for the hidden layer(Klambauer et al., 2017).\ninstead of the correct enabled domains of an input utterance with 50% probability during training so that the domain enablement is used as an auxiliary signal rather than determining signal. During inference, we always use the correct domain enablements of the given utterances.\nThe main loss function of our model is formulated as binary log loss between the confidence score and the ground-truth vector as follows:\nLm = − n∑\ni=1\nyi log oi + (1− yi) log (1− oi) ,\nwhere n is the number of all domains, o is an n-dimensional confidence score vector from the model, and y is an n-dimensional one-hot vector whose element corresponding to the position of the ground-truth domain is set to 1."
  }, {
    "heading": "2.1 Supervised Enablement Attention",
    "text": "Attention weights are originally intended to be automatically trained in an end-to-end fashion (Bahdanau et al., 2015), but it has been shown that applying proper explicit supervision to the attention improves the downstream tasks such as machine translation given the word alignment and constituent parsing given annotations between surface words and nonterminals (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017).\nWe hypothesize that if the ground-truth domain is one of the enabled domains, the attention weight for the ground-truth domain should be high and vice versa. To apply this hypothesis in the model training as a supervised attention method, we formulate an auxiliary loss function as follows:\nLa = − ∑ e∈E ye log ae + (1− ye) log (1− ae) ,\nwhere E is a set of enabled domains and ae is the attention weight for the enabled domain e."
  }, {
    "heading": "2.2 Self-Distilled Attention",
    "text": "One issue of supervised attention in Section 2.1 is that enabled domains that are not ground-truth domains are encouraged to have lower attention weights regardless of their relevancies to the input utterances and the ground-truth domains. Distillation methods utilize not only the ground-truth but also all the output activations of a source model so that all the prediction information from the source model can be utilized for more effective knowledge transfer between the source model\nand the target model (Hinton et al., 2014). Selfdistillation, which trains a model leveraging the outputs of the source model with the same architecture or capacity, has been shown to improve the target model’s performance with a distillation method (Furlanello et al., 2018).\nWe use a variant of self-distillation methods, where the model outputs at the previous epoch with the best dev set performance are used as the soft targets for the distillation,4 so that the enabled domains that are not ground-truths can also be used for the supervised attention. While conventional distillation methods utilize softmax activations as the target values, we show that distillation on top of sigmoid activations is also effective without loss of generality. The loss function for the self-distillation on the attention weights is formulated as follows:\nLd = − ∑ e∈E ãe log ae + (1− ãe) log (1− ae) ,\nwhere ãe is the attention weight of the model showing the dev set performance in the previous epochs. It is formulated as:\nãe = σ (u · ve\nT\n) ,\nwhere T is the temperature for sufficient usage of all the attention weights as the soft target. In this work, we set T to be 16, which shows the best dev set performance.\nWe have also evaluated soft-target regularization (Aghajanyan, 2017), where a weighted sum of the hard ground-truth target vector and the soft target vector is used as a single target vector, but it did not show better performance than selfdistillation.\nAll the described loss functions are added to compose a single loss function as follows:\nL = Lm + α { (1− β)La + βtLd } ,\nwhere α is a coefficient representing the degree of supervised enablement attention and βt denotes the degree of the self-distillation. We set α to be 0.01 in this work. Following Hu et al. (2016), βt = 1 − 0.95t, where t denotes the current training epoch starting from 0 so that the hard ground-truth targets are more influential in the early epochs and the self-distillation is more utilized in the late epochs.\n4This approach is closely related to Temporal Ensembling"
  }, {
    "heading": "3 Experiments",
    "text": "We evaluate our proposed model on domain classification leveraging enabled domains. The enabled domains can be a crucial disambiguating signal especially when there are multiple similar domains. For example, assume that the input utterance is “what’s the weather” and there are multiple weather-related domains such as NewYorkWeather, AccuWeather, and WeatherChannel. In this case, if WeatherChannel is included as an enabled domain of the current user, it is likely to be the most relevant domain to the user."
  }, {
    "heading": "3.1 Datasets",
    "text": "Following the data collection methods used in Kim et al. (2018b), our models are trained using utterances with explicit invocation patterns. For example, given a user’s utterance, “Ask {ZooKeeper} to {play peacock sound},” “play peacock sound” and ZooKeeper are extracted to compose a pair of the utterance and the groundtruth, respectively. In this way, we have generated train, development, and test sets containing 4.4M, 500K, and 500K utterances, respectively. All the utterances are from the usage log of Amazon Alexa and the ground-truth of each utterance is one of 1K frequently used domains. The average number of enabled domains per utterance in the test sets is 8.47.\nOne issue of this collected data sets is that the\n(Laine and Aila, 2017), but we just leverage the model outputs at the previous epoch rather than accumulating the outputs over multiple epochs.\nground-truth is included in the enabled domains for more than 90% of the utterances, where the ground-truths are biased to enabled domains.5 For more correct and unbiased evaluation of the models on the input utterances from real live traffic, we also evaluate the models on the same sized train, development, and test sets where the utterances are sampled to set the ratio of ground-truth inclusion in enabled domains to be 70%, which is closer to the ratio for actual input traffic."
  }, {
    "heading": "3.2 Results",
    "text": "Table 1 shows the accuracies of our proposed models on the two test sets. We also show mean reciprocal rank (MRR) and top-3, accuracy6 which is meaningful when utilizing post reranker, but we do not cover reranking issues in this paper (Robichaud et al., 2014; Kim et al., 2018a).\nFrom Table 1, we can first see that changing softmax attention to sigmoid attention significantly improves the performance. This means that having more expressive power for the domain enablement information by relaxing the softmax constraint is effective in terms of leveraging the domain enablement information for domain classification. Along with sigmoid attention, supervised attention leveraging ground-truth slightly improves the performance, and supervised attention combined with self-distillation shows significant performance improvement. It demon-\n5Since the data collection method leverages utterances where users already know the exact domain names, such domains are likely to be the enabled domains of the users.\n6Top-3 accuracy is calculated as # (utterances one of whose top three predictions is a ground-truth) / # (total utterances).\nstrates that supervised domain enablement attention leveraging ground-truth enabled domains is helpful, and utilizing attention information from other enabled domains is synergistic.\nKim et al. (2018b)’s model also adds a domain enablement bias vector to the final output, which is helpful when the ground-truth domain is one of the enabled domains. Such models (5) and (6) also show good performance for the test set where the ground-truth is one of the enabled domains with more than 90% probability. However, for the unbiased test set where the ground-truth is included in the enabled domains with a smaller probability, not adding the bias vector is shown to be better overall.\nTable 2 shows sample utterances correctly predicted with model (4) but not with model (1) and (2). For the first two utterances, the groundtruths are included in the enabled domains, but there were only hundreds or fewer training instances whose ground-truths are CryptoPrice or Expedia. In these cases, we can see that model (1) attends to unrelated domains, model (2) attends to none of the enabled domains, but model (4), which uses supervised attention, is shown to attend to the ground-truth even without many training examples. “find my phone” has a single enabled domain which is not a ground-truth. In this case, model (1) still fully attends to the unrelated domain because of softmax attention while model (2) and (4) do not highly attend to it so that the unrelated enabled domain is not impactive."
  }, {
    "heading": "3.3 Implementation Details",
    "text": "The word vectors are initialized with off-the-shelf GloVe vectors (Pennington et al., 2014), and all the other model parameters are initialized with Xavier initialization (Glorot and Bengio, 2010). Each model is trained for 25 epochs and the parameters showing the best performance on the development set are chosen as the model parameters. We use ADAM (Kingma and Ba, 2015) for the optimization with the initial learning rate 0.0002 and the mini-batch size 128. We use gradient clipping, where the threshold is set to 5. We use a variant of LSTM, where the input gate and the forget gate are coupled and peephole connections are used (Gers and Schmidhuber, 2000; Greff et al., 2017). We also use variational dropout for the LSTM regularization (Gal and Ghahramani, 2016). All the models are implemented with DyNet (Neubig et al.,\n2017)."
  }, {
    "heading": "4 Conclusion",
    "text": "We have introduced a novel domain enablement attention mechanism improving domain classification performance utilizing domain enablement information more effectively. The proposed attention mechanism uses sigmoid attentions for more expressive power of the attention weights, supervised attention leveraging ground-truth information for explicit guidance of the attention weight training, and self-distillation for the attention supervision leveraging enabled domains that are not ground truth domains. Evaluating on utterances from real usage in a large-scale IPDA, we have demonstrated that our proposed model significantly improves domain classification performance by better utilizing domain enablement information."
  }],
  "year": 2018,
  "references": [{
    "title": "Softtarget regularization: An effective technique to reduce over-fitting in neural networks",
    "authors": ["Armen Aghajanyan."],
    "venue": "IEEE Conference on Cybernetics (CYBCONF).",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes"],
    "venue": "In EMNLP,",
    "year": 2017
  }, {
    "title": "Born Again Neural Networks",
    "authors": ["Tommaso Furlanello", "Zachary C. Lipton", "Michael Tschannen", "Laurent Itti", "Anima Anandkumar."],
    "venue": "International Conference on Machine Learning (ICML), pages 1602–1611.",
    "year": 2018
  }, {
    "title": "A theoretically grounded application of dropout in recurrent neural networks",
    "authors": ["Yarin Gal", "Zoubin Ghahramani."],
    "venue": "Advances in Neural Information Processing Systems 29 (NIPS), pages 1019–1027.",
    "year": 2016
  }, {
    "title": "Recurrent Nets that Time and Count",
    "authors": ["Felix A. Gers", "Jürgen Schmidhuber."],
    "venue": "IJCNN, volume 3, pages 189–194.",
    "year": 2000
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 249–256.",
    "year": 2010
  }, {
    "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "LSTM: A search space odyssey",
    "authors": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutnı́k", "Bas R. Steunebrink", "Jürgen Schmidhuber"],
    "venue": "Transactions on Neural Network Learning and Systems (TNNLS),",
    "year": 2017
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."],
    "venue": "NIPS 2014 Deep Learning Workshop.",
    "year": 2014
  }, {
    "title": "Harnessing Deep Neural Networks with Logic Rules",
    "authors": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing."],
    "venue": "ACL, pages 2410–2420.",
    "year": 2016
  }, {
    "title": "Supervised attention for sequence-to-sequence constituency parsing",
    "authors": ["Hidetaka Kamigaito", "Katsuhiko Hayashi", "Tsutomu Hirao", "Masaaki Nagata."],
    "venue": "International Joint Conference on Natural Language Processing (IJCNLP).",
    "year": 2017
  }, {
    "title": "Joint learning of domain classification and out-of-domain detection with dynamic class weighting for satisficing false acceptance rates",
    "authors": ["Joo-Kyung Kim", "Young-Bum Kim."],
    "venue": "Interspeech.",
    "year": 2018
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1292–1302.",
    "year": 2014
  }, {
    "title": "Structured Attention Networks",
    "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "A scalable neural shortlisting-reranking approach for large-scale domain classification in natural language understanding",
    "authors": ["Young-Bum Kim", "Dongchan Kim", "Joo-Kyung Kim", "Ruhi Sarikaya."],
    "venue": "NAACL, pages 16–24.",
    "year": 2018
  }, {
    "title": "Efficient Large-Scale Neural Domain Classification with Personalized Attention",
    "authors": ["Young-Bum Kim", "Dongchan Kim", "Anjishnu Kumar", "Ruhi Sarikaya."],
    "venue": "ACL, pages 2214–2224.",
    "year": 2018
  }, {
    "title": "ADAM: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Lei Ba."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Self-normalizing neural networks",
    "authors": ["Günter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter."],
    "venue": "Advances in Neural Information Processing Systems 30 (NIPS), pages 972–981.",
    "year": 2017
  }, {
    "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding",
    "authors": ["Agnika Kumar."],
    "venue": "NIPS Workshop on Conversational AI.",
    "year": 2017
  }, {
    "title": "Temporal ensembling for semi-supervised learning",
    "authors": ["Samuli Laine", "Timo Aila."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Neural machine translation with supervised attention",
    "authors": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita."],
    "venue": "COLING, pages 3093– –3102.",
    "year": 2016
  }, {
    "title": "Effective Approaches to Attentionbased Neural Machine Translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification",
    "authors": ["André F.T. Martins", "Ramón Fernandez Astudillo."],
    "venue": "International Conference on Machine Learning (ICML), pages 1614–1623.",
    "year": 2016
  }, {
    "title": "Supervised Attentions for Neural Machine Translation",
    "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."],
    "venue": "EMNLP, pages 2283–2288.",
    "year": 2016
  }, {
    "title": "DyNet: The Dynamic Neural Network Toolkit",
    "authors": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"],
    "year": 2017
  }, {
    "title": "GloVe: Global Vectors for Word Representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "EMNLP, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Hypotheses ranking for robust domain classification and tracking in dialogue systems",
    "authors": ["Jean-Philippe Robichaud", "Paul A. Crook", "Puyang Xu", "Omar Zia Khan", "Ruhi Sarikaya."],
    "venue": "Interspeech, pages 145–149.",
    "year": 2014
  }, {
    "title": "The technology behind personal digital assistants: An overview of the system architecture and key components",
    "authors": ["Ruhi Sarikaya."],
    "venue": "IEEE Signal Processing Magazine, 34(1):67–81.",
    "year": 2017
  }, {
    "title": "An overview of endto-end language understanding and dialog",
    "authors": ["Ruhi Sarikaya", "Paul A Crook", "Alex Marin", "Minwoo Jeong", "Jean-Philippe Robichaud", "Asli Celikyilmaz", "Young-Bum Kim", "Alexandre Rochette", "Omar Zia Khan", "Xiaohu Liu"],
    "year": 2016
  }, {
    "title": "Spoken Language Understanding: Systems for Extracting Semantic Information from Speech",
    "authors": ["Gokhan Tur", "Renato de Mori."],
    "venue": "New York, NY: John Wiley and Sons.",
    "year": 2011
  }],
  "id": "SP:f453874d971db1a857c314eefdfd027e8933d5f5",
  "authors": [{
    "name": "Joo-Kyung Kim",
    "affiliations": []
  }, {
    "name": "Young-Bum Kim",
    "affiliations": []
  }],
  "abstractText": "In large-scale domain classification for natural language understanding, leveraging each user’s domain enablement information, which refers to the preferred or authenticated domains by the user, with attention mechanism has been shown to improve the overall domain classification performance. In this paper, we propose a supervised enablement attention mechanism, which utilizes sigmoid activation for the attention weighting so that the attention can be computed with more expressive power without the weight sum constraint of softmax attention. The attention weights are explicitly encouraged to be similar to the corresponding elements of the ground-truth’s one-hot vector by supervised attention, and the attention information of the other enabled domains is leveraged through self-distillation. By evaluating on the actual utterances from a large-scale IPDA, we show that our approach significantly improves domain classification performance.",
  "title": "Supervised Domain Enablement Attention for Personalized Domain Classification"
}