{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider high-dimensional statistical models where the ambient dimension p is much larger than the number of observations n. Under such high-dimensional scaling, it is still possible to obtain consistent estimators by imposing low-dimensional structural constraints upon the statistical models, such as sparsity (e.g. in compressed sens-\n1School of Computing, KAIST, Daejeon, South Korea 2AItrics, Seoul, South Korea 3IBM T.J. Watson Research Center, Yorktown Heights, NY, USA. Correspondence to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano <aclozano@us.ibm.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ning (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), lowrank structure (Recht et al., 2007; Negahban & Wainwright, 2010), sparse graphical model structure (Friedman et al., 2007; Ravikumar et al., 2008), and sparse additive structure for non-parametric models (Ravikumar et al., 2009). A widely used approach to structured learning is via specific regularization functions. For instance, `1-regularization is employed for sparse models (Tibshirani, 1996), `1/`q norms for group sparsity (Yuan & Lin, 2006), and nuclear norm for low-rank matrix-structured models (Candès & Tao, 2010). Much attention has been devoted to the study of these structured norms and their theoretical properties.\nSuch a “clean” regularization approach, however, might be too stringent in practice. For instance in linear regression, a blend of element-wise sparsity and group-sparsity might be more appropriate than a purely sparse or purely groupsparse solution. In multitask learning, while some parameters might be shared across tasks, others might only be relevant to a subset of tasks or a single task. To overcome this limitation, a line of work on so-called dirty models has emerged, which addresses this caveat by “mixing and matching” different structures. One basic approach consists in decomposing the model parameters as a sum of two components, each penalized separately: one component captures the common structure across tasks and the other task-specific characteristics (Jalali et al., 2010; Gong et al., 2012). For instance the dirty model in Jalali et al. (2010) employs `1,1 and `1,∞ regularizers to the two components. Chandrasekaran et al. (2011) consider the problem of recovering unknown low-rank and sparse matrices, given the sum of their sum, with application such as optical imaging systems. Robust principal component analysis and related extensions (Candès et al., 2011; Agarwal et al., 2012; Hsu et al., 2011) estimate a covariance matrix that is the sum of a low-rank matrix and a structured (e.g. sparse, column sparse) matrix.\nA general framework for studying dirty models was recently proposed in Yang & Ravikumar (2013), which bridges and extends several analyses for specific pairs of superposition structures and specific statistical models (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011); Candès et al. (2011); Agarwal et al. (2012); Hsu et al.\n(2011)) Specifically, this framework applies to a general class of M -estimators employing a so-called hybrid regularization function, which is the infimal convolution of weighted regularization functions, one for each structural component. This formulation is equivalent to an M - estimator that combines a loss function applied to the sum of multiple parameter vectors (one per structural component) and a weighted sum of regularization functions (one per parameter vector).\nFor the sparse + group sparse decomposition, however existing analyses are highly problematic. The key weakness is that they require some form of structural incoherence condition which captures the interaction between the different structured components. While such a structural incoherence is a reasonable assumption for e.g. sparse + low rank superposition, it is what too stringent for the sparse+group sparse case because the two structures are completely coherent for this case! This yields a key motivating question for this paper: Under the sparse + group sparse setting, can we bypass structural incoherence conditions and yet obtain tight error bounds?\nIn this paper we provide a positive answer by developing a novel proof technique. Prior analyses require ‘local’ restricted strong convexity conditions (RSC): one condition for the sparse component and one for the group sparse component. The use of structural incoherence between sparse and group sparse components in then needed to show ‘global’ RSC for the vector concatenating sparse and group sparse components. To avoid the need for structural incoherence, we use RSC in the summed space directly (namely for the summed sparse + group-sparse structure). However, this brings in a new issue: in this case, the dirty regularizer for the parameter vector is not decomposable. To circumvent this issue, our key ingredient is to introduce “surrogate” sparse and group sparse components depending on our estimators such that i) their sum equals the sum of the true parameter components and ii) corresponding error vectors are decomposable even though the regularizer itself is not decomposable. Using the decomposability of error vectors, we are then able to show `2 consistency for general loss functions.\nAs an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to non-convex regularizers, and show their `∞ consistency. Interestingly, these models are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models. In particular, our `∞ consistency results require neither incoherence in the loss function nor structural incoherence between sparse and group sparse parameters. We illustrate the practical impact of this superior theoretical results with simulation experiments.\nThe remainder of this paper is organized as follows. In Section 2 we review sparse+group-sparse dirty models with convex penalties and introduce their non-convex counterparts. In Section 3 we discuss the incoherence assumption required by prior analyses and explain why such an assumption is unreasonable. Section 4 introduces the key ingredient of our novel proof technique. Section 5 presents the convergence bounds for models with convex penalties. Those for non-convex penalties are stated in Section 6. Finally, simulation experiments are provided in Section 7 to illustrate the remarkable practical advantage of non-convex penalties, agreeing with their superior convergence rates."
  }, {
    "heading": "2. Sparse + Group-Sparse Dirty Models: Setup and Formulations",
    "text": "Consider a data collection Z = {Z1, . . . , Zn}, where each element is drawn independently from distribution P, and a loss function L(· ;Z) : Ω → R where L(θ ;Z) measures the goodness of fit of parameter θ ∈ Ω to the given data collection Z. Typically Ω = Rp (parameters are vectors) or Rp×r (parameters are matrices). Assume there are some known groups G = {G1, . . . , Gq} that partition the parameter index set: Gi ∩Gj = φ and ∪qg=1Gg = {1, . . . , p}.\nWe aim at recovering parameter θ∗ which is the unique minimizer of the population risk: θ∗ := argminθ∈Ω EZ [L(θ;Z)] in cases where\nθ∗ = α∗ + β∗, (1)\nwhere α∗ is a sparse component and β∗ is a group-sparse component obeying the group structure G. For that purpose, we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity. We consider both convex and non-convex regularizers as follows."
  }, {
    "heading": "2.1. Dirty models with convex regularizers",
    "text": "We focus on regularized M -estimators of the form\nminimize α,β\nL(α+ β;Z) + λ1‖α‖1 + λ2‖β‖1,a, (2)\nwhere the loss function L(· ;Z) is possibly nonconvex. Here, given known parameter groups G = {G1, G2, . . . , Gq}, the group regularizer is defined as ‖β‖1,a := ∑q t=1 ‖βGt‖a for a ≥ 2, where βGt denotes the parameter subset in group Gt. The constant a determines how the elements within each group are combined.\nWe provide examples for the popular settings of linear regression and inverse covariance estimation.\nLinear regression. Consider the standard linear model y = Xθ∗ +w where y ∈ Rn is the observation vector, θ∗\nis the true regression parameter which is the sum of sparse α∗ and group sparse β∗, X ∈ Rn×p is the design matrix, and w ∈ Rn is the observation noise. The “dirty” regularized least squares solves\nminimize α,β∈Rp\n1\n2n ‖y −X\n( α+ β)‖22 + λ1‖α‖1 + λ2‖β‖1,a\n(3)\nwhere groups are defined within a (single) parameter vector space via β. The formulation can be seamlessly extended to cover the dirty multitask learning setting of Jalali et al. (2010):\nminimize α,β∈Rp×m m∑ k=1 1 2n ∥∥y(k) −X(k)([α+ β](·,k))∥∥22 (4) +λ1‖α‖1 + λ2‖β‖1,∞\nwhere we have m related tasks in columns: α,β ∈ Rp×m, and the groups can be defined across tasks in rows. E.g. for predictor j, β(j,1), . . . ,β(j,m) belong to the same group. Here, [α + β](·,k) indicates k-th column of matrix input α+ β.\nGraphical Model Estimation. Another key example is a modified graphical Lasso where the goal is to estimate the structure of the underlying graphs representing conditional independences across variables. Assume that there are some known set of edge groups and that the true parameter Θ∗ has only a small number of active edge groups plus some individual edges. To recover Θ∗ we solve\nminimize S+B 0\ntrace ( (S +B) Σ̂ ) − log det(S +B) (5)\n+λ1‖S‖1 + λ2‖B‖1,a\nwhere Σ̂ is the sample covariance matrix and regularizers are applied to off-diagonal entries of S and B. As done in (4) for the linear model, the formulation (5) can be seamlessly extended to the multitask setting where we wish to estimate multiple precision matrices jointly, encouraging similar structure while allowing for some discrepancy across them. This estimator is discussed in Hara & Washio (2013).\nEquivalent Program. As shown in Yang & Ravikumar (2013), the formulation (2) can be rewritten as:\nminimize θ\nL(θ;Z) + ‖θ‖λ (6)\nwhere ‖θ‖λ is the infimal convolution of two regularizers\n‖θ‖λ := inf α,β\n{ λ1‖α‖1 + λ2‖β‖1,a : α+ β = θ } . (7)\nIt is known that ‖ · ‖λ is a norm and its dual is defined as ‖θ‖∗λ := max{‖θ‖∞/λ1, ‖θ‖∞,a∗/λ2} where 1/a + 1/a∗ = 1 so that ‖ · ‖∞,a∗ is the dual norm of ‖ · ‖1,a (see Yang & Ravikumar (2013) for details)."
  }, {
    "heading": "2.2. Dirty models with non-convex regularizers",
    "text": "In this paper, we introduce and study estimators of the form\nminimize α,β\nL(α+ β) + ρλ1 ( α ) + φλ2,a ( β). (8)\nHere ρλ1(·) is any regularizer inducing sparsity beyond the `1-norm (note that the notation encapsulates the regularization parameter λ1 itself within the regularizer) satisfying the following conditions (Loh & Wainwright, 2014):\n(C1) ρλ1(0) = 0 and is symmetric. For t > 0, ρλ1(t) is non-decreasing but ρλ1(t)/t is non-increasing in t. Besides, ρλ1(t) is differentiable for t 6= 0 with limt→0+ ρ ′ λ1 (t) = λ1, and is ρλ1(t) + µ 2 t\n2 is convex for some µ > 0.\n(C2) There exists some scalar γ ∈ (0,∞) such that ρ′λ1(t) = 0 when t ≥ γλ1.\nFollowing the notation of Loh & Wainwright (2014), we call ρλ1(·) µ-amenable if it satisfies (C1) and (µ, γ)amenable if it additionally satisfies (C2). The popular non-convex regularizers SCAD (Fan & Li, 2001) and MCP (Zhang, 2010) are both (µ, γ)-amenable (Loh & Wainwright, 2014).\nThe regularizer φλ2,a(·) a non-convex counterpart of the group regularizer λ2‖ · ‖1,a employed in (2) where we use ρλ2(·) instead of λ2‖ · ‖1, over groups:\nφλ2,a(β) := ρλ2(G(β))\nwhere G(β) := (‖βG1‖a, . . . , ‖βGq‖a)>. Example of non-convex regularizers include the Group-SCAD and Group-MCP penalties where SCAD and MCP penalties are respectively used on the norm of each group.\nRemarkably, the proof techniques developed in this paper make it possible to provide not only `2-error bounds under milder conditions than prior work on convex problem (2), but also support set recovery guarantees for non-convex one (8). In fact we shall see that dirty models with nonconvex regularizers (8) enjoy strictly better statistical guarantees than their convex counterpart (2), with practical consequences."
  }, {
    "heading": "3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",
    "text": "As our starting point, we focus on the case of convex dirty models in (2) or equivalently in (6). A key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer (Negahban et al., 2012). However, considering the form of regularizer in (6), it is not obvious to find the model space and its orthogonal complement with which we could directly derive\nerror bounds with optimal rates. To circumvent this problem, Yang & Ravikumar (2013) utilize the decomposability of each component separately, but this requires restricted strong convexity (RSC) to hold jointly for all component parameters. In order to have the “joint” RSC property from “local” RSC with respect to each individual component, Yang & Ravikumar (2013) assume a structural incoherence condition. Even if the loss function is strongly convex with respect to each component, such incoherence across components is essential for the joint RSC due to the linearity across components. To see this more clearly, suppose we have the function z2 for z ∈ R, which is obviously strongly convex. If we assume, however, that z is the sum of two components x, y ∈ R, then one can immediately see that (x + y)2 is not strongly convex jointly in x and y because x and y are completely coherent in this one dimensional example.\nThe problem is that the structural incoherence condition for the `1+`1,a setting is way too restrictive because the sparse and the group-sparse structures essentially share the same model and its orthogonal spaces1. In order to see this, we consider the popular linear model setting (3) for example. Let s∗ (and b∗) be the support set of true sparse (groupsparse) component and sc be its complement. Furthermore, [ 1nX\n>X](sc∩b∗) denotes the projection of the sample covariance onto sc ∩ b∗-coordinate space (j-th coordinate becomes zero if j /∈ sc ∩ b∗). Projections on other spaces are defined similarly. Then, the structural incoherence condition for joint RSC can be reduced as: for all (s, b) ∈ {(sc, b∗), (s∗, bc), (sc, bc)},\nσmax ( [ 1nX >X](s∩b) ) ≤ Cκ1 (9)\nwhere σmax(·) is the maximum singular value of a matrix, κ1 is the curvature of (restricted) eigenvalue condition, and C is some fixed constant. Informally, this condition requires the maximum singular value of sample covariance (modulo the projection onto the true model and its orthogonal space) to be smaller than its minimum singular value (Note that for linear models, the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance). This condition can be easily shown to fail in many cases. For instance consider the popular setting where the design matrix X is a set of samples from Gaussian ensemble with covariance Σ, and the true parameter is the sum of group sparse + a single nonzero component as depicted in Figure 1. Then, the incoherence condition in (9) implies maxi,j |[ 1nX\n>X]ij | ≤ 1/128σmin(Σ), which can be easily violated in many natural setting of Σ because the minimum eigenvalue of Σ is smaller than the maximum element of Σ\n1Note that the sparse + group sparse setting is outstanding. The structural incohence assumption makes sense in other dirty models settings, e.g. sparse + low rank dirty models.\nby the Rayleigh quotient.\nThis naturally leads to the following question:\nCan we provide tight error bounds for the problem (2) not requiring the joint RSC across individual structures and hence bypassing the incoherence condition?"
  }, {
    "heading": "4. Our key strategy: Constructing surrogate components that are always decomposable.",
    "text": "In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components dependent of our estimation. Consider arbitrary target parameter θ∗ such that θ∗ = α∗ + β∗. Note that we do not impose additional constraints on defining the sparse component α∗ and the group sparse component β∗, hence the possible combination of (α∗,β∗) is not unique. As we will see later, we provide estimation error bounds that depend on the selection of (α∗,β∗)–more precisely on the sparsity level of α∗ and the group sparsity level of β∗. In that sense our theorems provide sets of estimation bounds. However, it is important to note that we still do not need to worry about the identifiability between structures, because we only care about the `2 and `∞ error rates of the final or “summed” estimator (we do not recover (nor care about) the individual components).\nSuppose we compute θ̃ from the program (6) where α̃ and β̃ are minimizing its dirty regularizer (7). Then, rather than directly deriving error bounds of θ̃ − θ∗ from α̃−α∗ and β̃ − β∗, which are not decomposable, we introduce an additional set of vectors, ᾱ, β̄ and θ̄ from the following rules:\n1. If α∗j = β ∗ j = 0, then ᾱj = β̄j := 0.\n2. If α∗j 6= 0 and β∗j = 0, then β̄j := β̃j , and ᾱj := θ∗j − β̃j .\n3. If β∗j 6= 0, then ᾱj := α̃j and β̄j := θ∗j − α̃j .\n4. θ̄ is defined as the sum of ᾱ and β̄.\nSparse + Group Sparse Dirty Models\n3 1 1 1 2 1 0 0 1 0 1 0 0 0 0\nTheta^*\n(a) θ∗\n0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\na^*\n(b) α∗\n3 1 1 1 2 1 0 0 0 0 0 0 0 0 0\nb^*\n(c) β∗\n1.9 0 0.2 0 1.1 0 0.1 0 1 0 0.9 0.2 0.1 0 0\na_hat\n(d) α̃\n0.8 0.8 0.8 1.1 1.1 1.1 0.2 -0.2 0.2 0 0 0 0 0 0\nb_hat\n(e) β̃\n1.9 0 0.2 0 1.1 0 0.1 0 0 0 -0.1 0.2 0.1 0 0\nDel^*\n(f) α̃−α∗\n-2.2 -0.2 -0.2 0.1 -0.9 0.1 0.2 -0.2 0.2 0 0 0 0 0 0\nGam^*\n(g) β̃ − β∗\n1.9 0 0.2 0 1.1 0 0 0 0.8 0 1 0 0 0 0\na_bar\n(h) ᾱ"
  }, {
    "heading": "1.1 1 0.8",
    "text": "1 0.9 1 0 0 0.2 0 0 0 0 0 0\nb_bar\n(i) β̄\n0 0 0 0 0 0 0.1 0 0.2 0 -0.1 0.2 0.1 0 0\nDel^bar\n(j) α̃− ᾱ\n-0.3 -0.2 0 0.1 0.2 0.1 0.2 -0.2 0 0 0 0 0 0 0\nGam^bar\n(k) β̃ − β̄\nFigure 2. Example of constructing surrogate target parameters given (b) α∗, (c) β∗, (d) α̃ and (e) β̃ via transformation T (·). Error vectors based on surrogates are decomposable (see text for details).\nBy construction, θ̄ is same as θ∗, but ᾱ has different sparsity pattern and values fromα∗, depending on α̃. The same holds for β̄ as well. We denote the above transformation as (ᾱ, β̄) := T (α∗,β∗; α̃, β̃).\nIt turns out that the error vectors computed based on the surrogate ᾱ and β̄ are always decomposable as described in the following proposition, and the consequence of this decomposability plays a key role for showing i) `2-error bounds without incoherence condition and ii) support set recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).\nProposition 1. Consider any local optimum θ̃ of convex or non-convex dirty models, and corresponding θ̄ := T (α∗,β∗; α̃, β̃). Then, the error vectors for individual components, ∆ := α̃ − ᾱ and Γ := β̃ − β̄, are decomposable in the sense that\n∣∣[∆ + Γ]j∣∣ = |∆j |+ |Γj | for all j, and the overall `2 error ‖θ̃ − θ∗‖2 is lower bounded as follows:\n‖θ̃ − θ∗‖22 ≥ ‖∆‖22 + ‖Γ‖22 . (10)\nMoreover, let s∗ := supp(α∗) (the support set ofα∗), s̄ := supp(α∗) ∪ supp(ᾱ) and U := supp(α∗) ∪ supp(β∗). Similarly, we also define b∗ := supp(β∗) and b̄ := supp(β̄). Then, by construction of T , s∗ ⊆ s̄ ⊆ U and b∗ ⊆ b̄ ⊆ U . However, it is always guaranteed that\n∆s∗ = ∆s̄ = ∆U and Γb∗ = Γb̄ = ΓU (11)\nwhere ∆s∗ represents the projection of ∆ onto the s∗coordinate space; that is, [∆s∗ ]j is ∆j if j ∈ s∗, and 0 otherwise.\nIllustrative example. Figure 2 describes an example: consider a 5× 3 matrix parameter with 5 known groups in rows. Suppose (i) the target parameter is given by (a), (ii) we define (b) and (c) as the sparse and group sparse components of θ∗, and (iii) the minimizer of program (6) are computed as in (d) and (e), respectively for α̃ and β̃. Then, (f) and (g) show the error vectors for sparse and group-sparse components, which are not decomposable ((10) does not hold for (f) and (g)). On the other hand, for ᾱ in (h) and β̄ in (i) derived from T (·), we can verify that surrogate error vectors (shown in (j) and (k)) are decomposable; at every position, α̃ − ᾱ and β̃ − β̄ are sign consistent (or at least one of them is zero)."
  }, {
    "heading": "5. Statistical Guarantees of Models with Convex Regularizers",
    "text": "Throughout our analysis, we assume that the loss function L(·) is twice differentiable and and satisfies the restricted strong convexity condition\n(RSC) For any vector θ1,θ2 ∈ Rp, the loss function L(·) satisfies 〈\n∇L(θ1 + θ2)−∇L(θ1),θ2 〉\n≥ { κ1‖θ2‖22 − τ1‖θ2‖2η , for all ‖θ2‖2 ≤ 1 , (12) κ2‖θ2‖2 − τ2‖θ2‖η , for all ‖θ2‖2 ≥ 1 . (13)\nRSC of the loss is also used to guarantee `2-consistency (Negahban et al., 2012; Loh & Wainwright, 2015) or `∞- consistency (Loh & Wainwright, 2014) of “clean” structurally constrained problems (i.e. problems with a single\nstructure). Note that there are slight variations in the definition of RSC conditions in the literature. Here we adopt the form with tolerance terms in Loh & Wainwright (2014; 2015), to allow for a wide class of non quadratic and/or non-convex loss functions. We will show that RSC with tolerance in dirty norm holds with high probability under the popular setting of Gaussian ensembles, as an example.\nFor the analysis, we consider a slight modification of the program (6):\nminimize ‖θ‖η≤r\nL(θ) + ‖θ‖λ (14)\nwhere L is possibly non-convex, but satisfies (RSC). The additional constraint ‖θ‖η ≤ r also involves the dirty norm (7) but with a different parameter vector η. This constraint is a safety radius commonly used for analyzing non-convex problems to ensure that the global minimum exists (see e.g. Loh & Wainwright (2014; 2015)). In practice, we can disregard this additional constraint.\nTheorem 1. Consider the dirty model for problem (14) where L(·) is possibly non-convex but satisfies the restricted strong convexity (RSC). Suppose that θ∗ is feasible and the regularization parameters are set so that λ1 ≥ 4‖∇L(θ∗)‖∞ and λ2 ≥ 4‖∇L(θ∗)‖∞,a∗ . Suppose furthermore that r ≤ min { κ2 4τ2 , κ25 max{λ1/η1,λ2/η2} , λ1 8τ1η1 , λ28τ1η2 } . Then, any local optimum θ̃ of (14) is guaranteed to be `2 consistent:∥∥θ̃ − θ∗∥∥ 2 ≤ 3 κ1 max { λ1 √ s , λ2 √ sG } (15)\nwhere s is the number of nonzero elements in α∗ and sG is the number of nonzero groups in β∗.\nRemarks. The error bound in (15) scales with (n, p, s, sG) at the same rate as previous analysis (Yang & Ravikumar, 2013) for the sparse plus group sparse setting, which required a much stringent incoherence condition, as we already discussed. It is also instructive to note that Theorem 1 holds for any combination of (α∗,β∗) such that α∗ + β∗ = θ∗, but different views of (α∗,β∗) constructing θ∗ give different bounds depending on sparsity/group sparsity levels of (α∗,β∗) (i.e. s and sG). In this sense, Theorem 1 provides a set of `2 estimation upper bounds.\nLinear model and modified graphical Lasso. In the following corollaries, we apply Theorem 1 to the linear model (3) and the modified graphical Lasso problem (5) and derive their corresponding `2 estimation bounds.\nCorollary 1. Consider the linear model (3). Assume that (i) each row Xi of the observation matrix X is independently sampled fromN(0,Σ), (ii)X is (group) column normalized by scaling as in (Negahban et al., 2012), and (iii)\nw is independent sub-Gaussian with parameter σ. Now suppose that in (14) we set a := 2 (where a is the parameter for the group norm both for ‖θ‖η and ‖θ‖λ), r constant (r only depends on Σ and σ), λ1 = η1 := 8σ √ log p/n\nand λ2 = η2 := 8σ( √ m/n+ √ log q/n) for q groups and maximum group size m (maxg=1,...,q |Gg|). Suppose that θ∗ is feasible to program (14) with these settings. Then with probability at least 1− c1 exp(−c2nλ2s)− c3/q2, any local optimum θ̃ satisfies\n‖θ̃ − θ∗‖2 ≤ 24σ\nκ1 max\n{√ s log p\nn ,\n√ sGm\nn +\n√ sG log q\nn\n} .\nwhere κ1 is some constant depending on Σ.\nCorollary 2. Consider the modified graphical Lasso (5) to estimate inverse covariance Θ∗. Suppose we set the parameters of (14) as λ1 = η1 := 4 maxi 6=j ∣∣Σ̂ij − (Θ∗)−1ij ∣∣, λ2 = η2 := 4 maxg∈G\n∥∥Σ̂g − (Θ∗)−1g ∥∥a∗ and r ≤ 1\n5(|||Θ∗|||2+1)2 where ||| · |||2 is the spectral norm of the matrix and a ≥ 2. In addition, assume that Θ∗ is feasible for this problem. Then, any local optimum Θ̃ satisfies\n‖Θ̃−Θ∗‖F ≤ 3(|||Θ∗|||2 + 1)2 max { λ1 √ s , λ2 √ sG } .\n(16)\nRemark. Since ‖θ‖η scales with 1√n for the specified values of η, the constraint ‖θ‖η ≤ r gets milder as n increases. It is also important to note that this constraint is no more stringent than those of non-convex analyses with a single regularizer (Loh & Wainwright, 2015; 2014): their constraints can be written as η1‖θ‖1 ≤ r (since η1 √\nlog p/n for linear models for example.) in our notation, which directly implies ‖θ‖η ≤ r since ‖θ‖η ≤ η1‖θ‖1 by the definition of ‖ · ‖η ."
  }, {
    "heading": "6. Statistical Guarantees of Models with Non-convex Penalties",
    "text": "A natural extension of (14) is to incorporate non-convex regularizers that have some advantages such as unbiasedness. For that purpose, in this section we consider the following formulation\nminimize ‖θ‖η≤r\nL(θ) +R(θ;λ) (17)\nwhereR(θ;λ) = infα,β{ρλ1 ( α ) + φλ2,a ( β )\n: α+ β = θ}. While the `2 analysis in Theorem 1 can be extended to non-convex regularizers following proof techniques recently developed in Loh & Wainwright (2015), using nonconvex unbiased regularizers has no benefit in terms of asymptotic convergence rates of `2 estimation errors. Instead, we here investigate the `∞-norm bound and related support set recovery guarantees where non-convex unbiased regularizers help. To derive `∞ bounds, we use the\nprimal-dual witness method described in the supplementary materials. Theorem 2. Consider the dirty program with nonconvex penalties in (17), under (RSC). Suppose 2r(τ2 + 2 max{λ1η1 , λ2 η2 }) ≤ 1. Also suppose that for some\nδ ∈ [ max{ 4rτ1η1λ1 , 4rτ1η2 λ2 }, 1 ] , the strict dual feasibility of primal-dual witness holds. Then, any stationary point θ̃ of (17) is supported by U (recall U := supp(α∗) ∪ supp(β∗)) if the number of samples is large enough to satisfy max{λ1 √ s, λ2 √ sG}2 < c for some constant c depending only on κ1, τ1 and δ.\nAs in Theorem 1, the decomposability in (10) and (11) with respect to the surrogates ᾱ and β̄, plays a crucial role in establishing the support set recovery guarantee of any local optimum in Theorem 2.\nBased on Theorem 2, we can derive the `∞ bounds following the standard steps in (Loh & Wainwright, 2014): Corollary 3. Suppose the assumptions in Theorem 2 hold. Then,\n1. If κ1−µ2 ≥ τ1 ( max{η1 √ s, η2 √ sG} )2\nholds for large enough sample size n, the program (17) has a unique stationary point θ̂, specified by the construction of the primal dual witness.\n2. Letting Q̂ := ∫ 1\n0 ∇2L\n( θ∗ + t(θ̂ − θ∗) ) dt, it holds\nthat ‖θ̂ − θ∗‖∞ ≤ ∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞ +\nmin{λ1, λ2} ∣∣∣∣∣∣(Q̂UU)−1∣∣∣∣∣∣∞ where ||| · |||∞ denotes a matrix induced norm (maximum absolute row sum).\n3. Moreover, if ρ is (µ, γ)-amenable, and the minimum absolute value θ∗min := minj |θ∗j | is lowerbounded by θ∗min ≥ ∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞ + min{λ1, λ2}\n∣∣∣∣∣∣(Q̂UU)−1∣∣∣∣∣∣∞ + 2 max{λ1, λ2}γ. Then, the error bound in the statement 2 is reduced to tighter bound as ‖θ̂ − θ∗‖∞ ≤∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞.\nMulti-task high-dimensional linear regression. We consider the multi-task high-dimensional linear regression, as a concrete example of using non-convex dirty regularizers. This is the counterpart of model (4) which uses convex dirty regularizer. In the following corollary, we analyze the sparsistency of dirty multi-task linear regression with nonconvex regularizers:\nminimize Θ∈Rp×ms.t.‖Θ‖η≤r m∑ k=1 1 2n ∥∥y(k) −X(k)Θ(·,k)∥∥22 +R(Θ;λ) (18)\nwhere R(Θ;λ) = infα,β{ρλ1 ( α ) + φλ2,a ( β )\n: α+ β = Θ}. Now, we derive a corollary for this particular nonconvex dirty model.\nCorollary 4. Consider the multitask regression model. Suppose that for each task, design matrix X(k) is a zeromean Gaussian ensemble and is column normalized, w(k) is independent sub-Gaussian with parameter σ. Now suppose we set parameters of (18) as a :=∞, r constant (only depends on Σ and σ.), λ1 := c1σ √ log(pm)/n, λ2 :=\nc2σ √\n(log p+m log 2)/n, and Θ∗ is feasible to program (14) with these settings. Then, for any local optimum Θ̃, with probability at least 1 − c1 exp(−c2 log(pm)) − c3 exp ( −c4(log p+m log 2) ) (which is approaching to 1) for some positive constants c1 − c4,\n1. supp(Θ̃) ⊆ supp(Θ∗),\n2. if additionally the regularizer ρλ is (µ, γ)-amenable with µ < λmin(Σ) (the minimum eigenvalue of Σ) and Cmin := mink=1,...,m λmin ( Σ (k) UkUk ) > 0, then\nsupp(Θ̃) = supp(Θ∗) and the element-wise difference is bounded as follows:\n|||Θ̃−Θ∗|||max := max i,j |[Θ̃−Θ∗]i,j | ≤ σ\n√ 100 log(pm)\nnCmin\nprovided that θ∗min ≥ σ √ 100 log(pm) nCmin + min{λ1, λ2} maxk=1,...,m |||(Σ(k)UkUk) −1|||∞ + 2 max{λ1, λ2}γ.\nRemark. In order to highlight the benefit of using (µ, γ)-amenable regularizers, we briefly compare the result of Corollary 4 with that of `1 + `1,a case in (Jalali et al., 2010). Not only the result in (Jalali et al., 2010) requires the incoherence on X (specifically, maxj∈Uc ∑m k=1\n∥∥Σ(k)j,Uk(Σ(k)j,Uk)−1∥∥1 < 1), but it also has an additional sλ1\nCmin √ n term in |||Θ̃ − Θ∗|||max bound. Moreover, the required λ1 and λ2 there can converge\nto zero more slowly: λ1 √\nlog(pm) √ n− √ s log(pm)\nand λ2 √ m(m+log p)\n√ n− √ sm(m+log p) ."
  }, {
    "heading": "7. Experiments",
    "text": "To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties, we perform experiments on both simulated and real-world data and compare convex and non-convex dirty models for sparse + group-sparse structures.\nSimulated data. We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings of parameters (s, sG) ∈ {(2p/10, p/20), (p/10, p/10)} with respectively less / more support overlap across tasks (recall s and sG are the number of nonzero elements in α∗ and the number of nonzero groups in β∗, respectively). The\nrows of the design matrices X are sampled i.i.d. from a zero-mean Gaussian distribution with correlation of 0.2 between feature pairs. For each set of parameters (s, sG), we generate 100 instances of the problem where for each instance the non-zero entries of the true model parameter matrix are i.i.d. zero-mean Gaussian to agree with s and sG . Gaussian error with standard deviation of 4 is added to each observation. For varying sample size n we measure the `∞ error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty. We also evaluate the following baselines: Lasso, MCP, SCAD, Group-Lasso, Group-MCP and Group-SCAD 2. The regularization parameters of each method are tuned via 5-folds cross-validation. The results are presented in Figure 3 (To avoid cluttering the graphs, we do not display standard errors as these are much lower than the gaps between the pertinent groups of methods, and we only display the best group of baselines for each setting). As can be seen from the figure, dirty models with non-convex penalties enjoy superior performance over their counterparts with convex penalties as a function of the sample size. In terms of computational cost, (Group) coordinate descent steps for (group) lasso, (group) MCP and (group) SCAD all have simple closed-form expressions (Huang et al., 2012), similarly for proximal-based approaches. We noticed that for a wide range of (λ1, λ2), non-convex procedures took less time and converged faster (See supplements). As future work it would be interesting to study their theoretical numerical convergence rates.\nReal data analysis. We consider the problem of predicting biological activities of molecules given features extracted from their chemical structures. We analyze three biological activity datasets from the “molec-\n2Our theorem on `∞ consistency requires the sample size to be larger than the maximum of two terms, which precludes from presenting graphs with curve alignment across p (by rescaling the x-axis with a control parameter as in Jalali et al. (2010)).\nular activity challenge” (http://www.kaggle.com/ c/MerckActivity). Specifically we consider multitask regression with three tasks corresponding to predicting the raw value (− log(IC50)) of three different types of biological activities : ‘binding to cannabinoid receptor 1’, ‘inhibition of dipeptidyl peptidase 4’ and ‘time dependent 3A4 inhibitions’. For each task we used n = 200 observations with p = 3000 molecular features. We consider 20 random data splits into training and validation sets, using 2/3 of the data for tranining and 1/3 for validation, and report the average R2 over these random splits. As shown in table1, dirty models outperformed “clean” models suggesting the importance to strike a balanc e between task specificity and sharing for this data. Non-convex dirty models achieved the best R2, which illustrate their capability as a valuable tool for high-dimensional data analysis."
  }, {
    "heading": "8. Concluding Remarks",
    "text": "This paper finally resolved the outstanding case of sparse + group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not require implausible assumptions, thereby fully justifying their practical success. In addition we proposed and studied dirty models with non-convex penalties and showed that they enjoy superior theoretical guarantees that translate into significant practical impact. An interesting direction for future work is to investigate whether our proof technique might be applicable to other dirty models and beyond."
  }, {
    "heading": "Acknowledgments",
    "text": "E.Y. acknowledges the support of MSIP/NRF (National Research Foundation of Korea) via NRF2016R1A5A1012966 and MSIP/IITP (Institute for Information & Communications Technology Promotion of Korea) via ICT R&D program 2016-0-00563, 2017-0-00537."
  }],
  "year": 2017,
  "references": [{
    "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions",
    "authors": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"],
    "venue": "The Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Compressive sensing",
    "authors": ["R. Baraniuk"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2007
  }, {
    "title": "The power of convex relaxation: Near-optimal matrix completion",
    "authors": ["E.J. Candès", "T. Tao"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2010
  }, {
    "title": "Robust principal component analysis",
    "authors": ["E.J. Candès", "X. Li", "Y. Ma", "J. Wright"],
    "venue": "Journal of the ACM,",
    "year": 2011
  }, {
    "title": "Rank-sparsity incoherence for matrix decomposition",
    "authors": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2011
  }, {
    "title": "Variable selection via non-concave penalized likelihood and its oracle properties",
    "authors": ["J. Fan", "R. Li"],
    "venue": "Jour. Amer. Stat. Ass.,",
    "year": 2001
  }, {
    "title": "Sparse inverse covariance estimation with the graphical Lasso",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "year": 2007
  }, {
    "title": "Multi-stage multi-task feature learning",
    "authors": ["P. Gong", "J. Ye", "C. Zhang"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2012
  }, {
    "title": "Learning a common substructure of multiple graphical gaussian models",
    "authors": ["S. Hara", "T. Washio"],
    "venue": "Neural Networks,",
    "year": 2013
  }, {
    "title": "Robust matrix decomposition with sparse corruptions",
    "authors": ["D. Hsu", "S.M. Kakade", "T. Zhang"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2011
  }, {
    "title": "A selective review of group selection in high-dimensional models",
    "authors": ["J. Huang", "P. Breheny", "S. Ma"],
    "venue": "Statist. Sci., 27(4):481–499,",
    "year": 2012
  }, {
    "title": "A dirty model for multi-task learning",
    "authors": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"],
    "venue": "In Neur. Info. Proc. Sys. (NIPS),",
    "year": 2010
  }, {
    "title": "Support recovery without incoherence: A case for nonconvex regularization",
    "authors": ["P. Loh", "M.J. Wainwright"],
    "venue": "Arxiv preprint arXiv:1412.5632,",
    "year": 2014
  }, {
    "title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
    "authors": ["P. Loh", "M.J. Wainwright"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2015
  }, {
    "title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
    "authors": ["S. Negahban", "M.J. Wainwright"],
    "venue": "In Inter. Conf. on Machine learning (ICML),",
    "year": 2010
  }, {
    "title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers",
    "authors": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"],
    "venue": "Statistical Science,",
    "year": 2012
  }, {
    "title": "Restricted eigenvalue properties for correlated gaussian designs",
    "authors": ["G. Raskutti", "M.J. Wainwright", "B. Yu"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2010
  }, {
    "title": "Model selection in gaussian graphical models: Highdimensional consistency of `1-regularized mle",
    "authors": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"],
    "venue": "In Neur. Info. Proc. Sys. (NIPS),",
    "year": 2008
  }, {
    "title": "Sparse additive models",
    "authors": ["P. Ravikumar", "J. Lafferty", "H. Liu", "L. Wasserman"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB),",
    "year": 2009
  }, {
    "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
    "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"],
    "venue": "Allerton Conference",
    "year": 2007
  }, {
    "title": "Regression shrinkage and selection via the lasso",
    "authors": ["R. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society, Series B,",
    "year": 1996
  }, {
    "title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (Lasso)",
    "authors": ["M.J. Wainwright"],
    "venue": "IEEE Trans. Information Theory,",
    "year": 2009
  }, {
    "title": "Dirty statistical models",
    "authors": ["E. Yang", "P. Ravikumar"],
    "venue": "In Neur. Info. Proc. Sys. (NIPS),",
    "year": 2013
  }, {
    "title": "Graphical models via univariate exponential family distributions",
    "authors": ["E. Yang", "P. Ravikumar", "G.I. Allen", "Z. Liu"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2015
  }, {
    "title": "Model selection and estimation in regression with grouped variables",
    "authors": ["M. Yuan", "Y. Lin"],
    "venue": "Journal of the Royal Statistical Society B,",
    "year": 2006
  }, {
    "title": "Nearly unbiased variable selection under minimax concave penalty",
    "authors": ["C.H. Zhang"],
    "venue": "Annals of Statistics,",
    "year": 2010
  }],
  "id": "SP:d7be502dd2754557fc71c91a4ba03a2c2ac1df14",
  "authors": [{
    "name": "Eunho Yang",
    "affiliations": []
  }, {
    "name": "Aurélie C. Lozano",
    "affiliations": []
  }, {
    "name": "Aurelie C. Lozano",
    "affiliations": []
  }],
  "abstractText": "Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity. Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions. We also study non-convex counterparts of sparse + groupsparse models. Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.",
  "title": "Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity"
}