{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2431–2441 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2431"
  }, {
    "heading": "1 Introduction",
    "text": "Semantic parsing is a fundamental task within the field of natural language processing (NLP). Consider a natural language (NL) sentence and its corresponding meaning representation (MR) as illustrated in Figure 1. Semantic parsing aims to transform the natural language sentences into machine interpretable meaning representations automatically. The task has been popular for decades and keeps receiving significant attention from the NLP community. Various systems (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011) were proposed over the years to deal with different types of semantic representations. Such models include structure-based models (Wong and Mooney, 2006; Lu et al., 2008;\n1We make our system and code available at http:// statnlp.org/research/sp.\nKwiatkowski et al., 2010; Jones et al., 2012) and neural network based models (Dong and Lapata, 2016; Cheng et al., 2017).\nFollowing various previous research efforts (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), in this work, we adopt a popular class of semantic formalism – logical forms that can be equivalently represented as tree structures. The tree representation of an example MR is shown in the middle of Figure 1. One challenge associated with building a semantic parser is that the exact correspondence between the words and atomic semantic units are not explicitly given during the training phase. The key to the building of a successful semantic parsing model lies in the identification of a good joint latent representation of both the sentence and its corresponding semantics. Example joint representations proposed in the literature include a chart used in phrase-based translation (Wong and Mooney, 2006), a constituency tree-like representation known as hybrid tree (Lu et al., 2008), and a CCG-based derivation tree (Kwiatkowski et al., 2010).\nPrevious research efforts have shown the effec-\ntiveness of using dependency structures to extract semantic representations (Debusmann et al., 2004; Cimiano, 2009; Bédaride and Gardent, 2011; Stanovsky et al., 2016). Recently, Reddy et al. (2016, 2017) proposed a model to construct logical representations from sentences that are parsed into dependency structures. Their work demonstrates the connection between the dependency structures of a sentence and its underlying semantics. Although their setup and objectives are different from ours where externally trained dependency parsers are assumed available and their system was trained to use the semantics for a specific down-stream task, the success of their work motivates us to propose a novel joint representation that can explicitly capture dependency structures among words for the semantic parsing task.\nIn this work, we propose a new joint representation for both semantics and words, presenting a new model for semantic parsing. Our main contributions can be summarized as follows:\n• We present a novel dependency-based hybrid tree representation that captures both words and semantics in a joint manner. Such a dependency tree reveals semantic dependencies between words which are easily interpretable.\n• We show that exact dynamic programming algorithms for inference can be designed on top of our new representation. We further show that the model can be integrated with neural networks for improved effectiveness.\n• Extensive experiments conducted on the standard multilingual GeoQuery dataset show that our model outperforms the state-of-theart models on 7 out of 8 languages. Further analysis confirms the effectiveness of our dependency-based representation.\nTo the best of our knowledge, this is the first work that models the semantics as latent dependencies between words for semantic parsing."
  }, {
    "heading": "2 Related Work",
    "text": "The literature on semantic parsing has focused on various types of semantic formalisms. The λ-calculus expressions (Zettlemoyer and Collins, 2005) have been popular and widely used in semantic parsing tasks over recent years (Dong and Lapata, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2016, 2017; Susanto and Lu, 2017a; Cheng et al., 2017). Dependency-based composi-\ntional semantics (DCS)2 was introduced by Liang et al. (2011), whose extension, λ-DCS, was later proposed by Liang (2013). Various models (Berant et al., 2013; Wang et al., 2015; Jia and Liang, 2016) on semantic parsing with the λ-DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts (Wong and Mooney, 2006; Kate and Mooney, 2006; Lu et al., 2008; Kwiatkowski et al., 2010; Jones et al., 2012; Lu, 2014; Zou and Lu, 2018).\nWong and Mooney (2006) proposed the WASP semantic parser that regards the task as a phrasebased machine translation problem. Lu et al. (2008) proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called hybrid tree where both natural language words and semantics are encoded into a joint representation. The UBL-s (Kwiatkowski et al., 2010) parser applied the CCG grammar (Steedman, 1996) to model the joint representation of both semantic units and contiguous word sequences which do not overlap with one another. Jones et al. (2012) applied a generative process with Bayesian tree transducer and their model also simultaneously generates the meaning representations and natural language words. Lu (2014, 2015) proposed a discriminative version of the hybrid tree model of (Lu et al., 2008) where richer features can be captured. Dong and Lapata (2016) proposed a sequence-totree model using recurrent neural networks where the decoder can branch out to produce tree structures. Susanto and Lu (2017b) augmented the discriminative hybrid tree model with multilayer perceptron and achieved state-of-the-art performance.\nThere exists another line of work that applies given syntactic dependency information to semantic parsing. Titov and Klementiev (2011) decomposed a syntactic dependency tree into fragments and modeled the semantics as relations between the fragments. Poon (2013) learned to derive semantic structures based on syntactic dependency trees predicted by the Stanford dependency parser. Reddy et al. (2016, 2017) proposed a linguistically motivated procedure to transform syntactic dependencies into logical forms. Their semantic parsing performance relies on the quality of the syntactic dependencies. Unlike such efforts, we do not re-\n2Unlike ours, their work captures dependencies between semantic units but not natural language words.\nSentence: What rivers do not run through Tennessee ?\nquire external syntactic dependencies, but model the semantic units as latent dependencies between natural language words."
  }, {
    "heading": "3 Approach",
    "text": ""
  }, {
    "heading": "3.1 Variable-free Semantics",
    "text": "The variable-free semantic representations in the form of FunQL (Kate et al., 2005) used by the defacto GeoQuery dataset (Zelle and Mooney, 1996) encode semantic compositionality of the logical forms (Cheng et al., 2017). In the tree-structured semantic representations as illustrated in Figure 1, each tree node is a semantic unit of the following form:\nmi ≡ τα : pα(τ∗β)\nwhere mi denotes the complete semantic unit, which consists of semantic type τα, function symbol pα and an argument list of semantic types τ∗β (here ∗ denotes that there can be 0, 1, or 2 semantic types in the argument list. This number is known as the arity of mi). Each semantic unit can be regarded as a function that takes in other (partial) semantic representations of certain types as arguments and returns a semantic representation of a specific type. For example in Figure 1, the root unit is represented by m1, the type of this unit is QUERY, the function name is answer and it has a single argument RIVER which is a semantic type. With recursive function composition, we can obtain a complete MR as shown in Figure 1."
  }, {
    "heading": "3.2 Dependency-based Hybrid Trees",
    "text": "To jointly encode the tree-structured semantics m and a natural language sentence n, we in-\ntroduce our novel dependency-based hybrid tree. Figure 2 (right) shows the two equivalent ways of visualizing the dependency-based hybrid tree based on the example given in Figure 1. In this example, m is the tree-structured semantics m1(m2(m3,m4(m5(m6)))) and n is the sentence {w1, w2, · · · , w8}3. Our dependency-based hybrid tree t consists of a set of dependencies between the natural language words, each of which is labeled with a semantic unit. Formally, a dependency arc is represented as (wp, wc,mi), wherewp is the parent of this dependency, wc is the child, and mi is the semantic unit that serves as the label for the dependency arc. A valid dependency-based hybrid tree (with respect to a given semantic representation) allows one to recover the correct semantics from it. Thus, one constraint is that for any two adjacent dependencies (wp, wc,mi) and (w′p, w ′ c,mj), where wc ≡ w′p, mi must be the parent of mj in the tree-structured representation m. For example, in Figure 2, the dependencies (not, through, m4) and (through, Tennessee, m5) satisfy the above condition. However, we cannot replace (through, Tennessee, m5) with, for example, (through, Tennessee, m6), since m6 is not the child of m4. Furthermore, the number of children for a word in the dependency tree should be consistent with the arity of the corresponding semantic unit that points to it. For example, “not” has 2 children in our dependency-based hybrid tree representation because the semantic unit m2 (i.e., RIVER : exclude (RIVER, RIVER)) has arity 2. Also, “rivers” is the leaf as m3, which points to it, has arity 0. We will discuss in Section 3.3\n3We also introduce a special token “root” as w0.\non how to derive the set of allowable dependencybased hybrid trees for a given (m,n) pair.\nTo understand the potential advantages of our new joint representation, we compare it with the relaxed hybrid tree representation (Lu, 2014), which is illustrated on the left of Figure 2. We highlight some similarities and differences between the two representations from the span level and word level perspectives.\nIn a relaxed hybrid tree representation, words and semantic units jointly form a constituency tree-like structure, where the former are leaves and the latter are internal nodes of such a joint representation. Such a representation is able to capture alignment between the natural language words and semantics at the span level.4 For example, m2 covers the span from “rivers” to “Tennessee”, which allows the interactions between the semantic unit and the span to be captured. Similarly, in our dependency-based hybrid tree, such span level word-semantics correspondence can also be captured. For example, the arc between “not” and “through” is labeled by the semantic unitm4. This also allows the interactions betweenm4 and words within the span from “not” to “through” to be captured.\nWhile both models are able to capture the spanlevel correspondence between words and semantics, we can observe that in the relaxed hybrid tree, some words within the span are more directly related to the semantic unit (e.g., “do not” are more related to m2) and some are not. Specifically, in their representation, the span level information assigned to the parent semantic unit always contains the span level information assigned to all its child semantic units. This may not always be desirable and may lead to irrelevant features. In fact, Lu (2014) also empirically showed that the spanlevel features may not always be helpful in their representation. In contrast, in our dependencybased hybrid tree, the span covered by m2 is from “What” to “not”, which only consists of the span level information associated with its first child semantic units. Therefore, our representation is\n4We refer readers to (Lu, 2014) for more details.\nmore flexible in capturing the correspondence between words and semantics at the span level, allowing the model to choose the relevant span for features.\nFurthermore, our representation can also capture precise interactions between words through dependency arcs labeled with semantic units. For example, the semantic unit m4 on the dependency arc from “not” to “through” in our representation can be used to capture their interactions. However, such information could not be straightforwardly captured in a relaxed hybrid tree, which is essentially a constituency tree-like representation. In the same example, consider the word “not” that bridges two arcs labeled by m2 and m4. Lexical features defined over such arcs can be used to indirectly capture the interactions between semantic units and guide the tree construction process. We believe such properties can be beneficial in practice, especially for certain languages. We will examine their significance in our experiments later."
  }, {
    "heading": "3.3 Dependency Patterns",
    "text": "To define the set of allowable dependency-based hybrid tree representation so as to allow us to perform exact inference later, we introduce the dependency patterns as shown in Table 1. We use A, B or C to denote the abstract semantic units with arity 0, 1, and 2, respectively. We use W to denote a contiguous word span, and X and Y to denote the first and second child semantic unit, respectively.\nWe explain these patterns with concrete cases in Figure 3 based on the example in Figure 2. For the first case, the semantic unit m3 has arity 0, the pattern involved is WW, indicating both the lefthand and right-hand sides of “rivers” (under the dependency arc with semantic unit m3) are just word spans (W, whose length could be zero). In the second case, the semantic unit m4 has arity 1, the pattern involved is WX, indicating the lefthand side of “through” (under the arc of semantic unit m4) is a word span and the right-hand side should be handled by the first child of m4 in the\nsemantic tree, which is m5 in this case. In the third case, the semantic unit m2 has two arguments, and the pattern involved in the example is XY, meaning the left-hand and right-hand sides should be handled by the first and second child semantic units (i.e., m3 and m4), respectively.5 The final case illustrates that we also allow self-loops on our dependency-based hybrid trees, where an arc can be attached to a single word.6 To avoid an infinite number of self-loops over a word, we set a maximum depth c to restrict the maximum number of recurrences, which is similar to the method introduced in (Lu, 2015).\nBased on the dependency patterns, we are able to define the set of all possible allowable dependency-based hybrid tree representations. Each representation essentially belongs to a class of projective dependency trees where semantic units appear on the dependency arcs and (some of the) words are selected as nodes. The semantic tree can be constructed by following the arcs while referring to the dependency patterns involved."
  }, {
    "heading": "3.4 Model",
    "text": "Given the natural language words n, our task is to predict m, which is a tree-structured meaning representation, consisting of a set of semantic units as the nodes in the semantic tree. We use t to denote a dependency-based hybrid tree (as shown in Figure 2), which jointly encodes both natural language words and the gold meaning representation. Let T (n,m) denote all the possible dependencybased hybrid trees that contain the natural language words n and the meaning representation m. We adopt the widely-used structured prediction model conditional random fields (CRF) (Lafferty et al., 2001). The probability of a possible meaning representation m and dependency-based hybrid tree t for a sentence n is given by:\nPw(m, t|n) = ew·f(n,m,t)∑\nm′,t′∈T (n,m′) e w·f(n,m′,t′)\nwhere f(n,m, t) is the feature vector defined over the (n,m, t) tuple, and w is the parameter vector. Since we do not have the knowledge of the “true” dependencies during training, t is regarded as a latent-variable in our model. We marginalize\n5Analogously, the pattern YX would mean m4 handles the left-hand side and m3 right-hand side.\n6The limitations associated with disallowing such a pattern have been discussed in the previous work of (Lu, 2015).\nt in the above equation and the resulting model is a latent-variable CRF (Quattoni et al., 2005):\nPw(m|n) = ∑\nt∈T (n,m)\nPw(m, t|n)\n=\n∑ t∈T (n,m) e\nw·f(n,m,t)∑ m′,t′∈T (n,m′) e w·f(n,m′,t′)\n(1)\nGiven a datasetD of (n,m) pairs, our objective is to minimize the negative log-likelihood:7\nL(w) = − ∑\n(n,m)∈D\nlog ∑\nt∈T (n,m)\nPw(m, t|n) (2)\nThe gradient for model parameter wk is:\n∂L(w) ∂wk\n= ∑\n(n,m)∈D ∑ m′,t EPw(m′,t|n)[fk(n,m, t)]\n− ∑\n(n,m)∈D ∑ t EPw(t|n,m)[fk(n,m, t)]\nwhere fk(n,m, t) represents the number of occurrences of the k-th feature. With both the objective and gradient above, we can minimize the objective function with standard optimizers, such as L-BFGS (Liu and Nocedal, 1989) and stochastic gradient descent. Calculation of these expectations involves all possible dependency-based hybrid trees. As there are exponentially many such trees, an efficient inference procedure is required. We will present our efficient algorithm to perform exact inference for learning and decoding in the next section."
  }, {
    "heading": "3.5 Learning and Decoding",
    "text": "We propose dynamic-programming algorithms to perform efficient and exact inference, which will be used for calculating the objective and gradients discussed in the previous section. The algorithms are inspired by the inside-outside style algorithm (Baker, 1979), graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), and the relaxed hybrid tree model (Lu, 2014, 2015). As discussed in Section 3.3, our latent dependency trees are projective as in traditional dependency parsing (Eisner, 1996; Nivre and Scholz, 2004; McDonald et al., 2005) – the dependencies are non-crossing with respect to the word order (see bottom of Figure 1).\n7We ignore the L2 regularization term for brevity.\nThe objective function in Equation 2 can be further decomposed into the following form8:\nL(w) = − ∑\n(n,m)∈D\nlog ∑\nt∈T (n,m)\new·f(n,m,t)\n+ ∑\n(n,m)∈D\nlog ∑\nm′,t′∈T (n,m′)\new·f(n,m ′,t′)\nWe can see the first term is essentially the combined score of all the possible latent structures containing the pair (n,m). The second term is the combined score for all the possible latent structures containing n. We show how such scores can be calculated in a factorized manner, based on the fact that we can recursively decompose a dependency-based hybrid tree based on the dependency patterns we introduced.\nFormally, we introduce two interrelated dynamic-programming structures that are similar to those used in graph-based dependency parsing (Eisner, 2000; Koo and Collins, 2010; Shi et al., 2017), namely complete span and complete arc span. Figure 4a shows an example of complete span (left) and complete arc span (right). The complete span (over [i, j]) consists of a headword (at i) and its descendants on one side (they altogether form a subtree), a dependency pattern and a semantic unit. The complete arc span is a span (over [i, j]) with a dependency between the headword (at i) and the modifier (at k). We use Ci,j,p,m to denote a complete span, where i and j represent the indices of the headword and endpoint, p is the dependency pattern and m is the semantic unit. Analogously, we use Ai,k,j,p,m to denote a complete arc span where i and k are used to denote the additional dependency from the word at the i-th position as headword to the word at the k-th position as modifier.\nAs we can see from the derivation in Figure 4, each type of span can be constructed from smaller spans in a bottom-up manner. Figure 4a shows that a complete span is constructed from a complete arc span following the dependency patterns in Table 1. Figure 4b shows a complete arc span can be simply constructed from two smaller complete spans based on the dependency pattern. In Figure 4c and 4d, we further show how such two complete spans with pattern X (or Y) and W can be constructed. Figure 4c illustrates how to model a transition from one semantic unit to another where\n8Regularization term is excluded for brevity.\nthe parent is m1 and the child is m2 in the semantic tree. If m2 has arity 1, then the pattern is B following the dependency patterns in Table 1. For spans with a single word, we use the lowercase w as the pattern to indicate this fact, as shown in Figure 4d. They are the atomic spans used for building larger spans. As the complete span in Figure 4d is associated with pattern W, which means the words within this span are under the semantic unit m1, we can incrementally construct this span with atomic spans. We illustrate the construction of a complete dependency-based hybrid tree in the supplementary material.\nOur final goal during training for a sentence n = {w0, w1, · · · , wN} is to construct all the possible complete spans that cover the interval [0, N ], which can be represented asC0,N,·,·. Similar to the chart-based dependency parsing algorithms (Eisner, 1996, 2000; Koo and Collins, 2010), we can obtain the inside and outside scores using our dynamic-programming derivation in Figure 4 during the inference process, which can then be used to calculate the objective and feature expectations. Since the spans are defined by at most three free indices, the dependency pattern and the semantic unit, our dynamic-programming algorithm requires O(N3M) time9 where M is the number of semantic units. The resulting complexity is the same as the relaxed hybrid tree model (Lu, 2014).\nDuring decoding, we can find the optimal (treestructured) meaning representation m∗ for a given\n9We omit a small constant factor associated with patterns.\ninput sentence n by the Viterbi algorithm. This step can also be done efficiently with our dynamicprogramming approach, where we switch from marginal inference to MAP inference:\nm∗, t∗ = argmax m,t∈T (n,m) ew·f(n,m,t)\nA similar decoding procedure has been used in previous work (Lu, 2014; Durrett and Klein, 2015) with CKY-based parsing algorithm."
  }, {
    "heading": "3.6 Features",
    "text": "As shown in Equation 1, the features are defined on the tuple (n,m, t). With the dynamicprogramming procedure, we can define the features over the structures in Figure 2. Our feature design is inspired by the hybrid tree model (Lu, 2015) and graph-based dependency parsing (McDonald et al., 2005). Table 2 shows the feature templates for the example in Figure 2. Specifically, we define simple unigram features (concatenation of a semantic unit and a word that directly appears under the unit), pattern features (concatenation of the semantic unit and the child pattern) and transition features (concatenation of the parent and child semantic units). They form our basic feature set.\nAdditionally, with the structured properties of dependencies, we can define dependency-related features (McDonald et al., 2005). We use the parent (head) and child (modifier) words of the dependency as features. We also use the bag-of-words covered under a dependency as features. The dependency features are useful in helping improve the performance as we can see in the experiments section."
  }, {
    "heading": "3.7 Neural Component",
    "text": "Following the approach used in Susanto and Lu (2017b), we could further incorporate neural networks into our latent-variable graphical model. The integration is analogous to the approaches described in the neural CRF models (Do and\nArtieres, 2010; Durrett and Klein, 2015; Gormley, 2015; Lample et al., 2016), where we use neural networks to learn distributed feature representations within our graphical model.\nWe employ a neural architecture to calculate the score associated with each dependency arc (wp, wc,m) (here wp and wc are the parent and child words in the dependency andm is the semantic unit over the arc), where the input to the neural network consists of words (i.e., (wp, wc)) associated with this dependency and the neural network will calculate a score for each possible semantic unit, includingm. The two words are first mapped to word embeddings ep and ec (both of dimension d). Next, we use a bilinear layer10 (Socher et al., 2013; Chen et al., 2016) to capture the interaction between the parent and the child in a dependency:\nri = e T p Uiec\nwhere ri represents the score for the i-th semantic unit and Ui ∈ Rd×d. The scores are then incorporated into the probability expression in Equation 1 during learning and decoding. As a comparison, we also implemented a variant where our model directly takes in the average embedding of ep and ec as additional features, without using our neural component."
  }, {
    "heading": "4 Experiments",
    "text": "Data and evaluation methodology We conduct experiments on the publicly available variablefree version of the GeoQuery dataset, which has been widely used for semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012). The dataset consists of 880 pairs of natural language sentences and the corresponding treestructured semantic representations. This dataset is annotated with eight languages. The original annotation of this dataset is English (Zelle and Mooney, 1996) and Jones et al. (2012) annotated the dataset with three more languages: German, Greek and Thai. Lu and Ng (2011) released the Chinese annotation and Susanto and Lu (2017b) annotated the corpus with three additional languages: Indonesian, Swedish and Farsi. In order to compare with previous work (Jones et al., 2012; Lu, 2015), we follow the standard splits with 600 instances for training and 280 instances for testing. To evaluate the performance, we follow the\n10Empirically, we also tried multilayer perceptron but the bilinear model gives us better results.\nstandard evaluation procedure used in various previous works (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Lu, 2015) to construct the Prolog query from the tree-structured semantic representation using a standard and publicly available script. The queries are then used to retrieve the answers from the GeoQuery database, and we report accuracy and F1 scores.\nHyperparameters We set the maximum depth c of the semantic tree to 20, following Lu (2015). The L2 regularization coefficient is tuned from 0.01 to 0.05 using 5-fold cross-validation on the training set. The Polyglot (Al-Rfou et al., 2013) multilingual word embeddings11 (with 64 dimensions) are used for all languages. We use LBFGS (Liu and Nocedal, 1989) to optimize the DEPHT model until convergence and stochastic gradient descent (SGD) with a learning rate of 0.05 to optimize the neural DEPHT model. We implemented our neural component with the Torch7 library (Collobert et al., 2011). Our complete implementation is based on the StatNLP12 structured prediction framework (Lu, 2017)."
  }, {
    "heading": "4.1 Baseline Systems",
    "text": "We run the released systems of several state-ofthe-art semantic parsers, namely the WASP parser (Wong and Mooney, 2006), HYBRIDTREE model (Lu et al., 2008), UBL system (Kwiatkowski et al., 2010), relaxed hybrid tree (RHT) (Lu, 2015)13, the sequence-to-tree (SEQ2TREE) model (Dong and Lapata, 2016), the neural hybrid tree (NEURAL HT) model (Susanto and Lu, 2017b), and the multilingual semantic\n11The embeddings are fixed to avoid overfitting. 12https://gitlab.com/sutd nlp/statnlp-core 13(Lu, 2015) is an extension of the original relaxed hybrid\ntree (Lu, 2014), which reports improved results.\nSentence: San Antonio berada di negara bagian apa ? (San) (Antonio) (located) (in) ( state ) (what) (?)\nGold Meaning Representation: answer(loc(cityid(′san antonio′)))\nRelaxed Hybrid Tree\nm1\nSan Antonio breada di ?m4\nnegara bagian apa\nm1: QUERY : answer (STATE) m2: STATE : loc (CITY) m3: CITY : cityid (CITYNAME) m4: STATE : state (all) m5: CITYNAME : (′san antonio′)\nDependency-based Hybrid Tree\nparser (Susanto and Lu, 2017a) with single language (MSP-SINGLE) as input. The results for TREETRANS (Jones et al., 2012) are taken from their paper."
  }, {
    "heading": "4.2 Results and Discussion",
    "text": "Table 3 (top) shows the results of our dependencybased hybrid tree model compared with nonneural models which achieve state-of-the-art performance on the GeoQuery dataset. Our model DEPHT achieves competitive performance and outperforms the previous best system RHT on 6 languages. Improvements on the Indonesian dataset are particularly striking (+11.8 absolute points in F1). We further investigated the outputs from both systems on Indonesian by doing error analysis. We found 40 instances that are incorrectly predicted by RHT are correctly predicted by DEPHT. We found that 77.5% of the errors are due to incorrect alignment between words and semantic units. Figure 5 shows an example of such errors where the relaxed hybrid tree fails to capture the correct alignment. We can see the question is asking “What state is San Antonio located in?”. However, the natural language word order in Indone-\nsian is different from English, where the phrase “berada di” that corresponds to m2 (i.e., loc) appears between “San Antonio” (which corresponds to m5 – ′san antonio′) and “what” (which corresponds to m1 – answer). Such a structural non isomorphism issue between the sentence and the semantic tree makes the relaxed hybrid tree parser unable to produce a joint representation with valid word-semantics alignment. This issue makes the RHT model unable to predict the semantic unit m2 (i.e., loc) as RHT has to align the words “San Antonio” which should be aligned to m5 before aligning “berada di”. However, m5 has arity 0 and cannot have m2 as its child. Thus, it would be impossible for the RHT model to predict such a meaning representation as output. In contrast, we can see that our dependency-based hybrid tree representation appears to be more flexible in handling such cases. The dependency between the two words “di” (in) and “berada” (located) is also well captured by the arc between them that is labeled with m2. The error analysis reveals the flexibility of our joint representation in different languages in terms of the word ordering, indicating that the novel dependency-based joint representation is more robust and suffers less from languagespecific characteristics associated with the data.\nEffectiveness of dependency To investigate the helpfulness of the features defined over latent dependencies, we conduct ablation tests by removing the dependency-related features. Table 4 shows the performance of augmenting different dependency features in our DEPHT model with basic features. Specifically, we investigate the performance of head word and modifier word features (HM) and also the bag-of-words features (BOW) that can be extracted based on dependencies. It can be observed that dependency features associated with the words are crucial for all languages, especially the BOW features.\nEffectiveness of neural component The bottom part of Table 3 shows the performance comparison among models that involve neural networks. Our DEPHT model with embeddings as\nfeatures can outperform neural baselines across several languages (i.e., Chinese, Indonesian and Swedish). From the table, we can see the neural component is effective, which consistently gives better results than DEPHT and the approach that uses word embedding features only. Susanto and Lu (2017b) presented the NEURAL HT model with different window size J for their multilayer perceptron. Their performance will differ with different window sizes, which need to be tuned for each language. In our neural component, we do not require such a language-specific hyperparameter, yet our neural approach consistently achieves the highest performance on 7 out of 8 languages compared with all previous approaches. As both the embeddings and the neural component are defined on the dependency arcs, the superior results also reveal the effectiveness of our dependencybased hybrid tree representation."
  }, {
    "heading": "5 Conclusions and Future Work",
    "text": "In this work, we present a novel dependencybased hybrid tree model for semantic parsing. The model captures the underlying semantic information of a sentence as latent dependencies between the natural language words. We develop an efficient algorithm for exact inference based on dynamic-programming. Extensive experiments on benchmark dataset across 8 different languages demonstrate the effectiveness of our newly proposed representation for semantic parsing.\nFuture work includes exploring alternative approaches such as transition-based methods (Nivre et al., 2006; Chen and Manning, 2014) for semantic parsing with latent dependencies, applying our dependency-based hybrid trees on other types of logical representations (e.g., lambda calculus expressions and SQL (Finegan-Dollak et al., 2018)) as well as multilingual semantic parsing (Jie and Lu, 2014; Susanto and Lu, 2017a)."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their constructive comments on this work. We would also like to thank Yanyan Zou for helping us with running the experiments for baseline systems. This work is supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 2 Project MOE2017-T2-1-156, and is partially supported by project 61472191 under the National Natural Science Foundation of China."
  }],
  "year": 2018,
  "references": [{
    "title": "Polyglot: Distributed word representations for multilingual nlp",
    "authors": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."],
    "venue": "Proceedings of CoNLL.",
    "year": 2013
  }, {
    "title": "Trainable grammars for speech recognition",
    "authors": ["James K Baker."],
    "venue": "The Journal of the Acoustical Society of America, 65(S1):S132–S132.",
    "year": 1979
  }, {
    "title": "Deep semantics for dependency structures",
    "authors": ["Paul Bédaride", "Claire Gardent."],
    "venue": "Proceedings of CICLing.",
    "year": 2011
  }, {
    "title": "Semantic parsing on freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of EMNLP.",
    "year": 2013
  }, {
    "title": "A thorough examination of the cnn/daily mail reading comprehension task",
    "authors": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."],
    "venue": "Proceedings of ACL.",
    "year": 2016
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of EMNLP.",
    "year": 2014
  }, {
    "title": "Learning structured natural language representations for semantic parsing",
    "authors": ["Jianpeng Cheng", "Siva Reddy", "Vijay Saraswat", "Mirella Lapata."],
    "venue": "Proceedings of ACL.",
    "year": 2017
  }, {
    "title": "Flexible semantic composition with dudes",
    "authors": ["Philipp Cimiano."],
    "venue": "Proceedings of ICCS.",
    "year": 2009
  }, {
    "title": "Torch7: A matlab-like environment for machine learning",
    "authors": ["Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet."],
    "venue": "Proceedings of BigLearn, NIPS workshop.",
    "year": 2011
  }, {
    "title": "A relational syntax-semantics interface based on dependency grammar",
    "authors": ["Ralph Debusmann", "Denys Duchier", "Alexander Koller", "Marco Kuhlmann", "Gert Smolka", "Stefan Thater."],
    "venue": "Proceedings of COLING.",
    "year": 2004
  }, {
    "title": "Neural conditional random fields",
    "authors": ["Trinh-Minh-Tri Do", "Thierry Artieres."],
    "venue": "Proceedings of AISTAT.",
    "year": 2010
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of ACL.",
    "year": 2016
  }, {
    "title": "Neural crf parsing",
    "authors": ["Greg Durrett", "Dan Klein."],
    "venue": "Proceedings of ACL-IJCNLP.",
    "year": 2015
  }, {
    "title": "Bilexical grammars and their cubic-time parsing algorithms",
    "authors": ["Jason Eisner."],
    "venue": "Advances in probabilistic and other parsing technologies, pages 29–",
    "year": 2000
  }, {
    "title": "Three new probabilistic models for dependency parsing: An exploration",
    "authors": ["Jason M Eisner."],
    "venue": "Proceedings of COLING.",
    "year": 1996
  }, {
    "title": "Improving text-to-sql evaluation methodology",
    "authors": ["Catherine Finegan-Dollak", "Jonathan K. Kummerfeld", "Li Zhang", "Karthik Ramanathan", "Sesh Sadasivam", "Rui Zhang", "Dragomir Radev."],
    "venue": "Proceedings of ACL.",
    "year": 2018
  }, {
    "title": "Openvocabulary semantic parsing with both distributional statistics and formal knowledge",
    "authors": ["Matt Gardner", "Jayant Krishnamurthy."],
    "venue": "Proceedings of AAAI.",
    "year": 2017
  }, {
    "title": "Graphical Models with Structured Factors, Neural Factors, and Approximation-Aware Training",
    "authors": ["Matthew R Gormley."],
    "venue": "Ph.D. thesis.",
    "year": 2015
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "Proceedings of ACL.",
    "year": 2016
  }, {
    "title": "Multilingual semantic parsing: Parsing multiple languages into semantic representations",
    "authors": ["Zhanming Jie", "Wei Lu."],
    "venue": "Proceedings of COLING.",
    "year": 2014
  }, {
    "title": "Semantic parsing with bayesian tree transducers",
    "authors": ["Bevan Keeley Jones", "Mark Johnson", "Sharon Goldwater."],
    "venue": "Proceedings of ACL.",
    "year": 2012
  }, {
    "title": "Using string-kernels for learning semantic parsers",
    "authors": ["Rohit J Kate", "Raymond J Mooney."],
    "venue": "Proceedings of COLING/ACL.",
    "year": 2006
  }, {
    "title": "Learning to transform natural to formal languages",
    "authors": ["Rohit J Kate", "Yuk Wah Wong", "Raymond J Mooney."],
    "venue": "Proceedings of AAAI.",
    "year": 2005
  }, {
    "title": "Efficient thirdorder dependency parsers",
    "authors": ["Terry Koo", "Michael Collins."],
    "venue": "Proceedings of ACL.",
    "year": 2010
  }, {
    "title": "Inducing probabilistic ccg grammars from logical form with higherorder unification",
    "authors": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of EMNLP.",
    "year": 2010
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "Proceedings of ICML.",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of NAACL.",
    "year": 2016
  }, {
    "title": "Lambda dependency-based compositional semantics",
    "authors": ["Percy Liang."],
    "venue": "Technical Report arXiv.",
    "year": 2013
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael I Jordan", "Dan Klein."],
    "venue": "Proceedings of NNACL.",
    "year": 2011
  }, {
    "title": "On the limited memory bfgs method for large scale optimization",
    "authors": ["Dong C Liu", "Jorge Nocedal."],
    "venue": "Mathematical programming, 45(1):503–528.",
    "year": 1989
  }, {
    "title": "Semantic parsing with relaxed hybrid trees",
    "authors": ["Wei Lu."],
    "venue": "Proceedings of EMNLP.",
    "year": 2014
  }, {
    "title": "Constrained semantic forests for improved discriminative semantic parsing",
    "authors": ["Wei Lu."],
    "venue": "Proceedings of ACL.",
    "year": 2015
  }, {
    "title": "A unified framework for structured prediction: From theory to practice",
    "authors": ["Wei Lu."],
    "venue": "Proceedings of EMNLP (Tutorial).",
    "year": 2017
  }, {
    "title": "A probabilistic forest-to-string model for language generation from typed lambda calculus expressions",
    "authors": ["Wei Lu", "Hwee Tou Ng."],
    "venue": "Proceedings of EMNLP.",
    "year": 2011
  }, {
    "title": "A generative model for parsing natural language to meaning representations",
    "authors": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S Zettlemoyer."],
    "venue": "Proceedings of EMNLP.",
    "year": 2008
  }, {
    "title": "Online large-margin training of dependency parsers",
    "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."],
    "venue": "Proceedings of ACL.",
    "year": 2005
  }, {
    "title": "Maltparser: A data-driven parser-generator for dependency parsing",
    "authors": ["Joakim Nivre", "Johan Hall", "Jens Nilsson."],
    "venue": "Proceedings of LREC.",
    "year": 2006
  }, {
    "title": "Deterministic dependency parsing of english text",
    "authors": ["Joakim Nivre", "Mario Scholz."],
    "venue": "Proceedings of COLING.",
    "year": 2004
  }, {
    "title": "Grounded unsupervised semantic parsing",
    "authors": ["Hoifung Poon."],
    "venue": "Proceedings of ACL.",
    "year": 2013
  }, {
    "title": "Conditional random fields for object recognition",
    "authors": ["Ariadna Quattoni", "Michael Collins", "Trevor Darrell."],
    "venue": "Proceedings of NIPS.",
    "year": 2005
  }, {
    "title": "Transforming dependency structures to logical forms for semantic parsing",
    "authors": ["Siva Reddy", "Oscar Täckström", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."],
    "venue": "Transactions of the Association for Computational",
    "year": 2016
  }, {
    "title": "Universal semantic parsing",
    "authors": ["Siva Reddy", "Oscar Täckström", "Slav Petrov", "Mark Steedman", "Mirella Lapata."],
    "venue": "Proceedings of EMNLP.",
    "year": 2017
  }, {
    "title": "Fast (er) exact decoding and global training",
    "authors": ["Tianze Shi", "Liang Huang", "Lillian Lee"],
    "year": 2017
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of EMNLP.",
    "year": 2013
  }, {
    "title": "Getting more out of syntax with props",
    "authors": ["Gabriel Stanovsky", "Jessica Ficler", "Ido Dagan", "Yoav Goldberg."],
    "venue": "arXiv preprint arXiv:1603.01648.",
    "year": 2016
  }, {
    "title": "Surface structure and interpretation",
    "authors": ["Mark Steedman."],
    "venue": "Raymond Hendy Susanto and Wei Lu. 2017a. Neural architectures for multilingual semantic parsing. In Proceedings of ACL.",
    "year": 1996
  }, {
    "title": "Semantic parsing with neural hybrid trees",
    "authors": ["Raymond Hendy Susanto", "Wei Lu."],
    "venue": "Proceedings of AAAI.",
    "year": 2017
  }, {
    "title": "A bayesian model for unsupervised semantic parsing",
    "authors": ["Ivan Titov", "Alexandre Klementiev."],
    "venue": "Proceedings of ACL.",
    "year": 2011
  }, {
    "title": "Building a semantic parser overnight",
    "authors": ["Yushi Wang", "Jonathan Berant", "Percy Liang."],
    "venue": "Proceedings of ACL-IJCNLP, pages 1332–1342.",
    "year": 2015
  }, {
    "title": "Learning for semantic parsing with statistical machine translation",
    "authors": ["Yuk Wah Wong", "Raymond J Mooney."],
    "venue": "Proceedings of NAACL.",
    "year": 2006
  }, {
    "title": "Learning to parse database queries using inductive logic programming",
    "authors": ["John M Zelle", "Raymond J Mooney."],
    "venue": "Proceedings of AAAI.",
    "year": 1996
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of UAI.",
    "year": 2005
  }, {
    "title": "Learning cross-lingual distributed logical representations for semantic parsing",
    "authors": ["Yanyan Zou", "Wei Lu."],
    "venue": "Proceedings of ACL.",
    "year": 2018
  }],
  "id": "SP:7e1a93fc56076270aa61681738e313b92f9fe6b6",
  "authors": [{
    "name": "Zhanming Jie",
    "affiliations": []
  }, {
    "name": "Wei Lu",
    "affiliations": []
  }],
  "abstractText": "We propose a novel dependency-based hybrid tree model for semantic parsing, which converts natural language utterance into machine interpretable meaning representations. Unlike previous state-of-the-art models, the semantic information is interpreted as the latent dependency between the natural language words in our joint representation. Such dependency information can capture the interactions between the semantics and natural language words. We integrate a neural component into our model and propose an efficient dynamicprogramming algorithm to perform tractable inference. Through extensive experiments on the standard multilingual GeoQuery dataset with eight languages, we demonstrate that our proposed approach is able to achieve state-ofthe-art performance across several languages. Analysis also justifies the effectiveness of using our new dependency-based representation.1",
  "title": "Dependency-based Hybrid Trees for Semantic Parsing"
}