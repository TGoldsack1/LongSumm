{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1914"
  }, {
    "heading": "1 Introduction",
    "text": "Deep learning models work best when trained on large amounts of labeled data. However, acquiring labels is costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples. A widely successful semi-supervised learning strategy for neural NLP is pre-training word vectors (Mikolov et al., 2013). More recent work trains a Bi-LSTM sentence encoder to do language modeling and then incorporates its context-sensitive representations into supervised models (Dai and Le, 2015; Peters et al.,\n1Code will be made available at https: //github.com/tensorflow/models/tree/ master/research/cvt_text\n2018). Such pre-training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training.\nA key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task. Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data. Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models. This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models.\nIn self-training, the model learns as normal on labeled examples. On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions. Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on. Recent research on computer vision addresses this by adding noise to the student’s input, training the model so it is robust to input perturbations (Sajjadi et al., 2016; Wei et al., 2018). However, applying noise is difficult for discrete inputs like text.\nAs a solution, we take inspiration from multiview learning (Blum and Mitchell, 1998; Xu et al., 2013) and train the model to produce consistent predictions across different views of the input. Instead of only training the full model as a student, CVT adds auxiliary prediction modules – neural networks that transform vector representations into predictions – to the model and also trains them as students. The input to each student prediction module is a subset of the model’s intermediate rep-\nresentations corresponding to a restricted view of the input example. For example, one auxiliary prediction module for sequence tagging is attached to only the “forward” LSTM in the model’s first BiLSTM layer, so it makes predictions without seeing any tokens to the right of the current one.\nCVT works by improving the model’s representation learning. The auxiliary prediction modules can learn from the full model’s predictions because the full model has a better, unrestricted view of the input. As the auxiliary modules learn to make accurate predictions despite their restricted views of the input, they improve the quality of the representations they are built on top of. This in turn improves the full model, which uses the same shared representations. In short, our method combines the idea of representation learning on unlabeled data with classic self-training.\nCVT can be applied to a variety of tasks and neural architectures, but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi-LSTM encoder. We propose auxiliary prediction modules that work well for sequence taggers, graph-based dependency parsers, and sequence-to-sequence models. We evaluate our approach on English dependency parsing, combinatory categorial grammar supertagging, named entity recognition, partof-speech tagging, and text chunking, as well as English to Vietnamese machine translation. CVT improves over previously published results on all these tasks. Furthermore, CVT can easily and effectively be combined with multi-task learning: we just add additional prediction modules for the different tasks on top of the shared Bi-LSTM encoder. Training a unified model to jointly perform all of the tasks except machine translation improves results (outperforming a multi-task ELMo model) while decreasing the total training time."
  }, {
    "heading": "2 Cross-View Training",
    "text": "We first present Cross-View Training and describe how it can be combined effectively with multi-task learning. See Figure 1 for an overview of the training method."
  }, {
    "heading": "2.1 Method",
    "text": "Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} represent a labeled dataset and Dul = {x1, x2, ..., xM} represent an unlabeled dataset We use pθ(y|xi) to denote the output distribution over classes pro-\nduced by the model with parameters θ on input xi. During CVT, the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples. For labeled examples, CVT uses standard cross-entropy loss:\nLsup(θ) = 1 |Dl| ∑\nxi,yi∈Dl\nCE(yi, pθ(y|xi))\nCVT adds k auxiliary prediction modules to the model, which are used when learning on unlabeled examples. A prediction module is usually a small neural network (e.g., a hidden layer followed by a softmax layer). Each one takes as input an intermediate representation hj(xi) produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model). It outputs a distribution over labels pjθ(y|xi). Each h\nj is chosen such that it only uses a part of the input xi; the particular choice\ncan depend on the task and model architecture. We propose variants for several tasks in Section 3. The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces pθ.\nOn an unlabeled example, the model first produces soft targets pθ(y|xi) by performing inference. CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing LCVT(θ) = 1|Dul| ∑ xi∈Dul ∑k j=1D(pθ(y|xi), p j θ(y|xi))\nwhere D is a distance function between probability distributions (we use KL divergence). We hold the primary module’s prediction pθ(y|xi) fixed during training (i.e., we do not back-propagate through it) so the auxiliary modules learn to imitate the primary one, but not vice versa. CVT works by enhancing the model’s representation learning. As the auxiliary modules train, the representations they take as input improve so they are useful for making predictions even when some of the model’s inputs are not available. This in turn improves the primary prediction module, which is built on top of the same shared representations.\nWe combine the supervised and CVT losses into the total loss, L = Lsup + LCVT, and minimize it with stochastic gradient descent. In particular, we alternate minimizing Lsup over a minibatch of labeled examples and minimizing LCVT over a minibatch of unlabeled examples.\nFor most neural networks, adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations (such as an RNN or CNN). Therefore our method contributes little overhead to training time over other self-training approaches for most tasks. CVT does not change inference time or the number of parameters in the fullytrained model because the auxiliary prediction modules are only used during training."
  }, {
    "heading": "2.2 Combining CVT with Multi-Task Learning",
    "text": "CVT can easily be combined with multi-task learning by adding additional prediction modules for the other tasks on top of the shared Bi-LSTM encoder. During supervised learning, we randomly select a task and then update Lsup using a minibatch of labeled data for that task. When learning on the unlabeled data, we optimize LCVT\njointly across all tasks at once, first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules. As before, the model alternates training on minibatches of labeled and unlabeled examples.\nExamples labeled across many tasks are useful for multi-task systems to learn from, but most datasets are only labeled with one task. A benefit of multi-task CVT is that the model creates (artificial) all-tasks-labeled examples from unlabeled data. This significantly improves the model’s data efficiency and training time. Since running prediction modules is computationally cheap, computing LCVT is not much slower for many tasks than it is for a single one. However, we find the all-tasks-labeled examples substantially speed up model convergence. For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time."
  }, {
    "heading": "3 Cross-View Training Models",
    "text": "CVT relies on auxiliary prediction modules that have restricted views of the input. In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning."
  }, {
    "heading": "3.1 Bi-LSTM Sentence Encoder",
    "text": "All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder. It takes as input a sequence of words xi = [x1i , x 2 i , ..., x T i ]. First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v = [v1, v2, ..., vT ]. The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations. The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking vt as input at each step t) and the backward direction (taking vT−t+1 at each step) to produce vector sequences [ −→ h 11, −→ h 21, ... −→ h T1 ] and [ ←− h 11, ←− h 21, ... ←− h T1 ]. The output of the Bi-LSTM is the concatenation of these vectors: h1 = [ −→ h 11 ⊕←−\nh 11, ..., −→ h T1 ⊕ ←− h T1 ]. The second Bi-LSTM layer\nworks the same, producing outputs h2, except it takes h1 as input instead of v."
  }, {
    "heading": "3.2 CVT for Sequence Tagging",
    "text": "In sequence tagging, each token xti has a corresponding label yti . The primary prediction module for sequence tagging produces a probability distribution over classes for the tth label using a onehidden-layer neural network applied to the corresponding encoder outputs:\np(yt|xi) = NN(ht1 ⊕ ht2) = softmax(U · ReLU(W (ht1 ⊕ ht2)) + b)\nThe auxiliary prediction modules take −→ h 1(xi) and ←− h 1(xi), the outputs of the forward and backward LSTMs in the first2 Bi-LSTM layer, as inputs. We add the following four auxiliary prediction modules to the model (see Figure 2):\npfwdθ (y t|xi) = NNfwd( −→ h t1(xi)) pbwdθ (y t|xi) = NNbwd( ←− h t1(xi))\npfutureθ (y t|xi) = NNfuture( −→ h t−11 (xi))\np past θ (y t|xi) = NNpast( ←− h t+11 (xi))\nThe “forward” module makes each prediction without seeing the right context of the current token. The “future” module makes each prediction without the right context or the current token itself. Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next. The “backward” and “past” modules are analogous."
  }, {
    "heading": "3.3 CVT for Dependency Parsing",
    "text": "In a dependency parse, words in a sentence are treated as nodes in a graph. Typed directed edges connect the words, forming a tree structure describing the syntactic structure of the sentence. In particular, each word xti in a sentence xi = x 1 i , ..., x T i receives exactly one in-going edge (u, t, r) going from word xui (called the “head”) to it (the “dependent”) of type r (the “relation”). We use a graph-based dependency parser similar to the one from Dozat and Manning (2017). This treats dependency parsing as a classification task where the goal is to predict which in-going edge yti = (u, t, r) connects to each word x t i.\nFirst, the representations produced by the encoder for the candidate head and dependent are\n2Modules taking inputs from the second Bi-LSTM layer would not have restricted views because information about the whole sentence gets propagated through the first layer.\nLSTM LSTM ŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast  Backward LSTM Forward LSTM Predict LSTM LSTM LSTM LSTM Auxiliary Prediction Modules Primary Prediction Module\npassed through separate hidden layers. A bilinear classifier applied to these representations produces a score for each candidate edge. Lastly, these scores are passed through a softmax layer to produce probabilities. Mathematically, the probability of an edge is given as:\npθ((u, t, r)|xi) ∝ es(h u 1 (xi)⊕hu2 (xi),ht1(xi)⊕ht2(xi),r)\nwhere s is the scoring function:\ns(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr +W )\nReLU(Wdepz2 + bdep)\nThe bilinear classifier uses a weight matrix Wr specific to the candidate relation as well as a weight matrix W shared across all relations. Note that unlike in most prior work, our dependency parser only takes words as inputs, not words and part-of-speech tags.\nWe add four auxiliary prediction modules to our model for cross-view training:\npfwd-fwdθ ((u, t, r)|xi) ∝ es fwd-fwd(\n−→ h u1 (xi), −→ h t1(xi),r)\npfwd-bwdθ ((u, t, r)|xi) ∝ es fwd-bwd(\n−→ h u1 (xi), ←− h t1(xi),r)\npbwd-fwdθ ((u, t, r)|xi) ∝ es bwd-fwd(\n←− h u1 (xi), −→ h t1(xi),r)\npbwd-bwdθ ((u, t, r)|xi) ∝ es bwd-bwd(\n←− h u1 (xi), ←− h t1(xi),r)\nEach one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent."
  }, {
    "heading": "3.4 CVT for Sequence-to-Sequence Learning",
    "text": "We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015). Each example consists of an input (source) sequence xi = x1i , ..., x T i and output (target) sequence yi = y1i , ..., y K i . The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015). In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as αj ∝ eh\njWαh̄t where h̄t is the decoder’s current hidden state. The source hidden states weighted by the attention distribution form a context vector: ct = ∑ j αjh\nj . Next, the context vector and current hidden state are combined into an attention vector at = tanh(Wa[ct, ht]). Lastly, a softmax layer predicts the next token in the output sequence: p(yti |y<ti , xi) = softmax(Wsat).\nWe add two auxiliary decoders when applying CVT. The auxiliary decoders share embedding and LSTM parameters with the primary decoder, but have different parameters for the attention mechanisms and softmax layers. For the first one, we restrict its view of the input by applying attention dropout, randomly zeroing out a fraction of its attention weights. The second one is trained to predict the next word in the target sequence rather than the current one: pfutureθ (y t i |y<ti , xi) = softmax(W futures a future t−1 ). Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step. Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence. This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016)."
  }, {
    "heading": "4 Experiments",
    "text": "We compare Cross-View Training against several strong baselines on seven tasks:\nCombinatory Categorial Grammar (CCG) Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007).\nText Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000).\nNamed Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003).\nFine-Grained NER (FGN): We use the OntoNotes (Hovy et al., 2006) dataset.\nPart-of-Speech (POS) Tagging: We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).\nDependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0.\nMachine Translation: We use the EnglishVietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015). We report (tokenized) BLEU scores on the tst2013 test set.\nWe use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning."
  }, {
    "heading": "4.1 Model Details and Baselines",
    "text": "We apply dropout during training, but not when running the primary prediction module to produce soft targets on unlabeled examples. In addition to the auxiliary prediction modules listed in Section 3, we find it slightly improves results to add another one that sees the whole input rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations). Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers. See the supplementary material for full training details and hyperparameters. We compare CVT with the following other semi-supervised learning algorithms:\nWord Dropout. In this method, we only train the primary prediction module. When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token. This is similar to CVT in that it exposes the model to a restricted view of the input. However, it is less data efficient. By carefully designing the auxiliary prediction modules, it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once, rather than just one view at a time.\nVirtual Adversarial Training (VAT). VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words. Notably, the noise is chosen adversarially so it most changes the model’s prediction. This method was applied successfully to semi-supervised text classification\nby Miyato et al. (2017).\nELMo. ELMo incorporates the representations from a large separately-trained language model into a task-specific model. Our implementaiton follows Peters et al. (2018). When combining ELMo with multi-task learning, we allow each task to learn its own weights for the ELMo embeddings going into each prediction module. We found applying dropout to the ELMo embeddings was crucial for achieving good performance."
  }, {
    "heading": "4.2 Results",
    "text": "Results are shown in Table 1. CVT on its own outperforms or is comparable to the best previously published results on all tasks. Figure 3 shows an example win for CVT over supervised learning.\nOf the prior results listed in Table 1, only TagLM and ELMo are semi-supervised. These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier. Our base models use 1024 hidden units in their LSTMs (compared to 4096 in ELMo), require fewer training steps (around one pass over the billion-word benchmark rather than\nmany passes), and do not require a pipelined training procedure. Therefore, although they perform on par with ELMo, they are faster and simpler to train. Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi-task ones. We suspect there could be further gains from combining our method with language model pre-training, which we leave for future work.\nCVT + Multi-Task. We train a single sharedencoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones). Multi-task learning improves results on all of the tasks except fine-grained NER, sometimes by large margins. Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms. Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data.\nInterestingly, multi-task learning works better in conjunction with CVT than with ELMo. We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks. We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other ones, a well-known problem in many-task learning (Kirkpatrick et al., 2017). During multi-task CVT, the model makes predictions about unlabeled examples across all tasks, creating (artificial) all-tasks-labeled examples, so the model does not only see one task at a time. In fact, multi-task learning plus self training is similar to the Learning without Forgetting algorithm (Li and Hoiem, 2016), which trains the model to keep its predictions on an old task unchanged when learning a new task. To test the value of all-tasks-labeled examples, we trained a multi-task CVT model that only computes LCVT on one task at a time (chosen randomly for each unlabeled minibatch) instead of for all tasks in parallel. The one-at-a-time model performs substantially worse (see Table 2).\nModel Generalization. In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our different methods as they learn (see Figure 4). Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning. Interestingly, CVT continues to improve\nin dev set accuracy while close to 100% train accuracy for CCG, Chunking, and NER, perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set. We also show results for a smaller multi-task + CVT model. Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier. This suggests it is important to use sufficiently large neural networks for multitask learning: otherwise the model does not have the capacity to fit to all the training data.\nAuxiliary Prediction Module Ablation. We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3. We find that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps because they see a more restricted and challenging view of the input.\nTraining Models on Small Datasets. We explore how CVT scales with dataset size by varying the amount of training data the model has ac-\ncess to. Unsurprisingly, the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases (see Figure 5, left). Using only 25% of the labeled data, our approach already performs as well or better than a fully supervised model using 100% of the training data, demonstrating that CVT is particularly useful on small datasets.\nTraining Larger Models. Most sequence taggers and dependency parsers in prior work use small LSTMs (hidden state sizes of around 300) because larger models yield little to no gains in performance (Reimers and Gurevych, 2017). We found our own supervised approaches also do not benefit greatly from increasing the model size. In contrast, when using CVT accuracy scales better with model size (see Figure 5, right). This finding suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for NLP tasks with limited amounts of labeled data.\nGeneralizable Representations. Lastly, we explore training the CVT+multi-task model on five tasks, freezing the encoder, and then only training a prediction module on the sixth task. This tests whether the encoder’s representations generalize to a new task not seen during its training. Only training the prediction module is very fast because (1) the encoder (which is by far the slowest part of the model) has to be run over each example only once and (2) we do not back-propagate into the encoder. Results are shown in Table 4.\nTraining only a prediction module on top of multi-task representations works remarkably well,\noutperforming ELMo embeddings and sometimes even a vanilla supervised model, showing the multi-task model is building up effective representations for language. In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning."
  }, {
    "heading": "5 Related Work",
    "text": "Unsupervised Representation Learning. Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train\n“thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning.\nSelf-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set. Then, acting as a “student,” it is retrained on the new training set. Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015). It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018).\nConsistency Regularization. Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally flipping an image) to the student’s inputs. This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model. Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017). However, stochastic input alterations are more difficult to apply to discrete data like text, making consistency regularization less used for natural language processing. One solution is to add noise to the model’s word embeddings (Miyato et al., 2017); we compare against this approach in our experiments. CVT is easily applicable to text because it does not require changing the student’s inputs.\nMulti-View Learning. Multi-view learning on data where features can be separated into distinct subsets has been well studied (Xu et al., 2013). Particularly relevant are co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Belkin, 2005), which trains two models with disjoint views of the input. On unlabeled data, each one acts as a “teacher” for the other model. In contrast to these methods, our approach trains a single unified model where auxiliary prediction modules see different, but not necessarily indepen-\ndent views of the input.\nSelf Supervision. Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels. Recent work has jointly trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input.\nMulti-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks."
  }, {
    "heading": "6 Conclusion",
    "text": "We propose Cross-View Training, a new method for semi-supervised learning. Our approach allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available. We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Abi See, Christopher Clark, He He, Peng Qi, Reid Pryzant, Yuaho Zhang, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER. Kevin is supported by a Google PhD Fellowship."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning with pseudo-ensembles",
    "authors": ["Philip Bachman", "Ouais Alsharif", "Doina Precup."],
    "venue": "NIPS.",
    "year": 2014
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Combining labeled and unlabeled data with co-training",
    "authors": ["Avrim Blum", "Tom Mitchell."],
    "venue": "COLT. ACM.",
    "year": 1998
  }, {
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Machine Learning, 28:41–75.",
    "year": 1997
  }, {
    "title": "The IWSLT 2015 evaluation campaign",
    "authors": ["Mauro Cettolo", "Jan Niehues", "Sebastian Stüker", "Luisa Bentivogli", "Roldano Cattoni", "Marcello Federico."],
    "venue": "International Workshop on Spoken Language Translation.",
    "year": 2015
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."],
    "venue": "INTERSPEECH.",
    "year": 2014
  }, {
    "title": "Named entity recognition with bidirectional LSTM-CNNs",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "Transactions of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "ICML.",
    "year": 2008
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2011
  }, {
    "title": "Supervised learning of universal sentence representations from natural language inference data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes"],
    "year": 2017
  }, {
    "title": "Semi-supervised sequence learning",
    "authors": ["Andrew M Dai", "Quoc V Le."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Multitask self-supervised visual learning",
    "authors": ["Carl Doersch", "Andrew Zisserman."],
    "venue": "arXiv preprint arXiv:1708.07860.",
    "year": 2017
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "A joint many-task model: Growing a neural network for multiple nlp tasks",
    "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Learning distributed representations of sentences from unlabelled data",
    "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."],
    "venue": "HLT-NAACL.",
    "year": 2016
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."],
    "venue": "arXiv preprint arXiv:1503.02531.",
    "year": 2015
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn treebank",
    "authors": ["Julia Hockenmaier", "Mark Steedman."],
    "venue": "Computational Linguistics, 33(3):355–396.",
    "year": 2007
  }, {
    "title": "Ontonotes: the 90% solution",
    "authors": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."],
    "venue": "HLT-NAACL.",
    "year": 2006
  }, {
    "title": "Universal language model fine-tuning for text classification",
    "authors": ["Jeremy Howard", "Sebastian Ruder."],
    "venue": "ACL.",
    "year": 2018
  }, {
    "title": "Reinforcement learning with unsupervised auxiliary tasks",
    "authors": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "What is the best multi-stage architecture for object recognition",
    "authors": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann LeCun"],
    "venue": "In IEEE Conference on Computer Vision",
    "year": 2009
  }, {
    "title": "Sequencelevel knowledge distillation",
    "authors": ["Yoon Kim", "Alexander M. Rush."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Overcoming catastrophic forgetting in neural networks",
    "authors": ["Raia Hadsell."],
    "venue": "Proceedings of the National Academy of Sciences of the United States of America, 114 13:3521–3526.",
    "year": 2017
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Advances in neural information processing systems, pages 3294–3302.",
    "year": 2015
  }, {
    "title": "Temporal ensembling for semi-supervised learning",
    "authors": ["Samuli Laine", "Timo Aila."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Convolutional networks and applications in vision",
    "authors": ["Yann LeCun", "Koray Kavukcuoglu", "Clément Farabet."],
    "venue": "ISCAS. IEEE.",
    "year": 2010
  }, {
    "title": "Learning without forgetting",
    "authors": ["Zhizhong Li", "Derek Hoiem."],
    "venue": "ECCV.",
    "year": 2016
  }, {
    "title": "Neural machine translation (seq2seq) tutorial",
    "authors": ["Minh-Thang Luong", "Eugene Brevdo", "Rui Zhao."],
    "venue": "https://github.com/tensorflow/nmt.",
    "year": 2017
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "ICLR.",
    "year": 2016
  }, {
    "title": "Stanford neural machine translation systems for spoken language domains",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "IWSLT.",
    "year": 2015
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Stackpointer networks for dependency parsing",
    "authors": ["Xuezhe Ma", "Zecong Hu", "Jingzhou Liu", "Nanyun Peng", "Graham Neubig", "Eduard Hovy."],
    "venue": "ACL.",
    "year": 2018
  }, {
    "title": "Building a large annotated corpus of english: The Penn treebank",
    "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Computational linguistics, 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Learned in translation: Contextualized word vectors",
    "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher."],
    "venue": "NIPS.",
    "year": 2017
  }, {
    "title": "Effective self-training for parsing",
    "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson."],
    "venue": "ACL.",
    "year": 2006
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."],
    "venue": "NIPS.",
    "year": 2013
  }, {
    "title": "Adversarial training methods for semisupervised text classification",
    "authors": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Distributional smoothing with virtual adversarial training",
    "authors": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii."],
    "venue": "ICLR.",
    "year": 2016
  }, {
    "title": "Deep multitask learning for semantic dependency parsing",
    "authors": ["Hao Peng", "Sam Thomson", "Noah A. Smith."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "authors": ["Matthew E Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1802.05365.",
    "year": 2018
  }, {
    "title": "Improving language understanding by generative pre-training",
    "authors": ["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever."],
    "venue": "https://blog.openai.com/language-unsupervised.",
    "year": 2018
  }, {
    "title": "Unsupervised pretraining for sequence to sequence learning",
    "authors": ["Prajit Ramachandran", "Peter J Liu", "Quoc V Le."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Semi-supervised multitask learning for sequence labeling",
    "authors": ["Marek Rei."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging",
    "authors": ["Nils Reimers", "Iryna Gurevych."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "An overview of multi-task learning in deep neural networks",
    "authors": ["Sebastian Ruder."],
    "venue": "arXiv preprint arXiv:1706.05098.",
    "year": 2017
  }, {
    "title": "Strong baselines for neural semi-supervised learning under domain shift",
    "authors": ["Sebastian Ruder", "Barbara Plank."],
    "venue": "ACL.",
    "year": 2018
  }, {
    "title": "Regularization with stochastic transformations and perturbations for deep semisupervised learning",
    "authors": ["Mehdi Sajjadi", "Mehran Javanmardi", "Tolga Tasdizen."],
    "venue": "NIPS.",
    "year": 2016
  }, {
    "title": "Probability of error of some adaptive pattern-recognition machines",
    "authors": ["H Scudder."],
    "venue": "IEEE Transactions on Information Theory, 11(3):363–371.",
    "year": 1965
  }, {
    "title": "A coregularization approach to semi-supervised learning with multiple views",
    "authors": ["Vikas Sindhwani", "Mikhail Belkin."],
    "venue": "ICML Workshop on Learning with Multiple Views.",
    "year": 2005
  }, {
    "title": "Deep multi-task learning with low level tasks supervised at lower layers",
    "authors": ["Anders Søgaard", "Yoav Goldberg."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Fast and accurate sequence labeling with iterated dilated convolutions",
    "authors": ["Emma Strubell", "Patrick Verga", "David Belanger", "Andrew McCallum."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Learning general purpose distributed sentence representations via large scale multi-task learning",
    "authors": ["Sandeep Subramanian", "Adam Trischler", "Yoshua Bengio", "Christopher J Pal."],
    "venue": "ICLR.",
    "year": 2018
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "NIPS.",
    "year": 2014
  }, {
    "title": "Weightaveraged consistency targets improve semisupervised deep learning results",
    "authors": ["Antti Tarvainen", "Harri Valpola."],
    "venue": "Workshop on Learning with Limited Labeled Data, NIPS.",
    "year": 2017
  }, {
    "title": "Introduction to the CoNLL-2000 shared task: Chunking",
    "authors": ["Erik F Tjong Kim Sang", "Sabine Buchholz."],
    "venue": "CoNLL.",
    "year": 2000
  }, {
    "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F Tjong Kim Sang", "Fien De Meulder."],
    "venue": "HLT-NAACL.",
    "year": 2003
  }, {
    "title": "Improving the improved training of Wasserstein GANs",
    "authors": ["Xiang Wei", "Zixia Liu", "Liqiang Wang", "Boqing Gong."],
    "venue": "ICLR.",
    "year": 2018
  }, {
    "title": "Shortcut sequence tagging",
    "authors": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong."],
    "venue": "arXiv preprint arXiv:1701.00576.",
    "year": 2017
  }, {
    "title": "A survey on multi-view learning",
    "authors": ["Chang Xu", "Dacheng Tao", "Chao Xu."],
    "venue": "arXiv preprint arXiv:1304.5634.",
    "year": 2013
  }, {
    "title": "Unsupervised word sense disambiguation rivaling supervised methods",
    "authors": ["David Yarowsky."],
    "venue": "ACL.",
    "year": 1995
  }, {
    "title": "Stackpropagation: Improved representation learning for syntax",
    "authors": ["Yuan Zhang", "David Weiss."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Tri-training: Exploiting unlabeled data using three classifiers",
    "authors": ["Zhi-Hua Zhou", "Ming Li."],
    "venue": "IEEE Transactions on knowledge and Data Engineering.",
    "year": 2005
  }],
  "id": "SP:0c47cad9729c38d9db1f75491b1ee4bd883a5d4e",
  "authors": [{
    "name": "Kevin Clark",
    "affiliations": []
  }, {
    "name": "Minh-Thang Luong",
    "affiliations": []
  }, {
    "name": "Christopher D. Manning",
    "affiliations": []
  }, {
    "name": "Quoc V. Le",
    "affiliations": []
  }],
  "abstractText": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from taskspecific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multitask learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.1",
  "title": "Semi-Supervised Sequence Modeling with Cross-View Training"
}