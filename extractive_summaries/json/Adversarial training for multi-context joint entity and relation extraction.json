{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing (Zhang et al., 2017), POS tagging (Lample et al., 2016), relation extraction (dos Santos et al., 2015), translation (Bahdanau et al., 2015), and joint tasks (Miwa and Bansal, 2016). However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017)), this paper — to the best of our knowledge — is the first attempt investigating regularization effects of AT in a joint setting for two related tasks.\nWe start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once. Previously proposed models (summarized in Section 2) exhibit\nseveral issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016); Miwa and Bansal (2016); Li et al. (2017)), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Schütze (2017)), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017); Bekoulis et al. (2018a)). The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).\nTo evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness."
  }, {
    "heading": "2 Related work",
    "text": "Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once. These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity. Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal,\n2016; Zheng et al., 2017). Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). Gupta et al. (2016) propose the use of various manually extracted features along with RNNs. Adel and Schütze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, Bekoulis et al. (2018a) use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multiple relations from the same input. Then, we further extend this strong baseline using adversarial training.\nAdversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017), relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018). AT is considered as a regularization method. Unlike other regularization methods (i.e., dropout (Srivastava et al., 2014), word dropout (Iyyer et al., 2015)) that introduce random noise, AT generates perturbations that are variations of examples easily misclassified by the model."
  }, {
    "heading": "3 Model",
    "text": ""
  }, {
    "heading": "3.1 Joint learning as head selection",
    "text": "The baseline model, described in detail in Bekoulis et al. (2018b), is illustrated in Fig. 1. It aims to detect (i) the type and the boundaries of the entities and (ii) the relations between them. The input is a sequence of tokens (i.e., sentence) w = w1, ..., wn. We use character level embeddings to implicitly capture morphological features (e.g., prefixes and suffixes), representing each character by a vector (embedding). The character embeddings are fed to a bidirectional LSTM (BiLSTM) to obtain the character-based representation of the word. We also use pre-trained word embeddings.\nWord and character embeddings are concatenated to form the final token representation, which is then fed to a BiLSTM layer to extract sequential information.\nFor the NER task, we adopt the BIO (Beginning, Inside, Outside) encoding scheme. In Fig. 1, the B-PER tag is assigned to the beginning token of a ‘person’ (PER) entity. For the prediction of the entity tags, we use: (i) a softmax approach for the entity classification (EC) task (assuming entity boundaries given) or (ii) a CRF approach where we identify both the type and the boundaries for each entity. During decoding, in the softmax setting, we greedily detect the entity types of the tokens (i.e., independent prediction). Although independent distribution of types is reasonable for EC tasks, this is not the case when there are strong correlations between neighboring tags. For instance, the BIO encoding scheme imposes several constraints in the NER task (e.g., the B-PER and ILOC tags cannot be sequential). Motivated by this intuition, we use a linear-chain CRF for the NER task (Lample et al., 2016). For decoding, in the CRF setting, we use the Viterbi algorithm. During training, for both EC (softmax) and NER tasks (CRF), we minimize the cross-entropy loss LNER. The entity tags are later fed into the relation extraction layer as label embeddings (see Fig. 1), assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities.\nWe model the relation extraction task as a multi-label head selection problem (Bekoulis\net al., 2018b; Zhang et al., 2017). In our model, each word wi can be involved in multiple relations with other words. For instance, in the example illustrated in Fig. 1, “Smith” could be involved not only in a Lives in relation with the token “California” (head) but also in other relations simultaneously (e.g., Works for, Born In with some corresponding tokens). The goal of the task is to predict for each word wi, a vector of heads ŷi and the vector of corresponding relations r̂i. We compute the score s(wj , wi, rk) of word wj to be the head of wi given a relation label rk using a single layer neural network. The corresponding probability is defined as: P(wj , rk | wi; θ) = σ(s(wj , wi, rk)), where σ(.) is the sigmoid function. During training, we minimize the cross-entropy loss Lrel as:\nn∑ i=0 m∑ j=0 − logP(yi,j , ri,j | wi; θ) (1)\nwhere m is the number of associated heads (and thus relations) per word wi. During decoding, the most probable heads and relations are selected using threshold-based prediction. The final objective for the joint task is computed as LJOINT(w; θ) = LNER + Lrel where θ is a set of parameters. In the case of multi-token entities, only the last token of the entity can serve as head of another token, to eliminate redundant relations. If an entity is not involved in any relation, we predict the auxiliary “N” relation label and the token itself as head."
  }, {
    "heading": "3.2 Adversarial training (AT)",
    "text": "We exploit the idea of AT (Goodfellow et al., 2015) as a regularization method to make our model robust to input perturbations. Specifically, we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation (Miyato et al., 2017). This is similar to the concept introduced by Goodfellow et al. (2015) to improve the robustness of image recognition classifiers. We generate an adversarial example by adding the worst-case perturbation ηadv to the original embedding w that maximizes the loss function:\nηadv = argmax ‖η‖≤\nLJOINT(w + η; θ̂) (2)\nwhere θ̂ is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in Goodfellow et al. (2015) defined as: ηadv = g/ ‖g‖ , with g =\n∇wLJOINT(w; θ̂), where is a small bounded norm treated as a hyperparameter. Similar to Yasunaga et al. (2018), we set to be α √ D (where D is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the final loss is computed as: LJOINT(w; θ̂) + LJOINT(w + ηadv; θ̂)."
  }, {
    "heading": "4 Experimental setup",
    "text": "We evaluate our models on four datasets, using the code as available from our github codebase.1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset. For the CoNLL04 (Roth and Yih, 2004) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016); Adel and Schütze (2017). We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation. For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in Bekoulis et al. (2018a). For the Adverse Drug Events, ADE (Gurulingappa et al., 2012), we perform 10-fold cross-validation similar to Li et al. (2017). To obtain comparable results that are not affected by the input embeddings, we use the embeddings of the previous works. We employ early stopping in all of the experiments. We use the Adam optimizer (Kingma and Ba, 2015) and we fix the hyperparameters (i.e., α, dropout values, best epoch, learning rate) on the validation sets. The scaling parameter α is selected from {5e−2, 1e−2, 1e−3, 1e−4}. Larger values of α (i.e., larger perturbations) lead to consistent performance decrease in our early experiments. This can be explained from the fact that adding more noise can change the content of the sentence as also reported by Wu et al. (2017).\nWe use three types of evaluation, namely: (i) S(trict): we score an entity as correct if both the entity boundaries and the entity type are correct (ACE04, ADE, CoNLL04, DREC), (ii) B(oundaries): we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account (DREC) and (iii) R(elaxed): a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the\n1https://github.com/bekou/multihead_ joint_entity_relation_extraction\nboundaries are known (CoNLL04), to compare to previous works. In all cases, a relation is considered as correct when both the relation type and the argument entities are correct."
  }, {
    "heading": "5 Results",
    "text": "Table 1 shows our experimental results. The name of the dataset is presented in the first column while the models are listed in the second column. The proposed models are the following: (i) baseline: the baseline model shown in Fig. 1 with the CRF layer and the sigmoid loss, (ii) baseline EC: the proposed model with the softmax layer for EC, (iii) baseline (EC) + AT: the baseline regularized using AT. The final three columns present the F1 results for the two subtasks and their average performance. Bold values indicate the best results among models that use only automatically extracted features.\nFor ACE04, the baseline outperforms Katiyar and Cardie (2017) by ∼2% in both tasks. This improvement can be explained by the use of: (i) multi-label head selection, (ii) CRF-layer and (iii) character level embeddings. Compared to Miwa and Bansal (2016), who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand, Li et al. (2017) use the same model for\nthe ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various contexts. For the CoNLL04 dataset, we use two evaluation settings. We use the relaxed evaluation similar to Gupta et al. (2016); Adel and Schütze (2017) on the EC task. The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F1 score. We also report NER results on the same dataset and improve overall F1 score with∼1% compared to Miwa and Sasaki (2014), indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of ∼3% on both tasks compared to Bekoulis et al. (2018a), whose quadratic scoring layer complicates NER.\nTable 1 and Fig. 2 show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting. Moreover, as seen in Fig. 2, the performance of the models using AT is closer to maximum even from the early training epochs. Specifically, for ACE04, there is an improvement in both tasks as well as in the overall F1 performance (0.4%). For CoNLL04, we note an improvement in the overall F1 of 0.4% for the EC and 0.8% for the NER tasks, respectively. For the DREC dataset, in both settings, there is an overall improvement of ∼1%. Figure 2 shows that from the first epochs, the model obtains its maximum performance on the DREC validation set. Finally, for ADE, our AT model beats the baseline F1 by 0.7%.\nOur results demonstrate that AT outperforms the neural baseline model consistently, considering our experiments across multiple and more diverse datasets than typical related works. The im-\nprovement of AT over our baseline (depending on the dataset) ranges from∼0.4% to∼0.9% in terms of overall F1 score. This seemingly small performance increase is mainly due to the limited performance benefit for the NER component, which is in accordance with the recent advances in NER using neural networks that report similarly small gains (e.g., the performance improvement in Ma and Hovy (2016) and Lample et al. (2016) on the CoNLL-2003 test set is 0.01% and 0.17% F1 percentage points, while in the work of Yasunaga et al. (2018), a 0.07% F1 improvement on CoNLL2000 using AT for NER is reported). However, the relation extraction performance increases by∼1% F1 scoring points, except for the ACE04 dataset. Further, as seen in Fig. 2, the improvement for CoNLL04 is particularly small on the evaluation set. This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models, but this needs further investigation in future work."
  }, {
    "heading": "6 Conclusion",
    "text": "We proposed to use adversarial training (AT) for the joint task of entity recognition and relation extraction. The contribution of this study is twofold: (i) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model, with (ii) a large scale experimental evaluation. Experiments show that\nAT improves the results for each task separately, as well as the overall performance of the baseline joint model, while reaching high performance already during the first epochs of the training procedure."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for the time and effort they spent in reviewing our work, and for their valuable feedback."
  }],
  "year": 2019,
  "references": [{
    "title": "Global normalization of convolutional neural networks for joint entity and relation classification",
    "authors": ["Heike Adel", "Hinrich Schütze."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference for Learning Representations, San Diego, USA.",
    "year": 2015
  }, {
    "title": "Reconstructing the house from the ad: Structured prediction on real estate classifieds",
    "authors": ["Giannis Bekoulis", "Johannes Deleu", "Thomas Demeester", "Chris Develder."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Com-",
    "year": 2017
  }, {
    "title": "An attentive neural architecture for joint segmentation and parsing and its application to real estate ads",
    "authors": ["Giannis Bekoulis", "Johannes Deleu", "Thomas Demeester", "Chris Develder."],
    "venue": "Expert Systems with Applications, 102:100–112.",
    "year": 2018
  }, {
    "title": "Joint entity recognition and relation extraction as a multi-head selection problem",
    "authors": ["Giannis Bekoulis", "Johannes Deleu", "Thomas Demeester", "Chris Develder."],
    "venue": "Expert Systems with Applications, 114:34–45.",
    "year": 2018
  }, {
    "title": "The automatic content extraction (ace) program-tasks, data, and evaluation",
    "authors": ["George Doddington", "Alexis Mitchell", "Mark Przybocki", "Lance Ramshaw", "Stephanie Strassel", "Ralph Weischedel."],
    "venue": "Proceedings of the Fourth International Conference",
    "year": 2004
  }, {
    "title": "Explaining and harnessing adversarial examples",
    "authors": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy."],
    "venue": "Proceedings of the International Conference on Learning Representations, San Diego, USA.",
    "year": 2015
  }, {
    "title": "Table filling multi-task recurrent neural network for joint entity and relation extraction",
    "authors": ["Pankaj Gupta", "Hinrich Schütze", "Bernt Andrassy."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
    "year": 2016
  }, {
    "title": "Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical",
    "authors": ["Harsha Gurulingappa", "Abdul Mateen Rajput", "Angus Roberts", "Juliane Fluck", "Martin Hofmann-Apitius", "Luca Toldo"],
    "year": 2012
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }, {
    "title": "Going out on a limb: Joint extraction of entity mentions and relations without dependency trees",
    "authors": ["Arzoo Katiyar", "Claire Cardie."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the International Conference on Learning Representations, San Diego, USA.",
    "year": 2015
  }, {
    "title": "A neural joint model for entity and relation extraction from biomedical text",
    "authors": ["Fei Li", "Meishan Zhang", "Guohong Fu", "Donghong Ji."],
    "venue": "BMC Bioinformatics, 18(1):1–11.",
    "year": 2017
  }, {
    "title": "Joint models for extracting adverse drug events from biomedical text",
    "authors": ["Fei Li", "Yue Zhang", "Meishan Zhang", "Donghong Ji."],
    "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pages 2838–2844, New York,",
    "year": 2016
  }, {
    "title": "Incremental joint extraction of entity mentions and relations",
    "authors": ["Qi Li", "Heng Ji."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402–412, Baltimore, USA.",
    "year": 2014
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin,",
    "year": 2016
  }, {
    "title": "End-to-end relation extraction using LSTMs on sequences and tree structures",
    "authors": ["Makoto Miwa", "Mohit Bansal."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105–1116, Berlin,",
    "year": 2016
  }, {
    "title": "Modeling joint entity and relation extraction with table representation",
    "authors": ["Makoto Miwa", "Yutaka Sasaki."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1858–1869, Doha, Qatar. Association for",
    "year": 2014
  }, {
    "title": "Adversarial training methods for semisupervised text classification",
    "authors": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow."],
    "venue": "Proceedings of the International Conference on Learning Representations, Toulon, France.",
    "year": 2017
  }, {
    "title": "A linear programming formulation for global inference in natural language tasks",
    "authors": ["Dan Roth", "Wen-tau Yih."],
    "venue": "HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004), pages 1–8, Boston,",
    "year": 2004
  }, {
    "title": "Classifying relations by ranking with convolutional neural networks",
    "authors": ["Cicero dos Santos", "Bing Xiang", "Bowen Zhou."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Intriguing properties of neural networks",
    "authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus."],
    "venue": "Proceedings of the International Conference on Learning Representations, Banff,",
    "year": 2014
  }, {
    "title": "Adversarial training for relation extraction",
    "authors": ["Yi Wu", "David Bamman", "Stuart Russell."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1778–1783, Copenhagen, Denmark. Association for Computa-",
    "year": 2017
  }, {
    "title": "Robust multilingual part-of-speech tagging via adversarial training",
    "authors": ["Michihiro Yasunaga", "Jungo Kasai", "Dragomir Radev."],
    "venue": "Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2018
  }, {
    "title": "Dependency parsing as head selection",
    "authors": ["Xingxing Zhang", "Jianpeng Cheng", "Mirella Lapata."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: (Volume 1, Long Papers), pages 665–676,",
    "year": 2017
  }, {
    "title": "Joint entity and relation extraction based on a hybrid neural network",
    "authors": ["Suncong Zheng", "Yuexing Hao", "Dongyuan Lu", "Hongyun Bao", "Jiaming Xu", "Hongwei Hao", "Bo Xu."],
    "venue": "Neurocomputing, 257:59–66.",
    "year": 2017
  }],
  "id": "SP:2391d8bcef01fc55d928c44ce3b35462cb34a85d",
  "authors": [{
    "name": "Giannis Bekoulis",
    "affiliations": []
  }, {
    "name": "Johannes Deleu",
    "affiliations": []
  }, {
    "name": "Thomas Demeester",
    "affiliations": []
  }, {
    "name": "Chris Develder",
    "affiliations": []
  }],
  "abstractText": "Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).",
  "title": "Adversarial training for multi-context joint entity and relation extraction"
}