{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In machine learning, many fundamental quantities we care to optimize such as entropy, diversity, coverage, diffusion,\n1Harvard University. Correspondence to: Eric Balkanski <ericbalkanski@g.harvard.edu>, Yaron Singer <yaron@seas.harvard.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nand clustering are submodular functions. For the canonical problem of maximizing a non-decreasing submodular function under a cardinality constraint k, the celebrated greedy algorithm which iteratively adds elements whose marginal contribution is largest is known to achieve a 1 1/e approximation (Nemhauser et al., 1978) which is tight unless the algorithm uses exponentially-many queries in the size of the ground set n (Nemhauser & Wolsey, 1978).\nAlthough the simple greedy algorithm achieves an optimal approximation guarantee, it is highly adaptive. Informally, the adaptivity of an algorithm is the number of sequential rounds it requires when polynomially-many function evaluations can be executed in parallel in each round. The adaptivity of the greedy algorithm is k since it sequentially adds elements in k rounds, making linearly many function evaluations in each round to evaluate the marginal contribution of every element to the set of elements selected in the previous rounds. In general, k 2 ⌦(n) and the adaptivity, as well as the parallel runtime, of the greedy algorithm is hence linear in the size of the data.\nThe concept of adaptivity is generally well-studied in multiple areas of computer science as algorithms with low adaptivity lead to algorithms that can be parallelized efficiently (see Section 6 for further discussion). These areas include sorting and selection (Valiant, 1975; Cole, 1988; Braverman et al., 2016), communication complexity (Papadimitriou & Sipser, 1984; Duris et al., 1984; Nisan & Widgerson, 1991), multi-armed bandits (Agarwal et al., 2017), sparse recovery (Haupt et al., 2009a; Indyk et al., 2011; Haupt et al., 2009b), and property testing (Canonne & Gur, 2017; Buhrman et al., 2012; Chen et al., 2017).\nSince the greedy algorithm has linear adaptivity and the size of the ground set n can be large, a natural question is whether constant factor approximations with lower adaptivity are achievable. Somewhat surprisingly, until very recently ⌦(n) was the best known adaptivity required for a constant factor approximation to maximizing a monotone submodular maximization under a cardinality constraint.\nIn recent work (Balkanski & Singer, 2018) introduce an adaptive sampling technique for maximizing monotone submodular functions under a cardinality constraint. This technique produces an algorithm that is O(log n)-adaptive and achieves an approximation arbitrarily close to 1/3. Further-\nmore, this is tight in the sense that no algorithm can achieve a constant factor approximation with õ(log n) rounds.\nDespite this exponential improvement in adaptivity, the approximation ratio suffers. In experiments however, it seems that adaptive sampling does substantially better than 1/3 and in some cases comparable to those of the greedy algorithm that uses O(n) rounds. Ideally, if we can characterize the settings in which approximation guarantees for adaptive sampling are better than 1/3, these techniques could be implemented and dramatically reduce the parallel running time of applications that rely on large scale computing.\nWhy does adaptive sampling perform so well in practice?\nIn this paper we use the standard notion of curvature to reason about the strong performance of adaptive sampling. Curvature is a well-studied concept in the context of submodular optimization (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer & Bilmes, 2013; Iyer et al., 2013; Sviridenko et al., 2015; Balkanski et al., 2016). Recall that a function f : 2N ! R has curvature  if fS(a) (1 )f(a) for all S and a 62 S. Our main result in this paper is that even under very mild conditions of curvature on the function, adaptive sampling achieves an approximation guarantee that is arbitrarily close to 1/2 in O(log n) rounds. In particular we show:\n• An approximation arbitrarily close to max(1 , 1/2) in O ⇣ logn 1  ⌘ adaptive rounds if the function has\nbounded curvature  < 1,\n• An approximation arbitrarily close to 1 µ2µ+1 for a µ-homogeneous function with bounded curvature,\n• A tradeoff between the approximation guarantee and the number of adaptive rounds of the algorithm,\n• A tight lower bound of log n adaptive rounds, up to lower order terms, to obtain a 1/2 approximation for functions with bounded curvature  < 1,\n• Experiments over two real-world datasets demonstrating the effectiveness of adaptive sampling in practice and the effect of curvature.\nThe homogeneity condition, which we introduce to further improve the approximation guarantee, resembles the large market assumption in mechanism design, e.g. (Bei et al., 2012; Anari et al., 2014; Balkanski & Hartline, 2016), in the sense that it bounds the impact of a single element on the overall objective.\nWe consider a simple, yet useful, operation to alter general submodular functions into functions with bounded curvature, which we call curvaturing and which was previously\nused in Iyer et al. (2013). This technique can be interpreted as an analogue to regularization in convex optimization.\nInterestingly, we use curvaturing to obtain both upper and lower bounds. We use curvaturing on general submodular functions to extend the 1/2 approximation guarantee to functions with unbounded curvature, at the cost of an additional additive term in the approximation. We also give a reduction from lower bounds for functions with bounded curvature to lower bounds for general submodular functions using this same technique of curvaturing. With this reduction, a previous lower bound on the number of rounds needed to obtain a constant approximation implies the new lower bound for functions with bounded curvature.\nPaper organization. We first cover preliminary definitions in Section 2. The ADAPTIVE-SAMPLING algorithm is presented and analyzed in Section 3. We then give a lower bound in Section 4. The experiments are in Section 5. Finally, related work is in Section 6."
  }, {
    "heading": "2. Preliminaries",
    "text": "A function f : 2N ! R+ is submodular if the marginal contributions fS(a) := f(S [ a) f(S) of an element a 2 N to a set S ✓ N are diminishing, i.e., fS(a) fT (a) for all a 2 N \\ T and S ✓ T , and is monotone if f(S)  f(T ) for all S ✓ T . A submodular function f is also subadditive, meaning f(S [ T )  f(S) + f(T ) for all S ✓ T .We assume that f is non-negative, i.e., f(S) 0 for all S ✓ N , which is standard.\nInformally, the adaptivity of an algorithm is the number of sequential rounds of queries it makes, where every round allows for polynomially-many parallel queries. Definition. Given a function f , an algorithm is r-adaptive\nif every query f(S) for the value of a set S occurs at a round i 2 [r] such that S is independent of the values f(S0) of all other queries at round i.\nA submodular function f has curvature , 0    1, if\nfS(a) (1 )f(a)\nfor all sets S and elements a 62 S. A useful corollary is that fS(T ) (1 )f(T ) for all non intersecting sets |S \\ T | = 0. Given a function f , the curvaturing operation produces f̃(S) = f(S) + (1 )|S|. Even though f might have unbounded curvature, f̃ has curvature  when f is normalized such that maxa2N f(a)  1.\nFinally, a function f is µ-homogeneous, µ 0, if\nf(a)  (1 + µ) OPT\nk\nfor all a 2 N , where OPT := maxS:|S|k f(S) is the value of the optimal solution."
  }, {
    "heading": "3. The Algorithm",
    "text": "In this section, we present and analyze the ADAPTIVESAMPLING algorithm. By its design, this algorithm terminates after O(log n) rounds and its approximation ratio is arbitrarily close to 1/3. We show that if the function respects a mild curvature condition such that it has bounded curvature  < 1, the approximation ratio of the algorithm is arbitrarily close to max(1 , 1/2), in O ⇣ logn 1  ⌘ rounds. In addition, if the function is µ-homogeneous then the approximation ratio of the algorithm is further improved to being arbitrarily close to 1 µ2µ+1 .\nDescription of the algorithm. The ADAPTIVESAMPLING algorithm is a generalization of the algorithm in Balkanski & Singer (2018) designed to achieve superior approximation guarantees for bounded curvature. The algorithm maintains two solutions X and S, initialized to the empty set and the ground set N respectively. At every round, the algorithm either adds k\nr elements to X\nor discards from S a constant fraction of its remaining elements. The algorithm terminates when |X| = k or alternatively when sufficiently many elements have been discarded to get |X [ S|  k. Thus, with r = O(log n), the algorithm has at most logarithmic many rounds. The algorithm is formally described below.\nAlgorithm 1 ADAPTIVE-SAMPLING input threshold , approximation ↵, samples m, rounds r\nInitialize X ;, S N while |X| < k and |X [ S| > k do\nupdate D to be uniform over subsets of S of size k r R argmaxR2{Ri⇠D}mi=1 fX(R) M top k r\nvalued elements a with respect to fX(a) if max {fX(R), fX(M)} ↵\nr OPT then\nadd argmax{fX(R), fX(M)} to X , discard it from S else\ndiscard {a : ER⇠D ⇥ fX[R\\{a}(a) ⇤ < } from S\nreturn X if |X| = k, or X [ S otherwise\nAlgorithm 1 generalizes the adaptive sampling algorithm in Balkanski & Singer (2018) by not only considering the best sample R when adding elements to X , but also the set M of top k/r elements a with largest contribution fX(a). This generalization is needed to obtain, by a simple argument about curvature, the 1  term in the approximation.\nAlgorithm 1 is an idealized version of the algorithm since we cannot exactly compute expectations and OPT is unknown. In practice, the expectations can be estimated arbitrarily well by sampling and the algorithm can be executed multiple times with different guesses for OPT. The full algorithm is described in Appendix B.1. For readability we present\nthe analysis of the idealized version as above which easily extends to the full algorithm, as shown in Appendix B.3.\nGood and bad optimal elements. The key idea in analyzing the approximation ratio of adaptive sampling as a function of curvature requires partitioning the elements in the optimal solution O into good optimal elements and bad optimal elements, as we now define. The good and bad optimal elements play complementary roles in the analysis. The good optimal elements O+ allow analyzing the approximation ratio in terms of curvature and bad optimal elements O enable bounding the value lost in terms of homogeneity.\nDefinition 1. Let X be the set in ADAPTIVE-SAMPLING when the algorithm terminates and ✏ > 0. Given some arbitrary ordering on the elements in O s.t. O = {o1, . . . , ok}, for every i 2 [k], let Oi = {o1, . . . , oi}. The set of good optimal elements O + is the set of elements in O whose\nmarginal contribution to X [Oj 1 exceeds (1 + ✏) , i.e. O\n+ := {oj 2 O : fX[Oj 1(oj) (1 + ✏) }. The set of bad optimal elements is O = O \\O+."
  }, {
    "heading": "3.1. Curvature",
    "text": "The analysis of the approximation ratio requires bounding the value of the set of elements discarded S from the optimal solution in two major steps:\n1. We first bound the value f(S \\ O+) of good optimal elements that are discarded by 11 fX(S \\O\n+). We then bound fX(S \\O+) by |O+ \\ S |↵OPTk . It then follows that f(S \\ O+)  ↵1 OPT, which is arbitrarily bad as the curvature increases;\n2. A second important step in the analysis is bounding |O + \\ S | by ER⇠D [f(R)] / . The partition of O\ninto O+ and O is what makes this step possible as |O \\ S | can be arbitrarily close to k in general. The analysis distinguishes between elements in O+ that must have large value and elements in S that must have small value to improve the bound on |O+ \\ S |.\nLemma 1. Let f be a monontone submodular function with\ncurvature  and rd be the number of rounds where elements\nwith contribution less than are discarded, then w.h.p.,\nf(S \\O+) \n1 + ✏ 1\n· rd · (↵+ ✏)\n(1 ) · r · OPT.\nProof. Let Xi and Di denote the set X and distribution D at a round i of the algorithm. An optimal element o 2 O is among the elements S\ni discarded at round i\nif ER⇠Di ⇥ fXi[R\\{o}(o) ⇤ < . This bound on the value of elements o 2 S \\O is with respect to Xi. We use curvature to relate the value of the set S \\ O+ to its marginal\ncontribution to X as follows:\nf S \\O + \n1\n1  · fX(S \\O\n+)\n= 1\n1  · fX([ rd i=1(S i \\O +))\n 1\n1 \nrdX\ni=1\nfX O\n+ \\ S\ni\nwhere the first inequality is by curvature and the last by subadditivity. Next, using the definitions of O+ and S\ni , we\nboth lower and upper bound fX(O+ \\ S i ) by terms that are dependent on |O+ \\ S\ni |. First, the value of O+ \\ S i\nis upper bounded using the threshold for elements to be in S\ni :\nfX(O + \\ S i )\n fXi(O + \\ S i )\n E R⇠Di\n⇥ fXi[R(O + \\ S i ) ⇤ + E\nR⇠Di [fXi(R)]\n E R⇠Di\n2 4 X\na2O+\\S i\nfXi[R(a)\n3\n5+ E R⇠Di [fXi(R)]\n\nX\na2O+\\S i\nE R⇠Di\n⇥ fXi[R\\{a}(a) ⇤ + E\nR⇠Di [fXi(R)]\n |O + \\ S i | · + E\nR⇠Di [fXi(R)]\nwhere the first inequality is by submodularity, the second by monotonicity, the third by submodularity, the fourth by linearity of expectation and monotonicity, and the fifth by the definition of S\ni . Next, we lower bound fX(O+ \\ S i )\nusing submodularity and the definition of O+:\nfX(O + \\ S i )\nX\noj2O+\\S i\nfX[Oj 1(oj)\n|O + \\ S\ni | · (1 + ✏) .\nCombining these upper and lower bounds on f(O+ \\ S i ), we obtain the following bound on the number of good optimal elements that are discarded,\n|O + \\ S i |  (✏ ) 1 · E\nR⇠Di [f(R)] .\nThen, by adding this last bound to the upper bound for fX(O+ \\ S i ), we get\nfX(O + \\ S i )  |O+ \\ S i | · + ER⇠Di [f(R)]\n 1 + ✏ 1 · ER⇠Di [f(R)] .\nFinally, by standard concentration bounds (Lemma 7 in Appendix B.1), with m = (r/✏)2 log (2rd/ ), w.p. 1 /rd, fXi (Ri) ER⇠D [fXi(R)] ✏OPT/r where Ri is the sample with largest contribution to Xi at round i. By a\nunion bound this holds for all rd rounds where elements are discarded w.p. 1 . By the algorithm, we have fXi (Ri) < ↵ r OPT at a round where elements are discarded. Thus, ER⇠D [fXi(R)]  ↵+✏r OPT at rounds i 2 rd, and\nf S \\O + \n1\n1 \nrdX\ni=1\nfX(O + \\ S i )\n 1\n1 \nrdX\ni=1\n1 + ✏ 1 E\nR⇠Di [fXi(R)]\n 1\n1 \n1 + ✏ 1\nrd\n↵+ ✏\nr OPT.\nThe next lemma shows that the algorithm obtains a 1  approximation in one round with the k elements with largest marginal contribution, the proof is deferred to Appendix A and follows easily from the definition of curvature.\nLemma 2. Let f be a monotone submodular function with curvature , then ADAPTIVE-SAMPLING is a non-adaptive algorithm that obtains a (1 )-approximation with r = 1 and ↵ = 1 .\nCombining the two previous lemmas, we get the general theorem about the approximation ratio obtained for functions with bounded curvature. For the remaining of this section, the parameters of ADAPTIVE-SAMPLING are sample complexity m = (r/✏)2 log\n2 log1+✏(n)/ , ↵ = 1/2 ✏ and\n= (1 + ✏)OPT/(2k).\nTheorem 1. Let f be a monotone submodular function with curvature , then, for any ✏ > 0, ADAPTIVE-SAMPLING is a log1+✏(n) + r adaptive algorithm which obtains w.h.p. the following approximation:\nmax ✓ 1 , 1\n2\n3✏\n2\nlog1+✏(n)\n(1 ) · ✏ · r\n◆ .\nProof Sketch, full proof in Appendix A. First, we show that f(S [ X) f(O+ [ X) f(O+ \\ S ) by subadditivity and monotonicity. Then, by the definition of O+ and O\n, we get that f(O+ [X) OPT |O |(1 + ✏) . By combining these two inequalities with Lemma 1, we obtain f(S[X) 12 ✏ 1 1  1 + ✏ 1 log1+✏(n) r\n. When the algorithm returns X , f(X) P r\ni=1 ↵ r OPT = 1 2 ✏ OPT.\nBy Lemma 2, the algorithm obtains a 1  approximation. Finally, we show that a (1+ ✏) fraction of the remaining elements are discarded at every round, so the number of rounds where elements are discarded is at most log1+✏(n).\nTradeoff between approximation and rounds. An interesting characteristic of this result is the tradeoff between the approximation and the number of rounds of the algorithm as a function of . In contrast to previous curvature-dependent approximation guarantees that decrease as a function of ,\nCorollary 1 shows that as  increases, an increase in the number of rounds maintains an approximation arbitrarily close to 1/2. We illustrate this tradeoff in Figure 1.\nCorollary 1. Let f be a monotone submodular function with curvature , then, for any ✏ > 0, ADAPTIVESAMPLING is a ⇣ 1\n1  8 ✏2 + 1 ⌘ log1+✏(n)-adaptive algo-\nrithm that obtains w.h.p. a 1 2 ✏ approximation.\nIn particular, when   1 1/ poly(log n), this corollary gives a 1/2 ✏ approximation in poly(log n) rounds.\nUnbounded curvature. In the case where the curvature is unbounded (when  > 1 1/ poly(log n)), we obtain approximation guarantees by altering the function via curvaturing. Curvaturing creates a surrogate function with improved curvature such that the previous approximation guarantee holds for the surrogate function. This then implies an approximation guarantee for f with an additional additive loss. We assume that f is normalized. Recall that a function is normalized if maxa2N f(a)  1.\nCorollary 2. Let f be a normalized monotone submodular function and S be the solution obtained by ADAPTIVESAMPLING over the function f̃ with curvature  = 1 1/ log n obtained via curvaturing f . Then,\nf(S)\n✓ 1\n2 ✏\n◆ OPT\nk\nlog n\n✓ 1\n2 + ✏\n◆✓ 1 + 1\nlog n\n◆ .\nProof. The function f is curvatured with  = 1 1/ log n to obtain the following surrogate function f̃ :\nf̃(S) = ✓ 1\n1\nlog n\n◆ f(S) + |S|\nlog n .\nNote that the optimal solution O of size k for f is also an optimal solution for f̃ . Let S be the solution obtained by\nADAPTIVE-SAMPLING on f̃ . By Corollary 1, it is a 1/2 ✏ approximation to f̃(O). We get\nf(S) = ✓ f̃(S)\nk\nlog n\n◆✓ 1\n1\nlog n\n◆ 1\n✓✓ 1\n2 ✏\n◆ f̃(O)\nk\nlog n\n◆✓ 1\n1\nlog n\n◆ 1\n✓ 1\n2 ✏\n◆ f(O)\nk\nlog n\n✓ 1\n2 + ✏\n◆✓ 1 + 1\nlog n\n◆ ."
  }, {
    "heading": "3.2. Homogeneity",
    "text": "The bad optimal elements O are used to analyze the approximation in terms of the homogeneity condition. Homogeneity plays a complementary role to curvature which, as previously shown, bounds the loss from good optimal elements O+. The following lemma improves the bound on the number |O | of bad optimal elements as a function of the homogeneity parameter µ, which then implies an improved approximation guarantee (proof deferred to Appendix A). Lemma 3. Let f be a µ-homogeneous monotone submodu-\nlar function. Then,\n|O | \nµ\nµ+ (1 ✏)/2 · k.\nCombining the bounds on the losses due to both good and bad optimal elements, we obtain an approximation guarantee arbitrarily close to 1 for functions with arbitrarily good homogeneity when the curvature  is bounded. Theorem 3. Let f be a µ-homogeneous monotone submodular function with curvature  < 1, then ADAPTIVESAMPLING is a ⇣ 1\n1  1 ✏2 + 1 ⌘ log1+✏(n) adaptive algo-\nrithm which obtains w.h.p. the following approximation:\n1 µ · (1 + ✏)2\n2µ+ 1 ✏ ✏.\nProof Sketch, full proof in Appendix A. Similarly as for Theorem 1, we have f(S[X) f(O+[X) f(O+\\S ) and f(O+ [X) OPT |O |(1 + ✏) . By combining these two inequalities with Lemma 1 and Lemma 3, we then get the desired approximation guarantee. The approximation obtained when the algorithm returns X and the number of rounds follow similarly as for Theorem 1."
  }, {
    "heading": "4. Lower Bound",
    "text": "In this section, we show that the number of rounds needed to obtain a 1 + o(1) approximation is ⌦(log n/ log log n). Together with Corollary 1 from the previous section, this provides a tight, up to lower order factors, characterization\nof the number of rounds needed to obtain a 1/2 ✏ approximation for functions with bounded curvature. This hardness result is achieved with a general lemma that uses curvaturing to reduce the problem of showing lower bounds for submodular functions with bounded curvature to lower bounds for general submodular functions.\nLemma 4. Assume F is a class of normalized monotone\nsubmodular functions such that OPT (1 ✏)k, ✏ > 0, that cannot be ↵ approximated in r rounds. Then there exists a class of monotone submodular functions F 0 with curvature  that cannot be ↵+ 1 1 ✏ approximated in r rounds.\nProof Sketch, full proof in Appendix C. We consider the class of functions F 0 obtained by curvaturing F . We then show that an algorithm that is an ↵+ 1 1 ✏ approximation algorithm for F 0 is an algorithm that is an ↵ approximation for F , which does not exist in r rounds by the assumption on the class of functions F .\nWith this reduction, the hardness result in Balkanski & Singer (2018) for general monotone submodular functions implies the following lower bound for submodular functions with bounded curvature.\nTheorem 4. There is no logn\n12 log logn -adaptive algorithm that\nobtains, with probability !(1/n), an approximation of\n1 \n1 2logn +\n\nlog n\nfor monotone submodular functions with curvature .\nProof Sketch, full proof in Appendix C. The hardness result in Balkanski & Singer (2018) shows that there is no\nlogn 12 log logn -adaptive algorithm that obtains, w.p. !(1/n), a 1logn -approximation for general monotone submodular functions. After normalizing the hard class of functions, Lemma 4 immediately implies the hardness result."
  }, {
    "heading": "5. Experiments",
    "text": "We conduct experiments on two datasets to empirically evaluate the performance of the adaptive sampling algorithm. We observe that it performs almost as well as the standard greedy algorithm, which achieves the optimal 1 1/e approximation, and outperforms two simple algorithms with low adaptivity. These experiments indicate that in practice, adaptive sampling performs significantly better than its worst-case 1/3 approximation guarantee."
  }, {
    "heading": "5.1. Experimental setup",
    "text": "We begin by describing the two datasets and the benchmarks for the experiments."
  }, {
    "heading": "5.1.1. DATASETS",
    "text": "Movie recommendation system. The goal of a movie recommendation system is to find a personalized and diverse collection of movies to recommend to an individual user, given ratings of movies that this user has already seen. We use the MovieLens 1M dataset (Harper & Konstan., 2015) which contains 1 million ratings from 6000 users on 4000 movies. A standard approach to solve the problem of movie recommendation is low-rank matrix completion. This approach models the problem as an incomplete rating matrix with users as rows and movies as columns and aims to produce a complete matrix which agrees with the incomplete matrix and has low rank. For a given user ui, the completed matrix then gives a predicted score for each movie mj which we denote by vi,j . A high quality recommendation must also be diverse. We add a diversity term in the objective that is a coverage function C where C(S) is the number of different genres covered by movies in S.1 We obtain the following objective for user ui:\nfi,↵(S) = (1 ↵) X\nmj2S vi,j + ↵C(S)\nwhere ↵ is a parameter controlling the weight of the objective on the individual movie scores versus the diversity term. Similar submodular objectives for movie recommendation systems have previously been used, e.g., (Mitrovic et al., 2017; Lindgren et al., 2015; Mirzasoleiman et al., 2016; Feldman et al., 2017). The algorithm used for low-rank matrix completion is an iterative low-rank SVD decomposition algorithm from the python package fancyimpute (Rubinsteyn & Feldman, 2017) corresponding to the SVDimpute algorithm analyzed in Troyanskaya et al. (2001). Unless otherwise specified, we set k = 100, ↵ = 0.6, and number\n1Each movie has one genre, for example, ”romantic comedy” is one genre, which is different than the ”romantic drama” genre.\nof rounds of adding elements r = 4 for adaptive sampling.\nTaxi dispatch. In the taxi dispatch application, there are k taxis and the goal is to pick the k best locations to cover the maximium number of potential customers. We use 2 millions taxi trips in June 2017 from the New York City taxi and limousine commission trip record dataset (NYCTaxi-Limousine-Commission, 2017), illustrated in Figure 2. We assign a weight wi to each neighborhood ni 2 N that is equal to the number of trips where the pick-up was in neighborhood ni, where N is the collection of all neighborhoods. We then build a coverage function CR(S) which is equal to the sum of the weights of neighborhoods ni that are reachable from at least one location in S, where reachable means nj 2 S is at “as the crow flies” distance d(i, j)  R from ni. More precisely,\nCR(S) = X\nni2N 19nj2S:d(i,j)R · wi.\nUnless otherwise specified, the parameters are k = 30, radius R = 1.5km, and number of rounds of adding elements for adaptive sampling r = 3."
  }, {
    "heading": "5.1.2. BENCHMARKS",
    "text": "We compare the performance of ADAPTIVE-SAMPLING with three algorithms. The GREEDY algorithm, which adds the element with largest marginal contribution at each round, is the standard algorithm for submodular optimization and obtains the optimal 1 e 1 approximation (and (1 e )/ for functions with curvature  (Conforti & Cornuéjols, 1984)) in linearly many rounds. It is used as an upper bound to measure the performance cost of obtaining logarithmic adaptivity with ADAPTIVE-SAMPLING. The TOPK algorithm picks the k elements a with largest singleton value f(a). This simple algorithm has one adaptive round and obtains a 1  approximation for submodular functions with curvature . Its low adaptivity and its approximation guarantee make it a natural benchmark. Finally,\nRANDOM simply returns a random subset of size k and has 0 rounds of adaptivity."
  }, {
    "heading": "5.2. Experimental results",
    "text": "General performance. We first analyze how the value of the solutions maintained by each algorithm evolves at every round. In Figures 3(a) and 3(b), we observe that ADAPTIVE-SAMPLING achieves a final value that is close to the one obtained by GREEDY, but in a much smaller number of rounds. ADAPTIVE-SAMPLING also significantly outperforms the two simple algorithms. There are rounds where the value of the ADAPTIVE-SAMPLING solution does not increase, these correspond to rounds where elements are discarded, and which allow to then pick better elements in future rounds. For the movie recommender application, the value of the solution obtained by GREEDY increases linearly but we emphasize that this function is not linear, as movies that have the same genre as a movie already picked have their marginal contribution to the solution that decreases by ↵. In these experiments, ADAPTIVE-SAMPLING uses only 100 samples at every round. In fact, we observe very similar performance for ADAPTIVE-SAMPLING whether it uses 10 or 10K samples per round. Thus, the sample complexity is not an issue for ADAPTIVE-SAMPLING in practice and can be much lower than the theoretical sample complexity needed for the approximation guarantee.\nThe role of curvature and homogeneity. Next, we analyze the performance of the algorithms as a function of curvature and homogeneity. Both functions have curvature  = 0 when ↵ = 0 and R = 0 respectively, and the cuvature increase as ↵ and the radius increase. The movie application has good homogeneity with µ close to 0 regardless of ↵ since optimal movies all have similarly high predicted ratings and different genres. On the other hand, homogeneity gets worse as the radius increases for the taxi dispatch application since one neighborhood covers a larger number of neighborhoods as the radius increases.\nAgain, we observe in Figures 3(c) and 3(d) that ADAPTIVESAMPLING obtains a solution of value of very close to the value obtained by GREEDY, and significantly better than the two simple algorithms in general for any ↵ and any radius. As it is implied by the theoretical bounds, ADAPTIVESAMPLING, TOPK, and GREEDY all perform arbitrarily close to the optimal solution when the curvature is small. The gap between ADAPTIVE-SAMPLING and GREEDY is the largest for mid-range values of ↵ and R. This can be explained by the design of the functions, which become “easier” to optimize as ↵ and R increase since any neighborhood covers a large fraction of the total value when R is large and since there is always a large number of movies that have a genre that is not yet in the current solution.\nFigures 4(c) and 4(d) show how many rounds are needed by GREEDY and ADAPTIVE-SAMPLING to obtain 95 percent of the value of the solution of GREEDY. When the curvature is small, the k elements with largest contribution is a good solution so ADAPTIVE-SAMPLING only needs one round, whereas the value obtained by GREEDY grows linearly so it needs to be close to 95 percent of its k rounds. For the movie recommendation, since the value obtained by GREEDY always grows almost linearly, GREEDY always needs 95 rounds for k = 100. For the taxi dispatch, since a small number of elements can have very large value for large radius, the number of rounds needed by GREEDY decreases for large radius, as well as for ADAPTIVE-SAMPLING. Similarly as in the two previous figures with the approximation, we observe that the setting where ADAPTIVE-SAMPLING needs the most number of rounds is for mid-range radius.\nNumber of rounds r versus performance. There is a tradeoff between the number of rounds of ADAPTIVESAMPLING and its performance. This tradeoff is more apparent for the taxi application than for the movie recommender application where ADAPTIVE-SAMPLING obtains high value after 2 rounds (Figures 4(a) and 4(b)). Overall, ADAPTIVE-SAMPLING obtains a high value in a small number of rounds, but this value can be slightly improved by increasing the number of rounds of ADAPTIVE-SAMPLING."
  }, {
    "heading": "6. Related Work",
    "text": "Map-Reduce. There is a long line of work on distributed submodular optimization in the Map-Reduce model (Kumar et al., 2015; Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Mirzasoleiman et al., 2015; Barbosa et al., 2015; 2016; Epasto et al., 2017). Map-Reduce is designed to tackle issues related to massive data sets that are too large to either fit or be processed by a single machine. Instead of addressing distributed challenges, adaptivity addresses the issue of sequentiality, where query-evaluation time is the main runtime bottleneck and where these evaluations can be parallelized. The existing Map-Reduce algorithms for submodular optimization have adaptivity that is linear in n in the worst-case. This high adaptivity is caused by the distributed algorithms which are run on each machine, which are variants of the greedy algorithm and thus have adaptivity at least linear in k.\nParallel computing and depth. In the PRAM model, the notion of depth is closely related to the concept of adaptivity. The depth of a PRAM algorithm is the number of parallel steps of this algorithm on a shared memory machine with any number of processors, in other words, it is the longest chain of dependencies of the algorithm, including operations which are not necessarily queries. The problem of designing low-depth algorithms is well-studied , e.g. (Blelloch, 1996; Blelloch et al., 2011; Berger et al., 1989; Rajagopalan & Vazirani, 1998; Blelloch & Reid-Miller, 1998; Blelloch et al., 2012). Our positive results extend to the PRAM model with the adaptive sampling algorithm having Õ(log2 n · df ) depth, where df is the depth required to evaluate the function on a set. While the PRAM model assumes that the input is loaded in memory, we consider the value query model where the algorithm is given oracle access to a function of potentially exponential size.\nMore broadly, there has been recent interest in machine learning to scale submodular optimization algorithms for applications over large datasets (Jegelka et al., 2011; 2013; Wei et al., 2014; Nishihara et al., 2014; Pan et al., 2014)."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by a Google PhD Fellowship, NSF grant CAREER CCF-1452961, BSF grant 2014389, NSF USICCS proposal 1540428, Google research award, and a Facebook research award."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning with limited rounds of adaptivity: Coin tossing, multiarmed bandits, and ranking from pairwise comparisons",
    "authors": ["A. Agarwal", "S. Agarwal", "S. Assadi", "S. Khanna"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "Mechanism design for crowdsourcing: An optimal 1-1/e competitive budgetfeasible mechanism for large markets",
    "authors": ["N. Anari", "G. Goel", "A. Nikzad"],
    "venue": "In FOCS,",
    "year": 2014
  }, {
    "title": "Bayesian budget feasibility with posted pricing",
    "authors": ["E. Balkanski", "J.D. Hartline"],
    "venue": "In WWW,",
    "year": 2016
  }, {
    "title": "The adaptive complexity of maximizing a submodular function",
    "authors": ["E. Balkanski", "Y. Singer"],
    "venue": "In STOC,",
    "year": 2018
  }, {
    "title": "The power of optimization from samples",
    "authors": ["E. Balkanski", "A. Rubinstein", "Y. Singer"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "The power of randomization: Distributed submodular maximization on massive datasets",
    "authors": ["R. Barbosa", "A. Ene", "H. Nguyen", "J. Ward"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "A new framework for distributed submodular maximization",
    "authors": ["Barbosa", "R. d. P", "A. Ene", "H.L. Nguyen", "J. Ward"],
    "venue": "In FOCS,",
    "year": 2016
  }, {
    "title": "Budget feasible mechanism design: from prior-free to bayesian",
    "authors": ["X. Bei", "N. Chen", "N. Gravin", "P. Lu"],
    "venue": "In STOC,",
    "year": 2012
  }, {
    "title": "Efficient nc algorithms for set cover with applications to learning and geometry",
    "authors": ["B. Berger", "J. Rompel", "P.W. Shor"],
    "venue": "In FOCS,",
    "year": 1989
  }, {
    "title": "Programming parallel algorithms",
    "authors": ["G.E. Blelloch"],
    "venue": "Communications of the ACM,",
    "year": 1996
  }, {
    "title": "Fast set operations using treaps",
    "authors": ["G.E. Blelloch", "M. Reid-Miller"],
    "venue": "In SPAA, pp",
    "year": 1998
  }, {
    "title": "Linearwork greedy parallel approximate set cover and variants",
    "authors": ["G.E. Blelloch", "R. Peng", "K. Tangwongsan"],
    "venue": "In SPAA, pp",
    "year": 2011
  }, {
    "title": "Parallel and i/o efficient set covering algorithms",
    "authors": ["G.E. Blelloch", "H.V. Simhadri", "K. Tangwongsan"],
    "venue": "In SPAA,",
    "year": 2012
  }, {
    "title": "Parallel algorithms for select and partition with noisy comparisons",
    "authors": ["M. Braverman", "J. Mao", "S.M. Weinberg"],
    "venue": "In STOC,",
    "year": 2016
  }, {
    "title": "The non-adaptive query complexity of testing kparities",
    "authors": ["H. Buhrman", "D. Garcı́a-Soriano", "A. Matsliah", "R. de Wolf"],
    "venue": "arXiv preprint arXiv:1209.3849,",
    "year": 2012
  }, {
    "title": "An adaptivity hierarchy theorem for property testing",
    "authors": ["C. Canonne", "T. Gur"],
    "venue": "arXiv preprint arXiv:1702.05678,",
    "year": 2017
  }, {
    "title": "Settling the query complexity of non-adaptive junta testing",
    "authors": ["X. Chen", "R.A. Servedio", "Tan", "L.-Y", "E. Waingarten", "J. Xie"],
    "venue": "arXiv preprint arXiv:1704.06314,",
    "year": 2017
  }, {
    "title": "Parallel merge sort",
    "authors": ["R. Cole"],
    "venue": "SIAM Journal on Computing,",
    "year": 1988
  }, {
    "title": "Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds and some generalizations of the rado-edmonds theorem",
    "authors": ["M. Conforti", "G. Cornuéjols"],
    "venue": "Discrete applied mathematics,",
    "year": 1984
  }, {
    "title": "Lower bounds on communication complexity",
    "authors": ["P. Duris", "Z. Galil", "G. Schnitger"],
    "venue": "In STOC, pp",
    "year": 1984
  }, {
    "title": "Bicriteria distributed submodular maximization in a few rounds",
    "authors": ["A. Epasto", "V.S. Mirrokni", "M. Zadimoghaddam"],
    "venue": "In SPAA,",
    "year": 2017
  }, {
    "title": "Greed is good: Near-optimal submodular maximization via greedy optimization",
    "authors": ["M. Feldman", "C. Harshaw", "A. Karbasi"],
    "venue": "arXiv preprint arXiv:1704.01652,",
    "year": 2017
  }, {
    "title": "On the power of adaptivity in sparse recovery",
    "authors": ["P. Indyk", "E. Price", "D.P. Woodruff"],
    "venue": "In FOCS,",
    "year": 2011
  }, {
    "title": "Submodular optimization with submodular cover and submodular knapsack constraints",
    "authors": ["R.K. Iyer", "J.A. Bilmes"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Curvature and optimal algorithms for learning and minimizing submodular functions",
    "authors": ["R.K. Iyer", "S. Jegelka", "J.A. Bilmes"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "On fast approximate submodular minimization",
    "authors": ["S. Jegelka", "H. Lin", "J.A. Bilmes"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Reflection methods for user-friendly submodular optimization",
    "authors": ["S. Jegelka", "F. Bach", "S. Sra"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Fast greedy algorithms in mapreduce and streaming",
    "authors": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"],
    "venue": "ACM Transactions on Parallel Computing,",
    "year": 2015
  }, {
    "title": "Sparse and greedy: Sparsifying submodular facility location problems",
    "authors": ["E.M. Lindgren", "S. Wu", "A.G. Dimakis"],
    "venue": "In NIPS Workshop on Optimization for Machine Learning,",
    "year": 2015
  }, {
    "title": "Randomized composable core-sets for distributed submodular maximization",
    "authors": ["V. Mirrokni", "M. Zadimoghaddam"],
    "venue": "In STOC, pp",
    "year": 2015
  }, {
    "title": "Distributed submodular maximization: Identifying representative elements in massive data",
    "authors": ["B. Mirzasoleiman", "A. Karbasi", "R. Sarkar", "A. Krause"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Distributed submodular cover: Succinctly summarizing massive data",
    "authors": ["B. Mirzasoleiman", "A. Karbasi", "A. Badanidiyuru", "A. Krause"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Fast constrained submodular maximization: Personalized data summarization",
    "authors": ["B. Mirzasoleiman", "A. Badanidiyuru", "A. Karbasi"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Streaming robust submodular maximization: A partitioned thresholding approach",
    "authors": ["S. Mitrovic", "I. Bogunovic", "A. Norouzi-Fard", "J.M. Tarnawski", "V. Cevher"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Best algorithms for approximating the maximum of a submodular set function",
    "authors": ["G.L. Nemhauser", "L.A. Wolsey"],
    "venue": "Mathematics of operations research,",
    "year": 1978
  }, {
    "title": "An analysis of approximations for maximizing submodular set functionsi",
    "authors": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "Rounds in communication complexity revisited",
    "authors": ["N. Nisan", "A. Widgerson"],
    "venue": "In STOC, pp",
    "year": 1991
  }, {
    "title": "On the convergence rate of decomposable submodular function minimization",
    "authors": ["R. Nishihara", "S. Jegelka", "M.I. Jordan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Parallel double greedy submodular maximization",
    "authors": ["X. Pan", "S. Jegelka", "J.E. Gonzalez", "J.K. Bradley", "M.I. Jordan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Primal-dual rnc approximation algorithms for set cover and covering integer programs",
    "authors": ["S. Rajagopalan", "V.V. Vazirani"],
    "venue": "SIAM Journal on Computing,",
    "year": 1998
  }, {
    "title": "fancyimpute, matrix completion and feature imputation algorithms. 2017",
    "authors": ["A. Rubinsteyn", "S. Feldman"],
    "venue": "URL https://github.com/hammerlab/ fancyimpute",
    "year": 2017
  }, {
    "title": "Optimal approximation for submodular and supermodular optimization with bounded curvature",
    "authors": ["M. Sviridenko", "J. Vondrák", "J. Ward"],
    "venue": "In SODA,",
    "year": 2015
  }, {
    "title": "Missing value estimation methods for dna microarrays",
    "authors": ["O. Troyanskaya", "M. Cantor", "G. Sherlock", "P. Brown", "T. Hastie", "R. Tibshirani", "D. Botstein", "R.B. Altman"],
    "year": 2001
  }, {
    "title": "Parallelism in comparison problems",
    "authors": ["L.G. Valiant"],
    "venue": "SIAM Journal on Computing,",
    "year": 1975
  }, {
    "title": "Submodularity and curvature: the optimal algorithm",
    "authors": ["J. Vondrák"],
    "venue": "RIMS,",
    "year": 2010
  }, {
    "title": "Fast multi-stage submodular maximization",
    "authors": ["K. Wei", "R. Iyer", "J. Bilmes"],
    "venue": "In ICML, pp",
    "year": 2014
  }],
  "id": "SP:ff9f20283d5cf5057b40d541cc66c569c766ab0c",
  "authors": [{
    "name": "Eric Balkanski",
    "affiliations": []
  }, {
    "name": "Yaron Singer",
    "affiliations": []
  }],
  "abstractText": "In this paper we analyze an adaptive sampling approach for submodular maximization. Adaptive sampling is a technique that has recently been shown to achieve a constant factor approximation guarantee for submodular maximization under a cardinality constraint with exponentially fewer adaptive rounds than any previously studied constant factor approximation algorithm for this problem. Adaptivity quantifies the number of sequential rounds that an algorithm makes when function evaluations can be executed in parallel and is the parallel running time of an algorithm, up to low order terms. Adaptive sampling achieves its exponential speedup at the expense of approximation. In theory, it is guaranteed to produce a solution that is a 1/3 approximation to the optimum. Nevertheless, experiments show that adaptive sampling techniques achieve far better values in practice. In this paper we provide theoretical justification for this phenomenon. In particular, we show that under very mild conditions of curvature of a function, adaptive sampling techniques achieve an approximation arbitrarily close to 1/2 while maintaining their low adaptivity. Furthermore, we show that the approximation ratio approaches 1 in direct relationship to a homogeneity property of the submodular function. In addition, we conduct experiments on real data sets in which the curvature and homogeneity properties can be easily manipulated and demonstrate the relationship between approximation and curvature, as well as the effectiveness of adaptive sampling in practice.",
  "title": "Approximation Guarantees for Adaptive Sampling"
}