{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1329–1338 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1329\nIn this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method."
  }, {
    "heading": "1 Introduction",
    "text": "With the availability of large datasets and the recent progress made by neural methods, variants of sequence to sequence learning (seq2seq) (Sutskever et al., 2014) architectures have been successfully applied for building conversational systems (Serban et al., 2016, 2017b). However, despite these methods being the stateof-the art frameworks for conversation generation, they suffer from problems such as lack of diversity in responses and generation of short, repetitive and uninteresting responses (Liu et al., 2016; Serban et al., 2016, 2017b). A large body of recent\nliterature has focused on overcoming such challenges (Li et al., 2016a; Lowe et al., 2017).\nIn part, such problems arise as all information required to generate responses needs to be captured as part of the model parameters learnt from the training data. These model parameters alone may not be sufficient for generating natural conversations. Therefore, despite providing enormous amount of data, neural generative systems have been found to be ineffective for use in real world applications (Liu et al., 2016).\nIn this paper, we focus our attention on closed domain conversations. A characteristic feature of such conversations is that over a period of time, some conversation contexts1 are likely to have occurred previously (Lu et al., 2017b). For instance, Table 1 shows some contexts from the Ubuntu dialog corpus. Each row presents an input dialog context with its corresponding gold response followed by a similar context and response seen in training data – as can be seen, contexts for “installing dms”, “sharing files”, “blocking ufw ports” have all occurred in training data. We hypothesize that being able to refer to training responses for previously seen similar contexts could be a helpful signal to use while generating responses.\nIn order to exploit this aspect of closed domain conversations we build our neural encoderdecoder architecture called the Exemplar Encoder Decoder (EED), that learns to generate a response for a given context by exploiting similar contexts from training conversations. Thus, instead of having the seq2seq model learn patterns of language only from aligned parallel corpora, we assist the model by providing it closely related (similar) samples from the training data that it can refer to while generating text.\nSpecifically, given a context c, we retrieve a set 1We use the phrase “dialog context”, “conversation con-\ntext” and “context” interchangeably throughout the paper.\nof context-response pairs (c(k), r(k)), 1 ≤ k ≤ K using an inverted index of training data. We create an exemplar vector e(k) by encoding the response r(k) (also referred to as exemplar response) along with an encoded representation of the current context c. We then learn the importance of each exemplar vector e(k) based on the likelihood of it being able to generate the ground truth response. We believe that e(k) may contain information that is helpful in generating the response. Table 1 highlights the words in exemplar responses that appear in the ground truth response as well.\nContributions: We present a novel Exemplar Encoder-Decoder (EED) architecture that makes use of similar conversations, fetched from an index of training data. The retrieved contextresponse pairs are used to create exemplar vectors which are used by the decoder in the EED model, to learn the importance of training context-response pairs, while generating responses. We present detailed experiments on the publicly benchmarked Ubuntu dialog corpus data set (Lowe et al., 2015) as well a large collection of more than 127,000 technical support conversations. We compare the performance of the EED model with the existing state of the art generative models such as HRED (Serban et al., 2016) and VHRED (Serban et al., 2017b). We find that our model out-performs these models on a wide variety of metrics such as the recently proposed Activity Entity metrics (Serban et al., 2017a) as well as Embedding-based metrics (Lowe et al., 2015). In addition, we present qualitative insights into our results and we find that exemplar based responses\nare more informative and diverse. The rest of the paper is organized as follows. Section 2 briefly describes the recent works in neural dialogue generation The details of the proposed EED model for dialogue generation are described in detail in Section 3. In Section 4, we describe the datasets as well as the details of the models used during training. We present quantitative and qualitative results of EED model in Section 5."
  }, {
    "heading": "2 Related Work",
    "text": "In this section, we compare our work against other data-driven end-to-end conversation models. Endto-end conversation models can be further classified into two broad categories — generation based models and retrieval based models.\nGeneration based models cast the problem of dialogue generation as a sequence to sequence learning problem. Initial works treat the entire context as a single long sentence and learn an encoder-decoder framework to generate response word by word (Shang et al., 2015; Vinyals and Le, 2015). This was followed by work that models context better by breaking it into conversation history and last utterance (Sordoni et al., 2015b). Context was further modeled effectively by using a hierarchical encoder decoder (HRED) model which first learns a vector representation of each utterance and then combines these representations to learn vector representation of context (Serban et al., 2016). Later, an alternative hierarchical model called VHRED (Serban et al., 2017b) was proposed, where generated responses were conditioned on latent variables. This leads to more in-\nformative responses and adds diversity to response generation. Models that explicitly incorporate diversity in response generation have also been studied in literature (Li et al., 2016b; Vijayakumar et al., 2016; Cao and Clark, 2017; Zhao et al., 2017).\nOur work differs from the above as none of these above approaches utilize similar conversation contexts observed in the training data explicitly.\nRetrieval based models on the other hand treat the conversation context as a query and obtain a set of responses using information retrieval (IR) techniques from the conversation logs (Ji et al., 2014). There has been further work where the responses are further ranked using a deep learning based model (Yan et al., 2016a,b; Qiu et al., 2017). On the other hand of the spectrum, endto-end deep learning based rankers have also been employed to generate responses (Wu et al., 2017; Henderson et al., 2017). Recently a framework has also been proposed that uses a discriminative dialog network that ranks the candidate responses received from a response generator network and trains both the networks in an end to end manner (Lu et al., 2017a).\nIn contrast to the above models, we use the input contexts as well as the retrieved responses for generating the final responses. Contemporaneous to our work, a generative model for machine translation that employs retrieved translation pairs has also been proposed (Gu et al., 2017). We note that while the underlying premise of both the papers remains the same, the difference lies in the mechanism of incorporating the retrieved data."
  }, {
    "heading": "3 Exemplar Encoder Decoder",
    "text": ""
  }, {
    "heading": "3.1 Overview",
    "text": "A conversation consists of a sequence of utterances. At a given point in the conversation, the utterances expressed prior to it are jointly referred to as the context. The utterance that immediately follows the context is referred to as the response. As discussed in Section 1, given a conversational context, we wish to to generate a response by utilizing similar context-response pairs from the training data. We retrieve a set of K exemplar contextresponse pairs from an inverted index created using the training data in an off-line manner. The input and the retrieved context-response pairs are then fed to the Exemplar Encoder Decoder (EED)\nnetwork. A schematic illustration of the EED network is presented in Figure 1. The EED encoder combines the input context and the retrieved responses to create a set of exemplar vectors. The EED decoder then uses the exemplar vectors based on the similarity between the input context and retrieved contexts to generate a response. We now provide details of each of these modules."
  }, {
    "heading": "3.2 Retrieval of Similar Context-Response Pairs",
    "text": "Given a large collection of conversations as (context, response) pairs, we index each response and its corresponding context in tf − idf vector space. We further extract the last turn of a conversation and index it as an additional attribute of the context-response document pairs so as to allow directed queries based on it.\nGiven an input context c, we construct a query that weighs the last utterance in the context twice as much as the rest of the context and use it to retrieve the top-k similar context-response pairs from the index based on a BM25 (Robertson et al., 2009) retrieval model. These retrieved pairs form our exemplar context-response pairs (c(k), r(k)), 1 ≤ k ≤ K."
  }, {
    "heading": "3.3 Exemplar Encoder Network",
    "text": "Given the exemplar pairs (c(k), r(k)), 1 ≤ k ≤ K and an input context-response pair (c, r), we feed the input context c and the exemplar contexts c(1), . . . , c(K) through an encoder to generate the embeddings as given below:\nce = Encodec(c)\nc(k)e = Encodec(c (k)), 1 ≤ k ≤ K\nNote that we do not constrain our choice of encoder and that any parametrized differentiable architecture can be used as the encoder to generate the above embeddings. Similarly, we feed the exemplar responses r(1), . . . , r(K) through a response encoder to generate response embeddings r (1) e , . . . , r (K) e , that is,\nr(k)e = Encoder(r (k)), 1 ≤ k ≤ K (1)\nNext, we concatenate the exemplar response encoding r(k)e with an encoded representation of current context ce as shown in equation 2 to create the exemplar vector e(k). This allows us to include in-\nformation about similar responses along with the encoded input context representation.\ne(k) = [ce; r (k) e ], 1 ≤ k ≤ K (2)\nThe exemplar vectors e(k), 1 ≤ k ≤ K are further used by the decoder for generating the ground truth response as described in the next section."
  }, {
    "heading": "3.4 Exemplar Decoder Network",
    "text": "Recall that we want the exemplar responses to help generate the responses based on how similar the corresponding contexts are with the input context. More similar an exemplar context is to the input context, higher should be its effect in generating the response. To this end, we compute the similarity scores s(k), 1 ≤ k ≤ K using the encodings computed in Section 3.3 as shown below.\ns(k) = exp(cTe c (k) e )∑K\nl=1 exp(c T e c (l) e )\n(3)\nNext, each exemplar vector e(k) computed in Section 3.3, is fed to a decoder, where the decoder is responsible for predicting the ground truth response from the exemplar vector. Let pdec(r|e(k)) be the distribution of generating the ground truth response given the exemplar embedding. The objective function to be maximized, is expressed as a\nfunction of the scores s(k), the decoding distribution pdec and the exemplar vectors e(k) as shown below:\nll = K∑ k=1 s(k) log pdec(r|e(k)) (4)\nNote that we weigh the contribution of each exemplar vector to the final objective based on how similar the corresponding context is to the input context. Moreover, the similarities are differentiable function of the input and hence, trainable by back propagation. The model should learn to assign higher similarities to the exemplar contexts, whose responses are helpful for generating the correct response.\nThe model description uses encoder and decoder networks that can be implemented using any differentiable parametrized architecture. We discuss our choices for the encoders and decoder in the next section."
  }, {
    "heading": "3.5 The Encoders and Decoder",
    "text": "In this section, we discuss the various encoders and the decoder used by our model. The conversation context consists of an ordered sequence of utterances and each utterance can be further viewed as a sequence of words. Thus, context can be viewed as having multiple levels of\nhierarchies—at the word level and then at the utterance (sentence) level. We use a hierarchical recurrent encoder—popularly employed as part of the HRED framework for generating responses and query suggestions (Sordoni et al., 2015a; Serban et al., 2016, 2017b). The word-level encoder encodes the vector representations of words of an utterance to an utterance vector. Finally, the utterance-level encoder encodes the utterance vectors to a context vector.\nLet (u1, . . . ,uN ) be the utterances present in the context. Furthermore, let (wn1, . . . , wnMn) be the words present in the nth utterance for 1 ≤ n ≤ N . For each word in the utterance, we retrieve its corresponding embedding from an embedding matrix. The word embedding for wnm will be denoted as wenm. The encoding of the nth utterance can be computed iteratively as follows:\nhnm = f1(hnm−1, wenm), 1 ≤ m ≤Mn (5)\nWe use an LSTM (Hochreiter and Schmidhuber, 1997) to model the above equation. The last hidden state hnMn is referred to as the utterance encoding and will be denoted as hn.\nThe utterance-level encoder takes the utterance encodings h1, . . . , hN as input and generates the encoding for the context as follows:\ncen = f2(cen−1, hn), 1 ≤ n ≤ N (6)\nAgain, we use an LSTM to model the above equation. The last hidden state ceN is referred to as the context embedding and is denoted as ce.\nA single level LSTM is used for embedding the response. In particular, let (w1, . . . , wM ) be the sequence of words present in the response. For each word w, we retrieve the corresponding word embedding we from a word embedding matrix. The response embedding is computed from the word embeddings iteratively as follows:\nrem = g(rem−1, wem), 1 ≤ m ≤M (7)\nAgain, we use an LSTM to model the above equation. The last hidden state rem is referred to as the response embedding and is denoted as re."
  }, {
    "heading": "4 Experimental Setup",
    "text": ""
  }, {
    "heading": "4.1 Datasets",
    "text": ""
  }, {
    "heading": "4.1.1 Ubuntu Dataset",
    "text": "We conduct experiments on Ubuntu Dialogue Corpus (Lowe et al., 2015)(v2.0)2. Ubuntu dialogue corpus has about 1M context response pairs along with a label. The label value 1 indicates that the response associated with a context is the correct response and is incorrect otherwise. As we are only interested in positive labeled data we work with label = 1. Table 2 depicts some statistics for the dataset."
  }, {
    "heading": "4.1.2 Tech Support Dataset",
    "text": "We also conduct our experiments on a large technical support dataset with more than 127K conversations. We will refer to this dataset as Tech Support dataset in the rest of the paper. Tech Support dataset contains conversations pertaining to an employee seeking assistance from an agent (technical support) — to resolve problems such as password reset, software installation/licensing, and wireless access. In contrast to Ubuntu dataset, this dataset has clearly two distinct users — employee and agent. In our experiments we model the agent responses only.\nFor each conversation in the tech support data, we sample context and response pairs to create a dataset similar to the Ubuntu dataset format. Note that multiple context-response pairs can be generated from a single conversation. For each conversation, we sample 25% of the possible contextresponse pairs. We create validation pairs by selecting 5000 conversations randomly and sampling context response pairs). Similarly, we create test pairs from a different subset of 5000 conversations. The remaining conversations are used to\n2https://github.com/rkadlec/ ubuntu-ranking-dataset-creator\ncreate training context-response pairs. Table 3 depicts some statistics for this dataset:"
  }, {
    "heading": "4.2 Model and Training Details",
    "text": "The EED and HRED models were implemented using the PyTorch framework (Paszke et al., 2017). We initialize the word embedding matrix as well as the weights of context and response encoders from the standard normal distribution with mean 0 and variance 0.01. The biases of the encoders and decoder are initialized with 0. The word embedding matrix is shared by the context and response encoders. For Ubuntu dataset, we use a word embedding size of 600, whereas the size of the hidden layers of the LSTMs in context and response encoders and the decoder is fixed at 1200. For Tech support dataset, we use a word embedding size of 128. Furthermore, the size of the hidden layers of the multiple LSTMs in context and response encoders and the decoder is fixed at 256. A smaller embedding size was chosen for the Tech Support dataset since we observed much less diversity in the responses of the Tech Support dataset as compared to Ubuntu dataset.\nTwo different encoders are used for encoding the input context (not shown in Figure 1 for simplicity). The output of the first context encoder is concatenated with the exemplar response vectors to generate exemplar vectors as detailed in Section 3.3. The output of the second context encoder is used to compute the scoring function as detailed in Section 3.4. For each input context, we retrieve 5 similar context-response pairs for Ubuntu dataset and 3 context-response pairs for Tech support dataset using the tf-idf mechanism discussed in Section 3.2.\nWe use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e − 4 for training the model. A batch size of 20 samples was used\nduring training. In order to prevent overfitting, we use early stopping with log-likelihood on validation set as the stopping criteria. In order to generate the samples using the proposed EED model, we identify the exemplar context that is most similar to the input context based on the learnt scoring function discussed in Section 3.4. The corresponding exemplar vector is fed to the decoder to generate the response. The samples are generated using a beam search with width 5. The average per-word log-likelihood is used to score the beams."
  }, {
    "heading": "5 Results & Evaluation",
    "text": ""
  }, {
    "heading": "5.1 Quantitative Evaluation",
    "text": ""
  }, {
    "heading": "5.1.1 Activity and Entity Metrics",
    "text": "A traditional and popular metric used for comparing a generated sentence with a ground truth sentence is BLEU (Papineni et al., 2002) and is frequently used to evaluate machine translation. The metric has also been applied to compute scores for predicted responses in conversations, but it has been found to be less indicative of actual performance (Liu et al., 2016; Sordoni et al., 2015a; Serban et al., 2017a), as it is extremely sensitive to the exact words in the ground truth response, and gives equal importance to stop words/phrases and informative words.\nSerban et al. (2017a) recently proposed a new set of metrics for evaluating dialogue responses for the Ubuntu corpus. It is important to highlight that these metrics have been specifically designed for the Ubuntu corpus and evaluate a generated response with the ground truth response by comparing the coarse level representation of an utterance (such as entities, activities, Ubuntu OS commands). Here is a brief description of each metric:\n• Activity: Activity metric compares the activities present in a predicted response with the ground truth response. Activity can be thought of as a verb. Thus, all the verbs in a response are mapped to a set of manually identified list of 192 verbs.\n• Entity: This compares the technical entities that overlap with the ground truth response. A total of 3115 technical entities is identified using public resources such as Debian package manager APT.\n• Tense: This measure compares the time tense of ground truth with predicted response.\n• Cmd: This metric computes accuracy by comparing commands identified in ground truth utterance with a predicted response.\nTable 4 compares our model with other recent generative models (Serban et al., 2017a) — LSTM (Shang et al., 2015), HRED (Serban et al., 2016) & VHRED (Serban et al., 2017b).We do not compare our model with Multi-Resolution RNN (MRNN) (Serban et al., 2017a), as MRNN explicitly utilizes the activities and entities during the generation process. In contrast, the proposed EED model and the other models used for comparison are agnostic to the activity and entity information. We use the standard script3 to compute the metrics.\nThe EED model scores better than generative models on almost all of the metrics, indicating that we generate more informative responses than other state-of-the-art generative based approaches for Ubuntu corpus. The results show that responses associated with similar contexts may contain the activities and entities present in the ground truth response, and thus help in response generation. This is discussed further in Section 5.2. Additionally, we compared our proposed EED with a retrieval only baseline. The retrieval baseline achieves an activity F1 score of 4.23 and entity F1 score of 2.72 compared to 4.87 and 2.99 respectively achieved by our method on the Ubuntu corpus.\nThe Tech Support dataset is not evaluated using the above metrics, since activity and entity information is not available for this dataset.\n3https://github.com/julianser/Ubuntu-MultiresolutionTools/blob/master/ActEntRepresentation/eval file.sh"
  }, {
    "heading": "5.1.2 Embedding Metrics",
    "text": "Embedding metrics (Lowe et al., 2017) were proposed as an alternative to word by word comparison metrics such as BLEU. We use pre-trained Google news word embeddings4 similar to Serban et al. (2017b), for easy reproducibility as these metrics are sensitive to the word embeddings used. The three metrics of interest utilize the word vectors in ground truth response and a predicted response and are discussed below:\n• Average: Average word embedding vectors are computed for the candidate response and ground truth. The cosine similarity is computed between these averaged embeddings. High similarity gives as indication that ground truth and predicted response have similar words.\n• Greedy: Greedy matching score finds the most similar word in predicted response to ground truth response using cosine similarity.\n• Extrema: Vector extrema score computes the maximum or minimum value of each dimension of word vectors in candidate response and ground truth.\nOf these, the embedding average metric is the most reflective of performance for our setup. The extrema representation, for instance, is very sensitive to text length and becomes ineffective beyond single length sentences(Forgues et al., 2014). We use the publicly available script5 for all our computations. As the test outputs for HRED are not available for Technical Support dataset, we use our\n4GoogleNews-vectors-negative300.bin from https:// code.google.com/archive/p/word2vec/\n5https://github.com/julianser/ hed-dlg-truncated/blob/master/ Evaluation/embedding_metrics.py\nown implementation of HRED. Table 5 compares our model with HRED, and depicts that our model scores better on all metrics for Technical Support\ndataset, and on majority of the metrics for Ubuntu dataset.\nWe note that the improvement achieved by the\nEED model on activity and entity metrics are much more significant than those on embedding metrics. This suggests that the EED model is better able to capture the specific information (objects and actions) present in the conversations.\nFinally, we evaluate the diversity of the generated responses for EED against HRED by counting the number of unique tokens, token-pairs and token-triplets present in the generated responses on Ubuntu and Tech Support dataset. The results are shown in Table 6. As can be observed, the responses in EED have a larger number of distinct tokens, token-pairs and token-triplets than HRED, and hence, are arguably more diverse."
  }, {
    "heading": "5.2 Qualitative Evaluation",
    "text": "Table 7 presents the responses generated by HRED, VHRED and the proposed EED for a few selected contexts along with the corresponding similar exemplar responses. As can be observed from the table, the responses generated by EED tend to be more specific to the input context as compared to the responses of HRED and VHRED. For example, in conversations 1 and 2 we find that both HRED and VHRED generate simple generic responses whereas EED generates responses with additional information such as the type of disk partition used or a command not working. This is also confirmed by the quantitative results obtained using activity and entity metrics in the previous section. We further observe that the exemplar responses contain informative words that are utilized by the EED model for generating the responses as highlighted in Table 7."
  }, {
    "heading": "6 Conclusions",
    "text": "In this work, we propose a deep learning method, Exemplar Encoder Decoder (EED), that given a conversation context uses similar contexts and corresponding responses from training data for generating a response. We show that by utilizing this information the system is able to outperform state of the art generative models on publicly available Ubuntu dataset. We further show improvements achieved by the proposed method on a large collection of technical support conversations.\nWhile in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation. In our future work we plan to extend\nthe proposed method to these other applications."
  }, {
    "heading": "Acknowledgements",
    "text": "We are grateful to the anonymous reviewers for their comments that helped in improving the paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Latent variable dialogue models and their diversity",
    "authors": ["Kris Cao", "Stephen Clark."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, volume 2, pages 182–187.",
    "year": 2017
  }, {
    "title": "Bootstrapping dialog systems with word embeddings",
    "authors": ["Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchevêque", "Réal Tremblay."],
    "venue": "Nips, modern machine learning and natural language processing workshop, volume 2.",
    "year": 2014
  }, {
    "title": "Search engine guided nonparametric neural machine translation",
    "authors": ["Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor OK Li."],
    "venue": "arXiv preprint arXiv:1705.07267.",
    "year": 2017
  }, {
    "title": "Efficient natural language response suggestion for smart reply. CoRR, abs/1705.00652",
    "authors": ["Matthew Henderson", "Rami Al-Rfou", "Brian Strope", "Yun-Hsuan Sung", "László Lukács", "Ruiqi Guo", "Sanjiv Kumar", "Balint Miklos", "Ray Kurzweil"],
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jurgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "An information retrieval approach to short text conversation",
    "authors": ["Zongcheng Ji", "Zhengdong Lu", "Hang Li."],
    "venue": "CoRR, abs/1408.6988.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for",
    "year": 2016
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
    "authors": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."],
    "venue": "arXiv preprint",
    "year": 2016
  }, {
    "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."],
    "venue": "arXiv preprint arXiv:1506.08909.",
    "year": 2015
  }, {
    "title": "Training end-to-end dialogue systems with the ubuntu dialogue corpus",
    "authors": ["Ryan Thomas Lowe", "Nissan Pow", "Iulian Vlad Serban", "Laurent Charlin", "Chia-Wei Liu", "Joelle Pineau."],
    "venue": "Dialogue & Discourse, 8(1):31–65.",
    "year": 2017
  }, {
    "title": "Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model",
    "authors": ["Jiasen Lu", "Anitha Kannan", "Jianwei Yang", "Devi Parikh", "Dhruv Batra."],
    "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,",
    "year": 2017
  }, {
    "title": "A practical approach to dialogue response generation in closed domains",
    "authors": ["Yichao Lu", "Phillip Keung", "Shaonan Zhang", "Jason Sun", "Vikas Bhardwaj."],
    "venue": "arXiv preprint arXiv:1703.09439.",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Automatic differentiation in pytorch",
    "authors": ["Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer"],
    "year": 2017
  }, {
    "title": "Alime chat: A sequence to sequence and rerank based chatbot engine",
    "authors": ["Minghui Qiu", "Feng-Lin Li", "Siyu Wang", "Xing Gao", "Yan Chen", "Weipeng Zhao", "Haiqing Chen", "Jun Huang", "Wei Chu."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "The probabilistic relevance framework: Bm25 and beyond",
    "authors": ["Stephen Robertson", "Hugo Zaragoza"],
    "venue": "Foundations and Trends R",
    "year": 2009
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."],
    "venue": "AAAI.",
    "year": 2016
  }, {
    "title": "Multiresolution recurrent neural networks: An application to dialogue response generation",
    "authors": ["Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron C Courville."],
    "venue": "AAAI, pages 3288–3294.",
    "year": 2017
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "AAAI, pages 3295–3301.",
    "year": 2017
  }, {
    "title": "Neural responding machine for short-text conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
    "authors": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."],
    "venue": "Proceedings of the 24th ACM International",
    "year": 2015
  }, {
    "title": "A neural network approach to contextsensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "William B. Dolan."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
    "authors": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra."],
    "venue": "arXiv preprint arXiv:1610.02424.",
    "year": 2016
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc V. Le."],
    "venue": "CoRR, abs/1506.05869.",
    "year": 2015
  }, {
    "title": "A sequential matching framework for multi-turn response selection in retrievalbased chatbots",
    "authors": ["Yu Wu", "Wei Wu", "Chen Xing", "Can Xu", "Zhoujun Li", "Ming Zhou."],
    "venue": "CoRR, abs/1710.11344.",
    "year": 2017
  }, {
    "title": "Learning to respond with deep neural networks for retrievalbased human-computer conversation system",
    "authors": ["Rui Yan", "Yiping Song", "Hua Wu."],
    "venue": "SIGIR.",
    "year": 2016
  }, {
    "title": "shall i be your chat companion?”: Towards an online human-computer conversation system",
    "authors": ["Rui Yan", "Yiping Song", "Xiangyang Zhou", "Hua Wu."],
    "venue": "CIKM.",
    "year": 2016
  }, {
    "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
    "authors": ["Tiancheng Zhao", "Ran Zhao", "Maxine Eskenazi."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-",
    "year": 2017
  }],
  "id": "SP:e06202d9e26536edc8c827ee6877cc0570f01187",
  "authors": [{
    "name": "Gaurav Pandey",
    "affiliations": []
  }, {
    "name": "Danish Contractor",
    "affiliations": []
  }, {
    "name": "Vineet Kumar",
    "affiliations": []
  }, {
    "name": "Sachindra Joshi",
    "affiliations": []
  }],
  "abstractText": "In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.",
  "title": "Exemplar Encoder-Decoder for Neural Conversation Generation"
}