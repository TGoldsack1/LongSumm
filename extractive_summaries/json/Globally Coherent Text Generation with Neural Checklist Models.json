{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 329–339, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics\nRecurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said – especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda."
  }, {
    "heading": "1 Introduction",
    "text": "Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks (Mikolov et al., 2010; Mikolov et al., 2011; Sordoni et al., 2015; Xu et al., 2015; Wen et al., 2015; Mei et al., 2016). Previous neural generation models typically generate locally coherent language that is on topic; however, overall they can miss information that should have been introduced or introduce duplicated or superfluous content. These errors are particularly common in situations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neural model that maintains coherence while improv-\ning coverage by globally tracking what has been said and what is still left to be said in complete texts.\nFor example, consider the challenge of generating a cooking recipe, where the title and ingredient list are provided as inputs and the system must generate a complete text that describes how to produce the desired dish. Existing RNN models may lose track of which ingredients have already been mentioned, especially during the generation of a long recipe with many ingredients. Recent work has focused on adapting neural network architectures to improve coverage (Wen et al., 2015) with application to generating customer service responses, such as hotel information, where a single sentence is generated to describe a few key ideas. Our focus is instead on developing a model that maintains coherence while producing longer texts or covering longer\n329\ninput specifications (e.g., a long ingredient list). More specifically, our neural checklist model generates a natural language description for achieving a goal, such as generating a recipe for a particular dish, while using a new checklist mechanism to keep track of an agenda of items that should be mentioned, such as a list of ingredients (see Fig. 1). The checklist model learns to interpolate among three components at each time step: (1) an encoder-decoder language model that generates goal-oriented text, (2) an attention model that tracks remaining agenda items that need to be introduced, and (3) an attention model that tracks the used, or checked, agenda items. Together, these components allow the model to learn representations that best predict which words should be included in the text and when references to agenda items should be checked off the list (see check marks in Fig. 1).\nWe evaluate our approach on a new cooking recipe generation task and the dialogue act generation from Wen et al. (2015). In both cases, the model must correctly describe a list of agenda items: an ingredient list or a set of facts, respectively. Generating recipes additionally tests the ability to maintain coherence in long procedural texts. Experiments in dialogue generation demonstrate that our approach outperforms previous work with up to a 4 point BLEU improvement. Our model also scales to cooking recipes, where both automated and manual evaluations demonstrate that it maintains the strong local coherence of baseline RNN techniques while significantly improving the global coverage by effectively integrating the agenda items."
  }, {
    "heading": "2 Task",
    "text": "Given a goal g and an agenda E = {e1, . . . , e|E|}, our task is to generate a goal-oriented text x by making use of items on the agenda. For example, in the cooking recipe domain, the goal is the recipe title (“pico de gallo” in Fig. 1), and the agenda is the ingredient list (e.g., “lime,” “salt”). For dialogue systems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g =“inform” and E = {name(Hotel Stratford), has internet(no)}, an output text might be x =“Hotel Stratford does not have internet.”"
  }, {
    "heading": "3 Related Work",
    "text": "Attention models have been used for many NLP tasks such as machine translation (Balasubramanian et al., 2013; Bahdanau et al., 2014), abstractive sentence summarization (Rush et al., 2015), machine reading (Cheng et al., 2016), and image caption generation (Xu et al., 2015). Our model uses new types of attention to record what has been said and to select new agenda items to be referenced.\nRecently, other researchers have developed new ways to use attention mechanisms for related generation challenges. Most closely related, Wen et al. (2015) and Wen et al. (2016) present neural network models for generating dialogue system responses given a set of agenda items. They focus on generating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possible agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data.\nMaintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning (Jia et al., 2015; Xu et al., 2015) and machine translation (Tu et al., 2016b; Tu et al., 2016a). A variety of solutions have been developed to address infrequent or out-of-vocabulary words in particular (Gülçehre et al., 2016; Jia and Liang, 2016). Instead of directly copying input words or deterministically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word “steaks” when the original recipe ingredient was “ribeyes”). Finally, recent work in machine translation models has introduced new training objectives to encourage attention to all input words (Luong et al., 2015), but these models do not accumulate attention while decoding.\nGenerating recipes was an early task in planning (Hammond, 1986) and generating referring expression research (Dale, 1988). These can be seen as key steps in classic approaches to generating natural language text: a formal meaning representation is provided as input and the model first does content selection to determine the non-linguistic concepts to be conveyed by the output text (i.e., what to say) and then does realization to describe those concepts\nin natural language text (i.e., how to say it) (Thompson, 1977; Reiter and Dale, 2000). More recently, machine learning methods have focused on parts of this approach (Barzilay and Lapata, 2005; Liang et al., 2009) or the full two-stage approach (Angeli et al., 2010; Konstas and Lapata, 2013). Most of these models shorter texts, although Mori et al. (2014) did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given."
  }, {
    "heading": "4 Model",
    "text": "Fig. 2 shows a graphical representation of the neural checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a soft checklist of what agenda items have been used so far during generation. This checklist is updated every time an agenda item reference is generated and is used to compute the available agenda items at each time step. The available items are used as an input to the language model and to constrain which agenda items can still be referenced during generation. Agenda embeddings are also used when generating item references."
  }, {
    "heading": "4.1 Input variable definitions",
    "text": "We assume the goal g and agenda items E (see Sec. 2) are each defined by a set of tokens. Goal\ntokens come from a fixed vocabulary Vgoal, the item tokens come from a fixed vocabulary Vagenda, and the tokens of the text xt come from a fixed vocabulary Vtext. In an abuse of notation, we represent each goal g, agenda item ei, and text token xt as a k-dimensional word embedding vector. We compute these embeddings by creating indicator vectors of the vocabulary token (or set of tokens for goals and agenda items) and embed those vectors using a trained k × |Vz| projection matrix, where z ∈ {goal, agenda, text} depending whether we are generating a goal, agenda item, or text token.\nGiven a goal embedding g ∈ Rk, a matrix of L agenda items E ∈ RL×k, a checklist soft record of what items have been used at−1 ∈ RL, a previous hidden state ht−1 ∈ Rk, and the current input word embedding xt ∈ Rk, our architecture computes the next hidden state ht, an embedding used to generate the output word ot, and the updated checklist at."
  }, {
    "heading": "4.2 Generating output token probabilities",
    "text": "To generate the output token probability distribution (see “Generate output” box in Fig. 2), wt ∈ R|Vtext|, we project the output hidden state ot into the vocabulary space and apply a softmax:\nwt = softmax(Woot),\nwhere Wo ∈ R|V |×k is a trained projection matrix. The output hidden state is the linear interpolation of (1) content cgrut from a Gated Recurrent Unit\n(GRU) language model, (2) an encoding cnewt generated from the new agenda item reference model (Sec. 4.3), and (3) and an encoding cusedt generated from a previously used item model (Sec. 4.4):\not = f gru t c gru t + f new t c new t + f used t c used t .\nThe interpolation weights, fgrut , f new t , and f used t , are probabilities representing how much the output token should reflect the current state of the language model or a chosen agenda item. fgrut is the probability of a non-agenda-item token, fnewt is the probability of an new item reference token, and fusedt is the probability of a used item reference. In the Fig. 1 example, fnewt is high in the first row when new ingredient references “tomatoes” and “onion” are generated; fusedt is high when the reference back to “tomatoes” is made in the second row, and fgrut is high the rest of the time.\nTo generate these weights, our model uses a threeway probabilistic classifier, ref -type(ht), to determine whether the hidden state of the GRU ht will generate non-agenda tokens, new agenda item references, or used item references. ref -type(ht) generates a probability distribution ft ∈ R3 as\nft = ref -type(ht) = softmax(βSht),\nwhere S ∈ R3×k is a trained projection matrix and β is a temperature hyper-parameter. fgrut = f 1 t , fnewt = f 2 t , and f used t = f 3 t . ref -type() does not use the agenda, only the hidden state ht: ht must encode when to use the agenda, and ref -type() is trained to identify that in ht."
  }, {
    "heading": "4.3 New agenda item reference model",
    "text": "The two key features of our model are that it (1) predicts which agenda item is being referred to, if any, at each time step and (2) stores those predictions for use during generation. These components allow for improved output texts that are more likely to mention agenda items while avoiding repetition and references to irrelevant items not in the agenda.\nThese features are enabled by a checklist vector at ∈ RL that represents the probability each agenda item has been introduced into the text. The checklist vector is initialized to all zeros at t = 1, representing\nthat all items have yet to be introduced. The checklist vector is a soft record with each at,i ∈ [0, 1].1\nWe introduce the remaining items as a matrix Enewt ∈ RL×k, where each row is an agenda item embedding weighted by how likely it is to still need to be referenced. For example, in Fig. 1, after the first “tomatoes” is generated, the row representing “chopped tomatoes” in the agenda will be weighted close to 0. We calculate Enewt using the checklist vector (see “Update [...] items” box in Fig. 2):\nEnewt = ((1L − at−1)⊗ 1k) ◦ E,\nwhere 1L = {1}L, 1k = {1}k, and the outer product ⊗ replicates 1L − at−1 for each dimension of the embedding space. ◦ is the Hadamard product (i.e., element-wise multiplication) of two matrices with the same dimensions.\nThe model predicts when an agenda item will be generated using ref -type() (see Sec. 4.2 for details). When it does, the encoding cnewt approximates which agenda item is most likely. cnewt is computed using an attention model that generates a learned soft alignment αnewt ∈ RL between the hidden state ht and the rows of Enewt (i.e., available items). The alignment is a probability distribution representing how close ht is to each item:\nαnewt ∝ exp(γEnewt Pht),\nwhere P ∈ Rk×k is a learned projection matrix and γ is a temperature hyper-parameter. In Fig. 1, the shaded squares in the top line (i.e., the first “tomatoes” and the onion references) represent this alignment. The attention encoding cnewt is then the attention-weighted sum of the agenda items:\ncnewt = E Tαnewt .\nAt each step, the model updates the checklist vector based on the probability of generating a new agenda item reference, fnewt , and the attention alignment αnewt . We calculate the update to checklist, a new t , as anewt = f new t · αnewt . Then, the new checklist at is at = at−1 + anewt .\n1By definition, at is non-negative. We truncate any values greater than 1 using a hard tanh function."
  }, {
    "heading": "4.4 Previously used item reference model",
    "text": "We also allow references to be generated for previously used agenda items through the previously used item encoding cusedt . This is useful in longer texts – when agenda items can be referred to more than once – so that the agenda is always responsible for generating its own referring expressions. The example in Fig. 1 refers back to tomatoes when generating to what to add the diced onion.\nAt each time step t, we use a second attention model to compare ht to a used items matrix Eusedt ∈ RL×k. Like the remaining agenda item matrix Enewt , E used t is calculated using the checklist vector generated at the previous time step:\nEusedt = (at−1 ⊗ 1k) ◦ E.\nThe attention over the used items, αusedt ∈ RL, and the used attention encoding cusedt are calculated in the same way as those over the available items (see Sec. 4.3 for comparison):\nαusedt ∝ exp(γEusedt Pht), cusedt = E Tαusedt ."
  }, {
    "heading": "4.5 GRU language model",
    "text": "Our decoder RNN adapts a Gated Recurrent Unit (GRU) (Cho et al., 2014). Given an input xt ∈ Rk at time step t and the previous hidden state ht−1 ∈ Rk, a GRU computes the next hidden state ht as\nht = (1− zt)ht−1 + zth̃t.\nThe update gate, zt, interpolates between ht−1 and new content, h̃t, defined respectively as\nzt = σ(Wzxt + Uzht−1),\nh̃t = tanh(Wxt + rt Uht−1).\nis an element-wise multiplication, and the reset gate, rt, is calculated as\nrt = σ(Wrxt + Urht−1).\nWz , Uz , W , U , Wr, Ur ∈ Rk×k are trained projection matrices.\nWe adapted a GRU to allow extra inputs, namely the goal g and the available agenda items Enewt (see “GRU language model” box in Fig. 2). These extra\ninputs help guide the language model stay on topic. Our adapted GRU has a change to the computation of the new content h̃t as follows:\nh̃t = tanh(Whxt + rt Uhht−1 + st Y g + qt (1TLZEnewt )T ,\nwhere st is a goal select gate and qt is a item select gate, respectively defined as\nst = σ(Wsxt + Usht−1),\nqt = σ(Wqxt + Uqht−1).\n1L sums the rows of the available item matrixEnewt . Y , Z, Ws, Us, Wq, Uq ∈ Rk×k are trained projection matrices. The goal select gate controls when the goal should be taken into account during generation: for example, the recipe title may be used to decide what the imperative verb for a new step should be. The item select gate controls when the available agenda items should be taken into account (e.g., when generating a list of ingredients to combine). The GRU hidden state is initialized with a projection of the goal: h0 = Ugg, where Ug ∈ Rk×k.\nThe content vector cgrut that is used to compute the output hidden state ot is a linear projection of the GRU hidden state, cgrut = Pht, where P is the same learned projection matrix used in the computation of the attention weights (see Sections 4.3 and 4.4)."
  }, {
    "heading": "4.6 Training",
    "text": "Given a training set of (goal, agenda, output text) triples {(g(1), E(1),x(1)), . . . , (g(J), E(J),x(J))}, we train model parameters by minimizing negative log-likelihood: NLL(θ) =\n− J∑\nj=1\nNj∑\ni=2\nlog p(x (j) i |x (j) 1 , . . . ,x (j) i−1,g (j), E(j); θ),\nwhere x(j)1 is the start symbol. We use mini-batch stochastic gradient descent, and back-propagate through the goal, agenda, and text embeddings.\nIt is sometimes the case that weak heuristic supervision on latent variables can be easily gathered to improve training. For example, for recipe generation, we can approximate the linear interpolation weights ft and the attention updates anewt and a used t using string match heuristics comparing tokens in\nthe text to tokens in the ingredient list.2 When this extra signal is available, we add mean squared loss terms toNLL(θ) to encourage the latent variables to take those values; for example, if f∗t is the true value and ft is the predicted value, a loss term −(f∗t − ft)2 is added. When this signal is not available, as is the case with our dialogue generation task, we instead introduce a mean squared loss term that encourages the final checklist a(j)Nj to be a vector of 1s (i.e., every agenda item is accounted for)."
  }, {
    "heading": "4.7 Generation",
    "text": "We generate text using beam search, which has been shown to be fast and accurate for RNN decoding (Graves, 2012; Sutskever et al., 2014). When the beam search completes, we select the highest probability sequence that uses the most agenda items. This is the count of how many times the three-way classifier, ref -type(ht), chose to generate an new item reference with high probability (i.e., > 50%)."
  }, {
    "heading": "5 Experimental setup",
    "text": "Our model was implemented and trained using the Torch scientific computing framework for Lua.3\nExperiments We evaluated neural checklist models on two natural language generation tasks. The first task is cooking recipe generation. Given a recipe title (i.e., the name of the dish) as the goal and the list of ingredients as the agenda, the system must generate the correct recipe text. Our second evaluation is based on the task from Wen et al. (2015) for generating dialogue responses for hotel and restaurant information systems. The task is to generate a natural language response given a query type (e.g., informing or querying) and a list of facts to convey (e.g., a hotel’s name and address).\nParameters We constrain the gradient norm to 5.0 and initialize parameters uniformly on [−0.35, 0.35]. We used a beam of size 10 for generation. Based on dev set performance, a learning rate of 0.1 was chosen, and the temperature hyperparameters (β, γ) were (5, 2) for the recipe task and (1, 10) for the dialogue task. The models for the recipe task had a hidden state size of k = 256; the\n2Similar to anewt , ausedt = fusedt ·αusedt . 3http://torch.ch/\nmodels for the dialogue task had k = 80 to compare to previous models. We use a batch size 30 for the recipe task and 10 for the dialogue task.\nRecipe data and pre-processing We use the Now You’re Cooking! recipe library: the data set contains over 150,000 recipes in the Meal-MasterTM format.4 We heuristically removed sentences that were not recipe steps (e.g., author notes, nutritional information, publication information). 82,590 recipes were used for training, and 1,000 each for development and testing. We filtered out recipes to avoid exact duplicates between training and dev (test) sets.\nWe collapsed multi-word ingredient names into single tokens using word2phrase5 ran on the training data ingredient lists. Titles and ingredients were cleaned of non-word tokens. Ingredients additionally were stripped of amounts (e.g., “1 tsp”). As mentioned in Sec. 4.6, we approximate true values for the interpolation weights and attention updates for recipes based on string match between the recipe text and the ingredient list. The first ingredient reference in a sentence cannot be the first token or after a comma (e.g., the bold tokens cannot be ingredients in “oil the pan” and “in a large bowl, mix [...]”).\nRecipe data statistics Automatic recipe generation is difficult due to the length of recipes, the size of the vocabulary, and the variety of possible dishes. In our training data, the average recipe length is 102 tokens, and the longest recipe has 814 tokens. The vocabulary of the recipe text from the training data (i.e., the text of the recipe not including the title or ingredient list) has 14,103 unique tokens. About 31% of tokens in the recipe vocabulary occur at least 100 times in the training data; 8.6% of the tokens occur at least 1000 times. The training data also represents a wide variety of recipe types, defined by the recipe titles. Of 3793 title tokens, only 18.9% of the title tokens in the title vocabulary occur at least 100 times in the training data, which demonstrates the large variability in the titles.\nDialogue system data and processing We used the hotel and restaurant dialogue system corpus and the same train-development-test split from Wen et al. (2015). We used the same pre-processing, sets\n4Recipes and format at http://www.ffts.com/recipes.htm 5See https://code.google.com/p/word2vec/\nof reference samples, and baseline output, and we were given model output to compare against.6 For training, slot values (e.g., “Red Door Cafe”) were replaced by generic tokens (e.g., “NAME TOKEN”). After generation, generic tokens were swapped back to specific slot values. Minor post-processing included removing duplicate determiners from the relexicalization and merging plural “-s” tokens onto their respective words. After replacing specific slot values with generic tokens, the training data vocabulary size of the hotel corpus is 445 tokens, and that of the restaurant corpus is 365 tokens. The task has eight goals (e.g., inform, confirm).\nModels Our main baseline EncDec is a model using the RNN Encoder-Decoder framework proposed by Cho et al. (2014) and Sutskever et al. (2014). The model encodes the goal and then each agenda item in sequence and then decodes the text using GRUs. The encoder has two sets of parameters: one for the goal and the other for the agenda items. For the dialogue task, we also compare against the SC-LSTM system from Wen et al. (2015) and the handcrafted rule-based generator described in that paper.\nFor the recipe task, we also compare against three other baselines. The first is a basic attention model, Attention, that generates an attention encoding by comparing the hidden state ht to the agenda. That encoding is added to the hidden state, and a nonlinear transformation is applied to the result before projecting into the output space. We also present a nearest neighbor baseline (NN) that simply copies over an existing recipe text based on the input similarity computed using cosine similarity over the title and the ingredient list. Finally, we present a hybrid approach (NN-Swap) that revises a nearest neighbor recipe using the neural checklist model. The neural checklist model is forced to generate the returned recipe nearly verbatim, except that it can generate new strings to replace any extraneous ingredients.\nOur neural checklist model is labeled Checklist. We also present the Checklist+ model, which interactively re-writes a recipe to better cover the input agenda: if the generated text does not use every agenda item, embeddings corresponding to missing items are multiplied by increasing weights and a new recipe is generated. This process repeats until the\n6We thank the authors for sharing their system outputs.\nnew recipe does not contain new items. We also report the performance of our checklist model without the additional weak supervision of heuristic ingredient references (- no supervision) (see Sec. 4.6).7 we also evaluate two ablations of our checklist model on the recipe task. First, we remove the linear interpolation and instead use ht as the output (see Sec. 4.2). Second, we remove the previously used item reference model by changing ref -type() to a 2-way classifier between new ingredient references and all other tokens (see Sec. 4.4).\nMetrics We include commonly used metrics like BLEU-4,8 and METEOR (Denkowski and Lavie, 2014). Because neither of these metrics can measure how well the generated recipe follows the input goal and the agenda, we also define two additional metrics. The first measures the percentage of the agenda items corrected used, while the second measures the number of extraneous items incorrectly introduced. Both these metrics are computed based on simple string match and can miss certain referring expressions (e.g., “meat” to refer to “pork”). Because of the approximate nature of these automated metrics, we also report a human evaluation."
  }, {
    "heading": "6 Recipe generation results",
    "text": "Fig. 1 results for recipe generation. All BLEU and METEOR scores are low, which is expected for long texts. Our checklist model performs better than both neural network baselines (Attention and EncDec) in all metrics. Nearest neighbor baselines (NN and NN-Swap) perform the best in terms of BLEU and\n7For this model, parameters were initialized on [-0.2, 0.2] to maximize development accuracy.\n8See Moses system (http://www.statmt.org/moses/)\nMETEOR; this is due to a number of recipes that have very similar text but make different dishes.\nHowever, NN baselines are not successful in generating a goal-oriented text that follows the given agenda: compared to Checklist+ (83.4%), they use substantially less % of the given ingredients (40% - 58.2%) while also introducing extra ingredients not provided. EncDec and Attention baselines similarly generate recipes that are not relevant to the given input, using only 22.8% - 26.9% of the agenda items. Checklist models rarely introduce extraneous ingredients not provided (0.6 - 0.8), while other baselines make a few mistakes on average (2.0 - 4.2).\nThe ablation study demonstrates the empirical contribution of different model components. (ot = ht) shows the usefulness of the attention encodings when generating the agenda references, while (-no used) shows the need for separate attention mechanisms between new and used ingredient references for more accurate use of the agenda items. Similarly, (-no supervision) demonstrates that the weak supervision encourages the model to learn more accurate management of the agenda items.\nHuman evaluation Because neither BLEU nor METEOR is suitable for evaluating generated text in terms of their adherence to the provided goal and the agenda, we also report human evaluation using Amazon Mechanical Turk. We evaluate the generated recipes on (1) grammaticality, (2) how well the\nrecipe adheres to the provided ingredient list, and (3) how well the generated recipe accomplishes the desired dish. We selected 100 random test recipes. For each question we used a Likert scale (∈ [1, 5]) and report averaged ratings among five turkers.\nTable 2 shows the averaged scores over the responses. The checklist models outperform all baselines in generating recipes that follow the provided agenda closely and accomplish the desired goal, where NN in particular often generates the wrong dish. Perhaps surprisingly, both the Attention and EncDec baselines and the Checklist model beat the true recipes in terms of having better grammar. This can partly be attributed to noise in the parsing of the true recipes, and partly because the neural models tend to generate shorter, simpler texts.\nFig. 3 shows the counts of the most used vocabulary tokens in the true dev set recipes compared to the recipes generated by EncDec and Checklist+. Using the vocabulary from the training data, the true dev recipes use 5206 different tokens. The EncDec’s vocabulary is only ∼16% of that size, while the Checklist+ model is a third of the size.\nAn error analysis on the dev set shows that the EncDec baseline over-generates catch-all phrases like “all ingredients” or “the ingredients,” used in 21% of the generated recipes, whereas only 7.8% of true recipes use that construction. This phrase type simplifies the recipe, but using all ingredients in one step reduces the chance of accomplishing the desired dish correctly. The Checklist model only generates those phrases 13% of the time.\nQualitative analysis Fig. 4 shows two dev set recipes with generations from the EncDec and Checklist+ models. The EncDec model is much more likely to both use incorrect ingredients and to introduce ingredients more than once (e.g., “baking power” and “salt” in the bottom example are not in the ingredient list, and “milk” in the top example is duplicated). In the top example, the Checklist+ model refers to both Parmesean and Swiss cheese as “cheese”; generating more precise referring expressions is an important area for future work. The Checklist+ recipes generate the correct dishes to an extent: for example, the top recipe makes a casserole but does not cook the ingredients together before baking and mixes in biscuits instead of putting\nthem on top. Future work could better model the full set of steps needed to achieve the overall goal."
  }, {
    "heading": "7 Dialogue system results",
    "text": "Figure 3 shows our results on the hotel and restaurant dialogue system generation tasks. HDC is the rule-based baseline from Wen et al. (2015). For both domains, the checklist model achieved the highest BLEU-4 and METEOR scores, but both neural systems performed very well. The power of our model is in generating long texts, but this experiment shows that our model can generalize well to other tasks with different kinds of agenda items and goals."
  }, {
    "heading": "8 Future work and conclusions",
    "text": "We present the neural checklist model that generates globally coherent text by keeping track of what\nhas been said and still needs to be said from a provided agenda. Future work includes incorporating referring expressions for sets or compositions of agenda items (e.g., “vegetables”). The neural checklist model is sensitive to hyperparameter initialization, which should be investigated in future work. The neural checklist model can also be adapted to handle multiple checklists, such as checklists over composite entities created over the course of a recipe (see Kiddon (2016) for an initial proposal)."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by the Intel Science and Technology Center for Pervasive Computing (ISTC-PC), NSF (IIS-1252835 and IIS1524371), DARPA under the CwC program through the ARO (W911NF-15-1-0543), and gifts by Google and Facebook. We thank our anonymous reviewers for their comments and suggestions, as well as Yannis Konstas, Mike Lewis, Mark Yatskar, Antoine Bosselut, Luheng He, Eunsol Choi, Victoria Lin, Kenton Lee, and Nicholas FitzGerald for helping us read and edit. We also thank Mirella Lapata and Annie Louis for their suggestions for baselines."
  }],
  "year": 2016,
  "references": [{
    "title": "A simple domain-independent probabilistic approach to generation",
    "authors": ["Gabor Angeli", "Percy Liang", "Dan Klein."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512.",
    "year": 2010
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR 2015.",
    "year": 2014
  }, {
    "title": "Generating coherent event schemas at scale",
    "authors": ["Niranjan Balasubramanian", "Stephen Soderland", "Mausam", "Oren Etzioni."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing, pages 1721–1731.",
    "year": 2013
  }, {
    "title": "Collective content selection for concept-to-text generation",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing, pages 331– 338.",
    "year": 2005
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of the 2014 Conference on",
    "year": 2014
  }, {
    "title": "Generating Referring Expressions in a Domain of Objects and Processes",
    "authors": ["Robert Dale."],
    "venue": "Ph.D. thesis, Centre for Cognitive Science, University of Edinburgh.",
    "year": 1988
  }, {
    "title": "Meteor universal: Language specific translation evaluation for any target language",
    "authors": ["Michael Denkowski", "Alon Lavie."],
    "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation, pages 376–380.",
    "year": 2014
  }, {
    "title": "Sequence transduction with recurrent neural networks",
    "authors": ["Alex Graves."],
    "venue": "Representation Learning Worksop, ICML.",
    "year": 2012
  }, {
    "title": "Pointing the unknown words",
    "authors": ["Çaglar Gülçehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 140–149.",
    "year": 2016
  }, {
    "title": "CHEF: A model of casebased planning",
    "authors": ["Kristian J. Hammond."],
    "venue": "Proceedings of the Fifth National Conference on Artificial Intelligence (AAAI-86), pages 267–271.",
    "year": 1986
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["R. Jia", "P. Liang."],
    "venue": "Proceedings of the 54th Annual",
    "year": 2016
  }, {
    "title": "Guiding long-short term memory for image caption generation",
    "authors": ["Xu Jia", "Efstratios Gavves", "Basura Fernando", "Tinne Tuytelaars."],
    "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2407–2415.",
    "year": 2015
  }, {
    "title": "Learning to Interpret and Generate Instructional Recipes",
    "authors": ["Chloé Kiddon."],
    "venue": "Ph.D. thesis, Computer Science & Engineering, University of Washington.",
    "year": 2016
  }, {
    "title": "A global model for concept-to-text generation",
    "authors": ["Ioannis Konstas", "Mirella Lapata."],
    "venue": "Journal of Artificial Intelligence Research (JAIR), 48:305–346.",
    "year": 2013
  }, {
    "title": "Learning semantic correspondences with less supervision",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Process-",
    "year": 2009
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, September.",
    "year": 2015
  }, {
    "title": "What to talk about and how? Selective generation using lstms with coarse-to-fine alignment",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."],
    "venue": "The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics:",
    "year": 2016
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur."],
    "venue": "Proceedings of INTERSPEECH 2010, the 11th Annual Conference of the International Speech Communication Association,",
    "year": 2010
  }, {
    "title": "Extensions of recurrent neural network language model",
    "authors": ["Tomas Mikolov", "Stefan Kombrink", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur."],
    "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP",
    "year": 2011
  }, {
    "title": "FlowGraph2Text: Automatic sentence skeleton compilation for procedural text generation",
    "authors": ["Shinsuke Mori", "Hirokuni Maeta", "Tetsuro Sasada", "Koichiro Yoshino", "Atsushi Hashimoto", "Takuya Funatomi", "Yoko Yamakata."],
    "venue": "Proceedings of the 8th In-",
    "year": 2014
  }, {
    "title": "Building Natural Language Generation Systems",
    "authors": ["Ehud Reiter", "Robert Dale."],
    "venue": "Cambridge University Press, New York, NY, USA.",
    "year": 2000
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 379–389.",
    "year": 2015
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Conference of the North American",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages",
    "year": 2014
  }, {
    "title": "Strategy and tactics: a model for language production",
    "authors": ["Henry S. Thompson."],
    "venue": "Papers from the Thirteenth Regional Meeting of the Chicago Linguistics Society, pages 89–95. Chicago Linguistics Society.",
    "year": 1977
  }, {
    "title": "Context gates for neural machine translation",
    "authors": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."],
    "venue": "CoRR, abs/1608.06043.",
    "year": 2016
  }, {
    "title": "Modeling coverage for neural machine translation",
    "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 76–85.",
    "year": 2016
  }, {
    "title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-hao Su", "David Vandyke", "Steve J. Young."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in",
    "year": 2015
  }, {
    "title": "Multi-domain neural network language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina Maria Rojas-Barahona", "Pei-hao Su", "David Vandyke", "Steve J. Young."],
    "venue": "Proceedings of the 15th Annual",
    "year": 2016
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."],
    "venue": "Proceedings of the 32nd International Con-",
    "year": 2015
  }],
  "id": "SP:3a0a3fbae91d98597d3d7bf5c33ff3eb818dc0a9",
  "authors": [{
    "name": "Chloé Kiddon",
    "affiliations": []
  }, {
    "name": "Luke Zettlemoyer",
    "affiliations": []
  }, {
    "name": "Yejin Choi",
    "affiliations": []
  }],
  "abstractText": "Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said – especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.",
  "title": "Globally Coherent Text Generation with Neural Checklist Models"
}