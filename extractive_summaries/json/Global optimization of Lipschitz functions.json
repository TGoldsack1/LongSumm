{
  "sections": [{
    "text": "Keywords Lipschitz constant · Approximate function · Response surface · Global optimization · Black box function\nH. Liu · S. Xu (B) · Y. Ma · X. Wang School of Energy and Power Engineering, Dalian University of Technology, Dalian 116024, China e-mail: xusl@dlut.edu.cn\nH. Liu e-mail: lht@mail.dlut.edu.cn\nY. Ma e-mail: maying@mail.dlut.edu.cn\nX. Wang e-mail: dlwxf@dlut.edu.cn"
  }, {
    "heading": "1 Introduction",
    "text": "Let us consider a Lipschitz global optimization problem formulated as\nf ∗ = f (x∗) = min x∈D f (x), D ⊂ R n, (1) ∣ ∣ f (xi ) − f (x j ) ∣ ∣ ≤ K ∥∥xi − x j ∥ ∥ , xi , x j ∈ D, 0 < K < ∞, (2)\nwhere f is supposed to be a computationally expensive black box function. The function f is also supposed to be non-differentiable, which means some global optimization algorithms using gradient information can not be used to solve the problem (1)–(2). D is ann-dimensional design space. ‖.‖ in (2) denotes the Euclidean norm. K is an unknown Lipschitz constant. This type of global optimization problem is generally encountered in engineering applications [1,2].\nVarious algorithms [1–8] have been presented for solving problem (1)–(2). These Lipschitzian-based algorithms usually construct and optimize an approximate function that underestimates the original function f in a piecewise fashion. The approximate function can be referred as f̂L(X,Y, K ), where X is the sample set and Y is the corresponding response vector. It is found that the Lipschitz constant K ensures the global convergence in theory for Lipschitzian-based algorithms. Some researchers [9–11] proposed several methods for estimating the Lipschitz constant. Such global estimate K̂ of the Lipschitz constant, which is the upper bound of the rate of change of function f , is generally large and provides poor information about some interesting regions of f [5,12], which therefore brings slow convergence for optimization.\nTo speed up the convergence of the standard Lipschitzian algorithm [3,13], Jones et al. [5] proposed the DIRECT algorithm to carry out simultaneous searches by using all potential estimates of the Lipschitz constant. Modifications of the DIRECT algorithm were developed continuously in recent years to speed up the convergence [7,14–21], handle constraints [22, 23] and deal with high dimensional problems [18,24]. The DIRECT algorithm has several shortcomings in practice [7,21,25,26]. First, DIRECT is usually quick to catch a near-global solution but slow to refine the solution to high accuracy. Second, DIRECT usually requires numerous function evaluations to handle three kinds of functions: (a) function with many local optima, (b) function whose global optimum lies on flat region and (c) function whose global optimum lies on the boundary. Third, the sampling process of DIRECT is regular and exclusive so that it can not handle irregular points, e.g., some initial points obtained before optimization.\nOtherway to speed up the convergence of the standardLipschitzian algorithm is to estimate the lower bounds of the function f in a more compact way, i.e., making the approximate function f̂L closer to f . To achieve this aim, an alternative is to use the so-called local tuning strategy [2,12,27,28] to estimate local Lipschitz constants for different sub-regions of the domain D. Thiswayonly uses the information of sampled points to estimate the lower bounds. Moreover, another way is to use not only the sampled points but also the first derivative f ′ to construct a smooth support function so that the algorithm can obtain more compact lower bounds, i.e., more accurate f̂L [8,29,30]. This strategy does accelerate the optimization. But it usually has no knowledge about the first derivative of the studied problem in practice.\nSince the recently developed response surfaces (also known as metamodels) such as the Kriging model [31] and the RBF model [32] are able to approximate the real function f in a smooth, compact way, we state that the metamodeling techniques can help to estimate more compact lower bounds. For instance, Fig. 1 shows a 1D function f and two approximate functions, the linear piecewise f̂L and the smooth f̂M , by the standard Lipschitzian algorithm\nand the Kriging technique, respectively. It is observed that the fine f̂M provides a better approximation than the coarse f̂L . Note that the constructed metamodels not only contain the information of sampled points but also have the predicted information of unsampled regions. The predicted information is proved to accelerate the optimization in somemetamodel-based optimization algorithms [14,33–37].\nTherefore, this article proposes a new approximate function F̂(X,Y, K̂ ) to approximate f in a more compact way with the usage of the metamodel f̂M . Besides, the approximate level of F̂ can be adjusted by the estimated Lipschitz constant K̂ . When F̂ is constructed in a fine level with a small K̂ , it favors exploitation in the interesting regions. When F̂ is constructed in a coarse level with a large K̂ , it favors exploration over the entire domain. We refer to the new algorithm asGlobal Optimization using Potential Lipschitz Constants and Response Surfaces (PLRS). By cycling through a set of identified potential Lipschitz constants, PLRS constructs the approximate function F̂ from fine to coarse and, as a result, operates at both local and global levels. The similar balance operator between local and global information is also found in [2,7,38,39].\nThe remainder of the article is organized as follows. Section 2 presents the new global optimization algorithm. Section 3 conducts numerical investigations to confirm the performance of the proposed algorithm. Section 4 explores the sensitivity of the proposed algorithm to some optimization parameters. Section 5 applies the proposed algorithm to an engineering example. Finally, Sect. 6 provides some concluding remarks."
  }, {
    "heading": "2 The proposed PLRS algorithm",
    "text": "2.1 New approximate function for lower bounds\nSuppose that the entire domain D has been partitioned into hyper-intervals {Di } by the previously evaluated points X = {x1, x2, . . . , xm}. For a 1D case, with an estimate K̂ , Ri in Fig. 2 is the lower bound of f over the interval Di = [ xi , xi+1 ]\nin the standard Lipschitzian algorithm. The lower bound Ri is actually the minimum value of the approximate function f̂L , which is defined as\nf̂L(x) = max i=1,2,...,m{hi }, (3)\nhi = f (xi ) − K̂ ‖xi − x‖ , (4) where hi is the support function. The standard Lipschitzian algorithm repeats the estimation of lower bounds for all intervals and then selects an interval with the smallest lower bound\nto sample a new point. Mathematically, finding the next new point xm+1 can be defined as a min-max problem [3,13,40]:\nf̂L(xm+1) = min f̂L = min max i=1,2,...,m{hi }. (5)\nSince the metamodel f̂M provides more accurate approximation of f , it can help to estimate the lower bounds in a more compact way. Here, we replace f (xi ) with f̂M (x) in (4) to obtain a smooth support function\nHi = f̂M (x) − K̂ ‖xi − x‖ , (6) where f̂M (x) is the predicted function value at point x . Compared with hi in (4), the new support function Hi uses not only the information of point xi but also the information of unsampled regions predicted by f̂M . With the new support function Hi , we can construct a new approximate function F̂(X,Y, K̂ ) as\nF̂(x) = max i=1,2,...,m{Hi } = maxi=1,2,...,m\n{ f̂M (x) − K̂ ‖xi − x‖ } . (7)\nFigure 2 shows that F̂ approximates f better than the linear approximate function f̂L , and the newly estimated lower bound R′i is more compact than Ri .\nFurthermore, (7) can be simplified as\nF̂(x) = f̂M (x) − K̂ d(x), (8) d(x) = min\ni=1,2,...,m ‖xi − x‖ , (9)\nwhere d(x) is the minimum distance from x to the existing points. The first term f̂M (x) in (8) represents the predicted function value at point x . This term leads to select a point with lower predicted function value in order to lower the value of F̂(x). The second term −K̂ d(x) represents the minimum distance between x and the existing points. This term leads to select a point far away from the existing points (i.e, to fill the domain) in order to lower the value of F̂(x). In terms of exploration and exploitation, the first term favors local exploitation on f̂M and the second term favors global exploration over the entire domain. The estimate K̂ balances the global exploration and local exploitation. Larger K̂ places higher emphasis on global exploration. Note that Sergeyev [27] presented local tuning technique to estimate local Lipschitz constants in order to adapt to the behavior of the function, and\nfurthermore, employed some local improvement techniques [39] to help the local tuning technique improve optimization efficiency. Here, Eq. (8) uses the metamodel to predict the information of unsampled regions in order to identify the local characteristics of the objective function. The utilization of the information of unsampled regions provided by the metamodel is expected to accelerate the optimization.\nSimilar to (5), the next new point xm+1 is chosen so that\nF̂(xm+1) = min x∈D F̂(x) = minx∈D\n( f̂M (x) − K̂ d(x) ) . (10)\n2.2 Potential Lipschitz constants\nEquation (8) reveals that for an overestimate K̂ , it puts emphasis on global search; for an underestimate K̂ , it puts emphasis on local search. Given two extreme cases: when K̂ = 0, problem (10) degenerates to F̂(xm+1) = min\nx∈D f̂M (x). In this case, PLRS considers nothing\nbut to search on themetamodel f̂M to obtain the optimal point as the next point. This operation with K̂ = 0 is called the pure greedy search; when K̂ = +∞, problem (10) degenerates to F̂(xm+1) = min\nx∈D(−K̂ d(x)) = −K̂ maxx∈D mini=1,2,...,m ‖xi − x‖, which equals to the Maximin criterion [41] adopted in the space-filling sampling approach. In this case, the selection of the new point ignores the aim of optimization but tends to make the sampled points fill the entire domain evenly. This operation with K̂ = +∞ is called the space-filling search.\nLet us take a one-dimensional function\nf (x) = (1.2x + 0.8)2 sin(8 − 12x), x ∈ [0, 1] (11) for example to illustrate the impact of the estimate K̂ on the approximate function F̂(x). The Lipschitz constant of this function can be taken as the upper bound of the derivative, i.e., K = sup {‖∇ f (x)‖ : x ∈ [0, 1]} = 45. Figure 3 plots F̂(x) of function (11) with the estimate K̂ respectively being 0, 10, 45, 100. For the extreme case K̂ = 0, the second term of (8) reduces to zero and thus the approximate function is the same as current metamodel. In terms of exploitation and exploration, this case pays all the attention to exploitation. For function (11), since the current metamodel approximates the real function well in the right region, this case captures the global optimum at once (see Fig. 3a). For an underestimate K̂ = 10, this case considers both exploitation and exploration but places more emphasis on exploitation. For function (11), this case finds a near-global solution as the next point (see Fig. 3b). For the exact estimate K̂ = 45, compared with the case K̂ = 10, this case paysmore attention to exploration, and thus selects a new point in the interval with the largest size (i.e., the largest amount of unexplored territory) in Fig. 3c. When raised up to K̂ = 100, the effect of exploitation nearly vanishes. This case pays almost all the attention to the space-filling operation over the entire domain.\nThe key in the optimization procedure is to specify a proper K̂ in order to give a trade-off between exploration and exploitation. The proposed algorithm uses all potential estimates of the Lipschitz constant to search the global solution. Figure 4 introduces, similarly to [5], a two-dimensional diagram (d, f ) to identify the potential estimates of the Lipschitz constant. A dot xE in this two-dimensional diagram represents a hyper-interval DE . The vertices of the main diagonal of DE are xE and its closest neighbor point xi . The horizontal coordinate dE and the vertical coordinate fE of the dot xE are defined as\ndE = min i =E ‖xi − xE‖ , fE = f (xE ), xi , xE ∈ X. (12)\nThe lower bound of f over the hyper-interval DE is calculated as RE = fE − K̂ dE , see Fig. 4. Actually, this value is the vertical coordinate of the intersection point of the line passed through the dot xE with the slope K̂ and the vertical coordinate axis. Note that since some dots are close to each other, according to (12) two or more dots may have the same horizontal coordinate, which divides all the dots into several groups.\nBefore identifying the potential estimates of theLipschitz constant, the proposed algorithm first selects the potential dots. Mathematically, let ε be a positive value and fmin be the lowest function value encountered now, a dot x j is potentially optimal if there exists some rate-ofchange constant K̂ such that\nf (x j ) − K̂ d j ≤ f (xi ) − K̂ di , for any i, (13) f (x j ) − K̂ d j ≤ fmin − ε | fmin| . (14)\nAfter identifying the potential dots XP = { x1p, x 2 p, . . . , x t+1 p } , the elements in which are in an ascending sort order by their hyper-interval sizes (i.e., horizontal coordinates), the slopes of the lines passed through any two neighbor potential dots are marked as the potential estimates of the Lipschitz constant. The set of the potential Lipschitz constants is designated as K̂P = { K̂1, K̂2, . . . , K̂t } , where t is the number of elements and K̂i = ( f (xi+1p ) − f (xip) )/( di+1p − dip ) . In Fig. 4, the algorithm obtains two potential\nestimates K̂1 and K̂2 according to the identified three potential dots xA, xB and xC . Note that the proposed algorithmconsiders two extreme estimates K̂0 = 0 and K̂∞ = +∞ in the potential estimate set K̂P , i.e., K̂P = { K̂0, K̂1, K̂2, . . . , K̂t , K̂∞ } . Fig. 4 illustrates\nthe two extreme cases as two red slopes. In the optimization procedure, PLRS cycles through the potential estimate set K̂P by a potential index . If K̂ = K̂P ( ) = K̂0 in one iteration, PLRS does pure greedy search. This extreme case ensures doing the most nature thing of minimizing the metamodel f̂M . If K̂ = K̂P ( ) = K̂∞ in one iteration, PLRS does spacefilling search. This extreme case ensures that the proposed algorithmwill sample everywhere, thereby converging to the global optimum. As for other potential estimates, PLRS considers both the exploitation and exploration by doing potential search.\nAfter obtaining a potential Lipschitz constant K̂ = K̂P ( ) in one iteration, PLRS finds the next point xm+1 according to (10). By selecting points in this iterative manner, the PLRS algorithm can capture the global optimum.\nSo far, the basic framework of the PLRS algorithm has been presented. By using the potential Lipschitz constants from 0 to +∞, PLRS searches the domain from local to global. Note that K̂ in (10) serves as a trade-off between global exploration and local exploitation, while ε in (14), which describes the amount that the lower bound for a hyper-interval exceeds the current best solution [5], implicitly gives a floor value for K̂ . Small value of ε allows small floor value for K̂ , i.e., the algorithm can do a much locally-biased search with some very small values of K̂ to exploit the interesting regions. Large value of ε makes the algorithm ignore some left-hand dots, i.e., some small potential slopes, and the algorithm therefore conducts a globally-biased search over the design space with some relatively large values of K̂ . According to the discussion, it should adjust the value of ε in the optimization procedure in order to have high convergence speed.\nIntuitively, when PLRS finds an interesting region, it is suggested to exploit this region with a small ε so that the algorithm has a high probability of improving the current solution. When this interesting region has been scooped out completely, it is expected to explore other\nregions with a large ε in order to find the next interesting region. With this consideration PLRS starts with ε = 0, which is called the locally-biased (lb) mode, in order to exploit the interesting region found by the initial points. During the lb-mode, if PLRS could not improve the current solution in imaxlb continuous iterations, it will switch to the exploration over other regions with ε = 0.1, which is called the globally-biased (gb) mode. During the gb-mode, PLRSwill switch back to the lb-mode directly if it finds a better solution or it could not improve the current solution in imaxgb continuous iterations. With the two-mode strategy, which is similar to the two-phase approach presented in [7,21], PLRS switches many times between the lb-mode and the gb-mode in order to capture the global optimum with points as few as possible.\nIn the gb-mode, since the aim is to put emphasis on global exploration, the proposed\nalgorithm deletes the extreme case K̂0 so that K̂ P = { K̂1, K̂2, . . . , K̂t , K̂∞ } . Additionally, with ε = 0.1 the number of elements in K̂P is usually smaller than that in the lb-mode, but the values of the elements are usually larger.\nNote that for functions that have a local optimum around zero, as also observed in [7,21], the algorithm may have fmin = 0 in some iterations. In this case inequality (14) loses the ability to protect against excessive local exploitation in the gb-mode. To address this case, inequality (14) is modified by a threshold value:\nf (x j ) − K̂ d j ≤ { fmin − ε | fmin| , if | fmin| ≥ 0.0001. fmin − ε, if | fmin| < 0.0001. (15)\nThe threshold value is used to identify whether fmin is close to zero, thus it should not be too large (<0.001 is reasonable). In the gb-mode, PLRS aims to explore the entire design space, so the threshold value should not be too small. In our numerical investigation, the threshold value 0.0001 is found to work well.\n2.3 Step-by-step description of PLRS\nLet us give a formal scheme of the proposed PLRS approach as follow. Some notations used in the PLRS algorithm are listed below:\nf the deterministic black-box objective function whose value at point x is obtained by a computer simulator.\nf kmin the best function value encountered after k iterations. iMODE the number of continuous iterations that can not improve the current solution in either the lb-mode or the gb-mode. imaxlb , i max gb the maximal number of continuous iterations that can not improve the current solution in the lb-mode and the gb-mode, respectively. MODE the current mode of the PLRS algorithm. m the number of sampled points. nfe the number of function evaluations. Nini the number of initial points. NPL the number of potential Lipschitz constants in K̂P . the potential index to select the potential Lipschitz constant from K̂P .\nAlgorithm 1 Step-by-step description of the PLRS algorithm 1: Set inim N= , generate a set of initial points { }1 2, , , mx x x=X over the domain D. 2: Evaluate the function values { }1 2( ), ( ), , ( )mf x f x f x=Y at points { }1 2, , , mx x x=X . 3: Set k = 1, 1Θ = , MODEi = 0, nfe = m, { }1min 1 2min ( ), ( ), , ( )k mf f x f x f x− = , MODE =\nlb-mode and stopping criterion = FALSE. 4: while stopping criterion == FALSE do 5: Fit a metamodel M̂f using the current points X and their function values Y. 6: Determine the potential Lipschitz constants:\nif MODE == lb-mode then Obtain the potential Lipschitz constants { }0 1 2ˆ ˆ ˆ ˆ ˆ ˆ, , , , ,tP K K K K K∞=K using (13) and (15) with 0ε = . Set 2PLN t= + . elseif MODE == gb-mode then Obtain the potential Lipschitz constants { }1 2ˆ ˆ ˆ ˆ ˆ, , , ,tP K K K K∞=K using (13) and (15) with 0.1ε = . Set 1PLN t= + .\nend if 7: Select the next point 1mx + :\n( )ˆ ˆ PK = ΘK . if Θ == PLN then\nSet Θ = 0. end if Select 1mx + to be the solution of problem (10).\n8: Evaluate the function value ( )1mf x + at point 1mx + , set nfe = nfe + 1. 9: Update ( ){ }1min 1 minmin , kk mf f x f −+= . 10: Check the stopping criterion. 11: if stopping criterion == TRUE then\nReturn 1mx + , min kf and nfe.\nend if 12: if min kf < 1min kf − then\nSet MODEi = 0. MODE = lb-mode. else\nSet MODE MODE 1i i= + . end if\n13: if MODE == lb-mode and MODEi > maxlbi then Set MODE = gb-mode, MODEi = 0.\nelseif MODE == gb-mode and MODEi > maxgbi then Set MODE = lb-mode, MODEi = 0.\nend if 14: Update information: Set { }1mx +=X X , { }1( )mf x +=Y Y , k = k + 1, m = m + 1, 1Θ = Θ + . 15: end while\nIn Step 1, the proposed algorithm uses a set of initial points to explore the domain D before optimization. It is suggested that the initial points should spread over the entire domain so that all regions have an equal chance to be probed. Some frequently used space-filling\nsampling approaches [2,39,41–43] can be opted here. This article employs the translational propagation latin hypercube design (TPLHD) [42] approach to generate the initial points since this approach generates points with both space-filling and projective properties efficiently. Besides, in this article the number of initial points is set to 5n, where n is the number of input variables. In Step 4, the stopping criterion opted in this article is a relative error defined as\nE =\n⎧ ⎪⎨\n⎪⎩\n∣ ∣ ∣ fmin− fglobal\nfglobal\n∣ ∣ ∣ , if fglobal = 0,\n∣ ∣ fmin − fglobal ∣ ∣ , if fglobal = 0,\n(16)\nwhere fmin is the current best function value and fglobal is the known global optimum. Besides, considering the limited computing resource, the algorithm sets the allowedmaximal number of function evaluations Nmax. In Step 5, this article uses the Kriging model with a Gaussian function for metamodeling. In Step 6, the proposed algorithm updates the potential estimate set K̂P in each iteration such that the values and the number of elements in K̂P keep changing. In Step 7, when PLRS uses K̂ = K̂∞ to do space-filling search, the algorithm usually uses a sufficiently large value, e.g., 106, to represent K̂∞ in practice. To solve the auxiliary optimization problem in (10), the Genetic Algorithm (GA) is employed. Note that when PLRS uses K̂ = K̂0 to do greedy search in the lb-mode, the next point obtained on the metamodel f̂M may be a previously evaluated point. To address this situation, the algorithm simply gives K̂ a pretty small value such as 10−6 and then resolves problem (10). In Step 13, the algorithm sets imaxlb = 10 and imaxgb = 2imaxlb since the lb-mode refines the solution more efficiently.\nIt is found that PLRS can address the drawbacks of the DIRECT algorithm: (1) the utilization of metamodels can offer PLRS a more effective guide to refine the solution to high accuracy; (2) the PLRS algorithm can handle function with many local optima effectively because it can switch between the lb-mode and the gb-mode. The PLRS algorithm can handle function whose global optimum lies on flat region effectively because it uses the metamodel to describe the behavior of the function well. The PLRS algorithm can handle function whose global optimum lies on the boundary effectively because it conducts optimization on themetamodel over the entire domain when K̂ = K̂0; (3) the PLRS algorithm has a high information utilization because it can handle points with any distribution in any stage of the optimization process. The aforementioned advantages of the PLRS algorithm will be confirmed by the numerical investigations in Sect. 3.\n2.4 Convergence\nLet D ⊆ Rn be compact and f be a continuous function. Suppose that the proposed algorithm keeps proceeding (i.e., the iteration number k goes to infinity) and generates a sequence of sampled points {xi }1≤i≤k . We define a distance parameter dk = max\nx∈D min1≤i≤k ‖x − xi‖ which can be used to represent the largest amount of unexplored territory. The following proposition gives the so-called everywhere dense convergence [6,21,39] of the proposedPLRSalgorithm.\nProposition 1 For any point x ∈ D and any δ > 0, there exists an iteration number k(δ) ≥ 1 and a point xi ∈ {xi }1≤i≤k(δ) such that\n‖xi − x‖ ≤ dk(δ) < δ. (17) Proof Suppose that the proposed algorithm encounters the extreme case K̂∞ to do the space-filling search in the k-th iteration. With K̂∞ the approximate function F̂ degener-\nates to F̂(x) = −K̂∞ min 1≤i≤k−1 ‖xi − x‖ and (10) degenerates to F̂(xk) = minx∈D F̂(x) = −K̂∞ max x∈D min1≤i≤k−1 ‖xi − x‖ = −K̂∞dk−1. At this point, the PLRS algorithm ignores the aim of optimization but tries to reduce the amount of unexplored territory by sampling a new point xk in a region with the largest amount of unexplored territory like [41]. After adding the new point, the aforementioned region is partitioned, i.e., the amount of unexplored territory is reduced. Since PLRS does space-filling search every few iterations, dk keeps decreasing with the optimization proceeds. Thus, there exists an iteration number k(δ) such that dk(δ) < δ. For any point x ∈ D, there exists a point xi ∈ {xi }1≤i≤k(δ) to satisfy condition (17), which proves the convergence of the proposed algorithm.\n3 Numerical experiments\n3.1 Test functions\nThis section employs two benchmark function sets to confirm the performance of the PLRS algorithm. Table 1 shows the characteristics of the first test function set, including the abbreviation used, the problem dimension, the bounds of design space and the known global optimum. The mathematical formulas of these benchmark functions can be found in http://www. sfu.ca/~ssurjano/optimization.html. The function mH6 in the last row of Table 1 modifies the design space of the original H6 so that its global optimum lies on the boundary.\nHowever, as pointed by [7], the 13 test functions in Table 1 are characterized by a small chance to miss the region of the global optimum. To confirm the performance of the proposed algorithm comprehensively, this article additionally employs more sophisticated test functions, i.e., four classes of multi-dimensional test functions generated by the GKLS-generator from [44]. This generator allows the user to customize the function with known local and global optima, and thus is very suitable for numerical investigation of global optimization algorithms.\nThe generation procedure of the GKLS-generator consists of defining a convex quadratic function (paraboloid) systematically distorted by polynomials [44]. Each test class constructed by this generator has 100 functions and is defined completely by five parameters: (1) problem dimension n, (2) number of local optima u, (3) global minimum value f *, (4) radius r of the attraction region of the global optimum and (5) distance d from the global optimum x* to the paraboloid vertex P . Other necessary parameters are chosen randomly. Figure 5 shows a continuously differentiable test function from a GKLS class with n = 2, u = 10, f ∗ = −1, r = 0.10 and d = 0.90 over the domain D = [−1, 1]2. The number of this function in theGKLSclass is 60. The global optimum is x∗ = (−0.2864,−0.0198) and the paraboloid vertex is P = (−0.8250,−0.7408).\nTable 2 lists the characteristics of the four GKLS classes of continuously differentiable test functions. All the functions are defined over the domain D = [−1, 1]n . The study considers a “simple” test class and a “hard” test class for a particular dimension n. The difficulty of the GKLS classes is increased either by increasing the distance d from the global optimum to the paraboloid vertex, or by decreasing the radius r of the attraction region of the global optimum. Compared with the test functions in Table 1, the GKLS functions are more complex: they have 10 local optima and much smaller attraction regions (r = 0.10 or 0.20) of the global\na The algorithm only succeeded to achieve the required relative error within the limited computing resource in four out of ten runs for S5, in three out of ten runs for S7, in four out of ten runs for S10 and in five out of ten runs for H6. The data is the average of successful runs\noptimum. To handle the GKLS functions efficiently, the algorithm has to fully explore the design space in order to capture the small attraction region of the global optimum.\nThe comparison study selects 10 test functions f10, f20, . . . , f100 from the 100 functions in each class. It is reasonable that the total 40 GKLS functions with different properties are enough to demonstrate whether the PLRS algorithm is able to handle the GKLS functions efficiently.\nThe proposed algorithm is compared against twometamodel-based algorithms [the hybrid adaptive metamodeling algorithm (HAM) [37] and the constrained optimization using response surfaces (CORS) [35]] and the DIRECT algorithm [5] on the total 53 test functions. All algorithms start with 5n initial points generated by TPLHD except the DIRECT algorithm. The algorithms are required to achieve a relative error of E ≤ 0.01 %. All numerical experiments are performed on a personal computer with Intel 3.40GHz CPU in Matlab R2011b. In order to avoid unrepresentative numerical results, each experiment runs for 10 times except the experiments conducted by the deterministic DIRECT algorithm.\n3.2 Results and discussion\nFor the 13 test functions in Tables 1 and 3 shows the number of function evaluations [mean and standard deviation (SD)] required to achieve the specified relative error (0.01 %) by PLRS, HAM, CORS and DIRECT. The best result for each test function is marked in bold. Note that in the comparison study the allowed maximal number of function evaluations Nmax is set to 3000.\nThe comparative results indicate that the PLRS algorithm is consistently better than the DIRECT algorithm. For functions BR, BA, SC, GP and SH5 whose global optima lie on flat regions, PLRS locates the global optima more rapidly than DIRECT with the help of metamodels. For function SHU that has many local optima, PLRS converges faster with the\nswitch between lb-mode and gb-mode. For function mH6 whose global optimum lies on the boundary, PLRS quickly achieves the required accuracy, whereas DIRECT shows a much slower convergence than its result for the original H6.\nComparedwith the twometamodel-based algorithmsHAMandCORS, PLRS has a higher convergence speed in eight out of thirteen cases. CORS has a better performance than PLRS on functions H4, SHU, SH5 andH6. For function SHU, since it has many local optima, PLRS switched to the gb-mode for some times in order to avoid missing the attraction region of the global optimum. Thus, PLRS spends more function evaluations. On the contrary, since one initial point lies near the attraction region of the global optimum, the locally-biased search strategy employed in CORSmakes it require less points than PLRS. In addition, the standard deviation of the comparative results indicates that the performance of CORS is unstable on functions BA, S5, S7 and S10. For example, in 10 runs on function BA, the minimum number of function evaluations required by CORS to achieve the specified relative error is 65, while the maximum number of function evaluations is 345. Compared with PLRS, the HAM algorithm is unable to refine the solution to the required accuracy in some runs for functions S5, S7, S10, H6 and mH6. For instance, in one run for H6, HAM quickly obtains a solutionwith a relative error of<0.06 %after about 200 function evaluations. However, in the following steps, HAM is hard to refine the solution to the required accuracy. To summarize, the PLRS algorithm offers remarkable performance on the test functions in Table 1.\nFor the 40 GKLS functions in Tables 2 and 4 shows the number of function evaluations (mean and SD) required to achieve the specified relative error (0.01 %) by the four algorithms. Note that because of the auxiliary optimization problems in PLRS,HAMandCORS, the three algorithms are more time-consuming than the fast DIRECT algorithm. To save computing time, for simple class 1 we set the allowed maximal number of function evaluations as Nmax = 1000 for PLRS, HAM and CORS, for hard class 2 Nmax = 3000, for simple class 3 Nmax = 2500, and for hard class 4 Nmax = 4000. The best result for each GKLS function is marked in bold.\nIt is observed that PLRS performs better than DIRECT in 30 out of 40 GKLS functions. The remarkable performance of PLRS is related to the following fact. In the lb-mode, PLRS exploits current interesting region quickly with the help of the new approximate function, and this efficient exploitation also drives PLRS to duly switch to the gb-mode in order to find the next interesting region. For instance, Fig. 6 shows the convergence histories of two runs for f50 from class 4 by PLRS and DIRECT, respectively. Both of the two optimization processes can be partitioned into three stages. The first stage aims to jump out of a local optimum with function value f ∗ = 0, the second stage aims to jump out of a local optimum with function value f ∗ = −0.4054 and the third stage aims to exploit the attraction region of the global optimum to meet the required accuracy. In the first stage, DIRECT escapes from the local optimum more quickly than PLRS. In the second stage, PLRS escapes from the local optimum with much fewer function evaluations than DIRECT, which indicates that PLRS explores the domain in a more efficient way. In the third stage, PLRS uses significantly fewer function evaluations to meet the required accuracy, which indicates that PLRS is able to refine the solution to high accuracy efficiently.\nAdditionally, the standard deviation of the test results shows that the performance of PLRS on the GKLS functions are more volatile than that on the test functions in Table 1. This phenomenon occurs because of the complex characteristics of the GKLS functions. The GKLS functions have much smaller attraction region of the global optimum than the test functions in Table 1, and the global optimum is usually far away from the paraboloid vertex. In one iteration for GKLS functions, if PLRS samples a point within the attraction region, it\nTable 4 Results (mean and SD) for GKLS functions by different global optimization algorithms\n# Method f10 f20 f30 f40 f50 f60 f70 f80 f90 f100\n1 PLRS 213.4 182.0 148.8 200.2 156.6 147.6 133.2 193.2 42.8 170.4\n(81.0) (37.5) (61.8) (24.3) (62.1) (44.2) (31.4) (96.9) (10.5) (26.5)\nHAM >1000 >1000 >1000 >1000 >1000 >1000 >1000 >1000 >1000 >1000 (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) CORS 822.5 713.6 207.2 378.0 182.7 314.1 219.5 41.4 39.0 107.6\n(152.4) (288.6) (120.4) (164.8) (77.0) (82.5) (87.6) (7.6) (6.0) (30.2)\nDIRECT 305.0 321.0 175.0 135.0 125.0 151.0 663.0 127.0 101.0 133.0\n(0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0)\n2 PLRS 771.6 526.6 571.0 210.6 465.6 459.0 1008.0 756.0 829.0 278.4\n(156.7) (69.2) (124.3) (57.4) (74.2) (88.2) (194.8) (136.3) (123.8) (87.9)\nHAM >3000 >3000 >3000 >3000 >3000 >3000 >3000 >3000 >3000 >3000 (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) CORS 1920.0a 2114.7a 1218.3a 864.8 938.0 1337.2 2270.0a 2672.5a 2895.6 613.2\n(845.5) (362.4) (422.7) (284.3) (310.0) (342.0) (463.2) (115.6) (98.4) (137.0)\nDIRECT 1687.0 1411.0 1371.0 865.0 161.0 509.0 2991.0 93.0 975.0 1255.0\n(0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0)\n3 PLRS 468.0 188.2 288.8 332.2 252.2 515.0 793.6 315.4 418.2 632.2\n(59.7) (52.0) (84.6) (70.4) (83.5) (77.2) (178.1) (102.5) (114.2) (162.8)\nHAM >2500 >2500 >2500 >2500 >2500 >2500 >2500 >2500 >2500 >2500 (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) CORS 322.0 605.7 386.3 1525.7a 733.3a 1725.1a 1018.6 371.4 240.0 916.3\n(120.6) (363.5) (82.4) (904.1) (541.6) (827.0) (348.3) (146.5) (76.9) (311.4)\nDIRECT 3321.0 301.0 4641.0 367.0 2429.0 879.0 265.0 953.0 343.0 1773.0\n(0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0)\n4 PLRS 1004.0 667.8 1061.8 1145.2 688.6 1468.8 1136.0 811.0 719.8 848.6\n(190.2) (189.4) (178.0) (90.5) (154.7) (393.1) (460.9) (95.9) (127.7) (461.4)\nHAM >4000 >4000 >4000 >4000 >4000 >4000 >4000 >4000 >4000 >4000 (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) (-\\-) CORS 1152.3 652.5 1329.5 3464.0a >4000 >4000 975.0 3215.6a 1892.5 1874.0 (235.0) (358.6) (255.2) (500.4) (-\\-) (-\\-) (360.4) (306.4) (807.5) (661.9) DIRECT 1727.0 1069.0 1735.0 6307.0 5315.0 2487.0 1275.0 933.0 273.0 167.0\n(0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0) (0.0)\na The algorithm only succeeded to achieve the required relative error within the limited computing resource in some of the total 10 runs. The data is the average of successful runs\nwould quickly converge to the global optimum. However, even if PLRS samples a point near the attraction region, it may recognize the attraction region after many iterations.\nFor CORS, since this algorithm does not have a flexible switch between exploitation and exploration, it performs worse than PLRS in 33 out of 40 GKLS functions, and in terms of the standard deviation values, it has a less robust performance. For HAM, since this algorithm does not have an explicit global exploration term, it has been observed to produce a bad performance for some functions in Table 1. This disadvantage of HAM here is\nagain highlighted by the more sophisticated GKLS functions such that HAM can not achieve the required relative error within the limited number of function evaluations for all GKLS functions.\nTable 5 gives the average number of function evaluations for the four GKLS classes by the four algorithms, and also provides the improvements of PLRS over DIRECT. The results show that PLRS performs better for all the test classes, especially for the three-dimensional GKLS classes."
  }, {
    "heading": "4 Sensitivity analysis of optimization parameters",
    "text": "It is observed that some optimization parameters affect the performance of the PLRS algorithm. For instance, the number of initial points Nini in Step 1, the maximal number of continuous iterations imaxlb in the lb-mode in Step 12 and the metamodel type used in Step 5. Therefore, this section tries to explore the sensitivity of PLRS to these optimization parameters.\nTable 6 The sensitivity of the proposed algorithm to some optimization parameters\nFunc. Nini i max lb Metamodel\n2n 5na 10n 20n 5 10a 15 KRGa RBF\nBR 40.1 37.9 42.2 52.7 b 37.9 b 37.9 70.1 BA 72.1 55.5 58.9 71.4 b 55.5 b 55.5 145.0 SC 49.0 40.6 46.5 51.2 b 40.6 b 40.6 57.2\nGP 126.3 115.8 122.7 149.5 145.4 115.8 127.0 115.8 232.9\nSHU 219.1 219.9 275.9 294.0 231.3 219.9 228.3 219.9 208.8 H3 47.7 46.8 54.7 86.3 b 46.8 b 46.8 93.5 H4 64.5 67.1 83.3 126.3 b 67.1 b 67.1 72.2\nS5 113.4 104.5 144.6 194.4 140.9 104.5 116.7 104.5 101.5\nS7 94.2 97.5 469.3 566.0 144.4 97.5 106.5 97.5 100.5\nS10 111.7 98.2 447.3 684.4 146.1 98.2 114.9 98.2 100.2\nSH5 139.6 168.4 211.3 219.1 208.6 168.4 176.0 168.4 88.8\nH6 126.3 132.0 171.6 487.7 174.3 132.0 123.3 132.0 152.5\nmH6 113.5 124.7 180.0 516.2 144.8 124.7 162.8 124.7 142.6\n# 1 175.7 158.8 202.7 195.4 168.7 158.8 206.4 158.8 206.3\n# 2 684.8 587.6 688.7 554.6 666.5 587.6 681.9 587.6 673.3\n# 3 422.9 420.4 494.4 368.2 579.1 420.4 495.3 420.4 468.9\n# 4 1199.7 955.2 12667.0 1149.7 1190.0 955.2 947.4 955.2 1014.4\na This is the parameters used in the comparison of PLRS to other algorithms in Tables 2 and 4 b Indicates that PLRS did not enter the gb-mode when doing optimization for this function. Hence, there is no need to study the effect of imaxlb on this function\nTable 6 shows the average results of the PLRS algorithm with respect to the above optimization parameters for all the test functions used before. The “Nini” columns study the effect of the number of initial points on the performance of PLRS with Nini respectively being 2n, 5n, 10n and 20n. Note that in this part PLRS works with imaxlb = 10 and Kriging model for all test functions. The results show that too many initial points do not accelerate the optimization. For instance, Nini = 20n leads to the worst performance in most of the test functions, especially for functions S7, S10, H6 and mH6. These too many initial points, which just fill the entire domain evenly, place much emphasis on exploration and thus may hide the advantage of the proposed algorithm.\nThe “imaxlb ” columns study the effect of the parameter i max lb on the performance of PLRS with imaxlb respectively being 5, 10 and 15. Note that in this part PLRS works with Nini = 5n and Kriging model for all test functions. The comparative results show that imaxlb = 10 is a proper value for these test functions. Compared with imaxlb = 10, imaxlb = 5 forces the algorithm to switch to the gb-mode prematurely before the current interesting region has been scooped out completely, while imaxlb = 20 forces the algorithm to exploit the current interesting region unduly in the lb-mode.\nThe “Metamodel” columns study the effect of the metamodel type on the performance of PLRS with metamodels respectively constructed by Kriging and RBF. Note that in this part PLRS works with Nini = 5n and imaxlb = 10 for all test functions. Since the Kriging model is usuallymore accurate for nonlinear problems than theRBFmodel by using a time-consuming\noptimization process [45], PLRS with the Kriging model performs better in most of the test functions."
  }, {
    "heading": "5 Engineering application",
    "text": "This section applies the PLRS algorithm to an engineering design problem to demonstrate its capability and scalability. The engineering problem is the shape optimization of a highspeed energy storage flywheel. Flywheel works as kinetic storage and retrieval device. It delivers high output power at high rotational speed and is available in various stages of development, especially in advanced technological areas, e.g., spacecraft. Various methods have been applied to design the flywheel from different perspectives [46–49].\nFigure 7 shows the schematic for this example, which is constructed with 11 points. Points 1 through 11 are fit a spline. The radius of the flywheel is R = 250mm, the thickness is T = 120mm and the angular velocity is ω = 12, 000 r/min. A material with modulus of elasticity of E = 217GPa, density of ρ = 7850 kg/m3, Poisson’s ratio of ν = 0.3 and yield stress of σY = 816MPa is considered in this example.\nThe objective function for this example is the specific energy density (SED, J/kg) of the flywheel, which is defined as\nSED = 1 2 I mass ω2, (18)\nwhere I (kg · m2) is the mass moment of inertia:\nI = ∫\nv\nρr2dV . (19)\nThe design variables are the thicknesses represented byY coordinates of the 11 design points, which are designated as t1 ∼ t11 in Fig. 7. These design variables are bounded between 25 and 100mm. The maximum equivalent stress of the flywheel should satisfy the failure criterion. Considering the permissible stress and the limited variable ranges, the optimization problem can be formulated as\nMaximize SED\ns.t.\n{\nσemax ≤ [σ ] = σYk 25mm ≤ ti ≤ 100mm, i = 1, 2, . . . , 11 , (20)\nwhere σemax is the maximum equivalent stress; [σ ] denotes the maximum permissible stress; k denotes the safety factor and is set to 1.6 in this example.\nTa bl e 7\nR es ul ts of\nsh ap e op\ntim iz at io n of\nth e fly\nw he el vi a di ff er en tg\nlo ba lo\npt im\niz at io n al go\nri th m s\nD es ig n\nt 1 t 2\nt 3 t 4\nt 5 t 6\nt 7 t 8\nt 9 t 1 0\nt 1 1\nσ e m ax\n(M Pa )\nSE D (J /K g)\nI (k g. m 2 )\nM as s (k g)\nO ri gi na l\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n60 .0 0\n32 3. 69\n24 66\n9. 12\n5. 78\n18 4. 95\nPL R S\n57 .1 5\n32 .8 3\n31 .9 1\n30 .6 5\n30 .1 1\n30 .1 1\n29 .7 4\n29 .0 8\n34 .4 1\n88 .6 6\n85 .0 3\n49 7. 72\n31 02\n4. 43\n5. 71\n14 5. 37\nH A M\n69 .8 1\n37 .1 5\n37 .0 9\n31 .4 6\n30 .4 7\n27 .4 0\n27 .9 4\n29 .5 0\n73 .6 9\n85 .8 0\n56 .8 5\n48 7. 11\n30 08\n1. 09\n5. 94\n15 5. 98\nC O R S\n62 .1 2\n44 .0 8\n41 .8 0\n40 .9 1\n49 .6 2\n30 .1 6\n25 .4 4\n25 .0 0\n54 .4 3\n10 0. 0\n10 0. 0\n47 5. 89\n30 63\n3. 76\n6. 65\n17 1. 36\nD IR\nE C T\n37 .5 0\n37 .5 0\n37 .5 0\n37 .5 0\n37 .5 0\n37 .5 0\n29 .1 6\n37 .5 0\n62 .5 0\n95 .8 3\n87 .5 0\n50 5. 30\n30 23\n8. 99\n6. 67\n17 4. 27\nTo solve problem (20), we transform the nonlinear constrained problem into boxconstrained problem by using a simple penalty scheme (the penalty factor is set to 0.5). This article employs the ANSYS simulator, a finite element analysis (FEA) tool, to obtain the stress distribution of the flywheel. The optimization process is run in MATLAB environment automatically with different global optimization algorithms. For this engineering problem, each algorithm except DIRECT starts with 5n initial points generated by TPLHD, and is allowed to run for 1000 function evaluations. Each experiment runs for 5 times and their most successful results are shown in Table 7.\nThe second row of Table 7 offers the data of the original design. Results in Table 7 show that the design obtained by the PLRS algorithm offers a 25.71 % improvement in SED than the original design, while CORS offers a 24.18 % improvement, DIRECT offers a 22.58 % improvement and HAMoffers a 21.93 % improvement. Figure 8 illustrates the outlines of the original design and the optimal designs obtained by different algorithms. The optimization processes of the four algorithms in Fig. 9 show that PLRS has the highest convergence speed in the later stage of the optimization process. In addition, PLRS, CORS and HAM begin with a much better design than DIRECT since TPLHD generates initial samples with both good space-filling and projective properties."
  }, {
    "heading": "6 Conclusion",
    "text": "This article proposes the PLRS algorithm for the global optimization of computationally expensive black box functions. The PLRS algorithm presents a new approximate function F̂(X,Y, K̂ ), which describes the lower bounds of the function f in a more compact waywith the help of the metamodel f̂M . By adjusting the estimate K̂ , the new approximate function F̂ can approximate f in a fine way in order to exploit the interesting regions efficiently. Meanwhile, F̂ can approximate f in a coarse way in order to explore the entire domain. The\ncycle operation through a set of potential Lipschitz constants K̂P from fine to coarse makes the proposed algorithm converge to the global optimum efficiently. Comparative results on 53 test functions and an engineering application indicate that PLRS is competitive with existing global optimization algorithms for computationally expensive black box functions.\nAcknowledgments The authors appreciate the financial support from National Natural Science Foundation of China (11402047, 51308090), National Program on Key Basic Research Project (2015CB057301) and Ph.D. Programs Foundation of Liaoning Province (20131019)."
  }],
  "year": 2015,
  "references": [{
    "title": "Global Optimization in Action: Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications, vol",
    "authors": ["J. Pintér"],
    "year": 1996
  }, {
    "title": "Global Optimization With Non-convex Constraints: Sequential and Parallel Algorithms, vol. 45",
    "authors": ["R.G. Strongin", "Y.D. Sergeyev"],
    "year": 2000
  }, {
    "title": "A sequential method seeking the global maximum of a function",
    "authors": ["B.O. Shubert"],
    "venue": "SIAM J. Numer. Anal. 9(3),",
    "year": 1972
  }, {
    "title": "An algorithm for finding the global maximum of a multimodal, multivariate function",
    "authors": ["R.H. Mladineo"],
    "venue": "Math. Program",
    "year": 1986
  }, {
    "title": "Lipschitzian optimization without the Lipschitz constant",
    "authors": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"],
    "venue": "J. Optim. Theory Appl. 79(1),",
    "year": 1993
  }, {
    "title": "Global search based on efficient diagonal partitions and a set of Lipschitz constants",
    "authors": ["Y.D. Sergeyev", "D.E. Kvasov"],
    "venue": "SIAM J. Optim",
    "year": 2006
  }, {
    "title": "A univariate global search working with a set of Lipschitz constants for the first derivative",
    "authors": ["D.E. Kvasov", "Y.D. Sergeyev"],
    "venue": "Optim. Lett. 3(2),",
    "year": 2009
  }, {
    "title": "Estimation of the minimum of a function using order statistics",
    "authors": ["L. De Haan"],
    "venue": "J. Am. Stat. Assoc. 76(374),",
    "year": 1981
  }, {
    "title": "Estimation of the Lipschitz constant of a function",
    "authors": ["G. Wood", "B. Zhang"],
    "venue": "J. Glob. optim",
    "year": 1996
  }, {
    "title": "Index information algorithm with local tuning for solving multidimensional global optimization problems with multiextremal",
    "authors": ["Y.D. Sergeyev", "P. Pugliese", "D. Famularo"],
    "venue": "constraints. Math. Program",
    "year": 2003
  }, {
    "title": "An algorithm for finding the absolute extremum of a function",
    "authors": ["S. Piyavskii"],
    "venue": "USSR Comput. Math. Math. Phys. 12(4),",
    "year": 1972
  }, {
    "title": "S.,Wang,X.,Wu, J., Song,Y.:A global optimization algorithm for simulation-based problems via the extended DIRECT scheme",
    "authors": ["H. Liu", "Xu"],
    "year": 2014
  }, {
    "title": "A locally-biased form of the DIRECT algorithm",
    "authors": ["J.M. Gablonsky", "C.T. Kelley"],
    "venue": "J. Glob. Optim",
    "year": 2001
  }, {
    "title": "A fully-distributed parallel global search algorithm",
    "authors": ["L.T. Watson", "C.A. Baker"],
    "venue": "Eng. Comput. 18(1/2),",
    "year": 2001
  }, {
    "title": "Fast parameter optimization of large-scale electromagnetic objects using DIRECT with Kriging metamodeling",
    "authors": ["E.S. Siah", "M. Sasena", "J.L. Volakis", "P.Y. Papalambros", "R.W. Wiese"],
    "venue": "Microw. Theory Tech. IEEE Trans. 52(1),",
    "year": 2004
  }, {
    "title": "A DIRECT-based approach exploiting local minimizations for the solution of large-scale global optimization problems",
    "authors": ["G. Liuzzi", "S. Lucidi", "V. Piccialli"],
    "venue": "Comput. Optim. Appl. 45(2),",
    "year": 2010
  }, {
    "title": "A modified direct algorithm with bilevel partition",
    "authors": ["Q. Liu", "W. Cheng"],
    "venue": "J. Glob. Optim",
    "year": 2014
  }, {
    "title": "Simplicial Lipschitz optimization without the Lipschitz constant",
    "authors": ["R. Paulavičius", "J. Žilinskas"],
    "venue": "J. Glob. Optim",
    "year": 2013
  }, {
    "title": "Globally-biased Disimpl algorithm for expensive global optimization",
    "authors": ["R. Paulavičius", "Y.D. Sergeyev", "D.E. Kvasov", "J. Žilinskas"],
    "venue": "J. Glob. Optim. 59(2–3),",
    "year": 2014
  }, {
    "title": "Direct global optimization algorithm",
    "authors": ["D.R. Jones"],
    "venue": "Encyclopedia of Optimization,",
    "year": 2001
  }, {
    "title": "Advantages of simplicial partitioning for Lipschitz optimization problems with linear constraints",
    "authors": ["R. Paulavičius", "J. Žilinskas"],
    "venue": "Optim. Lett., pp",
    "year": 2014
  }, {
    "title": "Modification of DIRECT for high-dimensional design problems",
    "authors": ["A. Tavassoli", "K. Haji Hajikolaei", "S. Sadeqi", "G.G. Wang", "E. Kjeang"],
    "venue": "Eng. Optim. 46(6),",
    "year": 2014
  }, {
    "title": "Global optimization by multilevel coordinate search",
    "authors": ["W. Huyer", "A. Neumaier"],
    "venue": "J. Glob. Optim",
    "year": 1999
  }, {
    "title": "Simultaneous search for multiple QTL using the global optimization algorithm DIRECT",
    "authors": ["K. Ljungberg", "S. Holmgren", "Ö. Carlborg"],
    "venue": "Bioinformatics 20(12),",
    "year": 2004
  }, {
    "title": "An information global optimization algorithm with local tuning",
    "authors": ["Y.D. Sergeyev"],
    "venue": "SIAM J. Optim",
    "year": 1995
  }, {
    "title": "Local tuning and partition strategies for diagonal GO methods",
    "authors": ["D.E. Kvasov", "C. Pizzuti", "Y.D. Sergeyev"],
    "venue": "Numer. Math. 94(1),",
    "year": 2003
  }, {
    "title": "Global one-dimensional optimization using smooth auxiliary functions",
    "authors": ["Y.D. Sergeyev"],
    "venue": "Math. Program",
    "year": 1998
  }, {
    "title": "A deterministic global optimization using smooth diagonal auxiliary functions",
    "authors": ["Y.D. Sergeyev", "D.E. Kvasov"],
    "venue": "Commun. Nonlinear Sci. Numer. Simul. 21(1),",
    "year": 2015
  }, {
    "title": "Spatial prediction and ordinary kriging",
    "authors": ["N. Cressie"],
    "venue": "Math. Geol. 20(4),",
    "year": 1988
  }, {
    "title": "Global response approximation with radial basis functions",
    "authors": ["H. Fang", "M.F. Horstemeyer"],
    "venue": "Eng. Optim. 38(04),",
    "year": 2006
  }, {
    "title": "Efficient global optimization of expensive black-box functions",
    "authors": ["D.R. Jones", "M. Schonlau", "W.J. Welch"],
    "venue": "J. Glob. Optim",
    "year": 1998
  }, {
    "title": "A radial basis function method for global optimization",
    "authors": ["Gutmann", "H.-M"],
    "venue": "J. Glob. Optim",
    "year": 2001
  }, {
    "title": "Constrained global optimization of expensive black box functions using radial basis functions",
    "authors": ["R.G. Regis", "C.A. Shoemaker"],
    "venue": "J. Glob. Optim",
    "year": 2005
  }, {
    "title": "An adaptive radial basis algorithm (ARBF) for expensive black-box global optimization",
    "authors": ["K. Holmström"],
    "venue": "J. Glob. Optim",
    "year": 2008
  }, {
    "title": "Hybrid and adaptive meta-model-based global optimization",
    "authors": ["J. Gu", "G. Li", "Z. Dong"],
    "venue": "Eng. Optim",
    "year": 2012
  }, {
    "title": "Introduction to Global Optimization Exploiting Space-Filling Curves",
    "authors": ["Y.D. Sergeyev", "R.G. Strongin", "D. Lera"],
    "year": 2013
  }, {
    "title": "An algorithm for global optimization of Lipschitz continuous functions",
    "authors": ["C. Meewella", "D. Mayne"],
    "venue": "J. Optim. Theory Appl. 57(2),",
    "year": 1988
  }, {
    "title": "Minimax and maximin distance designs",
    "authors": ["M.E. Johnson", "L.M. Moore", "D. Ylvisaker"],
    "venue": "J. Stat. Plan. Inference 26(2),",
    "year": 1990
  }, {
    "title": "An algorithm for fast optimal Latin hypercube design of experiments",
    "authors": ["F.A. Viana", "G. Venter", "V. Balabanov"],
    "venue": "Int. J. Numer. Methods Eng. 82(2),",
    "year": 2010
  }, {
    "title": "Sequential sampling designs based on space reduction",
    "authors": ["H. Liu", "S. Xu", "X. Wang"],
    "venue": "Eng. Optim.,",
    "year": 2014
  }, {
    "title": "Algorithm 829: software for generation of classes of test functions with known local and global minima for global optimization",
    "authors": ["M. Gaviano", "D.E. Kvasov", "D. Lera", "Y.D. Sergeyev"],
    "venue": "ACM Trans. Math. Softw. (TOMS)",
    "year": 2003
  }, {
    "title": "Review ofmetamodeling techniques in support of engineering design optimization",
    "authors": ["G.G. Wang", "S. Shan"],
    "venue": "J. Mech. Des. 129(4),",
    "year": 2007
  }, {
    "title": "Multiobjective flywheel design: a doe-based concept exploration task",
    "authors": ["U. Lautenschlager", "H.A. Eschenauer", "F. Mistree"],
    "venue": "Advances in Design Automation, pp",
    "year": 1997
  }, {
    "title": "The optimization of flywheels using an injection island genetic algorithm",
    "authors": ["D. Eby", "R. Averill", "E. Goodman", "W. Punch"],
    "venue": "Evol. Des. Comput., pp",
    "year": 1999
  }, {
    "title": "Shape optimization of a flywheel",
    "authors": ["G. Kress"],
    "venue": "Struct. Multidiscip. Optim. 19(1),",
    "year": 2000
  }],
  "id": "SP:d1eed3b3c7e5b97239d83f39c54ab249f63bca2c",
  "authors": [{
    "name": "Haitao Liu",
    "affiliations": []
  }, {
    "name": "Shengli Xu",
    "affiliations": []
  }, {
    "name": "Ying Ma",
    "affiliations": []
  }, {
    "name": "Xiaofang Wang",
    "affiliations": []
  }],
  "abstractText": "This article develops a novel global optimization algorithm using potential Lipschitz constants and response surfaces (PLRS) for computationally expensive black box functions. With the usage of the metamodeling techniques, PLRS proposes a new approximate function F̂ to describe the lower bounds of the real function f in a compact way, i.e., making the approximate function F̂ closer to f . By adjusting a parameter K̂ (an estimate of the Lipschitz constant K ), F̂ could approximate f in a fine way to favor local exploitation in some interesting regions; F̂ can also approximate f in a coarse way to favor global exploration over the entire domain. When doing optimization, PLRS cycles through a set of identified potential estimates of the Lipschitz constant to construct the approximate function from fine to coarse. Consequently, the optimization operates at both local and global levels. Comparative studies with several global optimization algorithms on 53 test functions and an engineering application indicate that the proposed algorithm is promising for expensive black box functions.",
  "title": "Global optimization of expensive black box functions using potential Lipschitz constants and response surfaces"
}