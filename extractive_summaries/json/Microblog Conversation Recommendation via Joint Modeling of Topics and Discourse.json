{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 375–385 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Online platforms have revolutionized the way individuals collect and share information (O’Connor et al., 2010; Lee and Ma, 2012; Bakshy et al., 2015), but the vast bulk of online content is irrelevant or unpalatable to any given individual. A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015).\nHow do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users? We approach this problem from a microblog conversation recommendation framework. Where prior work has focused on the content of individual posts for recommendation (Chen\net al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new\n1In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument. The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes.\n375\nand repeated entry into conversations based on a combination of topical and discourse features.\nTo illustrate the interplay between topics and discourse, Figure 1 displays two snippets of conversations on Twitter collected during the 2016 United States presidential election. User U1 participates in both conversations. The first conversation is centered around Clinton, and U1, who is more typically involved with conversations about candidate Sanders, does not return. In the second conversation, however, U1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.\nEffective conversation prediction and recommendation requires an understanding of both user interests and discourse behaviors, such as agreement, disagreement, inquiry, backchanneling, and emotional reactions. However, acquiring manual labels for both is a time-consuming process and hard to scale for new datasets. We instead propose a unified statistical learning framework for conversation recommendation, which jointly learns (1) hidden factors that reflect user interests based on conversation history, and (2) topics and discourse modes in ongoing conversations, as discovered by a novel probabilistic latent variable model. Our model is built on the success of collaborative filtering (CF) in recommendation systems, where latent dimensions of product ratings or movie reviews are extracted to better capture user preferences (Linden et al., 2003; Salakhutdinov and Mnih, 2008; Wang and Blei, 2011; McAuley and Leskovec, 2013). To the best of our knowledge, we are the first to model both topics and discourse modes as part of a CF framework and apply it to microblog conversation recommendation.2\nExperimental results on two Twitter conversation datasets show that our proposed model yields significantly better performance than state-of-theart post-level recommendation systems. For example, by leveraging both topical content and discourse structure, our model achieves a mean average precision (MAP) of 0.76 on conversations about the U.S. presidential election, compared with 0.70 by McAuley and Leskovec (2013), which only considers topics. We further con-\n2To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications.\nducted detailed analysis on the latent topics and discourse modes and find that our model can discover reasonable topic and discourse representations, which play an important role in characterizing reply behaviors. Finally, we also provide a pilot study on recommendation for first time replies, which shows that our model outperforms comparable recommendation systems.\nThe rest of this paper is structured as follows. The related work is discussed in Section 2. We then present our microblog conversation recommendation model in Section 3. The experimental setup and results are described in Sections 4 and 5. Finally, we conclude in Section 6."
  }, {
    "heading": "2 Related Work",
    "text": "Social media has attracted increasing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011). The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post. Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015). These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016).\nDistinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011).\nOur work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017). Topic modeling\nhas been employed to identify conversation content on Twitter (Ritter et al., 2010). In this work, we propose a probabilistic model to capture both topics and discourse modes as latent variables. A further line of work studies the reposting and reply structure of conversations (Gómez et al., 2011; Laniado et al., 2011; Backstrom et al., 2013; Budak and Agrawal, 2013). But none of this work distinguishes the rich discourse functions of replies, which is modeled and exploited in our work."
  }, {
    "heading": "3 The Joint Model of Topic and Discourse for Recommendation",
    "text": "Our proposed microblog conversation recommendation framework is based on collaborative filtering and a novel probabilistic graphical model. Concretely, our objective function takes the form:\nminL+ µ ·NLL(C |Θ) (1)\nThis function encodes two types of information. First, L models user reply preference in a similar fashion to collaborative filtering (CF) (Hu et al., 2008; Pan et al., 2008). It captures topics of interests and discourse structures users are commonly involved (e.g., argumentation), and takes the form of mean square error (MSE) based on user reply history. This part is detailed in Section 3.1.\nThe second term, NLL(C |Θ), denotes the negative log-likelihood of a set of conversations C, with Θ containing all parameters. A probabilistic model is described in Section 3.2 that shows how the topical content and discourse structures of conversations are captured by these latent variables.\nThe hyperparameter µ controls the trade-off between the two effects. `2 regularization is also added for parameters to avoid model overfitting.\nFor the rest of this section, we first present the construction of L andNLL(C |Θ) in Sections 3.1 and 3.2. We then discuss how these two components can be mutually informed by each other in Section 3.3. Finally, the generative process and parameter learning are described in Section 3.4.\n3.1 Reply Preference (L) Our user reply preference modeling is built on the success of collaborative filtering (CF) for product ratings. However, classic CF problems, such as product recommendation, generally rely on explicit user feedback. Unlike user ratings on products, our input lacks explicit feedback from users about negative preferences and nonresponse. Therefore, we follow one-class Collaborative Filtering (Hu et al., 2008; Pan et al., 2008),\nwhich weights positive instances higher during training and is thus suited to our data. Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c. ru,c equals 1 if u is in the conversation history; otherwise, it is 0. The first term of objective (Eq. 1) takes the following form:\nL = |U|∑\nu=1\n|C|∑\nc=1\nfu,c · (pu,c − ru,c)2 (2)\nwhere U consists of users {u} and C is a set of conversations {c} in a dataset. fu,c is the corresponding weight for a conversation c and a target user u. Intuitively, it has a large value if positive feedback (user replied) is observed. Therefore, we adapt the formulation from Pan et al. (2008):\nfu,c = { s if ru,c = 1 (i.e., user replied) 1 if ru,c = 0\n(3)\nwhere s > 1, an integer hyperparameter to be tuned.\nInspired by prior models (Koren et al., 2009; McAuley and Leskovec, 2013), we propose the following latent factor model to describe pu,c:\npu,c = λ · γUu · γCc + (1− λ) · δUu · δCc + bu + bc + a (4)\nγUu and γ C c are K-dimensional latent vectors that encode topic-specific information (where K is the number of latent topics) for users and conversations. Specifically, γUu reflects the topical interests of u, with higher value γUu,k indicating greater interest by u in topic k. γCc captures the extents that topics are discussed in conversation c.\nSimilarly, D-dimensional vectors δUu and δ C c capture discourse structures in shaping reply behaviors (where D is the number of discourse clusters). δUu reflects the discourse behaviors u prefers, such as u1 often enjoys arguments as in the second conversation of Figure 1, while δCc captures the discourse modes used throughout conversation c. By multiplying user and conversation factors, we can measure the corresponding similarity. The predicted score pu,c thereby reflects the tendency for a user u to be involved in conversation c.\nAs pointed out by McAuley and Leskovec (2013), these latent vectors often encode hidden factors that are hard to interpret under a CF framework. Therefore, in Section 3.2, we present a novel probabilistic model which can extract interpretable topics and discourse modes as word\ndistributions. We then describe how they can be aligned with the latent vectors of γC and δU .\nParameter a is an offset parameter, bu and bc are user and conversation biases, and λ ∈ [0, 1] serves as the weight for trading offs of topic and discourse factors in reply preference modeling.\n3.2 Corpus Likelihood NLL(C |Θ) Here we present a novel probabilistic model that learns coherent word distributions for latent topics and discourse modes of conversations. Formally, we assume that each conversation c ∈ C contains Mc messages, and each message m has Nc,m words. We distinguish three latent components – discourse, topic, and background – underlying conversations, each with their own type of word distribution. At the corpus level, there are K topics represented by word distribution φTk (k = 1, 2, ...,K), while φDd (d = 1, 2, ..., D) represents the D discourse modes embedded in corpus. In addition, we add a background word distribution φB to capture general information (e.g., common words), which do not indicate either discourse or topic information. φDd , φ T k , and φ\nB are all multinomial word distributions over vocabulary size V . Below describes more details.\nMessage-level Modeling. Our model assigns two types of message-level multinomial variables to each message: zc,m reflects its latent topic and dc,m represents its discourse mode.\nTopic assignments. Due to the short nature of microblog posts, we assume each message m in conversation c contains only one topic, indexed as zc,m. This strategy has been proven useful to alleviate data sparsity for topic inference (Quan et al., 2015). We further assume messages in the same conversation would focus on similar topics. We thus draw topic zc,m ∼ θc, where θc denotes the fractions of topics discussed in conversation c.\nDiscourse assignments. To capture discourse behaviors of u, distribution πu is used to represent the discourse modes in messages posted by u. The discourse mode dc,m for message m is then generated from πuc,m , where uc,m is the author of m in c.\nWord-level Modeling. We aim to separate discourse, topic, and background information for conversations. Therefore, for each word wc,m,n of message m, a ternary switcher xc,m,n ∈ {DISC, TOPIC,BACK} controls word wc,m,n to\nfall into one of the three types: discourse, topic, and background.\nDiscourse words (DISC) are indicative of the discourse modes of messages. When xc,m,n = DISC (i.e., wc,m,n is assigned as a discourse word), word wc,m,n is generated from the discourse word distribution φDdc,m where dc,m is discourse assignment to message m.\nTopic words (TOPIC) describe the topical focus of a conversation. When xc,m,n = TOPIC, wc,m,n is assigned as a topic word and generated from φTzc,m – word distribution given topic of m.\nBackground words (BACK) capture the general information that is not related to discourse or topic. When word wc,m,n is assigned as a background word (xc,m,n = BACK), it is drawn from background distribution φB .\nSwitching among Topic, Discourse, and Background. We further assume the word type switcher xc,m,n is sampled from a multinomial distribution which depends on the current discourse mode dc,m. The intuition is that messages of different discourse modes may show different distributions of the three word types. For instance, a statement message may contain more content words than a rhetorical question. Specifically, xc,m,n ∼ Multi(τdc,m), where τd is a 3-dimension stochastic vector that expresses the appearing probabilities of three kinds of words (DISC, TOPIC, BACK), when the discourse assignment is d. Stop words and punctuations are forced to be labeled as discourse or background. By explicitly distinguishing different types of words with switcher xc,m,n, we can thus separate word distributions that reflect discourse, topic, and background information.\nLikelihood. Based on the message-level and the word-level generation process, the probability of observing words in the given corpus is:\nPr(C |θ,π,φ, τ , z,d,x)\n=\nC∏\nc=1\nMc∏\nm=1\nθc,zc,mπuc,m,dc,m\n× ∏\nxc,m,n=BACK\nτdc,m,BACKφ B wc,m,n\n× ∏\nxc,m,n=DISC\nτdc,m,DISCφ D dc,m,wc,m,n\n× ∏\nxc,m,n=TOPIC\nτdc,m,TOPICφ T zc,m,wc,m,n\n(5)\nAnd we use negative log likelihood to model corpus likelihood effect in Eq. 1, i.e., NLL(C |Θ) =\n− log(Pr(C |Θ), where parameters set Θ = {θ,π,φ, τ , z,d,x}."
  }, {
    "heading": "3.3 Mutually Informed User Preference and",
    "text": "Latent Variables\nAs mentioned above, the hidden factors discovered in Section 3.1 lack interpretability, which can be boosted by the learned latent topics and discourse modes in Section 3.2. However, it is nontrivial to link the topic-related parameters of γCc to the conversation topic distributions of θc, since the former takes real values from −∞ to +∞ while the latter is a stochastic vector. Therefore, we follow the strategy from McAuley and Leskovec (2013) to apply a softmax function over γCc :\nθc,k = exp(κT γCc,k)∑K\nk′=1 exp(κ T γCc,k′)\n(6)\nWe further assume that the discourse mode preference by users, δUu , can also be informed by the discourse mode distribution captured by πu, i.e., a user who enjoys arguments may be willing to participate another. So similarly, we define:\nπu,d = exp(κDδUu,d)∑D\nd′=1 exp(κ DδUu,d′)\n(7)\nwhere κT and κD are learnable parameters that control the “peakiness” of the transformation. For example, a larger κT indicates a more focused conversation, while a smaller κT means users discuss diverse topics.\nFinally, softmax transformation is also applied to φTk , φ D d , φ\nB , and τd, as done in McAuley and Leskovec (2013), with additional parameters ψTk , ψDd , ψ\nB , and χd (as shown in Figure 2). This is to ensure that the distributions φ∗∗ and τd are stochastic vectors. In doing so, these distributions can be learned via optimizing ψ∗∗ and χd, which take any value and thus ensure that the cost function in Eq. 1 is optimized without considering any parameter constraints."
  }, {
    "heading": "3.4 Generative Process and Model Learning",
    "text": "Our word generation process is displayed in Figure 2 and described as follows:\n• Compute topic distribution θc by Eq. 6 • For message m = 1 to Mc:\n– Compute discourse distribution πuc,m by Eq. 7 – Draw topic assignment zc,m ∼Multi(θc) – Draw discourse mode dc,m ∼Multi(πuc,m) – For word index n = 1 to Nc,m: ∗ Draw word type xc,m,n ∼Multi(τd)\n∗ if xc,m,n == BACK: Draw word wc,m,n ∼Multi(φB) ∗ if xc,m,n == DISC: Draw word wc,m,n ∼Multi(φDdc,m) ∗ if xc,m,n == TOPIC: Draw word wc,m,n ∼Multi(φTzc,m)\nParameter Learning. For learning, we randomly initialize all learnable parameters and then alternate between the following two steps: Step 1. Fix topic and discourse assignments z and d, and word type switcher x, then optimize the remaining parameters in Eq. 1 by L-BFGS (Nocedal, 1980):\nUpdate a, b, γ∗, δ∗, κ∗, ψ∗, χ = argminL+ µ ·NLL(C |Θ) (8)\nStep 2. Sample topic and discourse assignments z and d at the message level and word type switcher x at the word level, using the distributions, computed according to parameters optimized in step 1:\nSample zc,m, dc,m, xc,m,n with probabilities p(zc,m = k) = θc,k\np(dc,m = d) = πuc,m,d p(xc,m,n = BACK) = φ B wc,m,nτdc,m,BACK p(xc,m,n = DISC) = φ D dc,m,wc,m,nτdc,m,DISC p(xc,m,n = TOPIC) = φ T zc,m,wc,m,nτdc,m,TOPIC\n(9)\nStep 2 is analogous to Gibbs Sampling (Griffiths, 2002) in probabilistic graphical models, such as LDA (Blei et al., 2003). However, distinguishing from previous models, the multinomial distributions in our models are not drawn from a Dirichlet prior. Instead, they are computed based on the parameters learned in Step 1.\nOur learning process stops when the change of parameters is small (i.e., below a pre-specified\nthreshold). Multiple restarts are tried, and similar results are achieved."
  }, {
    "heading": "4 Experimental Setup",
    "text": "Datasets. We collected two microblog conversation datasets from Twitter for experiments3: one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC)4. US Election was collected from January to June of 2016 using Twitter’s Streaming API5 with a small set of political keywords.6 To recover conversations, Tweet Search API7 was used to retrieve messages with the “inreply-to” relations to collect tweets in a recursive way until full conversations were recovered.\nStatistics of the datasets are shown in Table 1. Figure 3 displays the number of conversations individual users participated in. As can be seen, most users are involved in only a few conversations. Simply leveraging personal chat history will not produce good performance for conversation\n3The datasets are available at http://www.ccs. neu.edu/home/luwang/\n4 http://trec.nist.gov/data/tweets/ 5https://developer.twitter.com/\nen/docs/tweets/filter-realtime/ api-reference/post-statuses-filter.html\n6Keyword list: “trump”, “hillary”, “clinton”, “president”, “politics”, and “election.”\n7https://developer.twitter.com/en/ docs/tweets/search/api-reference/ get-saved_searches-show-id\nrecommendation. In our experiments, we predict whether a user will engage in a conversation given the previous messages in that conversation and past conversations the user is involved. For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.8\nPreprocessing and Hyperparameter Tuning. For preprocessing, links, mentions (i.e., @username), and hashtags in tweets were replaced with generic tags of “URL”, “MENTION”, and “HASHTAG”. We then utilized the Twitter NLP tool9 (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization and non-alphabetic token removal. We removed stop words and punctuations for all comparisons to ensure comparable performance. We maintain a vocabulary with the 5,000 most frequent words.\nOur model parameters are tuned on the development set based on grid search, i.e. the parameters that give the lowest value for our objective are selected. Specifically, the number of discourse modes (D) and topics (K) are tuned to be 10. The trade-off parameter µ between user preference and corpus negative log-likelihood takes value of 0.1, and λ, the parameter for balancing topic and discourse, is set to 0.5. Finally, the confidence parameter s takes a value of 200 to give higher weight for positive instances, i.e., a user replied to a conversation.\nEvaluation Metrics. Following prior work on social media post recommendation (Chen et al., 2012; Yan et al., 2012), we treat our task on conversation recommendation as a ranking problem. Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (Manning et al., 2008), and normalized Discounted Cumulative Gain at K (nDCG@K) (Järvelin and Kekäläinen, 2002) are reported. The metrics are computed per user in the dataset and then averaged over all users. The values range from 0.0 to 1.0, with higher values indicating better performance.\nBaselines and Comparisons. For comparison, we first consider three baselines: 1) ranking\n8At least one turn per conversation is retained for training. It is possible that one user only replies in either development set or test set, but it is rather infrequent.\n9http://www.cs.cmu.edu/˜ark/TweetNLP/\nconversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).\nWe further compare results with three established recommendation models: • OCCF: one-class Collaborative Filtering (Pan et al., 2008), which only considers users’ reply history without modeling content in conversations. • RSVM: ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in Duan et al. (2010). • CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied (Chen et al., 2012).\nFinally, we also adapt the “hidden factors as topics” (HFT) model proposed in McAuley and Leskovec (2013) (henceforth ADAPTED HFT). Because the original model leverages the ratings for all product reviews and does not handle implicit user feedback well, we replace their user preference objective function with ours (Eq. 2)."
  }, {
    "heading": "5 Experimental Results",
    "text": "In this section, we first discuss our main evaluation in Section 5.1. A case study and corresponding discussion are provided in Section 5.2 to provide further insights, which is followed by an analysis of the topics and discourse modes discovered by our model (Section 5.3). We also examine our performance on first time replies (Section 5.4)."
  }, {
    "heading": "5.1 Conversation Recommendation Results",
    "text": "Experimental results are displayed in Table 2, where our model yields statistically significantly better results than baselines and comparisons\n(paired t-tests, p < 0.01). For P@K, we only report P@1, because a significant amount of users participate only in 1 or 2 conversations. For nDCG@K, different K values are experimented, which results in similar trend, so only nDCG@5 is reported.\nWe find that the baselines that rank conversations with simple features (e.g., length or popularity) perform poorly. This implies that generic algorithms that do not consider conversation content or user preference cannot produce reasonable recommendations.\nAlthough some non-baseline systems capture content in one way or another, only ADAPTED HFT and our model exploit latent topic models to better represent content in tweets, and outperform other methods.\nCompared to ADAPTED HFT, which only considers latent topics under a collaborative filtering framework, our model extracts both topics and discourse modes as latent variables, and shows superior performance on both datasets. Our discourse variables go beyond topical content to capture social behaviors that affect user engagement, such as\narguments, question-asking, agreement, and other discourse modes.\nTraining with Varying Conversation History. To test the model performance based different levels of user engagement history, we further experiment with varying the length of conversations for training. Specifically, in addition to using 75% of conversation history, we also extract the first 25% and 50% of history as training. The rest of a conversation is separated equally for development and test. Figure 4 shows the MAP scores for US Election and TREC datasets. The increasing MAP for all methods as the training history increases indicates that generally, conversation history is essential for recommendation. Our model performs consistently better over different lengths of conversation histories.\nResults for Varying Degree of Data Sparsity. From Table 1 and Figure 3, we observe that most users in our datasets are involved in only a few conversations. In order to study the effects of data sparsity on recommendation models, we examine in Figure 5 the MAP scores for users engaged in a varying number of conversations, as measured on the TREC dataset. The results on the US Election dataset have similar distributions. As we see, the prediction results become worse for users involved in fewer conversations. This indicates that data sparsity serves as a challenge for all recommendation models. We also observe that our model performs consistently better than other models over different degrees of sparsity. This implies that effectively capturing discourse structure in conversation context is useful to mitigating the effects of\ndata sparsity on conversation recommendation."
  }, {
    "heading": "5.2 Case Study and Discussion",
    "text": "Here we present a case study based on the sample conversations in Figure 1. Recall that user U1 is interested in conversations about Sanders, and also prefers more argumentative discourse, and thus returns in conversation c2 but not c1.\nTable 3 shows the predicted scores for the two conversations from OCCF, ADAPTED HFT, and our model (as in Eq. 2). Both ADAPTED HFT and our model more accurately recommend c2 over c1, with our model producing a slightly higher recommendation score for c2.\nTable 4 shows the latent dimension values for the learned topics and discourse modes for this user and these two conversations. Based on human inspection, topic 1 appears to contain words about Sanders, which is the main topic in conversation c2. Topic 2 is about Clinton, which is a dominating topic in conversation c1. Our model also picks up user interest in topic 1 (Sanders), and thus assigns γUu1,1 a high value. For discourse modes, our model also generates a high score for “argument” discourse (labeled via human inspection) for both the user and c2."
  }, {
    "heading": "5.3 Further Analysis of Topic and Discourse",
    "text": "Ablation Study. We have shown that joint modeling of topical content and discourse modes produces the superior performance for our model.\nHere we provide an ablation study to examine the relative contributions of those two aspects by setting the trade-off parameter λ to 1.0 (topic only) or 0.0 (discourse only). Table 5 shows that topics or discourse individually improve slightly upon the comparison ADAPTED HFT, but only jointly do they improve significantly upon it.\nTopic Coherence. To examine the quality of topics found by our model, we use the CV topic coherence score measured via the open-source toolkit Palmetto10, which has been shown to produce evaluation performance comparable to human judgment (Röder et al., 2015). Our model achieves topic coherence scores of 0.343 and 0.376 on TREC and US Election datasets, compared to 0.338 and 0.371 for the topics from ADAPTED HFT.\nSample Discourse Modes. While our topic word distributions are relatively unsurprising, of greater interest are the discourse mode word distributions. Table 6 shows a sample of discourse modes as labeled by human. Although this is merely a qualitative human judgment at this point, there does appear to be a notable overlap in discourse modes between the two datasets even though they were learned separately.\n10https://github.com/AKSW/Palmetto/"
  }, {
    "heading": "5.4 First Time Reply Results",
    "text": "From a recommendation perspective, users may be interested in joining new conversations. We thus compare each recommendation system for first time replies. For each user, we only evaluate for conversations where they are newcomers. Table 7 shows that, unsurprisingly, all systems perform poorly on this task, though our model performs slightly better. This suggests that other features, e.g., network structures or other discussion thread features, could usefully be included in future studies that target new conversations."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper has presented a framework for microblog conversation recommendation via jointly modeling topics and discourse modes. Experimental results show that our method can outperform competitive approaches that omit user discourse behaviors. Qualitative analysis shows that our joint model yields meaningful topics and discourse representations."
  }, {
    "heading": "Acknowledgements",
    "text": "This work is partly supported by Innovation and Technology Fund (ITF) Project No. 6904333, General Research Fund (GRF) Project No. 14232816 (12183516), and National Science Foundation Grant IIS-1566382. We thank Shuming Shi, Yan Song, and the three anonymous reviewers for the insightful suggestions on various aspects of this work."
  }],
  "year": 2018,
  "references": [{
    "title": "The political blogosphere and the 2004 U.S. election: Divided they blog",
    "authors": ["Lada A. Adamic", "Natalie Glance"],
    "venue": "In Proceedings of the 3rd International Workshop on Link Discovery. ACM,",
    "year": 2005
  }, {
    "title": "Finding high-quality content in social media",
    "authors": ["Eugene Agichtein", "Carlos Castillo", "Debora Donato", "Aristides Gionis", "Gilad Mishne."],
    "venue": "Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, pages 183–194.",
    "year": 2008
  }, {
    "title": "Network-aware recommendations of novel tweets",
    "authors": ["Noor Aldeen Alawad", "Aris Anagnostopoulos", "Stefano Leonardi", "Ida Mele", "Fabrizio Silvestri."],
    "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in",
    "year": 2016
  }, {
    "title": "Predicting responses to microblog posts",
    "authors": ["Yoav Artzi", "Patrick Pantel", "Michael Gamon."],
    "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-",
    "year": 2012
  }, {
    "title": "Characterizing and curating conversation threads: expansion, focus, volume, re-entry",
    "authors": ["Lars Backstrom", "Jon Kleinberg", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil."],
    "venue": "Proceedings of the sixth ACM International Conference on Web Search and",
    "year": 2013
  }, {
    "title": "Exposure to ideologically diverse news and opinion on Facebook",
    "authors": ["Eytan Bakshy", "Solomon Messing", "Lada A Adamic."],
    "venue": "Science 348(6239):1130– 1132.",
    "year": 2015
  }, {
    "title": "Latent Dirichlet Allocation",
    "authors": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."],
    "venue": "Journal of machine Learning research 3(Jan):993–1022.",
    "year": 2003
  }, {
    "title": "On participation in group chats on Twitter",
    "authors": ["Ceren Budak", "Rakesh Agrawal."],
    "venue": "Proceedings of the 22nd International Conference on World Wide Web. ACM, pages 165–176.",
    "year": 2013
  }, {
    "title": "Collaborative personalized tweet recommendation",
    "authors": ["Kailong Chen", "Tianqi Chen", "Guoqing Zheng", "Ou Jin", "Enpeng Yao", "Yong Yu."],
    "venue": "Proceedings of the 35th international ACM SIGIR Conference on Research and development in information retrieval.",
    "year": 2012
  }, {
    "title": "Anyone can become a troll: Causes of trolling behavior in online discussions",
    "authors": ["Justin Cheng", "Michael Bernstein", "Cristian DanescuNiculescu-Mizil", "Jure Leskovec."],
    "venue": "Proceedings of the 2017 ACM Conference on Computer Supported Coopera-",
    "year": 2017
  }, {
    "title": "Unsupervised classification of dialogue acts using a Dirichlet process mixture model",
    "authors": ["Nigel Crook", "Ramón Granell", "Stephen G. Pulman."],
    "venue": "Proceedings of The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIG-",
    "year": 2009
  }, {
    "title": "An empirical study on learning to rank of tweets",
    "authors": ["Yajuan Duan", "Long Jiang", "Tao Qin", "Ming Zhou", "Heung-Yeung Shum."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguis-",
    "year": 2010
  }, {
    "title": "Retweet or not?: personalized tweet re-ranking",
    "authors": ["Wei Feng", "Jianyong Wang."],
    "venue": "Proceedings of the sixth ACM International Conference on Web Search and Data Mining. ACM, pages 577–586.",
    "year": 2013
  }, {
    "title": "Part-of-speech tagging for Twitter: Annotation, features, and experiments",
    "authors": ["Kevin Gimpel", "Nathan Schneider", "Brendan O’Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"],
    "year": 2011
  }, {
    "title": "Modeling the structure and evolution of discussion cascades",
    "authors": ["Vicenç Gómez", "Hilbert J Kappen", "Andreas Kaltenbrunner."],
    "venue": "Proceedings of the 22nd ACM Conference on Hypertext and hypermedia. ACM, pages 181–190.",
    "year": 2011
  }, {
    "title": "Gibbs sampling in the generative model of Latent Dirichlet Allocation",
    "authors": ["Tom Griffiths"],
    "year": 2002
  }, {
    "title": "Study on sina microblog personalized recommendation based on semantic network",
    "authors": ["Yue He", "Jinxiu Tan."],
    "venue": "Expert Systems with Applications 42(10):4797–4804.",
    "year": 2015
  }, {
    "title": "Co-factorization machines: modeling user interests and predicting individual decisions in Twitter",
    "authors": ["Liangjie Hong", "Aziz S Doumith", "Brian D Davison."],
    "venue": "Proceedings of the sixth ACM International Conference on Web Search and Data Mining. ACM,",
    "year": 2013
  }, {
    "title": "Collaborative filtering for implicit feedback datasets",
    "authors": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky."],
    "venue": "Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on. IEEE, pages 263–272.",
    "year": 2008
  }, {
    "title": "Cumulated gain-based evaluation of IR techniques",
    "authors": ["Kalervo Järvelin", "Jaana Kekäläinen."],
    "venue": "ACM Trans. Inf. Syst. 20(4):422–446.",
    "year": 2002
  }, {
    "title": "Optimizing search engines using clickthrough data",
    "authors": ["Thorsten Joachims."],
    "venue": "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 133– 142.",
    "year": 2002
  }, {
    "title": "Unsupervised modeling of dialog acts in asynchronous conversations",
    "authors": ["Shafiq R. Joty", "Giuseppe Carenini", "Chin-Yew Lin."],
    "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence, IJCAI 2011. pages 1807–1813.",
    "year": 2011
  }, {
    "title": "Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual",
    "authors": ["D. Jurafsky", "E. Shriberg", "D. Biasca."],
    "venue": "Technical Report Draft 13, University of Colorado, Institute of Cognitive Science.",
    "year": 1997
  }, {
    "title": "Matrix factorization techniques for recommender systems",
    "authors": ["Yehuda Koren", "Robert M. Bell", "Chris Volinsky."],
    "venue": "IEEE Computer 42(8):30–37.",
    "year": 2009
  }, {
    "title": "What is Twitter, a social network or a news media? In Proceedings of the 19th International Conference on World wide Web",
    "authors": ["Haewoon Kwak", "Changhyun Lee", "Hosung Park", "Sue Moon."],
    "venue": "ACM, pages 591–600.",
    "year": 2010
  }, {
    "title": "When the wikipedians talk: Network and tree structure of wikipedia discussion pages",
    "authors": ["David Laniado", "Riccardo Tasso", "Yana Volkovich", "Andreas Kaltenbrunner."],
    "venue": "ICWSM.",
    "year": 2011
  }, {
    "title": "News sharing in social media: The effect of gratifications and prior experience",
    "authors": ["Chei Sian Lee", "Long Ma."],
    "venue": "Computers in Human Behavior 28(2):331–339.",
    "year": 2012
  }, {
    "title": "Amazon",
    "authors": ["Greg Linden", "Brent Smith", "Jeremy York."],
    "venue": "com recommendations: Item-to-item collaborative filtering. IEEE Internet computing 7(1):76–80.",
    "year": 2003
  }, {
    "title": "Conversation trees: A grammar model for topic structure in forums",
    "authors": ["Annie Louis", "Shay B. Cohen."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
    "year": 2015
  }, {
    "title": "Introduction to information retrieval",
    "authors": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze."],
    "venue": "Cambridge University Press.",
    "year": 2008
  }, {
    "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
    "authors": ["Julian McAuley", "Jure Leskovec."],
    "venue": "Proceedings of the 7th ACM Conference on Recommender Systems. ACM, pages 165–172.",
    "year": 2013
  }, {
    "title": "Updating quasi-newton matrices with limited storage",
    "authors": ["Jorge Nocedal."],
    "venue": "Mathematics of computation 35(151):773–782.",
    "year": 1980
  }, {
    "title": "From tweets to polls: Linking text sentiment to public opinion time series",
    "authors": ["Brendan O’Connor", "Ramnath Balasubramanyan", "Bryan R Routledge", "Noah A Smith"],
    "year": 2010
  }, {
    "title": "Improved part-of-speech tagging for online conversational text with word clusters",
    "authors": ["Olutobi Owoputi", "Brendan O’Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"],
    "venue": "In Human Language Technologies:",
    "year": 2013
  }, {
    "title": "One-class collaborative filtering",
    "authors": ["Rong Pan", "Yunhong Zhou", "Bin Cao", "Nathan Nan Liu", "Rajan M. Lukose", "Martin Scholz", "Qiang Yang."],
    "venue": "Proceedings of the 8th IEEE International Conference on Data Mining. pages 502–511.",
    "year": 2008
  }, {
    "title": "Diffusion-aware personalized social update recommendation",
    "authors": ["Ye Pan", "Feng Cong", "Kailong Chen", "Yong Yu."],
    "venue": "Proceedings of the 7th ACM Conference on Recommender Systems. ACM, pages 69–76.",
    "year": 2013
  }, {
    "title": "Short and sparse text topic modeling via self-aggregation",
    "authors": ["Xiaojun Quan", "Chunyu Kit", "Yong Ge", "Sinno Jialin Pan."],
    "venue": "Proceedings of the TwentyFourth International Joint Conference on Artificial Intelligence. pages 2270–2276.",
    "year": 2015
  }, {
    "title": "Unsupervised modeling of Twitter conversations",
    "authors": ["Alan Ritter", "Colin Cherry", "Bill Dolan."],
    "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics. pages 172–180.",
    "year": 2010
  }, {
    "title": "Exploring the space of topic coherence measures",
    "authors": ["Michael Röder", "Andreas Both", "Alexander Hinneburg."],
    "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining. pages 399–408.",
    "year": 2015
  }, {
    "title": "Bayesian probabilistic matrix factorization using markov chain monte carlo",
    "authors": ["Ruslan Salakhutdinov", "Andriy Mnih."],
    "venue": "Proceedings of the 25th International Conference on Machine learning. ACM, pages 880–887.",
    "year": 2008
  }, {
    "title": "Dialogue act modeling for automatic tagging and recognition",
    "authors": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer"],
    "year": 2000
  }, {
    "title": "Collaborative personalized Twitter search with topic-language models",
    "authors": ["Jan Vosecky", "Kenneth Wai-Ting Leung", "Wilfred Ng."],
    "venue": "Proceedings of the 37th international ACM SIGIR Conference on Research & Development in Information Retrieval.",
    "year": 2014
  }, {
    "title": "Collaborative topic modeling for recommending scientific articles",
    "authors": ["Chong Wang", "David M Blei."],
    "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, pages 448–456.",
    "year": 2011
  }, {
    "title": "Inferring linguistic structure in spoken language",
    "authors": ["M Woszczyna", "A Waibel."],
    "venue": "Proceedings of ICSLP. IC-SLP.",
    "year": 1994
  }, {
    "title": "Who says what to whom on Twitter",
    "authors": ["Shaomei Wu", "Jake M Hofman", "Winter A Mason", "Duncan J Watts."],
    "venue": "Proceedings of the 20th International Conference on World Wide Web. ACM, pages 705– 714.",
    "year": 2011
  }, {
    "title": "Tweet recommendation with graph co-ranking",
    "authors": ["Rui Yan", "Mirella Lapata", "Xiaoming Li."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational",
    "year": 2012
  }, {
    "title": "User embedding for scholarly microblog recommendation",
    "authors": ["Yang Yu", "Xiaojun Wan", "Xinjie Zhou."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. volume 2, pages 449–453.",
    "year": 2016
  }],
  "id": "SP:b2f9341b885ceb036857db9b2038b944074376b1",
  "authors": [{
    "name": "Xingshan Zeng",
    "affiliations": []
  }, {
    "name": "Jing Li",
    "affiliations": []
  }, {
    "name": "Lu Wang",
    "affiliations": []
  }, {
    "name": "Nicholas Beauchamp",
    "affiliations": []
  }, {
    "name": "Sarah Shugars",
    "affiliations": []
  }, {
    "name": "Kam-Fai Wong",
    "affiliations": []
  }],
  "abstractText": "Millions of conversations are generated every day on social media platforms. With limited attention, it is challenging for users to select which discussions they would like to participate in. Here we propose a new method for microblog conversation recommendation. While much prior work has focused on postlevel recommendation, we exploit both the conversational context, and user content and behavior preferences. We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics. Experimental results on two Twitter datasets demonstrate that our system outperforms methods that only model content without considering discourse.",
  "title": "Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse"
}