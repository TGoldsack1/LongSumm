{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1183–1191 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1183"
  }, {
    "heading": "1 Introduction",
    "text": "Language models (LMs) estimate the likelihood of a symbol sequence {xt}Tt=0, based on the joint probability,\np(x0, . . . , xT ) = p(x0) T∏ t=1 p(xt|x0, . . . , xt−1).\n(1)\nTo measure the quality of an LM, a commonly adopted metric is perplexity (PPL), defined as\nPPL , exp { − 1 T T∑ t=0 log p(xt|x0, . . . , xt−1) } ,\nA good language model has a small PPL, being able to assign higher likelihoods to sentences that are more likely to appear.\nLMs are widely applied in automatic speech recognition (ASR) (Yu and Deng, 2014) and machine translation (MT) (Koehn, 2009). Following Koehn (2009), one may interpret the language\n∗Contributions were made while at Baidu Research.\nmodel as prior knowledge on the text to be inferred, which provides information complementary to the ASR or MT system itself. In practice, there are several ways to incorporate the language model. The simplest way may be re-scoring an n-best list returned by the ASR or MT system (Mikolov et al., 2010; Sundermeyer et al., 2012). A slightly more sophisticated way is to jointly consider the ASR/MT and language model in a beam search decoder (Amodei et al., 2016). Specifically, at each time step, the decoder appends every symbol in the vocabulary to each sequence in the current candidate set. For every hypothesis, a score is calculated as a linear combination of the log-likelihoods given by both the ASR/MT and language models. Then, only the top K hypotheses with the highest scores are retained, as an updated candidate set. More recently, Gulcehre et al. (2015) and Sriram et al. (2017) propose to predict the next symbol based on a fusion of the hidden states in the ASR/MT and language models. A gating mechanism is jointly trained to determine how much the language model should contribute.\nThe afore-discussed language models are generative in the sense that they merely model the joint distribution of a symbol sequence (Eq. (1)). While the research community is mostly focused on pushing the limit of PPL (e.g., Jozefowicz et al., 2016), very limited attention has been paid to the discrimination power of language models when they are applied to real tasks, such as ASR and MT (Li and Khudanpur, 2008). By contrast, discriminative language modeling aims at enhancing the performance in downstream applications. For example, existing works (Roark et al., 2004, 2007) often target at improving ASR accuracy. The key motivation underlying them is that the model should be able to discriminate between “good” and “bad” sentences in a task-specific sense, instead\nof just modeling grammatical ones. The common methodology (Dikic et al., 2013) is to build a binary classifier upon hand-crafted features extracted from the sentences. However, it is not obvious how these methods can utilize large unannotated corpus, which is often easily available, and the hand-crafted features are also ad hoc and may result in suboptimal performance.\nIn this work, we study how to improve the discrimination ability of a recurrent network-based neural language model (RNNLM). The goal is to enlarge the difference between the log-likelihoods of “good” and “bad” sentences. In an contrast to the existing works (Roark et al., 2004, 2007), our method does not rely on hand-crafted features, and is trained in end-to-end manner and able to take advantage of large external text corpus. In fact, it is a general training criterion that is transparent to the network architecture of the RNNLM, and can be applied to various text generation tasks, including ASR and MT. Experiments on state-of-art ASR and MT systems show its significant advantage over an LM trained by minimizing PPL."
  }, {
    "heading": "2 Background on RNNLM",
    "text": "We first give some background knowledge on RNNLMs. The prototypical RNNLM (Mikolov et al., 2010) has one layer of recurrent cell and works as follows. Denote a sentence as x = [x0, . . . , xt, . . . ], where the xt’s are words. Let ~xt be the embedding vector for xt. The recurrent cell takes in the embedding and produces a hidden state ~ht by\n~ht = σ(U~xt + V~ht−1),\nwhere σ(z) = 1 1+e−z is sigmoid activation function. ~ht−1 is the hidden state at the last timestep. U and V are learnable parameters. The ~ht is then passed into a multi-way classifier to produce a probability distribution over the vocabulary (for the next word),\n~p = softmax(W~ht +~b).\nThe W and ~b are also trainable parameters. The training objective is to maximize the loglikelihood of the next word, and the parameters are learned by back-propagation algorithm.\nThe vanilla recurrent cell can also be replaced by one or multiple layers of LSTM cells, which produces better results (Zaremba et al.,\n2014). In a more general form, the RNNLM can be represented as a conditional probability, pθ(x\nt|x0, . . . , xt−1), parameterized by θ. In the prototypical case, θ = [U, V,W,~b]. We could define the LM-score of a sentence x as\nLM-score(x) , log pθ(x) = ∑ t log pθ(x t|x0, . . . , xt−1).\nThe RNNLM is trained by maximizing the average LM-score over all the x’s in a corpus, or equivalently, minimizing the PPL on the corpus."
  }, {
    "heading": "3 Problem Formulation",
    "text": "We motivate and formulate a large margin training criterion in this section. Suppose for every reference sentence xi, we have a collection of hypotheses xi,j , j = 1, . . . ,K, usually obtained as the top-K candidates by a beam search decoder."
  }, {
    "heading": "3.1 A Motivating Example",
    "text": "An RNNLM trained by minimizing PPL cannot guarantee a higher score on the “gold” reference than the inferior hypothesis, which is undesirable. One example is given in Tab. 1. The reference is taken from the text labels of dev93’ set of Wall Street Journal (WSJ) dataset. The hypothesis is generated by a CTC-based (Graves et al., 2006) ASR system trained on WSJ training set. Words in red are mistakes made by the hypothesis. We then train an RNNLM on Common Crawl1 copora by minimizing PPL. Training follows a typical setup (Jozefowicz et al., 2016) with a vocabulary of 400K the most frequent words. Any out-ofvocabulary word is replaced by an 〈UNK〉 token. The RNNLM is then employed to score the sentences. The LM-score of the erroneous hypothesis is higher than that of the reference. In fact, this is reasonable as “a decade as concerns” seems to be a more common phrase. In the training corpus, we find that “a decade as concerns” appears once, but “its defeat is confirmed” does not appear. Moreover, “a decade as” appears 2,280 times, but “its defeat is” appears only 24 times. However, this is undesirable because if there is another hypothesis that happens to be the same as reference, which will not be ranked as the best candidate.\nIt would be helpful if the LM can also learn from the imperfect hypotheses so that it can tell\n1http://web-language-models. s3-website-us-east-1.amazonaws.com/ wmt16/deduped/en-new.xz\napart “good” and “bad” candidates. With this motivation, we train to assign larger LM-scores for the xi’s but smaller ones for the (imperfect) xi,j’s. A quantity of particular interest is log p(xi) − log p(xi,j), the margin/difference between the LM-scores of the references and the (imperfect) hypotheses. The intuition is that the more positive the margin, the better the LM is at discrimination."
  }, {
    "heading": "3.2 Straightforward but Failed Formulation",
    "text": "Without loss of generality, we assume that all the xi,j’s are imperfect and different from xi. A straightforward way is to adopt the following objective:\nmin θ\n1\nN N∑ i=1 − log pθ(xi) + 1 K K∑ j=1 log pθ(xi,j)  . (2)\nSimilar formulation is also seen in (Tachioka and Watanabe, 2015), where they only utilize one beam candidate, i.e., K = 1. Optimization can be carried out by mini-batch stochastic gradient descent (SGD). Each iteration, SGD randomly samples a batch of i’s and j’s, computes stochastic gradient w.r.t. θ, and takes an update step. However, a potential problem with this formulation is that the second term (corresponding to the inferior hypotheses) may dominate the optimization. Specifically, the training is almost always driven by the xi,j’s, but does not effectively enhance the discrimination. We illustrate this fact in the following experiment.\nUsing the ASR system in section 3.1, we extract 256 beam candidates for every training example in Wall Street Journal (WSJ) dataset. Warm started from the pre-trained RNNLM in section 3.1, we apply SGD to minimize the loss in Eq. (2), with a mini-batch size of 128. The training loss is shown in Fig. 1a. We observe that the learning dynamic is very unstable, and deceases to be negative. The unbound decreasing is due to the second term in Eq. (2) being negative and dominating the training process. Next, we inspect log pθ(xi) − log pθ(xi,j), the margin between the scores of a ground-truth and a candidate. In Fig. 2a, we histogram the margins for all the i, j’s in a dev set. The distribution appears to be symmetric around zero, which indicates poor discrimination ability. Given these facts, we conclude that the straightforward formulation in Eq. (2) is not effective."
  }, {
    "heading": "3.3 Large Margin Formulation",
    "text": "To effectively utilize all the imperfect beam candidates, we propose the following objective,\nmin θ N∑ i=1 B∑ j=1 max { 0, τ−(log pθ(xi)−log pθ(xi,j)) } , (3) where log pθ(xi) − logθ(xi,j) is the margin between the scores of a ground-truth xi and a can-\ndidate xi,j . The hinge loss on the margin encourages the log-likelihood of the ground-truth to be at least τ larger than that of the imperfect hypothesis. We call an LM trained by the above formulation as Large Margin Language Model (LMLM).\nWe repeat the same experiment in section 3.2, but change the objective function to Eq. (3) and set τ = 1. Fig. 1b shows the training loss, which steadily decreases and approaches zero rapidly. Compared with the learning curve of naive formulation (Fig. 1a), the large margin based training is much more stable. In Fig. 2b, we also examine the histogram of log pθ(xi) − log pθ(xi,j), where pθ(·) is now the LM learned by LMLM. Compared with the histogram by the conventional RNNLM, LMLM significantly moves the distribution to the positive side, indicating more discrimination."
  }, {
    "heading": "3.4 Ranking Loss Type Formulation",
    "text": "In most cases, all beam candidates are imperfect. It may be beneficial to exploit the information that some candidates are relatively better than the others. We consider ranking them according to some metrics w.r.t. the ground-truth sentences. For ASR, the metric is WER, and for MT, the metric is BLEU score. We define xi,0 , xi and assume that the candidates {xi,j}Kj=1 are sorted such that\nWER(xi,xi,j−1) < WER(xi,xi,j)\nfor ASR, and\nBLEU(xi,xi,j−1) > BLEU(xi,xi,j)\nfor MT. In other words, xi,j−1 has better quality than xi,j .\nWe then enforce the “better” sentences to have a score at least τ larger than those “worse” ones. This leads to the following formulation,\nmin θ N∑ i=1 B−1∑ j=0 B∑ k=j+1 max { 0,\nτ − (log pθ(xi,j)− logθ(xi,k)), } . (4)\nCompared with LMLM formulation Eq. (3), the above introduces more comparisons among the candidates, and hence more computational cost during training. We call this formulation rankingloss-based LMLM (rLMLM).\nTo summarize this section, we have proposed LMLM and rLMLM that aim at discriminating between hypotheses in a task-specific (e.g., WER or BLEU) sense, instead of minimizing PPL."
  }, {
    "heading": "4 Experiments on ASR",
    "text": "We apply the LMs trained under different criteria to rescore the beams in various ASR systems. In particular, we are interested in knowing which of the two training mechanisms is better: minimizing PPL (e.g., the RNNLM in Section 3.1), or fitting to the WER metric by the proposed methods.\nAdapting an RNNLM to a specific domain has been of interest, especially to the speech community (Park et al., 2010; Chen et al., 2015; Ma et al., 2017). We adopt Ma et al. (2017) that fine-tune the softmax layer of RNNLM by minimizing the PPL on the text labels of training set. According to Ma et al. (2017), the reason not to fine-tune all the layers is due to the limited text labels in the target domain. Indeed, we also observe overfitting if adapting all layers, but adapting only the softmax layer effectively decreases the PPL on the text labels of dev sets. We refer to this fine-tuning as RNNLM-adapted in the following sections.\nTo make a fair comparison with the adapted model, we also use the RNNLM as an initialization for our LMLM and rLMLM. In total, there are four language models for rescoring the beams. RNNLM and its adapted version that aim at reducing PPL; and the two proposed methods, LMLM and rLMLM that try to fit to WER."
  }, {
    "heading": "4.1 WSJ Dataset",
    "text": "The WSJ corpora consists of about 80 hours of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news. We use the standard configuration of train si284 dataset for training, dev93 for development and eval92 for testing.\nOur ASR model has one convolution layer, followed by 5 bidirectional RNNs and one fully connected layer, with a CTC loss on top. The text labels of the training set are used to train a 4-gram language model, which is employed in the ASR decoder. The beam search decoder has a beam width of 2000. Before beam rescoring, this ASR system achieves a WER of 12.16 on dev93 set and 7.69 on eval92 set. To put this into perspective, we list some previous state-of-the-art system in Tab. 2. Compared with them, our baseline is already very competitive."
  }, {
    "heading": "4.1.1 WERs and PPLs",
    "text": "The out-of-vocabulary rate of WSJ text is only 0.28%, making the RNNLM reasonable to use.\nWe apply the RNNLM, RNNLM-adapted (Ma et al., 2017), LMLM and rLMLM to rescore the beams on dev and test set. The final score assigned to a beam is a weighted sum of the ASR and language model scores. The weight is found by minimizing the WER on the dev set.\nTab. 3 reports the WERs on dev93 and eval92 sets. All methods reduce the WER over the baseline without rescoring. However, LMLM and rLMLM are notably better than the other two methods. Moreover, although RNNLM and RNNLM-adapted achieve smaller PPLs on the text labels, the advantage does not transfer to WER."
  }, {
    "heading": "4.1.2 Correlation between scores and WERs",
    "text": "To better understand the proposed methods, we calculate the correlation coefficients between the hypotheses’ WERs and their scores (by different language models). In specific, for every utterance in the test set, we have a set of beam candidates, their word level accuracies (100-WER) and scores given by an LM, from which a Pearson correlation coefficient can be calculated. We calculate the coefficients for all the utterances in the test set, and boxplot these coefficients in Fig. 3. The correlation coefficients by LMLM and rLMLM tend\nto be higher than RNNLM and RNNLM-adapted. This indicates that LMLM and rLMLM are more aligned with the goal of reducing WER."
  }, {
    "heading": "4.1.3 Case Study",
    "text": "Tab. 4 posts some examples from the test set. The first column lists the ground-truth labels, and their corresponding best candidates as re-ranked by the four LMs (see notes in the second column). Words in red are mistakes made by the candidate sentences. Scores of these sentences are listed in the last four columns. We have the following observations:\n1. LMLM and rLMLM give worse scores on the ground-truth labels than RNNLM and RNNLM-adapted, which explains their higher PPL in Tab. 3.\n2. In the first example, RNNLM and RNNLMadapted assign higher scores to a shorter sentence. This is reasonable (though not necessarily desirable) as LM-score is a summation of log-probabilities, each of which is negative. In contrast, LMLM and rLMLM are able to assign higher scores to longer and better candidates.\n3. In the other two examples, LMLM and rLMLM seem to favor more sensible sentences, though they are not more grammatical than those picked by RNNLM and RNNLMadapted. We conjecture that since LMLM and rLMLM utilize beam candidates in their training, they capture and compensate for\nsome weakness in the ASR, which is not achieved by RNNLM and RNNLM-adapted."
  }, {
    "heading": "4.2 10K Speech Dataset",
    "text": "We further validate our methods on a larger noisy dataset collected by Liu et al. (2017). The dataset has about 10K hours of spontaneous speech. The utterances are corrupted by background noise, and a large portion of them are accented. Therefore it is much more challenging than WSJ. We adopt the same training-dev-test split as in Liu et al. (2017). In specific, there are 5.4M utterances for training, 2,066 for development and 2,054 for testing.\nThe ASR we build has the same architecture as in Liu et al. (2017), except that its decoder integrates an in-domain 5-gram language model. This system achieves a WER of 19.17 on dev set, better than the reported 19.77 baseline in Liu et al. (2017). Based on the ASR, we repeat the same experiments in section 4.1. Tab. 5 reports WERs and PPLs on dev and test sets. Both LMLM and rLMLM outperform the other methods in WER, although their PPLs are higher. This trend is similar to that in Tab. 3."
  }, {
    "heading": "5 Experiments on NMT",
    "text": "In this section, we experiment the large-margin criterion trained LM with a competitive Chineseto-English NMT system. The NMT model is trained from 2M parallel sentence pairs. Following Shen et al. (2016), we use NIST 06 newswire portion (616 sentences) for development and NIST 08 newswire portion (691 sentences) for testing. We use OpenNMT-py2 package with the default configuration to train the model: batch size is 64; word embedding size is 500; dropout rate is 0.3; target vocabulary size is 50K; number of epochs is 20, after which a minimum dev perplexity of 7.72\n2https://github.com/OpenNMT/OpenNMT-py\nis achieved."
  }, {
    "heading": "5.1 BLEUs and PPLs",
    "text": "We use a beam size of 10 for decoding, and report case-insensitive 4-reference BLEU-4 scores (by calling “multi bleu.perl”3). The NMT model achieves 35.18 BLEU score on dev set and 31.52 on test set (see table 6). To put this into perspective, Shen et al. (2016) trains their models on 2.56M pairs of sentences and reports a dev BLEU score of 32.7 (via MOSES) or 30.7 (via RNNsearch, beam size of 10). So our NMT model is already very competitive.\nTo construct the training data for LMLM and rLMLM, 10 beam candidates are extracted for every sentence in the training set. We then follow the same experimental steps outlined in section 4.1, except that the ASR score is now changed to NMT score. In addition, we also find that normalizing the LM score by sentence length can improve the re-scoring performance substantially. Tab. 6 compares the BLEU score after re-ranking by the different LMs. LMLM and rLMLM both improve upon the baseline significantly, and outperform RNNLM and RNNLM-adapted by a notable margin. We also observe that the PPLs of LMLM and rLMLM are much larger than those of RNNLM and RNNLM-adapted, suggesting that the PPL metric may be very poorly correlated with BLEU.\nInterestingly, RNNLM-adapted does not show any gain in BLEU score over RNNLM. To understand this, we recall that NMT is trained by minimizing PPL on target text. Its decoder is implicitly an RNNLM on target language. We conjecture that adapting an LM to the target domain can only duplicate the functionality of the NMT decoder, which does not bring any additional benefit."
  }, {
    "heading": "5.2 Correlation between scores and BLEUs",
    "text": "We measure the correlation between the LM scores and BLUEs. The calculation is done on dev06 set in the same way as Section 4.1.2, but now we change the WERs to BLEUs. The boxplot of the correlation coefficients are shown in Fig. 4. Compared with the boxplot in Fig. 3, now the correlation coefficients by all LMs are more dispersed. Sometimes, they even take negative values. The mean correlation by LMLM\n3https://github.com/OpenNMT/ OpenNMT-py/blob/master/tools/multi-bleu. perl\nand rLMLM, however, is considerably higher than those by RNNLM and RNNLM-adapted."
  }, {
    "heading": "6 Related Work",
    "text": "“Language modeling is an art of determining the probability of a sequence of words” (Goodman, 2001). In the past decades, there has been a trend of increasing the context that an LM can condition on. N-gram models (Chen and Goodman, 1996) assume that each symbol depends on the previous N − 1 symbols. Feed forward neural network based LMs (Bengio et al., 2003) are not count based but they inherit the restrictive assumption. To model longer-term dependencies, RNNLMs (Mikolov et al., 2010) are proposed. RNNLMs often achieve smaller PPLs than the N-gram counterparts (Sundermeyer et al., 2012; Zaremba et al., 2014; Jozefowicz et al., 2016). This paper focuses on RNNLM-type architectures.\nWhile these works all adopt PPL as the metric\nto optimize, sometimes one may optimize a taskspecific objective. For example, Kuo et al. (2002); Roark et al. (2007) and Dikic et al. (2013) propose discriminative LMs to improve speech recognition. The common methodology therein is to fit a probabilistic model, e.g., conditional random field (Roark et al., 2004), to the space of text candidates, and maximize the probability at the desired candidate. The problem is often solved by perceptron algorithm. However, these methods all rely on ad-hoc choice of features, e.g., counts of n-grams where n varies in a small range (e.g.,1 to 3). Moreover, it is also not clear how these methods would take advantage of an existing language model (trained on large unsupervised corpus). Nevertheless, the same methodology can be extended to RNNLMs, thus avoiding the aforementioned limitations. For example, Auli and Gao (2014) train an RNNLM by favoring sentences with high BLEU scores and integrate it into a phrase-based MT decoder.\nIf we cast the problem of picking the best text sequence as a ranking problem, the aforementioned works can be considered as “pointwise” learning-to-rank approaches (Cossock and Zhang, 2008). In contrast, the proposed method is a “pairwise” approach (Liu, 2009), as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to (Collins and Koo, 2005), which improves a semantic parser. Learning “by pairwise comparison” is also seen in several MT literatures. For example, Hopkins and May (2011) propose to train a phrase-based MT system by minimizing a pairwise ranking loss. Wiseman and Rush (2016) optimize the beam search process in a Neural Machine Translation (NMT) system. They enforce the score of a reference to be higher than that of its decoded k-th candidate by at least a unit margin.\nRather than optimizing the MT system itself, this work proposes a general method of training recurrent neural language models, which can benefit various text generation tasks, including speech recognition and machine translation."
  }, {
    "heading": "7 Conclusions",
    "text": "We have proposed a large margin criterion for training recurrent neural language models. Rather than minimizing PPL, the proposed criterion is based on comparison between pairs of sen-\ntences. We have formulated two algorithms that implement the training criterion. One compares between references and imperfect hypotheses (LMLM), the other compares between all pairs of hypotheses (rLMLM). We applied the language models trained by these two algorithms to speech recognition and machine translation. Both of them demonstrate superior performance over their minimum-PPL counterparts. However, the performance gain from LMLM to rLMLM is small, although rLMLM is built on more pairwise comparisons and requires more training efforts. The efficiency with respect to the number of pairs is a future research topic."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep speech 2: Endto-end speech recognition in english and mandarin",
    "authors": ["Dario Amodei", "Sundaram Ananthanarayanan", "Rishita Anubhai", "Jingliang Bai", "Eric Battenberg", "Carl Case", "Jared Casper"],
    "venue": "International Conference on Machine Learning.",
    "year": 2016
  }, {
    "title": "Decoder integration and expected bleu training for recurrent neural network language models",
    "authors": ["Michael Auli", "Jianfeng Gao."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Endto-end attention-based large vocabulary speech recognition",
    "authors": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio."],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing.",
    "year": 2016
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research, 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "An empirical study of smoothing techniques for language modeling",
    "authors": ["Stanley F. Chen", "Joshua Goodman."],
    "venue": "Computer Speech & Language, 13(4):359–394.",
    "year": 1996
  }, {
    "title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition",
    "authors": ["Xie Chen", "Tian Tan", "Xunying Liu", "Pierre Lanchantin", "Moquan Wan", "Mark J.F. Gales", "Philip C. Woodland."],
    "venue": "16th Annual Conference of the In-",
    "year": 2015
  }, {
    "title": "Discriminative reranking for natural language parsing",
    "authors": ["Michael Collins", "Terry Koo."],
    "venue": "Computational Linguistics, 31(1):25–70.",
    "year": 2005
  }, {
    "title": "Statistical analysis of bayes optimal subset ranking",
    "authors": ["David Cossock", "Tong Zhang."],
    "venue": "IEEE Transactions on Information Theory, 54(11):5140–5154.",
    "year": 2008
  }, {
    "title": "Classification and ranking approaches to discriminative language modeling",
    "authors": ["Erinç Dikic", "Murat Semerci", "Murat Saraçlar", "Ethem Alpaydin"],
    "year": 2013
  }, {
    "title": "A bit of progress in language modeling",
    "authors": ["Joshua T. Goodman."],
    "venue": "Computer Speech & Language, 15(4):403–434.",
    "year": 2001
  }, {
    "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
    "authors": ["Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber."],
    "venue": "In Proceedings of the 23rd international conference on",
    "year": 2006
  }, {
    "title": "On using monolingual corpora in neural machine translation",
    "authors": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1503.03535.",
    "year": 2015
  }, {
    "title": "Tuning as ranking",
    "authors": ["Mark Hopkins", "Jonathan May."],
    "venue": "EMNLP.",
    "year": 2011
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."],
    "venue": "arXiv preprint arXiv:1602.02410.",
    "year": 2016
  }, {
    "title": "Statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Cambridge University Press.",
    "year": 2009
  }, {
    "title": "Discriminative training of language models for speech recognition in acoustics",
    "authors": ["Hong-Kwang Jeff Kuo", "Eric Fosler-Lussier", "Hui Jiang", "Chin-Hui Lee."],
    "venue": "IEEE International Conference on Speech, and Signal Processing (ICASSP).",
    "year": 2002
  }, {
    "title": "Large-scale discriminative n-gram language models for statistical machine translation",
    "authors": ["Zhifei Li", "Sanjeev Khudanpur."],
    "venue": "AMTA.",
    "year": 2008
  }, {
    "title": "Gram-ctc: Automatic unit selection and target decomposition for sequence labelling",
    "authors": ["Hairong Liu", "Zhenyao Zhu", "Xiangang Li", "Sanjeev Satheesh."],
    "venue": "34th International Conference on Machine Learning.",
    "year": 2017
  }, {
    "title": "Learning to rank for information retrieval, volume 3",
    "authors": ["Tie-Yan Liu."],
    "venue": "Foundations and Trends R",
    "year": 2009
  }, {
    "title": "Approaches for neural-network language model adaptation",
    "authors": ["Min Ma", "Michael Nirschl", "Fadi Biadsy", "Shankar Kumar."],
    "venue": "Proceedings of Interspeech.",
    "year": 2017
  }, {
    "title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding, pages 167–174",
    "authors": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze."],
    "venue": "ieee, 2015. In Workshop on Automatic Speech Recognition and Understanding",
    "year": 2015
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Černocký", "Sanjeev Khudanpur."],
    "venue": "11th Annual Conference of the International Speech Communication Association.",
    "year": 2010
  }, {
    "title": "Improved neural network based language modelling and adaptation",
    "authors": ["Junho Park", "Xunying Liu", "Mark J.F. Gales", "Phil C. Woodland."],
    "venue": "11th Annual Conference of the International Speech Communication Association.",
    "year": 2010
  }, {
    "title": "Discriminative n-gram language modeling",
    "authors": ["Brian Roark", "Murat Saraclar", "Michael Collins."],
    "venue": "Computer Speech & Language, 21(2):373–392.",
    "year": 2007
  }, {
    "title": "Discriminative language modeling with conditional random fields and the perceptron algorithm",
    "authors": ["Brian Roark", "Murat Saraclar", "Michael Collins Mark Johnson."],
    "venue": "42nd Annual Meeting on Association for Computational",
    "year": 2004
  }, {
    "title": "Minimum risk training for neural machine translation",
    "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Cold fusion: Training seq2seq models together with language models",
    "authors": ["Anuroop Sriram", "Heewoo Jun", "Sanjeev Satheesh", "Adam Coates."],
    "venue": "arXiv preprint arXiv:1708.06426.",
    "year": 2017
  }, {
    "title": "Lstm neural networks for language modeling",
    "authors": ["Martin Sundermeyer", "Ralf Schlüter", "Hermann Ney."],
    "venue": "13th Annual Conference of the International Speech Communication Association.",
    "year": 2012
  }, {
    "title": "Discriminative method for recurrent neural network language models",
    "authors": ["Yuuki Tachioka", "Shinji Watanabe."],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "year": 2015
  }, {
    "title": "Sequence-to-sequence learning as beam-search optimization",
    "authors": ["Sam Wiseman", "Alexander M. Rush."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Automatic Speech Recognition: A Deep Learning Approach",
    "authors": ["Dong Yu", "Li Deng."],
    "venue": "Springer Publish-ing Company.",
    "year": 2014
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."],
    "venue": "arXiv preprint arXiv:1409.2329.",
    "year": 2014
  }],
  "id": "SP:ac5925e0be9695c70e01aa87aa7cf49d2a7f3db4",
  "authors": [{
    "name": "Jiaji Huang",
    "affiliations": []
  }, {
    "name": "Yi Li",
    "affiliations": []
  }, {
    "name": "Wei Ping",
    "affiliations": []
  }, {
    "name": "Liang Huang",
    "affiliations": []
  }],
  "abstractText": "We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the “good” and “bad” sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.",
  "title": "Large Margin Neural Language Model"
}