{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2214–2224 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2214"
  }, {
    "heading": "1 Introduction",
    "text": "Intelligent personal digital assistants (IPDAs) are one of the most advanced and successful artificial intelligence applications that have spoken language understanding (SLU). Many IPDAs have recently emerged in industry including Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana (Sarikaya, 2017). IPDAs have traditionally supported only dozens of well-separated domains, each defined in terms of a specific ap-\nplication or functionality such as calendar and local search (Tur and de Mori, 2011; Sarikaya et al., 2016). To rapidly increase domain coverage and extend capabilities, some IPDAs have released Software Development Toolkits (SDKs) to allow third-party developers to quickly build and integrate new domains, which we refer to as skills henceforth. Amazon’s Alexa Skills Kit (Kumar et al., 2017a), Google’s Actions and Microsoft’s Cortana Skills Kit are all examples of such SDKs. Alexa Skills Kit is the largest of these services with over 40,000 skills.\nFor IPDAs, finding the most relevant skill to handle an utterance is an open problem for three reasons. First, the sheer number of skills makes the task difficult. Unlike traditional systems that have on the order of 10-20 built-in domains, largescale IPDAs can have up to 40,000 skills. Second, the number of skills is rapidly expanding with 100+ new skills added per week. Largescale IPDAs should be able to accommodate new skills efficiently without compromising performance. Third, unlike traditional built-in domains that are carefully designed to be disjoint by a central team, skills are built independently by different developers and can cover overlapping functionalities. For instance, there are over 50 recipe skills in Alexa that can handle recipe-related utterances.\nOne simple solution to this problem has been to require the user to explicitly mention a skill name and follow a strict invocation pattern as in ”Ask {Uber} to {get me a ride}.” However, this significantly limits the natural interaction with IPDAs. Users have to remember skill names and invocation patterns, and it places a cognitive burden on users who tend to forget both. Skill discovery is difficult with a pure voice user interface, it is hard for users to know the capabilities of thousands of skills a priori, which may leads to limited user en-\ngagement with skills and potentially with IPDAs. In this paper, we propose a solution that addresses all three practical challenges without requiring skill names or invocation patterns. Our approach is based on a scalable neural model architecture with a shared encoder, a skill attention mechanism and skill-specific classification networks that can efficiently perform large-scale skill classification in IPDAs using a weakly supervised training dataset. We demonstrate that our model achieves a high accuracy on a manually transcribed test set after being trained with weak supervision. Moreover, our architecture is designed to efficiently integrate new skills that appear in-between full model retraining cycles into the model. Besides accuracy, we also keep practical constraints in mind and focus on minimizing memory footprint and runtime latency, while ensuring architecture is scalable to thousands of skills, all of which are important for real-time production systems. Furthermore, we investigate two different ways of incorporating user personalization information into the model, our naive baseline method adds the information as a 1-bit flag in the feature space of the skill-specific networks, the personalized attention technique computes a convex combination of skill embeddings for the user’s enabled skills and significantly outperforms the naive personalization baseline. We show the effectiveness of our approach with extensive experiments using 1,500 skills from a deployed IPDA system."
  }, {
    "heading": "2 Related Work",
    "text": "Traditional multi-domain SLU/NLU systems are designed hierarchically, starting with domain classification to classify an incoming utterance into one of many possible domains, followed by further semantic analysis with domain-specific intent classification and slot tagging (Tur and de Mori, 2011). Traditional systems have typically been limited to a small number of domains, designed by specialists to be well-separable. Therefore, domain classification has been considered a less complex task compared to other semantic analysis such as intent and slot predictions. Traditional domain classifiers are built using simple linear models such as Multinomial Logistic Regression or Support Vector Machines in a one-versusall setting for multi-class prediction. The models typically use word n-gram features and also those\nbased on static lexicon match, and there have been several recent studies applying deep learning techniques (Xu and Sarikaya, 2014).\nThere is also a line of prior work on enhancing sequential text classification or tagging. Hierarchical character-to-word level LSTM (Hochreiter and Schmidhuber, 1997) architectures similar to our models have been explored for the Named Entity Recognition task by Lample et al. (2016). Character-informed sequence models have also been explored for simple text classification with small sets of classes by Xiao and Cho (2016). Joulin et al. (2016) explored highly scalable text classification using a shared hierarchical encoder, but their hierarchical softmax-based output formulation is unsuitable for incremental model updates. Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a)."
  }, {
    "heading": "3 Weakly Supervised Training Data Generation",
    "text": "Our model addresses the domain classification task in SLU systems. In traditional IPDA systems, these domains are hand-crafted by experts to be well separable and can easily be annotated by humans because they are small in number. The emergence of self-service SLU results in a large number of potentially mutually overlapping SLU domains. This means that eliciting large volumes of high quality human annotations to train our model\nis no longer feasible, and we cannot assume that domains are designed to be well separable.\nInstead we can generate training data by adopting the weak supervision paradigm introduced by (Hoffmann et al., 2011), which proposes using heuristic labeling functions generate large numbers of noisy data samples. Clean data generation with weak supervision is a challenging problem, so we address it by decomposing it into two simpler problems, of candidate generation and noise suppression, however it remains important for our model to be noise robust."
  }, {
    "heading": "3.1 Data Programming",
    "text": "The key insight of the Data Programming approach is that O(1) simple labeling functions can be used to approximate O(n) human annotated data points with much less effort. We adopt the formalism used by (Ratner et al., 2016) to treat each of instance data generation rule as a rich generative model, defined by a labeling function λ and describe different families of labeling functions. Our data programming pipeline is analogous to the noisy channel model proposed for spelling correction by (Kernighan et al., 1990), and consists of a set of candidate generation and noise detection functions.\narg max µ P (µ|si) = arg max µ P (si|µ). P (µ)\nwhere µ and si represent utterances and the ith skill respectively. P (si|µ) the probability of a skill\nbeing valid for an utterance is approximated by simple functions that act as candidate data generators λg ∈ Λg based on recognitions produced by a family of query patterns λq ∈ Λq. P (µ) is represented by a family of simple functions that act as noise detectors λn ∈ Λn, which mark utterances as likely being noise.\nWe apply the technique to the query logs of a popular IPDA, which has support for personalized third party domains. Looking at the structure of utterances that match query pattern λq, each utterance of form ”Ask {Uber} to {get me a car}” can be considered as being parametrized by the underlying latent command µz , that is ”Get me a car”, a target domain corresponding to service st, which in this case is Uber and the query recognition pattern λq, in this case ”Ask {st} to {µz}”. Next we assume that the distribution of latent commands over domains are independent of the query pattern.\nP (µz, st) ≈ P (µ, st, λq)\nMaking this simple distributional approximation allows us to generate a large number of noisy training samples. The family of generator functions λg ∈ Λg is thus defined such that uz = λig(µ, λiq)"
  }, {
    "heading": "3.2 Noise Reduction",
    "text": "The distribution defined above contains a large number of noisy positive samples. Related to P (µ) in the noisy channel in the spell correction context, we defined a small family of heuristic noise detection functions λn ∈ Λn that discards\ntraining data instances that are not likely to be well formed. For instance,\n• λ1n requires u to contain a minimum threshold of information by removing those with µz that has token length fewer than 3. Utterances shorter than this mostly consist of nonactionable commands.\n• λ2n discards all data samples below a certain threshold of occurrences in live traffic, since utterances that are rarely observed are more likely to be ASR errors or unnatural.\n• λ3n discards the data samples for a domain if they come from an overly broad pattern with a catch-all behavior.\n• λ4n discards utterances that belong to shared intents provided by the SLU SDK.\nThe end result of this stage is to retain utterances such as ‘call me a cab’ from ‘Ask Uber to call me a cab’ but discard ‘Boston’ from ‘Ask Accuweather for Boston’. One can easily imagine extending this framework with other high recall noise detectors, for example, using language models to discard candidates that are unlikely to be spoken sentences."
  }, {
    "heading": "4 Model Architecture",
    "text": "Our model consists of a shared encoder network consisting of an orthography-sensitive hierarchical LSTM encoder that feeds into a set of domain specific classification layers trained to make a binary decision for each output label.\nOur main novel contribution is the extension of this architecture with a personalized attention mechanism which uses the attention mechanism of (Bahdanau et al., 2014) to attend to memory locations corresponding to the specific domains enabled by a user, and allows the system to learn semantic representations of each domain via domain embeddings. As we will show, incorporating personalization features is key to disambiguating between multiple overlapping domains1, and the personalized attention mechanism outperforms more naive forms of personalization. The personalized attention mechanism first computes an attention weight for each of enabled domains, performs a convex combination to compute a context\n1We assume that users can customize their IPDA settings to enable certain domains.\nvector and then concatenates this vector to the encoded utterance before the final domain classification. Figure 1 depicts the model in detail.\nOur model can efficiently accommodate new domains not seen during initial training by keeping the shared encoder frozen, bootstrapping a domain embedding based on existing ones, then optimizing a small number of network parameters corresponding to domain-specific classifier, which is orders of magnitude faster and more data efficient than retraining the full classifier.\nWe make design decisions to ensure that our model has a low memory and latency footprint. We avoid expensive large vocabulary matrix multiplications on both the input and output stages, and instead use a combination of character embeddings and word embeddings in the input stage.2 The output matrix is lightweight because each domain-specific classifier is a matrix of only 201×2 parameters. The inference task can be trivially parallelized across cores since there’s no requirement to compute a partition function across a high-dimensional softmax layer, which is the slowest component of large label multiclass neural networks. Instead, we achieve comparability between the probability scores generated by individual models by using a customized loss formulation.3"
  }, {
    "heading": "4.1 Shared Encoder",
    "text": "First we describe our shared hierarchical utterance encoder, which is marked by the almond colored box in Figure 1. Our hierarchical character to word to utterance design is motivated by the need to make the model operate on an open vocabulary in terms of words and to make it robust to small changes in orthography resulting from fluctuations in the upstream ASR system, all while avoiding expensive large matrix multiplications associated with one-hot word encoding in large vocabulary systems.\nWe denote an LSTM simply as a mapping φ : Rd × Rd′ → Rd′ that takes a d dimensional input vector x and a d′ dimensional state vector h to output a new d′ dimensional state vector h′ =\n2Using a one-hot representation of word vocabulary size 60,000 and hidden dimension 100 would require learning a matrix of size 60000 x 100 - using 100-dim word embeddings requires only a O(1) lookup followed by a 100 x 100 matrix, thus allowing our model to be significantly smaller and faster despite having what is effectively an open vocabulary\n3Current inference consumes 50MB memory and the p99 latency is 15ms.\nφ(x, h). Let C denote the set of characters andW the set of words in a given utterance. Let⊕ denote the vector concatenation operation. We encode an utterance using BiLSTMs, and the model parameters Θ associated with this BiLSTM layer are\n• Char embeddings ec ∈ R25 for each c ∈ C • Char LSTMs φCf , φCb : R25 × R25 → R25\n• Word embeddings ew ∈ R50 for each w ∈ W • Word LSTMs φWf , φWb : R100 × R50 → R50\nLet w1 . . . wn ∈ W denote a word sequence where wordwi has characterwi(j) ∈ C at position j. First, the model computes a character-sensitive word representation vi ∈ R100 as\nfCj = φ C f ( ewi(j), f C j−1 )\n∀j = 1 . . . |wi| bCj = φ C b ( ewi(j), b C j+1 ) ∀j = |wi| . . . 1 vi = f C |wi| ⊕ b C 1 ⊕ ewi\nfor each i = 1 . . . n.4 These word representation vectors are encoded by forward and backward LSTMs for word φWf , φ W b as\nfWi = φ W f ( vi, f W i−1 )\n∀i = 1 . . . n bWi = φ W b ( vi, b W i+1 ) ∀i = n . . . 1\nand induces a character and context-sensitive word representation hi ∈ R100 as\nhi = f W i ⊕ bWi\nfor each i = 1 . . . n. For convenience, we write the entire operation as a mapping BiLSTMΘ:\n(h1 . . . hn)← BiLSTMΘ(w1 . . . wn)\nh̄ = n∑ i=1 hi (1)"
  }, {
    "heading": "4.2 Domain Classification",
    "text": "Our Multitask domain classification formulation is motivated by a desire to avoid computing the full partition function during test time, which tends to be the slowest component of a multiclass neural network classifer, as has been documented before by (Jozefowicz et al., 2016) and (Mikolov et al., 2013), amongst others.\n4For simplicity, we assume some random initial state vectors such as fC0 and bC|wi|+1 when we describe LSTMs.\nHowever, we also want access to reliable probability estimates instead of raw scores - we accomplish this by constructing a custom loss function. During training, each domain classifier receives in-domain (IND) and out-of-domain (OOD) utterances, and we adapt the one-sided selection mechanism of (Kubat et al., 1997) to prevent OOD utterances from overpowering IND utterances, thus an utterance in a domain d ∈ D is considered as an IND utterance in the viewpoint of domain d and OOD for all other domains.\nWe first use the shared encoder to compute the utterance representation h̄ as previously described. Then we define the probability of domain d̃ for the utterance by mapping h̄ to a 2-dimensional output vector with a linear transformation for each domain d̃ as\nzd̃ = σ(W d̃ · h̄+ bd̃)\np(d̃|h̄) ∝ exp ( [zd̃]IND ) , if d̃ = d exp ( [zd̃]OOD ) , otherwise\nwhere σ is scaled exponential linear unit (SeLU) for normalized activation outputs (Klambauer et al., 2017) and [zd̃]IND and [zd̃]OOD denote the values in the IND and OOD position of vector zd̃.\nWe define the joint domain classification loss LD as the summation of positive (LP ) and negative (LN ) class loss functions 5:\nLP ( Θ,Θd̃ ) = − log p ( d̃|h̄ ) LN ( Θ,Θd̃ )\n= − 1 k − 1  ∑ d̄∈D,d̄ 6=d̃ log p ( d̄|h̄ )\nLD ( Θ,Θd̃ ) = LP ( Θ,Θd̃ ) + LN ( Θ,Θd̃ )\nwhere k is the total number of domains. We divide the second term by k − 1 so that LP and LN are balanced in terms of the ratio of the training examples for a domain to those for other domains. While a softmax over the entire domains tends to highlight only the ground-truth domain while suppressing all the rest, the our joint domain classification with a softmax over two classes is designed to produce a more balanced confidence score per domain independent of other domains.\n5Θd̃ denotes the additional parameters in the classification layer for domain d̃."
  }, {
    "heading": "4.3 Personalized Attention",
    "text": "We explore encoding a user’s domain preferences in two ways. Our baseline method is a 1-bit flag that is appended to the input features of each domain-specific classifier. Our novel personalized attention method induces domain embeddings by supervising an attention mechanism to attend to a user’s enabled domains with different weights depending on their relevance. The domain embedding matrix in Figure 1 represents the embeddings of a user’s enabled domains. We hypothesize that attention enables the network learn richer representations of user preferences and domain co-occurrence features.\nLet eD(d̃) ∈ R100 and h̄ ∈ R100 denote the domain embeddings for domain d̃ and the utterance representation calculated by Eq. (1), respectively. The domain attention weights for a given user u who has a preferred domain list d(u) =( d̃\n(u) 1 , . . . , d̃ (u) k\n) are calculated by the dot-product\noperation, ai = h̄ · eD ( d̃ (u) i ) ∀i = 1 . . . k\nThe final, normalized attention weights ā are obtained after normalization via a softmax layer,\nāi = exp(ai)∑k j=1 exp(aj)\n∀i = 1 . . . k\nThe weighted combination of domain embeddings is\nS̄attended = k∑ i=1 ( āi · eD ( d̃ (u) i )) Finally the two representations of enabled domains, namely the attention model and 1-bit flag are then concatenated with the utterance representation and used to make per-domain predictions via domain-specific affine transformations:\nz̄att = h̄⊕ S̄attended\nz̄1bit = h̄⊕ I(d̃ ∈ enabled)\nHere I(d̄ ∈ enabled) is a 1-bit indicator for whether the domain is enabled by the user or not. z̄att and z̄1bit represent the encoded hidden state of the Attention and 1-Bit Flag configurations of the model from the experiment section. In our experiments we will compare these two ways of encoding personalization information, as well\nas evaluate a variant that combines the two. In this way we can ascertain whether the two personalization signals are complementary via an ablation study on the full model."
  }, {
    "heading": "4.4 Domain Bootstrapping",
    "text": "Our model separates the responsibilities for utterance representation and domain classification between the shared encoder and the domain-specific classifiers. That is, the shared encoder needs to be retrained only if it cannot encode an utterance well (e.g., a new domain introduces completely new words) and the existing domain classifiers need to be retrained only when the shared encoder changes. For adding new domains efficiently without full retraining, the only two components in the architecture need to be updated for each new domain d̃new, are the domain embeddings for the new domain and its domain-specific classifier.6 We keep the weights of the encoder network frozen and use the hidden state vector h̄, calculated by Eq. 1, as a feature vector to feed into the downstream classifiers. To initialize the m-dimensional domain embeddings ed̃new , we use the column-wise average of all utterance vectors in the training data h̄avg, and project it to the domain embedding space using a matrix U ∈ Rm×m. Thus,\ned̃new = U ∗ · h̄avg\nThe parameters of U∗ are learned using the column-wise average utterance vectors h̄avgj and learned domain vectors for all existing domains dj\nU∗ = arg min U ||U · h̄avgj − edj || ∀dj ∈ D\nThis is a write-to-memory operation that creates a new domain representation after attending to all existing domain representations. We then train the parameters of the domain-specific classifier with the new domain’s data while keeping the encoder fixed. This mechanism allows us to efficiently support new domains that appear in-between full model deployment cycles without compromising performance on existing domains. A full model refresh would require us to fully retrain with the domains that have appeared in the intermediate period.\n6We have assumed that the shared encoder covers most of the vocabulary of new domains; otherwise, the entire network may need to be retrained. Based on our observation of live usage data, this assumption is reasonable since the shared encoder after initial training is still shown to cover 95% of the vocabulary of new domains added in the subsequent week."
  }, {
    "heading": "5 Experiments",
    "text": "In this section we aim to demonstrate the effectiveness of our model architecture in two settings. First, we will demonstrate that attention based personalization significantly outperforms the baseline approach. Secondly, we will show that our model new domain bootstrapping procedure results in accuracies comparable to full retraining while requiring less than 1% of the orignal training time."
  }, {
    "heading": "5.1 Experimental Setup",
    "text": "Weak: This is a weakly supervised dataset was generated by preprocessing utterances with strict invocation patterns according to the setup mentioned in Section 3. The dataset consists of 5.34M utterances from 637,975 users across 1,500 different skills. Since we are interested in capturing the temporal effects of the dataset as well as personalization effects, we partitioned the data based both on user and time. Our core training data for the experiments in this paper was drawn from one month of live usage, the validation data came from the next 15 days of usage, and the test data came from the subsequent 15 days. The training, validation and test sets are user-independent, and each user belongs to only one of the three sets to ensure no leakage of information.\nMTurk: Since the Weak dataset is generated by weak supervision, we verified the performance of our approach with human generated utterances. A random sample of 12,428 utterances from the test partition of users were presented to 300 human judges, who were asked to produce two natural ways to issue the same command. This dataset is treated as a representative clean held out test set on which we can observe the generalization of our weakly supervised training and validation data to\nnatural language.\nNew Skills: In order to simulate the scenario in which new skills appear within a week between model updates, we selected 250 new skills which do not overlap with the skills in the Weak dataset. The vocabulary size of 1,500 skills is 200K words, and on average, 5% of the vocabulary for new skills is not covered. We randomly sampled 4,000 unique utterances for each skill using the same weak supervision method, and split them into 3,000 utterances for training and 1,000 for testing."
  }, {
    "heading": "5.2 Results and Discussion",
    "text": "Generalization from Weakly Supervised to Natural Utterances We first show the progression of model performance as we add more components to show their individual contribution. Secondly, we show that training our models on a weakly supervised dataset can generalize to natural speech by showing their test performance on the human-annotated test data. Finally, we compare two personalization strategies.\nThe full results are summarized in Table 1, which shows the top-N test results separately for the Weak dataset (weakly supervised) and MTurk dataset (human-annotated). We report top-N accuracy to show the potential for further re-ranking or disambiguation downstream. For top-1 results on the Weak dataset, using a separate binary classifier for each domain (Binary) shows a prediction accuracy at 78.29% and using a softmax layer on top of the shared encoder (MultiClass) shows a comparable accuracy at 78.58%. The performance shows a slight improvement when using the Multitask neural loss structure, but adding personalization signals to the Multitask structure showed a significant boost in performance. We noted the large difference between the 1-bit and attention architecture. At 94.83% accuracy, attention resulted in 35.6% relative error reduction over the 1-bit baseline 91.97% on the Weak validation set and 23.25% relative on the MTurk test set. We hypothesize that this may be because the attention mechanism allows the model to focus on complementary features in case of overlapping domains as well as learning domain co-occurrence statistics, both of which are not possible with the simple 1-bit flag.\nWhen both personalization representations were combined, the performance peaked at 95.19% for the Weak dataset and a more modest\n89.65% for the MTurk dataset. The improvement trend is extremely consistent across all top-N results for both of the Weak and MTurk datasets across all experiments. The disambiguation task is complex due to similar and overlapping skills, but the results suggest that incorporating personalization signals equip the models with much better discriminative power. The results also suggest that the two mechanisms for encoding personalization provide a small amount of complementary information since combining them together is better than using them individually. Although the performance on the Weak dataset tends to be more optimistic, the best performance on the humanannotated test data is still close to 90% for top-1 accuracy, which suggests that training our model with the samples derived from the invocation patterns can generalize well to natural utterances.\nRapid Bootstrapping of New Skills We show the results when new domains are added to an IPDA and the model needs to efficiently accommodate them with a limited number of data samples. We show the classification performance on the skills in the New Skills dataset while assuming we have access to the WEAK dataset to pre-train our model for transfer learning. In the Binary setting, a domain-specific binary classifier is trained for each domain. Expand describes the case in which we incrementally train on top of an existing model. Refresh is the setting in which the model is fully retrained from scratch with the new data - which would be ideal in case there were no time constraints.\nWe record the average training time for each epoch and the performance is measured with top-1 classification accuracy over new skills. The experiment results can be found in Table 2. Adapting a new skill is two orders of magnitude faster (30.34 seconds) than retraining the model (5300.18 seconds) while achieving 94.03% accuracy which is comparable to 94.58% accuracy of full retraining. The first two techniques can also be easily parallelized unlike the Refresh configuration.\nBehavior of Attention Mechanism Our expectation is that the model is able to learn to attend the relevant skills during the inference process. To study the behavior of the attention layer, we compute the top-N prediction accuracy based on the most relevant skills defined by the attention weights. In this experiment, we considered the subset of users who had enabled more than 20 domains to exclude trivial cases7. The results are shown in Table 3. When the model attends to the entire set of 1500 (Full), the top-5 prediction accuracy is 20.41%, which indicates that a large number of skills can process the utterance, and thus it is highly likely to miss the correct one in the top-5 predictions. This ambiguity issue can be significantly improved by users’ enabled domain lists as proved by the accuracies (Enabled): 85.62% for top-1, 96.15% for top-3, and 98.06% for top-5.8 Thus the attention mechanism can thus be viewed as an initial soft selection which is then followed by a fine-grained selection at the classification stage.\nEnd-to-End User Evaluation All intermediate metrics on this task are proxies to a human customer’s eventual evaluation. In order to assess the user experience, we need to measure its end-toend performance. For a brief end-to-end system evaluation, 983 utterances from 283 domains were randomly sampled from the test set in the largescale IPDA setting. 15 human judges (male=12, female=3) rated the system responses, 1 judge per utterance, on a 5-point Likert scale with 1 being Terrible and 5 being Perfect. The judgment score of 3 or above was taken as SUCCESS and 2 or below as DEFECT. The end-to-end SUCCESS rate,\n7Thus, the random prediction accuracy on enabled domains is less than 5% and across the Full domain list is 0.066%\n8Visual inspection of the embeddings confirms that meaningful clusters are learned. We see clusters related to home automation, commerce, cooking, trivia etc, we show some examples in Figure 2, 3 and 4. However there are still other clusters where the the relationships cannot be established as easily. An example of these is show in Figure 5. The personalized attention mechanism is learned using the semantic content as well as personalization signals, so we hypothesize clusters like this may be capturing user tendencies to enable these domains in a correlated manner.\nthus user satisfaction, was shown to be 95.52%. The discrepancy between this score and the score produced on MTurk dataset indicates that even in cases in which the model makes classification mistakes, some of these interpretations remain perceptually meaningful to humans."
  }, {
    "heading": "6 Conclusions",
    "text": "We have described a neural model architecture to address large-scale skill classification in an IPDA used by tens of millions of users every day. We have described how personalization features and an attention mechanism can be used for handling ambiguity between domains. We have also shown that the model can be extended efficiently and incrementally for new domains, saving multiple orders of magnitude in terms of training time. The model also addresses practical constraints of having a low memory footprint, low latency and being easily parallelized, all of which are important characteristics for real-time production systems. In future work, we plan to incorporate various types of context (e.g. anaphora, device-specific capabilities) and dialogue history into a large-scale NLU system."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Towards zero shot frame semantic parsing for domain scaling",
    "authors": ["Ankur Bapna", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."],
    "venue": "Interspeech 2017.",
    "year": 2017
  }, {
    "title": "Easy contextual intent prediction and slot detection",
    "authors": ["A. Bhargava", "Asli Celikyilmaz", "Dilek Z. HakkaniTur", "Ruhi Sarikaya."],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8337–8341.",
    "year": 2013
  }, {
    "title": "A new pre-training method for training deep learning models with application to spoken language understanding",
    "authors": ["Asli Celikyilmaz", "Ruhi Sarikaya", "Dilek Hakkani-Tür", "Xiaohu Liu", "Nikhil Ramesh", "Gökhan Tür."],
    "venue": "Interspeech, pages 3255–3259.",
    "year": 2016
  }, {
    "title": "Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models",
    "authors": ["Yun-Nung Chen", "Dilek Hakkani-Tür", "Xiaodong He."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International",
    "year": 2016
  }, {
    "title": "End-toend memory networks with knowledge carryover for multi-turn spoken language understanding",
    "authors": ["Yun-Nung Chen", "Dilek Hakkani-Tür", "Gokhan Tur", "Jianfeng Gao", "Li Deng."],
    "venue": "Interspeech.",
    "year": 2016
  }, {
    "title": "Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs",
    "authors": ["Ali El-Kahky", "Xiaohu Liu", "Ruhi Sarikaya", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"],
    "year": 2014
  }, {
    "title": "Transfer learning for neural semantic parsing",
    "authors": ["Xing Fan", "Emilio Monti", "Lambert Mathias", "Markus Dreyer."],
    "venue": "CoRR, abs/1706.04326.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Knowledgebased weak supervision for information extraction of overlapping relations",
    "authors": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association",
    "year": 2011
  }, {
    "title": "Context-sensitive and roledependent spoken language understanding using bidirectional and attention lstms",
    "authors": ["Chiori Hori", "Takaaki Hori", "Shinji Watanabe", "John R Hershey."],
    "venue": "Interspeech, pages 3236–3240.",
    "year": 2016
  }, {
    "title": "Domain adaptation of recurrent neural networks for natural language understanding",
    "authors": ["Aaron Jaech", "Larry Heck", "Mari Ostendorf."],
    "venue": "Interspeech.",
    "year": 2016
  }, {
    "title": "Bag of tricks for efficient text classification",
    "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.01759.",
    "year": 2016
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."],
    "venue": "arXiv preprint arXiv:1602.02410.",
    "year": 2016
  }, {
    "title": "A spelling correction program based on a noisy channel model",
    "authors": ["Mark D Kernighan", "Kenneth W Church", "William A Gale."],
    "venue": "Proceedings of the 13th conference on Computational linguistics-Volume 2, pages 205–210. Association for Computational Lin-",
    "year": 1990
  }, {
    "title": "Weakly supervised slot tagging with partially labeled sequences from web search click logs",
    "authors": ["Young-Bum Kim", "Minwoo Jeong", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the",
    "year": 2015
  }, {
    "title": "Speaker-sensitive dual memory networks for multi-turn slot tagging",
    "authors": ["Young-Bum Kim", "Sungjin Lee", "Ruhi Sarikaya."],
    "venue": "Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pages 547–553. IEEE.",
    "year": 2017
  }, {
    "title": "Onenet: Joint domain, intent, slot prediction for spoken language understanding",
    "authors": ["Young-Bum Kim", "Sungjin Lee", "Karl Stratos."],
    "venue": "Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE, pages 547–553. IEEE.",
    "year": 2017
  }, {
    "title": "Adversarial adaptation of synthetic or stale data",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1297–1307. Association for Computational",
    "year": 2017
  }, {
    "title": "Domain attention with an ensemble of experts",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Annual Meeting of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Pre-training of hidden-unit crfs",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural",
    "year": 2015
  }, {
    "title": "Frustratingly easy neural domain adaptation",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 387–396.",
    "year": 2016
  }, {
    "title": "A framework for pre-training hidden-unit conditional random fields and its extension to long short term memory networks",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Computer Speech & Language, 46:311–326.",
    "year": 2017
  }, {
    "title": "New transfer learning techniques for disparate label sets",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International",
    "year": 2015
  }, {
    "title": "Self-normalizing neural networks",
    "authors": ["Gunter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter."],
    "venue": "CoRR, abs/1706.02515.",
    "year": 2017
  }, {
    "title": "Addressing the curse of imbalanced training sets: one-sided selection",
    "authors": ["Miroslav Kubat", "Stan Matwin"],
    "venue": "In ICML,",
    "year": 1997
  }, {
    "title": "Just ask: Building an architecture for extensible self-service spoken language understanding",
    "authors": ["Anjishnu Kumar", "Arpit Gupta", "Julian Chan", "Sam Tucker", "Bjorn Hoffmeister", "Markus Dreyer."],
    "venue": "arXiv preprint arXiv:1711.00549.",
    "year": 2017
  }, {
    "title": "Zero-shot learning across heterogeneous overlapping domains",
    "authors": ["Anjishnu Kumar", "Pavankumar Reddy Muddireddy", "Markus Dreyer", "Björn Hoffmeister."],
    "venue": "Proc. Interspeech 2017, pages 2914– 2918.",
    "year": 2017
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of NAACL-HLT, pages 260–270.",
    "year": 2016
  }, {
    "title": "Attention-based recurrent neural network models for joint intent detection and slot filling",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "Interspeech, pages 685–689.",
    "year": 2016
  }, {
    "title": "Multi-domain adversarial learning for slot filling in spoken language understanding",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "NIPS Workshop on Conversational AI.",
    "year": 2017
  }, {
    "title": "Visualizing high-dimensional data using t-sne",
    "authors": ["Laurens van der Maaten", "Geoffrey Hinton."],
    "venue": "Journal of Machine Learning Research, 9:2579– 2605.",
    "year": 2008
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Data programming: Creating large training sets, quickly",
    "authors": ["Alexander J Ratner", "Christopher M De Sa", "Sen Wu", "Daniel Selsam", "Christopher Ré."],
    "venue": "Advances in Neural Information Processing Systems, pages 3567–3575.",
    "year": 2016
  }, {
    "title": "The technology behind personal digital assistants: An overview of the system architecture and key components",
    "authors": ["Ruhi Sarikaya."],
    "venue": "IEEE Signal Processing Magazine, 34(1):67–81.",
    "year": 2017
  }, {
    "title": "An overview of end-to-end language understanding and dialog",
    "authors": ["Ruhi Sarikaya", "Paul A Crook", "Alex Marin", "Minwoo Jeong", "Jean-Philippe Robichaud", "Asli Celikyilmaz", "Young-Bum Kim", "Alexandre Rochette", "Omar Zia Khan", "Xiaohu Liu"],
    "year": 2016
  }, {
    "title": "Multitask learning in deep neural networks for improved phoneme recognition",
    "authors": ["Michael L Seltzer", "Jasha Droppo."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6965–6969.",
    "year": 2013
  }, {
    "title": "Multitask learning for spoken language understanding",
    "authors": ["Gokhan Tur."],
    "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, volume 1, pages I–I. IEEE.",
    "year": 2006
  }, {
    "title": "Spoken Language Understanding: Systems for Extracting Semantic Information from Speech",
    "authors": ["Gokhan Tur", "Renato de Mori."],
    "venue": "New York, NY: John Wiley and Sons.",
    "year": 2011
  }, {
    "title": "Efficient character-level document classification by combining convolution and recurrent layers",
    "authors": ["Yijun Xiao", "Kyunghyun Cho."],
    "venue": "arXiv preprint arXiv:1602.00367.",
    "year": 2016
  }, {
    "title": "Contextual domain classification in spoken language understanding systems using recurrent neural network",
    "authors": ["Puyang Xu", "Ruhi Sarikaya."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 136–",
    "year": 2014
  }, {
    "title": "Transfer learning for sequence tagging with hierarchical recurrent networks",
    "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W Cohen."],
    "venue": "International Conference on Learning Representation (ICLR).",
    "year": 2017
  }, {
    "title": "A joint model of intent determination and slot filling for spoken language understanding",
    "authors": ["Xiaodong Zhang", "Houfeng Wang."],
    "venue": "International Joint Conference on Artificial Intelligence (IJCAI), pages 2993–2999.",
    "year": 2016
  }],
  "id": "SP:4ae7ea53d4af4309c0836a5f4c57c08f79f9e8ea",
  "authors": [{
    "name": "Young-Bum Kim",
    "affiliations": []
  }, {
    "name": "Dongchan Kim",
    "affiliations": []
  }, {
    "name": "Anjishnu Kumar",
    "affiliations": []
  }, {
    "name": "Ruhi Sarikaya",
    "affiliations": []
  }],
  "abstractText": "In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment builtin first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.",
  "title": "Efficient Large-Scale Neural Domain Classification with Personalized Attention"
}