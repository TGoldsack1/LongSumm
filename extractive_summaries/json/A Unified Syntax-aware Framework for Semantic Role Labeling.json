{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2401–2411 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2401\nA Unified Syntax-aware Framework for Semantic Role Labeling Zuchao Li1,2,∗, Shexia He1,2,∗, Jiaxun Cai1,2, Zhuosheng Zhang1,2, Hai Zhao1,2,†, Gongshen Liu3, Linlin Li4, Luo Si4 1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3School of Cyber Security, Shanghai Jiao Tong University, China 4Alibaba Group, Hangzhou, China\n{charlee,heshexia,caijiaxun,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, lgshen@sjtu.edu.cn,\n{linyan.lll,luo.si}@alibaba-inc.com Abstract\nSemantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models."
  }, {
    "heading": "1 Introduction",
    "text": "The purpose of semantic role labeling (SRL) is to derive the predicate-argument structure of each predicate in a sentence. A popular formalism to represent the semantic predicate-argument structure is based on dependencies, namely dependency SRL, which annotates the heads of arguments rather than phrasal arguments. Given a sentence (in Figure 1), SRL is generally decomposed\n∗ These authors made equal contribution.† Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04) and the joint research project with Youtu Lab of Tencent.\nA2 AM-TMP\nA0\nSomeone always makes you happy make.02\nA1\nFigure 1: An example of dependency-based SRL.\ninto multiple subtasks in pipeline framework, consisting of predicate identification (makes), predicate disambiguation (make.02), argument identification (e.g., Someone) and argument classification (Someone is A0 for the predicate makes). SRL is beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016) and question answering (Berant et al., 2013; Yih et al., 2016).\nMost traditional SRL methods rely heavily on feature templates that struggle to capture sufficient discriminative information, while neural models are capable of extracting features automatically. In particular, recent works (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) propose syntax-agnostic models for SRL and achieve favorable results, which seems to be in conflict with the belief that syntactic information is an absolutely necessary prerequisite for high-performance SRL (Gildea and Palmer, 2002). Despite the success of these models, the main reasons for putting syntax aside are two-fold. First, it is still challenging to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation. Second, the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance.\nHowever, syntactic information is considered closely related to semantic relation and plays an essential role in SRL task (Punyakanok et al., 2008). Recently, Marcheggiani and Titov (2017)\nproposed a syntactic graph convolutional networks (GCNs) based SRL model and further improved the SRL performance with relatively better syntactic parser as input. Since syntax can provide rich structure and information for SRL, we seek to effectively model complex syntactic tree structure for incorporating syntax into neural SRL.\nIn this paper, we present a general framework1 for SRL, which enables us to integrate syntax into SRL in diverse ways. Following Marcheggiani and Titov (2017), we focus on argument labeling and formulate SRL as sequence labeling problem. However, we differ by (1) leveraging enhanced word representation, (2) applying recent advances in recurrent neural networks (RNNs), such as highway connections (Srivastava et al., 2015), (3) using deep encoder with residual connections (He et al., 2016), (4) further extending Syntax Aware Long Short-Term Memory (SA-LSTM) (Qian et al., 2017) for SRL, and (5) introducing the Tree-Structured Long Short-Term Memory (Tree-LSTM) (Tai et al., 2015) to model syntactic information for SRL.\nIn addition, as pointed out by He et al. (2017) for span SRL, the worse syntactic input will hurt performance if the syntactically-driven SRL model trusts syntactic information too much, and high-quality syntax can still make a large impact on SRL, which motivates us to investigate the effect of syntactic quality on dependency SRL. In summary, our major contributions are as follows: • We propose a unified neural framework for dependency SRL to more effectively integrate syntactic information with multiple methods. • Our SRL framework incorporated with syntax achieves the new state-of-the-art results on both English and Chinese CoNLL-2009 benchmarks. • We explore the impact of different quality of syntactic input on SRL performance, showing that high quality syntactic parse may indeed improve syntax-aware SRL."
  }, {
    "heading": "2 A Unified SRL Framework",
    "text": "In order to explore the effectiveness of the syntactic feature from various perspectives, we propose a unified neural framework that is capable of optionally accommodating various types of syntactic encoders for syntax-based SRL.\nSince the CoNLL-2009 shared task (Hajič et al.,\n1Our code is available here: https://github.com/ bcmi220/unified_syn_srl.\n2009) have beforehand indicated the predicate positions, we need to identify and label all arguments for each predicate, which is a typical sequence tagging problem. In this work, we construct a general SRL framework for argument labeling. As shown in Figure 2, our SRL framework includes three main modules, (1) BiLSTM encoder that directly takes sequential inputs, (2) MLP with highway connections for softmax output layer, and (3) an optional syntactic encoder that receives the outputs of the BiLSTM encoder and then let its own outputs integrate with the BiLSTM outputs through residual connections.\nNote that when the syntactic encoder is completely removed, MLP only takes inputs directly from the BiLSTM encoder, which let our framework become a syntax-agnostic labeler."
  }, {
    "heading": "2.1 Sentence Encoder",
    "text": "Word representation Given a sentence and known predicate, we consider predicate-specific word representation, following previous work (Marcheggiani and Titov, 2017). Specifically, each word embedding representation ei of input sentence is the concatenation of several features, a randomly initialized word embedding eri , a pretrained word embedding epi , a randomly initialized lemma embedding eli, a randomly initialized POS tag embedding eposi , and a predicate-specific feature efi , which is a binary flag set 0 or 1 indicating whether the current word is the given predicate.\nTo further enhance the word representation, we leverage an external embedding ELMo (Embeddings from Language Models) proposed by Peters et al. (2018). ELMo is obtained by deep bidirectional language model that takes characters as input, enriching subword information and contextual information, which has expressive representation power. Eventually, the resulting word representation is concatenated as ei = [eri , e p i , e l i, e pos i , e f i ,ELMoi].\nBiLSTM encoder We use bi-directional Long Short-term Memory neural network (BiLSTM) (Hochreiter and Schmidhuber, 1997) as the sentence encoder to model sequential inputs. Given an input sequence (e1, . . . , en), the BiLSTM processes these embedding vectors sequentially from both directions to obtain two separated hidden states, −→ h i and ←− h i respectively. By concatenating the two states, we get a contextual representation hi = [ −→ h i, ←− h i], which will be taken by the next\nWord representation\ncats love hats\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n+ + + +\nBiLSTM encoder\nThe\nThe cats love hats\nNMOD SBJ\nOBJ\nh3\nh2\nh1\nh4\nThe cats love\nhats\nReLU(∑∙) ReLU(∑∙) ReLU(∑∙) ReLU(∑∙)\nNMOD SBJ OBJ\nW se\nlf\nW se\nlf\nW se\nlf\nW se\nlf\nThe\ncats\nlove\nhats\nSyntactic Encoder\nGCNs\nTree-LSTM\nSA-LSTM +\nHidden Layer\nSoftmax\nHighway\nresidual connections\nFigure 2: The unified syntax-based SRL framework\nBiLSTM layer as input. In this work, we stack four layers of BiLSTM."
  }, {
    "heading": "2.2 Role Labeler",
    "text": "We adopt a Multi-Layer Perceptron (MLP) with highway connections (Srivastava et al., 2015) on the top of our deep encoder, which takesthe concatenated representation as input. The MLP consists of 10 layers and we employ ReLU activations for the hidden layer. To get the final predicted semantic roles, we use a softmax layer over the outputs to maximize the likelihood of labels. The MLP part takes inputs from both the BiLSTM encoder and syntactic encoder, which are joint through a residual connection (He et al., 2016) as shown in Figure 2. It is worth noting that our deep encoder is different from the one of Marcheggiani and Titov (2017), which directly applies a softmax transformation over the syntactic representation and predicts the role label for each word. That is, their syntactic encoder outputs are directly taken as the input of hidden layer."
  }, {
    "heading": "3 Syntactic Encoder",
    "text": "To integrate the syntactic information into sequential neural networks, we employ a syntactic encoder on top of the BiLSTM encoder.\nSpecifically, given a syntactic dependency tree\nT , for each node nk in T , let C(k) denote the syntactic children set of nk,H(k) denote the syntactic head of nk, and L(k, ·) be the dependency relation between node nk and those have a direct arc from or to nk. Then we formulate the syntactic encoder as a transformation f τ over the node nk, which may take some of C(k), H(k), or L(k, ·) as input, and compute a syntactic representation vk for node nk, namely, vk = f τ (C(k), H(k), L(k, ·), xk). When not otherwise specified, xk denotes the input feature representation of nk which may be either the word representation ek or the output of BiLSTM hk, σ denotes the logistic sigmoid function, and denotes the element-wise multiplication.\nIn practice, the transformation f τ can be any syntax encoding method. In this paper, we will consider three types of syntactic encoders, syntactic graph convolutional network (Syntactic GCN) (in Section 3.1), syntax aware LSTM (SA-LSTM) (in Section 3.2), tree-structured LSTM (TreeLSTM) (in Section 3.3). Then, we will provide a brief introduction in subsequent subsections."
  }, {
    "heading": "3.1 Syntactic GCN",
    "text": "GCN (Kipf and Welling, 2017) is proposed to induce the representations of nodes in a graph based on the properties of their neighbors. Given its effectiveness, Marcheggiani and Titov (2017) in-\ntroduce a generalized version for the SRL task, namely syntactic GCN, and shows that syntactic GCN is effective in incorporating syntactic information into neural models.\nSyntactic GCN captures syntactic information flows in two directions, one from heads to dependents (along), the other from dependents to heads (opposite). Besides, it also models the information flows from a node to itself, namely, it assumes that a syntactic graph contains self-loop for each node. Thus, the syntactic GCN transformation of a node nk is defined on its neighborhood N(k) = C(k)∪H(k)∪{nk}. For each edge connects nk and its neighbor nj , we can compute a vector representation for it,\nuk,j = W dir(k,j)xj + b L(k,j),\nwhere dir(k, j) denotes the direction type (along, opposite or self-loop) of the edge from nk to nj , W dir(k,j) is direction type specific parameter, bL(k,j) is label specific parameter. Considering that syntactic information from all the neighboring nodes may make different contribution to semantic role labeling, syntactic GCN introduces an additional edge-wise gating for each node pair (nk, nj) as\ngk,j = σ(W dir(k,j) g xk + b L(k,j) g ).\nThe syntactic representation vk for a node nk can be then computed as:\nvk = ReLU( ∑\nj∈N(k)\ngk,j uk,j)."
  }, {
    "heading": "3.2 SA-LSTM",
    "text": "SA-LSTM (Qian et al., 2017) is an extension of the standard BiLSTM architecture, which aims to simultaneously encode the syntactic and contextual information for a given word as shown in Figure 2. On one hand, the SA-LSTM calculates the hidden state in sequence timestep order like the standard LSTM,\nig = σ(W (i)xk + U (i)hk−1 + b (i)),\nfg = σ(W (f)xk + U (f)hk−1 + b (f)),\nog = σ(W (o)xk + U (o)hk−1 + b (o)),\nu = f(W (u)xk + U (u)hk−1 + b (u)),\nck = ig u+ fg ck−1.\nOn the other hand, it further incorporates the syntactic information into the representation of each word by introducing an additional gate,\nsg = σ(W (s)xk + U (s)hk−1 + b (s)),\nhk = og f(ck) + sg h̃k.\nwhere h̃k = f( ∑\ntj<tk αj × hj) is the weighted\nsum of all hidden state vectors hj which come from previous node (word) nj , the weight factor αj is actually a trainable weight related to the dependency relation L(k, ·) when there exists a directed edge from nj to nk.\nNote that h̃k is always the hidden state vector of the syntactic head of nk according to the definition of αj . Since a word will be assigned a single syntactic head, such a strict constraint prevents the SA-LSTM from incorporating complex syntactic structures. Inspire by the idea of GCN, we relax the directed constraint of αj , whenever there is an edge between nj and nk.\nAfter the SA-LSTM transformation, the outputs of the SA-LSTM layer from both directions are concatenated and taken as the syntactic representation of each word nk, i.e., vk = [ −→ hk, ←− hk]. Different from the syntactic GCN, SA-LSTM encoding both syntactic and contextual information in a single vector vk."
  }, {
    "heading": "3.3 Tree-LSTM",
    "text": "Tree-LSTM (Tai et al., 2015) can be considered as an extension of the standard LSTM, which aims to model the tree-structured topologies. At each timestep, it composes an input vector and the hidden states from arbitrarily many child units. Specifically, the main difference between TreeLSTM unit and the standard one is that the memory cell updating and the calculation of gating vectors are depended on multiple child units. A TreeLSTM unit can be connected to arbitrary number of child units and assigns a single forget gate for each child unit. This provides Tree-LSTM the flexibility to incorporate or drop the information from each child unit.\nGiven a syntactic tree, the Tree-LSTM transformation is defined on node nk and its children set C(k), which can be formulated as follows (Tai\net al., 2015):\nh̃k = ∑\nj∈C(k)\nhk, (1)\nig = σ(W (i)xk + U (i)h̃k + b (i)),\nfk,jg = σ(W (f)xk + U (f)hj + b (f)), (2)\nog = σ(W (o)xk + U (o)h̃k + b (o)),\nu = tanh(W (u)xk + U (u)h̃k + b (u)), ck = ig u+ ∑\nj∈C(k)\nfk,jg cj ,\nhk = og tanh(ck).\nwhere j ∈ C(k), hj is the hidden state of the j-th child node, ck is the memory cell of the head node k, and hk is the hidden state of node k. Note that in Eq.(2), a single forget gate fk,jg is computed for each hidden state hj .\nHowever, the primitive form of Tree-LSTM does not take the dependency relations into consideration. Given the importance of dependency relations in SRL task, we further extend the TreeLSTM by adding an additional gate rg and reformulate the Eq. (1),\nrk,jg = σ(W (r)xk + U (r)hj + b L(k,j)), h̃k = ∑\nj∈C(k)\nrk,jg hj .\nwhere bL(k,j) is a relation label specific bias term. After the Tree-LSTM transformation, the hidden state of each node in dependency tree is taken as its syntactic representation, i.e., vk = hk."
  }, {
    "heading": "4 Experiments",
    "text": "We evaluate our models performance of syntactic GCN (henceforth Syn-GCN), SA-LSTM and Tree-LSTM on CoNLL-2009 datasets both for English and Chinese with standard training, development and test splits. For predicate disambiguation, we follow previous work (Marcheggiani and Titov, 2017), using the off-the-shelf disambiguator from Roth and Lapata (2016). For syntactic dependency tree, we parse the corpus with Biaffine Parser (Dozat and Manning, 2017)."
  }, {
    "heading": "4.1 Experimental Settings",
    "text": "In our experiments, the pre-trained word embeddings for English are 100-dimensional GloVe vectors (Pennington et al., 2014). For Chinese, we\nexploit Wikipedia documents to train the same dimensional Word2Vec embeddings (Mikolov et al., 2013). All other vectors are randomly initialized, the dimension of lemma embeddings is 100, and the dimension of POS tag embedding is 32. In addition, we use 300-dimensional ELMo embedding for English2.\nDuring training, we use the categorical crossentropy as objective, with Adam optimizer (Kingma and Ba, 2015) the learning rate 0.001, and the batch size is set to 64. The BiLSTM encoder consists of 4-layer BiLSTM with 512- dimensional hidden units. We apply dropout for BiLSTM with a 90% keeping probability between time-steps and layers. We train models for a maximum of 20 epochs and obtain the nearly best model based on English development results."
  }, {
    "heading": "4.2 Results",
    "text": "We compare our models of Syn-GCN, SA-LSTM and Tree-LSTM with previous approaches for dependency SRL on both English and Chinese. Noteworthily, our model is local (argument identification and classification decisions are conditionally independent) and single without reranking, which neither includes global inference nor com-\n2For Chinese, we do not use pre-trained ELMo whose weights are only available for English.\nbines multiple models. The experimental results on the in-domain English and Chinese test sets are summarized in Tables 1 and 2, respectively.\nFor English, our models of Syn-GCN, SALSTM and Tree-LSTM overwhelmingly surpass most previously published single models, achieving state-of-the-art results of 89.8%, 89.7% and 89.4% in F1 scores respectively. In comparison to ensemble models, our Syn-GCN even performs better than the previous model (Marcheggiani and Titov, 2017) with a margin of 0.7% F1.\nFrom Table 1, we also see that our Syn-GCN model provides the best recall and F1 score, while our SA-LSTM model yields the competitive performance with higher precision at the expense of recall, which shows that SA-LSTM is better at classifying arguments. Overall, the Tree-LSTM gives slightly weaker performance, which may be attributed to tree-structured network topology. More specifically, Tree-LSTM only considers information from arbitrary child units so that each node lacks of the information from parent. However, our Syn-GCN and SA-LSTM combine bidirectional information, both head-to-dependent and dependent-to-head.\nFor Chinese (Table 2), even though we use the same parameters as for English, our models are still comparable with the best reported results.\nTable 3 presents the results on English out-ofdomain test set. Our models outperform the highest records achieved by He et al. (2018), with absolute improvements of 0.2-0.5% in F1 scores. These favorable results on both in-domain and outof-domain data demonstrate the effectiveness and robustness of our proposed unified framework."
  }, {
    "heading": "4.3 Ablation and Analysis",
    "text": "To investigate the contributions of word representation and deep encoder in our method, we conduct a series of ablation studies on the English development set, unless otherwise stated.\nEffect of word representation In order to better understand how the enhanced word representation influences our model performance, we train our Syn-GCN model with different settings in input word embeddings. Table 4 shows results for our system when we remove POS tag and ELMo embedding respectively. Interestingly, the impact of POS tag embedding (about 0.4% F1) is less compared to the previous works, which allows us to build an accuracy model even when the POS tag is unavailable. We also observe that effect of ELMo embedding is somewhat surprising (1.2% F1 performance degradation). Experimental results indicate that a combination of these features could enhance the word representation, leading to SRL performance improvement.\nEffect of deep encoder Table 5 reports F1 scores of our Syn-GCN model, Marcheggiani and Titov (2017), He et al. (2018) and Cai et al. (2018) on English test set in both syntax-agnostic and syntax-aware settings. The comparison shows that our framework is more effective for incorporating syntactic information by giving more performance improvement through introducing syntax over syntax-agnostic SRL than previous state-ofthe-art systems did.\nTo further investigate the impact of deep encoder, we perform our Syn-GCN, SA-LSTM and Tree-LSTM models with another alternative configuration, using the same encoder as (Marcheggiani and Titov, 2017) (M&T encoder for short), which removes the residual connections from our framework. The corresponding results of our models are also summarized in Table 6 for comparison. Note that the first row is the results of our syntax-agnostic model. Surprisingly, we observe a dramatical performance decline of 1.2% F1 for our Syn-GCN model with M&T encoder. A less significant performance loss for our SALSTM (−0.4%) and Tree-LSTM (−0.5%) models shows that the Syn-GCN is more sensitive to contextual information. Nevertheless, the overall results show that applying deep encoder could receive higher gains."
  }, {
    "heading": "4.4 Syntactic Role",
    "text": "As mentioned before, syntactic parsers are unreliable due to the risk of erroneous syntactic input, especially on out-of-domain data. This section thus attempts to explore the impact of different quality of syntactic input on SRL performance. To this end, we further carry out experiments on English test data with different syntactic inputs based on our Syn-GCN model.\nSyntactic Input Four types of syntactic inputs are used to explore the role of syntax in our unified framework, (1) the automatically predicted parse provided by CoNLL-2009 shared task, (2) the parsing results of the CoNLL-2009 data by state-of-theart syntactic parser, the Biaffine Parser (used in our previous experiments), (3) corresponding results from another parser, the BIST Parser (Kiperwasser and Goldberg, 2016), which is also adopted by Marcheggiani and Titov (2017), (4) the gold syntax available from the official data set.\nEvaluation Metric It is worth noting that for SRL task, the standard evaluation metric is the semantic labeled F1 score (Sem-F1), and we use the labeled attachment score (LAS) to quantify the quality of syntactic input. In addition, the ratio between labeled F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1/LAS) proposed by CoNLL2008 shared task3 (Surdeanu et al., 2008), are also given for reference. To a certain extent, the ratio Sem-F1/LAS could normalize the semantic score relative to syntactic parse, impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.\nComparison and Discussion Table 7 presents the comprehensive results of our Syn-GCN model on the four syntactic inputs aforementioned of different quality together with previous SRL models. A number of observations can be made from these results. First, our model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, ob-\n3CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one. Their main difference is that predicates have been pre-identified for the latter.\ntaining overall higher scores compared to previous state-of-the-arts. Second, It is interesting to note that the Sem-F1/LAS score of our model becomes relatively smaller as the syntactic input becomes better. Though not so surprised, these results show that our SRL component is even relatively stronger. Third, when we adopt a syntactic parser with higher parsing accuracy, our SRL system will achieve a better performance. Notably, our model yields a Sem-F1 of 90.5% taking gold syntax as input. It suggests that high-quality syntactic parse may indeed enhance SRL, which is consistent with the conclusion in (He et al., 2017)."
  }, {
    "heading": "5 Related Work",
    "text": "Semantic role labeling was pioneered by Gildea and Jurafsky (2002), also known as shallow semantic parsing. In early works of SRL, considerable attention has been paid to feature engineering (Pradhan et al., 2005; Zhao and Kit, 2008; Zhao et al., 2009a,b,c; Li et al., 2009; Björkelund et al., 2009; Zhao et al., 2013). Along with the the impressive success of deep neural networks (Zhang et al., 2016; Cai and Zhao, 2016; Qin et al., 2016; Wang et al., 2016b,a; Zhang et al., 2018; Li et al., 2018; Huang et al., 2018), a series of neural SRL systems have been proposed. For instance, Foland and Martin (2015) presented a semantic role labeler using convolutional and time-domain neural networks. FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation\napplying tensor-based approach.\nRecently, people have attempted to build endto-end systems for span SRL without syntactic input (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Similarly, Marcheggiani et al. (2017) also proposed a syntax-agnostic model for dependency SRL and obtained favorable results. Despite the success of syntax-agnostic models, there are several works focus on leveraging the advantages of syntax. Roth and Lapata (2016) employed dependency path embedding to model syntactic information and exhibited a notable success. Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into a neural SRL model. Qian et al. (2017) proposed SALSTM to model the whole tree structure of dependency relation in an architecture engineering way.\nBesides, syntax encoding has also successfully promoted other NLP tasks. Tree-LSTM (Tai et al., 2015) is a variant of the standard LSTM that can encode a dependency tree with arbitrary branching factors, which has shown effectiveness on semantic relatedness and the sentiment classification tasks. In this work, we extend the Tree-LSTM with a relation specific gate and employ it to recursively encode the syntactic dependency tree for SRL. RCNN (Zhu et al., 2015) is an extension of the recursive neural network (Socher et al., 2010) which has been popularly used to encode trees with fixed branching factors. The RCNN is able to encode a tree structure with arbitrary number of factors and is useful in a re-ranking model for dependency parsing (Zhu et al., 2015).\nIn our experiments, we simplify and reformulate the RCNN model. However, the simplified model performs poorly on the development and the test sets. The reason might be that the RCNN model with a single global composition parameter is too simple to cover all types of syntactic relation in a dependency tree. Because of the poor performance of the modified RCNN, we do not include it in this work. Considering there might be other approach to incorporate the recursive network in SRL model, we leave it as our future work and just provide a brief discussion here.\nIn this work, we extend existing methods and introduce Tree-LSTM for incorporating syntax into SRL. Rather than proposing completely new model, we synthesize these techniques and present a unified framework to take genuine superiority of syntactic information."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper presents a unified neural framework for dependency-based SRL, effectively incorporating syntactic information by directly modeling syntax based on syntactic parse tree. Rather than proposing completely new model, we extend existing models and apply tree-structured LSTM for SRL. Our approach significantly outperforms all previous models, achieving state-of-the-art results on the CoNLL-2009 benchmarks for both English and Chinese.\nOur experiments specially show that giving an enlarged performance gap from syntax-agnostic to syntax-aware setting, SRL can be further promoted with the help of deep enhanced representation and effective methods of integrating syntax. Furthermore, we explore the impact of the quality of syntactic input. The relevant results indicate that high-quality syntactic parse is more favorable to semantic role labeling."
  }],
  "year": 2018,
  "references": [{
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1533–1544,",
    "year": 2013
  }, {
    "title": "A high-performance syntactic and semantic dependency parser",
    "authors": ["Anders Björkelund", "Bohnet Bernd", "Love Hafdell", "Pierre Nugues."],
    "venue": "Proceedings of the 23rd International Conference on Com-",
    "year": 2010
  }, {
    "title": "Multilingual semantic role labeling",
    "authors": ["Anders Björkelund", "Love Hafdell", "Pierre Nugues."],
    "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 43–48, Boulder, Colorado.",
    "year": 2009
  }, {
    "title": "Neural word segmentation learning for Chinese",
    "authors": ["Deng Cai", "Hai Zhao."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 409–420.",
    "year": 2016
  }, {
    "title": "A full end-to-end semantic role labeler, syntacticagnostic over syntactic-aware",
    "authors": ["Jiaxun Cai", "Shexia He", "Zuchao Li", "Hai Zhao"],
    "venue": "In Proceedings of the 27th International Conference on Computational Linguistics (COLING",
    "year": 2018
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Semantic role labeling with neural network factors",
    "authors": ["Nicholas FitzGerald", "Oscar Tckstrm", "Kuzman Ganchev", "Dipanjan Das."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2015
  }, {
    "title": "Dependencybased semantic role labeling using convolutional neural networks",
    "authors": ["William Foland", "James Martin."],
    "venue": "Joint Conference on Lexical and Computational Semantics, pages 279–288.",
    "year": 2015
  }, {
    "title": "Automatic labeling of semantic roles",
    "authors": ["Daniel Gildea", "Daniel Jurafsky."],
    "venue": "Computational linguistics, 28(3):245–288.",
    "year": 2002
  }, {
    "title": "The necessity of parsing for predicate argument recognition",
    "authors": ["Daniel Gildea", "Martha Palmer."],
    "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 239–246, Philadelphia, Pennsylvania, USA.",
    "year": 2002
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
    "year": 2016
  }, {
    "title": "Deep semantic role labeling: What works and what’s next",
    "authors": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), pages 473–483, Vancou-",
    "year": 2017
  }, {
    "title": "Syntax for semantic role labeling, to be, or not to be",
    "authors": ["Shexia He", "Zuchao Li", "Hai Zhao", "Hongxiao Bai."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 2061–2071.",
    "year": 2018
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jrgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Moon ime: neural-based Chinese pinyin aided input method with customizable association",
    "authors": ["Yafang Huang", "Zuchao Li", "Zhuosheng Zhang", "Hai Zhao."],
    "venue": "Proceedings of ACL 2018, System Demonstrations, pages 140–145.",
    "year": 2018
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:313–327.",
    "year": 2016
  }, {
    "title": "Semisupervised classification with graph convolutional networks",
    "authors": ["Thomas N Kipf", "Max Welling."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "High-order lowrank tensors for semantic role labeling",
    "authors": ["Tao Lei", "Yuan Zhang", "Lluı́s Màrquez", "Alessandro Moschitti", "Regina Barzilay"],
    "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Improving nominal srl in Chinese language with verbal srl information and automatic predicate recognition",
    "authors": ["Junhui Li", "Guodong Zhou", "Hai Zhao", "Qiaoming Zhu", "Peide Qian."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in",
    "year": 2009
  }, {
    "title": "Seq2seq dependency parsing",
    "authors": ["Zuchao Li", "Jiaxun Cai", "Shexia He", "Hai Zhao."],
    "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3203–3214.",
    "year": 2018
  }, {
    "title": "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling",
    "authors": ["Diego Marcheggiani", "Anton Frolov", "Ivan Titov."],
    "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017),",
    "year": 2017
  }, {
    "title": "Encoding sentences with graph convolutional networks for semantic role labeling",
    "authors": ["Diego Marcheggiani", "Ivan Titov."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1506–1515,",
    "year": 2017
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
    "year": 2014
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
    "year": 2018
  }, {
    "title": "Semantic role labeling using different syntactic views",
    "authors": ["Sameer Pradhan", "Wayne Ward", "Kadri Hacioglu", "James Martin", "Daniel Jurafsky."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages",
    "year": 2005
  }, {
    "title": "The importance of syntactic parsing and inference in semantic role labeling",
    "authors": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."],
    "venue": "Computational Linguistics, 34(2):257–287.",
    "year": 2008
  }, {
    "title": "Syntax aware LSTM model for semantic role labeling",
    "authors": ["Feng Qian", "Lei Sha", "Baobao Chang", "Lu Chen Liu", "Ming Zhang."],
    "venue": "The Workshop on Structured Prediction for Natural Language Processing, pages 27–32.",
    "year": 2017
  }, {
    "title": "Implicit discourse relation recognition with contextaware character-enhanced embeddings",
    "authors": ["Lianhui Qin", "Zhisong Zhang", "Hai Zhao."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical",
    "year": 2016
  }, {
    "title": "Neural semantic role labeling with dependency path embeddings",
    "authors": ["Michael Roth", "Mirella Lapata."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1192–1202, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Knowledge-based semantic embedding for machine translation",
    "authors": ["Chen Shi", "Shujie Liu", "Shuo Ren", "Shi Feng", "Mu Li", "Ming Zhou", "Xu Sun", "Houfeng Wang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
    "year": 2016
  }, {
    "title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
    "authors": ["Richard Socher", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1–9.",
    "year": 2010
  }, {
    "title": "Training very deep networks",
    "authors": ["Rupesh K Srivastava", "Klaus Greff", "Jürgen Schmidhuber."],
    "venue": "Advances in neural information processing systems, pages 2377–2385.",
    "year": 2015
  }, {
    "title": "The conll 2008 shared task on joint parsing of syntactic and semantic dependencies",
    "authors": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Lluı́s Màrquez", "Joakim Nivre"],
    "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language",
    "year": 2008
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Deep semantic role labeling with self-attention",
    "authors": ["Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi."],
    "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Connecting phrase based statistical machine translation adaptation",
    "authors": ["Rui Wang", "Hai Zhao", "Bao-Liang Lu", "Masao Utiyama", "Eiichiro Sumita."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:",
    "year": 2016
  }, {
    "title": "A bilingual graph-based semantic model for statistical machine translation",
    "authors": ["Rui Wang", "Hai Zhao", "Sabine Ploux", "Bao-Liang Lu", "Masao Utiyama."],
    "venue": "IJCAI, pages 2950–2956.",
    "year": 2016
  }, {
    "title": "The value of semantic parse labeling for knowledge base question answering",
    "authors": ["Wen-tau Yih", "Matthew Richardson", "Chris Meek", "MingWei Chang", "Jina Suh."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
    "year": 2016
  }, {
    "title": "Probabilistic graph-based dependency parsing with convolutional neural network",
    "authors": ["Zhisong Zhang", "Hai Zhao", "Lianhui Qin."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol-",
    "year": 2016
  }, {
    "title": "Subword-augmented embedding for cloze reading comprehension",
    "authors": ["Zhuosheng Zhang", "Yafang Huang", "Hai Zhao."],
    "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 1802–1814.",
    "year": 2018
  }],
  "id": "SP:6b153288ac10a02bf520e63f2f4b66e025e28545",
  "authors": [{
    "name": "Zuchao Li",
    "affiliations": []
  }, {
    "name": "Shexia He",
    "affiliations": []
  }, {
    "name": "Jiaxun Cai",
    "affiliations": []
  }, {
    "name": "Zhuosheng Zhang",
    "affiliations": []
  }, {
    "name": "Hai Zhao",
    "affiliations": []
  }, {
    "name": "Gongshen Liu",
    "affiliations": []
  }, {
    "name": "Linlin Li",
    "affiliations": []
  }, {
    "name": "Luo Si",
    "affiliations": []
  }],
  "abstractText": "Brussels, Belgium, October 31 November 4, 2018. c ©2018 Association for Computational Linguistics 2401 A Unified Syntax-aware Framework for Semantic Role Labeling Zuchao Li1,2,∗, Shexia He1,2,∗, Jiaxun Cai, Zhuosheng Zhang, Hai Zhao1,2,†, Gongshen Liu, Linlin Li, Luo Si Department of Computer Science and Engineering, Shanghai Jiao Tong University Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China School of Cyber Security, Shanghai Jiao Tong University, China Alibaba Group, Hangzhou, China {charlee,heshexia,caijiaxun,zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, lgshen@sjtu.edu.cn, {linyan.lll,luo.si}@alibaba-inc.com Abstract",
  "title": "A Unified Syntax-aware Framework for Semantic Role Labeling"
}