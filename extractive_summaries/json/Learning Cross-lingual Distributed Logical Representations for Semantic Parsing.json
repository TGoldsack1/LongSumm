{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 673–679 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n673"
  }, {
    "heading": "1 Introduction",
    "text": "Semantic parsing, one of the classic tasks in natural language processing (NLP), has been extensively studied in the past few years (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi et al., 2015). With the development of datasets annotated in different languages, learning semantic parsers from such multilingual datasets also attracted attention of researchers (Susanto and Lu, 2017a). However, how to make use of such cross-lingual data to perform cross-lingual semantic parsing – using data annotated for one language to help improve the performance of another lan-\nguage remains a research question that is largely under-explored.\nPrior work (Chan et al., 2007) shows that semantically equivalent words coming from different languages may contain shared semantic level information, which will be helpful for certain semantic processing tasks. In this work, we propose a simple method to learn the distributed representations for output structured semantic representations which allow us to capture cross-lingual features. Specifically, following previous work (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b), we adopt a commonly used tree-shaped form as the underlying meaning representation where each tree node is a semantic unit. Our objective is to learn for each semantic unit a distributed representation useful for semantic parsing, based on multilingual datasets. Figure 1 depicts an instance of such tree-shaped semantic representations, which correspond to the two semantically equivalent sentences in English and German below it.\nFor such structured semantics, we consider each semantic unit separately. We learn distributed rep-\nresentations for individual semantic unit based on multilingual datasets where semantic representations are annotated with different languages. Such distributed representations capture shared information cross different languages. We extend two existing monolingual semantic parsers (Lu, 2015; Susanto and Lu, 2017b) to incorporate such crosslingual features. To the best of our knowledge, this is the first work that exploits cross-lingual embeddings for logical representations for semantic parsing. Our system is publicly available at http://statnlp.org/research/sp/."
  }, {
    "heading": "2 Related Work",
    "text": "Many research efforts on semantic parsing have been made, such as mapping sentences into lambda calculus forms based on CCG (Artzi and Zettlemoyer, 2011; Artzi et al., 2014; Kwiatkowski et al., 2011), modeling dependencybased compositional semantics (Liang et al., 2011; Zhang et al., 2017), or transforming sentences into tree structured semantic representations (Lu, 2015; Susanto and Lu, 2017b). With the development of multilingual datasets, systems for multilingual semantic parsing are also developed. Jie and Lu (2014) employed majority voting to combine outputs from different parsers for certain languages to perform multilingual semantic parsing. Susanto and Lu (2017a) presented an extension of one existing neural parser, SEQ2TREE (Dong and Lapata, 2016), by developing a shared attention mechanism for different languages to conduct multilingual semantic parsing. Such a model allows two types of input signals: single source SL-SINGLE and multi-source SL-MULTI. However, semantic parsing with cross-lingual features has not been explored, while many recent works in various NLP tasks show the effectiveness of shared information cross different languages. Examples include semantic role labeling (Kozhevnikov and Titov, 2013), information extraction (Wang et al., 2013; Pan et al., 2017; Ni et al., 2017), and question answering (Joty et al., 2017), which motivate this work.\nOur work involves exploiting distributed output representations for improved structured predictions, which is in line with works of (Srikumar and Manning, 2014; Rocktäschel et al., 2014; Xiao and Guo, 2015). The work of (Rocktäschel et al., 2014) is perhaps the most related to this research. The authors first map first-order logical statements\nproduced by a semantic parser or an information extraction system into expressions in tensor calculus. They then learn low-dimensional embeddings of such statements with the help of a given logical knowledge base consisting of first-order rules so that the learned representations are consistent with these rules. They adopt stochastic gradient descent (SGD) to conduct optimizations. This work learns distributed representations of logical forms from cross-lingual data based on co-occurrence information without relying on external knowledge bases."
  }, {
    "heading": "3 Approach",
    "text": ""
  }, {
    "heading": "3.1 Semantic Parser",
    "text": "In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015). The parser was designed based on the hybrid tree representation (HT-G) originally introduced in (Lu et al., 2008). The hybrid tree is a joint representation encoding both sentence and semantics that aims to capture the interactions between words and semantic units. A discriminative hybrid tree (HT-D) (Lu, 2014, 2015) learns the optimal latent wordsemantics correspondence where every word in the input sentence is associated with a semantic unit. Such a model allows us to incorporate rich features and long-range dependencies. Recently, Susanto and Lu (2017b) extended HT-D by attaching neural architectures, resulting in their neural hybrid tree (HT-D (NN)).\nSince the correct correspondence between words and semantics is not explicitly given in the training data, we regard the hybrid tree representation as a latent variable. Formally, for each sentence n with its semantic representation m from the training set, we assume the joint representation (a hybrid tree) is h. Now we can define a discriminative log-linear model as follows:\nPΛ(m|n) = ∑\nh∈H(n,m)\nPΛ(m,h|n)\n=\n∑ h∈H(n,m) e\nFΛ(n,m,h)∑ m′,h′∈H(n,m′) e FΛ(n,m′,h′)) (1)\nFΛ(n,m,h)) = Λ · Φ(n,m,h)) (2)\nwhereH(n,m) returns the set of all possible joint representations that contain both n and m exactly, and F is a scoring function that is calculated as a\ndot product between a feature function Φ defined over tuple (m, n, h) and a weight vector Λ.\nTo incorporate neural features, HT-D (NN) defines the following scoring function:\nFΛ,Θ(n,m,h)) = Λ · Φ(n,m,h) + GΘ(n,m,h) (3)\nwhere Θ is the set of parameters of the neural networks and G is the neural scoring function over the (n,m,h) tuple (Susanto and Lu, 2017b). Specifically, the neural features are defined over a fixed-size window surrounding a word in n paired with its immediately associated semantic unit. Following the work (Susanto and Lu, 2017b), we denote the window size as J ∈ {0, 1, 2}."
  }, {
    "heading": "3.2 Cross-lingual Distributed Semantic Representations",
    "text": "A multilingual dataset used in semantic parsing comes with instances consisting of logical forms annotated with sentences from multiple different languages. In this work, we aim to learn one monolingual semantic parser for each language, while leveraging useful information that can be extracted from other languages. Our setup is as follows. Each time, we train the parser for a target language and regard the other languages as auxiliary languages. To learn cross-lingual distributed semantic representations from such data, we first combine all data involving all auxiliary languages to form a large dataset. Next, for each target language, we construct a semantics-word co-occurrence matrix M ∈ Rm×n (where m is the number of unique semantic units, n is the number of unique words in the combined dataset). Each entry is the number of co-occurrences for a particular (semantic unit-word) pair. We will use this matrix to learn a low-dimensional cross-lingual representation for each semantic unit. To do so, we first apply singular value decomposition (SVD) to this matrix, leading to:\nM = UΣV∗ (4)\nwhere U ∈ Rm×m and V ∈ Rn×m are unitary matrices, V∗ is the conjugate transpose of V, and Σ ∈ Rm×m is a diagonal matrix. We truncate the diagonal matrix Σ and left multiply it with U:\nUΣ̃ (5)\nwhere Σ̃ ∈ Rm×d is a matrix that consists of only the left d columns of Σ, containing the d largest\nsingular values. We leave the rank d as a hyperparameter. Each row in the above matrix is a ddimensional vector, giving a low-dimensional representation for one semantic unit. Such distributed output representations can be readily used as continuous features in Φ(n,m,h)."
  }, {
    "heading": "3.3 Training and Decoding",
    "text": "During the training process, we optimize the objective function defined over the training set as:\nL(Λ,Θ) = ∑ i log ∑\nh∈H(ni,mi)\neFΛ,Θ(ni,mi,h)\n− ∑ i log ∑\nm′,h′∈H(ni,m′)\neFΛ,Θ(ni,m ′,h′) (6)\nWe follow the dynamic programming approach used in (Susanto and Lu, 2017b) to perform efficient inference, and follow the same optimization strategy as described there.\nIn the decoding phase, we are given a new input sentence n, and find the optimal semantic tree m∗:\nm∗ = arg max m,h∈H(n,m) FΛ,Θ(n,m,h) (7)\nAgain, the above equation can be efficiently computed by dynamic programming (Susanto and Lu, 2017b)."
  }, {
    "heading": "4 Experiments and Results",
    "text": ""
  }, {
    "heading": "4.1 Datasets and Settings",
    "text": "We evaluate our approach on the standard GeoQuery dataset annotated in eight languages (Wong and Mooney, 2006; Jones et al., 2012; Susanto and Lu, 2017b). We follow a standard practice for evaluations which has been adopted in the literature (Lu, 2014, 2015; Susanto and Lu, 2017b). Specifically, to evaluate the proposed model, predicted outputs are transformed into Prolog queries. An output is considered as correct if answers that queries retrieve from GeoQuery database are the same as the gold ones . The dataset consists of 880 instances. In all experiments, we follow the experimental settings and procedures in (Lu, 2014,\n2015; Susanto and Lu, 2017b). In particular, we use 600 instances for training and 280 for test and set the maximum optimization iteration to 150. In order to tune the rank d, we randomly select 80% of the training instances for learning and use the rest 20% for development. We report the value of d for each language in Table 1 and the F1 scores on the development set."
  }, {
    "heading": "4.2 Results",
    "text": "We compare our models against different existing systems, especially the two baselines HT-D (Lu, 2015) and HT-D (NN) (Susanto and Lu, 2017b) with different word window sizes J ∈ {0, 1, 2}.\nWASP (Wong and Mooney, 2006) is a semantic parser based on statistical phrase-based machine translation. UBL-S (Kwiatkowski et al., 2010) induced probabilistic CCG grammars with higherorder unification that allowed to construct general logical forms for input sentences. TREETRANS (Jones et al., 2012) is built based on a Bayesian inference framework. We run WASP, UBL-S, HT-G, UBL-S, SEQ2TREE and SL-SINGLE 1 for comparisons. Note that there exist multiple versions of logical representations used in the GEOQUERY dataset. Specifically, one version is based on lambda calculus expression, and the other is based on the variable free tree-shaped represen-\n1 Note that in Dong and Lapata (2016), they adopted a different split – using 680 instances as training examples and the rest 200 for evaluation. We ran the released source code for SEQ2TREE (Dong and Lapata, 2016) over eight different languages. For each language, we repeated the experiments 3 times with different random seed values, and reported the average results as shown in Table 2 to make comparisons. Likewise, we ran SL-SINGLE following the same procedure.\ntation. We use the latter representation in this work, while the SEQ2TREE and SL-SINGLE employ the lambda calculus expression. It was noted in Kwiatkowski et al. (2010); Lu (2014) that evaluations based on these two versions are not directly comparable – the version that uses tree-shaped representations appears to be more challenging. We do not compare against (Jie and Lu, 2014) due to their different setup from ours.2\nTable 2 shows results that we have conducted on eight different languages. The highest scores are highlighted. We can observe that when distributed logical representations are included, both HT-D and HT-D (NN) can lead to competitive results. Specifically, when such features are included, evaluation results for 5 out of 8 languages get improved.\nWe found that the shared information cross different languages could guide the model so that it can make more accurate predictions, eliminating certain semantic level ambiguities associated with the semantic units. This is exemplified by a real instance from the English portion of the dataset:\nInput: Which states have a river? Gold: answer(state(loc(river(all)))) Output: answer(state(traverse(river(all)))) Output (+O): answer(state(loc(river(all))))\n2 They trained monolingual semantic parsers. In the evaluation phase, they fed parallel text from different languages to each individual semantic parser, then employed a majority voting based ensemble method to combine predictions. Differently, we train monolingual semantic parsers augmented with cross-lingual distributed semantic information. In the evaluation phase, we only have one monolingual semantic parser. Hence, these two efforts are not directly comparable.\nHere the input sentence in English is “Which states have a river?”, and the correct output is shown below the sentence. Output is the prediction from HT-D (NN) and Output (+O) is the parsing result given by HT-D (NN+O) where the learned cross-lingual representations of semantics are included. We observe that, by introducing our learned cross-lingual semantic information, the system is able to distinguish the two semantically related concepts, loc (located in) and traverse (traverse), and further make more promising predictions.\nInterestingly, for German, the results become much lower when such features are included, indicating such features are not helpful in the learning process when such a language is considered. Reasons for this need further investigations. We note, however, previously it was also reported in the literature that the behavior of the performance associated with this language is different than other languages in the presence of additional features (Lu, 2014)."
  }, {
    "heading": "4.3 Visualizing Output Representations",
    "text": "To qualitatively understand how good the learned distributed representations are, we also visualize the learned distributed representations for semantic units. In the Figure 2, we plot the embeddings of a small set of semantic units which are learned from all languages other than English. Each representation is a 30-dimensional vector and is projected into a 2-dimensional space using Barnes-Hut-SNE (Maaten, 2013) for visualization. In general, we found that semantic units expressing similar meanings tend to appear to-\ngether. For example, the two semantic units STATE : smallest one ( density (STATE)) and STATE : smallest one ( population (STATE)) share similar representations. However, we also found that occasionally semantic units conveying opposite meanings are also grouped together. This reveals the limitations associated with such a simple cooccurrence based approach for learning distributed representations for logical expressions."
  }, {
    "heading": "5 Conclusions",
    "text": "In this paper, we empirically show that the distributed representations of logical expressions learned from multilingual datasets for semantic parsing can be exploited to improve the performance of a monolingual semantic parser. Our approach is simple, relying on an SVD over semantics-word co-occurrence matrix for finding such distributed representations for semantic units. Future directions include investigating better ways of learning such distributed representations as well as learning such distributed representations and semantic parsers in a joint manner."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the three anonymous ACL reviewers for their thoughtful and constructive comments. We would also like to thank Raymond H. Susanto for his help on this work. This work is supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 2 Project MOE2017-T2-1-156, and is partially supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 1 SUTDT12015008."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning compact lexicons for ccg semantic parsing",
    "authors": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."],
    "venue": "Proc. of EMNLP. https://doi.org/10.3115/v1/D14-1134.",
    "year": 2014
  }, {
    "title": "Broad-coverage ccg semantic parsing with amr",
    "authors": ["Yoav Artzi", "Kenton Lee", "Luke S. Zettlemoyer."],
    "venue": "Proc. of EMNLP. https://doi.org/10.18653/v1/D15-1198.",
    "year": 2015
  }, {
    "title": "Bootstrapping semantic parsers from conversations",
    "authors": ["Yoav Artzi", "Luke S. Zettlemoyer."],
    "venue": "Proc. of EMNLP. http://www.aclweb.org/anthology/D111039.",
    "year": 2011
  }, {
    "title": "Word sense disambiguation improves statistical machine translation",
    "authors": ["Yee Seng Chan", "Hwee Tou Ng", "David Chiang."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P07-1005.",
    "year": 2007
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proc. of ACL. https://doi.org/10.18653/v1/P16-1004.",
    "year": 2016
  }, {
    "title": "Multilingual semantic parsing and code-switching",
    "authors": ["Long Duong", "Hadi Afshar", "Dominique Estival", "Glen Pink", "Philip Cohen", "Mark Johnson."],
    "venue": "Proc. of CoNLL. https://doi.org/10.18653/v1/K171038.",
    "year": 2017
  }, {
    "title": "Multilingual semantic parsing : Parsing multiple languages into semantic representations",
    "authors": ["Zhanming Jie", "Wei Lu."],
    "venue": "Proc. of COLING. http://www.aclweb.org/anthology/C14-1122.",
    "year": 2014
  }, {
    "title": "Semantic parsing with bayesian tree transducers",
    "authors": ["Bevan Jones", "Mark Johnson", "Sharon Goldwater."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P12-1051.",
    "year": 2012
  }, {
    "title": "Cross-language learning with adversarial neural networks",
    "authors": ["Shafiq Joty", "Preslav Nakov", "Lluı́s Màrquez", "Israa Jaradat"],
    "venue": "In Proc. of CoNLL",
    "year": 2017
  }, {
    "title": "Cross-lingual transfer of semantic role labeling models",
    "authors": ["Mikhail Kozhevnikov", "Ivan Titov."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P13-1117.",
    "year": 2013
  }, {
    "title": "Lexical generalization in ccg grammar induction for semantic parsing",
    "authors": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proc. of EMNLP. http://www.aclweb.org/anthology/D11-1140.",
    "year": 2011
  }, {
    "title": "Inducing probabilistic ccg grammars from logical form with higher-order unification",
    "authors": ["Tom Kwiatkowski", "Luke S. Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proc. of EMNLP. http://www.aclweb.org/anthology/D10-1119.",
    "year": 2010
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael Jordan", "Dan Klein."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P11-1060.",
    "year": 2011
  }, {
    "title": "Semantic parsing with relaxed hybrid trees",
    "authors": ["Wei Lu."],
    "venue": "Proc. of EMNLP. https://doi.org/10.3115/v1/D14-1137.",
    "year": 2014
  }, {
    "title": "Constrained semantic forests for improved discriminative semantic parsing",
    "authors": ["Wei Lu."],
    "venue": "Proc. of ACL. https://doi.org/10.3115/v1/P15-2121.",
    "year": 2015
  }, {
    "title": "A generative model for parsing natural language to meaning representations",
    "authors": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S. Zettlemoyer."],
    "venue": "Proc. of EMNLP. https://doi.org/10.3115/1613715.1613815.",
    "year": 2008
  }, {
    "title": "Barnes-hut-sne",
    "authors": ["Laurens van der Maaten."],
    "venue": "Proc. of ICLR. https://arxiv.org/pdf/1301.3342.pdf.",
    "year": 2013
  }, {
    "title": "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
    "authors": ["Jian Ni", "Georgiana Dinu", "Radu Florian."],
    "venue": "Proc. of ACL. https://doi.org/10.18653/v1/P17-1135.",
    "year": 2017
  }, {
    "title": "Crosslingual name tagging and linking for 282 languages",
    "authors": ["Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji."],
    "venue": "Proc. of ACL. https://doi.org/10.18653/v1/P171178.",
    "year": 2017
  }, {
    "title": "Lowdimensional embeddings of logic",
    "authors": ["Tim Rocktäschel", "Matko Bošnjak", "Sameer Singh", "Sebastian Riedel."],
    "venue": "Proc. of the ACL 2014 Workshop on Semantic Parsing. https://doi.org/10.3115/v1/W14-2409.",
    "year": 2014
  }, {
    "title": "Learning distributed representations for structured output prediction",
    "authors": ["Vivek Srikumar", "Christopher D Manning."],
    "venue": "Proc. of NIPS. http://papers.nips.cc/paper/5323-learningdistributed-representations-for-structured-output-",
    "year": 2014
  }, {
    "title": "Neural architectures for multilingual semantic parsing",
    "authors": ["Raymond Hendy Susanto", "Wei Lu."],
    "venue": "Proc. of ACL. https://doi.org/10.18653/v1/P172007.",
    "year": 2017
  }, {
    "title": "Transfer learning based cross-lingual knowledge extraction for wikipedia",
    "authors": ["Zhigang Wang", "Zhixing Li", "Juanzi Li", "Jie Tang", "Jeff Z. Pan."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P13-1063.",
    "year": 2013
  }, {
    "title": "Learning for semantic parsing with statistical machine translation",
    "authors": ["Yuk Wah Wong", "Raymond Mooney."],
    "venue": "Proc. of NAACL. https://doi.org/10.3115/1220835.1220891.",
    "year": 2006
  }, {
    "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
    "authors": ["Yuk Wah Wong", "Raymond Mooney."],
    "venue": "Proc. of ACL. http://www.aclweb.org/anthology/P07-1121.",
    "year": 2007
  }, {
    "title": "Learning hidden markov models with distributed state representations for domain adaptation",
    "authors": ["Min Xiao", "Yuhong Guo."],
    "venue": "Proc. of ACL-IJCNLP. https://doi.org/10.3115/v1/P15-2086.",
    "year": 2015
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilis",
    "authors": ["Luke S Zettlemoyer", "Michael Collins"],
    "year": 2005
  }, {
    "title": "Macro grammars and holistic triggering for efficient semantic parsing",
    "authors": ["Yuchen Zhang", "Panupong Pasupat", "Percy Liang."],
    "venue": "Proc. of EMNLP.",
    "year": 2017
  }],
  "id": "SP:c0ded46a68619f8275495503ffc0e24d70f16892",
  "authors": [{
    "name": "Yanyan Zou",
    "affiliations": []
  }, {
    "name": "Wei Lu",
    "affiliations": []
  }],
  "abstractText": "With the development of several multilingual datasets used for semantic parsing, recent research efforts have looked into the problem of learning semantic parsers in a multilingual setup (Duong et al., 2017; Susanto and Lu, 2017a). However, how to improve the performance of a monolingual semantic parser for a specific language by leveraging data annotated in different languages remains a research question that is under-explored. In this work, we present a study to show how learning distributed representations of the logical forms from data annotated in different languages can be used for improving the performance of a monolingual semantic parser. We extend two existing monolingual semantic parsers to incorporate such cross-lingual distributed logical representations as features. Experiments show that our proposed approach is able to yield improved semantic parsing results on the standard multilingual GeoQuery dataset.",
  "title": "Learning Cross-lingual Distributed Logical Representations for Semantic Parsing"
}