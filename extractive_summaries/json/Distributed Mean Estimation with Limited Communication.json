{
  "sections": [{
    "heading": "1. Introduction",
    "text": ""
  }, {
    "heading": "1.1. Background",
    "text": "Given n vectors Xn def= X 1 , X 2 . . . , Xn 2 Rd that reside on n clients, the goal of distributed mean estimation is to estimate the mean of the vectors:\n¯X def =\n1\nn\nnX\ni=1\nXi. (1)\nThis basic estimation problem is used as a subroutine in several learning and optimization tasks where data is distributed across several clients. For example, in Lloyd’s algorithm (Lloyd, 1982) for k-means clustering, if data is distributed across several clients, the server needs to compute\n1Google Research, New York, NY, USA 2Google Research, Seattle, WA, USA. Correspondence to: Ananda Theertha Suresh <theertha@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe means of all clusters in each update step. Similarly, for PCA, if data samples are distributed across several clients, then for the power-iteration method, the server needs to average the output of all clients in each step.\nRecently, algorithms involving distributed mean estimation have been used extensively in training large-scale neural networks and other statistical models (McDonald et al., 2010; Povey et al., 2014; Dean et al., 2012; McMahan et al., 2016; Alistarh et al., 2016). In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model. The clients then update the model independently based on their local data. The updates (usually in the form of gradients) are then sent to a server, where they are averaged and used to update the global model. A critical step in all of the above algorithms is to estimate the mean of a set of vectors as in Eq. (1).\nOne of the main bottlenecks in distributed algorithms is the communication cost. This has spurred a line of work focusing on communication cost in learning (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani & Shamir, 2015; Chen et al., 2016). The communication cost can be prohibitive for modern applications, where each client can be a low-power and low-bandwidth device such as a mobile phone (Konečnỳ et al., 2016). Given such a wide set of applications, we study the basic problem of achieving the optimal minimax rate in distributed mean estimation with limited communication.\nWe note that our model and results differ from previous works on mean estimation (Zhang et al., 2013; Garg et al., 2014; Braverman et al., 2016) in two ways: previous works assume that the data is generated i.i.d. according to some distribution; we do not make any distribution assumptions on data. Secondly, the objective in prior works is to estimate the mean of the underlying statistical model; our goal is to estimate the empirical mean of the data."
  }, {
    "heading": "1.2. Model",
    "text": "Our proposed communication algorithms are simultaneous and independent, i.e., the clients independently send data to the server and they can transmit at the same time. In any independent communication protocol, each client transmits a function of Xi (say f(Xi)), and a central server estimates the mean by some function of f(X\n1 ), f(X 2 ), . . . , f(Xn).\nLet ⇡ be any such protocol and let Ci(⇡, Xi) be the expected number of transmitted bits by the i-th client during protocol ⇡, where throughout the paper, expectation is over the randomness in protocol ⇡.\nThe total number of bits transmitted by all clients with the protocol ⇡ is\nC(⇡, Xn) def= nX\ni=1\nCi(⇡, Xi).\nLet the estimated mean be ˆ¯X . For a protocol ⇡, the MSE of the estimate is\nE(⇡, Xn) = E  ˆ¯X ¯X 2\n2\n.\nWe allow the use of both private and public randomness. Private randomness refers to random values that are generated by each machine separately, and public randomness refers to a sequence of random values that are shared among all parties1.\nThe proposed algorithms work for any Xn. To measure the minimax performance, without loss of generality, we restrict ourselves to the scenario where each Xi 2 Sd, the ball of radius 1 in Rd, i.e., X 2 Sd iff\n||X|| 2  1, where ||X||\n2\ndenotes the ` 2 norm of the vector X . For a protocol ⇡, the worst case error for all Xn 2 Sd is\nE(⇡, Sd) def= max Xn:Xi2Sd 8i E(⇡, Xn).\nLet ⇧(c) denote the set of all protocols with communication cost at most c. The minimax MSE is\nE(⇧(c), Sd) def= min ⇡2⇧(c) E(⇡, Sd)."
  }, {
    "heading": "1.3. Results and discussion",
    "text": ""
  }, {
    "heading": "1.3.1. ALGORITHMS",
    "text": "We first analyze the MSE E(⇡, Xn) for three algorithms, when C(⇡, Xn) = ⇥(nd), i.e., each client sends a constant number of bits per dimension.\nStochastic uniform quantization. In Section 2.1, as a warm-up we first show that a naive stochastic binary quantization algorithm (denoted by ⇡sb) achieves an MSE of\nE(⇡sb, Xn) = ⇥ d\nn · 1 n\nnX\ni=1\n||Xi||2 2\n! ,\n1In the absence of public randomness, the server can communicate a random seed that can be used by clients to emulate public randomness.\nand C(⇡sb, Xn) = n · (d + ˜O(1))2, i.e., each client sends one bit per dimension. We further show that this bound is tight. In many practical scenarios, d is much larger than n and the above error is prohibitive (Konečnỳ et al., 2016).\nA natural way to decease the error is to increase the number of levels of quantization. If we use k levels of quantization, in Theorem 2, we show that the error deceases as\nE(⇡sk, Xn) = O\nd\nn(k 1)2 · 1 n\nnX\ni=1\n||Xi||2 2\n! . (2)\nHowever, the communication cost would increase to C(⇡sk, Xn) = n · (ddlog\n2 ke + ˜O(1)) bits, which can be expensive, if we would like the MSE to be o(d/n).\nIn order to reduce the communication cost, we propose two approaches.\nStochastic rotated quantization: We show that preprocessing the data by a random rotation reduces the mean squared error. Specifically, in Theorem 3, we show that this new scheme (denoted by ⇡srk) achieves an MSE of\nE(⇡srk, Xn) = O\nlog d\nn(k 1)2 · 1 n\nnX\ni=1\n||Xi||2 2\n! , 3\nand has a communication cost of C(⇡srk, Xn) = n · (ddlog\n2 ke + ˜O(1)). Note that the new scheme achieves much smaller MSE than naive stochastic quantization for the same communication cost.\nVariable length coding: Our second approach uses the same quantization as ⇡sk but encodes levels via variable length coding. Instead of using dlog\n2 ke bits per dimension, we show that using variable length encoding such as arithmetic coding to compress the data reduces the communication cost significantly. In particular, in Theorem 4 we show that there is a scheme (denoted by ⇡svk) such that\nC(⇡svk, Xn) = O(nd(1 + log(k2/d+ 1)) + ˜O(n)), (3) and E(⇡svk, Xn) = E(⇡sk, Xn). Hence, setting k = p d in Eqs. 2 and 3 yields\nE(⇡svk, Xn) = O 1 n · 1 n\nnX\ni=1\n||Xi||2 2\n! ,\nand with ⇥(nd) bits of communication i.e., constant number of bits per dimension per client. Of the three protocols, ⇡svk has the best MSE for a given communication cost. Note that ⇡svk uses k quantization levels but still uses O(1) bits per dimension per client for all k  pd. Theoretically, while variable length coding has better guarantees, stochastic rotated quantization has several practical\n2We use ˜O(1) to denote O(log(dn)). 3All logarithms are to base e, unless stated.\nadvantages: it uses fixed length coding and hence can be combined with encryption schemes for privacy preserving secure aggregation (Bonawitz et al., 2016). It can also provide lower quantization error in some scenarios due to better constants (see Section 7 for details).\nConcurrent to this work, Alistarh et al. (2016) showed that stochastic quantization and Elias coding can be used to obtain communication-optimal SGD. Recently, Konečnỳ & Richtárik (2016) showed that ⇡sb can be improved further by optimizing the choice of stochastic quantization boundaries. However, their results depend on the number of bits necessary to represent a float, whereas ours do not."
  }, {
    "heading": "1.3.2. MINIMAX MSE",
    "text": "In the above protocols, all of the clients transmit the data. We augment these protocols with a sampling procedure, where only a random fraction of clients transmit data. We show that a combination of k-level quantization, variable length coding, and sampling can be used to achieve information theoretically optimal MSE for a given communication cost. In particular, combining Corollary 1 and Theorem 5 yields our minimax result: Theorem 1. There exists a universal constant t < 1 such that for communication cost c  ndt and n 1/t,\nE(⇧(c), Sd) = ⇥ ✓ min ✓ 1, d\nc\n◆◆ .\nThis result shows that the product of communication cost and MSE scales linearly in the number of dimensions.\nThe rest of the paper is organized as follows. We first analyze the stochastic uniform quantization technique in Section 2. In Section 3, we propose the stochastic rotated quantization technique, and in Section 4 we analyze arithmetic coding. In Section 5, we combine the above algorithm with a sampling technique and state the upper bound on the minimax risk, and in Section 6 we state the matching minimax lower bounds. Finally, in Section 7 we discuss some practical considerations and apply these algorithms on distributed power iteration and Lloyd’s algorithm."
  }, {
    "heading": "2. Stochastic uniform quantization",
    "text": ""
  }, {
    "heading": "2.1. Warm-up: Stochastic binary quantization",
    "text": "For a vector Xi, let Xmaxi = max1jd Xi(j) and similarly let Xmini = min1jd Xi(j). In the stochastic binary quantization protocol ⇡sb, for each client i, the quantized value for each coordinate j is generated independently with private randomness as\nYi(j) =\n( Xmaxi w.p.\nXi(j) Xmini Xmaxi Xmini ,\nXmini otherwise.\nObserve EYi(j) = Xi(j). The server estimates ¯X by\nˆ ¯X⇡sb = 1\nn\nnX\ni=1\nYi.\nWe first bound the communication cost of the this protocol.\nLemma 1. There exists an implementation of stochastic binary quantization that uses d + ˜O(1) bits per client and hence C(⇡sb, Xn)  n · ⇣ d+ ˜O(1) ⌘ .\nProof. Instead of sending vectors Yi, clients transmit two real values Xmaxi and Xmini (to a desired error) and a bit vector Y 0i such that Y 0i (j) = 1 if Yi = Xmaxi and 0 otherwise. Hence each client transmits d + 2r bits, where r is the number of bits to transmit the real value to a desired error.\nLet B be the maximum norm of the underlying vectors. To bound r, observe that using r bits, one can represent a number between B and B to an error of B/2r 1. Thus using 3 log\n2 (dn) + 1 bits one can represent the minimum and maximum to an additive error of B/(nd)3. This error in transmitting minimum and maximum of the vector does not affect our calculations and we ignore it for simplicity. We note that in practice, each dimension of Xi is often stored as a 32 bit or 64 bit float, and r should be set as either 32 or 64. In this case, using an even larger r does not further reduce the error.\nWe now compute the estimation error of this protocol.\nLemma 2. For any set of vectors Xn,\nE(⇡sb, Xn) = 1 n2\nnX\ni=1\ndX\nj=1\n(Xmaxi Xi(j))(Xi(j) Xmini ).\nProof.\nE(⇡sb, Xn) = E ˆ¯X ¯X\n2\n2\n=\n1 n2 E\nnX\ni=1\n(Yi Xi)\n2\n2\n=\n1\nn2\nnX\ni=1\nE ||Yi Xi||2 2 ,\nwhere the last equality follows by observing that Yi Xi, 8i, are independent zero mean random variables. The proof follows by observing that for every i,\nE ||Yi Xi||2 2 =\ndX\nj=1\nE[(Yi(j) Xi(j))2]\n=\ndX\nj=1\n(Xmaxi Xi(j))(Xi(j) Xmini ).\nLemma 2 implies the following upper bound. Lemma 3. For any set of vectors Xn,\nE(⇡sb, Xn)  d 2n · 1 n\nnX\ni=1\n||Xi||2 2 .\nProof. The proof follows by Lemma 2 observing that 8j\n(Xmaxi Xi(j))(Xi(j) Xmini )  (Xmaxi Xmini )2\n4\n,\nand (Xmaxi Xmini )2  2 ||Xi||2\n2\n. (4)\nWe also show that the above bound is tight: Lemma 4. There exists a set of vectors Xn such that\nE(⇡sb, Xn) d 2 2n · 1 n\nnX\ni=1\n||Xi||2 2 .\nProof. For every i, let Xi be defined as follows. Xi(1) = 1/ p 2, Xi(2) = 1/ p 2, and for all j > 2, Xi(j) = 0. For every i, Xmaxi = 1p 2 and Xmini = 1p 2\n. Substituting these bounds in the conclusion of Lemma 2 (which is an equality) yields the theorem.\nTherefore, the simple algorithm proposed in this section gives MSE ⇥(d/n). Such an error is too large for realworld use. For example, in the application of neural networks (Konečnỳ et al., 2016), d can be on the order of millions, yet n can be much smaller than that. In such cases, the MSE is even larger than the norm of the vector.\n2.2. Stochastic k-level quantization\nA natural generalization of binary quantization is k-level quantization. Let k be a positive integer larger than 2. We propose a k-level stochastic quantization scheme ⇡sk to quantize each coordinate. Recall that for a vector Xi, Xmaxi = max1jd Xi(j) and Xmini = min1jd Xi(j). For every integer r in the range [0, k), let\nBi(r) def = Xmini + rsi k 1 ,\nwhere si satisfies Xmini + si Xmaxi . A natural choice for si would be Xmaxi Xmini .4 The algorithm quantizes each coordinate into one of Bi(r)s stochastically. In ⇡sk, for the i-th client and j-th coordinate, if Xi(j) 2 [Bi(r), Bi(r + 1)),\nYi(j) =\n( Bi(r + 1) w.p.\nXi(j) Bi(r) Bi(r+1) Bi(r)\nBi(r) otherwise.\n4We will show in Section 4, however, a higher value of si and variable length coding has better guarantees.\nThe server estimates ¯X by\nˆ ¯X⇡sk = 1\nn\nnX\ni=1\nYi.\nAs before, the communication complexity of this protocol is bounded. The proof is similar to that of Lemma 1 and hence omitted.\nLemma 5. There exists an implementation of stochastic klevel quantization that uses ddlog(k)e+ ˜O(1) bits per client and hence C(⇡sk, Xn)  n · ⇣ ddlog\n2\nke+ ˜O(1) ⌘ .\nThe mean squared loss can be bounded as follows. Theorem 2. If Xmaxi Xmini  si  p 2 ||Xi||\n2 8i, then for any Xn, the ⇡sk protocol satisfies,\nE(⇡sk, Xn)  d 2n(k 1)2 · 1 n\nnX\ni=1\n||Xi||2 2 .\nProof.\nE(⇡sk, Xn) = E ˆ¯X ¯X\n2\n2\n=\n1 n2 E\nnX\ni=1\n(Yi Xi)\n2\n2\n=\n1\nn2\nnX\ni=1\nE ||Yi Xi||2 2  1 n2\nnX\ni=1\nd s2i\n4(k 1)2 , (5)\nwhere the last equality follows by observing Yi(j) Xi(j) is an independent zero mean random variable with E(Yi(j) Xi(j))2  s 2 i 4(k 1)2 . si  p 2 ||Xi|| 2\ncompletes the proof.\nWe conclude this section by noting that si = Xmaxi Xmini satisfies the conditions for the above theorem by Eq. (4)."
  }, {
    "heading": "3. Stochastic rotated quantization",
    "text": "We show that the algorithm of the previous section can be significantly improved by a new protocol. The motivation comes from the fact that the MSE of stochastic binary quantization and stochastic k-level quantization is O( dn (X max\ni Xmini )2) (the proof of Lemma 3 and Theorem 2 with si = Xmaxi Xmini ). Therefore the MSE is smaller when Xmaxi and Xmaxi are close. For example, when Xi is generated uniformly on the unit sphere, with\nhigh probability, Xmaxi Xmini is O ✓q log d d ◆ (Dasgupta\n& Gupta, 2003). In such case, E(⇡sk, Xn) is O( log dn ) instead of O( dn ). In this section, we show that even without any assumptions on the distribution of the data, we can “reduce” Xmaxi Xmini with a structured random rotation, yielding\nan O( log dn ) error. We call the method stochastic rotated quantization and denote it by ⇡srk.\nUsing public randomness, all clients and the central server generate a random rotation matrix (random orthogonal matrix) R 2 Rd⇥d according to some known distribution. Let Zi = RXi and ¯Z = R ¯X . In the stochastic rotated quantization protocol ⇡srk(R), clients quantize the vectors Zi instead of Xi and transmit them similar to ⇡srk. The server estimates ¯X by\nˆ ¯X⇡srk = R 1 ˆ\n¯Z, ˆ¯Z = 1\nn\nnX\ni=1\nYi.\nThe communication cost is same as ⇡sk and is given by Lemma 5. We now bound the MSE.\nLemma 6. For any Xn, E(⇡srk(R), Xn) is at most\nd 2n2(k 1)2 nX\ni=1\nER h (Zmaxi ) 2 + Zmini 2 i ,\nwhere Zi = RXi and for every i, let si = Zmaxi Zmini .\nProof.\nE(⇡srk, Xn) = E⇡ ˆ¯X ¯X\n2\n= E⇡ R 1 ˆ¯Z R 1 ¯Z\n2 (a) = E⇡ ˆ¯Z ¯Z 2\n(b) = ERE⇡  ˆ¯Z ¯Z 2 |Zn 1\n d 4n2(k 1)2\nnX\ni=1\nER[(Zmaxi Zmini )2],\nwhere the last inequality follows Eq. (5) and the value of si. (a) follows from the fact that rotation does not change the norm of the vector, and (b) follows from the tower law of expectation. The lemma follows from observing that\n(Zmaxi Zmini )2  2(Zmaxi )2 + 2(Zmini )2.\nTo obtain strong bounds, we need to find an orthogonal matrix R that achieves low (Zmaxi )2 and (Zmini )2. In addition, due to the fact that d can be huge in practice, we need a type of orthogonal matrix that permits fast matrix-vector products. Naive orthogonal matrices that support fast multiplication such as block-diagonal matrices often result in high values of (Zmaxi )2 and (Zmini )2. Motivated by recent works of structured matrices (Ailon & Chazelle, 2006; Yu et al., 2016), we propose to use a special type of orthogonal matrix R = HD, where D is a random diagonal matrix with i.i.d. Rademacher entries (±1 with probability 0.5). H is a Walsh-Hadamard matrix (Horadam, 2012). The WalshHadamard matrix of dimension 2m for m 2 N is given by\nthe recursive formula,\nH(21) =\n 1 1\n1 1 , H(2m) =  H(2m 1) H(2m 1) H(2m 1) H(2m 1) .\nBoth applying the rotation and inverse rotation take O(d log d) time and O(1) additional space (with an inplace algorithm). The next lemma bounds E (Zmaxi )\n2 and E Zmini 2 for this choice of R. The lemma is similar to that of Ailon & Chazelle (2006), and we give the proof in Appendix A for completeness. Lemma 7. Let R = HD, where D is a diagonal matrix with independent Radamacher random variables. For every i and every sequence Xn,\nE ⇥ (Zmini ) 2 ⇤ = E ⇥ (Zmaxi ) 2 ⇤  ||Xi|| 2 2 (2 log d+ 2)\nd .\nCombining the above two lemmas yields the main result. Theorem 3. For any Xn, ⇡srk(HD) protocol satisfies,\nE(⇡srk(HD), Xn)  2 log d+ 2 n(k 1)2 · 1 n\nnX\ni=1\n||Xi||2 2 ."
  }, {
    "heading": "4. Variable length coding",
    "text": "Instead of preprocessing the data via a rotation matrix as in ⇡srk, in this section we propose to use a variable length coding strategy to minimize the number of bits.\nConsider the stochastic k-level quantization technique. A natural way of transmitting Yi is sending the bin number for each coordinate, thus the total number of bits the algorithm sends per transmitted coordinate would be ddlog\n2 ke. This naive implementation is sub-optimal. Instead, we propose to further encode the transmitted values using universal compression schemes (Krichevsky & Trofimov, 1981; Falahatgar et al., 2015). We first encode hr, the number of times each quantized value r has appeared, and then use arithmetic or Huffman coding corresponding to the distribution pr = hrd . We denote this scheme by ⇡svk. Since we quantize vectors the same way in ⇡sk and ⇡svk, the MSE of ⇡svk is also given by Theorem 2. We now bound the communication cost. Theorem 4. Let si = p 2 ||Xi||. There exists an implementation of ⇡svk such that C(⇡svk, Xn) is at most\nn ✓ d ✓ 2 + log2 ✓ (k 1)2\n2d +\n5\n4\n◆◆ + k log2\n(d+ k)e k +\n˜O(1) ◆ .\nProof. As in Lemma 1, ˜O(1) bits are used to transmit the si’s and Xmini . Recall that hr is the number of coordinates that are quantized into bin r, and r takes k possible values. Furthermore, P r hr = d. Thus the number of bits\nnecessary to represent the hr’s is ⇠ log\n2 ✓ d+ k 1 k 1 ◆⇡  k log 2 (d+ k)e k .\nOnce we have compressed the hr’s, we use arithmetic coding corresponding to the distribution pr = hr/d to compress and transmit bin values for each coordinate. The total number of bits arithmetic coding uses is (MacKay, 2003)\nd k 1X\nr=0\nhr d log 2 d hr + 2.\nLet pr = hr/d, a = (k 1)Xmini , b = si, and =Pk 1 r=0 1/((a+ br) 2 + ). Note that\nX\nr\npr log 2\n1\npr =\nX\nr\npr log 2\n1/(((a+ br)2 + ) )\npr\n+\nX\nr\npr log 2 (((a+ br)2 + ) )\n X\nr\npr log 2 (((a+ br)2 + ) )\n log 2 (\nX\nr\npr(a+ br) 2\n+ ) + log 2\n,\nwhere the first inequality follows from the positivity of KLdivergence. Choosing = s2i , yields  4/s2i and hence, X\nr\npr log 2\n1 pr  log 2 (\nX\nr\npr(a+br) 2 +s2i )+log2(4/s 2 i ).\nNote that if Yi(j) belongs to bin r, (a + br)2 = (k 1)\n2Y 2i (j). Recall that hr is the number of coordinates quantized into bin r. Hence P r hr(a + br)\n2 is the scaled norm-square of Yi, i.e.,\nX\nr\nhr(a+ br) 2\n= (k 1)2 dX\nj=1\nY 2i (j)\n=\ndX\nj=1\n((Xi(j) + ↵(j))(k 1))2 ,\nwhere the ↵(j) = Yi(j) Xi(j). Taking expectations on both sides and using the fact that the ↵(j) are independent zero mean random variables over a range of si/(k 1), we get\nE X\nr\nhr(a+ br) 2 =\ndX\nj=1\nE(Xi(j)2 + ↵(j)2)(k 1)2\n ||Xi||2 2\n✓ (k 1)2 + d\n2\n◆ .\nUsing Jensen’s inequality yields the result.\nThus if k = p d + 1, the communication complexity is O(nd) and the MSE is O(1/n)."
  }, {
    "heading": "5. Communication MSE trade-off",
    "text": "In the above protocols, all the clients transmit and hence the communication cost scales linearly with n. Instead, we show that any of the above protocols can be combined by client sampling to obtain trade-offs between the MSE and the communication cost. Note that similar analysis also holds for sampling the coordinates.\nLet ⇡ be a protocol where the mean estimate is of the form:\nˆ ¯X = R 1 1\nn\nnX\ni=1\nYi. (6)\nAll three protocols we have discussed are of this form. Let ⇡p be the protocol where each client participates independently with probability p. The server estimates ¯X by\nˆ ¯X⇡p = R 1 · 1\nnp\nX i2S Yi,\nwhere Yis are defined in the previous section and S is the set of clients that transmitted. Lemma 8. For any set of vectors Xn and protocol ⇡ of the form Equation (6), its sampled version ⇡p satisfies\nE(⇡p, Xn) = 1 p · E(⇡, Xn) + 1 p np\nnX\ni=1\n||Xi||2 2 .\nand C(⇡p, Xn) = p · C(⇡, Xn).\nProof. The proof of communication cost follows from Lemma 5 and the fact that in expectation, np clients transmit. We now bound the MSE. Let S be the set of clients that transmit. The error E(⇡p, Xn) is\nE  ˆ¯X ¯X 2\n2\n= E\n2\n4 1\nnp\nX i2S R 1Yi ¯X 2\n2\n3\n5\n=E\n2\n4 1\nnp\nX i2S Xi ¯X 2\n2\n+\n1\nn2p2\nX i2S (R 1Yi Xi) 2\n2\n3\n5 ,\nwhere the last equality follows by observing that R 1Yi Xi are independent zero mean random variables and hence for any i, E[(R 1Yi Xi)T ( P i2S Xi ¯X)] = 0. The first term can be bounded as\nE 1\nnp\nX i2S Xi ¯X 2\n2\n=\n1\nn2\nnX\ni=1\nE 1\np Xi i2S Xi\n2\n2\n=\n1\nn2\nnX\ni=1\n✓ p (1 p)2\np2 ||Xi||2 2 + (1 p) ||Xi||2 2\n◆\n= 1 p np · 1 n\nnX\ni=1\n||Xi||2 2 .\nFurthermore, the second term can be bounded as\nE\n2\n4 1 n2p2\nX i2S (R 1Yi Xi) 2\n2\n3\n5\n(a) = 1\nn2p2\nX i2S E h (R 1Yi Xi) 2 2 i\n=\n1\nn2p2\nnX\ni=1\nE h\n(R 1Yi Xi) 2 2 i2S\ni\n=\n1\nn2p\nnX\ni=1\nE h R 1Yi Xi\n2 2\ni\n=\n1 n2p E\n2\n4\nnX\ni=1\n(R 1Yi Xi)\n2\n2\n3\n5 = 1 p E(⇡, Xn)\nwhere the last equality follows from the assumption that ⇡’s mean estimate is of the form (6). (a) follows from the fact that R 1Yi Xi are independent zero mean random variables.\nCombining the above lemma with Theorem 4, and choosing k = p d+ 1 results in the following. Corollary 1. For every c  nd(2+log 2\n(7/4)), there exists a protocol ⇡ such that C(⇡, Sd)  c and\nE(⇡, Sd) = O ✓ min ✓ 1, d\nc\n◆◆ ."
  }, {
    "heading": "6. Lower bounds",
    "text": "The lower bound relies on the lower bounds on distributed statistical estimation due to Zhang et al. (2013). Lemma 9 ((Zhang et al., 2013) Proposition 2). There exists a set of distributions Pd supported on h 1p\nd , 1p d\nid such\nthat if any centralized server wishes to estimate the mean of the underlying unknown distribution, then for any independent protocol ⇡\nmax pd2Pd E  ✓(pd) ˆ✓⇡ 2 2\ntmin ✓ 1, d C(⇡) ◆ ,\nwhere C(⇡) is the communication cost of the protocol, ✓(pd) is the mean of pd, and t is a positive constant. Theorem 5. Let t be the constant in Lemma 9. For every c  ndt/4 and n 4/t,\nE(⇧(c), Sd) t 4 min\n✓ 1, d\nc\n◆ .\nProof. Given n samples from the underlying distribution where each sample belongs to Sd, it is easy to see that\nE ✓(pd) ˆ✓(pd)\n2\n2\n 1 n ,\n0 1 2 3 4 5 6 −25\n−20\n−15\n−10\n−5\nBits per dimension\nlo g\n(M S\nE )\nuniform rotation variable\nFigure 1. Distributed mean estimation on data generated from a Gaussian distribution.\nwhere ˆ✓(pd) is the empirical mean of the observed samples. Let Pd be the set of distributions in Lemma 9. Hence for any protocol ⇡ there exists a distribution pd such that\nE ˆ✓(pd) ˆ✓⇡\n2\n2\n(a) 1\n2\nE ✓(pd) ˆ✓⇡\n2\n2\nE ✓(pd) ˆ✓(pd)\n2\n2\n(b) t\n2\nmin ✓ 1, d C(⇡) ◆ 1 n (c) t 4 min ✓ 1, d C(⇡) ◆ ,\n(a) follows from the fact that 2(a b)2 + 2(b c)2 (a c)2. (b) follows from Lemma 9 and (c) follows from the fact that C(⇡, Sd)  ndt/4 and n 4/t. Corollary 1 and Theorem 5 yield Theorem 1. We note that the above lower bound holds only for communication cost c < O(nd). Extending the results for larger values of c remains an open problem.\nAt a first glance it may appear that combining structured random matrix and variable length encoding may improve the result asymptotically, and therefore violates the lower bound. However, this is not true.\nObserve that variable length coding ⇡svk and stochastic rotated quantization ⇡srk use different aspects of the data: the variable length coding uses the fact that bins with large values of index r are less frequent. Hence, we can use fewer bits to encode frequent bins and thus improve communication. In this scheme bin-width (si/(k 1)) is p 2||Xi||2/(k 1). Rotated quantization uses the fact that rotation makes the min and max closer to each other and hence we can make bins with smaller width. In such a case, all the bins become more or less equally likely and hence variable length coding does not help. In this scheme bin-width (si/(k 1)) is (Zmaxi Zmini )/(k 1) ⇡ ||Xi||2(log d)/(kd), which is much smaller than bin-width for variable length coding. Hence variable length coding and random rotation cannot be used simultaneously."
  }, {
    "heading": "7. Practical considerations and applications",
    "text": "Based on the theoretical analysis, the variable-length coding method provides the lowest quantization error asymptotically when using a constant number of bits. However in practice, stochastic rotated quantization may be preferred due to (hidden) constant factors and the fact that it uses a fixed amount of bits per dimension. For example, considering quantizing the vector [ 1, 1, 0, 0], stochastic rotated\nquantization can use 1 bit per dimension and gives zero error, whereas the other two protocols do not. To see this, observe that the naive quantization will quantize 0 to either 1 or 1 and variable length coding cannot achieve 0 error with 1 bit per dimension due to its constant factors.\nWe further note that the rotated quantization is preferred when applied on “unbalanced” data, due to the fact that the rotation can correct the unbalancedness. We demonstrate this by generating a dataset where the value of the last feature dimension entry is much larger than others. We generate 1000 datapoints each with 256 dimensions. The first 255 dimensions are generated i.i.d. from N(0, 1), and the last dimension is generated from N(100, 1). As shown in Figure 1, the rotated stochastic quantization has the best performance. The improvement is especially significant for low bit rate cases.\nWe demonstrate two applications in the rest of this section. The experiments are performed on the MNIST (d = 1024) and CIFAR (d = 512) datasets.\nDistributed Lloyd’s algorithm. In the distributed Lloyd’s (k-means) algorithm, each client has access to a subset of data points. In each iteration, the server broadcasts the cluster centers to all the clients. Each client updates the centers based on its local data, and sends the centers back to the server. The server then updates the centers by computing the weighted average of the centers sent from all clients. In\nthe quantized setting, the client compresses the new centers before sending to the server. This saves the uplink communication cost, which is often the bottleneck of distributed learning5. We set both the number of centers and number of clients to 10. Figure 2 shows the result.\nDistributed power iteration. Power iteration is a widely used method to compute the top eigenvector of a matrix. In the distributed setting, each client has access to a subset of data. In each iteration, the server broadcasts the current estimate of the eigenvector to all clients. Each client then updates the eigenvector based on one power iteration on its local data, and sends the updated eigenvector back to the server. The server updates the eigenvector by computing the weighted average of the eigenvectors sent by all clients. Similar to the above distributed Lloyd’s algorithm, in the quantized setting, the client compresses the estimated eigenvector before sending to the server. Figure 3 shows the result. The dataset is distributed over 100 clients.\nFor both of these applications, variable-length coding achieves the lowest quantization error in most of the settings. Furthermore, for low-bit rate, stochastic rotated quantization is competitive with variable-length coding.\n5In this setting, the downlink is a broadcast, and therefore its cost can be reduced by a factor of O(n/ log n) without quantization, where n is the number of clients."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Jayadev Acharya, Keith Bonawitz, Dan Holtmann-Rice, Jakub Konecny, Tengyu Ma, and Xiang Wu for helpful comments and discussions."
  }],
  "year": 2017,
  "references": [{
    "title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform",
    "authors": ["Ailon", "Nir", "Chazelle", "Bernard"],
    "venue": "In STOC,",
    "year": 2006
  }, {
    "title": "QSGD: Randomized quantization for communication-optimal stochastic gradient descent",
    "authors": ["Alistarh", "Dan", "Li", "Jerry", "Tomioka", "Ryota", "Vojnovic", "Milan"],
    "year": 2016
  }, {
    "title": "Communication complexity of distributed convex learning and optimization",
    "authors": ["Arjevani", "Yossi", "Shamir", "Ohad"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Distributed learning, communication complexity and privacy",
    "authors": ["Balcan", "Maria-Florina", "Blum", "Avrim", "Fine", "Shai", "Mansour", "Yishay"],
    "venue": "In COLT,",
    "year": 2012
  }, {
    "title": "Practical secure aggregation for federated learning on user-held data",
    "authors": ["Bonawitz", "Keith", "Ivanov", "Vladimir", "Kreuter", "Ben", "Marcedone", "Antonio", "McMahan", "H Brendan", "Patel", "Sarvar", "Ramage", "Daniel", "Segal", "Aaron", "Seth", "Karn"],
    "year": 2016
  }, {
    "title": "Communication lower bounds for statistical estimation problems via a distributed data processing",
    "authors": ["Braverman", "Mark", "Garg", "Ankit", "Ma", "Tengyu", "Nguyen", "Huy L", "Woodruff", "David P"],
    "year": 2016
  }, {
    "title": "Communication-optimal distributed clustering",
    "authors": ["Chen", "Jiecao", "Sun", "He", "Woodruff", "David", "Zhang", "Qin"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "An elementary proof of a theorem of johnson and lindenstrauss",
    "authors": ["Dasgupta", "Sanjoy", "Gupta", "Anupam"],
    "venue": "Random Structures & Algorithms,",
    "year": 2003
  }, {
    "title": "Large scale distributed deep networks",
    "authors": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "The jackknife estimate of variance",
    "authors": ["Efron", "Bradley", "Stein", "Charles"],
    "venue": "The Annals of Statistics,",
    "year": 1981
  }, {
    "title": "Universal compression of power-law distributions",
    "authors": ["Falahatgar", "Moein", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Pichapati", "Venkatadheeraj", "Suresh", "Ananda Theertha"],
    "venue": "In ISIT,",
    "year": 2015
  }, {
    "title": "On communication cost of distributed statistical estimation and dimensionality",
    "authors": ["Garg", "Ankit", "Ma", "Tengyu", "Nguyen", "Huy L"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Hadamard matrices and their applications",
    "authors": ["Horadam", "Kathy J"],
    "venue": "Princeton university press,",
    "year": 2012
  }, {
    "title": "Randomized distributed mean estimation: Accuracy vs communication",
    "authors": ["Konečnỳ", "Jakub", "Richtárik", "Peter"],
    "year": 2016
  }, {
    "title": "Federated learning: Strategies for improving communication efficiency",
    "authors": ["Konečnỳ", "Jakub", "McMahan", "H Brendan", "Yu", "Felix X", "Richtárik", "Peter", "Suresh", "Ananda Theertha", "Bacon", "Dave"],
    "year": 2016
  }, {
    "title": "The performance of universal encoding",
    "authors": ["R Krichevsky", "V. Trofimov"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1981
  }, {
    "title": "Least squares quantization in PCM",
    "authors": ["Lloyd", "Stuart"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1982
  }, {
    "title": "Information theory, inference and learning algorithms",
    "authors": ["MacKay", "David JC"],
    "venue": "Cambridge university press,",
    "year": 2003
  }, {
    "title": "Distributed training strategies for the structured perceptron",
    "authors": ["McDonald", "Ryan", "Hall", "Keith", "Mann", "Gideon"],
    "venue": "In HLT,",
    "year": 2010
  }, {
    "title": "Federated learning of deep networks using model averaging",
    "authors": ["McMahan", "H. Brendan", "Moore", "Eider", "Ramage", "Daniel", "y Arcas", "Blaise Aguera"],
    "year": 2016
  }, {
    "title": "Parallel training of deep neural networks with natural gradient and parameter averaging",
    "authors": ["Povey", "Daniel", "Zhang", "Xiaohui", "Khudanpur", "Sanjeev"],
    "venue": "arXiv preprint,",
    "year": 2014
  }, {
    "title": "Communication complexity of convex optimization",
    "authors": ["Tsitsiklis", "John N", "Luo", "Zhi-Quan"],
    "venue": "Journal of Complexity,",
    "year": 1987
  }, {
    "title": "Orthogonal random features",
    "authors": ["Yu", "Felix X", "Suresh", "Ananda Theertha", "Choromanski", "Krzysztof", "Holtmann-Rice", "Daniel", "Kumar", "Sanjiv"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
    "authors": ["Zhang", "Yuchen", "Duchi", "John", "Jordan", "Michael I", "Wainwright", "Martin J"],
    "venue": "In NIPS,",
    "year": 2013
  }],
  "id": "SP:047b1471ad92c0513d2cfd72788944e3dc6aec7c",
  "authors": [{
    "name": "Ananda Theertha Suresh",
    "affiliations": []
  }, {
    "name": "Felix X. Yu",
    "affiliations": []
  }, {
    "name": "Sanjiv Kumar",
    "affiliations": []
  }, {
    "name": "H. Brendan McMahan",
    "affiliations": []
  }],
  "abstractText": "Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of ⇥(d/n) and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n). We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd’s algorithm for kmeans and power iteration for PCA.",
  "title": "Distributed Mean Estimation with Limited Communication"
}