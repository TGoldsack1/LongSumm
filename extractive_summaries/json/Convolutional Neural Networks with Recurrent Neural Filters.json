{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 912–917 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n912"
  }, {
    "heading": "1 Introduction",
    "text": "Convolutional neural networks (CNNs) have been shown to achieve state-of-the-art results on various natural language processing (NLP) tasks, such as sentence classification (Kim, 2014), question answering (Dong et al., 2015), and machine translation (Gehring et al., 2017). In an NLP system, a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence. Hence, the key components of CNNs are a set of convolution filters that compose low-level word features into higher-level representations.\nConvolution filters are usually realized as linear systems, as their outputs are affine transformations of the inputs followed by some non-linear activation functions. Prior work directly adopts a linear convolution filter to NLP problems by utilizing a concatenation of embeddings of a window of words as the input, which leverages word order information in a shallow additive way. As early\n1The code is available at https://github.com/ bloomberg/cnn-rnf.\nCNN architectures have mainly drawn inspiration from models of the primate visual system, the necessity of coping with language compositionality is largely overlooked in these systems. Due to the linear nature of the convolution filters, they lack the ability to capture complex language phenomena, such as compositionality and long-term dependencies.\nTo overcome this, we propose to employ recurrent neural networks (RNNs) as convolution filters of CNN systems for various NLP tasks. Our recurrent neural filters (RNFs) can naturally deal with language compositionality with a recurrent function that models word relations, and they are also able to implicitly model long-term dependencies. RNFs are typically applied to word sequences of moderate lengths, which alleviates some well-known drawbacks of RNNs, including their vulnerability to the gradient vanishing and exploding problems (Pascanu et al., 2013). As in conventional CNNs, the computation of the convolution operation with RNFs can be easily parallelized. As a result, RNF-based CNN models can be 3-8x faster than their RNN counterparts.\nWe present two RNF-based CNN architectures for sentence classification and answer sentence selection problems. Experimental results on the Stanford Sentiment Treebank and the QASent and WikiQA datasets demonstrate that RNFs significantly improve CNN performance over linear filters by 4-5% accuracies and 3-6% MAP scores respectively. Analysis results suggest that RNFs perform much better than linear filters in detecting longer key phrases, which provide stronger cues for categorizing the sentences."
  }, {
    "heading": "2 Approach",
    "text": "The aim of a convolution filter is to produce a local feature for a window of words. We describe a novel approach to learning filters using RNNs, which is especially suitable for NLP problems.\nWe then present two CNN architectures equipped with RNFs for sentence classification and sentence matching tasks respectively."
  }, {
    "heading": "2.1 Recurrent neural filters",
    "text": "We briefly review the linear convolution filter implementation by Kim (2014), which has been widely adopted in later works. Consider an m-gram word sequence [xi, · · · ,xi+m−1], where xi ∈ Rk is a k-dimensional word vector. A linear convolution filter is a function applied to the m-gram to produce a feature ci,j :\nci,j =f(w > j xi:i+m−1 + bj),\nxi:i+m−1 =xi ⊕ xi+1 ⊕ · · · ⊕ xi+m−1, (1)\nwhere ⊕ is the concatenation operator, bj is a bias term, and f is a non-linear activation function. We typically use multiple independent linear filters to yield a feature vector ci for the word sequence. Linear convolution filters make strong assumptions about language that could harm the performance of NLP systems. First, linear filters assume local compositionality and ignore long-term dependencies in language. Second, they use separate parameters for each value of the time index, which hinders parameter sharing for the same word type (Goodfellow et al., 2016). The assumptions become more problematic if we increase the window size m.\nWe propose to address the limitations by employing RNNs to realize convolution filters, which we term recurrent neural filters (RNFs). RNFs compose the words of the m-gram from left to right using the same recurrent unit:\nht = RNN(ht−1,xt), (2)\nwhere ht is a hidden state vector that encoded information about previously processed words, and the function RNN is a recurrent unit such as Long Short-Term Memory (LSTM) unit (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) (Cho et al., 2014). We use the last hidden state hi+m−1 as the RNF output feature vector ci. Features learned by RNFs are interdependent of each other, which permits the learning of complementary information about the word sequence. The left-to-right word composing procedure in RNFs preserves word order information and implicitly models long-term dependencies in language. RNFs can be treated as simple dropin replacements of linear filters and potentially adopted in numerous CNN architectures."
  }, {
    "heading": "2.2 CNN architectures",
    "text": "Sentence encoder We use a CNN architecture with one convolution layer followed by one max pooling layer to encode a sentence. In particular, the RNFs are applied to every possible window of m words in the sentence {x1:m,x2:m+1, · · · ,xn−m+1:n} to generate feature maps C = [c1, c2, · · · , cn−m+1]. Then a max-over-time pooling operation (Collobert et al., 2011) is used to produce a fixed size sentence representation: v = max{C}.\nSentence classification We use a CNN architecture that is similar to the CNN-static model described in Kim (2014) for sentence classification. After obtaining the sentence representation v, a fully connected softmax layer is used to map v to an output probability distribution. The network is optimized against a cross-entropy loss function.\nSentence matching We exploit a CNN architecture that is nearly identical to the CNN-Cnt model introduced by Yang et al. (2015). Let v1 and v2 be the vector representations of the two sentences. A bilinear function is applied to v1 and v2 to produce a sentence matching score. The score is combined with two word matching count features and fed into a sigmoid layer. The output of the sigmoid layer is used by binary cross-entropy loss to optimize the model."
  }, {
    "heading": "3 Experiments",
    "text": "We evaluate RNFs on some of the most popular datasets for the sentence classification and sentence matching tasks. After describing the experimental setup, we compare RNFs against both linear filters and conventional RNN models, and report our findings in § 3.2."
  }, {
    "heading": "3.1 Experimental settings",
    "text": "Data We use the Stanford Sentiment Treebank (Socher et al., 2013) in our sentence classification experiments. We report accuracy results for both binary classification and fine-grained classification settings. Two answer sentence selection datasets, QASent (Wang et al., 2007) and WikiQA (Yang et al., 2015), are adopted in our sentence matching experiments. We use MAP and MRR to evaluate the performance of answer sentence selection models.\nCompetitive systems We consider CNN variants with linear filters and RNFs. For RNFs, we\nadopt two implementations based on GRUs and LSTMs respectively. We also compare against the following RNN variants: GRU, LSTM, GRU with max pooling, and LSTM with max pooling.2 We use the publicly available 300-dimensional GloVe vectors (Pennington et al., 2014) pre-trained with 840 billion tokens to initialize the word embeddings for all the models. The word vectors are fixed during downstream training. Finally, we report best published results for each dataset.3\nParameter tuning For all the experiments, we tune hyperparameters on the development sets and report results obtained with the selected hyperparameters on the test sets. After the preliminary search, we choose the number of hidden units (feature maps) from {200, 300, 400}, and the filter window width from {2, 3, 4, 5} for linear filters and from {5, 6, 7, 8} for RNFs. Linear filters tend to perform well with small window widths, while RNFs work better with larger window widths. We apply dropout to embedding layers, pooling layers, RNN input layers, and RNN recurrent layers. The dropout rates are selected from {0, 0.2, 0.4}, where 0 indicates that dropout is not used for the specific layer. Optimization is performed by Adam (Kingma and Ba, 2015) with early stopping. We conduct random search with a budget of 100 runs to seek the best hyperparameter configuration for each system."
  }, {
    "heading": "3.2 Results",
    "text": "The evaluation results on sentiment classification and answer sentence selection are shown in Table 1 and Table 2 respectively. RNFs significantly outperform linear filters on both tasks. In fact, we find that CNN-RNF variants significantly outperform CNN-linear-filter on nearly every hyperparameter configuration in our experiments. On the Stanford Sentiment Treebank, CNN-RNF-LSTM outperforms CNN-linear-filter by 5.4% and 3.9% accuracies on the fine-grained and binary classification settings respectively. We observe similar performance boosts by adopting RNFs for CNNs on the QASent and WikiQA test sets. CNN-RNFGRU improves upon CNN-linear-filter by 3.7% MRR score on QASent and CNN-RNF-LSTM performs better than CNN-linear-filter by 6.1%\n2Max pooling performed better than mean pooling in our preliminary experiments.\n3We exclude results obtained from systems using external resources beyond word embeddings.\nMAP score on WikiQA. In particular, CNN-RNFLSTM achieves 53.4% and 90.0% accuracies on the fine-grained and binary sentiment classification tasks respectively, which match the state-ofthe-art results on the Stanford Sentiment Treebank. CNN-RNF-LSTM also obtains competitive results on answer sentence selection datasets, despite the simple model architecture compared to state-of-the-art systems.\nConventional RNN models clearly benefit from max pooling, especially on the task of answer sentence selection. Like RNF-based CNN models, max-pooled RNNs also consist of two essential layers. The recurrent layer learns a set of hidden states corresponding to different time steps, and the max pooling layer extracts the most salient information from the hidden states. However, a hidden state in RNNs encodes information about all the previous time steps, while RNFs focus on detecting local features from a window of words that can be more relevant to specific tasks. As a result, RNF-based CNN models perform consistently better than max-pooled RNN models.\nCNN-RNF models are much faster to train than their corresponding RNN counterparts, despite they have the same numbers of parameters, as RNFs are applied to word sequences of shorter lengths and the computation is parallelizable. The training of CNN-RNF-LSTM models takes 10-20 mins on the Stanford Sentiment Treebank, which is 3-8x faster than the training of LSTM models, on an NVIDIA Tesla P100 GPU accelerator."
  }, {
    "heading": "4 Analysis",
    "text": "We now investigate why RNFs are more effective than linear convolution filters on the binary sentiment classification task. We perform quantitative analysis on the development set of the Stanford Sentiment Treebank (SST), in which sentiment labels for some phrases are also available.\nLocal label consistency (LLC) ratio We first inspect whether longer phrases have a higher tendency to express the same sentiment as the entire sentence. We define the local label consistency (LLC) ratio as the ratio of m-grams that share the same sentiment labels as the original sentences. The LLC ratios with respect to different phrase lengths are shown in Figure 1. Longer phrases are\nmore likely to convey the same sentiments as the original sentences. Therefore, the ability to model long phrases is crucial to convolution filters.\nKey phrase hit rate We examine linear filters and RNFs on the ability to detect a key phrase in a sentence. Specifically, we define the key phrases for a sentence to be the set of phrases that are labeled with the same sentiment as the original sentence. The key phrase hit rate is then defined as the ratio of filter-detected m-grams that fall into the corresponding key phrase sets. The filter-detected m-gram of a sentence is the one whose convolution feature vector has the shortest Euclidean distance to the max-pooled vector. The hit rate results computed on SST dev set are presented in Figure 2. As shown, RNFs consistently per-\nform better than linear filters on identifying key phrases across different phrase lengths, especially on phrases of moderate lengths (4-7). The results suggest that RNFs are superior to linear filters, as they can better model longer phrases whose labels are more consistent with the sentences.4"
  }, {
    "heading": "5 Related work",
    "text": "Linear convolution filters are dominated in CNNbased systems for both computer vision and natural language processing tasks. One exception is the work of Zoumpourlis et al. (2017), which proposes a convolution filter that is a function with quadratic forms through the Volterra kernels. However, this non-linear convolution filter is developed in the context of a computational model of the visual cortex, which is not suitable for NLP problems. In contrast, RNFs are specifically derived for NLP tasks, in which a different form of non-linearity, language compositionality, often plays a critical role.\nSeveral works have employed neural network architectures that contain both CNN and RNN components to tackle NLP problems. Tan et al. (2016) present a deep neural network for answer sentence selection, in which a convolution layer is applied to the output of a BiLSTM layer for extracting sentence representations. Ma and Hovy (2016) propose to compose character representations of a word using a CNN, whose output is then fed into a BiLSTM for sequence tagging. Kalchbrenner and Blunsom (2013) introduce a neural architecture that uses a sentence model based on CNNs and a discourse model based on RNNs. Their system achieves state-of-the-art results on the task of dialogue act classification. Instead of treating an RNN and a CNN as isolated components, our work directly marries RNNs with the convolution operation, which illustrates a new direction in mixing RNNs with CNNs."
  }, {
    "heading": "6 Conclusion and future work",
    "text": "We present RNFs, a new class of convolution filters based on recurrent neural networks. RNFs sequentially apply the same recurrent unit to words of a phrase, which naturally capture language compositionality and implicitly model long-term\n4Phrases of very long lengths (8-10) are rarely annotated in SST dev data, which could explain why linear filters and RNFs achieve similar hit rates, as small data sample often leads to high variance.\ndependencies. Experiments on sentiment classification and answer sentence selection tasks demonstrate that RNFs give a significant boost in performance compared to linear convolution filters. RNF-based CNNs also outperform a variety of RNN-based models, as they focus on extracting local information that could be more relevant to particular problems. The quantitive analysis reveals that RNFs can handle long phrases much better than linear filters, which explains their superiority over the linear counterparts. In the future, we would like to investigate the effectiveness of RNFs on a wider range of NLP tasks, such as natural language inference and machine translation."
  }, {
    "heading": "7 Acknowledgments",
    "text": "We thank Chunyang Xiao for helping us to run the analysis experiments. We thank Kazi Shefaet Rahman, Ozan Irsoy, Chen-Tse Tsai, and Lingjia Deng for their valuable comments on earlier versions of this paper. We also thank the EMNLP reviewers for their helpful feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Question answering over freebase with multicolumn convolutional neural networks",
    "authors": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu."],
    "venue": "Proceedings of the Association for Computational Linguistics (ACL).",
    "year": 2015
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin."],
    "venue": "Proceedings of the International Conference on Machine Learning (ICML).",
    "year": 2017
  }, {
    "title": "Deep learning, volume 1",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville", "Yoshua Bengio."],
    "venue": "MIT press Cambridge.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8).",
    "year": 1997
  }, {
    "title": "Recurrent convolutional neural networks for discourse compositionality",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality.",
    "year": 2013
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Deriving neural architectures from sequence and graph kernels",
    "authors": ["Tao Lei", "Wengong Jin", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Proceedings of the International Conference on Machine Learning (ICML).",
    "year": 2017
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the Association for Computational Linguistics (ACL).",
    "year": 2016
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Machine Learning (ICML).",
    "year": 2013
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "Noisecontrastive estimation for answer selection with deep neural networks",
    "authors": ["Jinfeng Rao", "Hua He", "Jimmy Lin."],
    "venue": "Proceedings of the ACM International on Conference on Information and Knowledge Management (CIKM).",
    "year": 2016
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of Empirical Methods for Nat-",
    "year": 2013
  }, {
    "title": "Lstm-based deep learning models for non-factoid answer selection",
    "authors": ["Ming Tan", "Cicero dos Santos", "Bing Xiang", "Bowen Zhou."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "What is the jeopardy model? a quasisynchronous grammar for qa",
    "authors": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."],
    "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).",
    "year": 2007
  }, {
    "title": "A compareaggregate model for matching text sequences",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Wikiqa: A challenge dataset for open-domain question answering",
    "authors": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."],
    "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).",
    "year": 2015
  }, {
    "title": "Nonlinear convolution filters for cnn-based learning",
    "authors": ["Georgios Zoumpourlis", "Alexandros Doumanoglou", "Nicholas Vretos", "Petros Daras."],
    "venue": "Proceedings of the International Conference on Computer Vision (ICCV).",
    "year": 2017
  }],
  "id": "SP:21458bb86450306ca227fbcfdaa1cf33141381fa",
  "authors": [{
    "name": "Yi Yang",
    "affiliations": []
  }],
  "abstractText": "We introduce a class of convolutional neural networks (CNNs) that utilize recurrent neural networks (RNNs) as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a nonlinear function, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.1",
  "title": "Convolutional Neural Networks with Recurrent Neural Filters"
}