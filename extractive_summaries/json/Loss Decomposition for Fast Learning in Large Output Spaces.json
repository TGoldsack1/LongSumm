{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Large output spaces are ubiquitous in several machine learning problems today: for example, extreme multiclass or multilabel classification problems with many classes, language modeling with big vocabularies, or metric learning with a large number of pairwise distance constraints. In all such problems, a key bottleneck in training models is evaluation of the loss function and its gradient. The loss functions used for such problems typically require an enumeration of all the possible outputs, and thus, naı̈vely, necessitate a linear running time in the number of outputs for\n1Carnegie Mellon University, Pittsburgh, USA 2Google, New York, USA. Correspondence to: Ian E.H. Yen <eyan@cs.cmu.edu>, Satyen Kale <satyenkale@google.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nevaluation. This can be a significant bottleneck in iterative methods such as gradient descent used to train the model, since each step now requires a huge number of operations.\nMany approaches have been proposed to mitigate this issue. One body of work imposes structure over the output space, such as low-rank (Yu et al., 2014), treestructure (Prabhu & Varma, 2014), locally low-rank (Bhatia et al., 2015), or hierarchical factorization (Morin & Bengio, 2005; Mnih & Hinton, 2009). However, structural assumptions can be violated in many situations. For example, while the low-rank structure is typically reasonable in a recommendation problem, it is usually not true in multiclass classification as for each instance there is exactly one correct answer (i.e. classes may not be correlated with each other). Additionally, even for valid structural assumptions, constructing the correct structure from data is hard, and in practice heuristics or human annotation are required (Morin & Bengio, 2005; Mnih & Hinton, 2009).\nAnother approach is sampling approximation (Mikolov et al., 2013; Gutmann & Hyvärinen, 2012; Jean et al., 2014), which computes an estimate of the gradient based on the scores of only a small fraction of the negative output classes and also a small set of classes labeled as positive. The approximation, however, has large variance when the loss has a skewed distribution over classes. For example, in extreme multiclass or multilabel classification, the loss typically only concentrates on a few confusing classes, which have small probabilities of being sampled. The variance in gradient estimation often leads to slow progress of the learning algorithm.\nIn this paper, we consider problems with large output spaces, but with each example having only a relatively small set of correct outputs. The learning objective for such tasks typically has its gradient concentrated on a relatively small number of classes, and therefore an efficient way to learn is to search for classes of significant gradient magnitude. For example, (Yen et al., 2016; 2017) proposed a method to search classes efficiently by maintaining a sparse model during training. However, this method applies only in problems of high input dimension. Another strategy that has received a lot of attention recently is to utilize data structures to find classes efficiently through Maximum Inner-Product Search (MIPS) or Nearest-Neighbor\nSearch (NNS) (Yen et al., 2013; Vijayanarasimhan et al., 2014; Mussmann & Ermon, 2016; Mussmann et al., 2017; Spring & Shrivastava, 2017b;a; Wu et al., 2017; Guo et al., 2016). The main challenge here is that as dimension grows, it becomes difficult to perform MIPS or NNS with both high recall and high precision, and therefore gradient approximation through MIPS or NNS often sacrifices accuracy to achieve efficiency.\nIn this work, we propose an algorithm based on an application of dual decomposition (Boyd et al., 2011) to the convex-conjugate representation of the loss function. This can be viewed as a complementary technique for applying search data structures to a learning problem. Essentially, the algorithm replaces the high dimensional search problem with several lower dimensional searches by decoupling the dimensions via dual decomposition. Lower dimensional search can be done much more efficiently, and the different searches are then coupled together via a greedy message passing algorithm. We prove that this greedy message passing technique is guaranteed to converge and thus we can obtain good approximations to the loss and its gradient. We term our overall approach LDGS for Loss Decomposition Guided Search.\nOur experiments on large-scale face recognition, document tagging and word embedding show that the proposed approach significantly improves the accuracy of the searchbased gradient approximation method and is orders of magnitude faster than other strategies of gradient approximation such as sampling."
  }, {
    "heading": "2. Problem Setup",
    "text": "Let X denote the input space and Y the output space, and let K := |Y|. In this paper we focus on the situation where K is extremely large, on the order of hundreds of thousands or larger. We are interested in learning a scoring function f : X → RK for a large output space Y from a given class of such functions, F . Labeled samples are pairs (x,P) with x ∈ X and P ⊆ Y which denotes the set of correct labels for the input point x. We use the notation N := Y \\ P to denote the set of negative labels for the example. Given a collection of training samples {(xi,Pi)}Ni=1, the learning objective takes the following form:\nmin f∈F\n1\nN N∑ i=1 L(f(xi),Pi).\nwhere L : RK × 2Y → R is a loss function such that L(z,P) penalizes the discrepancy between the score vector z ∈ RK and a set of positive labels P ⊆ Y . The evaluation of the loss function and its gradient with respect to the score vector, ∇zL(z,P), typically has cost growing linearly with the size of the output space K, and thus is\nexpensive for problems with huge output spaces.\nThe key to our method for reducing the complexity of loss and gradient evaluation will be the following linear structural assumption on the class of scoring functions F : there is an embedding dimension parameter D ∈ N such that for every f ∈ F , we can associate a weight matrix W ∈ RK×D and feature map φ : X → RD so that for all x ∈ X ,\nf(x) = Wφ(x). (1)\nWe will assume that D K, say on the order of a few hundreds or thousands, so that we can explicitly evaluate φ(x).\nThe problem we consider is the following: given f and a batch of samples {xi,Pi}Ni=1, compute an approximation to the empirical loss 1N ∑N i=1 L(f(xi),Pi) and its gradient. This is an important subroutine that naturally arises in either full batch gradient descent or minibatch stochastic gradient descent.\nThe main challenge here is to construct data structures that preprocess the matrix W so that good approximations to the loss f(xi,Pi) and its gradient can be computed without computing the vector f(x) entirely: i.e. we desire sublinear (in K) time computation of such approximations given access to an appropriate data structure.\nBefore proceeding to our dual decomposition based search technique, we give a few examples of problems with large output space that fit in our framework:\n1. Extreme Classification. In extreme classification problems, popular classification loss functions include Cross-Entropy Loss\nL(z,P) := ∑ k∈P log (∑K j=1 exp(zj) ) − zk (2)\nand Max-Margin Loss L(z,P) := [\nmax k∈P,j∈N zj − zk + 1 ] + . (3)\nFor multiclass problems, |P| = 1, while for multilabel problems we usually have |P| K. A typical scoring function takes the form\nf(x) := Wφ(x). (4)\nHere, φ(x) is a feature map constructed either from the domain knowledge or via learning (e.g., a neural network). Both of them fit the structural assumption (1).\n2. Metric Learning. In Metric Learning problems, during training we learn a function\nf(x) = [−d(x,y)]y∈Y , (5)\nthat denotes the dissimilarities of the point x to a collection of points y ∈ Y . Common choices of the dissimilarity function include the squared Euclidean distance d(x,y) = ‖ψ(x)−ψ(y)‖22 parameterized by a nonlinear transformation ψ : X → Rd for some d ∈ N, and, more generally, the squared Mahalanobis distance d(x,y) = (ψ(x)−ψ(y))>M(ψ(x)−ψ(y)) parameterized by ψ and a positive definite matrixM . The candidate set Y could be the whole set of training samples {xi}Ni=1, or a collection of latent proxies {yk}Kk=1 as suggested by a recent state-of-the-art method (Movshovitz-Attias et al., 2017). For each sample (x,P), the goal is to learn a distance function s.t. the positive candidates P are closer to x than the negative ones. Common loss functions for the task are Neighborhood Component Analysis (NCA) loss (Goldberger et al., 2005)\nL(z,P) := ∑ k∈P log (∑K j=1 exp(zj) ) − zk (6)\nand the Triplet loss (Weinberger & Saul, 2009) L(z,P) = ∑ k∈P ∑ j∈N [zj − zk + 1]+. (7)\nIt is easy to see that such scoring functions satisfy the structural assumption (1): for the scoring function f given by the squared Mahalanobis distance parameterized by ψ and M , the matrix W consists of the rows 〈−ψ(y)>Mψ(y), 2ψ(y)>M ,−1〉 for each y ∈ Y , and φ(x) = 〈1,ψ(x)>,ψ(x)>Mψ(x)〉>. Thus the embedding dimension D = d+ 2.\n3. Word Embeddings. In the standard word2vec training (Mikolov et al., 2013), the input space X is the vocabulary set, and the output space Y = X ; thus K is the vocabulary size. The Skip-gram objective learns a scoring function f of the following form:\nf(x) = 〈φ(y)>φ(x)〉y∈X , (8)\nwhere φ(·) is a latent word embedding. This clearly fits the structural assumption (1): the rows of the matrixW are the embeddings φ(y) for all y ∈ X . Then given a text corpus D, the loss function1 for a sample (x,P) where P is the set of words in the corpus appearing within a certain size window around the input word x, is given by\nL(z,P) = qx ∑ y∈P qy|x·[log (∑ y′∈X exp(zy′) ) −zy] (9) where qx is the empirical unigram frequency of x and qy|x is the empirical frequency of observing y within a window of x in the corpus D.\n1This is a more compact reformulation of the loss function in (Mikolov et al., 2013).\nAlgorithm 1 Loss and Gradient Approximation via Search\ninput A sample (x,P), accuracy parameter τ > 0, and access to a MIPS data structure T for the rows ofW . output Approximations to L(f(x),P),∇L(f(x),P). 1: Query T with φ(x) and threshold τ to find S := {k | |[f(x)]k| > τ}.\n2: Construct a sparse approximation z̃ for f(x) by setting z̃k = f(x)k for k ∈ S ∪P , and z̃k = 0 for k 6∈ S ∪P . 3: Return L(z̃,P) and ∇L(z̃,P)."
  }, {
    "heading": "2.1. Loss and Gradient Approximation via Search",
    "text": "All the loss functions we considered in the applications mentioned share a key feature: their value can be well approximated by the scores of the positive labels and the largest scores of the negative labels. Similarly, their gradients are dominated by the coordinates corresponding to the positive labels and the negative labels with the largest scores. For example, the Max-Margin loss (3) is completely determined by the largest score of the negative labels and the lowest scores of the positive labels, and its gradient is non-zero only on the negative label with largest score and the positive label with lowest score. Similarly, for the Cross-Entropy loss (2), the coordinates of the gradient corresponding to the negative classes are dominated by the ones with the highest score; the gradient coordinates decrease exponentially as the scores decrease.\nThis key property suggests the following natural idea for approximating these losses and their gradients: since the score function f satisfies the linear structural property (1), we can compute the largest scores efficiently via a Maximum Inner Product Search (MIPS) data structure (Shrivastava & Li, 2014). This data structure stores a large data set of vectors v1,v2, . . . ,vK ∈ RD and supports queries of the following form: given a target vector u ∈ RD and a threshold τ , it returns the vectors vi stored in it that satisfy |v>i u| ≥ τ in time that is typically sublinear in K. Thus, we can preprocess W by storing the rows of W in an efficient MIPS data structure. Then for each sample x, we can compute the highest scores by querying this data structure with the target vector φ(x) and some reasonable threshold τ , computing approximations to the loss and gradient from the returned vectors (and treating all other scores as 0). This method is depicted in Algorithm 1.\nThe error in this approximation is naturally bounded by τ times the `∞ Lipschitz constant of L(·,P). For most loss functions considered in this paper, the `∞ Lipschitz constant is reasonably small: 2 for Max-Margin loss, O(Pmax log(K)) for Cross-Entropy loss (here, Pmax is the maximum number of positive labels for any example), etc.\nThe main difficulty in applying this approach in practice\nis the curse of dimensionality: the dependence on D is exponential for exact methods, and even for approximate methods, such as Locality-Sensitive Hashing, the cost still implicitly depends on the dimension as points become far apart when the intrinsic dimensionality is high (Li & Malik, 2017).\nTo deal with the curse of dimensionality, we introduce a novel search technique based on dual decomposition. This method, and its analysis, are given in the following section.\nIn order to apply and analyze the technique, we need the loss functions to be smooth (i.e. have Lipschitz continuous gradients). For non-smooth losses like Max-Margin loss (3), we apply Nesterov’s smoothing technique (Nesterov, 2005), which constructs a surrogate loss function with guaranteed approximation quality by adding a strongly convex term to the Fenchel conjugate of the loss:\nLµ(z) := max α 〈z,α〉 −\n( L∗(α) + µ\n2 ‖α‖2\n) . (10)\nHere, µ is a smoothing parameter that ensures that the surrogate loss has 1µ Lipschitz continuous gradients while approximating the original loss function to withinO(µ). This Smoothed Max-Margin loss has gradient\n∇L(z) := projC(z+1Nµ ) (11)\nwhere 1N denotes a vector containing 0 for indices k ∈ P and 1 for k ∈ N , and projC(.) denotes the projection onto the bi-simplex C = {α | ∑ k∈N αk = ∑ k∈P −αk ≤ 1, αN ≥ 0, αP ≤ 0}. The Smoothed Max-Margin loss and its gradient can again be computed using the largest few scores."
  }, {
    "heading": "3. Loss Decomposition",
    "text": "We now describe our loss decomposition method. Recall the linear structural assumption (1): f(x) = Wφ(x) for all x ∈ X . In this section, we will keep (x,P) fixed, and we will drop the dependence on P in L for convenience and simply use the notation L(f(x)) and∇L(f(x)).\nWhile MIPS over the D-dimensional rows of W can be computationally expensive, we can exploit the linear structure of f by decomposing it: chunking the D coordinates of the vectors in RD into B blocks, each of size D/B. Here B ∈ N is an integer; larger B leads to easier MIPS problems but reduces accuracy of approximations produced. Let W (1),W (2), . . . ,W (B) be the corresponding block partitioning of W obtained by grouping together the columns corresponding to the coordinates in each block. Similarly, let φ(1)(x),φ(2)(x), . . . ,φ(B)(x) be the conformal partitioning of the coordinates of φ(x).\nNow define the overall score vector z := f(x) = Wφ(x), and per-chunk score vectors zj = W (j)φ(j)(x), for j ∈\n[B]. Then we have z = ∑B j=1 zj , in other words, we have a decomposition of the score vector. The following theorem states that the loss of a decomposable score vector can itself be decomposed into several parts connected through a set of message variables. This theorem is key to decoupling the variables into lower dimensional chunks that can be optimized separately via an efficient MIPS data structure. While this theorem can be derived by applying dual decomposition to the convex conjugate of the loss function, here we provide a simpler direct proof by construction. Theorem 1. Let L : RK → R be a convex function, and let z ∈ RK be decomposed as a sum of B vectors as follows: z = ∑B j=1 zj . Then L(z) is equal to the optimum value of the following convex minimization problem:\nmin λj∈RK , j∈[B]\n1\nB B∑ j=1 L(B(zj + λj)) s.t. B∑ j=1 λj = 0.\n(12)\nProof. First, for any λ1,λ2, . . . ,λB ∈ RK such that∑B j=1 λj = 0, by Jensen’s inequality applied to the con-\nvex function L, we have L(z) ≤ 1B ∑B j=1 L(B(zj +λj)). On the other hand, if we set λj = 1Bz− zj for all j ∈ [B], we have L(z) = 1B ∑B j=1 L(B(zj + λj))."
  }, {
    "heading": "3.1. Loss Decomposition Guided Search (LDGS)",
    "text": "Theorem (1) is the basis for our algorithm for computing approximations to the loss and its gradient. This approximation is computed by approximately solving the convex minimization problem (12) without computing the whole score vector z, using a form of descent method on the λj variables (which we refer to as “message passing”). The gradient computations required for each step can be (approximately) done using an efficient MIPS data structure storing the D/B dimensional rows ofW (j). The details of the algorithm are given in Algorithm 2. It can be viewed as running a version of the Frank-Wolfe algorithm on an appropriate convex function.\nA sublinear in K time implementation of step 5 in the algorithm relies on the fact that both z̃j and λj are sparse vectors, which in turn relies on the fact that gradients of the loss functions of interest are either sparse or concentrated on a few coordinates. Step 9 in the algorithm moves the current solution towards the optimal solution λ∗j that we have a closed form formula for, thanks to the constructive proof of Theorem (1). This movement is only done for the set of coordinates of the gradients of high magnitude identified in step 5 of the algorithm, thus ensuring that only a few coordinates are updated. Thus essentially the algorithm is performing a greedy descent towards the optimal solution. For more details on how the data structures are maintained in the algorithm, refer to Section 4.\nAlgorithm 2 Greedy Message Passing\ninput a sample x, threshold parameters τ1, τ2 > 0, and access to B MIPS data structures Tj storing the rows ofW (j), for j ∈ [B] output Approximation to ∇L(f(x)).\n1: Query Tj with φ(j)(x) and threshold τ to find Sj := {k | |[zj ]k| > τ1}. 2: Construct a sparse approximation z̃j for zj by setting [z̃j ]k = [zj ]k for k ∈ Sj ∪ P , and [z̃j ]k = 0 for k 6∈ S ∪ P . 3: for t = 1, 2, . . . (until converged) do 4: Compute the set\nA := ⋃ j∈[B] {k | |[∇L(B(z̃j + λj))]k| > τ2}.\n5: Compute [λ∗j ]k = 1 B [z̃]k − [z̃j ]k for all k ∈ A and all j ∈ [B]. 6: Compute the step size η = 2t+2 . 7: For all k ∈ A and all j ∈ [B], update\n[λj ]k ← η[λ∗j ]k + (1− η)[λj ]k.\n8: end for 9: Output 1B ∑B j=1∇L(B(z̃j + λj))."
  }, {
    "heading": "3.2. Error Analysis",
    "text": "Define z̃ = ∑B j=1 z̃j . Note that ‖z − z̃‖∞ ≤ Bτ1, so the error in approximating L(z) by L(z̃) is at most Bτ1 times the `∞ Lipschitz constant of L, which is typically small as explained earlier. The algorithm essentially runs a FrankWolfe type method to converge to L(z̃). In the following, we analyze the convergence rate of the greedy message passing algorithm (Algorithm 2) to L(z̃). The analysis relies on smoothness of the loss function. A function is said to be 1/µ-smooth if its gradients are Lipschitz continuous with constant 1/µ. For the Cross-Entropy loss (2) we have µ = 1, and for the smoothed max-margin loss (10), µ is a tunable parameter, and we found setting µ ∈ [1, 5] works well in our experiments.\nTo analyze the algorithm, denote by Λ the BK dimensional vector 〈λ1,λ2, . . . ,λB〉 in any given step in the loop of the algorithm. Similarly let Λ∗ denote the BK dimensional vector composed of λ∗j . Define G(Λ) = 1 B ∑B j=1 L(B(z̃j+λj)), i.e. the objective function in (12).\nTheorem 2 (Greedy Message Passing). Suppose the loss function L is 1/µ-smooth. Then the suboptimality gap of Λ in the t-th step of the loop can be bounded as follows:\nG(Λ)−G(Λ∗) ≤ 2B‖Λ ∗‖2\nµ(t+ 2) + 2τ2 ln(t)‖Λ∗‖1\nProof. Since the loss function L is 1/µ-smooth, it is easy to check that G is B/µ-smooth. Thus, if ∆Λ is the change in Λ in a given step of the loop in the algorithm, then\nG(Λ + ∆Λ)−G(Λ) ≤ η〈∇G(Λ),∆Λ〉+ η 2B\n2µ ‖∆Λ‖2.\nNote that ∆Λ equals Λ∗−Λ in all coordinates except those corresponding to k /∈ A for all j ∈ [B], and the magnitude of the gradient in those coordinates is at most τ2. Thus we have 〈∇G(Λ),∆Λ〉 ≤ 〈∇G(Λ),Λ∗−Λ〉+ τ2‖Λ∗‖1. Here, we used the fact that each coordinate of Λ lies between 0 and the corresponding coordinate of Λ∗. Next, by the convexity of G, we have 〈∇G(Λ),Λ∗ − Λ〉 ≤ G(Λ∗) − G(Λ). Putting all the bounds together and following some algebraic manipulations, we have\nG(Λ + ∆Λ)−G(Λ∗)\n≤ (1− η)(G(Λ)−G(Λ∗)) + ητ2‖Λ∗‖1 + η2B\n2µ ‖Λ∗‖2.\n(13)\nHere, we used the fact that each coordinate of Λ lies between 0 and the corresponding coordinate of Λ∗ to get the bound ‖∆Λ‖2 ≤ ‖Λ∗‖2.\nNow, using the fact that η = 2t+2 in iteration t, a simple induction on t implies the claimed bound on G(Λ)−G(Λ∗).\nThus, to ensure that the suboptimality gap is at most , it suffices to run the greedy procedure for T = B‖Λ\n∗‖2 4µ steps\nwith τ2 = 4 ln(T )‖Λ∗‖1 . While this theorem provides a proof of convergence for the algorithm to any desired error level, the bound it provides is quite weak. In practice, we found that running just one step of the loop suffices to improve performance over direct search-based methods.\nIf, in addition to being smooth, the loss function is also strongly convex (which can be achieved by adding some `22 regularization, for instance) then we can also show convergence of the gradients. This is because for strongly convex functions the convergence of gradients can be bounded in terms of the convergence of the loss value. This is a very standard analysis and we omit it for the sake of clarity.\nCost Analysis. Exact gradient evaluation for a single sample can be computed in O(DK) time. Directly applying a search-based gradient approximation (Algorithm 1) has a cost ofO(DQD(K)),whereQD(K) is the number of classes retrieved in the MIPS data structure in order to find all classes of significant gradients. The query cost QD(K) has a strong dependency on the dimension D. Exact MIPS has a cost QD(K) exponential in D (Shrivastava & Li, 2014; Li & Malik, 2017). For approximate search methods,\nsuch as Locality Sensitive Hashing (LSH), the costQD(K) typically only implicitly depends on the dimension. Our method (Algorithm 2) dividesD intoB subproblems of dimension D/B with a cost per message passing iteration of O(DQD/B(K)+DB|A|), whereA is the set computed in step 4 of Algorithm 2. Note QD/B(K) decreases with B rapidly (exponentially in the exact case) and therefore one can select B such that QD/B(K) QD(K) and balance two terms s.t. (DQD/B(K) +DB|A|) DK."
  }, {
    "heading": "4. Practical Considerations",
    "text": "MIPS queries. In practice when using the MIPS data strcuctures, instead of retrieving all classes with scores more than the threshold τ1, it is more efficient to retrieve the top Q classes with the highest scores. In our implementation, we use Spherical Clustering (Auvolat et al., 2015) as the MIPS data structure, where the number of clusters C is selected such that K/C ≤ Q and C ≤ Q. Note this requires Q ≥ √ K, leading to a speedup bounded by√\nK. Similarly, for computing the active set A in step 4 of Algorithm 2, we can compute an appropriate threshold τ2 using the properties of the loss function. In the case of margin-based losses, (3) and (7), and their smoothed versions (10), the gradient is sparse so τ2 can be set to 0 or some very small value (τ2 = 10−3 works well in our experiments). Loss functions like (2), (6) typically have exponentially decayed gradient magnitudes over the nonconfusing negative classes. For these losses, classes can be retrieved in decreasing order of gradient magnitude, using a lower bound on the partition function Z = ∑ k exp zk summing over only the subset of retrieved classes in order to decide whether more classes need to be retrieved or not.\nUpdates of data structures. During training the model parameters determining f will change, and the data structures Tj need to be updated. These data structures stores rows of W and treats φ(x) as query. For loss functions with a sparse gradient, such as (3) and (7), and their smoothed versions (10), the number of updated rows ofW , kr, is much smaller than K and Q (the number of classes retrieved for a query). Thus the cost for re-indexing rows ofW is krC(D/B)B = krCD, where C is the number of inner products required to index each row, which is much smaller than the costs of query and updates. For tasks with large number of updated rows (kr ≈ Q), the method is still effective with a larger mini-batch size Nb. As the costs of query and updates grow with Nb while the number of rows to re-index is bounded by K, the cost of maintaining data structure becomes insignificant.\nSampling for initialization. For a randomly initialized model, the early iterates of learning have gradients evenly distributed over the classes, as the scores of all classes are\nclose to each other. Therefore, it is unnecessary to search candidates of significant gradient magnitude in the early stage. In practice, one can switch from a sampling-based gradient approximation to a search-based gradient approximation after a number of mini-batch updates. In our experiments of unsupervised learning of word embeddings, we initialize the algorithm with a single epoch of SGD with sampling gradient approximation."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we conduct experiments on three types of problems: (i) multiclass classification (face recognition), (ii) multilabel classification (document tagging), and (iii) Unsupervised Word Embedding (Skip-gram objective (9)). For multiclass and multilabel classification, we employ a Stochastic Gradient Descent (SGD) optimization algorithm, with an initial step size chosen from {1, 0.1, 0.01} for the best performance of each method, with a 1/(1 + t) cooling scheme where t is the iteration counter. The minibatch size is 10 and all methods are parallelized with 10 CPU cores in a shared-memory architecture, running on a dedicated machine. All the implementation are in C++. The following loss functions and gradient evaluation methods are compared for the experiments on multiclass and multilabel classification:\n• Softmax: exact gradient evaluation of the crossentropy loss (2). For multiclass, we have |P| = 1 and for multilabel, |P| K.\n• Sampled-Softmax: the sampling strategy in (Jean et al., 2014; Chen et al., 2015), which includes all positive classes of the instances and uniformly subsamples from the remaining negative classes. Here we choose sample size as K/100.\n• Margin: exact gradient evaluation of the smoothed max-margin loss (10), where we choose µ = 1 for the case of multiclass, and µ = 5 for the case of multilabel. The bi-simplex projection (11) is computed in O(K logK) using the procedure described in (Yen et al., 2016). Note the gradient update for this loss is faster than that for cross-entropy, as the loss gradient is very sparse, making the backward pass much faster.\n• MIPS: search-based gradient evaluation (Algorithm 1) with smoothed max-margin loss (same setting to Margin). We use Spherical Clustering (Auvolat et al., 2015) with 100 centroids as the MIPS data structure, and a batch query of size K/100.\n• Decomp-MIPS: gradient evaluation via decomposed search (Algorithm 2, T = 1 iteration). We divide the inner product into B = 8 factors in the multiclass experiment and B = 4 in the multilabel case. The settings for MIPS data structure are the same as above."
  }, {
    "heading": "5.1. Multiclass Classificatoin",
    "text": "For multiclass classification we conduct experiments on the largest publicly available facial recognition dataset MegaFace (Challenge 2)2, where each identity is considered a class, and each sample is an image cropped by a face detector. The data set statistics are shown in Table 1.\nWe employ the FaceNet architecture (Schroff et al., 2015)3 pre-trained on the MS-Celeb-1M dataset, and fine-tune its last layer on the MegaFace dataset. The input of the last layer is an embedding of size 128, which is divided into B = 8 factors, each of dimension 16, in the Decomp-MIPS method.\nThe result is shown in Figure 1, where all methods are run for more than one day. Firstly, comparing methods\n2http://megaface.cs.washington.edu/. 3github.com/davidsandberg/facenet\nthat optimize the (smoothed) max-margin loss (DecompMIPS, MIPS and Margin) shows that both Decomp-MIPS, MIPS speed up the iterates by 1 ∼ 2 orders of magnitude. However, MIPS converges at an accuracy much lower than Decomp-MIPS and the gap gets bigger when running for more iterations. Note the time and epochs are in log scale. Secondly, Softmax has a much slower progress compared to Margin. Note both of them do not even finish one epoch (4.7M samples) after one day, while the progress of Margin is much better, presumably because its focus on the confusing identities. Sampled-Softmax has much faster iterates, but the progress per iterate is small, leading to slower overall progress compared to the MIPS-based approaches."
  }, {
    "heading": "5.2. Multilabel Classification",
    "text": "For multilabel classification, we conduct experiments on WikiLSHTC (Partalas et al., 2015), a benchmark data set in the Extreme Classification Repository4, where each class is a catalog tag in the Wikipedia, and each sample is a document with bag of words representation. The data statistics\n4manikvarma.org/downloads/XC/ XMLRepository.html\nare shown in Table 2.\nWe train a one-hidden-layer fully-connected feedforward network for the multilabel classification task. The first layer has input dimension equal to the vocabulary size (1.6M) and an output of dimension 100. The second layer has output size equal to the number of classes (325K), with different loss functions and approximations for different methods in comparison. The training result also produces document and work embedding as by-products. For Decomp-MIPS, the input of the last layer is divided into B = 4 factors, each of dimension 25.\nWe run all the compared methods for more than one day and the result is shown in Figure 2. First, for this multilabel task, Softmax has very good per-iteration progress, significantly more than that from the other three approaches based on the smoothed max-margin loss (Margin, MIPS, Decomp-MIPS). However, the iterates of Softmax are much slower than the others as it has a dense loss gradient and thus a slower backpropagation, so that when comparing training time, Softmax performs similarly to Margin. On the other hand, when comparing Margin Decomp-MIPS, and MIPS in progress per epoch, the updates of DecompMIPS achieve almost the same progress as the exact gradient calculation of Margin, while MIPS has a significant drop in its training accuracy compared with Margin and Decomp-MIPS, since it runs for more iterations. Overall, the MIPS-based methods lead to an order of magnitude speedup, while Decomp-MIPS retains the accuracy of the exact method. On the other hand, Sampled-Softmax has an extremely slow per-iteration progress despite its fast iterates, and could not reach a comparable accuracy to other methods even after one day."
  }, {
    "heading": "5.3. Unsupervised Word Embedding",
    "text": "In this section, we evaluate the proposed gradient approximation technique on the word embedding task with the Skip-gram learning objective (9) and compare it with two widely-used gradient approximation methods — Hierarchical Softmax (Word2vec-HS) and Negative Sampling (Word2vec-Neg) (Mikolov et al., 2013) implemented in the\nword2vec5 package released by the authors. The sample size for Word2vec-Neg is selected from {5, 10, 15, 20, 25}.\nWe use the benchmark data set BillonW6 of almost a half million vocabulary size. The data statistics are provided in Table 3. Following (Mikolov et al., 2013), we use a window of size 8 and subsample frequent words in the corpus. Each word w is dropped with probability max{1 − √ t fw , 0} where fw is the relative frequency of the word in the corpus, and t = 10−4 is a threshold parameter.\nNote that the Skip-gram objective (9) is presented in a collapsed form equivalent to the one in (Mikolov et al., 2013). Here, all terms of the same input-output pairs are grouped together and weighted by the frequency. We compute gradients from the positive outputs by summing over the empirical input-output distribution qx, qy|x in (9). Then we perform gradient descent (GD) updates on the parameters of input words {φ(x)}x∈X and output words {φ(y)}y∈X alternately. We use GD, GD-MIPS and GD-Decomp-MIPS to denote the algorithm with different strategies of loss approximations. As mentioned in Section 4, since in the early iterates the model has quite evenly distributed gradient over candidates, we use 1 epoch of Word2vec-Neg to initialize GD, GD-MIPS and GD-Decomp-MIPS. For this task, we have many more negative classes of significant gradient magnitude than in the multilabel and multiclass experiments. So we use a batch query of size K/20 instead of K/100 to the MIPS structure. All the compared methods are parallelized with 24 CPU cores.\nThe results are shown in Figure 3. After the first epoch, methods based on alternating gradient descent (GD) (with the collapsed objective (9)) have faster convergence per epoch, and the iterations of GD-Deomp-MIPS are 5 times faster than those of GD while having a significantly better objective value than GD-MIPS for the same training time.\n5code.google.com/archive/p/word2vec/ 6www.statmt.org/lm-benchmark/"
  }, {
    "heading": "Acknowledgements",
    "text": "I.Y. and P.R. acknowledge the support of NSF via IIS1149803."
  }],
  "year": 2018,
  "references": [{
    "title": "Clustering is efficient for approximate maximum inner product search",
    "authors": ["A. Auvolat", "S. Chandar", "P. Vincent", "H. Larochelle", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1507.05910,",
    "year": 2015
  }, {
    "title": "Sparse local embeddings for extreme multi-label classification",
    "authors": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
    "authors": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J Eckstein"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "Strategies for training large vocabulary neural language models",
    "authors": ["W. Chen", "D. Grangier", "M. Auli"],
    "venue": "arXiv preprint arXiv:1512.04906,",
    "year": 2015
  }, {
    "title": "Neighbourhood components analysis",
    "authors": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R.R. Salakhutdinov"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2005
  }, {
    "title": "Quantization based fast inner product search",
    "authors": ["R. Guo", "S. Kumar", "K. Choromanski", "D. Simcha"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1412.2007,",
    "year": 2014
  }, {
    "title": "Fast k-nearest neighbour search via prioritized dci",
    "authors": ["K. Li", "J. Malik"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2013
  }, {
    "title": "A scalable hierarchical distributed language model",
    "authors": ["A. Mnih", "G.E. Hinton"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Hierarchical probabilistic neural network language model",
    "authors": ["F. Morin", "Y. Bengio"],
    "venue": "In Aistats,",
    "year": 2005
  }, {
    "title": "No fuss distance metric learning using proxies",
    "authors": ["Y. Movshovitz-Attias", "A. Toshev", "T.K. Leung", "S. Ioffe", "S. Singh"],
    "venue": "arXiv preprint arXiv:1703.07464,",
    "year": 2017
  }, {
    "title": "Learning and inference via maximum inner product search",
    "authors": ["S. Mussmann", "S. Ermon"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Fast amortized inference and learning in log-linear models with randomly perturbed nearest neighbor search",
    "authors": ["S. Mussmann", "D. Levy", "S. Ermon"],
    "venue": "arXiv preprint arXiv:1707.03372,",
    "year": 2017
  }, {
    "title": "Smooth minimization of non-smooth functions",
    "authors": ["Y. Nesterov"],
    "venue": "Mathematical programming,",
    "year": 2005
  }, {
    "title": "Lshtc: A benchmark for large-scale text classification",
    "authors": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "Amini", "M.-R", "P. Galinari"],
    "venue": "arXiv preprint arXiv:1503.08581,",
    "year": 2015
  }, {
    "title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning",
    "authors": ["Y. Prabhu", "M. Varma"],
    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2014
  }, {
    "title": "Facenet: A unified embedding for face recognition and clustering",
    "authors": ["F. Schroff", "D. Kalenichenko", "J. Philbin"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2015
  }, {
    "title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)",
    "authors": ["A. Shrivastava", "P. Li"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "A new unbiased and efficient class of lsh-based samplers and estimators for partition function computation in log-linear models",
    "authors": ["R. Spring", "A. Shrivastava"],
    "venue": "arXiv preprint arXiv:1703.05160,",
    "year": 2017
  }, {
    "title": "Scalable and sustainable deep learning via randomized hashing",
    "authors": ["R. Spring", "A. Shrivastava"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2017
  }, {
    "title": "Deep networks with large output spaces",
    "authors": ["S. Vijayanarasimhan", "J. Shlens", "R. Monga", "J. Yagnik"],
    "venue": "arXiv preprint arXiv:1412.7479,",
    "year": 2014
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["K.Q. Weinberger", "L.K. Saul"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Multiscale quantization for fast similarity search",
    "authors": ["X. Wu", "R. Guo", "A.T. Suresh", "S. Kumar", "D.N. Holtmann-Rice", "D. Simcha", "X.Y. Felix"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification",
    "authors": ["Yen", "I.E.-H", "X. Huang", "P. Ravikumar", "K. Zhong", "I. Dhillon"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Large-scale multi-label learning with missing labels",
    "authors": ["Yu", "H.-F", "P. Jain", "P. Kar", "I. Dhillon"],
    "venue": "In International conference on machine learning,",
    "year": 2014
  }],
  "id": "SP:6504d5fdd9d7811211dc9f1b5ff85c93b0c0892e",
  "authors": [{
    "name": "Ian E.H. Yen",
    "affiliations": []
  }, {
    "name": "Satyen Kale",
    "affiliations": []
  }, {
    "name": "Felix X. Yu",
    "affiliations": []
  }, {
    "name": "Dan Holtmann-Rice",
    "affiliations": []
  }, {
    "name": "Sanjiv Kumar",
    "affiliations": []
  }, {
    "name": "Pradeep Ravikumar",
    "affiliations": []
  }],
  "abstractText": "For problems with large output spaces, evaluation of the loss function and its gradient are expensive, typically taking linear time in the size of the output space. Recently, methods have been developed to speed up learning via efficient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS). However, the performance of such data structures typically degrades in high dimensions. In this work, we propose a novel technique to reduce the intractable high dimensional search problem to several much more tractable lower dimensional ones via dual decomposition of the loss function. At the same time, we demonstrate guaranteed convergence to the original loss via a greedy message passing procedure. In our experiments on multiclass and multilabel classification with hundreds of thousands of classes, as well as training skip-gram word embeddings with a vocabulary size of half a million, our technique consistently improves the accuracy of search-based gradient approximation methods and outperforms sampling-based gradient approximation methods by a large margin.",
  "title": "Loss Decomposition for Fast Learning in Large Output Spaces"
}