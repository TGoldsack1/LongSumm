{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 414–419 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n414"
  }, {
    "heading": "1 Introduction",
    "text": "Event Detection (ED) is an important subtask of event extraction. It extracts event triggers from individual sentences and further identifies the type of the corresponding events. For instance, according to the ACE-2005 annotation guideline, in the sentence “Jane and John are married”, an ED system should be able to identify the word “married” as a trigger of the event “Marry”. However, it may be difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts.\nExisting ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., (McClosky et al., 2011; Hong et al., 2011; Li et al., 2014)) and representation-based methods (e.g., (Nguyen and Grishman, 2015; Chen et al.,\n2015; Liu et al., 2016a; Chen et al., 2017)). The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information. In general, most of these existing methods mainly exploit sentence-level contextual information. However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document. For example, there are the following sentences in ACE-2005:\n... I knew it was time to leave. Isn’t that a great argument for term limits? ... If we only examine the first sentence, it is hard to determine whether the trigger “leave” indicates a “Transport” event meaning that he wants to leave the current place, or an “End-Position” event indicating that he will stop working for his current organization. However, if we can capture the contextual information of this sentence, it is more confident for us to label “leave” as the trigger of an “End-Position” event. Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED. However, they suffer from two major limitations. First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible. Besides, a representation-based study has been presented in (Duan et al., 2017), which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier. However, as being limited by the unsupervised training\nprocess, the document-level representation cannot specifically capture event-related information.\nIn this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level. This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences. This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated. It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation (Zhang et al., 2017), question answering (Hao et al., 2017), document summarization (Tan et al., 2017), etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents.\nWe evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance. Experimental results show that the DEEBRNN model outperforms both feature-based and\nrepresentation-based state-of-the-art methods in terms of recall and F1-measure."
  }, {
    "heading": "2 The Proposed Model",
    "text": "We formalize ED as a multi-class classification problem. Given a sentence, we treat every word in it as a trigger candidate, and classify each candidate to a certain event type. In the ACE-2005 dataset, there are 8 event types, further being divided into 33 subtypes, and a “Not Applicable (NA)” type. Without loss of generality, in this paper we regard the 33 subtypes as 33 event types. Figure 1 presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules:\n1. The ED Oriented Document Embedding Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised attention mechanism.\n2. The Document-level Enhanced Event Detector (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents."
  }, {
    "heading": "2.1 The EDODEL Module",
    "text": "To learn the ED oriented embedding of a document, we apply the hierarchical and supervised attention network presented in Figure 1, which consists of a word-level Bi-GRU (Schuster and Paliwal, 2002) encoder with attention on event triggers\nand a sentence-level Bi-GRU encoder with attention on sentences with events. Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences.\nWord-level embeddings Given a sentence si (i = 1, 2, ..., L) consisting of words {wit|t = 1, 2, ..., T}. For each word wit, we first concatenate its embedding wit and its entity type embedding1 eit (Nguyen and Grishman, 2015) as the input git of a Bi-GRU and thus obtain the bidirectional hidden state hit:\nhit = [ −−−−→ GRUw(git), ←−−−− GRUw(git)]. (1)\nWe then feed hit to a perceptron with no bias to get uit = tanh(Wwhit) as a hidden representation of hit and also obtain an attention weight αit = u T itcw, which should be normalized through a softmax function. Here, similar to that in (Yang et al., 2016), cw is a vector representing the wordlevel context of wit, which is initialized at random. Finally, the embedding of the sentence si can be obtained by summing up hit with their weights:\nsi = T∑ t=1 αithit. (2)\nTo pay more attention to trigger words than other words, we construct the gold word-level attention signals α∗i for the sentence si, as illustrated in Figure 2a. We can then take the square error as the general loss of the attention at word level to supervise the learning process:\nEw(α ∗,α) = L∑ i=1 T∑ t=1 (α∗it − αit)2. (3)\n1The words in the ACE-2005 dataset are annotated with their entity types (annotated as “NA” if they are not an entity).\nSentence-level embeddings Given the sentence embeddings {si|i = 1, 2, ..., L}, we first get the hidden state qi via a Bi-GRU:\nqi = [ −−−→ GRUs(si), ←−−− GRUs(si)]. (4)\nThen we feed qi to a perceptron with no bias to get the hidden representation ti = tanh(Wsqi) and also obtain an attention weight βi = tTi cs to be normalized via softmax. Similarly, cs represents the sentence-level context of si to be randomly initialized. We eventually obtain the document embedding d as:\nd = L∑ i=1 βisi. (5)\nWe also think that the sentences containing event should obtain more attention than other ones. Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β∗ for the document d, as illustrated in Figure 2b, and further take the square error as the general loss of the attention at sentence level to supervise the learning process:\nEs(β ∗,β) = L∑ i=1 (β∗i − βi)2. (6)"
  }, {
    "heading": "2.2 The DEED Module",
    "text": "We employ another Bi-GRU encoder and a softmax output layer to model the ED task, which can handle event triggers with multiple words. Specifically, given a sentence sj (j = 1, 2, ..., L) in document d, for each of its word wjt (t = 1, 2, ..., T ), we concatenate its word embedding wjt and entity type embedding ejt with the corresponding document embedding d as the input rjt of the Bi-GRU and thus obtain the hidden state fjt:\nfjt = [ −−−→ GRUe(rjt), ←−−− GRUe(rjt)]. (7)\nFinally, we get the probability vector ojt with K dimensions through a softmax layer for wjt, where the k-th element, o(k)jt , of ojt indicates the probability of classifying wjt to the k-th event type. The loss function, J(y,o), can thus be defined in terms of the cross-entropy error of the real event type yjt and the predicted probability o(k)jt as follows:\nJ(y,o) = − L∑\nj=1 T∑ t=1 K∑ k=1 I(yjt = k)log o (k) jt , (8)\nwhere I(·) is the indicator function."
  }, {
    "heading": "2.3 Joint Training of the DEEB-RNN model",
    "text": "In the DEEB-RNN model, the above two modules are jointly trained. For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows:\nJ(θ)= ∑ ∀d∈ϕ (J(y,o)+λEw(α ∗,α)+µEs(β ∗,β)), (9) where θ denotes, as a whole, the parameters used in DEEB-RNN, ϕ is the training document set, and λ and µ are hyper-parameters for striking a balance among J(y,o), Ew(α∗,α) and Es(β∗,β)."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Datasets and Settings",
    "text": "We validate the proposed model through comparison with state-of-the-art methods on the ACE2005 dataset. In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents. All the data preprocessing and evaluation criteria follow those in (Ghaeini et al., 2016).\nHyper-parameters are tuned on the validation set. We set the dimension of the hidden layers corresponding to GRUw, GRUs, and GRUe to 300, 200, and 300, respectively, the output size of Ww and Ws to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5. In addition, we utilize the pre-trained word embeddings with 300 dimensions from (Mikolov et al., 2013) for initialization. For entity types, their embeddings are randomly initialized. We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout (Krizhevsky et al., 2012) for regularization."
  }, {
    "heading": "3.2 Baseline Models",
    "text": "In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines.\nSentence-level is a feature-based model proposed in (Hong et al., 2011), which regards entitytype consistency as a key feature to predict event mentions.\nJoint Local is a feature-based model developed in (Li et al., 2013), which incorporates such features that explicitly capture the dependency among multiple triggers and arguments.\nJRNN is a representation-based model proposed in (Nguyen et al., 2016), which exploits the inter-dependency between event triggers and argument roles via discrete structures.\nSkip-CNN is a representation-based model presented in (Nguyen and Grishman, 2016), which proposes a novel convolution to exploit nonconsecutive k-grams for event detection.\nANN-S2 is a representation-based model developed in (Liu et al., 2017), which explicitly exploits argument information for event detection via supervised attention mechanisms.\nCross-event is a feature-based model proposed in (Liao and Grishman, 2010), which learns relations among event types from training corpus and futher helps predict the occurrence of events.\nPSL is a feature-based model developed in (Liu et al., 2016b), which encods global information such as event-event association in the form of logic using the probabilistic soft logic model.\nDLRNN is a representation-based model proposed in (Duan et al., 2017), which automatically extracts cross-sentence clues to improve sentencelevel event detection."
  }, {
    "heading": "3.3 Impacts of Different Attention Strategies",
    "text": "In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effectiveness of different attention strategies.\nBi-GRU is the basic ED model, which does not employ document-level embeddings.\nDEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters λ and µ are set to 0.\nDEEB-RNN1/2/3 means they uses the gold attention signals as supervision information. Specifically, DEEB-RNN1 uses only the gold word-level attention signal (λ = 1 and µ = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (λ = 0 and µ = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-\ntence levels (λ = 1 and µ = 1). Table 1 compares these methods, where we can observe that the methods with document embeddings (i.e., the last four) significantly outperform the pure Bi-GRU method, which suggests that document-level information is very beneficial for ED. An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance. This is because of the following reasons. On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers. In this way, the BiRNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers. That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers. Therefore, as compared to DEEB-RNN, the precision of DEEBRNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset. Therefore, those sentences with such event triggers cannot be detected. The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN. Moreover, DEEB-RNN3 shows the best performance, indicating that the gold attention signals at both word and sentence levels are useful for ED."
  }, {
    "heading": "3.4 Performance Comparison",
    "text": "Table 2 presents the overall performance of all methods on ACE-2005. We can see that different versions of DEEB-RNN consistently out-\nperform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others. The better performance of DEEB-RNN can be explained by the following reasons: (1) Compared with feature-based methods, including Sentencelevel, Joint Local, and representation-based methods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which enhance the ability of identifying trigger words; (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods using document-level information, such as DLRNN, our method can learn event detection oriented embeddings of documents through the hierarchical and supervised attention based Bi-RNN network."
  }, {
    "heading": "4 Conclusions and Future Work",
    "text": "In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection. We explored different strategies to construct gold word- and sentence-level attentions to focus on event information. Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure. In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions. In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported by National Key Research and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61772501, 61572473, 61572469, and 91646120. We are grateful to Dr. Liu Kang of the Institute of Automation, Chinese Academy of Sciences for very helpful discussion on event detection."
  }],
  "year": 2018,
  "references": [{
    "title": "Automatically labeled data generation for large scale event extraction",
    "authors": ["Yubo Chen", "Shulin Liu", "Xiang Zhang", "Kang Liu", "Jun Zhao."],
    "venue": "Association for Computational Linguistics, pages 409–419.",
    "year": 2017
  }, {
    "title": "Event extraction via dynamic multipooling convolutional neural networks",
    "authors": ["Yubo Chen", "Liheng Xu", "Kang Liu", "daojian zeng", "jun zhao"],
    "venue": "In Association for Computational Linguistics,",
    "year": 2015
  }, {
    "title": "Exploiting document level information to improve event detection via recurrent neural networks",
    "authors": ["Shaoyang Duan", "Ruifang He", "Wenli Zhao."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing - Volume",
    "year": 2017
  }, {
    "title": "Event nugget detection with forward-backward recurrent neural networks",
    "authors": ["Reza Ghaeini", "Xiaoli Fern", "Liang Huang", "Prasad Tadepalli."],
    "venue": "Association for Computational Linguistics, pages 369– 373.",
    "year": 2016
  }, {
    "title": "An endto-end model for question answering over knowledge base with cross-attention combining global knowledge",
    "authors": ["Yanchao Hao", "Yuanzhe Zhang", "Kang Liu", "Shizhu He", "Zhanyi Liu", "Hua Wu", "Jun Zhao."],
    "venue": "Association for Computational Lin-",
    "year": 2017
  }, {
    "title": "Using cross-entity inference to improve event extraction",
    "authors": ["Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu."],
    "venue": "Association for Computational Linguistics, pages 1127–1136.",
    "year": 2011
  }, {
    "title": "Modeling textual cohesion for event extraction",
    "authors": ["Ruihong Huang", "Ellen Riloff."],
    "venue": "AAAI, pages 1664–1670.",
    "year": 2012
  }, {
    "title": "Refining event extraction through cross-document inference",
    "authors": ["Heng Ji", "Ralph Grishman."],
    "venue": "Association for Computational Linguistics, pages 254– 262.",
    "year": 2008
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton."],
    "venue": "Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, pages 1097–",
    "year": 2012
  }, {
    "title": "Constructing information networks using one single model",
    "authors": ["Qi Li", "Heng Ji", "Yu Hong", "Sujian Li."],
    "venue": "Empirical Methods in Natural Language Processing, pages 1846–1851.",
    "year": 2014
  }, {
    "title": "Joint event extraction via structured prediction with global features",
    "authors": ["Qi Li", "Heng Ji", "Liang Huang."],
    "venue": "Association for Computational Linguistics, pages 73–82.",
    "year": 2013
  }, {
    "title": "Using document level cross-event inference to improve event",
    "authors": ["Shasha Liao", "Ralph Grishman"],
    "year": 2010
  }, {
    "title": "Leveraging framenet to improve automatic event detection",
    "authors": ["Shulin Liu", "Yubo Chen", "Shizhu He", "Kang Liu", "Jun Zhao."],
    "venue": "Association for Computational Linguistics, pages 2134–2143.",
    "year": 2016
  }, {
    "title": "Exploiting argument information to improve event detection via supervised attention mechanisms",
    "authors": ["Shulin Liu", "Yubo Chen", "Kang Liu", "Jun Zhao."],
    "venue": "Association for Computational Linguistics, pages 1789–1798.",
    "year": 2017
  }, {
    "title": "A probabilistic soft logic based approach to exploiting latent and global information in event classification",
    "authors": ["Shulin Liu", "Kang Liu", "Shizhu He", "Jun Zhao."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2993–2999.",
    "year": 2016
  }, {
    "title": "Event extraction as dependency parsing for bionlp 2011",
    "authors": ["David McClosky", "Mihai Surdeanu", "Christopher D. Manning."],
    "venue": "Association for Computational Linguistics, pages 41–45.",
    "year": 2011
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems -",
    "year": 2013
  }, {
    "title": "Joint event extraction via recurrent neural networks",
    "authors": ["Thien Nguyen", "Kyunghyun Cho", "Ralph Grishman."],
    "venue": "NAACL, pages 300–309.",
    "year": 2016
  }, {
    "title": "Event detection and domain adaptation with convolutional neural networks",
    "authors": ["Thien Nguyen", "Ralph Grishman."],
    "venue": "IJCNLP, pages 365–371.",
    "year": 2015
  }, {
    "title": "Modeling skip-grams for event detection with convolutional neural networks",
    "authors": ["Thien Nguyen", "Ralph Grishman."],
    "venue": "Empirical Methods in Natural Language Processing, pages 886–891.",
    "year": 2016
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["M. Schuster", "K.K. Paliwal."],
    "venue": "IEEE Transactions on Signal Processing, 45:2673–2681.",
    "year": 2002
  }, {
    "title": "Abstractive document summarization with a graphbased attentional neural model",
    "authors": ["Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao."],
    "venue": "Association for Computational Linguistics, pages 1171–1181.",
    "year": 2017
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "NAACL, pages 1480–1489.",
    "year": 2016
  }, {
    "title": "Incorporating word reordering knowledge into attention-based neural machine translation",
    "authors": ["Jinchao Zhang", "Mingxuan Wang", "Qun Liu", "Jie Zhou."],
    "venue": "Association for Computational Linguistics, pages 1524–1534.",
    "year": 2017
  }],
  "id": "SP:f49e2ce3c40cc07315d0e55744b1c7c04e7fbf9f",
  "authors": [{
    "name": "Yue Zhao",
    "affiliations": []
  }, {
    "name": "Xiaolong Jin",
    "affiliations": []
  }, {
    "name": "Yuanzhuo Wang",
    "affiliations": []
  }, {
    "name": "Xueqi Cheng",
    "affiliations": []
  }],
  "abstractText": "Document-level information is very important for event detection even at sentence level. In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences. This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events. It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences. Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",
  "title": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention"
}