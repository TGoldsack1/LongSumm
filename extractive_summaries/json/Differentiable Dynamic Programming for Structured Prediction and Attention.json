{
  "sections": [{
    "text": "structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation.\nModern neural networks are composed of multiple layers of nested functions. Although layers usually consist of elementary linear algebraic operations and simple nonlinearities, there is a growing need for layers that output the value or the solution of an optimization problem. This can be used to design loss functions that capture relevant regularities in the input (Lample et al., 2016; Cuturi & Blondel, 2017) or to create layers that impose prior structure on the output (Kim et al., 2017; Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017).\nAmong these works, several involve a convex optimization\n1Inria, CEA, Université Paris-Saclay, Gif-sur-Yvette, France. Work performed at 2NTT Communication Science Laboratories, Kyoto, Japan. Correspondence to: AM <arthur.mensch@m4x.org>, MB <mathieu@mblondel.org>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nproblem (Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017); others solve certain combinatorial optimization problems by dynamic programming (Kim et al., 2017; Cuturi & Blondel, 2017; Nowak et al., 2018). However, because dynamic programs (Bellman, 1952) are usually non-differentiable, virtually all these works resort to the formalism of conditional random fields (CRFs) (Lafferty et al., 2001), which can be seen as changing the semiring used by the dynamic program — replacing all values by their exponentials and all (max,+) operations with (+,×) operations (Verdu & Poor, 1987). While this modification smoothes the dynamic program, it looses the sparsity of solutions, since hard assignments become soft ones. Moreover, a general understanding of how to relax and differentiate dynamic programs is lacking. In this work, we propose to do so by leveraging smoothing (Moreau, 1965; Nesterov, 2005) and backpropagation (Linnainmaa, 1970). We make the following contributions.\n1) We present a unified framework for turning a broad class of dynamic programs (DP) into differentiable operators. Unlike existing works, we propose to change the semiring to use (maxΩ,+) operations, where maxΩ is a max operator smoothed with a strongly convex regularizer Ω (§1).\n2) We show that the resulting DP operators, that we call DPΩ, are smoothed relaxations of the original DP algorithm and satisfy several key properties, chief among them convexity. In addition, we show that their gradient,∇DPΩ, is equal to the expected trajectory of a certain random walk and can be used as a sound relaxation to the original dynamic program’s solution. Using negative entropy for Ω recovers existing CRF-based works from a different perspective — we provide new arguments as to why this Ω is a good choice. On the other hand, using squared ℓ2 norm for Ω leads to new algorithms whose expected solution is sparse. We derive a clean and efficient method to backpropagate gradients, both through DPΩ and ∇DPΩ. This allows us to define differentiable DP layers that can be incorporated in neural networks trained end-to-end (§2).\n3) We illustrate how to to derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for supervised time-series alignment (§3). The latter is illustrated in Figure 1. Finally, we showcase these two instanti-\nations on structured prediction tasks (§4) and on structured attention for neural machine translation (§5).\nNotation. We denote scalars, vectors and matrices using lower-case, bold lower-case and bold upper-case letters, e.g., y, y and Y . We denote the elements of Y by yi,j and its rows by yi. We denote the Frobenius inner product between A and B by 〈A,B〉 , ∑\ni,j ai,jbi,j . We denote the\n(D− 1)-probability simplex by△D , {λ ∈ RD+ : ‖λ‖1 = 1}. We write conv(Y) , { ∑\nY ∈Y λY Y : λ ∈ △ |Y|} the\nconvex hull of Y , [N ] the set {1, . . . , N} and supp(x) , {j ∈ [D] : xj 6= 0} the support of x ∈ R D. We denote the Shannon entropy by H(q) , ∑\ni qi log qi.\nWe have released an optimized and modular PyTorch implementation for reproduction and reuse."
  }, {
    "heading": "1. Smoothed max operators",
    "text": "In this section, we introduce smoothed max operators (Nesterov, 2005; Beck & Teboulle, 2012; Niculae & Blondel, 2017), that will serve as a powerful and generic abstraction to define differentiable dynamic programs in §2. Let Ω : RD → R be a strongly convex function on△D and let x ∈ RD. We define the max operator smoothed by Ω as:\nmaxΩ(x) , max q∈△D 〈q,x〉 − Ω(q). (1)\nIn other words, maxΩ is the convex conjugate of Ω, restricted to the simplex. From the duality between strong convexity and smoothness, maxΩ is smooth: differentiable everywhere and with Lipschitz continuous gradient. Since the argument that achieves the maximum in (1) is unique,\nfrom Danskin’s theorem (1966), it is equal to the gradient:\n∇maxΩ(x) = argmax q∈△D 〈q,x〉 − Ω(q).\nThe gradient is differentiable almost everywhere for any strongly-convex Ω (everywhere for negentropy). Next, we state properties that will be useful throughout this paper.\nLemma 1. Properties of maxΩ operators\nLet x = (x1, . . . , xD) ⊤ ∈ RD.\n1. Boundedness: If Ω is lower-bounded by LΩ,D and upper-bounded by UΩ,D on the simplex△ D, then\nmax(x)− UΩ,D ≤ maxΩ(x) ≤ max(x)− LΩ,D.\n2. Distributivity of + over maxΩ: maxΩ(x+ c1) = maxΩ(x) + c ∀c ∈ R.\n3. Commutativity: If Ω(Pq) = Ω(q), where P is a permutation matrix, then maxΩ(Px) = maxΩ(x).\n4. Non-decreasingness in each coordinate:\nmaxΩ(x) ≤ maxΩ(y) ∀x ≤ y\n5. Insensitivity to −∞: xj = −∞⇒ ∇maxΩ(x)j = 0.\nProofs are given in §A.1. In particular, property 3 holds whenever Ω(q) =\n∑D i=1 ω(qi), for some function ω. We\nfocus in this paper on two specific regularizers Ω: the negentropy −H and the squared ℓ2 norm. For these choices, all properties above are satisfied and we can derive closedform expressions for maxΩ, its gradient and its Hessian — see §B.1. When using negentropy, maxΩ becomes the log-sum-exp and∇maxΩ the softmax. The former satisfies associativity, which as we shall see, makes it natural to use in dynamic programming. With the squared ℓ2 regularization, as observed by Martins & Astudillo (2016); Niculae & Blondel (2017), the gradient∇maxΩ is sparse. This will prove useful to enforce sparsity in the models we study."
  }, {
    "heading": "2. Differentiable DP layers",
    "text": "Dynamic programming (DP) is a generic way of solving combinatorial optimization problems by recursively solving problems on smaller sets. We first introduce this category of algorithms in a broad setting, then use smoothed max operators to define differentiable DP layers."
  }, {
    "heading": "2.1. Dynamic programming on a DAG",
    "text": "Every problem solved by dynamic programming reduces to finding the highest-scoring path between a start node and an end node, on a weighted directed acyclic graph (DAG). We therefore introduce our formalism on this generic problem, and give concrete examples in §3.\nFormally, let G = (V, E) be a DAG, with nodes V and edges E . We write N = |V| ≥ 2 the number of nodes.\nWithout loss of generality, we number the nodes in topological order, from 1 (start) to N (end), and thus V = [N ]. Node 1 is the only node without parents, and node N the only node without children. Every directed edge (i, j) from a parent node j to a child node i has a weight θi,j ∈ R. We gather the edge weights in a matrix θ ∈ Θ ⊆ RN×N , setting θi,j = −∞ if (i, j) /∈ E and θ1,1 = 1. We consider the set Y of all paths in G from node 1 to node N . Any path Y ∈ Y can be represented as a N × N binary matrix, with yi,j = 1 if the path goes through the edge (i, j) and yi,j = 0 otherwise. In the sequel, paths will have a one-to-one correspondence with discrete structures such as sequences or alignments. Using this representation, 〈Y ,θ〉 corresponds to the cumulated sum of edge weights, along the path Y . The computation of the highest score among all paths amounts to solving the combinatorial problem\nLP(θ) , max Y ∈Y 〈Y ,θ〉 ∈ R. (2)\nAlthough the size of Y is in general exponential in N , LP(θ) can be computed in one topologically-ordered pass over G using dynamic programming. We let Pi be the set of parent nodes of node i in graph G and define recursively\nv1(θ) , 0\n∀ i ∈ [2, . . . , N ] : vi(θ) , max j∈Pi θi,j + vj(θ). (3)\nThis algorithm outputs DP(θ) , vN (θ). We now show that this is precisely the highest score among all paths.\nProposition 1. Optimality of dynamic programming\n∀θ ∈ Θ : DP(θ) = LP(θ)\nThe optimality of recursion (3) is well-known (Bellman, 1952). We prove it again with our formalism in §A.2, since it exhibits the two key properties that the max operator must satisfy to guarantee optimality: distributivity of + over it and associativity. The cost of computing DP(θ) is O(|E|), which is exponentially better than O(|Y|).\nIn many applications, we will often rather be interested in the argument that achieves the maximum, i.e., one of the highest-scoring paths\nY ⋆(θ) ∈ argmax Y ∈Y 〈Y ,θ〉. (4)\nThis argument can be computed by backtracking, that we now relate to computing subgradients of LP(θ).\nLinear program, lack of differentiality. Unfortunately, LP(θ) is not differentiable everywhere. To see why this is the case, notice that (2) can be rewritten as a linear program over the convex polytope conv(Y):\nLP(θ) = max Y ∈conv(Y) 〈Y ,θ〉.\nFrom the generalized Danskin theorem (Bertsekas, 1971),\nY ⋆(θ) ∈ ∂LP(θ) = argmax Y ∈conv(Y) 〈Y ,θ〉,\nwhere ∂ denotes the subdifferential of LP(θ), i.e., the set of subgradients. When Y ⋆(θ) is unique, ∂LP(θ) is a singleton and Y ⋆ is equal to the gradient of LP(θ), that we write∇LP(θ). Unfortunately, Y ⋆(θ) is not always unique, meaning that LP(θ) is not differentiable everywhere. As we will show in §4.2, this hinders optimization as we can only train models involving LP(θ) with subgradient methods. Worse, Y ⋆(θ), a function from Θ to Y , is discontinuous and has null or undefined derivatives. It is thus impossible to use it in a model trained by gradient descent."
  }, {
    "heading": "2.2. Smoothed max layers",
    "text": "To address the lack of differentiability of dynamic programming, we introduce the operator maxΩ, presented in §1, and consider two approaches.\nSmoothing the linear program. Let us define the Ωsmoothed maximum of a function f : Y → R over a finite set Y using the following shorthand notation:\nmaxΩ Y ∈Y f(Y ) , maxΩ((f(Y ))Y ∈Y).\nA natural way to circumvent the lack of differentiability of LP(θ) is then to replace the global max operator by maxΩ:\nLPΩ(θ) , maxΩ Y ∈Y 〈Y ,θ〉 ∈ R. (5)\nFrom §1, LPΩ(θ) is convex and, as long as Ω is strongly convex, differentiable everywhere. In addition, ∇LPΩ(θ) is Lipschitz continuous and thus differentiable almost everywhere. Unfortunately, solving (5) for general Ω is likely intractable when Y has an exponential size.\nSmoothing the dynamic program. As a tractable alternative, we propose an algorithmic smoothing. Namely, we replace max by maxΩ locally within the DP recursion. Omitting the dependence on Ω, this defines a smoothed recursion over the new sequence (vi(θ)) N i=1:\nv1(θ) , 0\n∀i ∈ [2, . . . , N ] : vi(θ) , maxΩ j∈Pi θi,j + vj(θ). (6)\nThe new algorithm outputs DPΩ(θ), vN (θ), the smoothed highest score. Smoothing the max operator locally brings the same benefit as before — DPΩ(θ) is smooth and ∇DPΩ(θ) is differentiable almost everywhere. However, computing DPΩ(θ) is now always tractable, since it simply requires to evaluate (vi(θ)) N i=1 in topological order, as in\nthe original recursion (3). Although LPΩ(θ) and DPΩ(θ) are generally different (in fact, LPΩ(θ) ≥ DPΩ(θ) for all θ ∈ Θ), we now show that DPΩ(θ) is a sensible approximation of LP(θ) in several respects.\nProposition 2. Properties of DPΩ\n1. DPΩ(θ) is convex\n2. LP(θ)−DPΩ(θ) is bounded above and below: it lies in (N − 1)[LΩ,N , UΩ,N ], with Lemma 1 notations.\n3. When Ω is separable, DPΩ(θ) = LPΩ(θ) if and only if Ω = −γH , where γ ≥ 0.\nProofs are given in §A.3. The first claim can be surprising due to the recursive definition of DPΩ(θ). The second claim implies that DPγΩ(θ) converges to LP(θ) when the regularization vanishes: DPγΩ(θ)→γ→0 LP(θ); LPγΩ(θ) also satisfies this property. The “if” direction of the third claim follows by showing that max−γH satisfies associativity. This recovers known results in the framework of message passing algorithms for probabilistic graphical models (e.g., Wainwright & Jordan, 2008, Section 4.1.3), with a more algebraic point of view. The key role that the distributive and associative properties play into breaking down large problems into smaller ones has long been noted (Verdu & Poor, 1987; Aji & McEliece, 2000). However, the “and only if” part of the claim is new to our knowledge. Its proof shows that max−γH is the only maxΩ satisfying associativity, exhibiting a functional equation from information theory (Horibe, 1988). While this provides an argument in favor of entropic regularization, ℓ22 regularization has different benefits in terms of sparsity of the solutions."
  }, {
    "heading": "2.3. Relaxed argmax layers",
    "text": "It is easy to check that ∇LPΩ(θ) belongs to conv(Y) and can be interpreted as an expected path under some distribution induced by ∇maxΩ, over all possible Y ∈ Y — see §A.4 for details. This makes ∇LPΩ(θ) interpretable as a continuous relaxation of the highest-scoring path Y ⋆(θ) defined in (4). However, like LPΩ(θ), computing ∇LPΩ(θ) is likely intractable in the general case. Fortunately, ∇DPΩ(θ) is always easily computable by backpropagation and enjoys similar properties, as we now show.\nComputing ∇DPΩ(θ). Computing ∇DPΩ(θ) can be broken down into two steps. First, we compute and record the local gradients alongside the recursive step (6):\n∀ i ∈ [N ] : qi(θ) , ∇maxΩ(θi + v(θ)) ∈ △ N ,\nwhere v(θ) , (v1(θ), . . . , vN (θ)). Since we assume that θi,j = −∞ if (i, j) 6∈ E , we have supp(qi(θ)) = Pi. This ensures that, similarly to vi(θ), qi(θ) exclusively depends on (vj(θ))j∈Pi . Let Cj be the children of node j ∈ [N ]. A\nstraighforward application of backpropagation (cf. §A.5) yields a recursion run in reverse-topological order, starting from node j = N − 1 down to j = 1:\n∀ i ∈ Cj : ei,j ← ēiqi,j then ēj ← ∑\ni∈Cj\nei,j ,\nwhere ēN ← 1 and ei,j ← 0 for (i, j) /∈ E . The final output is ∇DPΩ(θ) = E. Assuming maxΩ can be computed in linear time, the total cost is O(|E|), the same as DP(θ). Pseudo-code is summarized in §A.5.\nAssociated path distribution. The backpropagation we derived has a probabilistic interpretation. Indeed, Q(θ) ∈ R N×N can be interpreted as a transition matrix: it defines a random walk on the graph G, i.e., a finite Markov chain with states V and transition probabilities supported by E . The random walk starts from node N and, when at node i, hops to node j ∈ Pi with probability qi,j . It always ends at node 1, which is absorbing. The walk follows the path Y ∈ Y with a probability pθ,Ω(Y ), which is simply the product of the qi,j of visited edges. Thus, Q(θ) defines a path distribution pθ,Ω. Our next proposition shows that ∇DPΩ(Y ) ∈ conv(Y) and is equal to the expected path Eθ,Ω[Y ] under that distribution.\nProposition 3. ∇DPΩ(θ) as an expected path\n∀θ ∈ Θ : ∇DPΩ(θ) = Eθ,Ω[Y ] = E ∈ conv(Y).\nProof is provided in §A.5. Moreover, ∇DPΩ(θ) is a principled relaxation of the highest-scoring path Y ⋆(θ), in the sense that it converges to a subgradient of LP(θ) as the regularization vanishes: ∇DPγΩ(θ) −−−→\nγ→0 Y ⋆(θ) ∈ ∂LP(θ).\nWhen Ω = −γH , the distributions underpinning LPΩ(θ) and DPΩ(θ) coincide and reduce to the Gibbs distribution pθ,Ω(Y ) ∝ exp(〈θ,Y 〉/γ). The value LPΩ(θ) = DPΩ(θ) is then equal to the log partition. When Ω = γ‖ · ‖2, some transitions between nodes have zero probability and hence some paths have zero probability under the distribution pθ,Ω. Thus, ∇DPΩ(θ) is typically sparse — this will prove interesting to introspect the various models we consider (typically, the smaller γ, the sparser ∇DPΩ(θ))."
  }, {
    "heading": "2.4. Multiplication with the Hessian∇2DPΩ(θ)Z",
    "text": "Using ∇DPΩ(θ) as a layer involves backpropagating through ∇DPΩ(θ). This requires to apply the Jacobian of ∇DPΩ operator (a linear map from R N×N to RN×N ), or in other words to apply the Hessian of DPΩ, to an input sensibility vector Z, computing\n∇2DPΩ(θ)Z = ∇〈∇DPΩ(θ),Z〉 ∈ R N×N ,\nwhere derivatives are w.r.t. θ. The above vector may be computed in two ways, that differ in the order in which\nderivatives are computed. Using automatic differentiation frameworks such as PyTorch (Paszke et al., 2017), we may backpropagate over the computational graph a first time to compute the gradient ∇DPΩ(θ), while recording operations. We may then compute 〈∇DPΩ(θ),Z〉, and backpropagate once again. However, due to the structure of the problem, it proves more efficient, adapting Pearlmutter’s approach (1994), to directly compute 〈∇DPΩ(θ),Z〉 ∈ R, namely, the directional derivative at θ along Z. This is done by applying the chain rule in one topologicallyordered pass over G. Similarly to the gradient computation, we record products with the local Hessians Hi(θ) , ∇2maxΩ(θi + v(θ)) along the way. We then compute the gradient of the directional derivative using backpropagation. This yields a recursion for computing ∇2DPΩ(θ)Z in reverse topological-order over G. The complete derivation and the pseudo-code are given in §A.7. They allow to implement DPΩ as as a custom twice-differentiable module in existing software. For both approaches, the computational cost is O(|E|), the same as for gradient computation. In our experiments in §4.2, our custom Hessian-vector product computation brings a 3×/12× speed-up during the backward pass on GPU/CPU vs. automatic differentiation.\nRelated works. Smoothing LP formulations was also used for MAP inference (Meshi et al., 2015) or optimal transport (Blondel et al., 2018) but these works do not address how to differentiate through the smoothed formulation. An alternative approach to create structured prediction layers, fundamentally different both in the forward and backward passes, is SparseMAP (Niculae et al., 2018).\nSummary. We have proposed DPΩ(θ), a smooth, convex and tractable relaxation to the value of LP(θ). We have also shown that ∇DPΩ(θ) belongs to conv(Y) and is therefore a sound relaxation to solutions of LP(θ). To conclude this section, we formally define our proposed two layers.\nDefinition 1. Differentiable dynamic programming layers\nValue layer: DPΩ(θ) ∈ R\nGradient layer: ∇DPΩ(θ) ∈ conv(Y)"
  }, {
    "heading": "3. Examples of computational graphs",
    "text": "We now illustrate two instantiations of our framework for specific computational graphs."
  }, {
    "heading": "3.1. Sequence prediction",
    "text": "We demonstrate in this section how to instantiate DPΩ to the computational graph of the Viterbi algorithm (Viterbi, 1967; Rabiner, 1990), one of the most famous instances of DP algorithm. We call the resulting operator VitΩ. We wish to tag a sequence X = (x1, . . . ,xT ) of vectors in R D\n(e.g., word representations) with the most probable output sequence (e.g., entity tags) y = (y1, . . . , yT ) ∈ [S] T . This problem can be cast as finding the highest-scoring path on a treillis G. While y can always be represented as a sparse N×N binary matrix, it is convenient to represent it instead as a T×S×S binary tensor Y , such that yt,i,j = 1 if y transitions from node j to node i on time t, and 0 otherwise — we set y0 = 1. The potentials can similarly be organized as a T×S×S real tensor, such that θt,i,j = φt(xt, i, j). Traditionally, the potential functions φt were human-engineered (Sutton et al., 2012, §2.5). In recent works and in this paper, they are learned end-to-end (Bottou et al., 1997; Collobert et al., 2011; Lample et al., 2016).\nUsing the above binary tensor representation, the inner product 〈Y ,θ〉 is equal to ∑T\nt=1 φt(xt, yt, yt−1), y’s cumulated score. This is illustrated in Figure 2 on the task of part-of-speech tagging. The bold arrows indicate one possible output sequence y, i.e., one possible path in G.\nWhen Ω = −H , we recover linear-chain conditional random fields (CRFs) (Lafferty et al., 2001) and the probability of y (Y in tensor representation) given X is\npθ,−H(y|X)∝ exp(〈Y ,θ〉)= exp (\nT∑\nt=1\nφt(xt, yt, yt−1) ) .\nFrom Prop. 3, the gradient ∇Vit−H(θ) = E ∈ R T×S×S is such that et,i,j = pθ,−H(yt = i, yt−1 = j|X). The marginal probability of state i at time t is simply pθ,−H(yt = i|X) = ∑S j=1 et,i,j . Using a different Ω simply changes the distribution over state transitions. When Ω = ‖ · ‖2, the marginal probabilities are typically sparse. Pseudo-code for VitΩ(θ), as well as gradient and Hessian-product computations, is provided in §B.2. The case Ω = ‖ · ‖2 is new to our knowledge.\nWhen Ω = −H , the marginal probabilities are traditionally computed using the forward-backward algorithm (Baum & Petrie, 1966). In contrast, we compute ∇Vit−H(θ) using backpropagation while efficiently maintaining the marginalization. An advantage of our approach is that all operations are numerically stable. The relation between forward-backward and backpropagation has been noted before (e.g., Eisner (2016)). However, the analysis is led using (+,×) operations, instead of (maxΩ,+) as we do. Our\nViterbi instantiation can be generalized to graphical models with a tree structure, and to approximate inference in general graphical models, since unrolled loopy belief propagation (Pearl, 1988) yields a dynamic program. We note that continuous beam search (Goyal et al., 2017) can also be cleanly rewritten and extended using VitΩ operators."
  }, {
    "heading": "3.2. Time-series alignment",
    "text": "We now demonstrate how to instantiate DPΩ to the computational graph of dynamic time warping (DTW) (Sakoe & Chiba, 1978), whose goal is to seek the minimal cost alignment between two time-series. We call the resulting operator DTWΩ. Formally, let NA and NB be the lengths of two time-series, A and B. Let ai and bj be the i th and jth observations of A and B, respectively. Since edge weights only depend on child nodes, it is convenient to rearrange Y and θ as NA × NB matrices. Namely, we represent an alignment Y as a NA × NB binary matrix, such that yi,j = 1 if ai is aligned with bj , and 0 otherwise. Likewise, we represent θ as a NA × NB matrix. A classical example is θi,j = d(ai, bj), for some differentiable discrepancy measure d. We write Y the set of all monotonic alignment matrices, such that the path that connects the upper-left (1, 1) matrix entry to the lower-right (NA, NB) one uses only ↓,→,ցmoves. The DAG associated with Y is illustrated in Figure 3 with NA = 4 and NB = 3 below.\nAgain, the bold arrows indicate one possible path Y ∈ Y from start to end in the DAG, and correspond to one possible alignment. Using this representation, the cost of an alignment (cumulated cost along the path) is conveniently computed by 〈Y ,θ〉. The value DTWΩ(θ) can be used to define a loss between alignments or between time-series. Following Proposition 3, ∇DTWΩ(θ) = E ∈ R NA×NB can be understood as a soft alignment matrix. This matrix is sparse when Ω = ‖ · ‖2, as illustrated in Figure 1 (right).\nPseudo-code to compute DTWΩ(θ) as well as its gradient and its Hessian products are provided in §B.3. When Ω = −H , DTWΩ(θ) is a conditional random field known as soft-DTW, and the probability pθ,Ω(Y |A,B) is a Gibbs distribution similar to §3.1 (Cuturi & Blondel, 2017).\nHowever, the case Ω = ‖ · ‖2 and the computation of ∇2DTWΩ(θ)Z are new and allow new applications."
  }, {
    "heading": "4. Differentiable structured prediction",
    "text": "We now apply the proposed layers, DPΩ(θ) and∇DPΩ(θ), to structured prediction (Bakır et al., 2007), whose goal is to predict a structured output Y ∈ Y associated with a structured input X ∈ X . We define old and new structured losses, and demonstrate them on two structured prediction tasks: named entity recognition and time-series alignment."
  }, {
    "heading": "4.1. Structured loss functions",
    "text": "Throughout this section, we assume that the potentials θ ∈ Θ have already been computed using a function from X to Θ and let C : Y×Y → R+ be a cost function between the ground-truth output Ytrue and the predicted output Y .\nConvex losses. Because C is typically non-convex, the cost-augmented structured hinge loss (Tsochantaridis et al., 2005) is often used instead for linear models\nℓC(Ytrue;θ) , max Y ∈Y C(Ytrue,Y )+〈Y ,θ〉−〈Ytrue,θ〉. (7)\nThis is a convex upper-bound on C(Ytrue,Y ⋆(θ)), where Y ⋆(θ) is defined in (4). To make the cost-augmented decoding tractable, it is usually assumed that C(Ytrue,Y ) is linear in Y , i. e., it can be written as 〈CYtrue ,Y 〉 for some matrix CYtrue . We can then rewrite (7) using our notation as\nℓC(Ytrue;θ) = LP(θ +CYtrue)− 〈Ytrue,θ〉.\nHowever, this loss function is non-differentiable. We therefore propose to relax LP by substituting it with DPΩ:\nℓC,Ω(Ytrue;θ) , DPΩ(θ +CYtrue)− 〈Ytrue,θ〉.\nLosses in this class are convex, smooth, tractable for any Ω, and by Proposition 2 property 2 a sensible approximation of ℓC . In addition, they only require to backpropagate through DPΩ(θ) at training time. It is easy to check that we recover the structured perceptron loss with ℓ0,0 (Collins, 2002), the structured hinge loss with ℓC,0 (Tsochantaridis et al., 2005) and the CRF loss with ℓ0,−H (Lafferty et al., 2001). The last one has been used on top of LSTMs in several recent works (Lample et al., 2016; Ma & Hovy, 2016). Minimizing ℓ0,−H(θ) is equivalent to maximizing the likelihood pθ,−H(Ytrue). However, minimizing ℓ0,‖·‖2 is not equivalent to maximizing pθ,‖·‖2(Ytrue). In fact, the former is convex while the latter is not.\nNon-convex losses. A direct approach that uses the output distribution pθ,Ω consists in minimizing the risk∑ y∈Y pθ,−H(Y )C(Ytrue,Y ). As shown by Stoyanov &\nEisner (2012), this can be achieved by backpropagating through the minimum risk decoder. However, the risk is usually non-differentiable, piecewise constant (Smith & Eisner, 2006) and several smoothing heuristics are necessary to make the method work (Stoyanov & Eisner, 2012).\nAnother principled approach is to consider a differentiable approximation ∆: Y × conv(Y) → R+ of the cost C. We can then relax C(Ytrue,Y\n⋆(θ)) by ∆(Ytrue,∇DPΩ(θ)). Unlike minimum risk training, this approach is differentiable everywhere when Ω = −H . Both approaches require to backpropagate through ∇DPΩ(θ), which is twice as costly as backpropagating through DPΩ(θ) (see §2.4)."
  }, {
    "heading": "4.2. Named entity recognition",
    "text": "Let X = (x1, · · · ,xT ) be an input sentence, where each word xt is represented by a vector in R D, computed using a neural recurrent architecture trained end-to-end. We wish to tag each word with named entities, i.e., identify blocks of words that correspond to names, locations, dates, etc. We use the specialized operator VitΩ described in §3.1. We construct the potential tensor θ(X) ∈ RT×S×S as\n∀ t > 1, θ(X)t,i,j , w ⊤ i xt + bi + ti,j ,\nand θ(X)1,i,j , w ⊤ i xt + bi, where (wi, bi) ∈ R D × R is the linear classifier associated with tag i and T ∈ RS×S is a transition matrix. We learn W , b and T along with the network producing X , and compare two losses:\nSurrogate convex loss: ℓ0,Ω(Ytrue;θ),\nRelaxed loss: ∆(Ytrue,∇DPΩ(θ)),\nwhere ∆(Ytrue,Y ) is the squared ℓ2 distance when Ω = ‖ · ‖22 and the Kullback-Leibler divergence when Ω = −H , applied row-wise to the marginalization of Ytrue and Y .\nExperiments. We measure the performance of the different losses and regularizations on the four languages of the CoNLL 2003 dataset. Following Lample et al. (2016), who use the ℓ0,−H loss, we use a character LSTM and FastText (Joulin et al., 2016) pretrained embeddings computed using on Wikipedia. Those are fed to a word bidirectional LSTM to obtain X . Architecture details are provided in §C.1. Results are reported in Table 1, along with reference results with different pretrained embeddings. We first note that the non-regularized structured perceptron loss ℓ0,0, that involves working with subgradients of DP(θ), perform significantly worse than regularized losses. With proper parameter selections, all regularized losses perform within 1% F1-score of each other, although entropyregularized losses perform slightly better on 3/4 languages. However, the ℓ22-regularized losses yield sparse predictions, whereas entropy regularization always yields dense probability vectors. Qualitatively, this allows to identify ambiguous predictions more easily, as illustrated in §C.1. Sparse\npredictions also allows to enumerate all non-zero probability entities, and to trade precision for recall at test time."
  }, {
    "heading": "4.3. Supervised audio-to-score transcription",
    "text": "We use our framework to perform supervised audio-toscore alignment on the Bach 10 dataset (Duan & Pardo, 2011). The dataset consists of 10 music pieces with audio tracks, MIDI transcriptions, and annotated alignments between them. We transform the audio tracks into a sequence of audio frames using a feature extractor (see §C.2) to obtain a sequence A ∈ RNA×D, while the associated score sequence is represented by B ∈ RNB×K (each row bj is a one-hot vector corresponding to one key bj). Each pair (A,B) is associated to an alignment Ytrue ∈ R NA×NB . As described in §3.2, we define a discrepancy matrix θ ∈ R NA×NB between the elements of the two sequences. We set the cost between an audio frame and a key to be the loglikelihood of this key given a multinomial linear classifier:\n∀ i ∈ [NA], li , − log(softmax(W ⊤ai + c)) ∈ R K\nand ∀ j ∈ [NB ], θi,j , li,bj ,\nwhere (W , c) ∈ RD×K×RK are learned classifier parameters. We predict a soft alignment by Y = ∇DTW−H(θ). Following (Garreau et al., 2014), we define the relaxed loss\n∆(Ytrue,Y ) , ‖L(Y − Ytrue) ⊤‖2F ,\nwhere L a the lower triangular matrix filled with 1. When Y ∈ Y is a true alignement matrix, ∆(Ytrue,Y ) is the area between the path of Ytrue and Y , which corresponds to the mean absolute deviation in the audio literature. When Y ∈ conv(Y), it is a convex relaxation of the area. At test time, once θ is learned, we use the non-regularized DTW algorithm to output a hard alignment Y ⋆(θ) ∈ Y .\nResults. We perform a leave-one-out cross-validation of our model performance, learning the multinomial classifier on 9 pieces and assessing the quality of the alignment on the remaining piece. We report the mean absolute deviation on both train and test sets. A solid baseline consists in learning the multinomial classifier (W , c) beforehand, i.e., without end-to-end training. We then use this model to compute θ as in (4.3) and obtain Y ⋆(θ). As shown in Table 2, our end-to-end technique outperforms this baseline by a large margin. We also demonstrate in §C.2 that the alignments obtained by end-to-end training are visibly closer to the ground truth. End-to-end training thus allows to fine-tune the distance matrix θ for the task at hand."
  }, {
    "heading": "5. Structured and sparse attention",
    "text": "We show in this section how to apply our framework to neural sequence-to-sequence models augmented with an attention mechanism (Bahdanau et al., 2015). An encoder first produces a list of vectors X = (x1, . . . ,xT ) representing the input sequence. A decoder is then used to greedily produce the corresponding output sequence. To simplify the notation, we focus on one time step of the decoding procedure. Given the decoder’s current hidden state z and X as inputs, the role of the attention mechanism is to produce a distribution w ∈ △T over X , for the current time step. This distribution is then typically used to produce a context vector c , X⊤w, that is in turn invoved in the computation of the output sequence’s next element.\nStructured attention layers. Kim et al. (2017) proposed a segmentation attention layer, which is capable of taking into account the transitions between elements of X . They use a linear-chain CRF to model the probability pθ,−H(y|X) of a sequence y = (y1, . . . , yT ), where each yt is either 1 (“pay attention”) or 0. They then propose to use normalized marginal probabilities as attention weights: wt ∝ pθ,−H(yt = 1|X). They show how to backpropagate gradients through the forward-backward algorithm, which they use to compute the marginal probabilities.\nGeneralizing structured attention. Using the notation from §3.1, any y can be represented as a tensor Y ∈ {0, 1}T×2×2 and the potentials as a tensor θ ∈ RT×2×2. Similarly to Kim et al. (2017), we define\nθt,1,j , xtMz + t1,j and θt,0,j , t0,j ,\nwhere xMz is a learned bilinear form and T ∈ R2×2 is a learned transition matrix. Following §3.1, the gradient ∇VitΩ(θ) is equal to the expected matrix E ∈ R T×2×2 and the marginals are obtained by marginalizing that matrix. Hence, we can set wt ∝ pθ,Ω(yt = 1|X) = et,1,0 + et,1,1. Backpropagating through ∇VitΩ(θ) can be carried out using our approach outlined in §2.4. This approach is not only more general, but also simpler and more robust to under-\nflow problems than backpropagating through the forwardbackward algorithm as done by Kim et al. (2017).\nExperiments. We demonstrate structured attention layers with an LSTM encoder and decoder to perform French to English translation using data from a 1 million sentence subset of the WMT14 FR-EN challenge. We illustrate an example of attenion map obtained with negentropy and ℓ22 regularizations in Figure 4. Non-zero elements are underlined with borders: ℓ22-regularized attention maps are sparse and more interpretable — this provides a structured alternative to sparsemax attention (Martins & Astudillo, 2016). Results were all within 0.8 point of BLEU score on the newstest2014 dataset. For French to English, standard softmax attention obtained 27.96, while entropy and ℓ22 regularized structured attention obtained 27.96 and 27.19 — introducing structure and sparsity therefore provides enhanced interpretability with comparable peformance. We provide model details, full results and further visualizations in §C.3."
  }, {
    "heading": "6. Conclusion",
    "text": "We proposed a theoretical framework for turning a broad class of dynamic programs into convex, differentiable and tractable operators, using the novel point of view of smoothed max operators. Our work sheds a new light on how to transform dynamic programs that predict hard assignments (e.g., the maximum a-posteriori estimator in a probabilistic graphical model or an alignment matrix between two time-series) into continuous and probabilistic ones. We provided a new argument in favor of negentropy regularization by showing that it is the only one to preserve associativity of the smoothed max operator. We showed that different regularizations induce different distributions over outputs and that ℓ22 regularization has other benefits, in terms of sparsity of the expected outputs. Generally speaking, performing inference in a graphical model and backpropagating through it reduces to computing the first and second-order derivatives of a relaxed maximum-likelihood estimation — leveraging this observation yields elegant and efficient algorithms that are readily usable in deep learning frameworks, with various promising applications."
  }, {
    "heading": "Acknowledgements",
    "text": "MB thanks Vlad Niculae and Marco Cuturi for many fruitful discussions. AM thanks Julien Mairal, Inria Thoth and Inria Parietal for lending him the computational resources necessary to run the experiments. He thanks University Paris-Saclay for allowing him to do an internship at NTT, and Olivier Grisel for his insightful comments on natural language processing models."
  }],
  "year": 2018,
  "references": [{
    "title": "The generalized distributive law",
    "authors": ["S.M. Aji", "R.J. McEliece"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2000
  }, {
    "title": "OptNet: Differentiable optimization as a layer in neural networks",
    "authors": ["B. Amos", "J.Z. Kolter"],
    "venue": "In Proc. of ICML,",
    "year": 2017
  }, {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "venue": "In Proc. of ICLR,",
    "year": 2015
  }, {
    "title": "Predicting Structured Data",
    "authors": ["G. Bakır", "T. Hofmann", "B. Schölkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"],
    "year": 2007
  }, {
    "title": "Why Delannoy numbers",
    "authors": ["C. Banderier", "S. Schwer"],
    "venue": "Journal of Statistical Planning and Inference,",
    "year": 2005
  }, {
    "title": "Statistical inference for probabilistic functions of finite state Markov chains",
    "authors": ["L.E. Baum", "T. Petrie"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1966
  }, {
    "title": "Smoothing and first order methods: A unified framework",
    "authors": ["A. Beck", "M. Teboulle"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2012
  }, {
    "title": "On the theory of dynamic programming",
    "authors": ["R. Bellman"],
    "venue": "Proc. of the National Academy of Sciences,",
    "year": 1952
  }, {
    "title": "Control of uncertain systems with a setmembership description of the uncertainty",
    "authors": ["D.P. Bertsekas"],
    "venue": "PhD thesis, Massachusetts Institute of Technology,",
    "year": 1971
  }, {
    "title": "Smooth and sparse optimal transport",
    "authors": ["M. Blondel", "V. Seguy", "A. Rolet"],
    "venue": "In Proc. of AISTATS,",
    "year": 2018
  }, {
    "title": "Global training of document processing systems using graph transformer networks",
    "authors": ["L. Bottou", "Y. Bengio", "Y.L. Cun"],
    "venue": "In Proc. of CVPR,",
    "year": 1997
  }, {
    "title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms",
    "authors": ["M. Collins"],
    "venue": "In Proc. of ACL, pp",
    "year": 2002
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Soft-DTW: a differentiable loss function for time-series",
    "authors": ["M. Cuturi", "M. Blondel"],
    "venue": "In Proc. of ICML,",
    "year": 2017
  }, {
    "title": "The theory of max-min, with applications",
    "authors": ["J.M. Danskin"],
    "venue": "SIAM Journal on Applied Mathematics,",
    "year": 1966
  }, {
    "title": "Differentiable learning of submodular functions",
    "authors": ["J. Djolonga", "A. Krause"],
    "venue": "In Proc. of NIPS,",
    "year": 2017
  }, {
    "title": "Soundprism: An online system for score-informed source separation of music audio",
    "authors": ["Z. Duan", "B. Pardo"],
    "venue": "IEEE Journal of Selected Topics in Signal Processing,",
    "year": 2011
  }, {
    "title": "Structured Prediction for NLP",
    "authors": ["D. Garreau", "R. Lajugie", "S. Arlot", "F. Bach"],
    "year": 2016
  }, {
    "title": "Structured attention networks",
    "authors": ["Y. Kim", "C. Denton", "L. Hoang", "A.M. Rush"],
    "venue": "In Proc. of ICLR,",
    "year": 2017
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["J. Lafferty", "A. McCallum", "F.C. Pereira"],
    "venue": "In Proc. of ICML, pp",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"],
    "venue": "In Proc. of NAACL,",
    "year": 2016
  }, {
    "title": "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors",
    "authors": ["S. Linnainmaa"],
    "venue": "PhD thesis, Univ. Helsinki,",
    "year": 1970
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["T. Luong", "H. Pham", "C.D. Manning"],
    "venue": "In Proc. of EMNLP,",
    "year": 2015
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
    "authors": ["X. Ma", "E. Hovy"],
    "venue": "In Proc. of ACL,",
    "year": 2016
  }, {
    "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
    "authors": ["A.F. Martins", "R.F. Astudillo"],
    "venue": "In Proc. of ICML,",
    "year": 2016
  }, {
    "title": "Smooth and strong: MAP inference with linear convergence",
    "authors": ["O. Meshi", "M. Mahdavi", "A.G. Schwing"],
    "venue": "In Proc. of NIPS,",
    "year": 2015
  }, {
    "title": "A finite algorithm for finding the projection of a point onto the canonical simplex of R",
    "authors": ["C. Michelot"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 1986
  }, {
    "title": "Proximité et dualité dans un espace hilbertien",
    "authors": ["Moreau", "J.-J"],
    "venue": "Bullet de la Société Mathémathique de France,",
    "year": 1965
  }, {
    "title": "Smooth minimization of non-smooth functions",
    "authors": ["Y. Nesterov"],
    "venue": "Mathematical Programming,",
    "year": 2005
  }, {
    "title": "A regularized framework for sparse and structured neural attention",
    "authors": ["V. Niculae", "M. Blondel"],
    "venue": "In Proc. of NIPS,",
    "year": 2017
  }, {
    "title": "Sparsemap: Differentiable sparse structured inference",
    "authors": ["V. Niculae", "A.F. Martins", "M. Blondel", "C. Cardie"],
    "venue": "In Proc. of ICML,",
    "year": 2018
  }, {
    "title": "Pytorch: Tensors and dynamic neural networks in Python with strong GPU acceleration, 2017",
    "authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan"],
    "year": 2017
  }, {
    "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
    "authors": ["J. Pearl"],
    "year": 1988
  }, {
    "title": "Fast exact multiplication by the Hessian",
    "authors": ["B.A. Pearlmutter"],
    "venue": "Neural computation,",
    "year": 1994
  }, {
    "title": "A tutorial on hidden Markov models and selected applications in speech recognition",
    "authors": ["L.R. Rabiner"],
    "venue": "In Readings in Speech Recognition,",
    "year": 1990
  }, {
    "title": "Dynamic programming algorithm optimization for spoken word recognition",
    "authors": ["H. Sakoe", "S. Chiba"],
    "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,",
    "year": 1978
  }, {
    "title": "Minimum risk annealing for training log-linear models",
    "authors": ["D.A. Smith", "J. Eisner"],
    "venue": "In Proc. of COLING/ACL,",
    "year": 2006
  }, {
    "title": "Minimum-risk training of approximate CRF-based NLP systems",
    "authors": ["V. Stoyanov", "J. Eisner"],
    "venue": "In Proc. of NAACL, pp",
    "year": 2012
  }, {
    "title": "Objects counted by the central Delannoy numbers",
    "authors": ["R.A. Sulanke"],
    "venue": "Journal of Integer Sequences,",
    "year": 2003
  }, {
    "title": "An introduction to conditional random fields",
    "authors": ["C. Sutton", "A McCallum"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Large margin methods for structured and interdependent output variables",
    "authors": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Abstract dynamic programming models under commutativity conditions",
    "authors": ["S. Verdu", "H.V. Poor"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 1987
  }, {
    "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
    "authors": ["A. Viterbi"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1967
  }, {
    "title": "Graphical models, exponential families, and variational inference",
    "authors": ["M.J. Wainwright", "M.I. Jordan"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2008
  }],
  "id": "SP:fd3ecdc679fa8b5d5a4fa0f9adadda5191a54cdb",
  "authors": [{
    "name": "Arthur Mensch",
    "affiliations": []
  }, {
    "name": "Mathieu Blondel",
    "affiliations": []
  }],
  "abstractText": "Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation. Modern neural networks are composed of multiple layers of nested functions. Although layers usually consist of elementary linear algebraic operations and simple nonlinearities, there is a growing need for layers that output the value or the solution of an optimization problem. This can be used to design loss functions that capture relevant regularities in the input (Lample et al., 2016; Cuturi & Blondel, 2017) or to create layers that impose prior structure on the output (Kim et al., 2017; Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017). Among these works, several involve a convex optimization Inria, CEA, Université Paris-Saclay, Gif-sur-Yvette, France. Work performed at NTT Communication Science Laboratories, Kyoto, Japan. Correspondence to: AM <arthur.mensch@m4x.org>, MB <mathieu@mblondel.org>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s). problem (Amos & Kolter, 2017; Niculae & Blondel, 2017; Djolonga & Krause, 2017); others solve certain combinatorial optimization problems by dynamic programming (Kim et al., 2017; Cuturi & Blondel, 2017; Nowak et al., 2018). However, because dynamic programs (Bellman, 1952) are usually non-differentiable, virtually all these works resort to the formalism of conditional random fields (CRFs) (Lafferty et al., 2001), which can be seen as changing the semiring used by the dynamic program — replacing all values by their exponentials and all (max,+) operations with (+,×) operations (Verdu & Poor, 1987). While this modification smoothes the dynamic program, it looses the sparsity of solutions, since hard assignments become soft ones. Moreover, a general understanding of how to relax and differentiate dynamic programs is lacking. In this work, we propose to do so by leveraging smoothing (Moreau, 1965; Nesterov, 2005) and backpropagation (Linnainmaa, 1970). We make the following contributions. 1) We present a unified framework for turning a broad class of dynamic programs (DP) into differentiable operators. Unlike existing works, we propose to change the semiring to use (maxΩ,+) operations, where maxΩ is a max operator smoothed with a strongly convex regularizer Ω (§1). 2) We show that the resulting DP operators, that we call DPΩ, are smoothed relaxations of the original DP algorithm and satisfy several key properties, chief among them convexity. In addition, we show that their gradient,∇DPΩ, is equal to the expected trajectory of a certain random walk and can be used as a sound relaxation to the original dynamic program’s solution. Using negative entropy for Ω recovers existing CRF-based works from a different perspective — we provide new arguments as to why this Ω is a good choice. On the other hand, using squared l2 norm for Ω leads to new algorithms whose expected solution is sparse. We derive a clean and efficient method to backpropagate gradients, both through DPΩ and ∇DPΩ. This allows us to define differentiable DP layers that can be incorporated in neural networks trained end-to-end (§2). 3) We illustrate how to to derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for supervised time-series alignment (§3). The latter is illustrated in Figure 1. Finally, we showcase these two instantiDifferentiable Dynamic Programming for Structured Prediction and Attention DTW H(θ) = 7.49 DTWk·k2(θ) = 9.61 Figure 1. DTWΩ(θ) is an instantiation of the proposed smoothed dynamic programming operator, DPΩ(θ), to the dynamic time warping (DTW) computational graph. In this picture, θ is the squared Euclidean distance matrix between the observations of two time-series. The gradient ∇DTWΩ(θ) is equal to the expected alignment under a certain random walk characterized in §2.3 and is a sound continuous relaxation to the hard DTW alignment between the two time-series (here depicted with a yellow path). Unlike negentropy regularization (left), l22 regularization leads to exactly sparse alignments (right). Our framework allows to backpropagate through both DTWΩ(θ) and ∇DTWΩ(θ), which makes it possible to learn the distance matrix θ end-to-end. ations on structured prediction tasks (§4) and on structured attention for neural machine translation (§5). Notation. We denote scalars, vectors and matrices using lower-case, bold lower-case and bold upper-case letters, e.g., y, y and Y . We denote the elements of Y by yi,j and its rows by yi. We denote the Frobenius inner product between A and B by 〈A,B〉 , ∑ i,j ai,jbi,j . We denote the (D− 1)-probability simplex by△ , {λ ∈ R+ : ‖λ‖1 = 1}. We write conv(Y) , { ∑ Y ∈Y λY Y : λ ∈ △ } the convex hull of Y , [N ] the set {1, . . . , N} and supp(x) , {j ∈ [D] : xj 6= 0} the support of x ∈ R . We denote the Shannon entropy by H(q) , ∑ i qi log qi. We have released an optimized and modular PyTorch implementation for reproduction and reuse. 1. Smoothed max operators In this section, we introduce smoothed max operators (Nesterov, 2005; Beck & Teboulle, 2012; Niculae & Blondel, 2017), that will serve as a powerful and generic abstraction to define differentiable dynamic programs in §2. Let Ω : R → R be a strongly convex function on△ and let x ∈ R. We define the max operator smoothed by Ω as: maxΩ(x) , max q∈△D 〈q,x〉 − Ω(q). (1) In other words, maxΩ is the convex conjugate of Ω, restricted to the simplex. From the duality between strong convexity and smoothness, maxΩ is smooth: differentiable everywhere and with Lipschitz continuous gradient. Since the argument that achieves the maximum in (1) is unique, from Danskin’s theorem (1966), it is equal to the gradient: ∇maxΩ(x) = argmax q∈△D 〈q,x〉 − Ω(q). The gradient is differentiable almost everywhere for any strongly-convex Ω (everywhere for negentropy). Next, we state properties that will be useful throughout this paper. Lemma 1. Properties of maxΩ operators Let x = (x1, . . . , xD) ⊤ ∈ R. 1. Boundedness: If Ω is lower-bounded by LΩ,D and upper-bounded by UΩ,D on the simplex△ , then max(x)− UΩ,D ≤ maxΩ(x) ≤ max(x)− LΩ,D. 2. Distributivity of + over maxΩ: maxΩ(x+ c1) = maxΩ(x) + c ∀c ∈ R. 3. Commutativity: If Ω(Pq) = Ω(q), where P is a permutation matrix, then maxΩ(Px) = maxΩ(x). 4. Non-decreasingness in each coordinate: maxΩ(x) ≤ maxΩ(y) ∀x ≤ y 5. Insensitivity to −∞: xj = −∞⇒ ∇maxΩ(x)j = 0. Proofs are given in §A.1. In particular, property 3 holds whenever Ω(q) = ∑D i=1 ω(qi), for some function ω. We focus in this paper on two specific regularizers Ω: the negentropy −H and the squared l2 norm. For these choices, all properties above are satisfied and we can derive closedform expressions for maxΩ, its gradient and its Hessian — see §B.1. When using negentropy, maxΩ becomes the log-sum-exp and∇maxΩ the softmax. The former satisfies associativity, which as we shall see, makes it natural to use in dynamic programming. With the squared l2 regularization, as observed by Martins & Astudillo (2016); Niculae & Blondel (2017), the gradient∇maxΩ is sparse. This will prove useful to enforce sparsity in the models we study. 2. Differentiable DP layers Dynamic programming (DP) is a generic way of solving combinatorial optimization problems by recursively solving problems on smaller sets. We first introduce this category of algorithms in a broad setting, then use smoothed max operators to define differentiable DP layers. 2.1. Dynamic programming on a DAG Every problem solved by dynamic programming reduces to finding the highest-scoring path between a start node and an end node, on a weighted directed acyclic graph (DAG). We therefore introduce our formalism on this generic problem, and give concrete examples in §3. Formally, let G = (V, E) be a DAG, with nodes V and edges E . We write N = |V| ≥ 2 the number of nodes. Differentiable Dynamic Programming for Structured Prediction and Attention Without loss of generality, we number the nodes in topological order, from 1 (start) to N (end), and thus V = [N ]. Node 1 is the only node without parents, and node N the only node without children. Every directed edge (i, j) from a parent node j to a child node i has a weight θi,j ∈ R. We gather the edge weights in a matrix θ ∈ Θ ⊆ R , setting θi,j = −∞ if (i, j) / ∈ E and θ1,1 = 1. We consider the set Y of all paths in G from node 1 to node N . Any path Y ∈ Y can be represented as a N × N binary matrix, with yi,j = 1 if the path goes through the edge (i, j) and yi,j = 0 otherwise. In the sequel, paths will have a one-to-one correspondence with discrete structures such as sequences or alignments. Using this representation, 〈Y ,θ〉 corresponds to the cumulated sum of edge weights, along the path Y . The computation of the highest score among all paths amounts to solving the combinatorial problem LP(θ) , max Y ∈Y 〈Y ,θ〉 ∈ R. (2) Although the size of Y is in general exponential in N , LP(θ) can be computed in one topologically-ordered pass over G using dynamic programming. We let Pi be the set of parent nodes of node i in graph G and define recursively v1(θ) , 0 ∀ i ∈ [2, . . . , N ] : vi(θ) , max j∈Pi θi,j + vj(θ). (3) This algorithm outputs DP(θ) , vN (θ). We now show that this is precisely the highest score among all paths. Proposition 1. Optimality of dynamic programming ∀θ ∈ Θ : DP(θ) = LP(θ) The optimality of recursion (3) is well-known (Bellman, 1952). We prove it again with our formalism in §A.2, since it exhibits the two key properties that the max operator must satisfy to guarantee optimality: distributivity of + over it and associativity. The cost of computing DP(θ) is O(|E|), which is exponentially better than O(|Y|). In many applications, we will often rather be interested in the argument that achieves the maximum, i.e., one of the highest-scoring paths Y (θ) ∈ argmax Y ∈Y 〈Y ,θ〉. (4) This argument can be computed by backtracking, that we now relate to computing subgradients of LP(θ). Linear program, lack of differentiality. Unfortunately, LP(θ) is not differentiable everywhere. To see why this is the case, notice that (2) can be rewritten as a linear program over the convex polytope conv(Y): LP(θ) = max Y ∈conv(Y) 〈Y ,θ〉. From the generalized Danskin theorem (Bertsekas, 1971), Y (θ) ∈ ∂LP(θ) = argmax Y ∈conv(Y) 〈Y ,θ〉, where ∂ denotes the subdifferential of LP(θ), i.e., the set of subgradients. When Y (θ) is unique, ∂LP(θ) is a singleton and Y ⋆ is equal to the gradient of LP(θ), that we write∇LP(θ). Unfortunately, Y (θ) is not always unique, meaning that LP(θ) is not differentiable everywhere. As we will show in §4.2, this hinders optimization as we can only train models involving LP(θ) with subgradient methods. Worse, Y (θ), a function from Θ to Y , is discontinuous and has null or undefined derivatives. It is thus impossible to use it in a model trained by gradient descent. 2.2. Smoothed max layers To address the lack of differentiability of dynamic programming, we introduce the operator maxΩ, presented in §1, and consider two approaches. Smoothing the linear program. Let us define the Ωsmoothed maximum of a function f : Y → R over a finite set Y using the following shorthand notation: maxΩ Y ∈Y f(Y ) , maxΩ((f(Y ))Y ∈Y). A natural way to circumvent the lack of differentiability of LP(θ) is then to replace the global max operator by maxΩ: LPΩ(θ) , maxΩ Y ∈Y 〈Y ,θ〉 ∈ R. (5) From §1, LPΩ(θ) is convex and, as long as Ω is strongly convex, differentiable everywhere. In addition, ∇LPΩ(θ) is Lipschitz continuous and thus differentiable almost everywhere. Unfortunately, solving (5) for general Ω is likely intractable when Y has an exponential size. Smoothing the dynamic program. As a tractable alternative, we propose an algorithmic smoothing. Namely, we replace max by maxΩ locally within the DP recursion. Omitting the dependence on Ω, this defines a smoothed recursion over the new sequence (vi(θ)) N i=1: v1(θ) , 0 ∀i ∈ [2, . . . , N ] : vi(θ) , maxΩ j∈Pi θi,j + vj(θ). (6) The new algorithm outputs DPΩ(θ), vN (θ), the smoothed highest score. Smoothing the max operator locally brings the same benefit as before — DPΩ(θ) is smooth and ∇DPΩ(θ) is differentiable almost everywhere. However, computing DPΩ(θ) is now always tractable, since it simply requires to evaluate (vi(θ)) N i=1 in topological order, as in Differentiable Dynamic Programming for Structured Prediction and Attention the original recursion (3). Although LPΩ(θ) and DPΩ(θ) are generally different (in fact, LPΩ(θ) ≥ DPΩ(θ) for all θ ∈ Θ), we now show that DPΩ(θ) is a sensible approximation of LP(θ) in several respects. Proposition 2. Properties of DPΩ",
  "title": "Differentiable Dynamic Programming for Structured Prediction and Attention"
}