{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 105–114 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n105\nWe propose to consider these two aspects jointly. We develop TWOWINGOS (twowing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TWOWINGOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim. We treat this challenge as coupled optimization problems, training a joint model for it. TWOWINGOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim verification and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.1"
  }, {
    "heading": "1 Introduction",
    "text": "A claim, e.g., “Marilyn Monroe worked with Warner Brothers”, is an assertive sentence that may be true or false. While the task of claim verification will not tell us the absolute truth of this claim, it is expected to determine whether the claim is supported by evidence in a given text collection. Specifically, given a claim and a text corpus, evidential claim verification, demonstrated in\n1cogcomp.org/page/publication_view/847\nFigure 1, aims at identifying text snippets in the corpus that act as evidence that supports or refutes the claim.\nThis problem has broad applications. For example, knowledge bases (KB), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), can be augmented with a new relational statement such as “(Afghanistan, is source of, Kushan Dynasty)”. This needs to be first verified by a claim verification process and supported by evidence (Roth et al., 2009; Chaganty et al., 2017). More broadly, claim verification is a key component in any technical solution addressing recent concerns about the trustworthiness of online content (Vydiswaran et al., 2011; Pasternack and Roth, 2013; Hovy et al., 2013). In both scenarios, we care about whether or not a claim holds, and seek reliable evidence in support of this decision.\nEvidential claim verification requires that we address three challenges. First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim. This differs from the conventional textual entailment (TE) problem (Dagan et al., 2013) as here we first look for the premises given a hypothesis. Clearly, the evidence one seeks depends on the claim, as well as on the eventual entailment\ndecision – the same claim would require different supporting than refuting evidence. This motivates us to develop an approach that can transfer knowledge from claim verification to evidence identification. Second, the evidence for a claim might require aggregating information from multiple sentences and even multiple documents (rf. #3 in Table 4). Therefore, a set, rather than a collection of independent text snippets, should be chosen to act as evidence. And, finally, in difference from TE, given a set of evidence sentences as a premise, the truth value of the claim should depend on all of the evidence, rather than on a single sentence there.\nThe discussion above suggests that claim verification and evidence identification are tightly coupled. Claim should influence the identification of appropriate evidence, and “trusted evidence boosts the claim’s veracity” (Vydiswaran et al., 2011). Consequently, we propose TWOWINGOS, a twowing optimization strategy2, to support this process. As shown in Figure 2, we consider a set of sentences S as the candidate evidence space, a claim x, and a decision space Y for the claim verification. In the optimal condition, a one-hot vector over Y indicates which decision to make towards the claim, and a binary vector over S indicates a subset of sentences Se (in blue in Figure 2) to act as evidence.\nPrior work mostly approached this problem as a pipeline procedure – first, given a claim x, determine Se by some similarity matching; then, conduct textual entailment over (Se, x) pairs. Our framework, TWOWINGOS, optimizes the two\n2By “two-wing optimization”, we mean that the same object, i.e., the claim, is mapped into two target spaces in a joint optimization scheme.\nsubtasks jointly, so that both claim verification and evidence identification can enhance each other. TWOWINGOS is a generic framework making use of a shared representation of the claim to cotrain evidence identification and claim verification.\nTWOWINGOS is tested on the FEVER benchmark (Thorne et al., 2018), showing≈30% F1 improvement for evidence identification, and ≈23% accuracy increase in claim verification. Our analysis shows that (i) entity mentions in claims provide a strong clue for retrieving relevant passages; (ii) composition of evidence clues across sentences helps claim verification; and that (iii) the joint training scheme provides significant benefits of a pipeline architecture."
  }, {
    "heading": "2 Related Work",
    "text": "Most work focuses on the dataset construction while lacking advanced models to handle the problem. Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution. Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking. Each claim is accompanied by an article headline as evidence. Then a three-way logistic regression model is used over some rule-based features. No need to search for evidence. Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification. However, the presentation of evidences is ignored. Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning.\nSome work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference. Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as claim verification problem, if we treat (question, correct answer) as a claim. However, little work has studied how well a QA system can identify all the answer evidence.\nOnly a few works considered improving the evidence presentation in claim verification problems.\nRoth et al. (2009) introduce the task of Entailed Relation Recognition – given a set of short paragraphs and a relational fact in the triple form of (argument1, relation, argument2), finding the paragraphs that can entail this fact. They first use Expanded Lexical Retrieval to rank and keep the topk paragraphs as candidates, then build a TE classifier over each (candidate, statement) pair. The work directly related to us is by Thorne et al. (2018). Given claims and a set of Wikipages, Thorne et al. (2018) use a retrieval model based on TF-IDF to locate top-5 sentences in top-5 pages as evidence, then utilize a neural entailment model to classify (evidence, claim) pairs.\nIn contrast, our work tries to optimize the claim verification as well as the evidence identification in a joint training scheme, which is more than just supporting or refuting the claims."
  }, {
    "heading": "3 The TWOWINGOS Model",
    "text": "Figure 2 illustrates the two-wing optimization problem addressed in this work: given a collection of evidence candidates S={s1, s2, · · · , si, · · · , sm}, a claim x and a decision set Y = {y1 · · · , yn}, the model TWOWINGOS predicts a binary vector p over S and a one-hot vector o over Y against the ground truth, a binary vector q and a one-hot vector z, respectively. A binary vector over S means a subset of sentences (Se) act as evidence, and the one-hot vector indicates a single decision (yi) to be made towards the claim x given the evidence Se. Next, we use two separate subsections to elaborate the process of evidence identification (i.e., optimize p to q) and the claim verification (i.e., optimize o to z)."
  }, {
    "heading": "3.1 Evidence identification",
    "text": "A simple approach to identifying evidence is to detect the top-k sentences that are lexically similar to the claim, as some pipeline systems (Roth et al., 2009; Thorne et al., 2018) do. However, a claimunaware fixed k is less optimal, adding noise or missing key supporting factors, consequently limiting the performance.\nIn this work, we approach the evidence by modeling sentences S={s1, · · · , si, · · · , sm} with the claim x as context in a supervised learning scheme. For each si, the problem turns out to be learning a probability: how likely si can entail the claim conditioned on other candidates as context, as shown by the blue items in Figure 2.\nTo start, a piece of text t (t ∈ S ∪ {x}) is represented as a sequence of l hidden states, forming a feature map T ∈ Rd×l, where d is the dimensionality of hidden states. We first stack a vanilla CNN (convolution & max-pooling) (LeCun et al., 1998) over T to get a representation for t. As a result, each evidence candidate si has a representation si, and the claim x has a representation x. To get a probability for each si, we need first to build its claim-aware representation ri.\nCoarse-grained representation. We directly concatenate the representation of si and x, generated by the vanilla CNN, as:\nri = [si,x, si · xT ] (1)\nThis coarse-grained approach makes use of merely the sentence-level representations while neglecting more fine-grained interactions between the sentences and the claim.\nFine-grained representation. Instead of directly employing the sentence-level representations, here we explore claim-aware representations for each word in sentence si, then compose them as the sentence representation ri, inspired by the Attentive Convolution (Yin and Schütze, 2017).\nFor each word sji in si, we first calculate its matching score towards each word xz in x, by dot product over their hidden states. Then the representation of the claim, as the context for the word sji , is formed as:\ncji = ∑ z softmax(sji · (x z)T ) · xz (2)\nNow, word sji has left context s j−1 i , right context sj+1i in si, and the claim-aware context c j i from x. A convolution encoder generates its claim-aware representation iji :\niji = tanh(W · [s j−1 i , s j i , s j+1 i , c j i ] + b) (3) where parameters W ∈ Rd×4d, b ∈ Rd. To compose those claim-aware word representations as the representation for sentence si, we use a max-pooling over {iji} along with j, generating ii.\nWe use term fint(si, x) to denote this whole process, so that:\nii = fint(si, x) (4)\nAt this point, the fine-grained representation for evidence candidate si is:\nri = [si,x, si · xT , ii] (5)\nLoss function. With a claim-aware representation ri, the sentence si subsequently gets a probability, acting as the evidence, αi ∈ (0, 1) via a non-linear sigmoid function:\nαi = sigmoid(v · rTi ) (6)\nwhere parameter vector v has the same dimensionality as ri.\nIn the end, all evidence candidates in S have a ground-truth binary vector q and the predicted probability vector α; then loss lev (“ev”: evidence) is implemented as a binary cross-entropy:\nlev = m∑ i=1 −(qi log(αi)+(1−qi) log(1−αi)) (7)\nAs the output of this evidence identification module, we binarize the probability vector α by pi = [αi > 0.5] (“[x]” is 1 if x is true or 0 otherwise). pi indicates si is evidence or not. All {si} with pi = 1 act as evidence set Se."
  }, {
    "heading": "3.2 Claim verification",
    "text": "As shown in Figure 2, to figure out an entailment decision yi for the claim x, the evidence Se possibly consists of more than one sentence. Furthermore, those evidence sentences are not necessarily in textual order nor from the same passage. So, we need a mechanism that enables each evidence or even each word inside to be aware of the content from other evidence sentences. Similar to the aforementioned approach to evidence identification, we come up with three methods, with different representation granularity, to learn a representation for (Se, x), i.e., the input for claim verification, shown in Figure 3.\nCoarse-grained representation. In this case, we treat Se as a whole, constructing its representation e by summing up the representations of all sentences in Se in a weighted way:\ne = m∑ i=1 αi · pi · si (8)\nwhere αi, from Equation 6, is the probability of si being the evidence.\nThen the (Se, x) pair gets a coarse-grained concatenated representation: [e,x]. It does not model the interactions within the evidence nor the interactions between the evidence and the claim. Based on our experience in evidence identification\nmodule, the representation of a sentence is better learned by composing context-aware word-level representations. Next, we introduce how to learn fine-grained representation for the (Se, x) pair.\nSingle-channel fine-grained representation. By “single-channel,” we mean each sentence si is aware of the claim x as its single context.\nFor a single pair (si, x), we utilize the function fint() in Equation 4 to build the fine-grained representations for both si and x, obtaining ii =\nfint(si, x) for si and xi = fint(x, si) for x. For (Se, x), we compose all the {ii} and all the {xi} along with i, via a weighted max-pooling:\ne = maxpooli(αi · pi · ii) (9) x = maxpooli(αi · pi · xi) (10)\nThis weighted max-pooling ensures that the sentences with higher probabilities of being evidence have a higher chance to present their features. As a result, (Se, x) gets a concatenated representation: [e, x]\nTwo-channel fine-grained representation. By “two-channel,” we mean that each evidence si is aware of two kinds of context, one from the claim x, the other from the remaining evidences.\nOur first step is to accumulate evidence clues within Se. To start, we concatenate all sentences in Se as a fake long sentence Ŝ consisting of hidden states {ŝ}. Similar to Equation 2, for each word sji in sentence si, we accumulate all of its related clues (cji ) from Ŝ as follows:\ncji = ∑ z softmax(sji · (ŝ z)T ) · ŝz (11)\nThen we update sji , the representation of word sji , by element-wise addition:\nsji = s j i ⊕ c j i (12)\nThis step enables the word sji to “see” all related clues from Se. The reason we add s j i and c j i is motivated by a simple experience: Assume the claim “Lily lives in the biggest city in Canada”, and one sentence contains a clue “· · · Lily lives in Toronto · · · ” and another sentence contains a clue “· · · Toronto is Canada’s largest city· · · ”. The most simple yet effective approach to aggregating the two clues is to sum up their representation vectors (Blacoe and Lapata, 2012) (we do not concatenate them, as those clues have no consistent textual order across different sji ).\nAfter updating the representation of each word in si, we perform the aforementioned “singlechannel fine-grained representation” between the updated si and the claim x, generating [e, x].\nLoss function. For the claim verification input (Se, x), we forward its representation [e, x] to a\nlogistic regression layer in order to infer a probability distribution o over the label space Y :\no = softmax(W · [e,x] + b) (13)\nwhere W ∈ Rn×2d, b ∈ Rn The loss lcv (“cv”: claim verification) is implemented as negative log-likelihood:\nlcv = − log(o · zT ) (14)\nwhere z is the ground truth one-hot label vector for the claim x on the space Y ."
  }, {
    "heading": "3.3 Joint optimization",
    "text": "Given the loss lev in evidence identification and the loss lcv in claim verification, the overall training loss is represented by:\nl = lev + lcv (15)\nTo ensure that we jointly train the two coupled subtasks with intensive knowledge communication instead of simply putting two pipeline neural networks together, our TWOWINGOS has following configurations: • Both subsystems share the same set of word embeddings as parameters; the vanilla CNNs for learning sentence and claim representations share parameters as well. • The output binary vector p by the evidence identification module is forwarded to the module of claim verification, as shown in Equations 8-10. • Though the representation of a claim’s decision yi is not put explicitly into the module of evidence identification, the claim’s representation x will be fine-tuned by the yi, so that the evidence candidates can get adjustment from the decision yi, since the claims are shared by two modules."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Setup",
    "text": "Dataset. In this work, we use FEVER (Thorne et al., 2018). The claims in FEVER were generated from the introductory parts of about 50K\nWikipedia pages of a June 2017 dump. Annotators construct claims about a single fact of the title entity with arbitrarily complex expressions and entity forms. To increase the claim complexity so that claims would not be trivially verified, annotators adopt two routes: (i) Providing additional knowledge: Annotators can explore a dictionary of terms that were (hyper-)linked, along with their pages; (ii) Mutate claims in six ways: negation, paraphrasing, substitution of a relation/entity with a similar/dissimilar one, and making the claims more general/specific. All resulting claims have 9.4 tokens in average. Apart from claims, FEVER also provides a Wikipedia corpus in size of about 5.4 million.\nEach claim is labeled as SUPPORTED, REFUTED or NOTENOUGHINFO (NEI). In addition, evidence sentences, from any wiki page, are required to be provided for SUPPORTED and REFUTED. Table 1 lists the data statistics. Figure 4 shows the distributions of sentence sizes and page sizes in FEVER’s evidence set. We can see that roughly 28% of the evidence covers more than one sentence, and approximately 16.3% of the evidence covers more than one wiki page.\nThis task has three evaluations: (i) NOSCOREEV – accuracy of claim verification, neglecting the validity of evidence; (ii) SCOREEV – accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for SUPPORTED and REFUTED; (iii) F1 – between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer 3.\n3https://github.com/sheffieldnlp/fever-scorer\nWiki page retrieval4. For each claim, we search in the given dictionary of wiki pages in the form of {title: sentence list}, and keep the top-5 ranked pages for fair comparison with Thorne et al. (2018). Algorithm 1 briefly shows the steps of wiki page retrieval. To speed up, we first build an inverted index from words to titles, then for each claim, we only search in the titles that cover at least one claim word.\nInput: A claim, wiki={title: page vocab} Output: A ranked top-k wiki titles Generate entity mentions from the claim; while each title do\nif claim.vocab∩title.vocab is empty then discard this title else title score = the max recall value of title.vocab\nin claim and in entity mentions of the claim; if title score = 1.0 then\ntitle.score = title score else\npage score = recall of claim in page vocab;\ntitle.score = title score + page score end\nend end Sort titles by title.score in descending order\nAlgorithm 1: Algorithm description of wiki page retrieval for FEVER claims.\nAll sentences of the top-5 retrieved wiki pages are kept as evidence candidates for claims in train, dev and test. It is worth mentioning that this page retrieval step is a reasonable preprocessing which controls the complexity of evidence searching in real-world, such as the big space – 5.4 million – in this work.\nTraining setup. All words are initialized by 300D Word2Vec (Mikolov et al., 2013) embeddings, and are fine-tuned during training. The whole system is trained by AdaGrad (Duchi et al., 2011). Other hyperparameter values include: learning rate 0.02, hidden size 300, mini-batch size 50, filter width 3.\nBaselines. In this work, we first consider the two systems reported by Thorne et al. (2018): (i) MLP: A multi-layer perceptron with one hidden layer, based on TF-IDF cosine similarity between the claim and the evidence (all evidence sentences are concatenated as a longer text piece) (Riedel et al., 2017); (ii) Decomp-Att (Parikh et al., 2016): A decomposable attention model that develops atten-\n4Our retrieval results are released as well.\ntion mechanisms to decompose the problem into subproblems to solve in parallel. Note that both systems first employed an IR system to keep top5 relevant sentences from the retrieved top-5 wiki pages as static evidence for claims.\nWe further consider the following variants of our own system TWOWINGOS: • Coarse-coarse: Both evidence identification and claim verification adopt coarse-grained representations.\nTo further study our system, we test this “coarse-coarse” in three setups: (i) “pipeline” – train the two modules independently. Forward the predicted evidence to do entailment for claims; (ii) “diff-CNN” – joint training with separate CNN parameters to learn sentence/claim representations; (iii) “share-CNN” – joint training with shared CNN parameters.\nThe following variants are in joint training. • Fine&sentence-wise: Given the evidence with multiple sentences, a natural baseline is to do entailment reasoning for each (sentence, claim), then compose. We do entailment reasoning between each predicted evidence sentence and the claim, generating a probability distribution over the label space Y . Then we sum up all the distribution vectors element-wise, as an ensemble system, to predict the label; • Four combinations of different grained representation learning: “coarse&fine(single)”, “coarse&fine(two)”, “fine&coarse” and “fine&fine(two)”. “Single” and “two” refer to the single/two-channel cases respectively."
  }, {
    "heading": "4.2 Results",
    "text": "Performance of passage retrieval. Table 2 compares our wikipage retriever with the one in\n(Thorne et al., 2018), which used a document retriever5 from DrQA (Chen et al., 2017).\nOur document retrieval module surpasses the competitor by a big margin in terms of the coverage of gold passages: 89.63% vs. 55.30% (k = 5 in all experiments). Its powerfulness should be attributed to: (i) Entity mention detection in the claims. (ii) As wiki titles are entities, we have a bi-channel way to match the claim with the wiki page: one with the title, the other with the page body, as shown in Algorithm 1.\nPerformance on FEVER Table 3 lists the performances of baselines and the TWOWINGOS variants on FEVER (dev&test). From the dev block, we observe that: • TWOWINGOS (from “share-CNN”) surpasses prior systems in big margins. Overall, fine-grained schemes in each subtask contribute more than the coarse-grained counterparts; • In the three setups – “pipeline”, “diff-CNN” and “share-CNN” – of coarse-coarse, “pipeline” gets better scores than (Thorne et al., 2018) in terms of evidence identification. “Share-CNN” has comparable F1 as “diff-CNN” while gaining a lot on NOSCOREEV (72.32 vs. 39.22) and SCOREEV (50.12 vs. 21.04). This clearly shows that the claim verification gains much knowledge transferred from the evidence identification module. Both “diff-CNN” and “share-CNN” perform better than “pipeline” (except for the slight inferiority at SCOREEV: 21.04 vs. 22.26). • Two-channel fine-grained representations show more effective than the single-channel counterpart in claim verification (NOSCOREEV: 78.77 vs. 75.65, SCOREEV: 53.64 vs. 52.65). As we expected, evidence sentences should collaborate in inferring the truth value of the claims. Two-channel setup enables an evidence candidate aware of other candidates as well as the claim. • In the last three rows of dev, there is no clear difference among their evidence identification scores. Recall that “sent-wise” is essentially an ensemble system over each (sentence, claim) entailment result. “Coarse-grained”, instead, first sums up all sentence representation, then performs ( ∑\n(sentence), claim) reasoning. We can also treat this “sum up” as an ensemble. Their comparison shows that these two kinds of tricks do not\n5It compares passages and claims as TF-IDF weighted bag-of-bigrams.\nmake much difference. If we adopt “two-channel fine-grained representation” in claim verification, big improvements are observed in both NOSCOREEV (+7.42%) and SCOREEV (+3%).\nIn the test block, our system (fine&fine(two)) beats the prior top system across all measurements by big margins – F1: 47.15 vs. 17.47; SCOREEV: 54.33 vs. 31.87; NOSCOREEV: 75.99 vs. 50.91.\nIn both dev and test blocks, we can observe that our evidence identification module consistently\nobtains balanced recall and precision. In contrast, the pipeline system by Thorne et al. (2018) has much higher recall than precision (45.89 vs. 10.79). It is worth mentioning that the SCOREEV metric is highly influenced by the recall value, since SCOREEV is computed on the claim instances whose evidences are fully retrieved, regardless of the precision. So, ideally, a system can set all sentences as evidence, so that SCOREEV can be promoted to be equal to NOSCOREEV. Our system is more reliable in this perspective.\nPerformance vs. #sent. in evidence. Figure 5 shows the results of the five evaluation measures against different sizes of gold evidence sentences in test set. We observe that: (i) Our system has robust precisions across #sentence; however, the recall decreases. This is not that surprising, since the more ground-truth sentences in evidence, the harder it is to retrieve all of them; (ii) Due to the decrease in recall, the SCOREEV also gets influenced for bigger #sentence. Interestingly, high precision and worse recall in evidence with more sentences still make consistently strong overall performance, i.e., NOSCOREEV. This should be due to the fact that the majority (83.18% (Thorne et al., 2018)) of claims can be correctly entailed by a single ground truth sentence, even if any remaining ground truth sentences are unavailable.\nError analysis. The case #1 in Table 4 shows that our system identifies two pieces of evidence\n(i.e., (Telemundo, 0) and (Telemundo, 4)) correctly; however, it falsely predicts the claim label. (Telemundo, 0): Telemundo is an American Spanish-language terrestrial television · · · . We can easily find that the keyword “Spanishlanguage” should refute the claim. However, both “Spanish-language” in this evidence and the “English-language” in the claim are unknown tokens with randomly initialized embeddings. This hints that a more careful data preprocessing may be helpful. In addition, to refute the claim, another clue comes from the combination of (Telemundo, 4) and (Hispanic and Latino Americans, 0). (Telemundo, 4): “The channel · · · aimed at Hispanic and Latino American audiences”; (Hispanic and Latino Americans, 0): “Hispanic Americans and Latino Americans · · · are descendants of people from countries of Latin America and Spain.”. Our system only retrieved (Telemundo, 4). And this clue is hard to grasp as it requires some background knowledge – people from Latin America and Spain usually are not treated as English-speaking.\nIn the case #2, our system fails to identify any evidence. This is due to the failure of our passage retrieval module: it detects entity mentions “Home”, “Holidays” and “American”, and the top-5 retrieved passages are “Home”, “Home for the Holidays”, “American Home”, “American” and “Home for the Holidays (song)”, which unfortunately cover none of the four ground truth passages. Interestingly, (i) given the falsely retrieved passages, our system predicts “no sentence is valid evidence” (denoted as ∅ in Table 4); (ii) given the empty evidence, our system predicts “NoEnoughInfo” for this claim. Both make sense.\nIn the case #3, a successful classification of the\nclaim requires information aggregation over the three gold evidence sentences: (Weekly Idol, 0): “Weekly Idol is a South Korean variety show · · · ”; (Weekly Idol, 1): “The show is hosted by comedian Jeong Hyeong-don and rapper Defconn.”; (Defconn, 0): “Defconn (born Yoo Dae-joon; January 6 , 1977 ) is a · · · ”. To successfully retrieve the three sentences as a whole set of evidence is challenging in evidence identification. Additionally, this example relies on the recognition and matching of digital numbers (1983 vs. 1977), which is beyond the expressivity of word embeddings, and is expected to be handled by rules more easily."
  }, {
    "heading": "5 Summary",
    "text": "In this work, we build TWOWINGOS, a two-wing optimization framework to address the claim verification problem by presenting precise evidence. Differing from a pipeline system, TWOWINGOS ensures the evidence identification module and the claim verification module are trained jointly, in an end-to-end scheme. Experiments show the superiority of TWOWINGOS in the FEVER benchmark."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank group colleagues (Nitish Gupta and Jennifer Sheffield) and Dr. Mo Yu from IBM AI Foundations Lab for providing insightful comments and critiques. This work was supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government."
  }],
  "year": 2018,
  "references": [{
    "title": "Naturalli: Natural logic inference for common sense reasoning",
    "authors": ["Gabor Angeli", "Christopher D. Manning."],
    "venue": "Proceedings of EMNLP, pages 534– 545.",
    "year": 2014
  }, {
    "title": "A comparison of vector-based representations for semantic composition",
    "authors": ["William Blacoe", "Mirella Lapata."],
    "venue": "Proceedings of EMNLP-CoNLL, pages 546–556.",
    "year": 2012
  }, {
    "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "Proceedings of SIGMOD, pages 1247–1250.",
    "year": 2008
  }, {
    "title": "Importance sampling for unbiased on-demand evaluation of knowledge base population",
    "authors": ["Arun Tejasvi Chaganty", "Ashwin Paranjape", "Percy Liang", "Christopher D. Manning."],
    "venue": "Proceedings of EMNLP, pages 1038–1048.",
    "year": 2017
  }, {
    "title": "Reading wikipedia to answer opendomain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "Proceedings of ACL, pages 1870–1879.",
    "year": 2017
  }, {
    "title": "Recognizing textual entailment: Models and applications",
    "authors": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzoto"],
    "year": 2013
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "JMLR, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Emergent: a novel data-set for stance classification",
    "authors": ["William Ferreira", "Andreas Vlachos."],
    "venue": "Proceedings of NAACL, pages 1163–1168.",
    "year": 2016
  }, {
    "title": "Learning whom to trust with MACE",
    "authors": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard H. Hovy."],
    "venue": "Proceedings of NAACL, pages 1120–1130.",
    "year": 2013
  }, {
    "title": "Automated historical fact-checking by passage retrieval, word statistics, and virtual question-answering",
    "authors": ["Mio Kobayashi", "Ai Ishii", "Chikara Hoshino", "Hiroshi Miyashita", "Takuya Matsuzaki."],
    "venue": "Proceedings of IJCNLP, pages 967–975.",
    "year": 2017
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner."],
    "venue": "Proceedings of the IEEE, pages 2278–2324.",
    "year": 1998
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Proceedings of NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur P. Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "Proceedings of EMNLP, pages 2249–2255.",
    "year": 2016
  }, {
    "title": "Latent credibility analysis",
    "authors": ["Jeff Pasternack", "Dan Roth."],
    "venue": "Proceedings of WWW, pages 1009– 1020.",
    "year": 2013
  }, {
    "title": "A simple but tough-to-beat baseline for the fake news challenge stance detection task",
    "authors": ["Benjamin Riedel", "Isabelle Augenstein", "Georgios P. Spithourakis", "Sebastian Riedel."],
    "venue": "CoRR, abs/1707.03264.",
    "year": 2017
  }, {
    "title": "A framework for entailed relation recognition",
    "authors": ["Dan Roth", "Mark Sammons", "V.G. Vinod Vydiswaran."],
    "venue": "Proceedings of ACL, pages 57–60.",
    "year": 2009
  }, {
    "title": "Yago: a core of semantic knowledge",
    "authors": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."],
    "venue": "Proceedings of WWW, pages 697–706.",
    "year": 2007
  }, {
    "title": "FEVER: a large-scale dataset for fact extraction and verification",
    "authors": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal."],
    "venue": "Proceedings of NAACL.",
    "year": 2018
  }, {
    "title": "Fact checking: Task definition and dataset construction",
    "authors": ["Andreas Vlachos", "Sebastian Riedel."],
    "venue": "Proceedings of the Workshop on Language Technologies and Computational Social Science@ACL, pages 18–22.",
    "year": 2014
  }, {
    "title": "Content-driven trust propagation framework",
    "authors": ["V.G. Vinod Vydiswaran", "ChengXiang Zhai", "Dan Roth."],
    "venue": "Proceedings of SIGKDD, pages 974–982.",
    "year": 2011
  }, {
    "title": "R3: Reinforced ranker-reader for open-domain question answering",
    "authors": ["Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerry Tesauro", "Bowen Zhou", "Jing Jiang."],
    "venue": "Proceedings of AAAI.",
    "year": 2018
  }, {
    "title": "Liar, Liar Pants on Fire”: A new benchmark dataset for fake news detection",
    "authors": ["William Yang Wang."],
    "venue": "Proceedings of ACL, pages 422–426.",
    "year": 2017
  }, {
    "title": "Attention-based convolutional neural network for machine comprehension",
    "authors": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Schütze."],
    "venue": "Proceedings of the NAACL Workshop on Human-Computer Question Answering, pages 15–21.",
    "year": 2016
  }, {
    "title": "Attentive convolution",
    "authors": ["Wenpeng Yin", "Hinrich Schütze."],
    "venue": "CoRR, abs/1710.00519.",
    "year": 2017
  }],
  "id": "SP:8cd595ee1d8b8ff5716f74b9223d736d21d12ce1",
  "authors": [{
    "name": "Wenpeng Yin",
    "affiliations": []
  }, {
    "name": "Dan Roth",
    "affiliations": []
  }],
  "abstractText": "Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence identification from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop TWOWINGOS (twowing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TWOWINGOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim. We treat this challenge as coupled optimization problems, training a joint model for it. TWOWINGOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim verification and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.1",
  "title": "TWOWINGOS: A Two-Wing Optimization Strategy for Evidential Claim Verification"
}