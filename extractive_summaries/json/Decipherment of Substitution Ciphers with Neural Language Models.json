{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 869–874 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n869"
  }, {
    "heading": "1 Introduction",
    "text": "Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013). Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014). Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010). However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time."
  }, {
    "heading": "2 Decipherment Model",
    "text": "We use the notation from Nuhn et al. (2013). Ciphertext fN1 = f1..fi..fN and plaintext e N 1 = e1..ei..eN consist of vocabularies fi ∈ Vf and ei ∈ Ve respectively. The beginning tokens in the ciphertext (f0) and plaintext (e0) are set to “$” denoting the beginning of a sentence. The substitutions are represented by a function φ : Vf → Ve such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function φwhich does not have every φ(f) fixed is called a partial cipher function (Corlett and Penn, 2010). The number of fs that are fixed in φ is given by its cardinality. φ′ is called an extension of φ, if f is fixed in φ′ such that δ(φ′(f), φ(f)) yields true ∀f ∈ Vf which are already fixed in φ where δ is Kronecker delta. Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized.\nφ̂ = argmax φ p(φ(f1)...φ(fN )) (1)\nwhere p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most likely substitutions using the language model scores as the ranking."
  }, {
    "heading": "2.1 Neural Language Model",
    "text": "The advantage of a neural LM is that it can be used to score the entire candidate plaintext for a hypothesized partial decipherment. In this work, we use a state of the art byte (character) level neural LM using a multiplicative LSTM (Radford et al., 2017).\nConsider a sequence S = w1, w2, w3, ..., wN . The LM score of S is SCORE(S):\nP (S) = P (w1, w2, w3, ..., wN )\nP (S) = N∏ i=1 P (wi | w1, w2, ..., wi−1))\nSCORE(S) = − N∑ i=1 log(P (wi | w<i))\n(2)"
  }, {
    "heading": "2.2 Beam Search",
    "text": "Algorithm 1 is the beam search algorithm (Nuhn et al., 2013, 2014) for solving substitution ciphers. It monitors all partial hypotheses in lists Hs and Ht based on their quality. As the search progresses, the partial hypotheses are extended, scored with SCORE and appended to Ht. EXT LIMITS determines which extensions should be allowed and EXT ORDER picks the next cipher symbol for extension. The search continues after pruning: Hs ← HISTOGRAM_PRUNE(Ht). We augment this algorithm by updating the SCORE function with a neural LM.\nAlgorithm 1 Beam Search for Decipherment 1: function (BEAM SEARCH (EXT ORDER, EXT LIM-\nITS)) 2: initialize sets Hs, Ht 3: CARDINALITY = 0 4: Hs.ADD((∅,0)) 5: while CARDINALITY < |Vf | do 6: f = EXT ORDER[CARDINALITY] 7: for all φ ∈ Hs do 8: for all e ∈ Ve do 9: φ’ := φ ∪ {(e, f)}\n10: if EXT LIMITS(φ’) then 11: Ht.ADD(φ’,SCORE(φ’)) 12: HISTOGRAM PRUNE(Ht) 13: CARDINALITY = CARDINALITY + 1 14: Hs = Ht 15: Ht.CLEAR() 16: return WINNER(Hs)\n3 Score Estimation (SCORE)\nScore estimation evaluates the quality of the partial hypotheses φ. Using the example from Nuhn et al. (2014), consider the vocabularies Ve = {a, b, c, d} and Vf = {A,B,C,D}, extension order (B,A,C,D), and ciphertext $ ABDDCABCDADCABDC $. Let φ = {(a,A), (b, B))} be the partial hypothesis. Then SCORE(φ) scores this hypothesized partial decipherment (only A and B are converted to plaintext) using a pre-trained language model in the hypothesized plaintext language."
  }, {
    "heading": "3.1 Baseline",
    "text": "The initial rest cost estimator introduced by Nuhn et al. nuhnbeam computes the score of hypotheses only based on partially deciphered text that builds a shard of n adjacent solved symbols. As a heuristic, n-grams which still consist of unsolved cipher-symbols are assigned a trivial estimate of probability 1. An improved version of rest cost es-\ntimation (Nuhn et al., 2014) consults lower order n-grams to score each position."
  }, {
    "heading": "3.2 Global Rest Cost Estimation",
    "text": "The baseline scoring method greatly relies on local context, i.e. the estimation is strictly based on partial character sequences. Since this depends solely on the n-gram LM, the true conditional probability under Markov assumption is not modeled and, therefore, context dependency beyond the window of (n− 1) is ignored. Thus, attempting to utilize a higher amount of context can lower the probability of some tokens resulting in poor scores.\nWe address this issue with a new improved version of the rest cost estimator by supplementing the partial decipherment φ(fN1 ) with predicted plaintext text symbols using our neural language model (NLM). Applying φ = {(a,A), (b, B))} to the ciphertext above, we get the following partial hypothesis: φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$ We introduce a scoring function that is able to score the entire plaintext including the missing plaintext symbols. First, we sample1 the plaintext symbols from the NLM at all locations depending on the deciphered tokens from the partial hypothesis φ such that these tokens maintain their respective positions in the sequence, and at the same time are sampled from the neural LM to fit (probabilistically) in this context. Next, we determine the probability of the entire sequence including the scores of sampled plaintext as our rest cost estimate.\nNLM\nIn our running example, this would yield a score estimation of the partial decipherment, φ(fN1 ) :\nφ(fN1 ) = $ a1b2d3c4c5a6b7c8d9a10d11d12a13b14d15c16 $\nThus, the neural LM is used to predict the score of the full sequence. This method of global scoring evaluates each candidate partial decipherment by scoring the entire message, augmented by the sam-\n1The char-level sampling is done incrementally from left to right to generate a sequence that contains the deciphered tokens from φ at the exact locations they occur in the above φ(fN1 ). If the LM prediction contradicts the hypothesized decipherment we stop sampling and start from the next character.\npled plaintext symbols from the NLM. Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search."
  }, {
    "heading": "3.3 Frequency Matching Heuristic",
    "text": "Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution. We use this heuristic to augment the score estimation (SCORE):\nFMH(φ′) = ∣∣∣∣log(ν(f)ν(e) )∣∣∣∣ f ∈ Vf , e ∈ Ve (3) ν(f) is the percentage relative frequency of the ciphertext symbol f , while ν(e) is the percentage relative frequency of the plaintext token e in the plaintext language model. The closer this value to 0, the more likely it is that f is mapped to e.\nThus given a φ with the SCORE(φ), the extension φ′ (Algo. 1) is scored as: SCORE(φ′) = SCORE(φ) + NEW(φ′)− FMH(φ′) (4) where NEW is the score for symbols that have been newly fixed in φ′ while extending φ to φ′. Our experimental evaluations show that the global rest cost estimator and the frequency matching heuristic contribute positively towards the accuracy of different ciphertexts."
  }, {
    "heading": "4 Experimental Evaluation",
    "text": "We carry out 2 sets of experiments: one on letter based 1:1, and another on homophonic substitution ciphers. We report Symbol Error Rate (SER) which is the fraction of characters in the deciphered text that are incorrect.\nThe character NLM uses a single layer multiplicative LSTM (mLSTM) (Radford et al., 2017) with 4096 units. The model was trained for a single epoch on mini-batches of 128 subsequences of length 256 for a total of 1 million weight updates. States were initialized to zero at the beginning of each data shard and persisted across updates to simulate full-backprop and allow for the forward propagation of information outside of a given sub-\nsequence. In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer2."
  }, {
    "heading": "4.1 1:1 Substitution Ciphers",
    "text": "In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014). The text is from English Wikipedia articles about history3, preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution.\n2https://en.wikisource.org/wiki/Zodiac Killer letters 3http://en.wikipedia.org/wiki/History\nFig 1 plots the results of our method for cipher lengths of 16, 32, 64, 128 and 256 alongside Beam 6-gram (the best performing model) model (Nuhn et al., 2013)"
  }, {
    "heading": "4.2 An Easy Cipher: Zodiac-408",
    "text": "Zodiac-408, a homophonic cipher, is commonly used to evaluate decipherment algorithms.\nOur neural LM model with global rest cost estimation and frequency matching heuristic with a beam size of 1M has SER of 1.2% compared to the beam search algorithm (Nuhn et al., 2013) with beam size of 10M with a 6-gram LM which gives an SER of 2%. The improved beam search (Nuhn et al., 2014) with an 8-gram LM, however, gets 52 out of 54 mappings correct on the Zodiac-408 cipher."
  }, {
    "heading": "4.3 A Hard Cipher: Beale Pt 2",
    "text": "Part 2 of the Beale Cipher is a more challenging homophonic cipher because of a much larger search space of solutions. Nunh et al. (2014) were the first to automatically decipher this Beale Cipher.\nWith an error of 5% with beam size of 1M vs 5.4% with 8-gram LM and a pruning size of 10M, our system outperforms the state of the art (Nuhn et al., 2014) on this task.\n!1"
  }, {
    "heading": "5 Related Work",
    "text": "Automatic decipherment for substitution ciphers started with dictionary attacks (Hart, 1994; Jakobsen, 1995; Olson, 2007). Ravi and Knight (2008) frame the decipherment problem as an integer linear programming (ILP) problem. Knight et al. (2006) use an HMM-based EM algorithm for solving a variety of decipherment problems. Ravi and Knight (2011) extend the HMM-based EM approach with a Bayesian approach, and report the\nfirst automatic decipherment of the Zodiac-408 cipher.\nBerg-Kirkpatrick and Klein (2013) show that a large number of random restarts can help the EM approach.Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers. Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm. Nuhn et al. (2014) present various improvements to the beam search algorithm in Nuhn et al. (2013) including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. Hauer et al. (2014) propose a novel approach for solving mono-alphabetic substitution ciphers which combines character-level and word-level language model. They formulate decipherment as a tree search problem, and use Monte Carlo Tree Search (MCTS) as an alternative to beam search. Their approach is the best for short ciphers.\nGreydanus (2017) frames the decryption process as a sequence-to-sequence translation task and uses a deep LSTM-based model to learn the decryption algorithms for three polyalphabetic ciphers including the Enigma cipher. However, this approach needs supervision compared to our approach which uses a pre-trained neural LM. Gomez et al. (2018) (CipherGAN) use a generative adversarial network to learn the mapping between the learned letter embedding distributions in the ciphertext and plaintext. They apply this approach to shift ciphers (including Vigenère ciphers). Their approach cannot be extended to homophonic ciphers and full message neural LMs as in our work."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper presents, to our knowledge, the first application of large pre-trained neural LMs to the decipherment problem. We modify the beam search algorithm for decipherment from Nuhn et al. (2013; 2014) and extend it to use global scoring of the plaintext message using neural LMs. To enable full plaintext scoring we use the neural LM to sample plaintext characters which reduces the beam size required. For challenging ciphers such as Beale Pt 2 we obtain lower error rates with smaller beam sizes when compared to the state of the art in decipherment for such ciphers."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their helpful remarks. The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada grants NSERC RGPIN-2018-06437 and RGPAS-2018522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025 to the third author."
  }],
  "year": 2018,
  "references": [{
    "title": "Decipherment with a million random restarts",
    "authors": ["Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874– 878.",
    "year": 2013
  }, {
    "title": "An exact A* method for deciphering letter-substitution ciphers",
    "authors": ["Eric Corlett", "Gerald Penn."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040–1047. Association for Computational Linguis-",
    "year": 2010
  }, {
    "title": "Unsupervised cipher cracking using discrete gans",
    "authors": ["Aidan N. Gomez", "Scng Huang", "Ivan Zhang", "Bryan M. Li", "Muhammad Osama", "ukasz Kaiser"],
    "venue": "arXiv preprint arXiv:1801.04883",
    "year": 2018
  }, {
    "title": "Learning the enigma with recurrent neural networks",
    "authors": ["Sam Greydanus."],
    "venue": "arXiv preprint arXiv:1708.07576.",
    "year": 2017
  }, {
    "title": "To decode short cryptograms",
    "authors": ["George W Hart."],
    "venue": "Communications of the ACM, 37(9):102–108.",
    "year": 1994
  }, {
    "title": "Solving substitution ciphers with combined language models",
    "authors": ["Bradley Hauer", "Ryan Hayward", "Grzegorz Kondrak."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2314–2325.",
    "year": 2014
  }, {
    "title": "A fast method for cryptanalysis of substitution ciphers",
    "authors": ["Thomas Jakobsen."],
    "venue": "Cryptologia, 19(3):265– 274.",
    "year": 1995
  }, {
    "title": "Unsupervised analysis for decipherment problems",
    "authors": ["Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada."],
    "venue": "Proceedings of the COLING/ACL on Main conference poster sessions, pages 499–506. Association for Computational Linguis-",
    "year": 2006
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomáš Mikolov", "Martin Karafiát", "Lukáš Burget", "Jan Černockỳ", "Sanjeev Khudanpur."],
    "venue": "Eleventh Annual Conference of the International Speech Communication Association.",
    "year": 2010
  }, {
    "title": "Beam search for solving substitution ciphers",
    "authors": ["Malte Nuhn", "Julian Schamper", "Hermann Ney."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1568–1576.",
    "year": 2013
  }, {
    "title": "Improved decipherment of homophonic ciphers",
    "authors": ["Malte Nuhn", "Julian Schamper", "Hermann Ney."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1764–1768.",
    "year": 2014
  }, {
    "title": "Robust dictionary attack of short simple substitution ciphers",
    "authors": ["Edwin Olson."],
    "venue": "Cryptologia, 31(4):332–342.",
    "year": 2007
  }, {
    "title": "Learning to generate reviews and discovering sentiment",
    "authors": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1704.01444.",
    "year": 2017
  }, {
    "title": "Attacking decipherment problems optimally with low-order ngram models",
    "authors": ["Sujith Ravi", "Kevin Knight."],
    "venue": "proceedings of the conference on Empirical Methods in Natural Language Processing, pages 812–819. Association for Computational",
    "year": 2008
  }, {
    "title": "Bayesian inference for zodiac and other homophonic ciphers",
    "authors": ["Sujith Ravi", "Kevin Knight."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 239–247. As-",
    "year": 2011
  }, {
    "title": "Minimally supervised morphological analysis by multimodal alignment",
    "authors": ["David Yarowsky", "Richard Wicentowski."],
    "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 207–216. Association for Com-",
    "year": 2000
  }],
  "id": "SP:be9fc6faa8d60808b3f7e56d61f6a15369bbf534",
  "authors": [{
    "name": "Nishant Kambhatla",
    "affiliations": []
  }, {
    "name": "Anahita Mansouri Bigvand",
    "affiliations": []
  }, {
    "name": "Anoop Sarkar",
    "affiliations": []
  }],
  "abstractText": "Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. (2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.",
  "title": "Decipherment of Substitution Ciphers with Neural Language Models"
}