{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Inference of parameters in a probabilistic model is an essential ingredient in model-based statistical approaches, both in the frequentist and Bayesian paradigms. Given a probabilistic model P (y|θ), which is a conditional distribution of observations y given a parameter θ, the aim is to make inference about the parameter θ∗ that generated an observed data y∗. When the model P (y|θ) admits a conditional density `(y|θ), such an inference can be made on the basis of evaluations of `(y∗|θ); this is the likelihood of y∗ as a function of θ. However, in modern scientific and engineering problems in which the model P (y|θ) is required to be sophisticated and complex, the likelihood function `(y∗|θ) might no longer be available. This may be because the density form of P (y|θ) is elusive, or the evaluation of the likelihood `(y∗|θ) is computationally very expensive. Such situations, in which `(y|θ) (or P (y|θ)) are referred to as intractable likelihood, make the inference problem quite challenging and are commonly found in the literature on\n1NEC Corporation 2National Institute of Advanced Industrial Science and Technology 3Max Planck Institute for Intelligent Systems 4The Institute of Statistical Mathematics. Correspondence to: Takafumi Kajihara <t-kajihara@ct.jp.nec.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\npopulation genetics (Pritchard et al., 1999) and dynamical systems (Toni et al., 2009), to name just two.\nApproximate Bayesian Computation (ABC) is a class of computational methods for Bayesian inference with intractable likelihood (Tavaré et al., 1997; Pritchard et al., 1999; Beaumont et al., 2002) that is applicable as long as sampling from the model P (y|θ) is possible. Given a prior π(θ) on the parameter space, the basic ABC constructs a Monte Carlo approximation to the posterior Py∗(θ) ∝ P (y∗|θ)π(θ) in the following way: i) sample pairs (yi, θi) of pseudo data yi and parameter θi from the joint distribution P (y|θ)π(θ), where i = 1, . . . , n for some n ∈ N, ii) maintain only those parameters θi associated with yi that are “close enough” to the observed data y∗, and iii) regard them as samples from the posterior Py∗(θ). ABC has been extensively studied in statistics and machine learning; see, e.g., Del Moral et al. (2012); Fukumizu et al. (2013); Meeds and Welling (2014); Park et al. (2016); Mitrovic et al. (2016).\nIn this paper, we rather take the frequentist perspective, and deal with the problem of maximum likelihood estimation (MLE) with intractable likelihood. That is, we consider situations in which one believes that there is a “true” parameter θ∗ that generated the data y∗ and wishes to obtain a point estimate for it. This problem is also motivated by the following situations encountered in practice: 1) Consider a situation in which the model is computationally expensive (e.g., a state-space model) and one wants to perform prediction based on it. In this case fully Bayesian prediction would require simulation from each of sampled parameters, which might be quite costly. If one has a point estimate of the true parameter θ∗, then the computational cost can be drastically reduced. 2) Consider a situation in which one only has limited knowledge w.r.t. model parameters. In this case, it is generally difficult to specify an appropriate prior distribution over the parameter space, and thus the resulting posterior may not be reliable.1 Methods for point estimation with intractable likelihood have been reported in the literature, including the method of simulated-moments (McFadden, 1989), indirect inference (Gourieroux et al.,\n1For point estimation, one may think of using the maximum a posterior (MAP) estimate, but it may again be unreliable (as for the posterior distribution itself), if the prior distribution cannot be specified appropriately.\n1993), ABC-based MAP estimation (Rubio et al., 2013), noisy ABC-MLE (Dean et al., 2014; Yıldırım et al., 2015), an approach based on Bayesian optimization (Gutmann and Corander, 2016), and data-cloning ABC (Picchini and Anderson, 2017). We will discuss these existing approaches in Sec. 4.\nOur contribution is in proposing a novel approach to point estimation with intractable likelihood on the basis of kernel mean embedding of distributions (Muandet et al., 2017), a framework for statistical inference using reproducing kernel Hilbert spaces. Specifically, our approach extends kernel ABC (Fukumizu et al., 2013; Nakagome et al., 2013), a method for ABC using kernel embedding of conditional distributions (Song et al., 2009; 2013), to point estimation with intractable likelihood. The novelty lies in combining kernel ABC with kernel herding (Chen et al., 2010), a deterministic sampling method similar to quasi-Monte Carlo (Dick et al., 2013), and in applying these two methods iteratively to the same observed data in a recursive way. We term this approach kernel recursive ABC. A theoretical explanation will be provided for this approach, discussing how such recursion yields a point estimate for the true parameter. We also discuss that the combination of kernel ABC and kernel herding leads to robustness against misspecification of a prior for the true parameter; this is an advantage over existing methods, and will be demonstrated experimentally.\nThis paper is organized as follows. We briefly review kernel ABC and kernel herding in Sec. 2 and propose kernel recursive ABC in Sec. 3. We report experimental results of comparisons with existing methods in Sec. 4. The experiments include parameter estimation for a real-world pedestrian flow simulator (Yamashita et al., 2010), which may be of independent interest as application."
  }, {
    "heading": "2. Background",
    "text": ""
  }, {
    "heading": "2.1. Kernel ABC",
    "text": "Kernel ABC is an algorithm that executes ABC in a reproducing kernel Hilbert space (RKHS) and produces a reliable solution even in moderately large dimensional problems (Fukumizu et al., 2013; Nakagome et al., 2013). It is based on the framework of kernel mean embeddings, in which all probability measures are represented as elements in an RKHS (see Muandet et al. (2017) for a recent survey of this field). Let Θ be a measurable space, k : Θ × Θ → R be a measurable positive definite kernel, and H be its RKHS. In this framework, any probability measure P on Θ will be represented as a Bochner integral\nµP := ∫ Θ k(·, θ)dP (θ) ∈ H, (1)\nwhich is called the kernel mean of P . If the mapping P → µP is injective, in which case µP preserves all the\ninformation in P , the kernel k is referred to as being characteristic (Fukumizu et al., 2008). Characteristic kernels on Θ = Rd, for example, include Gaussian and Matérn kernels (Sriperumbudur et al., 2010).\nLet Y be another measurable space and assume that an observed data y∗ ∈ Y is provided. (y∗ is often a set of sample points.) Given a conditional probability P (y|θ) and a prior π(θ), we wish to obtain the posterior distribution Py∗(θ) ∝ P (y∗|θ)π(θ).2 As in a standard ABC, kernel ABC achieves this by first generating pairs of pseudo data and parameter {(yi, θi)}ni=1 from the joint distribution P (y|θ)π(θ). It then estimates the kernel mean of the posterior Py∗ , which we denote by\nµPy∗ := ∫ Θ k(·, θ)dPy∗(θ) ∈ H.\nGiven a measurable positive definite kernel kY on Y , the estimator is given by\nµ̂Py∗ = n∑ i=1 wik(·, θi) ∈ H, (2)\nw := (w1, . . . , wn) T := (G+ nδI)−1k(y∗), (3)\nwhere k(y∗) := (kY(y1, y∗), . . . , kY(yn, y∗))T ∈ Rn, G := (kY(yi, yj)) n i,j=1 ∈ Rn×n, δ > 0 is a regularization constant, and I ∈ Rn×n is an identity matrix. The estimator (2) is essentially an (RKHS-valued) kernel ridge regression (Grünewälder et al., 2012): Given training data {(yi, k(·, θi))}ni=1, the weights (3) provide an estimator for the mapping y∗ ⇒ k(·, θ∗). For consistency and convergence results, which require δ → 0 as n→∞, we re refer to Fukumizu et al. (2013) and Muandet et al. (2017)."
  }, {
    "heading": "2.2. Kernel herding",
    "text": "Kernel herding is a deterministic sampling technique based on the kernel mean representation of a distribution (Chen et al., 2010) and can be seen as a greedy approach to quasiMonte Carlo (Dick et al., 2013). Consider sampling from P using the kernel mean µP (1), and assume that one is able to evaluate function values of µP . Kernel herding greedily obtains sample points θ1, θ2, . . . , θn by iterating the following steps: Defining h0 := µP ,\nθt+1 = argmax θ∈Θ ht(θ), (4) ht+1 = ht + µP − k(·, θt+1) ∈ H, (5)\nwhere t = 0, . . . , n− 1. Chen et al. (2010) has shown that, if there exists a constant C > 0 such that k(θ, θ) = C for all θ ∈ Θ, this procedure will be identical to the greedy\n2There is abuse of notation here, as P (y|θ) does not denote a conditional density but a conditional distribution.\nAlgorithm 1 Kernel Recursive ABC Input: A prior distribution π, an observed data y∗, a data generator P (y|θ), the number Niter of iterations, the number n of simulated pairs, a kernel k on Θ, a kernel kY on Y , and a regularization constant δ > 0 Output: A point estimate θ́. for N = 1, ..., Niter do\nif N = 1 then for i = 1, ..., n do\nSample θ1,i ∼ π(θ) i.i.d. end for\nend if for i = 1, ..., n do\nGenerate yN,i ∼ P (·|θN,i) end for Compute G := (kY(yN,i, yN,i))ni,j=1 ∈ Rn×n and k(y) := (kY(yN,i, y\n∗))ni=1 ∈ Rn. Calculate w = (w1, . . . , wn)T ∈ Rn by Eq.(3). Construct a kernel mean estimate of the powered posterior µ̂PN := ∑n i=1 wik(·, θN,i) Sample {θN+1,t}nt=1 by performing kernel herding Eqs.(4) (5) with µP := µ̂PN .\nend for Obtain a point estimate θ́ := θNiter+1,1\nminimization of the maximum mean discrepancy (MMD) (Gretton et al., 2007; 2012):\nn := ∥∥∥∥∥µP − 1n n∑ t=1 k(·, θt) ∥∥∥∥∥ H , (6)\nwhere ‖ · ‖H denotes the norm of H. That is, the points θ1, . . . , θn are obtained so as to (greedily) minimize the distance εn between µP and the empirical kernel mean 1 n ∑n t=1 k(·, θt). The generated points θ1, . . . , θn are also called super-samples because they are more informative than those from random sampling; this is in the sense that error decreases at the rate n = O(n−1) if the RKHS is finite-dimensional (Bach et al., 2012), which is faster than the rate n = O(n−1/2) of random sampling (Smola et al., 2007). Convergence guarantees are also provided even when the optimization problem in (4) is solved approximately (Lacoste-Julien et al., 2015) and when the kernel mean µP is replaced by an empirical estimate µ̂P of the form (2) (Kanagawa et al., 2016b). Note that the decay n → 0 of the error (6) as n→∞ implies the convergence of expectation 1 n ∑n t=1 f(θt) → ∫ f(x)dP (x) for all functions f in the RKHSH and for functions f that can be approximated well by the RKHS functions (Kanagawa et al., 2016a)."
  }, {
    "heading": "3. Proposed method",
    "text": "Our idea is to recursively apply Bayes’ rule to the same\nobserved data y∗ by using the posterior obtained in one iteration as a prior for the next iteration. For this, let `(θ) := `(y∗|θ) be a likelihood function and π(θ) be a prior density, where θ ∈ Θ, with Θ being a measurable space. Consider the population setting in which no estimation procedure is involved. After theN -th recursion, the posterior distribution becomes\npN (θ) := C −1 N π(θ)(`(θ)) N , (7)\nwhere CN := ∫\nΘ π(θ) (`(θ))\nN dθ is a normalization con-\nstant. We refer here to this as a powered posterior. If ` has a unique global maximum at θ∞ ∈ Θ and the support of π contains θ∞, one can show that pN converges weakly to the Dirac distribution δθ∞ at θ∞ under certain conditions (Lele et al., 2010). In other words, the effect of the prior diminishes as the recursion proceeds, and the powered posterior degenerates at the maximum likelihood point, providing a method for MLE. A similar idea has been discussed by Doucet et al. (2002); Lele et al. (2010) in the context of data augmentation and data cloning, in which one replicates the observed data y∗ multiple times and applies Bayes’ rule once; our approach is different, as we employ recursive applications of Bayes’ rule multiple times (this turns out to be beneficial in our approach, as is shown below).\nBased on the above idea, we propose to recursively applying kernel ABC (Sec. 2.1) and kernel herding (Sec. 2.2). Specifically, the proposed method (Algorithm 1) iterates the following procedures: (i) At the N -th iteration, the kernel mean µPN := ∫ k(·, θ)pN (θ)dθ of the powered posterior (7) is estimated using simulated pairs {(θN,i, yN,i)}ni=1 via kernel ABC; (ii) from the estimate µ̂PN of µPN given in (i), new parameters {θN+1,i}ni=1 are generated via kernel herding, and new pseudo-data {yN+1,i}ni=1 are generated from the simulator P (yN+1,i|θN+1,i) in the N + 1-th iteration. After iterating these procedures Niter times, point estimate θ́ for the true parameter is given as the first point θNiter+1,1 from kernel herding at the last iteration.\nAuto-correction mechanism. An interesting feature of the proposed approach is that, as experimentally indicated in Sec. 4.2, it is equipped with an auto-correction mechanism: If the parameters θN,1, . . . , θN,n at theN -th iteration are far apart from the true parameter θ∗, then Algorithm 1 searches for the parameters θN+1,1, . . . , θN+1,n at the next iteration, so as to explore the parameter space Θ. For instance, if the prior π(θ) is misspecified, meaning that the true parameter θ∗ is not contained in the support of π(θ), then the initial parameters θ1,1, . . . , θ1,n from π(θ) are likely to be apart from the true parameter θ∗. The auto-correction mechanism makes the proposed method robust to such misspecification and makes it suitable for use in situations in which one lacks appropriate prior knowledge about the true parameter.\nTo explain how this works, let us explicitly write down the procedure (4) (5) of kernel herding as used in Algorithm 1.\nGiven that t (< n) points θN+1,1, . . . , θN+1,t have already been generated, the next point θN+1,t+1 is obtained as\nθN+1,t+1 := (8)\nargmax θ∈Θ n∑ i=1 wik(θ, θN,i)− 1 t+ 1 t∑ i=1 k(θ, θN+1,i),\nwhere the weights w1, . . . , wn are given as (3). Assume that all the simulated parameters θN,1, . . . , θN,n at the N -th iteration are far apart from the true parameter θ∗: If N = 1, these are the parameters sampled from the prior π(θ). Then it is likely the resulting simulated data yN,1, . . . , yN,n are dissimilar to the observed data y∗. In this case, each component of the vector k(y) := (kY(yN,i, y∗))ni=1 ∈ Rn becomes nearly 0, since kY(yN,i, y∗) quantifies the similarity between y∗ and yN,i. As a result, each of the weights w1, . . . , wn given by kernel ABC (3) also become nearly 0, and thus the first term on the right side in (8) will be ignorable. The point θN+1,t+1 is then obtained so as to roughly maximize the second term − 1t+1 ∑t i=1 k(θN+1,t+1, θN+1,i), or, equivalently, so as\nto minimize ∑t i=1 k(θN+1,t+1, θN+1,i). Since the kernel k(θN+1,t+1, θN+1,i) measures the similarity between θN+1,t+1 and θN+1,i, the new point θN+1,t+1 is located apart from the points θN+1,1, . . . , θN+1,t generated so far. In this way, the parameters θN+1,1, . . . , θN+1,n at theN+1th iteration are made to explore the parameter space Θ if parameters θN,1, . . . , θN,n at the N -th iteration are far apart from the true parameter θ∗."
  }, {
    "heading": "3.1. Theoretical analysis",
    "text": "We provide here a theoretical basis for the proposed recursive approach. Since the consistency of kernel ABC and kernel herding have already been established in the literature (Fukumizu et al., 2013; Bach et al., 2012), we focus on convergence analysis in the population setting, that is, convergence analysis for the kernel mean of the powered posterior (7) and for the resulting point estimate. We nevertheless note that convergence analysis of the overall procedure of Algorithm 1 remains an important topic for future research. All the proofs can be found in the Supplementary Materials.\nBelow, we let Θ be a Borel measurable set in Rd. Denote by PN the probability measure induced by the powered posterior density pN (7), and let µPN := ∫ k(·, θ)dPN (θ) ∈ H be its kernel mean, where k is a kernel on Θ and H is its RKHS. We require the following assumption for the likelihood function ` and the prior π for theoretical analysis. Assumption 1. (i) ` has a unique global maximum at θ∞ ∈ Θ, and π(θ∞) > 0; (ii) π is continuous at θ∞, ` has continuous second derivatives in the neighborhood of θ∞, and the Hessian of ` at θ∞ is strictly negative-definite.\nOur first result below shows that, under Assumption 1, the\npowered posterior PN (7) converges to the Dirac distribution δθ∞ in the RKHSH as N →∞; this provides a theoretical basis for recursively applying the kernel ABC.\nProposition 1. Let Θ ⊂ Rd be a Borel measurable set and k : Θ × Θ → R be a continuous bounded kernel. Under Assumption 1, we have limN→∞ ‖µPN − k(·, θ∞)‖H = 0.\nProposition 2 below provides a justification for the use of the first point of kernel herding (here this is θN := argminθ̃∈Θ ‖µPN − k(·, θ̃)‖H; see Sec. 2.2) as a point estimate of θ∞. To this end, we introduce the following assumption on the kernel, which is satisfied by, for example, Gaussian and Matérn kernels.\nAssumption 2. (i) There exists a constant C > 0 such that k(θ, θ) = C for all θ ∈ Θ. (ii) It holds that k(θ, θ′) < C for all θ, θ′ ∈ Θ with θ 6= θ′. Proposition 2. Let Θ ⊂ Rd be a compact set, and k : Θ × Θ → R be a continuous, bounded kernel. Let θN := argminθ̃∈Θ\n∥∥∥µPN − k(·, θ̃)∥∥∥H. If Assumptions 1 and 2 hold, then we have θN → θ∞ as N →∞.\nWe make a few remarks regarding Assumption 1. The assumption that ` has a unique global maximum is not satisfied if the model is singular, an example being mixture models: In this case there are multiple global maximums. However, our experiment in Sec. 4.5 shows that even for mixture models, the proposed method works reasonably well. This suggests that, in an empirical setting, a point estimate may converge to one of the global maximums. The assumption π(θ∞) > 0 will also not be satisfied if the support π does not contain θ∞, but the proposed method performs well even in this case (as shown in 4.2), possibly thanks to the auto-correction mechanism explained above. We reserve further analysis of these properties for future work."
  }, {
    "heading": "4. Experiments",
    "text": "We have conducted a variety of experiments comparing the proposed method with existing approaches. We begin with a quick review of these approaches (Sec. 4.1), and report experimental results on point estimation with a misspecified prior (Sec. 4.2), population dynamics of the blowfly (Sec. 4.3), alpha stable distributions (Sec. 4.4), Gaussian mixture models with redundant components (Sec. 4.5), and a real-world pedestrian simulator (Sec. 4.6)."
  }, {
    "heading": "4.1. Existing approaches and experimental settings",
    "text": "K2-ABC (Park et al., 2016) is an ABC method that represents the empirical distributions of simulated and test observations as kernel means in an RKHS. For each of simulated parameters, the associated weight is calculated by using the RKHS distance between the kernel means (i.e., MMD), and the resulting weighted sample is treated as a posterior\ndistribution. Adaptive SMC-ABC (Del Moral et al., 2012) is a rejection-based approach based on sequential Monte Carlo, which sequentially updates the tolerance level and the associated proposal distribution in an adaptive manner. This method is a state-of-the-art ABC approach. The approach by Gutmann and Corander (2016), which we refer to as Bayesian Optimization for simplicity, is a method for MLE with intractable likelihood based on Bayesian optimization (Brochu et al., 2010). This method optimizes the parameters in a intractable model so as to minimize the discrepancy between the simulated and test observations. Note that comparison with this method in terms of computation time may not make sense (although we report them for purposes of completeness), as we used publicly available code3 for implementation. The method of simulated moments (MSM) (McFadden, 1989) optimizes the parameter in the model so that the resulting moments of simulated data match those of observe data. MSM may be seen a special case of indirect inference (Gourieroux et al., 1993), an approach studied in econometrics.4 Data-cloning ABC (ABC-DC) (Picchini and Anderson, 2017) is an approach combining ABC-MCMC (Marjoram et al., 2003) and Data Cloning (Lele et al., 2010), replicating observed data multiple times to achieve MLE with intractable likelihood.\nExperimental settings. Unless otherwise specified, the following settings were applied in the experiments. For all the methods that employed kernels, we used Gaussian kernels. The discrepancy between the simulated and observed data was measured by the energy distance (Székely and Rizzo, 2013), which is a standard metric for distributions in statistics and can be computed only from pairwise Euclidean distances between data points. Since the usual quadratic time estimator was too costly, we used a linear time estimator for computing the energy distance (see the Supplementary Materials for details).\nFor each method, unless otherwise specified, we determined the hyper-parameters on the basis of the cross-validationlike approach described in Park et al. (2016, Sec. 4). That is, to evaluate one configuration of hyper-parameters, we first used 75% of the observed data for point estimation and then computed the discrepancy between the rest of the observed data and the ones simulated from point estimates; after applying this procedure to all candidate configurations, the one with the lowest discrepancy was finally selected. The bandwidth of a Gaussian kernel was selected from candidate values, each of which is the median (of pairwise distances) multiplied by logarithmically equally spaced values between 2−4 and 24 (Takeuchi et al., 2006, Sec. 5.1.1). Regularization constants for the proposed method and kernel ABC, as well as the soft threshold for K2-ABC, were selected\n3https://sheffieldml.github.io/GPyOpt/ 4MSM is a special case of indirect inference because the mo-\nments can be regarded as the parameters of an auxiliary model.\nfrom logarithmically spaced values between 10−4 and 1. To compute MMD for K2-ABC, a linear time estimator (Gretton et al., 2012, Sec. 6) was used to reduce computational time, as the usual quadratic time estimator was too costly. For Adaptive SMC-ABC, the initial tolerance level was set as the median of pairwise distances between the observed and simulated data. For Bayesian Optimization, we used Expected Improvement as an acquisition function, and all the hyper-parameters were marginalized out following the approach of Snoek et al. (2012, Sec. 3.2). For MSM, the number of moments were selected from a range up to 30 by the cross-validation like approach. For ABC-DC, we employed, in particular, dynamic ABC-DC, which automatically adjusts its associated parameters. To obtain point estimates with kernel ABC and K2-ABC, we computed the means of the resulting posterior distributions. For Adaptive SMC-ABC, point estimates were obtained as posterior means as well as MAP estimates by applying the mean shift algorithm to posterior weighted samples (Fukunaga and Hostetler, 1975), the latter essentially being an approach suggested by Rubio et al. (2013).\nThe following abbreviations may be used for the sake of simplicity; kernel recursive ABC is referred to as KR-ABC, kernel ABC as K-ABC, adaptive SMC-ABC as SMC-ABC, Bayesian Optimization as BO, and Dynamic ABC-DC as ABC-DC. For our method, we also report results based on a half number of iterations, which we call KR-ABC (less)."
  }, {
    "heading": "4.2. Multivariate Gaussian distribution with a severely misspecified prior",
    "text": "As a proof of concept regarding the auto-correction mechanism of the proposed method described in Sec.3, we have performed an experiment for when the prior distribution is severely misspecified (see the Supplementary Materials for an illustration). The task is to estimate the mean vector of a 20-dimensional Gaussian distribution Normal(µ,Σ), where the true mean vector is µ := (10, 50, 90, 130, 180, 280, 390, 430, 520, 630, 1010, 1050, 1090, 1130, 1180, 1280, 1390, 1430, 1520, 1630)T ∈ R20. The covariance matrix Σ ∈ R20×20 is assumed to be known and is a diagonal matrix with all diagonal elements being 40. Test data y∗ consisted of 100 i.i.d. observations from this Gaussian distribution. As a prior for the mean vector µ, we used the uniform distribution on [9× 106, 107]20, which is extremely misspecified. For Bayesian optimization, the space to be explored was set as [0, 107]20.\nIn this experiment, each pseudo data was made from 100 observations simulated with one parameter configuration. K2-ABC and K-ABC used 3000 pairs of a parameter and pseudo-data. For the proposed method and SMC-ABC, we generated 100 pairs of a parameter and pseudo-data for the initial iteration, and then the iterations were repeated 30\ntimes, resulting in a total of 3000 simulations. For the proposed method, the bandwidth of the kernel kY on observed data was recomputed for each iteration, using the median heuristic. For SMC-ABC, the parameter α ∈ (0, 1), which controls the trade-off between the speed of convergence and the accuracy of posterior approximation, was set to be 0.3, as we found this value to be the best in terms of the trade-off.\nFor each method, we ran 30 independent trials, and the results in averages and standard deviations are shown in Table 1, where the parameter error is the mean (over 20 dimensions) of the absolute difference between the estimated and the true parameter values divided by the true value, and the data error is the energy distance between the true data and pseudo data simulated with the estimated parameter. Surprisingly, the proposed method successfully approached the true parameter even when the prior was severely misspecified. As discussed in Sec 3 and demonstrated in the Supplementary Materials, this would appear to be because of the use of kernel herding, which automatically widens the space to explore when simulated data is far apart from test data. As expected, other methods were unable to approach the true parameter."
  }, {
    "heading": "4.3. Ecological dynamic systems: blowfly",
    "text": "Following Park et al. (2016), we performed an experiment on parameter estimation with a dynamical system of blowfly populations (Wood, 2010), which is defined as\nNt+1 = PNt−τ exp (−Nt−τ/N0) et + Nt exp(−δ t),\nwhere t = 1, . . . , T are time indices, Nt is the population at time t, et ∼ Gam( 1σ2pσ 2 p) and t ∼ Gam( 1σ2d , σ 2 d) are independent Gamma-distributed noise, and θ := (P ∈ N, N0 ∈ N, σd ∈ R+, σp ∈ R+, τ ∈ N, δ ∈ R+) are the parameters of the system. The task is to estimate θ from observed values of N1, . . . , NT . We set the true parameters as θ = (29, 260, 0.6, 0.3, 7, 0.2), and the time-length T for both the observed and pseudo data as T = 1000. Following Park et al. (2016, Sec. 4), for each parameter we defined a Gaussian prior on its logarithm (see the Supplementary Materials for a definition). In this experiment, for all methods we converted the observed and pseudo-data into histograms\nwith 1000 bins (i.e., we treated each data as a 1000 dim. vector), as this produced better results. K2-ABC and K-ABC used 1300 pairs of a parameter and a pseudo-data item. For the proposed method and SMC-ABC, we generated 100 pairs of a parameter and pseudo-data for the initial iteration, and the iterations were then repeated 13 times, resulting in total 1300 simulations. For SMC-ABC, we set the parameter α ∈ (0, 1) to be 0.3, as in Sec. 4.2.\nFor each method we performed 30 independent trials, and the results are summarized in Table 2. The proposed method performed the best, even when the number of simulations was halved (i.e., KR-ABC (less))."
  }, {
    "heading": "4.4. Multivariate elliptically contoured alpha stable distribution",
    "text": "In addition to the competitive methods described earlier, we performed a comparison with the method called noisy ABC-MLE (Yıldırım et al., 2015). This method assumes that sampling from the intractable model can be realized by deterministic mapping applied to a simple random variable, and that the gradient of the deterministic mapping is available. This method can, then, only be applied to a limited class of generative models, though for such models it can perform well. Although the main scope of this paper is on simulation models in which such gradient information is unavailable, we performed this experiment in order to see how the proposed method compared with this method without relying on the gradient information.\nWe also considered parameter estimation with multivariate elliptically contoured alpha stable distributions (Nolan, 2013), which subsume heavy-tailed and skewed distributions and are popular for modeling financial data. This family of distributions in general does not admit closedform expressions for density functions, which means they are “intractable” in the sense that the standard procedure for parameter estimation cannot be employed. However, sampling of a random vector X ∈ Rd from this family is possible in the following way:\nX := A1/2G + δ ∈ Rd, G ∼ Normal(0, Q), A := τθ(U1, U2) ∈ R,\nU1 ∼ Unif(−π/2, π/2), U2 ∼ Exp(1),\nwhere Q ∈ Rd×d is positive definite, δ ∈ Rd, θ := (α, β, µ, σ) ∈ (0, 2] × [−1, 1] × R × [0,∞), τθ is a deterministic mapping whose concrete form is described in the Supplementary Materials, and Unif and Exp denote uniform and exponential distributions, respectively.\nWe dealt with estimation of α := 1.3 and Q, while fixing the other parameters as δ := 0, β := 1, µ := 0, and σ := 1. We restricted Q to be a positive definite matrix such that all diagonal elements are the same and so are the off-diagonal elements. We defined true Q to be a matrix whose diagonal elements are 1.0 and off-diagonals are 0.2. Therefore the task was to estimate these three values (i.e., 1.3 for α, and 1.0 and 0.2 for Q). We used Unif[0, 2] as a prior for α, and Unif[0, 5] as a prior for each of the diagonal and offdiagonal values of Q.\nFigure 1 shows results for the averages of mean square errors in parameter estimation over 30 independent trials, with variation in the dimensionality d from 2 to 16. For each method we sampled a total of 1400 pairs of a parameter and a pseudo-data item, and for iterative methods we used 100 pairs in each iteration. Each pseudo-data (and observed-data) was made up of 1000 points. The noisy ABC-MLE exploited the gradient information in τθ, while the other methods did not. The proposed method was competitive with BO and outperformed the other methods with the exception of the noisy ABC-MLE. Although the noisy ABC-MLE was accurate for lower-dimensionality (as expected), it exhibited a steep increase in errors for higher dimensionality. In contrast, the performance degradation of the proposed method was mild for higher dimensionality."
  }, {
    "heading": "4.5. Gaussian mixture with redundant components",
    "text": "We consider here a parametric model in which there exist redundant parameters for expressing given data. We are interested in whether point estimation with the proposed method results in elimination of the redundant parameters\nwhen applied to such a model. This was motivated by Yamazaki and Kaji (2013), who argued that, for mixture models, the use of a Dirichlet prior with a sufficiently small concentration parameter leads to elimination of unnecessary components. We therefore focus on mixture models with redundant components.\nSpecifically, we considered Gaussian mixture models. We defined the true model as a two-component Gaussian mixture ∑2 i=1 φiNormal(µi, 20) of equal variances. The task was to estimate the mixture coefficients (φ1, φ2, ) := (0.7, 0.3) and the associate means (µ1, µ2) := (110, 70), provided 3000 i.i.d. sample points from the model as observed data y∗. We employed an over-parametrized model for point estimation (i.e., no method used the knowledge that the truth consisted of 2 components), which is a fourcomponent Gaussian mixture ∑4 i=1 φiNormal(µi, 20). We used a 4-dimensional Dirichlet distribution with equal concentration parameters 0.01 as a prior for the coefficients (φ1, . . . , φ4), and Normal(0, 100) as a prior for each of µ1, . . . , µ4.\nFor each method, we generated a total of 1000 pairs of a parameter and pseudo-data, and for iterative methods, we made use of 100 pairs in each iteration, resulting in 10 iterations. Each pseudo-data consisted of 3000 simulated observations. For all the methods, we converted each data item into a histogram of 300 bins and treated it as a 300 dim. vector since this resulted in better performances. We set the parameter α ∈ (0, 1) of SMC-ABC to be 0.2, as this performed well in this experiment.\nWe ran each algorithm 30 times, and the resulting average errors and standard deviations are shown in Table 3, where the φ error and µ error denote the errors for the coefficients and the means, respectively, as measured in terms of Euclidean distance. More precisely, since any permutation of component labels will result in the same model, we first sorted the estimated parameters {(φi, µi)} so that φ1 ≥ · · · ≥ φ4, and we then measured the errors w.r.t. the ground truth φ := (0.7, 0.3, 0, 0) and µ := (110, 70). For the µ error, we computed the errors only for the estimated means µ1, µ2 associated with the two largest coefficients since there was no ground truth for the redundant components µ3, µ4. Results show that the proposed KR-ABC performed best, indicating that the dominant components were successfully estimated."
  }, {
    "heading": "4.6. Real-world pedestrian simulator",
    "text": "Our final experiment was parameter estimation with CrowdWalk, a publicly available real-world simulator5 for the movements of pedestrians in a commercial district (Yamashita et al., 2010). It has been used to gain insights into pedestrian behavior at a variety of events and occurrences, such as fireworks festivals and evacuations after earthquakes. As this simulator is complicated and also computationally expensive, its likelihood function is intractable.\nUsing CrowdWalk, we simulated the movements of pedestrians in Ginza, a commercial district in Tokyo (see Supplementary Materials for an illustration). Specifically, we modeled pedestrians as a mixture of multiple groups, each of which has the following 6 parameters (below i denotes the index of a group): (1) θ(N)i ∈ N: the number of pedestrians in the group; (2) θ(T )i ∈ R+: the time when the group starts to move; (3) θ(S)i ∈ R2: the starting location of the group (e.g., stations); (4) θ(G)i ∈ R2: the goal location of the group; (5) θ(P )i ∈ R2: the intermediate location(s) that the pedestrians in the group visit (e.g., stores); and (6) θ\n(R) i ∈ R+: the time duration(s) of the pedestrians’ visit(s)\nat the intermediate location(s).\nIn this experiment, we focused on estimation of the first two parameters θ(N)i , θ (T ) i , and fixed the other parameters. We defined the true model as a mixture of 5 pedestrian groups, and set their parameters as (θ∗(N)1 , . . . , θ ∗(N) 5 ) := (100, 100, 100, 100, 100) and (θ ∗(T ) 1 , . . . , θ ∗(T ) 5 ) := (30, 60, 90, 120, 150). As in Sec. 4.5, we used a redundant model of a mixture of 10 groups for parameter estimation. The goal was to detect the active 5 groups of the true model, without knowing that the truth consists of 5 groups. For simplicity, 5 (unknown) groups among the 10 candidate groups included the parameters of the true model other than θ∗(N)i , θ ∗(T ) i ; see the Supplementary Materials for details.\nWe defined prior distributions as follows. First we assumed the total number 500 of pedestrians to be known. The mixing coefficients of the mixture of 10 groups are given by (φ1, . . . , φ10) = (θ (N) 1 , . . . , θ (N) 10 )/500. Thus, rather than directly putting a prior on (θ(N)1 , . . . , θ (N) 10 ), we defined a prior on the mixing coefficients (φ1, . . . , φ10). Specifically, we used a Dirichlet prior with a small concentration parameter, as in Sec. 4.5, in order to eliminate 5 redundant components:\n(φ1, . . . , φ10) ∼ Dirichlet(α1, ..., α10), θ\n(N) i := φi ∗ 500, (i = 1, . . . , 10)\nwhere α1 = · · · = α10 = 0.01 denote the concentration 5https://github.com/crest-cassia/CrowdWalk\nparameters. For each of θ(T )1 , . . . , θ (T ) 10 , we defined a broad uniform prior θ(T )i ∼ Unif(0, 480).\nFrom the true model, we simulated 4200 time steps of pedestrian flow as observed data. We made 5× 5 = 25 grids in a map of Ginza and computed a histogram of the corresponding 25 bins for each time step. Thus, observed data was made up of 4200 vectors in R25. In the same way, each method generated a total of 4200 vectors, and each iterative method made use of 200 vectors in each iteration, running 21 iterations in total. For SMC-ABC, we set the parameter α ∈ (0, 1) to be 0.2, as in the previous experiment.\nWe ran each method 20 times, and the resulting averages and standard deviations for errors are summarized in Table 4, where “θ(N) error” and “θ(T ) error” denote the errors of the corresponding estimated parameters, as measured in terms of Euclidean distance. These errors were computed in the same way as in Sec. 4.5 (e.g., the estimated parameters were sorted according to the magnitudes of the mixing coefficients). Results show that our method performed the best, confirming its effectiveness. In the Supplementary Materials, we also report the point estimates made using the proposed method, showing that the true parameters were estimated reasonably accurately."
  }, {
    "heading": "5. Summary and future work",
    "text": "We have proposed kernel recursive ABC for point estimation with intractable likelihood and have empirically investigated the effectiveness of this approach. While we have also provided theoretical analysis to a certain extent, there remain important theoretical topics, as discussed in Sec. 3.1, that we wish to reserve for future research."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the anonymous reviewers as well as Itaru Nishioka, Itsuki Noda, Takashi Washio, Shinji Ito, Wittawat Jitkrittum, and Marie Oshima for their helpful comments and support. We also thank Shuhei Mano for providing his code. MK has been supported by the European Research Council (StG Project PANAMA). KF has been supported by JSPS KAKENHI 26280009."
  }],
  "year": 2018,
  "references": [{
    "title": "On the equivalence between herding and conditional gradient algorithms",
    "authors": ["F. Bach", "S. Lacoste-Julien", "G. Obozinski"],
    "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML2012), pages 1359–1366.",
    "year": 2012
  }, {
    "title": "Approximate Bayesian computation in population genetics",
    "authors": ["M.A. Beaumont", "W. Zhang", "D.J. Balding"],
    "venue": "Genetics, 162(4), 2025–2035.",
    "year": 2002
  }, {
    "title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
    "authors": ["E. Brochu", "V.M. Cora", "N. De Freitas"],
    "venue": "arXiv preprint arXiv:1012.2599.",
    "year": 2010
  }, {
    "title": "Super-samples from kernel herding",
    "authors": ["Y. Chen", "M. Welling", "A. Smola"],
    "venue": "Proceedings of the TwentySixth Conference on Uncertainty in Artificial Intelligence, pages 109–116. AUAI Press.",
    "year": 2010
  }, {
    "title": "Parameter estimation for hidden Markov models with intractable likelihoods",
    "authors": ["T.A. Dean", "S.S. Singh", "A. Jasra", "G.W. Peters"],
    "venue": "Scandinavian Journal of Statistics, 41(4), 970–987.",
    "year": 2014
  }, {
    "title": "An adaptive sequential Monte Carlo method for approximate Bayesian computation",
    "authors": ["P. Del Moral", "A. Doucet", "A. Jasra"],
    "venue": "Statistics and Computing, 22(5), 1009– 1020.",
    "year": 2012
  }, {
    "title": "High dimensional numerical integration - The Quasi-Monte Carlo way",
    "authors": ["J. Dick", "F.Y. Kuo", "I.H. Sloan"],
    "venue": "Acta Numerica, 22(133-288).",
    "year": 2013
  }, {
    "title": "Marginal maximum a posteriori estimation using Markov chain Monte Carlo",
    "authors": ["A. Doucet", "S.J. Godsill", "C.P. Robert"],
    "venue": "Statistics and Computing, 12(1), 77–84.",
    "year": 2002
  }, {
    "title": "Kernel measures of conditional dependence",
    "authors": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Schölkopf"],
    "venue": "Advances in neural information processing systems, pages 489–496.",
    "year": 2008
  }, {
    "title": "Kernel Bayes’ rule: Bayesian inference with positive definite kernels",
    "authors": ["K. Fukumizu", "L. Song", "A. Gretton"],
    "venue": "The Journal of Machine Learning Research, 14(1), 3753–3783.",
    "year": 2013
  }, {
    "title": "The estimation of the gradient of a density function, with applications in pattern recognition",
    "authors": ["K. Fukunaga", "L. Hostetler"],
    "venue": "IEEE Transactions on information theory, 21(1), 32–40.",
    "year": 1975
  }, {
    "title": "Indirect inference",
    "authors": ["C. Gourieroux", "A. Monfort", "E. Renault"],
    "venue": "Journal of applied econometrics, 8(S1).",
    "year": 1993
  }, {
    "title": "A kernel method for the twosample-problem",
    "authors": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Schölkopf", "A.J. Smola"],
    "venue": "Advances in neural information processing systems, pages 513–520.",
    "year": 2007
  }, {
    "title": "A kernel two-sample test",
    "authors": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Schölkopf", "A. Smola"],
    "venue": "Journal of Machine Learning Research, 13(Mar), 723–773.",
    "year": 2012
  }, {
    "title": "Conditional mean embeddings as regressors",
    "authors": ["S. Grünewälder", "G. Lever", "L. Baldassarre", "S. Patterson", "A. Gretton", "M. Pontil"],
    "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML2012), pages 1823–1830.",
    "year": 2012
  }, {
    "title": "Bayesian optimization for likelihood-free inference of simulator-based statistical models",
    "authors": ["M.U. Gutmann", "J. Corander"],
    "venue": "Journal of Machine Learning Research, 17(125), 1–47.",
    "year": 2016
  }, {
    "title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings",
    "authors": ["M. Kanagawa", "B.K. Sriperumbudur", "K. Fukumizu"],
    "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Sys-",
    "year": 2016
  }, {
    "title": "Filtering with state-observation examples via kernel Monte Carlo filter",
    "authors": ["M. Kanagawa", "Y. Nishiyama", "A. Gretton", "K. Fukumizu"],
    "venue": "Neural Computation, 28(2), 382–444.",
    "year": 2016
  }, {
    "title": "Sequential kernel herding: Frank-Wolfe optimization for particle filtering",
    "authors": ["S. Lacoste-Julien", "F. Lindsten", "F. Bach"],
    "venue": "G. Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, vol-",
    "year": 2015
  }, {
    "title": "Estimability and likelihood inference for generalized linear mixed models using data cloning",
    "authors": ["S.R. Lele", "K. Nadeem", "B. Schmuland"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2010
  }, {
    "title": "Markov chain Monte Carlo without likelihood",
    "authors": ["P. Marjoram", "J. Molitor", "V. Plagnol", "S. Tavaré"],
    "venue": "Proceedings of the National Academy of Sciences, 100(26), 15324–15328.",
    "year": 2003
  }, {
    "title": "A method of simulated moments for estimation of discrete response models without numerical integration",
    "authors": ["D. McFadden"],
    "venue": "Econometrica: Journal of the Econometric Society, pages 995–1026.",
    "year": 1989
  }, {
    "title": "GPS-ABC: Gaussian process surrogate approximate Bayesian computation",
    "authors": ["E. Meeds", "M. Welling"],
    "venue": "Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pages 593–602. AUAI Press.",
    "year": 2014
  }, {
    "title": "DRABC: approximate Bayesian computation with kernelbased distribution regression",
    "authors": ["J. Mitrovic", "D. Sejdinovic", "Teh", "Y.-W."],
    "venue": "International Conference on Machine Learning, pages 1482–1491.",
    "year": 2016
  }, {
    "title": "Kernel mean embedding of distributions : A review and beyond",
    "authors": ["K. Muandet", "K. Fukumizu", "B.K. Sriperumbudur", "B. Schölkopf"],
    "venue": "Foundations and Trends in Machine Learning, 10(1–2), 1–141.",
    "year": 2017
  }, {
    "title": "Kernel approximate Bayesian computation in population genetic inferences",
    "authors": ["S. Nakagome", "K. Fukumizu", "S. Mano"],
    "venue": "Statistical applications in genetics and molecular biology, 12(6), 667–678.",
    "year": 2013
  }, {
    "title": "Multivariate elliptically contoured stable distributions: theory and estimation",
    "authors": ["J.P. Nolan"],
    "venue": "Computational Statistics, 28(5), 2067–2089.",
    "year": 2013
  }, {
    "title": "K2ABC: Approximate Bayesian computation with kernel embeddings",
    "authors": ["M. Park", "W. Jitkrittum", "D. Sejdinovic"],
    "venue": "A. Gretton and C. C. Robert, editors, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings",
    "year": 2016
  }, {
    "title": "Approximate maximum likelihood estimation using data-cloning abc",
    "authors": ["U. Picchini", "R. Anderson"],
    "venue": "Computational Statistics & Data Analysis, 105, 166–183.",
    "year": 2017
  }, {
    "title": "Population growth of human y chromosomes: a study of y chromosome microsatellites",
    "authors": ["J.K. Pritchard", "M.T. Seielstad", "A. Perez-Lezaun", "M.W. Feldman"],
    "venue": "Molecular biology and evolution, 16(12), 1791–1798.",
    "year": 1999
  }, {
    "title": "A simple approach to maximum intractable likelihood estimation",
    "authors": ["F.J. Rubio", "Johansen", "A. M"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2013
  }, {
    "title": "A Hilbert space embedding for distributions",
    "authors": ["A. Smola", "A. Gretton", "L. Song", "B. Schölkopf"],
    "venue": "Proceedings of the International Conference on Algorithmic Learning Theory, volume 4754, pages 13–31. Springer.",
    "year": 2007
  }, {
    "title": "Practical Bayesian optimization of machine learning algorithms",
    "authors": ["J. Snoek", "H. Larochelle", "R.P. Adams"],
    "venue": "Advances in neural information processing systems, pages 2951–2959.",
    "year": 2012
  }, {
    "title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems",
    "authors": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"],
    "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 961–968. ACM.",
    "year": 2009
  }, {
    "title": "Kernel embeddings of conditional distributions: A unified kernel",
    "authors": ["L. Song", "K. Fukumizu", "A. Gretton"],
    "year": 2013
  }, {
    "title": "Hilbert space embeddings and metrics on probability measures",
    "authors": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Schölkopf", "G.R. Lanckriet"],
    "venue": "Jounal of Machine Learning Research, 11, 1517–1561.",
    "year": 2010
  }, {
    "title": "Energy statistics: A class of statistics based on distances",
    "authors": ["G.J. Székely", "M.L. Rizzo"],
    "venue": "Journal of statistical planning and inference, 143(8), 1249–1272.",
    "year": 2013
  }, {
    "title": "Nonparametric quantile estimation",
    "authors": ["I. Takeuchi", "Q.V. Le", "T.D. Sears", "A.J. Smola"],
    "venue": "Journal of Machine Learning Research, 7(Jul), 1231–1264.",
    "year": 2006
  }, {
    "title": "Inferring coalescence times from DNA sequence data",
    "authors": ["S. Tavaré", "D.J. Balding", "R.C. Griffiths", "P. Donnelly"],
    "venue": "Genetics, 145(2), 505–518.",
    "year": 1997
  }, {
    "title": "Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems",
    "authors": ["T. Toni", "D. Welch", "N. Strelkowa", "A. Ipsen", "M.P. Stumpf"],
    "venue": "Journal of the Royal Society Interface, 6(31), 187–202.",
    "year": 2009
  }, {
    "title": "Statistical inference for noisy nonlinear ecological dynamic systems",
    "authors": ["S.N. Wood"],
    "venue": "Nature, 466(7310), 1120–4.",
    "year": 2010
  }, {
    "title": "Assistance of evacuation planning with high-speed network modelbased pedestrian simulator",
    "authors": ["T. Yamashita", "S. Soeda", "I. Noda"],
    "venue": "Proceedings of Fifth International Conference on Pedestrian and Evacuation Dynamics (PED 2010), page 58. PED 2010.",
    "year": 2010
  }, {
    "title": "Comparing two Bayes methods based on the free energy functions in Bernoulli mixtures",
    "authors": ["K. Yamazaki", "D. Kaji"],
    "venue": "Neural Networks, 44, 36–43.",
    "year": 2013
  }, {
    "title": "Parameter estimation in hidden Markov models with intractable likelihoods using sequential Monte Carlo",
    "authors": ["S. Yıldırım", "S.S. Singh", "T. Dean", "A. Jasra"],
    "venue": "Journal of Computational and Graphical Statistics, 24(3), 846–865.",
    "year": 2015
  }],
  "id": "SP:fa19a0734665d6d9f3d51e85db05e4d0031fdc41",
  "authors": [{
    "name": "Takafumi Kajihara",
    "affiliations": []
  }, {
    "name": "Motonobu Kanagawa",
    "affiliations": []
  }, {
    "name": "Keisuke Yamazaki",
    "affiliations": []
  }, {
    "name": "Kenji Fukumizu",
    "affiliations": []
  }],
  "abstractText": "We propose a novel approach to parameter estimation for simulator-based statistical models with intractable likelihood. Our proposed method involves recursive application of kernel ABC and kernel herding to the same observed data. We provide a theoretical explanation regarding why the approach works, showing (for the population setting) that, under a certain assumption, point estimates obtained with this method converge to the true parameter, as recursion proceeds. We have conducted a variety of numerical experiments, including parameter estimation for a realworld pedestrian flow simulator, and show that in most cases our method outperforms existing approaches.",
  "title": "Kernel Recursive ABC: Point Estimation with Intractable Likelihood"
}