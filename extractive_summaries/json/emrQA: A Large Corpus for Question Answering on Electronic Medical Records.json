{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357–2368 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2357"
  }, {
    "heading": "1 Introduction",
    "text": "Automatic question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets (Voorhees et al., 1999; Ferrucci et al., 2010; Rajpurkar et al., 2016; Joshi et al., 2017). However, in the clinical domain this problem remains relatively unexplored. Physicians frequently seek answers to questions from unstructured electronic medical records (EMRs) to support clinical decision-making (Demner-Fushman et al., 2009). But in a significant majority of cases, they are unable to unearth the information they want from EMRs (Tang et al., 1994). Moreover to date, there is no general system for answering natural language questions asked by physicians on a patient’s EMR (Figure 1) due to lack of largescale datasets (Raghavan and Patwardhan, 2016).\nEMRs are a longitudinal record of a patient’s health information in the form of unstructured clinical notes (progress notes, discharge summaries etc.) and structured vocabularies. Physi-\n∗This work was conducted during an internship at IBM §https://www.i2b2.org/NLP/DataSets/\ncians wish to answer questions about medical entities and relations from the EMR, requiring a deeper understanding of clinical notes. While this may be likened to machine comprehension, the longitudinal nature of clinical discourse, little to no redundancy in facts, abundant use of domain-specific terminology, temporal narratives with multiple related diseases, symptoms, medications that go back and forth in time, and misspellings, make it complex and difficult to apply existing NLP tools (Demner-Fushman et al., 2009; Raghavan and Patwardhan, 2016). Moreover, answers may be implicit or explicit and may require domain-knowledge and reasoning across clinical notes. Thus, building a credible QA system for patient-specific EMR QA requires largescale question and answer annotations that sufficiently capture the challenging nature of clinical narratives in the EMR. However, serious privacy concerns about sharing personal health information (Devereaux, 2013; Krumholz et al., 2016), and the tedious nature of assimilating answer annotations from across longitudinal clinical notes, makes this task impractical and possibly erroneous to do manually (Lee et al., 2017).\nIn this work, we address the lack of any publicly available EMR QA corpus by creating a large-scale dataset, emrQA, using a novel gener-\nation framework that allows for minimal expert involvement and re-purposes existing annotations available for other clinical NLP tasks (i2b2 challenge datasets (Guo et al., 2006)). The annotations serve as a proxy-expert in generating questions, answers, and logical forms. Logical forms provide a human-comprehensible symbolic representation, linking questions to answers, and help build interpretable models, critical to the medical domain (Davis et al., 1977; Vellido et al., 2012). We analyze the emrQA dataset in terms of question complexity, relations, and the reasoning required to answer questions, and provide neural and heuristic baselines for learning to predict questionlogical forms and question-answers.\nThe main contributions of this work are as follows:\n• A novel framework for systematic generation of domain-specific large-scale QA datasets that can be used in any domain where manual annotations are challenging to obtain but limited annotations may be available for other NLP tasks.\n• The first accessible patient-specific EMR QA dataset, emrQA∗, consisting of 400,000 question-answer pairs and 1 million questionlogical form pairs. The logical forms will allow users to train and benchmark interpretable models that justify answers with corresponding logical forms.\n• Two new reasoning challenges, namely arithmetic and temporal reasoning, that are absent in open-domain datasets like SQuAD (Rajpurkar et al., 2016). ∗https://github.com/panushri25/emrQA, scripts to generate emrQA from i2b2 data. i2b2 data is accessible by everyone subject to a license agreement."
  }, {
    "heading": "2 Related Work",
    "text": "Question Answering (QA) datasets are classified into two main categories: (1) machine comprehension (MC) using unstructured documents, and (2) QA using Knowledge Bases (KBs).\nMC systems aim to answer any question that could be posed against a reference text. Recent advances in crowd-sourcing and search engines have resulted in an explosion of large-scale (100K) MC datasets for factoid QA, having ample redundant evidence in text (Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Dhingra et al., 2017). On the other hand, complex domainspecific MC datasets such as MCTest (Richardson et al., 2013), biological process modeling (Berant et al., 2014), BioASQ (Tsatsaronis et al., 2015), InsuranceQA (Feng et al., 2015), etc have been limited in scale (500-10K) because of the complexity of the task or the need for expert annotations that cannot be crowd-sourced or gathered from the web. In contrast to the open-domain, EMR data cannot be released publicly due to privacy concerns (Šuster et al., 2017). Also, annotating unstructured EMRs requires a medical expert who can understand and interpret clinical text. Thus, very few datasets like i2b2, MIMIC (Johnson et al., 2016) (developed over several years in collaboration with large medical groups and hospitals), share small-scale annotated clinical notes. In this work, we take advantage of the limited expertly annotated resources to generate emrQA.\nKB-based QA datasets, used for semantic parsing, are traditionally limited by the requirement of annotated question and logical form (LF) pairs for supervision where the LF are used to retrieve answers from a schema (Cai and Yates, 2013; Lopez et al., 2013; Bordes et al., 2015). Roberts and Demner-Fushman (2016) generated a corpus by\nmanually annotating LFs on 468 EMR questions (not released publicly), thus limiting its ability to create large scale datasets. In contrast, we only collect LFs for question templates from a domainexpert - the rest of our corpus is automatically generated.\nRecent advances in QA combine logic-based and neural MC approaches to build hybrid models (Usbeck et al., 2015; Feng et al., 2016; Palangi et al., 2018). These models are driven to combine the accuracy of neural approaches (Hermann et al., 2015) and the interpretability of the symbolic representations in logic-based methods (Gao et al.; Chabierski et al., 2017). Building interpretable yet accurate models is extremely important in the medical domain (Shickel et al., 2017). We generate large-scale ground truth annotations (questions, logical forms, and answers) that can provide supervision to learn such hybrid models. Our approach to generating emrQA is in the same spirit as Su et al. (2016), who generate graph queries (logical forms) from a structured KB and use them to collect answers. In contrast, our framework can be applied to generate QA dataset in any domain with minimal expert input using annotations from other NLP tasks."
  }, {
    "heading": "3 QA Dataset Generation Framework",
    "text": "Our general framework for generating a largescale QA corpus given certain resources consists of three steps: (1) collecting questions to capture domain-specific user needs, followed by normalizing the collected questions to templates by replacing entities (that may be related via binary or composite relations) in the question with placeholders. The entity types replaced in the question are grounded in an ontology like WordNet (Miller, 1995), UMLS (Bodenreider, 2004), or a usergenerated schema that defines and relates different entity types. (2) We associate question templates with expert-annotated logical form templates; logical forms are symbolic representations using relations from the ontology/schema to express the relations in the question, and associate the ques-\ntion entity type with an answer entity type. (3) We then proceed to the important step of re-purposing existing NLP annotations to populate questionlogical form templates and generate answers. QA is a complex task that requires addressing several fundamental NLP problems before accurately answering a question. Hence, obtaining expert manual annotations in complex domains is infeasible as it is tedious to expert-annotate answers that may be found across long document collections (e.g., longitudinal EMR) (Lee et al., 2017). Thus, we reverse engineer the process where we reuse expert annotations available in NLP tasks such as entity recognition, coreference, and relation learning, based on the information captured in the logical forms to populate entity placeholders in templates and generate answers. Reverse engineering serves as a proxy expert ensuring that the generated QA annotations are credible. The only manual effort is in annotating logical forms, thus significantly reducing expert labor. Moreover, in domain specific instances such as EMRs, manually annotated logical forms allow the experts to express information essential for natural language understanding such as domain knowledge, temporal relations, and negation (Gao et al.; Chabierski et al., 2017). This knowledge, once captured, can be used to generate QA pairs on new documents, making the framework scalable."
  }, {
    "heading": "4 Generating the emrQA Dataset",
    "text": "We apply the proposed framework to generate the emrQA corpus consisting of questions posed by physicians against longitudinal EMRs of a patient, using annotations provided by i2b2 (Figure 2)."
  }, {
    "heading": "4.1 Question Collection and Normalization",
    "text": "We collect questions for EMR QA by, 1) polling physicians at the Veterans Administration for what they frequently want to know from the EMR (976 questions), 2) using an existing source of 5,696 questions generated by a team of medical experts from 71 patient records (Raghavan et al., 2017) and 3) using 15 prototypical questions from an ob-\nservational study done by physicians (Tang et al., 1994). To obtain templates, the questions were automatically normalized by identifying medical entities (using MetaMap (Aronson, 2001)) in questions and replacing them with generic placeholders. The resulting ∼2K noisy templates were expert reviewed and corrected (to account for any entity recognition errors by MetaMap). We align our entity types to those defined in the i2b2 concept extraction tasks (Uzuner et al., 2010a, 2011) - problem, test, treatment, mode and medication. E.g., The question What is the dosage of insulin? from the collection gets converted to the template What is the dosage of |medication|? as shown in Fig.2. This process resulted in 680 question templates. We do not correct for the usage/spelling errors in these templates, such as usage of \"pt\" for \"patient\", or make the templates gender neutral in order to provide a true representation of physicians’ questions. Further, analyzing these templates shows that physicians most frequently ask about test results (11%), medications for problem (9%), and problem existence (8%). The long tail following this includes questions about medication dosage, response to treatment, medication duration, prescription date, etiology, etc. Temporal constraints were frequently imposed on questions related to tests, problem diagnosis and medication start/stop."
  }, {
    "heading": "4.2 Associating Templates w/ Logical Forms",
    "text": "The 680 question templates were annotated by a physician with their corresponding logical form (LF) templates, which resulted in 94 unique LF templates. More than one question template that map to the same LF are considered paraphrases of each other and correspond to a particular question type (Table 2). Logical forms are defined based\non an ontology schema designed by medical experts (Figure 3). This schema captures entities in unstructured clinical notes through medical events and their attributes, interconnected through relations. We align the entity and relation types of i2b2 to this schema.\nA formal representation of the LF grammar using this schema (Figure 3) is as follows. Medical events are denoted as MEi (e.g LabEvent, ConditionEvent) and relations are denoted as REi (e.g conducted/reveals). Now, ME[a1, .., aj , .., oper(an)] is a medical event where aj represents the attribute of the event (such as result in LabEvent). An event may optionally include constraints on attributes captured by an operator (oper() ∈ sort, range, check for null values, compare). These operators sometimes require values from external medical KB (indicated by ref, e.g. lab.ref low/lab.ref high to indicate range of reference standards considered healthy in lab results) indicating the need for medical knowledge to answer the question. Using these constructs, a LF can be defined using the following rules, LF →MEi |M1 relation M2 M1 →MEi, M2 →MEj M1 →M1 relation M2, M2 →M1 relation M2 relation→ OR | AND | REi\nAdvantages of our LF representation include the ability to represent composite relations, define attributes for medical events and constrain the attributes to precisely capture the information need in the question. While these can be achieved using different methods that combine lambda calculus and first order logic (Roberts and Demner-Fushman, 2016), our representation is more human comprehensible. This allows a physician to consider an ontology like Figure 3 and easily define a logical form. Some example question templates with their LF annotations are described in Table 3 using the above notation. The LF representation of the question in Figure 2 is MedicationEvent(|medication|) [dosage=x]. The entities seen in LF are the entities posed in the question and entity marked x indicates the answer entity type."
  }, {
    "heading": "4.3 Template Filling and Answer Extraction",
    "text": "The next step in the process is to populate the question and logical form (QL) templates with existing annotations in the i2b2 clinical datasets and extract answer evidence for the questions.\nThe i2b2 datasets are expert annotated with fine-grained annotations (Guo et al., 2006) that were developed for various shared NLP challenge tasks, including (1) smoking status classification (Uzuner et al., 2008), (2) diagnosis of obesity and its co-morbidities (Uzuner, 2009), extraction of (3) medication concepts (Uzuner et al., 2010a), (4) relations, concepts, assertions (Uzuner et al., 2010b, 2011) (5) co-reference resolution (Uzuner et al., 2012) and (6) heart disease risk factor identification (Stubbs and Uzuner, 2015). In Figure 2, this would correspond to leveraging annotations from medications challenge between medications and their dosages, such as medication=Nitroglycerin, dosage=40mg, to populate |medication| and generate several instances of the question “What is the dosage of |medication|?\" and its corresponding logical form MedicationEvent(|medication|)[dosage=x]. The answer would be derived from the value of the dosage entity in the dataset.\nPreprocessing: The i2b2 entities are preprocessed before using them with our templates to ensure syntactic correctness of the generated questions. The pre-processing steps are designed based on the i2b2 annotations syntax guidelines (Guo et al., 2006). To estimate grammatical correctness, we randomly sampled 500 generated questions and found that <5% had errors. These errors include, among others, incorrect usage of article with the entity and incorrect entity phrasing.\nAnswer Extraction: The final step in the process is generating answer evidence corresponding to each question. The answers in emrQA are defined differently; instead of a single word or phrase we provide the entire i2b2 annotation line from the clinical note as the answer. This is because the context in which the answer entity or phrase is mentioned is extremely important in clinical decision making (Demner-Fushman et al., 2009).\nHence, we call them answer evidence instead of just answers. For example, consider the question Is the patient’s hypertension controlled?. The answer to this question is not a simple yes/no since the status of the patient’s hypertension can change through the course of treatment. The answer evidence to this question in emrQA are multiple lines across the longitudinal notes that reflect this potentially changing status of the patients condition, e.g. Hypertension-borderline today. Additionally, for questions seeking specific answers we also provide the corresponding answer entities.\nThe overall process for answer evidence generation was vetted by a physician. Here is a brief overview of how the different i2b2 datasets were used in generating answers. The relations challenge datasets have various event-relation annotations across single/multiple lines in a clinical note. We used a combination of one or more of these, to generate answers for a question; in doing so we used the annotations provided by the i2b2 co-reference datasets. Similarly, the medications challenge dataset has various event-attribute annotations but since this dataset is not provided with co-reference annotations, it is currently not possible to combine all valid answers. The heart disease challenge dataset has longitudinal notes (∼5 per patient) with record dates. The events in this dataset are also provided with time annotations and are rich in quantitative entities. This dataset was primarily used to answer questions that require temporal and arithmetic reasoning on events. The patient records in the smoking and obesity challenge datasets are categorized into classes with no entity annotations. Thus, for questions generated on these datasets, the entire document acts as evidence and the annotated class information (7 classes) needs to be predicted as the answer.\nThe total questions, LFs and answers gener-\nated using this framework are summarized in Table 1. Consider the question How much does the patient smoke? for which we do not have i2b2annotations to provide an answer. In cases where the answer entity is empty, we only generate the question and LF, resulting in more question types being used for QL than QA pairs: only 53% of question types have answers."
  }, {
    "heading": "5 emrQA Dataset Analysis",
    "text": "We analyze the complexity of emrQA by considering the LFs for question characteristics, variations in paraphrases, and the type of reasoning required for answering questions (Table 2, 3, 4)."
  }, {
    "heading": "5.1 Question/Logical Form Characteristics",
    "text": "A quantitative and qualitative analysis of emrQA question templates is shown in Table 3, where logical forms help formalize their characteristics (Su et al., 2016). Questions may request specific finegrained information (attribute values like dosage) or may express a more coarse-grained need (event entities like medications etc), or a combination of both. 25% of questions require complex operators (e.g compare(>)) and 12% of questions express the need for external medical knowledge (e.g. lab.refhigh). The questions in emrQA are highly compositional, where 47% of question templates have at least one event relation."
  }, {
    "heading": "5.2 Paraphrase Complexity Analysis",
    "text": "Questions templates that map to the same LF are considered paraphrases (e.g, Table 2) and correspond to the same question type. In emrQA, an average of 7 paraphrase templates exist per question type. This is representative of FAQ types that are perhaps more important to the physician. Good paraphrases are lexically dissimilar to each other (Chen and Dolan, 2011). In order to understand the lexical variation within our paraphrases, we randomly select a question from the list of paraphrases as a reference and evaluate the others with respect to the reference, and report the average BLEU (0.74 ± 0.06) and Jaccard Score (0.72 ± 0.19). The low BLEU and Jaccard score with large standard deviation indicates the lexical diversity captured by emrQA’s paraphrases (Papineni et al., 2002; Niwattanakul et al., 2013)."
  }, {
    "heading": "5.3 Answer Evidence Analysis",
    "text": "33% of the questions in emrQA have more than one answer evidence, with the number ranging\nfrom 2 to 61. E.g., the question Medications Record? has all medications in the patient’s longitudinal record as answer evidence. In order to analyze the reasoning required to answer emrQA questions, we sampled 35 clinical notes from the corpus and analyzed 3 random questions per note by manually labeling them with the categories described in Table 4. Categories are not mutually exclusive: a single example can fall into multiple categories. We compare and contrast this analysis with SQuAD (Rajpurkar et al., 2016), a popular MC dataset generated through crowdsourcing, to show that the framework is capable of generating a corpus as representative and even more complex. Compared to SQuAD, emrQA offers two new reasoning categories, temporal and arithmetic which make up 31% of the dataset. Additionally, over two times as many questions in emrQA require reasoning over multiple sentences. Long and noisy documents make the question answering task more difficult (Joshi et al., 2017). EMRs are inherently noisy and hence 29% have incomplete context and the document length is 27 times more than SQuAD which offers new challenges to existing QA models. Owing to the domain specific nature of the task, 39% of the examples required some form of medical/world knowledge.\nAs discussed in Section 4.3, 12% of the questions in emrQA corpus require a class category from i2b2 smoking and obesity datasets to be predicted. We also found 6% of the questions had other possible answers that were not included by emrQA, this is because of the lack of co-reference annotations for the medications challenge."
  }, {
    "heading": "6 Baseline Methods",
    "text": "We implement baseline models using neural and heuristic methods for question to logical form (QL) and question to answer (Q-A) mapping."
  }, {
    "heading": "6.1 Q-L Mapping",
    "text": "Heuristic Models: We use a template-matching approach where we first split the data into train/test sets, and then normalize questions in the test set into templates by replacing entities with placeholders. The templates are then scored against the ground truth templates of the questions in the train set, to find the best match. The placeholders in the LF template corresponding to the best matched question template is then filled with the normalized entities to obtain the predicted LF. To normalize the test questions we use CLiNER\n(Boag et al., 2015) for emrQA and Jia and Liang (2016)’s work for ATIS and GeoQuery. Scoring and matching is done using two heuristics: (1) HM-1, which computes an identical match, and (2) HM-2, which generates a GloVe vector (Arora et al., 2016) representation of the templates using sentence2vec and then computes pairwise cosine similarity.\nNeural Model: We train a sequence-tosequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al., 2014; Luong et al., 2017) as our neural baseline (2 layers, each with 64 hidden units). The same setting when used with Geoquery and ATIS gives poor results because the parameters are not appropriate for the nature of that dataset. Hence, for comparison with GeoQuery and ATIS, we use the results of seq2seq model with a single 200 hidden units layer (Jia and Liang, 2016). At test time we automatically balance missing right parentheses.\n†results from Jia and Liang (2016)"
  }, {
    "heading": "6.1.1 Experimental Setup",
    "text": "We randomly partition the QL pairs in the dataset in train(80%) and test(20%) sets in two ways. (1) In emrQL-1, we first split the paraphrase templates corresponding to a single LF template into train and test, and then generate the instances of QL pairs. (2) In emrQL-2, we first generate the instances of QL pairs from the templates and then distribute them into train and test sets. As a result, emrQL-1 has more lexical variation between train and test distribution compared to emrQL-2, resulting in increased paraphrase complexity. We use accuracy i.e, the total number of logical forms predicted correctly as a metric to evaluate our model."
  }, {
    "heading": "6.1.2 Results",
    "text": "The performance of the proposed models is summarized in Table 5. emrQL results are not directly comparable with GeoQuery and ATIS because of the differences in the lexicon and tools available for the domains. However, it helps us establish that QL learning in emrQA is non-trivial and supports significant future work.\nError analysis of heuristic models on emrQL-1 and emrQL-2 showed that 70% of the errors occurred because of incorrect question normalization. In fact, 30% of these questions had not been normalized at all. This shows that the entities\nadded to the templates are complex and diverse and make the inverse process of template generation non trivial. This makes a challenging QL corpus that cannot trivially be solved by template matching based approaches.\nErrors made by the neural model on both emrQL-1 and emrQL-2 are due to long LFs (20%) and incorrectly identified entities (10%), which are harder for the attention-based model (Jia and Liang, 2016). The increased paraphrase complexity in emrQL-1 compared to emrQL-2 resulted in 20% more structural errors in emrQL-1, where the predicted event/grammar structure deviates significantly from the ground truth. This shows that the model is not adequately capturing the semantics in the questions to generalize to new paraphrases. Therefore, emrQL-1 can be used to benchmark QL models robust to paraphrasing."
  }, {
    "heading": "6.2 Q-A Mapping",
    "text": "Question-answering on emrQA consists of two different tasks, (1) extraction of answer line from the clinical note (machine comprehension (MC)) and (2) prediction of answer class based on the entire clinical note. We provide baseline models to illustrate the complexity in doing both these tasks.\nMachine Comprehension: To do extractive QA on EMRs, we use DrQA’s (Chen et al., 2017) document reader which is a multi-layer RNN based MC model. We use their best performing settings trained for SQuAD data using Glove vectors (300 dim-840B).\nClass Prediction: We build a multi-class logistic regression model for predicting a class as an answer based on the patient’s clinical note. Features input to the classifier are TF-IDF vectors of the question and the clinical notes taken from i2b2 smoking and obesity datasets."
  }, {
    "heading": "6.2.1 Experimental setup",
    "text": "We consider a 80-20 split of the data for train-test. In order to evaluate worst-case performance, we train on question-evidence pairs in a clinical note obtained by using only one random paraphrase for a question instead of all the paraphrases. We use a slightly modified‡ version of the two popularly reported metrics in MC for evaluation since our evidence span is longer: Exact Match (EM) and F1. Wherever the answer entity in an evidence is explicitly known, EM checks if the answer entity is\n‡using the original definitions, the evaluated values were far less than those obtained in Table 7\npresent within the evidence, otherwise it checks if the predicted evidence span lies within ±20 characters of the ground truth evidence. For F1 we construct a bag of tokens for each evidence string and measure the F1 score of the overlap between the two bags of tokens. Since there may be multiple evidence for a given question, we consider only the top 10 predictions and report an average of EM and F1 over ground truth number of answers. In the class prediction setting, we report the subset accuracy."
  }, {
    "heading": "6.2.2 Results",
    "text": "The performance of the proposed models is summarized in Table 7. DrQA is one of the best performing models on SQuAD with an F1 of 78.8 and EM of 69.5. The relatively low performance of the models on emrQA (60.6 F1 and 59.2 EM) shows that QA on EMRs is a complex task and offers new challenges to existing QA models.\nTo understand model performance, we macroaverage the EM across all the questions corresponding to a LF template. We observe that LFs representing temporal and arithmetic§ needs had < 16% EM. LFs expressing the need for medical KB§ performed poorly since we used general Glove embeddings. An analysis of LFs which had approximately equal number of QA pair representation in the test set revealed an interesting relation between the model performance and LF complexity, as summarized in Table 6. The trend shows that performance is worse on multiple relation questions as compared to single relation and attribute questions, showing that the LFs sufficiently capture the complexity of the questions and give us an ability to do a qualitative model analysis.\nError analysis on a random sample of 50 questions containing at least one answer entity in an evidence showed that: (1) 38% of the examples required multiple sentence reasoning of which 16% were due to a missing evidence in a multiple evidence question, (2) 14% were due to syntactic variation, (3) 10% required medical reasoning and (4) in 14%, DrQA predicted an incomplete evidence span missing the answer entity in it.\n§maximum representation of these templates comes from the i2b2 heart disease risk dataset"
  }, {
    "heading": "7 Discussion",
    "text": "In this section, we describe how our generation framework may also be applied to generate opendomain QA datasets given the availability of other NLP resources. We also discuss possible extensions of the framework to increase the complexity of the generated datasets.\nOpen domain QA dataset generation: Consider the popularly used SQuAD (Rajpurkar et al., 2016) reading comprehension dataset generated by crowdworkers, where the answer to every question is a segment of text from the corresponding passage in the Wikipedia article. This dataset can easily be generated or extended using our proposed framework with existing NLP annotations on Wikipedia (Auer et al., 2007; Nothman et al., 2008; Ghaddar and Langlais, 2017).\nFor instance, consider DBPedia (Auer et al., 2007), an existing dataset of entities and their relations extracted from Wikipedia. It also has its own ontology which can serve as the semantic frames schema to define logical forms. Using these resources, our reverse engineering technique for QA dataset generation can be applied as follows. (1) Question templates can be defined for each entity type and relation in DBPedia. For example¶, consider the relation [place, country] field in DBpedia. For this we can define a question template In what country is |place| located?. (2) Every such question template can be annotated with a logical form template using existing DBPedia ontology. (3) By considering the entity values of DBPedia fields such as [place=Normandy, dbo:country=France], we can automatically generate the question In what country is Normandy located? and its corresponding logical form from the templates. The text span of country=France from the Wikipedia passage is then used as the answer (Daiber et al., 2013). Currently, this QA pair instance is a part of the SQuAD dev set. Using our framework we can generate many more instances like this example from different Wikipedia passages - without crowdsourcing efforts. ¶example reference: http://dbpedia.org/page/Normandy\nExtensions to the framework: The complexity of the generated dataset can be further extended as follows. (1) We can use a coreferred or a lexical variant of the original entity in the question-logical form generation. This can allow for increased lexical variation between the question and answer line entities in the passage. (2) It is possible to combine two or more question templates to make compositional questions with the answers to these questions similarly combined. This can also result in more multiple sentence reasoning questions. (3) We can generate questions with entities not related to the context in the passage. This can increase empty answer questions in the dataset, resulting in increased negative training examples."
  }, {
    "heading": "8 Conclusions and Future Work",
    "text": "We propose a novel framework that can generate a large-scale QA dataset using existing resources and minimal expert input. This has the potential to make a huge impact in domains like medicine, where obtaining manual QA annotations is tedious and infeasible. We apply this framework to generate a large scale EMR QA corpus (emrQA), consisting of 400,000 question-answers pairs and 1 million question-logical forms, and analyze the complexity of the dataset to show its non-trivial nature. We show that the logical forms provide a symbolic representation that is very useful for corpus generation and for model analysis. The logical forms also provide an opportunity to build interpretable systems by perhaps jointly (or latently) learning the logical form and answer for a question. In future, this framework may be applied to also re-purpose and integrate other NLP datasets such as MIMIC and generate a more diverse and representative EMR QA corpus (Johnson et al., 2016)."
  }, {
    "heading": "Acknowledgments",
    "text": "This project is partially funded by Sloan Research Fellowship, PhRMA Foundation Award in Informatics, and NSF Career Award (1652815). The authors would like to thank Siddharth Patwardhan for his valuable feedback in formatting the paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Effective mapping of biomedical text to the umls metathesaurus: the metamap program",
    "authors": ["Alan R Aronson."],
    "venue": "Proceedings of the AMIA Symposium, page 17. American Medical Informatics Association.",
    "year": 2001
  }, {
    "title": "A simple but tough-to-beat baseline for sentence embeddings",
    "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"],
    "year": 2016
  }, {
    "title": "Dbpedia: A nucleus for a web of open data",
    "authors": ["Sören Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives."],
    "venue": "The semantic web, pages 722–735. Springer.",
    "year": 2007
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Modeling biological processes for reading comprehension",
    "authors": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D Manning."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Cliner: A lightweight tool for clinical named entity recognition",
    "authors": ["William Boag", "Kevin Wacome", "Tristan Naumann", "Anna Rumshisky."],
    "venue": "AMIA Joint Summits on Clinical Research Informatics (poster).",
    "year": 2015
  }, {
    "title": "The unified medical language system (umls): integrating biomedical terminology",
    "authors": ["Olivier Bodenreider."],
    "venue": "Nucleic acids research, 32(suppl_1):D267– D270.",
    "year": 2004
  }, {
    "title": "Large-scale simple question answering with memory networks",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1506.02075.",
    "year": 2015
  }, {
    "title": "Large-scale semantic parsing via schema matching and lexicon extension",
    "authors": ["Qingqing Cai", "Alexander Yates."],
    "venue": "ACL (1), pages 423–433.",
    "year": 2013
  }, {
    "title": "Logic-based approach to machine comprehension of text",
    "authors": ["Piotr Chabierski", "Alessandra Russo", "Mark Law"],
    "year": 2017
  }, {
    "title": "Reading wikipedia to answer open-domain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "arXiv preprint arXiv:1704.00051.",
    "year": 2017
  }, {
    "title": "Collecting highly parallel data for paraphrase evaluation",
    "authors": ["David L Chen", "William B Dolan."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190–200.",
    "year": 2011
  }, {
    "title": "Improving efficiency and accuracy in multilingual entity extraction",
    "authors": ["Joachim Daiber", "Max Jakob", "Chris Hokamp", "Pablo N. Mendes."],
    "venue": "Proceedings of the 9th International Conference on Semantic Systems (I-Semantics).",
    "year": 2013
  }, {
    "title": "Production rules as a representation for a knowledge-based consultation program",
    "authors": ["Randall Davis", "Bruce Buchanan", "Edward Shortliffe."],
    "venue": "Artificial intelligence, 8(1):15–45.",
    "year": 1977
  }, {
    "title": "What can natural language processing do for clinical decision support",
    "authors": ["Dina Demner-Fushman", "Wendy Webber Chapman", "Clement J. McDonald"],
    "venue": "Journal of Biomedical Informatics,",
    "year": 2009
  }, {
    "title": "The use of patient records (ehr) for research",
    "authors": ["Mary Devereaux"],
    "year": 2013
  }, {
    "title": "Quasar: Datasets for question answering by search and reading",
    "authors": ["Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen."],
    "venue": "arXiv preprint arXiv:1707.03904.",
    "year": 2017
  }, {
    "title": "Applying deep learning to answer selection: A study and an open task",
    "authors": ["Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou."],
    "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on,",
    "year": 2015
  }, {
    "title": "Hybrid question answering over knowledge base and free text",
    "authors": ["Yansong Feng", "Songfang Huang", "Dongyan Zhao"],
    "venue": "In Proceedings of COLING 2016,",
    "year": 2016
  }, {
    "title": "Building watson: An overview of the deepqa project",
    "authors": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"],
    "venue": "AI magazine,",
    "year": 2010
  }, {
    "title": "Winer: A wikipedia annotated corpus for named entity recognition",
    "authors": ["Abbas Ghaddar", "Phillippe Langlais."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1,",
    "year": 2017
  }, {
    "title": "Identifying personal health information using support vector machines",
    "authors": ["Y. Guo", "R. Gaizauskas", "I. Roberts", "G. Demetriou", "M. Hepple."],
    "venue": "i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, pages 10–11.",
    "year": 2006
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Advances in Neural Information Processing Systems, pages",
    "year": 2015
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "arXiv preprint arXiv:1606.03622.",
    "year": 2016
  }, {
    "title": "Mimic-iii, a freely accessible critical care database",
    "authors": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark"],
    "year": 2016
  }, {
    "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
    "authors": ["Mandar Joshi", "Eunsol Choi", "Daniel S Weld", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1705.03551.",
    "year": 2017
  }, {
    "title": "Data acquisition, curation, and use for a continuously learning health system",
    "authors": ["Harlan M Krumholz", "Sharon F Terry", "Joanne Waldstreicher."],
    "venue": "Jama, 316(16):1669–1670.",
    "year": 2016
  }, {
    "title": "Big healthcare data analytics: Challenges and applications",
    "authors": ["Chonho Lee", "Zhaojing Luo", "Kee Yuan Ngiam", "Meihui Zhang", "Kaiping Zheng", "Gang Chen", "Beng Chin Ooi", "Wei Luen James Yip."],
    "venue": "Handbook of Large-Scale Distributed Computing in",
    "year": 2017
  }, {
    "title": "Evaluating question answering over linked data",
    "authors": ["Vanessa Lopez", "Christina Unger", "Philipp Cimiano", "Enrico Motta."],
    "venue": "Web Semantics: Science, Services and Agents on the World Wide Web, 21:3–13.",
    "year": 2013
  }, {
    "title": "Neural machine translation (seq2seq) tutorial",
    "authors": ["Minh-Thang Luong", "Eugene Brevdo", "Rui Zhao."],
    "venue": "https://github.com/tensorflow/nmt.",
    "year": 2017
  }, {
    "title": "Wordnet: a lexical database for english",
    "authors": ["George A Miller."],
    "venue": "Communications of the ACM, 38(11):39–",
    "year": 1995
  }, {
    "title": "Using of jaccard coefficient for keywords similarity",
    "authors": ["Suphakit Niwattanakul", "Jatsada Singthongchai", "Ekkachai Naenudorn", "Supachanun Wanapu."],
    "venue": "Proceedings of the International MultiConference of Engineers and Computer",
    "year": 2013
  }, {
    "title": "Transforming wikipedia into named entity training data",
    "authors": ["Joel Nothman", "James R Curran", "Tara Murphy."],
    "venue": "Proceedings of the Australasian Language Technology Association Workshop 2008, pages 124–132.",
    "year": 2008
  }, {
    "title": "Question-answering with grammatically-interpretable representations",
    "authors": ["Hamid Palangi", "Paul Smolensky", "Xiaodong He", "Li Deng."],
    "venue": "Proceedings of the 32nd AAAI Conference on Artificial Intelligence, New Orleans, LA.",
    "year": 2018
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Associa-",
    "year": 2002
  }, {
    "title": "Question answering on electronic medical records",
    "authors": ["Preethi Raghavan", "Siddharth Patwardhan."],
    "venue": "Proceedings of the 2016 Summit on Clinical",
    "year": 2016
  }, {
    "title": "Annotating electronic medical records for question answering",
    "authors": ["Preethi Raghavan", "Siddharth Patwardhan", "Jennifer J. Liang", "Murthy V. Devarakonda."],
    "venue": "arXiv:1805.06816.",
    "year": 2017
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "arXiv preprint arXiv:1606.05250.",
    "year": 2016
  }, {
    "title": "Mctest: A challenge dataset for the open-domain machine comprehension of text",
    "authors": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."],
    "venue": "EMNLP, volume 3, page 4.",
    "year": 2013
  }, {
    "title": "Annotating logical forms for ehr questions",
    "authors": ["Kirk Roberts", "Dina Demner-Fushman."],
    "venue": "LREC... International Conference on Language Resources & Evaluation:[proceedings]. International Conference on Language Resources and Evaluation, volume",
    "year": 2016
  }, {
    "title": "Deep ehr: A survey of recent advances in deep learning techniques for electronic health record (ehr) analysis",
    "authors": ["Benjamin Shickel", "Patrick James Tighe", "Azra Bihorac", "Parisa Rashidi."],
    "venue": "IEEE Journal of Biomedical and Health Informatics.",
    "year": 2017
  }, {
    "title": "Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/uthealth corpus",
    "authors": ["Amber Stubbs", "Özlem Uzuner."],
    "venue": "Journal of biomedical informatics, 58:S20–S29.",
    "year": 2015
  }, {
    "title": "On generating characteristic-rich question sets for qa evaluation",
    "authors": ["Yu Su", "Huan Sun", "Brian Sadler", "Mudhakar Srivatsa", "Izzeddin Gur", "Zenghui Yan", "Xifeng Yan."],
    "venue": "EMNLP, pages 562–572.",
    "year": 2016
  }, {
    "title": "A short review of ethical challenges in clinical natural language processing",
    "authors": ["Simon Šuster", "Stéphan Tulkens", "Walter Daelemans."],
    "venue": "arXiv preprint arXiv:1703.10090.",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Traditional medical records as a source of clinical data in the outpatient setting",
    "authors": ["Paul C Tang", "Danielle Fafchamps", "Edward H Shortliffe."],
    "venue": "Proceedings of the Annual Symposium on Computer Application in Medical Care, page 575. American",
    "year": 1994
  }, {
    "title": "Newsqa: A machine comprehension dataset",
    "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."],
    "venue": "arXiv preprint arXiv:1611.09830.",
    "year": 2016
  }, {
    "title": "An overview of the bioasq large-scale biomedical semantic indexing and question answering competition",
    "authors": ["Krithara", "Sergios Petridis", "Dimitris Polychronopoulos"],
    "venue": "BMC bioinformatics,",
    "year": 2015
  }, {
    "title": "Hawk– hybrid question answering using linked data",
    "authors": ["Ricardo Usbeck", "Axel-Cyrille Ngonga Ngomo", "Lorenz Bühmann", "Christina Unger."],
    "venue": "European Semantic Web Conference, pages 353– 368. Springer.",
    "year": 2015
  }, {
    "title": "Recognizing obesity and comorbidities in sparse data",
    "authors": ["Özlem Uzuner."],
    "venue": "Journal of the American Medical Informatics Association, 16(4):561–570.",
    "year": 2009
  }, {
    "title": "Evaluating the state of the art in coreference resolution for electronic medical records",
    "authors": ["Ozlem Uzuner", "Andreea Bodnari", "Shuying Shen", "Tyler Forbush", "John Pestian", "Brett R South."],
    "venue": "Journal of the American Medical Informatics Association,",
    "year": 2012
  }, {
    "title": "Identifying patient smoking status from medical discharge records",
    "authors": ["Özlem Uzuner", "Ira Goldstein", "Yuan Luo", "Isaac Kohane."],
    "venue": "Journal of the American Medical Informatics Association, 15(1):14–24.",
    "year": 2008
  }, {
    "title": "Extracting medication information from clinical text",
    "authors": ["Özlem Uzuner", "Imre Solti", "Eithon Cadag."],
    "venue": "Journal of the American Medical Informatics Association, 17(5):514–518.",
    "year": 2010
  }, {
    "title": "Community annotation experiment for ground truth generation for the i2b2 medication challenge",
    "authors": ["Özlem Uzuner", "Imre Solti", "Fei Xia", "Eithon Cadag."],
    "venue": "Journal of the American Medical Informatics Association, 17(5):519–523.",
    "year": 2010
  }, {
    "title": "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
    "authors": ["Özlem Uzuner", "Brett R South", "Shuying Shen", "Scott L DuVall."],
    "venue": "Journal of the American Medical Informatics Association, 18(5):552–556.",
    "year": 2011
  }, {
    "title": "Making machine learning models interpretable",
    "authors": ["Alfredo Vellido", "José David Martín-Guerrero", "Paulo JG Lisboa."],
    "venue": "ESANN, volume 12, pages 163–172. Citeseer.",
    "year": 2012
  }, {
    "title": "The trec-8 question answering track report",
    "authors": ["Ellen M Voorhees"],
    "venue": "Trec, volume 99, pages 77–",
    "year": 1999
  }],
  "id": "SP:6cbdeb96380e028fb204f9802694e77bb772ed60",
  "authors": [{
    "name": "Anusri Pampari",
    "affiliations": []
  }, {
    "name": "Preethi Raghavan",
    "affiliations": []
  }, {
    "name": "Jennifer Liang",
    "affiliations": []
  }, {
    "name": "Jian Peng",
    "affiliations": []
  }],
  "abstractText": "We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets§. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.",
  "title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records"
}