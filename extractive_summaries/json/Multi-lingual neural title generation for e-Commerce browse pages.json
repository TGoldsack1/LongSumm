{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 162–169 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Natural language generation (NLG) has a broad range of applications, from question answering systems to story generation, summarization etc. In this paper, we target a particular use case that is important for e-Commerce websites, which group multiple items on common pages called browse pages (BP). Each browse page contains an overview of various items which share some characteristics expressed as slot/value pairs.\nFor example, we can have a browse page for Halloween decoration, which will display different types like lights, figurines, and candy bowls. These different items of decoration have their own browse pages, which are linked from the BP for Halloween decoration. A ceramic candy bowl for Halloween can appear on various browse pages, e.g. on the BP for Halloween decoration, BP for Halloween candy bowls, as well as the (non Halloween-specific) BP for ceramic candy bowls.\nTo show customers which items are grouped on a browse page, we need a human-readable title of the content of that particular page. Different combinations of characteristics bijectively correspond to different browse pages, and consequently to different browse page titles.\nNote that here, different from other natural language generation tasks described in the literature, slot names are already given; the task is to generate a title for a set of slots. Moreover, we do not perform any selection of the slots that the title should realize; but all slots need to be realized in order to have a unique title. E-Commerce sites may have tens of millions of such browse pages in many different languages. The number of unique slot-value pairs are in the order of hundreds of thousands. All these factors render the task of human creation of BP titles infeasible.\nMathur, Ueffing, and Leusch (2017) developed several different systems which generated titles for these pages automatically. These systems include rule-based approaches, statistical models, and combinations of the two. In this work, we investigate the use of neural sequence-to-sequence models for browse page title generation. These models have recently received much attention in the research community, and are becoming the new state of the art in machine translation (refer Section 4).\nWe will compare our neural generation models\n162\nagainst two state-of-the-art systems.\n1. The baseline system for English and French implements a hybrid generation approach, which combines a rule-based approach (with a manually created grammar) and statistical machine translation (SMT) techniques. For French, we have monolingual data for training language model, which can be used in the SMT system. For English, we also have human-curated titles and can use those for training additional “translation” components for this hybrid system.\n2. The system for German is an Automatic Post-Editing (APE) system – first introduced by Simard et al. (2007) – which generates titles with the rule-based approach, and then uses statistical machine translation techniques for automatically correcting the errors made by the rule-based approach.\nIn the following section, we describe a few of the previous works in the field of language generation from a knowledge base or linked data. Section 3 addresses the idea of lexicalization of a browse node in linear form along with the normalization step to replace the slot values with placeholders. Sequence-to-sequence models for generation of titles are described in Section 4, followed by a description of joint learning over multiple languages in Section 5. Experiments and results are described in Sections 6 and 7."
  }, {
    "heading": "2 Related work",
    "text": "The first works on NLG were mostly focused on rule-based language generation (Dale et al., 1998; Reiter et al., 2005; Green, 2006). NLG systems typically perform three different steps: content selection, where a subset of relevant slot/value pairs are selected, followed by sentence planning, where these selected pairs are realized into their respective linguistic variations, and finally surface realization, where these linguistic structures are combined to generate text. Our use case differs from the above in that there is no selection done on the slot/value pairs, but all of them undergo the sentence planning step. In rule-based systems, all of the above steps rely on hand-crafted rules.\nData driven approaches, on the other hand, either try to learn each of the steps automatically from the data Barzilay and Lapata (2005)\nDale et al. (1998) described the problem of generating natural language titles and short descriptions of structured nodes which consist of slot/value pairs. There are many research which deal with learning a generation model from parallel data. These parallel data consist of the structured data and natural-language text, so that the model can learn to transform the structured data into text. Duma and Klein (2013) generate short natural-language descriptions, taking structured DBPedia data as input. Their approach learns text templates which are filled with the information from the structured data.\nMei et al. (September, 2015) use recurrent neural network (LSTM) models to generate text from facts given in a knowledge base. Chisholm et al. (2017) solve the same problem by applying a machine translation system to a linearized version of the pairs. Several recent papers tackle the problem of generating a one-sentence introduction for a biography given structured biographical slot/value pairs. One difference between our work and the papers above, (Mei et al., September, 2015), and (Chisholm et al., 2017), is that they perform selective generation, i.e. they run a selection step that determines the slot/value pairs which will be included in the realization. In our use case however, all slot/value pairs are relevant and need to be realized.\nSerban et al. (2016) generate questions from facts (structured input) by leveraging fact embeddings and then employing placeholders for handling rare words. In their work, the placeholders are heuristically mapped to the facts, however, we map our placeholders depending on the neural attention (for details, see Section 4)."
  }, {
    "heading": "3 Lexicalization",
    "text": "Our first step towards title generation is verbalization of all slot/value pairs. This can be achieved by a rule-based approach as described in (Mathur et al., 2017). However, in the work presented here, we do not directly lexicalize the slot/value pairs, but realize them in a pseudo language first. For example, the pseudo-language sequence for the slot/value pairs in Table 1 is “ brand ACME cat Cell Phones & Smart Phones color white capacity 32GB”.1\n1 cat refers to an e-Commerce category in the browse page."
  }, {
    "heading": "3.1 Normalization",
    "text": "Pseudo-language browse pages can still contain a large number of unique slot values. For example, there exist many different brands for smart phones (Samsung, Apple, Huawei, etc.). Large vocabulary is a known problem for neural systems, because rare or less frequent words tend to translate incorrectly due to data sparseness (Luong et al., 2015). At the same time, the softmax computation over the large vocabulary becomes intractable in current hardware. To avoid this issue, we normalize the pseudo-language sequences and thereby reduce the vocabulary size. For each language, we computed the 30 most frequent slot names and normalized their values via placeholders (Luong et al., August, 2015). For example, the lexicalization of “Brand: ACME” is “ brand ACME”, but after normalization, this becomes brand $brand|ACME. This representation means that the slot name brand has the value of a placeholder brand which contains the entity called “ACME”. During training, we remove the entity from the normalized sequence, while keeping them during translation of development or evaluation set. The mapping of placeholders in the target text back to entity names is described in Section 4.\nThe largest reduction in vocabulary size would be achieved by normalizing all slots. However, this would create several issues in generation. Consider the pseudo-language sequence “ bike Road bike type Racing”. If we replace all slot values with placeholders, i.e. “ bike $bike type $type”, then the system will not have enough information for generating the title “Road racing bike”. Moreover, the boolean slots, such as “ comic Marvel comics signed No” would be normalized to placeholders as “ comic $comic signed $signed”, and we would loose the information (“No”) necessary to realize this title as “Unsigned Marvel comics”."
  }, {
    "heading": "3.2 Sub-word units",
    "text": "We applied another way of reducing the vocabulary, called byte pair encoding (BPE) (Sennrich\net al., 2016), a technique often used in NMT systems (Bojar et al., 2017). BPE is essentially a data compression technique which splits each word into sub-word units and allows the NMT system to train on a smaller vocabulary. One of the advantages of BPE is that it propagates generation of unseen words (even with different morphological variations). However, in our use case, this can create issues, because if BPE splits a brand and generates an incorrect brand name in the target, an e-Commerce company could be legally liable for the mistake. In such case, one can first run the normalization with placeholders followed by BPE, but due to time constraints, we do not report experiments on the same."
  }, {
    "heading": "4 Sequence-to-Sequence Models",
    "text": "Sequence-to-sequence models in this work are based on an encoder-decoder model and an attention mechanism as described by Bahdanau et al. (May, 2016). In this network, the encoder is a bidirectional RNN which encodes the information of a sentence X = (x1, x2, . . . xm) of length m into a fixed length vector of size |hi|, where hi is the hidden state produced by the encoder for token xi. Since our encoder is a bi-directional model, the encoded hidden state is hi = hi,fwd + hi,bwd, where hfwd and hbwd are unidirectional encoders, running from left to right and right to left, respectively. That is, they are encoding the context to the left and to the right of the current token.\nOur decoder is a simple recurrent neural network (RNN) consisting of gated recurrent units (GRU) (Cho et al., 2014) because of their computationally efficiency. The RNN predicts the target sequence Y = (y1, y2, . . . , yj , . . . , yl) based on the final encoded state h. Basically, the RNN predicts the target token yj ∈ V (with target vocabulary V) and emits a hidden state sj based on the previous recurrent state sj−1, the previous sequence of words Yj−1 = (y1, y2, . . . , yj−1) and Cj , a weighted attention vector. The attention vector is a weighted average of all the hidden source states hi, where i = 1, . . . ,m. Attention weight (aij) is computed between the hidden states hi and sj and is leveraged as a weight of that source state hi. In generation, we make use of these alignment scores to align our placeholders.2 The target placeholders are bijectively mapped to those\n2These placeholders are not to be confused with the placeholder for a tensor.\nsource placeholders whose alignment score (aij) is the highest at the time of generation.\nThe decoder predicts a score for all the tokens in the target vocabulary, which is then normalized by a softmax function, and the token with the highest probability is predicted."
  }, {
    "heading": "5 Multilingual Generation",
    "text": "In this section, we present the extension of our work from a single-language setting to multilanguage settings. There have been various studies in the past that target neural machine translation from multiple source languages into a single target language (Zoph and Knight, Jan, 2016), from single source to multiple target languages (Dong et al., 2015) and multiple source to multiple target languages (Johnson et al., June, 2016). One of the main motivation of joint learning in above works is to improve the translation quality on a low-resource language pair via transfer learning between related languages. For example, Johnson et al. (June, 2016) had no parallel data available to train a Japanese-to-Korean MT system, but training Japanese-English and English-Korean language pairs allowed their model to learn translations from Japanese to Korean without seeing any parallel data. In our case, the amount of training data for French is small compared to English and German (cf. Section 6.1). We propose joint learning of English, French and German, because we expect that transfer learning will improve generation for French. We investigate the joint training of pairs of these languages as well the combination of all three.\nOn top of the multi-lingual approach, we follow the work of Currey et al. (2017) who proposed copying monolingual data on both sides (source and target) as a way to improve the performance of NMT systems on low-resource languages. In machine translation, there are often named entities and nouns which need to be translated verbatim, and this copying mechanism helps in identifying them. Since our use case is monolingual generation, we expect a large gain from this copying approach because we have many brands and other slot values which need to occur verbatim in the generated titles."
  }, {
    "heading": "6 Experiments",
    "text": ""
  }, {
    "heading": "6.1 Data",
    "text": "We have access to a large number of humancreated titles (curated titles) for English and German, and a small number of curated titles for French. When generating these titles, human annotators were specifically asked to realize all slots in the title.\nWe make use of a large monolingual out-ofdomain corpus for French, as it is a low-resource language. We collect item description data from an e-Commerce website and clean the data in the following way: 1) we train a language model (LM) on the small amount of French curated titles, 2) we tokenize the out-of-domain data, 3) we remove all sentences with length less than 5, 4) we compute the LM perplexity for each sentence in the out-ofdomain data, 5) we sort the sentences in increasing order of their perplexities and 6) select the top 500K sentences. Statistics of the data sets are reported in Table 2."
  }, {
    "heading": "6.2 Systems",
    "text": "We compared the NLG systems in the single-, dual-, and multi-lingual settings.\nSingle-language setting: This is the baseline NLG system, a straightforward sequence-tosequence model with attention as described in Luong et al. (August, 2015), trained separately for each language. The vocabulary is computed on the concatenation of both source and target data, and the same vocabulary is used for both source and target languages in the experiments.\nWe use Adam (Kingma and Ba, December, 2014) as a gradient descent approach for faster convergence. Initial learning rate is set to 0.0002 with a decay rate of 0.9. The dimension of word embeddings is set to 620 and hidden layer size to\n1000. Dropout is set to 0.2 and is activated for all layers except the initial word embedding layer, because we want to realize all aspects, we cannot afford to zero out any token in the source. We continue training of the model and evaluate on the development set after each epoch, stopping the training if the BLEU score on the development set does not increase for 10 iterations.\nBaselines: We compare our neural system with a fair baseline system (Baseline 1), which is a statistical MT system trained on the same parallel data as the neural system: the source side is the linearized pseudo-language sequence, and the target side is the curated title in natural language. Baseline 2 is the either the hybrid system (for French and English) or the APE system (for German), both described in Section 1. These are unfair baselines, because (1) the hybrid system employs a large number of hand-made rules in combination with statistical models (Mathur, Ueffing, and Leusch, 2017), while the neural systems are unaware of the knowledge encoded in those rules, (2) the APE system and neural systems learn from same amount of parallel data, but the APE system aims at correcting rule-based generated titles, whereas the neural system aims at generating titles directly from a linearized form, which is a harder task. We compare our systems with the best performing systems of (Mathur et al., 2017), i.e. hybrid system for English and French, and APE system for German.\nMulti-lingual setting: We train the neural model jointly on multiple languages to leverage transfer learning from a high-resource language to a low-resource one. In our multi-lingual setting, we experiment with three different combinations to improve models for French: 1) English+French (en-fr) 2) German+French (de-fr) 3) English+French+German (en-fr-de). English and French being close languages, we expect the enfr system to benefit more from transfer learning across languages than any other combination. Although, as evident in Zoph and Knight (Jan, 2016), joint learning between the distant languages works better as they tend to disambiguate each other better than two languages which are close. For comparison, we also run a combination of two highresource languages, i.e. English and German (ende), to see if transfer learning works for them. It is important to note that in all multi-lingual sys-\ntems the low-resourced language is over-sampled to balance the data.\nWe used the same design parameters on the neural network in both the single-language and the multi-lingual setting.\nNormalized setting: On top of the systems above, we also experimented with the normalization scheme presented in Section 3.1. Normalization is useful in two ways: 1) It reduces the vocabulary size and 2) it avoids spurious generation of important aspect values (slot values). The second point is especially important in our case because this avoids highly sensitive issues such as brand violations. MT researches have observed that NMT systems often generate very fluent output, but have a tendency to generate inadequate output, i.e. sentences or words which are not related to the given input (Koehn and Knowles, June, 2017). We alleviate this problem through the normalization described above. After normalization, we see vocabulary reductions of 15% for French, 20% for German and as high as 35% for English.\nAs described in Section 5, we also use byte pair encoding, with a BPE code size of 30,000 for all systems (with BPE). We train the codes on the concatenation of source and target since (in this monolingual generation task) the vocabularies are very similar; the vocabulary size is around 30k for systems using BPE for both source and target."
  }, {
    "heading": "7 Results",
    "text": "We evaluate our systems with three different automatic metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and character FScore (Popović, 2016). Note that BLEU and character F-score are quality metrics, i.e. higher scores mean higher quality, while TER is an error metric, where lower scores indicate higher quality. All metrics compare the automatically generated title against a human-curated title and determine sequence matches on the word or character level.\nTable 3 summarizes results from all systems on the English test set. All neural systems are better than the fair Baseline 1 system.\nNormalization with tags (i.e. using placeholders) has a negative effect on English title quality both in the single-language setting en (67.1 vs. 68.4 BLEU) and in the dual-language setting en-fr (67.1 vs. 70.7 BLEU). However, title quality increases when using BPE instead (71.9 vs. 70.7 BLEU). On en-de, we observe gains\nboth from normalization with tags and from BPE. Again, BPE normalization works best. Both duallanguage systems with BPE achieve better performance that the best monolingual English system (71.9 and 72.7 vs. 68.4 BLEU).\nThe system en-frbig contains monolingual French data added via the copying mechanism, which improves title quality. It outperforms any other neural system and is on par with Baseline 2 (unfair baseline), even outperforming it in terms of TER. The multi-lingual system en-fr-de is very close to en-frbig according to all three metrics.\nTable 4 collects the results for all systems on the German test set. For the single-language setting, we see a loss of 7 BLEU points when normalizing the input sequence, which is caused by incorrect morphology in the titles. When using placeholders, the system generates entities in the title in the exact form in which they occur in the input. In German, however, the words often need to be inflected. For example, the slot “ brand Markenlos” should be realized as “Markenlose” (Unbranded) in the title, but the placeholder generates the input form “Markenlos” (without suffix ‘e’). This causes a huge deterioration in the word-level met-\nrics BLEU and TER, but not as drastic in chrF1, which evaluates on the character level.\nFor German, there is a positive effect of transfer learning for both dual-language systems ende and de-frbig with BPE (79.6 and 80.0 vs. 78.2 BLEU). However, the combination of languages hurts when we combine languages at token level, i.e. without normalization or with tags. The performance of systems with BPE is even on par with or better than the strong baseline of 79.4 BLEU, both for combinations of two and of three languages.\nTable 5 summarizes the results from all systems on the French test set. The single-language fr NMT system achieves a low BLEU score compared to the SMT system Baseline 1 (23.0 vs. 44.6). This is due to the very small amount of parallel data, which is a setting where SMT typically outperforms NMT as evidenced in Zoph et al. (April, 2016). Normalization has a big positive impact on all French systems (e.g. 27.4\nvs. 23.0 BLEU for fr). The de-fr systems show a much larger gain from transfer learning than the en-fr systems, which validates Zoph and Knight (Jan, 2016)’s results, who show that transfer learning is better for distant languages than for similar languages.\nFor all three languages, copying monolingual data improves the NMT system by a large margin.\nThe multi-lingual en-fr-de (BPE) system (with copied monolingual data) is the best system for all three languages. It has the additional advantage of being one single model that can cater to all three languages at once.\nTable 6 presents the example titles comparing different phenomena. The first block shows the usefulness of placeholders in system fr small ,tags (i.e. fr small , normalized with tags) where in comparison to fr small the brand is generated verbatim. The second block shows the effectiveness of copying the data where “Cylindres” is generated correctly in the frbig (with BPE) system in comparison to fr small . The last block shows that reordering and adequacy in generation can be improved with the helpful signals from high-resourced English and German languages."
  }, {
    "heading": "8 Conclusion",
    "text": "We developed neural language generation systems for an e-Commerce use case for three languages with very different amounts of training data and came to the following conclusions:\n(1) The lack of resources in French leads to generation of low quality titles, but this can be drastically improved upon with transfer learning between French and English and/or German.\n(2) In case of low-resource languages, copying monolingual data (even if out-of-domain) improves the performance of the system.\n(3) Normalization with placeholders usually helps for languages with relatively easy morphology.\n(4) It is important to over-sample the lowresourced languages in order to balance the high& low-resourced data, thereby, creating a stable NLG system.\n(5) For French, a low-resource language in our use case, the hybrid system which combines manual rules and SMT technology is still far better than the best neural system.\n(6) The multi-lingual model has the best tradeoff, as it achieves the best results among the neural\nsystems in all three languages and it is one single model which can be deployed easily on a single GPU machine."
  }, {
    "heading": "Acknowledgments",
    "text": "Thanks to our colleague Pavel Petrushkov for all the help with the neural MT toolkit."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"],
    "year": 2016
  }, {
    "title": "Collective content selection for concept-to-text generation",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computa-",
    "year": 2005
  }, {
    "title": "Learning to generate one-sentence biographies from Wikidata",
    "authors": ["Andrew Chisholm", "Will Radford", "Ben Hachey."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long",
    "year": 2017
  }, {
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Struc-",
    "year": 2014
  }, {
    "title": "Copied monolingual data improves low-resource neural machine translation",
    "authors": ["Anna Currey", "Antonio Valerio Miceli Barone", "Kenneth Heafield."],
    "venue": "Proceedings of the Second Conference on Machine Translation. Association for Computational",
    "year": 2017
  }, {
    "title": "The realities of generating natural language from databases",
    "authors": ["Robert Dale", "Stephen J Green", "Maria Milosavljevic", "Cécile Paris", "Cornelia Verspoor", "Sandra Williams."],
    "venue": "Proceedings of the 11th Australian Joint Conference on Artificial Intel-",
    "year": 1998
  }, {
    "title": "Multi-task learning for multiple language translation",
    "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."],
    "venue": "ACL (1). The Association for Computer Linguistics.",
    "year": 2015
  }, {
    "title": "Generating natural language from linked data: Unsupervised template extraction",
    "authors": ["Daniel Duma", "Ewan Klein."],
    "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers. Association for Com-",
    "year": 2013
  }, {
    "title": "Generation of biomedical arguments for lay readers",
    "authors": ["Nancy Green."],
    "venue": "Proceedings of the Fourth International Natural Language Generation Conference. Association for Computational Linguistics, Stroudsburg, PA, USA, INLG ’06.",
    "year": 2006
  }, {
    "title": "Adam: A method for stochastic optimization. CoRR abs/1412.6980 [cs.LG",
    "authors": ["Diederik P. Kingma", "Jimmy Ba"],
    "year": 2014
  }, {
    "title": "Six challenges for neural machine translation. CoRR abs/1706.03872 [cs.CL",
    "authors": ["Philipp Koehn", "Rebecca Knowles"],
    "year": 2017
  }, {
    "title": "Effective approaches to attention-based neural machine translation. CoRR abs/1508.04025 [cs.CL",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"],
    "year": 2015
  }, {
    "title": "Addressing the rare word problem in neural machine translation",
    "authors": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }, {
    "title": "Generating titles for millions of browse pages on an e-commerce site",
    "authors": ["Prashant Mathur", "Nicola Ueffing", "Gregor Leusch."],
    "venue": "Proceedings of the International Conference on Natural Language Generation.",
    "year": 2017
  }, {
    "title": "What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment. Computing Research Repository (CoRR) abs/1509.00838 [cs.CL",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"],
    "year": 2015
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of",
    "year": 2002
  }, {
    "title": "chrF deconstructed: beta parameters and n-gram weights",
    "authors": ["Maja Popović."],
    "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Choosing words in computer-generated weather forecasts",
    "authors": ["Ehud Reiter", "Somayajulu Sripada", "Jim Hunter", "Ian Davy."],
    "venue": "Artificial Intelligence 167:137– 169.",
    "year": 2005
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
    "year": 2016
  }, {
    "title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus",
    "authors": ["Iulian Vlad Serban", "Alberto Garcı́a-Durán", "Çaglar Gülçehre", "Sungjin Ahn", "Sarath Chandar", "Aaron C. Courville", "Yoshua Bengio"],
    "year": 2016
  }, {
    "title": "Statistical phrase-based post-editing",
    "authors": ["Michel Simard", "Cyril Goutte", "Pierre Isabelle."],
    "venue": "In Proceedings of NAACL.",
    "year": 2007
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223–231.",
    "year": 2006
  }, {
    "title": "Multisource neural translation",
    "authors": ["Barret Zoph", "Kevin Knight. Jan"],
    "year": 2016
  }, {
    "title": "Transfer learning for low-resource neural machine translation",
    "authors": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"],
    "year": 2016
  }],
  "id": "SP:ee29db1d395594a17a328af2f0d91c9073ae2be0",
  "authors": [{
    "name": "Prashant Mathur",
    "affiliations": []
  }, {
    "name": "Nicola Ueffing",
    "affiliations": []
  }, {
    "name": "Gregor Leusch",
    "affiliations": []
  }],
  "abstractText": "To provide better access of the inventory to buyers and better search engine optimization, e-Commerce websites are automatically generating millions of easily searchable browse pages. A browse page groups multiple items with shared characteristics together. It consists of a set of slot name/value pairs within a given category that are linked among each other and can be organized in a hierarchy. This structure allows users to navigate laterally between different browse pages (i.e. browse between related items) or to dive deeper and refine their search. These browse pages require a title describing the content of the page. Since the number of browse pages is huge, manual creation of these titles is infeasible. Previous statistical and neural generation approaches depend heavily on the availability of large amounts of data in a language. In this research, we apply sequence-tosequence models to generate titles for high& low-resourced languages by leveraging transfer learning. We train these models on multilingual data, thereby creating one joint model which can generate titles in various different languages. Performance of the title generation system is evaluated on three different languages; English, German, and French, with a particular focus on low-resourced French language.",
  "title": "Multi-lingual neural title generation for e-Commerce browse pages"
}