{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–394 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n390"
  }, {
    "heading": "1 Introduction",
    "text": "Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable.\nOne prevalent strategy involves creating multilingual word embeddings, where each language’s vocabulary is embedded in the same latent space (Vulić and Moens, 2013; Mikolov et al., 2013a; Artetxe et al., 2016); however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary.\nMore recent work has focused on reducing that constraint. Vulić and Moens (2016) and Vulic and Korhonen (2016) use document-aligned data to learn bilingual embeddings instead of a seed dictionary. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; Zhang et al. (2017) use an adversarial training method to learn a similar mapping. Lample et al. (2018a) use a series of techniques to align monolingual embedding spaces in a completely unsupervised\nway; their method is used by Lample et al. (2018b) as the initialization for a completely unsupervised machine translation system.\nThese recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: Dyer et al. (2011) and Berg-Kirkpatrick et al. (2010) investigate using linguistic features for word alignment, and Haghighi et al. (2008) use linguistic features for unsupervised bilingual lexicon induction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni).\nThe addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embeddingbased approach of Artetxe et al. (2017) with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction."
  }, {
    "heading": "2 Background",
    "text": "This work is directly based on the work of Artetxe et al. (2017). Following their work, let X ∈ R|Vs|×d and Z ∈ R|Vt|×d be the word embedding matrices of two distinct languages, referred to respectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of\nthese matrices as Xi∗ or Zi∗. The vocabularies for each language are Vs and Vt, respectively. Also let D ∈ {0, 1}|Vs|×|Vt| be a binary matrix representing a dictionary such that Dij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W ∈ Rd×d that maps source embeddings onto their aligned target embeddings. Artetxe et al. (2017) define the optimal mapping matrix W ∗ with the following equation,\nW ∗ = arg min W ∑ i ∑ j Dij ‖Xi∗W − Zj∗‖2\nwhich minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.\nBy normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W TW = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embeddings, such that\nW ∗ = arg max W Tr(XWZTDT )\nwhere Tr(·) is the trace operator, the sum of all diagonal entries. The optimal solution to this equation is W ∗ = UV T , where XTDZ = UΣV T is the singular value decomposition of XTDZ.\nThis formulation requires a seed dictionary. To reduce the need for a large seed dictionary, Artetxe et al. (2017) propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates until convergence. In the dictionary induction step, they set Dij = 1 if j = arg maxk (Xi∗W ) · Zk∗ and Dij = 0 otherwise.\nWe propose two methods for extending this system using orthographic information, described in the following two sections."
  }, {
    "heading": "3 Orthographic Extension of Word Embeddings",
    "text": "This method augments the embeddings for all words in both languages before using them in the self-learning framework of Artetxe et al. (2017). To do this, we append to each word’s embedding a vector of length equal to the size of the union of the two languages’ alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the\nspelling of the word. This letter count vector is then scaled by a constant before being appended to the base word embedding. After appending, the resulting augmented vector is normalized to have magnitude 1.\nMathematically, let A be an ordered set of characters (an alphabet), containing all characters appearing in both language’s alphabets:\nA = Asource ∪Atarget\nLet Osource and Otarget be the orthographic extension matrices for each language, containing counts of the characters appearing in each word wi, scaled by a constant factor ce:\nOij = ce · count(Aj , wi), O ∈ {Osource, Otarget}\nThen, we concatenate the embedding matrices and extension matrices:\nX ′ = [X;Osource], Z ′ = [Z;Otarget]\nFinally, in the normalized embedding matrices X ′′ and Z ′′ , each row has magnitude 1:\nX ′′ i∗ =\nX ′ i∗\n‖X ′i∗‖ , Z\n′′ i∗ =\nZ ′ i∗\n‖Z ′i∗‖\nThese new matrices are used in place of X and Z in the self-learning process."
  }, {
    "heading": "4 Orthographic Similarity Adjustment",
    "text": "This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of Artetxe et al. (2017), which uses the dot product of two words’ embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words.\nThe normalized edit distance is defined as the Levenshtein distance (L(·, ·)) (Levenshtein, 1966) divided by the length of the longer word. The Levenshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normalized edit distance function is denoted as NL(·, ·).\nNL(w1, w2) = L(w1, w2)\nmax(|w1|, |w2|)\nWe define the orthographic similarity of two words w1 and w2 as log(2.0−NL(w1, w2)). These\nsimilarity scores are used to form an orthographic similarity matrix S, where each entry corresponds to a source-target word pair. Each entry is first scaled by a constant factor cs. This matrix is added to the standard similarity matrix, XWZT .\nSij = cs·log(2.0−NL(wi, wj)), wi ∈ Vs, wj ∈ Vt\nThe vocabulary for each language is 200,000 words, so computing a similarity score for each pair would involve 40 billion edit distance calculations. Also, the vast majority of word pairs are orthographically very dissimilar, resulting in a normalized edit distance close to 1 and an orthographic similarity close to 0, having little to no effect on the overall estimated similarity. Therefore, we only calculate the edit distance for a subset of possible word pairs.\nThus, the actual orthographic similarity matrix that we use is as follows:\nS ′ ij = { Sij 〈wi, wj〉 ∈ symDelete(Vt,Vs,k) 0 otherwise\nThis subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012), which we denote as symDelete(·,·,·). This algorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical after k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair modernomodern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.\nThe algorithm works by computing all strings formed by k or fewer deletions from each target\nword, stores them in a hash table, then does the same for each source word and generates sourcetarget pairs that share an entry in the hash table. The complexity of this algorithm can be expressed as O(|V |lk), where V = Vt ∪ Vs is the combined vocabulary and l is the length of the longest word in V . This is linear with respect to the vocabulary size, as opposed to the quadratic complexity required for computing the entire matrix. However, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 < k < 4), and used k = 1 for our final results, finding no significant benefit from using a larger value."
  }, {
    "heading": "5 Experiments",
    "text": "We use the datasets used by Artetxe et al. (2017), consisting of three language pairs: EnglishItalian, English-German, and English-Finnish. The English-Italian dataset was introduced in Dinu and Baroni (2014); the other datasets were created by Artetxe et al. (2017). Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b)) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in Artetxe et al. (2017).1 However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel-\n1https://github.com/artetxem/vecmap\nopment data.2 In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations. On average, the number of translations for each source (non-English) word was 1.2 for English-Italian, 1.3 for English-German, and 1.4 for English-Finnish."
  }, {
    "heading": "6 Results and Discussion",
    "text": "For our experiments with orthographic extension of word embeddings, each embedding was extended by the size of the union of the alphabets of both languages. The size of this union was 199 for English-Italian, 200 for English-German, and 287 for English-Finnish.\nThese numbers are perhaps unintuitively high. However, the corpora include many other characters, including diacritical markings and various symbols (%, [, !, etc.) that are an indication that tokenization of the data could be improved. We did not filter these characters in this work.\nFor our experiments with orthographic similarity adjustment, the heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings.\n2We use all source-target pairs containing one of 1,000 randomly-selected target words.\nFigure 1 shows the results on the development data. Based on these results, we selected ce = 18 and cs = 1 as our hyperparameters. The local optima were not identical for all three languages, but we felt that these values struck the best compromise among them.\nTable 1 compares our methods against the system of Artetxe et al. (2017), using scaling factors selected based on development data results. Because approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experimented with both methods together, and found that this was the best of the settings that did not include the identity translation component; with the identity component included, however, the embedding extension method alone was best for EnglishFinnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its ancestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs.\nIn addition to identifying orthographically similar words, the extension method is capable of\nlearning a mapping between source and target letters, which could partially explain its improved performance over our edit distance method.\nTable 2 shows some correct translations from our system that were missed by the baseline."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "In this work, we presented two techniques (which can be combined) for improving embedding-based bilingual lexicon induction for related languages using orthographic information and no parallel data, allowing their use with low-resource language pairs. These methods increased accuracy in our experiments, with both the combined and embedding extension methods providing significant gains over the baseline system.\nIn the future, we want to extend this work to related languages with different alphabets (experimenting with transliteration or phonetic transcription) and to extend other unsupervised bilingual lexicon induction systems, such as that of Lample et al. (2018a).\nAcknowledgments We are grateful to the anonymous reviewers for suggesting useful additions. This research was supported by NSF grant number 1449828."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
    "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-",
    "year": 2016
  }, {
    "title": "Learning bilingual word embeddings with (almost) no bilingual data",
    "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 451–462, Vancouver,",
    "year": 2017
  }, {
    "title": "Painless unsupervised learning with features",
    "authors": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-Côté", "John DeNero", "Dan Klein."],
    "venue": "Proceedings of the 2010 Meeting of the North American chapter of the Association for Computational Linguistics",
    "year": 2010
  }, {
    "title": "Improving zero-shot learning by mitigating the hubness problem",
    "authors": ["Georgiana Dinu", "Marco Baroni."],
    "venue": "CoRR, abs/1412.6568.",
    "year": 2014
  }, {
    "title": "Unsupervised word alignment with arbitrary features",
    "authors": ["Chris Dyer", "Jonathan Clark", "Alon Lavie", "Noah A Smith."],
    "venue": "Proceedings of the 49th Annual",
    "year": 2011
  }, {
    "title": "1000x faster spelling correction algorithm",
    "authors": ["Wolf Garbe."],
    "venue": "http://blog.faroo.com/ 2012/06/07/improved-edit-distancebased-spelling-correction/. Accessed: 2018-02-12.",
    "year": 2012
  }, {
    "title": "Learning bilingual lexicons from monolingual corpora",
    "authors": ["Aria Haghighi", "Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 771–779.",
    "year": 2008
  }, {
    "title": "2018a. Word translation without parallel data",
    "authors": ["Guillaume Lample", "Alexis Conneau", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou"],
    "venue": "In International Conference on Learning Representations (ICLR)",
    "year": 2018
  }, {
    "title": "Unsupervised machine translation using monolingual corpora only",
    "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc’Aurelio Ranzato"],
    "venue": "In International Conference on Learning Representations (ICLR)",
    "year": 2018
  }, {
    "title": "Binary codes capable of correcting deletions, insertions, and reversals",
    "authors": ["V.I. Levenshtein."],
    "venue": "Cybernetics and Control Theory, 10(8):707–710. Original in Doklady Akademii Nauk SSSR 163(4): 845–848 (1965).",
    "year": 1966
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1309.4168.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "On the role of seed lexicons in learning bilingual word embeddings",
    "authors": ["Ivan Vulic", "Anna Korhonen."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16), pages 247–257, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)",
    "authors": ["Ivan Vulić", "Marie-Francine Moens."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-13), pages 1613–",
    "year": 2013
  }, {
    "title": "Bilingual distributed word representations from documentaligned comparable data",
    "authors": ["Ivan Vulić", "Marie-Francine Moens."],
    "venue": "J. Artif. Int. Res., 55(1):953–994.",
    "year": 2016
  }, {
    "title": "Adversarial training for unsupervised bilingual lexicon induction",
    "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17), pages 1959–1970,",
    "year": 2017
  }],
  "id": "SP:1e576ce04f2d43c0e3d45da9afbe086759334d6a",
  "authors": [{
    "name": "Parker Riley",
    "affiliations": []
  }, {
    "name": "Daniel Gildea",
    "affiliations": []
  }],
  "abstractText": "Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance, which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.",
  "title": "Orthographic Features for Bilingual Lexicon Induction"
}