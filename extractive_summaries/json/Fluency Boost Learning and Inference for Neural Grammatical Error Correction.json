{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1055–1065 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1055"
  }, {
    "heading": "1 Introduction",
    "text": "Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is\ncommon that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models usually cannot perfectly correct a sentence with many grammatical errors through single-round seq2seq inference, as shown in Figure 1(b) and 1(c), because some errors in a sentence may make the context strange, which confuses the models to correct other errors.\nTo address the above-mentioned limitations in model learning and inference, this paper proposes a novel fluency boost learning and inference mechanism, illustrated in Figure 2.\nFor fluency boosting learning, not only is a seq2seq model trained with original errorcorrected sentence pairs, but also it generates less fluent sentences (e.g., from its n-best outputs) to establish new error-corrected sentence pairs by pairing them with their correct sentences during training, as long as the sentences’ fluency1 is be-\n1A sentence’s fluency score is defined to be inversely proportional to the sentence’s cross entropy, as is in Eq (3).\nShe see Tom is catched by policeman in park at last night.\nShe saw Tom caught by a policeman in the park last night.\nShe see Tom is caught by a policeman in park last night.\nShe sees Tom caught by a policeman in the park last night.\nShe saw Tom caught by a policeman in the park last night.\nShe saw Tom was caught by a policeman in the park last night.\nShe sees Tom is catched by policeman in park at last night. ……\n0.119\n0.147 0.144\n0.135\n0.181\n0.121\n0.147\nn-best outputs\noriginal sentence pair fluency boost sentence pair\nShe sees Tom is catched by policeman in park at last night.\nShe sees Tom caught by a policeman in the park last night.\nShe saw Tom caught by a policeman in the park last night.\nShe saw Tom caught by a policeman in the park last night.\n1st round seq2seq inference\n2nd round seq2seq inference\n3rd round seq2seq inference\n0.121\n0.144\n0.147\n0.147\nboost\nno boost\n(a) (b)\nboost\nsentence fluency fluency sentence\nseq2seq inference\nFigure 2: Fluency boost learning and inference: (a) given a training instance (i.e., an error-corrected sentence pair), fluency boost learning establishes multiple fluency boost sentence pairs from the seq2seq’s n-best outputs during training. The fluency boost sentence pairs will be used as training instances in subsequent training epochs, which helps expand the training set and accordingly benefits model learning; (b) fluency boost inference allows an error correction model to correct a sentence incrementally through multi-round seq2seq inference until its fluency score stops increasing.\nlow that of their correct sentences, as Figure 2(a) shows. Specifically, we call the generated errorcorrected sentence pairs fluency boost sentence pairs because the sentence in the target side always improves fluency over that in the source side. The generated fluency boost sentence pairs during training will be used as additional training instances during subsequent training epochs, allowing the error correction model to see more grammatically incorrect sentences during training and accordingly improving its generalization ability.\nFor model inference, fluency boost inference mechanism allows the model to correct a sentence incrementally with multi-round inference as long as the proposed edits can boost the sentence’s fluency, as Figure 2(b) shows. For a sentence with multiple grammatical errors, some of the errors will be corrected first. The corrected parts will make the context clearer, which may benefit the model to correct the remaining errors.\nExperiments demonstrate fluency boost learning and inference enable neural seq2seq models to perform better for GEC and achieve state-of-theart results on multiple GEC benchmarks.\nOur contributions are summarized as follows:\n• We present a novel learning and inference mechanism to address the limitations in previous seq2seq models for GEC.\n• We propose and compare multiple novel fluency boost learning strategies, exploring the learning methodology for neural GEC.\n• Our approaches are proven to be effective to improve neural seq2seq GEC models to achieve state-of-the-art results on CoNLL2014 and JFLEG benchmark datasets."
  }, {
    "heading": "2 Background: Neural grammatical error correction",
    "text": "As neural machine translation (NMT), a typical neural GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model (Sutskever et al., 2014; Cho et al., 2014) with attention mechanism (Bahdanau et al., 2014) to edit a raw sentence into the grammatically correct sentence it should be, as Figure 1(a) shows.\nGiven a raw sentence xr = (xr1, · · · , xrM ) and its corrected sentence xc = (xc1, · · · , xcN ) in which xrM and x c N are the M -th and N -th words of sentence xr and xc respectively, the error correction seq2seq model learns a probabilistic mapping P (xc|xr) from error-corrected sentence pairs through maximum likelihood estimation (MLE), which learns model parameters Θcrt to maximize the following equation:\nΘ∗crt = argmax Θcrt ∑ (xr ,xc)∈S∗ logP (xc|xr;Θcrt) (1) where S∗ denotes the set of error-corrected sentence pairs.\nFor model inference, an output sequence xo = (xo1, · · · , xoi , · · · , xoL) is selected through beam search, which maximizes the following equation:\nP (xo|xr) = L∏\ni=1\nP (xoi |xr,xo<i;Θcrt) (2)"
  }, {
    "heading": "3 Fluency boost learning",
    "text": "Conventional seq2seq models for GEC learns model parameters only from original errorcorrected sentence pairs. However, such errorcorrected sentence pairs are not sufficiently available. As a result, many neural GEC models are not very well generalized.\nFortunately, neural GEC is different from NMT. For neural GEC, its goal is improving a sentence’s fluency2 without changing its original meaning; thus, any sentence pair that satisfies this condition (we call it fluency boost condition) can be used as a training instance.\nIn this paper, we define f(x) as the fluency score of a sentence x:\nf(x) = 1\n1 +H(x) (3)\nH(x) = − ∑|x|\ni=1 logP (xi|x<i) |x| (4)\nwhere P (xi|x<i) is the probability of xi given context x<i, computed by a language model, and |x| is the length of sentence x. H(x) is actually the cross entropy of the sentence x, whose range is [0,+∞). Accordingly, the range of f(x) is (0, 1].\nThe core idea of fluency boost learning is to generate fluency boost sentence pairs that satisfy the fluency boost condition during training, as Figure 2(a) illustrates, so that these pairs can further help model learning.\nIn this section, we present three fluency boost learning strategies: back-boost, self-boost, and\n2Fluency of a sentence in this paper refers to how likely the sentence is written by a native speaker. In other words, if a sentence is very likely to be written by a native speaker, it should be regarded highly fluent.\ndual-boost that generate fluency boost sentence pairs in different ways, as illustrated in Figure 3."
  }, {
    "heading": "3.1 Back-boost learning",
    "text": "Back-boost learning borrows the idea from back translation (Sennrich et al., 2016) in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors. Since the less fluent sentences are generated by the error generation seq2seq model trained with error-corrected data, they usually do not change the original sentence’s meaning; thus, they can be paired with their correct sentences, establishing fluency boost sentence pairs that can be used as training instances for error correction models, as Figure 3(a) shows.\nSpecifically, we first train a seq2seq error generation model Θgen with S̃∗ which is identical to S∗ except that the source sentence and the target sentence are interchanged. Then, we use the model Θgen to predict n-best outputs xo1, · · · , xon given a correct sentence xc. Given the fluency boost condition, we compare the fluency of each output xok (where 1 ≤ k ≤ n) to that of its correct sentence xc. If an output sentence’s fluency score is much lower than its correct sentence, we call it a disfluency candidate of xc.\nTo formalize this process, we first define Yn(x;Θ) to denote the n-best outputs predicted by model Θ given the input x. Then, disfluency candidates of a correct sentence xc can be derived:\nDback(xc) = {xok |xok ∈ Yn(xc;Θgen) ∧ f(xc)\nf(xok) ≥ σ}\n(5)\nAlgorithm 1 Back-boost learning 1: Train error generation model Θgen with S̃∗; 2: for each sentence pair (xr,xc) ∈ S do 3: Compute Dback(xc) according to Eq (5); 4: end for 5: for each training epoch t do 6: S ′ ← ∅; 7: Derive a subset St by randomly sampling |S∗| ele-\nments from S; 8: for each (xr,xc) ∈ St do 9: Establish a fluency boost pair (x′,xc) by ran-\ndomly sampling x′ ∈ Dback(xc); 10: S ′ ← S ′ ∪ {(x′,xc)}; 11: end for 12: Update error correction model Θcrt with S∗ ∪ S ′; 13: end for\nwhere Dback(xc) denotes the disfluency candidate set for xc in back-boost learning. σ is a threshold to determine if xok is less fluent than xc and it should be slightly larger3 than 1.0, which helps filter out sentence pairs with unnecessary edits (e.g., I like this book. → I like the book.).\nIn the subsequent training epochs, the error correction model will not only learn from the original error-corrected sentence pairs (xr,xc), but also learn from fluency boost sentence pairs (xok ,xc) where xok is a sample of Dback(xc).\nWe summarize this process in Algorithm 1 where S∗ is the set of original error-corrected sentence pairs, and S can be tentatively considered identical to S∗ when there is no additional native data to help model training (see Section 3.4). Note that we constrain the size of St not to exceed |S∗| (the 7th line in Algorithm 1) to avoid that too many fluency boost pairs overwhelm the effects of the original error-corrected pairs on model learning."
  }, {
    "heading": "3.2 Self-boost learning",
    "text": "In contrast to back-boost learning whose core idea is originally from NMT, self-boost learning is original, which is specially devised for neural GEC. The idea of self-boost learning is illustrated by Figure 3(b) and was already briefly introduced in Section 1 and Figure 2(a). Unlike back-boost learning in which an error generation seq2seq model is trained to generate disfluency candidates, self-boost learning allows the error correction model to generate the candidates by itself. Since the disfluency candidates generated by the error correction seq2seq model trained with error-corrected data rarely change the input\n3In this paper, we set σ = 1.05 since the corrected sentence in our training data improves its corresponding raw sentence about 5% fluency on average.\nAlgorithm 2 Self-boost learning 1: for each sentence pair (xr,xc) ∈ S do 2: Dself (xc)← ∅; 3: end for 4: S ′ ← ∅ 5: for each training epoch t do 6: Update error correction model Θcrt with S∗ ∪ S ′; 7: S ′ ← ∅ 8: Derive a subset St by randomly sampling |S∗| ele-\nments from S; 9: for each (xr,xc) ∈ St do\n10: Update Dself (xc) according to Eq (6); 11: Establish a fluency boost pair (x′,xc) by ran-\ndomly sampling x′ ∈ Dself (xc); 12: S ′ ← S ′ ∪ {(x′,xc)}; 13: end for 14: end for\nsentence’s meaning; thus, they can be used to establish fluency boost sentence pairs.\nFor self-boost learning, given an error corrected pair (xr,xc), an error correction model Θcrt first predicts n-best outputs xo1 , · · · ,xon for the raw sentence xr. Among the n-best outputs, any output that is not identical to xc can be considered as an error prediction. Instead of treating the error predictions useless, self-boost learning fully exploits them. Specifically, if an error prediction xok is much less fluent than that of its correct sentence xc, it will be added to xc’s disfluency candidate set Dself (xc), as Eq (6) shows:\nDself (xc) = Dself (xc) ∪ {xok |xok ∈ Yn(xr;Θcrt) ∧ f(xc)\nf(xok) ≥ σ}\n(6)\nIn contrast to back-boost learning, self-boost generates disfluency candidates from a different perspective – by editing the raw sentence xr rather than the correct sentence xc. It is also noteworthy that Dself (xc) is incrementally expanded because the error correction model Θcrt is dynamically updated, as shown in Algorithm 2."
  }, {
    "heading": "3.3 Dual-boost learning",
    "text": "As introduced above, back- and self-boost learning generate disfluency candidates from different perspectives to create more fluency boost sentence pairs to benefit training the error correction model. Intuitively, the more diverse disfluency candidates generated, the more helpful for training an error correction model. Inspired by He et al. (2016) and Zhang et al. (2018), we propose a dual-boost learning strategy, combining both back- and selfboost’s perspectives to generate disfluency candidates.\nAlgorithm 3 Dual-boost learning 1: for each (xr,xc) ∈ S do 2: Ddual(xc)← ∅; 3: end for 4: S ′ ← ∅; S ′′ ← ∅; 5: for each training epoch t do 6: Update error correction model Θcrt with S∗ ∪ S ′; 7: Update error generation model Θgen with S̃∗ ∪ S ′′; 8: S ′ ← ∅; S ′′ ← ∅; 9: Derive a subset St by randomly sampling |S∗| ele-\nments from S; 10: for each (xr,xc) ∈ St do 11: Update Ddual(xc) according to Eq (7); 12: Establish a fluency boost pair (x′,xc) by ran-\ndomly sampling x′ ∈ Ddual(xc); 13: S ′ ← S ′ ∪ {(x′,xc)}; 14: Establish a reversed fluency boost pair (xc,x′′)\nby randomly sampling x′′ ∈ Ddual(xc); 15: S ′′ ← S ′′ ∪ {(xc,x′′)}; 16: end for 17: end for\nAs Figure 3(c) shows, disfluency candidates in dual-boost learning are from both the error generation model and the error correction model :\nDdual(xc) = Ddual(xc) ∪ {xok |xok ∈ Yn(xr;Θcrt) ∪ Yn(xc;Θgen) ∧ f(xc)\nf(xok) ≥ σ}\n(7)\nMoreover, the error correction model and the error generation model are dual and both of them are dynamically updated, which improves each other: the disfluency candidates produced by error generation model can benefit training the error correction model, while the disfluency candidates created by error correction model can be used as training data for the error generation model. We summarize this learning approach in Algorithm 3."
  }, {
    "heading": "3.4 Fluency boost learning with large-scale native data",
    "text": "Our proposed fluency boost learning strategies can be easily extended to utilize the huge volume of native data which is proven to be useful for GEC.\nAs discussed in Section 3.1, when there is no additional native data, S in Algorithm 1–3 is identical to S∗. In the case where additional native data is available to help model learning, S becomes: S = S∗ ∪ C where C = {(xc,xc)} denotes the set of selfcopied sentence pairs from native data."
  }, {
    "heading": "4 Fluency boost inference",
    "text": "As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference\nwhich does only single-round inference. Fortunately, neural GEC is different from NMT: its source and target language are the same. The characteristic allows us to edit a sentence more than once through multi-round model inference, which motivates our fluency boost inference. As Figure 2(b) shows, fluency boost inference allows a sentence to be incrementally edited through multiround seq2seq inference as long as the sentence’s fluency can be improved. Specifically, an error correction seq2seq model first takes a raw sentence xr as an input and outputs a hypothesis xo1 . Instead of regarding xo1 as the final prediction, fluency boost inference will then take xo1 as the input to generate the next output xo2 . The process will not terminate unless xot does not improve xot−1 in terms of fluency."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Dataset and evaluation",
    "text": "As previous studies (Ji et al., 2017), we use the public Lang-8 Corpus (Mizumoto et al., 2011; Tajiri et al., 2012), Cambridge Learner Corpus (CLC) (Nicholls, 2003) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. Table 1 shows the stats of the datasets. In addition, we also collect 2,865,639 non-public errorcorrected sentence pairs from Lang-8.com. The native data we use for fluency boost learning is English Wikipedia that contains 61,677,453 sentences.\nWe use CoNLL-2014 shared task dataset with original annotations (Ng et al., 2014), which contains 1,312 sentences, as our main test set for evaluation. We use MaxMatch (M2) precision, recall andF0.5 (Dahlmeier and Ng, 2012b) as our evaluation metrics. As previous studies, we use CoNLL2013 test data as our development set."
  }, {
    "heading": "5.2 Experimental setting",
    "text": "We set up experiments in order to answer the following questions:\n• Whether is fluency boost learning mechanism helpful for training the error correction model, and which of the strategies (back-boost, selfboost, dual-boost) is the most effective?\n• Whether does our fluency boost inference improve normal seq2seq inference for GEC?\n• Whether can our approach improve neural GEC to achieve state-of-the-art results?\nThe training details for our seq2seq error correction model and error generation model are as follows: the encoder of the seq2seq models is a 2-layer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism (Luong et al., 2015). Both the dimensionality of word embeddings and the hidden size of GRU cells are 500. The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 respectively. The models’ parameters are uniformly initialized in [-0.1,0.1]. We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128. Dropout is applied to non-recurrent connections at a ratio of 0.15. For fluency boost learning, we generate disfluency candidates from 10-best outputs. During model inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN language model (Mikolov et al., 2010) through shallow fusion (Gülçehre et al., 2015) with weight β = 0.15. The RNN language model is trained from the native data mentioned in Section 5.1, which is also used for computing fluency score in Eq (3). UNK tokens are replaced with the source token with the highest attention weight.\nWe resolve spelling errors with a public spell checker4 as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.\n4https://azure.microsoft.com/en-us/services/cognitiveservices/spell-check/"
  }, {
    "heading": "5.3 Experimental results",
    "text": ""
  }, {
    "heading": "5.3.1 Effectiveness of fluency boost learning",
    "text": "Table 2 compares the performance of seq2seq error correction models with different learning and inference methods. By comparing by row, one can observe that our fluency boost learning approaches improve the performance over normal seq2seq learning, especially on the recall metric, since the fluency boost learning approaches generate a variety of grammatically incorrect sentences, allowing the error correction model to learn to correct much more sentences than the conventional learning strategy. Among the proposed three fluency boost learning strategies, dual-boost achieves the best result in most cases because it produces more diverse incorrect sentences (average |Ddual| ≈ 9.43) than either back-boost (avg |Dback| ≈ 1.90) or self-boost learning (avg |Dself | ≈ 8.10). With introducing large amounts of native text data, the performance of all the fluency boost learning approaches gets improved. One reason is that our learning approaches produce more error-corrected sentence pairs to let the model be better generalized. In addition, the huge volume of native data benefits the decoder to learn better to generate a fluent and error-free sentence.\nWe test the effect of hyper-parameter σ in Eq (5–7) on fluency boost learning and show the result in Table 3. When σ is slightly larger than 1.0 (e.g., σ = 1.05), the model achieves the best performance because it effectively avoids generating sentence pairs with unnecessary or undesirable edits that affect the performance, as we discussed in Section 3.1. When σ continues increasing, the disfluency candidate set |Ddual| drastically decreases, making the dual-boost learning gradually degrade to normal seq2seq learning.\nTable 4 shows some examples of disfluency\ncandidates5 generated in dual-boost learning given a correct sentence in the native data. It is clear that our approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning."
  }, {
    "heading": "5.3.2 Effectiveness of fluency boost inference",
    "text": "The effectiveness of various inference approaches can be observed by comparing the results in Table 2 by column. Compared to the normal seq2seq inference and seq2seq (+LM) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on F0.5 respectively, which is a significant6 improvement, demonstrating multi-round edits by fluency boost inference is effective.\nTake our best system (the last row in Table 2) as an example, among 1,312 sentences in the CoNLL-2014 dataset, seq2seq inference with shallow fusion LM edits 566 sentences. In contrast, fluency boost inference additionally edits 23 sentences during the second round inference, improving F0.5 from 52.59 to 52.72."
  }, {
    "heading": "5.3.3 Towards the state-of-the-art for GEC",
    "text": "Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result.\nWe first compare our best models – dual-boost learning (+native) with fluency boost inference and shallow fusion LM – to top-performing GEC systems evaluated on CoNLL-2014 dataset:\n5We give more details about disfluency candidates, including error type proportion, in the supplementary notes.\n6p < 0.0005 according to Wilcoxon Signed-Rank Test.\n• CAMB14, CAMB16SMT, CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University.\n• AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU.\n• CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach.\n• NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers).\n• Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation.\n• Nested-seq2seq: a nested attention neural hybrid seq2seq model (Ji et al., 2017).\n• Adapt-seq2seq: a seq2seq model adapted to incorporate edit operations (Schmaltz et al., 2017).\nTable 5 shows the evaluation results on the CoNLL-2014 dataset. Without using the nonpublic training data from Lang-8.com, our sin-\ngle model obtains 50.04 F0.5, larlgely outperforming the other seq2seq models and only inferior to CAMB17 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16’s framework); thus, they are ensemble models. When we build our approach on top of AMU16 (i.e., we take AMU16’s outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset.\nMoreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning.\nTable 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets."
  }, {
    "heading": "6 Related work",
    "text": "Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011)\n7The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5=54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5=56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models.\n8The recently proposed SMT-NMT hybrid system (Grundkiewicz and Junczys-Dowmunt, 2018), which is tuned towards GLEU on JFLEG Dev set, reports a higher result (GLEU=61.50 on JFLEG test set).\nor MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017).\nIn addition to the studies on GEC, there is also much research on grammatical error detection\n(Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions."
  }, {
    "heading": "7 Conclusion",
    "text": "We propose a novel fluency boost learning and inference mechanism to overcome the limitations of previous neural GEC models. Our proposed fluency boost learning fully exploits both errorcorrected data and native data, largely improving the performance over normal seq2seq learning, while fluency boost inference utilizes the characteristic of GEC to incrementally improve a sentence’s fluency through multi-round inference. The powerful learning and inference mechanism enables the seq2seq models to achieve state-ofthe-art results on both CoNLL-2014 and JFLEG benchmark datasets."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank all the anonymous reviewers for their professional and constructive comments. We also thank Shujie Liu for his insightful discussions and suggestions."
  }],
  "year": 2018,
  "references": [{
    "title": "Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems",
    "authors": ["Hiroki Asano", "Tomoya Mizumoto", "Kentaro Inui."],
    "venue": "IJCNLP.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "CoRR, abs/1409.0473.",
    "year": 2014
  }, {
    "title": "Automated grammar correction using hierarchical phrase-based statistical machine translation",
    "authors": ["Bibek Behera", "Pushpak Bhattacharyya."],
    "venue": "IJCNLP.",
    "year": 2013
  }, {
    "title": "Correcting esl errors using phrasal smt techniques",
    "authors": ["Chris Brockett", "William B Dolan", "Michael Gamon."],
    "venue": "COLING/ACL.",
    "year": 2006
  }, {
    "title": "Automatic annotation and evaluation of error types for grammatical error correction",
    "authors": ["Christopher Bryant", "Mariano Felice", "E Briscoe."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Detection of grammatical errors involving prepositions",
    "authors": ["Martin Chodorow", "Joel R Tetreault", "Na-Rae Han."],
    "venue": "ACL-SIGSEM workshop on prepositions.",
    "year": 2007
  }, {
    "title": "Adapting grammatical error correction based on the native language of writers with neural network joint models",
    "authors": ["Shamil Chollampatt", "Duc Tam Hoang", "Hwee Tou Ng."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Connecting the dots: Towards human-level grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "Workshop on Innovative Use of NLP for Building Educational Applications.",
    "year": 2017
  }, {
    "title": "A multilayer convolutional encoder-decoder neural network for grammatical error correction",
    "authors": ["Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "arXiv preprint arXiv:1801.08831.",
    "year": 2018
  }, {
    "title": "Neural network translation models for grammatical error correction",
    "authors": ["Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng."],
    "venue": "arXiv preprint arXiv:1606.00189.",
    "year": 2016
  }, {
    "title": "Correcting semantic collocation errors with l1-induced paraphrases",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "EMNLP.",
    "year": 2011
  }, {
    "title": "A beamsearch decoder for grammatical error correction",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "EMNLP/CoNLL.",
    "year": 2012
  }, {
    "title": "Better evaluation for grammatical error correction",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "NAACL.",
    "year": 2012
  }, {
    "title": "Better evaluation for grammatical error correction",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng."],
    "venue": "NAACL.",
    "year": 2012
  }, {
    "title": "Building a large annotated corpus of learner english: The nus corpus of learner english",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu."],
    "venue": "Workshop on innovative use of NLP for building educational applications.",
    "year": 2013
  }, {
    "title": "Helping our own: The hoo 2011 pilot shared task",
    "authors": ["Robert Dale", "Adam Kilgarriff."],
    "venue": "European Workshop on Natural Language Generation.",
    "year": 2011
  }, {
    "title": "A classifier-based approach to preposition and determiner error correction in l2 english",
    "authors": ["Rachele De Felice", "Stephen G Pulman."],
    "venue": "COLING.",
    "year": 2008
  }, {
    "title": "Generating artificial errors for grammatical error correction",
    "authors": ["Mariano Felice", "Zheng Yuan."],
    "venue": "Student Research Workshop at EACL.",
    "year": 2014
  }, {
    "title": "Grammatical error correction using hybrid systems and type filtering",
    "authors": ["Mariano Felice", "Zheng Yuan", "Øistein E Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar."],
    "venue": "CoNLL (Shared Task).",
    "year": 2014
  }, {
    "title": "Generrate: generating errors for use in grammatical error detection",
    "authors": ["Jennifer Foster", "Øistein E Andersen."],
    "venue": "Workshop on innovative use of nlp for building educational applications.",
    "year": 2009
  }, {
    "title": "Near human-level performance in grammatical error correction with hybrid machine translation",
    "authors": ["Roman Grundkiewicz", "Marcin Junczys-Dowmunt."],
    "venue": "arXiv preprint arXiv:1804.05945.",
    "year": 2018
  }, {
    "title": "On using monolingual corpora in neural machine translation. CoRR, abs/1503.03535",
    "authors": ["Çaglar Gülçehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loı̈c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"],
    "year": 2015
  }, {
    "title": "Using an error-annotated learner corpus to develop an esl/efl error correction system",
    "authors": ["Na-Rae Han", "Joel R Tetreault", "Soo-Hwa Lee", "JinYoung Ha."],
    "venue": "LREC.",
    "year": 2010
  }, {
    "title": "Dual learning for machine translation",
    "authors": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tieyan Liu", "Wei-Ying Ma."],
    "venue": "NIPS.",
    "year": 2016
  }, {
    "title": "Exploiting n-best hypotheses to improve an smt approach to grammatical error correction",
    "authors": ["Duc Tam Hoang", "Shamil Chollampatt", "Hwee Tou Ng."],
    "venue": "IJCAI.",
    "year": 2016
  }, {
    "title": "A nested attention neural hybrid model for grammatical error correction",
    "authors": ["Jianshu Ji", "Qinlong Wang", "Kristina Toutanova", "Yongen Gong", "Steven Truong", "Jianfeng Gao."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "The amu system in the conll-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation",
    "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."],
    "venue": "CoNLL (Shared Task).",
    "year": 2014
  }, {
    "title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction",
    "authors": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."],
    "venue": "arXiv preprint arXiv:1605.06353.",
    "year": 2016
  }, {
    "title": "Grammatical error detection using error-and grammaticality-specific word embeddings",
    "authors": ["Masahiro Kaneko", "Yuya Sakaizawa", "Mamoru Komachi."],
    "venue": "IJCNLP.",
    "year": 2017
  }, {
    "title": "Automated grammatical error detection for language learners",
    "authors": ["Claudia Leacock", "Martin Chodorow", "Michael Gamon", "Joel Tetreault."],
    "venue": "Synthesis lectures on human language technologies, 3(1):1–134.",
    "year": 2010
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems",
    "authors": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow", "Alla Rozovskaya."],
    "venue": "ACL.",
    "year": 2011
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafit", "Lukas Burget", "Jan Cernock", "Sanjeev Khudanpur."],
    "venue": "INTERSPEECH.",
    "year": 2010
  }, {
    "title": "Mining revision log of language learning sns for automated japanese error correction of second language learners",
    "authors": ["Tomoya Mizumoto", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto."],
    "venue": "IJCNLP.",
    "year": 2011
  }, {
    "title": "Discriminative reranking for grammatical error correction with statistical machine translation",
    "authors": ["Tomoya Mizumoto", "Yuji Matsumoto."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Ground truth for grammatical error correction metrics",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault."],
    "venue": "ACL/IJCNLP.",
    "year": 2015
  }, {
    "title": "There’s no comparison: Referenceless evaluation metrics in grammatical error correction",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Jfleg: A fluency corpus and benchmark for grammatical error correction",
    "authors": ["Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault."],
    "venue": "arXiv preprint arXiv:1702.04066.",
    "year": 2017
  }, {
    "title": "The conll-2014 shared task on grammatical error correction",
    "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant."],
    "venue": "CoNLL (Shared Task).",
    "year": 2014
  }, {
    "title": "The cambridge learner corpus: Error coding and analysis for lexicography and elt",
    "authors": ["Diane Nicholls."],
    "venue": "Corpus Linguistics 2003 conference.",
    "year": 2003
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "ACL.",
    "year": 2002
  }, {
    "title": "Artificial error generation with machine translation and syntactic patterns",
    "authors": ["Marek Rei", "Mariano Felice", "Zheng Yuan", "Ted Briscoe."],
    "venue": "arXiv preprint arXiv:1707.05236.",
    "year": 2017
  }, {
    "title": "Compositional sequence labeling models for error detection in learner writing",
    "authors": ["Marek Rei", "Helen Yannakoudakis."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "The illinoiscolumbia system in the conll-2014 shared task",
    "authors": ["Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash."],
    "venue": "CoNLL (Shared Task).",
    "year": 2014
  }, {
    "title": "Training paradigms for correcting errors in grammar and usage",
    "authors": ["Alla Rozovskaya", "Dan Roth."],
    "venue": "NAACL.",
    "year": 2010
  }, {
    "title": "Algorithm selection and model adaptation for esl correction tasks",
    "authors": ["Alla Rozovskaya", "Dan Roth."],
    "venue": "ACL.",
    "year": 2011
  }, {
    "title": "Grammatical error correction: Machine translation and classifiers",
    "authors": ["Alla Rozovskaya", "Dan Roth."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "The ui system in the hoo 2012 shared task on error correction",
    "authors": ["Alla Rozovskaya", "Mark Sammons", "Roth Dan."],
    "venue": "Workshop on Building Educational Applications Using NLP.",
    "year": 2012
  }, {
    "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
    "authors": ["Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault."],
    "venue": "Transactions of the Association of Computational Linguistics, 4(1):169–182.",
    "year": 2016
  }, {
    "title": "Grammatical error correction with neural reinforcement learning",
    "authors": ["Keisuke Sakaguchi", "Matt Post", "Benjamin Van Durme."],
    "venue": "IJCNLP.",
    "year": 2017
  }, {
    "title": "Adapting sequence models for sentence correction",
    "authors": ["Allen Schmaltz", "Yoon Kim", "Alexander Rush", "Stuart Shieber."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Improving neural machine translation models with monolingual data",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "System combination for grammatical error correction",
    "authors": ["Raymond Hendy Susanto", "Peter Phandi", "Hwee Tou Ng."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "CoRR, abs/1409.3215.",
    "year": 2014
  }, {
    "title": "Tense and aspect error correction for esl learners using global context",
    "authors": ["Toshikazu Tajiri", "Mamoru Komachi", "Yuji Matsumoto."],
    "venue": "ACL.",
    "year": 2012
  }, {
    "title": "Using parse features for preposition selection and error detection",
    "authors": ["Joel Tetreault", "Jennifer Foster", "Martin Chodorow."],
    "venue": "ACL.",
    "year": 2010
  }, {
    "title": "Rethinking grammatical error annotation and evaluation with the amazon mechanical turk",
    "authors": ["Joel R Tetreault", "Elena Filatova", "Martin Chodorow."],
    "venue": "Workshop on Innovative Use of NLP for Building Educational Applications.",
    "year": 2010
  }, {
    "title": "Deliberation networks: Sequence generation beyond one-pass decoding",
    "authors": ["Yingce Xia", "Fei Tian", "Lijun Wu", "Jianxin Lin", "Tao Qin", "Nenghai Yu", "Tie-Yan Liu."],
    "venue": "NIPS.",
    "year": 2017
  }, {
    "title": "Neural language correction with character-based attention",
    "authors": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y Ng."],
    "venue": "arXiv preprint arXiv:1603.09727.",
    "year": 2016
  }, {
    "title": "Neural sequencelabelling models for grammatical error correction",
    "authors": ["Helen Yannakoudakis", "Marek Rei", "Øistein E Andersen", "Zheng Yuan."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Naist at 2013 conll grammatical error correction shared task",
    "authors": ["Ippei Yoshimoto", "Tomoya Kose", "Kensuke Mitsuzawa", "Keisuke Sakaguchi", "Tomoya Mizumoto", "Yuta Hayashibe", "Mamoru Komachi", "Yuji Matsumoto."],
    "venue": "CoNLL (Shared Task).",
    "year": 2013
  }, {
    "title": "Grammatical error correction using neural machine translation",
    "authors": ["Zheng Yuan", "Ted Briscoe."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Candidate re-ranking for smt-based grammatical error correction",
    "authors": ["Zheng Yuan", "Ted Briscoe", "Mariano Felice", "Zheng Yuan", "Ted Briscoe", "Mariano Felice."],
    "venue": "Workshop on Innovative Use of NLP for Building Educational Applications.",
    "year": 2016
  }, {
    "title": "Constrained grammatical error correction using statistical machine translation",
    "authors": ["Zheng Yuan", "Mariano Felice."],
    "venue": "CoNLL (Shared Task).",
    "year": 2013
  }, {
    "title": "Joint training for neural machine translation models with monolingual data",
    "authors": ["Zhirui Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Enhong Chen."],
    "venue": "arXiv preprint arXiv:1803.00353.",
    "year": 2018
  }],
  "id": "SP:f6d06993e003fa6fec5bf630efded9e4fd90a030",
  "authors": [{
    "name": "Tao Ge Furu",
    "affiliations": []
  }, {
    "name": "Wei Ming Zhou",
    "affiliations": []
  }],
  "abstractText": "Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence’s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally through multi-round seq2seq inference until the sentence’s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL2014 and JFLEG benchmark datasets.",
  "title": "Fluency Boost Learning and Inference for Neural Grammatical Error Correction"
}