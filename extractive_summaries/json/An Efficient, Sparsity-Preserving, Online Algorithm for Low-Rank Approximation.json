{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Low-rank approximation is an essential data processing technique for understanding large or noisy data in diverse areas including data compression, image and pattern recognition, signal processing, compressed sensing, latent semantic indexing, anomaly detection, and recommendation systems. Recent machine learning applications include training neural networks (Jaderberg et al., 2014; Kirkpatrick et al., 2017), second order online learning (Luo et al., 2016), representation learning (Wang et al., 2016), and reinforcement learning (Ghavamzadeh et al., 2010).\n*Equal contribution 1University of California, Berkeley. Correspondence to: David Anderson <davidanderson@berkeley.edu>, Ming Gu <mgu@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nAdditionally, a recent trend in machine learning is to include an approximation of second order information for better accuracy and faster convergence (Krummenacher et al., 2016).\nIn this work, we introduce a novel low-rank approximation algorithm called Spectrum-Revealing LU (SRLU) that can be efficiently computed and updated. Furthermore, SRLU preserves sparsity and can identify important data variables and observations. Our algorithm works on any data matrix, and achieves an approximation accuracy that only differs from the accuracy of the best approximation possible for any given rank by a constant factor.1\nThe major innovation in SRLU is the efficient calculation of a truncated LU factorization of the form\nΠ1AΠ T 2 = ( k m − k k L11 m − k L21 In−k ) ( k n − k U11 U12 S ) ≈ ( L11 L21 )( U11 U12\n) def = L̂Û,\nwhere Π1 and Π2 are judiciously chosen permutation matrices. The LU factorization is unstable, and in practice is implemented by pivoting (interchanging) rows during factorization, i.e. choosing permutation matrix Π1. For the truncated LU factorization to have any significance, nevertheless, complete pivoting (interchanging rows and columns) is necessary to guarantee that the factors L̂ and Û are well-defined and that their product accurately represents the original data. Previously, complete pivoting was impractical as a matrix factorization technique because it requires accessing the entire data matrix at every iteration, but SRLU efficiently achieves complete pivoting through randomization and includes a deterministic follow-up procedure to ensure a hight quality low-rank matrix approximation, as supported by rigorous theory and numeric experiments.\n1The truncated SVD is known to provide the best low-rank matrix approximation, but it is rarely used for large scale practical data analysis. See a brief discussion of the SVD in supplemental material."
  }, {
    "heading": "1.1. Background on the LU factorization",
    "text": "Algorithm 1 presents a basic implementation of the LU factorization, where the result is stored in place such that the upper triangular part of A becomes U and the strictly lower triangular part becomes the strictly lower part of L, with the diagonal of L implicitly known to contain all ones. LU with partial pivoting finds the largest entry in the ith column from row i tom and pivots the row with that entry to the ith row. LU with complete pivoting finds the largest entry in the submatrix Ai+1:m,i+1:n and pivots that entry to Ai,i. It is generally known and accepted that partial pivoting is sufficient for general, real-world data matrices in the context of linear equation solving.\nAlgorithm 1 The LU factorization\n1: Inputs: Data matrix A ∈ Rm×n 2: for i = 1, 2, · · · ,min(m,n) do 3: Perform row and/or column pivots 4: for k = i+ 1, · · · ,m do 5: Ak,i = Ak,i/Ai,i 6: end for 7: Ai+1:m,i+1:n −= Ai+1:m,1:i ·A1:i,i+1:n 8: end for\nAlgorithm 2 Crout LU\n1: Inputs: Data matrix A ∈ Rm×n, block size b 2: for j = 0, b, 2b, · · · ,min(m,n)/b− 1 do 3: Perform column pivots 4: Aj+1:m,j+1:j+b− = 5: Aj+1:m,1:j ·A1:j,j+1:j+b. 6: Apply Algorithm 1 on Aj+1:m,j+1:j+b 7: Apply the row pivots to other columns of A 8: Aj+1:j+b,j+b+1:n −= 9: Aj+1:j+b,1:j ·A1:j,j+b+1:n\n10: end for\nLine 7 of Algorithm 1 is known as the Schur update. Given a sparse input, this is the only step of the LU factorization that causes fill. As the algorithm progresses, fill will compound and may become dense, but the LU factorization, and truncated LU in particular, generally preserves some, if not most, of the sparsity of a sparse input. A numeric illustration is presented below.\nThere are many variations of the LU factorization. In Algorithm 2 the Crout version of LU is presented in block form. The column pivoting entails selecting the next b columns so that the in-place LU step is performed on a non-singular matrix (provided the remaining entries are not all zero). Note that the matrix multiplication steps are the bottleneck of this algorithm, requiring O(mnb) operations each in general.\nThe LU factorization has been studied extensively since long before the invention of computers, with notable results from many mathematicians, including Gauss, Turing, and Wilkinson. Current research on LU factorizations includes communication-avoiding implementations, such as tournament pivoting (Khabou et al., 2013), sparse implementations (Grigori et al., 2007), and new computation of preconditioners (Chow & Patel, 2015). A randomized approach to efficiently compute the LU factorization with complete pivoting recently appeared in (Melgaard & Gu, 2015). These results are all in the context of linear equation solving, either directly or indirectly through an incomplete factorization used to precondition an iterative method. This work repurposes the LU factorization to create a novel efficient and effective low-rank approximation algorithm using modern randomization technology."
  }, {
    "heading": "2. Previous Work",
    "text": ""
  }, {
    "heading": "2.1. Low-Rank Matrix Approximation (LRMA)",
    "text": "Previous work on low-rank data approximation includes the Interpolative Decomposition (ID) (Cheng et al., 2005), the truncated QR with column pivoting factorization (Gu & Eisenstat, 1996), and other deterministic column selection algorithms, such as in (Batson et al., 2012).\nRandomized algorithms have grown in popularity in recent years because of their ability to efficiently process large data matrices and because they can be supported with rigorous theory. Randomized low-rank approximation algorithms generally fall into one of two categories: sampling algorithms and black box algorithms. Sampling algorithms form data approximations from a random selection of rows and/or columns of the data. Examples include (Deshpande et al., 2006; Deshpande & Vempala, 2006; Frieze et al., 2004; Mahoney & Drineas, 2009). (Drineas et al., 2008) showed that for a given approximate rank k, a randomly drawn subset C of c = O ( k log(k) −2 log (1/δ)\n) columns of the data, a randomly drawn subset R of r = O ( c log(c) −2 log (1/δ) ) rows of the data, and setting U = C†AR†, then the matrix approximation error ‖A−CUR‖F is at most a factor of 1+ from the optimal rank k approximation with probability at least 1− δ. Black box algorithms typically approximate a data matrix in the form\nA ≈ QTQA,\nwhere Q is an orthonormal basis of the random projection (usually using SVD, QR, or ID). The result of (Johnson & Lindenstrauss, 1984) provided the theoretical groundwork for these algorithms, which have been extensively studied (Clarkson & Woodruff, 2012; Halko et al., 2011; Martinsson et al., 2006; Papadimitriou et al., 2000; Sarlos, 2006; Woolfe et al., 2008; Liberty et al., 2007; Gu, 2015). Note\nthat the projection of an m-by-n data matrix is of size mby-`, for some oversampling parameter ` ≥ k, and k is the target rank. Thus the computational challenge is the orthogonalization of the projection (the random projection can be applied quickly, as described in these works). A previous result on randomized LU factorizations for low-rank approximation was presented in (Aizenbud et al., 2016), but is uncompetitive in terms of theoretical results and computational performance with the work presented here.\nFor both sampling and black box algorithms the tuning parameter cannot be arbitrarily small, as the methods become meaningless if the number of rows and columns sampled (in the case of sampling algorithms) or the size of the random projection (in the case of black box algorithms) surpasses the size of the data. A common practice is ≈ 12 ."
  }, {
    "heading": "2.2. Guaranteeing Quality",
    "text": "Rank-revealing algorithms (Chan, 1987) are LRMA algorithms that guarantee the approximation is of high quality by also capturing the rank of the data within a tolerance (see supplementary materials for definitions). These methods, nevertheless, attempt to build an important submatrix of the data, and do not directly compute a low-rank approximation. Furthermore, they do not attempt to capture all positive singular values of the data. (Miranian & Gu, 2003) introduced a new type of high-quality LRMA algorithms that can capture all singular values of a data matrix within a tolerance, but requires extra computation to bound approximations of the left and right null spaces of the data matrix. Rank-revealing algorithms in general are designed around a definition that is not specifically appropriate for LRMA.\nA key advancement of this work is a new definition of high quality low-rank approximation:\nDefinition 1 A rank-k truncated LU factorization is spectrum-revealing if∥∥∥A− L̂Û∥∥∥\n2 ≤ q1(k,m, n)σk+1 (A)\nand\nσj\n( L̂Û ) ≥ σj (A)\nq2(k,m, n)\nfor 1 ≤ j ≤ k and q1(k,m, n) and q2(k,m, n) are bounded by a low degree polynomial in k, m, and n.\nDefinition 1 has precisely what we desire in an LRMA, and no additional requirements. The constants, q1(k,m, n) and q2(k,m, n) are at least 1 for any rank-k approximation by (Eckart & Young, 1936). This work shows theoretically and numerically that our algorithm, SRLU, is spectrumrevealing in that it always finds such q1 and q2, often with q1, q2 = O(1) in practice.\nAlgorithm 3 TRLUCP\n1: Inputs: Data matrix A ∈ Rm×n, target rank k, block size b, oversampling parameter p ≥ b, random Gaussian matrix Ω ∈ Rp×m, L̂ and Û are initially 0 matrices 2: Calculate random projection R = ΩA 3: for j = 0, b, 2b, · · · , k − b do 4: Perform column selection algorithm on R and swap columns of A 5: Update block column of L̂ 6: Perform block LU with partial row pivoting and swap rows of A 7: Update block row of Û 8: Update R 9: end for"
  }, {
    "heading": "2.3. Low-Rank and Other Approximations in Machine Learning",
    "text": "Low-rank and other approximation algorithms have appeared recently in a variety of machine learning applications. In (Krummenacher et al., 2016), randomized lowrank approximation is applied directly to the adaptive optimization algorithm ADAGRAD to incorporate variable dependence during optimization to approximate the full matrix version of ADAGRAD with a significantly reduced computational complexity. In (Kirkpatrick et al., 2017), a diagonal approximation of the posterior distribution of previous data is utilized to alleviate catastrophic forgetting."
  }, {
    "heading": "3. Main Contribution: Spectrum-Revealing",
    "text": "LU (SRLU)\nOur algorithm for computing SRLU is composed of two subroutines: partially factoring the data matrix with randomized complete pivoting (TRLUCP) and performing swaps to improve the quality of the approximation (SRP). The first provides an efficient algorithm for computing a truncated LU factorization, whereas the second ensures the resulting approximation is provably reliable."
  }, {
    "heading": "3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP)",
    "text": "Intuitively, TRLUCP performs deterministic LU with partial row pivoting for some initial data with permuted columns. TRLUCP uses a random projection of the Schur complement to cheaply find and move forward columns that are more likely to be representative of the data. To accomplish this, Algorithm 3 performs an iteration of block LU factorization in a careful order that resembles Crout LU reduction. The ordering is reasoned as follows: LU with partial row pivoting cannot be performed until the needed\ncolumns are selected, and so column selection must first occur at each iteration. Once a block column is selected, a partial Schur update must be performed on that block column before proceeding. At this point, an iteration of block LU with partial row pivoting can be performed on the current block. Once the row pivoting is performed, a partial Schur update of the block of pivoted rows of U can be performed, which completes the factorization up to rank j+ b. Finally, the projection matrix R can be cheaply updated to prepare for the next iteration. Note that any column selection method may be used when picking column pivots from R, such as QR with column pivoting, LU with row pivoting, or even this algorithm can again be run on the subproblem of column selection of R. The flop count of TRLUCP is dominated by the three matrix multiplication steps (lines 2, 5, and 7). The total number of flops is\nF TRLUCP = 2pmn+ (m+ n)k2 +O (k(m+ n)) .\nNote the transparent constants, and, because matrix multiplication is the bottleneck, this algorithm can be implemented efficiently in terms of both computation as well as memory usage. Because the output of TRLUCP is only written once, the total number of memory writes is (m + n − k)k. Minimizing the number of data writes by only writing data once significantly improves efficiency because writing data is typically one of the slowest computational operations. Also worth consideration is the simplicity of the LU decomposition, which only involves three types of operations: matrix multiply, scaling, and pivoting. By contrast, state-of-the-art calculation of both the full and truncated SVD requires a more complex process of bidiagonalization. The projection R can be updated efficiently to become a random projection of the Schur complement for the next iteration. This calculation involves the current progress of the LU factorization and the random matrix Ω, and is described in detail in the appendix."
  }, {
    "heading": "3.2. Spectrum-Revealing Pivoting (SRP)",
    "text": "TRLUCP produces high-quality data approximations for almost all data matrices, despite the lack of theoretical guarantees, but can miss important rows or columns of the data. Next, we develop an efficient variant of the existing rank-revealing LU algorithms (Gu & Eisenstat, 1996; Miranian & Gu, 2003) to rapidly detect and, if necessary, correct any possible matrix approximation failures of TRLUCP.\nIntuitively, the quality of the factorization can be tested by searching for the next choice of pivot in the Schur complement if the factorization continued and determining if the addition of that element would significantly improve the approximation quality. If so, then the row and column with this element should be included in the approximation and another row and column should be excluded to maintain rank. Because TRLUCP does not provide an updated\nSchur complement, the largest element in the Schur complement can be approximated by finding the column of R with largest norm, performing a Schur update of that column, and then picking the largest element in that column. Let α be this element, and, without loss of generality, assume it is the first entry of the Schur complement. Denote:\nΠ1AΠ T 2 = L11`T 1 L31 I U11 u U13α sT12 s21 S22  . (1) Next, we must find the row and column that should be replaced if the row and column containing α are important. Note that the smallest entry of L11U11 may still lie in an important row and column, and so the largest element of the inverse should be examined instead. Thus we propose defining\nA11 def = ( L11 `T 1 )( U11 u α ) and testing\n‖A−111 ‖max ≤ f\n|α| (2)\nfor a tolerance parameter f > 1 that provides a control of accuracy versus the number of swaps needed. Should the test fail, the row and column containing α are swapped with the row and column containing the largest element in A −1 11 . Note that this element may occur in the last row or last column of A −1 11 , indicating only a column swap or row swap respectively is needed. When the swaps are performed, the factorization must be updated to maintain truncated LU form. We have developed a variant of the LU updating algorithm of (Gondzio, 2007) to efficiently update the SRLU factorization.\nSRP can be implemented efficiently: each swap requires at most O (k(m+ n)) operations, and ‖A−111 ‖max can be quickly and reliably estimated using (Higham & Relton, 2015). An argument similar to that used in (Miranian & Gu, 2003) shows that each swap will increase\n∣∣det (A11)∣∣ by a factor at least f , hence will never repeat. At termination, SRP will ensure a partial LU factorization of the form (1) that satisfies condition (2). We will discuss spectrumrevealing properties of this factorization in Section 4.2.\nIt is possible to derive theoretical upper bounds on the worst number of swaps necessary in SRP, but in practice, this number is zero for most matrices, and does not exceed 3− 5 in the most pathological data matrix of dimension at most 1000 we can contrive.\nSRLU can be used effectively to approximate second order information in machine learning. SRLU can be used as a modification to ADAGRAD in a manner similar to the\nAlgorithm 4 Spectrum-Revealing Pivoting (SRP)\n1: Input: Truncated LU factorization A ≈ L̂Û, tolerance f > 1 2: while ‖A−111 ‖max > f |α| do 3: Set α to be the largest element in S (or find an approximate α using R) 4: Swap row and column containing α with row and column of largest element in A −1 11 5: Update truncated LU factorization 6: end while\nlow-rank approximation method in (Krummenacher et al., 2016). Applying the initialization technique in this work, SRLU would likely provide an efficient and accurate adaptive stochastic optimization algorithm. SRLU can also become a full-rank approximation (low-rank plus diagonal) by adding a diagonal approximation of the Schur complement. Such an approximation could be appropriate for improving memory in artificial intelligence, such as in (Kirkpatrick et al., 2017). SRLU is also a freestanding compression algorithm."
  }, {
    "heading": "3.3. The CUR Decomposition with LU",
    "text": "A natural extension of truncated LU factorizations is a CUR-type decomposition for increased accuracy (Mahoney & Drineas, 2009):\nΠ1AΠ T 2 ≈ L̂\n( L̂†AÛ† ) Û def = L̂MÛ.\nAs with standard CUR, the factors L̂ and Û retain (much of) the sparsity of the original data, while M is a small, k-by-k matrix. The CUR decomposition can improve the accuracy of an SRLU with minimal extra needed memory. Extra computational time, nevertheless, is needed to calculate M. A more efficient, approximate CUR decomposition can be obtained by replacing A with a high quality approximation (such as an SRLU factorization of high rank) in the calculation of M."
  }, {
    "heading": "3.4. The Online SRLU Factorization",
    "text": "Given a factored data matrix A ∈ Rm×n and new observations BΠT2 = ( k m − k B1 B2 ) ∈ Rs×m, an augmented LU decomposition takes the form( Π1AΠ T 2\nBΠT2\n) = L11L21 I L31 I U11 U12S Snew  , where L31 = B1U−111 and S\nnew = B2 −B1U−111 U12. An SRLU factorization can then be obtained by simply performing correcting swaps. For a rank-1 update, at most 1\nswap is expected (although examples can be constructed that require more than one swap), which requires at most O (k (m+ n)) flops. By contrast, the URV decomposition of (Stewart, 1992) is O ( n2 ) , while SVD updating\nrequires O ( (m+ n) min2 (m,n) ) operations in general,\nor O ( (m+ n) min (m,n) log22 ) for a numerical approximation with the fast multipole method."
  }, {
    "heading": "4. Theoretical Results for SRLU Factorizations",
    "text": ""
  }, {
    "heading": "4.1. Analysis of General Truncated LU Decompositions",
    "text": "Theorem 1 Let (·)s denote the rank-s truncated SVD for s ≤ k m,n. Then for any truncated LU factorization with Schur complement S:\n‖Π1AΠT2 − L̂Û‖ = ‖S‖\nfor any norm, and ‖Π1AΠT2 − ( L̂Û ) s ‖2 ≤ 2‖S‖2 + σs+1 (A) .\nTheorem 2 For a general rank-k truncated LU decomposition, we have for all 1 ≤ j ≤ k,\nσj (A) ≤ σj ( L̂Û )1 + 1 + ‖S‖2 σk ( L̂Û )  ‖S‖2 σj (A)  . Theorem 3 CUR Error Bounds.\n‖Π1AΠT2 − L̂MÛ‖2 ≤ 2‖S‖2\nand\n‖Π1AΠT2 − L̂MÛ‖F ≤ ‖S‖F .\nTheorem 1 simply concludes that the approximation is accurate if the Schur complement is small, but the singular value bounds of Theorem 2 are needed to guarantee that the approximation retains structural properties of the original data, such as an accurate approximation of the rank and the spectrum. Furthermore, singular values bounds can be significantly stronger than the more familiar norm error bounds that appear in Theorem 1. Theorem 2 provides a general framework for singular value bounds, and bounding the terms in this theorem provided guidance in the design and development of SRLU. Just as in the case of deterministic LU with complete pivoting, the sizes of ‖S‖2\nσk(L̂Û)\nand ‖S‖2 σj(L̂Û) range from moderate to small for almost all data matrices of practical interest. They, nevertheless, cannot be effectively bounded for a general TRLUCP factorization, implying the need for Algorithm 4 to ensure that\nthese terms are controlled. While the error bounds in Theorem 3 for the CUR decomposition do not improve upon the result in Theorem 1, CUR bounds for SRLU specifically will be considerably stronger. Next, we present our main theoretical contributions."
  }, {
    "heading": "4.2. Analysis of the Spectrum-Revealing LU Decomposition",
    "text": "Theorem 4 (SRLU Error Bounds.) For j ≤ k and γ = O (fk √ mn), SRP produces a rank-k SRLU factorization with\n‖Π1AΠT2 − L̂Û‖2 ≤ γσk+1 (A) , ‖Π1AΠT2 − ( L̂Û ) j ‖2 ≤ σj+1 (A) ( 1 + 2γ σk+1(A)σj+1(A) ) Theorem 4 is a special case of Theorem 1 for SRLU factorizations. For a data matrix with a rapidly decaying spectrum, the right-hand side of the second inequality is close to σj+1 (A), a substantial improvement over the sharpness of the bounds in (Drineas et al., 2008).\nTheorem 5 (SRLU Spectral Bound). For 1 ≤ j ≤ k, SRP produces a rank-k SRLU factorization with\nσj (A)\n1 + τ σk+1(A)σj(A)\n≤ σj ( L̂Û ) ≤ σj (A) ( 1 + τ σk+1 (A)\nσj (A) ) for τ ≤ O ( mnk2f3 ) .\nWhile the worst case upper bound on τ is large, it is dimension-dependent, and j and k may be chosen so that σk+1(A) σj(A)\nis arbitrarily small compared to τ . In particular, if k is the numeric rank of A, then the singular values of the approximation are numerically equal to those of the data.\nThese bounds are problem-specific bounds because their quality depends on the spectrum of the original data, rather than universal constants that appear in previous results. The benefit of these problem-specific bounds is that an approximation of data with a rapidly decaying spectrum is guaranteed to be high-quality. Furthermore, if σk+1 (A) is not small compared to σj (A), then no high-quality low-rank approximation is possible in the 2 and Frobenius norms. Thus, in this sense, the bounds presented in Theorems 4 and 5 are optimal.\nGiven a high-quality rank-k truncated LU factorization, Theorem 5 ensures that a low-rank approximation of rank ` with ` < k of the compressed data is an accurate rank-` approximation of the full data. The proof of this theorem centers on bounding the terms in Theorems 1 and 2. Experiments will show that τ is small in almost all cases.\nStronger results are achieved with the CUR version of SRLU:\nTheorem 6\n‖Π1AΠT2 − L̂MÛ‖2 ≤ 2γσk+1 (A)\nand\n‖Π1AΠT2 − L̂MÛ‖F ≤ ωσk+1 (A) ,\nwhere γ = O (fk √ mn) is the same as in Theorem 4, and ω = O (fkmn).\nTheorem 7 If σ2j (A) > 2‖S‖22 then σj (A) ≥ σj ( L̂MÛ ) ≥ σj (A) √ 1− 2γ ( σk+1 (A)\nσj (A) )2 for γ = O ( mnk2f2 ) and f is an input parameter controlling a tradeoff of quality vs. speed as before.\nAs before, the constants are small in practice. Observe that for most real data matrices, their singular values decay with increasing j. For such matrices this result is significantly stronger than Theorem 5."
  }, {
    "heading": "5. Experiments",
    "text": ""
  }, {
    "heading": "5.1. Speed and Accuracy Tests",
    "text": "In Figure 1, the accuracy of our method is compared to the accuracy of the truncated SVD. Note that SRLU did not perform any swaps in these experiments. “CUR” is the CUR version of the output of SRLU. Note that both methods exhibits a convergence rate similar to that of the truncated SVD (TSVD), and so only a constant amount of extra work is needed to achieve the same accuracy. When the singular values decay slowly, the CUR decomposition provides a greater accuracy boost. In Figure 2, the runtime of SRLU is compared to that of the truncated SVD, as well as Subspace Iteration (Gu, 2015). Note that for Subspace Iteration, we choose iteration parameter q = 0 and do not measure the time of applying the random projection, in acknowledgement that fast methods exist to apply a random projection to a data matrix. Also, the block size implemented in SRLU is significantly smaller than the block size used by the standard software LAPACK, as the size of the block size affects the size of the projection. See supplement for additional details. All numeric experiments were run on NERSC’s Edison. For timing experiments, the truncated SVD is calculated with PROPACK.\nEven more impressive, the factorization stage of SRLU becomes arbitrarily faster than the standard implementation of the LU decomposition. Although the standard LU decomposition is not a low-rank approximation algorithm, it is known to be roughly 10 times faster than the SVD (Demmel, 1997). See appendix for details.\nNext, we compare SRLU against competing algorithms. In (Ubaru et al., 2015), error-correcting codes are introduced to yield improved accuracy over existing random projection low-rank approximation algorithms. Their algorithm, denoted Dual BCH, is compared against SRLU as well as two other random projection methods: Gaus., which uses a Gaussian random projection, and SRFT, which uses a Fourier transform to apply a random projection. We test the spectral norm error of these algorithms on matrices from the sparse matrix collection in (Davis & Hu, 2011).\nIn Table 1, results for SRLU are averaged over 5 experiments. Using tuning parameter f = 5, no swaps were needed in all cases. The matrices being tested are sparse matrices from various engineering problems. S80PIn1 is 4,028 by 4,028, deter3 is 7,647 by 21,777, and lp ceria3d (abbreviated lc3d) is 3,576 by 4,400. Note\nthat SRLU, a more efficient algorithm, provides a better approximation in two of the three experiments. With a little extra oversampling, a practical assumption due to the speed advantage, SRLU achieves a competitive quality approximation. The oversampling highlights an additional and unique advantage of SRLU over competing algorithms: if more accuracy is desired, then the factorization can simply continue as needed."
  }, {
    "heading": "5.2. Sparsity Preservation Tests",
    "text": "The SRLU factorization is tested on sparse, unsymmetric matrices from (Davis & Hu, 2011). Figure 3 shows the sparsity patterns of the factors of an SRLU factorization of a sparse data matrix representing a circuit simulation (oscil dcop), as well as a full LU decomposition of the data. Note that the LU decomposition preserves the sparsity of the data initially, but the full LU decomposition becomes dense. Several more experiments are shown in the supplement."
  }, {
    "heading": "5.3. Towards Feature Selection",
    "text": "An image processing example is now presented to illustrate the benefit of highlighting important rows and columns selection. In Figure 4 an image is compressed to a rank50 approximation using SRLU. Note that the rows and columns chosen overlap with the astronaut and the planet, implying that minimal storage is needed to capture the\nblack background, which composes approximately two thirds of the image. While this result cannot be called feature selection per se, the rows and columns selected highlight where to look for features: rows and/or columns are selected in a higher density around the astronaut, the curvature of the planet, and the storm front on the planet."
  }, {
    "heading": "5.4. Online Data Processing",
    "text": "Online SRLU is tested here on the Enron email corpus (Lichman, 2013). The documents were initially reversesorted by the usage of the most common word, and then reverse-sorted by the second most, and this process was repeated for the five most common words (the top five words were used significantly more than any other), so that the most common words occurred most at the end of the corpus. The data contains 39,861 documents and 28,102\nwords/terms, and an initial SRLU factorization of rank 20 was performed on the first 30K documents. The initial factorization contained none of the top five words, but, after adding the remaining documents and updating, the top three were included in the approximation. The fourth and fifth words ‘market’ and ‘california’ have high covariance with at least two of the three top words, and so their inclusion may be redundant in a low-rank approximation."
  }, {
    "heading": "6. Conclusion",
    "text": "We have presented SRLU, a low-rank approximation method with many desirable properties: efficiency, accuracy, sparsity-preservation, the ability to be updated, and the ability to highlight important data features and variables. Extensive theory and numeric experiments have illustrated the efficiency and effectiveness of this method."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by NSF Award CCF1319312."
  }],
  "year": 2017,
  "references": [{
    "title": "Randomized lu decomposition using sparse projections",
    "authors": ["Y. Aizenbud", "G. Shabat", "A. Averbuch"],
    "year": 2016
  }, {
    "title": "Rank revealing qr factorizations",
    "authors": ["T.F. Chan"],
    "venue": "Linear algebra and its applications,",
    "year": 1987
  }, {
    "title": "On the compression of low rank matrices",
    "authors": ["H. Cheng", "Z. Gimbutas", "Martinsson", "P.-G", "V. Rokhlin"],
    "venue": "SIAM J. Scientific Computing,",
    "year": 2005
  }, {
    "title": "Fine-grained parallel incomplete lu factorization",
    "authors": ["E. Chow", "A. Patel"],
    "venue": "SIAM J. Scientific Computing,",
    "year": 2015
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "CoRR, abs/1207.6365,",
    "year": 2012
  }, {
    "title": "The university of florida sparse matrix collection",
    "authors": ["T.A. Davis", "Y. Hu"],
    "venue": "ACM Transactions on Mathematical Software,",
    "year": 2011
  }, {
    "title": "Adaptive sampling and fast low-rank matrix approximation",
    "authors": ["A. Deshpande", "S. Vempala"],
    "venue": "In APPROX-RANDOM,",
    "year": 2006
  }, {
    "title": "Matrix approximation and projective clustering via volume sampling",
    "authors": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"],
    "venue": "Theory of Computing,",
    "year": 2006
  }, {
    "title": "Relative-error cur matrix decompositions",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"],
    "venue": "SIAM J. Matrix Analysis Applications,",
    "year": 2008
  }, {
    "title": "The approximation of one matrix by another of lower rank",
    "authors": ["C. Eckart", "G. Young"],
    "year": 1936
  }, {
    "title": "Fast montecarlo algorithms for finding low-rank approximations",
    "authors": ["A.M. Frieze", "R. Kannan", "S. Vempala"],
    "venue": "J. ACM,",
    "year": 2004
  }, {
    "title": "Lstd with random projections",
    "authors": ["M. Ghavamzadeh", "A. Lazaric", "Maillard", "O.-A", "R. Munos"],
    "venue": "In NIPS, pp",
    "year": 2010
  }, {
    "title": "Stable algorithm for updating dense lu factorization after row or column exchange and row and column addition or deletion",
    "authors": ["J. Gondzio"],
    "venue": "Optimization: A journal of Mathematical Programming and Operations Research,",
    "year": 2007
  }, {
    "title": "Parallel symbolic factorization for sparse lu with static pivoting",
    "authors": ["L. Grigori", "J. Demmel", "X.S. Li"],
    "venue": "SIAM J. Scientific Computing,",
    "year": 2007
  }, {
    "title": "Subspace iteration randomization and singular value problems",
    "authors": ["M. Gu"],
    "venue": "SIAM J. Scientific Computing,",
    "year": 2015
  }, {
    "title": "Efficient algorithms for computing a strong rank-revealing qr factorization",
    "authors": ["M. Gu", "S.C. Eisenstat"],
    "venue": "SIAM J. Sci. Comput.,",
    "year": 1996
  }, {
    "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
    "authors": ["N. Halko", "Martinsson", "P.-G", "J.A. Tropp"],
    "venue": "SIAM Review,",
    "year": 2011
  }, {
    "title": "URL http://eprints",
    "authors": ["N.J. Higham", "Relton", "S.D. Estimating the largest entries of a matrix."],
    "venue": "ma.man.ac.uk/.",
    "year": 2015
  }, {
    "title": "Speeding up convolutional neural networks with low rank expansions",
    "authors": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"],
    "venue": "CoRR, abs/1405.3866,",
    "year": 2014
  }, {
    "title": "Extensions of lipschitz mappings into a hilbert space",
    "authors": ["W.B. Johnson", "J. Lindenstrauss"],
    "venue": "Contemporary Mathematics,",
    "year": 1984
  }, {
    "title": "Lu factorization with panel rank revealing pivoting and its communication avoiding version",
    "authors": ["A. Khabou", "J. Demmel", "L. Grigori", "M. Gu"],
    "venue": "SIAM J. Matrix Analysis Applications,",
    "year": 2013
  }, {
    "title": "Scalable adaptive stochastic optimization using random projections",
    "authors": ["G. Krummenacher", "B. McWilliams", "Y. Kilcher", "J.M. Buhmann", "N. Meinshausen"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Randomized algorithms for the low-rank approximation of matrices",
    "authors": ["E. Liberty", "F. Woolfe", "P.G. Martinsson", "V. Rokhlin", "M. Tygert"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Efficient second order online learning by sketching",
    "authors": ["H. Luo", "A. Agarwal", "N. Cesa-Bianchi", "J. Langford"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Cur matrix decompositions for improved data analysis",
    "authors": ["M.W. Mahoney", "P. Drineas"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2009
  }, {
    "title": "A randomized algorithm for the approximation of matrices",
    "authors": ["Martinsson", "P.-G", "V. Rokhlin", "M. Tygert"],
    "venue": "Tech. Rep., Yale University, Department of Computer Science,",
    "year": 2006
  }, {
    "title": "Gaussian elimination with randomized complete pivoting",
    "authors": ["C. Melgaard", "M. Gu"],
    "venue": "CoRR, abs/1511.08528,",
    "year": 2015
  }, {
    "title": "Stong rank revealing lu factorizations",
    "authors": ["L. Miranian", "M. Gu"],
    "venue": "Linear Algebra and its Applications,",
    "year": 2003
  }, {
    "title": "Latent semantic indexing: A probabilistic analysis",
    "authors": ["C.H. Papadimitriou", "P. Raghavan", "H. Tamaki", "S. Vempala"],
    "venue": "J. Comput. Syst. Sci.,",
    "year": 2000
  }, {
    "title": "Improved approximation algorithms for large matrices via random projections",
    "authors": ["T. Sarlos"],
    "venue": "In FOCS,",
    "year": 2006
  }, {
    "title": "An updating algorithm for subspace tracking",
    "authors": ["G.W. Stewart"],
    "venue": "IEEE Trans. Signal Processing,",
    "year": 1992
  }, {
    "title": "Low rank approximation using error correcting coding matrices",
    "authors": ["S. Ubaru", "A. Mazumdar", "Y. Saad"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization",
    "authors": ["W.Y. Wang", "Y. Mehdad", "D.R. Radev", "A. Stent"],
    "year": 2016
  }, {
    "title": "A fast randomized algorithm for the approximation of matrices",
    "authors": ["F. Woolfe", "E. Liberty", "V. Rokhlin", "M. Tygert"],
    "venue": "Applied and Computational Harmonic Analysis,",
    "year": 2008
  }],
  "id": "SP:eda7486c250346b77b73e3ab9403e95de65348e4",
  "authors": [{
    "name": "David Anderson",
    "affiliations": []
  }, {
    "name": "Ming Gu",
    "affiliations": []
  }],
  "abstractText": "Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and finding important signals. In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and develop a fast algorithm to compute an SRLU factorization. We provide both matrix and singular value approximation error bounds for the SRLU approximation computed by our algorithm. Our analysis suggests that SRLU is competitive with the best low-rank matrix approximation methods, deterministic or randomized, in both computational complexity and approximation quality. Numeric experiments illustrate that SRLU preserves sparsity, highlights important data features and variables, can be efficiently updated, and calculates data approximations nearly as accurately as possible. To the best of our knowledge this is the first practical variant of the LU factorization for effective and efficient low-rank matrix approximation.",
  "title": "An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation"
}