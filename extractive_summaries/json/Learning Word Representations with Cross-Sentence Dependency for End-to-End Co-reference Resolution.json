{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4829–4833 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4829"
  }, {
    "heading": "1 Introduction",
    "text": "Co-reference resolution requires models to cluster mentions that refer to the same physical entities. The models based on neural networks typically require different levels of semantic representations of input sentences. The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and wordlevel embeddings (Turian et al., 2010; Pennington et al., 2014) before predicting antecedents. The mention-level embeddings are used to make coreference decisions, typically by scoring mention pairs and making links (Lee et al., 2017; Clark and Manning, 2016a; Wiseman et al., 2016). Long short-term memories (LSTMs) are often used to encode the syntactic and semantic information of input sentences.\nArticles and conversations include more than one sentences. Considering the accuracy and efficiency of co-reference resolution models, the encoder LSTM usually processes input sentences separately as a batch (Lee et al., 2017). The disadvantage of this method is that the models do not consider the dependency among words from different sentences, which plays a significant role in word representation learning and co-reference predicting. For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency information. As a result, a word representation model cannot learn an informative embedding of a pronoun without considering cross-sentence dependency in this case.\nIt is also problematic if we encode the input document considering cross-sentence dependency and treat the entire document as one sentence. An input article or conversation can be too long for a single LSTM cell to memorize. If the LSTM updates itself for too many steps, gradients will vanish or explode (Pascanu et al., 2013), and the coreference resolution model will be very difficult to optimize. Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model.\nTo solve the problem that traditional LSTM encoders, which treat the input sentences as a batch, lack an ability to capture cross-sentence dependency, and to avoid the time complexity and difficulties of training the model concatenating all input sentences, we propose a cross-sentence encoder for end-to-end co-reference (E2E-CR). Borrowing the idea of an external memory module from Sukhbaatar et al. (2015), an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model. With this context memory block, the proposed model is able to encode\ninput sentences as a batch, and also calculate the representations of input words by taking both target sentences and context sentences into consideration. Experiments showed that this approach improved the performance of co-reference resolution models."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Co-reference Resolution",
    "text": "A popular method of co-reference resolution is mention ranking (Durrett and Klein, 2013). Reading each mention, the model calculates coreference scores for all antecedent mentions, and picks the mention with the highest positive score to be its co-reference. Many recent works are based on this approach. Durrett and Klein (2013) designed a set of feature templates to improve the mention-ranking model. Peng et al. (2015) proposed a mention-ranking model by jointly learning mention heads and co-references. Clark and Manning (2016a) proposed a reinforcement learning framework for the mention ranking approach. Based on similar ideas but without using parsing features, the authors of Lee et al. (2017) proposed the current state-of-the-art model which uses neural networks to embed mentions and calculate mention and antecedent scores. Lee et al. (2018) applied ELMo embeddings (Peters et al., 2018) to improve within-sentence dependency modeling and word representation learning. Wiseman et al. (2016) and Clark and Manning (2016b) proposed models using global entity-level features."
  }, {
    "heading": "2.2 Language Representation Learning",
    "text": "Distributed word embeddings has been used as the basic unit of language representation for over a decade (Bengio et al., 2003). Pre-trained word embeddings, for example GloVe (Pennington et al., 2014) and Skip-Gram (Mikolov et al., 2013) are widely used as the input of natural language processing models.\nLong short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are widely used for sentence modeling. A single-layer LSTM network was applied in the previous state-of-theart co-reference model (Lee et al., 2017) to generate word and mention representations. To capture dependency of longer distances, Campos et al. (2017) proposed a recurrent model that outputs hidden states by skipping input tokens.\nRecently, memory networks (Sukhbaatar et al.,\n2015) have been applied in language modeling (Cheng et al., 2016; Tran et al., 2016). Applying an attention mechanism on memory cells, memory networks allow the model to focus on significant words or segments for classification and generation tasks. Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction (Yogatama et al., 2018)."
  }, {
    "heading": "3 Learning Cross-Sentence dependency",
    "text": "To improve the word representation learning model for better co-reference resolution performance, we propose two word representation models that learn cross-sentence dependency."
  }, {
    "heading": "3.1 Linear Sentence Linking",
    "text": "Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM, the most direct way to consider cross-sentence dependency is to initialize LSTM states with the encodings of adjacent sentences. We name this method linear sentence linking (LSL).\nIn LSL, we encode input sentences with a 2- layer bidirectional LSTM. Give input sentences [s1, s2 . . . sn], the outputs of the first layer are [[−→s 1;←−s 1], [−→s 2;←−s 2], . . . [−→s n;←−s n]]. In the second LSTM layer, the initial state of the forward LSTM of si is initialized as\n−→ S i = [ −→c 20; [−→s i−1;←−s i−1]]\nwhile the backward state is initialized as\n←− S i = [ ←−c 20; [−→s i−1;←−s i−1]]\nwhere ci0 stands for the initial cell of the ith layer, and x stands for the final output of the LSTMs in first layer. We then concatenate the outputs of the forward and backward LSTMs in the second layer as the word representations for coreference prediction."
  }, {
    "heading": "3.2 Attentional Sentence Linking",
    "text": "It is difficult for LSTMs to embed enough information about a long sentence into a lowdimensional distributed vector. To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.\nTo describe the architecture of the proposed model, we focus on adjacent input sentences si−1\nand si. We present the input embeddings of the j-th word in the i-th sentence with xi,j ."
  }, {
    "heading": "3.2.1 Long Short-Term Memory RNNs",
    "text": "To solve the traditional recurrent neural networks, Hochreiter and Schmidhuber (1997) proposed the LSTM architecture. The detail of recurrent state updating in LSTMs ht = flstm(xt, ht−1, ct−1) is shown in following equations.\nit = σ(Wxixt +Whiht−1 + bi) ft = σ(Wxfxt +Whfht−1 + bf ) ct = ft ct−1 + it tanh(Wxcxt +Whcht−1 + bc) ot = σ(Wxoxt +Whoht−1 + bo) ht = ot tanh(ct)\nwhere xt is the input embedding and ht is the output representation of the t-th word."
  }, {
    "heading": "3.2.2 LSTMs with Cross-Sentence Attention",
    "text": "We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency. We name this method attentional sentence linking (ASL). Considering input word xi,t in the ith sentence and all words from the previous sentence Xi−1 = [xi−1,1, xi−1,2, . . . , xi−1,m], we regard the matrix Xi−1 as an external memory module and calculate an attention on its cells, where each cell contains a word embedding.\nαj = ecj∑ k e ck (1)\nck = fc([xi,t;ht−1;xi−1,k] T ) (2)\nWith the attention distribution α, we can get a vector summarizing related information from si−1,\nvi−1 = ∑ j αj · xi−1,j (3)\nThe model decides if it needs to pay more attention on the current input or cross-sentence information with a context gate.\ngt = σ(fg([xi,t;ht−1; vi−1] T )) (4)\nx̂i,t = gt · xi,t + (1− gt) · vi−1 (5)\nσ(·) stands for the Sigmoid function. The word representation of the target word is calculated as\nhi,t = flstm(x̂i,t, hi,t−1, ci,t−1) (6)\nwhere flstm stands for standard LSTM update described in section 3.2.1."
  }, {
    "heading": "3.3 Co-reference Prediction",
    "text": "In this work, we apply the mention-ranking endto-end co-reference resolution (E2E-CR) model proposed by Lee et al. (2017) for co-reference prediction. The word representations applied in E2ECR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs. In our work, we represent words by concatenating pre-trained word embeddings and the outputs of LSL- and ASL-LSTMs."
  }, {
    "heading": "4 Experiments",
    "text": "We train and evaluate our model on the English corpus of the CoNLL-2012 shared task (Pradhan et al., 2012). We implement our model based on the published implementation of the baseline E2ECR model (Lee et al., 2017) 1. Our implementation is also available online for reproducing the results reported in this paper 2. In this section, we first describe our hyperparameter setup, and then show the experimental results of previous work and our proposed models."
  }, {
    "heading": "4.1 Model and Hyperparameter Setup",
    "text": "In practice, the LSTM modules applied in our model have 200 output units. In ASL, we calculate cross-sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units. The initial learning rate is set as 0.001 and decays 0.001% every 100 steps. The model is optimized with the Adam algorithm (Kingma and Ba, 2014). We randomly select up to 40 continuous sentences for training if the input is too long. In co-reference prediction, we select 250 candidate antecedents as our baseline model."
  }, {
    "heading": "4.2 Experiment Results and Discussion",
    "text": "We evaluate our model on the test set of the CoNLL-2012 shared task. The performance of previous work and our model are shown in Table 1. We mainly focus on the average F1 score of MUC, B3, and CEAF metrics. Comparing with the baseline model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1. Experiments\n1https://github.com/kentonl/e2e-coref 2https://github.com/luohongyin/\ncoatt-coref\nshow that the models that consider cross-sentence dependency significantly outperform the baseline model, which encodes each sentence from the input document separately.\nExperiments also indicated that the ASL model has better performance than the LSL model, since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence-level embeddings. This gives the model a better ability to model cross-sentence dependency.\nExamples for comparing the performance of the ASL model and the baseline are shown in Table 2. Each example contains two continuous sentences with co-references distritubed in different sentences. Underlined spans in bold are target mentions and annotated co-references. Spans in\ngreen are ASL predictions, and spans in red are baseline predictions. A prediction on “-” means that no mention is predicted as a co-reference.\nTable 2 shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence. The pretrained embeddings of pronouns are not informative enough. In the first example, “it” is not semantically similar with “SMS” in GloVe without any context, and in this case, “it” and “SMS” are in different sentences. As a result, if reading this two sentences separately, it is hard for the encoder to represent “it” with the semantics of “SMS”. This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention, or cannot find any co-reference.\nHowever, with ASL, the model learns the semantics of pronouns with an attention to words in other sentences. With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough. Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "We proposed linear and attentional sentence linking models for learning word representations that captures cross-sentence dependency. Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state-of-the-art co-reference resolution model, indicating that cross-sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences. It worth exploring if our model can improve the performance of other natural language processing\napplications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis."
  }],
  "year": 2018,
  "references": [{
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research, 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834",
    "authors": ["Vı́ctor Campos", "Brendan Jou", "Xavier Giró-i Nieto", "Jordi Torres", "Shih-Fu Chang"],
    "year": 2017
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "arXiv preprint arXiv:1601.06733.",
    "year": 2016
  }, {
    "title": "Deep reinforcement learning for mention-ranking coreference models",
    "authors": ["Kevin Clark", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1609.08667.",
    "year": 2016
  }, {
    "title": "Improving coreference resolution by learning entitylevel distributed representations",
    "authors": ["Kevin Clark", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1606.01323.",
    "year": 2016
  }, {
    "title": "Easy victories and uphill battles in coreference resolution",
    "authors": ["Greg Durrett", "Dan Klein."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971–1982.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "End-to-end neural coreference resolution",
    "authors": ["Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1707.07045.",
    "year": 2017
  }, {
    "title": "Higher-order coreference resolution with coarse-tofine inference",
    "authors": ["Kenton Lee", "Luheng He", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1804.05392.",
    "year": 2018
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "International Conference on Machine Learning, pages 1310–1318.",
    "year": 2013
  }, {
    "title": "A joint framework for coreference resolution and mention head detection",
    "authors": ["Haoruo Peng", "Kai-Wei Chang", "Dan Roth."],
    "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 12–21.",
    "year": 2015
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1802.05365.",
    "year": 2018
  }, {
    "title": "Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
    "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."],
    "venue": "Joint Conference on EMNLP and CoNLL-Shared Task, pages 1–",
    "year": 2012
  }, {
    "title": "End-to-end memory networks. In Advances in neural information processing systems, pages 2440–2448",
    "authors": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"],
    "year": 2015
  }, {
    "title": "Recurrent memory networks for language modeling",
    "authors": ["Ke Tran", "Arianna Bisazza", "Christof Monz."],
    "venue": "arXiv preprint arXiv:1601.01272.",
    "year": 2016
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for",
    "year": 2010
  }, {
    "title": "Learning global features for coreference resolution",
    "authors": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber."],
    "venue": "arXiv preprint arXiv:1604.03035.",
    "year": 2016
  }, {
    "title": "Memory architectures in recurrent neural network language models",
    "authors": ["Dani Yogatama", "Yishu Miao", "Gabor Melis", "Wang Ling", "Adhiguna Kuncoro", "Chris Dyer", "Phil Blunsom."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }],
  "id": "SP:564f1fc21e2c5c8a8b638ce3dfa449ce5fe8d83f",
  "authors": [{
    "name": "Hongyin Luo",
    "affiliations": []
  }],
  "abstractText": "In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR). While the traditional E2ECR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency. Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word. With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document. Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.",
  "title": "Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution"
}