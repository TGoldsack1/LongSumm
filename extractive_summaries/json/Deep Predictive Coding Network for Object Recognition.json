{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Convolutional neural networks (CNN) have achieved great success in image recognition. Classical CNN models, e.g. AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNet (He et al., 2016a), have improved the performance in computer vision, while these models generally become deeper and wider by using more layers (He et al., 2016a) or/and filters (Zagoruyko & Komodakis, 2016). Despite various ways of architectural reconfiguration, these models all scale up from the same principle of computation: extracting image\n1School of Electrical and Computer Engineering, Purdue University 2Weldon School of Biomedical Engineering, Purdue University. Correspondence to: Zhongming Liu <zmliu@purdue.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfeatures by a feedforward pass through stacks of convolutional layers.\nAlthough it is inspired by hierarchical processing in biological visual systems (Hubel & Wiesel, 1968), CNN differs from the brain in many aspects. Unlike CNN, the brain achieves robust visual perception by using feedforward, feedback and recurrent connections (Felleman & Van, 1991). Information is processed not only through a bottom-up pathway running from lower to higher visual areas, but also through a top-down pathway running in the opposite direction. Such bi-directional processes enable humans to perform a wide range of visual tasks, including object recognition. For human vision, feedforward processing is essential to rapid recognition (Serre et al., 2007), e.g. when visual input is too brief to recruit feedback and recurrent processing (Thorpe et al., 1996). However, feedback processing improves object recognition and enables cognitive processes to influence perception (Wyatte et al., 2014). In neuroscience, the interplay between feedforward and feedback processes is described by hierarchical predictive coding (Rao & Ballard, 1999; Friston & Kiebel, 2009; Bastos et al., 2012; Clark, 2013). It states that the feedback connections from a higher visual area to a lower visual area carry predictions of lowerlevel neural activities; feedforward connections carry the errors between the predictions and the actual lower-level activities. As a result, the brain dynamically updates its representations to progressively refine its perceptual and behavioral decisions.\nBased on this brain theory, we designed a bi-directional and recurrent neural net (i.e. PCN). Given image input to PCN, it runs recursive cycles of bottom-up and top-down computation to update its internal representations towards minimization of the residual error between bottom-up input and top-down prediction at every layer in the network. Using predictive coding as its computational mechanism, PCN differs from feedforward-only CNNs that currently dominate computer vision. It is a model with dynamics that uses recursive and bi-directional computation to extract better representations of the input such that the input is predictable by the internal representation. When it is unfolded in time, PCN runs a longer cascade of nonlinear transformations by running more cycles of bottom-up and top-down computation through the same architecture without adding more layers, units, or connections.\nTo explore its value, we designed PCN with convolutional layers stacked in both feedforward and feedback directions. We trained and tested PCN for image classification with benchmark datasets: CIFAR-10 (Krizhevsky & Hinton, 2009) CIFAR-100 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011) and MNIST (LeCun et al., 1998). Our focus was to explore the intrinsic advantages of PCN over its feedforward-only counterpart: a plain CNN model without feedback connection or any mechanism for recurrent dynamics. It turned out that PCN always outperformed the plain model, and its accuracy tended to improve given more cycles of computation over time. Relative to the classical models, PCN yielded competitive performance in all benchmark tests despite much less layers in PCN. As we did not attempt to optimize the performance by trying many learning parameters or model architectures, there is much room for future studies (e.g. Han et al. (2018)) to further improve or extend the model on the basis of a similar notion."
  }, {
    "heading": "2. Related Work",
    "text": "Recent studies demonstrate that deep convolutional neural networks use representations similar to those in the brain (Khaligh-Razavi & Kriegeskorte, 2014; Yamins et al., 2014; Güçlü & van Gerven, 2015; Cichy et al., 2016; Eickenberg et al., 2017; Wen et al., 2017). However, many gaps are yet to be filled to bridge biological and artificial visual systems. A biologically plausible model should take into account feedback and recurrent connections, which are abundant in primate brains (Felleman & Van, 1991). A limited number of studies have taken on this direction from the perspective of computational neuroscience or computer vision.\nO’Reilly et al. demonstrated that feedback connections could enable top-down representations to fill incomplete bottom-up representations to improve recognition of partially occluded objects (O’Reilly et al., 2013). Exploiting a similar idea, Spoerer et al. built a recurrent CNN using feedforward, feedback, and lateral connections to enable recurrent processing that dynamically updated the internal representations as the sum of bottom-up, top-down, and lateral contributions (Spoerer et al., 2017). Trained and tested with synthesized digital images, their model yielded more robust recognition of digits in cluttered and occluded images. However, that model did not embody an explicit computational mechanism to ensure recurrent processing dynamics to converge over time. Although compelling from the neuroscience perspective, the models in the above studies were relatively simple and shallow, and they were not tested in naturalistic visual scenarios of primary interest to computer vision.\nIn computer vision, feedback has also played an important role in some vision tasks. For example, feedback was used to select the internal attention to achieve better object\nrecognition performance (Stollenga et al., 2014) or used to model the visual saliency in images (Mahdi et al., 2017) Many studies also used a feedback network to reconstruct the visual input in unsupervised learning like autoencoders (Hinton & Salakhutdinov, 2006), deconvolutional networks (Zeiler et al., 2010) and generative models (Hinton, 2012; Canziani & Culurciello, 2017). What remains unresolved is a biologically plausible mechanism that allows feedforward, feedback, and recurrent processes to interact with one another in order for the model to manifest internal dynamics that support various learning objectives.\nIn this regard, we may seek inspiration from the brain. Predictive coding is an influential theory of neural processing in vision and beyond (Huang & Rao, 2011) as supported by empirical evidence (Gómez et al., 2014; van Pelt et al., 2016). In a seminal paper (Rao & Ballard, 1997), Rao and Ballard postulated that the brain learns a hierarchical internal model of the visual world. Each level in this model attempts to predict the responses at its lower level via feedback connections; the error between this prediction and the actual response is sent to the higher level via feedforward connections. Friston et al. further generalized this notion into a unified brain theory for perception and action (Friston & Kiebel, 2009). Chalasani et al. used predictive coding to train a deep neural net to learn a hierarchy of sparse representations of data without supervision (Chalasani & Principe, 2013). Lotter et al. explored video prediction as an unsupervised learning objective based on predictive coding (Lotter et al., 2016); however the model trained in this way may not be able to learn sufficiently abstract representation to support such tasks as object recognition. Spratling et al. explored the use of predictive coding for object recognition; however, their model was limited a shallow network architecture for much simplified scenarios (Spratling, 2017).\nInspired by but different from models in prior studies (Spratling, 2008; 2017; Rao & Ballard, 1999), a hierarchical, bidirectional, and recurrent neural network is proposed herein for object recognition. This model operates with the theory of predictive coding to generate dynamic internal representations by recursive bottom-up and top-down computation. To train this network, the representations at the highest level, after multiple cycles of recursive updating, are used to classify the input image. With labeled images, the model parameters are trained through backpropagation in time and across layers."
  }, {
    "heading": "3. Method",
    "text": ""
  }, {
    "heading": "3.1. Predictive Coding",
    "text": "Central to the theory of predictive coding is that the brain continuously generates top-down predictions of bottom-up inputs. The representation at a higher level predicts the\nrepresentation at its lower level. The difference between the predicted and actual representation elicits an error of prediction, and propagates to the higher level to update its representation towards improved prediction. This repeats throughout the hierarchy until the errors of prediction diminish, or the bottom-up process no longer conveys any new (or unpredicted) information to update the hidden representation. Thus, predictive coding is a computational mechanism for the model to recursively update its internal representations of the visual input towards convergence.\nIn the following mathematical description of this dynamic process in PCN, italic lowercase letters are used as symbols for scalars, bold lowercase letters for column vectors, and bold uppercase letters for MATRICES. The representation at layer l and time t is denoted as rl(t). The weights of feedforward connections from layer l − 1 to layer l are denoted as Wl−1,l. The weights of feedback connections from layer l to layer l − 1 are denoted as Wl,l−1.\nIn PCN, the higher-level representation, rl(t), predicts its lower-level representation as pl−1(t) via linear weighting Wl,l−1, as shown in Eq. (1). The prediction error, el−1(t), is the difference between pl−1(t) and rl−1(t) as in Eq. (2).\npl−1(t) = (Wl,l−1) T rl(t) (1) el−1(t) = rl−1(t)− pl−1(t) (2)"
  }, {
    "heading": "3.1.1. FEEDFORWARD PROCESS",
    "text": "For the feedforward process, the prediction error at layer l − 1, el−1(t), propagates to the upper layer l to update its representation, rl(t), so the updated representation reduces the error. To minimize el−1(t), lets define a loss as the sum of the squared errors normalized by the variance of the representation, σ2l−1, as in Eq. (3).\nel−1(t) = 1\nσ2l−1 ‖ el−1(t) ‖22 (3)\nThe gradient of el−1(t) with respect to rl(t) is as Eq. (4).\n∂el−1(t) ∂rl(t) = − 2 σ2l−1 Wl,l−1el−1(t) (4)\nTo minimize el−1(t), rl(t) is updated by gradient descent with an updating rate, αl, as shown in Eq. (5).\nrl(t+ 1) = rl(t)− αl ( ∂el−1(t)\n∂rl(t) ) = rl(t) +\n2αl σ2l−1\nWl,l−1el−1(t) (5)\nIf the weights of feedback connections are the transpose of those of feedforward connections Wl,l−1 = (Wl−1,l)T ,\nthe update rule in Eq. (5) can be rewritten as a feedforward operation, as in Eq. (6).\nrl(t+ 1) = rl(t) + al(Wl−1,l)T el−1(t) (6)\nwhere the last term indicates forwarding the prediction error from layer l − 1 to layer l to update the representation with an updating rate al = 2αlσ2l−1 ."
  }, {
    "heading": "3.1.2. FEEDBACK PROCESS",
    "text": "For the feedback process, the top-down prediction is used to update the representation at layer l, rl(t), to reduce the prediction error el(t). Similar to feedforward process, the error is minimized by gradient descent, where the gradient of el(t) with respect to rl(t) is as Eq. (7), and rl(t) is updated with an updating rate βl as shown in Eq. (8).\n∂el(t) ∂rl(t) = 2 σ2l (rl(t)− pl(t)) (7)\nrl(t+ 1) = rl(t)− βl ∂el(t)\n∂rl(t)\n= (1− 2βl σ2l )rl(t) + 2βl σ2l\npl(t) (8)\nLet bl = 2βlσ2l and Eq. (8) is rewritten as follows.\nrl(t+ 1) = (1− bl)rl(t) + blpl(t) (9)\nE. (9) reflects a feedback process that the representation at the higher layer, rl+1(t), generates a top-down prediction, pl(t), and influences the lower-layer representation, rl(t)."
  }, {
    "heading": "3.1.3. NONLINEARITY",
    "text": "To add nonlinearity to the above feedforward and feedback processes, a nonlinear activation function is applied to the output of each convolutional layer (except the input layer, i.e. l = 0). A rectified linear unit (ReLU) (Nair & Hinton, 2010) converts Eqs. (6) and (9) to nonlinear processes as below. Nonlinear feedforward process:\nrl(t+ 1) = ReLU(rl(t) + al(Wl−1,l)T el−1(t)) (10)\nNonlinear feedback process:\nrl(t+ 1) = ReLU((1− bl)rl(t) + blpl(t)) (11)"
  }, {
    "heading": "3.2. Network Architecture",
    "text": "We used the nonlinear feedforward and feedback processes defined in Eq. (10) and (11) as a computational mechanism of predictive coding. We implemented this computational mechanism in several PCNs, all of which included stacked\nconvolutional layers with feedforward, feedback, and recurrent connections as shown in Fig. 1a. These PCNs were trained and tested for object recognition with four benchmark datasets. For comparison, several feedforward-only CNNs were built with the same architecture as the feedforward pathway in corresponding PCNs, and were trained and tested with the same datasets. We refer to these feedforwardonly CNNs as the plain models.\nPlain CNN Models: The architectural setting of our plain CNN models were similar to the VGG nets (Simonyan & Zisserman, 2014) (see Table 1). Briefly, the basic architecture included 6 or 8 convolutional layers and 1 classification layer. All convolutional layers used 3×3 filters but different numbers of filters, and used rectified linear unit (ReLU) as the nonlinear activation function. For some layers where the number of filters is doubled, the feature maps were reduced by applying 2×2 max-pooling with a stride of 2 after convolution. Batch normalization (Ioffe & Szegedy, 2015) was not used. The classification layer included global average pooling and a fully-connected (FC) layer followed by softmax. On the basis of this setting, we built 5 VGG-like architectures that varied in the number of layers and filters, and trained and tested the models with 4 datasets.\nPredictive Coding Network (PCN): Starting from each of the plain CNN architectures, we added feedback and recurrent connections to form a corresponding PCN. Fig. 1a shows a 9-layer PCN, running recursive bottom-up and top-down processing based on predictive coding. In PCN, feedback connections from one layer to its lower layer were constrained to be the transposed convolution (Dumoulin & Visin, 2016) which is the transpose of the feedforward counterparts. As such, both feedforward and feedback connections encoded spatial filters. The former was applied to the errors of the top-down prediction of lower-level representation; the latter was applied to high-level representation in order to predict the lower-level representation. The weights of feedback connections had the identical dimension as the transposed weights of feedforward connections. For layers where max-pooling was applied after feedforward convolution, bilinear upsampling was applied before feedback convolution to ensure that the dimension of top-down prediction\nAlgorithm 1 Deep Predictive Coding Network 1: Input: static image x 2: r0(t)← x 3: % initialize representations 4: for l = 0 to L− 1 do 5: rl+1(0)← ReLU(FFConv(rl(0))) 6: end for 7: % recurrent computation with T cycles 8: for t = 1 to T do 9: % nonlinear feedback process 10: for l = L to 1 do 11: pl−1(t− 1)← FBConv(rl(t− 1)) 12: if l > 1 then 13: rl−1(t− 1)← ReLU((1− b)rl−1(t− 1) + bpl−1(t− 1)) 14: end if 15: end for 16: % nonlinear feedforward process 17: for l = 0 to L− 1 do 18: el(t)← rl(t)− pl(t− 1) 19: rl+1(t)← ReLU(rl+1(t− 1) + aFFConv(el(t))) 20: end for 21: end for 22: % classification. 23: Output rL(T ) for classification Note: FFConv represents the feedforward convolution, FBConv represents the feedback convolution. a and b are specific to each filter in each layer. %comments are comments.\ncould match the dimension of lower-level representation.\nAn optional constraint to PCN was to use the same set of weights for both feedforward and feedback connections as in some prior studies (Rao & Ballard, 1999; Spratling, 2008; 2017). In other words, the weights of feedback connections were the transposed weights of feedforward connections. With this weight sharing, top-down predictions via feedback connections tended to approach lower-level representations. The PCN would have the same number of parameters as the corresponding plain model. Without this optional constraint of weight sharing, feedforward and feedback weights were assumed to be independent."
  }, {
    "heading": "3.3. Recursive Computation",
    "text": "Unlike feedforward-only networks, PCN runs a dynamic process to update its internal representation throughout the hierarchy (Fig. 1.b). Given an input image, PCN first runs through the feedforward path from the input layer to the last convolutional layer at t = 0, equivalent to a plain CNN model. For t = 1, PCN first runs a feedback process and then a feedforward process to update the representations in the hierarchy. In the feedback process, the representation at each layer is updated by a top-down prediction from the higher layer according to Eq. (11). The feedback process runs from the highest convolutional layer to the input layer. In the feedforward process, the representation at each layer is updated by a bottom-up error according to Eq. (10). This\nprocedure is repeated over time as shown in Fig. 1.b. After some cycles, the representation is used as the input to the classification layer to classify the image (see Algorithm 1)."
  }, {
    "heading": "3.4. Model Training",
    "text": "We evaluated two types of PCNs with regard to an optional constraint: the feedforward and feedback connections share the same convolutional weights. With this weight sharing, the feedforward operation and the feedback operation use the same weights. Without the constraint, the feedforward and feedback weights are initialized independently.\nIn this work, we evaluated these two types of PCNs with a varying number of recursive cycles (t = 0, 1, 2, · · · , 6) and with different model architectures (labeled as A through E in Table 1). We use Plain-A to represent the plain network with architecture A, and use PCN-A-t to represent the PCN with architecture A and t cycles of recursive computation. The numbers of recursive cycles for training and testing a model are the same. PCN-A-t (tied) and PCN-A-t represent the PCNs with and without weight sharing, respectively.\nWe used PyTorch to implement, train, and test the models described above. When PCN is trained for image classification, the classification error backpropagates across layers and in time to update the model parameters. The feedforward and feedback update rates (al and bl) are set to be\nspecific to each filter in each layer, are constrained to be non-negative by using ReLU, and are trained with initial values al = 1.0 and bl = 0.5, respectively. The convolutional weights and linear weights were initialized to be uniformly random (the default setting in PyTorch). The models were trained using mini-batches of a size 128 and without using dropout regularization (Srivastava et al., 2014)."
  }, {
    "heading": "4. Experiments",
    "text": "We trained and tested PCN for image classification with data in CIFAR-10/100, SVHN and MNIST, in comparison with plain CNN using the same feedforward architecture. With random initialization, PCN (or CNN) was trained for 5 times; the best and meanstd top-1 accuracy was reported as below."
  }, {
    "heading": "4.1. CIFAR-10 and CIFAR-100",
    "text": "The CIFAR-10/100 dataset includes 50,000 training images and 10,000 testing images in 10 or 100 object categories. Each image is a 32× 32 RGB image. PCN (or CNN) were trained on the training set and evaluated on the test set. All images were normalized per channel (i.e. subtract the mean and divide by the standard deviation). For training, we used translation and horizontal flipping for data augmentation. We used mini-batch gradient decent to train PCN (or CNN)\nwith a weight decay of 0.0005 and a momentum of 0.9. The learning rate was initialized as 0.01 and was divided by 10 when the error reached the plateau after training for 80, 140, 200 epochs. We stopped after 250 epochs. The hyper-parameters for learning were set based on validation with 10,000 images in the training set."
  }, {
    "heading": "4.1.1. PCN VS. CNN",
    "text": "During training, PCN converged much faster than its CNN counterpart (Fig. 2, top), especially when feedforward and feedback connections did not share weights. Meanwhile, increasing the recursive cycles tends to make PCN converge faster. With testing data, PCN also yielded better accuracy than the plain CNN (Fig. 2, bottom). For example, PCN improved the classification accuracy from 62.11% to 72.48% on CIFAR-100, relative to the plain CNN. See Table 2 for more results for comparison with other classical or stateof-the-art models. Without being pushed for high accuracy, PCN showed a similar accuracy as ResNet, but relatively lower than the pre-activation ResNet (Pre-act-ResNet) or the wide residual network (WRN), which used a much deeper or wider architecture than the models explored in this study."
  }, {
    "heading": "4.1.2. RECURSIVE COMPUTATION IN PCN",
    "text": "The accuracy of PCN depended on the number of cycles that recursively updated its internal representations. Fig. 3 shows that the accuracy of PCN tended to increase given more cycles of computation, especially if feedforward and feedback processes did not share the same weights.\nTo understand why this was the case, we looked into some testing images that were mis-classified by CNN but not by PCN. At each time step (0 through 6), PCN computed a different representation of an image that yielded a different probability distribution across different categories (Fig. 4). Classification was less definitive and/or inaccurate at early time steps. At later time steps, the network corrected itself to yield more definitive and accurate classification. It was true especially for ambiguous images, where a cat looked like a dog, or a deer looked like a horse, even for humans. See more examples in Fig. 4."
  }, {
    "heading": "4.1.3. GENERATIVE PREDICTION IN PCN",
    "text": "When it was trained for image classification, PCN was not explicitly optimized to reconstruct the input image, unlike a previous work that used video prediction as the learning objective (Lotter et al., 2016). Nevertheless, the top-down process in PCN was able to reconstruct the input with high accuracy. Although this was sort of expected for PCN with\nweight sharing, reconstruction was also reasonable even for PCN without weight sharing (Fig. 5). This result was surprising, and implied that PCN, without any architectural constraint to enable image reconstruction, is able to reshape itself to predict or reconstruct the input, even when it is trained for a discriminative task, e.g. object recognition. PCN potentially provides a new way to simultaneously train a discriminative network for object recognition and a generative network for prediction or reconstruction."
  }, {
    "heading": "4.1.4. COMPUTATIONAL REQUIREMENT",
    "text": "Given a static image, CNN processes it with a single feedforward pass in testing, but PCN needs several cycles of recursive computation. For example, PCN-A-t requires around 2t times the FLOPs of the plain CNN (0.68 billion FLOPs, multiply-adds). However, if the input is a video, CNN processes every video frame with a feedforward pass. PCN processes every frame with a feedback pass and a feedforward pass. Thus, PCN only doubles the FLOPs compared to the plain CNN given video input."
  }, {
    "heading": "4.2. SVHN and MNIST",
    "text": "SVHN is a dataset of Googles Street View House Numbers images (Netzer et al., 2011) and contains more than 600,000 color images of size 32×32, divided into training set, testing\nset and an extra set. The task of this dataset is to classify the digit located at the center of each image. Since the task is easier than CIFAR datasets, we implemented PCN with simpler network architectures (see Table 1). To validate the hyper parameters, we randomly selected 400 samples per class from the training set and 200 samples per class from the extra set for validation, as in (Goodfellow et al., 2013). The remainder of the training set and the extra set were used for training. The preprocessing for SVHN was the same as for CIFAR, i.e. per-channel normalization. No data augmentation was used. We used the Adam (Kingma & Ba, 2014) optimization with a weight decay of 0.0005 and an initial learning rate of 0.001 for a 20-10-10 epoch schedule. The exponential decay rates for the first and second moment estimates were 0.9 and 0.999, respectively. Table 3 shows the classification performance for this dataset. Like what we found for the CIFAR dataset, PCN always outperformed the plain CNN counterpart.\nThe MNIST dataset consists of hand written digits 0-9. There are 60,000 training images and 10,000 testing images in total. Each image is a gray image of size 28 × 28. For this dataset, the network architecture and training procedure were the same as for SVHN. PCN consistently performed better than its CNN counterpart (see Table 4). The best PCN achieves 0.36% error rate, comparable to some previous state-of-the-art models."
  }, {
    "heading": "5. Discussion and Conclusion",
    "text": "What defines PCN are 1) the use of bi-directional and recurrent connections as opposed to feedforward-only connections, and 2) the use of predictive coding as a mechanism for the model to recursively run bottom-up and top-down processes. When it is trained for image classification, the model dynamically refines its representation of the input image towards more accurate and definitive recognition. As this computation is unfolded in time, PCN reuses a single architecture and the same set of parameters to run an increasingly longer cascade of nonlinear transformation.\nWe say it is longer instead of deeper, because the notion behind PCN is different from the mindset in deep learning that more layers are required to model more complex and nonlinear relationships in data. In contrast, the brain does\nnot use a deeper network to do more challenging tasks. A more challenging task simply takes the brain longer time to process information through the same network.\nPredictive coding tells PCN how to compute but not how to learn. In this study, PCN is trained for image classification based on the representation emerging from the top layer after multiple cycles of computation. This helps the learning to converge faster, while utilizing full knowledge in training data. If an image takes the model more cycles of computation to converge its representation, it means that the image has more information than what the model can explain or generate, and thus the image carries a greater value for the model to learn. Therefore, it is more desirable to train PCN for more challenging visual tasks, e.g. images that are ambiguous or difficult to recognize, while reducing the need for a large number of simple examples.\nFor image classification, PCN takes an image as the input for all cycles of its recursive computation, while the errors of top-down prediction sent to the first hidden layer vary across cycles or in time. When the input is not a static image but a video, the input to the first hidden layer represents the errors of prediction of the present video frame given the models representations from the past frames. This would enable the model to compute and learn representations of both spatial and temporal information in videos, which is an important aspect that awaits to be explored in future studies.\nAs an initial step to explore predictive coding in computer vision, it was our intention to start and compare with models with a basic CNN architecture (e.g. VGG) in order to focus on evaluation of the value of using predictive coding as a computational mechanism. We expect that such predictive coding based computation can also be used to other network structures, e.g. ResNet and DenseNet. In a recent work (Han et al., 2018), a variant of PCN with a deeper structure and residual connections, has been developed and tested with ImageNet. It used notably fewer layers and parameters and but achieved competitive performance compared to classical and state-of-the-art models."
  }, {
    "heading": "Acknowledgements",
    "text": "The research was supported by NIH R01MH104402 and Purdue University."
  }],
  "year": 2018,
  "references": [{
    "title": "Canonical microcircuits for predictive coding",
    "authors": ["A.M. Bastos", "W.M. Usrey", "R.A. Adams", "G.R. Mangun", "P. Fries", "K.J. Friston"],
    "year": 2012
  }, {
    "title": "Cortexnet: a generic network family for robust visual temporal representations",
    "authors": ["A. Canziani", "E. Culurciello"],
    "venue": "arXiv preprint arXiv:1706.02735,",
    "year": 2017
  }, {
    "title": "Deep predictive coding networks",
    "authors": ["R. Chalasani", "J.C. Principe"],
    "venue": "arXiv preprint arXiv:1301.3541,",
    "year": 2013
  }, {
    "title": "Comparison of deep neural networks to spatiotemporal cortical dynamics of human visual object recognition reveals hierarchical correspondence",
    "authors": ["R.M. Cichy", "A. Khosla", "D. Pantazis", "A. Torralba", "A. Oliva"],
    "venue": "Scientific reports,",
    "year": 2016
  }, {
    "title": "Whatever next? predictive brains, situated agents, and the future of cognitive science",
    "authors": ["A. Clark"],
    "venue": "Behavioral and brain sciences,",
    "year": 2013
  }, {
    "title": "A guide to convolution arithmetic for deep learning",
    "authors": ["V. Dumoulin", "F. Visin"],
    "venue": "arXiv preprint arXiv:1603.07285,",
    "year": 2016
  }, {
    "title": "Seeing it all: Convolutional network layers map the function of the human visual system",
    "authors": ["M. Eickenberg", "A. Gramfort", "G. Varoquaux", "B. Thirion"],
    "year": 2017
  }, {
    "title": "Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex",
    "authors": ["D.J. Felleman", "D.E. Van"],
    "year": 1991
  }, {
    "title": "Predictive coding under the freeenergy principle",
    "authors": ["K. Friston", "S. Kiebel"],
    "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,",
    "year": 2009
  }, {
    "title": "Reduced predictable information in brain signals in autism spectrum disorder",
    "authors": ["C. Gómez", "J.T. Lizier", "M. Schaum", "P. Wollstadt", "C. Grützner", "P. Uhlhaas", "C.M. Freitag", "S. Schlitt", "S. Bölte", "R Hornero"],
    "venue": "Frontiers in neuroinformatics,",
    "year": 2014
  }, {
    "title": "Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream",
    "authors": ["U. Güçlü", "M.A. van Gerven"],
    "venue": "Journal of Neuroscience,",
    "year": 2015
  }, {
    "title": "Deep predictive coding network with local recurrent processing for object recognition",
    "authors": ["K. Han", "H. Wen", "Y. Zhang", "D. Fu", "E. Culurciello", "Z. Liu"],
    "venue": "arXiv preprint arXiv:1805.07526,",
    "year": 2018
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2016
  }, {
    "title": "Identity mappings in deep residual networks",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "A practical guide to training restricted boltzmann machines",
    "authors": ["G.E. Hinton"],
    "venue": "In Neural networks: Tricks of the trade,",
    "year": 2012
  }, {
    "title": "Reducing the dimensionality of data with neural",
    "authors": ["G.E. Hinton", "R.R. Salakhutdinov"],
    "venue": "networks. science,",
    "year": 2006
  }, {
    "title": "Receptive fields and functional architecture of monkey striate cortex",
    "authors": ["D.H. Hubel", "T.N. Wiesel"],
    "venue": "The Journal of physiology,",
    "year": 1968
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "arXiv preprint arXiv:1502.03167,",
    "year": 2015
  }, {
    "title": "Deep supervised, but not unsupervised, models may explain it cortical representation",
    "authors": ["Khaligh-Razavi", "S.-M", "N. Kriegeskorte"],
    "venue": "PLoS computational biology,",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "year": 2009
  }, {
    "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "year": 2012
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Recurrent convolutional neural network for object recognition",
    "authors": ["M. Liang", "X. Hu"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Deep predictive coding networks for video prediction and unsupervised learning",
    "authors": ["W. Lotter", "G. Kreiman", "D. Cox"],
    "venue": "arXiv preprint arXiv:1605.08104,",
    "year": 2016
  }, {
    "title": "A comparison study of saliency models for fixation prediction on infants and adults",
    "authors": ["A. Mahdi", "M. Su", "M. Schlesinger", "J. Qin"],
    "venue": "IEEE Transactions on Cognitive and Developmental Systems,",
    "year": 2017
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["V. Nair", "G.E. Hinton"],
    "venue": "In Proceedings of the 27th international conference on machine learning",
    "year": 2010
  }, {
    "title": "Reading digits in natural images with unsupervised feature learning",
    "authors": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"],
    "venue": "In NIPS workshop on deep learning and unsupervised feature learning,",
    "year": 2011
  }, {
    "title": "Recurrent processing during object recognition",
    "authors": ["R.C. O’Reilly", "D. Wyatte", "S. Herd", "B. Mingus", "D.J. Jilk"],
    "venue": "Frontiers in psychology,",
    "year": 2013
  }, {
    "title": "Dynamic model of visual recognition predicts neural response properties in the visual cortex",
    "authors": ["R.P. Rao", "D.H. Ballard"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Predictive coding in the visual cortex: a functional interpretation of some extraclassical receptive-field effects",
    "authors": ["R.P. Rao", "D.H. Ballard"],
    "venue": "Nature neuroscience,",
    "year": 1999
  }, {
    "title": "Fitnets: Hints for thin deep nets",
    "authors": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1412.6550,",
    "year": 2014
  }, {
    "title": "A feedforward architecture accounts for rapid categorization",
    "authors": ["T. Serre", "A. Oliva", "T. Poggio"],
    "venue": "Proceedings of the national academy of sciences,",
    "year": 2007
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "Recurrent convolutional neural networks: a better model of biological object recognition",
    "authors": ["C.J. Spoerer", "P. McClure", "N. Kriegeskorte"],
    "venue": "Frontiers in psychology,",
    "year": 2017
  }, {
    "title": "Predictive coding as a model of biased competition in visual attention",
    "authors": ["M.W. Spratling"],
    "venue": "Vision research,",
    "year": 2008
  }, {
    "title": "A hierarchical predictive coding model of object recognition in natural images",
    "authors": ["M.W. Spratling"],
    "venue": "Cognitive computation,",
    "year": 2017
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "Deep networks with internal selective attention through feedback connections",
    "authors": ["M.F. Stollenga", "J. Masci", "F. Gomez", "J. Schmidhuber"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Beta-and gamma-band activity reflect predictive coding in the processing of causal events",
    "authors": ["S. van Pelt", "L. Heil", "J. Kwisthout", "S. Ondobaka", "I. van Rooij", "H. Bekkering"],
    "venue": "Social cognitive and affective neuroscience,",
    "year": 2016
  }, {
    "title": "Neural encoding and decoding with deep learning for dynamic natural vision",
    "authors": ["H. Wen", "J. Shi", "Y. Zhang", "Lu", "K.-H", "J. Cao", "Z. Liu"],
    "venue": "Cerebral Cortex,",
    "year": 2017
  }, {
    "title": "Early recurrent feedback facilitates visual object recognition under challenging conditions",
    "authors": ["D. Wyatte", "D.J. Jilk", "R.C. O’Reilly"],
    "venue": "Frontiers in psychology,",
    "year": 2014
  }, {
    "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
    "authors": ["D.L. Yamins", "H. Hong", "C.F. Cadieu", "E.A. Solomon", "D. Seibert", "J.J. DiCarlo"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2014
  }, {
    "title": "Stochastic pooling for regularization of deep convolutional neural networks",
    "authors": ["M.D. Zeiler", "R. Fergus"],
    "venue": "arXiv preprint arXiv:1301.3557,",
    "year": 2013
  }],
  "id": "SP:590d4bf689b271125c0c81cf7e4f4fdf7ffaa3c0",
  "authors": [{
    "name": "Haiguang Wen",
    "affiliations": []
  }, {
    "name": "Kuan Han",
    "affiliations": []
  }, {
    "name": "Junxing Shi",
    "affiliations": []
  }, {
    "name": "Yizhen Zhang",
    "affiliations": []
  }, {
    "name": "Eugenio Culurciello",
    "affiliations": []
  }, {
    "name": "Zhongming Liu",
    "affiliations": []
  }],
  "abstractText": "Based on the predictive coding theory in neuroscience, we designed a bi-directional and recurrent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the prediction of its lower-layer representation; feedforward connections carry the prediction errors to its higher-layer. Given image input, PCN runs recursive cycles of bottom-up and top-down computation to update its internal representations and reduce the difference between bottom-up input and top-down prediction at every layer. After multiple cycles of recursive updating, the representation is used for image classification. With benchmark datasets (CIFAR-10/100, SVHN, and MNIST), PCN was found to always outperform its feedforward-only counterpart: a model without any mechanism for recurrent dynamics, and its performance tended to improve given more cycles of computation over time. In short, PCN reuses a single architecture to recursively run bottom-up and top-down processes to refine its representation towards more accurate and definitive object recognition.",
  "title": "Deep Predictive Coding Network for Object Recognition"
}