{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 24–34 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Previous work showed that treating semantic dependency parsing as the search for Maximum Subgraphs is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017). In particular, our previous work showed that 1-endpoint-crossing, pagenumber-2 (1EC/P2) graphs are an appropriate graph class for modelling semantic dependency structures (Cao et al., 2017). On the one hand, it is highly expressive to cover a majority of semantic analysis. On the other hand, the corresponding Maximum Subgraph problem with an arc-factored disambiguation model can be solved in low-degree polynomial time.\nDefining disambiguation models on wider contexts than individual bi-lexical dependencies improves various syntactic parsers in different architectures. This paper studies exact algorithms for second-order parsing for 1EC/P2 graphs. The existing algorithm, viz. our previous algorithm\n(GCHSW, hereafter), has two properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW. In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity.\nIn this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1EC/P2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it separates the construction for noncrossing edges and possible crossing edges; (2) whether an edge is created is deterministic in each construction rule. We then introduce a new algorithm to perform secondorder parsing. When all second-order scores are greater than or equal to 0, it exactly solves the corresponding optimization problem.\nWe implement a practical parser with a statistical disambiguation model and evaluate it on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our second-order parsing models are more ac-\n24\ncurate than the first-order baseline. If we do not use features derived from syntactic trees, we get an absolute unlabeled F-score improvement of 1.3 on average. When syntactic analysis is used, we get an improvement of 0.4 on average."
  }, {
    "heading": "2 Preliminaries",
    "text": ""
  }, {
    "heading": "2.1 Maximum Subgraph Parsing",
    "text": "Semantic dependency parsing can be formulated as the search for Maximum Subgraph for graph class G: Given a graph G = (V,A), find a subset A′ ⊆ A with maximum total score such that the induced subgraph G′ = (V,A′) belongs to G. Formally, we have the following optimization problem:\narg max G∗∈G(s,G) ∑ p in G∗ spart(s, p)\nG(s, G) denotes the set of all graphs that belong to G and are compatible with s and G. G is usually a complete digraph. spart(s, p) evaluates the event that part p (from a candidate graph G∗) is good. We define the order of p according to the number of arcs it contains, in analogy with tree parsing in terminology. Previous work only discussed the first-order case:\narg max G∗∈G(G) ∑ d∈ARC(G∗) sarc(d)\nIf G is the set of noncrossing or 1EC/P2 graphs, the above optimization problem can be solved in cubic-time (Kuhlmann and Jonsson, 2015) and quintic-time (Cao et al., 2017) respectively. Furthermore, ignoring one linguistically-rare structure in 1EC/P2 graphs descreases the complexity to O(n4). This paper is concerned with secondorder parsing, with a special focus on the following factorizations:\nAnd the objective function turns to be:∑ d∈ARC(G∗) sarc(d) + ∑ s∈SIB(G∗) ssib(s)\nSun et al. (2017) introduced a dynamic programming algorithm for second-order planar parsing. Their empirical evaluation showed that secondorder features are effective to improve parsing accuracy. It is still unknown how to incorporate such features for 1EC/P2 parsing."
  }, {
    "heading": "2.2 1-Endpoint-Crossing, Pagenumber-2 Graphs",
    "text": "The formal description of the 1-endpoint-crossing property is adopted from (Pitler et al., 2013).\nDefinition 1. Edges e1 and e2 cross if e1 and e2 have distinct endpoints and exactly one of the endpoints of e1 lies between the endpoints of e2.\nDefinition 2. A dependency graph is 1-EndpointCrossing if for any edge e, all edges that cross e share an endpoint p named pencil point.\nGiven a sentence s = w0w1 · · ·wn−1 of length n, the vertices, i.e. words, are indexed with integers, an arc from wi to wj as a(i,j), and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j), if we do not consider its direction. Figure 1 is an example.\nDefinition 3. A pagenumber-k graph means it consists at most k half-planes, and arcs on each half-plane are noncrossing.\nThese half-planes may be thought of as the pages of a book, with the vertex line corresponding to the books spine, and the embedding of a graph into such a structure is known as a book embedding. Figure 2 is an example.\n(Pitler et al., 2013) proved that 1-endpointcrossing trees are a subclass of graphs whose pagenumber is at most 2. In Cao et al. (2017), we studied graphs that are constrained to be both 1-endpoint-crossing and pagenumber-2. In this paper, we ignored a complex and linguistic-rare\nstructure and studied a subset of 1EC/P2 graphs. The complex structure is named as C structures in our previous paper, and Figure 3 is the prototype of C structures. In this paper, we present new algorithms for finding optimal 1EC/P2, C-free graphs."
  }, {
    "heading": "2.3 The GCHSWAlgorithm",
    "text": "Cao et al. (2017) designed a polynomial time Maximum Subgraph algorithm, viz. GCHSW, for 1EC/P2 graphs by exploring the following property: Every subgraph of a 1EC/P2 graph is also a 1EC/P2 graph. GCHSW defines a number of prototype backbones for decomposing a 1EC/P2 graph in a principled way. In each decomposition step, GCHSW focuses on the edges that can be created without violating either the 1EC nor P2 restriction. Sometimes, multiple edges can be created simultaneously in one single step. Figure 4 is an example.\nThere is an important difference between GCHSW and Eisner-style Maximum Spanning Tree algorithms (MST; Eisner, 1996; McDonald and Pereira, 2006; Koo and Collins, 2010). In each construction step, GCHSW allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1EC/P2 graphs. For the higher-order MST algorithms, in a single construction step, it is clear whether adding a new arc, and which one. There is no local search. This deterministic strategy is also followed by Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. Higher-order MST models associate higher-order score functions with the construction of individual dependencies. Therefore the deterministic strategy is a prerequisite to incorporate higher-order features. The design of GCHSW is incompatible with this strategy."
  }, {
    "heading": "2.4 Challenge of Second-Order Decoding",
    "text": "It is very difficult to enumerate all high-order features for crossing arcs. Figure 5 illustrates the idea. There is a pair of corssing arcs, viz. e(x,k) and e(i,j). The key strategy to develop a dynamic programming algorithm to generate such crossing structure is to treat parts of this structures as intervals/spans together with an external vertex (Pitler et al., 2013; Cao et al., 2017). Without loss of generality, we assume [i, j] makes up such an interval and x is the corresponding external vertex. When we consider e(i,j), its neighboring edges can be e(i,ri) and e(lj ,j), and therefore we need to consider searching the best positions of both ri and lj . Because we have already taken into account three vertices, viz. x, i and j, the two new positions increase the time complexity to be at least quintic.\nNow consider e(x,k). When we decompose the whole graph into inverval [i, j] plus x and remaining part, we will factor out e(x,k) in a successive decomposition for resolving [i, j] plus x. We cannot capture the second features associated to e(x,k) and e(x,rx), because they are in different intervals, and when these intervals are combined, we have already hidden the position information of k. Explicitly encoding k increases the time complexity to be at least quintic too.\nPitler (2014) showed that it is still possible to build accurate tree parsers by considering only higher-order features of noncrossing arcs. This is in part because only a tiny fraction of neighboring arcs involve crossing arcs. However, this strategy is not easy to by applied to GCHSW, because GCHSW does not explicitly analyze sub-graphs of noncrossing arcs."
  }, {
    "heading": "3 A New Maximum Subgraph Algorithm",
    "text": "Based on the discussion of Section 2.3 and 2.4, we can see that it is not easy to extend the existing algorithm, viz. GCHSW, to handle second-order features. In this paper, we propose an alternative first-order dynamic programming algorithm. Because ignoring one linguistically-rare structure associated with the C problem in GCHSW descreases the complexity, we exclude this structure in our algorithm. Formally, we introduce a new algorithm\nIntO(i, j)← max IntO(i + 1, j) IntC(i, j) IntC(i, k) + IntO(k, j) RC(i, k, x) + IntO(k, x) + LO(x, j, k) + sarc(i, k) LR(i, k, x) + IntO(k, x) + IntO(x, j, k) + sarc(i, k) IntO[i, x] + LC [x, k, i] + NO[k, j, x] + sarc(i, k) RO[i, x, k] + IntO[x, k] + LO[k, j, x] + sarc(i, k)\nIntC(i, j)← sarc(i, j) + max IntO(i + 1, j) IntC(i, k) + IntO(k, j) RC(i, k, x) + IntO(k, x) + LO(x, j, k) + sarc(i, k) LR(i, k, x) + IntO(k, x) + IntO(x, j, k) + sarc(i, k) IntO[i, x] + LC [x, k, i] + NO[k, j, x] + sarc(i, k) RO[i, x, k] + IntO[x, k] + LO[k, j, x] + sarc(i, k)\nto solve the following optimization problem:\narg max G∗∈G(G) ∑ d∈ARC(G∗) sarc(d)\nwhere G means 1EC/P2, C-free graphs. Our algorithm has the same time and space complexity to the degenerated version of GCHSW. We represent our algorithm using undirected graphs."
  }, {
    "heading": "3.1 Sub-problems",
    "text": "Following GCHSW, we consider five sub-problems when we construct a maximum dependency graph on a given interval [i, k]. Though the subproblems introduced by GCHSW and us handle similar structures, their definitions are quite different. The sub-problems are explained as follows:\nInt Int[i, j] represents an interval from i to j inclusively. And there is no edge e(i′,j′) such that i′ ∈ [i, j] and j′ /∈ [i, j]. We distinguish two sub-types for Int. IntO[i, j] may or may not contain e(i,j), while IntC [i, j] contains e(i,j).\nLR LR[i, j, x] represents an interval from i to j inclusively and an external vertex x. ∀p ∈\n[i, j], pt(x, p) = i or j. LR[i, j, x] implies the existence of e(i,j), but does not contain e(i,j). When LR[i, j, x] is combined with other DP sub-structures, e(i,j) is immediately created. LR[i, j, x] disallows neither e(x,i) nor e(x,j).\nN N [i, j, x] represents an interval from i to j inclusively and an external vertex x. ∀p ∈ [i, j], pt(x, p) /∈ [i, j]. N [i, j, x] could contain e(i,j) but disallows e(x,i). We distinguish two sub-types. NO[i, j, x] may or may not contain e(x,j). NC [i, j, x] implies the existence of but does not contain e(x,j). When N [i, j, x] is combined with others, e(x,j) is immediately created.\nL L[i, j, x] represents an interval from i to j inclusively as well as an external vertex x. ∀p ∈ [i, j], pt(x, p) = i. L[i, j, x] could contain e(i,j) but disallows e(x,i). We distinguish sub-two types for L. LO[i, j, x] may or may not contain e(x,j). LC [i, j, x] implies the existence of but does not contain e(x,j). When it is combined with others, e(x,j) is immediately created.\nR R[i, j, x] represents an interval from i to j inclusively as well as an external vertex x. ∀p ∈ [i, j], pt(x, p) = j. R[i, j, x] disallows e(x,j) and e(x,i). We distinguish two sub-types for R. RO[i, j, x] may or may not contain e(i,j). RC [i, j, x] implies the existence of but does not contain e(i,j). When it is combined with others, e(i,j) is immediately created."
  }, {
    "heading": "3.2 Decomposing Sub-problems",
    "text": "Figure 7 gives a sketch of our dynamic programming algorithm. We give a detailed illustration for Int, a rough idea for L and LR, and omit other sub-problems. More details about the whole algorithm can be found in the supplementary note."
  }, {
    "heading": "3.2.1 Decomposing an Int Sub-problem",
    "text": "Consider IntO[i, j] and IntC [i, j] sub-problem. Because the decomposition for IntC [i, j] is very similar to IntO[i, j] and needs to be modified by our second-order parsing algorithm, we only show the decomposition of IntC [i, j]. Assume that k(k ∈ (i, j)) is the farthest vertex that is adjacent to i, and x = pt(i, k). If there is no such k (i.e. there no arc from i to some other node in this interval), then we denote k as ∅. So it is to x. We illustrate different cases as following and give a graphical representation in Figure 8.\nCase a: k = ∅. We can directly consider interval [i + 1, j]. Because there is no edge from i to any node in [i + 1, j], [i + 1, j] is an IntO.\nCase b: x = ∅. x = ∅ means that e(i,k) does not cross other arcs. So [i, k] and [k, j] are Int.\nCase c: x ∈ (k, j]. e(i,k) is taken as a possible crossing edge. k and x divide the interval [i, j] into three parts: [i, k], [k, x], [x, j]. Because x may be j, interval [x, j] may only contain j and become an empty interval. We define x′ as the pencil point of all edges from (i, k) to x, and distinguish two sub-problems as follows.\nc.1 Assume that there exists an edge from k to some node r in (x, j], so x′ can only be k and pencil point of edges from k to (x, j] is x. Thus interval [i, k, x] is an R. Due to the existence of e(i,k), its sub-type is RC. The e(i,k) is created in this construction and thus not contained by RC [i, k, x]. An edge from within [k, x] to outside violates the 1EC restriction, so [k, x] is an Int. Since x is endpoint of edge\nfrom k to [x, r], interval [k, j] is an LO with external vertex k.\nc.2 We assume no edge from k to any node in [x, j], x′ thus can be i or k. As a result, [x, j] is an Int and [i, k, x] is an LR.\nCase d: x ∈ (i, k).\nd.1 Assume that there exist edges from i to (x, k), so the pencil point of edges from x to (k, j] is i. Therefore [k, j] is an N. Because x is pencil point of edges from i to (x, k], [x, k] is an L. Furthmore, it is an LC because we generate e(i,k) in this step. It is obvious that [i, x] is an Int.\nd.2 Assume that there exists edges from k to (i, x), and the pencil point of edges from x to (k, j] is thus k. Similar to the above analysis, we reach RO[i, x, k]+IntO[x, k]+ LO[k, j, x] + e(i,k) + e(i,j).\nFor IntO[i, j], because there may be e(i,j), we add one more rule: IntO[i, j] = IntC [i, j]. And we do not need to create e(i,j) in all cases."
  }, {
    "heading": "3.2.2 Decomposing an L Sub-problem",
    "text": "Without loss of generality, we show the decomposition of LO[i, j, x] as follows. For LC [i, j, x], we ignore Case b but follow the others.\nCase a. If there is no more edge from x to (i, j], then it will degenerate to IntO[i, j].\nCase b. If there exists e(x,j), then it will degenerate to LC [i, j, x] + e(x,j).\nCase c. Assume that there are edges from x to (i, j) and e(x,k) is the farthest one. It divides [i, j] into [i, k] and [k, j].\nc.1 If there is an edge from x to (i, k), [i, k] and [k, j] are LC [i, k, x] and NO[k, j, i].\nc.2 If there is no edge from x to (i, k), [i, k] and [k, j] are IntO[i, k] and LO[k, j, i].\nFigure 8 is a graphical representation."
  }, {
    "heading": "3.2.3 Decomposing an LR Sub-problem",
    "text": "LR[i, j, x] means i or j is the pencil point of edges from x to (i, j). We show the decomposition of LR[i, j, x] as follows:\nCase a. If there is a vertex k within (i, j), which divides [i, j] into [i, k] and [k, j]. And it guarantees no edge from [i, k) to (k, j]. i is the pencil point of edges from x to (i, k] because no edge from j to (i, k) can cross these edges. Similarly j has to be the pencil point of edges from x to (k, j). Obviously, [i, k] is an LO and [k, j] is an RO with external x. Thus the problem is decomposed as LO[i, k, x] + RO[k, j, x].\nCase b. If there is no such vertex k, there must be edges from [i, k′) to (k′, j] for every k′ in (i, j) without considering e(i,j). For i + 1, we assume e(i,a1) is the farthest edge that goes from i. For a1, we assume e(b1,b2) is the farthest edge from b1 where b1 is in (i, a1) and b2 is in (a1, j). For b2, we assume e(a1,a3) is the farthest edge from a1 where a3 is in (b2, j) and a1 is the pencil point. We then get the series {a1, a2, a3...an} and {b1, b2...bm}which guarantees bi < ai , ai < bi+1 and max(an, bm) = j.\nIf bm = j, we will get a graph like Figure 10. If e(x,b1) exists, this LR subproblem degenerates to an L subproblem. If e(x,an) exists, this subproblem degenerates to an R subproblem.\nIf am = j, we will get a graph like Figure 11. If there exists only e(x,b1) or e(x,bm), we can solve it like bm = j. If both exist, this is a typical C-\nstructure like Figure 3 and we cannot get it through other decompostion.\nThe above discussion gives the rough idea of the correctness of the following conclusion. Theorem 1. Our new algorithm is sound and complete with respect to 1EC/P2, C-free graphs."
  }, {
    "heading": "3.3 Spurious Ambiguity",
    "text": "An LR, L, R or N sub-problem allows to build crossing arcs, but does not necessarily create crossing arcs. For example, LC [i, j, x] allows e(i,j) to cross with e(x,y) (y ∈ (i, j)). Because every subgraph of a 1EC/P2 graph is also a 1EC/P2 graph, we allow an LC [i, j, x] to be directly degenerated to IO[i, j]. In this way, we can make sure that all subgraphs can be constructed by our algorithm. Figure 12 shows the rough idea. To generate the same graph, we have different derivations. The spurious ambiguity in our algorithm does not affect the correctness of first-order parsing, because scores are assigned to individual dependencies, rather than derivation processes. There is no need to distinguish one special derivation here."
  }, {
    "heading": "4 Quasi-Second-Order Extension",
    "text": "We propose a second-order extension of our new algorithm. We focus on factorizations introduced in Section 2.1. Especially, the two arcs in a factor should not cross other arcs. Formally, we introduce a new algorithm to solve the optimization problem with the following objective:∑\nd∈ARC(G∗) sarc(d) + ∑ s∈SIB(G∗) max(ssib(s), 0)\nIn the first-order algorithm, all noncrossing edges can be constructed as the frontier edge of an IntC.\nSo we can develop an exact decoding algorithm by modifying the composition for IntC while keeping intact the decomposition for LR, N, L, R.\n4.1 New Decomposition for IntC In order to capture the second-order features from noncrossing neighbors, we need to find the rightmost node adjacent to i, denoted as ri, and the leftmost node adjacent to j, denoted as lj ,while i < ri ≤ lj < j. To do this, we split IntC [i, j] into at most three parts to capture the sibling factors. Denote the score of adjacent edges e(i,j1) and e(i,j2) as s2(i, j1, j2). When j is the inner most node adjacent to i, we denote the score as s2(i, ∅, j). We give a sketch of the decomposition in Figure 14 and a graphical representation in Figure 13. The following is a rough illustration.\nCase a: ri = ∅. We further distinguish three sub-problems:\na.1 If lj = ∅ too, both sides are the inner most second-order factor.\na.2 There is a crossing arc from j. This case is handled in the same way as the first-order algorithm.\na.3 lj 6= ∅. We introduce a new decomposition rule.\nCase b: There is a crossing arc from i.\nb.1 lj = ∅. Similar case to (a.2). b.2 There is a crossing arc from j. Similar case\nto (a.2).\nb.3 There is a noncrossing arc from j. We introduce a new rule to calculate SIB(j, lj , i).\nCase c: There is a noncrossing arc from i.\nc.1 lj = ∅. Similar to (a.3).\nc.2 There is a crossing arc from j. Similar to (b.3).\nc.3 There is a noncrossing arc from j too. We introduce a new rule to calculate SIB(i, ri, j) and SIB(j, lj , i)."
  }, {
    "heading": "4.2 Complexity",
    "text": "The complexity of both first- and second-order algorithms can be analyzed in the same way. The sub-problem Int is of size O(n2), with a calculating time of order O(n2) at most. For sub-problems L, R, LR, and N, each has O(n3) elements, with a unit calculating time O(n). Therefore both algorithms run in time of O(n4) with a space requirement of O(n3)."
  }, {
    "heading": "4.3 Discussion",
    "text": "A traditional second-order model takes as the objective function ∑ s∈SIB(G∗) ssib(s).\nOur model instead tries to optimize∑ s∈SIB(G∗) max(ssib(s), 0). This model is somehow inadequate given that the second-order score function cannot penalize a bad factor. When a negative score is assigned to a second-order factor, it will be taken as 0 by our algorithm.\nThis inadequacy is due to the spurious ambiguity problem that is illustrated in Section 3.3. Take the two derivations in Figure 12 for example. The derivation that starts from IntC [a, e]⇒ IntC [a, c]+IntO[c, e] incorporates the second-order score ssib(a, c, e). This is different when we consider the derivation that starts from IntC [a, e] ⇒ LR[a, c, d] + IntO[k, d] + LO[d, e, c]. Because we assume temporarily that e(a,c) crosses others, we do not consider ssib(a, c, e). We can see from this example that second-order scores not only depend on the derived graphs but also sensitive to the derivation processes.\nIf a second-order score is greater than 0, our algorithm selects the derivation that takes it into account since it increases the total score. If a secondorder score is negative, our algorithm avoids including it by selecting other paths. In other words, our algorithm treats this score as 0."
  }, {
    "heading": "5 Practical Parsing",
    "text": ""
  }, {
    "heading": "5.1 Derivation-Sensitive Training",
    "text": "We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002).\nOur training proceudre is sensitive to derivation rather then derived graphs. For each sentence, we first apply our algorithm to find the optimal prediction derivation. The we collect all first- and second-order factors from this derivation to update parameters. To train a first-order model, because our algorithm includes all factors, viz. depencies, there is no difference between our derivationbased method and a traditional derived structurebased method. For the second-order model, our method increases the second-order scores somehow."
  }, {
    "heading": "5.2 Data and Preprocessing",
    "text": "We evaluate first- and second-order models on four representative data sets: CCGBank (Hockenmaier and Steedman, 2007), DeepBank (Flickinger et al., 2012), Enju HPSGBank (Miyao et al., 2005) and Prague Dependency TreeBank (Hajic et al., 2012). We use “standard” training, validation, and test splits to facilitate comparisons.\n• Following previous experimental setup for English CCG parsing, we use section 02-21 as training data, section 00 as the development data, and section 23 for testing.\n• The DeepBank, Enju HPSGBank and Prague Dependency TreeBank are from SemEval 2014 Task 8 (Oepen et al., 2014), and the data splitting policy follows the shared task.\nExperiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. All experimental results consider directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task."
  }, {
    "heading": "5.3 Accuracy",
    "text": "Table 1 lists the accuracy of our system. The output of our parser was evaluated against each dependency in the corpus. We can see that the firstorder parser obtains a considerably good accuracy, with rich syntactic features. Furthermore, we can see that the introduction of higher-order features improves parsing substantially for all data sets, as expected. When syntactic trees are utilized, the\nimprovement is smaller but still significant on the three SemEval data sets.\nTable 2 lists the parsing results on the test data together with the result obtained by Sun et al. (SJW; 2017)’s system. The building architectures of both systems are comparable.\n1. Both systems have explicit control of the output structures. While Sun et al.’s system constrain the output graph to be P2 only, our system adds an additional 1EC restriction.\n2. Their system’s second-order features also includes both-side neighboring features.\n3. Their system uses beam search and dual decomposition and therefore approximate, while ours perform exact decoding.\nWe can see that while our purely Maximum Subgraph parser obtains better results on DeepBank and CCGBank; while the book embedding parser is better on the other two data sets."
  }, {
    "heading": "5.4 Analysis",
    "text": "Our algorithm is sensitive to the derivation process and may exclude a couple of negative secondorder scores by selecting misleading derivations. Neverthess, our algorithm works in an exact way to include all positive second-order scores. Table 3 shows the coverage of all second-order factors. On average, 99.67% second-order factors are calculated by our algorithm. This relatively satisfactory coverage suggests that our algorithm is very effective to include second-order features. Only a very small portion is dropped."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper proposed two exact, graph-based algorithms for 1EC/P2 parsing with first-order and quasi-second-order scores. The resulting parser has the same asymptotic run time as Cao et al. (2017)’s algorithm. An exploration of other factorizations that facilitate semantic dependency parsing may be an interesting avenue for future work. Recent work has investigated faster decoding for higher-order graph-based projective parsing e.g. vine pruning (Rush and Petrov, 2012) and cube pruning (Zhang and McDonald, 2012). It would be interesting to extend these lines of work to decrease the complexity of our quartic algorithm."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by 863 Program of China (2015AA015403), NSFC (61331011), and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). Weiwei Sun is the corresponding author."
  }],
  "year": 2017,
  "references": [{
    "title": "Top accuracy and fast dependency parsing is not a contradiction",
    "authors": ["Bernd Bohnet."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling 2010 Organizing Committee, Beijing, China, pages 89–97.",
    "year": 2010
  }, {
    "title": "Parsing to 1-endpoint-crossing, pagenumber-2 graphs",
    "authors": ["Junjie Cao", "Sheng Huang", "Weiwei Sun", "Xiaojun Wan."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational",
    "year": 2017
  }, {
    "title": "Experiments with a higherorder projective dependency parser",
    "authors": ["Xavier Carreras."],
    "venue": "In Proc. EMNLP-CoNLL.",
    "year": 2007
  }, {
    "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing. Asso-",
    "year": 2002
  }, {
    "title": "Three new probabilistic models for dependency parsing: an exploration",
    "authors": ["Jason M. Eisner."],
    "venue": "Proceedings of the 16th conference on Computational linguistics - Volume 1. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 340–345.",
    "year": 1996
  }, {
    "title": "Deepbank: A dynamically annotated treebank of the wall street journal",
    "authors": ["Daniel Flickinger", "Yi Zhang", "Valia Kordoni."],
    "venue": "Proceedings of the Eleventh International Workshop on Treebanks and Linguistic Theories. pages 85–96.",
    "year": 2012
  }, {
    "title": "CCGbank: A corpus of CCG derivations and dependency structures extracted from the penn treebank",
    "authors": ["Julia Hockenmaier", "Mark Steedman."],
    "venue": "Computational Linguistics 33(3):355–396.",
    "year": 2007
  }, {
    "title": "Self-training with products of latent variable grammars",
    "authors": ["Zhongqiang Huang", "Mary Harper", "Slav Petrov."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
    "year": 2010
  }, {
    "title": "Efficient thirdorder dependency parsers",
    "authors": ["Terry Koo", "Michael Collins."],
    "venue": "Proceedings of the",
    "year": 2010
  }, {
    "title": "Parsing to noncrossing dependency graphs",
    "authors": ["Marco Kuhlmann", "Peter Jonsson."],
    "venue": "Transactions of the Association for Computational Linguistics 3:559– 570.",
    "year": 2015
  }, {
    "title": "Online learning of approximate dependency parsing algorithms",
    "authors": ["Ryan McDonald", "Fernando Pereira."],
    "venue": "Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006)). volume 6, pages",
    "year": 2006
  }, {
    "title": "Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank",
    "authors": ["Yusuke Miyao", "Takashi Ninomiya", "Jun’ichi Tsujii"],
    "venue": "In IJCNLP",
    "year": 2005
  }, {
    "title": "Semeval 2014 task 8: Broad-coverage semantic dependency parsing",
    "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajic", "Angelina Ivanova", "Yi Zhang."],
    "venue": "Proceedings of the 8th International Work-",
    "year": 2014
  }, {
    "title": "A crossing-sensitive thirdorder factorization for dependency parsing",
    "authors": ["Emily Pitler."],
    "venue": "TACL 2:41–54. http://www.transacl.org/wpcontent/uploads/2014/02/39.pdf.",
    "year": 2014
  }, {
    "title": "Finding optimal 1-endpoint-crossing trees",
    "authors": ["Emily Pitler", "Sampath Kannan", "Mitchell Marcus."],
    "venue": "TACL 1:13–24. http://www.transacl.org/wpcontent/uploads/2013/03/paper13.pdf.",
    "year": 2013
  }, {
    "title": "Vine pruning for efficient multi-pass dependency parsing",
    "authors": ["Alexander Rush", "Slav Petrov."],
    "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2012
  }, {
    "title": "Semantic dependency parsing via book embedding",
    "authors": ["Weiwei Sun", "Junjie Cao", "Xiaojun Wan."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Generalized higher-order dependency parsing with cube pruning",
    "authors": ["Hao Zhang", "Ryan McDonald."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
    "year": 2012
  }, {
    "title": "Transition-based parsing for deep dependency structures",
    "authors": ["Xun Zhang", "Yantao Du", "Weiwei Sun", "Xiaojun Wan."],
    "venue": "Computational Linguistics 42(3):353–389. http://aclweb.org/anthology/J163001.",
    "year": 2016
  }],
  "id": "SP:1fb6966d6a36b2c5c631dc71dcfc5a53cac2e69e",
  "authors": [{
    "name": "Junjie Cao",
    "affiliations": []
  }, {
    "name": "Sheng Huang",
    "affiliations": []
  }, {
    "name": "Weiwei Sun",
    "affiliations": []
  }, {
    "name": "Xiaojun Wan",
    "affiliations": []
  }],
  "abstractText": "We propose a new Maximum Subgraph algorithm for first-order parsing to 1endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing.",
  "title": "Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs"
}