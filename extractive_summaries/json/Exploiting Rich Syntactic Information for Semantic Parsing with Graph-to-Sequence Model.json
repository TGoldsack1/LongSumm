{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 918–924 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n918"
  }, {
    "heading": "1 Introduction",
    "text": "The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse.\nHowever, the syntactic features capture important structural information of the natural lan-\n∗ Work done when the author was at IBM Research.\nguage input, which complements the simple word sequence. For example, a dependency graph presents grammatical relations that hold among the words; and a constituent tree provides a phrase structure representation. Intuitively, by incorporating such additional information, the encoder could produce a more meaningful and robust sentence representation. The combination of these features (i.e., sequence + trees) forms a general graph structure (see Figure 1). This inspires us to apply a graph encoder to produce a representation of a graph-structured input. The graph encoder also has the advantages that it could simultaneously encode all types of syntactic contexts, and incorporate multiple types of syntactic structures in a unified way.\nIn this paper, we first introduce a structure, namely syntactic graph, to represent three types of syntactic information, i.e., word order, dependency and constituency features (see §2). We then employ a novel graph-to-sequence (Graph2Seq) model (Xu et al., 2018), which consists of a graph encoder and a sequence decoder, to learn the representation of the syntactic graph (see §3). Specifically, the graph encoder learns the representation of each node by aggregating information from its K-hop neighbors. Given the learned node embeddings, the graph encoder uses a pooling-based method to generate the graph embedding. On the decoder side, a Recurrent Neural Network (RNN) decoder takes the graph embedding as its initial hidden state to generate the logical form while employing an attention mechanism over the node embeddings. Experimental results show that our model achieves the competitive performance on Jobs640, ATIS, and Geo880 datasets.\nDifferent from existing works, we also investigate the robustness of our model by evaluating the model on two types of adversarial examples (Belinkov and Bisk, 2017; Cheng et al., 2018).\nExperimental results show that the model coupling all syntactic features has the best robustness, achieving the best performance. Our code and data is available at https://github.com/IBM/ Text-to-LogicForm."
  }, {
    "heading": "2 Syntactic Graph",
    "text": "We represent three types of syntactic features, i.e., word order, dependency parse and constituency parse, as the syntactic graph (see Figure 1). •Word Order Features. Previous neural semantic parsers mainly use these features by building a SeqLSTM that works on the word sequence. Our syntactic graph also incorporates this information by generating a node for each word and connecting them in the chain form. In order to capture the forward and backward contextual information, we link these nodes in two directions, that is, from left to right and from right to left. • Dependency Features. A dependency parse describes the grammatical relations that hold among words. Reddy et al. (2016, 2017) have demonstrated that the dependency parse tree could be directly transformed to a logical form, which indicates that the dependency information (i.e., tree structure and dependency labels) is critical to the semantic parsing task. We incorporate this information into the syntactic graph by adding directed edges between the word nodes and assign them with dependency labels. • Constituency Features. Similar to the dependency parse, the constituency parse represents the phrase structure, which is also important to the semantic parsing task. Take Figure 1 as an example: given the constituent tree that explicitly annotates “not related with AI” (node #1) is a proposition phrase, the model could learn a meaningful embedding for this phrase by encoding this structure into the model. Motivated by this observation, we\nadd the non-terminal nodes of the constituent tree and the edges describing their parent-child relationships into the syntactic graph."
  }, {
    "heading": "3 Graph-to-sequence Model for Semantic Parsing",
    "text": "After building the syntactic graph for the input text, we employ a novel graph-to-sequence model (Xu et al., 2018), which includes a graph encoder and a sequence decoder with attention mechanism, to map the syntactic graph to the logical form. Conceptually, the graph encoder generates node embeddings for each node by accumulating information from its K-hop neighbors, and then produces a graph embedding for the entire graph by abstracting all these node embeddings. Next, the sequence decoder takes the graph embedding as the initial hidden state, and calculates the attention over all node embeddings on the encoder side to generate logical forms. Note that this graph encoder does not explicitly encode the edge label information, therefore, for each labeled edge, we add a node whose text attribute is the edge’s label.\nNode Embedding. Given the syntactic graph G = (V, E), we take the embedding generation process for node v ∈ V as an example to explain the node embedding generation algorithm1: (1) We first transform node v’s text attribute to a feature vector, av, by looking up the embedding matrix We; (2) The neighbors of v are categorized into forward neighbors N`(v) and backward neighbors Na(v) according to the edge direction. In particular, N`(v) returns the nodes that v directs to and Na(v) returns the nodes that direct to v; (3) We aggregate the forward representations of v’s forward neighbors {hk−1u` , ∀u ∈ N`(v)} into\n1Interested readers may refer to (Xu et al., 2018) for more implementation details.\na single vector, hkN`(v), where k∈{1, ...,K} is the iteration index. Specifically, this aggregator feeds each neighbor’s vector to a fully-connected neural network and applies an element-wise max-pooling operation to capture different aspects of the neighbor set. Notice that, at iteration k, this aggregator only uses the representations generated at k − 1. The initial forward representation of each node is its feature vector calculated in step (1); (4) We concatenate v’s current forward representation hk−1v` with the newly generated neighborhood vector hkN`(v). The resulted vector is fed into a fully connected layer with nonlinear activation function σ, which updates the forward representation of v, hkv`, to be used at the next iteration; (5) We update the backward representation of v, hkva using the similar procedure as introduced in step (3) and (4) except that operating on the backward representations; (6) We repeat steps (3)∼(5) K times, and the concatenation of the final forward and backward representations is used as the final representation of v. Since the neighbor information from different hops may have different impacts on the node embedding, we learn a distinct aggregator at each iteration. Graph Embedding. We feed the obtained node embeddings into a fully-connected neural network, and apply the element-wise max-pooling operation on all node embeddings. We did not find substantial performance improvement using mean-pooling. Sequence Decoding. The decoder is an RNN which predicts the next token yi given all the previous words y<i = y1, ..., yi−1, the RNN hidden state si for time-step i and the context vector ci that captures the attention of the encoder side. In particular, the context vector ci depends on a set of node representations (h1,...,hV ) to which the encoder maps the input graph. The context vector ci is dynamically computed using an attention mechanism over the node representations. The whole model is jointly trained to maximize the conditional log-probability of the correct description given a source graph. In the inference phase, we use the beam search algorithm to generate a description with beam size = 3."
  }, {
    "heading": "4 Experiments",
    "text": "We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings;\nGeo880, a set of 880 queries to a database of U.S. geography; and ATIS, a set of 5,410 queries to a flight booking system. We use the standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric.\nThe model is trained using the Adam optimizer (Kingma and Ba, 2014), with mini-batch size 30. Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hidden state size is 300. The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting. We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized. We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees.\nResults and Discussion. Table 1 summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information. Comparison with Baseline. To better demonstrate that our work is an effective way to utilize both multiple trees and the word sequence for semantic parsing, we compare with an addi-\ntional straightforward baseline method (referred as BASELINE in Table 1). To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq; (2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings. The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder.\nWe can see that our model significantly outperforms the BASELINE. One possible reason is that our graph encoder jointly extracts these features in a unified model by propagating the dependency and constituency information to all nodes in the syntactic graph. However, BASELINE separately models these features using two distinct TreeLSTMs. As a result, the non-terminal tree nodes only retain only one type of syntactic information propagated from their descendants in the tree.\nAblation Study. In Table 1, we also report the results of three ablation variants of our model, i.e., without word order features/dependency features/constituency features. We find that Graph2Seq is superior to Seq2Seq (Dong and Lapata, 2016) which is expected since Graph2Seq exploits more syntactic information. Among these features, the word order feature have more impact on the performance than other two syntactic features. By incorporating either the dependency or the constituency features, the model could gain further performance improvement, which underlines the importance of utilizing more aspects of syntactic information. Finally, removing both syntactic features (w/ ONLY word order) performs slightly worse compared to the Seq2Seq baseline. This shows that using K=10 hops is good enough for memorizing the sentences in our benchmarks, although still weaker compared to a bidirectional LSTM encoder.\nA natural question here is on which type of queries our model could benefit from incorporating these parse features. By analyzing the queries and our predicted logical forms, we find that the parse features mainly improve the prediction accuracy for the queries with complex logical forms. Table 2 gives some running examples of complicated queries in three datasets. We find that the model that exploits three syntactic information\ncould correctly predict these logical forms while the model that only uses word order features may fail.\nRobustness Study. Different from previous works, we evaluate the robustness of our model by creating adversarial examples with the hope to investigate the impact of introducing more syntactic information on robustness. Specifically, we create two types of adversarial examples and conduct experiments on the ATIS dataset. Following Belinkov and Bisk (2017), we first experiment with the synthetic noise, SWAP, which swaps two letters (e.g. noise→nosie). It is common to see such noisy information when typing quickly. Given a text, we randomly perform swap on m ∈ {1, 2, 3, 4, 5} words that not correspond to the operators or arguments in logical forms, ensuring the meaning of the text is not changed. We train Graph2Seq on the training data and first evaluate it on the original development data, Devori. Then we use the same model but evaluate it on a variant of Devori, whose queries contain m swapped words.\nFigure 2 summarizes the results of our model on the first type of adversarial examples, i.e., the ATIS development set with the SWAP noise. From Figure 2, we can see that (1) the performance of our model on all combinations of features degrade significantly when increasing the number of swapped words; (2) the model that uses three syntactic features (our default model) always achieves the best performance, and the performance gap\ncompared to others increases when rising the number of swapped words; (3) word order features are the most sensitive to the word sequence while the dependency and constituency features seem more robust to such noisy information; (4) thanks to the robustness of the dependency and constituency features, the default model performs significantly better than the one that only uses word order features on the noisy sentences. These findings demonstrate that incorporating more aspects of syntactic information could enhance the robustness of the model.\nWe also experiment with the paraphrase of the input text as the second type of adversarial examples. More specifically, we collect the paraphrase of a text by first translating it to the other language such as Chinese and then translating it back to English, using the Google Translate service. We use this method to collect a new variant of Devori whose queries are the paraphrases of the original ones. By manually reading these queries, we find 94% queries convey the same meaning as original ones. Similar to the first experiment, we still train the model on Devori and evaluate it on the newly created dataset.\nTable 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased ATIS development set. We also report the\nresult of our model on the original ATIS development set. We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset; (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features. (3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model. These results again demonstrate that our model could benefit from incorporating more aspects of syntactic information."
  }, {
    "heading": "5 Related Work",
    "text": "Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018)."
  }, {
    "heading": "6 Conclusions",
    "text": "Existing neural semantic parsers mainly leverage word order features while neglecting other valuable syntactic information. To address this, we propose to build a syntactic graph which represents three types of syntactic information, and further apply a novel graph-to-sequence model to map the syntactic graph to a logical form. Experimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems."
  }],
  "year": 2018,
  "references": [{
    "title": "A simple but tough-to-beat baseline for sentence embeddings",
    "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Synthetic and natural noise both break neural machine translation",
    "authors": ["Yonatan Belinkov", "Yonatan Bisk."],
    "venue": "arXiv preprint arXiv:1711.02173.",
    "year": 2017
  }, {
    "title": "Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples",
    "authors": ["Minhao Cheng", "Jinfeng Yi", "Huan Zhang", "Pin-Yu Chen", "Cho-Jui Hsieh."],
    "venue": "arXiv preprint arXiv:1803.01128.",
    "year": 2018
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "CoRR, abs/1601.01280.",
    "year": 2016
  }, {
    "title": "Deep sparse rectifier neural networks",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13,",
    "year": 2011
  }, {
    "title": "Improved relation extraction with feature-rich compositional embedding models",
    "authors": ["Matthew R Gormley", "Mo Yu", "Mark Dredze."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1774–1784.",
    "year": 2015
  }, {
    "title": "Dialsql: Dialogue based structured query generation",
    "authors": ["Izzeddin Gur", "Semih Yavuz", "Yu Su", "Xifeng Yan."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1339–",
    "year": 2018
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "CoRR, abs/1606.03622.",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Neural semantic parsing with type constraints for semi-structured tables",
    "authors": ["Jayant Krishnamurthy", "Pradeep Dasigi", "Matt Gardner."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1516–1526.",
    "year": 2017
  }, {
    "title": "Scaling semantic parsers with on-the-fly ontology matching",
    "authors": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Lexical generalization in CCG grammar induction for semantic parsing",
    "authors": ["Tom Kwiatkowski", "Luke S. Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-",
    "year": 2011
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "Computational Linguistics, 39:389–446.",
    "year": 2011
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Association for Computational Linguistics (ACL) System Demonstrations,",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "The importance of syntactic parsing and inference in semantic role labeling",
    "authors": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."],
    "venue": "Computational Linguistics, 34(2):257–287.",
    "year": 2008
  }, {
    "title": "Abstract syntax networks for code generation and semantic parsing",
    "authors": ["Maxim Rabinovich", "Mitchell Stern", "Dan Klein."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2017
  }, {
    "title": "Transforming dependency structures to logical forms for semantic parsing",
    "authors": ["Siva Reddy", "Oscar Täckström", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."],
    "venue": "Transactions of the Association for Computational",
    "year": 2016
  }, {
    "title": "Universal semantic parsing",
    "authors": ["Siva Reddy", "Oscar Täckström", "Slav Petrov", "Mark Steedman", "Mirella Lapata."],
    "venue": "arXiv preprint arXiv:1702.03196.",
    "year": 2017
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Cross-domain semantic parsing via paraphrasing",
    "authors": ["Yu Su", "Xifeng Yan."],
    "venue": "arXiv preprint arXiv:1704.05974.",
    "year": 2017
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1503.00075.",
    "year": 2015
  }, {
    "title": "Morpho-syntactic lexical generalization for ccg semantic parsing",
    "authors": ["Adrienne Wang", "Tom Kwiatkowski", "Luke S. Zettlemoyer."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2014
  }, {
    "title": "Word mover’s embedding: From word2vec to document embedding",
    "authors": ["Lingfei Wu", "Ian E.H. Yen", "Kun Xu", "Fangli Xu", "Avinash Balakrishnan", "Pin-Yu Chen", "Pradeep Ravikumar", "Michael J. Witbrock."],
    "venue": "EMNLP.",
    "year": 2018
  }, {
    "title": "D2ke: From distance to kernel and embedding",
    "authors": ["Lingfei Wu", "Ian En-Hsu Yen", "Fangli Xu", "Pradeep Ravikuma", "Michael Witbrock."],
    "venue": "arXiv preprint arXiv:1802.04956.",
    "year": 2018
  }, {
    "title": "Question Answering on Freebase via Relation Extraction and Textual Evidence",
    "authors": ["Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."],
    "venue": "ACL 2016.",
    "year": 2016
  }, {
    "title": "Graph2seq: Graph to sequence learning with attention-based neural networks",
    "authors": ["Kun Xu", "Lingfei Wu", "Zhiguo Wang", "Vadim Sheinin."],
    "venue": "arXiv preprint arXiv:1804.00823.",
    "year": 2018
  }, {
    "title": "Online learning of relaxed ccg grammars for parsing to logical form",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "EMNLP-CoNLL.",
    "year": 2007
  }, {
    "title": "Sentencestate lstm for text representation",
    "authors": ["Yue Zhang", "Qi Liu", "Linfeng Song."],
    "venue": "arXiv preprint arXiv:1805.02474.",
    "year": 2018
  }, {
    "title": "Type-driven incremental semantic parsing with polymorphism",
    "authors": ["Kai Zhao", "Liang Huang."],
    "venue": "HLT-NAACL.",
    "year": 2015
  }],
  "id": "SP:305f8c705c034b6179fc4db19eb6163265432baa",
  "authors": [{
    "name": "Kun Xu",
    "affiliations": []
  }, {
    "name": "Lingfei Wu",
    "affiliations": []
  }, {
    "name": "Zhiguo Wang",
    "affiliations": []
  }, {
    "name": "Mo Yu",
    "affiliations": []
  }, {
    "name": "Liwei Chen",
    "affiliations": []
  }, {
    "name": "Vadim Sheinin",
    "affiliations": []
  }],
  "abstractText": "Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees. In this paper, we first propose to use the syntactic graph to represent three types of syntactic information, i.e., word order, dependency and constituency features; then employ a graph-tosequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.",
  "title": "Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model"
}