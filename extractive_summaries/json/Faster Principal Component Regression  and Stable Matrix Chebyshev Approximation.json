{
  "sections": [{
    "heading": "1 Introduction",
    "text": "In machine learning and statistics, it is often desirable to represent a large-scale dataset in a more tractable, lowerdimensional form, without losing too much information. One of the most robust ways to achieve this goal is through principal component projection (PCP):\nPCP: project vectors onto the span of the top principal components of the a matrix.\nIt is well-known that PCP decreases noise and increases efficiency in downstream tasks. One of the main applications is principal component regression (PCR):\nPCR: linear regression but restricted to the subspace of top principal components.\nClassical algorithms for PCP or PCR rely on a principal component analysis (PCA) solver to recover the top principal components first; with these components available, the\n*Equal contribution . Future version of this paper shall be found at https://arxiv.org/abs/1608.04773. 1Microsoft Reseaerch 2Princeton University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.\nUnfortunately, PCA solvers demand a running time that at least linearly scales with the number of top principal components chosen for the projection. For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylovbased (Musco & Musco, 2015) or Lanczos-based (AllenZhu & Li, 2016a) methods require a running time that is proportional to 1000×40 = 4×104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations. This is usually computationally intractable."
  }, {
    "heading": "1.1 Approximating PCP Without PCA",
    "text": "In this paper, we propose the following notion of PCP approximation. Given a data matrix A ∈ Rd′×d (with singular values no greater than 1) and a threshold λ > 0, we say that an algorithm solves (γ, ε)-approximate PCP if — informally speaking and up to a multiplicative 1±ε error— it projects (see Def. 3.1 for a formal definition)\n1. eigenvector ν of A>A with value in [ λ(1 +γ), 1 ] to ν,\n2. eigenvector ν of A>A with value in [ 0, λ(1− γ) ] to ~0,\n3. eigenvector ν of A>A with value in [ λ(1 − γ), λ(1 +\nγ) ] to “anywhere between ~0 and ν.”\nSuch a definition also extends to (γ, ε)-approximate PCR (see Def. 3.2).\nIt was first noticed by Frostig et al. (Frostig et al., 2016) that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold λ. More specifically, they reduced (γ, ε)approximate PCP and PCR to\nO ( γ−2 log(1/ε) ) black-box calls of\nany ridge regression subroutine\nwhere each call computes (A>A + λI)−1u for some vector u.1 Our main focus of this paper is to quadratically improve this performance and reduce PCP and PCR to\n1Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG (Johnson & Zhang, 2013), one can usually solve ridge regression to an 10−8 accuracy with at most 40 passes of the data.\nO ( γ−1 log(1/γε) ) black-box calls of\nany ridge regression subroutine\nwhere each call again computes (A>A + λI)−1u. Remark 1.1. Frostig et al. only showed their algorithm satisfies the properties 1 and 2 of (γ, ε)-approximation (but not the property 3), and thus their proof was only for matrix A with no singular value in the range [ √ λ(1− γ), √ λ(1 + γ)]. This is known as the eigengap assumption, which is rarely satisfied in practice (Musco & Musco, 2015). In this paper, we prove our result both with and without such eigengap assumption. Since our techniques also imply the algorithm of Frostig et al. satisfies property 3, throughout the paper, we say Frostig et al. solve (γ, ε)-approximate PCP and PCR."
  }, {
    "heading": "1.2 From PCP to Polynomial Approximation",
    "text": "The main technique of Frostig et al. is to construct a polynomial to approximate the sign function sgn(x) : [−1, 1]→ {±1}:\nsgn(x) := { +1, x ≥ 0; −1, x < 0.\nIn particular, given any polynomial g(x) satisfying∣∣g(x)− sgn(x)∣∣ ≤ ε ∀x ∈ [−1,−γ] ∪ [γ, 1] , (1.1)∣∣g(x)∣∣ ≤ 1 ∀x ∈ [−γ, γ] , (1.2) the problem of (γ, ε)-approximate PCP can be reduced to computing the matrix polynomial g(S) for S := (A>A + λI)−1(A>A− λI) (cf. Fact 7.1). In other words, • to project any vector χ ∈ Rd to top principal compo-\nnents, we can compute g(S)χ instead; and\n• to compute g(S)χ, we can reduce it to ridge regression for each evaluation of Su for some vector u. Remark 1.2. Since the transformation from A>A to S is not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A>A. We restrict to polynomial choices of g(·) because in this way, the final rational function has all the denominators being A>A + λI, thus reduces to ridge regressions. Remark 1.3. The transformation from A>A to S ensures that all the eigenvalues of A>A in the range (1 ± γ)λ roughly map to the eigenvalues of S in the range [−γ, γ].\nMain Challenges. There are two main challenges regarding the design of polynomial g(x).\n• EFFICIENCY. We wish to minimize the degree n = deg(g(x)) because the computation of g(S)χ usually requires n calls of ridge regression.\n• STABILITY. We wish g(x) to be stable; that is, g(S)χ must be given by a recursive formula where if we make ε′ error in each recursion (due to error incurred from ridge regression), the final error of g(S)χ must be at most ε′ × poly(d).\nRemark 1.4. Efficient routines such as SVRG (Johnson & Zhang, 2013) solve ridge regression and thus compute Su for any u ∈ Rd, with running times only logarithmically in 1/ε′. Therefore, by setting ε′ = ε/poly(d), one can blow up the running time by a small factor O(log(d)) in order to obtain an ε-accurate solution for g(S)χ.\nThe polynomial g(x) constructed by Frostig et al. comes from truncated Taylor expansion. It has degree O ( γ−2 log(1/ε) ) and is stable. This γ−2 dependency limits the practical performance of their proposed PCP and PCR algorithms, especially in a high accuracy regime. At the same time,\n• the optimal degree for a polynomial to satisfy even only (1.1) is Θ ( γ−1 log(1/ε) ) (Eremenko & Yuditskii,\n2007; 2011).\nFrostig et al. were unable to find a stable polynomial matching this optimal degree and left it as open question.2"
  }, {
    "heading": "1.3 Our Results and Main Ideas",
    "text": "We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(γ−1 log(1/γε)). At a high level, we construct a polynomial q(x) that approximately equals ( 1+κ−x\n2 )−1/2 for some κ = Θ(γ2); then we set g(x) := x·q(1+κ−2x2) which approximates sgn(x).\nTo construct q(x), we first note that (\n1+κ−x 2\n)−1/2 has\nno singular point on [−1, 1] so we can apply Chebyshev approximation theory to obtain some q(x) of degree O(γ−1 log(1/γε)) satisfying∣∣∣q(x)− (1 + κ− x\n2 )−1/2∣∣∣ ≤ ε for every x ∈ [−1, 1] . This can be shown to imply\n∣∣g(x)− sgn(x)∣∣ ≤ ε for every x ∈ [−1,−γ] ∪ [γ, 1], so (1.1) is satisfied. In order to prove (1.2) , we prove a separate lemma:3\nq(x) ≤ (1 + κ− x\n2\n)−1/2 for every x ∈ [1, 1 + κ] .\nNote that this does not follow from standard Chebyshev theory because Chebyshev approximation guarantees are only with respect to x ∈ [−1, 1]. This proves the “EFFICIENCY” part of the main challenges discussed earlier. As for the “STABILITY” part, we prove a general theorem regarding any weighted sum of Chebyshev polynomials applied to matrices. We provide a backward recurrence algorithm and show that it is stable under noisy\n2Using degree reduction, Frostig et al. found an explicit polynomial g(x) of degree O ( γ−1 log(1/γε) ) satisfying (1.1). However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front of each monomial. Furthermore, it is not clear if their polynomial satisfies the (1.2).\n3We proved a general lemma which holds for any function whose all orders of derivatives are non-negative at x = 0.\ncomputations. This may be of independent interest.\nFor interested readers, we compare our polynomial q(x) with that of Frostig et al. in Figure 1."
  }, {
    "heading": "1.4 Related Work",
    "text": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix APλ where Pλ is the PCP projection matrix (Chan & Hansen, 1990; Boutsidis & Magdon-Ismail, 2014). However, they cost a running time that linearly scales with the number of principal components above λ.\nA significant number of papers have focused on the lowrank case of PCA (Musco & Musco, 2015; Allen-Zhu & Li, 2016a; 2017) and its online variant (Allen-Zhu & Li, 2016b). Unfortunately, all of these methods require a running time that scales at least linearly with respect to the number of top principal components.\nMore related to this paper is work on matrix sign function, which plays an important role in control theory and quantum chromodynamics. Several results have addressed Krylov methods for applying the sign function in the socalled Krylov subspace, without explicitly constructing any approximate polynomial (van den Eshof et al., 2002; Schilders et al., 2008). However, Krylov methods are not (γ, ε)-approximate PCP solvers, and there is no supporting stability theory behind them.4 Other iterative methods have also been proposed, see Section 5 of textbook (Higham, 2008). For instance, Schur’s method is a slow one and also requires the matrix to be explicitly given. The Newton’s iteration and its numerous variants (e.g. (Nakatsukasa & Freund, 2016)) provide rational approximations to the matrix sign function as opposed to polynomial approximations. Our result and Frostig et al. (Frostig et al., 2016) differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.\nUsing matrix Chebyshev polynomials to approximate matrix functions is not new. Perhaps the most celebrated example is to approximate S−1 using polynomials on S, used in the analysis of conjugate gradient (Shewchuk, 1994). Independent from this paper,5 Han et al. (Han et al., 2016) used Chebyshev polynomials to approximate the trace of the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the\n4We anyways have included Krylov method in our empirical evaluation section and shall discuss its performance there, see the full version of this paper.\n5Their paper appeared online two months before us, and we became aware of their work in March 2017.\n6In particular, their degree of the Chebyshev polynomial is O ( γ−1(log2(1/γ)+ log(1/γ) log(1/ε)) ) in the language of this\npaper; in contrast, we have degree O ( γ−1 log(1/γε) ) .\ncase when the matrix-vector multiplication oracle is only approximate (like we do in this paper), or the case when S has eigenvalues in the range [−γ, γ].\nRoadmap. In Section 2, we provide notions for this paper and basics for Chebyshev polynomials. In Section 3, we formally define approximate PCP and PCR, and reduce PCR to PCP. In Section 4, we show a general lemma for Chebyshev approximations. In Section 5, we design our polynomial approximation to sgn(x). In Section 6, we show how to stably compute Chebyshev polynomials. In Section 7, we state our main theorems regarding PCP and PCR. In Section 8, we provide empirical evaluations."
  }, {
    "heading": "2 Preliminaries",
    "text": "We denote by 1[e] ∈ {0, 1} the indicator function for event e, by ‖v‖ or ‖v‖2 the Euclidean norm of a vector v, by M† the Moore-Penrose pseudo-inverse of a symmetric matrix M, and by ‖M‖2 its spectral norm. We sometimes use ~v to emphasize that v is a vector.\nGiven a symmetric d × d matrix M and any f : R → R, f(M) is the matrix function applied to M, which is equal to Udiag{f(D1), . . . , f(Dd)}U> if M = Udiag{D1, . . . , Dd}U> is its eigendecomposition. Throughout the paper, matrix A is of dimension d′ × d. We denote by σmax(A) the largest singular value of A. Following the tradition of (Frostig et al., 2016) and keeping the notations light, we assume without loss of generality that σmax(A) ≤ 1. We are interested in PCP and PCR problems with an eigenvalue threshold λ ∈ (0, 1). Throughout the paper, we denote by λ1 ≥ · · · ≥ λd ≥ 0 the eigenvalues of A>A, and by ν1, . . . , νd ∈ Rd the eigenvectors of A>A corresponding to λ1, . . . , λd. We denote by Pλ the projection matrix Pλ := (ν1, . . . , νj)(ν1, . . . , νj)\n> where j is the largest index satisfying λj ≥ λ. In other words, Pλ is a projection matrix to the eigenvectors of A>A with eigenvalues ≥ λ. Definition 2.1. The principal component projection (PCP) of χ ∈ Rd at threshold λ is ξ∗ = Pλχ. Definition 2.2. The principal component regression (PCR) of regressand b ∈ Rd′ at threshold λ is x∗ = arg miny∈Rd ‖APλy − b‖2 or equivalently x∗ = (A>A)†Pλ(A >b) ."
  }, {
    "heading": "2.1 Ridge Regression",
    "text": "Definition 2.3. A black-box algorithm ApxRidge(A, λ, u) is an ε-approximate ridge regression solver, if for every u ∈ Rd, it satisfies ‖ApxRidge(A, λ, u)−(A>A+λI)−1u‖ ≤ ε‖u‖.\nRidge regression is equivalent to solving well-conditioned linear systems, or minimizing strongly convex and smooth objectives f(y) := 12y >(A>A + λI)y − u>y.\nRemark 2.4. There is huge literature on efficient algorithms solving ridge regression. Most notably,\n(1) Conjugate gradient (Shewchuk, 1994) or accelerated gradient descent (Nesterov, 2004) gives fastest fullgradient methods;\n(2) SVRG (Johnson & Zhang, 2013) and its acceleration Katyusha (Allen-Zhu, 2017) give the fastest stochasticgradient method; and\n(3) NUACDM (Allen-Zhu et al., 2016) gives the fastest coordinate-descent method.\nThe running time of (1) is O(nnz(A)λ−1/2 log(1/ε)) where nnz(A) is time to multiply A to any vector. The running times of (2) and (3) depend on structural properties of A and are always faster than (1).\nBecause the best complexity of ridge regression depends on the structural properties of A, following Frostig et al., we only compute our running time in terms of the “number of black-box calls” to a ridge regression solver."
  }, {
    "heading": "2.2 Chebyshev Polynomials",
    "text": "Definition 2.5. Chebyshev polynomials of 1st and 2nd kind are {Tn(x)}n≥0 and {Un(x)}n≥0 where T0(x) := 1, T1(x) := x, Tn+1(x) := 2x · Tn(x)− Tn−1(x) U0(x) := 1, U1(x) := 2x, Un+1(x) := 2x · Un(x)− Un−1(x)\nFact 2.6 ((Trefethen, 2013)). It satisfies ddxTn(x) = nUn−1(x) for n ≥ 1 and\n∀n ≥ 0: Tn(x) =  cos(n arccos(x)), if |x| ≤ 1;cosh(n arccosh(x)), if x ≥ 1;(−1)n cosh(n arccosh(−x)), if x ≤ −1. In particular, when x ≥ 1,\nTn(x) = 1\n2\n[( x− √ x2 − 1 )n + ( x+ √ x2 − 1 )n] Un(x) = 1\n2 √ x2 − 1\n[( x+ √ x2 − 1 )n+1 − (x−√x2 − 1)n+1] Definition 2.7. For function f(x) whose domain contains [−1, 1], its degree-n Chebyshev truncated series and degree-n Chebyshev interpolation are respectively\npn(x) := n∑ k=0 akTk(x) and qn(x) := n∑ k=0 ckTk(x) ,\nwhere ak := 2− 1[k = 0]\nπ\n∫ 1 −1 f(x)Tk(x)√ 1− x2 dx\nck := 2− 1[k = 0]\nn+ 1\nn∑ j=0 f ( xj ) Tk ( xj ) .\nAbove, xj := cos ( (j+0.5)π\nn+1\n) ∈ [−1, 1] is the j-th Cheby-\nshev point of order n.\nThe following lemma is known as the aliasing formula for Chebyshev coefficients:\nLemma 2.8 (cf. Theorem 4.2 of (Trefethen, 2013)). Let f be Lipschitz continuous on [−1, 1] and {ak}, {ck} be defined in Def. 2.7, then\nc0 = a0+a2n+a4n+... , cn = an+a3n+a5n+... , and\nfor every k ∈ {1, 2, . . . , n− 1}, ck = ak+(ak+2n+ak+4n+...)+(a−k+2n+a−k+4n+...)\nDefinition 2.9. For every ρ > 0, let Eρ be the ellipse E of foci ±1 with major radius 1 + ρ. (This is also known as Bernstein ellipse with parameter 1 + ρ+ √ 2ρ+ ρ2.)\nThe following lemma is the main theory regarding Chebyshev approximation: Lemma 2.10 (cf. Theorem 8.1 and 8.2 of (Trefethen, 2013)). Suppose f(z) is analytic on Eρ and |f(z)| ≤ M on Eρ. Let pn(x) and qn(x) be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x) on [−1, 1]. Then, • max x∈[−1,1] |f(x)−pn(x)| ≤ 2M ρ+ √ 2ρ+ρ2 ( 1+ρ+ √ 2ρ+ ρ2\n)−n; • max x∈[−1,1] |f(x)−qn(x)| ≤ 4M ρ+ √ 2ρ+ρ2 ( 1+ρ+ √ 2ρ+ ρ2\n)−n. • |a0| ≤M and |ak| ≤ 2M ( 1+ρ+ √ 2ρ+ ρ2\n)−k for k ≥ 1."
  }, {
    "heading": "3 Approximate PCP and PCR",
    "text": "We formalize our notions of approximation for PCP and PCR, and provide a reduction from PCR to PCP."
  }, {
    "heading": "3.1 Our Notions of Approximation",
    "text": "Recall that Frostig et al. (Frostig et al., 2016) work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range\n[ √ λ(1− γ), √ λ(1 + γ)]. Their approximation guarantees are very straightforward:\n• an output ξ is ε-approximate for PCP on vector χ if ‖ξ − ξ∗‖ ≤ ε‖χ‖;\n• an output x is ε-approximate for PCR with regressand b if ‖x− x∗‖ ≤ ε‖b‖.\nUnfortunately, these notions are too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold λ.\nIn this paper we propose the following more general (but yet very meaningful) approximation notions.\nDefinition 3.1. An algorithm B(χ) is (γ, ε)-approximate PCP for threshold λ, if for every χ ∈ Rd\n1. ∥∥P(1+γ)λ(B(χ)− χ)∥∥ ≤ ε‖χ‖.\n2. ∥∥(I−P(1−γ)λ)B(χ)∥∥ ≤ ε‖χ‖.\n3. ∀i such that λi ∈ [ (1 − γ)λ, (1 + γ)λ ] , it satisfies\n|〈νi,B(χ)− χ〉| ≤ |〈νi, χ〉|+ ε‖χ‖.\nIntuitively, the first property above states that, if projected to the eigenspace with eigenvalues above (1 + γ)λ, then B(χ) and χ are almost identical; the second property states that, if projected to the eigenspace with eigenvalues below (1 − γ)λ, then B(χ) is almost zero; and the third property states that, for each eigenvector νi with eigenvalue in the range [(1− γ)λ, (1 + γ)λ], the projection 〈νi,B(χ)〉 must be between 0 and 〈νi, χ〉 (but up to an error ε‖χ‖). Naturally, Pλ(χ) itself is a (0, 0)-approximate PCP.\nWe propose the following notion for approximate PCR:\nDefinition 3.2. An algorithm C(b) is (γ, ε)-approximate PCR for threshold λ, if for every b ∈ Rd′\n1. ∥∥(I−P(1−γ)λ)C(b)∥∥ ≤ ε‖b‖.\n2. ‖AC(b)− b‖ ≤ ‖Ax∗ − b‖+ ε‖b‖. where x∗ = (A>A)†P(1+γ)λA>b is the exact PCR solution for threshold (1 + γ)λ.\nThe first notion states that the output x = C(b) has nearly no correlation with eigenvectors below threshold (1− γ)λ; and the second states that the regression error should be nearly optimal with respect to the exact PCR solution but at a different threshold (1 + γ)λ.\nRelationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.: Fact 3.3. If A has no singular value in [ √ λ(1− γ), √ λ(1 + γ)], then\n• Def. 3.1 is equivalent to ‖B(χ)−Pλ(χ)‖ ≤ O(ε)‖χ‖.\n• Def. 3.2 implies ‖C(χ) − x∗‖ ≤ O(ε/λ)‖b‖ and ‖C(χ)− x∗‖ ≤ O(ε)‖b‖ implies Def. 3.2.\nAbove, x∗ = (A>A)†PλA>b is the exact PCR solution."
  }, {
    "heading": "3.2 Reductions from PCR to PCP",
    "text": "If the PCP solution ξ = Pλ(A>b) is computed exactly, then by definition one can compute (A>A)†ξ which gives a solution to PCR by solving a linear system. However, as pointed by Frostig et al. (Frostig et al., 2016), this computation is problematic if ξ is only approximate. The following approach has been proposed to improve its accuracy by Frostig et al.\n• “compute p((A>A + λI)−1)ξ where p(x) is a polynomial that approximates function x1−λx .” This is a good approximation to (A>A)†ξ because the composition of functions x1−λx and 1 1+λx is exactly x\n−1. Frostig et al. picked p(x) = pm(x) = ∑m t=1 λ\nt−1xt which is a truncated Taylor series, and used the following procedure to compute sm ≈ pm((A>A + λI)−1)ξ:\ns0 = B(A>b), s1 = ApxRidge(A, λ, s0), ∀k ≥ 1: sk+1 = s1 + λ · ApxRidge(A, λ, sk) . (3.1)\nAbove, B is an approximate PCP solver and ApxRidge is an approximate ridge regression solver. Under eigengap assumption, Frostig et al. (Frostig et al., 2016) showed\nLemma 3.4 (PCR-to-PCP). For fixed λ, γ, ε ∈ (0, 1), let A be a matrix whose singular values lie in[ 0, √ (1− γ)λ ] ∪ [√ (1− γ)λ, 1 ] . Let ApxRidge be any O( εm2 )-approximate ridge regression solver, and let B be any (γ,O( ελm2 ))-approximate PCP solver\n7. Then, procedure (3.1) satisfies\n‖sm−(A>A)†PλA>b‖ ≤ ε‖b‖ if m = Θ(log(1/εγ)) .\nUnfortunately, the above lemma does not hold without eigengap assumption. In this paper, we fix this issue by proving the following analogous lemma:\nLemma 3.5 (gap free PCR-to-PCP). For fixed λ, ε ∈ (0, 1) and γ ∈ (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( εm2 )-approximate ridge regression solver, and B be any (γ,O( ελm2 ))-approximate PCP solver. Then, procedure (3.1) satisfies,{\n‖(I−P(1−γ)λ)sm‖ ≤ ε‖b‖ , and\n‖Asm − b‖ ≤ ‖A(A>A)†P(1+γ)λA>b− b‖+ ε‖b‖ } if m = Θ(log(1/εγ)\nNote that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2. The proof of Lemma 3.5 is not hard, but requires a very careful case analysis by decomposing vectors b and each sk into three components, each corresponding to eigenvalues of A>A in the range [0, (1−γ)λ], [(1−γ)λ, (1+γ)λ] and [(1+γ)λ, 1].\n7Recall from Fact 3.3 that this requirement is equivalent to saying that ‖B(χ)−Pλχ‖ ≤ O( ε √ λ\nm2 )‖χ‖.\nWe defer the details to the full version."
  }, {
    "heading": "4 Chebyshev Approximation Outside [−1, 1]",
    "text": "Classical Chebyshev approximation theory (such as Lemma 2.10) only talks about the behaviors of pn(x) or gn(x) on interval [−1, 1]. However, for the purpose of this paper, we must also bound its value for x > 1. We prove the following general lemma in the full version, and believe it could be of independent interest: (we denote by f (k)(x) the k-th derivative of f at x)\nLemma 4.1. Suppose f(z) is analytic on Eρ and for every k ≥ 0, f (k)(0) ≥ 0. Then, for every n ∈ N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n∀y ∈ [0, ρ] : 0 ≤ pn(1 + y), qn(1 + y) ≤ f(1 + y) .\n5 Our Polynomial Approximation of sgn(x) For fixed κ ∈ (0, 1], we consider the degree-n Chebyshev interpolation qn(x) = ∑n k=0 ckTk(x) of the function\nf(x) = (\n1+κ−x 2\n)−1/2 on [−1, 1]. Def. 2.7 tells us that\nck := 2− 1[k = 0]\nn+ 1\nn∑ j=0 (√ 2 cos (k(j + 0.5)π n+ 1 )) × ( 1 + κ− cos ( (j + 0.5)π\nn+ 1\n))−1/2 .\nOur final polynomial to approximate sgn(x) is therefore\ngn(x) = x·qn(1+κ−2x2) and deg(gn(x)) = 2n+1 .\nWe prove the following theorem in this section:\nTheorem 5.1. For every α ∈ (0, 1], ε ∈ (0, 1/2), choosing κ = 2α2, our function gn(x) := x · qn(1 + κ− 2x2) satisfies that as long as n ≥ 1√\n2α log 3εα2 , then (see also\nFigure 1)\n• |gn(x)− sgn(x)| ≤ ε for every x ∈ [−1, α] ∪ [α, 1]. • gn(x) ∈ [0, 1] for every x ∈ [0, α] and gn(x) ∈\n[−1, 0] for every x ∈ [−α, 0].\nNote that our degree n = O ( α−1 log(1/αε) ) is nearoptimal, because the minimum degree for a polynomial to satisfy even only the first item is Θ ( α−1 log(1/ε) ) (Eremenko & Yuditskii, 2007; 2011). However, the results of (Eremenko & Yuditskii, 2007; 2011) are not constructive, and thus may not lead to stable matrix polynomials.\nWe prove Theorem 5.1 by first establishing two simple lemmas. The following lemma is a consequence of Lemma 2.10:\nLemma 5.2. For every ε ∈ (0, 1/2) and κ ∈ (0, 1], if n ≥ 1√\nκ ( log 1κ + log 4 ε ) then\n∀x ∈ [−1, 1], |f(x)− qn(x)| ≤ ε .\nProof of Lemma 5.2. Denoting by f(z) = (\n1+κ−z 2\n)−0.5 ,\nwe know that f(z) is analytic on ellipse Eρ with ρ = κ/2, and it satisfies |f(z)| ≤ √ 2/κ in Eρ. Applying Lemma 2.10, we know that when n ≥ 1√ κ ( log 1κ + log 4 ε\n) it satisfies |f(x)− qn(x)| ≤ ε.\nThe next lemma an immediate consequence of our Lemma 4.1 with f(z) = ( 1+κ−z\n2\n)−0.5 :\nLemma 5.3. For every ε ∈ (0, 1/2), κ ∈ (0, 1], n ∈ N, and x ∈ [0, κ], we have\n0 ≤ qn(1 + x) ≤ (κ− x\n2\n)−1/2 .\nProof of Theorem 5.1. We are now ready to prove Theorem 5.1.\n• When x ∈ [−1, α] ∪ [α, 1], it satisfies 1 + κ − 2x2 ∈ [−1, 1]. Therefore, applying Lemma 5.2 we have whenever n ≥ 1√\nκ log 6εκ = 1√ 2α log 3εα2 it satisfies |f(1 + κ−2x2)−qn(1+κ−2x2)|∞ ≤ ε. This further implies\n|gn(x)−sgn(x)| = |xqn(1+κ−2x2)−xf(1+κ−2x2)| ≤ |x||f(1 + κ− 2x2)− qn(1 + κ− 2x2)| ≤ ε .\n• When |x| ≤ α, it satisfies 1 + κ − 2x2 ∈ [1, 1 + κ]. Applying Lemma 5.3 we have for all x ∈ [0, α], 0 ≤ gn(x) = x · qn(1 + κ− 2x2) ≤ x · (x2)−1/2 = 1 and similarly for x ∈ [−α, 0] it satisfies 0 ≥ gn(x) ≥ −1.\nA Bound on Chebyshev Coefficients. We also give an upper bound to the coefficients of polynomial qn(x). Its proof can be found in the full version, and this upper bound shall be used in our final stability analysis. Lemma 5.4 (coefficients of qn). Let qn(x) =∑n k=0 ckTk(x) be the degree-n Chebyshev interpolation\nof f(x) = (\n1+κ−x 2\n)−1/2 on [−1, 1]. Then, for all i ∈\n{0, 1, . . . , n}, |ci| ≤ e √ 32(i+ 1)\nκ\n( 1 + κ+ √ 2κ+ κ2 )−i"
  }, {
    "heading": "6 Stable Computation of Matrix Chebyshev Polynomials",
    "text": "In this section we show that any polynomial that is a weighted summation of Chebyshev polynomials with bounded coefficients, can be stably computed when applied to matrices with approximate computations. We achieve so by first generalizing Clenshaw’s backward method to matrix case in Section 6.1 in order to compute a matrix variant of Chebyshev sum, and then analyze its stability in Section 6.2 with the help from Elloit’s forward-backward transformation (Elliott, 1968).\nRemark 6.1. We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars (Gil et al., 2007), it is not immediately clear why it holds also for matrices. Recall that Chebyshev polynomials satisfy Tn+1(x) = 2xTn(x) − Tn−1(x). In the matrix case, we have Tn+1(M)χ = 2MTn(M)χ − Tn−1(M)χ where χ ∈ Rd is a vector. If we analyzed this formula coordinate by coordinate, error could blow up by a factor d per iteration.\nIn addition, we need to ensure that the stability theorem holds for matrices M with eigenvalues that can exceed 1. This is not standard because Chebyshev polynomials are typically analyzed only on domain [−1, 1]."
  }, {
    "heading": "6.1 Clenshaw’s Method in Matrix Form",
    "text": "Consider any computation of the form\n~sN := N∑ k=0 Tk(M)~ck ∈ Rd (6.1)\nwhere M ∈ Rd×d is symmetric and each ~ck is in Rd. (Note that for PCP and PCR purposes, we it suffices to consider ~ck = c ′ kχ where c ′ k ∈ R is a scalar and χ ∈ Rd is a fixed vector for all k. However, we need to work on this more general form for our stability analysis.)\nVector sN can be computed using the following procedure:\nLemma 6.2 (backward recurrence). ~sN = ~b0−M~b1 where ~bN+1 := ~0, ~bN := ~cN , and\n∀r ∈ {N − 1, . . . , 0} : ~br := 2M~br+1 −~br+2 + ~cr ∈ Rd ."
  }, {
    "heading": "6.2 Inexact Clenshaw’s Method in Matrix Form",
    "text": "We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably computed. We define the following model to capture the error with respect to matrix-vector multiplications.\nDefinition 6.3 (inexact backward recurrence). Let M be an approximate algorithm that satisfies ‖M(u)−Mu‖2 ≤ ε‖u‖2 for every u ∈ Rd. Then, define inexact backward recurrence to be\nb̂N+1 := 0, b̂N := ~cN , and ∀r ∈ {N − 1, . . . , 0} : b̂r := 2M ( b̂r+1 ) − b̂r+2 + ~cr ∈ Rd ,\nand define the output as ŝN := b̂0 −M(̂b1).\nThe following theorem gives an error analysis to our inexact backward recurrence. We prove it in full version, and the main idea of our proof is to convert each error vector of a recursion of the backward procedure into an error vector corresponding to some original ~ck.\nTheorem 6.4 (stable Chebyshev sum). For every N ∈ N∗, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥\n0 satisfying ∀k ∈ {0, 1, . . . , N} :{ ρk‖~ck‖ ≤ Cc ∧ ∀x ∈ [a, b] : |Tk(x)|≤CT ρ k\n|Uk(x)|≤CUρk\n} .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with ε ≤ 14NCU , we have\n‖ŝN − ~sN‖ ≤ ε · 2(1 + 2NCT )NCUCc ."
  }, {
    "heading": "7 Algorithms and Main Theorems for PCP and PCR",
    "text": "We are now ready to state our main theorems for PCP and PCR. We first note a simple fact:\nFact 7.1. (Pλ)χ = I+sgn(S)2 where S := 2(A >A + λI)−1A>A− I = (A>A + λI)−1(A>A− λI).\nIn other words, for every vector χ ∈ Rd, the exact PCP solution Pλ(χ) is the same as computing (Pλ)χ = I+sgn(S)\n2 χ. Thus, we can use our polynomial gn(x) introduced in Section 5 and compute gn(S)χ ≈ sgn(S)χ. Finally, in order to compute gn(S), we need to multiply S to deg(gn) vectors; whenever we do so, we call perform ridge regression once.\nSince the high-level structure of our PCP algorithm is very clear, due to space limitation, we present the pseudocodes of our PCP and PCR algorithms in the full version."
  }, {
    "heading": "7.1 Our Main Theorems",
    "text": "We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that of Frostig et al. (Frostig et al., 2016).\nTheorem 7.2 (eigengap assumption). Given A ∈ Rd′×d and λ, γ ∈ (0, 1), assume that the singular values of A are in the range [0, √ (1− γ)λ]∪ [ √ (1 + γ)λ, 1]. Given χ ∈ Rd and b ∈ Rd′ , denote by ξ∗ = Pλχ and x∗ = (A>A)−1PλA>b\nthe exact PCP and PCR solutions, and by ApxRidge any ε′-approximate ridge regression solver. Then,\n• QuickPCP outputs ξ satisfying ‖ξ∗ − ξ‖ ≤ ε‖χ‖ with O ( γ−1 log 1γε ) oracle calls to ApxRidge as long as\nlog(1/ε′) = Θ ( log 1γε ) .\n• QuickPCR outputs x satisfying ‖x−x∗‖ ≤ ε‖b‖ with O ( γ−1 log 1γλε ) oracle calls to ApxRidge, as long as\nlog(1/ε′) = Θ ( log 1γλε ) .\nIn contrast, the number of ridge-regression oracle calls was Θ(γ−2 log 1γε ) for PCP and Θ(γ\n−2 log 1γλε ) for PCR in (Frostig et al., 2016). We include the proof of Theorem 7.2 in the full version.\nWe state our theorem without the eigengap assumption.\nTheorem 7.3 (gap-free). Given A ∈ Rd′×d, λ ∈ (0, 1), and γ ∈ (0, 2/3], assume that ‖A‖2 ≤ 1. Given χ ∈ Rd and b ∈ Rd′ , and suppose ApxRidge is an ε′approximate ridge regression solver, then\n• QuickPCP outputs ξ that is (γ, ε)-approximate PCP withO ( γ−1 log 1γε ) oracle calls to ApxRidge as long\nas log(1/ε′) = Θ ( log 1γε ) .\n• QuickPCR outputs x that is (γ, ε)-approximate PCR with O ( γ−1 log 1γλε ) oracle calls to ApxRidge as\nlong as elog(1/ε′) = Θ ( log 1γλε ) .\nWe make a final remark here regarding the practical usage of QuickPCP and QuickPCR. Remark 7.4. Since our theory is for (γ, ε)-approximations that have two parameters, the user in principle has to feed in both γ and n where n is the degree of the polynomial approximation to the sign function. In practice, however, it is usually sufficient to obtain (ε, ε)-approximate PCP and PCR. Therefore, our pseudocodes allow users to set γ = 0 and thus ignore this parameter γ; in such a case, we shall use γ = log(n)/n which is equivalent to setting γ = Θ(ε) because n = Θ(γ−1 log(1/γε))."
  }, {
    "heading": "8 Experiments",
    "text": "We provide empirical evaluations in the full version of this paper."
  }, {
    "heading": "9 Conclusion",
    "text": "We summarize our contributions.\n• We put forward approximate notions for PCP and PCR that do not rely on any eigengap assumption. Our notions reduce to standard ones under the eigengap assumption. • We design near-optimal polynomial approximation g(x) to sgn(x) satisfying (1.1) and (1.2). • We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary, our g(x) can be applied to matrices in a stable manner. • We obtain faster, provable PCA-free algorithms for PCP and PCR than known results."
  }],
  "year": 2017,
  "references": [{
    "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
    "authors": ["Allen-Zhu", "Zeyuan"],
    "venue": "In STOC,",
    "year": 2017
  }, {
    "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and Near-Optimal Rate",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Even faster accelerated coordinate descent using non-uniform sampling",
    "authors": ["Allen-Zhu", "Zeyuan", "Richtárik", "Peter", "Qu", "Zheng", "Yuan", "Yang"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Faster SVD-truncated regularized least-squares",
    "authors": ["Boutsidis", "Christos", "Magdon-Ismail", "Malik"],
    "venue": "IEEE International Symposium on Information Theory,",
    "year": 2014
  }, {
    "title": "Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations",
    "authors": ["Chan", "Tony F", "Hansen", "Per Christian"],
    "venue": "SIAM Journal on Scientific and Statistical Computing,",
    "year": 1990
  }, {
    "title": "Error analysis of an algorithm for summing certain finite series",
    "authors": ["Elliott", "David"],
    "venue": "Journal of the Australian Mathematical Society,",
    "year": 1968
  }, {
    "title": "Uniform approximation of sgn x by polynomials and entire functions",
    "authors": ["Eremenko", "Alexandre", "Yuditskii", "Peter"],
    "venue": "Journal d’Analyse Mathématique,",
    "year": 2007
  }, {
    "title": "Polynomials of the best uniform approximation to sgn (x) on two intervals",
    "authors": ["Eremenko", "Alexandre", "Yuditskii", "Peter"],
    "venue": "Journal d’Analyse Mathématique,",
    "year": 2011
  }, {
    "title": "Principal Component Projection Without Principal Component Analysis",
    "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Numerical Methods for Special Functions. Society for Industrial and Applied Mathematics, jan 2007. ISBN 978-0-89871-634-4",
    "authors": ["Gil", "Amparo", "Segura", "Javier", "Temme", "Nico M"],
    "year": 2007
  }, {
    "title": "Approximating the spectral sums of largescale matrices using chebyshev approximations",
    "authors": ["Han", "Insu", "Malioutov", "Dmitry", "Avron", "Haim", "Shin", "Jinwoo"],
    "venue": "arXiv preprint arXiv:1606.00942,",
    "year": 2016
  }, {
    "title": "Functions of Matrices",
    "authors": ["N. Higham"],
    "venue": "Society for Industrial and Applied Mathematics,",
    "year": 2008
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition",
    "authors": ["Musco", "Cameron", "Christopher"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Computing fundamental matrix decompositions accurately via the matrix sign function in two iterations: The power of zolotarev’s functions",
    "authors": ["Nakatsukasa", "Yuji", "Freund", "Roland W"],
    "venue": "SIAM Review,",
    "year": 2016
  }, {
    "title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
    "authors": ["Nesterov", "Yurii"],
    "year": 2004
  }, {
    "title": "Model order reduction: theory, research aspects and applications, volume",
    "authors": ["Schilders", "Wilhelmus H.A", "Van der Vorst", "Henk A", "Rommes", "Joost"],
    "year": 2008
  }, {
    "title": "An introduction to the conjugate gradient method without the agonizing pain",
    "authors": ["Shewchuk", "Jonathan Richard"],
    "year": 1994
  }, {
    "title": "Numerical methods for the qcdd overlap operator",
    "authors": ["van den Eshof", "Jasper", "Frommer", "Andreas", "Lippert", "Th", "Schilling", "Klaus", "van der Vorst", "Henk A"],
    "venue": "i. sign-function and error bounds. Computer Physics Communications,",
    "year": 2002
  }],
  "id": "SP:858c514a3524cdbdc2f5120038ab8f72694c78e2",
  "authors": [{
    "name": "Zeyuan Allen-Zhu",
    "affiliations": []
  }, {
    "name": "Yuanzhi Li",
    "affiliations": []
  }],
  "abstractText": "We solve principal component regression (PCR), up to a multiplicative accuracy 1+γ, by reducing the problem to Õ(γ−1) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires Õ(γ−2) such black-box calls. We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.",
  "title": "Faster Principal Component Regression  and Stable Matrix Chebyshev Approximation"
}