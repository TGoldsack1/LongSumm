{
  "sections": [{
    "text": "We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time T behaves as Ω( √ T ) and O( √ T log T ). This gives a tight characterization up to a √\nlog T factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Matérn-ν kernels, with the latter requiring ν > 2. Our results certify the near-optimality of existing bounds (Srinivas et al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Matérn kernel with ν > 2."
  }, {
    "heading": "1. Introduction",
    "text": "Bayesian optimization (BO) (Shahriari et al., 2016) is a powerful and versatile tool for black-box function optimization, with applications including parameter tuning, robotics, molecular design, sensor networks, and more. The idea is to model the unknown function as a Gaussian process with a given kernel function dictating the smoothness properties. This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.\nOne of the most attractive properties of BO is its efficiency in terms of the number of function samples used. Consequently, algorithms with rigorous guarantees on the tradeoff between samples and optimization performance are particularly valuable. Perhaps the most prominent work in the literature giving such guarantees is that of (Srinivas et al.,\n1Department of Computer Science & Department of Mathematics, National University of Singapore. Correspondence to: Jonathan Scarlett <scarlett@comp.nus.edu.sg>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n2010), who consider the cumulative regret:\nRT = T∑ t=1 ( max x f(x)− f(xt) ) , (1)\nwhere f is the function being optimized, and xt is the point chosen at time t. Under a Gaussian process (GP) prior and Gaussian noise, it is shown in (Srinivas et al., 2010) that an algorithm called Gaussian Process Upper Confidence Bound (GP-UCB) achieves a cumulative regret of the form\nRT = O ∗( √ TγT ), (2)\nwhere γT = maxx1,...,xT I(f ;y) (with function values f = (f(x1), . . . , f(xT )) and noisy samples y = (y1, . . . , yT )) is known as the maximum information gain. Here I(f ;y) denotes the mutual information (Cover & Thomas, 2001) between the function values and noisy samples, and O∗(·) denotes asymptotic notation up to logarithmic factors.\nThe guarantee (2) ensures sub-linear cumulative regret for many kernels of interest. However, the literature is severely lacking in algorithm-independent lower bounds, and without these, it is impossible to know to what extent the upper bounds, including (2), can be improved. In this work, we address this gap in detail in the special case of a onedimensional function. We show that the best possible cumulative regret behaves as Θ∗( √ T ) under mild assumptions on the kernel, thus identifying both cases where (2) is nearoptimal, and cases where it is strictly suboptimal."
  }, {
    "heading": "1.1. Related Work",
    "text": "An extensive range of BO algorithms have been proposed in the literature, typically involving the maximization of an acquisition function (Hennig & Schuler, 2012; HernándezLobato et al., 2014; Russo & Van Roy, 2014; Wang et al., 2016); see (Shahriari et al., 2016) for a recent overview. As mentioned above, the most relevant algorithm to this work for the noisy setting is GP-UCB (Srinivas et al., 2010), which constructs confidence bounds in which the function lies with high probability, and samples the point with the highest upper confidence bound. Several extensions to GPUCB have also been proposed, including contextual (Krause & Ong, 2011; Bogunovic et al., 2016a), batch (Contal et al., 2013; Desautels et al., 2014), and high-dimensional (Kandasamy et al., 2015; Rolland et al., 2018) variants.\nIn the noiseless setting, it has been shown that it is possible to achieve bounded cumulative regret (de Freitas et al., 2012; Kawaguchi et al., 2015) under some technical assumptions. In (de Freitas et al., 2012), this is done by keeping track of a set of potential maximizers, and sampling increasingly finely in order to shrink that set and “zoom in” towards the optimal point. Similar ideas have also been used in the noisy setting for studying batch variants of GP-UCB (Contal et al., 2013), simultaneous online optimization (SOO) methods (Wang et al., 2014), and lookahead algorithms that use confidence bounds (Bogunovic et al., 2016b). Returning to the noiseless setting, upper and lower bounds were given in (Grünewälder et al., 2010) for kernels satisfying certain smoothness assumptions, with the lower bounds showing that bounded cumulative regret is not always to be expected.\nAlongside the Bayesian view of the Gaussian process model, several works have also considered a non-Bayesian counterpart assuming that the function has a bounded norm in the associated reproducing kernel Hilbert space (RKHS). Interestingly, GP-UCB still provides similar guarantees to (2) in this setting (Srinivas et al., 2010). Moreover, lower bounds have been proved; see (Bull, 2011) for the noiseless setting, and (Scarlett et al., 2017) for the noisy setting. In the latter, the lower bounds nearly match the GP-UCB upper bound for the squared exponential (SE) kernel, but gaps remain for the Matérn kernel. For reference, we note that these kernels are defined as follows:\nkSE(x, x ′) = exp ( − ‖x− x\n′‖2 2l2\n) (3)\nkMatérn(x, x ′) =\n21−ν\nΓ(ν)\n(√ 2ν‖x− x′‖\nl )ν ×Bν (√ 2ν‖x− x′‖\nl\n) , (4)\nwhere l > 0 is a lengthscale parameter, ν > 0 is a smoothness parameter, Bν is the modified Bessel function, and Γ is the gamma function.\nThe multi-armed bandit (MAB) (Bubeck & Cesa-Bianchi, 2012) literature has developed alongside the BO literature, with the two often bearing similar concepts. The MAB literature is far too extensive to cover here, but it is worth mentioning that sharp lower bounds are known in numerous settings (Bubeck & Cesa-Bianchi, 2012), and the abovementioned concept of “zooming in” to the optimal point has also been explored (Kleinberg et al., 2008). To our knowledge, however, none of the existing MAB results are closely related to our own."
  }, {
    "heading": "1.2. Our Results and Their Implications",
    "text": "The main results of this paper are informally summarized as follows.\nMain Results (Informal). Under mild technical assumptions on the kernel, satisfied (for example) by the SE kernel and Matérn-ν kernel with ν > 2, the best possible cumulative regret of noisy BO in one dimension behaves as Ω( √ T ) and O( √ T log T ).\nOur results have several important implications:\n• To our knowledge, our lower bound is the first of any kind in the noisy Bayesian setting, and is tight up to a√\nlog T factor under our technical assumptions.\n• Our lower bound also establishes the order-optimality of the O∗( √ T ) upper bound of (Srinivas et al., 2010)\napplied to the SE kernel, up to logarithmic factors.\n• On the other hand, our upper bound establishes that the upper bound of (Srinivas et al., 2010) for the Matérn-ν kernel, namely O∗(T ν+2 2ν+2 ), is strictly suboptimal for\nν > 2. For example, if ν = 3, then this is O∗(T 0.625), as opposed to our upper bound of O∗(T 0.5). (See also (Shekhar & Javidi, 2017) for recent improvements over (Srinivas et al., 2010) under the Matérn kernel in higher dimensions and/or with smaller ν).\n• Another important implication for the Matérn kernel with ν > 2 is that the Bayesian setting is provably harder than the non-Bayesian RKHS counterpart; the latter has cumulative regret Ω(T ν+1 2ν+1 ) (Scarlett et al.,\n2017), which is strictly worse than O( √ T log T ).\nOur upper bound is stated formally in Section 3, and its technical assumptions are given in Section 2.1. We build on the ideas of (de Freitas et al., 2012) for the noiseless setting, while addressing highly non-trivial challenges arising in the presence of noise.\nOur lower bound is stated formally in Section 4, and its technical assumptions are given in Section 2.1. The analysis is based on a reduction to binary hypothesis testing and an application of Fano’s inequality (Cover & Thomas, 2001). This approach is inspired by previous work on lower bounds for stochastic convex optimization (Raginsky & Rakhlin, 2011), but the details are very different."
  }, {
    "heading": "2. Problem Setup",
    "text": ""
  }, {
    "heading": "2.1. Bayesian Optimization",
    "text": "We seek to sequentially optimize an unknown reward function f(x) over the one-dimensional domain D = [0, 1]; note that any interval can be transformed to this choice via re-scaling. At time t, we query a single point xt ∈ D and observe a noisy sample yt = f(xt) + zt, where zt ∼ N(0, σ2) for some noise variance σ2 > 0, with independence across different times. We measure the performance using the cumulative regret RT , defined in (1).\nWe henceforth assume f to be distributed according to Gaussian process (GP) (Rasmussen, 2006) having mean zero and kernel function k(x, x′). The posterior distribution of f given the points xt = [x1, . . . , xt]T and observations yt = [y1, . . . , yt]\nT up to time t is again a GP, with the posterior mean and variance given by (Rasmussen, 2006)\nµt(x) = kt(x) T ( Kt + σ 2It )−1 yt (5)\nσt(x) 2 = k(x, x)− kt(x)T ( Kt + σ 2It )−1 kt(x), (6)\nwhere kt(x) = [ k(xi, x) ]t i=1 , Kt = [ k(xi, xj) ] i,j\n, and It is the t× t identity matrix."
  }, {
    "heading": "2.2. Technical Assumptions",
    "text": "Here we introduce several assumptions that will be adopted in our main results, some of which were also used in the noiseless setting (de Freitas et al., 2012).\nAssumption 1. We have the following:\n1. The kernel k is stationary, depending on its inputs (x, x′) only through τ = x− x′;\n2. The kernel k satisfies k(x, x′) ≤ 1 for all (x, x′), and k(x, x) = 1 for all x ∈ D;\nGiven the stationarity assumption, the assumptions k(x, x′) ≤ 1 and k(x, x) = 1 are without loss of generality, as one can always re-scale the function and adjust the noise variance σ2 accordingly.\nNext, we give some high-probability assumptions on the random function f itself.\nAssumption 2. There exists a constant δ1 ∈ (0, 1) such that, with probability at least 1− δ1, we have the following:\n1. The function f has a unique maximizer x∗ such that\nf(x∗) ≥ f(x′) + (7)\nfor any local maximum x′ that differs from x∗, for some constant > 0.\n2. The function f is twice differentiable;\n3. The function f and its first two derivatives are bounded:\n|f(x)| ≤ c0, |f ′(x)| ≤ c1, |f ′′(x)| ≤ c2 (8)\nfor all x ∈ D and some constants (c0, c1, c2). This implies that f is c1-Lipschitz continuous, and f ′ is c2-Lipschitz continuous.\nThe assumption of a unique maximizer holds with probability one in most non-trivial cases (de Freitas et al., 2012), and\n(7) simply formally defines the gap to the second-highest peak. Moreover, given twice differentiability, the remaining conditions in (8) are very mild, only requiring that the function value and its derivatives are bounded, and formally defining the corresponding constants.\nNext, we provide assumptions regarding the derivatives of f and the resulting Taylor expansions (typically around the optimizer x∗). We adopt slightly different assumptions for the upper and lower bounds, starting with the former. Assumption 3. There exist constants δ2 ∈ (0, 1) and ρ0 ∈( 0, 12 )\nsuch that conditioned on the events in Assumption 2, we have with probability at least 1 − δ2 that one of the following is true:\n1. The maximizer is at an endpoint (i.e., x∗ = 0 or x∗ = 1), and satisfies the following locally linear behavior: For all ξ ∈ [0, ρ0] (if x∗ = 0) or ξ ∈ [−ρ0, 0] (if x∗ = 1), it holds that\nf(x∗)− c1|ξ| ≥ f(x∗ + ξ) ≥ f(x∗)− c1|ξ| (9) for some constants c1 ≥ c1 > 0.\n2. The maximizer satisfies x∗ ∈ (ρ0, 1− ρ0), and f satisfies the following locally quadratic behavior: For all ξ ∈ [−ρ0, ρ0], we have f(x∗)− c2ξ2 ≥ f(x∗ + ξ) ≥ f(x∗)− c2ξ2 (10)\nfor some constants c2 ≥ c2 > 0.\nThis assumption is near-identical to the main assumption adopted in the noiseless setting (de Freitas et al., 2012), and is also mild given the assumption of twice differentiability. Indeed, (9) and (10) amount to standard Taylor expansions, with the assumptions c1 > 0 and c2 > 0 only requiring non-vanishing gradient at the endpoint (first case) or non-vanishing second derivative at the function maximizer (second case). These conditions typically hold with probability one (de Freitas et al., 2012).\nThe following assumption will be used for the lower bound. Assumption 4. There exists constants δ′2 ∈ (0, 1) and ρ0 ∈( 0, 12 )\nsuch that conditioned on the events in Assumption 2, both of the following hold with probability at least 1− δ′2:\n1. For any x ∈ D and ξ ∈ [−ρ0, ρ0] for which x+ξ ∈ D, we have\nξ ·f ′(x)+c′2ξ2 ≤ f(x+ξ)−f(x) ≤ ξ ·f ′(x)+c′2ξ2. (11) for some (possibly negative) constants c′2, c ′ 2.\n2. The maximizer satisfies x∗ ∈ (ρ0, 1− ρ0), and f satisfies the following for all ξ ∈ [−ρ0, ρ0]: f(x∗)− c2ξ2 ≥ f(x∗ + ξ) ≥ f(x∗)− c2ξ2 (12)\nfor some constants c2 ≥ c2 > 0.\nx\n0 0.2 0.4 0.6 0.8 1\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2 2⇢0\nLocally Quadratic\nSlope  c1\nGap ✏\nRange  2c0\nFigure 1. Illustration of some of the main assumptions: The function is bounded within [−c0, c0] and its derivative within [−c1, c1], the gap to the second highest peak is at least , and the function is locally quadratic for points within a distance ρ0 of the maximizer.\nThe first part is similar to (10), but performs a Taylor expansion around an arbitrary point rather than the specific point x∗, and the second part is precisely (10). Note, however, that here we are assuming both of two conditions to hold, rather than one of two. Hence, we are implicitly assuming that the first item of Assumption 3 does not have a significant probability of occurring. For stationary kernels, the only situations where an endpoint has a high probability of being optimal are those where f varies very slowly (e.g., the SE kernel with a larger lengthscale than the domain width). Such functions are of limited practical interest.\nSimilarly to the noiseless setting (de Freitas et al., 2012), all of the above assumptions hold for the SE kernel, as well as the Matérn-ν kernel with ν > 2, with the added caveat that δ′2 in Assumption 4 is a function of the lengthscale and cannot be chosen arbitrarily. Specifically, a smaller lengthscale implies a smaller value of δ′2. In contrast, δ1 and δ2 in Assumptions 2 and 3 can be made arbitrary small by suitably changing the constants , c0, c1, c2, ρ0, and so on.\nAn illustration of some of the main assumptions and their associated constants is given in Figure 1."
  }, {
    "heading": "3. Upper Bound",
    "text": "Our upper bound is formally stated as follows.\nTheorem 1. (Upper Bound) Consider the problem of BO in one dimension described in Section 2.1, with time horizon T and noise variance σ2 satisfying σ2 ≥ cσ\nT 1−ζ for some cσ >\n0 and ζ > 0. Under Assumptions 1, 2, and 3, there exists an algorithm satisfying the following: With probability at least 1 − δ1 − δ2 (with respect to the Gaussian process f ), the average cumulative regret (averaged over the noisy\nsamples) satisfies E[RT ] ≤ C ( 1 + σ √ T log T ) . (13)\nHere δ1 and δ2 are defined in Assumptions 2 and 3, and C depends only on the constants therein and (cσ, ζ).\nThe assumption that σ2 ≥ cσ T 1−ζ for some (cσ, ζ) is very mild, since typically σ2 is constant with respect to T . The proof of Theorem 1 extends immediately to a high probability guarantee with respect to both f and the noisy samples (i.e., holding with probability 1−δ1−δ2−δ for δ in Lemma 1 below). We have stated the above form for consistency with the lower bound, which will be given in Section 4."
  }, {
    "heading": "3.1. High-Level Description of the Algorithm",
    "text": "The algorithm considered in the proof of Theorem 1 is described informally in Algorithm 1; the details will be established throughout the proof of Theorem 1, and a complete description is given in Appendix B.\nAlgorithm 1 Informal description of our algorithm, based on reducing uncertainty in epochs via repeated sampling.\nRequire: Domain D, GP prior (µ0, k0), discrete subdomain L ⊆ D, time horizon T .\n1: Initialize t = 1, epoch number i = 1, potential maximizers M(0) = L, and target confidence η(0). 2: while less than T samples have been taken do 3: Set η(i) = 12η(i−1). 4: Sample each point within a subset L(i) ⊆ L repeat-\nedlyK(i) times, where L(i) andK(i) are chosen such that after this sampling, all points x ∈M(i−1) satisfy upper and lower confidence bounds of the form\nLCBt(x) ≤ f(x) ≤ UCBt(x),\nwith the gap between the two bounded by |UCBt(x)− LCBt(x)| ≤ 2η(i).\n5: Update the set of potential maximizers:\nM(i) = { x ∈M(i−1) :\nUCBt(x) ≥ max x′∈∈M(i−1)\nLCBt(x ′) } .\n6: Increment i. 7: end while\nAs in the noiseless setting (de Freitas et al., 2012), the idea is to operate in epochs and sample a set of increasingly closelypacked points L(i) to reduce the posterior variance, but only within a set of potential maximizers that are updated according to the confidence bounds. As a simple means of bringing the effective noise level down, we perform resampling, i.e.,\nsampling the same point K(i) times consecutively. In each epoch, we sample enough to be able to produce upper and lower confidence bounds UCBt(x) and LCBt(x) that differ by at most a target value 2η(i) within M(i−1), and then the target is halved for the next epoch.\nWe do not expect our algorithm to perform well in practice by any means, but it still suffices for our purposes in establishing O( √ T log T ) regret. Indeed, we have made no attempt to optimize the corresponding constant factors, and doing so would require more sophisticated techniques. Moreover, the quantities L(i), K(i), UCBt, and LCBt in Algorithm 1 are chosen as functions of both the kernel and the constants appearing in our assumptions, which limits the algorithm’s practical utility even further. Note, however, that these constants are merely a function of the kernel, and that suitable bounds suffice in place of exact values (e.g., lower bound on ρ0, upper bound on c0, etc.).\nWhile our algorithm assumes a known time horizon T (which is used when selecting K(i); see Appendix B), this assumption can easily be dropped via a standard doubling trick. The details are given in Appendix A."
  }, {
    "heading": "3.2. Auxiliary Lemmas",
    "text": "Here we present two very standard auxiliary lemmas. We begin with a simpler version of the conditions of Srinivas et al. (Srinivas et al., 2010) guaranteeing that the posterior mean and variance provide valid confidence bounds with high probability. The reason for being slightly simpler is that we are considering a fixed time horizon.\nLemma 1. Fix δ ∈ (0, 1). For any finite set of points L ⊆ D and time horizon T , under the choice βT = 2 log |L|·T δ , it holds that\n|f(x)− µt(x)| ≤ β1/2T σt(x), ∀x ∈ L, t = 1, . . . , T, (14)\nwith probability at least 1− δ.\nProof. It was shown in (Srinivas et al., 2010) that for fixed x and t, the event |f(x)− µt(x)| ≤ β1/2T σt(x) holds with probability at least 1− e−βT /2. the lemma follows by substituting the choice of βT and taking the union bound over the |L| · T values of x and t.\nThe following lemma is also standard, and has been used (implicitly or explicitly) in the study of multiple algorithms that eliminate suboptimal points based on confidence bounds (de Freitas et al., 2012; Contal et al., 2013; Bogunovic et al., 2016b). For completeness, we give a short proof.\nLemma 2. Suppose that at time t, for all x within a set of points L̃ ⊆ D, it holds that\nLCBt(x) ≤ f(x) ≤ UCBt(x) (15)\nfor some bounds UCBt and LCBt such that\nmax x∈L̃ ∣∣UCBt(x)− LCBt(x)∣∣ ≤ 2η. (16) Then any point x ∈ L̃ satisfying f(x) < maxx′∈L̃ f(x′)− 4η must also satisfy\nUCBt(x) < max x∈L̃ LCBt(x). (17)\nThat is, any 4η-suboptimal point can be ruled out according to the confidence bounds (15).\nProof. We have\nUCBt(x) ≤ LCBt(x) + 2η (18) ≤ f(x) + 2η (19) < max\nx′∈L̃ f(x′)− 2η (20)\n≤ max x′∈L̃ UCBt(x ′)− 2η (21) ≤ max x′∈L̃ LCBt(x ′), (22)\nwhere (18) and (22) follow from (16), (19) and (21) follow from the confidence bounds in (15), and (20) follows from the assumption f(x) < maxx′∈L̃ f(x ′)− 4η."
  }, {
    "heading": "3.3. Outline of Proof of Theorem 1",
    "text": "Here we provide a high-level outline of the Proof of Theorem 1; the details are given in Appendix B.\nAlgorithm 1 only samples on a discrete sub-domain L. This set is chosen to be a set of regularly-spaced points that are fine enough to ensure that the cumulative regret with respect to maxx∈L f(x) is within a constant value of the cumulative regret with respect to maxx∈D f(x). Working with the finite set L helps to simplify the subsequent analysis. We split the epochs into two classes, which we call early epochs and late epochs. The late epochs are those in which we have shrunk the potential maximizers down enough to be entirely within the locally quadratic region, cf., Figure 1; here we only discuss the second case of Assumption 3, which is the more interesting of the two. Since the width of the locally quadratic region is constant, we can show that this occurs after a finite number of epochs, each lasting for at most O(log T ) time. Hence, even if we naively upper bound the instant regret by 2c0 according to (8), the overall regret incurred within the early epochs is insignificant.\nIn the later epochs, we exploit the locally quadratic behavior to show that the set of potential maximizers shrinks rapidly, i.e., by a constant factor after each epoch. As a result, we can let the repeatedly-sampled set L(i) in Algorithm 1 lie within a given interval that similarly shrinks, thereby controlling the number of samples we need to take in the epoch.\nBy Lemma 2, after we attain uniform η(i)-confidence, the instant regret incurred at each time thereafter is at most 4η(i). Using the fact that η(i) = η(0)2−i and summing over the epochs, we find that the overall regret behaves as in (13).\nA notable difficulty that we omitted above is how we attain the confidence bounds in order to update the potential maximizers M(i). While we directly apply Lemma 1 for the points that were repeatedly sampled, we found it difficult to do this for the non-sampled points. For those, we instead use Lipschitz properties of the function. In the early epochs, we use the global Lipschitz constant c1 from Assumption 2, whereas in the later epochs, we find a considerably smaller Lipschitz constant due to the locally quadratic behavior."
  }, {
    "heading": "4. Lower Bound",
    "text": "Our lower bound is formally stated as follows. Theorem 2. (Lower Bound) Consider the one-dimensional BO problem from Section 2.1, with time horizon T and noise variance σ2 satisfying σ2 ≤ c′σT 1−ζ ′ for some c′σ > 0 and ζ ′ > 0. Under Assumptions 1, 2, and 4, any algorithm must yield the following: With probability at least 1−δ1−δ′2 (with respect to the Gaussian process f ), the average cumulative regret (averaged over the noisy samples) satisfies\nE[RT ] ≥ C ′ ( 1 + σ √ T ) . (23)\nHere δ1 and δ′2 are defined in Assumptions 2 and 4, and C ′ depends only on the constants therein and (c′σ, ζ ′).\nThe assumption that σ2 ≤ c′σT 1−ζ ′ for some (c′σ, ζ ′) is very mild, since typically σ2 is constant with respect to T . The assumption is required to avoid (23) contradicting the trivial O(T ) upper bound. We also note that Theorem 2 immediately implies an Ω ( 1 + σ √ T )\nlower bound on the expected regret E[RT ] with respect to both f and the noisy samples, as long as 1−δ1−δ′2 > 0. As discussed following Assumption 4, the latter condition is mild.\nIn the remainder of the section, we introduce some of the main tools and ideas, and then outline the proof. We note that E[RT ] = Ω(1) is trivial, as the average regret of the first sample alone is lower bounded by a constant. As a result, we only need to show that E[RT ] = Ω(σ √ T )."
  }, {
    "heading": "4.1. Reduction to Binary Hypothesis Testing",
    "text": "Recall that f is a one-dimensional GP on [0, 1] with a stationary kernel k(x, x′). We fix ∆ > 0, and think of the GP as being generated by the following procedure:\n1. Generate a GP f0 with the same kernel on the larger domain [−∆, 1 + ∆];\n2. Randomly shift f0 along the x-axis by +∆ or−∆ with equal probability, to obtain f̃ ;\n3. Let f(x) = f̃(x) for x ∈ [0, 1].\nSince the kernel is stationary, the shifting does not affect the distribution, so the induced distribution of f is indeed the desired GP on [0, 1].\nWe consider a genie argument in which f0 is revealed to the algorithm. Clearly this additional information can only help the algorithm, so any lower bound still remains valid for the original setting. Stated differently, the algorithm knows that f is either f+ or f−, where\nf+(x) = f0(x+ ∆), (24) f−(x) = f0(x−∆). (25)\nSee Figure 2 for an illustrative example.\nThis argument allows us to reduce the BO problem to a binary hypothesis test with adaptive sampling, as depicted in Figure 3. The hypothesis, indexed by v ∈ {−,+}, is that the underlying function is fv . We show that under a suitable choice of ∆, achieving small cumulative regret means that we can construct a decision rule V̂ (x) such that V̂ = v with high probability, i.e., the hypothesis test is successful. The contrapositive statement is then that if the hypothesis test cannot be successful, we cannot achieve small cumulative regret, from which it only remains to prove the former. This idea was used previously for stochastic convex optimization in (Raginsky & Rakhlin, 2011).\nIn the remainder of the analysis, we implicitly condition on an arbitrary realization of f0, meaning that all expectations and probabilities are only with respect to the random index V and/or the noise. We also assume that f0 satisfies the conditions in Assumptions 1, 2, and 4, which holds with probability at least 1 − δ1 − δ′2. For sufficiently small ∆, the same assumptions are directly inherited by f+ and f−.\nWe henceforth assume that ∆ is indeed sufficiently small; we will verify that this is the case when we set its value.\nWe introduce some further notation. Letting x∗+, x ∗ −, and x∗0 denote the maximizers of f+, f− and f0 (which are unique by Assumption 2), we see that Assumption 4 ensures these are in the interior (0, 1), and hence the optimal values coincide: f+(x∗+) = f−(x ∗ −) = f0(x ∗ 0) =: f\n∗. To simplify some of the notation, instead of working with these functions directly, we consider the equivalent problem of minimizing the corresponding regret functions:\nr+(x) = f ∗ − f+(x), (26) r−(x) = f ∗ − f−(x). (27)\nIndeed, since we assume the algorithm knows f0 and hence also the optimal value f∗, it can always choose to transform the samples as y → f∗ − y. In this form, we have the convenient normalization r+(x∗+) = r−(x ∗ −) = 0."
  }, {
    "heading": "4.2. Auxiliary Lemmas",
    "text": "We first state the following useful properties of r+ and r−.\nLemma 3. The functions r+ and r− constructed above satisfy the following for sufficiently small ∆ under the conditions in Assumptions 2 and 4:\n1. We have for all x ∈ D that\nr+(x) < c2∆ 2 =⇒ r−(x) > c2∆2, (28)\nwhere c2 is defined in Assumption 4.\n2. There exists a constant c′ > 0 such that, for all x ∈ D,\n|r+(x)− r−(x)| ≤ c′ ( ∆|x− x∗0|+ ∆2 ) . (29)\n3. There exists a constant c′′ > 0 such that, for all x ∈ D,\nr+(x) ≥ c′′((x− x∗0) + ∆)2, r−(x) ≥ c′′((x− x∗0)−∆)2. (30)\nProof. See Appendix C.\nThe first part states that any point can be better than c2∆ 2- optimal for at most one of the two functions, the second part shows that the two functions are close for points near x∗0, and the third part shows that the instant regret is lower bounded by a quadratic function.\nThe first part of the Lemma 3 allows us to bound the cumulative regret using Fano’s inequality for binary hypothesis testing with adaptive sampling (Raginsky & Rakhlin, 2011). This inequality lower bounds the success probability of such a hypothesis test in terms of a mutual information quantity (Cover & Thomas, 2001). The resulting lower bound on regret is stated in the following; it is worth noting that the consideration of cumulative regret here provides a distinction from the analogous bound on the instant regret in (Raginsky & Rakhlin, 2011).\nLemma 4. Under the preceding setup, we have E[RT ] ≥ c2T∆2 ·H−12 ( log 2− I(V ;x,y) ) , (31)\nwhere V is equiprobable on {+,−}, and (x,y) are the selected points and samples when the minimization algorithm is applied to rV . Here H−12 : [0, log 2] → [ 0, 12 ] is the functional inverse of the binary entropy function H2(α) = α log 1 α + (1− α) log 11−α .\nSince this result is particularly fundamental to our analysis, we provide a proof at the end of this section."
  }, {
    "heading": "4.3. Outline of Proof of Theorem 2",
    "text": "Here we provide a high-level outline of the proof of Theorem 2; the details are given in Appendix D.\nOnce the lower bound in Lemma 4 is established, the main technical challenge is upper bounding the mutual information. A useful property called tensorization (e.g., see (Raginsky & Rakhlin, 2011)) allows us to simplify the mutual information with the vectors (x,y) to a sum of mutual informations containing only a single pair (xt, yt): I(V ;x,y) ≤∑Tt=1 I(V ; yt|xt). Each such mutual information term I(V ; yt|xt) can further be upper bounded by the KL divergence (Cover & Thomas,\n2001) between the conditional output distributions corresponding to r+ and r−, which in turn equals (r+(x)−r−(x))2 2σ2 when xt = x. By substituting the property (29) given in Lemma 3, we find that I(V ;x,y) is upper bounded by a constant times 1σ2 ( ∆2E [∑T t=1 |xt − x∗0|2 ] + T∆4 ) . If we can further upper bound I(V ;x,y) by a constant in (0, log 2), then (31) establishes an Ω(T∆2) lower bound.\nWe proceed by considering the cases E[RT ] ≥ c′′T∆2 and E[RT ] < c′′T∆2 separately, with c′′ given in (30). The former case will immediately give the lower bound in Theorem 2 when we set ∆, whereas in the latter case, we can use (30) to show that E [∑T t=1 |xt − x∗0|2 ] is upper bounded by a constant times T∆2, which means that the desired mutual information upper bound (see the previous paragraph) is attained under a choice of ∆ scaling as ( σ2\nT\n)1/4 . Under\nthis choice, the lower bound E[RT ] = Ω(T∆2) evaluates to Ω(σ √ T ), as required."
  }, {
    "heading": "4.4. Proof of Lemma 4",
    "text": "As mentioned above, the proof of Lemma 4 follows along the lines of (Raginsky & Rakhlin, 2011), which in turn builds on previous works using Fano’s inequality to establish minimax lower bounds in statistical estimation problems; see for example (Yu, 1997).\nIn the following, we useRT,+ = ∑T t=1 r+(xt) andRT,− =∑T\nt=1 r−(xt) to denote the cumulative regret associated with r+ and r−, respectively, and we generically write RT,v to denote one of the two with v ∈ {+,−}. We first use Markov’s inequality to write\nE[RT ] ≥ (1− α)c2T∆2 · P[RT ≥ (1− α)c2T∆2] (32)\nfor any α ∈ (0, 1). We proceed by analyzing the probability on the right-hand side.\nRecall that V is equiprobable on {+,−}, and (x,y) are generated by running the optimization algorithm on rV . Given the sequence of inputs x, let V̂ be the index v̂ ∈ {+,−} with the lower cumulative regret RT,v̂ = ∑T t=1 rv̂(xt). By Lemma 3, RT can be less than c2T∆ 2 for at most one of the two functions, and hence, ifRT,v ≤ (1−α)c2T∆2 then we must have V̂ = v. Therefore,\nPv [ RT ≥ (1− α)c2T∆2 ] ≥ Pv[V̂ 6= v], (33)\nwhere, here and subsequently, Pv and Ev denote probabilities and expectations when the underlying instant regret function is rv (i.e., the underlying function that the algorithm seeks to maximize is fv).\nContinuing, we can lower bound the probability appearing\nin (32) as follows:\nP[RT ≥ (1− α)c2T∆2]\n= 1\n2 ∑ v∈{+,−} Pv [ RT ≥ (1− α)c2T∆2 ] (34)\n≥ 1 2 ∑ v∈{+,−} Pv[V̂ 6= v] (35)\n≥ H−12 ( log 2− I(V ;x,y) ) , (36)\nwhere (35) follows from (33), and (36) follows from Fano’s inequality for binary hypothesis testing with adaptive sampling (see Eq. (22) and (24) of (Raginsky & Rakhlin, 2011)). The proof is completed by combining (32) and (36), and recalling that α can be arbitrarily small."
  }, {
    "heading": "5. Conclusion and Discussion",
    "text": "We have established tight scaling laws on the regret for Bayesian optimization in one dimension, showing that the optimal scaling is Ω( √ T ) and O( √ T log T ) under mild technical assumptions on the kernel. Our results highlight some limitations of the widespread upper bounds based on the information gain, as well as providing cases where the noisy Bayesian setting is provably more difficult than its non-Bayesian RKHS counterpart.\nAn immediate direction for further work is to sharpen the constant factors in the upper and lower bounds, and to establish whether the upper bound is attained by any algorithm that can also provide state-of-the-art performance in practice. We re-iterate that our algorithm is certainly not suitable for this purpose, as its cumulative regret contains large constant factors, and the algorithm makes use of a variety of specific constants present in the assumptions (though they are merely a function of the kernel).\nWe expect our techniques to extend to any constant dimension d ≥ 1; the main ideas from the noiseless upper bound still apply (de Freitas et al., 2012), and in the lower bound we can choose an arbitrary single dimension and introduce a random shift in that direction as per Section 4.1. While these extensions may still yield √ T poly(logT ) regret, the dependence on d would be exponential or worse in the upper bound, but constant in the lower bound, with the latter dependence certainly being suboptimal. Multi-dimensional lower bounding techniques based on Fano’s inequality (Raginsky & Rakhlin, 2011) may improve the latter to poly(d), but overall, attaining a sharp joint dependence on T and d appears to require different techniques.\nAcknowledgments. I would like to thank Ilija Bogunovic for his helpful comments and suggestions. This work was supported by an NUS startup grant."
  }],
  "year": 2018,
  "references": [{
    "title": "Time-varying Gaussian process bandit optimization",
    "authors": ["I. Bogunovic", "J. Scarlett", "V. Cevher"],
    "venue": "In Int. Conf. Art. Intel. Stats. (AISTATS),",
    "year": 2016
  }, {
    "title": "Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation",
    "authors": ["I. Bogunovic", "J. Scarlett", "A. Krause", "V. Cevher"],
    "venue": "In Conf. Neur. Inf. Proc. Sys. (NIPS),",
    "year": 2016
  }, {
    "title": "Regret Analysis of Stochastic and Nonstochastic Multi-Armed Bandit Problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "Found. Trend. Mach. Learn. Now Publishers,",
    "year": 2012
  }, {
    "title": "Convergence rates of efficient global optimization algorithms",
    "authors": ["A.D. Bull"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2011
  }, {
    "title": "Machine Learning and Knowledge Discovery in Databases, chapter Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration",
    "authors": ["E. Contal", "D. Buffoni", "A. Robicquet", "N. Vayatis"],
    "year": 2013
  }, {
    "title": "Exponential regret bounds for Gaussian process bandits with deterministic observations",
    "authors": ["N. de Freitas", "M. Zoghi", "A.J. Smola"],
    "venue": "In Int. Conf. Mach. Learn. (ICML),",
    "year": 2012
  }, {
    "title": "Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization",
    "authors": ["T. Desautels", "A. Krause", "J.W. Burdick"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2014
  }, {
    "title": "Regret bounds for Gaussian process bandit problems",
    "authors": ["S. Grünewälder", "Audibert", "J.-Y", "M. Opper", "J. ShaweTaylor"],
    "venue": "In Int. Conf. Art. Intel. Stats. (AISTATS),",
    "year": 2010
  }, {
    "title": "Entropy search for informationefficient global optimization",
    "authors": ["P. Hennig", "C.J. Schuler"],
    "venue": "J. Mach. Learn. Research,",
    "year": 2012
  }, {
    "title": "Predictive entropy search for efficient global optimization of black-box functions",
    "authors": ["J.M. Hernández-Lobato", "M.W. Hoffman", "Z. Ghahramani"],
    "venue": "In Adv. Neur. Inf. Proc. Sys. (NIPS),",
    "year": 2014
  }, {
    "title": "High dimensional Bayesian optimisation and bandits via additive models",
    "authors": ["K. Kandasamy", "J. Schneider", "B. Póczos"],
    "venue": "In Int. Conf. Mach. Learn.,",
    "year": 2015
  }, {
    "title": "Bayesian optimization with exponential convergence",
    "authors": ["K. Kawaguchi", "L.P. Kaelbling", "T. Lozano-Pérez"],
    "venue": "In Conf. Neur. Inf. Proc. Sys. (NIPS),",
    "year": 2015
  }, {
    "title": "Multi-armed bandits in metric spaces",
    "authors": ["R. Kleinberg", "A. Slivkins", "E. Upfal"],
    "venue": "In Proc. ACM Symp. Theory Comp. (STOC),",
    "year": 2008
  }, {
    "title": "Contextual Gaussian process bandit optimization",
    "authors": ["A. Krause", "C.S. Ong"],
    "venue": "In Conf. Neur. Inf. Proc. Sys. (NIPS),",
    "year": 2011
  }, {
    "title": "Information-based complexity, feedback and dynamics in convex programming",
    "authors": ["M. Raginsky", "A. Rakhlin"],
    "venue": "IEEE Trans. Inf. Theory,",
    "year": 2011
  }, {
    "title": "Gaussian processes for machine learning",
    "authors": ["C.E. Rasmussen"],
    "year": 2006
  }, {
    "title": "Highdimensional Bayesian optimization via additive models with overlapping groups",
    "authors": ["P. Rolland", "J. Scarlett", "I. Bogunovic", "V. Cevher"],
    "venue": "In Int. Conf. Art. Intel. Stats. (AISTATS),",
    "year": 2018
  }, {
    "title": "Learning to optimize via information-directed sampling",
    "authors": ["D. Russo", "B. Van Roy"],
    "venue": "In Conf. Neur. Inf. Proc. Sys. (NIPS),",
    "year": 2014
  }, {
    "title": "Lower bounds on regret for noisy Gaussian process bandit optimization",
    "authors": ["J. Scarlett", "I. Bogunovic", "V. Cevher"],
    "venue": "In Conf. Learn. Theory (COLT)",
    "year": 2017
  }, {
    "title": "Taking the human out of the loop: A review of Bayesian optimization",
    "authors": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"],
    "year": 2016
  }, {
    "title": "Gaussian process bandits with adaptive discretization",
    "authors": ["S. Shekhar", "T. Javidi"],
    "year": 2017
  }, {
    "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
    "authors": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"],
    "venue": "In Int. Conf. Mach. Learn. (ICML),",
    "year": 2010
  }, {
    "title": "Bayesian multi-scale optimistic optimization",
    "authors": ["Z. Wang", "B. Shakibi", "L. Jin", "N. de Freitas"],
    "venue": "In Int. Conf. Art. Intel. Stats. (AISTATS),",
    "year": 2014
  }, {
    "title": "Optimization as estimation with Gaussian processes in bandit settings",
    "authors": ["Z. Wang", "B. Zhou", "S. Jegelka"],
    "venue": "In Int. Conf. Art. Intel. Stats. (AISTATS),",
    "year": 2016
  }],
  "id": "SP:50daedc8b43632dfc0ea6901dd8e96022edd70be",
  "authors": [{
    "name": "Jonathan Scarlett",
    "affiliations": []
  }],
  "abstractText": "We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time T behaves as Ω( √ T ) and O( √ T log T ). This gives a tight characterization up to a √ log T factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Matérn-ν kernels, with the latter requiring ν > 2. Our results certify the near-optimality of existing bounds (Srinivas et al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Matérn kernel with ν > 2.",
  "title": "Tight Regret Bounds for Bayesian Optimization in One Dimension"
}