{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep convolutional neural networks (CNNs) have been crucial to the success of deep learning. Architectures based on CNNs have achieved unprecedented accuracy in domains ranging across computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), natural language processing (Collobert et al., 2011; Kalchbrenner et al., 2014;\n1Google Brain 2Work done as part of the Google AI Residency program (g.co/airesidency). Correspondence to: Lechao Xiao <xlc@google.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nKim, 2014), and recently even the board game Go (Silver et al., 2016; 2017).\nThe performance of deep convolutional networks has improved as these networks have been made ever deeper. For example, some of the best-performing models on ImageNet (Deng et al., 2009) have employed hundreds or even a thousand layers (He et al., 2016a;b). However, these extremely deep architectures have been trainable only in conjunction with techniques like residual connections (He et al., 2016a) and batch normalization (Ioffe & Szegedy, 2015). It is an open question whether these techniques qualitatively improve model performance or whether they are necessary crutches that solely make the networks easier to train. In this work, we study vanilla CNNs using a combination of theory and experiment to disentangle the notions of trainability and generalization performance. In doing so, we show that through a careful, theoretically-motivated initialization scheme, we can train vanilla CNNs with 10,000 layers using no architectural tricks.\nRecent work has used mean field theory to build a theoretical understanding of neural networks with random parameters (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018). These studies revealed a maximum depth through which signals can propagate at initialization, and verified empirically that networks are trainable precisely when signals can travel all the way through them. In the fully-connected setting, the theory additionally predicts the existence of an order-to-chaos phase transition in the space of initialization hyperparameters. For networks initialized on the critical line separating these phases, signals can propagate indefinitely and arbitrarily deep networks can be trained. While mean field theory captures the “average” dynamics of random neural networks it does not quantify the scale of gradient fluctuations that are crucial to the stability of gradient descent. A related body of work (Saxe et al., 2013; Pennington et al., 2017; 2018) has examined the input-output Jacobian and used random matrix theory to quantify the distribution of its singular values in terms of the activation function and the distribution from which the initial random weight matrices are drawn. These works concluded that networks can be trained most efficiently when the Jacobian is well-conditioned, a criterion that can be achieved with orthogonal, but not Gaussian, weight matrices. Together, these approaches have allowed researchers to efficiently train extremely deep network architectures, but so far they have been limited to neural networks composed of fully-connected layers.\nIn the present work, we continue this line of research and extend it to the convolutional setting. We show that a welldefined mean-field theory exists for convolutional networks in the limit that the number of channels is large, even when the size of the image is small. Moreover, convolutional networks have precisely the same order-to-chaos transition as fully-connected networks, with vanishing gradients in the ordered phase and exploding gradients in the chaotic phase. And just like fully-connected networks, very deep CNNs that are initialized on the critical line separating those two phases can be trained with relative ease.\nMoving beyond mean field theory, we additionally show that the random matrix analysis of (Pennington et al., 2017; 2018) carries over to the convolutional setting. Furthermore, we identify an efficient construction from the wavelet literature that generates random orthogonal matrices with the block-circulant structure that corresponds to convolution operators. This construction facilitates random orthogonal initialization for convolulational layers and enables good conditioning of the end-to-end Jacobian matrices of arbitrarily deep networks. We show empirically that networks with this initialization can train significantly more quickly than standard convolutional networks.\nFinally, we emphasize that although the order-to-chaos phase boundaries of fully-connected and convolutional networks look identical, the underlying mean-field theories are in fact quite different. In particular, a novel aspect of the convolutional theory is the existence of multiple depth scales that control signal propagation at different spatial frequencies. In the large depth limit, signals can only propagate along modes with minimal spatial structure; all other modes end up deteriorating, even at criticality. We hypothesize that this type of signal degradation is harmful for generalization, and we develop a modified initialization scheme that allows for balanced propagation of signals among all frequencies. In this scheme, which we call Delta-Orthogonal initialization, the orthogonal kernel is drawn from a spatially non-uniform distribution, and it allows us to train vanilla CNNs of 10,000 layers or more with no degradation in performance."
  }, {
    "heading": "2. Theoretical results",
    "text": "In this section, we first derive a mean field theory for signal propagation in random convolutional neural networks. We will follow the general methodology established in Poole et al. (2016); Schoenholz et al. (2017); Yang & Schoenholz (2017). We will then arrive at a theory for the singular value distribution of the Jacobian following Pennington et al. (2017; 2018). Together, this will allow us to derive theoretically motivated initialization schemes for convolutional neural networks that we call orthogonal kernels and Delta-Orthogonal kernels. Later we will demonstrate experimentally that these kernels outperform existing initialization schemes for very deep vanilla convolutional networks."
  }, {
    "heading": "2.1. A mean field theory for CNNs",
    "text": ""
  }, {
    "heading": "2.1.1. RECURSION RELATION FOR COVARIANCE",
    "text": "Consider an L-layer 1D1 CNN with periodic boundary conditions, filter width 2k + 1, number of channels c, spatial size n, per-layer weight tensors !l 2 R(2k+1)⇥c⇥c, and biases bl 2 Rc. Let : R! R be the activation function and let hlj(↵) denote the pre-activation unit at layer l, channel j, and spatial location ↵ 2 sp, where we define the set of spatial locations sp = {1, ..., n}. The forward-propagation dynamics can be described by the recurrence relation,\nhl+1j (↵) = X\ni2chn 2ker\n(hli(↵ + ))! l+1 ij ( ) + b l+1 j , (2.1)\nwhere ker = { 2 Z : | |  k} and chn = {1, . . . , c}. At initialization, we take the weights !lij ( ) to be drawn i.i.d. from the Gaussian N (0, 2!/(c(2k + 1))) and the biases blj\n1For notational simplicity, we consider one-dimensional convolutions, but the d-dimensional case proceeds identically.\nto be drawn i.i.d. from the Gaussian N (0, 2b ). Note that hli(↵) = h l i(↵ + n) = h l i(↵ n) since we assume periodic boundary conditions. We wish to understand how signals propagate through these networks. As in previous work in this vein, we will take the large network limit, which in this context corresponds to the number of channels c!1. This allows us to use powerful theoretical tools such as mean field theory and random matrix theory. Moreover, this approximation has been shown to give results that agree well with experiments on finite-size networks.\nIn the limit of a large number of channels, the central limit theorem implies that the pre-activation vectors hlj are i.i.d. Gaussian with mean zero and covariance matrix ⌃l↵,↵0 = E[hlj(↵)hlj(↵0)]. Here, the expectation is taken over the weights and biases and it is independent of the channel index j. In this limit, the covariance matrix takes the form (see Supplemental Materials (SM)),\n⌃ l+1 ↵,↵0 = 2 b + 2w 2k+1\nX\n2ker E ⇥ (hlj(↵+ )) (h l j(↵ 0 + )) ⇤ ,\n(2.2) and is independent of j. A more compact representation of this equation can be given as,\n⌃l+1 ⌘ A ? C(⌃l) , (2.3)\nwhere A = 12k+1I2k+1 and ? denotes 2D circular crosscorrelation, i.e. for any matrix C, A ? C is defined as,\n[A ? C]↵,↵0 = 1\n2k + 1\nX 2ker C↵+ ,↵0+ . (2.4)\nThe function C : PSDn ! PSDn is related to the C-map defined in Poole et al. (2016) (see also (Daniely et al., 2016)) and is given by,\n[C(⌃)]↵,↵0 = 2 ! Eh⇠N (0,⌃) [ (h↵) (h↵0)] + 2b . (2.5)\nAll but the two dimensions ↵ and ↵0 in eqn. (2.5) marginalize, so, as in (Poole et al., 2016), the C-map can be computed by a two-dimensional integral. Unlike in (Poole et al., 2016), ↵ and ↵0 do not correspond to different examples but rather to different spatial positions and eqn. (2.5) characterizes how signals from a single input propagate through convolutional networks in the mean-field approximation2."
  }, {
    "heading": "2.1.2. DYNAMICS OF SIGNAL PROPAGATION",
    "text": "We now seek to study the dynamics induced by eqn. (2.3). Schematically, our approach will be to identify fixed points of eqn. (2.3) and then linearize the dynamics around these\n2The multi-input analysis proceeds in precisely the same manner as we present here, but comes with increased notational complexity and features no qualitatively different behavior, so we focus our presentation on the single-input case.\nfixed points. These linearized dynamics will dictate the stability and rate of decay towards the fixed points, which determines the depth scales over which signals in the network can propagate.\nSchoenholz et al. (2017) found that for many activation functions (e.g. tanh) and any choice of w and b, the C-map has a fixed point ⌃⇤ (i.e. C(⌃⇤) = ⌃⇤) of the form,\n⌃ ⇤ ↵,↵0 = q ⇤ ( ↵,↵0 + (1 ↵,↵0)c ⇤ ) , (2.6)\nwhere a,b is the Kronecker- , q⇤ is the fixed-point variance of a single input, and c⇤ is the fixed-point correlation between two inputs. It follows from the form of eqn. (2.4) that ⌃⇤ is also a fixed point of the layer-to-layer covariance map in the convolutional case (eqn. (2.3)), i.e. ⌃⇤ = A ? C(⌃⇤).\nTo analyze the dynamics of the iteration map (2.3) near the fixed point ⌃⇤, we define ✏l = ⌃⇤ ⌃l and expand eqn. (2.3) to lowest order in ✏. This expansion requires the Jacobian of the C-map evaluated at the fixed point, the properties of which we analyze in the SM. In brief, perturbations in q⇤ and c⇤ evolve independently and the Jacobian decomposes into a diagonal eigenspace Vd with eigenvalue q⇤ , and an off-diagonal eigenspace Vo.d. with eigenvalue c⇤ . The eigenvalues are given by3,\nc⇤ = 2 wEh⇠N (0,C⇤)[ 0(h1) 0(h2)] , h1 6= h2 , q⇤ = 2 wEh⇠N (0,C⇤)[ 00(h1) (h1) + 0(h1)2] , (2.7)\nand the eigenspaces have bases,\nBd = {M ↵,↵ : M↵,↵↵̄,↵̄0 = ↵,↵̄ ↵,↵̄0 + ↵̄,↵ + ↵̄0,↵}↵2sp\nBo.d. = {M ↵,↵0 : M↵,↵ 0 ↵̄,↵̄0 = ↵,↵̄ ↵0,↵̄0 + ↵,↵̄0 ↵0,↵̄}↵ 6=↵0 ,\n(2.8)\ni.e. Vd = span(Bd) and Vo.d = span(Bo.d.). Note that q⇤ and c⇤ also were found in Schoenholz et al. (2017) to control signal propagation in the fully-connected case. The constant is given in Lemma B.2 of the SM but does not concern us here. This eigen-decomposition implies that the layer-wise deviations from the fixed point evolve under eqn. (2.3) as,\n✏l+1 = q⇤A ? ✏ l d + c⇤A ? ✏ l o.d. + O((✏ l ) 2 ) , (2.9)\nwhere ✏d and ✏o.d. are decomposition of ✏ into the eigenspaces Vd and Vo.d..\nEqn. (2.9) defines the linear dynamics of random convolutional neural networks near their fixed points and is the basis for the in-depth analysis of the following subsections.\n3By the symmetry of ⌃⇤, these expectations are independent of spatial location and of the choice of h1 and h2."
  }, {
    "heading": "2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION",
    "text": "In the fully-connected setting, the dynamics of signal propagation near the fixed point are governed by scalar evolution equations. In contrast, the convolutional setting enjoys much richer dynamics, as eqn. (2.9) describes a multi-dimensional system that we now analyze.\nIt follows from eqns. (2.4) and (2.8) (see also the SM) that A does not mix the diagonal and off-diagonal eigenspaces, i.e. A ? ✏d 2 Vd and A ? ✏o.d. 2 Vo.d.. To see this, note that for M↵,↵ 0 2 Vo.d., the definition implies M↵,↵ 0\n↵̄+ ,↵̄0+ =\nM↵ ,↵ 0 ↵̄,↵̄0 . This property ensures that A ? M ↵,↵0 can be expressed as a linear combination of matrices in Vo.d., which means it also belongs to Vo.d. The same argument applies to M↵,↵ 2 Vd.. As a result, these eigenspaces evolve entirely independently under the linearization of the covariance iteration map (2.3).\nLet l0 denote the depth over which transient effects persist and after which eqn. (2.9) accurately describes the linearized dynamics. Therefore, at depths larger than l0, we have\n✏l ⇡ A ? · · · A ?| {z } l l0 ( l l0q⇤ ✏ l0 d + l l0 c⇤ ✏ l0 o.d.) . (2.10)\nThis matrix-valued equation is still somewhat complicated owing to the nested applications of A. To further elucidate the dynamics, we can move to a Fourier basis, which diagonalizes the circular cross-correlation operator and decouples the modes of eqn. (2.10). In particular, let F denote the 2D discrete Fourier transform and ✏̃↵,↵0 ⌘ F(✏)↵,↵0 denote a Fourier mode of ✏. Then eqn. (2.10) becomes a simple scalar equation,\n✏̃l↵,↵0 ⇡ ( ↵,↵0 q⇤) l l0 [✏̃l0d ]↵,↵0+( ↵,↵0 c⇤) l l0 [✏̃l0o.d.]↵,↵0 , (2.11) with ↵,↵0 = F(A)⇤↵,↵0 . Thus, the linearized dynamics of convolutional neural networks decouple into independentlyevolving Fourier modes that evolve near the fixed point at frequency-dependent rates."
  }, {
    "heading": "2.1.4. FIXED-POINT ANALYSIS",
    "text": "The stability of the fixed point ⌃⇤ is determined by whether nearby points move closer or farther from ⌃⇤ under the dynamics described by eqn. (2.9). Eqn. (2.11) shows that this condition depends on the whether the quantities ↵,↵0 q⇤ and ↵,↵0 c⇤ are less than or greater than one.\nSince A is a diagonal matrix, the eigenvalues ↵,↵0 have a specific structure. In particular, the set of eigenvalues is comprised of n copies of the 1D discrete Fourier transform of the diagonal entries of A. Furthermore, since the diagonal entries of A are non-negative and sum to one, their Fourier coefficients have absolute value no larger than one and the zero-frequency coefficient is equal to one; see Figure 4\nfor the full distribution in the case of 2D convolutions. It follows that the fixed point ⌃⇤ will be stable if and only if q⇤ < 1 and c⇤ < 1.\nThese stability conditions are precisely the ones found to govern fully-connected networks (Poole et al., 2016; Schoenholz et al., 2017). Moreover, the fixed point matrix ⌃\n⇤ is also the same as in the fully-connected case. Together, these observations imply that the entire fixed-point structure of the convolutional case is identical to that of the fullyconnected case. In particular, based on the results of (Poole et al., 2016), we can immediately conclude that the ( w, b) hyperparameter plane is separated by the line 1 = 1 into an ordered phase with c⇤ = 1 in which all pixels approach the same value, and a chaotic phase with c⇤ < 1 in which the pixels become decorrelated with one another; see the SM for a review of this phase diagram analysis."
  }, {
    "heading": "2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION",
    "text": "We now assume that the conditions for a stable fixed point are met, i.e. q⇤ < 1 and c⇤ < 1, and we consider the rate at which the fixed point is approached. As in (Schoenholz et al., 2017), it is convenient to additionally assume q⇤ < c⇤ so that the dynamics in the diagonal subspace can be neglected. In this case, eqn. (2.11) can be rewritten as\n✏̃l↵,↵0 ⇡ e (l l0)/⇠↵,↵0 [✏̃o.d.] l0 ↵,↵0 , (2.12)\nwhere ⇠↵,↵0 = 1/ log( c⇤ ↵,↵0) are depth scales governing the convergence of the different modes. In particular, we expect signals corresponding to a specific Fourier mode\nf↵,↵0 to be able to travel a depth commensurate to ⇠↵,↵0 through the network. Thus, unlike fully-connected networks which exhibit only a single depth scale, convolutional networks feature a hierarchy of depth scales.\nRecalling that ↵,n ↵ = 1, it follows that ⇠c ⌘ ⇠↵,n ↵ = 1/ log c⇤ , which is identical to the depth scale governing signal propagation through fully-connected networks. It follows from (Schoenholz et al., 2017) that when 1 = 1, ⇠↵,n ↵ diverges and thus convolutional networks can propagate signals arbitrarily far through the f↵,n ↵ modes. Since | ↵,↵0 | < 1 for ↵0 6= n ↵, these are the only modes through which signals can propagate without attenuation. Finally, we note that the f↵,n ↵ modes correspond to perturbations that are spatially uniform along the cyclic diagonals of the covariance matrix. The fact that all signals with additional spatial structure attenuate for large depth suggests that deep critical convolutional networks behave quite similarly to fully-connected networks, which also cannot propagate spatially-structured signals."
  }, {
    "heading": "2.1.6. NON-UNIFORM KERNELS",
    "text": "The similarities between signal propagation in convolutional neural networks and fully-connected networks in the limit of large depth are surprising. A consequence may be that the performance of very deep convolutional networks degrades as the signal is forced to propagate along modes with minimal spatial structure. Indeed, Fig. 3 shows that the generalization performance decreases with depth, and that for very large depth it barely surpasses the performance of a\nfully-connected network.\nIf increased spatial uniformity is the problem, eqn. (2.12) holds the solution. In order for all modes to propagate without attenuation, it is necessary that ↵,↵0 = 1 for all ↵, ↵0. In fact, it is easy to show that the distribution of { ↵,↵0} can be modified by allowing for spatial non-uniformity in the variance of the weights within the kernel. To this end, we introduce a non-negative vector v = (v ) 2ker chosen such that P v = 1, and initialize the weights of the net-\nwork according to wlij( ) ⇠ N (0, 2wv /c). Each choice of v will induce a new dynamical equation analogous to eqn. (2.3) (see SM),\n⌃l+1 = Av ? C(⌃ l ) , (2.13)\nwhere Av = diag(v). It follows directly from the previous analysis that the linearized dynamics of eqn. (2.13) will be identical to the dynamics of eqn (2.3), only now with ↵,↵0 = F(Av)⇤↵,↵0 . By the same argument presented in Section 2.1.3, the set of eigenvalues is now comprised of n copies of the 1D Fourier transform of v. As a result, it is possible to control the depth scales over which different modes of the signal can propagate through the network by changing the variance vector v. We will return to this point in section 2.4."
  }, {
    "heading": "2.2. Back-propagation of signal",
    "text": "We now turn our attention to the back-propgation of error signals through a convolutional network. Let E denote the loss and lj(↵) the back-propagated signal at layer l, channel j and spatial location ↵, i.e.,\nlj(↵) = @E\n@hlj(↵) . (2.14)\nThe recurrence relation is given by\nlj(↵) = X\ni2chn\nX 2ker l+1i (↵ )! l+1 ji ( ) 0 (hlj(↵)).\nAs in (Schoenholz et al., 2017), we additionally make the assumption that the weights used during back-propagation are drawn independently from the weights used in forward propagation, in which case the random variables { lj}j2chn are independent for each l. The covariance matrices ⌃̃l ⌘ E ⇥ lj( l j) T ⇤ back-propagate according to, ⌃̃l↵,↵0 = X\n2ker v ⌃̃\nl+1 ↵ ,↵0 · 2 wEh⇠N (0,⌃l)[ 0(h↵) 0(h↵0)] .\n(2.15) We are primarily interested in the diagonal of ⌃̃l, which measures the variance of back-propagated signals. We will also assume l > l0 (see section 2.1.3) so that ⌃l is wellapproximated by ⌃⇤. In this case,\n⌃̃l↵,↵ ⇡ 1 X\n2ker v ⌃̃\nl+1 ↵ ,↵ , (2.16)\nwhere we used eqn. (2.7). Therefore we find that, ⌃̃l↵,↵ ⇠ L l1 ⌃̃ L ↵,↵, where L is the total depth of the network. As in the fully-connected case, 1 = 1 is a necessary condition for gradient signals to neither explode nor vanish as they back-propagate through a convolutional network. However, as discussed in (Pennington et al., 2017; 2018), this is not always a sufficient condition for trainability. To further understand backward signal propagation, we need to push our analysis beyond mean field theory."
  }, {
    "heading": "2.2.1. BEYOND MEAN FIELD THEORY",
    "text": "We have observed that the quantity 1 is crucial for determining signal propagation in CNNs, both in the forward and backward directions. As discussed in (Poole et al., 2016), 1 equals the the mean squared singular value of the Jacobian J l of the layer-to-layer transition operator. Beyond just the second moment, higher moments and indeed the whole distribution of singular values of the entire end-to-end Jacobian J = Q l J\nl are important for ensuring trainability of very deep fully-connected networks (Pennington et al., 2017; 2018). Specifically, networks train well when their input-output Jacobians exhibit dynamical isometry, namely the property that the entire distribution of singular values is close to 1.\nIn fact, we can adopt the entire analysis of (Pennington et al., 2017; 2018) into the convolutional setting with essentially no modification. The reason stems from the fact that, because convolution is a linear operator, it has a matrix representation, W l, which appears in the end-to-end Jacobian in precisely the same manner as do the weight matrices in the fully-connected case. In particular, J = QL l=1 D\nlW l, where Dl is the diagonal matrix whose diagonal elements contain the vectorized representation of derivatives of postactivation neurons in layer l. Roughly speaking, since this is the same expression as in (Pennington et al., 2017; 2018), the conclusions found in that work regarding dynamical isometry apply equally well in the convolutional setting.\nThe analysis of Pennington et al. (2017; 2018) reveals that the singular values of J depends crucially on the distribution of singular values of W l and Dl. In particular, to achieve dynamical isometry, all of these matrices should be close to orthogonal. As in the fully-connected case, the singular values of Dl can be made arbitrarily close to 1 by choosing a small value for q⇤ and by using an activation function like tanh that is smooth and linear near the origin. In the convolutional setting, the matrix representation of the convolution operator W l is a c ⇥ c block matrix with n ⇥ n circulant blocks. Note that in the large c limit, n/c! 0 and the relative size of the blocks vanishes. Therefore, if the weights are i.i.d. random variables, we can invoke universality results from random matrix theory to conclude its singular value distribution converges to the Marcenko-Pastur distribution; see Fig. S3 in the SM. As such, we find that CNNs with i.i.d. weights cannot achieve dynamical isometry. We address this issue in the next section."
  }, {
    "heading": "2.3. Orthogonal Initialization for CNNs",
    "text": "In (Pennington et al., 2017; 2018), it was observed that dynamical isometry can lead to dramatic improvements in training speed, and that achieving these favorable conditions requires orthogonal weight initializations. While the procedure to generate random orthogonal weight matrices in the fully-connected setting is well-known, it is less obvious how to do so in the convolutional setting, and at first sight it is not at all clear whether it is even possible. We resolve this question by invoking a result from the wavelet literature (Kautsky & Turcajov, 1994) and provide an explicit construction. We will focus on the two-dimensional convolution here and begin with some notation.\nDefinition 2.1. We say K 2 Rk⇥k⇥cin⇥cout is an orthogonal kernel if for all x 2 Rn⇥n⇥cin , kK ⇤ xk2 = kxk2.\nDefinition 2.2. Consider the block matrices B = {Bi,j}0i,jp 1 2 Rpn⇥pn and C = {Ci,j}0i,jq 1 2 Rqn⇥qn, with constituent blocks Bi,j 2 Rn⇥n and Ci,j 2\nRn⇥n. Define the block-wise convolution operator ⇤ by,\n[B⇤C]i,j = X\ni0,j0\nBi0,j0Ci i0,j j0 , (2.17)\nwhere the out-of-range matrices are taken to be zero.\nAlgorithm 1 shows how to construct orthogonal kernels for 2D convolutions of size k ⇥ k ⇥ cin ⇥ cout with cin  cout. One can employ the same method to construct kernels of higher (or lower) dimensions. This new initialization method can dramatically boost the learning speed of deep CNNs; see Fig. 5 and Section 3.2."
  }, {
    "heading": "2.4. Delta-Orthogonal Initialization",
    "text": "In Section 2.1.5 it was observed that, in contrast to fullyconnected networks, CNNs have multiple depth scales controlling propagation of signals along different Fourier modes. Even at criticality, for generic variance-averaging vectors v, the majority of these depth scales are finite. However, there does exist one special averaging vector for which all of the depth scales are infinite: a one-hot vector, i.e. vi = k,i. This kernel places all of its variance in the spatial center of the kernel and zero variance elsewhere. In this case, the eigenvalues ↵,↵0 are all equal to 1 and all depth scales diverge, implying that signals can propagate arbitrarily far along all Fourier modes.\nIf we combine this special averaging vector with the orthogonal initialization of the previous section, we obtain a powerful new initialization scheme that we call Delta-Orthogonal Initialization. Matrices of this type can be generated from Algorithm 1 with k = 1 and padding with appropriate zeros or directly from Algorithm 2 in the SM.\nIn the following sections, we demonstrate experimentally that extraordinarily deep convolutional networks can be trained with these initialization techniques.\nAlgorithm 1 2D orthogonal kernels for CNNs, available in TensorFlow via the ConvolutionOrthogonal initializer.\nInput: k kernel size, cin number of input channels, cout number of output channels. Return: a k⇥ k⇥ cin ⇥ cout tensor K. Step 1. Let K be the 1⇥ 1⇥ cout⇥ cout tensor such that K[0, 0] = I , where I is the cout ⇥ cout identity matrix. Step 2. Repeat the following (k 1) times: Randomly generate two orthogonal projection matrices P and Q of size cout ⇥ cout and set (see eqn. (2.17))\nK K⇤ \nPQ P (1 Q) (1 P )Q (1 P )(1 Q) .\nStep 3. Randomly generate a cin ⇥ cout matrix H with orthonormal rows and for i = 0, . . . , k 1 and j = 0, . . . , k 1, set K[i, j] HK[i, j]. Return K."
  }, {
    "heading": "3. Experiments",
    "text": "To support the theoretical results built up in Section 2, we trained a large number of very deep CNNs on MNIST and CIFAR-10 with tanh as the activation function. We use the following vanilla CNN architecture. First we apply three 3 ⇥ 3 ⇥ c convolutions with strides 1, 2 and 2 in order to increase the channel size to c and reduce the spatial dimension to 7 ⇥ 7 (or 8 ⇥ 8 for CIFAR-10), and then a block of d 3 ⇥ 3 ⇥ c convolutions with d varying from 2 to 10, 000. Finally, an average pooling layer and a fullyconnected layer are applied. Here c = 256 when d  256 and c = 128 otherwise. To maximally support our theories, we applied no common techniques (including learning rate decay). Note that the early downsampling is necessary from a computational perspective, but it does diminish the maximum achievable performance; e.g. our best achieved test accuracy with downsampling was 82% on CIFAR-10. We performed an additional experiment training a 50 layers network without downsampling. This resulted in a test accuracy of 89.90%, which is comparable to the best performance on CIFAR-10 using a tanh architecture that we were able to find (89.82%, (Mishkin & Matas, 2015))."
  }, {
    "heading": "3.1. Trainability and Critical Initialization",
    "text": "The analysis in Section 2.1 gives a prediction for precisely which initialization hyperparameters a CNN will be trainable. In particular, we predict that the network ought to be trainable provided L . ⇠c. To test this, we train a large number of convolutional neural networks on MNIST with depth varying between L = 10 and L = 600 and with weights initialized with 2w 2 [0, 4]. In Fig. 2 we plot – using a heatmap – the training accuracy obtained by these networks after different numbers of steps. Additionally we\noverlay the depth scale predicted by our theory, ⇠c. We find strikingly good agreement between our theory of random networks and the results of our experiments."
  }, {
    "heading": "3.2. Orthogonal Initialization and Ultra-deep CNNs",
    "text": "We argued in Section 2.2.1 that the input-output Jacobian of CNNs with i.i.d. weights will become increasingly illconditioned as the number of layers grows. On the other hand, orthogonal weight initializations can achieve dynamical isometry and dramatically boost the training speed. To verify this, we train a 4,000-layer CNN on MNIST using a critically-tuned Gaussian weight initialization and the orthogonal initialization scheme developed in Section 2.3. Fig. 5 shows that the network with Gaussian initialization learns slowly (test and training accuracy is below 60% after 90, 000 steps, about 60 epochs). In contrast, orthogonal initialization learns quickly with test accuracy above 60% after only 1 epoch, and achieves 95% after 10, 000 steps or about 7 epochs."
  }, {
    "heading": "3.3. Multi-dimensional Signal Propagation",
    "text": "The analysis in Section 2.1.3 and Section 2.1.6 suggest that CNNs initialized with kernels with spatially uniform variance may suffer a degradation in generalization performance as the depth increases. Fig. 3 shows the learning curves of CNNs on CIFAR-10 with depth varying from 32 to 8192. Although the orthogonal initialization enables even the deepest model to reach 100% training accuracy, the test accuracy decays as the depth increases with the deepest mode generalizing only marginally better than a fully-connected network.\nTo test whether this degradation in performance may be the result of attenuation of spatially non-uniform signals, we trained a variety of models on CIFAR-10 whose kernels were initialized with spatially non-uniform variance. According to the analysis in Section 2.1.6, changing the shape of this non-uniformity controls the depth scales over which different Fourier components of the signal can propagate through the network. We examined five different non-uniform critical Gaussian initialization methods. The variance vectors v were chosen in the following way: GS0 refers to the one-hot delta initialization for which the eigenvalues ↵,↵0 are all equal to 1. GS1, GS2 and GS3 are obtained by interpolating between GS0 and GS4, which is the uniform variance initialization.\nEach variance vector has exactly 8⇥ 8 singular values, plotted in Fig. 4(b) in descending order. Note that from GS0 to GS4, the singular values become more poorly-conditioned (the distribution becomes more concentrated around 0). Fig. 4(a) shows that the relative fall-off of generalization performance with depth follows the same pattern: the more poorly-conditioned the singular values the worse the model generalizes. These observations suggest that salient infor-\nmation may be propagating along multiple Fourier modes."
  }, {
    "heading": "3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",
    "text": "Our theory predicts that an ultra-deep CNNs can train faster and perform better if critically initialized using DeltaOrthogonal kernels. To test this theory, we train CNNs of 1,250, 2,500, 5,000 and 10,000 layers on both MNIST and CIFAR-10 (Fig. 1). All these networks learn surprisingly quickly and, remarkably, the learning time measured in number of training epochs is independent of depth. Furthermore, our experimental results match well with the predicted benefits of this initialization: 99% test accuracy on MNIST for a 10,000-layer network, and 82% on CIFAR-10. To isolate the benefits of the Delta-Orthogonal init, we also train a 2048- layer CNN (Fig. 3) using the spatially-uniform orthogonal initialization proposed in Section 2.3; the testing accuracy is about 70%. Note that the test accuracy using (spatially uniform) Gaussian (non-orthogonal) initialization is already below 70% when the depth is 259."
  }, {
    "heading": "4. Discussion",
    "text": "In this work, we developed a theoretical framework based on mean field theory to study the propagation of signals in deep convolutional neural networks. By examining the necessary conditions for signals to flow both forward and backward through the network without attenuation, we derived an initialization scheme that facilitates training of vanilla CNNs of unprecedented depths. We presented an algorithm for the generation of random orthogonal convolutional kernels, an ingredient that is necessary to enable dynamical isometry, i.e. good conditioning of the network’s input-output Jacobian. In contrast to the fully-connected case, signal propagation in CNNs is intrinsically multi-dimensional – we showed how to decompose those signals into independent Fourier modes and how to promote uniform signal propagation across them. By leveraging these various theoretical insights, we demonstrated empirically that it is possible to train vanilla CNNs with 10,000 layers or more.\nOur results indicate that we have removed all the major fundamental obstacles to training arbitrarily deep vanilla convolutional networks. In doing so, we have layed the groundwork to begin addressing some outstanding questions in the deep learning community, such as whether depth alone can deliver enhanced generalization performance. Our initial results suggest that past a certain depth, on the order of tens or hundreds of layers, the test performance for vanilla convolutional architecture saturates. These observations suggest that architectural features such as residual connections and batch normalization are likely to play an important role in defining a good model class, rather than simply enabling efficient training."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Xinyang Geng, Justin Gilmer, Alex Kurakin, Jaehoon Lee, Hoang Trieu Trinh, and Greg Yang for useful discussions and feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Natural language processing (almost) from scratch",
    "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "ImageNet: A Large-Scale Hierarchical Image Database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"],
    "venue": "In CVPR09,",
    "year": 2009
  }, {
    "title": "How to start training: The effect of initialization and architecture",
    "authors": ["B. Hanin", "D. Rolnick"],
    "venue": "arXiv preprint arXiv:1803.01719,",
    "year": 2018
  }, {
    "title": "On the selection of initialization and activation function for deep neural networks",
    "authors": ["S. Hayou", "A. Doucet", "J. Rousseau"],
    "venue": "arXiv preprint arXiv:1805.08266,",
    "year": 2018
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2016
  }, {
    "title": "Identity mappings in deep residual networks",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"],
    "venue": "arXiv preprint arXiv:1404.2188,",
    "year": 2014
  }, {
    "title": "Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach",
    "authors": ["R. Karakida", "S. Akaho", "Amari", "S.-i"],
    "year": 2018
  }, {
    "title": "A matrix approach to discrete wavelets",
    "authors": ["J. Kautsky", "R. Turcajov"],
    "year": 1994
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Y. Kim"],
    "venue": "arXiv preprint arXiv:1408.5882,",
    "year": 2014
  }, {
    "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "year": 2012
  }, {
    "title": "All you need is a good init",
    "authors": ["D. Mishkin", "J. Matas"],
    "venue": "CoRR, abs/1511.06422,",
    "year": 2015
  }, {
    "title": "The emergence of spectral universality in deep networks",
    "authors": ["J. Pennington", "S.S. Schoenholz", "S. Ganguli"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2018
  }, {
    "title": "Exponential expressivity in deep neural networks through transient chaos",
    "authors": ["B. Poole", "S. Lahiri", "M. Raghu", "J. Sohl-Dickstein", "S. Ganguli"],
    "year": 2016
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"],
    "venue": "arXiv preprint arXiv:1312.6120,",
    "year": 2013
  }, {
    "title": "A correspondence between random neural networks and statistical field theory",
    "authors": ["S.S. Schoenholz", "J. Pennington", "J. Sohl-Dickstein"],
    "venue": "arXiv preprint arXiv:1710.06570,",
    "year": 2017
  }, {
    "title": "Mastering the game of go with deep neural networks and tree search",
    "authors": ["J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"],
    "venue": "Nature, 529(7587):484–489,",
    "year": 2016
  }, {
    "title": "Mastering the game of go without human knowledge",
    "authors": ["D. Silver", "J. Schrittwieser", "K. Simonyan", "I. Antonoglou", "A. Huang", "A. Guez", "T. Hubert", "L. Baker", "M. Lai", "A Bolton"],
    "year": 2017
  }, {
    "title": "Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion, 2018",
    "authors": ["G. Yang", "S.S. Schoenholz"],
    "venue": "URL https: //openreview.net/forum?id=rJGY8GbR-",
    "year": 2018
  }],
  "id": "SP:ad6309d1ea001098189425f54d069ef12abcb583",
  "authors": [{
    "name": "Lechao Xiao",
    "affiliations": []
  }, {
    "name": "Yasaman Bahri",
    "affiliations": []
  }, {
    "name": "Jascha Sohl-Dickstein",
    "affiliations": []
  }, {
    "name": "Samuel S. Schoenholz",
    "affiliations": []
  }, {
    "name": "Jeffrey Pennington",
    "affiliations": []
  }],
  "abstractText": "In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.",
  "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks"
}