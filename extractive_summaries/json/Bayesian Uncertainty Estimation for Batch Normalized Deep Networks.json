{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep learning has dramatically advanced the state of the art in a number of domains. Despite their unprecedented discriminative power, deep networks are prone to make mistakes. Nevertheless, they can already be found in settings where errors carry serious repercussions such as autonomous vehicles (Chen et al., 2016) and high frequency trading. We can soon expect automated systems to screen for various types of cancer (Esteva et al., 2017; Shen, 2017) and diagnose biopsies (Djuric et al., 2017). As autonomous systems based on deep learning are increasingly deployed in settings with the potential to cause physical or economic harm, we need to develop a better understanding of when we can be confident in the estimates produced by deep networks, and when we should be less certain.\nStandard deep learning techniques used for supervised learning lack methods to account for uncertainty in the model. This can be problematic when the network encounters conditions it was not exposed to during training,\n* Co-first authorship 1School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden 2Current address: Electronic Arts, SEED, Stockholm, Sweden. This work was carried out at Budbee AB. 3Science for Life Laboratory. Correspondence to: Kevin Smith <ksmith@kth.se>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nor if the network is confronted with adversarial examples (Goodfellow et al., 2014). When exposed to data outside the distribution it was trained on, the network is forced to extrapolate, which can lead to unpredictable behavior.\nIf the network can provide information about its uncertainty in addition to its point estimate, disaster may be avoided. In this work, we focus on estimating such predictive uncertainties in deep networks (Figure 1).\nThe Bayesian approach provides a theoretical framework for modeling uncertainty (Ghahramani, 2015), which has prompted several attempts to extend neural networks (NN) into a Bayesian setting. Most notably, Bayesian neural networks (BNNs) have been studied since the 1990’s (Neal, 2012), but do not scale well and struggle to compete with modern deep learning architectures. Recently, (Gal & Ghahramani, 2015) developed a practical solution to obtain uncertainty estimates by casting dropout training in conventional deep networks as a Bayesian approximation of a Gaussian Process (its correspondence to a general approximate Bayesian model was shown in (Gal, 2016)). They showed that any network trained with dropout is an approximate Bayesian model, and uncertainty estimates can be obtained by computing the variance on multiple predictions with different dropout masks.\nThe inference in this technique, called Monte Carlo Dropout (MCDO), has an attractive quality: it can be applied to any pre-trained networks with dropout layers. Uncertainty estimates come (nearly) for free. However, not all architectures use dropout, and most modern networks have adopted other regularization techniques. Batch normalization (BN), in particular, has become widespread thanks to its ability to stabilize learning with improved generalization (Ioffe & Szegedy, 2015).\nAn interesting aspect of BN is that the mini-batch statistics used for training each iteration depend on randomly selected batch members. We exploit this stochasticity and show that training using batch normalization, like dropout, can be cast as an approximate Bayesian inference. We demonstrate how this finding allows us to make meaningful estimates of the model uncertainty in a technique we call Monte Carlo Batch Normalization (MCBN) (Figure 1). The method we propose can be applied to any network using standard batch normalization.\nWe validate our approach by empirical experiments on a variety of datasets and tasks, including regression and image classification. We measure uncertainty quality relative to a baseline of fixed uncertainty, and show that MCBN outperforms the baseline on nearly all datasets with strong statistical significance. We also show that the uncertainty quality of MCBN is on par with other recent approximate Bayesian networks."
  }, {
    "heading": "2. Related Work",
    "text": "Bayesian models provide a natural framework for modeling uncertainty, and several approaches have been developed to adapt NNs to Bayesian reasoning. A common approach is to place a prior distribution (often a Gaussian) over each parameter. The resulting model corresponds to a Gaussian process for infinite parameters (Neal, 1995), and a Bayesian NN (MacKay, 1992) for a finite number of parameters. Inference in BNNs is difficult however (Gal, 2016), so focus has thus shifted to techniques that approximate the posterior, approximate BNNs. Methods based on variational inference (VI) typically rely on a fully factorized approximate distribution (Kingma & Welling, 2014; Hinton & Van Camp, 1993), but often do not scale. To alleviate these difficulties, (Graves, 2011) proposed a model using sampling methods to estimate a factorized posterior. Probabilistic backpropagation (PBP), estimates a factorized posterior via expectation propagation (HernándezLobato & Adams, 2015).\nUsing several strategies to address scaling issues, Deep\nGaussian Processes show superior performance in terms of RMSE and uncertainty quality compared to state-of-the-art approximate BNNs (Bui et al., 2016)1. Another recent approach to Bayesian learning, Bayesian hypernetworks, use a NN to learn a distribution of parameters over another network (Krueger et al., 2017). Multiplicative Normalizing Flows for variational Bayesian networks (MNF) (Louizos & Welling, 2017) is a recent model that formulates a posterior dependent on auxiliary variables. MNF achieves a highly flexible posterior by the application of normalizing flows to the auxiliary variables.\nAlthough these recent techniques address some of the difficulties with approximate BNNs, they all require modifications to the architecture or the way networks are trained, as well as specialized knowledge from practitioners. Recently, (Gal & Ghahramani, 2015) showed that a network trained with dropout implicitly performs the VI objective. Therefore any network trained with dropout can be treated as an approximate Bayesian model by making multiple predictions through the network while sampling different dropout masks for each prediction. The mean and variance of the predictions are used in the estimation of the mean and variance of the predictive distribution 2."
  }, {
    "heading": "3. Method",
    "text": "In the following, we introduce Bayesian models and a variational approximation using Kullback-Leibler (KL) divergence following (Gal, 2016). We continue by showing that a batch normalized deep network can be seen as an approximate Bayesian model. Employing theoretical insights and empirical analysis, we study the induced prior on the parameters when using batch normalization. Finally, we describe the procedure for estimating the uncertainty of a batch normalized network’s output.3"
  }, {
    "heading": "3.1. Bayesian Modeling",
    "text": "We assume a finite training set D = {(xi,yi)}i=1:N where each (xi,yi) is a sample-label pair. Using D, we are interested in learning an inference function fω(x,y) with parameters ω. In deterministic models, the estimated label ŷ is obtained as follows:\nŷ = arg max y fω(x,y)\nIn probabilistic models we let fω(x,y) = p(y|x,ω). In Bayesian modeling, in contrast to finding a point estimate\n1By uncertainty quality, we refer to predictive probability distributions as measured by PLL and CRPS.\n2This technique is referred to as “MC Dropout” in the original work, though we refer to it here as MCDO.\n3While the method applies to FC or Conv layers, the induced prior from weight decay (Section 3.3) is studied for FC layers.\nof the model parameters, the idea is to estimate an (approximate) posterior distribution of the model parameters p(ω|D) to be used for probabilistic prediction:\np(y|x,D) = ∫ fω(x,y)p(ω|D)dω\nThe predicted label, ŷ, can then be accordingly obtained by sampling p(y|x,D) or taking its maxima.\nVariational Approximation In approximate Bayesian modeling, a common approach is to learn a parameterized approximating distribution qθ(ω) that minimizes KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of the true posterior w.r.t. its approximation. Minimizing this KL divergence is equivalent to the following minimization while being free of the data term p(D) 4:\nLVA(θ) :=− N∑ i=1 ∫ qθ(ω) ln fω(xi,yi)dω\n+ KL(qθ(ω)||p(ω))\nDuring optimization, we want to take the derivative of the expected likelihood w.r.t. the learnable parameters θ. We use the same MC estimate as in (Gal, 2016) (explained in Appendix Section 1.1), such that one realized ω̂i is taken for each sample i 5. Optimizing over mini-batches of size M , the approximated objective becomes:\nL̂VA(θ) := − N\nM M∑ i=1 ln fω̂i(xi,yi) + KL(qθ(ω)||p(ω)) (1)\nThe first term is the data likelihood and the second term is the divergence of the prior w.r.t. the approximated posterior."
  }, {
    "heading": "3.2. Batch Normalized Deep Nets as Bayesian Modeling",
    "text": "We now describe the optimization procedure of a deep network with batch normalization and draw the resemblance to the approximate Bayesian modeling in Eq (1).\nThe inference function of a feed-forward deep network with L layers can be described as:\nfω(x) = W La(WL−1...a(W2a(W1x))\n4Achieved by constructing the Evidence Lower Bound, called ELBO, and assuming i.i.d. observation noise; details can be found in Appendix Section 1.1.\n5While a MC integration using a single sample is a weak approximation, in an iterative optimization for θ several samples will be taken over time.\nwhere a(.) is an element-wise nonlinearity function and Wl is the weight vector at layer l. Furthermore, we denote the input to layer l as xl with x1 = x and we then set hl = Wlxl. Parenthesized super-index for matrices (e.g. W(j)) and vectors (e.g. x(j)) indicates jth row and element respectively. Super-index u refers to a specific unit at layer l, (e.g. Wu = Wl,(j), hu = hl,(j)). 6\nBatch Normalization Each layer of a deep network is constructed by several linear units whose parameters are the rows of the weight matrix W. Batch normalization is a unit-wise operation proposed in (Ioffe & Szegedy, 2015) to standardize the distribution of each unit’s input. For FC layers, it converts a unit’s input hu in the following way:\nĥu = hu − E[hu]√\nVar[hu]\nwhere the expectations are computed over the training set during evaluation, and mini-batch during training (in deep networks, the weight matrices are often optimized using back-propagated errors calculated on mini-batches of data)7. Therefore, during training, the estimated mean and variance on the mini-batch B is used, which we denote by µB and σB respectively. This makes the inference at training time for a sample x a stochastic process, varying based on other samples in the mini-batch.\nLoss Function and Optimization Training deep networks with mini-batch optimization involves a (regularized) risk minimization with the following form:\nLRR(ω) := 1\nM M∑ i=1 l(ŷi,yi) + Ω(ω)\nwhere the first term is the empirical loss on the training data and the second term is a regularization penalty acting as a prior on model parameters ω. If the loss l is cross-entropy for classification or sum-of-squares for regression problems (assuming i.i.d. Gaussian noise on labels), the first term is equivalent to minimizing the negative log-likelihood:\nLRR(ω) := − 1\nMτ M∑ i=1 ln fω(xi,yi) + Ω(ω)\n6For a (softmax) classification network, fω(x) is a vector with fω(x,y) = fω(x)\n(y), for regression networks with i.i.d. Gaussian noise we have fω(x,y) = N (fω(x), τ−1I).\n7It also learns an affine transformation for each unit with parameters γ and β, omitted for brevity: x̂(j)affine = γ (j)x̂(j) + β(j).\nwith τ = 1 for classification. In a network with batch normalization, the model parameters include {W1:L,γ1:L,β1:L,µ1:LB ,σ1:LB }. If we decouple the learnable parameters θ = {W1:L,γ1:L,β1:L} from the stochastic parameters ω = {µ1:LB ,σ1:LB }, we get the following objective at each step of the mini-batch optimization:\nLRR(θ) := − 1\nMτ M∑ i=1 ln f{θ,ω̂i}(xi,yi) + Ω(θ) (2)\nwhere ω̂i is the means and variances for sample i’s minibatch at a certain training step. Note that while ω̂i formally needs to be i.i.d. for each training example, a batch normalized network samples the stochastic parameters once per training step (mini-batch). For a large number of epochs, however, the distribution of sampled batch members for a given training example converges to the i.i.d. case.\nIn a batch normalized network, qθ(ω) corresponds to the joint distribution of the weights, induced by the randomness of the normalization parameters µ1:LB ,σ 1:L B , as implied by the repeated sampling from D during training. This is an approximation of the true posterior, where we have restricted the posterior to lie within the domain of our parametric network and source of randomness. With that, we can estimate the uncertainty of predictions from a trained batch normalized network using the inherent stochasticity of BN (Section 3.4)."
  }, {
    "heading": "3.3. Prior p(ω)",
    "text": "Equivalence between the VA and BN training procedures requires ∂∂θ of Eq. (1) and Eq. (2) to be equivalent up to a scaling factor. This is the case if ∂∂θKL(qθ(ω)||p(ω)) = Nτ ∂∂θΩ(θ).\nTo reconcile this condition, one option is to let the prior p(ω) imply the regularization term Ω(θ). Eq. (1) reveals that the contribution of KL(qθ(ω)||p(ω)) to the optimization objective is inversely scaled with N . For BN, this corresponds to a model with a small Ω(θ) when N is large. In the limit as N →∞, the optimization objectives of Eq. (1) and Eq. (2) become identical with no regularization.8\nAnother option is to let some Ω(θ) imply p(ω). In Appendix Section 1.4 we explore this with L2-regularization, also called weight decay (Ω(θ) = λ ∑ l=1:L ||W l||2). We find that unlike in MCDO (Gal, 2016), some simplifying\n8To prove the existence and find an expression of KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we find that BN approximately induces Gaussian distributions over BN units’ means and standard deviations, centered around the population values given by D. We assume a factorized distribution and Gaussian priors, and find the corresponding KL(qθ(ω)||p(ω)) components in Appendix Section 1.4 Eq. (7). These could be used to construct a custom Ω(θ) for any Gaussian choice of p(ω).\nassumptions are necessary to reconcile the VA and BN objectives with weight decay: no scale and shift applied to BN layers, uncorrelated units in each layer, BN applied on all layers, and large N and M . Given these conditions:\np(µuB) = N (µµ,p, σµ,p) p(σuB) = N (µσ,p, σσ,p)\nwhere µµ,p = 0, σµ,p →∞, µσ,p = 0 and σσ,p → 12Nτλl .\nThis corresponds to a wide and narrow distribution on BN units’ means and std. devs respectively, where N accounts for the narrowness of the prior. Due to its popularity in deep learning, our experiments in Section 4 are performed with weight decay."
  }, {
    "heading": "3.4. Predictive Uncertainty in Batch Normalized Deep Nets",
    "text": "In the absence of the true posterior, we rely on the approximate posterior to express an approximate predictive distribution:\np∗(y|x,D) := ∫ fω(x,y)qθ(ω)dω\nFollowing (Gal, 2016) we estimate the first (for regression and classification) and second (for regression) moments of the predictive distribution empirically (see Appendix Section 1.5 for details):\nEp∗ [y] ≈ 1\nT T∑ i=1 fω̂i(x)\nCovp∗ [y] ≈ τ−1I + 1\nT T∑ i=1 fω̂i(x) ᵀfω̂i(x)\n− Ep∗ [y]ᵀEp∗ [y]\nwhere each ω̂i corresponds to sampling the net’s stochastic parameters ω = {µ1:LB ,σ1:LB } the same way as during training. Sampling ω̂i therefore involves sampling a batch B from the training set and updating the parameters in the BN units, just as if we were taking a training step with B. From a VA perspective, training the network amounted to minimizing KL(qθ(ω)||p(ω|D)) wrt θ. Sampling ω̂i from the training set, and keeping the size of B consistent with the mini-batch size used during training, ensures that qθ(ω) during inference remains identical to the approximate posterior optimized during training.\nThe network is trained just as a regular BN network, but instead of replacing ω = {µ1:LB ,σ1:LB } with population values from D for inference, we update these parameters stochastically, once for each forward pass.9 Pseudocode for estimating predictive mean and variance is given in Algorithm 1.\n9As an alternative to using the training set D to sample ω̂i,\nAlgorithm 1 MCBN Algorithm Input: sample x, number of inferences T , batchsize b Output: mean prediction ŷ, predictive uncertainty σ2\n1: y = {} 2: loop for T iterations 3: B ∼ D // mini batch 4: ω̂ = {µB ,σB} // mini batch mean and variance 5: y = y ∪ fω̂(x) 6: end loop 7: ŷ = E[y] 8: σ2 = Cov[y] + τ−1I // for regression"
  }, {
    "heading": "4. Experiments and Results",
    "text": "We assess the uncertainty quality of MCBN quantitatively and qualitatively. Our quantitative analysis relies on CIFAR10 for image classification and eight standard regression datasets, listed in Appendix Table 1. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani, 1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hernández-Lobato & Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)). We report results using standard metrics, and also propose useful upper and lower bounds to normalize these metrics for an easier interpretation in Section 4.2.\nOur qualitative results include the toy dataset in Figure 1 in the style of (Karpathy, 2015), a new visualization of uncertainty quality that plots test errors sorted by predicted variance (Figure 2 and Appendix), and image segmentation results (Figure 2 and Appendix)."
  }, {
    "heading": "4.1. Metrics",
    "text": "We evaluate uncertainty quality based on two standard metrics, described below: Predictive Log Likelihood (PLL) and Continuous Ranked Probability Score (CRPS). To improve the interpretability of the metrics, we propose to normalize them by upper and lower bounds.\nPredictive Log Likelihood (PLL) Predictive Log Likelihood is widely accepted as the main uncertainty quality metric for regression (Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017). A key property of PLL is that it makes no assumptions about the form of the distribution. The measure is defined for a probabilistic model fω(x) and a single observation\nwe could sample from the implied qθ(ω) as modeled in the Appendix. This would alleviate having to store D for use during prediction. In our experiments we used D to sample ω̂i however, and leave the evaluation of the modeled qθ(ω) for future research.\n(yi,xi) as:\nPLL(fω(x), (yi,xi)) = log p(yi|fω(xi))\nwhere p(yi|fω(xi)) is the model’s predicted PDF evaluated at yi, given the input xi. A more detailed description is given in the Appendix Section 1.5. The metric is unbounded and maximized by a perfect prediction (mode at yi) with no variance. As the predictive mode moves away from yi, increasing the variance tends to increase PLL (by maximizing probability mass at yi). While PLL is an elegant measure, it has been criticized for allowing outliers to have an overly negative effect on the score (Selten, 1998).\nContinuous Ranked Probability Score (CRPS) Continuous Ranked Probability Score is a measure that takes the full predicted PDF into account with less sensitivity to outliers. A prediction with low variance that is slightly offset from the true observation will receive a higher score form CRPS than PLL. In order for CRPS to be analytically tractable, we need to assume a Gaussian unimodal predictive distribution. CRPS is defined as\nCRPS(fω(xi), (yi, xi)) = ∫ ∞ −∞ ( F (y)− 1(y ≥ yi) )2 dy\nwhere F (y) is the predictive CDF, and 1(y ≥ yi) = 1 if y ≥ yi and 0 otherwise (for univariate distributions) (Gneiting & Raftery, 2007). CRPS is interpreted as the sum of the squared area between the CDF and 0 where y < yi and between the CDF and 1 where y ≥ yi. A perfect prediction with no variance yields a CRPS of 0; for all other cases the value is larger. CRPS has no upper bound."
  }, {
    "heading": "4.2. Benchmark models and normalized metrics",
    "text": "It is difficult to interpret the quality of uncertainty from raw PLL and CRPS values. We propose to normalize the metrics between useful lower and upper bounds. The normalized measures estimate the performance of an uncertainty model between the trivial solution (constant uncertainty) and optimal uncertainty for each prediction. For the lower bound, we define a baseline that predicts constant variance regardless of input. The variance is set to a fixed value that optimizes CRPS on validation data. We call this model Constant Uncertainty BN (CUBN). It reflects our best guess of constant variance on test data – thus, any improvement in uncertainty quality over CUBN indicates a sensible estimate of uncertainty. We similarly define a baseline for dropout, Constant Uncertainty Dropout (CUDO). The modeling of variance (uncertainty) by MCBN and CUBN are visualized in Figure 1.\nAn upper bound on uncertainty performance can also be defined for a probabilistic model f with respect to CRPS or PLL. For each observation (yi, xi), a value\nfor the predictive variance Ti can be chosen that maximizes PLL or minimizes CRPS10. Using CUBN as a lower bound and the optimized CRPS score as the upper bound, uncertainty estimates can be normalized between these bounds (1 indicating optimal performance, and 0 indicating same performance as fixed uncertainty). We call this normalized measure CRPS =\nCRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))×100."
  }, {
    "heading": "4.3. Test setup",
    "text": "Our evaluation compares MCBN to MCDO (Gal & Ghahramani, 2015) and MNF (Louizos & Welling, 2017) using the datasets and metrics described above. Our setup is similar to (Hernández-Lobato & Adams, 2015), which was also followed by (Gal & Ghahramani, 2015). However, our comparison implements a different hyperparameter selection, allows for a larger range of dropout rates, and uses larger networks with two hidden layers.\nFor the regression task, all models share a similar architecture: two hidden layers with 50 units each, and ReLU activations, with the exception of Protein Tertiary Structure dataset (100 units per hidden layer). Inputs and outputs were normalized during training. Results were averaged over five random splits of 20% test and 80% training and cross-validation (CV) data. For each split, 5-fold CV by grid search with a RMSE minimization objective was used to find training hyperparameters and optimal n.o. epochs, out of a maximum of 2000. For BN-based models, the hyperparameter grid consisted of a weight decay factor ranging from 0.1 to 1−15 by a log 10 scale, and a batch size range from 32 to 1024 by a log 2 scale. For DO-based models, the hyperparameter grid consisted of the same weight decay range, and dropout probabilities in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models used a batch size of 32 in all evaluations. For MNF11, the n.o. epochs was optimized, the batch size was set to 100, and early stopping test performed each epoch (compared to every 20th for MCBN, MCDO).\nFor MCBN and MCDO, the model with optimal training hyperparameters was used to optimize τ numerically. This optimization was made in terms of average CV CRPS for MCBN, CUBN, MCDO, and CUDO respectively.\nEstimates for the predictive distribution were obtained by taking T = 500 stochastic forward passes through the network. For each split, test set evaluation was done 5 times with different seeds. Implementation was done in TensorFlow with the Adam optimizer and a learning rate of 0.001.\n10Ti can be found analytically for PLL, but must be found numerically for CRPS.\n11Where we used an adapted version of the authors’ code.\nFor the image classification test we use CIFAR10 (Krizhevsky & Hinton, 2009) which includes 10 object classes with 5,000 and 1,000 images in the training and test sets, respectively. Images are 32x32 RGB format. We trained a ResNet32 architecture with a batch size of 32, learning rate of 0.1, weight decay of 0.0002, leaky ReLU slope of 0.1, and 5 residual units. SGD with momentum was used as the optimizer.\nCode for reproducing our experiments is available at https://github.com/icml-mcbn/mcbn."
  }, {
    "heading": "4.4. Test results",
    "text": "The regression experiment comparing uncertainty quality is summarized in Table 1. We report CRPS and PLL, expressed as a percentage, which reflects how close the model is to the upper bound, and check to see if the model significantly exceeds the lower bound using a one sample t-test (significance level is indicated by *’s). Further details are provided in Appendix Section 1.7.\nIn Figure 2 (left), we present a novel visualization of uncertainty quality for regression problems. Data are sorted by estimated uncertainty in the x-axis. Grey dots show the errors in model predictions, and the shaded areas show the model uncertainty. A running mean of the errors appears as a gray line. If uncertainty estimation is working well, a correlation should exist between the mean error (gray line) and uncertainty (shaded area). This indicates that the uncertainty estimation recognizes samples with larger (or smaller) potential for predictive errors.\nWe applied MCBN on the image classification task of CIFAR10. The baseline in this case is the softmax distribution using the moving average for BN units. Log likelihood (PLL) is the metric used to compare with the baseline. The baseline achieves a PLL of -0.32 on the test set, while MCBN obtains a PLL of -0.28. Table 2 shows the performance of MCBN when using different number of stochastic forward passes (the MCBN batchsize is fixed to the training batch size at 32). PLL improves as the number of the stochastic passes increases, until it is significantly better than the softmax baseline.\nTo demonstrate how model uncertainty can be obtained from an existing network with minimal effort, we applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC models in (Kendall et al., 2015). We simply ran multiple forward passes with different mini-batches randomly taken from the train set. The models obtained from the online model zoo have BN blocks after each layer. We recalculate mean and variance for the first 2 blocks only and use the training statistics for the rest of the blocks. Mini-batches of size 10 and 36 were used for CamVid and VOC respectively\ndue to memory limits. The results in Figure 2 (right) were obtained from 20 stochastic forward passes, showing high uncertainty near object boundaries. The VOC results are more appealing because of larger mini-batches.\nWe provide additional experimental results in the Appendix. Appendix Tables 2 and 3 show the mean CRPS and PLL values from the regression experiment. Table 4 provides the raw CRPS and PLL scores. In Table 5 we provide RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the predictive accuracy compared to their nonBayesian counterparts. In Tables 6 and 7, we investigate the effect of varying batch size while keeping other hyperparameters fixed. We see that performance deteriorates with small batch sizes (≤16), a known issue of BN (Ioffe, 2017). Similarly, results varying the number of stochastic forward passes T is reported in Tables 8 and 9. While performance benefits from large T , in some cases T = 50 (i.e. 1/10 of T in the main evaluation) performs well. Uncertainty-error plots for all the datasets are provided in the Appendix."
  }, {
    "heading": "5. Discussion",
    "text": "The results presented in Tables 1-2 and Appendix Tables 2-9 indicate that MCBN generates meaningful uncertainty\nestimates that correlate with actual errors in the model’s prediction. In Table 1, we show statistically significant improvements over CUBN in the majority of the datasets, both in terms of CRPS and PLL. The visualizations in Figure 2 and in the Appendix Figures 2-3 show correlations between the estimated model uncertainty and errors of the network’s predictions. We perform the same experiments using MCDO and MNF, and find that MCBN generally performs on par with both methods. Looking closer, MCBN outperforms MCDO and MNF in more cases than not, measured by CRPS. However, care must be used. The learned parameters are different, leading to different predictive means and confounding direct comparison.\nThe results on the Yacht Hydrodynamics dataset seem contradictory. The CRPS score for MCBN are extremely negative, while the PLL score is extremely positive. The opposite trend is observed for MCDO. To add to the puzzle, the visualization in Figure 2 depicts an extremely promising uncertainty estimation that models the predictive errors with high fidelity. We hypothesize that this strange behavior is due to the small size of the data set, which only contains 60 test samples, or due to the Gaussian assumption of CRPS. There is also a large variability in the model’s accuracy on this dataset, which further confounds the measurements for such limited data.\nOne might criticize the overall quality of uncertainty estimates observed in all the models we tested, due to the magnitude of CRPS and PLL in Table 1. The scores rarely exceed 10% improvement over the lower bound. However, we caution that these measures should be taken in context. The upper bound is very difficult to achieve in practice – it is optimized for each test sample individually – and the lower bound is a quite reasonable estimate.\nThe study of MCBN sensitivity to batch size revealed that a certain batch size is required for the best performance, dependent on the data. When doing inference on a GPU, large\nbatch sizes may cause memory issues for cases where the input is large and the network has a large number of parameters, as is common for state-of-the-art image classification networks. However, there are various workarounds to this problem. One can store BN statistics, instead of batches, to reduce memory issues. Furthermore, we can use the Gaussian estimate of the BN statistics as discussed previously, which makes memory and computation extremely efficient."
  }, {
    "heading": "6. Conclusion",
    "text": "In this work, we have shown that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We show evidence that the uncertainty estimates from MCBN correlate with actual errors in the model’s prediction, and are useful for practical\ntasks such as regression, image classification, and image segmentation. Our experiments show that MCBN yields a significant improvement over the optimized constant uncertainty baseline, on par with MCDO and MNF. Our evaluation also suggests new normalized metrics based on useful upper and lower bounds, and a new visualization which provides an intuitive explanation of uncertainty quality.\nFinally, it should be noted that over the past few years, batch normalization has become an integral part of most – if not all – cutting edge deep networks. We have shown that it is possible to obtain meaningful uncertainty estimates from existing models without modifying the network or the training procedure. With a few lines of code, robust uncertainty estimates can be obtained by computing the variance of multiple stochastic forward passes through an existing network."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation",
    "authors": ["T.D. Bui", "D. Hernández-Lobato", "Y. Li", "J.M. HernándezLobato", "R.E. Turner"],
    "year": 2016
  }, {
    "title": "Monocular 3d object detection for autonomous driving",
    "authors": ["X. Chen", "K. Kundu", "Z. Zhang", "H. Ma", "S. Fidler", "R. Urtasun"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Precision histology: how deep learning is poised to revitalize histomorphology for personalized cancer",
    "authors": ["U. Djuric", "G. Zadeh", "K. Aldape", "P. Diamandis"],
    "venue": "care. npj Precision Oncology,",
    "year": 2017
  }, {
    "title": "Dermatologist-level classification of skin cancer",
    "authors": ["A. Esteva", "B. Kuprel", "R.A. Novoa", "J. Ko", "S.M. Swetter", "H.M. Blau", "S. Thrun"],
    "year": 2017
  }, {
    "title": "Uncertainty in Deep Learning",
    "authors": ["Y. Gal"],
    "venue": "PhD thesis, University of Cambridge,",
    "year": 2016
  }, {
    "title": "Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning",
    "authors": ["Y. Gal", "Z. Ghahramani"],
    "year": 2015
  }, {
    "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
    "authors": ["T. Gneiting", "A.E. Raftery"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2007
  }, {
    "title": "Explaining and harnessing adversarial examples",
    "authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"],
    "venue": "arXiv preprint arXiv:1412.6572,",
    "year": 2014
  }, {
    "title": "Practical Variational Inference for Neural Networks",
    "authors": ["A. Graves"],
    "year": 2011
  }, {
    "title": "Probabilistic backpropagation for scalable learning of bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R. Adams"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Keeping the neural networks simple by minimizing the description length of the weights",
    "authors": ["G.E. Hinton", "D. Van Camp"],
    "venue": "In Proceedings of the sixth annual conference on Computational learning theory,",
    "year": 1993
  }, {
    "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
    "authors": ["S. Ioffe"],
    "venue": "CoRR, abs/1702.03275,",
    "year": 2017
  }, {
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "Arxiv,",
    "year": 2015
  }, {
    "title": "Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.edu/people/ karpathy/convnetjs/demo/regression",
    "authors": ["A. Karpathy"],
    "year": 2015
  }, {
    "title": "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding",
    "authors": ["A. Kendall", "V. Badrinarayanan", "R. Cipolla"],
    "venue": "CoRR, abs/1511.0,",
    "year": 2015
  }, {
    "title": "Auto-Encoding Variational Bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "year": 2009
  }, {
    "title": "Dropout Inference in Bayesian Neural Networks with Alpha-divergences",
    "authors": ["Y. Li", "Y. Gal"],
    "year": 2017
  }, {
    "title": "A practical bayesian framework for backpropagation networks",
    "authors": ["D.J. MacKay"],
    "venue": "Neural computation,",
    "year": 1992
  }, {
    "title": "Bayesian Learning for Neural Networks",
    "authors": ["R.M. Neal"],
    "venue": "PhD thesis, University of Toronto,",
    "year": 1995
  }, {
    "title": "Bayesian learning for neural networks, volume 118",
    "authors": ["R.M. Neal"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Axiomatic characterization of the quadratic scoring rule",
    "authors": ["R. Selten"],
    "venue": "Experimental Economics,",
    "year": 1998
  }, {
    "title": "End-to-end training for whole image breast cancer diagnosis using an all convolutional design",
    "authors": ["L. Shen"],
    "venue": "arXiv preprint arXiv:1708.09427,",
    "year": 2017
  }],
  "id": "SP:e6af04307958d82e2512a79f82ca463ced259ca2",
  "authors": [{
    "name": "Mattias Teye",
    "affiliations": []
  }, {
    "name": "Hossein Azizpour",
    "affiliations": []
  }, {
    "name": "Kevin Smith",
    "affiliations": []
  }],
  "abstractText": "We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.",
  "title": "Bayesian Uncertainty Estimation for Batch Normalized Deep Networks"
}