{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis. It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006;\n∗Corresponding author. This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).\nYoshida et al., 2014). Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016). In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014). This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs.\nMost previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015). Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014). However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance. Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014). They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and\n2263\nZhao, 2016), also including discourse parsing. Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics. Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance. Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving.\nThe concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives. Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments. The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments. To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015). Our method will be evaluated on the benchmark dataset against state-of-the-art methods.\nThe rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4 concludes this paper."
  }, {
    "heading": "2 Method",
    "text": "The architecture of the model, as shown in Figure 1, is straightforward. It can be divided into three parts: 1) CNN for modeling arguments; 2) CGNN unit for feature transformation; 3) a conventional softmax layer for the final classification. CNN is used to obtain the vector representations for the sentences, CGNN further captures and transforms the features for the final classification."
  }, {
    "heading": "2.1 Convolutional Neural Network",
    "text": "As CNN has been broadly adopted for modeling sentences, we will explain it in brevity. For two arguments, typical sentence modeling process\nwill be applied: sentence embedding (including embeddings for words and part-of-speech (POS) tags) through projection layer, convolution operations (with multiple groups of filters) through the convolution layer, obtaining the sentence representation through one-max-pooling. The two arguments will get their sentence vectors independently without any interfering, and the convolution operation will be the same by sharing parameters. The final argument-pair representation will be the vector v which is concatenated from two sentence vectors and this vector will be used as the input of the CGNN unit."
  }, {
    "heading": "2.2 Collaborative Gated Neural Network",
    "text": "For implicit sense classification, the key is how to effectively capture the interactions between the two arguments. The interactions could be word pairs, phrase pairs or even the latent meaning of the two full arguments. Pitler et al. (2009) has shown that word pair features are helpful. To model these interactions, we have to make a full use of the sentence vectors obtained from CNN. However, common neural hidden layers might be insufficient to deal with the challenge. We need to seek more powerful neural models, i.e., gated neural network.\nIn recent years, gated mechanism has gained popularity in neural models. Although it is first introduced in the cells of recurrent neural networks, like Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014), traditional feed-forward neural models such as the Highway Network could also benefit from it (Srivastava et al., 2015). The existing studies show that the gated mechanism in highway network serves not only a means for easier training, but also a tool to route information in a trained network.\nMotivated by the idea of highway network, we propose a collaborative gated neural network (CGNN) for this task. The architecture of CGNN is illustrated in Figure 1, and it contains a sequence of transformations. First, the inner-cell ĉ is obtained through linear transformation and non-linear activation on the input v, and this process is exactly the operation of an ordinary neural layer.\nĉ = tanh(Wc · v + bc)\nMeanwhile, the two gates gi and go are calculated independently because they are only influenced by\nthe original input through different parameters:\ngi = σ(W i · v + bi) go = σ(W o · v + bo)\nwhere the σ denotes sigmoid function which guarantees the values in the gates are in [0,1]. Two gated operations are applied sequentially, where a gated operation indicates the element-wise multiplication of an inner-cell and a gate. Between the two gated operations, a non-linear activation operation is applied. The procedure could be formulated as follows:\nc = ĉ gi h = tanh(c) go\nwhere denotes element-wise multiplication, c is the second inner-cell and h is the output of CGNN unit.\nAlthough the two gates are generated independently, they will work collaboratively because they control the information flow of the inner-cells sequentially which resembles logical AND operation in a probabilistic version. In fact, the transformations after ĉ will concern only element-wise operations which might give finer controls for each dimension, and the information can only flow on the dimensions where both gates are “open”. This procedure will help select the most crucial features.\nThe gates in this model are mainly used for routing information from sentence-pairs vectors. When there is only one gate in our network, the model works similar to the highway network (Srivastava et al., 2015)."
  }, {
    "heading": "2.3 Output and Training",
    "text": "After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification.\nThe training object J will be the cross-entropy error E with L2 regularization:\nE(ŷ, y) = − l∑\nj\nyj × log(Pr(ŷj))\nJ(θ) = 1\nm\nm∑\nk\nE(ŷ(k), y(k)) + λ\n2 ‖θ‖2\nwhere yj is the gold label and ŷj is the predicted one. We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Setting",
    "text": "As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation. In the PDTB, each discourse relation is annotated between two argument spans.\nTo be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: COMPARISON (COMP.), CONTINGENCY (CONT.), EXPANSION (EXP.) and TEMPORAL (TEMP.). While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009). Following dataset splitting convention of the previous works, we use sections 2-20 for training, sections 21-22 for testing and sections 0-1 for development set. The proposed model is possible to be extended for multi-class classification of discourse parsing, but for the comparisons with most of previous works, we will follow them and focus on the binary classification problems.\nFor other hyper-parameters of the model and training process, we fix the lengths of both the input arguments to be 80, and apply truncating or zero-padding when necessary. The dimensions for word embeddings and POS embeddings are respectively 300 and 50, and the embedding layer adopts a dropout of 0.2. The word embeddings are initialized with pre-trained word vectors using word2vec 2 (Mikolov et al., 2013) and other parameters are randomly initialized including POS embeddings. We\n1http://www.seas.upenn.edu/˜pdtb/ 2http://www.code.google.com/p/word2vec\nset the starting learning rate to 0.001. For CNN model, we utilize three groups of filters with window widths of (2, 2, 2) and their filter numbers are all set to 1024. The hyper-parameters are the same for all models and we do not tune them individually."
  }, {
    "heading": "3.2 Model Analysis",
    "text": "For transformation of sentence vectors, a simple Multilayer Perceptron (MLP) layer could be a straightforward choice, while more complex neural modules, such as LSTM and highway network, could also be considered. Our model utilizes a CGNN unit with refined gated mechanism for the transformation. Will the proposed CGNN really bring about further performance improvement? We now answer this question empirically.\nAs shown in Table 1, CNN model usually performs well on its own. Utilizing an MLP layer or a Highway layer could improve the accuracies on CONTINGENCY, EXPANSION, TEMPORARY except for COMPARISON. Though the primary motivation of Highway is to ease gradient-based training of highly deep networks through utilizing gated units, it works merely as an ordinary MLP in the proposed model, which explains the reason that it performs like MLP. Despite one of four classes, COMPARISON, not receiving performance improvement, introducing a non-linear transformation layer lets the classification benefit as a whole. “CNN+LSTM” denotes the method of using LSTM to read the convolution sequence (without pooling operation), and it even does not perform better than MLP.\nThe CGNN achieves the best performance on all classes including COMPARISON. It gains 3.97% imrovement on average F1 score using CNN only model. We assume that CGNN is well-suited to work with CNN, adaptively transforming and combining local features detected by the individual filters."
  }, {
    "heading": "3.3 Results",
    "text": "We show the main results in Tables 2 and 3. The metrics include precision (P), recall (R), accuracy (Acc) and F1 score. Since not all of these metrics are reported in previous work, the comparisons are correspondingly in Table 2 and 3. Some previous work merges Entrel with Expansion, which is also explored in our study and noted as EXP.+.\nWe compare with best-performed or competitive models including both traditional linear methods and recent neural methods. For traditional methods: Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features; Zhou et al. (2010) improve the performance through predicting connective words as features; Park and Cardie (2012) propose a locallyoptimal feature set and further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up; Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning; Rutherford and Xue (2014) employ Brown cluster pair and coreference patterns for performance enhancement. Several neural methods have also been included for comparison: Zhang et al. (2015) propose a simplified neural network which has only\nthree different pooling operations (max, min, average); Ji and Eisenstein (2015) compute distributed semantics representation by composition up the syntactic parse tree through recursive neural network; Braud and Denis (2015) consider shallow lexical features and word embeddings. Chen et al. (2016) replace the original words by word embeddings to overcome the data sparsity problem and they also utilize gated relevance network to capture the semantic interaction between word pairs. The gated network is different from ours but also works well.\nOur model achieves F-measure improvements of 1.85% on COMPARISON, 1.56% on CONTINGENCY, 1.27% on EXPANSION, 0.94% on EXPANSION+, 4.89% on TEMPORAL, against the state-ofthe-art of each class. We improve by 4.73% on average F1 score when not including ENTREL in EXPANSION as reported in Table 2 and 3.19% on average F1 score otherwise as reported in Table 3. The results show that our model achieves the best performance and especially makes the most remarkable progress on TEMPORAL."
  }, {
    "heading": "4 Conclusion",
    "text": "In this paper, we propose a stacking gated neural architecture for implicit discourse relation classification. Our model includes convolution and collaborative gated neural network. The analysis and experiments show that CNN performs well on its own and combining CGNN provides further gains. Our evaluation on PTDB shows that the proposed model outperforms previous state-of-the-art systems."
  }],
  "year": 2016,
  "references": [{
    "title": "Aggregated word pair features for implicit discourse relation disambiguation",
    "authors": ["Or Biran", "Kathleen McKeown."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 69–73, Sofia, Bulgaria, August.",
    "year": 2013
  }, {
    "title": "Comparing word representations for implicit discourse relation classification",
    "authors": ["Chloé Braud", "Pascal Denis."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2201–2211, Lisbon, Portugal,",
    "year": 2015
  }, {
    "title": "Neural Word Segmentation Learning for Chinese",
    "authors": ["Deng Cai", "Hai Zhao."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 409–420, Berlin, Germany, August.",
    "year": 2016
  }, {
    "title": "Shallow discourse parsing using constituent parsing tree",
    "authors": ["Changge Chen", "Peilu Wang", "Hai Zhao."],
    "venue": "Proceedings of the CoNLL-15 shared task, pages 37–41, Beijing, China, July.",
    "year": 2015
  }, {
    "title": "Implicit discourse relation detection via a deep architecture with gated relevance network",
    "authors": ["Jifan Chen", "Qi Zhang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1412.3555.",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Discourse complements lexical semantics for nonfactoid answer reranking",
    "authors": ["Peter Jansen", "Mihai Surdeanu", "Peter Clark."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 977–986, Baltimore, Mary-",
    "year": 2014
  }, {
    "title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations",
    "authors": ["Yangfeng Ji", "Jacob Eisenstein."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL), 3:329– 344.",
    "year": 2015
  }, {
    "title": "A Joint Graph Model for Pinyin-to-Chinese Conversion with Typo Correction",
    "authors": ["Zhongye Jia", "Hai Zhao."],
    "venue": "Proceedings of the 52nd Annual Meeting of",
    "year": 2014
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 655–665, Baltimore, Maryland,",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar, October.",
    "year": 2014
  }, {
    "title": "A constituent syntactic parse tree based discourse parser",
    "authors": ["Zhongyi Li", "Hai Zhao", "Chenxi Pang", "Lili Wang", "Huan Wang."],
    "venue": "Proceedings of the CoNLL16 shared task, pages 60–64, Berlin, Germany, August.",
    "year": 2016
  }, {
    "title": "A discourse-driven content model for summarising scientific articles evaluated in a complex question answering task",
    "authors": ["Maria Liakata", "Simon Dobnik", "Shyamasree Saha", "Colin Batchelor", "Dietrich Rebholz-Schuhmann."],
    "venue": "Proceedings of the 2013 Conference",
    "year": 2013
  }, {
    "title": "A pdtb-styled end-to-end discourse parser",
    "authors": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."],
    "venue": "Natural Language Engineering, 20(02):151–184.",
    "year": 2014
  }, {
    "title": "Using sense-labeled discourse connectives for statistical machine translation",
    "authors": ["Thomas Meyer", "Andrei Popescu-Belis."],
    "venue": "Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation (ESIRMT) and",
    "year": 2012
  }, {
    "title": "Implicitation of discourse connectives in (machine) translation",
    "authors": ["Thomas Meyer", "Bonnie Webber."],
    "venue": "Proceedings of the Workshop on Discourse in Machine Translation, pages 19–26, Sofia, Bulgaria, August.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems (NIPS), pages 3111–3119, South Lake Tahoe, Nevada,",
    "year": 2013
  }, {
    "title": "Incorporating speaker and discourse features into speech summarization",
    "authors": ["Gabriel Murray", "Steve Renals", "Jean Carletta", "Johanna Moore."],
    "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the",
    "year": 2006
  }, {
    "title": "The Penn Discourse TreeBank",
    "authors": ["nie L Webber"],
    "year": 2008
  }, {
    "title": "Highway networks",
    "authors": ["Schmidhuber."],
    "venue": "arXiv",
    "year": 2015
  }, {
    "title": "Neural network based bilin",
    "authors": ["Eiichiro Sumita"],
    "year": 2014
  }, {
    "title": "Learning distributed word representations for bidirectional LSTM recurrent neural network",
    "authors": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-",
    "year": 2016
  }, {
    "title": "Converting continuous-space language models into n-gram language models with efficient bilingual pruning for statistical machine translation",
    "authors": ["Rui Wang", "Masao Utiyama", "Isao Goto", "Eiichiro Sumita", "Hai Zhao", "Bao-Liang Lu."],
    "venue": "ACM Transactions on",
    "year": 2016
  }, {
    "title": "The CoNLL-2015 Shared Task on Shallow Discourse Parsing",
    "authors": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Rashmi Prasad", "Christopher Bryant", "Attapol Rutherford."],
    "venue": "Proceedings of the CoNLL-15 shared task, pages 1–16, Beijing, China, July.",
    "year": 2015
  }, {
    "title": "CoNLL 2016 Shared Task on Multilingual Shallow Discourse Parsing",
    "authors": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Attapol Rutherford", "Bonnie Webber", "Chuan Wang", "Hongmin Wang."],
    "venue": "Proceedings of the CoNLL-16 shared task, pages 1–19, Berlin, Ger-",
    "year": 2016
  }, {
    "title": "Dependency-based discourse parser for single-document summarization",
    "authors": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2014
  }, {
    "title": "Shallow convolutional neural network for implicit discourse relation recognition",
    "authors": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-",
    "year": 2015
  }, {
    "title": "Learning local word reorderings for hierarchical phrase-based statistical machine translation",
    "authors": ["Jingyi Zhang", "Masao Utiyama", "Eiichro Sumita", "Hai Zhao", "Graham Neubig", "Satoshi Nakamura."],
    "venue": "Machine Translation, pages 1–18.",
    "year": 2016
  }, {
    "title": "Probabilistic graph-based dependency parsing with convolutional neural network",
    "authors": ["Zhisong Zhang", "Hai Zhao", "Lianhui Qin."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1382–1392, Berlin,",
    "year": 2016
  }, {
    "title": "Predicting discourse con2269",
    "authors": ["Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan"],
    "year": 2010
  }],
  "id": "SP:cfd468bf8b138b1eed6b32ad262a1a794f9440b4",
  "authors": [{
    "name": "Lianhui Qin",
    "affiliations": []
  }, {
    "name": "Zhisong Zhang",
    "affiliations": []
  }, {
    "name": "Hai Zhao",
    "affiliations": []
  }],
  "abstractText": "Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks. Implicit discourse relation classification is the bottleneck for discourse parsing. Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred. This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation. Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.",
  "title": "A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification"
}