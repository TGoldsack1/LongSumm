{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider the design of adaptive, nonparametric statistical tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals PxPy with the null hypothesis that H0 : X and Y are independent. While classical tests of dependence, such as Pearson’s correlation and Kendall’s τ , are able to detect monotonic relations between univariate variables, more modern tests can address complex interactions, for instance changes in variance of X with the value of Y . Key to many recent tests is to examine covariance or correlation between data features. These interactions become significantly harder to detect, and the features are more difficult to design, when the data reside in high dimensions.\nZoltán Szabó’s ORCID ID: 0000-0001-6183-7603. Arthur Gretton’s ORCID ID: 0000-0003-3169-7624. 1Gatsby Unit, University College London, UK. 2CMAP, École Polytechnique, France. Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nA basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the Hilbert-Schmidt norm of the covariance operator between feature mappings of the random variables (Gretton et al., 2005; 2008). Each random variable X and Y is mapped to a respective reproducing kernel Hilbert space Hk and Hl. For sufficiently rich mappings, the covariance operator norm is zero if and only if the variables are independent. A second basic nonlinear dependence measure is the smoothed difference between the characteristic function of the joint distribution, and that of the product of marginals. When a particular smoothing function is used, the statistic corresponds to the covariance between distances ofX and Y variable pairs (Feuerverger, 1993; Székely et al., 2007; Székely & Rizzo, 2009), yielding a simple test statistic based on pairwise distances. It has been shown by Sejdinovic et al. (2013) that the distance covariance (and its generalization to semi-metrics) is an instance of HSIC for an appropriate choice of kernels. A disadvantage of these feature covariance statistics, however, is that they require quadratic time to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo & Székely (2016) achieve an O(n log n) cost). Moreover, the feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of an expensive eigenvalue problem (e.g. Zhang et al., 2011) is required for consistent estimation of the quantiles. Several approaches were proposed by Zhang et al. (2017) to obtain faster tests along the lines of HSIC. These include computing HSIC on finite-dimensional feature mappings chosen as random Fourier features (RFFs) (Rahimi & Recht, 2008), a block-averaged statistic, and a Nyström approximation to the statistic. Key to each of these approaches is a more efficient computation of the statistic and its threshold under the null distribution: for RFFs, the null distribution is a finite weighted sum of χ2 variables; for the block-averaged statistic, the null distribution is asymptotically normal; for Nyström, either a permutation approach is employed, or the spectrum of the Nyström approximation to the kernel matrix is used in approximating the null distribution. Each of these methods costs significantly less than theO(n2) cost of the full HSIC (the cost is linear in n, but also depends quadratically on the number of features retained). A potential disadvantage of the Nyström and Fourier approaches is that the features are not optimized to maximize test power,\nbut are chosen randomly. The block statistic performs worse than both, due to the large variance of the statistic under the null (which can be mitigated by observing more data).\nIn addition to feature covariances, correlation measures have also been developed in infinite dimensional feature spaces: in particular, Bach & Jordan (2002); Fukumizu et al. (2008) proposed statistics on the correlation operator in a reproducing kernel Hilbert space. While convergence has been established for certain of these statistics, their computational cost is high at O(n3), and test thresholds have relied on permutation. A number of much faster approaches to testing based on feature correlations have been proposed, however. For instance, Dauxois & Nkiet (1998) compute statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order B-splines. The cost of this approach is O(n). This idea was extended by Lopez-Paz et al. (2013), who computed the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they performed a copula transform on the inputs, with a total cost of O(n log n). Finally, space partitioning approaches have also been proposed, based on statistics such as the KL divergence, however these apply only to univariate variables (Heller et al., 2016), or to multivariate variables of low dimension (Gretton & Györfi, 2010) (that said, these tests have other advantages of theoretical interest, notably distribution-independent test thresholds).\nThe approach we take is most closely related to HSIC on a finite set of features. Our simplest test statistic, the Finite Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each of X and Y . A normalized version of the statistic (NFSIC) yields a distribution-independent asymptotic test threshold. We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. (2015). As in recent work on two-sample testing by Jitkrittum et al. (2016), our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power. The design of features for independence testing turns out to be quite different to the case of two-sample testing, however: the task is to find correlated feature pairs on the respective marginal domains, rather than attempting to find a single, high-dimensional feature representation on the tensor product of the marginals, as we would need to do if we were comparing distributions Pxy and Qxy . While the use of coupled feature pairs on the marginals entails a smaller feature space dimension, it introduces significant complications in the proof of the lower bound, compared with the two-sample case. We demonstrate the performance of our tests on several challenging artificial and real-world datasets, including detection of dependence between music and its year of appearance, and between videos and captions.\nIn these experiments, we outperform competing linear and O(n log n) time tests."
  }, {
    "heading": "2. Independence Criteria and Statistical Tests",
    "text": "We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle that dependence can be measured in terms of the covariance between data features. Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution when Pxy = PxPy. We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test."
  }, {
    "heading": "2.1. The Finite Set Independence Criterion",
    "text": "We begin by recalling the Hilbert-Schmidt Independence Criterion (HSIC) as proposed in Gretton et al. (2005), since our unnormalized statistic is built along similar lines. Consider two random variables X ∈ X ⊆ Rdx and Y ∈ Y ⊆ Rdy . Denote by Pxy the joint distribution betweenX and Y ; Px and Py are the marginal distributions of X and Y . Let⊗ denote the tensor product, such that (a⊗ b) c = a 〈b, c〉. Assume that k : X × X → R and l : Y × Y → R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS)Hk andHl, respectively. Let ‖ · ‖HS be the norm on the space ofHl → Hk Hilbert-Schmidt operators. Then, HSIC between X and Y is defined as\nHSIC(X,Y ) = ∥∥µxy − µx ⊗ µy∥∥2HS\n= E(x,y),(x′,y′) [k(x,x′)l(y,y′)] + ExEx′ [k(x,x′)]EyEy′ [l(y,y′)] − 2E(x,y) [Ex′ [k(x,x′)]Ey′ [l(y,y′)]] , (1)\nwhere Ex := Ex∼Px , Ey := Ey∼Py , Exy := E(x,y)∼Pxy , and x′ is an independent copy of x. The mean embedding of Pxy belongs to the space of Hilbert-Schmidt operators from Hl to Hk, µxy := ∫ X×Y k(x, ·) ⊗ l(y, ·) dPxy(x,y) ∈\nHS(Hl,Hk), and the marginal mean embeddings are µx :=∫ X k(x, ·) dPx(x) ∈ Hk and µy := ∫ Y l(y, ·) dPy(y) ∈ Hl (Smola et al., 2007). Gretton et al. (2005, Theorem 4) show that if the kernels k and l are universal (Steinwart & Christmann, 2008) on compact domains X and Y , then HSIC(X,Y ) = 0 if and only if X and Y are independent. Given a joint sample Zn = {(xi,yi)}ni=1 ∼ Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.\nWe now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC). Let X ⊆ Rdx and Y ⊆ Rdy be open sets. Let µxµy(x,y) := µx(x)µy(y) The idea is to see µxy(v,w) = Exy[k(x,v)l(y,w)], µx(v) = Ex[k(x,v)] and µy(w) = Ey[l(y,w)] as smooth functions, and consider a new dis-\ntance between µxy and µxµy instead of a Hilbert-Schmidt distance as in HSIC (Gretton et al., 2005). The new measure is given by the average of squared differences between µxy and µxµy, evaluated at J random test locations VJ := {(vi,wi)}Ji=1 ⊂ X × Y .\nFSIC2(X,Y ) := 1\nJ J∑ i=1 [µxy(vi,wi)− µx(vi)µy(wi)]2\n= 1\nJ J∑ i=1 u2(vi,wi) = 1 J ‖u‖22,\nwhere\nu(v,w) := µxy(v,w)− µx(v)µy(w) = Exy[k(x,v)l(y,w)]− Ex[k(x,v)]Ey[l(y,w)], (2) = covxy[k(x,v), l(y,w)],\nu := (u(v1,w1), . . . , u(vJ ,wJ)) >, and {(vi,wi)}Ji=1 are realizations from an absolutely continuous distribution (wrt the Lebesgue measure).\nOur first result in Proposition 2 states that FSIC(X,Y ) almost surely defines a dependence measure for the random variables X and Y , provided that the product kernel on the joint space X × Y is characteristic and analytic (see Definition 1).\nDefinition 1 (Analytic kernels (Chwialkowski et al., 2015)). Let X be an open set in Rd. A positive definite kernel k : X ×X → R is said to be analytic on its domain X ×X if for all v ∈ X , f(x) := k(x,v) is an analytic function on X . Assumption A. The kernels k : X × X → R and l : Y × Y → R are bounded by Bk and Bl respectively [supx,x′∈X k(x,x\n′) ≤ Bk, supy,y′∈Y l(y,y′) ≤ Bl] , and the product kernel g((x,y), (x′,y′)) := k(x,x′)l(y,y′) is characteristic (Sriperumbudur et al., 2010, Definition 6), and analytic (Definition 1) on (X × Y)× (X × Y). Proposition 2 (FSIC is a dependence measure). Assume that assumption A holds, and that the test locations VJ = {(vi,wi)}Ji=1 are drawn from an absolutely continuous distribution η. Then, η-almost surely, it holds that FSIC(X,Y ) = 1√\nJ ‖u‖2 = 0 if and only if X and Y are\nindependent.\nProof. Since g is characteristic, the mean embedding map Πg : P 7→ E(x,y)∼P [g((x,y), ·)] is injective (Sriperumbudur et al., 2010, Section 3), where P is a probability distribution on X × Y . Since g is analytic, by Lemma 10 (Appendix), µxy and µxµy are analytic functions. Thus, Lemma 11 (Appendix, setting Λ = Πg) guarantees that FSIC(X,Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are independent almost surely.\nFSIC uses µxy as a proxy for Pxy , and µxµy as a proxy for PxPy. Proposition 2 states that, to detect the dependence between X and Y , it is sufficient to evaluate the difference of the population joint embedding µxy and the embedding of the product of the marginal distributions µxµy at a finite number of locations (defined by VJ ). The intuitive explanation of this property is as follows. If Pxy = PxPy, then u(v,w) = 0 everywhere, and FSIC(X,Y ) = 0 for any VJ . If Pxy 6= PxPy, then u will not be a zero function, since the mean embedding map is injective (requires the product kernel to be characteristic). Using the same argument as in Chwialkowski et al. (2015), since k and l are analytic, u is also analytic, and the set of roots Ru := {(v,w) | u(v,w) = 0} has Lebesgue measure zero. Thus, it is sufficient to draw (v,w) from an absolutely continuous distribution to have (v,w) /∈ Ru η-almost surely, and hence FSIC(X,Y ) > 0. We note that a characteristic kernel which is not analytic may produce u such thatRu has a positive Lebesgue measure. In this case, there is a positive probability that (v,w) ∈ Ru, resulting in a potential failure to detect the dependence.\nThe next proposition shows that Gaussian kernels k and l yield a product kernel which is characteristic and analytic; in other words, this is an example when Assumption A holds. Proposition 3 (A product of Gaussian kernels is characteristic and analytic). Let k(x,x′) = exp ( −(x− x′)>A(x− x′) ) and l(y,y′) =\nexp ( −(y − y′)>B(y − y′) ) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x′,y′)) = k(x,x′)l(y,y′) is characteristic and analytic on (Rdx × Rdy )× (Rdx × Rdy ). Proof (sketch). The main idea is to use the fact that a Gaussian kernel is analytic, and a product of Gaussian kernels is a Gaussian kernel on the pair of variables. See the full proof in Appendix D.\nPlug-in Estimator Assume that we observe a joint sample Zn := {(xi,yi)}ni=1\ni.i.d.∼ Pxy. Unbiased estimators of µxy(v,w) and µxµy(v,w) are µ̂xy(v,w) := 1n ∑n i=1 k(xi,v)l(yi,w) and µ̂xµy(v,w) := 1 n(n−1) ∑n i=1 ∑ j 6=i k(xi,v)l(yj ,w), respectively. A straightforward empirical estimator of FSIC2 is then given by\nF̂SIC2(Zn) = 1\nJ J∑ i=1 û(vi,wi) 2,\nû(v,w) := µ̂xy(v,w)− µ̂xµy(v,w) (3)\n= 2 n(n− 1) ∑ i<j h(v,w)((xi,yi), (xj ,yj)), (4)\nwhere h(v,w)((x,y), (x′,y′)) := 12 (k(x,v) − k(x′,v))(l(y,w) − l(y′,w)). For conciseness, we\ndefine û := (û1, . . . , ûJ)> ∈ RJ where ûi := û(vi,wi) so that F̂SIC2(Zn) = 1J û >û.\nF̂SIC2 can be efficiently computed inO((dx+dy)Jn) time which is linear in n [see (3) which does not have nested double sums], assuming that the runtime complexity of evaluating k(x,v) is O(dx) and that of l(y,w) is O(dy). Since FSIC satisfies FSIC(X,Y ) = 0 ⇐⇒ X ⊥ Y , in principle its empirical estimator can be used as a test statistic for an independence test proposing a null hypothesis H0 : “X and Y are independent” against an alternative H1 : “X and Y are dependent.” The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however, and depends on the unknown Pxy. This prompts us to consider a normalized version of FSIC whose asymptotic null distribution takes a more convenient form. We first derive the asymptotic distribution of û in Proposition 4, which we use to derive the normalized test statistic in Theorem 5. As a shorthand, we write z := (x,y), t := (v,w), covz is covariance,Vz stands for variance. Proposition 4 (Asymptotic distribution of û). Define u := (u(t1), . . . , u(tJ))\n>, k̃(x,v) := k(x,v) − Ex′k(x′,v), and l̃(y,w) := l(y,w) − Ey′ l(y′,w). Let Σ = [Σij ] ∈ RJ×J be the positive semi-definite matrix with entries Σij = covz(û(ti), û(tj)) = Exy[k̃(x,vi)l̃(y,wi)k̃(x,vj)l̃(y,wj)]−u(ti)u(tj). Then, under both H0 and H1, for any fixed test locations {t1, . . . , tJ} for which Σ is full rank, and 0 < Vz[htj (z)] < ∞ for j = 1, . . . , J , it holds that √ n(û − u) d→ N (0,Σ).\nProof. For a fixed {t1, . . . , tJ}, û is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht. Thus, by Lehmann (1999, Theorem 6.1.6) and Kowalski & Tu (2008, Section 5.1, Theorem 1), it follows directly that √ n(û − u) d→ N (0,Σ) where we note that Exy[k̃(x,v)l̃(y,w)] = u(v,w).\nRecall from Proposition 2 that u = 0 holds almost surely under H0. The asymptotic normality described in Proposition 4 implies that nF̂SIC2 = nJ û\n>û converges in distribution to a sum of J dependent weighted χ2 random variables. The dependence comes from the fact that the coordinates û1 . . . , ûJ of û all depend on the sample Zn. This null distribution is not analytically tractable, and requires a large number of simulations to compute the rejection threshold Tα for a given significance value α."
  }, {
    "heading": "2.2. Normalized FSIC and Adaptive Test",
    "text": "For the purpose of an independence test, we will consider a normalized variant of F̂SIC2, which we call N̂FSIC2, whose tractable asymptotic null distribution is χ2(J), the\nchi-squared distribution with J degrees of freedom. We then show that the independence test defined by N̂FSIC2 is consistent. These results are given in Theorem 5.\nTheorem 5 (Independence test based on N̂FSIC2 is consistent). Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn, where Σ is defined in Proposition 4. Assume that VJ = {(vi,wi)}Ji=1 ∼ η where η is absolutely continuous wrt the Lebesgue measure. The N̂FSIC2 statistic is\ndefined as λ̂n := nû> ( Σ̂ + γnI )−1 û where γn ≥ 0 is a\nregularization parameter. Assume that\n1. Assumption A holds.\n2. Σ is invertible η-almost surely.\n3. limn→∞ γn = 0.\nThen, for any k, l and VJ satisfying the assumptions,\n1. Under H0, λ̂n d→ χ2(J) as n→∞. 2. Under H1, for any r ∈ R, limn→∞ P ( λ̂n ≥ r ) = 1\nη-almost surely. That is, the independence test based on N̂FSIC2 is consistent.\nProof (sketch) . Under H0, nû>(Σ̂ + γnI)−1û asymptotically follows χ2(J) because √ nû is asymptotically normally distributed (see Proposition 4). Claim 2 builds on the result in Proposition 2 stating that u 6= 0 under H1; it follows using the convergence of û to u. The full proof can be found in Appendix E.\nTheorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for any fixed threshold. Asymptotically the test threshold Tα is given by the (1− α)-quantile of χ2(J) and is independent of n. The assumption on the consistency of Σ̂ is required to obtain the asymptotic chi-squared distribution. The regularization parameter γn is to ensure that (Σ̂ + γnI)−1 can be stably computed. In practice, γn requires no tuning, and can be set to be a very small constant. We emphasize that J need not increase with n for test consistency.\nThe next proposition states that the computational complexity of the N̂FSIC2 estimator is linear in both the input dimension and sample size, and that it can be expressed in terms of the K =[Kij ] = [k(vi,xj)] ∈ RJ×n,L = [Lij ] = [l(wi,yj)] ∈ RJ×n matrices. In contrast to typical kernel methods, a large Gram matrix of size n × n is not needed to compute N̂FSIC2.\nProposition 6 (An empirical estimator of N̂FSIC2). Let 1n := (1, . . . , 1)\n> ∈ Rn. Denote by ◦ the element-wise matrix product. Then,\n1. û = (K◦L)1nn−1 − (K1n)◦(L1n) n(n−1) .\n2. A consistent estimator for Σ is Σ̂ = ΓΓ >\nn where\nΓ := (K− n−1K1n1>n ) ◦ (L− n−1L1n1>n )− ûb1>n , ûb = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .\nAssume that the complexity of the kernel evaluation is linear in the input dimension. Then the test statistic λ̂n =\nnû> ( Σ̂ + γnI )−1 û can be computed in O(J3 + J2n +\n(dx + dy)Jn) time.\nProof (sketch). Claim 1 for û is straightforward. The expression for Σ̂ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4. The consistency of Σ̂ can be obtained by noting that the finite sample bound for P(‖Σ̂−Σ‖F > t) decreases as n increases. This is implicitly shown in Appendix F.2.2 and its following sections.\nAlthough the dependency of the estimator on J is cubic, we empirically observe that only a small value of J is required (see Section 3). The number of test locations J relates to the number of regions in X × Y of pxy and pxpy that differ (see Figure 1).\nTheorem 5 asserts the consistency of the test for any test locations VJ drawn from an absolutely continuous distribution. In practice, VJ can be further optimized to increase the test power for a fixed sample size. Our final theoretical result gives a lower bound on the test power of N̂FSIC2 i.e., the probability of correctly rejecting H0. We will use this lower bound as the objective function to determine VJ and the kernel parameters. Let ‖ · ‖F be the Frobenius norm. Theorem 7 (A lower bound on the test power). Let NFSIC2(X,Y ) := λn := nu\n>Σ−1u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that\n1. There exist finite Bk and Bl such that supk∈K supx,x′∈X |k(x,x′)| ≤ Bk and supl∈L supy,y′∈Y |l(y,y′)| ≤ Bl.\n2. c̃ := supk∈K supl∈L supVJ∈V ‖Σ −1‖F <∞.\nThen, for any k ∈ K, l ∈ L, VJ ∈ V , and λn ≥ r, the test power satisfies P ( λ̂n ≥ r ) ≥ L(λn) where\nL(λn) = 1− 62e−ξ1γ 2 n(λn−r) 2/n − 2e−b0.5nc(λn−r)2/[ξ2n2]\n− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2nn(n−1)] 2 /[ξ4n2(n−1)],\nb·c is the floor function, ξ1 := 132c21J2B∗ , B ∗ is a constant depending on onlyBk andBl, ξ2 := 72c22JB 2,B := BkBl,\nξ3 := 8c1B 2J , c3 := 4B2Jc̃2, ξ4 := 28B4J2c21, c1 :=\n4B2J √ Jc̃, and c2 := 4B √ Jc̃. Moreover, for sufficiently large fixed n, L(λn) is increasing in λn.\nWe provide the proof in Appendix F. To put Theorem 7 into perspective, assume that K ={ (x,v) 7→ exp ( −‖x−v‖ 2\n2σ2x\n) | σ2x ∈ [σ2x,l, σ2x,u] } =: Kg\nfor some 0 < σ2x,l < σ 2 x,u < ∞ and L ={ (y,w) 7→ exp ( −‖y−w‖ 2\n2σ2y\n) | σ2y ∈ [σ2y,l, σ2y,u] } =: Lg\nfor some 0 < σ2y,l < σ 2 y,u < ∞ are Gaussian kernel classes. Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2. The assumption c̃ < ∞ is a technical condition to guarantee that the test power lower bound is finite for all θ defined by the feasible sets K,L, and V . Let V ,r := { VJ | ‖vi‖2, ‖wi‖2 ≤\nr and ‖vi−vj‖22 + ‖wi−wj‖22 ≥ , for all i 6= j }\n. If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c̃ <∞ as Kg,Lg, and V ,r are compact. In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly. We show in Appendix C that the objective function used to choose VJ will discourage any two locations to be in the same neighborhood.\nParameter Tuning Let θ be the collection of all tuning parameters of the test. If k ∈ Kg and l ∈ Lg (i.e., Gaussian kernels), then θ = {σ2x, σ2y, VJ}. The test power lower bound L(λn) in Theorem 7 is a function of λn = nu>Σ−1u which is the population counterpart of the test statistic λ̂n. As in FSIC, it can be shown that λn = 0 if and only if X are Y are independent (from Proposition 2). According to Theorem 7, for a sufficiently large n, the test power lower bound is increasing in λn. One can therefore think of λn (a function of θ) as representing how easily the test rejects H0 given a problem Pxy . The higher the λn, the greater the lower bound on the test power, and thus the more likely it is that the test will reject H0 when it is false.\nIn light of this reasoning, we propose to set θ by maximizing the lower bound on the test power i.e., set θ to θ∗ = arg maxθ L(λn). Assume that n is sufficiently large so that λn 7→ L(λn) is an increasing function. Then, arg maxθ L(λn) = arg maxθ λn. That this procedure is also valid under H0 can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be arbitrary. Since Theorem 7 guarantees that λ̂n\nd→ χ2(J) as n→∞ for any θ, the asymptotic null distribution does not change by using θ∗. In practice, λn is a population quantity which is unknown. We propose dividing the sample Zn into two disjoint sets: training and test sets. The training set is used to compute λ̂n (an estimate of λn) to optimize for θ∗, and the test set is used for the actual independence test with the optimized θ∗. The splitting is to guarantee the independence of θ∗ and the test sample to avoid overfitting.\nTo better understand the behaviour of N̂FSIC2, we visualize µ̂xy(v,w), µ̂xµy(v,w) and Σ̂(v,w) as a function of one test location (v,w) on a simple toy problem. In this problem, Y = −X + Z where Z ∼ N (0, 0.32) is an independent noise variable. As we consider only one location (J = 1), Σ̂(v,w) is a scalar. The statistic can be written as λ̂n = n (µ̂xy(v,w)−µ̂xµy(v,w)) 2\nΣ̂(v,w) . These components are\nshown in Figure 1, where we use Gaussian kernels for both X and Y , and the horizontal and vertical axes correspond to v ∈ R and w ∈ R, respectively. Intuitively, û(v,w) = µ̂xy(v,w) − µ̂xµy(v,w) captures the difference of the joint distribution and the product of the marginals as a function of (v,w). Squaring û(v,w) and dividing it by the variance shown in Figure 1c gives the statistic (also the parameter tuning objective) shown in Figure 1d. The latter figure illustrates that the parameter tuning objective function can be non-convex: non-convexity arises since there are multiple ways to detect the difference between the joint distribution and the product of the marginals. In this case, the lower left and upper right regions equally indicate the largest difference. A convex objective would not be able to capture this phenomenon."
  }, {
    "heading": "3. Experiments",
    "text": "In this section, we empirically study the performance of the proposed method on both toy (Section 3.1) and real problems (Section 3.2). We are interested in challenging problems requiring a large number of samples, where a quadratic-time test might be computationally infeasible. Our goal is not to outperform a quadratic-time test with a linear-time test uniformly over all testing problems. We will find, however, that our test does outperform the quadratic-time test in some cases. Code is available at https://github.com/wittawatj/fsic-test.\nWe compare the proposed NFSIC with optimization (NFSICopt) to five multivariate nonparametric tests. The N̂FSIC2 test without optimization (NFSIC-med) acts as a baseline, allowing the effect of parameter optimization to be clearly\nseen. For pedagogical reason, we consider the original HSIC test of Gretton et al. (2005) denoted by QHSIC, which is a quadratic-time test. Nyström HSIC (NyHSIC) uses a Nyström approximation to the kernel matrices ofX and Y when computing the HSIC statistic. FHSIC is another variant of HSIC in which a random Fourier feature approximation (Rahimi & Recht, 2008) to the kernel is used. NyHSIC and FHSIC are studied in Zhang et al. (2017) and can be computed in O(n), with quadratic dependency on the number of inducing points in NyHSIC, and quadratic dependency on the number of random features in FHSIC. Finally, the Randomized Dependence Coefficient (RDC) proposed in Lopez-Paz et al. (2013) is also considered. The RDC can be seen as the primal form (with random Fourier features) of the kernel canonical correlation analysis of Bach & Jordan (2002) on copula-transformed data. We consider RDC as a linear-time test even though preprocessing by an empirical copula transform costs O((dx + dy)n log n). We use Gaussian kernel classes Kg and Lg for both X and Y in all the methods. Except NFSIC-opt, all other tests use full sample to conduct the independence test, where the Gaussian widths σx and σy are set according to the widely used median heuristic i.e., σx = median ({‖xi − xj‖2 | 1 ≤ i < j ≤ n}), and σy is set in the same way using {yi}ni=1. The J locations for NFSICmed are randomly drawn from the standard multivariate normal distribution in each trial. For a sample of size n, NFSIC-opt uses half the sample for parameter tuning, and the other disjoint half for the test. We permute the sample 300 times in RDC1 and HSIC to simulate from the null distribution and compute the test threshold. The null distributions for FHSIC and NyHSIC are given by a finite sum of weighted χ2(1) random variables given in Eq. 8 of Zhang et al. (2017). Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1 − α)-quantile of χ2(J). To provide a fair comparison, we set J = 10, use 10 inducing points in NyHSIC, and 10 random Fourier features in FHSIC and RDC.\nOptimization of NFSIC-opt The parameters of NFSIC-opt are σx, σy, and J locations of size (dx + dy)J . We treat all the parameters as a long vector in R2+(dx+dy)J and use gradient ascent to optimize λ̂n/2. We observe that initializing VJ by randomly picking J points from the training sample yields good performance. The regularization parameter γn in NFSIC is fixed to a small value, and is not optimized. It is worth emphasizing that the complexity of the optimization procedure is still linear-time.2\n1We use a permutation test for RDC, following the authors’ implementation (https://github.com/lopezpaz/ randomized_dependence_coefficient, referred commit: b0ac6c0).\n2Our claim on linear runtime (with respect to n) is for the gradient ascent procedure to find a local optimum for θ. We do not\nSince FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n. By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed. We refer the reader to Appendix C for a brief investigation of the test power vs. increasing J . The test power does not necessarily monotonically increase with J ."
  }, {
    "heading": "3.1. Toy Problems",
    "text": "We consider three toy problems.\n1. Same Gaussian (SG). The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx) and Y ∼ N (0, Idy ) where Id is the d× d identity matrix. This problem represents a case in which H0 holds.\n2. Sinusoid (Sin). Let pxy be the probability density of Pxy . In the Sinusoid problem, the dependency ofX and Y is characterized by (X,Y ) ∼ pxy(x, y) ∝ 1 + sin(ωx) sin(ωy), where the domains of X ,Y = (−π, π) and ω is the frequency of the sinusoid. As the frequency ω increases, the drawn sample becomes more similar to a sample drawn from Uniform((−π, π)2). That is, the higher ω, the harder to detect the dependency between X and Y . This problem was studied in Sejdinovic et al. (2013). Plots of the density for a few values of ω are shown in Figures 6 and 7 in the appendix. The main characteristic of interest in this problem is the local change in the density function. 3. Gaussian Sign (GSign). In this problem, Y = |Z|∏dxi=1 sgn(Xi), where X ∼ N (0, Idx), sgn(·) is the sign function, and Z ∼ N (0, 1) serves as a source of noise. The full interaction of X = (X1, . . . , Xdx) is what makes the problem challenging. That is, Y is dependent on X , yet it is independent of any proper subset of {X1, . . . , Xd}. Thus, simultaneous consideration of all the coordinates of X is required to successfully detect the dependency.\nWe fix n = 4000 and vary the problem parameters. Each problem is repeated for 300 trials, and the sample is redrawn each time. The significance level α is set to 0.05. The re-\nclaim a linear runtime to find a global optimum.\nsults are shown in Figure 2. It can be seen that in the SG problem (Figure 2b) where H0 holds, all the tests achieve roughly correct type-I errors at α = 0.05. In particular, we point out that NFSIC-opt’s rejection rate is well controlled as the sample used for testing and the sample used for parameter tuning are independent. The rejection rate would have been much higher had we done the optimization and testing on the same sample (i.e., overfitting). In the Sin problem, NFSIC-opt achieves high test power for all considered ω = 1, . . . , 6, highlighting its strength in detecting local changes in the joint density. The performance of NFSIC-med is significantly lower than that of NFSIC-opt. This phenomenon clearly emphasizes the importance of the optimization to place the locations at the relevant regions in X×Y . RDC has a remarkably high performance in both Sin and GSign (Figure 2c, 2d) despite no parameter tuning. The ability to simultaneously consider interacting features of NFSIC-opt is indicated by its superior test power in GSign, especially at the challenging settings of dx = 5, 6.\nNFSIC vs. QHSIC. We observe that NFSIC-opt outperforms the quadratic-time QHSIC in these two problems. QHSIC is defined as the RKHS norm of the witness function u (see (2)). Intuitively, one can think of the RKHS norm as taking into account all the locations (v,w). By contrast, the proposed NFSIC evaluates the witness function at J locations. If the differences in pxy and pxpy are local (e.g., Sin problem), or there are interacting features (e.g., GSign problem), then only small regions in the space of (X,Y ) are relevant in detecting the difference of pxy and pxpy. In these cases, pinpointing exact test locations by the optimization of NFSIC performs well. On the other hand, taking into account all possible test locations as done implicitly in QHSIC also integrates over regions where the difference between pxy and pxpy is small, resulting in a weaker indication of dependence. Whether QHSIC is better than NFSIC depends heavily on the problem, and there is no one best answer. If the difference between pxy and pxpy is large only in localized regions, then the proposed linear time statistic has an advantage. If the difference is spatially diffuse, then QHSIC has an advantage. No existing work has proposed a procedure to optimally tune kernel parameters for QHSIC; by contrast, NFSIC has a clearly defined objective for parameter tuning.\nTo investigate the sample efficiency of all the tests, we fix dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and increase n. Figure 3 shows the results. The quadratic dependency on n in QHSIC makes it infeasible both in terms of memory and runtime to consider n larger than 6000 (Figure 3a). By constrast, although not the most time-efficient, NFSIC-opt has the highest sample-efficiency for GSign, and for Sin in the low-sample regime, significantly outperforming QHSIC. Despite the small additional overhead from the optimization, we are yet able to conduct an accurate test with n = 105, dx = dy = 250 in less than 100 seconds. We observe in Figure 3b that the two NFSIC variants have correct type-I errors across all sample sizes. We recall from Theorem 5 that the NFSIC test with random test locations will asymptotically reject H0 if it is false. A demonstration of this property is given in Figure 3c, where the test power of NFSIC-med eventually reaches 1 with n higher than 105."
  }, {
    "heading": "3.2. Real Problems",
    "text": "We now examine the performance of our proposed test on real problems.\nMillion Song Data (MSD) We consider a subset of the Million Song Data3 (Bertin-Mahieux et al., 2011), in which each song (X) out of 515,345 is represented by 90 features, of which 12 features are timbre average (over all segments) of the song, and 78 features are timbre covariance. Most of the songs are western commercial tracks from 1922 to 2011. The goal is to detect the dependency between each song and its year of release (Y ). We set α = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial. Other settings are the same as in the toy problems. To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold. Figure 4b shows the test powers as n increases from 500 to 2000. To simulate the case whereH0 holds in the problem, we permute the sample to break the dependency of X and Y . The results are shown in Figure 5 in the appendix.\nEvidently, NFSIC-opt has the highest test power among all\n3Million Song Data subset: https://archive.ics. uci.edu/ml/datasets/YearPredictionMSD.\nthe linear-time tests for all the sample sizes. Its test power is second to only QHSIC. We recall that NFSIC-opt uses half of the sample for parameter tuning. Thus, at n = 500, the actual sample for testing is 250, which is relatively small. The fact that there is a vast power gain from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that the optimization procedure can perform well even at a lower sample sizes.\nVideos and Captions Our last problem is based on the VideoStory46K4 dataset (Habibian et al., 2014). The dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding text captions (Y ) uploaded by the users. Each video is represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors of Wang & Schmid (2013). Each caption is represented as a bag of words with each feature being the frequency of one word. After filtering only words which occur in at least six video captions, we obtain dy = 1878 words. We examine the test powers as n increases from 2000 to 8000. The results are given in Figure 4. The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000. QHSIC performs exceptionally well on this problem, achieving a maximum power throughout. NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting.\n4VideoStory46K dataset: https://ivi.fnwi.uva.nl/ isis/mediamill/datasets/videostory.php."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank the Gatsby Charitable Foundation for the financial support. The major part of this work was carried out while Zoltán Szabó was a research associate at the Gatsby Computational Neuroscience Unit, University College London."
  }],
  "year": 2017,
  "references": [{
    "title": "An Introduction to Multivariate Statistical Analysis",
    "authors": ["Anderson", "Theodore W"],
    "year": 2003
  }, {
    "title": "Kernel independent component analysis",
    "authors": ["Bach", "Francis R", "Jordan", "Michael I"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "The million song dataset",
    "authors": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel P.W", "Whitman", "Brian", "Lamere", "Paul"],
    "venue": "In International Conference on Music Information Retrieval (ISMIR),",
    "year": 2011
  }, {
    "title": "Fast Two-Sample Testing with Analytic Representations of Probability Measures",
    "authors": ["Chwialkowski", "Kacper P", "Ramdas", "Aaditya", "Sejdinovic", "Dino", "Gretton", "Arthur"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 1981
  }, {
    "title": "Nonlinear canonical analysis and independence tests",
    "authors": ["Dauxois", "Jacques", "Nkiet", "Guy Martial"],
    "venue": "The Annals of Statistics,",
    "year": 1998
  }, {
    "title": "A consistent test for bivariate dependence",
    "authors": ["Feuerverger", "Andrey"],
    "venue": "International Statistical Review,",
    "year": 1993
  }, {
    "title": "Kernel measures of conditional dependence",
    "authors": ["Fukumizu", "Kenji", "Gretton", "Arthur", "Sun", "Xiaohai", "Schölkopf", "Bernhard"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2008
  }, {
    "title": "Consistent nonparametric tests of independence",
    "authors": ["Gretton", "Arthur", "Györfi", "László"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Measuring Statistical Dependence with Hilbert-Schmidt Norms",
    "authors": ["Gretton", "Arthur", "Bousquet", "Olivier", "Smola", "Alex", "Schölkopf", "Bernhard"],
    "venue": "In Algorithmic Learning Theory (ALT),",
    "year": 2005
  }, {
    "title": "A Kernel Statistical Test of Independence",
    "authors": ["Gretton", "Arthur", "Fukumizu", "Kenji", "Teo", "Choon H", "Song", "Le", "Schölkopf", "Bernhard", "Smola", "Alex J"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2008
  }, {
    "title": "Consistent distribution-free ksample and independence tests for univariate random variables",
    "authors": ["Heller", "Ruth", "Yair", "Kaufman", "Shachar", "Brill", "Barak", "Gorfine", "Malka"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Fast computing for distance",
    "authors": ["Huo", "Xiaoming", "Székely", "Gábor J"],
    "venue": "covariance. Technometrics,",
    "year": 2016
  }, {
    "title": "Interpretable Distribution Features with Maximum Testing Power",
    "authors": ["Jitkrittum", "Wittawat", "Szabó", "Zoltán", "Chwialkowski", "Kacper", "Gretton", "Arthur"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Modern Applied UStatistics",
    "authors": ["Kowalski", "Jeanne", "Tu", "Xin M"],
    "year": 2008
  }, {
    "title": "The Randomized Dependence Coefficient",
    "authors": ["Lopez-Paz", "David", "Hennig", "Philipp", "Schölkopf", "Bernhard"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Random features for large-scale kernel machines",
    "authors": ["Rahimi", "Ali", "Recht", "Benjamin"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2008
  }, {
    "title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing",
    "authors": ["Sejdinovic", "Dino", "Sriperumbudur", "Bharath", "Gretton", "Arthur", "Fukumizu", "Kenji"],
    "venue": "The Annals of Statistics,",
    "year": 2013
  }, {
    "title": "Approximation Theorems of Mathematical Statistics",
    "authors": ["Serfling", "Robert J"],
    "year": 2009
  }, {
    "title": "A Hilbert space embedding for distributions",
    "authors": ["Smola", "Alex", "Gretton", "Arthur", "Song", "Le", "Schölkopf", "Bernhard"],
    "venue": "In International Conference on Algorithmic Learning Theory (ALT),",
    "year": 2007
  }, {
    "title": "Hilbert Space Embeddings and Metrics on Probability Measures",
    "authors": ["Sriperumbudur", "Bharath K", "Gretton", "Arthur", "Fukumizu", "Kenji", "Schölkopf", "Bernhard", "Lanckriet", "Gert R. G"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Support vector machines",
    "authors": ["Steinwart", "Ingo", "Christmann", "Andreas"],
    "venue": "Springer Science & Business Media,",
    "year": 2008
  }, {
    "title": "Brownian distance covariance",
    "authors": ["Székely", "Gábor J", "Rizzo", "Maria L"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2009
  }, {
    "title": "Measuring and testing dependence by correlation of distances",
    "authors": ["Székely", "Gábor J", "Rizzo", "Maria L", "Bakirov", "Nail K"],
    "venue": "The Annals of Statistics,",
    "year": 2007
  }, {
    "title": "Asymptotic Statistics",
    "authors": ["van der Vaart", "Aad"],
    "year": 2000
  }, {
    "title": "Action recognition with improved trajectories",
    "authors": ["Wang", "Heng", "Schmid", "Cordelia"],
    "venue": "In IEEE International Conference on Computer Vision (ICCV),",
    "year": 2013
  }, {
    "title": "Kernel-based conditional independence test and application in causal discovery",
    "authors": ["Zhang", "Kun", "Peters", "Jonas", "Janzing", "Dominik", "Schölkopf", "Bernhard"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),",
    "year": 2011
  }, {
    "title": "Large-Scale Kernel Methods for Independence Testing",
    "authors": ["Zhang", "Qinyi", "Filippi", "Sarah", "Gretton", "Arthur", "Sejdinovic", "Dino"],
    "venue": "Statistics and Computing,",
    "year": 2017
  }],
  "id": "SP:4e4398bb466571abb90ad2eb76a2fdae29adade1",
  "authors": [{
    "name": "Wittawat Jitkrittum",
    "affiliations": []
  }, {
    "name": "Zoltán Szabó",
    "affiliations": []
  }, {
    "name": "Arthur Gretton",
    "affiliations": []
  }],
  "abstractText": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.",
  "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings"
}