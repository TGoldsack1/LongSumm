{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 125–131 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2020\nRecently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks. Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods. This work proposes a direct hidden Markov model (HMM) with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm. The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% BLEU scores on two different translation tasks."
  }, {
    "heading": "1 Introduction",
    "text": "The hidden Markov model (HMM) was first introduced to statistical machine translation for addressing the word alignment problem (Vogel et al., 1996). Then the HMM-based approach was widely used along with the IBM models (Brown et al., 1993) for aligning the source and target words. In the conventional approach, the Bayes’ theorem is used and the HMM is applied to the inverse translation model\nPr(eI1|fJ1 ) = Pr(eI1) · Pr(fJ1 |eI1) = ∑\naJ1\nPr(fJ1 , a J 1 |eI1) (1)\nIn this case, as a part of a noisy channel model, the marginalisation becomes intractable for every e.\nThis work proposes a novel concept focusing on direct HMM for Pr(eI1|fJ1 ), in which the alignment direction is from target to source positions. This specific property allows us to introduce dependencies into the translation model that take the full source sentence into account. This aspect will be important for the future decoder to be developed. The lexicon and alignment probabilities in the HMM are modeled using feedforward neural networks (FFNN) and they are trained jointly. The trained HMM is then applied for reranking the n-best lists created by a state-of-the-art open source phrase-based translation system. The experiments are conducted on the IWSLT 2016 German→English and BOLT Chinese→English translation tasks. The FFNNbased hybrid HMM provides improvements by up to 1.0% BLEU scores."
  }, {
    "heading": "2 Related Work",
    "text": "In order to discuss related work, we will consider the following two key concepts that are essential for the work to be presented:\n• Neural lexicon and alignment models The idea of using neural networks for lexicon modeling is not new (Schwenk, 2012; Sundermeyer et al., 2014; Devlin et al., 2014). Apart from differences in the neural network architecture, the important difference to this work is that those approaches did not include the concepts of HMM models and end-to-end training. In addition to neural lexicon modeling, (Alkhouli et al., 2016) also applied a neural network for alignment modeling like this work, but their training procedure was based on the maximum approximation and on predefined GIZA++ (Och and Ney, 2003) alignments.\n125\nThere were other studies that focused on feature-rich alignment models (Blunsom and Cohn, 2006; Berg-Kirkpatrick et al., 2010; Dyer et al., 2011), but those studies did not use a neural network to automatically learn features (as we do in this work). (Yang et al., 2013) used neural network-based lexicon and alignment models inside the HMM alignment model, but they model alignments using a simple distortion model that has no dependence on lexical context. Their goal was to improve the alignment quality in the context of a phrase-based translation system. However, apart from (Dyer et al., 2011), no results on translation were reported.\nThe idea of using neural networks is the basis of the state-of-the-art attention-based approach to machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, that approach is not based on the principle of an explicit and separate lexicon model.\n• End-to-end training The HMM in combination with the neural translation model lends itself to what is usually called end-to-end training. The training criterion is the logarithm of the target sentence posterior probability. This criterion results in a specific training algorithm that can be interpreted as a combination of forwardbackward algorithm (as in EM style training of HHMs) and backpropagation. To the best of our knowledge, this end-to-end training has not been considered before for machine translation. In the context of signal processing and recognition, the connectionist temporal classification (CTC) approach (Graves et al., 2006) leads to a similar training procedure. (Tran et al., 2016) studied neural networks for unsupervised training for a part-ofspeech tagging task. In their approach, the training criterion for this problem results in a combination of EM framework and backpropagation, which has a certain similarity to the training algorithm for translation as presented in this work."
  }, {
    "heading": "3 Definition of neural network-based HMM",
    "text": "Similar to hidden alignments aj = j → i between the source string fJ1 = f1...fj ...fJ and the target\nstring eI1 = e1...ei...eI in the conventional HMM, we define the alignments in direct HMM as bi = i→ j. Then the model can be defined as:\nPr(eI1|fJ1 ) = ∑\nbI1\nPr(eI1, b I 1|fJ1 ) (2)\nPr(eI1, b I 1|fJ1 ) = I∏\ni=1\np(ei, bi|bi−11 , ei−11 , fJ1 )\n= I∏\ni=1 p(ei|bi1, ei−11 , fJ1 )︸ ︷︷ ︸ lexicon model · p(bi|bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸ alignment model\n(3)\nOur feed-forward alignment model has the same architecture (Figure 1) as the one proposed in (Alkhouli et al., 2016). Thus the alignment probability can be modeled by:\np(bi|bi−11 , ei−11 , fJ1 ) = p(∆i|f bi−1+γm bi−1−γm , e i−1 i−n)\n(4) where γm = m−12 and m indicates the window size. ∆i = bi − bi−1 denotes the jump from the predecessor position to the current position. Thus, the jump over the source is estimated based on a m-words source context window and n predecessor target words.\nFor the lexicon model, we assume a similar dependence as in the alignment model with a shift, namely on the source words within a window centred on the aligned source word and n predecessor target words. To overcome the high costs of the softmax function for large vocabularies, we adopt the class-factored output layer consisting of a class layer and a word layer (Goodman, 2001; Morin\nand Bengio, 2005). The model in this case is defined as\np(ei|bi1, ei−11 , fJ1 ) = p(ei|f bi+γmbi−γm , e i−1 i−n)\n= p(ei|c(ei), f bi+γmbi−γm , e i−1 i−n) · p(c(ei)|f bi+γm bi−γm , e i−1 i−n)\n(5) where c denotes a word mapping that assigns each target word to a single class, where the number of classes is chosen to be much smaller than the vocabulary size. The lexicon model architecture is shown in Figure 2."
  }, {
    "heading": "4 Training",
    "text": "The training data of the direct HMM are the source and target sequences, without any alignment information. In the training of direct HMM including neural network-based models, the weights have to be updated along with the posterior probabilities calculated by the Baum-Welch algorithm. Similar to the training procedure used in (BergKirkpatrick et al., 2010), we apply the EM algorithm and define the auxiliary function as\nQ(θ; θ̂)\n= ∑\nbI1\np(bI1|fJ1 , eI1, θ) log p(eI1, bI1|fJ1 , θ̂)\n= ∑\nbI1\np(bI1|fJ1 , eI1, θ) I∑\ni=1\n[log p(ei|fbi+γmbi−γm , e i−1 i−n, α̂)\n+ log p(∆i|fbi−1+γmbi−1−γm , e i−1 i−n, β̂)]\n= ∑\ni\n∑\nj\npi(j|eI1, fJ1 , θ) log p(ei|f j+γmj−γm , e i−1 i−n, α̂)\n+ ∑\ni\n∑\nj′\n∑\nj\npi(j ′, j|eI1, fJ1 , θ) log p(∆i|f j ′+γm j′−γm , e i−1 i−n, β̂)\n(6)\nwhere θ̂ = {α̂, β̂}, j′ = bi−1 and\npi(j|eI1, fJ1 , θ) = ∑\nbI1:bi=j\np(bI1|eI1, fJ1 , θ) (7)\nThen the parameters can be separated for lexicon model and alignment model:\nQ(θ; θ̂) = Qlex(θ; α̂) +Qalign(θ; β̂) (8)\nwhere\n∂Qlex(θ, α̂)\n∂α̂ =\n∑\ni\n∑\nj\nforward-backward algorithm︷ ︸︸ ︷ pi(j|eI1, fJ1 , θ)\n· ∂ ∂α̂ log p(ei|f j+γmj−γm , e i−1 i−n, α̂)\n︸ ︷︷ ︸ backpropagation\n(9)\n∂Qalign(θ, β̂)\n∂β̂ =\n∑\ni\n∑\nj′\n∑\nj\nforward-backward algorithm︷ ︸︸ ︷ pi(j ′, j|eI1, fJ1 , θ)\n· ∂ ∂β̂ log p(∆i|f j ′+γm j′−γm , e i−1 i−n, β̂)\n︸ ︷︷ ︸ backpropagation\n(10) From Equations (9) and (10) we can observe that the marginalisation of hidden alignments ( ∑\nj pi(j|eI1, fJ1 , θ)) is the only difference compared to the derivative of neural network training based on word-aligned data. In this approach we iterate over all source positions and the word alignment toolkit such as GIZA++ is not required. Furthermore, the word-aligned data generated e.g. by GIZA++ might contain unaligned and multiply aligned words, which make the data difficult to use for training neural networks. Thus the heuristicbased approaches (Sundermeyer et al., 2014; Devlin et al., 2014) have to be used in order to guarantee the one-on-one alignments, which may negatively influence the quality of the alignments. By contrast, the neural network-based HMM is not constrained by these heuristics. In addition, even though the training process of the direct HMM takes more time than the neural network training on the word-aligned data, we should note that generating the word-aligned data using GIZA++ is also a time-consuming process.\nIn general, our training procedure can be summarized as follows:\n1. One iteration IBM-1 model training to create lexicon table for initializing the forwardbackward table.\n2. In the first epoch, for each sentence pair calculate and save the entire table of posterior probabilities pi(b|eI1, fJ1 ) (also pi(b\n′, b|eI1, fJ1 ) for alignment model) using forward-backward algorithm based on the results of IBM-1 model.\n3. Training neural network lexicon and alignment models based on the posterior probabilities.\n4. From the second epoch onwards:\n(a) For each sentence pair, calculating the posterior probabilities based on the lexicon and alignment probabilities estimated by neural network models.\n(b) Updating weights of neural networks based on the posterior probabilities.\n(c) Repeating step 4 until the perplexity converges.\nIn this work the IBM-1 initialization is required. We tried to train neural network models from scratch, but the perplexity converges towards a bad local minimum and gets stuck in it. We also attempted other heuristics for initialization, such as assigning probability 0.9 to diagonal alignments and spreading the left 0.1 evenly among other source positions. The resulted perplexity is much higher compared to initializing using IBM-1."
  }, {
    "heading": "5 Experimental Results",
    "text": "The experiments are conducted on the IWSLT 2016 German→English and BOLT Chinese→English translation tasks, which consist of 20M and 4M parallel sentence pairs respectively. The feed-forward neural network alignment and lexicon models are jointly trained on the subset of about 200K sentence pairs. As an initial research of this topic, our new model is only applied for reranking n-best lists created by a phrase-based decoder. The maximum size of the n-best lists is 500. The translation quality is evaluated by case-insensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics using MultEval (Clark et al., 2011). The scaling factors are tuned with MERT (Och, 2003) with BLEU as optimization criterion on the development sets. For the translation experiments, the\naveraged scores are presented on the development set from three optimization runs.\nOur direct HMM consists of a feed-forward neural network lexicon model with following configuration:\n• Five one-hot input vectors for source words and three for target words\n• Projection layer size 100 for each word • Two non-linear hidden layers with 1000 and\n500 nodes respectively\n• A class-factored output layer with 1000 singleton classes dedicated to the most frequent words, and 1000 classes shared among the rest of the words.\nand a feed-forward neural network alignment model with the same configuration as the lexicon model, except a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from −100 to 100 (Alkhouli et al., 2016).\nWe conducted experiments on the source and target window size of both network models. Larger source and target windows could not provide significant improvements on BLEU scores, at least for rescoring experiments.\nThe model is applied for reranking the n-best lists created by the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012) with a log-linear framework containing phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (Galley and Manning, 2008), a word class language mode (Wuebker et al., 2013) and an n-gram language model. The word alignments used for the training of phrase-tables are generated by GIZA++, which performs the alignment training sequentially for IBM-1, HMM and IBM-4. More details about our phrase-based baseline system can be found in (Peter et al., 2015).\nThe experimental results are demonstrated in Table 1. The rescoring experiments are conducted by adding HMM probability as feature and tuned with MERT. The applied attention-based neural network is a neural machine translation system similar to (Bahdanau et al., 2015). The decoder and encoder word embeddings are of size 620, the encoder uses a bidirectional layer with 1000 LSTMs (Hochreiter and Schmidhuber, 1997) to encode the source side. A layer with 1000 LSTMs\nis used by the decoder. The data is converted into subword units using byte pair encoding with 20000 operations (Sennrich et al., 2016). During training a batch size of 50 is used. More details about our neural machine translation system can be found in (Peter et al., 2016).\nWith n-best rescoring, all neural network-based systems achieve significant improvements over the phrase-based system. The neural network-based HMMs provide promising performance, even with simple feed-forward neural networks. The direct HMM trained by the EM procedure with marginalizing the hidden alignments outperformed the same model trained on the word-aligned data. For the rescoring tasks, it provides comparable performance with the attention-based network. The neural network-based HMM also helps the phrase-based system achieve comparable results with the stand-alone attention-based system on the German→English task."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "This work aims to close the gap between the conventional word alignment models and the novel neural machine translation. The proposed direct HMM consists of neural network-based alignment and lexicon models, both models are trained jointly and without any alignment information. With the simple feed-forward neural network models, the HMM model already provides promising results and significantly improves the strong phrase-based translation system.\nAs future work, we are searching for alternatives to initialize the training instead of using IBM-1. We will investigate recurrent model struc-\ntures, such as the LSTM representation for source and target word embeddings (Luong et al., 2015). In addition to the network structure, we will implement a stand-alone decoder based on this novel model. The first step would be to apply maximum approximation for the search problem as elucidated in (Yu et al., 2017). Then we plan to investigate heuristics for marginalizing the hidden alignment during search."
  }, {
    "heading": "Acknowledgments",
    "text": "The work reported in this paper results from two projects, SEQCLAS and QT21. SEQCLAS has received funding from\nthe European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme under grant agreement no 694537. QT21 has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no 645452. The work reflects only the authors’ views and neither the European Commission nor the European Research Council Executive Agency are responsible for any use that may be made of the information it contains.\nTamer Alkhouli was partly funded by the 2016 Google PhD Fellowship for North America, Europe and the Middle East.\nThe authors would like to thank Jan-Thorsten Peter for providing the attention-based neural network models."
  }],
  "year": 2017,
  "references": [{
    "title": "Alignment-Based Neural Machine Translation",
    "authors": ["Tamer Alkhouli", "Gabriel Bretschner", "Jan-Thorsten Peter", "Mohammed Hethnawi", "Andreas Guta", "Hermann Ney."],
    "venue": "Proceedings of the ACL 2016 First Conference on Machine Translation (WMT 2016).",
    "year": 2016
  }, {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations. San Diego, CA, USA.",
    "year": 2015
  }, {
    "title": "Painless Unsupervised Learning with Features",
    "authors": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-Côté", "John DeNero", "Dan Klein."],
    "venue": "Proceedings of the 2010 Annual Conference of the North American Chapter of the ACL. Los Angeles, CA, USA,",
    "year": 2010
  }, {
    "title": "Discriminative Word Alignment with Conditional Random Fields",
    "authors": ["Phil Blunsom", "Trevor Cohn."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL. Sydney, Australia, pages 65–72.",
    "year": 2006
  }, {
    "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."],
    "venue": "Computational Linguistics 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Unpacking and Transforming Feature Functions: New Ways to Smooth Phrase Tables",
    "authors": ["Boxing Chen", "Roland Kuhn", "George Foster", "Howard Johnson."],
    "venue": "Proceedings of MT Summit XIII. Xiamen, China, pages 269–275.",
    "year": 2011
  }, {
    "title": "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational",
    "year": 2011
  }, {
    "title": "Fast and robust neural network joint models for statistical machine translation",
    "authors": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Com-",
    "year": 2014
  }, {
    "title": "Unsupervised Word Alignment with Arbitrary Features",
    "authors": ["Chris Dyer", "Jonathan Clark", "Alon Lavie", "Noah A. Smith."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. Portland, OR, USA, pages 409–419.",
    "year": 2011
  }, {
    "title": "A simple and effective hierarchical phrase reordering model",
    "authors": ["Michel Galley", "Christopher D. Manning."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Honolulu, HI, USA, pages 848–856.",
    "year": 2008
  }, {
    "title": "Classes for fast maximum entropy training",
    "authors": ["Joshua Goodman."],
    "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. Salt Lake City, UT, USA, pages 561– 564.",
    "year": 2001
  }, {
    "title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
    "authors": ["Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber."],
    "venue": "Proceedings of the 23rd International Conference",
    "year": 2006
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "Hierarchical probabilistic neural network language model",
    "authors": ["Frederic Morin", "Yoshua Bengio."],
    "venue": "Proceedings of the international workshop on artificial intelligence and statistics. Barbados, pages 246–252.",
    "year": 2005
  }, {
    "title": "Minimum Error Rate Training in Statistical Machine Translation",
    "authors": ["Franz Josef Och."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. Sapporo, Japan, pages 160– 167.",
    "year": 2003
  }, {
    "title": "A Systematic Comparison of Various Statistical Alignment Models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Computational Linguistics 29:19–51.",
    "year": 2003
  }, {
    "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, PA, USA,",
    "year": 2002
  }, {
    "title": "The RWTH Aachen Machine Translation System for IWSLT 2016",
    "authors": ["Jan-Thorsten Peter", "Andreas Guta", "Nick Rossenbach", "Miguel Graça", "Hermann Ney."],
    "venue": "International Workshop on Spoken Language Translation. Seattle, WA, USA.",
    "year": 2016
  }, {
    "title": "The RWTH Aachen German-English Machine Translation System for WMT 2015",
    "authors": ["Jan-Thorsten Peter", "Farzad Toutounchi", "Joern Wuebker", "Hermann Ney."],
    "venue": "EMNLP 2015 Tenth Workshop on Statistical Machine Translation. Lisbon, Portugal,",
    "year": 2015
  }, {
    "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
    "authors": ["Holger Schwenk."],
    "venue": "Proceedings of the 24th International Conference on Computational Linguistics. Mumbai, India, pages 1071–1080.",
    "year": 2012
  }, {
    "title": "Neural Machine Translation of Rare Words with Subword Units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 1715–1725.",
    "year": 2016
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of the Conference of the Association for Machine Translation in the Americas.",
    "year": 2006
  }, {
    "title": "Translation Modeling with Bidirectional Recurrent Neural Networks",
    "authors": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney."],
    "venue": "Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 14–25.",
    "year": 2014
  }, {
    "title": "Unsupervised Neural Hidden Markov Models",
    "authors": ["Ke Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight."],
    "venue": "Proceedings of the Workshop on Structured Prediction for NLP. Austin, TX, USA, pages 63–71.",
    "year": 2016
  }, {
    "title": "Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models",
    "authors": ["David Vilar", "Daniel Stein", "Matthias Huck", "Hermann Ney."],
    "venue": "ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR.",
    "year": 2010
  }, {
    "title": "HMM-based Word Alignment in Statistical Translation",
    "authors": ["Stephan Vogel", "Hermann Ney", "Christoph Tillmann."],
    "venue": "Proceedings of the 16th Conference on Computational Linguistics - Volume 2. Copenhagen, Denmark, COLING ’96, pages 836–841.",
    "year": 1996
  }, {
    "title": "Jane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation",
    "authors": ["Joern Wuebker", "Matthias Huck", "Stephan Peitz", "Malte Nuhn", "Markus Freitag", "Jan-Thorsten Peter", "Saab Mansour", "Hermann Ney."],
    "venue": "International Confer-",
    "year": 2012
  }, {
    "title": "Improving Statistical Machine Translation with Word Class Models",
    "authors": ["Joern Wuebker", "Stephan Peitz", "Felix Rietig", "Hermann Ney."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle, WA, USA,",
    "year": 2013
  }, {
    "title": "Word Alignment Modeling with Context Dependent Deep Neural Network",
    "authors": ["Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Sofia, Bulgaria, pages 166–",
    "year": 2013
  }, {
    "title": "The Neural Noisy Channel",
    "authors": ["Lei Yu", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Tomáš Kočiský."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations. Toulon, France.",
    "year": 2017
  }],
  "id": "SP:5fc73e2c2fc396377d3b252e6d3547978b8cfd42",
  "authors": [{
    "name": "Weiyue Wang",
    "affiliations": []
  }, {
    "name": "Tamer Alkhouli",
    "affiliations": []
  }, {
    "name": "Derui Zhu",
    "affiliations": []
  }, {
    "name": "Hermann Ney",
    "affiliations": []
  }],
  "abstractText": "Recently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks. Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods. This work proposes a direct hidden Markov model (HMM) with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm. The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% BLEU scores on two different translation tasks.",
  "title": "Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation"
}