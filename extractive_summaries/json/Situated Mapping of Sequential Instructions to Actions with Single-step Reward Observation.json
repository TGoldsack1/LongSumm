{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2072–2082 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2072"
  }, {
    "heading": "1 Introduction",
    "text": "An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals. For example, consider the environment and instructions in Figure 1, in which a user describes moving chemicals between beakers and mixing chemicals together. To execute the second instruction, the agent needs to resolve sixth beaker and last one to objects in the environment. The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown. In this paper, we describe a model and learning approach to map sequences of instructions to actions. Our model considers previous utterances and the world state to select actions, learns to combine simple actions to achieve complex goals, and can be trained using\ngoal states without access to demonstrations. The majority of work on executing sequences of instructions focuses on mapping instructions to high-level formal representations, which are then evaluated to generate actions (e.g., Chen and Mooney, 2011; Long et al., 2016). For example, the third instruction in Figure 1 will be mapped to mix(prev_arg1), indicating that the mix action should be applied to first argument of the previous action (Long et al., 2016; Guu et al., 2017). In contrast, we focus on directly generating the sequence of actions. This requires resolving references without explicitly modeling them, and learning the sequences of actions required to complete high-level actions; for example, that mixing requires removing everything in the beaker and replacing with the same number of brown items.\nA key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world. Instructions often refer to previously\nmentioned objects (e.g., it in Figure 1) or actions (e.g., do it again). The world state provides the set of objects the instruction may refer to, and implicitly determines the available actions. For example, liquid can not be removed from an empty beaker. Both types of contexts continuously change during an interaction. As new instructions are given, the instruction history expands, and as the agent acts the world state changes. We propose an attentionbased model that takes as input the current instruction, previous instructions, the initial world state, and the current state. At each step, the model computes attention encodings of the different inputs, and predicts the next action to execute.\nWe train the model given instructions paired with start and goal states without access to the correct sequence of actions. During training, the agent learns from rewards received through exploring the environment with the learned policy by mapping instructions to sequences of actions. In practice, the agent learns to execute instructions gradually, slowly correctly predicting prefixes of the correct sequences of increasing length as learning progress. A key challenge is learning to correctly select actions that are only required later in execution sequences. Early during learning, these actions receive negative updates, and the agent learns to assign them low probabilities. This results in an exploration problem in later stages, where actions that are only required later are not sampled during exploration. For example, in the ALCHEMY domain shown in Figure 1, the agent behavior early during execution of instructions can be accomplished by only using POP actions. As a result, the agent quickly learns a strong bias against PUSH actions, which in practice prevents the policy from exploring them again. We address this with a learning algorithm that observes the reward for all possible actions for each visited state, and maximizes the immediate expected reward.\nWe evaluate our approach on SCONE (Long et al., 2016), which includes three domains, and is used to study recovering predicate logic meaning representations for sequential instructions. We study the problem of generating a sequence of low-level actions, and re-define the set of actions for each domain. For example, we treat the beakers in the ALCHEMY domain as stacks and use only POP and PUSH actions. Our approach robustly learns to execute sequential instructions with up to 89.1% task-completion\naccuracy for single instruction, and 62.7% for complete sequences. Our code is available at https://github.com/clic-lab/scone."
  }, {
    "heading": "2 Technical Overview",
    "text": "Task and Notation Let S be the set of all possible world states, X be the set of all natural language instructions, and A be the set of all actions. An instruction x̄ ∈ X of length |x̄| is a sequence of tokens 〈x1, ...x|x̄|〉. Executing an action modifies the world state following a transition function T : S ×A → S. For example, the ALCHEMY domain includes seven beakers that contain colored liquids. The world state defines the content of each beaker. We treat each beaker as a stack. The actions are POP N and PUSH N C, where 1 ≤ N ≤ 7 is the beaker number and C is one of six colors. There are a total of 50 actions, including the STOP action. Section 6 describes the domains in detail.\nGiven a start state s1 and a sequence of instructions 〈x̄1, . . . , x̄n〉, our goal is to generate the sequence of actions specified by the instructions starting from s1. We treat the execution of a sequence of instructions as executing each instruction in turn. The execution ē of an instruction x̄i starting at a state s1 and given the history of the instruction sequence 〈x̄1, . . . , x̄i−1〉 is a sequence of state-action pairs ē = 〈(s1, a1), ..., (sm, am)〉, where ak ∈ A, sk+1 = T (sk, ak). The final action am is the special action STOP, which indicates the execution has terminated. The final state is then sm, as T (sk, STOP) = sk. Executing a sequence of instructions in order generates a sequence 〈ē1, ..., ēn〉, where ēi is the execution of instruction x̄i. When referring to states and actions in an indexed execution ēi, the k-th state and action are si,k and ai,k. We execute instructions one after the other: ē1 starts at the interaction initial state s1 and si+1,1 = si,|ēi|, where si+1,1 is the start state of ēi+1 and si,|ēi| is the final state of ēi.\nModel We model the agent with a neural network policy (Section 4). At step k of executing the i-th instruction, the model input is the current instruction x̄i, the previous instructions 〈x̄1, . . . , x̄i−1〉, the world state s1 at the beginning of executing x̄i, and the current state sk. The model predicts the next action ak to execute. If ak = STOP, we switch to the next instruction, or if at the end of the instruction sequence, terminate. Otherwise, we update the state to sk+1 = T (sk, ak). The model uses attention to\nprocess the different inputs and a recurrent neural network (RNN) decoder to generate actions (Bahdanau et al., 2015).\nLearning We assume access to a set of N instruction sequences, where each instruction in each sequence is paired with its start and goal states. During training, we create an example for each instruction. Formally, the training set is {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . . , x̄ (j) i−1〉, g (j) i )} N,n(j) j=1,i=1, where x̄(j)i is an instruction, s (j) i,1 is a start state, 〈x̄(j)1 , . . . , x̄ (j) i−1〉 is the instruction history, g (j) i is the goal state, and n(j) is the length of the j-th instruction sequence. This training data contains no evidence about the actions and intermediate states required to execute each instruction.1 We use a learning method that maximizes the expected immediate reward for a given state (Section 5). The reward accounts for task-completion and distance to the goal via potential-based reward shaping.\nEvaluation We evaluate exact task completion for sequences of instructions on a test set {(s(j)1 , 〈x̄ (j) 1 , . . . , x̄ (j) nj 〉, g(j))}Nj=1, where g(j) is the oracle goal state of executing instructions x̄\n(j) 1 , . . . ,x̄ (j) nj in order starting from s (j) 1 . We also evaluate single-instruction task completion using per-instruction annotated start and goal states."
  }, {
    "heading": "3 Related Work",
    "text": "Executing instructions has been studied using the SAIL corpus (MacMahon et al., 2006) with focus on navigation using high-level logical representations (Chen and Mooney, 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Artzi et al., 2014) and lowlevel actions (Mei et al., 2016). While SAIL includes sequences of instructions, the data demonstrates limited discourse phenomena, and instructions are often processed in isolation. Approaches that consider as input the entire sequence focused on segmentation (Andreas and Klein, 2015). Recently, other navigation tasks were proposed with focus on single instructions (Anderson et al., 2018; Janner et al., 2018). We focus on sequences of environment manipulation instructions and modeling contextual cues from both the changing environment and instruction history. Manipulation using single-sentence instructions has been stud-\n1This training set is a subset of the data used in previous work (Section 6, Guu et al., 2015), in which training uses all instruction sequences of length 1 and 2.\nied using the Blocks domain (Bisk et al., 2016, 2018; Misra et al., 2017; Tan and Bansal, 2018). Our work is related to the work of Branavan et al. (2009) and Vogel and Jurafsky (2010). While both study executing sequences of instructions, similar to SAIL, the data includes limited discourse dependencies. In addition, both learn with rewards computed from surface-form similarity between text in the environment and the instruction. We do not rely on such similarities, but instead use a state distance metric.\nLanguage understanding in interactive scenarios that include multiple turns has been studied with focus on dialogue for querying database systems using the ATIS corpus (Hemphill et al., 1990; Dahl et al., 1994). Tür et al. (2010) surveys work on ATIS. Miller et al. (1996), Zettlemoyer and Collins (2009), and Suhr et al. (2018) modeled context dependence in ATIS for generating formal representations. In contrast, we focus on environments that change during execution and directly generating environment actions, a scenario that is more related to robotic agents than database query.\nThe SCONE corpus (Long et al., 2016) was designed to reflect a broad set of discourse context-dependence phenomena. It was studied extensively using logical meaning representations (Long et al., 2016; Guu et al., 2017; Fried et al., 2018). In contrast, we are interested in directly generating actions that modify the environment. This requires generating lower-level actions and learning procedures that are otherwise hardcoded in the logic (e.g., mixing action in Figure 1). Except for Fried et al. (2018), previous work on SCONE assumes access only to the initial and final states during training. This form of supervision does not require operating the agent manually to acquire the correct sequence of actions, a difficult task in robotic agents with complex control. Goal state supervision has been studied for instructional language (e.g., Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Bisk et al., 2016), and more extensively in question answering when learning with answer annotations only (e.g., Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014, 2015; Liang et al., 2017)."
  }, {
    "heading": "4 Model",
    "text": "We map sequences of instructions 〈x̄1, . . . , x̄n〉 to actions by executing the instructions in or-\nder. The model generates an execution ē = 〈(s1, a1), . . . , (smi , ami)〉 for each instruction x̄i. The agent context, the information available to the agent at step k, is s̃k = (x̄i, 〈x̄1, . . . , x̄i−1〉, sk, ē[: k]), where ē[: k] is the execution up until but not including step k. In contrast to the world state, the agent context also includes instructions and the execution so far. The agent policy πθ(s̃k, a) is modeled as a probabilistic neural network parametrized by θ, where s̃k is the agent context at step k and a is an action. To generate executions, we generate one action at a time, execute the action, and observe the new world state. In step k of executing the i-th instruction, the network inputs are the current utterance x̄i, the previous instructions 〈x̄1, . . . , x̄i−1〉, the initial state s1 at beginning of executing x̄i, and the current state sk. When executing a sequence of instructions, the initial state s1 is either the state at the beginning of executing the sequence or the final state of the execution of the previous instruction. Figure 2 illustrates our architecture.\nWe generate continuous vector representations for all inputs. Each input is represented as a set of vectors that are then processed with an attention function to generate a single vector representation (Luong et al., 2015). We assume access to a domain-specific encoding function ENC(s) that, given a state s, generates a set of vectors S representing the objects in the state. For example, in the ALCHEMY domain, a vector is generated for each beaker using an RNN. Section 6 describes the different domains and their encoding functions.\nWe use a single bidirectional RNN with a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to encode the instructions. All instructions x̄1,. . . ,x̄i are encoded\nwith a single RNN by concatenating them to x̄′. We use two delimiter tokens: one separates previous instructions, and the other separates the previous instructions from the current one. The forward LSTM RNN hidden states are computed as:2\n−−→ hj+1 = −−−−−→ LSTME ( φI(x′j+1); −→ hj ) ,\nwhere φI is a learned word embedding function and −−−−−→ LSTME is the forward LSTM recurrence function. We use a similar computation to compute the backward hidden states ←− hj . For each token x′j in x̄ ′, a vector representation h′j =[−→\nhj ; ←− hj ] is computed. We then create two sets of vectors, one for all the vectors of the current instruction and one for the previous instructions:\nXc = {h′j} J+|x̄i| j=J Xp = {h′j}j<Jj=0\nwhere J is the index in x̄′ where the current instruction x̄i begins. Separating the vectors to two sets will allows computing separate attention on the current instruction and previous ones.\nTo compute each input representation during decoding, we use a bi-linear attention function (Luong et al., 2015). Given a set of vectors H , a query vector hq, and a weight matrix W, the attention function ATTEND(H,hq,W) computes a context vector z:\nαi ∝ exp(hᵀiWh q) : i = 0, . . . , |H|\nz = |H|∑ i=1 αihi .\n2To simplify the notation, we omit the memory cell (often denoted as cj) from all LSTM descriptions. We use only the hidden state hj to compute the intended representations (e.g., for the input text tokens). All LSTMs in this paper use zero vectors as initial hidden state h0 and initial cell memory c0.\nWe use a decoder to generate actions. At each time step k, we compute an input representation using the attention function, update the decoder state, and compute the next action to execute. Attention is first computed over the vectors of the current instruction, which is then used to attend over the other inputs. We compute the context vectors zck and z p k for the current instruction and previous instructions:\nzck = ATTEND(X c,hdk−1,W c) zpk = ATTEND(X p, [hdk−1, z c k],W p) ,\nwhere hdk−1 is the decoder hidden state for step k− 1, and Xc and Xp are the sets of vector representations for the current instruction and previous instructions. Two attention heads are used over both the initial and current states. This allows the model to attend to more than one location in a state at once, for example when transferring items from one beaker to another in ALCHEMY. The current state is computed by the transition function sk = T (sk−1, ak−1), where sk−1 and ak−1 are the state and action at step k − 1. The context vectors for the initial state s1 and the current state sk are:\nzs1,k = [ATTEND(ENC(s1), [h d k−1, z c k],W sb,1);\nATTEND(ENC(s1), [hdk−1, z c k],W sb,2)]\nzsk,k = [ATTEND(ENC(sk), [h d k−1, z c k],W sc,1);\nATTEND(ENC(sk), [hdk−1, z c k],W sc,2)] ,\nwhere all W∗,∗ are learned weight matrices.\nWe concatenate all computed context vectors with an embedding of the previous action ak−1 to create the input for the decoder:\nhk = tanh([z c k; z p k; z s 1,k; z s k,k;φ O(ak−1)]W d + bd) hdk = LSTM D ( hk;h d k−1 ) ,\nwhere φO is a learned action embedding function and LSTMD is the LSTM decoder recurrence.\nGiven the decoder state hdk, the next action ak is predicted with a multi-layer perceptron (MLP). The actions in our domains decompose to an action type and at most two arguments.3 For example, the action PUSH 1 B in ALCHEMY has the type PUSH and two arguments: a beaker number and a color. Section 6 describes the actions of each domain. The probability of an action is:\n3We use a NULL argument for unused arguments.\nhak = tanh(h d kW a)\nsk,aT = h a kbaT sk,a1 = h a kba1 sk,a2 = h a kba2\np(ak = aT (a1, a2) | s̃k; θ) ∝ exp(sk,aT + sk,a1 + sk,a2) ,\nwhere aT , a1, and a2 are an action type, first argument, and second argument. If the predicted action is STOP, the execution is complete. Otherwise, we execute the action ak to generate the next state sk+1, and update the agent context s̃k to s̃k+1 by appending the pair (sk, ak) to the execution ē and replacing the current state with sk+1.\nThe model parameters θ include: the embedding functions φI and φO; the recurrence parameters for −−−−−→ LSTME , ←−−−−− LSTME , and LSTMD; WC , WP , Wsb,1, Wsb,2, Wsc,1, Wsc,2, Wd, Wa, and bd; and the domain dependent parameters, including the parameters of the encoding function ENC and the action type, first argument, and second argument weights baT , ba1 , and ba2 ."
  }, {
    "heading": "5 Learning",
    "text": "We estimate the policy parameters θ using an exploration-based learning algorithm that maximizes the immediate expected reward. Broadly speaking, during learning, we observe the agent behavior given the current policy, and for each visited state compute the expected immediate reward by observing rewards for all actions. We assume access to a set of training examples {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . . , x̄ (j) i−1〉, g (j) i )} N,n(j) j=1,i=1, where each instruction x̄(j)i is paired with a start state s\n(j) i,1 , the previous instructions in the sequence\n〈x̄(j)1 , . . . , x̄ (j) i−1〉, and a goal state g (j) i .\nReward The reward R(j)i : S × S × A → R is defined for each example j and instruction i:\nR (j) i (s, a, s ′) = P (j) i (s, a, s ′) + φ (j) i (s ′)− φ(j)i (s) ,\nwhere s is a source state, a is an action, and s′ is a target state.4 P (j)i (s, a, s\n′) is a problem reward and φ\n(j) i (s ′)− φ(j)i (s) is a shaping term. The problem reward P (j)i (s, a, s\n′) is positive for stopping at the goal g(j)i and negative for stopping in an incorrect\n4While the reward function is defined for any state-actionstate tuple, in practice, it is used during learning with tuples that follow the system dynamics, s′ = T (s, a).\nAlgorithm 1 SESTRA: Single-step Reward Observation.\nInput: Training data {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . . , x̄ (j) i−1〉,\ng (j) i )}\nN,n(j)\nj=1,i=1, learning rate µ, entropy regularization coefficient λ, episode limit horizon M . Definitions: πθ is a policy parameterized by θ, BEG is a special action to use for the first decoder step, and STOP indicates end of an execution. T (s, a) is the state transition function, H is an entropy function, R(j)i (s, a, s\n′) is the reward function for example j and instruction i, and RMSPROP divides each weight by a running average of its squared gradient (Tieleman and Hinton, 2012).\nOutput: Parameters θ defining a learned policy πθ . 1: for t = 1, . . . , T, j = 1, . . . , N do 2: for i = 1, . . . , n(j) do 3: ē← 〈 〉, k ← 0, a0 ← BEG 4: » Rollout up to STOP or episode limit. 5: while ak 6= STOP ∧ k < M do 6: k ← k + 1 7: s̃k ← (x̄i, 〈x̄1, . . . , x̄i−1〉, sk, ē[: k]) 8: » Sample an action from policy. 9: ak ∼ πθ(s̃k, ·) 10: sk+1 ← T (sk, ak) 11: ē← [ē; 〈(sk, ak)〉] 12: ∆← 0̄ 13: for k′ = 1, . . . , k do 14: » Compute the entropy of πθ(s̃k′ , ·). 15: ∆← ∆ + λ∇θH(πθ(s̃k′ , ·)) 16: for a ∈ A do 17: s′ ← T (sk′ , a) 18: » Compute gradient for action a. 19: ∆← ∆ +R(j)i (sk′ , a, s ′)∇θπθ(s̃k′ , a)\n20: θ ← θ + µRMSPROP ( ∆\nk ) 21: return θ\nstate or taking an invalid action:\nP (j) i (s, a, s ′) =  1.0 a = STOP ∧ s′ = g(j)i −1.0 a = STOP ∧ s′ 6= g(j)i −1.0− δ s = s′\n−δ otherwise\n,\nwhere δ is a verbosity penalty. The case s = s′ indicates that a was invalid in state s, as in this domain, all valid actions except STOP modify the state. We use a potential-based shaping term φ(j)i (s\n′)− φ(j)i (s) (Ng et al., 1999), where φ\n(j) i (s) = −||s− g (j) i || computes the edit distance between the state s and the goal, measured over the objects in each state. The shaping term densifies the reward, providing a meaningful signal for learning in nonterminal states.\nObjective We maximize the immediate expected reward over all actions and use entropy regularization. The gradient is approximated by sampling an execution ē = 〈(s1, a1), . . . , (sk, ak)〉 using our current policy:\n∇θJ = 1\nk k∑ k′=1 (∑ a∈A R (sk, a, T (sk, a))∇θπ(s̃k, a)\n+λ∇θH(π(s̃k, ·)) ) ,\nwhere H(π(s̃k, ·) is the entropy term.\nAlgorithm Algorithm 1 shows the Single-step Reward Observation (SESTRA) learning algorithm. We iterate over the training data T times (line 1). For each example j and turn i, we first perform a rollout by sampling an execution ē from πθ with at most M actions (lines 5-11). If the rollout reaches the horizon without predicting STOP, we set the problem reward P (j)i to−1.0 for the last step. Given the sampled states visited, we compute the entropy (line 15) and observe the immediate reward for all actions (line 19) for each step. Entropy and rewards are used to accumulate the gradient, which is applied to the parameters using RMSPROP (Dauphin et al., 2015) (line 20).\nDiscussion Observing the rewards for all actions for each visited state addresses an on-policy learning exploration problem. Actions that consistently receive negative reward early during learning will be visited with very low probability later on, and in practice, often not explored at all. Because the network is randomly initialized, these early negative rewards are translated into strong general biases that are not grounded well in the observed context. Our algorithm exposes the agent to such actions later on when they receive positive rewards even though the agent does not explore them during rollout. For example, in ALCHEMY, POP actions are sufficient to complete the first steps of good executions. As a result, early during learning, the agent learns a strong bias against PUSH actions. In practice, the agent then will not explore PUSH actions again. In our algorithm, as the agent learns to roll out the correct POP prefix, it is then exposed to the reward for the first PUSH even though it likely sampled another POP. It then unlearns its bias towards predicting POP.\nOur learning algorithm can be viewed as a costsensitive variant of the oracle in DAGGER (Ross et al., 2011), where it provides the rewards for all actions instead of an oracle action. It is also related to Locally Optimal Learning to Search (LOLS; Chang et al., 2015) with two key distinctions: (a) instead of using different roll-in and roll-out policies, we use the model policy; and (b) we branch at each step, instead of once, but do not rollout\nfrom branched actions since we only optimize the immediate reward. Figure 3 illustrates the comparison. Our summation over immediate rewards for all actions is related the summation of estimated Q-values for all actions in the Mean Actor-Critic algorithm (Asadi et al., 2017). Finally, our approach is related to Misra et al. (2017), who also maximize the immediate reward, but do not observe rewards for all actions for each state."
  }, {
    "heading": "6 SCONE Domains and Data",
    "text": "SCONE has three domains: ALCHEMY, SCENE, and TANGRAMS. Each interaction contains five instructions. Table 1 shows data statistics. Table 2 shows discourse reference analysis. State encodings are detailed in the Supplementary Material.\nALCHEMY Each environment in ALCHEMY contains seven numbered beakers, each containing up to four colored chemicals in order. Figure 1 shows an example. Instructions describe pouring chemicals between and out of beakers, and mixing beakers. We treat all beakers as stacks. There\nare two action types: PUSH and POP. POP takes a beaker index, and removes the top color. PUSH takes a beaker index and a color, and adds the color at the top of the beaker. To encode a state, we encode each beaker with an RNN, and concatenate the last output with the beaker index embedding. The set of vectors is the state embedding.\nSCENE Each environment in SCENE contains ten positions, each containing at most one person defined by a shirt color and an optional hat color. Instructions describe adding or removing people, moving a person to another position, and moving a person’s hat to another person. There are four action types: ADD_PERSON, ADD_HAT, REMOVE_PERSON, and REMOVE_HAT. ADD_PERSON and ADD_HAT take a position to place the person or hat and the color of the person’s shirt or hat. REMOVE_PERSON and REMOVE_HAT take the position to remove a person or hat from. To encode a state, we use a bidirectional RNN over the ordered positions. The input for each position is a concatenation of the color embeddings for the person and hat. The set of RNN hidden states is the state embedding.\nTANGRAMS Each environment in TANGRAMS is a list containing at most five unique objects. Instructions describe removing or inserting an object into a position in the list, or swapping the positions of two items. There are two action types: INSERT and REMOVE. INSERT takes the position to insert an object, and the object identifier. REMOVE takes an object position. We embed each object by concatenating embeddings for its type and position. The resulting set is the state embedding."
  }, {
    "heading": "7 Experimental Setup",
    "text": "Evaluation Following Long et al. (2016), we evaluate task completion accuracy using exact match between the final state and the annotated goal state. We report accuracy for complete interactions (5utts), the first three utterances of each interaction (3utts), and single instructions (Inst). For single instructions, execution starts from the annotated start state of the instruction.\nSystems We report performance of ablations and two baseline systems: POLICYGRADIENT: policy gradient with cumulative episodic reward without a baseline, and CONTEXTUALBANDIT: the contextual bandit approach of Misra et al. (2017). Both systems use the reward with the\nshaping term and our model. We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using contextaction demonstration pairs. Only the supervised approach uses the heuristically generated labels. Although the results are not comparable, we also report the performance of previous approaches to SCONE. All three approaches generate logical representations based on lambda calculus. In contrast to our approach, this requires an ontology of hand built symbols and rules to evaluate the logical forms. Fried et al. (2018) uses supervised learning with annotated logical forms.\nTraining Details For test results, we run each experiment five times and report results for the model with best validation interaction accuracy. For ablations, we do the same with three experiments. We use a batch size of 20. We stop training using a validation set sampled from the training data. We hold the validation set constant for each domain for all experiments. We use patience over the average reward, and select the best model using interaction-level (5utts) validation accuracy. We tune λ, δ, and M on the development set. The selected values and other implementation details are described in the Supplementary Material."
  }, {
    "heading": "8 Results",
    "text": "Table 3 shows test results. Our approach significantly outperforms POLICYGRADIENT and CONTEXTUALBANDIT, both of which suffer due to biases learned early during learning, hindering later exploration. This problem does not appear in TANGRAMS, where no action type is dominant at the beginning of executions, and all methods perform well. POLICYGRADIENT completely fails to learn ALCHEMY and SCENE due to observing only negative total rewards early during learning.\nUsing a baseline, for example with an actor-critic method, will potentially close the gap to CONTEXTUALBANDIT. However, it is unlikely to address the on-policy exploration problem.\nTable 4 shows development results, including model ablation studies. Removing previous instructions (– previous instructions) or both states (– current and initial state) reduces performance across all domains. Removing only the initial state (– initial state) or the current state (– current state) shows mixed results across the domains. Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains. We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains. In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts). We also report mean and standard deviation for our approach over five trials. We observe exceptionally high variance in performance on SCENE, where some experiments fail to learn and training performance remains exceptionally low (Figure 4). This highlights the sensitivity of the model to the random effects of initialization, dropout, and ordering of training examples.\nWe analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction. We study fifty examples in each domain to identify the type of failures. Table 5 shows the counts of major error categories. We consider multiple reference resolution errors. State reference errors indicate a failure to resolve a reference to the world state. For example, in ALCHEMY, the phrase leftmost red beaker specifies a beaker in the environment. If the model picked the correct action, but the wrong beaker, we count it as a state reference. We distinguish between multi-turn reference errors that should be feasible, and these that that are impossible to solve without access to states before executing previous utterances, which are not provided to our model. For example, in TANGRAMS, the instruction put it back in the same place refers to a previouslyremoved item. Because the agent only has access to the world state after following this instruction, it does not observe what kind of item was previously removed, and cannot identify the item to add. We\nalso find a significant number of errors due to ambiguous or incorrect instructions. For example, the SCENE instruction person in green appears on the right end is ambiguous. In the annotated goal, it is interpreted as referring to a person already in the environment, who moves to the 10th position. However, it can also be interpreted as a new person in green appearing in the 10th position.\nWe also study performance with respect to multi-turn coreference by observing whether the model was able to identify the correct referent for each occurrence included in the analysis in Table 2. The models were able to correctly resolve 92.3%, 88.7%, and 76.0% of references in ALCHEMY, SCENE, and TANGRAMS respectively.\nFinally, we include attention visualization for examples from the three domains in the Supplementary Material."
  }, {
    "heading": "9 Discussion",
    "text": "We propose a model to reason about contextdependent instructional language that display strong dependencies both on the history of the\ninteraction and the state of the world. Future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data. We propose to train our model using SESTRA, a learning algorithm that takes advantage of single-step reward observations to overcome learned biases in on-policy learning. Our learning approach requires additional reward observations in comparison to conventional reinforcement learning. However, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies. When the domain and model are less susceptible to such biases, the benefit of the additional reward observations is less pronounced. One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by the NSF (CRII1656998), Schmidt Sciences, and cloud computing credits from Amazon. We thank John Langford and Dipendra Misra for helpful and insightful discussions with regards to our learning algorithm. We also thank the anonymous reviewers for their helpful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Visionand-Language Navigation: Interpreting visually",
    "authors": ["Peter Anderson", "Qi Wu", "Damien Teney", "Jake Bruce", "Mark Johnson", "Niko Sünderhauf", "Ian Reid", "Stephen Gould", "Anton van den Hengel"],
    "year": 2018
  }, {
    "title": "Alignmentbased compositional semantics for instruction following",
    "authors": ["Jacob Andreas", "Dan Klein."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Learning compact lexicons for CCG semantic parsing",
    "authors": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2014
  }, {
    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
    "authors": ["Yoav Artzi", "Luke S. Zettlemoyer."],
    "venue": "Transactions of the Association of Computational Linguistics, 1:49–62.",
    "year": 2013
  }, {
    "title": "Mean actor critic",
    "authors": ["Kavosh Asadi", "Cameron Allen", "Melrose Roderick", "Abdel-rahman Mohamed", "George Konidaris", "Michael L. Littman."],
    "venue": "CoRR, abs/1709.00503.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Semantic parsing via paraphrasing",
    "authors": ["Jonathan Berant", "Percy Liang."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Imitation learning of agenda-based semantic parsers",
    "authors": ["Jonathan Berant", "Percy Liang."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:545–558.",
    "year": 2015
  }, {
    "title": "Learning interpretable spatial operations in a rich 3D blocks world",
    "authors": ["Yonatan Bisk", "Kevin Shih", "Yejin Choi", "Daniel Marcu."],
    "venue": "Proceedings of the Thirty-Second Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Natural language communication with robots",
    "authors": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."],
    "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
    "year": 2016
  }, {
    "title": "Reinforcement learning",
    "authors": ["S.R.K. Branavan", "Harr Chen", "Luke S. Zettlemoyer", "Regina Barzilay"],
    "year": 2009
  }, {
    "title": "Learning to search better than your teacher",
    "authors": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daumé", "John Langford."],
    "venue": "Proceedings of the International Conference on Machine Learning.",
    "year": 2015
  }, {
    "title": "Fast online lexicon learning for grounded language acquisition",
    "authors": ["David Chen."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Learning to interpret natural language navigation instructions from observations",
    "authors": ["David L. Chen", "Raymond J. Mooney."],
    "venue": "Proceedings of the National Conference on Artificial Intelligence.",
    "year": 2011
  }, {
    "title": "Driving semantic parsing from the world’s response",
    "authors": ["James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth."],
    "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning.",
    "year": 2010
  }, {
    "title": "Expanding the scope of the ATIS task: The ATIS-3 corpus",
    "authors": ["Deborah A. Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate Hunicke-Smith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg."],
    "venue": "Proceedings of the",
    "year": 1994
  }, {
    "title": "Equilibrated adaptive learning rates for nonconvex optimization. CoRR, abs/1502.04390",
    "authors": ["Yann Dauphin", "Harm de Vries", "Yoshua Bengio"],
    "year": 2015
  }, {
    "title": "Unified pragmatic models for generating and following instructions",
    "authors": ["Daniel Fried", "Jacob Andreas", "Dan Klein."],
    "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2018
  }, {
    "title": "Traversing knowledge graphs in vector space",
    "authors": ["Kelvin Guu", "John Miller", "Percy Liang."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
    "authors": ["Kelvin Guu", "Panupong Pasupat", "Evan Liu", "Percy Liang."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "The ATIS spoken language systems pilot corpus",
    "authors": ["Charles T. Hemphill", "John J. Godfrey", "George R. Doddington."],
    "venue": "Proceedings of the DARPA speech and natural language workshop.",
    "year": 1990
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9.",
    "year": 1997
  }, {
    "title": "Representation learning for grounded spatial reasoning",
    "authors": ["Michael Janner", "Karthik Narasimhan", "Regina Barzilay."],
    "venue": "Transactions of the Association for Computational Linguistics, 6:49–61.",
    "year": 2018
  }, {
    "title": "Scaling semantic parsers with on-the-fly ontology matching",
    "authors": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
    "authors": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D. Forbus", "Ni Lao."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-",
    "year": 2017
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael Jordan", "Dan Klein."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.",
    "year": 2011
  }, {
    "title": "Simpler context-dependent logical forms via model projections",
    "authors": ["Reginald Long", "Panupong Pasupat", "Percy Liang."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Walk the talk: Connecting language, knowledge, action in route instructions",
    "authors": ["Matthew MacMahon", "Brian Stankiewics", "Benjamin Kuipers."],
    "venue": "Proceedings of the National Conference on Artificial Intelligence.",
    "year": 2006
  }, {
    "title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."],
    "venue": "Association for the Advancement of Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "A fully statistical approach to natural language interfaces",
    "authors": ["Scott Miller", "David Stallard", "Robert Bobrow", "Richard Schwartz."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 1996
  }],
  "id": "SP:16c0062e91c9003072291cf728f893bca08a1cde",
  "authors": [{
    "name": "Alane Suhr",
    "affiliations": []
  }],
  "abstractText": "We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of singlestep reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%25.3% across the domains over approaches that use high-level logical representations.",
  "title": "Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation"
}