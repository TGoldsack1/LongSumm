{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Reinforcement learning is a formalism for trial-and-error interaction between an agent and an unknown environment. This interaction is typically specified by a Markov decision process (MDP), which contains a transition model, reward model, and potentially discount parameters specifying a discount on the sum of future values in the return. Domains are typically separated into two cases: episodic problems (finite horizon) and continuing problems (infinite horizon). In episodic problems, the agent reaches some terminal state, and is teleported back to a start state. In continuing problems, the agent interaction is continual, with a discount to ensure a finite total reward (e.g., constant < 1).\nThis formalism has a long and successful tradition, but is limited in the problems that can be specified. Progressively there have been additions to specify a broader range of ob-\n1Department of Computer Science, Indiana University. Correspondence to: Martha White <martha@indiana.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\njectives, including options (Sutton et al., 1999), state-based discounting (Sutton, 1995; Sutton et al., 2011) and interest functions (Reza and Sutton, 2010; Sutton et al., 2016). These generalizations have particularly been driven by offpolicy learning and the introduction of general value functions for Horde (Sutton et al., 2011; White, 2015), where predictive knowledge can be encoded as more complex prediction and control tasks. Generalizations to problem specifications provide exciting learning opportunities, but can also reduce clarity and complicate algorithm development and theory. For example, options and general value functions have significant overlap, but because of different terminology and formalization, the connections are not transparent. Another example is the classic divide between episodic and continuing problems, which typically require different convergence proofs (Bertsekas and Tsitsiklis, 1996; Tsitsiklis and Van Roy, 1997; Sutton et al., 2009) and different algorithm specifications.\nIn this work, we propose a formalism for reinforcement learning task specification that unifies many of these generalizations. The focus of the formalism is to separate the specification of the dynamics of the environment and the specification of the objective within that environment. Though natural, this represents a significant change in the way tasks are currently specified in reinforcement learning and has important ramifications for simplifying implementation, algorithm development and theory. The paper consists of two main contributions. First, we demonstrate the utility of this formalism by showing unification of previous tasks specified in reinforcement learning, including options, general value functions and episodic and continuing, and further providing case studies of utility. We demonstrate how to specify episodic and continuing tasks with only modifications to the discount function, without the addition of states and modifications to the underlying Markov decision process. This enables a unification that significantly simplifies implementation and easily generalizes theory to cover both settings. Second, we prove novel contraction bounds on the Bellman operator for these generalized RL tasks, and show that previous bounds for both episodic and continuing tasks are subsumed by this more general result. Overall, our goal is to provide an RL task formalism that requires minimal modifications to previous task specification, with significant gains in simplicity and unification across common settings."
  }, {
    "heading": "2. Generalized problem formulation",
    "text": "We assume the agent interacts with an environment formalized by a Markov decision process (MDP): (S,A,Pr) where S is the set of states, n = |S|; A is the set of actions; and Pr : S ⇥ A ⇥ S ! [0, 1] is the transition probability function where Pr(s, a, s0) is the probability of transitioning from state s into state s0 when taking action a. A reinforcement learning task (RL task) is specified on top of these transition dynamics, as the tuple (P, r, , i) where\n1. P is a set of policies ⇡ : S ⇥A ! [0, 1];\n2. the reward function r : S ⇥ A ⇥ S ! R specifies reward received from (s, a, s0);\n3. : S⇥A⇥S ! [0, 1] is a transition-based discount function1;\n4. i : S ! [0,1) is an interest function that specifies the user defined interest in a state.\nEach task could have different reward functions within the same environment. For example, in a navigation task within an office, one agent could have the goal to navigate to the kitchen and the other the conference room. For a reinforcement learning task, whether prediction or control, a set or class of policies is typically considered. For prediction (policy evaluation), we often select one policy and evaluate its long-term discounted reward. For control, where a policy is learned, the set of policies may consist of all policies parameterized by weights that specify the action-value from states, with the goal to find the weights that yield the optimal policy. For either prediction or control in an RL task, we often evaluate the return of a policy: the cumulative discounted reward obtained from following that policy\nGt =\n1X\ni=0\n0 @ i 1Y\nj=0\n(st+j , at+j , st+1+j)\n1\nA Rt+1+i\nwhere Q 1\nj=0 (st+j , at+j , st+1+j) := 1. Note that this subsumes the setting with a constant discount c 2 [0, 1), by using (s, a, s0) = c for every s, a, s0 and so givingQi 1\nj=0 (st+j , at+j , st+1+j) = i c for i > 0 and 0c = 1 for i = 0. As another example, the end of the episode, (s, a, s 0 ) = 0, making the product of these discounts zero and so terminating the recursion. We further explain how transition-based discount enables specification of episodic tasks and discuss the utility of the generalization to transition-based discounting throughout this paper. Finally, the interest function i specifies the degree of importance\n1We describe a further probabilistic generalization in Appendix A; much of the treatment remains the same, but the notation becomes cumbersome and the utility obfuscated.\nof each state for the task. For example, if an agent is only interested in learning an optimal policy for a subset of the environment, the interest function could be set to one for those states and to zero otherwise.\nWe first explain the specification and use of such tasks, and then define a generalized Bellman operator and resulting algorithmic extensions and approximation bounds."
  }, {
    "heading": "2.1. Unifying episodic and continuing specification",
    "text": "The RL task specification enables episodic and continuing problems to be easily encoded with only modification to the transition-based discount. Previous approaches, including the absorbing state formulation (Sutton and Barto, 1998b) and state-based discounting (Sutton, 1995; Reza and Sutton, 2010; Sutton et al., 2011)(van Hasselt, 2011, Section 2.1.1), require special cases or modifications to the set of states and underlying MDP, coupling task specification and the dynamics of the environment.\nWe demonstrate how transition-based discounting seamlessly enables episodic or continuing tasks to be specified in an MDP via a simple chain world. Consider the chain world with three states s1, s2 and s3 in Figure 1. The start state is s1 and the two actions are right and left. The reward is -1 per step, with termination occurring when taking action right from state s3, which causes a transition back to state s1. The discount is 1 for each step, unless specified otherwise. The interest is set to 1 in all states, which is the typical case, meaning performance from each state is equally important.\nFigure 1a depicts the classical approach to specifying episodic problems using an absorbing state, drawn as a square. The agent reaches the goal—transitioning right from state s3—then forever stays in the absorbing state, receiving a reward of zero. This encapsulates the definition of the return, but does not allow the agent to start another episode. In practice, when this absorbing state is reached, the agent is “teleported\" to a start state to start another episode. This episodic interaction can instead be represented the same way as a continuing problem, by specifying a transition-based discount (s3, right, s1) = 0. This defines the same return, but now the agent simply transitions normally to a start state, and no hypothetical states are added.\nTo further understand the equivalence, consider the updates made by TD (see equation (3)). Assume linear function approximation with feature function x : S ! Rd, with weights w 2 Rd. When the agent takes action right from s3, the agent transitions from s3 to s1 with probability one and so t+1 = (s3, right, s1) = 0. This correctly gives\nt = rt+1 + t+1x(s1) > w x(s3)>w = rt+1 x(s3)>w\nand correctly clears the eligibility trace for the next step\net+1 = t+1 t+1et + x(s1) = x(s1).\nThe stationary distribution is also clearly equal to the original episodic task, since the absorbing state is not used in the computation of the stationary distribution.\nAnother strategy is to still introduce hypothetical states, but use state-based , as discussed in Figure 1c. Unlike absorbing states, the agent does not stay indefinitely in the hypothetical state. When the agent goes right from s3, it transitions to hypothetical state s4, and then transition deterministically to the start state s1, with s(s4) = 0. As before, we get the correct update, because t+1 = s(s4) = 0. Because the stationary distribution has some non-zero probability in the hypothetical state s4, we must set x(s4) = x(s1) (or x(s4) = 0). Otherwise, the value of the hypothetical state will be learned, wasting function approximation resources and potentially modifying the approximation quality of the value in other states. We could have tried state-based discounting without adding an additional state s4. However, this leads to incorrect value estimates, as depicted in Figure 1d; the relationship between transition-based and state-based is further discussed in Appendix B.1. Overall, to keep the specification of the RL task and the MDP separate, transition-based discounting is necessary to enable the unified specification of episodic and continuing tasks."
  }, {
    "heading": "2.2. Options as RL tasks",
    "text": "The options framework (Sutton et al., 1999) generically covers a wide range of settings, with discussion about macroactions, option models, interrupting options and intra-option value learning. These concepts at the time merited their own language, but with recent generalizations can be more conveniently cast as RL subtasks.\nProposition 1. An option, defined as the tuple (Sutton et al., 1999, Section 2) (⇡, , I) with policy ⇡ : S ⇥ A ! [0, 1], termination function : S ! [0, 1] and an initiation set I ⇢ S from which the option can be run, can be equivalently cast as an RL task.\nThis proof is mainly definitional, but we state it as an explicit proposition for clarity. The discount function (s, a, s 0 ) = 1 (s0) for all s, a, s0 specifies termination. The interest function, i(s) = 1 if s 2 I and i(s) = 0 otherwise, focuses learning resources on the states of interest. If a value function for the policy is queried, it would only make sense to query it from these states of interest. If the policy for this option is optimized for this interest function, the policy should only be run starting from s 2 I, as elsewhere will be poorly learned. The rewards for the RL task correspond to the rewards associated with the MDP.\nRL tasks generalize options, by generalizing termination conditions to transition-based discounting and by providing degrees of interest rather than binary interest. Further, the policies associated with RL subtasks can be used as macro-\nactions, to specify a semi-Markov decision process (Sutton et al., 1999, Theorem 1)."
  }, {
    "heading": "2.3. General value functions",
    "text": "In a similar spirit of abstraction as options, general value functions were introduced for single predictive or goaloriented questions about the world (Sutton et al., 2011). The idea is to encode predictive knowledge in the form of value function predictions: with a collection or horde of prediction demons, this constitutes knowledge (Sutton et al., 2011; Modayil et al., 2014; White, 2015). The work on Horde (Sutton et al., 2011) and nexting (Modayil et al., 2014) provide numerous examples of the utility of the types of questions that can be specified by general value functions, and so by RL tasks, because general value functions can\nnaturally can be specified as an RL task.\nThe generalization to RL tasks provide additional benefits for predictive knowledge. The separation into underlying MDP dynamics and task specification is particularly useful in off-policy learning, with the Horde formalism, where many demons (value functions) are learned off-policy. These demons share the underlying dynamics, and even feature representation, but have separate prediction and control tasks; keeping these separate from the MDP is key for avoiding complications (see Appendix B.2). Transition-based discounts, over state-based discounts, additionally enable the prediction of a change, caused by transitioning between states. Consider the taxi domain, described more fully in Section 3, where the agent’s goal is to pick up and drop off passengers in a grid world with walls. The taxi agent may wish to predict the probability of hitting a wall, when following a given policy. This can be encoded by setting (s, a, s) = 0 if a movement action causes the agent to remain in the same state, which occurs when trying to move through a wall. In addition to episodic problems and hard termination, transition-based questions also enable soft termination for transitions. Hard termination uses (s, a, s0) = 0 and soft termination (s, a, s0) = ✏ for some small positive value ✏. Soft terminations can be useful for incorporating some of the value of a policy right after the soft termination. If two policies are equivalent up to a transition, but have very different returns after the transition, a soft termination will reflect that difference. We empirically demonstrate the utility of soft termination in the next section."
  }, {
    "heading": "3. Demonstration in the taxi domain",
    "text": "To better ground this generalized formalism and provide some intuition, we provide a demonstration of RL task specification. We explore different transition-based discounts in the taxi domain (Dietterich, 2000; Diuk et al., 2008). The goal of the agent is to take a passenger from a source platform to a destination platform, depicted in Figure 2. The agent receives a reward of -1 on each step, except for successful pickup and drop-off, giving reward 0. We modify the domain to include the orientation of the taxi, with additional cost for not continuing in the current orientation. This encodes that turning right, left or going backwards are more costly than going forwards, with additional negative rewards of -0.05, -0.1 and -0.2 respectively. This additional cost is further multiplied by a factor of 2 when there is a passenger in the vehicle. For grid size g and the number of pickup/dropoff locations l, the full state information is a 5-tuple: (x position of taxi 2 {1, . . . , g}, y position of taxi 2 {1, . . . , g}, location of passenger 2 {1, . . . , l + 1}, location of destination 2 {1, . . . , l}, orientation of car 2 {N,E, S,W} ). The location of the passenger can be in one of the pickup/drop-off locations, or in the taxi. Optimal\npolicies and value functions are computed iteratively, with an extensive number of iterations.\nFigure 2 illustrates three policies for one part of the taxi domain, obtained with three different discount functions. The optimal policy is learned using a soft-termination, which takes into consideration the importance of approaching the passenger location with the right orientation to minimize turns after picking up the passenger. A suboptimal policy is in fact learned with hard termination, as the policy prefers to greedily minimize turns to get to the passenger. For further details, refer to the caption in Figure 2.\nWe also compare to a constant , which corresponds to an average reward goal, as demonstrated in Equation (8). The table in Figure 2(e) summarizes the results. Though in theory it should in fact recognize the relative values of orientation before and after picking up a passenger, and obtain the same solution as the soft-termination policy, in practice we find that numerical imprecision actually causes a suboptimal policy to be learned. Because most of the rewards are negative per step, small differences in orientation can be more difficult to distinguish amongst for an infinite discounted sum. This result actually suggests that having multiple subgoals, as one might have with RL subtasks, could enable better chaining of decisions and local evaluation of the optimal action. The utility of learning with a smaller c has been previously described (Jiang et al., 2015), however, here we further advocate that enabling that provides subtasks is another strategy to improve learning."
  }, {
    "heading": "4. Objectives and algorithms",
    "text": "With an intuition for the specification of problems as RL tasks, we now turn to generalizing some key algorithmic concepts to enable learning for RL tasks. We first generalize the definition of the Bellman operator for the value function. For a policy ⇡ : S ⇥A ! [0, 1], define P⇡,P⇡, 2 Rn⇥n and r⇡,v⇡ 2 Rn, indexed by states s, s0 2 S ,\nP⇡(s, s 0 ) : =\nX\na2A\n⇡(s, a)Pr(s, a, s0)\nP⇡, (s, s 0 ) : =\nX\na2A\n⇡(s, a)Pr(s, a, s0) (s, a, s0)\nr⇡(s) := X\na2A\n⇡(s, a) X\ns02S\nPr(s, a, s0)r(s, a, s0)\nv⇡(s) := r⇡(s) + X\ns02S\nP⇡, (s, s 0 )v⇡(s 0 ).\nwhere v⇡(s) is the expected return, starting from a state s 2 S . To compute a value function that satisfies this recursion, we define a Bellman operator. The Bellman operator has been generalized to include state-based discounting and a state-based trace parameter2 (Sutton et al., 2016, Eq. 29).\n2A generalization to state-based trace parameters has been considered (Sutton, 1995; Sutton and Barto, 1998b; Reza and\nWe further generalize the definition to the transition-based setting. The trace parameter : S ⇥ A ⇥ S ! [0, 1] influences the fixed point and provides a modified (biased) return, called the -return; this parameter is typically motivated as a bias-variance trade-off parameter (Kearns and Singh, 2000). Because the focus of this work is on generalizing the discount, we opt for a simple constant c in the main body of the text; we provide generalizations to transition-based trace parameters in the appendix.\nThe generalized Bellman operator T( ) : Rn ! Rn is\nT ( ) v : = r\n⇡ +P ⇡v, 8v 2 Rn (1)\nwhere P ⇡ := (I cP⇡, ) 1\nP⇡, (1 c) (2) r\n⇡ := (I cP⇡, )\n1 r⇡\nTo see why this is the definition of the Bellman operator, we define the expected -return, v⇡, 2 Rn for a given approximate value function, given by a vector v 2 Rn.\nv⇡, (s) := r⇡(s)+\nX\ns02S P⇡, (s, s\n0 ) [(1 c)v(s0)+ cv⇡, (s0)]\n= r⇡(s) + (1 c)P⇡, (s, :)v + cP⇡, (s, :)v⇡, .\nSutton, 2010; Sutton et al., 2014; Yu, 2012).\nContinuing the recursion, we obtain3\nv⇡, =\n\" 1X\ni=0\n( cP⇡, ) i # (r⇡ + (1 c)P⇡, v)\n= (I cP⇡, ) 1 (r⇡ + (1 c)P⇡, v) = T( )v\nThe fixed point for this formula satisfies T( )v = v for the Bellman operator defined in Equation (1).\nTo see how this generalized Bellman operator modifies the algorithms, we consider the extension to temporal difference algorithms. Many algorithms can be easily generalized by replacing c or s(st+1) with transition-based (st, at, st+1). For example, the TD algorithm is generalized by setting the discount on each step to t+1 = (st, at, st+1),\nwt+1 = wt + ↵t tet . for some step-size ↵t t := rt+1 + t+1x(st+1) > w x(st)>w (3)\net = t cet 1 + x(st).\n3For a matrix M with maximum eigenvalue less than 1,P1 i=0 M i = (I M) 1. We show in Lemma 3 that P⇡, satisfies this condition, implying cP⇡, satisfies this condition and so this infinite sum is well-defined.\nThe generalized TD fixed-point, under linear function approximation, can be expressed as a linear system Aw = b\nA = X > D(I cP⇡, ) 1(I P⇡, )X\nb = X > D(I cP⇡, ) 1r⇡\nwhere each row in X 2 Rn⇥d corresponds to features for a state, and D 2 Rn⇥n is a diagonal weighting matrix. Typically, D = diag(dµ), where dµ 2 Rn is the stationary distribution for the behavior policy µ : S ⇥ A ! [0, 1] generating the stream of interaction. In on-policy learning, dµ = d⇡. With the addition of the interest function, this weighting changes to D = diag(dµ i), where denotes element-wise product (Hadamard product). More recently, a new algorithm, emphatic TD (ETD) (Mahmood et al., 2015; Sutton et al., 2016) specified yet another weighting, D = M where M = diag(m) with m = (I P ⇡) 1(dµ i). Importantly, even for off-policy sampling, with this weighting, A is guaranteed to be positive definite. We show in the next section that the generalized Bellman operator for both the on-policy and emphasis weighting is a contraction, and further in the appendix that the emphasis weighting with a transition-based trace function is also a contraction."
  }, {
    "heading": "5. Generalized theoretical properties",
    "text": "In this section, we provide a general approach to incorporating transition-based discounting into approximation bounds. Most previous bounds have assumed a constant discount. For example, ETD was introduced with state-based s; however, (Hallak et al., 2015) analyzed approximation error bounds of ETD using a constant discount c. By using matrix norms on P⇡, , we generalize previous approximation bounds to both the episodic and continuing case.\nDefine the set of bounded vectors for the general space of value functions V = {v 2 Rn : kvkDµ < 1}. Let Fv ⇢ V be a subspace of possible solutions, e.g., Fv = {Xw|w 2 Rd, kwk2 < 1}.\nA1. The action space A and state space S are finite.\nA2. For polices µ,⇡ : S ⇥A ! [0, 1], there exist unique invariant distributions dµ,d⇡ such that d⇡P⇡ = d⇡ and dµPµ = dµ. This assumption is typically satisfied by assuming an ergodic Markov chain for the policy.\nA3. There exist transition s, a, s0 such that (s, a, s0) < 1 and ⇡(s, a)P (s, a, s0) > 0. This assumptions states that the policy reaches some part of the space where the discount is less than 1.\nA4. Assume for any v 2 Fv, if v(s) = 0 for all s 2 S where i(s) > 0, then v(s) = 0 for all s 2 S s.t. i(s) = 0. For linear function approximation, this requires F = span{x(s) : s 2 S, i(s) 6= 0}.\nFor weighted norm kvk D = p v > Dv, if we can take the square root of D, the induced matrix norm is kP ⇡kD = D 1/2 P\n⇡D 1/2 sp , where the spectral norm k·ksp is the largest singular value of the matrix. For simplicity of notation below, define s\nD\n: = kP ⇡kD. For any diagonalizable, nonnegative matrix D, the projection ⇧\nD : V ! Fv onto Fv exists and is defined ⇧Dz = argmin v2Fv kz vkD."
  }, {
    "heading": "5.1. Approximation bound",
    "text": "We first prove that the generalized Bellman operator in Equation 1 is a contraction. We extend the bound from (Tsitsiklis and Van Roy, 1997; Hallak et al., 2015) for constant discount and constant trace parameter, to the general transition-based setting. The normed difference to the true value function could be defined by multiple weightings. A well-known result is that for D = D⇡ the Bellman operator is a contraction for constant c and c (Tsitsiklis and Van Roy, 1997); recently, this has been generalized for a variant of ETD to M, still with constant parameters (Hallak et al., 2015). We extend this result for transition-based for both D⇡ and the transition-based emphasis matrix M. Lemma 1. For D = D⇡ or D = M,\ns\nD = kP ⇡kD < 1.\nProof: For D = M: let ⇠ 2 Rn be the vector of row sums for P ⇡: P ⇡1 = ⇠. Then for any v 2 V , with v 6= 0,\nkP ⇡vk2M = X\ns2S m(s)\nX\ns02S P\n⇡(s, s\n0 )v(s 0 )\n!2\n=\nX s2S m(s)⇠(s)2 X s02S P ⇡(s, s 0 ) ⇠(s) v(s 0 )\n!2\n X\ns2S m(s)⇠(s)2\nX\ns02S\nP ⇡(s, s\n0 )\n⇠(s) v(s\n0 ) 2\n=\nX\ns02S v(s\n0 )\n2 X\ns2S m(s)⇠(s)P ⇡(s, s\n0 )\n= v > diag (m ⇠)>P ⇡ v\nwhere the first inequality follows from Jensen’s inequality, because P ⇡(s, :) is normalized. Now because ⇠ has entries that are less than 1, because the row sums of P ⇡ are less than 1 as shown in Lemma 4, and because each of the values in the above product are nonnegative,\nv > diag (m ⇠)>P ⇡ v\n v> diag m > P\n⇡ v\n= v > diag m > (P ⇡ I) +m> v\n= v > diag (d⇡ i)> +m> v\n= v > diag m > v v> diag\n(d⇡ i)\n> v\n< kvk2 M\nThe last inequality is a strict inequality because d⇡ i has at least one positive entry where v has a positive entry. Otherwise, if v(s) = 0 everywhere with i(s) > 0, then v = 0, which we assumed was not the case.\nTherefore, kP ⇡vkM < kvkM for any v 6= 0, giving kP ⇡kM := maxv2Rn,v 6=0 kP ⇡vkM kvkM < 1. This exact same proof follows through verbatim for the generalization of P ⇡ to transition-based trace .\nFor D = D⇡: Again, we use Jensen’s inequality, but now rely on the property d⇡P⇡ = d⇡. Because of Assumption A3, for some s < 1, for any non-negative v+,\nd⇡P⇡, v+ =\nX\ns\nX\na\nd⇡(s)Pr(s, a, :) (s, a, :)v+\n s X\ns\nX\na\nd⇡(s)Pr(s, a, :)v+ = sd⇡v.\nTherefore, because vectors P⇡, v+ are also non-negative,\nd⇡P ⇡v+ = d⇡\n1X\nk=0\n(P⇡, c) k P⇡, (1 c) ! v+\n (1 c) 1X\nk=0\n(s c) k d⇡P⇡, v+\n (1 c)(1 s c) 1sd⇡v+ and so\nkP ⇡vk2D⇡  X\ns2S d⇡(s)⇠(s)\n2 X\ns02S\nP ⇡(s, s\n0 )\n⇠(s) v(s\n0 ) 2\n=\nX\ns02S v(s\n0 )\n2 X\ns2S d⇡(s)⇠(s)P ⇡(s, s\n0 )\n X\ns02S v(s\n0 )\n2 X\ns2S d⇡(s)P ⇡(s, s\n0 )\n s(1 c)1 cs X\ns02S d(s\n0 )v(s 0 ) 2\n s s c1 cskvk 2 D⇡\nwhere s s c1 cs < 1 since s < 1. ⌅ Lemma 2. Under assumptions A1-A3, the Bellman operator T( ) in Equation (1) is a contraction under a norm weighted by D = D⇡ or D = M⇡ , i.e., for v 2 V\nkT( )vk D < kvk D .\nFurther, because the projection ⇧ D is a contraction, ⇧\nD\nT ( ) is also a contraction and has a unique fixed point ⇧\nD\nT ( ) v = v for v 2 Fv .\nProof: Because any vector v can be written v = v1 v2,\nkT( )vk D = kT( )(v1 v2)kD = kP ⇡(v1 v2)kD  kP ⇡kDkvkD < kvk\nD\nwhere the last inequality follows from Lemma 1. By the Banach Fixed Point theorem, because the Bellman operator is a contraction under D, it has a unique fixed point. ⌅ Theorem 1. If D satisfies s\nD < 1, then there exists v 2 Fv such that ⇧\nD\nT ( ) v = v, and the error to the true value\nfunction is bounded as\nkv v⇤k D  (1 s D ) 1k⇧ D v ⇤ v⇤k D . (4)\nFor constant discount c 2 [0, 1) and constant trace parameter c 2 [0, 1], this bound reduces to the original bound (Tsitsiklis and Van Roy, 1997, Lemma 6):\n(1 s D ) 1  1 c c 1 c .\nProof: Let v be the unique fixed point of ⇧ D T ( ), which exists by Lemma 2.\nkv v⇤k D  kv ⇧ D v ⇤k D + k⇧ D v ⇤ v⇤k D\n= k⇧T( )v ⇧ D v ⇤k D + k⇧ D v ⇤ v⇤k D\n kT( )v v⇤k D + k⇧ D v ⇤ v⇤k D\n= kT( )(v v⇤)k D + k⇧ D v ⇤ v⇤k D = kP ⇡(v v⇤)kD + k⇧Dv⇤ v⇤kD = kP ⇡kDkv v⇤kD + k⇧Dv⇤ v⇤kD = s\nD kv v⇤k D + k⇧ D v ⇤ v⇤k D\nwhere the second inequality is due to k⇧T( )vk D  kT( )vk\nD , the second equality due to T( )v⇤ = v⇤ and the third equality due to T( )v T( )v⇤ = P ⇡(v v⇤) because the r⇡ cancels. By rearranging terms, we get\n(1 s D )kv v⇤k D⇡  k⇧v⇤ v⇤kD⇡\nand since s D < 1, we get the final result.\nFor constant c < 1 and c, because P⇡, = P⇡\ns\nD = kP ⇡kD\n= kD1/2 1X\ni=0\ni c i cP i ⇡ ! c(1 c)P⇡D1/2k2\n c(1 c) 1X\ni=0\ni c i ckD1/2Pi+1⇡ D1/2k2\n= c(1 c) 1X\ni=0\ni c i ckPi+1⇡ kD\n c(1 c) 1X\ni=0\ni c i c\n= c(1 c) 1 c c ⌅\nWe provide generalizations to transition-based trace parameters in the appendix, for the emphasis weighting, and also discuss issues with generalizing to state-based termination for a standard weighting with d⇡. We show that for any transition-based discounting function : S⇥A⇥S ! [0, 1], the above contraction results hold under emphasis weighting. We then provide a general form for an upper bound on kP ⇡kD⇡ for transition-based discounting, based on the contraction properties of two matrices within P ⇡. We further provide an example where the Bellman operator is not a contraction even under the simpler generalization to state-based discounting, and discuss the requirements for the transition-based generalizations to ensure a contraction with weighting d⇡. This further motivates the emphasis weighting as a more flexible scheme for convergence under general setting—both off-policy and transition-based generalization."
  }, {
    "heading": "5.2. Properties of TD algorithms",
    "text": "Using this characterization of P ⇡, we can re-examine previous results for temporal difference algorithms that either used state-based or constant discounts.\nConvergence of Emphatic TD for RL tasks. We can extend previous convergence results for ETD, for learning value functions and action-value functions, for the RL task formalism. For policy evaluation, ETD and ELSTD, the least-squares version of ETD that uses the above defined A and b with D = M, have both been shown to converge with probability one (Yu, 2015). As an important component of this proof is convergence in expectation, which relies on A being positive definite. In particular, for appropriate step-sizes ↵t (see (Yu, 2015)), if A is positive definite, the iterative update is convergent wt+1 = wt + ↵t(b Awt). For the generalization to transition-based discounting, convergence in expectation extends for the emphatic algorithms. We provide these details in the appendix for completeness, with theorem statement and proof in Appendix F and pseudocode in Appendix D.\nConvergence rate of LSTD( ). Tagorti and Scherrer (2015) recently provided convergence rates for LSTD( ) for continuing tasks, for some c < 1. These results can be extended to the episodic setting with the generic treatment of P ⇡ . For example, in (Tagorti and Scherrer, 2015, Lemma 1), which describes the sensitivity of LSTD, the proof extends by replacing the matrix (1 c) cP⇡(I c cP⇡) 1 (which they call M in their proof) with the generalization P\n⇡, resulting instead in the constant 1 1 sD in the bound rather than 1 c c1 c . Further, this generalizes convergence rate results to emphatic LSTD, since M satisfies the required convergence properties, with rates dictated by s\nM\nrather than sDµ for standard LSTD.\nInsights into s D . Though the generalized form enables unified episodic and continuing results, the resulting bound parameter s\nD is more difficult to interpret than for constant c, c. With c increasing to one, the constant 1 c c1 c in the upper bound decreased to one. For c decreasing to zero, the bound also decreases to one. These trends are intuitive, as the problem should be simpler when c is small, and bias should be less when c is close to one. More generally, however, the discount can be small or large for different transitions, making it more difficult to intuit the trend.\nTo gain some intuition for s D , consider a random policy in the taxi domain, with s\nD summarized in Table 1. As c goes to one, s\nD goes to zero and so (1 s D ) 1 goes to one. Some outcomes of note are that 1) hard or soft termination for the pickup results in the exact same s\nD ; 2) for a constant gamma of c = 0.99, the episodic discount had a slightly smaller s\nD ; and 3) increasing c has a much stronger effect than including more terminations. Whereas, when we added random terminations, so that from 1% and 10% of the states, termination occurred on at least one path within 5 steps or even more aggressively on every path within 5 steps, the values of s\nD\nwere similar."
  }, {
    "heading": "1% ALL PATHS 0.978 0.956 0.813 0.304 0.042",
    "text": ""
  }, {
    "heading": "10% ALL PATHS 0.898 0.815 0.468 0.081 0.009",
    "text": ""
  }, {
    "heading": "6. Discussion and conclusion",
    "text": "The goal of this paper is to provide intuition and examples of how to use the RL task formalism. Consequently, to avoid jarring the explanation, technical contributions were not emphasized, and in some cases included only in the appendix. For this reason, we would like to highlight and summarize the technical contributions, which include 1) the introduction of the RL task formalism, and of transition-based discounts; 2) an explicit characterization of the relationship between state-based and transition-based discounting; and 3) generalized approximation bounds, applying to both episodic and continuing tasks; and 4) insights into—and issues with—extending contraction results for both statebased and transition-based discounting. Through intuition from simple examples and fundamental theoretical extensions, this work provides a relatively complete characterization of the RL task formalism, as a foundation for use in practice and theory."
  }, {
    "heading": "Acknowledgements",
    "text": "Thanks to Hado van Hasselt for helpful discussions about transition-based discounting, and probabilistic discounts."
  }],
  "year": 2017,
  "references": [{
    "title": "Neuro-dynamic programming",
    "authors": ["Dimitri P Bertsekas", "John N Tsitsiklis"],
    "venue": "Athena Scientific Press,",
    "year": 1996
  }, {
    "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition",
    "authors": ["Thomas G Dietterich"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2000
  }, {
    "title": "An object-oriented representation for efficient reinforcement learning",
    "authors": ["Carlos Diuk", "Andre Cohen", "Michael L Littman"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2008
  }, {
    "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis",
    "authors": ["Assaf Hallak", "Aviv Tamar", "Rémi Munos", "Shie Mannor"],
    "venue": "CoRR abs/1509.05172,",
    "year": 2015
  }, {
    "title": "The Dependence of Effective Planning",
    "authors": ["Nan Jiang", "Alex Kulesza", "Satinder P Singh", "Richard L Lewis"],
    "venue": "Horizon on Model Accuracy. International Conference on Autonomous Agents and Multiagent Systems,",
    "year": 2015
  }, {
    "title": "Bias-Variance error bounds for temporal difference updates",
    "authors": ["Michael J Kearns", "Satinder P Singh"],
    "venue": "In Annual Conference on Learning Theory,",
    "year": 2000
  }, {
    "title": "Emphatic temporal-difference learning",
    "authors": ["Ashique Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard S Sutton"],
    "venue": "In European Workshop on Reinforcement Learning,",
    "year": 2015
  }, {
    "title": "Multitimescale nexting in a reinforcement learning robot. Adaptive Behavior - Animals, Animats, Software Agents, Robots",
    "authors": ["Joseph Modayil", "Adam White", "Richard S Sutton"],
    "venue": "Adaptive Systems,",
    "year": 2014
  }, {
    "title": "A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
    "authors": ["Hamid Maei Reza", "Richard S Sutton"],
    "venue": "AGI,",
    "year": 2010
  }, {
    "title": "TD models: Modeling the world at a mixture of time scales",
    "authors": ["Richard S Sutton"],
    "venue": "In International Conference on Machine Learning,",
    "year": 1995
  }, {
    "title": "Introduction to reinforcement learning",
    "authors": ["Richard S Sutton", "A G Barto"],
    "year": 1998
  }, {
    "title": "Reinforcement Learning: An Introduction",
    "authors": ["Richard S Sutton", "A G Barto"],
    "venue": "MIT press,",
    "year": 1998
  }, {
    "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
    "authors": ["Richard S Sutton", "Doina Precup", "Satinder Singh"],
    "venue": "Artificial intelligence,",
    "year": 1999
  }, {
    "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
    "authors": ["Richard S Sutton", "Hamid Maei Reza", "Doina Precup", "Shalab Bhatnagar"],
    "venue": "International Conference on Machine Learning,",
    "year": 2009
  }, {
    "title": "A new Q(lambda) with interim forward view and Monte Carlo equivalence",
    "authors": ["Richard S Sutton", "Ashique Rupam Mahmood", "Doina Precup", "Hado van Hasselt"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "An emphatic approach to the problem of off-policy temporal-difference learning",
    "authors": ["Richard S Sutton", "Ashique Rupam Mahmood", "Martha White"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "On the Rate of Convergence and Error Bounds for LSTD",
    "authors": ["Manel Tagorti", "Bruno Scherrer"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "An analysis of temporal-difference learning with function approximation",
    "authors": ["Johnathan N Tsitsiklis", "Benjamin Van Roy"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 1997
  }, {
    "title": "Insights in Reinforcement Learning",
    "authors": ["Hado Philip van Hasselt"],
    "venue": "PhD thesis, Hado van Hasselt,",
    "year": 2011
  }, {
    "title": "True online TD(lambda)",
    "authors": ["Harm van Seijen", "Rich Sutton"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Developing a predictive approach to knowledge",
    "authors": ["Adam White"],
    "venue": "PhD thesis, University of Alberta,",
    "year": 2015
  }, {
    "title": "Least Squares Temporal Difference Methods: An Analysis under General Conditions",
    "authors": ["Huizhen Yu"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 2012
  }, {
    "title": "On convergence of emphatic temporaldifference learning",
    "authors": ["Huizhen Yu"],
    "venue": "In Annual Conference on Learning Theory,",
    "year": 2015
  }],
  "id": "SP:c4d62df11a41875c297b346af7721f1f611eba70",
  "authors": [{
    "name": "Martha White",
    "affiliations": []
  }],
  "abstractText": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.",
  "title": "Unifying Task Specification in Reinforcement Learning"
}