{
  "sections": [{
    "heading": "1. Introduction",
    "text": "This article deals with the estimation of the regression vector β ∈ Rp in the linear regression model y = Xβ + w, where X ∈ Rn×p is a known design matrix with unit Euclidean norm columns, w is the noise vector and y is the observation vector. Throughout this article, we assume that the entries of the noise w are independent, zero mean and Gaussian distributed with variance σ2. We consider the high dimensional and sample starved scenario of n < p or n p where classical techniques like ordinary least squares (OLS) are no longer applicable. This problem of estimating high dimensional vectors in sample starved scenarios is ill-posed even in the absence of noise unless strong structural assumptions are made on X and β. A widely used and practically valid assumption is sparsity. The vector β ∈ Rp is sparse if the support of β given by S = supp(β) = {k : βk 6= 0} has cardinality k0 = card(S) p.\n*Equal contribution 1Department of Electrical Engineering, IIT Madras, India 2Department of Electrical Engineering, IIT Madras, India. Correspondence to: Sreejith Kallummil <sreejith.k.venugopal@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nA number of algorithms like least absolute shrinkage and selection operator (LASSO)(Tropp, 2006; Tibshirani, 1996), Dantzig selector (DS)(Candes & Tao, 2007), subspace pursuit (SP)(Dai & Milenkovic, 2009), OMP (Pati et al., 1993; Mallat & Zhang, 1993; Tropp, 2004; Cai & Wang, 2011), elastic net (Zou & Hastie, 2005) etc. are proposed to efficiently estimate β. Tuning the hyper parameters of aforementioned algorithms to achieve optimal performance require a priori knowledge of signal parameters like sparsity k0 or noise statistics like σ2 etc. Unfortunately, these parameters are rarely known a priori. To the best of our knowledge, no computationally efficient technique to estimate k0 is reported in open literature. However, limited success on the estimation of σ2 has been reported in literature (Dicker, 2014; Fan et al., 2012; Dicker & Erdogdu, 2016; Bayati et al., 2013). However, the performance of these σ2 estimates when used for tuning hyper parameters in LASSO, DS, OMP etc. are largely unknown. Generalised techniques for hyper parameter selection like cross validation (CV)(Arlot et al., 2010), re-sampling (Meinshausen & Bühlmann, 2010) etc. are computationally challenging. Further, CV is reported to have poor variable selection behaviour(Chichignoud et al., 2016; Arlot et al., 2010). Indeed, algorithms that are oblivious to signal and noise statistics are also proposed in literature. This include algorithms inspired or related to LASSO like square root LASSO(Belloni et al., 2011), AV∞ (Chichignoud et al., 2016), approximate message passing (Mousavi et al., 2013; Bayati et al., 2013) etc. and ridge regression inspired techniques like least squares adaptive thresholding (LAT), ridge adaptive thresholding (RAT)(Wang et al., 2016) etc. However, most of existing signal and noise statistics oblivious sparse recovery techniques have only large sample performance guarantees. Further, many of these techniques assume that design matrix X is sampled from a random ensemble, a condition which is rarely satisfied in practice."
  }, {
    "heading": "1.1. Contributions of this paper",
    "text": "This article present a novel technique called residual ratio thresholding (RRT) for finding a “good” estimate of support S from the data dependent/adaptive sequence of supports generated by OMP. RRT is analytically shown to accomplish exact support recovery, (i.e., identifying S) under the same finite sample and deterministic constraints on X like\nrestricted isometry constants (RIC) or mutual coherence required by OMP with a priori knowledge of k0 or σ2. However, the signal to noise ratio (SNR=‖Xβ‖22/nσ2) required for support recovery using RRT is slightly higher than that of OMP with a priori knowledge of k0 or σ2. This extra SNR requirement is shown to decrease with the increase in sample size n. RRT and OMP with a priori knowledge of k0 or σ2 are shown to be equivalent as n → ∞ in terms of the SNR required for support recovery. RRT involves a tuning parameter α that can be set independent of ambient SNR or noise statistics. The hyper parameter α in RRT have an interesting semantic interpretation of being the high SNR upper bound on support recovery error. Also RRT is asymptotically tuning free in the sense that a very wide range of α deliver similar performances as n → ∞. Numerical simulations indicate that RRT can deliver a highly competitive performance when compared to OMP having a priori knowledge of k0 or σ2, OMP with k0 estimated using CV and the recently proposed LAT algorithm. Further, RRT also delivered a highly competitive performance when applied to identify outliers in real data sets, an increasingly popular application of sparse estimation algorithms(Mitra et al., 2010; 2013).\nThe remainder of this article is organised as follows. In section 2 we discuss OMP algorithm. RRT algorithm is presented in Section 3. Section 4 presents theoretical performance guarantees for RRT. Section 5 presents numerical simulation results. All the proofs are provided in the supplementary material.\n1.2. Notations used ‖x‖q = ( p∑ k=1 |xk|q ) 1 q is the lq norm of x ∈ Rp. 0n is the n × 1 zero vector and In is the n × n identity matrix. span(X) is the column space of X. X† = (XTX)−1XT is the Moore-Penrose pseudo inverse of X. XJ denotes the sub-matrix of X formed using the columns indexed by J . N (u,C) represents a Gaussian random vector (R.V) with mean u and covariance matrix C. B(a, b) denotes a Beta R.V with parameters a and b. a ∼ b implies that a and b are identically distributed. [p] represents the floor operator. φ represents the null set. For any two sets J1 and J2, J1/J2 denotes the set difference. a\nP→ b represents the convergence of R.V a to R.V b in probability."
  }, {
    "heading": "2. Orthogonal Matching Pursuit (OMP)",
    "text": "OMP (Algorithm 1) starts with a null support estimate and in each iteration it adds that column index to the current support which is the most correlated with the previous residual rk−1, i.e., tk = arg max\nj |XTj rk−1|. Then a\nLS estimate of β restricted to the current support Skomp is\nAlgorithm 1 Orthogonal matching pursuit Input: Observation y, matrix X Initialize Somp0 = φ. k = 1 and residual r0 = y repeat\nIdentify the next column tk = arg max j |XTj rk−1| Expand current support Skomp = Sk−1omp ∪ tk Restricted LS estimate: β̂Skomp = X † Skomp y.\nβ̂{1,...,p}/Skomp = 0p−k.\nUpdate residual: rk = y −Xβ̂ = (In −Pk)y. Increment k ← k + 1.\nuntil stopping condition (SC) is true Output: Support estimate Ŝ = Skomp. Vector estimate β̂\ncomputed as an intermediate estimate of β and this estimate is used to update the residual. Note that Pk in Algorithm 1 refers to XSkompX † Skomp , the projection matrix onto span(XSkomp). Since the residual r k is orthogonal to span(XSkomp), X T j r\nk = 0 for all j ∈ Skomp. Consequently, tk+1 /∈ Skomp, i.e., the same index will not be selected in two different iterations. Hence, Sk+1omp ⊃ Skomp, i.e. the support sequence is monotonically increasing. The monotonicity of Skomp in turn implies that the residual norm ‖rk‖2 is a non increasing function of k, i.e, ‖rk+1‖2 ≤ ‖rk‖2.\nMost of the theoretical properties of OMP are derived assuming a priori knowledge of true sparsity level k0 in which case OMP stops after exactly k0 iterations(Tropp, 2004; Wang, 2015). When k0 is not known, one has to rely on stopping conditions (SC) based on the properties of the residual rk as k varies. For example, one can stop OMP iterations once the residual power is too low compared to the expected noise power. Mathematically, when the noise w is l2 bounded, i.e., ‖w‖2 ≤ 2 for some a priori known 2, then OMP can be stopped if ‖rk‖2 ≤ 2. For a Gaussian noise vector w ∼ N (0n, σ2In), σ = σ √ n+ 2 √ n log(n) satisfies(Cai & Wang, 2011)\nP(‖w‖2 ≤ σ) ≥ 1− 1\nn , (1)\ni.e., Gaussian noise is l2 bounded with a very high probability. Consequently, one can stop OMP iterations in Gaussian noise once ‖rk‖2 ≤ σ .\nA number of deterministic recovery guarantees are proposed for OMP. Among these guarantees the conditions based on RIC are the most popular. RIC of order j denoted by δj is defined as the smallest value of δ such that\n(1− δ)‖b‖22 ≤ ‖Xb‖22 ≤ (1 + δ)‖b‖22 (2)\nhold true for all b ∈ Rp with ‖b‖0 = card(supp(b)) ≤ j. A smaller value of δj implies that X act as a near orthogonal\nmatrix for all j sparse vectors b. Such a situation is ideal for the recovery of a j-sparse vector b using any sparse recovery technique. The latest RIC based support recovery guarantee using OMP is given in Lemma 1(Liu et al., 2017).\nLemma 1. OMP with k0 iterations or SC ‖rk‖2 ≤ ‖w‖2 can recover any k0 sparse vector β provided that δk0+1 < 1/ √ k0 + 1 and ‖w‖2 ≤ omp =\nβmin √ 1− δk0+1  1−√k0 + 1δk0+1 1 + √ 1− δ2k0+1 − √ k0 + 1δk0+1 . Since P(‖w‖2 < σ) ≥ 1− 1/n when w ∼ N (0n, σ2In), it follows from Lemma 1 that OMP with k0 iterations or SC ‖rk‖2 ≤ σ can recover any k0-sparse vector β with probability greater than 1 − 1/n provided that δk0+1 < 1/ √ k0 + 1 and σ ≤ omp. Lemma 1 implies that OMP with a priori knowledge of k0 or σ2 can recover support S once the matrix satisfies the regularity condition δk0+1 < 1/ √ k0 + 1 and the SNR is high. It is also known that this RIC condition is worst case necessary. Consequently, Lemma 1 is one of the best deterministic guarantee for OMP available in literature. Note that the mutual incoherence condition given by µX = max\nj 6=k |XTj Xk| < 1/(2k0 − 1)\nalso ensures exact support recovery at high SNR. Note that the a priori knowledge of k0 or σ2 required to materialise the recovery guarantees in Lemma 1 are not available in practical problems. Further, k0 and σ2 are very difficult to estimate. This motivates the proposed RRT algorithm which does not require a priori knowledge of k0 or σ2."
  }, {
    "heading": "3. Residual Ratio Thresholding (RRT)",
    "text": "RRT is a novel signal and noise statistics oblivious technique to estimate the support S based on the behaviour of the residual ratio statistic RR(k) = ‖rk‖2/‖rk−1‖2 as k increases from k = 1 to a predefined value k = kmax > k0. As aforementioned, identifying the support using the behaviour of ‖rk‖2 requires a priori knowledge of σ2. However, as we will show in this section, support detection using RR(k) does not require a priori knowledge of σ2. Since the residual norms are non negative and non increasing, RR(k) always satisfy 0 ≤ RR(k) ≤ 1."
  }, {
    "heading": "3.1. Minimal superset and implications",
    "text": "Consider running kmax > k0 iterations of OMP and let {Skomp}kmaxk=1 be the support sequence generated by OMP. Recall that Skomp is monotonically increasing.\nDefinition 1:- The minimal superset in the OMP support sequence {Skomp}kmaxk=1 is given by Skminomp , where kmin = min({k : S ⊆ Skomp}). When the set {k : S ⊆ Skomp} = φ, we set kmin =∞ and Skminomp = φ.\nIn words, minimal superset is the smallest superset of support S present in a particular realization of the support estimate sequence {Skomp}kmaxk=1 . Note that both kmin and Skminomp are unobservable random variables. Since card(Skomp) = k, Skomp for k < k0 cannot satisfy S ⊆ Skomp and hence kmin ≥ k0. Further, the monotonicity of Skomp implies that S ⊂ Skomp for all k ≥ kmin. Case 1:- When kmin = k0, then Sk0omp = S and Skomp ⊃ S for k ≥ k0, i.e., S is present in the solution path. Further, when kmin = k0, it is true that Skomp ⊆ S for k ≤ k0. Case 2:- When k0 < kmin ≤ kmax, then Skomp 6= S for all k and Sompk ⊃ S for k ≥ kmin, i.e., S is not present in the solution path. However, a superset of S is present. Case 3:- When kmin = ∞, then Skomp 6⊇ S for all k, i.e., neither S nor a superset of S is present in {Skomp}kmaxk=1 . To summarize, exact support recovery using any OMP based scheme including the signal and noise statistics aware schemes is possible only if kmin = k0. Whenever kmin > k0, it is possible to estimate true support S without having any false negatives. However, one then has to suffer from false positives. When kmin =∞, any support in {Skomp}kmaxk=1 has to suffer from false negatives and all supports Skomp for k > k0 − 1 has to suffer from false positives also. Note that the matrix and SNR conditions required for exact support recovery in Lemma 1 automatically implies that kmin = k0. We formulate the proposed RRT scheme assuming that kmin = k0.\n3.2. Behaviour of RR(k0)\nNext we consider the behaviour of residual ratio statistic at the k0 iteration, i.e., RR(k0) = ‖rk0‖2/‖rk0−1‖2 under the assumption that ‖w‖2 ≤ omp and δk0+1 < 1/ √ k0 + 1 which ensures kmin = k0 and Skomp ⊆ S for all k ≤ k0. Since Xβ = XSβS ∈ span(XS), (In − Pk)Xβ 6= 0n if S 6⊆ Skomp and (In − Pk)Xβ = 0n if S ⊆ Skomp. This along with the monotonicity of Skomp implies the following. (In − Pk)Xβ 6= 0n for k < kmin = k0 and (In − Pk)Xβ = 0n for k ≥ kmin = k0. Thus rk = (In − Pk)y = (In − Pk)XSβS + (In − Pk)w for k < kmin = k0, whereas, rk = (In − Pk)w for k ≥ kmin = k0. Consequently, at k = k0, the numerator ‖rk0‖2 ofRR(k0) contains contribution only from the noise term ‖(In−Pk0)w‖2, whereas, the denominator ‖rk0−1‖2 in RR(k0) contain contributions from both the signal term i.e., (In−Pk)XSβS and the noise term (In−Pk)w. This behaviour of RR(k0) along with the fact that ‖w‖2\nP→ 0 as σ2 → 0 implies the following theorem.\nTheorem 1. Assume that the matrix X satisfies the RIC constraint δk0+1 < 1/ √ k0 + 1 and kmax > k0. Then a). RR(kmin) P→ 0 as σ2 → 0. b). lim σ2→0 P(kmin = k0) = 1.\nAlgorithm 2 Residual ratio thresholding Input: Observation y, matrix X Step 1: Run kmax iterations of OMP. Step 2: Compute RR(k) for k = 1, . . . , kmax. Step 3: Estimate kRRT = max{k : RR(k) ≤ ΓαRRT (k)} Output: Support estimate Ŝ = SkRRTomp . Vector estimate β̂(SkRRTomp ) = X\n† SkRRTomp y, β̂({1, . . . , p}/SkRRTomp ) = 0p−kRRT .\n3.3. Behaviour of RR(k) for k > kmin\nNext we discuss the behaviour of RR(k) for k > kmin. By the definition of kmin we have S ⊆ Skomp which implies that rk = (In − Pk)w for k ≥ kmin. The absence of signal terms in numerator and the denominator of RR(k) = ‖(In−Pk)w‖2‖(In−Pk−1)w‖2 for k > kmin implies that even when ‖w‖2 → 0 or σ2 → 0, RR(k) for k > kmin does not converge to zero. This behaviour of RR(k) for k > kmin is captured in Theorem 2 where we provide explicit σ2 or SNR independent lower bounds on RR(k) for k > kmin.\nTheorem 2. Let Fa,b(x) denotes the cumulative distribution function of a B(a, b) random variable. Then ∀σ2 > 0,\nΓαRRT (k) = √ F−1n−k\n2 ,0.5\n( α\nkmax(p− k + 1)\n) satisfies\nP(RR(k) > ΓαRRT (k),∀k > kmin) ≥ 1− α. (3)\nTheorem 2 states that the residual ratio statistic RR(k) for k > kmin is lower bounded by the deterministic sequence {ΓαRRT (k)} kmax k=kmin+1\nwith a high probability (for small values of α). Please note that kmin is itself a R.V. Note that the sequence ΓαRRT (k) is dependent only on the matrix dimensions n and p. Further, Theorem 2 does not make any assumptions on the noise variance σ2 or the design matrix X. Theorem 2 is extremely non trivial considering the fact that the support estimate sequence {Skomp} kmax k=1 produced by OMP is adaptive and data dependent.\nLemma 2. The following important properties of ΓαRRT (k) are direct consequences of the monotonicity of CDF and the fact that a Beta R.V take values only in [0, 1]. 1). ΓαRRT (k) is defined only in the interval α ∈ [0, kmax(p− k + 1)]. 2). 0 ≤ ΓαRRT (k) ≤ 1. 3). ΓαRRT (k) is a monotonically increasing function of α. 4). ΓαRRT (k) = 0 when α = 0 and Γ α RRT (k) = 1 when α = kmax(p− k + 1)."
  }, {
    "heading": "3.4. Residual ratio thresholding framework",
    "text": "From Theorem 1, it is clear that P(kmin = k0) and\nP(Sompk0 = S) increases with increasing SNR (or decreasing σ2), whereas, RR(kmin) decreases to zero with increasing SNR. At the same time, for small values of α like α = 0.01, RR(k) for k > kmin is lower bounded by ΓαRRT (k) with a very high probability at all SNR. Hence, finding the last index k such that RR(k) ≤ ΓαRRT (k), i.e., kRRT = max{k : RR(k) ≤ ΓαRRT (k)} gives k0 and equivalently Sk0omp = S with a probability increasing with increasing SNR. This motivates the proposed signal and noise statistics oblivious RRT algorithm presented in Algorithm 2. Remark 1. An important aspect regarding the RRT in Algorithm 2 is the choice of kRRT when the set {k : RR(k) ≤ ΓαRRT (k)} = φ. This situation happens only at very low SNR. When {k : RR(k) ≤ ΓαRRT (k)} = φ for a given value of α, we increase the value of α to the smallest value αnew > α such that {k : RR(k) ≤ ΓαnewRRT (k)} 6= φ. Mathematically, we set kRRT = max{k : RR(k) < ΓαnewRRT (k)}, where αnew = min\na>α {a : {k : RR(k) ≤ ΓαRRT (k)} 6= φ}.\nSince α = p kmax gives ΓαRRT (1) = 1 and RR(1) ≤ 1, a value of αnew ≤ pkmax always exists. αnew can be easily computed by first pre-computing {ΓaRRT (k)} kmax k=1 for say 100 prefixed values of a in the interval (α, pkmax]. Remark 2. RRT requires performing kmax iterations of OMP. All the quantities required for RRT including RR(k) and the final estimates can be computed while performing these kmax iterations itself. Consequently, RRT has complexity O(kmaxnp). As we will see later, a good choice of kmax is kmax = [0.5(n + 1)] which results in a complexity order O(n2p). This complexity is approximately n/k0 times higher than the O(npk0) complexity of OMP when k0 or σ2 are known a priori. This is the computational cost being paid for not knowing k0 or σ2 a priori. In contrast, L fold CV requires running (1− 1/L)n iterations of OMP L times resulting in a O(L(1 − 1/L)n2p) = O(Ln2p) complexity, i.e., RRT is L times computationally less complex than CV. Remark 3. RRT algorithm is developed only assuming that the support sequence generated by the sparse recovery algorithm is monotonically increasing. Apart from OMP, algorithms such as orthogonal least squares(Wen et al., 2017) and OMP with thresholding(Yang & de Hoog, 2015) also produce monotonic support sequences. RRT principle can be directly applied to operate these algorithms in a signal and noise statistics oblivious fashion."
  }, {
    "heading": "4. Analytical Results for RRT",
    "text": "In this section we present support recovery guarantees for RRT and compare it with the results available for OMP with a priori knowledge of k0 or σ2. The first result in this section deals with the finite sample and finite SNR performance for RRT.\nTheorem 3. Let kmax ≥ k0 and suppose that the matrix X satisfies δk0+1 <\n1√ k0+1 . Then RRT can recover the true support S with probability greater than 1− 1/n− α provided that σ < min( omp, rrt), where\nrrt = ΓαRRT (k0)\n√ 1− δk0βmin\n1 + ΓαRRT (k0) . (4)\nTheorem 3 implies that RRT can identify the support S at a higher SNR or lower noise level than that required by OMP with a priori knowledge of k0 and σ2. For small values of α like α = 0.01, the probability of exact support recovery, i.e., 1− α− 1/n is similar to that of the 1− 1/n probability of exact support recovery in Lemma 1. Also please note that the RRT framework does not impose any extra conditions on the design matrix X. Consequently, the only appreciable difference between RRT and OMP with a priori knowledge of k0 and σ2 is in the extra SNR required by RRT which is quantified next using the metric extra = omp/ rrt. Note that the larger the value of extra, larger should be the SNR or equivalently smaller should be the noise level required for RRT to accomplish exact support recovery. Substituting the values of omp and rrt and using the bound δk0 ≤ δk0+1 gives\nextra ≤ 1 + 1ΓαRRT (k0)\n1 +\n√ 1−δ2k0+1\n1− √ k0+1δk0+1\n. (5)\nNote that\n√ 1−δ2k0+1\n1− √ k0+1δk0+1\n= (\n1−δk0+1 1− √ k0+1δk0+1 )√ 1+δk0+1 1−δk0+1\n≥ 1. Consequently,\nextra ≤ 0.5 ( 1 + 1\nΓαRRT (k0)\n) . (6)\nSince 0 ≤ ΓαRRT (k0) ≤ 1, it follows that 0.5 (\n1 + 1ΓαRRT (k0)\n) is always greater than or equal to one.\nHowever, extra decreases with the increase in ΓαRRT (k0). In particular, when ΓαRRT (k0) = 1, there is no extra SNR requirement. Remark 4. RRT algorithm involves two hyper parameters viz. kmax and α. Exact support recovery using RRT requires only that kmax ≥ k0. However, k0 is an unknown quantity. In our numerical simulations, we set kmax = min(p, [0.5(rank(X) + 1)]). This choice is motivated by the facts that k0 < [0.5(rank(X)+1)] is a necessary condition for exact support recovery using any sparse estimation algorithm(Elad, 2010) when n < p and min(n, p) is the maximum possible number of iterations in OMP. Since evaluating rank(X) requires extra computations, one can always use rank(X) ≤ n to set kmax = min(p, [0.5(n+1)]). Please note that this choice of kmax is independent of the operating SNR, design matrix and the vector to be estimated and the user is not required to tune this parameter. Hence, α is the only user specified hyper parameter in RRT algorithm."
  }, {
    "heading": "4.1. Large sample behaviour of RRT",
    "text": "Next we discuss the behaviour of RRT as n→∞. From (6), it is clear that the extra SNR required for support recovery using RRT decreases with increasing ΓαRRT (k0). However, by Lemma 2 increasing ΓαRRT (k0) requires an increase in the value of α. However, increasing α decreases the probability of support recovery given by 1 − α − 1/n. In other words, one cannot have exact support recovery using RRT at lower SNR without increasing the probability of error in the process. An answer to this conundrum is available in the large sample regime where it is possible to achieve both α ≈ 0 and ΓαRRT (k0) ≈ 1, i.e., no extra SNR requirement and no decrease in probability of support recovery. The following theorem states the conditions required for ΓαRRT (k0) ≈ 1 for large values of n. Theorem 4. Define klim = lim\nn→∞ k0/n, plim =\nlim n→∞ log(p)/n and αlim = lim n→∞ log(α)/n. Let\nkmax = min(p, [0.5(n + 1)]). Then ΓαRRT (k0) =√ F−1n−k0\n2 ,0.5\n( α\nkmax(p− k0 + 1)\n) satisfies the following\nasymptotic limits. Case 1:-). lim\nn→∞ ΓαRRT (k0) = 1, whenever klim < 0.5,\nplim = 0 and αlim = 0. Case 2:-). 0 < lim\nn→∞ ΓαRRT (k0) < 1 if klim < 0.5,\nαlim = 0 and plim > 0. In particular, lim n→∞ ΓαRRT (k0) = exp( −plim1−klim ). Case 3:- lim\nn→∞ ΓαRRT (k0) = 0 if klim < 0.5, αlim = 0 and\nplim =∞.\nTheorem 4 states that all choices of (n, p, k0) satisfying plim = 0 and klim < 0.5 can result in lim\nn→∞ ΓαRRT (k0) = 1\nprovided that the parameter α satisfies αlim = 0. Note that αlim = 0 for a wide variety of α including α = constant, α = 1/nδ for some δ > 0, α = 1/ log(n) etc. It is interesting to see which (n, p, k0) scenario gives plim = 0 and klim < 0.5. Note that exact recovery in n < p scenario is possible only if k0 ≤ [0.5(n + 1)]. Thus, the assumption klim < 0.5 will be satisfied in all interesting problem scenarios.\nRegime 1:- lim n→∞ ΓαRRT (k0) = 1 in low dimensional regression problems with p fixed and n → ∞ or all (n, p) → (∞,∞) with lim\nn→∞ p/n ≤ 1.\nRegime 2:- lim n→∞ ΓαRRT (k0) = 1 in high dimensional case with p increases sub exponentially with n as exp(nδ) for some δ < 1 or p increases polynomially w.r.t n, i.e., p = nδ for some δ > 1. In both cases, plim = lim n→∞ log(nδ)/n = 0 and plim = lim n→∞ log(exp(nδ))/n = 0. Regime 3:- lim n→∞ ΓαRRT (k0) = 1 in the extreme high dimensional case where (n, p, k0) → (∞,∞,∞) satisfy-\ning n ≥ ck0 log(p) for some constant c > 0. Here plim = lim\nn→∞ log(p)/n ≤ lim n→∞\n1\nck0 = 0 and klim =\nlim n→∞ 1/c log(p) = 0. Note that the sampling regime n ≈ 2k0 log(p) is the best known asymptotic guarantee available for OMP(Fletcher & Rangan, 2012). Regime 4:- Consider a sampling regime where (n, p) → (∞,∞) such that k0 is fixed and n = ck0 log(p), i.e., p is exponentially increasing with n. Here plim = 1/(ck0) and klim = 0. Consequently, lim\nn→∞ ΓαRRT (k0) = exp ( −1 ck0 ) <\n1. A good example of this sampling regime is (Tropp & Gilbert, 2007) where it was shown that OMP can recover a (not every) particular k0 dimensional signal from n random measurements (in noiseless case) when n = ck0 log(p). Note that c ≤ 20 for all k0 and c ≈ 4 for large k0. Even if we assume that only n = 4k0 log(p) measurements are sufficient for recovering a k0 sparse signal, we have lim n→∞\nΓαRRT (k0) = exp(−0.125) = 0.9512 for k0 = 5 (i.e., extra ≤ 1.0257) and lim\nn→∞ ΓαRRT (k0) = exp(−0.125) =\n0.9753 for k0 = 10 (i.e., extra ≤ 1.0127).\nNote that ΓαRRT (k0)→ 1 as n→∞ implies that extra → 1 and min( omp, rrt)→ 1. This asymptotic behaviour of ΓαRRT (k0) and extra imply the large sample consistency of RRT as stated in the following theorem.\nTheorem 5. Suppose that the sample size n → ∞ such that the matrix X satisfies δk0+1 <\n1√ k0+1 , σ ≤ omp and plim = 0. Then, a). OMP running k0 iterations and OMP with SC ‖rk‖2 ≤ σ are large sample consistent, i.e.. lim\nn→∞ P(Ŝ = S) = 1.\nb). RRT with hyper parameter α satisfying lim n→∞ α = 0 and αlim = 0 is also large sample consistent.\nTheorem 5 implies that at large sample sizes, RRT can accomplish exact support recovery under the same SNR and matrix conditions required by OMP with a priori knowledge of k0 or σ2. Theorem 5 has a very important corollary. Remark 5. Theorem 1 implies that all choices of α satisfying α→ 0 and αlim = 0 deliver similar performances as n→ ∞. Note that the range of adaptations satisfying α → 0 and αlim = 0 include α = 1/ log(n), α = 1/nδ for δ > 0 etc. Since a very wide range of tuning parameters deliver similar results as n → ∞, RRT is in fact asymptotically tuning free. Remark 6. Based on the large sample analysis of RRT, one can make the following guidelines on the choice of α. When the sample size n is large, one can choose α as a function of n that satisfies both lim\nn→∞ α = 0 and αlim = 0. Also since\nthe support recovery guarantees are of the form 1−1/n−α, it does not make sense to choose a value of α that decays to zero faster than 1/n. Hence, it is preferable to choose values of α that decreases to zero slower than 1/n like\nα = 1/ log(n), α = 1/ √ n etc."
  }, {
    "heading": "4.2. A high SNR operational interpretation of α",
    "text": "Having discussed the large sample behaviour of RRT, we next discuss the finite sample and high SNR behaviour of RRT. Define the events support recovery error E = {Ŝ 6= S} and false positive F = card(Ŝ/S) > 0 and missed discovery or false negativeM = card(S/Ŝ) > 0. The following theorem characterizes the likelihood of these events as SNR increases to infinity or σ2 → 0.\nTheorem 6. Let kmax > k0 and the matrix X satisfies δk0+1 < 1/ √ k0 + 1. Then, a). lim σ2→0\nP(M) = 0. b). lim\nσ2→0 P(E) = lim σ2→0 P(F) ≤ α.\nTheorem 6 states that when the matrix X allows for exact support recovery in the noiseless or low noise situation, RRT will not suffer from missed discoveries. Under such favourable conditions, α is a high SNR upper bound on both the probability of error and the probability of false positives. Please note that such explicit characterization of hyper parameters are not available for hyper parameters in Square root LASSO, RAT, LAT etc."
  }, {
    "heading": "5. Numerical Simulations",
    "text": "In this section, we provide extensive numerical simulations comparing the performance of RRT with state of art sparse recovery techniques. In particular, we compare the performance of RRT with OMP with k0 estimated using five fold CV and the least squares adaptive thresholding (LAT) proposed in (Wang et al., 2016). In synthetic data sets, we also compare RRT with OMP running exactly k0 itera-\ntions and OMP with SC ‖rk‖2 ≤ σ √ n+ 2 √ n log(n)(Cai & Wang, 2011). These algorithms are denoted in Figures 1-4 by “CV”, “LAT”, “OMP1” and “OMP2” respectively. RRT1 and RRT2 represent RRT with parameter α set to α = 1/ log(n) and α = 1/ √ n respectively. By Theorem 5, RRT1 and RRT2 are large sample consistent."
  }, {
    "heading": "5.1. Synthetic data sets",
    "text": "The synthetic data sets are generated as follows. We consider two models for the matrix X. Model 1 sample each entry of the design matrix X ∈ Rn×p independently according toN (0, 1). Matrix X in Model 2 is formed by concatenating In with a n× n Hadamard matrix Hn, i.e., X = [In,Hn]. This matrix guarantee exact support recovery using OMP at high SNR once k0 < 1+ √ n\n2 (Elad, 2010). The columns of X in both models are normalised to have unit l2-norm. Based on the choice of X and support S , we conduct 4 experiments. Experiments 1-2 involve matrix of model 1 with (n, p) given\nby (200, 300) and (200, 900) respectively with support S sampled randomly from the set {1, . . . , p}. Experiment 3 and 4 involve matrix of model 2 with (n = 128, p = 256). For experiment 3, support S is sampled randomly from the set {1, . . . , p}, whereas, in experiment 4, support S is fixed at {1, 2, . . . , k0}. The noise w is sampled according toN (0n, σ2In) with σ2 = 1. The non zero entries of β are randomly assigned βj = ±1. Subsequently, these entries are scaled to achieve SNR = ‖Xβ‖22/n = 3. The number of non zero entries k0 in all experiments are fixed at six. We compare the algorithms in terms of the l2 error, the number of false positives and the number of false negatives produced in 100 runs of each experiment.\nFrom the box plots given in Figures 1-4, it is clear that RRT with both values of α perform very similar to OMP1. They differ only in one run of experiment 3 where RRT1 and RRT2 suffer from a false negative. Further, RRT1 and RRT2 outperform CV and LAT in all the four experiments in terms of all the three metrics considered for evaluation. This is primarily because LAT and CV are more prone to make false positives, whereas RRT1 and RRT2 does not report any false positives. OMP2 consistently made false negatives which explains its poor performance in terms of l2 error. We have observed that once the SNR is made slightly higher, OMP2 delivers a performance similar to OMP1. Also note that RRT with two significantly different choices of α viz. α = 1/ √ n and α = 1/ log(n) delivered similar performances. This observation is in agreement with the claim of asymptotic tuning freeness made in Remark 5. Similar trends are also visible in the simulation results presented in supplementary materials."
  }, {
    "heading": "5.2. Outlier detection in real data sets",
    "text": "We next consider the application of sparse estimation techniques including RRT to identify outliers in low dimensional or full column rank (i.e., n > p) real life data sets, an approach first considered in (Mitra et al., 2010; 2013). Consider a robust regression model of the form y = Xβ + w + gout with usual interpretations for X, β and w. The extra term gout ∈ Rn represents the gross errors in the regression model that cannot be modelled using the distributional assumptions on w. Outlier detection problem in linear regression refers to the identification of the support Sg = supp(gout). Since X has full rank, one can always annihilate the signal component Xβ by projecting onto a subspace orthogonal to span(X). This will result in a simple linear regression model of the form given by\nỹ = (In −XX†)y = (In −XX†)gout + (In −XX†)w, (7) i.e., identifying Sg in robust regression is equivalent to a sparse support identification problem in linear regression. Even though this is a regression problem with n observa-\ntions and n variables, the design matrix (In−XX†) in (7) is rank deficient (i.e., rank(In−XX†) = n−rank(X) < n). Hence, classical techniques based on LS are not useful for identifying Sg. Since card(Sg) and variance of w are unknown, we only consider the application of RRT, OMP with CV and LAT in detecting Sg . We consider four widely studied real life data sets and compare the outliers identified by these algorithms with the existing and widely replicated studies on these data sets. More details on these data sets are given in the supplementary materials. The outliers detected by the aforementioned algorithms and outliers reported in existing literature are tabulated in TABLE 1.\nAmong the four data sets considered, outliers detected by RRT and existing results are in consensus in two data sets viz. Stack loss and Stars data sets. In AR2000 data set, RRT identifies all the outliers. However, RRT also include observations 14 and 50 as outliers. These identifications can be potential false positives. In Brain and Body Weight data set, RRT agrees with the existing results in 4 observations. However, RRT misses two observations viz. 14 and 17 which are claimed to be outliers by existing results. LAT agrees with RRT in all data sets except the stack loss data set where it missed outlier indices 1 and 3. CV correctly identified all the outliers identified by other algorithms in all four data sets. However, it made lot of false positives in three data sets. To summarize, among all the three algorithms considered, RRT delivered an outlier detection performance which is the most similar to the results reported in literature."
  }, {
    "heading": "6. Conclusions",
    "text": "This article proposed a novel signal and noise statistics independent sparse recovery technique based on OMP called residual ratio thresholding and derived finite and large sample guarantees for the same. Numerical simulations in real and synthetic data sets demonstrates a highly competitive performance of RRT when compared to OMP with a priori knowledge of signal and noise statistics. The RRT technique developed in this article can be used to operate sparse recovery techniques that produce a monotonic sequence of support estimates in a signal and noise statistics oblivious fashion. However, the support estimate sequence generated by algorithms like LASSO, DS, SP etc. are not monotonic in nature. Hence, extending the concept of RRT to operate sparse estimation techniques that produce non monotonic support sequence in a signal and noise statistics oblivious fashion is an interesting direction of future research."
  }],
  "year": 2018,
  "references": [{
    "title": "A survey of cross-validation procedures for model selection",
    "authors": ["S. Arlot", "A Celisse"],
    "venue": "Statistics surveys,",
    "year": 2010
  }, {
    "title": "Robust diagnostic regression analysis",
    "authors": ["A. Atkinson", "M. Riani"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Estimating lasso risk and noise level",
    "authors": ["M. Bayati", "M.A. Erdogdu", "A. Montanari"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Square-root lasso: pivotal recovery of sparse signals via conic programming",
    "authors": ["A. Belloni", "V. Chernozhukov", "L. Wang"],
    "year": 2011
  }, {
    "title": "Orthogonal matching pursuit for sparse signal recovery with noise",
    "authors": ["T.T. Cai", "L. Wang"],
    "venue": "IEEE Transactions on Information theory,",
    "year": 2011
  }, {
    "title": "The Dantzig selector: Statistical estimation when p is much larger than n",
    "authors": ["E. Candes", "T. Tao"],
    "venue": "The Annals of Statistics,",
    "year": 2007
  }, {
    "title": "A practical scheme and fast algorithm to tune the lasso with optimality guarantees",
    "authors": ["M. Chichignoud", "J. Lederer", "M.J. Wainwright"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Subspace pursuit for compressive sensing signal reconstruction",
    "authors": ["W. Dai", "O. Milenkovic"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2009
  }, {
    "title": "Variance estimation in high-dimensional linear models",
    "authors": ["L.H. Dicker"],
    "venue": "Biometrika, 101(2):269–284,",
    "year": 2014
  }, {
    "title": "Maximum likelihood for variance estimation in high-dimensional linear models",
    "authors": ["L.H. Dicker", "M.A. Erdogdu"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing",
    "authors": ["M. Elad"],
    "year": 2010
  }, {
    "title": "Variance estimation using refitted cross-validation in ultra high dimensional regression",
    "authors": ["J. Fan", "S. Guo", "N. Hao"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2012
  }, {
    "title": "Orthogonal matching pursuit: A Brownian motion analysis",
    "authors": ["A.K. Fletcher", "S. Rangan"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2012
  }, {
    "title": "Some new results about sufficient conditions for exact support recovery of sparse signals via orthogonal matching pursuit",
    "authors": ["C. Liu", "Y. Fang", "J. Liu"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }, {
    "title": "Matching pursuits with timefrequency dictionaries",
    "authors": ["S.G. Mallat", "Z. Zhang"],
    "venue": "IEEE Transactions on signal processing,",
    "year": 1993
  }, {
    "title": "Stability selection",
    "authors": ["N. Meinshausen", "P. Bühlmann"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2010
  }, {
    "title": "Robust regression using sparse learning for high dimensional parameter estimation problems",
    "authors": ["K. Mitra", "A. Veeraraghavan", "R. Chellappa"],
    "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
    "year": 2010
  }, {
    "title": "Analysis of sparse regularization based robust regression approaches",
    "authors": ["K. Mitra", "A. Veeraraghavan", "R. Chellappa"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2013
  }, {
    "title": "Parameterless optimal approximate message passing",
    "authors": ["A. Mousavi", "A. Maleki", "R.G. Baraniuk"],
    "venue": "arXiv preprint arXiv:1311.0035,",
    "year": 2013
  }, {
    "title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition",
    "authors": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"],
    "venue": "In Signals, Systems and Computers,",
    "year": 1993
  }, {
    "title": "Robust regression and outlier detection, volume 589",
    "authors": ["P.J. Rousseeuw", "A.M. Leroy"],
    "venue": "John wiley & sons,",
    "year": 2005
  }, {
    "title": "Regression shrinkage and selection via the lasso",
    "authors": ["R. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp",
    "year": 1996
  }, {
    "title": "Greed is good: Algorithmic results for sparse approximation",
    "authors": ["J.A. Tropp"],
    "venue": "IEEE Transactions on Information theory,",
    "year": 2004
  }, {
    "title": "Just relax: Convex programming methods for identifying sparse signals in noise",
    "authors": ["J.A. Tropp"],
    "venue": "IEEE transactions on information theory,",
    "year": 2006
  }, {
    "title": "Signal recovery from random measurements via orthogonal matching pursuit",
    "authors": ["J.A. Tropp", "A.C. Gilbert"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2007
  }, {
    "title": "Support recovery with orthogonal matching pursuit in the presence of noise",
    "authors": ["J. Wang"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "No penalty no tears: Least squares in high-dimensional linear models",
    "authors": ["X. Wang", "D. Dunson", "C. Leng"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Nearly optimal bounds for orthogonal least squares",
    "authors": ["J. Wen", "J. Wang", "Q. Zhang"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }, {
    "title": "Orthogonal matching pursuit with thresholding and its application in compressive sensing",
    "authors": ["M. Yang", "F. de Hoog"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2015
  }, {
    "title": "Regularization and variable selection via the elastic net",
    "authors": ["H. Zou", "T. Hastie"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2005
  }],
  "id": "SP:4de4245a2988176d0b5b63e6b2e3506f156171b9",
  "authors": [{
    "name": "Sreejith Kallummil",
    "affiliations": []
  }, {
    "name": "Sheetal Kalyani",
    "affiliations": []
  }],
  "abstractText": "Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics. Both these statistics are rarely known a priori and are very difficult to estimate. In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.",
  "title": "Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit "
}