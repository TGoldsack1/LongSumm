{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A permutation is a 1-to-1 mapping from a finite set into itself, and allows to represent mathematically a complete ordering or n items. We consider the problem of machine learning when data are permutations, which has many applications such as analyzing preferences or votes (Diaconis, 1988; Marden, 1996), tracking objects (Huang et al., 2009), or learning robustly from high-dimensional biological data (Geman et al., 2004; Lin et al., 2009; Jiao & Vert, 2018).\nA promising direction to learn over permutations is to first embed them to a vector space, i.e., to first represent each permutation π by a vector Φ(π) ∈ Rd, and then to learn a parametric linear or nonlinear model over Φ(π) using standard machine learning approaches. For example, Kondor & Barbosa (2010) proposed an embedding to Rd with d = n!,\n1University of Oxford, Oxford, UK 2MINES ParisTech & Institut Curie & Ecole Normale Supérieure, PSL Research University, Paris, France. Correspondence to: Jean-Philippe Vert <jeanphilippe.vert@mines-paristech.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhere n is the number of items; however, in spite of computational tricks, this leads to algorithms with O(nn) complexity which become impractical as soon as we consider more than a few items. Recently, Jiao & Vert (2018) showed that the well-known Kendall τ correlation defines implicitly an embedding in d = O(n2) dimensions, in which linear models can be learned with O(n ln(n)) complexity thanks to the so-called kernel trick (Schölkopf & Smola, 2002; Shawe-Taylor & Cristianini, 2004); they showed promising results on gene expression classification where n is a few thousands. In this paper, we propose to extend this work by tackling several limitations of the Kendall kernel.\nFirst, the Kendall kernel compares two permutations by computing the number of pairs of items which are in the same order in both permutations. When n is large, it may be more interesting to weight differently the contributions of different pairs, e.g., to focus more on the top-ranked items. Many weighted versions of Kendall’s τ correlation have been proposed in the literature, as reviewed below in Section 3; however, to our knowledge, none of them is associated to a valid embedding of permutations in a vector space, like the original Kendall correlation. Here we propose such an extension, and show that it inherits the computational benefits from the Kendall kernel while allowing to weight differently the importance of items based on their rank.\nSecond, we discuss how to choose the position-dependent weights that define this embedding. We propose to see them as parameters of the model, and to optimize them jointly with other parameters during training. This is similar in spirit to the way different kernels are weighted and combined in multiple kernel learning (Lanckriet et al., 2004; Bach et al., 2004), or more generally to the notion of representation learning which became central in recent years in conjunction with neural networks or linear models (Mikolov et al., 2013; Le Morvan & Vert, 2017). We show in particular that efficient alternative optimization schemes are possible in the resulting optimization problem.\nThird, observing that the features defining the embedding associated to the Kendall kernel are based on pairwise comparisons only, we consider extensions to higher-order comparisons, e.g., the relative ranking among triplets or quadruplets of items. Such features are naturally captured by higher-\norder representations of the symmetric group (Diaconis, 1988; Kondor & Barbosa, 2010), and we show that both the definition of the embedding and the algorithms to learn the weights can be effortlessly extended to such higher-order scenarios.\nWe finally present preliminary experimental results highlighting the potential of these new embeddings. Code to reproduce all the experiments in the present paper is available at https://github.com/YunlongJiao/ weightedkendall."
  }, {
    "heading": "2. The Kendall kernel",
    "text": "Let us first fix some notations. Given an integer n ∈ N, a permutation is a 1-to-1 mapping σ : [1, n] → [1, n] such that σ(i) 6= σ(j) for i 6= j. We denote by Sn the set of all such permutations. A permutation σ ∈ Sn can for example represent the preference ordering of a user over n items, in which case σ(i) is the rank of item i among all items (e.g., the preferred item is the item i such that σ(i) = 1, namely, σ−1(1)). Endowed with the composition operation (σ1σ2)(i) = σ1(σ2(i)), Sn is a group called the symmetric group, of cardinality n!. The identity permutation will be denoted e.\nA positive definite (p.d.) kernel on Sn is a symmetric function K : Sn × Sn → R that satisfies, for any l ∈ N, α ∈ Rl and (σ1, . . . , σl) in Sln,\nl∑ i=1 l∑ j=1 αiαjK(σi, σj) ≥ 0 .\nEquivalently, a functionK : Sn×Sn → R is a p.d. kernel if and only if there exists a mapping Φ : Sn → Rd (for some d ∈ N) such that K(σ, σ′) = Φ(σ)>Φ(σ′). For example, taking d = n(n − 1) and Φτ (σ) = ( 1σ(i)<σ(j) ) 1≤i 6=j≤n, we see that the number of concordant pairs between two permutations is a p.d. kernel, i.e., ∀σ, σ′ ∈ Sn,\nKτ (σ, σ ′) = Φτ (σ) >Φτ (σ ′) = ∑\n1≤i6=j≤n\n1σ(i)<σ(j)1σ′(i)<σ′(j) . (1)\nUp to constant shift and scaling (by taking 2Kτ/ ( n 2 ) − 1), this is equivalent to the Kendall kernel of Jiao & Vert (2018). To lighten notations, we will simply call Kτ the Kendall kernel in the rest of this paper.\nBesides being p.d., the Kendall kernel has another interesting property: it is right-invariant, in the sense that for any σ, σ′ and π ∈ Sn, it holds that\nKτ (σ, σ ′) = Kτ (σπ, σ ′π) .\nRight-invariance implies that the kernel does not change if we relabel the items to be ranked, which is a natural\nrequirement in most ranking problems (e.g., when items are only presented in alphabetical order for no other particular reason). Note that any right-invariant kernel can be rewritten as, ∀σ, σ′ ∈ Sn,\nK(σ, σ′) = K(e, σ′σ−1) =: κ(σ′σ−1) , (2)\nwhere κ : Sn → R. Hence a right-invariant kernel is a semigroup kernel (Berg et al., 1984), and we say that a function κ : Sn → R is p.d. when the kernel K defined by (2) is p.d. In particular, note that K is symmetric if and only if κ satisfies κ(σ) = κ(σ−1) for any σ ∈ Sn. For example, the p.d. function associated to the Kendall kernel (1) is:\nκτ (σ) = ∑\n1≤i 6=j≤n\n1i<j1σ(i)<σ(j) = ∑\n1≤i<j≤n\n1σ(i)<σ(j) .\nDenoting I(2) = { (i, j) ∈ [1, n]2 : 1 ≤ i < j ≤ n }\n, this can be rewritten as\nκτ (σ) = ∑\n(i,j)∈I(2) 1(σ(i),σ(j))∈I(2) . (3)\nThis notation will be convenient in Section 7 when we generalize Kendall to high-order kernels for permutations."
  }, {
    "heading": "3. Related work",
    "text": "The Kendall kernel (1) compares two permutations by assessing how many pairs of items are ranked in the same order. In many cases, however, one may want to weight differently the contributions of different pairs, based for example on their rankings in both permutations. Typically in applications involving preference ranking and information retrieval, one may expect that the relative ordering of items ranked at the bottom of the list is usually less relevant than that at the top. For example, when participants are asked to express their preference over a list of predefined items, it is usually impractical for participants to accurately express their preference towards less preferable items. As another example from information retrieval, when we wish to compare two ranked lists of documents outputted by two search engines, differences towards the top usually matter more as those query results will be eventually presented to the user.\nMany authors have proposed weighted versions of the Kendall’s τ correlation coefficient. A classical one, for example, is the weighted Kendall τ statistics studied by Shieh (1998):\nτw(σ, σ ′) = ∑ 1≤i6=j≤n w(σ(i), σ(j))1σ(i)<σ(j)1σ′(i)<σ′(j) , (4) for some weight function w : [1, n]2 → R. Typical examples of the weight function include\n• w(i, j) = 1/(j − 1) if j ≥ 2 and 0 otherwise, which gives the average precision correlation coefficient (Yilmaz et al., 2008).\n• Weights of the form w(i, j) = wiwj for some vector w ∈ Rn (Kumar & Vassilvitskii, 2010).\nWhile (4) looks like a weighted version of (1), it is not symmetric in σ and σ′ (except if w is constant) since the weight of a pair of items only depends on their rankings in σ, and in particular τw is not a p.d. kernel. To enforce symmetry, Vigna (2015) proposes a weighted correlation of the form\nτw(σ, σ ′) = ∑ 1≤i6=j≤n (w(σ(i), σ(j)) + w(σ′(i), σ′(j)))\n× 1σ(i)<σ(j)1σ′(i)<σ′(j) , (5)\nwhich is symmetric and right-invariant, i.e., is invariant by relabeling of the items. However, (5) is not a valid p.d. inner product. Kumar & Vassilvitskii (2010) proposes a weighted correlation of the form:\nτw,p(σ, σ ′) = ∑ 1≤i 6=j≤n w(σ(i), σ(j))\n× pσ(i) − pσ′(i) σ(i)− σ′(i) pσ(j) − pσ′(j) σ(j)− σ′(j) 1σ(i)<σ(j)1σ′(i)<σ′(j) ,\n(6)\nwhere w : [1, n]2 → R and p ∈ Rn. In particular, the additional set of weights p in (6) is motivated by cumulatively weighting the cost of swaps between adjacent positions needed to transform σ into σ′. Like (4), τw,p is not symmetric hence not p.d. Farnoud & Milenkovic (2014) notices that Kendall’s τ induces a Euclidean metric which is the shortest path distance over a Cayley’s graph (i.e., the smallest number of swaps between adjacent positions to transform a permutation into another), and proposes to set weights on the edges (i.e., on swaps between adjacent positions) in order to define the shortest path over the weighted graph as a new metric; although a valid metric, it does not induce a valid p.d. kernel in general. In summary, to our knowledge, no existing weighted variant of the Kendall kernel is p.d. and right-invariant."
  }, {
    "heading": "4. The weighted Kendall kernel",
    "text": "The following result provides a generic way to construct a weighted Kendall kernel that is p.d. and right-invariant.\nTheorem 1. Let W : N2×N2 → R be a p.d. kernel on N2.\nThen the function KW : Sn × Sn → R defined by\nKW (σ, σ ′) = ∑ 1≤i 6=j≤n W ((σ(i), σ(j)), (σ′(i), σ′(j)))\n× 1σ(i)<σ(j)1σ′(i)<σ′(j) (7)\nis a right-invariant p.d. kernel on Sn.\nTheorem 1 is easy to prove by writing explicitly W as an inner product, and deducing from (7) that KW can also be written as an inner product. Note that KW is always rightinvariant whether or not W is p.d. Note also that there may exist non p.d. weight W leading to p.d. kernels KW ; we leave it as an open question to characterize the necessary conditions on the weight function W such that KW is p.d.\nLet us now consider particular cases of the weighted Kendall kernel of the following form: Corollary 1. Let the weights in Theorem 1 take the form W ((a, b), (c, d)) = UabUcd for some matrix U ∈ Rn×n, then\nKU (σ, σ ′) = ∑ 1≤i6=j≤n Uσ(i),σ(j)Uσ′(i),σ′(j)\n× 1σ(i)<σ(j)1σ′(i)<σ′(j) (8)\nis a right-invariant p.d. kernel on Sn. Remark. It is interesting to make explicit how Corollary 1 is derived from Theorem 1. Let us fix the number of items to rank n ∈ N. The conditions in Theorem 1 on W being a kernel over N2 can now be relaxed to being a kernel over [1, n]2. With a slight abuse of notation, any p.d. kernel W over finite [1, n]2 is uniquely determined by its full Gram matrixW of size n2×n2, which can always be decomposed into W = UU> for some matrix U of size n2 × n2 due to matrix W being s.p.d. In other words, this factorizationbased definition W = UU> is necessary and sufficient under the conditions set in Theorem 1. It is now easy to see that, if we further assume that Gram matrix W has rank 1 such that U is a vector of size n2, i.e., a matrix of size n×n, Theorem 1 reduces to Corollary 1.\nNotably in (8), Uab should not be interpreted as a weight for items a and b, but rather as a weight for (those items sent by a permutation to) positions a and b. More precisely, if two items are ranked in the same relative order in both permutations, say positions a < b in σ and c < d in σ′, then this pair contributes UabUcd to the kernel value. Note that if Uij is constant for any (i, j), the weighted Kendall kernel (8) reduces to the standard Kendall kernel (1).\nWhile U ∈ Rn×n encodes the weights of pairs of positions, it is usually intuitive to start from individual positions. Suppose now we are given a vector u ∈ Rn that encodes our belief of relevance of each position in [1, n], some particularly interesting choices for U ∈ Rn×n are\n• Top-k weight: For some predetermined k ∈ [2, n], define Uij = 1 if i ≤ k and j ≤ k and 0 otherwise.\n• Additive weight: Given some u ∈ Rn, define Uij = ui + uj .\n• Multiplicative weight: Given some u ∈ Rn, define Uij = uiuj .\nEach of these choices can be relevant in certain applications. For example, the top-k weight compares two permutations using exclusively the top k ranked items, and checking if they appear at the top of both permutations and in the same relative order. As another example, if one wishes to attenuate the importance of items when their rank increases, the additive or multiplicative weights can be a natural choice when combined with, for example, hyperbolic reduction factor ui = 1/(i+ 1) of the average precision correlation (Yilmaz et al., 2008), or logarithmic reduction factor ui = 1/ log2(i+1) of the discounted cumulative gain widely used in information retrieval (Manning et al., 2008). In particular, the top-k weight can be seen as a multiplicative weight with a hard cutoff factor ui = 1 if i ≤ k and 0 otherwise. Figure 1 illustrates the aforementioned three different types of u.\nNote that the hyperparameter k in the top-k Kendall kernel needs to be determined in practice, which can be achieved by cross-validation for instance. A straightforward way to bypass this and construct a kernel agnostic to the particular choice of k is to take the average of all top-k Kendall kernels with k ranging over [1, n]. Specifically, the top-k Kendall kernel is written\nK@kU (σ, σ ′) = ∑ 1≤i 6=j≤n 1σ(i)≤k1σ(j)≤k1σ′(i)≤k1σ′(j)≤k\n× 1σ(i)<σ(j)1σ′(i)<σ′(j) . (9)\nThe average Kendall kernel is then derived as\nKavgW (σ, σ ′) :=\n1\nn n∑ k=1 K@kU (σ, σ ′)\n= ∑\n1≤i 6=j≤n\n1 n min {σ(i), σ′(i)}1σ(i)<σ(j)1σ′(i)<σ′(j) .\n(10)\nThe positive definiteness of the average Kendall kernel is evident due to the fact that it is a sum of p.d. kernels, or we can verify that the average Kendall kernel is a weighted Kendall kernel of form (7) whose weight function is indeed the min kernel (Shawe-Taylor & Cristianini, 2004) thus satisfying the condition of Theorem 1. However, we see that the average Kendall kernel is no longer of the special form (8) considered in Corollary 1."
  }, {
    "heading": "5. Fast computation",
    "text": "Despite being a sum of O(n2) terms, the Kendall kernel (1) can be computed efficiently in O(n ln(n)). For example, Knight (1966) proposed such an algorithm by adapting a merge sort algorithm in order to count the inversion number of any permutation. While the general weighted Kendall kernel (7) does not enjoy comparable computational efficiency in general, it does for certain choices discussed in Section 4.\nTheorem 2. The weighted Kendall kernel (8) can be computed in O(n ln(n)) for the top-k, additive or multiplicative weights. Besides, the average Kendall kernel (10) can also also be computed in O(n ln(n)).\nThe proof is constructive and deferred to the supplements, where we also detail the pseudo-code for a fast computational algorithm.1 Notably, Vigna (2015) proposed a different fast algorithm based on the form of a weighted correlation (5), which applies to our additive and multiplicative cases as well. Theorem 2 shows that, just like the standard Kendall kernel, any of the weighted Kendall kernels concerned by the theorem benefits from the kernel trick and can be used efficiently by a kernel machine such as a SVM or kernel k-means."
  }, {
    "heading": "6. Learning the weights",
    "text": "So far, we have been focusing on studying the properties of the weighted Kendall kernel for which weights are given and fixed. In practice, it is usually not clear how to choose the weights so that the resulting kernel best suits the learning task at hand. We thus propose a systematic approach to learn the weights in the context of supervised learning\n1C++/R implementation available in the package kernrank at https://github.com/YunlongJiao/kernrank.\nwith discriminative models. More specifically, instead of first choosing a priori the weights in the weighted Kendall kernel and then learning a function by a kernel machine, the weights can be learned jointly with the function estimated by the kernel machine.\nWe start by making explicit the feature embedding for permutations underlying the weighted Kendall kernel (8). For any permutation σ ∈ Sn, let Πσ ∈ {0, 1}n×n be the permutation matrix of σ ∈ Sn defined by\n(Πσ)ij = 1i=σ(j) .\nIn fact, Π : Sn → Rn×n is the first-order permutation representation of Sn satisfying Π>σ = Πσ−1 and ΠσΠσ′ = Πσσ′ (Diaconis, 1988). Lemma 1. For any matrix of weights U ∈ Rn×n, let ΦU : Sn → Rn×n be defined by\n∀σ ∈ Sn , ΦU (σ) = Π>σ UΠσ or elementwise ( ΦU (σ) ) ij = Uσ(i),σ(j) , (11)\nand let kernel GU over Sn defined by\n∀σ, σ′ ∈ Sn , GU (σ, σ′) = 〈 ΦU (σ),ΦU (σ′) 〉 F\n= n∑ i,j=1 Uσ(i),σ(j)Uσ′(i),σ′(j) . (12)\nwhere 〈·, ·〉F denotes the Frobenius inner product for matrices. In particular, if U is upper (or lower) triangular with Uij = 0 for all i ≥ j (or i ≤ j), then GU = KU .\nIn the remainder of the paper, we will focus on GU (12), which we call the weighted kernel for permutations. Lemma 1 shows that using the weighted kernel GU in a kernel machine amounts to learning in the feature space induced by the weighted embedding ΦU . In the context of supervised learning with discriminative models, this reduces to fitting a linear model to data through embedding ΦU .\nTheorem 3. A general linear function on the weighted embedding ΦU with coefficients B ∈ Rn×n can be written equivalently as\nhU,B(σ) := 〈 B,ΦU (σ) 〉 F\n= 〈 U,ΦB(σ−1) 〉 F\n= 〈 vec(U)⊗ (vec(B))> ,Πσ ⊗Πσ 〉\nF ,\n(13)\nwhere vec(·) denotes the vectorization of a matrix, ⊗ denotes the Kronecker product for matrices or the outer product for vectors, wherever appropriate.\nProof. The first equality is the definition of hU,B . Plugging\nin (11) from Lemma 1, we have the second equality hU,B(σ) = 〈 B,ΦU (σ) 〉 F\n= tr ( B> ( Π>σ UΠσ )) = tr (( ΠσBΠ > σ )> U )\n= tr (( Π>σ−1BΠσ−1 )> U )\n= 〈 U,ΦB(σ−1) 〉 F .\nThe last equality follows from the relationship that vec(PXQ) = ( Q> ⊗ P ) vec(X) holds for any matrices P,Q,X . Note that the first two inner products are over n × n matrices, while the last one is over n2 × n2 matrices.\nRemark. Theorem 3 has several interesting implications. The first and second equalities show that the weights U and linear coefficients B are conceptually interchangeable. We can arbitrarily swap the roles of the coefficients B of the linear model and the weights U of the embedding, as long as we also change the representation of data from σ to σ−1. The last equality shows that a linear function with coefficients B on an n × n dimensional embedding ΦU (σ) is equivalent to a linear function with coefficients vec(U)⊗(vec(B))> on an n2×n2 dimensional embedding Πσ ⊗Πσ .\nTheorem 3 suggests that, instead of first fixing some weights U for the weighted kernel GU and then learning a function B using this embedding, U itself can be learned jointly with B. Consequently, ΦU with the learned weights U is thus a data-driven discriminative feature embedding. Typically, fitting a kernel machine such as SVM is formulated as solving some optimization problem over B (in case of fixed GU ). Now, we propose to optimize over both U and B by joint optimization, which amounts to a non-convex optimization problem over vec(U)⊗ (vec(B))> according to Theorem 3.\nA commonly used approach to finding a stationary point of such optimization problems is by alternatively optimizing over B for U fixed, and over U for B fixed, until convergence or until a given number of iterations is reached. Interestingly, this can be easily implemented due to the conceptual interchangeability of B and U thanks to Theorem 3. Specifically, when U is fixed, optimizing over B amounts to training a kernel machine, with the kernel GU (σ, σ′); in turn, forB fixed, optimizing overU also amounts to training a kernel machine, with the kernel GB(σ−1, (σ′)−1) instead. Note that this naive alternating procedure implemented in kernel machines such as SVM, it will result in regularizing B and U in the same way, in the sense that the feasible region remain the same for both.\nAlternatively, it is possible to bypass the need for alternative optimization of B and U . Due to Theorem 3, when data\nare represented through Πσ ⊗ Πσ, the linear coefficients vec(U) ⊗ (vec(B))> can be directly learned by exerting some low-rank assumption on the linear model. This is similar to the supervised quantile normalisation (SUQUAN) model proposed by Le Morvan & Vert (2017), who studied a similar optimization over a quantile distribution by learning a rank-1 linear model."
  }, {
    "heading": "7. High-order kernels for permutations",
    "text": "While the Kendall kernel (1) relies only on pairwise comparison between items, it may be interesting in some applications to include high-order comparison among tuples beyond pairs (Diaconis, 1988). In this section, we extend the Kendall kernel to higher-order kernels for permutations, establish their weighted variants and show that our systematic approach of learning the weights also applies to high-order cases.\nFollowing the notation of (3), let us denote for any d ≤ n, I(d) = { (i1, . . . , id) ∈ [1, n]d : 1 ≤ i1 < · · · < id ≤ n } ,\nand define the p.d. function associated with the order-d kernel for permutations by\nκ(d)(σ) = ∑\n(i1,...,id)∈I(d) 1(σ(i1),...,σ(id))∈I(d) . (14)\nThe order-d kernel is then given by the right-invariance, i.e.,\nK(d)(σ, σ′) = κ(d)(σ′σ−1)\n= n∑ i1,...,id=1 1(σ(i1),...,σ(id))∈I(d)1(σ′(i1),...,σ′(id))∈I(d) .\n(15)\nThe order-d kernel compares two permutations by counting the number of d-tuples whose relative ordering is concordant. In particular, order-2 kernel reduces to the Kendall kernel (1), i.e., K(2) = Kτ .\nSimilarly to (11) and (12), we define a weighted order-d embedding for permutations ΦU : Sn → R d︷ ︸︸ ︷\nn×···×n elementwise by\n∀σ ∈ Sn , ( ΦU (σ) ) i1...id = Uσ(i1),...,σ(id) , (16)\nwhere the weights U ∈ R d︷ ︸︸ ︷\nn×···×n is a order-d (cubical) tensor of size n at each dimension. Notably, given U , computation of ΦU (σ) for any σ ∈ Sn can be done simply by permuting the entries in each dimension of the tensor U by\nσ. The weighted order-d kernel is then defined as ∀σ, σ′ ∈ Sn , GU (σ, σ′) = 〈 ΦU (σ),ΦU (σ′) 〉 F\n= n∑ i1,...,id=1 Uσ(i1),...,σ(id)Uσ′(i1),...,σ′(id) ,\nwhere 〈·, ·〉F denotes the Frobenius inner product for tensors. In particular, when U is a matrix (order-2 tensor), the weighted order-2 kernel reduces to the weighted kernel (12). Taking the special case Ui1...id = 1i1<···<id , the weighted order-d kernel GU reduced to the standard order-d kernel K(d) in (15).\nAgain, we would like to learn the weights U adapted to data by jointly optimizing them with the function estimated by a kernel machine. Recall that, in the context of supervised learning with discriminative models, working with the weighted order-d kernel GU for permutations with kernel machines amounts to fitting a linear model to data through the weighted order-d embedding ΦU . In case of weighted high-order embedding and kernel, we can establish highorder counterpart of the results in Theorem 3.\nTheorem 4. A general linear function on the weighted order-d embedding ΦU with some order-d tensor of linear coefficients B ∈ R d︷ ︸︸ ︷\nn×···×n can be written equivalently as\nhU,B(σ) := 〈 B,ΦU (σ) 〉 F\n= 〈 U ,ΦB(σ−1) 〉 F\n= 〈 U ⊗ B,Π⊗dσ 〉 F ,\n(17)\nwhere ⊗ denotes the outer product2 for tensors, Π⊗dσ ∈ ({0, 1}) 2d︷ ︸︸ ︷ n×···×n is defined element-wise by\n( Π⊗dσ ) i1...idj1...jd = d∏ k=1 (Πx)ikjk\n= 1(i1,...,id)=πx((j1,...,jd)) .\nProof. By definition, the weighted order-d embedding ΦU (σ) (16) is the contracted product (Bader & Kolda, 2006, Section 3.3) of two tensors Π⊗dσ ∈ ({0, 1}) 2d︷ ︸︸ ︷ n×···×n and U ∈ R d︷ ︸︸ ︷\nn×···×n respectively at dimensions [1, . . . , d; 1, . . . , d] of both tensors, resulting in an order-d tensor (inheriting the remaining indices of the first tensor). Specifically, we can\n2With respect to standard index order (consecutively concatenating the dimensions of each tensor).\nwrite ΦU in a compact representation\nΦU (σ) = 〈 Π⊗dσ ,U 〉 [1,...,d;1,...,d]\n= ( Uσ(i1),...,σ(id) ) i1...id ∈ R d︷ ︸︸ ︷ n×···×n ,\nwhere 〈·, ·〉[1,...,d;1,...,d] denotes the contracted product respectively at dimensions [1, . . . , d; 1, . . . , d].\nLinear function are now of the form: hU,B(σ) = 〈 B,ΦU (σ) 〉 F\n= 〈 B, 〈 Π⊗dσ ,U 〉 [1,...,d;1,...,d] 〉 F\n= n∑ i1,...,id=1 Bi1...idUσ(i1),...,σ(id)\n= n∑ i1,...,id=1 Ui1...idBσ−1(i1),...,σ−1(id)\n= 〈 U , 〈 Π⊗dσ ,B 〉 [d,...,2d;1,...,d] 〉 F\n= 〈 U ,ΦB(σ−1) 〉 F\n= 〈 U ⊗ B,Π⊗dσ 〉 F ,\nwhere 〈·, ·〉F denotes the Frobenius inner product for tensors, whereas 〈·, ·〉[d,...,2d;1,...,d] denotes the contracted product respectively at dimensions [d, . . . , 2d; 1, . . . , d].\nTheorem 4 together with Theorem 3 ensures that there exists no substantial difference when we move from order-2 to order-d embedding, or their underlying weighted kernel. Therefore, the discussion elaborated in Section 6 regarding Theorem 3 can be seamlessly migrated to higher-order cases. For instance, as joint optimization over B and U is non-convex, alternative optimization can be employed to find a stationary point in practice. Thanks to the conceptual interchangeability between B and U , alternating the optimization in B and U with a kernel machines amounts to simply changing the underlying kernel function accordingly. Note that the square unfolding studied by Jiang et al. (2015, Definition 2.2) of the order-2d tensor Π⊗dσ is a matrix of dimension nd × nd, which is exactly the d-fold Kronecker product of the matrix Πσ with itself. Therefore, we can still follow approaches developed by Le Morvan & Vert (2017) in order to directly learn U ⊗B by asserting low-rank assumptions on a linear model."
  }, {
    "heading": "8. Numerical experiments",
    "text": "In this section, we demonstrate the use of the proposed weighted kernels compared with the standard Kendall kernel for classification on a real dataset from the European Union survey Eurobarometer 55.2 (Christensen, 2010). As part\nof this survey, participants were asked to rank, according to their opinion, the importance of six sources of information regarding scientific developments: TV, radio, newspapers and magazines, scientific magazines, the internet, school/university. The dataset also includes demographic information of the participants such as gender, nationality or age. We removed all respondents who did not provide a complete ranking over all six sources, leaving a total of 12, 216 participants. Then, we split the dataset across age groups, where 5, 985 participants were 40 years old or younger, 6, 231 were over 40. The objective is to predict the age group of participants from their ranking of n = 6 sources of news. In order to compare different kernels, we chose to fit kernel SVMs with different kernels.\nIn order to assess the classification performance, we randomly sub-sampled a training set of 400 participants and a test set of 100 participants. This random sub-sampling process was repeated 50 times and we report the classification accuracy on 50 test sets. Different types of weighted kernels considered are: the weighted Kendall kernels (8) with top-k weight for k = 2, . . . , 6, additive weight for hyperbolic or logarithmic reduction factor, multiplicative weight for hyperbolic or logarithmic reduction factor, the average Kendall kernel (10), the weighted kernel (12) with learned weights via alternative optimization or via SUQUAN-SVD, an SVD-based low-rank approximation algorithm proposed by Le Morvan & Vert (2017, Algorithm 1). In particular, the top-6 Kendall kernel is equivalent to the standard Kendall kernel, which is seen as the baseline to benchmark with.\nTable 1 and Figure 2 summarize the classification accuracy across 50 random experiments using kernel SVM with different types of weighted kernels. Results show that the average Kendall kernel is the best-performing kernel of all the kernels considered, followed by the weighted kernel with weights learned via SUQUAN-SVD. Since we observed that\nthe standard deviation (SD) of accuracy scores is relatively large compared to the mean, we carried out paired Wilcoxon rank test to assess whether or not the better performance is significant where certain weighted kernels improved the standard Kendall kernel. The p-values are reported in the last column of Table 1. In fact, the average Kendall outperformed the standard Kendall kernel at a significance of 0.01 whereas the weighted kernel with weights learned via SUQUAN-SVD has a significance of 0.07. However, compared to to SUQUAN-SVD, the weighted kernel with weights learned via alternative optimization is particularly not a good choice, which may be due to the fact that fully optimizing over the weights under a rather simple ridge-type regularization can overfit the data, leading to poor generalization. In conclusion, these results show that the weighted Kendall kernel can be promising for certain prediction tasks as they can outperform the standard one.\nIt is interesting to make a note on the performance of topk Kendall kernels varying the cutoff parameter k and the average Kendall kernel (i.e. boxplots to the left of the first vertical dotted line in Figure 2). When we move away from top-6 with all pairs of items included (or equivalently the baseline standard Kendall kernel), as k decreases, the performance increases until peaking at k = 3, then as k continues to decrease, the performance decreases until k = 2 with only the top ranked pair of items considered. This implies that choosing the number of top ranked items should be crucial in learning with rankings. Further, the performance can be drastically improved when we use the average Kendall kernel which crowdsourced all top-k Kendall kernels.\nFinally, we show the weights learned via SUQUAN-SVD for a data-driven weighted kernel in Figure 3. Recall that the entries Uab of a weight matrix encodes the importance of\npairs of ranked positions a and b. First, the diagonal can be seen as importance of individual positions, and we observe a generally decreasing trend of importance as the position rank increases, i.e., as items become less preferred. This is in line with the pattern widely recognized in such data involved in preference ranking. Second, the matrix presents a near skew-symmetric pattern, where the upper triangle contains mostly negative importance and the lower triangle mostly positive. This suggests that shifting the relative order of σ(i) > σ(j) or σ(i) < σ(j) results in opposite direction of contribution in evaluating the kernel GU (12). Finally, we observe that, the more we move away from diagonal, the larger the magnitude of the values, which indicates that it is crucial to focus more on pairs of items whose ranks are more distinctively placed by a permutation. This pattern of gradient on pairwise position importance in U identifies our foremost motivation of proposing the weighted kernels."
  }, {
    "heading": "9. Discussion",
    "text": "We have proposed a general framework to build computationally efficient kernels for permutations, extending the Kendall kernel to incorporate weights and higher-order information, and showing how weights can be systematically optimized in a data-driven way. The price to pay for these extensions is that the dimensionality of the embedding and the number of free parameters of the model can quickly increase, raising computational and statistical challenges that could be addressed in future work. On the theoretical side, the kernels we proposed could naturally be analyzed in the Fourier domain (Kondor & Barbosa, 2010; Mania et al., 2016) which may lead to new interpretations of their regularization properties and reproducing kernel Hilbert spaces (Berg et al., 1984)."
  }],
  "year": 2018,
  "references": [{
    "title": "Multiple kernel learning, conic duality, and the SMO algorithm",
    "authors": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"],
    "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning",
    "year": 2004
  }, {
    "title": "Algorithm 862: MATLAB tensor classes for fast algorithm prototyping",
    "authors": ["B.W. Bader", "T.G. Kolda"],
    "venue": "ACM Transactions on Mathematical Software (TOMS),",
    "year": 2006
  }, {
    "title": "Eurobarometer 55.2: Science and technology, agriculture, the euro, and internet access, mayjune",
    "authors": ["T. Christensen"],
    "venue": "https://doi.org/10.3886/ICPSR03341.v3,",
    "year": 2001
  }, {
    "title": "Group Representations in Probability and Statistics, volume 11 of Lecture Notes–Monograph Series",
    "authors": ["P. Diaconis"],
    "venue": "Institut of Mathematical Statistics,",
    "year": 1988
  }, {
    "title": "An axiomatic approach to constructing distances for rank comparison and aggregation",
    "authors": ["F. Farnoud", "O. Milenkovic"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2014
  }, {
    "title": "Classifying gene expression profiles from pairwise mRNA comparisons",
    "authors": ["D. Geman", "C. d’Avignon", "D.Q. Naiman", "R.L. Winslow"],
    "venue": "Statistical Applications in Genetics and Molecular Biology,",
    "year": 2004
  }, {
    "title": "Fourier theoretic probabilistic inference over permutations",
    "authors": ["J. Huang", "C. Guestrin", "L. Guibas"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "New ranks for evenorder tensors and their applications in low-rank tensor optimization",
    "authors": ["B. Jiang", "S. Ma", "S. Zhang"],
    "venue": "Technical report,",
    "year": 2015
  }, {
    "title": "The Kendall and Mallows kernels for permutations",
    "authors": ["Y. Jiao", "Vert", "J.-P"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2018
  }, {
    "title": "A computer method for calculating Kendall’s tau with ungrouped data",
    "authors": ["W.R. Knight"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1966
  }, {
    "title": "Ranking with kernels in Fourier space",
    "authors": ["R. Kondor", "M. Barbosa"],
    "venue": "In The 23rd Conference on Learning Theory (COLT),",
    "year": 2010
  }, {
    "title": "Generalized distances between rankings",
    "authors": ["R. Kumar", "S. Vassilvitskii"],
    "venue": "In Proceedings of the 19th International Conference on World Wide Web,",
    "year": 2010
  }, {
    "title": "Learning the kernel matrix with semidefinite programming",
    "authors": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2004
  }, {
    "title": "Supervised quantile normalisation",
    "authors": ["M. Le Morvan", "Vert", "J.-P"],
    "venue": "arXiv preprint arXiv:1706.00244,",
    "year": 2017
  }, {
    "title": "The ordering of expression among a few genes can provide simple cancer biomarkers and signal BRCA1 mutations",
    "authors": ["X. Lin", "B. Afsari", "L. Marchionni", "L. Cope", "G. Parmigiani", "D. Naiman", "D. Geman"],
    "venue": "BMC Bioinformatics,",
    "year": 2009
  }, {
    "title": "Universality of Mallows’ and degeneracy of Kendall’s kernels for rankings",
    "authors": ["H. Mania", "A. Ramdas", "M.J. Wainwright", "M.I. Jordan", "B. Recht"],
    "year": 2016
  }, {
    "title": "Introduction to information retrieval",
    "authors": ["C.D. Manning", "P. Raghavan", "H. Schütze"],
    "year": 2008
  }, {
    "title": "Analyzing and modeling rank data",
    "authors": ["J.I. Marden"],
    "venue": "CRC Press,",
    "year": 1996
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
    "authors": ["B. Schölkopf", "A.J. Smola"],
    "year": 2002
  }, {
    "title": "Kernel methods for pattern analysis",
    "authors": ["J. Shawe-Taylor", "N. Cristianini"],
    "year": 2004
  }, {
    "title": "A weighted Kendall’s tau statistic",
    "authors": ["G.S. Shieh"],
    "venue": "Statistics & Probability Letters,",
    "year": 1998
  }, {
    "title": "A weighted correlation index for rankings with ties",
    "authors": ["S. Vigna"],
    "venue": "In Proceedings of the 24th International Conference on World Wide Web,",
    "year": 2015
  }, {
    "title": "A new rank correlation coefficient for information retrieval",
    "authors": ["E. Yilmaz", "J.A. Aslam", "S. Robertson"],
    "venue": "In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
    "year": 2008
  }],
  "id": "SP:80bcc8e5b102e2061a3fe5b4f62bdd03c7193cae",
  "authors": [{
    "name": "Yunlong Jiao",
    "affiliations": []
  }, {
    "name": "Jean-Philippe Vert",
    "affiliations": []
  }],
  "abstractText": "We propose new positive definite kernels for permutations. First we introduce a weighted version of the Kendall kernel, which allows to weight unequally the contributions of different item pairs in the permutations depending on their ranks. Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be computed efficiently in O(n ln(n)) operations, where n is the number of items in the permutation. Second, we propose a supervised approach to learn the weights by jointly optimizing them with the function estimated by a kernel machine. Third, while the Kendall kernel considers pairwise comparison between items, we extend it by considering higher-order comparisons among tuples of items and show that the supervised approach of learning the weights can be systematically generalized to higher-order permutation kernels.",
  "title": "The Weighted Kendall and High-order Kernels for Permutations"
}