{
  "sections": [{
    "heading": "1. Introduction",
    "text": "One of the most successful neural network architectures is convolutional neural networks (CNNs) (LeCun et al., 1989). In the image recognition domain, where CNNs were originally conceived, convolution plays two crucial roles. First, it ensures that in any given layer, exactly the same filters are applied to each part of the image. Consequently, if the input image is translated, the activations of the network in each layer will translate the same way. This property is called equivariance (Cohen & Welling, 2016). Second, in conjunction with pooling, convolution ensures that each neuron’s effective receptive field is a spatially contiguous domain. As we move higher in the network, these domains generally get larger, allowing the CNN to capture structure in images at multiple different scales.\n1Departments of Statistics and Computer Science, The University of Chicago 2Toyota Technological Institute at Chicago. Correspondence to: Risi Kondor <risi@cs.uchicago.edu>, Shubhendu Trivedi <shubhendu@ttic.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nRecently, there has been considerable interest in extending neural networks to more exotic types of data, such as graphs or functions on manifolds (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015; Li et al., 2016; Cohen et al., 2018; Monti et al., 2017; Masci et al., 2015). In these domains, equivariance and multiscale structure are just as important as for images, but finding the right notion of convolution is not obvious.\nOn the other hand, mathematics does offer a sweeping generalization of convolution tied in deeply with some fundamental ideas of abstract algebra: if G is a compact group and f and g are two functions G→ C, then the convolution of f with g is defined\n(f ∗ g)(u) = ∫ G f(uv−1) g(v) dµ(v). (1)\nNote the striking similarity of this formula to the ordinary notion of convolution, except that in the argument of f , u− v has been replaced by the group operation uv−1, and integration is with respect to the Haar measure, µ.\nThe goal of this paper is to relate (1) to the various looser notions of convolution used in the neural networks literature, and show that several practical neural networks implicitly already take advantange of the above group theoretic concept of convolution. In particular, we prove the following theorem (paraphrased here for simplicity).\nTheorem 1. A feed forward neural network N is equivariant to the action of a compact group G on its inputs if and only if each layer of N implements a generalized form of convolution derived from (1).\nTo the best of our knowledge, this is the first time that the connection between equivariance and convolution in neural networks has been stated at this level of generality. One of the technical challenges in proving our theorem is that the activations in each layer of a neural net correspond to functions on a sequence of spaces acted on by G (called homogeneous spaces or quotient spaces) rather than functions on G itself. This necessitates a discussion of group convolution that is rather more thoroughgoing than is customary in pure algebra.\nThis paper does not present any new algorithms or neural\nnetwork architectures. Rather, its goal is to provide the language for thinking about generalized notions of equivariance and convolution in neural networks, and thereby facilitate the development of future architectures for data with nontrivial symmetries. To avoid interruptions in the flow of our exposition, we first present the theory in its abstract form, and then illustrate it with examples in Section 6. For better understanding, the reader might choose to skip back and forth between these sections. One work that is close in spirit to the present paper but only considers discrete groups is (Ravanbakhsh et al., 2017)."
  }, {
    "heading": "2. Notation",
    "text": "In the following [a] will denote the set {1, 2, . . . , a}. Given a set X and a vector space V , LV (X ) will denote the space of functions {f : X → V }."
  }, {
    "heading": "3. Equivariance in neural networks",
    "text": "A feed-forward neural network consists of some number of “neurons” arranged in L+1 distinct layers. Layer `= 0 is the input layer, where data is presented to the network, while layer ` = L is where the output is read out. Each neuron n`x (denoting neuron number x in layer `) has an activation f `x. For the input layer, the activations come directly from the data, whereas in higher layers they are computed via a simple function of the activations of the previous layer, such as\nf `x = σ ( b`x + ∑ y w ` x,y f `−1 y ) . (2)\nHere, the {b`x} bias terms and the {w`x,y} weights are the network’s learnable parameters, while σ is a fixed nonlinear function, such as the ReLU function σ(z) = max(0, z). In the simplest case, each f `x is a scalar, but, in the second half of the paper we consider neural networks with more general, vector or tensor valued activations.\nFor the purposes of the following discussion it is actually helpful to take a slightly more abstract view, and, instead of focusing on the individual activations, consider the activations in any given layer collectively as a function f ` : X` → V`, where X` is a set indexing the neurons and V` is a vector space. Omitting the bias terms in (2) for simplicity, each layer ` = 1, 2, . . . , L can then just be thought of as implementing a linear transformation φ` : LV`−1(X`−1) → LV`(X`) followed by the pointwise nonlinearity σ. Our operational definition of neural networks for the rest of this paper will be as follows.\nDefinition 1. Let X0, . . . ,XL be a sequence of index sets, V0, . . . , VL vector spaces, φ1, . . . , φL linear maps φ` : LV`−1(X`−1) −→ LV`(X`), and σ` : V` → V` appropriate pointwise nonlinearities, such as the ReLU operator. The corresponding multilayer feed-forward neural network (MFF-NN) is then a\nsequence of maps f0 7→ f1 7→ f2 7→ . . . 7→ fL, where f`(x) = σ`(φ`(f`−1)(x)).\nIf we are interested in constructing a neural net for recognizing m × m pixel images, it is tempting to take X0 = [m]× [m] and define X1, . . . ,XL similarly. However, again for notational simplicity, we extend each of these index sets to the entire integer plane Z2, and simply assume that outside of the square region [m]× [m], f0(x1, x2) = 0. A traditional convolutional neural network (CNN) is a network of this type where the φ` functions are constrained to have the special form\nφ`(f`−1)(x1, x2) = w∑\nu1=1 w∑ u2=1 f`−1(x1−u1, x2−u2) χ`(u1, u2). (3)\nThe above function is known as the discrete convolution of f `−1 with the filter χ, and is usually denoted f`−1 ∗ χ`. In most CNNs the width w of the filters is quite small, on the order of 3 ∼ 10, while the number of layers can be as small as 3 or as large as a few dozen.\nSome of the key features of CNNs are immediately apparent from the convolution formula (3): 1. The number of parameters in CNNs is much smaller\nthan in general (fully connected) feed-forward networks, since we only have to learn the w2 numbers defining the χ` filters rather than O((m2)2) weights. 2. (3) applies the same filter to every part of the image. Therefore, if the networks learns to recognize a certain feature, e.g., eyes, in one part of the image, then it will be able to do so in any other part as well. 3. Equivalently to the above, if the input image is translated by any vector (t1, t2) (i.e., f0 ′ (x1, x2) = f\n0(x1− t1, x2−t2), then all higher layers will translate in exactly the same way. This property is called equivariance (sometimes covariance) to translations.\nThe goal of the present paper is to understand the mathematical generalization of the above properties to other domains, such as graphs, manifolds, and so on."
  }, {
    "heading": "3.1. Group actions",
    "text": "The jumping off point to our analysis is the observation that the above is a special case of the following scenario. 1. We have a set X and a function f : X → C. 2. We have a group G acting on X . This means that each g ∈G has a corresponding transformation Tg : X → X , and for any g1, g2 ∈G, Tg2g1 = Tg2 ◦ Tg1 .\n3. The action of G on X extends to functions on X by Tg : f 7→ f ′ f ′(Tg(x)) = f(x).\nIn the case of translation invariant image recognition, X = Z2, G is the group of integer translations, which is isomorphic to Z2 (note that this is a very special case, in general\nX and G are different objects), the action is T(t1,t2)(x1, x2) = (x1 + t1, x2 + t2) (t1, t2)∈Z 2,\nand the corresponding (induced) action on functions is\nT : f 7→ f ′ f ′(x1, x2) = f(x1− t1, x2− t2).\nWe give several other (more interesting) examples of group actions in Section 6, but for now continue with our abstract development. Also note that to simplify notation, in the following, where this does not cause confusion, we will simply write group actions as x 7→ g(x) rather than the more cumbersome x 7→ Tg(x).\nMost of the actions considered in this paper have the property that taking any x0 ∈X , any other x∈X can be reached by the action of some g ∈ G, i.e., x = g(x0). This property is called transitivity, and if the action of G on X is transitive, we say that X is a homogeneous space of G."
  }, {
    "heading": "3.2. Equivariance",
    "text": "Equivariance is a concept that applies very broadly, whenever we have a group acting on a pair of spaces and there is a map from functions on one to functions on the other.\nDefinition 2. Let G be a group and X1,X2 be two sets with corresponding G-actions Tg : X1 → X1, T ′g : X2 → X2. Let V1 and V2 be vector spaces, and T and T′ be the induced actions ofG onLV1(X1) andLV2(X2). We say that a (linear or non-linear) map φ : LV1(X1)→ LV2(X2) is equivariant with the action of G (or G–equivariant for short) if φ(Tg(f)) = T′g(φ(f)) ∀f ∈LV1(X1) for any group element g ∈G.\nEquivariance is represented graphically by a so-called commutative diagram, in our case\nLV1(X1) Tg //\nφ\nLV1(X1)\nφ\nLV2(X2) T′g // LV2(X2)\nWe are finally in a position to define the objects that we study in this paper, namely generalized equivariant neural networks.\nDefinition 3. Let N be a feed-forward neural network as defined in Definition 1, and G be a group that acts on each index space X0, . . . ,XL. Let T0,T1, . . . ,TL be the corresponding actions on LV0(X0), . . . , LVL(XL). We say that N is a G–equivariant feed-forward network if, when the inputs are transformed f0 7→ T0g(f0) (for any g ∈ G), the activations of the other layers correspondingly transform as f` 7→ T`g(f`).\nIt is important to note how general the above framework is. In particular, we have not said whether G and X0, . . . ,XL are discrete or continuous. In any actual implementation of a neural network, the index sets would of course be finite. However, it has been observed before that in certain cases, specifically when X0 is an object such as the sphere or other manifold which does not have a discretization that fully takes into account its symmetries, it is easier to describe the situation in terms of abstract “continuous” neural networks than seemingly simpler discrete ones (Cohen et al., 2018).\nNote also that invariance is a special case of equivariance, where Tg = id for all g. In fact, this is another major reason why equivariant architectures are so prevalent in the literature: any equivariant network can be turned into a G–invariant network simply by tacking on an extra layer that is equivariant in this degenerate sense (in practice, this often means either averaging or creating a histogram of the activations of the last layer). Nowhere is this more important than in graph learning, where it is a hard constraint that whatever representation is learnt by a neural network, it must be invariant to reordering the vertices. Today’s state of the art solution to this problem are message passing networks (Gilmer et al., 2017), whose invariance behavior we discuss in section 6. Another architecture that achieves invariance by stacking equivariant layers followed by a final invariant one is that of scattering networks (Mallat, 2012)."
  }, {
    "heading": "4. Convolution on groups and quotient spaces",
    "text": "According to its usual definition in signal processing, the convolution of two functions f, g : R→ R is\n(f ∗ g)(x) = ∫ f(x−y) g(y) dy. (4)\nIntuitively, we can think of f as a template and g as a modulating function (or the other way round, since convolution on R is commutative): we get f ∗ g by a placing a “copy” of f at each point on the x axis, but scaled by the value of g at that point, and superimposing the results. The discrete variant of (4) for f, g : Z→ R is of course\n(f ∗ g)(x) = ∑ y∈Z f(x− y) g(y), (5)\nand both the above formulae have natural generalizations to higher dimensions. In particular, (3) is just the two dimensional version of (5) with a limited width filter.\nWhat we are interested in for this paper, however, is the much broader generalization of convolution to the case when f and g are functions on a compact group G. As mentioned in the Introduction, this takes the form\n(f ∗ g)(u) = ∫ G f(uv−1) g(v) dµ(v). (6)\nNote that (6) only differs from (4) in that x−y is replaced by the group operation uv−1, which is not surprising, since the group operation on R in fact is exactly (x, y) 7→ x+y, and the “inverse” of y in the group sense is −y. Furthermore, the Haar measure µ makes an appearance. At this point, the main reason that we restrict ourselves to compact groups is because this guarantees that µ is essentially unique1. The discrete counterpart of (6) for countable (including finite) groups is\n(f ∗ g)(u) = ∑ v∈G f(uv−1) g(v). (7)\nAll these definitions are standard and have deep connections to the algebraic properties of groups. In contrast, the various extensions of convolution to homogeneous spaces that we derive below are not often discussed in pure algebra."
  }, {
    "heading": "4.1. Convolution on quotient spaces",
    "text": "The major complication in neural networks is that X0, . . . ,XL (which are the spaces that the f0, . . . , fL activations are defined on) are homogeneous spaces ofG, rather than being G itself. Fortunately, the strong connection between the structure of groups and their homogeneous spaces (see boxed text) allows generalizing convolution to this case as well. Note that from now on, to keep the exposition as simple as possible, we present our results assuming that G is countable (or finite). The generalization to continuous groups is straightforward. We also allow all our functions to be complex valued, because representation theory itself, which is the workhorse behind our results, is easiest to formulate over C.\nDefinition 4. Let G be a finite or countable group, X and Y be (left or right) quotient spaces of G, f : X → C, and g : Y → C. We then define the convolution of f with g as\n(f ∗ g)(u) = ∑ v∈G f↑G(uv−1) g↑G(v), u∈G. (8) This definition includes X =G or Y =G as special cases, since any group is a quotient space of itself with respect to the trivial subgroup H = {e}.\nDefinition 4 hides the facts that depending on the choice of X and Y : (a) the summation might only have to extend over a quotient space of G rather than the entire group, (b) the result f ∗ g might have symmetries that effectively make it a function on a quotient space rather than G itself (this is exactly what the case will be in generalized convolutional networks). Therefore we now discuss three special cases.\n1Non-compact groups would also cause trouble because their representation theory is much more involved. R2, which is the group behind traditional CCNs, is of course not compact. The reason that it is still amenable to our analysis (with small modifications) is that it belongs to one of a handful of families of exceptional non-compact groups that are easy to handle.\nESSENTIAL DEFINITIONS FOR QUOTIENT SPACES Certain connections between the structure of a group G and its homogeneous space X are crucial for our exposition. First, by definition, fixing an “origin” x0 ∈X , any x∈X can be reached as x= g(x0) for some g ∈G. This allows us to “index” elements of X by elements of G. Since we use this mechanism so often, we introduce the shorthand [g ]X = g(x0), which hides the dependence on the (arbitrary) choice of x0. Second, elementary group theory tells us that the set of group elements that fix x0 actually form a subgroup H . By further elementary results (see Appendix), the set of group elements that map x0 7→ x is a so-called left coset gH := {gh | h∈H }. The set of all such cosets forms the (left) quotient space G/H . Therefore, X can be identified with G/H . Now for each gH coset we may pick a coset representative g′ ∈ gH , and let x denote the representative of the coset of group elements that map x0 to x. Note that while the map g 7→ [g ]G/H is well defined, the map x 7→ x going in the opposite direction is more arbitary, since it depends on the choice of coset representatives. The right quotient space H\\G is similarly defined as the space of right cosets Hg := {hg | h∈H }. Furthermore, if K is another subgroup of G, we can talk about double cosets HgK = {hgk | h∈H, k ∈K } and the corresponding space H\\G/K. Given f : G→C, we define its projection to X =G/H\nf↓X : X → C f↓X (x) = 1 |H| ∑ g∈xH f(g).\nConversely, given f : X → C, we define the lifting of f to G\nf↑G : G→ C f↑G(g) = f([g ]X ).\nProjection and lifting to/from right quotient spaces and double quotient spaces is defined analogously.\nCASE I: X =G AND Y =G/H\nWhen f : G→ C but g : G/H→ C for some subgroup H of G, (8) reduces to\n(f ∗ g)(u) = ∑ v∈G f(uv−1) g↑G(v).\nPlugging u′ = uh into this formula (for any h ∈ H) and changing the variable of summation to w := vh−1 gives\n(f ∗ g)(u′) = ∑ v∈G f(uhv−1) g↑G(v)\n= ∑ w∈G f(uw−1) g↑G(wh).\nHowever, since w and wh are in the same left H–coset, g↑G(wh) = g↑G(w), so (f ∗ g)(u′) = (f ∗ g)(u), i.e.,\nf ∗ g is constant on left H–cosets. This makes it natural to interpret f ∗ g as a function on G/H rather than the full group. Thus, we have the following definition.\nIf f : G→C, and g : G/H→C then f ∗g : G/H → C with (f ∗ g)(x) = ∑ v∈G f(xv−1) g([v ]G/H). (9)\nCASE II: X =G/H AND Y =H\\G\nWhen f : G/H → C, but g : G→ C, (8) reduces to\n(f ∗ g)(u) = ∑ v∈G f↑G(uv−1) g(v). (10)\nThis time it is not f∗g, but g that shows a spurious symmetry. Letting v′= hv (for any h∈H), by the right H–invariance of f↑G, f↑G(uv′−1) = f↑G(uv−1h−1) = f↑G(uv). Considering that any v can be uniquely written as v = hy, where y is the representative of one of its cosets, while h∈H , we get that (10) factorizes in the form\n(f ∗ g)(u) = ∑\ny∈H\\G f↑G(uy−1) ∑ h∈H g(hy)\n= ∑\ny∈H\\G\nf↑G(uy−1) g̃(y),\nwhere g̃(y) := ∑ h∈H g(hy). In other words, without loss of generality we can take g to be a function on H\\G rather than the full group.\nIf f : G/H → C, and g : H\\G→ C, then f ∗g : G→ C with\n(f ∗ g)(u) = |H| ∑\ny∈H\\G\nf([uy−1 ]G/H) g(y). (11)\nCASE III: X =G/H AND Y =H\\G/K\nFinally, we consider the case when f : G/H → C and g : G/K → C for two subgroups H,K of G, which might or might not be the same. This combines features of the above two cases in the sense that, similarly to Case I, setting u′= uk for any k ∈K and letting w = vk−1,\n(f ∗ g)(u′) = ∑ v∈G f↑G(u′v−1) g↑G(v) =\n= ∑ v∈G f↑G(ukv−1) g↑G(v) = ∑ w∈G f↑G(uw−1) g↑G(wk)\n= ∑ w∈G f↑G(uw−1) g↑G(w) = (f ∗ g)(u),\nshowing that f ∗ g is right K–invariant, and therefore can be regarded as a function G/K → C. At the same time, similarly to (10), letting v = hy,\n(f ∗ g)(u) = ∑\ny∈H\\G f↑G(uy−1) ∑ h∈H g↑G(hy)\n= ∑\ny∈H\\G\nf↑G(uy−1) g̃(y),\nwhere g̃(y) := ∑ h∈H g(hy), which is left H–invariant. Therefore, without loss of generality, we can take g to be a function H\\G/K → C.\nIf f : G/H → C, and g : H\\G/K → C then we define the convolution of f with g as f ∗ g : G/K → C with\n(f ∗ g)(x) = |H| ∑\ny∈H\\G\nf([xy−1 ]X ) g([y ]H\\G/K).\n(12)\nSince f 7→ f ∗ g is a map from one homogeneous space, X = G/H , to another homogeneous space, Y = H/K, it is this last defintion that will be of most relevance to us in constructing neural networks."
  }, {
    "heading": "4.2. Relationship to Fourier analysis",
    "text": "The nature of convolution on homogeneous spaces is further explicated by considering its form in Fourier space (see (Terras, 1999)). Recall that the Fourier transform of a function f on a countable group is defined\nf̂(ρi) = ∑ u∈G f(u)ρi(u), i = 0, 1, 2, . . . , (13)\nwhere ρ0, ρ1, . . . are matrix valued functions called irreducible representations or irreps of G (see the Appendix for details). As expected, the generalization of this to the case when f is a function on G/H , H\\G or H\\G/K is\nf̂(ρi) = ∑ u∈G ρi(u)f↑G(u), i = 1, 2, . . . .\nAnalogous formulae hold for continuous groups, involving integration with respect to the Haar measure.\nAt first sight it might be surprising that the Fourier transform of a function on a quotient space consists of the same number of matrices of the same sizes as the Fourier transform of a function on G itself, since G/H , H\\G or H\\G/K are smaller objects than G. This puzzle is resolved by the following proposition, which tells us that in the latter cases, the Fourier matrices have characteristic sparsity patterns.\nProposition 1. Let ρ be an irrep of G, and assume that on restriction to H it decomposes into irreps of H in the\nform ρ|H = µ1 ⊕ µ2 ⊕ . . . ⊕ µk. Let f̂ be the Fourier transform of a function f : G/H → C. Then [f̂(ρ)]∗,j = 0 unless the block at column j in the decomposition of ρ|H is the trivial representation. Similarly, if f : H\\G → C, then [f̂(ρ)]i,∗ = 0 unless the block of ρ|H at row i is the trivial representation. Finally, if f : H\\G/K → C, then [f̂(ρ)]i,j = 0 unless the block of ρ|H at row i is the trivial representation of H and the block at column j in the decomposition of ρ|K is the trivial representation of K.\nSchematically, this proposition implies that in the three different cases, the Fourier matrices have three different forms of sparsity:\nG/K H\\G H\\G/K\nFortuitously, just like in the classical, Euclidean case, convolution also takes on a very nice form in the Fourier domain, even when f or g (or both) are defined on homogeneous spaces.\nProposition 2 (Convolution theorem on groups). Let G be a compact group, H and K subgroups of G, and f, g be complex valued functions on G, G/H , H\\G or H\\G/K. In any combination of these cases,\nf̂ ∗g(ρi) = f̂(ρi) ĝ(ρi) (14)\nfor any given system of irrepsRG = {ρ0, ρ1, . . .}.\nPlugging in matrices with the appropriate sparsity patterns into (14) now gives us an intuitive way of thinking about Cases I–III above.\nCASE I: X =G AND Y =G/H\nMutiplying a column sparse matrix with a dense matrix from the left gives a column sparse matrix with the same pattern, therefore f ∗ g is a function on G/H: \nf̂ ∗ g(ρ)\n=   f̂(ρ) ×   ĝ↑G(ρ) .\nCASE II: X =G/H AND Y =H\\G\nMultiplying a column sparse matrix from the right by another matrix picks out the corresponding rows of the second matrix. Therefore, if f is a function on G/H , then\nw.l.o.g. we can take g to be a function on H\\G.  f̂ ∗ g(ρ) =   f̂↑G(ρ) ×   ĝ↑G(ρ) .\nCASE III: f : G/H → C AND g : H\\G/K → C\nFinally, if f is a function onG/H , and we want to make f∗g to be a function on G/K, then we should take g : H\\G/K: \nf̂ ∗ g(ρ)\n=   f̂↑G(ρ) ×   ĝ↑G(ρ) ."
  }, {
    "heading": "5. Main result: the connection between convolution and equivariance",
    "text": "We are finally in a position to define the notion of generalized convolutional networks, and state our main result connecting convolutions and equivariance.\nDefinition 5. Let G be a compact group andN an L+1 layer feed-forward network in which the i’th index set is G/Hi for some subgroup Hi of G. We say that N is a G–convolutional neural network (or G-CNN for short) if each of the linear maps φ1, . . . , φL in N is a generalized convolution (see Definition 4) of the form\nφ`(f`−1) = f`−1 ∗ χ`\nwith some filter χ` ∈LV`−1×V`(H`−1\\G/H`).\nTheorem 1. Let G be a compact group and N be an L + 1 layer feed-forward neural network in which the `’th index set is of the form X` = G/H`, where H` is some subgroup of G. ThenN is equivariant to the action of G in the sense of Definition 3 if and only if it is a G-CNN.\nProving this theorem in the forward direction is relatively easy and only requires some elementary facts about cosets and group actions.\nProof of Theorem 1 (forward direction). Assume that we translate f`−1 by some group element g ∈G and get f ′`−1, i.e., f ′`−1 = T`−1g (f`−1), where f ′`−1(x) = f`−1(g−1x). Then\nφ`(f ′ `−1)(u) = (f ′ `−1 ∗ χ`)(u) = ∑ v∈G f ′`−1([uv −1]X )χ`(v)\n= ∑ v∈G f`−1(g −1([uv−1]X ))χ`(v).\nBy g−1([uv−1]X ) = [g−1uv−1]X this is further equal to∑ v∈G f`−1([g −1uv−1]X )χ`(v)\n= (f`−1 ∗ χ`)(g−1u) = φ`(f`−1)(g−1u).\nTherefore, φ`(f`−1) is equivariant with f`−1. Since σ` is a pointwise operator, so is f` = σ`(φ`(f`−1)). By induction on `, using the transitivity of equivariance, this implies that every layer of N is equivariant with layer 0. Note that this proof holds not only in the base case, when each f` is a function X → C, but also in the more general case when f` : X` → V` and the filters are χ` : X` → V`−1 × V`.\nProving the “only if” part of Theorem 1 is more technical, therefore we leave it to the Appendix."
  }, {
    "heading": "6. Examples of algebraic convolution in neural networks",
    "text": "We are not aware of any prior papers that have exposed the above algebraic theory of equivariance and convolution in its full generality. However, there are a few recent publications that implicitly exploit these ideas in specific contexts."
  }, {
    "heading": "6.1. Rotation equivariant networks",
    "text": "In image recognition applications it is a natural goal to achieve equivariance to both translation and rotation. The most common approach is to use CNNs, but with filters that are replicated at a certain number of rotational angles (typically multiples of 90 degrees), connected in such as a way as to achieve a generalization of equivariance called steerability. Steerability also has a group theoretic interpretation, which is most lucidly explained in (Cohen & Welling, 2017).\nThe recent papers (Marcos et al., 2017) and (Worrall et al., 2017) extend these architectures by considering continuous rotations at each point of the visual field. Thus, putting aside the steerability aspect for now and only considering the behavior of the network at a single point, both these papers deal with the case where G = SO(2) (the two dimensional rotation group) and X is the circle S1. The group SO(2) is commutative, therefore its irreducible representations are one dimensional, and are, in fact, ρj(θ) = e2πιjθ, where ι = √ −1. While not calling it a group Fourier transform, Worrall et al. (2017) explicitly expand the local activations\nin this basis and scale them with weights, which, by virtue of Proposition 2, amounts to convolution on the group, as prescribed by our main theorem.\nThe form of the nonlinearity in (Worrall et al., 2017) is different from that prescribed in Definition 3, which leads to a coupling between the indices of the Fourier components in any path from the input layer to the output layer. This is compensated by what they call their “equivariance condition”, asserting that only Fourier components for which M = ∑ ` j` is the same may mix. This restores equivariance in the last layer, but analyzing it group theoretically is beyond the scope of the present paper."
  }, {
    "heading": "6.2. Spherical CNNs",
    "text": "Very close in spirit to our present exposition are the recent papers (Cohen et al., 2018; Kondor et al., 2018), which propose convolutional architectures for recognizing images painted on the sphere, satisfying equivariance with respect to rotations. Thus, in this case, G = SO(3), the group of three dimensional rotations, and X` is the sphere, S2.\nThe case of rotations acting on the sphere is one of the textbook examples of continuous group actions. In particular, letting x0 be the North pole, we see that two-dimensional rotations in the x–z plane fix x0, therefore, S2 is identified with the quotient space SO(3)/SO(2). The irreducible representations of SO(3) are given by the so-called Wigner matrices. The `’th irreducible representation is 2`+ 1 dimensional and of the form\n[ρ`(θ, φ, ψ)]m,m′ = e −ιm′φ d`m′,m(θ) e −ιmψ,\nwhere m,m′ ∈ {−`, . . . , `}, (θ, φ, ψ) are the Euler angles of the rotation and the d`m′,m(θ) funcion is related to the spherical harmonics. It is immediately clear that on restriction to SO(2) (corresponding to θ, φ = 0) only the middle column in each of these matrices reduces to the trivial representation of SO(2), therefore, by Proposition 1, in the case f : SO(3)/SO(2) → C, only the middle column of each f̂(ρ`) matrix will be nonzero. In fact, up to constant scaling factors, the entries in that middle column are just the customary spherical harmonic expansion coefficients.\nCohen et al. (2018) explicitly make this connection between spherical harmonics and SO(3) Fourier transforms, and store the activations in terms of this representation. Moreover, just like in the present paper, they define convolution in terms of the noncommutative convolution theorem (Proposition 2), use pointwise nonlinearities, and prove that the resulting neural network is SO(3)–equivariant. However, they do not prove the converse, i.e., that equivariance implies that the network must be convolutional. To apply the nonlinearity, the algorithm presented in (Cohen et al., 2018) requires repeated forward and backward SO(3) fast Fourier transforms. While this leads to a non-conventional archi-\ntecture, the discussion echoes our observation that when dealing with continuous symmetries such as rotations, one must generalize to more abstract “continuous” neural networks, as afforded by Definition 3."
  }, {
    "heading": "6.3. Message passing neural networks",
    "text": "There has been considerable interest in extending the convolutional network formalism to learning from graphs (Niepert et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015), and the current consensus for approaching this problem is to use neural networks based on the message passing idea (Gilmer et al., 2017). Let G be a graph with n vertices. Message passing neural networks (MPNNs) are usually presented in terms of an iterative process, where in each round `, each vertex v collects the labels of its neighbors w1, . . . , wk, and updates its own label f̃v according to a simple formula such as\nf̃ `v = Φ ( f̃ `−1w1 + . . .+ f̃ `−1 wk ) .\nAn equivalent way of seeing this process, however, is in terms of the “effective receptive fields” S`v of each vertex at round `, i.e., the set of all vertices from which information can propagate to v by round `.\nMPNNs can also be viewed as group convolutional networks. A receptive field of size k is just a subset {s1, . . . , sk} ⊆ {1, 2, . . . , n}, and the symmetric group Sn (the group of permutations of {1, 2, . . . , n}) acts on the set of such subsets transitively by {s1, . . . , sk} σ7→ {σ(s1), . . . , σ(sk)} σ ∈ Sn.\nSince permuting the n− k vertices not in S amongst themselves, as well as permuting the k vertices that are in S both leave S invariant, the stablizier of this action is Sn−k × Sk. Thus, the set of all k-subsets of vertices is identified with the quotient space X = Sn/(Sk × Sn−k), and the labeling function for k-element receptive fields is identified with fk : X → C. Effectively, this turns the MPNN into a generalized feed-forward network in the sense of Definition 3. Note that fk is a redundant representation of the labeling function because Sn/(Sk×Sn−k) also includes subsets that do not correspond to contiguous neighborhoods. However this is not a problem because for such S we simply set fk(S) = 0.\nThe key feature of the message passing formalism is that, by construction, it ensures that the f̃ `v labels only depend on the graph topology and are invariant to renumbering the vertices of G. In terms of our “k–subset network” this means that each fk must be Sn–equivariant. Thus, in contrast to the previous two examples, now each index set X` = Sn/(Sn−` × S`) is different. The form of the corresponding convolutions LV`−1(X`−1) → LV`(X`) are best described in the Fourier domain. Unfortunately, the representation theory of symmetric groups is beyond the scope of the present paper (Sagan, 2001). We content ourselves\nby stating that the irreps of Sn are indexed by so-called integer partitions, (λ1, . . . , λm), where λ1 ≥ . . . ≥ λm and∑ i λi = n. Moreover, the structure of the Fourier transform of a function f : Sn/(Sn−`×S`) dictated by Proposition 1 in this case is that each of the Fourier matrices are zero except for a single column in each of the f̂((n−p, p)) components, where 0 ≤ p ≤ `. The main theorem of our paper dictates that the linear map φ` in each layer must be a convolution. In the case of Fourier matrices with such extreme sparsity structure, this means that each of the `+1 Fourier matrices can be multiplied by a scalar, χ`p. These are the learnable parameters of the network. A real MPNN of course has multiple channels and various corresponding parameters, which could also be introduced in the k–subset network. The above observation about the form of χ` is nonetheless interesting, because it at once implies that permutation equivariance is a severe constraint the significantly limits the form of the convolutional filters, yet the framework is still richer than traditional MPNNs where the labels of the neighbors are simply summed."
  }, {
    "heading": "7. Conclusions",
    "text": "Convolution has emerged as one of the key organizing principles of deep neural network architectures. Nonetheless, depending on their background, the word “convolution” means different things to different researchers. The goal of this paper was to show that in the common setting when there is a group acting on the data that the architecture must be equivariant to, convolution has a specific mathematical meaning that has far reaching consequences: we proved that a feed forward network is equivariant to the group action if and only if it respects this notion of convolution.\nOur theory gives a clear prescription to practitioners on how to design neural networks for data with non-trivial symmetries, such as data on the sphere, etc.. In particular, we argue for Fourier space representations, similar to those that have appeared in (Worrall et al., 2017; Cohen et al., 2018; Kondor et al., 2018)), and, even more recently, since the submission of the original version of the present paper in (Thomas et al., 2018; Kondor, 2018; Weiler et al., 2018)."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported in part by DARPA Young Faculty Award D16AP00112."
  }],
  "year": 2018,
  "references": [{
    "title": "Group equivariant convolutional networks",
    "authors": ["T.S. Cohen", "M. Welling"],
    "venue": "Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
    "authors": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Convolutional networks on graphs for learning molecular fingerprints",
    "authors": ["D. Duvenaud", "D. Maclaurin", "J. Aguilera-Iparraguirre", "R. Gomez-Bombarelli", "T. Hirzel", "A. Aspuru-Guzik", "R.P. Adams"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Neural message passing for quantum chemistry",
    "authors": ["J. Gilmer", "S.S. Schoenholz", "P.F. Riley", "O. Vinyals", "G.E. Dahl"],
    "venue": "Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials",
    "authors": ["R. Kondor"],
    "venue": "ArXiv e-prints,",
    "year": 2018
  }, {
    "title": "Clebsch–Gordan nets: a fully Fourier space spherical convolutional neural network",
    "authors": ["R. Kondor", "Z. Lin", "S. Trivedi"],
    "venue": "ArXiv e-prints,",
    "year": 2018
  }, {
    "title": "Backpropagation applied to handwritten zip code recognition",
    "authors": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"],
    "venue": "Neural Computation,",
    "year": 1989
  }, {
    "title": "Gated graph sequence neural networks",
    "authors": ["Y. Li", "D. Tarlow", "M. Brockschmidt", "R. Zemel"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2016
  }, {
    "title": "Group invariant scattering",
    "authors": ["S. Mallat"],
    "venue": "Communications on Pure and Applied Mathematics,",
    "year": 2012
  }, {
    "title": "Rotation equivariant vector field networks",
    "authors": ["D. Marcos", "M. Volpi", "N. Komodakis", "D. Tuia"],
    "venue": "In International Conference on Computer Vision (ICCV),",
    "year": 2017
  }, {
    "title": "Geodesic convolutional neural networks on Riemannian manifolds",
    "authors": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"],
    "venue": "International Conference on Computer Vision Workshop (ICCVW),",
    "year": 2015
  }, {
    "title": "Geometric deep learning on graphs and manifolds using mixture model cnns",
    "authors": ["F. Monti", "D. Boscaini", "J. Masci", "E. Rodolà", "J. Svoboda", "M.M. Bronstein"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2017
  }, {
    "title": "Learning convolutional neural networks for graphs",
    "authors": ["M. Niepert", "M. Ahmed", "K. Kutzkov"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Equivariance through parameter-sharing",
    "authors": ["S. Ravanbakhsh", "J. Schneider", "B. Poczos"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "The Symmetric Group",
    "authors": ["B.E. Sagan"],
    "venue": "Graduate Texts in Mathematics. Springer,",
    "year": 2001
  }, {
    "title": "Fourier analysis on finite groups and applications, volume 43 of London Mathematical Society Student Texts",
    "authors": ["A. Terras"],
    "year": 1999
  }, {
    "title": "Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds",
    "authors": ["N. Thomas", "T. Smidt", "S.M. Kearnes", "L. Yang", "L. Li", "K. Kohlhoff", "P. Riley"],
    "venue": "Arxiv e-prints,",
    "year": 2018
  }, {
    "title": "3D steerable CNNs: Learning rotationally equivariant features in volumetric data",
    "authors": ["M. Weiler", "M. Geiger", "M. Welling", "W. Boomsma", "T. Cohen"],
    "venue": "ArXiv e-prints,",
    "year": 2018
  }, {
    "title": "Harmonic networks: Deep translation and rotation equivariance",
    "authors": ["D.E. Worrall", "S.J. Garbin", "D. Turmukhambetov", "G.J. Brostow"],
    "venue": "In Proceedings of International Conference on Computer Vision and Pattern Recognition",
    "year": 2017
  }],
  "id": "SP:e5733c587e84825396c71c1b7d6d99b3488ceaff",
  "authors": [{
    "name": "Risi Kondor",
    "affiliations": []
  }, {
    "name": "Shubhendu Trivedi",
    "affiliations": []
  }],
  "abstractText": "Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.",
  "title": "On the Generalization of Equivariance and Convolution in Neural Networks  to the Action of Compact Groups"
}