{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2013, pages 221–229, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics\nAddressee detection (AD) is an important problem for dialog systems in human-humancomputer scenarios (contexts involving multiple people and a system) because systemdirected speech must be distinguished from human-directed speech. Recent work on AD (Shriberg et al., 2012) showed good results using prosodic and lexical features trained on in-domain data. In-domain data, however, is expensive to collect for each new domain. In this study we focus on lexical models and investigate how well out-of-domain data (either outside the domain, or from single-user scenarios) can fill in for matched in-domain data. We find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts, and that human-computer utterances can be modeled using single-user data: the resulting AD system outperforms a system trained only on matched in-domain data. Further gains (up to a 4% reduction in equal error rate) are obtained when in-domain and out-of-domain models are interpolated. Finally, we examine which parts of an utterance are most useful. We find that the first 1.5 seconds of an utterance contain most of the lexical information for AD, and analyze which lexical items convey this. Overall, we conclude that the H-H-C scenario can be approximated by combining data from H-C and H-H scenarios only.\n∗Work done while first author was an intern with Microsoft."
  }, {
    "heading": "1 Introduction",
    "text": "Before a spoken dialog system can recognize and interpret a user’s speech, it should ideally determine if speech was even meant to be interpreted by the system. We refer to this task as addressee detection (AD). AD is often overlooked, especially in traditional single-user scenarios, because with the exception of self-talk, side-talk or background speech, the majority of speech is usually system-directed. As dialog systems expand to more natural contexts and multiperson environments, however, AD can become a crucial part of the system’s operational requirements. This is particularly true for systems in which explicit system addressing (e.g., push-to-talk or required keyword addressing) is undesirable.\nPast research on addressee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009). Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (Paek et al., 2000), or because they yielded low speech recognition confidence (Dowding et al., 2006). Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004). Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR) (Reich et al., 2011).\nWith some exceptions (Bohus and Horvitz, 2011; Shriberg et al., 2012), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other.\n221\nShriberg et al. (2012) found that novel prosodic features were more accurate than lexical or semantic features based on speech recognition for the addressee task. The corpus, also used herein, is comprised of H-H-C dialog in which roughly half of the computer-addressed speech consisted of a small set of fixed commands. While the word-based features map directly to the commands, they had trouble distinguishing all other (noncommand) computerdirected speech from human-directed speech. This is because addressee detection in the H-H-C scenario becomes even more challenging when the system is designed for natural speech, i.e., utterances that are conversational in form and not limited to command phrases with restricted syntax. Furthermore, H-H utterances can be about the domain of the system (e.g., discussing the dialog task), making AD based on language content more difficult. The prosodic features were good at both types of distinctions—even improving performance significantly when combined with true-word (cheating) lexical features that have 100% accuracy on the commands. Nevertheless, the prior work showed that lexical n-grams are useful for addressee detection in the H-H-C scenario.\nA problem with lexical features is that they are highly task- and domain-dependent. As with other language modeling tasks, one usually has to collect matched training data in significant quantities. Data collection is made more cumbersome and expensive by the multi-user aspect of the scenario. Thus, for practical reasons alone, it would be much better if the language models for AD could be trained on out-of-domain data, and if whatever in-domain data is needed could be limited to single-user interaction. We show in this paper that precisely this training scenario is feasible and achieves results that are comparable or better than using completely matched H-H-C training data.\nIn addition to studying the role of out-of-domain data for lexical AD models, we also examine which words are useful, and how soon in elapsed time they are available. Whereas most prior work in AD has looked at processing of entire utterances, we consider an online processing version where AD decisions are to be made as soon as possible after an utterance was initiated. We find that most of the addressee-relevant lexical information can be found\nin the first 1.5 seconds, and analyze which words convey this information."
  }, {
    "heading": "2 Data",
    "text": "We use in-domain and out-of-domain data from various sources. The corpora used in this work differ in size, domain, and scenario."
  }, {
    "heading": "2.1 In-domain data",
    "text": "In-domain data is collected from interactions between two users and a “Conversational Browser” (CB) spoken dialog system. We used the same methodology as Shriberg et al. (2012), but using additional data. As depicted in Figure 1, the system shows a browser on a large TV screen and users are asked to use natural language for a variety of information-seeking tasks. For more details about the dialog system and language understanding approach, see Hakkani-Tür et al. (2011a; 2011b).\nWe split the in-domain data into training, development, and test sets, preserving sessions. Each session is about 5 to 40 minutes long. Even though the whole conversation is recorded, only the segments captured by the speech recognition system are used in our experiments. Each utterance segment belongs to one of four types: computercommand (C-command), comprising navigational commands to the system, computer-noncommand (C-noncommand), which are computer-directed utterances other than commands, human-directed (H), and mixed (M) utterances, which contain a combina-\ntion of human- and computer-directed speech. The sizes and distribution of all utterance types, as well as sample utterances are shown in Table 1.\nThe ASR system used in the system was based on off-the-shelf acoustic models and had only the language model adapted to the domain, using very limited data. Consequently, as shown in the right-most column of Table 1(a), the word error rates (WERs) are quite high, especially for human-directed utterances. While these could be improved with targeted effort, we consider this a realistic application scenario, where in-domain training data is typically scarce, at least early in the development process. Therefore, any lexically based AD methods need to be robust to poor ASR accuracy."
  }, {
    "heading": "2.2 Out-of-domain data",
    "text": "To replace the hard-to-obtain in-domain H-H-C data for training, we use the four out-of-domain corpora (two H-C and two H-H) shown in Table 2.\nSingle-user CB data comes from the same Conversational Browser system as the in-domain data, but with only one user present. This data can therefore be used for modeling H-C speech. Bing anchor text (Huang et al., 2010) is a large n-gram corpus of anchor text associated with links on web pages en-\ncountered by the Bing search engine. When users want to follow a link displayed on screen, they usually speak a variant of the anchor text for the link. We hypothesized that this corpus might aid the modeling of computer-noncommand type utterances in which such “verbal clicks” are frequent. Fisher telephone conversations and ICSI meetings are both corpora of human-directed speech. The Fisher corpus (Cieri et al., 2004) comprises two-person telephone conversations between strangers on prescribed topics. The ICSI meeting corpus (Janin et al., 2003) contains multiparty face-to-face technical discussions among colleagues."
  }, {
    "heading": "3 Method",
    "text": ""
  }, {
    "heading": "3.1 Language modeling for addressee detection",
    "text": "We use a lexical AD system that is based on modeling word n-grams in the two addressee-based utterance classes, H (for H-H) and C (for H-C utterances). This approach is similar to language modelbased approaches to speaker and language recognition, and was shown to be quite effective for this task (Shriberg et al., 2012). Instead of making hard decisions, the system outputs a score that is\nthe length-normalized likelihood ratio of the two classes:\n1\n|w| log P (w|C) P (w|H) , (1)\nwhere |w| is the number of words in the recognition output w for an utterance. P (w|C) and P (w|H) are obtained from class-specific language models. Figure 2 gives a flow-chart of the score computation.\nClass likelihoods are obtained from standard trigram backoff language models, using Witten-Bell discounting for smoothing (Witten and Bell, 1991). For combining various training data sources, we use language model adaptation by interpolation (Bellegarda, 2004). First, a separate model is trained from each source. The probability estimates from in-domain and out-of-domain models are then averaged in a weighted fashion:\nP (wk|hk) = λPin(wk|hk) + (1− λ)Pout(wk|hk) (2) where wk is the k-th word, hk is the (n − 1)-gram history for the wordwk. λ is the interpolation weight and is obtained by tuning a task-related metric on the development set. We investigated optimizing λ for either model perplexity or classification accuracy, as discussed below."
  }, {
    "heading": "3.2 Part-of-speech-based modeling",
    "text": "So far we have only been modeling the lexical forms of words in utterances. If we encounter a word never before seen, it would appear as an out-of-vocabulary item in all class-specific language models, and not contribute much to the decision. More generally, if a word is rare, its n-gram statistics will be unreliable and poorly modeled by the system. (The sparseness issue is exacerbated by small amounts of training data as in our scenario.)\nOne common approach to deal with data sparseness in language modeling is to model n-grams over word classes rather than raw words (Brown et al., 1992). For example, if we have an utterance How is the weather in Paris?, the addressee probabilities are likely to be similar had we seen London instead of Paris. Therefore, replacing words with properly chosen word class labels can give better generalization from the observed training data. Among the many methods proposed to class words for language modeling purposes we chose part-of-speech (POS)\ntagging over other, purely data-derived classing algorithms (Brown et al., 1992), for two reasons. First, our goal here is not to minimize the perplexity of the data, but to enhance discrimination among utterance classes. Second, a data-driven class inference algorithm would suffer from the same sparseness issues when it comes to unseen and rare words (as no robust statistics are available to infer an unseen word’s best class in the class induction step). A POS tagger, on the other hand, can do quite well on unseen words, using context and morphological cues.\nA hidden Markov model tagger using POStrigram statistics and context-independent class membership probabilities was used for tagging all LM training data. The tagger itself had been trained on the Switchboard (conversational telephone speech) transcripts of the Penn Treebank3 corpus (Marcus et al., 1999), and used the 39 Treebank POS labels. To strike a compromise between generalization and discriminative power in the language model, we retained the topN most frequent word types from the in-domain training data as distinct tokens, and varied N as a metaparameter. Barzilay and Lee (2003) used a similar idea to generalize patterns by substituting words with slots. This strategy will tend to preserve words that are either generally frequent function and domainindependent words, capturing stylistic and syntactic patterns, or which are frequent domain-specific words, and can thus help characterize computerdirected utterances.\nHere is a sample sentence and its transformed version:\nOriginal: Let’s find an Italian restaurant around this area. POS-tagged: Let’s find an JJ NN around this area.\nThe words except Italian and restaurant are unchanged because they are in the list of N most frequent words. We transformed all training and test data in this fashion and then modeled n-gram statistics as before. The one exception was the Bing anchor-text data, which was only available in the form of word n-grams (the sentence context required for accurate POS tagging was missing)."
  }, {
    "heading": "3.3 Evaluation metrics",
    "text": "Typically, an application-dependent threshold would be applied to the decision score to convert it into a binary decision. The optimal threshold is a function of prior class probabilities and error costs. As in Shriberg et al. (2012), we used equal error rate (EER) to compare systems, since we are interested in the discriminative power of the decision score independent of priors and costs. EER is the probability of false detections and misses at the operating point at which the two types of errors are equally probable. A prior-free metric such as EER is more meaningful than classification accuracy because the utterance type distribution is heavily skewed (Table 1), and because the rate of human- versus computerdirected speech can vary widely depending on the particular people, domain, and context. We also use classification accuracy (based on data priors) in one analysis below, because EERs are not comparable for different test data subdivisions."
  }, {
    "heading": "3.4 Online model",
    "text": "The actual dialog system used in this work processes utterances after receiving an entire segment of speech from the recognition subsystem. However, we envision that a future version of the system would perform addressee detection in an online manner, making a decision as soon as enough evidence is gathered. This raises the question how soon the addressee can be detected once the user starts speaking. We simulate this processing mode using a windowed AD model.\nAs shown in Figure 3, we define windows starting at the beginning of the utterance and investigate how AD performance changes as a function of window size. We use only the words and n-grams falling completely within a given window. For example, the word find would be excluded from Window 1 in Fig-\nLet’s find an Italian restaurant around this area\nure 3. The benefit of early detection in this case is that once speech is classified as human-directed, it does not need to be sent to the speech recognizer and subsequent semantic processing. This saves processing time, especially if processing happens on a server. Based on the window model performance, we can assess the feasibility of an online AD model, which can be approached by shifting the detection window through time and finding addressee changes."
  }, {
    "heading": "4 Results and Discussion",
    "text": "Table 3 compares the performance of our system using various training data sources. For diagnostic purposes we also compare performance based on recognized words (the realistic scenario) to that based on human transcripts (idealized, best-case word recognition).\nSomewhat surprisingly, the system trained on outof-domain data alone performs better by 3.3 EER points on ASR output and 3.1 points on transcripts compared to the in-domain baseline. Combining in-domain and out-of-domain data (both-all, bothsmall) gives about 1 point additional EER gain. Note that training on in-domain data plus the smaller-size out-of-domain corpora (both-small) is better than using all available data (both-all).\nFigure 4 shows the detection error trade-off (DET) between false alarm and miss errors for the\nsystems in Table 3. The DET plot depicts performance not only at the EER operating point (which lies on the diagonal), but over the range of possible trade-offs between false alarm and miss error rates. As can be seen, replacing or combining in-domain data with out-of-domain data gives clear performance gains, regardless of operating point (score threshold), and for both reference and recognized words.\nFigure 5 shows H-H vs. H-C classification accuracies on each of the four utterance subtypes listed in Table 1. It is clear that computer-command utterances are the easiest to classify; the accuracy is more than 90% using transcripts, and more than 85% using ASR output. This is not surprising, since commands are from a fixed small set of phrases. The biggest gain from use of out-of-domain data is found for computer-directed noncommand utterances. This is helpful, since in general it is the noncommand computer-directed utterances (rather than the commands) that are highly confusable with human-directed utterances: both use unconstrained natural language. We note that H-H utterance are very poorly recognized in the ASR condition when only out-of-domain data is used. This may be be-\ncause the human-human corpora used in training consist of transcripts, whereas the ASR output for human-directed utterances is very errorful, creating a severe train-test mismatch.\nAs for the optimization of the mixing weight λ, we found that minimizing perplexity on the development set of each class is effective. This is a standard optimization approach for interpolated language models, and can be carried out efficiently using an expectation maximization algorithm. We also tried search-based optimization using the classification metric (EER) as the criterion. While this approach could theoretically give better results (since perplexity is not a discriminative criterion) we found no significant improvement in our experiments.\nTable 4 shows the perplexities by class of language models trained on different corpora. We can take these as an indication of training/test mismatch (lower perplexity indicating better match). We also find substantial perplexity reductions from interpolating models. In order to make perplexities comparable, we trained all models using the union of the vocabularies from the different sources.\nIn spite of perplexity being a good way to optimize the weighting of sources, it is not clear that it is a good criterion for selecting data sources. For example, we see that the Fisher model has a much lower perplexity on H-H utterances than the ICSI meeting model. However, as reflected in Table 3, the H language model that leaves out the Fisher data actually performed better. The most likely explanation is that the Fisher corpus is an order of magnitude larger than the ICSI corpus, and that sheer data size, not stylistic similarity, may account for the lower perplexity of the Fisher model. Further investigation is needed regarding good criteria for corpus selection for classification tasks such as AD.\nTable 5 shows the EER performance of the POSbased model, for various sizes N of the mostfrequent word list. We observe that the partial replacement of words with POS tags indeed improves over the baseline model performance, by 1.5 points on ASR output and by 1.1 points on transcripts. We also see that the gain over the corresponding word-only model is largest for the in-domain baseline model, and less or non-existent for the out-ofdomain model. This is consistent with the notion that the in-domain model suffers the most from data sparseness, and therefore has the most to gain from better generalization.\nInterpolating with out-of-domain data still helps here. The optimal N differs for ASR output versus transcripts. The POS-based model with N = 300 improves the EER by 0.5 points on ASR output, and N = 1000 improves the EER by 0.8 points on transcripts. Here we use relatively large amounts of training data, thus the performance gain is smaller, though still meaningful.\nFigure 6 shows the performance of the system using time windows anchored at the beginnings of utterances. We incrementally increase the window width from 0.5 seconds to 3 seconds and compare results to using full utterances. The leveling off of\nthe error plots indicates that most addressee information is contained in the first 1 to 1.5 seconds, although some additional information is found in the later part of utterances (the plots never level off completely). This pattern holds for both in-domain and out-of-domain training, as well as for combined models.\nTo give an intuitive understanding of where this early addressee-relevant information comes from, we tabulated the top 15 word unigrams in each utterance class, are shown in Table 6. Note that the substantial differences between the third and fourth columns in the table reflect the high ASR error rate for human-directed utterances, whereas\nfor computer-directed utterances, the frequent first words are mostly recognized correctly.\nIn computer-directed utterances we see mostly command verbs, which, due to the imperative syntax of these commands occur in utterance-initial position. Human-directed utterances are characterized by subject pronouns such as I and it, or answer particles such as yeah and okay, which likewise occur in initial position. Based on word frequency and syntax alone it is thus clear why the beginnings of utterances contain strong lexical cues."
  }, {
    "heading": "5 Conclusion",
    "text": "We explored the use of outside data for training lexical addressee detection systems for the humanhuman-computer scenario. Advantages include saving the time and expense of an in-domain data collection, as well as performance gains even when some in-domain data is available. We show that HC training data can be obtained from a single-user H-C collection, and that H-H speech can be modeled using general conversational speech. Using the outside training data, we obtain results that are even better than results using matched (but smaller) HH-C training data. Results can be improved considerably by adapting H-C and H-H language models with small amounts of matched H-H-C data, via interpolation. The main reason for the improvement is better detection of computer-directed noncommand utterances, which tend to be confusable with humandirected utterances. Another effective way to overcome scarce training data is to replace the less frequent words with part-of-speech labels. In both baseline and interpolated model, we found that POS-\nbased models that keep an appropriate number of the topN most frequent word types can further improve the system’s performance.\nIn a second study we found that the most salient phrases for lexical addressee detection occur within the first 1 to 1.5 seconds of speech in each utterance. It reflects a syntactic tendency of class-specific words to occur utterance-initially, which shows the feasibility of the online AD system."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank our Microsoft colleagues Madhu Chinthakunta, Dilek Hakkani-Tür, Larry Heck, Lisa Stiefelman, and Gokhan Tür for developing the dialog system used in this work, as well as for many valuable discussions. Ashley Fidler was in charge of much of the data collection and annotation required for this study. We also thank Dan Jurafsky for useful feedback."
  }],
  "year": 2013,
  "references": [{
    "title": "Learning to paraphrase: An unsupervised approach using multiplesequence alignment",
    "authors": ["Regina Barzilay", "Lillian Lee."],
    "venue": "Proceedings HLT-NAACL 2003, pages 16–23, Edmonton, Canada.",
    "year": 2003
  }, {
    "title": "Statistical language model adaptation: review and perspectives",
    "authors": ["Jerome R. Bellegarda."],
    "venue": "Speech Communication, 42:93–108.",
    "year": 2004
  }, {
    "title": "Multiparty turn taking in situated dialog: Study, lessons, and directions",
    "authors": ["Dan Bohus", "Eric Horvitz."],
    "venue": "Proceedings ACL SIGDIAL, pages 98–109, Portland, OR.",
    "year": 2011
  }, {
    "title": "Classbased n-gram models of natural language",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Peter V. deSouza", "Jenifer C. Lai", "Robert L. Mercer."],
    "venue": "Computational Linguistics, 18(4):467–479.",
    "year": 1992
  }, {
    "title": "The Fisher corpus: a resource for the next generations of speech-to-text",
    "authors": ["Christopher Cieri", "David Miller", "Kevin Walker."],
    "venue": "Proceedings 4th International Conference on Language Resources and Evaluation, pages 69–71, Lisbon.",
    "year": 2004
  }, {
    "title": "Are you talking to me? dialogue systems supporting mixed teams of humans and robots",
    "authors": ["John Dowding", "Richard Alena", "William J. Clancey", "Maarten Sierhuis", "Jeffrey Graham."],
    "venue": "Proccedings AAAI Fall Symposium: Aurally Informed Performance: Inte-",
    "year": 2006
  }, {
    "title": "Research challenges and opportunities in mobile applications [dsp education",
    "authors": ["Dilek Hakkani-Tür", "Gokhan Tur", "Larry Heck."],
    "venue": "IEEE Signal Processing Magazine, 28(4):108 –110.",
    "year": 2011
  }, {
    "title": "Bootstrapping domain detection using query click logs for new domains",
    "authors": ["Dilek Z. Hakkani-Tür", "Gökhan Tür", "Larry P. Heck", "Elizabeth Shriberg."],
    "venue": "Proceedings Interspeech, pages 709–712.",
    "year": 2011
  }, {
    "title": "Exploring web scale language models for search query processing",
    "authors": ["Jian Huang", "Jianfeng Gao", "Jiangbo Miao", "Xiaolong Li", "Kuansang Wang", "Fritz Behr."],
    "venue": "Proceedings 19th International Conference on World Wide Web, pages 451–460, Raleigh, NC.",
    "year": 2010
  }, {
    "title": "The ICSI meeting corpus",
    "authors": ["Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke", "Chuck Wooters."],
    "venue": "Proceedings IEEE ICASSP, volume 1, pages 364–367,",
    "year": 2003
  }, {
    "title": "Identifying the addressee in humanhuman-robot interactions based on head pose and speech",
    "authors": ["Michael Katzenmaier", "Rainer Stiefelhagen", "Tanja Schultz."],
    "venue": "Proceedings 6th International Conference on Multimodal Interfaces, ICMI, pages 144–151, New",
    "year": 2004
  }, {
    "title": "Treebank-3",
    "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz", "Ann Taylor."],
    "venue": "Linguistic Data Consortium, catalog item LDC99T42.",
    "year": 1999
  }, {
    "title": "A comparison of addressee detection methods for multiparty conversations",
    "authors": ["Rieks op den Akker", "David Traum."],
    "venue": "Proceedings of Diaholmia, pages 99–106.",
    "year": 2009
  }, {
    "title": "Continuous listening for unconstrained spoken dialog",
    "authors": ["Tim Paek", "Eric Horvitz", "Eric Ringger."],
    "venue": "Proceedings ICSLP, volume 1, pages 138–141, Beijing.",
    "year": 2000
  }, {
    "title": "A real-time speech command detector for a smart control room",
    "authors": ["Daniel Reich", "Felix Putze", "Dominic Heger", "Joris Ijsselmuiden", "Rainer Stiefelhagen", "Tanja Schultz."],
    "venue": "Proceedings Interspeech, pages 2641– 2644, Florence.",
    "year": 2011
  }, {
    "title": "Learning when to listen: Detecting system-addressed speech in human-humancomputer dialog",
    "authors": ["Elizabeth Shriberg", "Andreas Stolcke", "Dilek Hakkani-Tür", "Larry Heck."],
    "venue": "Proceedings Interspeech, Portland, OR.",
    "year": 2012
  }, {
    "title": "The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression",
    "authors": ["Ian H. Witten", "Timothy C. Bell."],
    "venue": "IEEE Transactions on Information Theory, 37(4):1085– 1094.",
    "year": 1991
  }],
  "id": "SP:2b6d81f11b33cb73b216b894186b0016f42948eb",
  "authors": [{
    "name": "Heeyoung Lee",
    "affiliations": []
  }, {
    "name": "Andreas Stolcke",
    "affiliations": []
  }, {
    "name": "Elizabeth Shriberg",
    "affiliations": []
  }],
  "abstractText": "Addressee detection (AD) is an important problem for dialog systems in human-humancomputer scenarios (contexts involving multiple people and a system) because systemdirected speech must be distinguished from human-directed speech. Recent work on AD (Shriberg et al., 2012) showed good results using prosodic and lexical features trained on in-domain data. In-domain data, however, is expensive to collect for each new domain. In this study we focus on lexical models and investigate how well out-of-domain data (either outside the domain, or from single-user scenarios) can fill in for matched in-domain data. We find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts, and that human-computer utterances can be modeled using single-user data: the resulting AD system outperforms a system trained only on matched in-domain data. Further gains (up to a 4% reduction in equal error rate) are obtained when in-domain and out-of-domain models are interpolated. Finally, we examine which parts of an utterance are most useful. We find that the first 1.5 seconds of an utterance contain most of the lexical information for AD, and analyze which lexical items convey this. Overall, we conclude that the H-H-C scenario can be approximated by combining data from H-C and H-H scenarios only. ∗Work done while first author was an intern with Microsoft.",
  "title": "Using Out-of-Domain Data for Lexical Addressee Detection in Human-Human-Computer Dialog"
}