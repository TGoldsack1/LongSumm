{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 968–974 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n968"
  }, {
    "heading": "1 Introduction",
    "text": "Telling stories about what we experience is a central part of human communication (Mateas and Sengers, 2003). Increasingly, stories about our experiences are captured in the form of videos and then shared on social media platforms. One goal of automatically understanding and describing such videos with natural language is to generate multi-sentence descriptions which convey the story, making them accessible to situationally (e.g. bandwidth) or physically (“blind”) disabled people. However, it is still a challenge for vision and language models to automatically encode and describe temporal content in videos with multi-sentence descriptions (Rohrbach et al., 2014; Zhou et al., 2018b). To better understand the stories shared on social media we collect and annotate a novel dataset consisting of videos from a social media platform. Importantly, we collect descriptions containing multiple sentences,\n∗*Work done while SG was intern at Facebook AI Research.\nas single sentences would typically not be able to capture the narration and plot of the video.\nWe introduce a large-scale multi-sentence description dataset for videos. To build a dataset of high quality, diverse and narratively interesting videos, we choose videos that had high engagement on a social media platform. Existing video captioning datasets, such as ActivityNet Captions (Krishna et al., 2017) or cooking video datasets (Regneri et al., 2013; Zhou et al., 2018a), have focused on sets of pre-selected human activities, whereas social media videos contain a great diversity of topics. Videos with high engagement tend to be narratively interesting, because humans find very predictable videos less enjoyable, meaning that captioning of the videos accurately requires integrating information from the entire video to describe a sequence of events (see Figure 1). Together, this creates a diverse and challenging new benchmark for video and language understanding.\nWe present a thorough analysis of the new benchmark, demonstrating that linguistic and video context is crucial to accurate captioning and that the captions have a temporal consistency. We also show baseline results using state-of-the-art models."
  }, {
    "heading": "2 Multi-Sentence VideoStory Dataset",
    "text": "In Table 1 we summarize existing video description datasets; most provide only single-sentence descriptions or are restricted to narrow domains. Other multi-sentence description datasets are proposed for story narration of sets of images taken from a Flickr album (Huang et al., 2016; Krause et al., 2017). Other related work includes visual summarization of Flickr photo albums (Sigurdsson et al., 2016a) or videos (De Avila et al., 2011; Zhang et al., 2016) where the idea is to pick the key images or frames that summarize the visual content.\nWe select videos posted on a social media platform to create our dataset because of the variability in topics, length, viewpoints, and quality. They also tend to represent a good distribution of stories communicated by humans. We select videos from social media that are public and popular with a large number of comments and shares that triggered interactions between people. In total, our dataset consists of 20k videos with duration ranging from 20s-180s and spanning across diverse topics that are observed on social media platforms. We follow Krishna et al. (2017) to create temporally annotated sentences where each task is divided into two steps: (i) describing the video in multiple sentences, covering objects, situations and important details of the video; (ii) aligning each sentence in the paragraph with the corresponding timestamps in the video. We refer to these as video segments. In Figure 1, we present two example annotated videos describing (i) a scene where two girls are playing with horses; (ii) a wedding with a bride walking down the aisle. We summarize the statistics of our dataset in Table 2 and compare it to prior work in Table 1. Each of the 20k videos in our VideoStory dataset is annotated with a paragraph which has on average 4.67 temporally localized sentences. As we have three paragraphs per video for validation and test set, we have a total of 26,245 paragraphs with a total of 123k sentences. Each sentence in the dataset\nhas an average length of 13.32 words, and each video has the average paragraph length of 62.23 words. Each sentence is aligned to a clip of on average 18.33 seconds which covers on average 26.04% of the full video. However, the entire paragraph for each video on average describes 96.7% of the whole video, demonstrating that each paragraph annotation covers the majority of the video. Furthermore, we found that 22% of the temporal descriptions overlap, showing that our annotation allows co-occurring or simultaneous events. We divide our dataset in training (17098 videos), validation (999), test (1011) and blind test splits (1039). Each video in the training set has a single annotation, but videos in validation, test, and blind test splits have three temporally localized paragraph annotations, for evaluation. While the test set can be used to compare model variants in a paper, only the best model per paper should be evaluated on the blind test set annotations, which will only be possible on an evaluation server. Annotations for the blind test set will not be released.\nTo explore the different domains in our dataset vs. ActivityNet captions we use the normalized pointwise mutual information to identify the words most closely associated with each dataset. Highest ranked words for ActivityNet are almost exclusively sports related, whereas in our dataset they include animals, baby, and words related to social events such as weddings. Most dominant actions in ActivityNet are either sports or household activity related whereas actions in our dataset are related to social activities such as laughing, waving, cheering etc. Our analysis of the distribution of POS categories show that nouns are the most dominant category observed in the VideoStory captions dataset with 24% of the total tokens followed by verbs (18.5%), determiners (15.9%), adjectives (4.36%), adverbs (5.16%) and propositions (5.04%). We\nalso observe the similar distribution of POS categories in ActivityNet captions.\nWe also find that ActivityNet has 50% of the videos where at least one segment in the video describes more than half of the video duration whereas in our dataset only 30% of videos have that phenomenon. In Figure 2, we show the distribution of sentence/segment annotations in time. The average number of (temporally localized) sentences is 4.67 compared to 3.65 in ActivityNet, despite having shorter videos, indicating the high information content of our videos.\nIn Table 3 we present all three paragraph annotations for a video showing a wedding ceremony. Out of 3 annotations, Annotation 2 is more descriptive compared to 1 and 3. However, it misses details about the presence of the photographer and taking the pictures.\nTemporal Analysis. High quality video descriptions are more than bags of single-sentence captions; they should tell a coherent story. To identify the importance of sentence ordering or temporal coherence in our video paragraphs, we train a neural language model (Merity et al., 2017) on the training paragraphs of the VideoStory dataset and report perplexity on the correct order of sentences vs. randomly shuffled order of sentences in the descriptions created to understand the importance of temporal coherence in the video descriptions of our dataset. Results in Table 2 show that shuffled sentences have higher perplexity scores, demonstrating that order of sentences in the paragraphs are important for the coherence in the story."
  }, {
    "heading": "3 Baseline Captioning Models",
    "text": "We explore learning to caption the videos using ground truth video segments.\nImage Captioning Models. To understand if the temporal component of the video is contributing\nto the description, we trained image captioning models on a frame sampled from the middle of the each segment of a video. We use the Show and Tell (Vinyals et al., 2015) image captioning architecture to generate captions. Video Captioning Models. We study various video captioning models. First, we use sequence to sequence (seq-seq) recurrent neural network (RNN) model which has a two-layer encoder RNN to encode video features and a decoder RNN to generate descriptions. In the seq-seq approach we treat each description/segment individually and use an RNN decoder to describe each segment of the video, similar to Venugopalan et al. (2015), but using Gated Recurrent Units, GRUs, (Cho et al., 2014) for both the encoder and decoder.\nIn most videos, events are correlated with previous and future events. For example, for the first video description shown in Figure 1 once the girl is thrown into the water, she gets hold of herself, and the horse shakes off water on her. To capture such contextual correlations, we incorporate context from previous segment description into the captioning module. We build a model (seq-seq + context) which takes current segment video features and hidden representation of previous segment’s sentence generation RNN at every timestamp in the decoder. For a given video segment, with hidden encoded video representation hvi and hidden representation of previous segment hsi−1, the concatenation of (hvi , h s i−1) is fed as input to the decoder that describes the segment (shown in Figure 3). Prior work has shown using previous video context has improved generated captions (Krishna et al., 2017). Visual representation. For the image caption-\ning models, we used features extracted from pretrained ResNet-152 on ImageNet (He et al., 2016). For video captioning models we extract features from pre-trained 3D convolution ResNext-101 architecture trained on Kinetics (Kay et al., 2017), denoted as R3D, which achieved state-of-the-art results on various activity recognition tasks (Hara et al., 2018). Since a significant percentage of our videos has objects other than humans (e.g., animals) we also experiment with image-video fusion features(denoted by RNEXT, R3D) i.e., concatenation of ResNext-101 features extracted from pre-trained ImageNet with R3D features described above. We extract image features from the same frames which were used to extract R3D features."
  }, {
    "heading": "4 Experiments and Results",
    "text": "For every segment, we set the maximum number of the sequence of features to 120 (i.e., 16X120 frames from the video) and maximum sentence length to 30. We trained using Adam optimizer with learning rate 0.0001. We use GRU as recurrent architecture to encode frames and decode captions with 512 dimensional hidden representation. We measure the captioning performance with most\ncommonly-used evaluation metrics: BLEU{3,4}, METEOR, ROUGE-L, and CIDEr following previous works of image and video captioning (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Vedantam et al., 2015).\nIn Table 5, we present the performance of our baseline models on VideoStory test dataset. We observe that models that consider context (seqseq+context) from the previously generated sentence have better performance than the corresponding models without context (seq-seq), with both 3D convolution based features (R3D) as well as image-video fusion features (RNEXT,R3D). This indicates that our model benefited from contextual information, and that sentences in our stories are contextual, rather than independent.\nTo validate the strength of our baseline model, we train our best performing model on ActivityNet Captions. It achieves 10.92 (METEOR) and 43.42 (CIDEr) on the val set, close to state-of-the-art results of 11.06 and 44.71 by Zhou et al. (2018b), indicating that it is a strong baseline. However, when evaluating our ActivityNet model on our VideoStory dataset (Table 5, last row), we see significantly lower performance compared to a model trained on our dataset, highlighting the complementary nature of our dataset.\nOur image only (single frame) model has the lowest scores across all metrics suggesting that a single image is not enough to generate contextual descriptions. We observed that our fusion models consistently outperform models with video-only R3D features, indicating features extracted using pre-trained ImageNet complement activity based R3D features. We show qualitative results from the variants of our models in Table 4. We observe that single frame models tend to repeat same captions and seq-seq model without context repeats phrases in the descriptions."
  }, {
    "heading": "5 Conclusions",
    "text": "This paper introduces a dataset which we sourced from videos on social media and annotated with multi-sentence descriptions. We benchmark strong baseline approaches on the dataset, and our evaluations show that our dataset is complementary from prior work due to more diverse topics and the selection of engaging videos which tell a story. Our VideoStory dataset can serve as a good benchmark to build models for story understanding and multi-sentence video description."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank Ranjay Krishna for providing the annotation interface used in Krishna et al. (2017) which we adapted to collect our dataset. We would also like to thank Haoqi Fan, Boris Vassilev, Jamie Ray, Sasha Sheng, Nikhila Ravi, and Evan Numbers for their help collecting the dataset, Devi Parikh for feedback on the annotation interface and Anna Rohrbach for useful feedback on drafts of this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
    "authors": ["Satanjeev Banerjee", "Alon Lavie."],
    "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
    "year": 2005
  }, {
    "title": "Collecting highly parallel data for paraphrase evaluation",
    "authors": ["David Chen", "William B. Dolan."],
    "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June,",
    "year": 2011
  }, {
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Struc-",
    "year": 2014
  }, {
    "title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
    "authors": ["Pradipto Das", "Chenliang Xu", "Richard F. Doell", "Jason J. Corso."],
    "venue": "2013 IEEE Conference on Computer Vision and Pattern Recog-",
    "year": 2013
  }, {
    "title": "Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method",
    "authors": ["Sandra Eliza Fontes De Avila", "Ana Paula Brandão Lopes", "Antonio da Luz Jr", "Arnaldo de Albuquerque Araújo."],
    "venue": "Pattern Recognition Let-",
    "year": 2011
  }, {
    "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
    "authors": ["Kensho Hara", "Hirokatsu Kataoka", "Yutaka Satoh."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, pages 18–22.",
    "year": 2018
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
    "year": 2016
  }, {
    "title": "The kinetics human action video",
    "authors": ["Will Kay", "Joao Carreira", "Karen Simonyan", "Brian Zhang", "Chloe Hillier", "Sudheendra Vijayanarasimhan", "Fabio Viola", "Tim Green", "Trevor Back", "Paul Natsev"],
    "year": 2017
  }, {
    "title": "A hierarchical approach for generating descriptive image paragraphs",
    "authors": ["Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei."],
    "venue": "Computer Vision and Patterm Recognition (CVPR).",
    "year": 2017
  }, {
    "title": "Dense-captioning events in videos",
    "authors": ["Ranjay Krishna", "Kenji Hata", "Frederic Ren", "Li Fei-Fei", "Juan Carlos Niebles."],
    "venue": "International Conference on Computer Vision (ICCV).",
    "year": 2017
  }, {
    "title": "TGIF: A new dataset and benchmark on animated GIF description",
    "authors": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel R. Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recog-",
    "year": 2016
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out.",
    "year": 2004
  }, {
    "title": "Narrative intelligence",
    "authors": ["Michael Mateas", "Phoebe Sengers."],
    "venue": "J. Benjamins Pub.",
    "year": 2003
  }, {
    "title": "Regularizing and Optimizing LSTM Language Models",
    "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher."],
    "venue": "arXiv preprint arXiv:1708.02182.",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Grounding action descriptions in videos",
    "authors": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal."],
    "venue": "TACL, 1:25–36.",
    "year": 2013
  }, {
    "title": "Coherent multi-sentence video description with variable level of detail",
    "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Wei Qiu", "Annemarie Friedrich", "Manfred Pinkal", "Bernt Schiele."],
    "venue": "Pattern Recognition - 36th German Conference, GCPR",
    "year": 2014
  }, {
    "title": "A dataset for movie description",
    "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Niket Tandon", "Bernt Schiele."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3202–3212.",
    "year": 2015
  }, {
    "title": "Movie description",
    "authors": ["Anna Rohrbach", "Atousa Torabi", "Marcus Rohrbach", "Niket Tandon", "Christopher Pal", "Hugo Larochelle", "Aaron Courville", "Bernt Schiele."],
    "venue": "International Journal of Computer Vision, 123(1):94–120.",
    "year": 2017
  }, {
    "title": "Learning visual storylines with skipping recurrent neural networks",
    "authors": ["Gunnar A Sigurdsson", "Xinlei Chen", "Abhinav Gupta."],
    "venue": "European Conference on Computer Vision, pages 71–88. Springer.",
    "year": 2016
  }, {
    "title": "Hollywood in homes: Crowdsourcing data collection for activity understanding",
    "authors": ["Gunnar A. Sigurdsson", "Gül Varol", "Xiaolong Wang", "Ali Farhadi", "Ivan Laptev", "Abhinav Gupta."],
    "venue": "Computer Vision - ECCV 2016 - 14th European Conference, Amster-",
    "year": 2016
  }, {
    "title": "Using descriptive video services to create a large data source for video annotation research",
    "authors": ["Atousa Torabi", "Christopher J. Pal", "Hugo Larochelle", "Aaron C. Courville."],
    "venue": "CoRR, abs/1503.01070.",
    "year": 2015
  }, {
    "title": "Cider: Consensus-based image description evaluation",
    "authors": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575.",
    "year": 2015
  }, {
    "title": "Sequence to sequence-video to text",
    "authors": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."],
    "venue": "Proceedings of the IEEE international conference on computer vision, pages 4534–4542.",
    "year": 2015
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156–3164. IEEE.",
    "year": 2015
  }, {
    "title": "MSRVTT: A large video description dataset for bridging video and language",
    "authors": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages",
    "year": 2016
  }, {
    "title": "Generation for user generated videos",
    "authors": ["Kuo-Hao Zeng", "Tseng-Hung Chen", "Juan Carlos Niebles", "Min Sun."],
    "venue": "European conference on computer vision, pages 609–625. Springer.",
    "year": 2016
  }, {
    "title": "Video summarization with long shortterm memory",
    "authors": ["Ke Zhang", "Wei-Lun Chao", "Fei Sha", "Kristen Grauman."],
    "venue": "European conference on computer vision, pages 766–782. Springer.",
    "year": 2016
  }, {
    "title": "Towards automatic learning of procedures from web instructional videos",
    "authors": ["Luowei Zhou", "Chenliang Xu", "Jason J. Corso."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.",
    "year": 2018
  }, {
    "title": "End-to-end dense video captioning with masked transformer",
    "authors": ["Luowei Zhou", "Yingbo Zhou", "Jason J Corso", "Richard Socher", "Caiming Xiong."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739–8748.",
    "year": 2018
  }],
  "id": "SP:fd29785309feb8f2e061e35bf90e2a33c575d2f7",
  "authors": [{
    "name": "Spandana Gella",
    "affiliations": []
  }, {
    "name": "Mike Lewis",
    "affiliations": []
  }, {
    "name": "Marcus Rohrbach",
    "affiliations": []
  }],
  "abstractText": "Video content on social media platforms constitutes a major part of the communication between people, as it allows everyone to share their stories. However, if someone is unable to consume video, either due to a disability or network bandwidth, this severely limits their participation and communication. Automatically telling the stories using multi-sentence descriptions of videos would allow bridging this gap. To learn and evaluate such models, we introduce VideoStory, a new large-scale dataset for video description as a new challenge for multisentence video description. Our VideoStory captions dataset is complementary to prior work and contains 20k videos posted publicly on a social media platform amounting to 396 hours of video with 123k sentences, temporally aligned to the video.",
  "title": "A Dataset for Telling the Stories of Social Media Videos"
}