{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Statistical modeling of complex real-world networks is an important problem, drawing attention from diverse domains, such as social network analysis, biology, political science, etc. (Fortunato, 2010; Goldenberg et al.; Schmidt & Morup, 2013). The goal in statistical modeling of networks is usually to discover the underlying groups or community structure in the network, and/or predicting the existence of potential links between nodes. A common way\n1Yahoo! Research, New York, NY, USA 2CSE Department, IIT Kanpur, Kanpur, UP, India 3Duke University, Durham, NC, USA. Correspondence to: Changwei Hu <changweih@yahooinc.com>, Piyush Rai <piyush@cse.iitk.ac.in>, Lawrence Carin <lcarin@duke.edu>. This work was done when Changwei Hu was a Ph.D. student at Duke University.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nof accomplishing this is by embedding the nodes in a latent space via latent space models (Hoff et al., 2002). Extensions of the latent space model include stochastic blockmodels (Nowicki & Snijders, 2001), and variants thereof (Miller et al., 2009; Airoldi et al., 2008; Latouche et al., 2011), which can learn node embeddings that are interpretable (e.g., sparse) and can therefore reflect the underlying structure of the network. An appealing class of models is the latent feature relational model (LFRM) (Miller et al., 2009), often also called the overlapping stochastic blockmodel (Latouche et al., 2011), which associates with each node a latent binary vector that can be thought of as the node’s overlapping memberships to one or more latent clusters in the network.\nThe modeling flexibility offered by overlapping stochastic blockmodels, however, comes at a price. Inference in these models, typically performed by MCMC methods (Miller et al., 2009; Latouche et al., 2011), can be particularly challenging and is not easy to scale to networks with very large number of nodes. Moreover, many real-world networks exhibit considerably more complex interactions which may not be explained by the flat embeddings learned via these models. This problem can be further exacerbated due to the extreme sparsity of the observed links, and although leveraging some side information that might be available for each node can help alleviate this issue to some extent (Kim et al., 2011), this can make inference even more complex to scale to large networks (Kim et al., 2011). Besides, communities in real-world networks often tend to have inter-dependencies/hierarchical structures that are usually ignored by these single layer models.\nMotivated by these limitations, we present an overarching framework that enables us to address these challenges via a unified, fully Bayesian model. Specifically, we develop a model that learns multiple layers of latent features of the nodes in the network, effectively learning a more expressive representation of the nodes which can better explain the interactions among the nodes in more complex networks, as compared to the existing methods. At the same time, the hierarchy of multiple layers of latent features allows imposing/exploiting the correlations among the clusters, which is usually not possible with single layer models.\nAnother appealing aspect of our model is its ability to in-\ncorporate side information (given as node attributes) via a regression model that maps the node attributes to node latent features. This provides the model robustness when the network is highly sparse and/or in “cold-start” problems where a new node may not have formed links with any existing nodes and we may still want to predict its cluster memberships and/or links with the existing nodes.\nOur model also enjoys excellent computational scalability. In particular, leveraging data-augmentation techniques allows us achieve full local conjugacy and enables us to develop a simple Gibbs sampler for model inference. Moreover, the inference cost in our model scales in the number of observed edges in the network, which makes it especially appealing for large real-world networks which are inherently sparse. Finally, although in this exposition, we only focus on unweighted networks (given as binary symmetric/asymmetric adjacency matrix), our framework, based on a gamma-Poisson construction, can readily be applied to weighted networks (Aicher et al., 2013) where the edges may have count-valued weights."
  }, {
    "heading": "2. The Model",
    "text": "We denote the network being modeled as a binary adjacency matrix A ∈ {0, 1}N×N , where N is the number of nodes. The network may be symmetric (undirected) or asymmetric (directed). In addition to A, we may be also given side information associated with each node. The side information, when given, will be denoted using an N ×D matrix S, with D being the number of observed features in the side information, and si ∈ RD (row i in S) denoting the side information associated with node i.\nFollowing the overlapping stochastic blockmodel (Latouche et al., 2011; Miller et al., 2009; Zhu, 2012) approach to statistical network modeling, we assume that each node i in the network can be described as binary latent feature vector zi of size K, where zik = 1 if node i belongs to the latent cluster/community k (and 0 otherwise). Note that the model allows each node to belong to more than one cluster/community. Given the node latent features, the probability of a link Aij between nodes i and j can then be defined as a (bilinear) function of the latent features zi and zj , i.e., p(Aij = 1) = g(z>i Λzj) where Λ is a K×K matrix, with Λk` modulating the probability of link between two nodes belonging to clusters k and `. Here g is a function (described in Sec. 2.3)) which turns real-valued scores z>i Λzj into probabilities.\nUnlike overlapping stochastic blockmodels (Latouche et al., 2011; Miller et al., 2009; Zhu, 2012) for relational data, however, which can only learn a single layer binary latent feature representation for the nodes in form of an N×K binary matrix Z = [z>1 . . . z>N ], we present a hierarchical architecture (shown in Fig. 1) which allows learning\nmultiple layers of latent features {Z(`)}L`=1 where L denotes the number of layers, Z(`) ∈ {0, 1}N×K` , and K` is the number of latent features in layer `.\nNote that our proposed framework is similar in spirit to deep sigmoid belief-nets (Neal, 1992; Gan et al., 2015), originally proposed for vector-valued observations. In contrast, our focus here is to model relational data given as pairwise observations. Moreover, our framework also allows conditioning the latent features directly on the side information using a regression model. We now describe the various components of our framework in the following subsections."
  }, {
    "heading": "2.1. A Structured Hierarchical Latent Feature Model",
    "text": "Akin to the deep sigmoid belief-nets (Neal, 1992; Gan et al., 2015), we condition each node’s latent features in layer ` on its latent features in layer `+ 1 via a weight matrix W(`) ∈ RK`×K`+1 (Fig. 1). Thus, for node i, we have\np(z (`) ik = 1) = π (`) ik = σ((w (`) k ) >z (`+1) i + b (`) k ),\n∀k = 1, . . . ,K`,∀` = 1, . . . , L− 1 p(z\n(L) ik = 1) = π (L) ik = σ(b (L) k ) ∀k = 1, . . . ,KL\nwhere w(`)k ∈ RK`+1 denotes the k-th row of W(`), and b(`) = (b\n(`) 1 , . . . , b (`) K`\n) is vector of biases. Note that L = 1 corresponds to a single layer model.\nA key benefit of the multi-layer (2 layers or more) architecture is that the 2nd layer latent features allow modeling and leveraging the correlations among the layer 1 latent features (i.e., clusters) which directly touch the data. In contrast, a flat 1 layer model will not be able to model correlations."
  }, {
    "heading": "2.2. Incorporating Side Information",
    "text": "If available, side information associated with the nodes in the network can be incorporated in this framework by conditioning the bottom-most layer (i.e., layer 1) latent features Z(1) on the side information (Fig. 1). Conditional distributions of the latent features Z(2), . . . ,Z(L) in all other\nlayers remain unchanged as before (Sec. 2.1), whereas the layer 1 latent features for node i are now modeled as\np(z (1) ik = 1) = σ((w (1) k ) >z (2) i +m > k si + b (1) k ) (1)\nwhere mk ∈ RD denotes the regression weights, which map the observed features si to the latent features zi. Note that although we only condition the layer 1 latent features on the side information, rest of the layers can also be conditioned on the side information in the same way.\nNote that, as opposed to conditioning the link Aij on the side information, in our model construction we choose to condition the latent features of each node on its side information. This allows the side information to directly influence the latent features, which is useful for predicting the latent features for new nodes that do not have any existing links in the network. This modeling choice has also been employed in (Kim et al., 2011), an extension of mixedmembership stochastic blockmodels (Airoldi et al., 2008), where each node’s cluster membership probabilities are directly conditioned on the metadata (observed features) associated with that node."
  }, {
    "heading": "2.3. Generating the Network",
    "text": "The layer 1 latent features Z(1) generate the observed network A (graphical model shown in Fig. 1). Specifically, each edge Aij ∈ {0, 1} is generated by thresholding a latent count random variableXij . Each of these latent counts Xij , in turn, is defined as a summation of another set of (smaller) latent counts Xijk1k2 , which are defined as bilinear functions of the layer 1 latent features Z(1). Formally,\nAij = 1(Xij ≥ 1), Xij = K∑\nk1=1 K∑ k2=1 Xijk1k2\nXijk1k2 ∼ Poisson(z (1) ik1 Λk1k2z (1) jk2 ) (2)\nMarginalizing out the latent counts Xij from Eq. 2 p(Aij = 1) = Bernoulli ( 1− exp(−z(1)i > Λz (1) j ) ) (3)\nNote that only the bottom layer (layer 1) latent features directly touch the data layer (the observed links/non-links). The construction in Eq. 2 based on decomposing a discrete random variable into a set of latent counts has also been used previously in modeling discrete (count or binary) data (Dunson & Herring, 2005; Gopalan et al., 2014; Zhou, 2015). This construction has the appealing property that if Aij = 0 then the associated latent counts are zero with probability one and need not be estimated during model inference. Therefore, this can lead to huge computational speed-ups for sparse data with many zeros (which is usually the case with real world networks which are very sparse). In the case of modeling relational data such as networks, this implies that the inference cost scales in the number\nof edges in the network, unlike other overlapping stochastic blockmodels such as LFRM (Miller et al., 2009; Zhu, 2012). These models use a logistic link function for the edges, which requires likelihood evaluations for both edges as well as non-edges. Consequently, the inference cost is quadratic in the number of nodes, making these models prohibitive for large networks. Note that the model readily applies to graphs with count-valued edges (the additional step of latent-count thresholding would not be required).\nNote that a similar construction for network generation was recently employed in (Zhou, 2015). However, our framework differs from (Zhou, 2015) in a number of key ways. In particular, unlike (Zhou, 2015) in which the latent features are positive reals, in our framework the latent features are binary (in the spirit of stochastic blockmodels (Miller et al., 2009; Zhu, 2012)). The binary latent features are also crucial for a deep sigmoid belief net construction. Moreover, unlike the model in (Zhou, 2015) which cannot leverage side information, our framework allows incorporating the side information of each node in predicting the node’s latent features. This capability allows our framework to work in the cold-start settings where a new node may not yet have formed any links with the existing nodes."
  }, {
    "heading": "2.4. The Full Generative Model",
    "text": "The full generative model for the observed network A, along with the latent variables, parameters, and hyperparameters of the model, is given below\nAij = 1(Xij ≥ 1) (4)\nXij = K∑ k1=1 K∑ k2=1 Xijk1k2 (5)\nXijk1k2 ∼ Poisson(z (1) ik1 Λk1k2z (1) jk2 ) (6)\nz (`) ik ∼ Bernoulli(π (`) ik ) (7)\n(∀k = 1, . . . ,K`,∀` = 1, . . . , L) (8)\nπ (`) ik =  σ((w (`) k ) >z (`+1) i +m > k si + b (`) k ) ( if ` = 1 and side info si available) σ((w (`) k ) >z (`+1) i + b (`) k )\n( if ` < L and side info si not available) σ(b\n(L) k ) if ` = L\n(9)\nΛk1k2 ∼ Gamma(gk1k2 , 1/ck1k2) (10)\ngk1k2 = { γk1γk2 if k1 6= k2 ξγk1 if k1 = k2\n(11)\nγk ∼ Gamma(γa, 1/γb), ξ ∼ Gamma(ξa, 1/ξb)(12) w\n(`) k ∼ N (0,Γ (w) k,` ), mk ∼ N (0,Γ (m) k ) (13)\nTo impose sparsity on the between layer connection weights {w(`)k } K,L k,`=1 and the regression weights {mk}Kk=1, we use automatic relevance determination (ARD) priors on these. In particular, the ARD prior on the regression weights mk also helps in selecting the relevant features in\nthe side information that are the most relevant in predicting the node’s binary latent features.\nAnother appealing property of the resulting link function (Eq. 3) is that it encourages generation of networks that are inherently sparse. To see this, note that using the likelihood model given by Eq. 3 readily leads to a lower bound on the number of zeros in the A matrix.\nLemma 1. The level of sparsity of the observed network A, as measured by the expected number of zeros in A, i.e., E[ ∑N i,j=1 I{Aij = 0}] is lower\nbounded by N2Ezi,zj ,Λ [ − ∑K k1,k2=1 Λk1k2zik1zjk2 ] =\nN2 exp ( − [\nζγc γbck1k2\n+ γ2a\nγ2b ck1k2 ] E z (1) i z (1) j [ z (1) ik1 z (1) jk2 ]) ,\nwhere we have made use of the fact that the expectation of the term ∑∞ k1=1 ∑∞ k2=1\nΛk1k2 is finite. The proof of the Lemma is given in the Supplementary Material."
  }, {
    "heading": "3. Inference",
    "text": "The model described in (4)-(13) is not conjugate. However, leveraging data augmentation techniques, we endow the model with full local conjugacy and derive a simple and efficient Gibbs sampler for model inference. The first data augmentation technique we use is based on the Poissonmultinomial equivalence (Zhou et al., 2012).\nLemma 2. Suppose that x1, . . . , xR are independent random variables with xr ∼ Pois(θr) and x = ∑R r=1 xr. Set\nθ = ∑R r=1 θr; let (y, y1, . . . , yR) be another set of random variables such that y ∼ Pois(θ), and (y1, . . . , yR)|y ∼ Mult(y; θ1θ , . . . , θR θ ). Then the distribution of x = (x, x1, . . . , xR) is the same as the distribution of y = (y, y1, . . . , yR).\nUsing this equivalence, givenXij , the smaller latent counts Xijk1k2 ’s can be easily sampled as\n{Xijk1k2} ∼ Mult ( Xij ; {z(1)ik1Λk1k2z (1) jk2 }∑K\nk1=1 ∑K k2=1 z (1) ik1 Λk1k2z (1) jk2 ) The second data augmentation we use is based on the Pólya-Gamma (PG) strategy (Polson et al., 2013) which allows re-expressing logistic-Bernoulli likelihoods on the z(`)ik ’s as Gaussians and consequently allows deriving closed-form posterior updates for the between-layer weights w(`)k and the regression weights mk (note that each of these are given Gaussian priors). According to the PG augmentation, given likelihoods expressible in the form\n(exp(τ))a\n(1+exp(τ))b , and given Pólya-Gamma random variable random variables ω ∼ PG(b, 0)\n(exp(τ))a\n(1 + exp(τ))b = 2−b exp(κτ) ∫ ∞ 0 exp(−κτ2/2)p(ω)dω\nwhere p(ω) is the density of the PG variable, and κ =\na − b/2. This result transforms the logistic-Bernoulli into a Gaussian, when conditioned on the PG random variables.\nFor example, for samplingw(`)k , we drawN Pólya-Gamma random variables α(`)k = [α (`) 1k , · · · , α (`) Nk], one for each (Bernoulli-drawn) z(`)ik , as\nα (`) ik ∼ PG(1, (w (`) k ) >z (`+1) i + b (`) k )\nwhere PG(.) denotes the Pólya-Gamma distribution (Polson et al., 2013). Conditioned on α(`)k , the logisticBernoulli likelihood on z(`)ik turns into a Gaussian and consequently the posterior distribution of w(`)k will also be a Gaussian. The same data augmentation strategy is followed for sampling the regression weights mk, for which conditioned on the layer 1 PG variablesα(1)k , the posterior ofmk is a Gaussian. The details are given in the next subsection."
  }, {
    "heading": "3.1. Gibbs Sampling",
    "text": "Gibbs sampling for our model proceeds as follows.\nSample Xij : For each nonzero observation Aij = 1 in the network, the associated latent count Xij is sampled from a zero-truncated Poisson distribution as\nXij ∼ Pois+( K1∑ k1=1 K1∑ k2=1 z (1) ik1 Λk1k2z (1) jk2 )\nSample Xijk1k2 : Having sampled Xij , the latent counts, Xijk1k2 can be sampled using the Poisson-multinomial equivalence (Lemma 2).\nSample z(1)ik1 : Layer 1 latent features z (1) ik1 are sampled as\nz (1) ik1 ∼ δ(Xi·k1· = 0)Bern(\nπ̃ik1\nπ̃ik1 + 1− π (1) ik1\n)+δ(Xi·k1· > 0)\nwhere the “marginal” latent counts are defined as summations, i.e., Xi·k1· = ∑ j>i ∑ k2 Xijk1k2 +∑\nj<i ∑ k2 Xjik2k1 , π̃ik1 = π (1) ik1 ∏K1 k2 ( ck1k2 1+ck1k2 )Λk1k2z (1) ·k2 ,\nand z(1)·k2 = ∑N i=1 z (1) ik2 .\nSample Λk1k2 : The latent feature interaction weights Λk1k2 are sampled as\nΛk1k2 ∼ Gamma(X··k1k2 + gk1k2 , 1\nQk1k2 + ck1k2 ) where Qk1k2 = ∑I i=1 ∑J j=1 z (1) ik1 z (1) jk2\n, X··k1k2 = 2−δk1k2 ∑ i ∑ j>i(Xijk1k2 + Xijk2k1), with δk1k2 = 1 if k1 = k2 and δk1k2 = 0 otherwise.\nSample z(`)ip (` ≥ 2): We consider the update of a single z(2)ip as an example, and assume the side information is available. z(2)ip is updated as z (2) ip ∼ Bern(σ(d (2) ip )),\nwith d(2)ip = b (2) p + (z (1) i ) Tw (1) p − 12 ∑K1 k=1 ( w (1) kp +\nα (1) ik (2ψ \\p ikw (1) kp +(w (1) kp ) 2) ) , wherew(1)p ∈ RK1 is the p-th\ncolumn in W(1), and ψ\\pik is defined as ψ \\p ik = m T k si + wTk z (2) i − z (2) ip w (1) kp .\nSample w(`)k , b (`) k , mk, γk1 , ξ, Γ (w) k,` and Γ (m) k : For brevity, the detailed equations are provided in the Supplementary Material."
  }, {
    "heading": "4. Related Work",
    "text": "A number of extensions have been proposed to enhance the modeling capabilities of stochastic blockmodels and its variants such as the latent feature relational model (LFRM), when applied to complex graph-structured data. In particular, in the context of LFRM, recent work on infinite latent attributes (ILA) (Palla et al., 2012) is designed to learn binary latent features for the nodes in the network, and each latent feature is further assumed to be partitioned into disjoint groups. ILA however cannot incorporate side information, and while ILA assumes a specific two-level representation of the nodes (via latent features in level one and clusters in level two), our model is capable of learning a more general hierarchical latent feature representation.\nStochastic blockmodels have also been extended for inferring nested communities using nested Chinese Restraurant Process (Ho et al., 2012). Such methods can learn clusters of varying granularities at multiple levels in a hierarchy. However, the focus of these class of methods is different as these methods do not learn a binary latent feature based representation unlike our model and can only learn disjoint clusterings (organized at multiple scales) of nodes.\nAmong methods that can incorporate side information in stochastic blockmodels, the nonparametric metadata dependent relational (NMDR) model (Kim et al., 2011) is somewhat similar in spirit to our model in the way the side information is incorporated into the model. The NMDR model builds on the nonparametric Bayesian mixed-membership stochastic blockmodel (Airoldi et al., 2008) and the side information is incorporated by conditioning the cluster membership probabilities (the weights of the sticks in the stick-breaking process) on the side information via a regression model. However, it is a single layer model, requires retrospective MCMC sampling for inference, and is difficult to scale to large networks. We use NMDR as one of the baselines in our experiments.\nAmong other methods that can incorporate side information in link prediction models beyond stochastic blockmodels, (Menon & Elkan, 2011) presented a number of nonprobabilistic approaches based on latent space models that directly use the side information in the link prediction objective function (note that LFRM also proposes doing the same (Miller et al., 2009) to incorporate side information).\nHowever, the embeddings are not conditioned on the side information and these models cannot predict the embedding of a new node from its side information.\nOur model is also similar in spirit to the recently proposed infinite edge partition model (Zhou, 2015) (we also use it as one of our baselines in the experiments) which also uses the Bernoulli-Poisson link to model each edge. However, EPM assumes positive-valued node embeddings (given gamma priors), is limited to a single layer representation, and cannot incorporate side information.\nTo the best of our knowledge, none of the existing methods for network modeling can learn hierarchical latent representations of the nodes. Recently, DeepWalk (Perozzi et al., 2014) was introduced as a way to learn embeddings of nodes in a network using a skip-gram model by considering short random walks along the network and using these walks as “sentences” and nodes being “words”, and learns the node embedding in a manner like learning word2vec embeddings. However, these embeddings are single layer real-valued embeddings. In addition to this, some other simultaneous development on deep learning for graph-structured, relational data include graph convolutional networks (Schlichtkrull et al., 2017) and graph variational autoencoders (Kipf & Welling, 2016).\nIn contrast to the aforementioned methods, our framework provides a unified model which not only learns a hierarchical, interpretable latent feature representation of the nodes, but also incorporates node side information via a regression model. Notably, both these enrichments are naturally formulated under a multilayer sigmoid belief-net type model architecture. Moreover, the model is simple to do inference on, and can easily scale to massive, sparse networks (with binary as well as count-valued edges)."
  }, {
    "heading": "5. Experiments",
    "text": "We consider three instances of our hierarchical latent feature model (HLFM): one-layer HLFM, two-layer HLFM, and two-layer HLFM with side information. While our framework straightforwardly extends to more than two layers, we specifically focus our experimental analysis to consider the single and two layer cases (with/without side information), to carefully explicate the advantage of our model in: (1) going from flat to hierarchical latent features, and (2) the advantage of incorporate the side information, especially when the network is highly sparse.\nWe apply our model on several benchmark relational data sets, and compare with three state-of-the-art methods for stochastic blockmodeling and link prediction as baseline, including stochastic blockmodels based methods that can also incorporate side information. Our baselines include:\n• Hierarchical Gamma Process Edge Partition Model\n(HGP-EPM) (Zhou, 2015): This is a state-of-the-art, highly scalable Bayesian model for learning overlapping communities. The model is based on learning non-negative embeddings for each node.\n• Community-Affiliation Graph Model (AGM) (Yang & Leskovec, 2012): This model is an overlapping community detection model based on learning a binary latent feature vector (akin to our approach and latent feature relational models (Miller et al., 2009)).\n• Nonparametric Metadata Dependent Relational Model (NMDR) (Kim et al., 2011): This model is based on the nonparametric Bayesian mixedmembership blockmodel and, in the same spirit as our model, allows conditioning a node’s cluster memberships on metadata associated with that node."
  }, {
    "heading": "5.1. Data Sets",
    "text": "We consider seven real-world data sets, with five data sets associated with side information, and the remaining two having no side information. The description of each data set (and the associated side information) is given below:\nProtein230: This data set contains information about protein-protein interactions of 230 proteins, with 595 edges. This network has no side information.\nNIPS234: Coauthor network consists of the top 234 authors in NIPS 1-17 conferences in terms of the number of publications, as studied in (Miller et al., 2009). There are 598 edges. This network has no side information.\nConflicts: Network of military disputes between countries in year 1990-2000 (Ghosn et al., 2004). The graph is symmetric, i.e., two countries have a link if either initiated conflict with the other. There are 130 countries and 320 edges. Each country has 3 features: GDP, population, and polity.\nFacebook: User-user interactions extracted from Facebook social network (McAuley & Leskovec, 2012). There are 228 users from 14 ego-network communities. Each user is associated with 92 profile information features (e.g., age, gender, education).\nMetabolic: Metabolic pathway interaction data for Saccharomyces cerevisiae provided in the KEGG/PATHWAY database (Yamanishi et al., 2005). There are 668 nodes in total. Each node is associated with three sets of features: phylogenetic information (157 features), gene expression information(145 features), and gene location (23 features).\nNIPS 1-17: NIPS coauthorship network containing 2865 authors, and 9466 edges. For this dataset, we also know what words each author used in their publications. We decompose the author-word matrix using SVD, and introduce first 100 SVD-based author features as side information.\nCiteSeer: A citation network consisting of 3312 scientific\npublications from six categories: agents, AI, databases, human computer interaction, machine learning, and information retrieval. The side information for the dataset is the category label for each paper which is converted into a onehot representation.\nWe evaluate our model on both quantitative tasks (in its ability to predict missing links in the network) as well as qualitative tasks (interpreting the inferred clusters)."
  }, {
    "heading": "5.2. Predicting Held-out Links",
    "text": "We use Area Under the ROC Curve (AUC) to evaluate our model and the other baselines on the task of link prediction. For the two data sets without side information (Protein230 and NIPS234), we hold out 20% data as our test data. For the remaining five data sets, we hold out 80% data as our test data as we were interested in highly missing data regimes to investigate how much the side information is benefitting in such difficult cases.\nThe shrinkage priors used in our model and the other baselines can automatically prune out the unnecessary latent features. We set K to a large enough number (K = 100) so that all models are evaluated with sufficient number of latent features. Our models and the other baselines (except HGP-EPM) are run with 1000 burn-in iterations, and another 1000 iterations for sample collection. For the HGPEPM baseline, we use the default setting from (Zhou, 2015) and run their model for 3000 burn-in and 1000 collection iterations. The samplers are initialized randomly. Each experiment is repeated 5 times with different training and test splits and averaged results are reported.\nTable 1 reports the results on the two data sets that do not have side information and Table 2 reports the results on the other four data sets with side information. On the data sets with side information, Figure 2 separately compares the three variants of our model: the model with one layer, two layers, and two layers with side information.\nAs shown in Table 1, our two layer model outperforms all the other methods. Also note that, on NIPS234, the one layer model is outperformed by HGP-EPM and performs comparably to AGM (which like our model learns binary latent feature for each model). However, there is a marked improvement in the performance when using the two layer model and the model ourperforms all the baselines by a significant margin. This shows the benefit of the better and more expressive latent features learned by the hierarchy in our model, even when no side information is available.\nTable 2 shows the results in the presence of side information. Except for Conflicts data, where our model gets outperformed by HGP-EPN, our two layer model with side information significantly outperforms the baselines on most of the data sets. In particular, our model yields better AUC scores than the other best performing baseline\nNMDR (Kim et al., 2011) which can, like our model, incorporate side information. The better performance of our model can be attributed to a combination of several factors, e.g., (1) unlike NMDR, our model allows overlapping membership to multiple clusters (and multiple layers of latent features) leading to more expressive latent features; and (2) inference is simpler in our model, which leads to better mixing of the sampler.\nIt is interesting to note that neither NMDR nor the twolayer HLFM with side information outperform HGP-EPM on Conflicts data in terms of the link prediction performance. This is probably because the side-information is too simple and not very informative for link prediction.\nIn Figure 2, we also separately compare the three variants of our model: the model with one layer, two layers, and two layers with side information. As the figure shows, the two layer model usually performs better than the one layer model, and incorporating the side information leads to further improvements in the AUC scores, with the strength of improvement depending on how informative the side information is in predicting the latent features of the nodes."
  }, {
    "heading": "5.3. Qualitative Analyses via Inferred Latent Features",
    "text": "The node embedding learned by our model (with or without side information) can be used for qualitative analyses. Note that each column of the binary latent feature matrix Z(`)\nrepresents a cluster of nodes in the network. Essentially, the nonzero entries in each column of the matrix correspond to the nodes that belong to a cluster in layer `.\nWe use Z(`) to present clustering results for the NIPS234 and Conflicts datasets in Table 3 and 4. In Table 3, showing results on the NIPS234 data, note that some authors (e.g., Michael Jordan) are inferred as belonging to more than one cluster (since the model allows overlapping clusters).\nLikewise, Table 4, shows the results on Conflicts data, with the inferred clusters of countries. To further show the discovered clusters at multiple layers and the interrelationships between clusters: (1) In Table 4, we show the learned clusters of countries in layer 1 and layer 2; (2) In Figure 4, we show the inferred correlation-based pairwise similarities between the layer 1 clusters. To compute these correlations, we use the between-layer weights w(`)k as the feature vector for the k-th cluster (of layer ` = 1) and use cosine similarity between the feature vectors of each pair of clusters.\nFrom the left plot of Figure 4, it can be observed, for example, that layer 1 clusters 3 and 4 have a high similarity, and clusters 1 and 10 have a high similarity. Looking at these four clusters, which are also shown in Table 4, we find that the countries in each of these layer 1 clusters are usually bordering countries (as shown in the right plot of Figure 4) having military disputes or other types of bilateral relations\n(e.g., military aid). Interestingly, unlike layer 1 clusters, the countries grouped together in layer 2 clusters are not necessarily related by the virtue of being geographically close. As seen in Table 4, the clusters in layer 2 (e.g., cluster 3) are more coarse-grained, and can be regarded as a ”super group” of clusters . Such clusters consist of countries from multiple geographic regions, such as Europe, Middle East and Asia, some of which are known to be related via some military disputes, despite not being geographically close. For example, during the Gulf war (1991, recorded in Conflicts data between 1990-2000), Iraq (Middle East) was involved disputes with the coalition members which included countries like Hungary, Italy, Netherlands (Europe). Interestingly all these countries are grouped together in cluster 3 of layer 2. This analysis demonstrates that the multilayer architecture of our model not only yields significantly improved link-prediction accuracies but also enables us in gaining better insights into the data by means of more interpretable latent features and clusterings, which may be useful in their own right in many applications."
  }, {
    "heading": "5.4. Computational Efficiency And Convergence",
    "text": "We also perform an experiment to assess the computational efficiency of our framework. We compare (on four data sets) the run time of the three variants of our model (one layer, two layers, and two layers with side information) with the run times of the NMDR (Kim et al., 2011), AGM(Yang & Leskovec, 2012) and HGP-EPM (Zhou, 2015), all of which are state-of-the-art community detection/link prediction methods. All the models are imple-\nmented in MATLAB and were run on a standard machine with 2.40GHz processor and 16GB RAM. Our inference routines are based on batch Gibbs sampling. The periteration computation times are shown in Table 5.\nAs shown in Table 5, our models have very small periteration run times which are comparable with the other baselines. Among all the methods compared, note that AGM has smallest computational cost. This is due to the simplicity of the model (however it also gives the lowest AUC scores on all the data sets). Besides AGM, our models have run times that are comparable to the baseline HGPEPM (which is a single layer model and cannot incorporate side information) and are considerably faster than the other baseline NMDR which, although capable of incorporating side information, is computationally much more expensive as compared with our models.\nWe also compare (Figure 3) the empirical convergence of the various models on the Metabolic data (80/20 training/testing split). As the figure shows, the convergence time for our two-layer HLFM is comparable with HGPEPM model, while AGM takes the longest to converge."
  }, {
    "heading": "6. Conclusion",
    "text": "We presented a deep generative model for relational data for which side information may also be available for each node. Our model enriches the latent feature relational models for networks using a hierarchical structure, and allows incorporating side information seamlessly via a regression model. To the best of our knowledge, ours is the first framework that extends overlapping stochastic blockmodels to a deep architecture. A key benefit of the deep architecture (even with 2 hidden layers) is that the layer 2 latent features allow modeling/leveraging correlations among layer 1 latent features/clusters which directly touch the data. A flat model will not be able to leverage such correlations. The modeling flexibility is also accompanied by simplicity of inference. In particular, leveraging data augmentation schemes, the model enjoys full local conjugacy and admits efficient inference via a simple Gibbs sampler. Networks/graphs with binary as well as count-weighted edges can be analyzed using our model (by replacing the truncated Poisson likelihood by a Poisson likelihood). The model can be easily scaled up even further using online Bayesian inference, and by leveraging recognition models (Kingma & Welling, 2013) for fast inference of the latent features. Another possible extension of the model will be in modeling multi-relational data, such as knowledgegraphs (Schlichtkrull et al., 2017; Hu et al., 2016).\nAcknowledgements: This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF. Piyush Rai also acknowledges support from IBM Faculty Award, DST-SERB Early Career Research Award, Dr. Deep Singh and Daljeet Kaur Faculty Fellowship, and the Research-I Foundation, IIT Kanpur."
  }],
  "year": 2017,
  "references": [{
    "title": "Adapting the stochastic block model to edge-weighted networks",
    "authors": ["Aicher", "Christopher", "Jacobs", "Abigail Z", "Clauset", "Aaron"],
    "venue": "arXiv preprint arXiv:1305.5782,",
    "year": 2013
  }, {
    "title": "Mixed membership stochastic blockmodels",
    "authors": ["Airoldi", "Edoardo M", "Blei", "David M", "Fienberg", "Stephen E", "Xing", "Eric P"],
    "year": 2008
  }, {
    "title": "Bayesian latent variable models for mixed discrete outcomes",
    "authors": ["Dunson", "David B", "Herring", "Amy H"],
    "year": 2005
  }, {
    "title": "Community detection in graphs",
    "authors": ["Fortunato", "Santo"],
    "venue": "Physics reports,",
    "year": 2010
  }, {
    "title": "Learning deep sigmoid belief networks with data augmentation",
    "authors": ["Gan", "Zhe", "Henao", "Ricardo", "Carlson", "David E", "Carin", "Lawrence"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "The mid3 data set, 1993-2001: Procedures, coding rules, and description",
    "authors": ["Ghosn", "Faten", "Palmer", "Glenn", "Bremer", "Stuart"],
    "venue": "Conflict Management and Peace Science,",
    "year": 2004
  }, {
    "title": "Bayesian nonparametric poisson factorization for recommendation systems",
    "authors": ["Gopalan", "Prem", "Ruiz", "Francisco J", "Ranganath", "Rajesh", "Blei", "David M"],
    "venue": "In AISTATS,",
    "year": 2014
  }, {
    "title": "A multiscale community blockmodel for network exploration",
    "authors": ["Ho", "Qirong", "Parikh", "Ankur P", "Xing", "Eric P"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2012
  }, {
    "title": "Latent space approaches to social network analysis",
    "authors": ["Hoff", "Peter D", "Raftery", "Adrian E", "Handcock", "Mark S"],
    "venue": "JASA,",
    "year": 2002
  }, {
    "title": "Topic-based embeddings for learning from large knowledge graphs",
    "authors": ["Hu", "Changwei", "Rai", "Piyush", "Carin", "Lawrence"],
    "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "The nonparametric metadata dependent relational model",
    "authors": ["Kim", "Dae Il", "Hughes", "Michael", "Sudderth", "Erik"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["Kingma", "Diederik P", "Welling", "Max"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Variational graph autoencoders",
    "authors": ["Kipf", "Thomas N", "Welling", "Max"],
    "venue": "arXiv preprint arXiv:1611.07308,",
    "year": 2016
  }, {
    "title": "Overlapping stochastic block models with application to the french political blogosphere",
    "authors": ["Latouche", "Pierre", "Birmelé", "Etienne", "Ambroise", "Christophe"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2011
  }, {
    "title": "Learning to discover social circles in ego networks",
    "authors": ["McAuley", "Julian", "Leskovec", "Jure"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Link prediction via matrix factorization",
    "authors": ["Menon", "Aditya Krishna", "Elkan", "Charles"],
    "venue": "In Machine Learning and Knowledge Discovery in Databases",
    "year": 2011
  }, {
    "title": "Nonparametric latent feature models for link prediction",
    "authors": ["Miller", "Kurt", "Griffiths", "Thomas", "Jordan", "Michael"],
    "year": 2009
  }, {
    "title": "Connectionist learning of belief networks",
    "authors": ["Neal", "Radford M"],
    "venue": "Artificial intelligence,",
    "year": 1992
  }, {
    "title": "Estimation and prediction for stochastic blockstructures",
    "authors": ["Nowicki", "Krzysztof", "Snijders", "Tom A B"],
    "venue": "JASA,",
    "year": 2001
  }, {
    "title": "An infinite latent attribute model for network data",
    "authors": ["Palla", "Konstantina", "Knowles", "David", "Ghahramani", "Zoubin"],
    "venue": "In ICML,",
    "year": 2012
  }, {
    "title": "Deepwalk: Online learning of social representations",
    "authors": ["Perozzi", "Bryan", "Al-Rfou", "Rami", "Skiena", "Steven"],
    "venue": "In KDD,",
    "year": 2014
  }, {
    "title": "Bayesian inference for logistic models using pólya–gamma latent variables",
    "authors": ["Polson", "Nicholas G", "Scott", "James", "Windle", "Jesse"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2013
  }, {
    "title": "Modeling relational data with graph convolutional networks",
    "authors": ["Schlichtkrull", "Michael", "Kipf", "Thomas N", "Bloem", "Peter", "Berg", "Rianne van den", "Titov", "Ivan", "Welling", "Max"],
    "venue": "arXiv preprint arXiv:1703.06103,",
    "year": 2017
  }, {
    "title": "Nonparametric bayesian modeling of complex networks: An introduction",
    "authors": ["Schmidt", "Mikkel N", "Morup", "Morten"],
    "venue": "Signal Processing Magazine, IEEE,",
    "year": 2013
  }, {
    "title": "Supervised enzyme network inference from the integration of genomic data and chemical information",
    "authors": ["Yamanishi", "Yoshihiro", "Vert", "Jean-Philippe", "Kanehisa", "Minoru"],
    "year": 2005
  }, {
    "title": "Community-affiliation graph model for overlapping network community detection",
    "authors": ["Yang", "Jaewon", "Leskovec", "Jure"],
    "venue": "In ICDM,",
    "year": 2012
  }, {
    "title": "Beta-negative binomial process and poisson factor analysis",
    "authors": ["M. Zhou", "L.A. Hannah", "D. Dunson", "L. Carin"],
    "venue": "In AISTATS,",
    "year": 2012
  }, {
    "title": "Infinite edge partition models for overlapping community detection and link prediction",
    "authors": ["Zhou", "Mingyuan"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Max-margin nonparametric latent feature models for link prediction",
    "authors": ["Zhu", "Jun"],
    "venue": "In ICML,",
    "year": 2012
  }],
  "id": "SP:52de76662a2be057f3ba7cff1a4f49d60b472a1e",
  "authors": [{
    "name": "Changwei Hu",
    "affiliations": []
  }, {
    "name": "Piyush Rai",
    "affiliations": []
  }, {
    "name": "Lawrence Carin",
    "affiliations": []
  }],
  "abstractText": "We present a probabilistic framework for overlapping community discovery and link prediction for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of latent features/communities for each node, providing superior link prediction performance on more complex networks and better interpretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs sampling. Moreover, inference cost scales in the number of edges which is attractive for massive but sparse networks. Our framework is also easily extendable to model weighted networks with count-valued edges. We compare with various state-of-the-art methods and report results, both quantitative and qualitative, on several benchmark data sets.",
  "title": "Deep Generative Models for Relational Data with Side Information"
}