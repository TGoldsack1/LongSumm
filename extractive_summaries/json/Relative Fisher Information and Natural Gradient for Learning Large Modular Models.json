{
  "sections": [{
    "heading": "1. Fisher Information Metric",
    "text": "The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ). Under light regularity conditions, FIM can be rewritten equivalently as\nIij = −Ep [ ∂2l\n∂Θi∂Θj\n] = 4 ∫ ∂ √ p(x |Θ) ∂Θi ∂ √ p(x |Θ) ∂Θj dx.\nAs its empirical counterpart, the observed FIM (Efron & Hinkley, 1978) with respect to (wrt) a sample set Xn = {xk}nk=1 is Î(Θ |Xn) = −∇2l(Θ |Xn), which is often evaluated at the maximum likelihood estimate Θ = Θ̂(Xn). By the law of large numbers, Î(Θ) converges to the (expected) FIM I(Θ) as n→∞.\n1King Abdullah University of Science and Technology (KAUST), Saudi Arabia 2École Polytechnique, France 3Sony Computer Science Laboratories Inc., Japan. Correspondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen <Frank.Nielsen@acm.org>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nThe FIM is not invariant and depends on the parameterization. We can optionally write I(Θ) as IΘ(Θ) to emphasize the coordinate system. By definition, IΘ(Θ) = JᵀIΛ(Λ)J where J = (Jij), Jij = ∂Λi∂Θj is the Jacobian matrix. For example, the FIM of regular natural exponential families (NEFs) l(Θ) = Θᵀt(x) − F (Θ) (loglinear models with sufficient statistics t(x)) is I(Θ) = ∇2F (Θ) 0, the Hessian of the log-normalizer function F (Θ). Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the lognormalizer function may not be available in closed-form nor computationally tractable (Montanari, 2015).\nThe FIM is an important concept for statistical machine learning. It gives a Riemannian metric (Hotelling, 1929; Rao, 1945) of the learning parameter space which is unique (Čencov, 1982; Dowty, 2017). Hence any learning is in a space that is intrinsically curved based on the FIM, regardless of the choice of the coordinate system. It also gives a bound (Fréchet, 1943; Cramér, 1946; Nielsen, 2013) of learning efficiency saying that the variance of any unbiased learning of Θ is at least I−1(Θ)/n, where n is the i.i.d. sample size. The FIM is applied to neural network optimization (Amari, 1997), metric learning (Lebanon, 2005), reinforcement learning (Thomas, 2014) and manifold learning (Sun & Marchand-Maillet, 2014).\nHowever computing the FIM is expensive. Besides the fact that learning machines have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large neuron system (e.g. one with millions of parameters, Szegedy, Christian et al. 2015) is very challenging due to the finiteness of data, and the huge number D(D+1)2 of matrix coefficients to evaluate. Furthermore, gradient descent techniques require inverting this large matrix and tuning the learning rate.\nTo tackle this problem, past works mainly focus on how to approximate the FIM with a block diagonal form (Kurita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu & Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016). This global approach faces increasing approximation error and increasing computational cost as the system scales up\nand as complex and dynamic structures (Looks et al., 2017) emerge.\nThis work aims at a different local approach. The idea is to accurately describe the information geometry (IG) in a subsystem of the large learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.\nFor this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined. Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometries of subsystems. This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic. It can be used to characterize the efficiency of a local learning process. Taking this stance has potential in deep learning because a deep neural network can be decomposed into many local components such as neurons or layers. The RFIM is well suited to the compositional block structures of neural networks. The RFIM can be used for out-of-core learning.\nThe paper is organized as follows. Sec. 2 reviews natural gradient within the context of Multi-Layer Perceptrons (MLPs). Sec. 3 formally defines the RFIM, and gives a table of RFIMs of several commonly used subsystems. Sec. 4 discusses the advantages of using the RFIM as compared to the FIM. Sec. 5 gives an algorithmic framework and proof-of-concept experiments on neural network optimization. Sec. 6 presents related works on parameter diagonalization. Sec. 7 concludes this work and further hints at perspectives."
  }, {
    "heading": "2. Natural Gradient: Review and Insights",
    "text": "Consider a MLP x θ1−→ h1 · · ·hL−1 θL−−→ y, whose statistical model is the following conditional distribution\np(y |x,Θ) = ∑\nh1,··· ,hL−1\np(h1 |x,θ1) · · · p(y |hL−1,θL).\nThe often intractable sum over h1, · · · ,hL−1 can be get rid off by deteriorating p(h1 |x,θ1), · · · , p(hL−1 |hL−2,θL−1) to Dirac’s deltas δ, and letting merely the last layer p(y |hL−1,θL) be stochastic. Other models such as restricted Boltzmann machines (Nair & Hinton, 2010; Montavon & Müller, 2012), deep belief networks (Hinton et al., 2006), dropout (Wager et al., 2013), and variational autoencoders (Kingma & Welling, 2014) do consider the hi’s to be stochastic.\nThe tensor metric of the neuromanifold (Amari, 1995) M, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM. Because a MLP corresponds to a con-\nditional distribution, its FIM is a function of the input x. By taking an empirical average over the input samples {xk}nk=1, the FIM of a MLP can be expressed as IΘ(Θ) = 1n ∑n k=1Ep(y |xk,Θ) [ ∂lk ∂Θ ∂lk ∂Θᵀ ] , where lk(Θ) = log p(y |xk, Θ) denotes the conditional log-likelihood function wrt xk.\nTo understand the meaning of the Riemannian metric IΘ(Θ), it measures the intrinsic difference between two nearby neural networks around Θ ∈ M. A learning step can be regarded as a tiny displacement δΘ onM. According to the FIM, the infinitesimal square distance\n〈δΘ, δΘ〉IΘ(Θ) = 1\nn n∑ k=1 Ep(y |xk,Θ)\n[( δΘᵀ\n∂lk ∂Θ )2] (1)\nmeasures how much δΘ (with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).\nConsider the negative log-likelihood function L(Θ) = − ∑n k=1 log p(yk |xk,Θ) wrt the observed pairs {(xk,yk)}nk=1, we try to minimize the loss while maintaining a small learning step size 〈δΘ, δΘ〉IΘ(Θ) on M. At Θt ∈ M, the target is to minimize wrt δΘ the Lagrange function\nL(Θt + δΘ) + 1\n2γ 〈δΘ, δΘ〉IΘ(Θt)\n≈ L(Θt) + δΘᵀ 5Θ L(Θt) + 1\n2γ δΘᵀIΘ(Θt)δΘ,\nwhere γ > 0 is a learning rate. The optimal solution of the above quadratic optimization gives a learning step\nδΘt = −γI−1Θ (Θt)5Θ L(Θt).\nIn this update procedure, ∇̃ΘL(Θ) = I−1Θ (Θ)5Θ L(Θ) replaces the role of the usual gradient ∇ΘL(Θ) and is called the natural gradient (Amari, 1997).\nAlthough the FIM depends on the chosen parameterization, the natural gradient is invariant to reparameterization. Let Λ be another coordinate system and J be the Jacobian matrix of the mapping Θ→ Λ. Then we have\nI−1Θ (Θ)5Θ L(Θ) = (J ᵀIΛ(Λ)J)−1 Jᵀ 5Λ L(Λ)\n= J−1I−1Λ (Λ)5Λ L(Λ),\nshowing that ∇̃ΘL(Θ) and ∇̃ΛL(Λ) are the same dynamic up to coordinate transformation. As the learning rate γ is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system (see e.g. Martens 2014). Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 Amari 1998).\nMΘ\nΘ yx\nMθ1\nx\nx+ ∆x\nθ1x\nMθ2h1\nh1 + ∆h1\nθ2h1\nMθ3\nh2\nh2 + ∆h2\nθ3h2 y\nModel:\nManifold:\nComputational graph:\nMetric:\nΘ\nΘ I(Θ)\nθ3 h2\nθ3\nh2\ngy(θ3)\nθ2 h1\nθ2\nh1\ngh2(θ2)\nθ1\nθ1 gh1(θ1)\np(y |Θ,x) = ∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2)\nFigure 1. (left) The traditional global geometry of a MLP; (right) information geometry of subsystems. The gray and blue meshes show that the subsystem geometry is dynamic when the reference variable makes a tiny move. The square under the (sub-)system means the (R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.\nFor the sake of simplicity, we do not discuss singular FIMs with a subset of parameters having zero metric. This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said to be non-regular (and the parameter Θ is not identifiable). The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).\nIn the family of 2nd-order optimization methods, a fuzzy line can be drawn from the natural gradient and alternative methods such as the Hessian-free optimization (Martens, 2010). By definition, the FIM is a property of the parameter space which is independent or weakly dependent on the input samples. For example, the FIM of a MLP is independent of {yi}. In contrast, the Hessian (or related concepts such as the Gauss-Newton matrix, Martens 2014) is a property of the learning cost function wrt the input samples.\nBonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a gradient descent step, thus ensuring to stay on the manifold for any chosen learning rate. Convergence is proven for Hadamard manifolds (of negative curvatures). However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds like the neuromanifold."
  }, {
    "heading": "3. RFIM: Definition and Expressions",
    "text": "In general, for large parametric systems, it is impossible to diagonalize or decorrelate all the parameters, so that we split instead all random variables into three parts θf , θ and h. We examine their intuitive meanings before giving the formal definition. The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis). This is in analogy to the notion of a reference frame in physics. θ is the\nsubsystem parameters, resembling the long-term memory adapting slowly to the observations (e.g. neural network weights). The response h is a random variable that reacts to the variations of θ. Usually, h is the output of the subsystem that is connected to neighbour subsystems (e.g. hidden layer outputs). Formally, a subsystem which factorizes the learning machine is characterized by the conditional distribution p(h |θ,θf ), where θ can be estimated based on h and θf . We make the following definition. Definition 1 (RFIM). Given θf , the RFIM 1 of θ wrt h is\ngh (θ |θf ) def = Ep(h | θ, θf ) [ ∂\n∂θ log p(h |θ, θf )\n∂\n∂θᵀ log p(h |θ, θf )\n] ,\nor simply gh (θ), corresponding to the estimation of θ based on observations of h given θf .\nFor example, consider a MLP. If we choose θf to be the input features x, choose h to be the final output y, and choose θ to be all the network weights Θ, then the RFIM becomes the FIM: I(Θ) = gy(Θ |x).\nMore generally, we can choose the response h to be other than the observables to compute the Fisher information of subsystems, especially dynamically during the learning of the global machine. To see the meaning of the RFIM, similar to eq. (1), the infinitesimal square distance 〈δθ, δθ〉gh(θ) = Ep(h | θ, θf ) [( δθᵀ ∂∂θ log p(h |θ, θf )\n)2] measures how much δθ impacts intrinsically the stochastic mapping θ → h which features the subsystem. We have the following proposition following definition 1.\nProposition 2 (Relative Geometry Consistency). If θ1 consists of a subset of θ2 so that θ2 = (θ1, θ̃1), then ∀θ̃1, Mθ1 with the metric gh(θ1 | θ̃1) has exactly the same Rie-\n1We use the same term “relative FIM” (Zegers, 2015) with a different definition.\nmannian metric with the sub-manifold {θ2 ∈ Mθ2 : θ̃1 is fixed} induced by the ambient metric gh (θ2).\nWhen the response h is chosen, then different splits of (θ,θf ) are consistent with the same ambient geometry.\nFigure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s parameter sensitivity to the external environment (x and y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h. The two-colored meshes show that the geometry structure is dynamic and varies with the reference variable θf .\nOne should not confuse the RFIM with the diagonal blocks of the FIM (Kurita, 1994). Both their meanings and expressions are different. The RFIM is computed by integrating out the hidden response variables h. The FIM is always computed by integrating out the observables x and y. Hence the RFIM is a more general concept and includes the FIM as a special case. This highlights a main difference with the backpropagated metric (Ollivier, 2013), which essentially considers parameter sensitivity wrt the final output. Despite the fact that the FIMs of small parametric structures such as single neurons was studied (Amari, 1997), we are not looking at a small single-component system but a component embedded in a large system, targeting at improving the large system.\nIn the following we provide a short table of commonly used RFIMs for future reference (the RFIMs listed are mostly straightforward from definition 1, with detailed derivations given in the supplementary material). This is meaningful since the RFIM is a new concept. We also want to demonstrate these simple closed form expressions without any approximations."
  }, {
    "heading": "3.1. RFIMs of One Neuron",
    "text": "We start from the RFIM of single neuron models. Consider a stochastic neuron with input x and weights w. After a nonlinear activation function f , the output y is randomized surrounding the mean f(wᵀx̃) with a variance. Throughout this paper x̃ = (xᵀ, 1)ᵀ denotes the augmented vector of x (homogeneous coordinates) so that wᵀx̃ contains a bias term, and a general linear transformation can be written simply asAx̃.\nUsing x as the reference, the RFIM of w with respect to y has a common form gy(w |x) = νf (w,x)x̃x̃ᵀ, where νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron. This agrees with early studies on single neuron FIMs (Amari, 1997; Kurita, 1994).\nIf f(t) = tanh(t) is the hyperbolic tangent func-\ntion, then νf (w,x) = sech2(wᵀx̃), where sech(t) = 2 exp(t)+exp(−t) is the hyperbolic secant function. Similarly, if f(t) = sigm(t) is the sigmoid function, then νf (w,x) = sigm (w ᵀx̃) [ 1− sigm (wᵀx̃) ] .\nIf f is defined by Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) as a special case, so that f(t) = t (t ≥ 0), f(t) = ιt (t < 0), 0 ≤ ι < 1, then under certain approximations (see supplementary material)\nνf (w,x) =\n[ ι+ (1− ι)sigm ( 1− ι ω wᵀx̃ )]2 ,\nwhere ω > 0 is a hyper-parameter (e.g. ω = 1).\nFor the exponential linear unit (ELU) (Clevert et al., 2015), f(t) = t (t ≥ 0), f(t) = α (exp(t)− 1) (t < 0), where α > 0 is a hyper-parameter. We get\nνf (w,x) = { 1 if wᵀx̃ ≥ 0 α2 exp (2wᵀx̃) if wᵀx̃ < 0."
  }, {
    "heading": "3.2. RFIM of One Layer",
    "text": "Let D denote the dimensionality of the corresponding variable. A linear layer with input x, connection weights W = [ w1, · · · ,wDy ] , and stochastic output y can be represented by y ∼ G(W ᵀx̃, σ2I), where I is the identity matrix, and σ is the scale of the observation noise, and G(µ,Σ) is a multivariate Gaussian distribution with mean µ and covariance matrix Σ. We vectorize W by stacking its columns {wi}. Then gy(W |x) is a tensor of size (Dx + 1)Dy× (Dx + 1)Dy , given by gy(W |x) = diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ], where diag(·) means the (block) diagonal matrix constructed by the given matrix entries.\nA nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀx̃, and then randomized wrt the choice of the neuron. By definition 1, its RFIM is given by\ngy (W |x) = diag [ νf (w1,x)x̃x̃ ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ] , (2)\nwhere νf (wi,x) is given in Subsec. 3.1.\nA softmax layer, which often appears as the last layer of a MLP, is given by y ∈ {1, . . . ,m}, where p(y) = ηy = exp(wyx̃)∑m i=1 exp(wix̃) . Its RFIM is a dense matrix given by\ngy(W ) =  (η1 − η21)x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... . . .\n... −ηmη1x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ  . Notice that its i’th diagonal block (ηi − η2i )x̃x̃ᵀ resembles the RFIM of a single sigm neuron."
  }, {
    "heading": "3.3. RFIM of Two Layers",
    "text": "By eq. (2), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations, which must be obtained by looking at a larger subsystem. Consider a two-layer model with stochastic output y around the mean vector f(Cᵀh̃), where h = f (W ᵀx̃). For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the interneuron correlations within the first layer. To do this, both x and C are considered as references to compute the RFIM of W . By definition 1, gy(W |x,C) = [Gij ]Dh×Dh and each block has the form\nGij = Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ.\nNow that we have the one-layer and two-layer RFIMs, we can either split a given feed-forward neural network into one-layer subsystems or into two-layer subsystems. A trade-off is that using a larger subsystem entails greater analytical and computational difficulty, although it could more accurately model the global system dynamics. In the extreme case, the FIM is obtained if the whole system is considered as one single subsystem."
  }, {
    "heading": "4. RFIM: Key Advantages",
    "text": "This section discusses the theoretical advantages of the RFIM over the FIM. Consider wlog a MLP with Bernoulli outputs y ∈ {0, 1}m, whose mean µ is a deterministic function depending on the input x and the network parameters Θ. By Sec. 2, the FIM of the MLP can be computed as (see supplementary for proof)\nI(Θ) = 1 n n∑ i=1 m∑ j=1\n1 µj(xi)(1− µj(xi)) ∂µj(xi) ∂Θ ∂µj(xi) ∂Θᵀ .\n(3) Therefore rank(I(Θ)) ≤ nm. The rank of a diagonal block of I(Θ) corresponding to one layer is even smaller. In a deep neural network (e.g. Szegedy, Christian et al. 2015), if the sample size n < dim(Θ)/m, then I(Θ) is doomed to be singular. All methods trying to approximate the FIM suffer from this problem and therefore rely on proper regularizations. If the network is decomposed into layers, the RFIM of each subsystem (layer) is given by eq. (2). Each sample can contribute maximally 1 to the rank of the neuron-RFIM and can contribute maximally Dy to the rank of the layer-RFIM. It only requires maxi{dim(wi)} (the maximum layer width) observations to have a full rank RFIM, where wi is the weight vector of the i’th neuron. The RFIM is expected to have a much higher rank than the FIM. Higher rank means less singularity and more information is captured. Models that can\nbe distinguished by the RFIM may be identical in the sense of the FIM. Essentially, the RFIM integrates the internal randomness (Bengio, 2013) of the neural system by considering the output of each layer as a random variable. In theory, the FIM should also consider stochastic neurons. However it requires marginalizing the joint distribution of h1, h2, · · · , y. This makes the already infeasible computation even more challenging.\nThe RFIM is not an approximation of the FIM but is an accurate metric, defining the geometry of θ wrt to its direct response h in the system, or adjacent nodes in a graphical model. By the example in fig. 1, gy(θL) of the last layer is exactly the corresponding block in I(Θ): they both characterize how θL affects the mapping hL−1 → y. They start to diverge from the second to last layer. To compute the geometry of θL−1, the RFIM looks at how θL−1 affects the local mapping hL−2 → hL−1, which can be measured reliably regardless of the rest of the system (think of a “debugging” process to separate and measure a single component). In contrast, the FIM examines how θL−1 affects the non-local mapping hL−2 → y. This is a difficult task because it must consider the correlation between different layers. As an approximation, the block diagonalized version of the FIM ignores such correlations and therefore faces the loss of accuracy.\nThe RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems are balanced during learning. Consider a set of interconnected subsystems with internal parameters {θl} and the corresponding response variables {hl}. The RFIM ghl(θl) measures how much the likelihood surface of hl is curved wrt a small learning step δθl. By constraining the squared Riemannian distance δθᵀl g\nhl(θl)δθl having similar scales, different subsystems will present similar variations during learning. Within one subsystem, the learning along sensitive parameter directions is penalized. Among different subsystems, the learning of sensitive subsystems is penalized. Globally, the inter-subsystem stochastic connections have similar variance, maintaining a stable reference system and achieving efficient learning. This is similar to the idea of batch normalization (BN) (Ioffe & Szegedy, 2015) but has a deeper theoretical foundation.\nFormally, we have the following theorem.\nTheorem 3. Consider a learning system represented by a joint distribution p(x,h) of x (observables) and h (hidden variables which connect subsystems). The joint FIM J (Θ) = Ep ( log p(x,h |Θ) ∂Θ log p(x,h |Θ) ∂Θᵀ ) has a block diagonal form. Each block isEp(gh(θ)), where θ is the parameters within a subsystem and h is its response variables to neighour subsystems.\nThe global correspondence of the local RFIM is the joint\nFIM. By theorem 3, the square distance dΘᵀJ (Θ)dΘ = Ep( ∑ l dθ ᵀ l g hl(θl)dθl) measures the system variance, including both the observables x and the hidden variables h. An intrinsic trade-off between the RFIM and the FIM is learning system stability versus efficiency. Normalizing the FIM is more efficient because it helps to achieve Fisher efficiency (Amari, 1998). Normalizing the RFIM is more stable since the hidden variations are bounded, which only guarantees subsystem Fisher efficiency characterized by the Cramér-Rao lower bound of local parameters."
  }, {
    "heading": "5. Relative Natural Gradient Descent",
    "text": "The traditional non-parametric way of applying natural gradient requires re-calculating the FIM and solving a large linear system in each learning step. Besides the huge computational cost, it has a large approximation error. For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations. That is, the FIM of a mini-batch is likely to be singular or poorly conditioned.\nA recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry. For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers parametrize the geometry of the neural manifold.\nBy dividing the learning system into subsystems, the RFIM potentially gives a systematical implementation of parametric natural gradient descent. The memory complexity of storing the Riemannian metric has been reduced from O(D2) to O( ∑ iD 2 i ), where Di = dim(wi) is the size of the i’th neuron. Consider there are M neurons in total, then the memory cost is reduced by a factor of M . The computational complexity has been reduced from O(D%) (% ≈ 2.373, Williams 2012) to O( ∑ iD % i ). Optimization based on RFIM is called Relative Natural Gradient Descent (RNGD).\nThe good performance of batch normalization (Ioffe & Szegedy, 2015) provides an empirical support for the RFIM. Basically, BN uses an inter-sample normalization layer to transform the layer input x to z with zero mean and unit variance and thus reduces “internal covariate shift”. In a typical case, above this normalization layer is a linear layer given by y = W ᵀz̃. If each dimension of z is normalized, then the diagonal blocks of the linear layer RFIM gy(W ) = diag[z̃z̃ᵀ, · · · , z̃z̃ᵀ] become a covariance matrix with identity diagonal entries (after taking an empirical average). This gives the coordinate system W a well conditioned RFIM for efficient learning.\n5.1. RNGD with a relu MLP\nThis subsection builds a proof-of-concept experiment on MLP optimization. We partition the MLP into layers (one layer consists of a linear layer plus an element-wise nonlinear activation function) as the subsystems. By eq. (2), the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 = x) and weights {wl1, · · · ,wlml} is\ndiag [ νf (wl1,hl−1)h̃l−1h̃ ᵀ l−1, · · · , νf (wlml ,hl−l)h̃l−1h̃ ᵀ l−1 ] .\nThe subsystem stability during one learning step δw can be measured geometrically by∑L l=1 ∑ml i=1 νf (wli,hl−1)(δw ᵀ lih̃l−1)\n2. Using this term as the geometric cost (the Lagrange term) in the trust region approach in Sec. 2, we get the following RNGD method. In a stochastic gradient descent scenario, each neuron i in layer l is updated by\nwnewli ← woldli −G−1li ∂E\n∂wli ,\nwhere E is the cost function and Gli is a learned metric. The consideration is that a mini-batch of samples do not contain enough information to compute the RFIM, which should be averaged over all training samples. Therefore, for the i’th neuron in layer l, Gli is initialized to identity, and is updated based on\nGnewli ← (1− λ)Goldli + λνf (wli,hl−1)h̃l−1h̃ ᵀ l−1 + I,\nwhere > 0 is a hyper-parameter to avoid singularity caused by small sample size, and the average is taken over all samples in a mini-batch, and λ is a learning rate. In theory, λ should be gradually reduced to zero to guarantee the convergence of this geometry learning. To avoid solving a linear system in each iteration, every T iterations we recompute and store G−1li based on the most updated Gli. In the next T iterations, this G−1li will be used as an approximation of the inverse RFIM. For the input layer which scales with the number of input features, and the final softmax layer, we apply instead the RFIM of the corresponding linear layer to improve the computational efficiency.\nWe compare different optimizers on classifying MNIST digits. The network has shape 784-80-80-80-10, with relu activation units, a final soft-max layer, and uses the persample average cross-entropy with L2-regularization as the learning cost function. We experiment on two different architectures: one is a plain MLP (PLAIN); the other has a batch normalization layer after each hidden layer (BNA), where a rescaling parameter is applied to ensure enough flexibility of the parametric structure (Ioffe & Szegedy, 2015). For simplicity, the architecture, mini-batch size (50), and L2 regularization strength (10−3) are fixed to be the same for all compared methods. The observations are consistent when these configurations vary.\nFigure 2 shows the learning curves of different methods. SGD is stochastic gradient descent. ADAM is the Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999 and = 10−8. Our RNGD is implemented by modifying TensorFlow’s (Abadi, Martı́n et al., 2015) SGD optimizer. We set empirically T = 100, λ = 0.005 and ω = 1.\nRNGD presents a sharper learning curve and better generalization, especially when it is combined with BN. In this case, the final tranining error of RNGD is slightly larger than ADAM because by validation it favors a larger learning rate, which is applied on the neural network weights (based on RNGD) and BN parameters (based on SGD). For the ReLU activation, νf (wi,x) is approximately binary, emphasizing such informative samples with wᵀi x̃ > 0, which are the ones contributing to the learning of wi with non-zero gradient values. Each output neuron has a different subset of informative samples. RNGD normalizes x differently wrt different output neurons, so that the in-\nformative samples for each output neuron are centered and decorrelated.\nIn the above experiment, RNGD’s computational time per each epoch is roughly 4 ∼ 10 times more than SGD and ADAM on a modern graphic card. Therefore in terms of wall clock time RNGD does not show advantages. This can be improved by more efficient implementations with low rank approximation techniques and early stopping. Our RNGD prototype hints at a promising direction to develop scalable 2nd-order deep learning optimizers based on the RFIM."
  }, {
    "heading": "6. Related Works on FIM Diagonalization",
    "text": "One may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert. This fundamental problem of parameter orthogonalization was first investigated by Jeffreys (1998) for decorrelating the parameters of interest from the nuisance parameters. Fisher diagonalization yields parameter orthogonalization (Cox & Reid, 1987), and is proved useful when estimating Θ̂ using a maximum likelihood estimator (MLE) that is asymptotically normally distributed, Θ̂n ∼ G(Θ, I−1(Θ)/n), and efficient since the variance of the estimator matches the Cramér-Rao lower bound. Using the chain rule, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying∑\ni,j\nE\n[ ∂2l\n∂Θi∂Θj ] ∂Θi ∂Ωk ∂Θj ∂Ωl = 0, ∀k 6= l.\nThus in general, we end up with ( D 2 ) = D(D−1)2 (nonlinear) partial differential equations to satisfy (Huzurbazar, 1950). Therefore, in general there is no solution when( D 2 ) > D, that is when D > 3. When D = 2, the single differential equation is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1σp0( x−µ σ )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1998) reported a parameter orthogonalization for Pearson’s distributions of type I which is of orderD = 4. Cox and Reid (1987) further investigated this topic with application to conditional inference, and provide examples (including the Weibull distribution).\nFrom the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ). When the FIM may be degenerate, this yields a pseudoRiemannian manifold (Thomas, 2014). In differential geometry, orthogonalization amounts to transforming the square length infinitesimal element gijdΘiΘj of a Riemannian geometry into an orthogonal system ω with match-\ning square length infinitesimal element ΩiidΩ2i . However, such a global orthogonal metric does not exist (Huzurbazar, 1950) when D > 3 for an arbitrary metric tensor, although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant & Vickers, 2009).\nFor NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e denotes the subvector (vb, ..., ve)ᵀ of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function. It defines a dual affine coordinate systems ei = ∂i = ∂∂Hi and ej = ∂\nj = ∂∂Θj that are orthogonal: 〈ei, ej〉 = δij , where δij = 1 iff i = j and δij = 0 otherwise. Hence the FIM has two diagonal blocks. Those dual affine coordinate systems are defined up to an affine invertible transformation: Θ̃ = AΘ + b, H̃ = A−1H + c. In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (Θ1, H2) or (H1,Θ2).\nThe RFIM contributes another line of thought in parameter diagonalization. We investigate the Fisher information of hidden variables, or internal interfaces in the learning machine. This is novel since the majority of previous works concentrate on the FIM of the observables, or the external interface of the machine. From a causality perspective, we factor out the main cause (parameters within the subsystem) of the response variable with a direct action-reaction relationship, and regard the remaining parameters as a reference that can be easily estimated by the empirical distribution. This simplification may lead to broader applications of Fisher information in machine learning.\nThe particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations. Our splits in RFIMs consider general non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (see section 3.7 of Amari & Nagaoka 2000)."
  }, {
    "heading": "7. Conclusion and Discussions",
    "text": "We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric. The key advantage of this approach is that the local learning dynamics can be analyzed in an accurate way without approximation. We present a core list of such local structures in neural networks, and give their corresponding RFIMs. This list of recipes can be used to provide guiding principles to design new optimizers for deep learning.\nOur work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti & Mukherjee, 2015) as follows: In mirror descent to minimize a cost function E(Θ), given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as:\nΘt+1 = arg min Θ\n{ Θ>∇E(Θt) + 1\nγ D(Θ,Θt)\n} .\nWhen D(Θ,Θ′) is chosen as a Bregman divergence BF (Θ,Θ\n′) = F (Θ)− F (Θ′)− (Θ−Θ′)>∇F (Θ′) wrt to a convex function F , it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti & Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇2F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ).\nIn general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I) converges. Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient. The energy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X,Y ) − dp(Θ1)(X,X\n′) − dp(Θ1)(Y, Y ′)] where X,X ′ ∼ p(Θ1) and Y, Y ′ ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance metric over the support. Using a Taylor’s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor’s expansion of any f -divergence like the Kullback-Leibler divergence). Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance. Twisting the geometry of the support (say, Wasserstein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015). In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of the support to itself (Čencov, 1982). There is no neighbourhood structure on the support in IG. Markov morphism includes deterministic transformation of a random variable by a statistic. It is well-known that IT (Θ) IX(Θ) with equality iff. T = T (X) is a sufficient statistic of X . Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y )) = dp(Θ)(X,Y ).\nWe believe that RFIMs will provide a sound methodology to build further efficient systems for deep learning. The full source codes to reproduce the experimental results are available at https://www.lix.polytechnique. fr/˜nielsen/RFIM."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors would like to thank the anonymous reviewers and Yann Ollivier for the helpful comments. This work was mainly conducted when the first author was a postdoctoral researcher at École Polytechnique."
  }],
  "year": 2017,
  "references": [{
    "title": "TensorFlow: Large-scale machine learning",
    "authors": ["Abadi", "Martı́n"],
    "venue": "on heterogeneous systems,",
    "year": 2015
  }, {
    "title": "Information geometry of the EM and em algorithms for neural networks",
    "authors": ["Amari", "Shun’ichi"],
    "venue": "Neural Networks,",
    "year": 1995
  }, {
    "title": "Neural learning in structured parameter spaces – natural Riemannian gradient",
    "authors": ["Amari", "Shun’ichi"],
    "venue": "In NIPS",
    "year": 1997
  }, {
    "title": "Natural gradient works efficiently in learning",
    "authors": ["Amari", "Shun’ichi"],
    "venue": "Neural Comput.,",
    "year": 1998
  }, {
    "title": "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences",
    "authors": ["Amari", "Shun’ichi"],
    "year": 2016
  }, {
    "title": "Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs",
    "authors": ["Amari", "Shun’ichi", "Nagaoka", "Hiroshi"],
    "venue": "AMS and OUP,",
    "year": 2000
  }, {
    "title": "Estimating or propagating gradients through stochastic neurons",
    "authors": ["Bengio", "Yoshua"],
    "venue": "CoRR, abs/1305.2982,",
    "year": 2013
  }, {
    "title": "Stochastic gradient descent on Riemannian manifolds",
    "authors": ["Bonnabel", "Silvère"],
    "venue": "IEEE Trans. Automat. Contr.,",
    "year": 2013
  }, {
    "title": "Statistical decision rules and optimal inference, volume 53 of Translations of Mathematical Monographs",
    "authors": ["Čencov", "Nikolaı̌ Nikolaevich"],
    "venue": "American Mathematical Society,",
    "year": 1982
  }, {
    "title": "An Interpolating Distance between Optimal Transport and Fisher-Rao",
    "authors": ["Chizat", "Lenaic", "Schmitzer", "Bernhard", "Peyré", "Gabriel", "Vialard", "François-Xavier"],
    "venue": "arXiv e-prints,",
    "year": 2015
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (ELUs)",
    "authors": ["Clevert", "Djork-Arné", "Unterthiner", "Thomas", "Hochreiter", "Sepp"],
    "venue": "CoRR, abs/1511.07289,",
    "year": 2015
  }, {
    "title": "Estimation and moment recursion relations for multimodal distributions of the exponential family",
    "authors": ["Cobb", "Loren", "Koppstein", "Peter", "Chen", "Neng Hsin"],
    "year": 1983
  }, {
    "title": "Parameter orthogonality and approximate conditional inference",
    "authors": ["D.R. Cox", "N. Reid"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
    "year": 1987
  }, {
    "title": "Mathematical Methods of Statistics, volume 9 of Princeton Mathematical Series",
    "authors": ["Cramér", "Harald"],
    "year": 1946
  }, {
    "title": "Natural neural networks",
    "authors": ["Desjardins", "Guillaume", "Simonyan", "Karen", "Pascanu", "Razvan", "Kavukcuoglu", "Koray"],
    "venue": "In NIPS",
    "year": 2015
  }, {
    "title": "Chentsov’s theorem for exponential families",
    "authors": ["Dowty", "James G"],
    "venue": "arXiv preprints,",
    "year": 2017
  }, {
    "title": "Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information",
    "authors": ["Efron", "Bradley", "Hinkley", "David V"],
    "year": 1978
  }, {
    "title": "Sur l’extension de certaines evaluations statistiques au cas de petits echantillons",
    "authors": ["Fréchet", "Maurice"],
    "venue": "Revue de l’Institut International de Statistique / Review of the International Statistical Institute,",
    "year": 1943
  }, {
    "title": "Block diagonalization of four-dimensional metrics",
    "authors": ["Grant", "James DE", "Vickers", "JA"],
    "venue": "Classical and Quantum Gravity,",
    "year": 2009
  }, {
    "title": "Delving deep into rectifiers: Surpassing humanlevel performance on ImageNet classification",
    "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "A fast learning algorithm for deep belief nets",
    "authors": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "YeeWhye"],
    "venue": "Neural Comput.,",
    "year": 2006
  }, {
    "title": "Spaces of statistical parameters",
    "authors": ["Hotelling", "Harold"],
    "venue": "American Mathematical Society Meeting,",
    "year": 1929
  }, {
    "title": "Probability distributions and orthogonal parameters",
    "authors": ["Huzurbazar", "Vasant Shankar"],
    "venue": "Mathematical Proceedings of the Cambridge Philosophical Society,",
    "year": 1950
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"],
    "venue": "In ICML; JMLR: W&CP",
    "year": 2015
  }, {
    "title": "Theory of Probability. Oxford Classic Texts in the Physical Sciences",
    "authors": ["Jeffreys", "Harold"],
    "venue": "OUP, 3rd edition,",
    "year": 1998
  }, {
    "title": "Riemannian Geometry and Geometric Analysis",
    "authors": ["Jost", "Jürgen"],
    "venue": "Springer, 6th edition,",
    "year": 2011
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"],
    "venue": "CoRR, abs/1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["Kingma", "Diederik P", "Welling", "Max"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Iterative weighted least squares algorithms for neural networks classifiers",
    "authors": ["Kurita", "Takio"],
    "venue": "New Generation Computing,",
    "year": 1994
  }, {
    "title": "Topmoumoute online natural gradient algorithm",
    "authors": ["Le Roux", "Nicolas", "Manzagol", "Pierre-Antoine", "Bengio", "Yoshua"],
    "venue": "In NIPS",
    "year": 2008
  }, {
    "title": "Riemannian geometry and statistical machine learning",
    "authors": ["Lebanon", "Guy"],
    "venue": "PhD thesis,",
    "year": 2005
  }, {
    "title": "Deep learning with dynamic computation graphs",
    "authors": ["Looks", "Moshe", "Herreshoff", "Marcello", "Hutchins", "DeLesley", "Norvig", "Peter"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Deep learning via Hessian-free optimization",
    "authors": ["Martens", "James"],
    "venue": "In ICML, pp",
    "year": 2010
  }, {
    "title": "New perspectives on the natural gradient method",
    "authors": ["Martens", "James"],
    "venue": "CoRR, abs/1412.1193,",
    "year": 2014
  }, {
    "title": "Optimizing neural networks with Kronecker-factored approximate curvature",
    "authors": ["Martens", "James", "Grosse", "Roger"],
    "venue": "In ICML; JMLR: W&CP",
    "year": 2015
  }, {
    "title": "Computational implications of reducing data to sufficient statistics",
    "authors": ["Montanari", "Andrea"],
    "venue": "Electron. J. Statist.,",
    "year": 2015
  }, {
    "title": "Deep Boltzmann machines and the centering trick",
    "authors": ["Montavon", "Grégoire", "Müller", "Klaus-Robert"],
    "year": 2012
  }, {
    "title": "Rectified linear units improve restricted Boltzmann machines",
    "authors": ["Nair", "Vinod", "Hinton", "Geoffrey E"],
    "venue": "In ICML, pp",
    "year": 2010
  }, {
    "title": "Cramér-Rao lower bound and information",
    "authors": ["Nielsen", "Frank"],
    "venue": "geometry. CoRR,",
    "year": 2013
  }, {
    "title": "Riemannian metrics for neural networks",
    "authors": ["Ollivier", "Yann"],
    "venue": "CoRR, abs/1303.0818,",
    "year": 2013
  }, {
    "title": "Revisiting natural gradient for deep networks",
    "authors": ["Pascanu", "Razvan", "Bengio", "Yoshua"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Deep learning made easier by linear transformations in perceptrons",
    "authors": ["Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann"],
    "venue": "In AISTATS; JMLR W&CP",
    "year": 2012
  }, {
    "title": "Information and accuracy attainable in the estimation of statistical parameters",
    "authors": ["Rao", "Calyampudi Radhakrishna"],
    "venue": "Bull. Cal. Math. Soc.,",
    "year": 1945
  }, {
    "title": "The information geometry of mirror descent",
    "authors": ["Raskutti", "Garvesh", "Mukherjee", "Sayan"],
    "venue": "In Geometric Science of Information (GSI),",
    "year": 2015
  }, {
    "title": "An information geometry of statistical manifold learning",
    "authors": ["Sun", "Ke", "Marchand-Maillet", "Stéphane"],
    "venue": "In ICML; JMLR W&CP",
    "year": 2014
  }, {
    "title": "Going deeper with convolutions",
    "authors": ["Szegedy", "Christian"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "GeNGA: A generalization of natural gradient ascent with positive and negative convergence results",
    "authors": ["Thomas", "Philip"],
    "venue": "In ICML; JMLR W&CP",
    "year": 2014
  }, {
    "title": "Energetic natural gradient descent",
    "authors": ["Thomas", "Philip", "B.C. da Silva", "C. Dann", "E. Brunskill"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Dropout training as adaptive regularization",
    "authors": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"],
    "venue": "In NIPS",
    "year": 2013
  }, {
    "title": "Algebraic Geometry and Statistical Learning Theory, volume",
    "authors": ["Watanabe", "Sumio"],
    "venue": "Cambridge Monographs on Applied and Computational Mathematics. CUP,",
    "year": 2009
  }, {
    "title": "Multiplying matrices faster than Coppersmith-Winograd",
    "authors": ["Williams", "Virginia Vassilevska"],
    "venue": "In Annual ACM Symposium on Theory of Computing,",
    "year": 2012
  }],
  "id": "SP:a4f7bfdfc3207e061f5d67d3db1f6136e3787e9d",
  "authors": [{
    "name": "Ke Sun",
    "affiliations": []
  }, {
    "name": "Frank Nielsen",
    "affiliations": []
  }],
  "abstractText": "Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner’s structure turns large and complex. This paper makes a preliminary step towards a new direction. We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization. 1. Fisher Information Metric The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ). Under light regularity conditions, FIM can be rewritten equivalently as Iij = −Ep [ ∂l ∂Θi∂Θj ]",
  "title": "Relative Fisher Information and Natural Gradient for Learning Large Modular Models"
}