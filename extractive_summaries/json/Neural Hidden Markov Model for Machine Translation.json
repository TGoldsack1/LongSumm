{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n377"
  }, {
    "heading": "1 Introduction",
    "text": "Attention-based neural translation models (Bahdanau et al., 2015; Luong et al., 2015) attend to specific positions on the source side to generate translation. Using the attention component provides significant improvements over the pure encoder-decoder sequence-to-sequence approach (Sutskever et al., 2014) that uses no such attention mechanism. In this work, we aim to compare the performance of attention-based models to another baseline, namely, neural hidden Markov models.\nThe neural HMM has been successfully applied in the literature on top of conventional phrasebased systems (Wang et al., 2017). In this work, our purpose is to explore its application in standalone decoding, i.e. the model is used to generate and score candidates without assistance from a phrase-based system. Because translation is done standalone using only neural models, we still refer to this as NMT. In addition, while Wang et al. (2017) applied feedforward networks to model alignment and translation, the recurrent structures proposed in this work surpass the feedforward variants by up to 1.3% in BLEU.\nBy comparing neural HMM and attention-based NMT, we shed light on the role of the attention component. To this end, we use an alignmentbased model that has a recurrent bidirectional encoder and a recurrent decoder, but use no attention component. We replace the attention mechanism by a first-order HMM alignment model. Attention levels are deterministic normalized similarity scores part of the architecture design of an otherwise fully supervised classifier. HMM-style alignments on the other hand are discrete random variables and (unlike attention levels) must be marginalized. Once alignments are marginalized, which is tractable for a first-order HMM, parameters can be estimated to attain a local optimum of log-likelihood of observations as usual."
  }, {
    "heading": "2 Motivation",
    "text": "In attention-based approaches, the alignment distribution is used to select the positions in the source sentence that the decoder attends to during translation. Thus the alignment model can be considered as an implicit part of the translation model. On the other hand, separating the alignment model from the lexicon model has its own advantages: First of all, this leads to more flexibility in modeling and training: The models can not only be trained separately, but they can also have different model types, such as neural models, count-based models, etc. Second, the separation avoids propagating errors from one model to another. In attention-based systems, the translation score is based on the alignment distribution, in which errors can be propagated from the alignment part to the translation part. Third, probabilistic treatment to alignments in NMT typically implies an extended degree of interpretability (e.g. one can inspect posteriors) and control over the model (e.g. one can impose priors over alignments\nand lexical distributions)."
  }, {
    "heading": "3 Neural Hidden Markov Model",
    "text": "Given a source sentence fJ1 = f1...fj ...fJ and a target sentence eI1 = e1...ei...eI , where j = bi is the source position aligned to the target position i, we model translation using an alignment model and a lexicon model:\np(eI1|fJ1 ) = ∑ bI1 p(eI1, b I 1|fJ1 ) (1)\n:= ∑ bI1 I∏ i=1 p(ei|bi1, ei−10 , f J 1 )︸ ︷︷ ︸ lexicon model · p(bi|bi−11 , e i−1 0 , f J 1 )︸ ︷︷ ︸ alignment model\n(2)\nInstead of predicting the absolute source position bi, we use an alignment model p(∆i|bi−11 , e i−1 0 , f J 1 ) that predicts the jump ∆i = bi − bi−1. Wang et al. (2017) applied feedforward neural networks for modeling the lexicon and alignment probabilities. In this work, we would like to model these distributions using recurrent neural networks (RNN). RNNs have been shown to outperform feedforward variants in language and translation modeling. This is mainly due to that RNN can handle arbitrary input lengths and thus include unbounded context information. Unfortunately, the recurrent hidden layer cannot be easily applied for the neural hidden Markov model, since it will significantly complicate the computation of forward-backward messages when running Baum-Welch. Nevertheless, we can apply long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) structure for source and target words embedding. With this technique we can take the essence of LSTM RNN and do not break any sequential generative model assumptions.\nOur models are close in structure to the model proposed in Luong et al. (2015), where we have a component that encodes the source sentence, and another that encodes the target sentence. As shown in Figure 1, we use a source side bidirectional LSTM embedding hj = −→ h j + ←− h j , where −→ h j = LSTM(W, fj , −→ h j−1) and ←− h j = LSTM(V, fj , ←− h j+1), as well as a target side LSTM embedding si−1 = LSTM(U, ei−1, si−2). hj , −→ h j , ←− h j and si−1, si−2 are vectors, W , V and U are weight matrices. Before the non-linear hidden layers, there is a projection layer which\nconcatenates hj , si−1 and ei−1. Then the neural network-based lexicon model is given by\np(ei|bi1, ei−10 , f J 1 ) := p(ei|hj , si−1, ei−1) (3)\nand the neural network-based alignment model\np(bi|bi−11 , e i−1 0 , f J 1 ) := p(∆i|hj′ , si−1, ei−1) (4)\nwhere j′ = bi−1. The training criterion is the logarithm of sentence posterior probabilities over training sentence pairs (Fr, Er), r = 1, ..., R:\narg max θ {∑ r log pθ(Er|Fr) } (5)\nThe derivative for a single sentence pair (F,E) = (fJ1 , e I 1) is:\n∂\n∂θ log pθ(E|F ) = ∑ j′,j ∑ i pi(j ′, j|fJ1 , eI1; θ)\n· ∂ ∂θ log p(j, ei|j′, ei−10 , f J 1 ; θ)\n(6)\nwith HMM posterior weights pi(j′, j|fJ1 , eI1; θ), which can be computed using the forwardbackward algorithm.\nThe entire training procedure can be summarized as backpropagation in an EM framework:\n1. compute: • the posterior HMM weights • the local gradients (backpropagation)\n2. update neural network weights"
  }, {
    "heading": "4 Decoding",
    "text": "In the decoding stage we still calculate the sum over alignments and apply a target-synchronous beam search for the target string.\nThe auxiliary quantity for each unknown partial string ei0 is specified as Q(i, j; e i 0). During search, the partial hypothesis is extended from ei−10 to e i 0:\nQ(i, j; ei0) = ∑ j′ [ p(j, ei|j′, ei−10 , f J 1 ) ·Q(i− 1, j′; ei−10 ) ] (7)\nThe decoder is shown in Algorithm 1. In the innermost loop (line 11-13), alignments are hypothesized and used to calculate the auxiliary quantity Q(i, j; ei0). Then for each source position j, the lexical distribution over the full target vocabulary is computed (line 14). The distributions are accumulated (Q(i; ei0) = ∑ j Q(i, j; e i 0), line 16), then sorted (line 18) and the best candidate translations (arg maxei Q(i; e i 0)) lying within the beam are used to expand the partial hypotheses (line 19-23). cache is a two-dimensional list of size J × |Vsrc| (source vocabulary size), which is used to cache the current quantities.\nWhenever a partial hypothesis in the beam ends with the sentence end symbol (<EOF>), the counter will be increased by 1 (line 26-28). The translation is terminated if the counter reaches the beam size or hypothesis sentence length reaches three times the source sentence length (line 6). If a hypothesis stops but its score is worse than other hypotheses, it is eliminated from the beam, but it still contests non-terminated hypotheses. During comparison the scores are normalized by hypothesis sentence length. Note that we have no explicit coverage constraints. This means that a source position can be revisited many times, whereby creating one-to-many alignment cases. This also allows unaligned source words.\nIn the neural HMM decoder, word alignments are estimated and scored according to the distribution calculated by the neural network alignment model, leading alignment decisions to become part of the beam search. The search space consists of both alignment and translation decisions. In contrast, the search space in attentionbased decoding consists only of translation decisions.\nThe decoding complexity is O(J2 · I) (J = source sentence length, I = target sentence length)\nAlgorithm 1 Neural HMM Decoder\n1: function TRANSLATE(fJ1 , beam size) 2: count = 0 3: i = 1 4: hyps = {e0} 5: new hyps = {} 6: while count < beam size and i < 3 · J do 7: for hyp in hyps do 8: sum dist = [0] ∗ |V |src 9: for j from 1 to J do\n10: sum = 0 11: for j′ from 1 to J do 12: sum = sum + SCORES(hyp, j′) ·palign(fj′ , j − j′) 13: end for 14: cache[j] = sum · lex dist(fj) 15: #Element wise addition 16: sum dist = sum dist⊕ cache[j] 17: end for 18: dist = SORT(sum dist, beam size) 19: for word in dist[:beam size] do 20: new hyp = EXTEND(hyp, word) 21: SETSCORES(new hyp, cache) 22: new hyps.INSERT(new hyp) 23: end for 24: end for 25: PRUNE(new hyps, beam size) 26: for <EOF> in new hyps do 27: count = count + 1 28: end for 29: hyps = new hyps 30: i = i+ 1 31: end while 32: return GETBEST(hyps) 33: end function\ncompared to O(J · I) for attention-based models. These are theoretical complexities of decoding on a CPU only considering source and target sentence lengths. In practice, the size of the neural network must also be taken into account, and there are some optimized matrix multiplications for decoding on a GPU. In general, the decoding speed of our model is about 3 times slower than that of a standard attention model (1.07 sentences per second vs. 3.00 sentences per second) on a single GPU. This is still an initial decoder and we did not spend much time on accelerating its decoding yet. The optimization of our decoder would be a promising future work."
  }, {
    "heading": "5 Experiments",
    "text": "The experiments are conducted on the WMT 2017 German↔English and Chinese→English translation tasks, which consist of 5M and 23M parallel sentence pairs respectively. Translation quality is measured with the case sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metric on newstests 2017, which contain 3004 (German↔English) and 2001 (Chinese→English) sentence pairs.\nFor German and English preprocessing, we use the Moses tokenizer with hyphen splitting, and perform truecasing with Moses scripts (Koehn et al., 2007). For German↔English subword segmentation (Sennrich et al., 2016), we use 20K joint BPE operations. For the Chinese data, we segment it using the Jieba1 segmenter. We then learn a BPE model on the segmented Chinese, also using 20K merge operations. During training, sentences with a length greater than 50 subwords are filtered out."
  }, {
    "heading": "5.1 Attention-Based System",
    "text": "The attention-based systems are trained with Sockeye (Hieber et al., 2017), which implement an attentional encoder-decoder with small modifications to the model in Bahdanau et al. (2015). The encoder and decoder word embeddings are of size 620. The encoder consists of a bidirectional layer with 1000 LSTMs with peephole connections to encode the source side. We use Adam (Kingma and Ba, 2015) as optimizer with a learning rate of 0.001, and a batch size of 50. The network is trained with 30% dropout for up to 500K iterations and evaluated every 10K iterations on the development set with BLEU. Decoding is done using beam search with a beam size of 12. In the end the four best models are averaged as described in\n1https://github.com/fxsjy/jieba\nthe beginning of Junczys-Dowmunt et al. (2016)."
  }, {
    "heading": "5.2 Neural Hidden Markov Model",
    "text": "The entire neural hidden Markov model is implemented in TensorFlow (Abadi et al., 2016). The feedforward models have three hidden layers of sizes 1000, 1000 and 500 respectively, with a 5- word source window and a 3-gram target history. 200 nodes are used for word embeddings.\nThe output layer of the neural lexicon model consists of around 25K nodes for all subword units, while the neural alignment model has a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from −100 to 100.\nApart from the basic projection layer, we also applied LSTM layers for the source and target words embedding. The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, Figure 1). We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance."
  }, {
    "heading": "5.3 Results",
    "text": "We compare the neural HMM approach (Subsection 5.2) with the state-of-the-art attention-based approach (Subsection 5.1) on different translation tasks. The results are presented in Table 1. Compare to the model presented in Wang et al. (2017), switching to LSTM models has a clear advantage, which improves the FFNN-based system by up to 1.3% BLEU and 1.8% TER. It seems that the HMM model benefits from richer features, such as LSTM states, which are very similar to what an attention mechanism would require. We actually\nexpected it to do with less, the reason being that alignment distributions get refined a posteriori and so they do not have to be as strong a priori. We can also observe that the performance of our approach is comparable with the state-of-the-art attentionbased system with 25M more parameters on all three tasks."
  }, {
    "heading": "5.4 Alignment Analysis",
    "text": "We show an example from the German→English newstest 2017 in Figure 2, along with the attention and alignment matrices. We can observe that the neural network-based HMM could generate a more clear alignment path compared to the attention weights. In this example, it can exactly estimate the alignment positions for words wanted and of."
  }, {
    "heading": "6 Discussion",
    "text": "We described a novel formulation for a neural network-based machine translation system, which applied neural networks to the conventional hidden Markov model. The training is end-to-end, the model is monolithic and can be used as a standalone decoder. This results in a more modern and efficient way to use HMM in machine translation and enables neural networks to benefit from HMMs.\nExperiments show that replacing attention with alignment does not improve the translation performance of NMT significantly. One possible reason is that alignment may fail to capture relevant contexts as attention does. While alignment aims to identify translation equivalents between two lan-\nguages, attention is designed to find relevant context for predicting the next target word. Source words with high attention weights are not necessarily translation equivalents of the target word. Although using alignment does not lead to significant improvements in terms of BLEU over attention, we think alignment-based NMT models are still useful for automatic post editing and developing coverage-based models. These might be interesting future directions to explore."
  }, {
    "heading": "Acknowledgments",
    "text": "This project has received funding from the European Union’s Horizon 2020 research and innovation programme under\ngrant agreement no 645452 (QT21), and from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme, grant agreement no 694537 (SEQCLAS). The work reflects only the authors’ views and neither the European Research Council Executive Agency nor the European Commission is responsible for any use that may be made of the information it contains. The GPU cluster used for the experiments was partially funded by Deutsche Forschungsgemeinschaft (DFG) Grant INST 222/1168-1. Tamer Alkhouli was partly funded by the 2016 Google PhD Fellowship for North America, Europe and the Middle East."
  }],
  "year": 2018,
  "references": [{
    "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "authors": ["Martı́n Abadi", "Ashish Agarwal", "Paul Barham"],
    "venue": "CoRR abs/1603.04467",
    "year": 2016
  }, {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations. San Diego, CA, USA.",
    "year": 2015
  }, {
    "title": "Sockeye: A Toolkit for Neural Machine Translation",
    "authors": ["Felix Hieber", "Tobias Domhan", "Michael Denkowski", "David Vilar", "Artem Sokolov", "Ann Clifton", "Matt Post."],
    "venue": "ArXiv e-prints https://arxiv.org/abs/1712.05690.",
    "year": 2017
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attentionbased NMT Models as Feature Functions in Phrasebased SMT",
    "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich."],
    "venue": "Proceedings of the First Conference",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Lei Ba."],
    "venue": "Proceedings of the Third International Conference on Learning Representations. San Diego, CA, USA.",
    "year": 2015
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, PA, USA,",
    "year": 2002
  }, {
    "title": "How to Construct Deep Recurrent Neural Networks",
    "authors": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the Second International Conference on Learning Representations. Banff, Canada.",
    "year": 2014
  }, {
    "title": "Neural Machine Translation of Rare Words with Subword Units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 1715–1725.",
    "year": 2016
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of the Conference of the Association for Machine Translation in the Americas.",
    "year": 2006
  }, {
    "title": "Sequence to Sequence Learning with Neural Networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Proceedings of the Advances in Neural Information Processing Systems 27. Montréal, Canada, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation",
    "authors": ["Weiyue Wang", "Tamer Alkhouli", "Derui Zhu", "Hermann Ney."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Compu-",
    "year": 2017
  }],
  "id": "SP:28bd7b83eb1c06c3d6b48b29d65921a0e7907638",
  "authors": [{
    "name": "Weiyue Wang",
    "affiliations": []
  }, {
    "name": "Derui Zhu",
    "affiliations": []
  }, {
    "name": "Tamer Alkhouli",
    "affiliations": []
  }, {
    "name": "Zixuan Gan",
    "affiliations": []
  }, {
    "name": "Hermann Ney",
    "affiliations": []
  }],
  "abstractText": "This work aims to investigate alternative neural machine translation (NMT) approaches and thus proposes a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models. The neural models make use of encoder and decoder components, but drop the attention component. The training is end-to-end and the standalone decoder is able to provide comparable performance with the state-of-the-art attention-based models on three different translation tasks.",
  "title": "Neural Hidden Markov Model for Machine Translation"
}