{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 210–220 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Multi-document summarization provides users with summary that reflects the main information in a set of given documents. The documents are often related and talk about more than one topics. Generic multi-document summarization and topic-focused multi-document summarization are two typical kinds of summarization. The former is a summarization delivering the main information of the documents with no bias while the latter is a one delivering the main information biased to a given topic description (a few sentences or even phrases). Most existing summarization systems are designed for these two kinds of summarization.\nThere are two goals for generic multi-document summarization. The first one is saliency. Summary sentences should be central sentences that capture the majority of information in a docu-\nment cluster. The sentences with little information about the document cluster should not be included in the summary. The second one is diversity. The information overlap between summary sentences should be as minimal as possible due to the length limit of summary. In other words, the information coverage of summary is a determinant, which requires that the summary sentences should cover diverse aspects of information. Besides the two goals, there is another goal for the topic-focused summarization and that is relevancy. It requires that the summary sentences be relevant to the topic description. A series of conferences and workshops on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, EMNLP and SIGIR have advanced the techniques to achieve these goals and many approaches have been proposed so far.\nIn this paper, we focus on the extractive summarization methods, which extract the summary sentences from the input document cluster. We propose affinity-preserving random walk for multidocument summarization. The method is a graphbased ranking method, which takes into account the global information collectively computed from the entire sentence affinity graph. Different from the previous graph-based ranking methods, our method adopts “global normalization” to transform sentence affinity matrix into sentence transition matrix and formulates the sentence ranking process in an absorbing random walk model. Meanwhile, the adjustable affinity-preserving random walk is proposed to facilitate the diversity of summary by adjusting the sentence transition matrix after each iteration of random walk. Experimental results on DUC generic and topic-focused multi-document summarization tasks show the competitive performance of our method. To our best knowledge, our system has the best ROUGE2 recall on both tasks among all existing graph-\n210\nbased ranking methods, which defeats most other methods.\nWe summarize our contributions as follows. (1) We preserve the original affinity relations between sentences in a novel affinity-preserving random walk view for multi-document summarization. The preservation of affinity leads to a more salient summary. And it is applicable to both generic and topic-focused summarization. (2) We propose adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. (3) Experiments on DUC 2003 and DUC 2004 tasks demonstrate the competitive performance of our method.\nThe rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 describes traditional random walk model for summarization. Section 4 proposes affinity-preserving random walk for the saliency goal of summarization and this section also proposes adjustable affinity-preserving random walk to produce both salient and diverse summary. Section 5 gives our evaluation results and the conclusion is made in Section 6."
  }, {
    "heading": "2 Related Work",
    "text": "Our method belongs to the graph-based ranking methods to select sentences in the documents to produce the summary. Erkan and Radev (2004) proposed LexPageRank to compute the sentence saliency based on the concept of eigenvector centrality. It constructs the sentence affinity graph and computes the sentence saliency based on an algorithm similar to PageRank (Page et al., 1999). Like PageRank, the affinity matrix is converted into the row-stochastic matrix, which is used as the transition matrix of random walk on the weighted graph. Wan (2007) proposed manifold ranking for topic-focused multi-document summarization. It makes full use of both the relationships among all sentences in the documents and the relationships between the given topic description and the sentences. Manifold ranking conducts a different normalization on the sentence affinity matrix to guarantee the algorithm’s convergence. GRASSHOPPER (Zhu et al., 2007) ranks sentences with an emphasis on the diversity constraint of summarization. It turns already ranked sentences into absorbing states, which effectively prevents redundant sentences from receiving a high rank. The algorithm is based on an absorbing random\nwalk and produces only one summary sentence after one particular random walk becomes stationary. And the normalization from sentence affinity matrix to sentence transition matrix is the same as PageRank. DivRank (Mei et al., 2010) is a method to balance the saliency and diversity of the top ranked sentences in a reinforced random walk model. Also, the normalization in DivRank from affinity matrix to transition matrix is the same as PageRank. Another notable diversified graphbased ranking method GCD (Dubey et al., 2011) relies on large amounts of training data to learn edge conductances.\nOur method formulates the multi-document summarization as an affinity-preserving random walk and uses the “global normalization” to transform sentence affinity matrix into sentence transition matrix, which is different from all those proposed methods. And the adjustable transition matrix in our method balances the saliency and diversity goals of summarization. Like GRASSHOPPER, our method relies on the absorbing random walk model. The difference is that our method does not turn the sentence vertex into absorbing state but add an absorbing vertex to the original sentence affinity graph. And all summary sentences are extracted after the random walk reaches a stationary state in our method. Like DivRank, the sentence transition matrix is adjustable in our method to enforce the diversity constraint of summarization. However, our method differs from DivRank in the mechanism to adjust the transition matrix."
  }, {
    "heading": "3 Traditional Random Walk for Summarization",
    "text": "Suppose G = (S,E) is a graph with vertex set S and edge set E ⊂ S2. Suppose there is a conductance c(si, sj) > 0 associated with each edge (si, sj) ∈ E and c(si, sj) = 0 associated with the set S2 − E (the conductance of nonexistent edge is zero). Let\nC(si) = ∑ sj∈S c(si, sj), si ∈ S (1)\nso that C(si) is the total conductance of the edges coming from si. And the traditional random walk on graph is defined as\nDefinition 3.1. The discrete-time Markov chain X = (X0, X1, X2, ...) with state space and tran-\nsition probability matrix P given by\nP (si, sj) = c(si, sj) C(si) , (si, sj) ∈ S2 (2)\nis called a random walk on the graph G.\nThis chain governs a particle moving along the vertices of G. If the particle in the state Xm is at vertex si ∈ S, it will be at a neighbor of si in the next state Xm+1, which is chosen randomly in proportion to the conductance. We can prove that∑\nsj∈S P (si, sj) = 1 for any si ∈ S (C(si) 6= 0) so P is a row-stochastic matrix by P = D−1W, where D is a diagonal matrix with entries Dii = C(si) and W is the adjacency matrix of G where Wij = c(si, sj).\nFor the summarization task, G is the sentence affinity graph. The vertex set S = {s1, s2, ..., sn} contains every sentence in the document cluster and the edge set E contains the pairwise affinity between sentences. We use the tf*isf formula to calculate the weight associated with each term occurring in the sentence, where tf is the term frequency in the sentence and isf is the inverse sentence frequency of the term among all sentences. isf is often calculated as 1+log(n/nt), where n is the total number of sentences and nt is the number of sentences containing the term t. Wij is computed using the standard cosine measure (BaezaYates et al., 1999).\nWij = simcosine(vi, vj) = vi · vj\n‖vi‖2 × ‖vj‖2 (3)\nwhere vi and vj are the corresponding term vectors of si and sj . Two vertices are connected if their affinity is larger than 0 and Wii is set as 0 to avoid self transition. We get an undirected graph G as well as a symmetric sentence affinity matrix W in this way. Then we transform W into P by P = D−1W and use the stationary distribution of random walk as sentence ranking scores. The traditional random walk model is a simple practice of PageRank algorithm for multi-document summarization."
  }, {
    "heading": "4 Affinity-Preserving Random Walk for Summarization",
    "text": ""
  }, {
    "heading": "4.1 Prior of Multi-Document Summarization",
    "text": "In the above traditional random walk on graph, the normalization from affinity matrix W to transition matrix P is to make P a row-stochastic ma-\ntrix. This can be interpreted as a democratic normalization because the surfer of a traditional random walk visits neighbors of a vertex with probability 1. The surfer has to choose a neighbor to visit next although it is a random choice w.r.t. the conductance distribution of the vertex. However this democratic normalization is not suitable for multi-document summarization due to the fact that most sentences are not salient and should not be normalized democratically as the few salient ones. The prior here is that the number ratio of good candidate sentences over bad candidate sentences is very low due to the summary length limit. Good candidate sentences are the sentences highly overlapping with sentences in the reference summary written by humans. And the remaining sentences are bad candidate sentences. The democratic normalization of P = D−1W will extend the adverse effect of bad candidate sentences and suppress the effect of good candidate sentences, because the total conductance of the bad candidate sentence is usually smaller than that of the good candidate sentence. In this case, the random surfer has to choose a neighbor to visit even when she is currently at a bad candidate sentence, which will direct her to visit other bad candidate sentences neighboring to the current sentence. The invariant behavior of the surfer at all vertices in the graph is not consistent with the prior which makes a distinction between good and bad candidate sentences. It may pervert the random walk process to get an ideal distribution in which only few sentences are assigned with a high ranking score.\nIt is worth noting here that manifold ranking (Wan et al., 2007) for summarization uses a different normalization: P = D− 1 2 WD− 1 2 . It is a symmetric normalization on both endpoints of an edge, which makes P a suitable choice in the manifold ranking process to smooth the scores of neighboring vertices. The symmetric normalization can be deduced from the objective function of manifold ranking (Zhou et al., 2003) and does not make a distinction between the good and bad candidate sentences. It is also not consistent with the prior. We can conclude that existing graphbased ranking methods can not well characterize the prior of multi-document summarization."
  }, {
    "heading": "4.2 Affinity-Preserving Random Walk",
    "text": "We need a new normalization method that distinguishes between good and bad candidate sen-\ntences to satisfy the prior of multi-document summarization. Affinity-preserving random walk has an intrinsic mechanism that preserves the original affinity relations between sentences in the documents, which is proposed and defined as follows.\nDefinition 4.1. For the graph G, the vertex set S has (n + 1) vertices: s0, s1, s2, ..., sn. The maximum conductance Cmax = maxi=1,2,...nC(si). Of the (n + 1) vertices, s0 is an absorbing vertex with c(si, s0) > 0, c(s0, si) = 0, and c(s0, s0) = Cmax for i = 1, 2, ..., n. The remaining vertices are the normal vertices with c(si, sj) > 0 for i, j = 1, 2, ..., n. The discrete-time Markov chain X = (X0, X1, X2, ...) with state space and transition probability matrix P given by\nP (si, sj) = c(si, sj) Cmax P (si, s0) = 1− C(si) Cmax , P (s0, si) = 0 P (s0, s0) = 1 for i, j = 1, 2, ..., n\n(4)\nis called an affinity-preserving random walk on the graph G.\nFor our summarization task, we construct a sentence augmented graph GA (as shown in Figure 4.1) by adding an absorbing vertex s0 to the sentence affinity graph G. The unabsorbed vertices si (i = 1, 2, ..., n) represent sentences in the documents. The affinity-preserving random walk process as defined above is implemented on GA to rank sentences in the documents. In the affinity-preserving random walk, once the surfer\nreaches the absorbing vertex, she cannot walk out of there. Because P (si, s0) is small for the vertex si with a large conductance, it is less likely for the surfer at si to walk into s0. As for the vertex with a small conductance, the surfer has a tendency to be absorbed by s0. The absorbing vertex here plays a role of soaking unreliable ranking scores from large numbers of bad candidate sentences and highlighting the few good candidate sentences. The affinity matrix W is normalized by its first norm (equivalent to Cmax) in the affinitypreserving random walk, which results in a kind of “soft” stochastic matrix for n unabsorbed vertices. “soft” here means that the sum of row elements in the matrix can be less than 1. By contrast, P in the traditional random walk is a “hard” stochastic matrix in which every sum of row elements has to be 1. Meanwhile, P in this absorbing Markov chain (Seneta, 2006) preserves the original affinity relations in W as all sentences are globally normalized by the same factor (i.e. Cmax). We call this approach an “affinity-preserving random walk” as it is used in (Cho et al., 2010), which deals with a graph matching problem that aims at assigning 1-vs-1 correspondences between two graphs. The similar idea is also applied in the case of ontology matching (Xiang et al., 2015). Transition matrix P including the absorbing vertex is formulated in (Cho et al., 2010) as follows\nP = (\n1 0 T\ne− c/‖W‖1 W/‖W‖1\n) (5)\nwhere 0 T is a 1 × n vector with all elements 0, e is an n × 1 vector with all elements 1, c = [C(s1), C(s2), ..., C(sn)]\nT is a vector containing the conductances of n sentences and W/‖W‖1 is the n × n soft substochastic matrix. However, the stationary distribution of such a random walk on graph with one absorbing vertex is always( 1 0T ) , which is not a good characterization of the sentence ranking distribution. We turn to the quasi-stationary distribution x̄ (Cho et al., 2010; Darroch and Seneta, 1965) of absorbing random walk for ranking sentences. x̄ (K) is defined as\nx̄ (K)i = Prob(X (K) = si|X(K) 6= s0)\n= x (K)i∑ j x (K) j\n(6)\nwhere X(K) denotes the position of random surfer at time K. It can be proven that x̄ (K) has its\nstationary distribution x̄ if W is irreducible (Darroch and Seneta, 1965). We remove the sentences that have the total conductance 0 (i.e. the isolated sentences) when constructing the sentence affinity graphG. In this way,Gwill be strongly connected and has an irreducible adjacency matrix W.\nWe introduce the teleport vector y as used in personalized PageRank (Page et al., 1999; Haveliwala, 2002; Jeh and Widom, 2003) for the summarization task. In the generic summarization case, we define the vector y in a way that reflects the position of each sentence in a document. If the sentence si+1 is right after the sentence si in the same document, then\nyi+1 yi = λ−1, λ > 1 (7)\nwhere λ is the decay factor. In the topic-focused summarization case, we incorporate the topic description as a vertex in the random walk process, which is a standard way of achieving the relevancy goal of this kind of summarization. Vector y is defined to be [y1, y2, ..., yn, yn+1]T in which yi = 0(1 6 i 6 n) and yn+1 = 1 when the first n elements represent sentences in the documents and the last one represents the topic description. We normalize y by its first norm to get a prior sentence ranking for multi-document summarization. Based on W and y, sentence ranking scores in affinity-preserving random walk can be formulated in a recursive form as follows\nx = µWT /‖W‖1x + (1− µ)y ‖µWT /‖W‖1x + (1− µ)y‖1\n(8)\nwhere x = [Score(si)]n×1 is the vector of sentence ranking scores. µ is the damping factor that trades off between two actions: the transition according to WT /‖W‖1 and the teleport specified by y. Transpose operation in Eq.(8) can be removed because of symmetry of W. The final transition matrix of affinity-preserving random walk is given by A = µW/‖W‖1 + (1− µ)y · e T and x should be normalized by its first norm after each iteration of random walk. Like PageRank, the quasistationary distribution is obtained by the normalized principal eigenvector of A.\nFor implementation, the initial ranking scores of all sentences are set to 1/n and the iterative process in Eq.(8) is adopted to compute new ranking scores of sentences. Usually convergence of the iterative algorithm is achieved when the difference between scores computed at two successive iterations falls below a given threshold."
  }, {
    "heading": "4.3 Adjustable Affinity-Preserving Random Walk for Summarization",
    "text": "Affinity-preserving random walk preserves the affinity relations between sentences and gives high ranking scores to the salient sentences. However, the diversity constraint of summarization has not been taken into account. The surfer of affinitypreserving random walk has no knowledge about what a diverse summary should be. If we just take redundancy removing as the post-processing separate step to improve diversity, sentences that highly overlap with other summary sentences may be chosen and sentences that include information about different topics may be submerged. This phenomenon can be explained by the theorem as follows.\nTheorem 4.1. Suppose x̄ is the quasi-stationary distribution of affinity-preserving random walk as defined in Sec.4.2 and x is the solution of a continuous quadratic optimization problem argmax(x T A x) s.t. x ∈ [0, 1]n, ‖x‖2 = 1 and A has definition in Sec.4.2. The following equation holds x̄ = x/‖x‖1 (9) when µ = 1.\nProof. In mathematics, for a given symmetric real matrix A (when µ = 1) and nonzero real vector x, the Rayleigh quotient R(A, x) is defined as\nR(A, x) = x T Ax x T x\nand it reaches its maximum value when x is the principal eigenvector of A. If ‖x‖2 = 1, R(A, x) is equivalent to x T A x. So the solution x is the principal eigenvector of A. From Section 4.2, x̄ is the normalized principal eigenvector of A. x̄ and x have the relation in Eq.(9). The conclusion is made.\nFrom Theorem 4.1, affinity-preserving random walk tends to produce a stationary distribution in which the total sum of affinity between sentences (i.e. x T Ax) is large if there is a subtle teleporting effect. It will lead to a summary consisting of many sentences overlapping with each other, which clearly violates the diversity constraint of summarization. Good candidate sentences may not have high affinity with other sentences and are likely to be submerged by affinity-preserving random walk. Conversely, some bad candidate sentences could have high affinity with others and will\nbe highlighted by the random walk process. An extreme example is that a cluster of sentences will all get high ranking scores if they are very similar to each other. However, only one sentence in this cluster should be included in the summary and the others should be suppressed. The random surfer is caught in a trap of larger sentence cluster, which operates against the exploration of good candidates in smaller cluster.\nWe introduce adjustable affinity-preserving random walk to enforce the diversity constraint of summarization. In the original affinity-preserving random walk, sentence transition matrix A is fixed and set as µW/‖W‖1+(1−µ)y·e T . Edge (si, s0) (if C(si) 6= Cmax) always exists and has an invariant conductance c(si, s0), which means that the surfer at si walks into s0 in the same manner for the entire random walk process. The random surfer makes her decision only based on invariant A to select salient sentences and form the summary. To equip the surfer with knowledge about what a diverse summary should be, we propose to adjust sentence transition matrix A in each iteration of random walk. The key point is that good candidate sentences should be normalized locally while bad ones should be normalized globally in the transformation from affinity matrix W to transition matrix A.\nA “virtual” summary V is produced based on x in each iteration of affinity-preserving random walk. “Virtual” here means that V is a summary based on transient distribution x, which differs from the final summary based on quasi-stationary distribution x̄. The method of diversity penalty imposition (Wan et al., 2007) is used to produce V , which is denoted by the producingSummary function in Algorithm 4.1. It is a simple greedy algorithm to select sentences that are both salient and diverse, which often plays a role of greedy post-processing step to produce the final summary. Rather, we use it to produce virtual summary V that satisfies both the saliency and diversity constraints based on a specific iteration. V is an indicator for the diversity constraint of summarization.\nThe sentence transition matrix A in the iteration (K+1) is then constructed with help of the virtual summary V in the iterationK. Here, different normalization methods are used to transform W into A. If V includes the sentence si, elements in the corresponding i-th row of W will be normalized by the sum of the row (i.e. C(si)). Otherwise, el-\nements will be normalized by the maximum sum of row elements in W (i.e. Cmax). In this way, “local normalization” is adopted for the sentences in V while “global normalization” is adopted for the sentences not in V . We differentiate the normalization methods to lead the surfer of affinitypreserving random walk to explore more in the neighborhood of the sentences in V rather than end in the absorbing vertex s0. As a result, the sentences that satisfy the saliency and diversity constraints will be highlighted even though they are in a small sentence cluster. We characterize differ-\nAlgorithm 4.1: Adjustable AffinityPreserving Random Walk for Multi-Document Summarization Input: The sentence affinity matrix, W; The\nstarting and maximum number of iteration, B and M ; The teleport vector, y; The damping factor, µ;\nOutput: The multi-document summary, V ; 1 A← µW/Cmax + (1− µ)y · eT 2 Initialize the starting distribution x as uniform 3 for i← 1, 2, ...,M do 4 if i > B then 5 V ← producingSummary(x) 6 D← adjustingNormalization(V ) 7 A← µ(D−1W)T + (1− µ)y · eT 8 x̄← Ax 9 x̄← x̄/‖x̄‖1\n10 if ‖x̄− x‖1 < ε then 11 break\n12 x← x̄ 13 V ← producingSummary(x) 14 Return V\nent normalizations in the diagonal matrix D. Dii is C(si) if si ∈ V and Dii is Cmax if si /∈ V ,which is denoted by the adjustingNormalization function in Algorithm 4.1. D in the current iteration is here dependent on x in the previous iteration. We will have different sentence augmented graphs GA in each iteration. Figure 4.2 shows an example of GA(K) and GA(K + 1) in the respective iterations K and (K + 1). The probability distribution in the adjustable affinity-preserving random walk is updated as follows.\nx = µ(D−1W)T x + (1− µ)y ‖µ(D−1W)T x + (1− µ)y‖1\n(10)\nThis is an adjustable Markov chain for which the transition matrix A is µ(D−1W)T + (1−µ)y · e T . In this setting, A is dependent on the transient distribution x in the previous iteration, which differs from the invariant transition matrix in Eq.(8). As the diversity constraint is embedded in A, subsequent random walks move to the solution that induces a better summary. The algorithm of the adjustable affinity-preserving random walk for multi-document summarization is demonstrated in Algorithm 4.1.\nThe parameter B in Algorithm 4.1 is used to produce a transient distribution which is reliable enough to adjust the transition matrix. To get the final multi-document summary, we use the same producingSummary function."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Data Sets",
    "text": "Generic and topic-focused multi-document summarization have been the main tasks in DUC1. Task 2 of DUC 2004 is a generic summarization task and task 3 of DUC 2003 is a topic-focused summarization task. Both tasks are used for performance evaluation of our method. In the experiments, task 2 of DUC 2003 is used for the pa-\n1http://www-nlpir.nist.gov/projects/duc/intro.html\nrameter tuning of our method. We preprocess the document data sets by removing stopwords from each sentence and stemming the remaining words using the Porter’s stemmer2. Also, the sentences containing the said clause (if a said, says, told, tells word and quotation marks appear simultaneously) are filtered out.\nFor evaluation, four reference summaries generated by human judges for each document cluster are provided by DUC as the ground truth. A brief summary over the evaluation datasets is shown in Table 5.1. According to (Hong et al., 2014), we adjust the length limit of summary in DUC 2004 from 665 bytes to 100 words as it provides the same setting for system evaluations."
  }, {
    "heading": "5.2 Evaluation Metric",
    "text": "We use the ROUGE-1.5.5 (Lin and Hovy, 2003) toolkit for evaluation, which has been officially adopted by DUC for automatic summarization evaluation. The toolkit measures summary quality by counting overlapping units such as the ngram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE-N is an n-gram based measure and the ROUGE-N recall is computed as follows\nROUGE-NR =\n∑ S∈{RefSum} ∑ n-gram∈S\nCountmatch(n-gram)∑ S∈{RefSum} ∑ n-gram∈S Count(n-gram) (11)\nwhere n stands for the length of the n-gram, and Countmatch(n-gram) is the maximum number of ngrams co-occurring in the candidate summary and the set of reference summaries. Count(n-gram) is the number of n-grams in the reference summaries.\nWe conduct our ROUGE experiments following the recommended standard in (Owczarzak et al.,\n2https://tartarus.org/martin/PorterStemmer/\n2012; Hong et al., 2014)3. We compute ROUGE-2 recall with stemming and stopwords not removed, which provides the best agreement with manual evaluations. We also compute ROUGE-1 recall which has the highest recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to identify the better summary in a pair (Owczarzak et al., 2012)."
  }, {
    "heading": "5.3 Experimental Results",
    "text": "In the experiments, the parameters of our method are set as follows: the decay factor λ is 2, the maximum number of iterationM is 100, the number of starting iteration B is 30, the damping factor µ is 0.85 and the minimum error ε is 1E-30.\nTable 5.2 shows the performance of our method and other eleven well-known systems on task 2 of DUC 2004 according to ROUGE-1,2,4 recall, sorted by ROUGE-2 recall in the ascending order. Some of the results are from (Hong et al., 2014). Cont. LexPageRank (Erkan and Radev, 2004) is a graph-based ranking method and a representative of traditional random walk approach. Here we employ the continuous version of LexPageRank. FreqSum (Nenkova et al., 2006) is a simple approach to approximate the importance of words with their probability in the input and then select sentences with high average word probability. CLASSY 04\n3ROUGE-1.5.5 with the parameters: -n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0\n(Conroy et al., 2004) was the participant of the official DUC 2004 evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. ICSISumm (Gillick and Favre, 2009) aims at finding the globally optimal summary by formulating the summarization task in Integer Linear Programming. WFS-NMF (Wang et al., 2010) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking models mentioned in Section 2. APRW and AAPRW are our methods. APRW is the method of affinitypreserving random walk described in Section 4.2 and AAPRW is the method of adjustable affinitypreserving random walk described in Section 4.3.\nTable 5.3 shows the evaluation results on task 3 of DUC 2003 according to ROUGE-1,2,4 recall, sorted also by ROUGE-2 recall in the ascending order. S13, S16 and S17 are the system IDs of the top performing systems in the official DUC 2003 evaluation, whose details are described in DUC publications (Zhou and Hovy, 2003; Chali et al., 2003). Manifold Ranking is the method proposed in (Wan et al., 2007) to make use of both the relationships among all sentences in the documents and the relationships between the given topic de-\nscription and the sentences. APRW and AAPRW are our methods.\nFrom Tables 5.2 and 5.3, our method has the best ROUGE-2 score among all graph-based ranking methods for generic multi-document summarization, and it also has the best ROUGE-2 score for topic-focused multi-document summarization. AAPRW has the ROUGE-2 score 10.06% on DUC 2004 task 2, which is 0.28% higher than the best system ICSISumm reported by (Hong et al., 2014) and 1.1% higher than the official best system CLASSY 04. WFS-NMF has the overall best score on DUC 2004 task 2 due to the sentence feature selection and the weights on the document side, which is reported by (Wang et al., 2010; Alguliev et al., 2013). AAPRW has the ROUGE-2 score 8.21% on DUC 2003 task 3, which is 0.53% higher than Manifold Ranking and 0.9% higher than the official best system S16. In DUC 2004 AAPRW has 0.67% more ROUGE-2 score than APRW and the gap is 0.49% in DUC 2003, which proves the effectiveness of the adjustable transition matrix in the random walk process. It is worth mentioning that our method has the best ROUGE4 score on the DUC 2003 topic-focused summarization task.\nWe conducted the two-sided Wilcoxon signedrank tests between each pair of AAPRW and other methods. For the generic summarization in DUC 2004, our method provides a significant improvement over the official best system CLASSY 04 on ROUGE-2 (with p-value lower than 0.05). For the query-focused summarization in DUC 2003, our method also provides a significant improvement over S17, S13 and S16 on ROUGE-2.\nIn order to further investigate the influences of the parameter in our proposed method, the damping factor µ is varied from 0 to 1. Figures 5.1 and 5.2 show the ROUGE-1 and ROUGE-2 recall curves of our method on the two data sets, respectively. We can see from the figures that the damping factor has an effect on the performance of multi-document summarization."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "In this paper we propose the adjustable affinitypreserving random walk for generic and topicfocused multi-document summarization, which deals with the saliency and diversity goals in a unified framework. Experiments demonstrate the effectiveness of our method.\nIn the future work, we will focus on the self transition of adjustable affinity preserving random walk, which could be used to remove the redundancy between summary sentences."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank our three anonymous reviewers for their helpful advice on various aspects of this work. This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact authors for this paper are Zhifang Sui and Baobao Chang."
  }],
  "year": 2017,
  "references": [{
    "title": "Multiple documents summarization",
    "authors": ["Rasim M Alguliev", "Ramiz M Aliguliyev", "Nijat R Isazade"],
    "year": 2013
  }, {
    "title": "Modern information retrieval, volume 463",
    "authors": ["Ricardo Baeza-Yates", "Berthier Ribeiro-Neto"],
    "year": 1999
  }, {
    "title": "The university of lethbridge text summarizer at duc 2003",
    "authors": ["Yllias Chali", "Maheedhar Kolla", "Nanak Singh", "Zhenshuan Zhang."],
    "venue": "the Proceedings of the HLT/NAACL workshop on Automatic Summarization/Document Understanding Confer-",
    "year": 2003
  }, {
    "title": "Reweighted random walks for graph matching",
    "authors": ["Minsu Cho", "Jungmin Lee", "Kyoung Lee."],
    "venue": "Computer Vision–ECCV 2010, pages 492–505.",
    "year": 2010
  }, {
    "title": "Left-brain/rightbrain multi-document summarization",
    "authors": ["John M Conroy", "Judith D Schlesinger", "Jade Goldstein", "Dianne P Oleary."],
    "venue": "Proceedings of the Document Understanding Conference (DUC 2004).",
    "year": 2004
  }, {
    "title": "Classy 2011 at tac: Guided and multi-lingual summaries and evaluation",
    "authors": ["John M Conroy", "Judith D Schlesinger", "Jeff Kubina", "Peter A Rankel", "Dianne P O’Leary"],
    "year": 2011
  }, {
    "title": "On quasistationary distributions in absorbing discrete-time finite markov chains",
    "authors": ["John N Darroch", "Eugene Seneta."],
    "venue": "Journal of Applied Probability, 2(1):88–100.",
    "year": 1965
  }, {
    "title": "Diversity in ranking via resistive graph centers",
    "authors": ["Avinava Dubey", "Soumen Chakrabarti", "Chiranjib Bhattacharyya."],
    "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 78–86. ACM.",
    "year": 2011
  }, {
    "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
    "authors": ["Günes Erkan", "Dragomir R Radev."],
    "venue": "Journal of Artificial Intelligence Research, 22:457–479.",
    "year": 2004
  }, {
    "title": "A scalable global model for summarization",
    "authors": ["Dan Gillick", "Benoit Favre."],
    "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 10–18. Association for Computational Linguistics.",
    "year": 2009
  }, {
    "title": "Topic-sensitive pagerank",
    "authors": ["Taher H Haveliwala."],
    "venue": "Proceedings of the 11th international conference on World Wide Web, pages 517–526. ACM.",
    "year": 2002
  }, {
    "title": "A repository of state of the art and competitive baseline summaries for generic news summarization",
    "authors": ["Kai Hong", "John M Conroy", "Benoit Favre", "Alex Kulesza", "Hui Lin", "Ani Nenkova."],
    "venue": "LREC, pages 1608–1616.",
    "year": 2014
  }, {
    "title": "Scaling personalized web search",
    "authors": ["Glen Jeh", "Jennifer Widom."],
    "venue": "Proceedings of the 12th international conference on World Wide Web, pages 271–279. ACM.",
    "year": 2003
  }, {
    "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
    "authors": ["Chin-Yew Lin", "Eduard Hovy."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-",
    "year": 2003
  }, {
    "title": "A class of submodular functions for document summarization",
    "authors": ["Hui Lin", "Jeff Bilmes."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 510–520. As-",
    "year": 2011
  }, {
    "title": "Divrank: the interplay of prestige and diversity in information networks",
    "authors": ["Qiaozhu Mei", "Jian Guo", "Dragomir Radev."],
    "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009–1018.",
    "year": 2010
  }, {
    "title": "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization",
    "authors": ["Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown."],
    "venue": "Proceedings of the 29th annual international ACM SIGIR confer-",
    "year": 2006
  }, {
    "title": "An assessment of the accuracy of automatic evaluation in summarization",
    "authors": ["Karolina Owczarzak", "John M Conroy", "Hoa Trang Dang", "Ani Nenkova."],
    "venue": "Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Sum-",
    "year": 2012
  }, {
    "title": "The pagerank citation ranking: Bringing order to the web",
    "authors": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd."],
    "venue": "Technical report, Stanford InfoLab.",
    "year": 1999
  }, {
    "title": "Non-negative matrices and Markov chains",
    "authors": ["Eugene Seneta."],
    "venue": "Springer Science & Business Media.",
    "year": 2006
  }, {
    "title": "Manifold-ranking based topic-focused multidocument summarization",
    "authors": ["Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao."],
    "venue": "IJCAI, volume 7, pages 2903–2908.",
    "year": 2007
  }, {
    "title": "Weighted feature subset non-negative matrix factorization and its applications to document understanding",
    "authors": ["Dingding Wang", "Tao Li", "Chris Ding."],
    "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 541–550. IEEE.",
    "year": 2010
  }, {
    "title": "An ontology matching approach based on affinity-preserving random walks",
    "authors": ["Chuncheng Xiang", "Baobao Chang", "Zhifang Sui."],
    "venue": "IJCAI, pages 1471–1478.",
    "year": 2015
  }, {
    "title": "Ranking on data manifolds",
    "authors": ["Dengyong Zhou", "Jason Weston", "Arthur Gretton", "Olivier Bousquet", "Bernhard Schölkopf."],
    "venue": "NIPS, volume 3. 219",
    "year": 2003
  }, {
    "title": "Headline summarization at isi",
    "authors": ["Liang Zhou", "Eduard Hovy."],
    "venue": "Document Understanding Conference (DUC-2003), Edmonton, Alberta, Canada.",
    "year": 2003
  }, {
    "title": "Improving diversity in ranking using absorbing random walks",
    "authors": ["Xiaojin Zhu", "Andrew B Goldberg", "Jurgen Van Gael", "David Andrzejewski."],
    "venue": "HLTNAACL, pages 97–104. 220",
    "year": 2007
  }],
  "id": "SP:9397c3545ada8e139684c6f479d6f0e125885b32",
  "authors": [{
    "name": "Kexiang Wang",
    "affiliations": []
  }, {
    "name": "Tianyu Liu",
    "affiliations": []
  }, {
    "name": "Zhifang Sui",
    "affiliations": []
  }, {
    "name": "Baobao Chang",
    "affiliations": []
  }],
  "abstractText": "Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinitypreserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE2 recall among the graph-based ranking methods.",
  "title": "Affinity-Preserving Random Walk for Multi-Document Summarization"
}