{
  "sections": [{
    "text": "intelligent transportation system applications. To capture the complex non-stationary temporal dynamics and spatial dependency in multistep traffic-condition prediction, we propose a novel deep learning framework named attention graph convolutional sequence-to-sequence model (AGC-Seq2Seq). In the proposed deep learning framework, spatial and temporal dependencies are modeled through the Seq2Seq model and graph convolution network separately, and the attention mechanism along with a newly designed training method based on the Seq2Seq architecture is proposed to overcome the difficulty in multistep prediction and further capture the temporal heterogeneity of traffic pattern. We conduct numerical tests to compare AGC-Seq2Seq with other benchmark models using a real-world dataset. The results indicate that our model yields the best prediction performance in terms of various prediction error measures. Furthermore, the variation of spatiotemporal correlation of traffic conditions\n Corresponding author. Email: fanghe@tsinghua.edu.cn.\nPreprint submitted to Elsevier (Transportation Research Part C)\nunder different perdition steps and road segments is revealed through sensitivity analyses.\nKeywords: traffic forecasting; deep learning; attention mechanism; graph convolution; multistep prediction; sequence-to-sequence model"
  }, {
    "heading": "1. INTRODUCTION",
    "text": "Automobile use has significantly increased in the past few decades owing to the steady development in both technology and economy. However, the increased automobile use has resulted in a series of social problems such as traffic congestion, traffic accidents, energy overconsumption, and carbon emissions (Gao et al., 2011). The intelligent transportation system (ITS) has been considered as a promising solution to improve transportation management and services (Qureshi and Abdullah, 2013; Lin et al., 2017). The success of ITS applications relies on accurate and timely traffic status information. The applications with high-precision traffic prediction (e.g., advanced traffic management systems and advanced traveler information systems) not only benefit travelersâ€™ route planning and departure time scheduling (Yuan et al., 2011) but also provide insightful information for traffic control to improve traffic efficiency and safety (Belletti et al., 2017). Therefore, short-term traffic flow forecasting has always attracted many scholarsâ€™ interest (Vlahogianni et al., 2004).\nSubstantial efforts have been conducted to develop methods for traffic prediction in the literature; however, some major challenges remain. Vlahogianni et al. (2014) provided a comprehensive review of the entire spectrum of the short-term traffic forecasting literature up to 2014 and reported the following potential directions for future research:\n Traffic prediction on traffic networks should be emphasized more.  Multistep for medium-long term forecasting is more adaptive to practical\napplications.\n Research on incorporating both temporal characteristics of traffic flow and spatial\ndependencies on traffic network still deserves more comprehensive investigation.\nMultistep traffic forecasting on road networks is challenging primarily due to the nonEuclidean topology structure and stochastic characteristic of the non-stationary timevarying traffic patterns, and inherent difficulty in multistep prediction. Hence, we propose a novel deep learning structure, named the attention graph convolutional sequence-to-sequence model (AGC-Seq2Seq). Specifically, we integrate the graph convolutional network and attention mechanism into a Seq2Seq framework to develop the prediction model that can depict the spatial-temporal correlation in multistep traffic prediction. Furthermore, considering that the existing training method for the Seq2Seq model is not suitable for time-series problems, we hereby design a new training method in our proposed framework. To summarize, the primary contributions of this paper are listed as follows:\ni. We propose a novel deep learning framework, named AGC-Seq2Seq, which\nextracts the features from temporal and spatial domains simultaneously through the Seq2Seq model and graph convolution layer. To overcome the multistep prediction challenge and capture the temporal heterogeneity of urban traffic pattern, the attention mechanism is further incorporated into the model. Validated by the realworld traffic data provided by A-map (Gaode navigation), the proposed model yields a significant improvement over other state-of-the-art benchmarks in terms of various major error measures under different prediction intervals.\nii. We design a new training method for the Seq2Seq framework aiming at multistep\ntraffic prediction to replace the existing ones (e.g., teacher forcing and scheduled sampling). It coordinates multidimensional features (e.g., historical statistic information and time-of-day) with spatial-temporal speed variables in one end-toend deep learning structure and enables the input for the testing periods to agree with the training periods.\niii. Based on the proposed model, we explore the variation of spatial and temporal\ncorrelations of traffic conditions under different perdition steps and road segments.\nThe remainder of this paper is organized as follows. Section 2 first reviews the existing research. Section 3 formulates the short-term traffic speed forecasting problem, and describes the structure and mathematical formulation of the proposed AGC-Seq2Seq model. Section 4 compares the prediction performances of the proposed model with other benchmark models based on the real-world dataset in Beijing and presents sensitive analyses. Finally, Section 5 concludes the paper."
  }, {
    "heading": "2. LITERATURE REVIEW",
    "text": "Traffic flow/condition forecasting has been studied for decades, and various emerging methods are constantly used to model traffic characteristics. With the rapid development of real-time traffic data collection methods, data-driven approaches through enormous historical data to capture similar traffic patterns prevail in recent years. As reported by Li et al. (2017), statistical models, shallow machine learning models and deep learning models are three major representative categories.\nStatistical models can predict future values based on previously observed values by time-series analysis. The autoregressive integrated moving average (ARIMA) model (Ahmed and Cook, 1979), Kalman filter (Okutani and Stephanedes, 1984), and their variations (Williams and Hoel, 2003; Guo et al., 2014) are among the most consolidated approaches. However, simple time-series models typically rely on the stationary assumption, which is inconsistent with non-stationary characteristics of urban traffic dynamics. Specifically, for multistep prediction, the posterior predicted values are based on the prior predicted values; thus, the prediction errors could propagate step by step. In this context, it is difficult to satisfy the high-precision requirement using simple time-series models.\nMeanwhile, machine learning methods have shown promising capabilities in traffic forecasting studies. The artificial neural network model (Vlahogianni et al., 2005),\nBayesian networks (Fusco et al., 2016), support vector machine model (Castro-Neto et al., 2009), K-nearest neighbors model (Zhang et al., 2013; Habtemichael and Cetin, 2016; Cai et al., 2016) and random forest model (Hamner, 2011) all yield satisfactory results in traffic flow forecasting. However, the performances of machine learning models depend heavily on manually selected features, and well-recognized guidelines to choose the appropriate features are not available in general since the key features are problem-wise. Therefore, using elementary machine learning approaches may not yield the prospective outcomes for complicated prediction tasks.\nMore recently, deep learning models have been widely and successfully employed in computer science; meanwhile, it has drawn substantial attention in the transportation field. Huang et al. (2014) employed the deep belief network for unsupervised feature learning, which was proven efficient in traffic flow prediction. Lv et al. (2015) applied a stacked auto encoder model to learn generic traffic flow features. Ma et al. (2015) used the long short-term memory neural network (LSTM) to capture nonlinear traffic dynamics effectively. Polson and Sokolov (2017) combined ğ¿1 regularization and a multilayer network activated by the tanh function to detect the sharp nonlinearities of traffic flow. However, the models with deep architectures above mainly aim at modeling a single sequence, which fails to reflect spatial correlations on traffic networks.\nMeanwhile, convolutional neural networks (CNN) offer an efficient architecture to extract meaningful statistical patterns in large-scale, and high-dimensional datasets. The capability of CNNs in learning local stationary structures resulted in breakthroughs in image and video recognition tasks (Defferrard et al., 2016). In transportation, efforts have been conducted to apply CNN structures to extract spatial correlation on traffic networks. Ma et al. (2017) proposed a deep convolutional neural network for traffic speed prediction, where spatial-temporal traffic dynamics are converted to images. Wang et al. (2017) processed an expressway as a band image, and subsequently proposed the error-feedback recurrent convolutional neural network structure for continuous traffic speed prediction. Ke et al. (2017) partitioned the urban area into\nuniform grids and subsequently combined a convolutional layer with an LSTM layer to predict the on-demand passenger demand in each grid. All of the aforementioned research converted traffic network to regular grids because the CNNs are restricted to processing Euclidean-structured data. However, the time series on road networks in traffic forecasting are continuous sequences distributed over a topology graph, which is a typical representative of non-Euclidean-structured data (Narang et al., 2013); in this case, the original CNN structure may not be applicable. To fill this gap, the graph convolutional network (GCN) was developed to generalize the convolution on nonEuclidean domains in the context of spectral graph theory (Kipf and Welling, 2016). Several newly published studies conducted graph convolution on traffic prediction. Spectral-based graph convolution was adopted and combined with temporal convolution (Yu et al., 2017) and the recurrent neural network (RNN) (Li et al., 2017) to forecast traffic states. Later, Cui et al. (2018) applied high-order graph convolution to learn the interactions between links on the traffic network. The studies above do not directly define the graph convolution on road networks, but construct the traffic detector graph through computing the pairwise distances between sensors with threshold Gaussian kernel. Moreover, the temporal correlation of traffic conditions is not considered, either.\nTo summarize, the evolution of traffic conditions on urban networks exhibits spatial and temporal dependencies, substantially. In this paper, we are devoted to proposing a customized deep learning framework, which integrates the attention mechanism and the graph convolutional network into a Seq2Seq model structure, to simultaneously capture the complex non-stationary temporal dynamics and spatial dependency in multistep traffic-condition prediction."
  }, {
    "heading": "3. AGC-SEQ2SEQ DEEP LEARNING FRAMEWORK",
    "text": ""
  }, {
    "heading": "3.1. Preliminaries",
    "text": "In this subsection, we interpret the definitions and notations of the variables used herein.\n(1) Road network topology The road network is modeled as a directed graph ğ’¢(ğ’©, â„’) according to the driving direction, where the node set ğ’© represents the intersections (detectors or selected demarcation points on the freeway), and the link set â„’ represents the road segments, as shown in Figure 1. ğ‘¨ is the adjacency matrix of the link set, and the dummy variable ğ‘¨(ğ‘–, ğ‘—) denotes whether link ğ‘– and link ğ‘— are connected, i.e., ğ‘¨(ğ‘–, ğ‘—) = { 1, ğ‘™ğ‘– and ğ‘™ğ‘— are connected along driving direction\n0, otherwise .\n(2) Traffic speed The speed at the ğ‘¡th time slot (e.g., 5 min) of road segment ğ‘™ğ‘– (âˆ€ğ‘™ğ‘– âˆˆ â„’) is defined as the average speed of floating cars during this time interval on the road segment, which is denoted by ğ‘£ğ‘¡ ğ‘–. The speed of the road network at the ğ‘¡th time slot is defined as the\nvector ğ‘½ğ‘¡ âˆˆ â„ |â„’| (|â„’| is the cardinality of link set â„’ in the underlying road network), where the ğ‘–th element is (ğ‘½ğ‘¡)ğ‘– = ğ‘£ğ‘¡ ğ‘–.\nAs a classical time-series prediction problem, the nearest m-step observation data can provide valuable information for multistep traffic speed forecasting. In addition to the real-time traffic speed information, some exogenous variables such as the time-of-day, weekday-or-weekend, and historical statistic information are also helpful to predict the future traffic speed. We introduce these variables in the following part.\n(3) Time-of-day and weekday-or-weekend Because the speed of each road segment is aggregated as the average value in 5 min, the time-of-day is transformed into an ordered integer ğ‘, e.g., 00:00-00:05 as ğ‘t = 1, and 7:00-7:05 as ğ‘ğ‘¡ = 85 (7 âˆ— 12 + 1). The weekday-or-weekend is denoted by the dummy variable ğ‘ğ‘¡ that distinguishes different traffic characteristics between weekdays and weekends.\n(4) Historical statistic information The daily trend of the traffic status can be captured by introducing historical statistic information into the prediction model. The historical average speed, median speed, maximum speed, minimum speed, and standard deviation at the ğ‘¡th time slot of road segment ğ‘™ğ‘– are defined as the average value, median value, maximum value, minimum value, and standard deviation in the training dataset, respectively, which are denoted by ğ‘£ğ‘¡,ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘– , ğ‘£ğ‘¡,ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘› ğ‘– , ğ‘£ğ‘¡,ğ‘šğ‘ğ‘¥ ğ‘– , ğ‘£ğ‘¡,ğ‘šğ‘–ğ‘› ğ‘– and ğ‘‘ğ‘¡ ğ‘– , respectively.\n(5) Problem formulation The task of traffic speed prediction is to use the previously observed speed records to forecast the future values of each road segment in a certain period. The multistep traffic speed problem can be formulated as\n?Ì‚?ğ‘¡+ğ‘› = argmax ğ‘½ğ‘¡+ğ‘› Pr(ğ‘½ğ‘¡+ğ‘›|ğ‘½ğ‘¡, ğ‘½ğ‘¡âˆ’1,â‹¯ , ğ‘½ğ‘¡âˆ’ğ‘š; ğ’¢) (1)\nwhere ?Ì‚?ğ‘¡+ğ‘›(ğ‘› = 1,2,3,â‹¯ ) represents the ğ‘› th-step predicted speed of the underlying road network, and {ğ‘½ğ‘¡, ğ‘½ğ‘¡âˆ’1, â‹¯ , ğ‘½ğ‘¡âˆ’ğ‘š âˆ£ ğ‘š = 1,2,â‹¯ } is the relevant previously observed value vector. Pr(âˆ™ | âˆ™) is the conditional probability function."
  }, {
    "heading": "3.2 Graph Convolution on Traffic Networks",
    "text": "Graph convolution extends the applicable scope of standard convolution from regular grids to general graphs by manipulating in the spectral domain. To introduce the general ğ¾ -order graph convolution, we first define the ğ¾ -hop neighborhoods for each road\nsegment ğ‘™ğ‘– âˆˆ â„’ as â„‹ğ‘–(ğ¾) = {ğ‘™ğ‘— âˆˆ â„’|ğ‘‘(ğ‘™ğ‘–, ğ‘™ğ‘—) â‰¤ ğ¾} in the context of road network\ntopology (introduced in section 3.1), where ğ‘‘(ğ‘™ğ‘–, ğ‘™ğ‘—) represents the minimum number of needed links among all the walks from ğ‘™ğ‘– to ğ‘™ğ‘—.\nIt is typical that the adjacency matrix is exactly the one-hop neighborhood matrix ğ‘¨, and the ğ¾-hop neighborhood matrix can be acquired by calculating the ğ¾th power of ğ‘¨. To imitate the Laplacian matrix, we add the diagonal element to the neighborhood matrix, which is defined as\nğ‘¨ğºğ¶ ğ¾ = Ci(ğ‘¨ğ¾ + ğ‘°) (2)\nwhere Ci(âˆ™) is a clip function for the matrix by modifying each nonzero element to 1; thus, ğ‘¨ğºğ¶ ğ¾ (ğ‘–, ğ‘—) = 1 ğ‘“ğ‘œğ‘Ÿ ğ‘™ğ‘— âˆˆ â„‹ğ‘–(ğ¾) ğ‘œğ‘Ÿ ğ‘– = ğ‘— ; otherwise, ğ‘¨ğºğ¶ ğ¾ (ğ‘–, ğ‘—) = 0 . The identity matrix ğ‘° added to ğ‘¨ğ¾ renders the convolution self-accessible in the topology graph. Based on the abovementioned neighborhood matrix, a concise version of graph convolution (e.g., Cui et al., 2018) can be defined as follows.\nğ‘½ğ‘¡(ğ¾) = (ğ‘¾ğºğ¶â¨€ğ‘¨ğºğ¶ ğ¾ ) âˆ™ ğ‘½ğ‘¡ (3)\nwhere ğ‘¾ğºğ¶ is a trainable matrix with the same size of ğ‘¨. The operator â¨€ refers to the Hadamard product that conducts the element-wise multiplication operation. Through the element-wise multiplication, (ğ‘¾ğºğ¶â¨€ğ‘¨ğºğ¶ ğ¾ ) will produce a new matrix with trainable parameters on the ğ¾-hop neighbor positions and zero on the remaining positions. Therefore, (ğ‘¾ğºğ¶â¨€ğ‘¨ğºğ¶ ğ¾ ) âˆ™ ğ‘½ğ‘¡ can be understood as spatial discrete convolution for ğ‘½ğ‘¡. In consequence, ğ‘½ğ‘¡(ğ¾) is the spatially-fused speed vector at the time ğ‘¡. Its ğ‘–th element ğ‘£ğ‘¡ ğ‘–(ğ¾) represents the spatially-fused speed of the road segment ğ‘™ğ‘– âˆˆ â„’ at the time ğ‘¡ that incorporates the information of all the neighbor road segments in â„‹ğ‘–(ğ¾).\nFurther, Equation (3) can be decomposed into a one-dimensional convolution that is flexible and suitable for parallel computing.\nğ‘£ğ‘¡ ğ‘–(ğ¾) = (ğ‘¾ğºğ¶[ğ‘–]â¨€ğ‘¨ğºğ¶ ğ¾ [ğ‘–])ğ‘‡ âˆ™ ğ‘½ğ‘¡ (4)\nğ‘¾ğºğ¶[ğ‘–] and ğ‘¨ğºğ¶ ğ¾ [ğ‘–] are the ğ‘–th row of ğ‘¾ğºğ¶ and ğ‘¨ğºğ¶ ğ¾ , respectively. An example of ğ‘¨ğºğ¶ ğ¾ [ğ‘–] on the road network is shown in Figure 2 , where the road segment ğ‘– is in red line and neighbor links are in blue lines."
  }, {
    "heading": "3.3 Attention Graph Convolutional Sequence-to-Sequence Model (AGC-Seq2Seq)",
    "text": "In this subsection, we propose the novel AGC-Seq2Seq model that integrates spatialtemporal variables and exogenous information into the deep learning architecture for multistep traffic speed prediction.\nTo capture time-series characteristics and obtain multistep outputs, we adopt the Seq2Seq model as the basic structure for the whole approach that is composed of two connected RNN modules with independent parameters (Sutskever et al., 2014; Cho et al., 2014). To overcome the fixed output timestamp of the RNN structure, the Seq2Seq model encodes the time-series input in the encoder part to satisfy the temporal dependencies, and the decoder produces the target outputs organized by time steps from the context vector. The framework of the proposed AGC-Seq2Seq model is shown in Figure 3. Specifically, the graph convolution operation is first utilized to capture the spatial characteristics. Subsequently, the spatial-temporal variable ğ‘£ğ‘¡âˆ’ğ‘— ğ‘– (ğ¾) is fused with exogenous variable ğ‘¬ğ‘¡âˆ’ğ‘— (including the information of time-of-day and weekday-or-weekend) to construct the input vector, which is then fed into the encoder of Seq2Seq model. The procedure above is demonstrated in the following equations.\nğ‘£ğ‘¡âˆ’ğ‘— ğ‘– (ğ¾) = (ğ‘¾ğºğ¶[ğ‘–]â¨€ğ‘¨ğºğ¶ ğ¾ [ğ‘–])ğ‘‡ âˆ™ ğ‘½ğ‘¡âˆ’ğ‘— , 0 â‰¤ ğ‘— â‰¤ ğ‘š (5) ğ‘¬ğ‘¡âˆ’ğ‘— = [ğ‘ğ‘¡âˆ’ğ‘—; ğ‘ğ‘¡âˆ’ğ‘—] (6)\nğ‘¿ğ‘¡âˆ’ğ‘— ğ‘– = [ğ‘£ğ‘¡âˆ’ğ‘— ğ‘– (ğ¾); ğ‘¬ğ‘¡âˆ’ğ‘—] (7)\nwhere ğ‘ğ‘¡âˆ’ğ‘— and ğ‘ğ‘¡âˆ’ğ‘— are defined in section 3.1; and the operator [âˆ™ ; âˆ™] concatenates two tensors along the same dimensions.\nThen, in the encoder part, as demonstrated in Equations (8)â€“(9) below, at the time step ğ‘¡ âˆ’ ğ‘—, ğ‘— âˆˆ {0,â‹¯ ,ğ‘š}, the previous hidden status ğ’‰ğ‘¡âˆ’ğ‘—âˆ’1 is passed to the current time stamp together with input ğ‘¿ğ‘¡âˆ’ğ‘— to calculate ğ’‰ğ‘¡âˆ’ğ‘— . Therefore, the context vector ğ‘ª stores all the information of the encoder including the hidden states (ğ’‰ğ‘¡âˆ’ğ‘š, ğ’‰ğ‘¡âˆ’ğ‘š+1, â‹¯ , ğ’‰ğ‘¡âˆ’1) and input vector (ğ‘¿ğ‘¡âˆ’ğ‘š, ğ‘¿ğ‘¡âˆ’ğ‘š+1, â‹¯ , ğ‘¿ğ‘¡), which is further designed as a connector between encoder and decoder parts.\nğ’‰ğ‘¡âˆ’ğ‘— = { Cellencoder(ğ’‰0, ğ‘¿ğ‘¡âˆ’ğ‘—), ğ‘— = ğ‘š\nCellencoder(ğ’‰ğ‘¡âˆ’ğ‘—âˆ’1, ğ‘¿ğ‘¡âˆ’ğ‘—) , ğ‘— âˆˆ {0,â‹¯ ,ğ‘š âˆ’ 1}\n(8)\nğ‘ª = ğ’‰ğ‘¡ (9)\nwhere ğ’‰0 is the initial hidden status and typically set as a zero vector; Cellencoder(âˆ™) is the calculation function for the encoder that is decided by the adopted RNN structure.\nIn the decoder part, the core idea is leveraging the context vector ğ‘ª as the initial hidden status, and subsequently decoding the output sequence step by step. In consequence, at the time stamp ğ‘¡ + ğ‘—, ğ‘— âˆˆ {1,â‹¯ , ğ‘›}, the hidden state ğ’‰ğ‘¡+ğ‘— not only contains the input\ninformation, but also considers the previous output status (ğ’‰ğ‘¡+1, ğ’‰ğ‘¡+2, â‹¯ , ğ’‰ğ‘¡+ğ‘—âˆ’1).\nThe inputs of the decoder are dependent on the training method. Teacher forcing is a popular training strategy used in natural language processing. In the teacher-forcing training strategy, the ground truths (target sequence) are fed into the decoder for the training, and at the testing stage, the previously generated predictions are utilized as input for the later time stamp. However, this method is not suitable for the time-series problem primarily because of the discrepant distributions of the decoder inputs between the training and testing periods. Li et al. (2017) mitigated this issue by using scheduled sampling that randomly selects either the ground truth or the previous prediction to feed the model with the setting probability ğœ– . However, it will inevitably increase the complexity of the model and calculation burden.\nTo overcome the issues above, we propose a new training method employing the historical statistic information and time-of-day as inputs. In the time-series prediction problem, historical statistic information can be obtained both in the training and testing stages; in this context, the distribution of decoder inputs between the training and testing periods will synchronize with each other, thus solving the dilemma of teacher forcing. Moreover, because historical statistic information is critical in multistep forecasting, adding it to the model is expected to enhance the prediction accuracy. Accordingly, the equations below are used to calculate the hidden state in the decoder at the time ğ‘¡ + ğ‘—, ğ‘— âˆˆ {1,â‹¯ , ğ‘›}.\nğ’—ğ‘¡+ğ‘— ğ‘– ( ) = [ğ‘ğ‘¡+ğ‘—; ğ‘£ğ‘¡+ğ‘—,ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘– ; ğ‘£ğ‘¡+ğ‘—,ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘› ğ‘– ; ğ‘£ğ‘¡+ğ‘—,ğ‘šğ‘ğ‘¥ ğ‘– ; ğ‘£ğ‘¡+ğ‘—,ğ‘šğ‘–ğ‘› ğ‘– ; ğ‘‘ğ‘¡+ğ‘— ğ‘– ] (10)\nğ’‰ğ‘¡+ğ‘— = { Celldecoder (ğ‘ª, ğ’—ğ‘¡+ğ‘—\nğ‘– ( )) , ğ‘— = 1\nCelldecoder (ğ’‰ğ‘¡+ğ‘—âˆ’1, ğ’—ğ‘¡+ğ‘— ğ‘– ( )) , ğ‘— âˆˆ {2,â‹¯ , ğ‘›}\n(11)\nwhere Celldecoder(âˆ™) is the calculation function for the decoder, which is similar to that of the encoder.\nWe employ the Gated Recurrent Unit (Chung et al., 2014) as the inner structure for both the encoder and decoder (shown in Figure 4). It demonstrates competitive performance and a simpler structure than the standard LSTM. The calculation procedure of Cellencoder(âˆ™) and Celldecoder(âˆ™) is shown in Equations (12)â€“(17) below.\nğ‘§ğ‘¡ = ğœ(ğ‘¾ğ‘§ âˆ™ [ğ’‰ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘ğ‘§) (12) ğ‘Ÿğ‘¡ = ğœ(ğ‘¾ğ‘Ÿ âˆ™ [ğ’‰ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘ğ‘Ÿ) (13) ğ‘ğ‘¡ = tanh(ğ‘¾ğ‘ âˆ™ [ğ‘Ÿğ‘¡â¨€ğ’‰ğ‘¡âˆ’1; ğ‘¥ğ‘¡] + ğ‘ğ‘) (14) ğ’‰ğ‘¡ = (1 âˆ’ ğ‘§ğ‘¡)â¨€ğ’‰ğ‘¡âˆ’1 + ğ‘§ğ‘¡â¨€ğ’„ğ‘¡ (15) ğœ(ğ‘¥) = 1\n1 + ğ‘’âˆ’ğ‘¥\n(16)\ntanh(ğ‘¥) = ğ‘’ğ‘¥ âˆ’ ğ‘’âˆ’ğ‘¥\nğ‘’ğ‘¥ + ğ‘’âˆ’ğ‘¥\n(17)\nIn the equations above, ğ‘§ğ‘¡ and ğ‘Ÿğ‘¡ are the update gate and the reset gate, respectively.\nğ‘ğ‘¡ is the candidate output. ğœ(âˆ™) and tanh(âˆ™) are the two widely used nonlinear activation functions that map the input into (0,1) and (-1,1), respectively. ğ‘Šğ‘§, ğ‘Šğ‘Ÿ, and ğ‘Šğ‘ are the weight matrices that achieve the fully connected layer, while ğ‘ğ‘§ , ğ‘ğ‘Ÿ , ğ‘ğ‘ are the corresponding bias vectors.\nTo capture the temporal heterogeneity of traffic pattern, we further integrate the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) into the model. The key concept of the attention mechanism is adding the attention vector for each time step that captures the relevance of the source-side information to help predict the traffic speed. At the time step ğ‘¡ + ğ‘— , ğ‘— âˆˆ {1,â‹¯ , ğ‘›} , the attention function, defined by Equations (18)-(20), maps query ğ’‰ğ‘¡+ğ‘— and a set of keys (ğ’‰ğ‘¡âˆ’ğ‘š, â‹¯ , ğ’‰ğ‘¡âˆ’1, ğ’‰ğ‘¡) to the attention vector ğ‘ºğ‘¡+ğ‘—. As given by Equations (18)â€“(20) below, ğ‘ºğ‘¡+ğ‘— is computed as a weighed sum of the keys, where the weight assigned to each key is obtained by a compatibility function of the query with the corresponding key.\nğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– = ğ’’ğ‘‡ tanh(ğ’‰ğ‘¡+ğ‘—ğ‘¾ğ‘“ğ’‰ğ‘¡âˆ’ğ‘–) , ğ‘– = 0,1,â‹¯ ,ğ‘š\n(18)\nğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– = softmax( ğ‘¡+ğ‘—\nğ‘¡âˆ’ğ‘–) = exp( ğ‘¡+ğ‘—\nğ‘¡âˆ’ğ‘–)\nâˆ‘ exp( ğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘Ÿ)ğ‘šğ‘Ÿ=1\n, ğ‘– = 0,1,â‹¯ ,ğ‘š\n(19)\nğ‘ºğ‘¡+ğ‘— = âˆ‘ ğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘–ğ’‰ğ‘¡âˆ’ğ‘– ğ‘š ğ‘–=1\n(20)\nwhere ğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– can be used construed as to measure the similarity between ğ’‰ğ‘¡+ğ‘— and\nğ’‰ğ‘¡âˆ’ğ‘–, calculated by Equation (18), and in this study, we employ the Luong Attention form (Luong et al., 2015) as the compatibility function with trainable weight matrix ğ‘¾ğ‘“ and vector ğ’’ ğ‘‡ to adjust the dimension of the result; ğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– is the normalization of\nğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– and is further used as the weight coefficient with the corresponding encoder\nhidden state ğ’‰ğ‘¡âˆ’ğ‘– to calculate ğ‘ºğ‘¡+ğ‘—.\nAs shown in Figure 3, the attentional hidden state ?Ìƒ?ğ‘¡+ğ‘— is composed of the attention vector ğ‘ºğ‘¡+ğ‘— and original hidden state ğ’‰ğ‘¡+ğ‘— through a simple concatenation, as shown in Equation (21). Equation (22) denotes the linear transformation from the hidden state to the output. The dimensions of the weighted parameter matrix ğ‘¾ğ‘£ and intercept parameter ğ‘ğ‘£ are consistent with the output.\n?Ìƒ?ğ‘¡+ğ‘— = tanh(ğ‘¾â„ âˆ™ [ğ‘ºğ‘¡+ğ‘˜; ğ’‰ğ‘¡+ğ‘—]) (21) ğ‘£ ğ‘¡+ğ‘— = ğ‘¾ğ‘£?Ìƒ?ğ‘¡+ğ‘— + ğ‘ğ‘£ (22)\nTo jointly reduce the predictive errors in multiple step prediction, we define the loss as the mean absolute error between (ğ‘£ ğ‘¡+1, ğ‘£ ğ‘¡+2, â‹¯ . ğ‘£ ğ‘¡+ğ‘›) and (ğ‘£ğ‘¡+1, ğ‘£ğ‘¡+2, â‹¯ . ğ‘£ğ‘¡+ğ‘›) , which is given by\nğ‘™ğ‘œğ‘ ğ‘  = 1\nğ‘› âˆ‘ |ğ‘£ ğ‘¡+ğ‘—\nğ‘– âˆ’ ğ‘£ğ‘¡+ğ‘— ğ‘– |ğ‘›ğ‘—=1\n(23)\nAll the parameters are updated by minimizing the loss function through the mini-batch gradient descent algorithm in the training stage. A detailed discussion regarding why the Seq2Seq framework is suitable for multistep prediction is presented in the appendix."
  }, {
    "heading": "4 NUMERICAL EXAMPLES",
    "text": ""
  }, {
    "heading": "4.1. Dataset",
    "text": "The datasets utilized in this study were collected from the users of A-map, which is a smartphone-based navigation application with the most active users in China (Sohu, 2018). The studied site is selected as the entire 2nd ring road, which is the most congested among the ring roads in Beijing. As shown in Figure 5(a), we partition the 33KM-in-length 2nd ring road into 163 road segments with 200m in length. Furthermore, we calculate the 5-min average speed for each link using the collected trajectory points of anonymous users. The plots of the traffic speed in the 2nd ring road on weekdays and weekends are shown in Figure 5(b)-(c) with the x-axis for the longitude, y-axis for the latitude, z-axis for the time and color map for speed.\n(a) Sketch of road segments in GIS map\nWe extract the data from October 1, 2016 to November 30, 2016 for experimental purposes. The extracted dataset is divided into the training set comprised of records between October 1 and November 20, and the testing dataset consisting of the remaining observations between November 21 and November 27. The prediction time horizon is set as 06:00-22:00; thus, every road segment contains 192 data points per day. Figure 6 shows the split of the dataset and the corresponding data size for training and testing. In addition, after the data cleaning procedure, the missing values are filled by the linear interpolation method.\nOct 1 st , 2016-Nov 20 th , 2016\n51 days of data\nNov 21 st , 2016-Nov 27 th , 2016\nMonday-Sunday (one week)\nTrain\nTest\nData Size\n51*163*192 1,600,000\nData Size\n7*163*192 220,000\nFigure 6. Split of dataset"
  }, {
    "heading": "4.2 Model comparisons",
    "text": "In this subsection, the proposed model is compared with other benchmark models, including the traditional time-series analysis approaches (i.e., HA and ARIMA) and state-of-the-art machine learning (i.e., ANN, KNN, SVR, and XGBOOST) /deep learning models (i.e., LSTM, GCN, and Seq2Seq-Att).\n HA: The historical average model predicts the future speed in the testing dataset\nbased on the empirical statistics in the training set, i.e., ğ‘£ğ‘¡,ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘– . For example,\nthe average speed during 8:00â€“8:05 of road segment ğ‘™ğ‘– âˆˆ â„’ is estimated by the mean of all historical speeds in the training dataset during 8:00â€“8:05 of the same link.\n ARIMA: For the autoregressive integrated moving average (ğ‘, ğ‘‘, ğ‘) model (Box\nand Pierce, 1970), the degree of differencing is set as ğ‘‘ = 1 , and the order of autoregressive part and moving average part (ğ‘, ğ‘) are determined through computing the corresponding Akaike information criterion of the training dataset with ğ‘ âˆˆ [0,2], ğ‘ âˆˆ [7,12].\n ANN: We establish a three-layer artificial neural network (Rumelhart et al., 1988)\nactivated by the sigmoid function, and set the number of hidden neurons twice the dimension of the feature vector. Because the ANN does not differentiate variables across time steps, it fails to capture the temporal dependencies.\n KNN: K-nearest neighbor (Denoeux, 1995) is a lazy learning algorithm that obtains\nthe K-most similar observations in the training set through the Euclidean distance\nbetween feature vectors. The predicted value is calculated through the weighted summation of the corresponding future values belonging to the selected observations. The hyper parameter ğ¾ is chosen through cross validation from 5 to 25.\n SVR: In support vector regression (Suykens and Vandewalle, 1999), the fitting\ncurve is calculated through the mapping feature vectors into the high-dimensional space aided by the kernel function. The kernel function and hyper parameters in the model are selected through cross validation.\n XGBOOST: XGBOOST (Chen and Guestrin, 2016) yields outstanding\nperformance in a broad range of machine learning tasks; it is a scalable end-to-end boosting system based on the tree structure. All the features are reshaped to a vector and fed into XGBOOST for training.\n LSTM: In LSTM (Hochreiter and Schmidhuber, 1997), all the features of each road\nsegment are reshaped to a matrix with one axis as the time steps, and the other axis as the feature category. LSTM takes temporal dependencies into account, but does not capture spatial dependencies.\n GCN: In GCN, the features of all road segments in the underlying traffic network\nare reshaped to a matrix with one axis as each road segment, and the other axis as the feature category. GCN generalizes the convolution to non-Euclidean domains by the Laplacian matrix of the graph; therefore, it considers spatial correlation, but does not capture temporal dependencies.\n Seq2Seq-Att: In Seq2Seq-Att, the attention mechanism based on the Seq2Seq\nstructure is utilized for traffic prediction along with the new proposed training method. The only difference between the Seq2Seq-Att and AGC-Seq2Seq models is the graph convolution layer.\nTo ensure fairness, the aforementioned benchmark prediction models have the same input features (the same category and look-back time windows) as those of the AGCSeq2Seq model, while the traditional time-series model utilizes the whole time-series of speed records in the training set. We consider the look-back time windows as 12 (i.e.,\nğ‘š = 11), implying that the speed records in the past hour are adopted to predict the future value. The designed 19-dimensional feature vector containing speed observations in the past hour, time-of-day, weekday-or-weekend, and historical statistic information is shown in Table 1.\nAll the notations are defined in section 3.1. ğ‘› is fixed according to the prediction step. We evaluate the models via three classical error indexes: mean absolute percentage error (MAPE), mean absolute error (MAE), and root mean squared error (RMSE), given by MAPE = 1\nğ‘„ âˆ‘\n|ğ‘£ğ‘–âˆ’ğ‘£ ğ‘–|\nğ‘£ğ‘–\nğ‘„ ğ‘–=1 , MAE =\n1 ğ‘„ âˆ‘ |ğ‘£ğ‘– âˆ’ ğ‘£ ğ‘–| ğ‘„ ğ‘–=1 , and RMSE = âˆš 1 ğ‘„ âˆ‘ (ğ‘£ğ‘– âˆ’ ğ‘£ ğ‘–)2 ğ‘„ ğ‘–=1 ,\nwhere ğ‘£ğ‘– and ğ‘£ ğ‘– are the ğ‘– th ground truth and prediction values of the traffic speed, respectively; ğ‘„ is the size of the testing dataset.\nOur experimental platform is on the server with two CPUs (Intel(R) Xeon(R) CPU E52673 v3 @2.40Ghz, 24 cores), 256-GB RAM, and four GPUs (NVIDIA Quadro P5000, 16 GB memory). All the algorithms are coded in the parallel computation structure.\nTable 2 shows the comparison of the proposed model and benchmark algorithms for 5 min, 15 min, and 30 min ahead forecasting on the testing dataset. The following phenomena can be observed from the experimental results.\ni. AGC-Seq2Seq model outperforms the other benchmarks in terms of all the metrics\nunder all prediction intervals.\nii. The performance of HA is invariant to the increases in the forecasting horizon\nbecause it depends only on the historical data.\niii. The performances of all the models under the 5-min forecasting horizons are\nsimilar because the traffic status is relatively stable within 5 min.\niv. The deep-learning approaches yield better predictive performances but longer\ncomputational time than the traditional machine-learning models.\nv. The GCN (which models spatial correlations) outperforms LSTM (which captures\nthe temporal characteristics), providing verification that the consideration of spatial correlations is important in traffic speed forecasting.\nvi. The AGC-Seq2Seq model exhibits a distinct improvement over the GCN and\nSeq2Seq-Att; this emphasizes the importance of capturing the spatial-temporal characteristics simultaneously for the traffic speed forecasting. The running time of the AGC-Seq2Seq model is only slightly higher than those of the GCN and Seq2Seq-Att; it primarily benefits from the advanced parallel computation technology in the GPU module.\nWe select ARIMA, XGBOOST, and LSTM as the representatives for the time-series models, machine learning models, and deep learning approaches, respectively, to compare their performances with AGC-Seq2Seq under the 5â€“30-min prediction intervals, as shown in Figure 7. AGC-Seq2Seq tends to demonstrate better performance than other models with the increase in the prediction horizon. Additionally, the ARIMA model performs the worst because of the step-by-step error accumulation in the multistep forecasting scenario.\nFigure 8 shows the prediction values of XGBOOST and AGC-Seq2Seq on the morning peak hours of Nov 23rd (link 29) under the 15-min horizon. It is obvious that the prediction values of XGBOOST lags behind the ground truth when the traffic status oscillates seriously while AGC-Seq2Seq alleviates such problem."
  }, {
    "heading": "4.3 Sensitivity analyses",
    "text": "(1) Feature importance Figure 9 shows the F score (the number of times a feature is used to split the data across all trees, and a higher score indicates the corresponding feature being more import) of the feature vector (shown in Table 1) in the XGBOOST model under a 15-min prediction horizon, which is used widely to assess the importance of the features (Ke et al., 2017). To evaluate the trend of feature importance under different prediction intervals, we divide the feature vector into two major categories: 1) speed records T1 in the past hour (f7-f18) and exogenous information T2 (f0-f6). The F score of feature ğ‘“ğ‘–, ğ‘– = 0,1,â‹¯ ,18 is denoted as ğ¹(ğ‘“ğ‘–). The relative importance of T1 and T2 can be calculated as âˆ‘ ğ¹(ğ‘“ğ‘–)18ğ‘–=7 âˆ‘ ğ¹(ğ‘“ğ‘–)18ğ‘–=0 and âˆ‘ ğ¹(ğ‘“ğ‘–)6ğ‘–=0 âˆ‘ ğ¹(ğ‘“ğ‘–)18ğ‘–=0 , respectively. Figure 10 shows the relative importance of T1 and T2 under different prediction intervals. The results indicate that the exogenous variables are more important in the long-term prediction than in the short-term prediction. This is because the value of look-back observations degrades gradually with the increase in the forecasting interval.\n(2) Effect of spatial features in multistep prediction Figure 11 shows the curves of prediction error varying with k-hop neighbors in the\nAGC-Seq2Seq model under 5-min and 15-min time intervals (ğ‘˜ = 0 is just the case of Seq2Seq-Att model). The slopes of the curves in Figure 11(b) are smaller than those in Figure 11(a), thereby indicating that the effect of increasing spatial information on error reduction is compromised with the increase in the forecasting horizon.\n(3) Relevance between temporal traffic pattern and attention coefficient In the attention mechanism, the coefficient ğ‘¡+ğ‘— ğ‘¡âˆ’ğ‘– provides a criterion to measure the\nrelevance between the target and source-side information. Figure 12 visualizes the attention coefficients under two typical scenarios. The attention heatmap of the road segment with drastic status changes (Scenario I) exhibits high values on the look-back observations within the past 15 min, while the corresponding attention coefficients of the smoothly changed traffic status (Scenario II) distribute uniformly among the temporal dimension. This indicates that the prediction model tends to rely on the recent information (within past 15 min) when the traffic status oscillates severely."
  }, {
    "heading": "5 CONCLUSIONS",
    "text": "To tackle the challenge of multistep traffic speed prediction, we are devoted to proposing a sophisticated deep learning approach, i.e., the attention graph convolutional sequence-to-sequence model (AGC-Seq2Seq). The Seq2Seq architecture and graph convolutional operators are combined to learn the spatial-temporal dependencies on traffic networks. The attention mechanism is integrated into the model to capture the temporal heterogeneity of traffic patterns, and the entire architecture is trained with a newly designed method. To validate the effectiveness of the proposed model, we compare it with several benchmark models including the HA, ARIMA, XGBOOST, ANN, LSTM, SVR, KNN, GCN, and Seq2Seq-Att, based on the real-world data of the 2nd ring road in Beijing. The results indicate that the proposed model outperforms the benchmark models in terms of the RMSE, MAE, and MAPE under different prediction intervals. Based on the proposed model, we further explore the feature importance, effect of spatial information on multistep prediction, and relevance between traffic temporal pattern and attention coefficients. The evidence from the experiment implies that both the relative importance of the features regarding the speed records in the past hour and the effect of increasing spatial information degrade with the increase in the prediction intervals; for the road segments whose traffic condition changes rapidly, the corresponding attention coefficients take high values for the look-back observations within the past 15 min.\nFuture studies could include experiments on large urban road networks and further integrating the traffic flow theories into the prediction model, e.g., utilizing the propagation waves of traffic flow to determine the spatial neighbors in a more sophisticated model. From the application perspective, the proposed framework can be integrated with advanced transportation management systems, e.g., providing systemlevel real-time routing services to reduce peak-hour congestions.\nACKOWNLEDGMENTS\nThis research is supported partially by grants from National Natural Science Foundation of China (71871126, 51622807, U1766205) and partially by Tsinghua-Daimler Joint Research Center for Sustainable Transportation. The authors are grateful to A-map (https://www.amap.com/) for providing their anonymous usersâ€™ GPS trajectory data."
  }],
  "year": 2018,
  "references": [{
    "title": "Predicting Freeway Crashes from Loop Detector Data by Matched Case-Control Logistic Regression",
    "authors": ["M. Abdel-Aty", "N. Uddin", "A. Pande", "F. Abdalla", "H. Liang"],
    "venue": "Transportation Research Record Journal of the Transportation Research Board",
    "year": 2004
  }, {
    "title": "ANALYSIS OF FREEWAY TRAFFIC TIMESERIES DATA BY USING BOX-JENKINS TECHNIQUES",
    "authors": ["M.S. Ahmed", "A.R. Cook"],
    "year": 1979
  }, {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "year": 2014
  }, {
    "title": "Expert Level Control of Ramp Metering Based on Multi-Task Deep Reinforcement Learning",
    "authors": ["F. Belletti", "D. Haziza", "G. Gomes", "A.M. Bayen"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems",
    "year": 2017
  }, {
    "title": "Distribution of Residual Autocorrelations in Autoregressive-Integrated Moving Average Time Series Models",
    "authors": ["G. Box", "E.P", "Pierce", "A. David"],
    "venue": "Publications of the American Statistical Association",
    "year": 1970
  }, {
    "title": "A spatiotemporal correlative k-nearest neighbor model for short-term traffic multistep forecasting",
    "authors": ["P. Cai", "Y. Wang", "G. Lu", "P. Chen", "C. Ding", "J. Sun"],
    "venue": "Transportation Research Part C",
    "year": 2016
  }, {
    "title": "Online-SVR for shortterm traffic flow prediction under typical and atypical traffic conditions",
    "authors": ["M. Castro-Neto", "Y.S. Jeong", "M.K. Jeong", "L.D. Han"],
    "venue": "Expert Systems with Applications: An International Journal",
    "year": 2009
  }, {
    "title": "XGBoost: A Scalable Tree Boosting System",
    "authors": ["T. Chen", "C. Guestrin"],
    "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    "year": 2016
  }, {
    "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
    "authors": ["J. Chung", "C. Gulcehre", "K.H. Cho", "Y. Bengio"],
    "year": 2014
  }, {
    "title": "High-Order Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting",
    "authors": ["Z. Cui", "K. Henrickson", "R. Ke", "Y. Wang"],
    "venue": "Eprint arXiv",
    "year": 2018
  }, {
    "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
    "authors": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"],
    "year": 2016
  }, {
    "title": "A k-nearest neighbor classification rule based on Dempster-Shafer theory",
    "authors": ["T. Denoeux"],
    "venue": "Systems Man & Cybernetics IEEE Transactions on 25(5),",
    "year": 1995
  }, {
    "title": "Short-term speed predictions exploiting big data on large urban road networks",
    "authors": ["G. Fusco", "C. Colombaroni", "N. Isaenko"],
    "venue": "Transportation Research Part C",
    "year": 2016
  }, {
    "title": "Network-Scale Traffic Modeling and Forecasting with Graphical Lasso",
    "authors": ["Y. Gao", "S. Sun", "D. Shi"],
    "venue": "International Conference on Advances in Neural Networks,",
    "year": 2011
  }, {
    "title": "Adaptive Kalman filter approach for stochastic short-term traffic flow rate prediction and uncertainty quantification",
    "authors": ["J. Guo", "W. Huang", "B.M. Williams"],
    "venue": "Transportation Research Part C Emerging Technologies",
    "year": 2014
  }, {
    "title": "Short-term traffic flow rate forecasting based on identifying similar traffic patterns",
    "authors": ["F.G. Habtemichael", "M. Cetin"],
    "venue": "Transportation Research Part C",
    "year": 2016
  }, {
    "title": "Deep Architecture for Traffic Flow Prediction: Deep Belief Networks With Multitask Learning",
    "authors": ["W. Huang", "G. Song", "H. Hong", "K. Xie"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems",
    "year": 2014
  }, {
    "title": "Short-Term Forecasting of Passenger Demand under On-Demand Ride Services: A Spatio-Temporal Deep Learning Approach",
    "authors": ["J. Ke", "H. Zheng", "H. Yang", "Xiqun", "Chen"],
    "venue": "Transportation Research Part C Emerging Technologies",
    "year": 2017
  }, {
    "title": "Semi-Supervised Classification with Graph Convolutional Networks",
    "authors": ["T.N. Kipf", "M. Welling"],
    "venue": "Eprint arXiv",
    "year": 2016
  }, {
    "title": "Foundations of Sequence-to-Sequence Modeling",
    "authors": ["V. Kuznetsov", "Z. Mariet"],
    "year": 2018
  }, {
    "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
    "authors": ["Y. Li", "R. Yu", "C. Shahabi", "Y. Liu"],
    "year": 2017
  }, {
    "title": "Intelligent Transportation System (ITS): Concept, Challenge and Opportunity",
    "authors": ["Y. Lin", "P. Wang", "M. Ma"],
    "venue": "IEEE International Conference on Big Data Security on Cloud,",
    "year": 2017
  }, {
    "title": "Short-Term Traffic Flow Forecasting: An Experimental Comparison of Time-Series Analysis and Supervised Learning",
    "authors": ["M. Lippi", "M. Bertini", "P. Frasconi"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems",
    "year": 2013
  }, {
    "title": "Nonlinear network traffic prediction based on BP neural network",
    "authors": ["J. Liu", "Y.L. Huang"],
    "venue": "Journal of Computer Applications",
    "year": 2007
  }, {
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "authors": ["M.T. Luong", "H. Pham", "C.D. Manning"],
    "year": 2015
  }, {
    "title": "Traffic Flow Prediction With Big Data: A Deep Learning Approach",
    "authors": ["Y. Lv", "Y. Duan", "W. Kang", "Z. Li", "F.Y. Wang"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems",
    "year": 2015
  }, {
    "title": "Learning Traffic as Images: A Deep Convolutional Neural Network for Large-Scale Transportation Network Speed Prediction",
    "authors": ["X. Ma", "Z. Dai", "Z. He", "J. Ma", "Y. Wang"],
    "year": 2017
  }, {
    "title": "Long short-term memory neural network for traffic speed prediction using remote microwave sensor data",
    "authors": ["X. Ma", "Z. Tao", "Y. Wang", "H. Yu"],
    "venue": "Transportation Research Part C Emerging Technologies",
    "year": 2015
  }, {
    "title": "Dynamic prediction of traffic volume through Kalman filtering theory",
    "authors": ["I. Okutani", "Y.J. Stephanedes"],
    "venue": "Transportation Research Part B",
    "year": 1984
  }, {
    "title": "Deep learning for short-term traffic flow prediction",
    "authors": ["N.G. Polson", "V.O. Sokolov"],
    "venue": "Transportation Research Part C Emerging Technologies",
    "year": 2017
  }, {
    "title": "A Survey on Intelligent Transportation Systems",
    "authors": ["K.N. Qureshi", "A.H. Abdullah"],
    "venue": "Middle East Journal of Scientific Research",
    "year": 2013
  }, {
    "title": "Learning representations by backpropagating errors",
    "authors": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"],
    "year": 1988
  }, {
    "title": "Variational Inference for Infinite Mixtures of Gaussian Processes With Applications to Traffic Flow Prediction",
    "authors": ["S. Sun", "X. Xu"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems",
    "year": 2011
  }, {
    "title": "Least Squares Support Vector Machine Classifiers",
    "authors": ["J.A.K. Suykens", "J. Vandewalle"],
    "venue": "Neural Processing Letters",
    "year": 1999
  }, {
    "title": "Shortâ€term traffic forecasting: Overview of objectives and methods",
    "authors": ["E.I. Vlahogianni", "J.C. Golias", "M.G. Karlaftis"],
    "venue": "Transport Reviews",
    "year": 2004
  }, {
    "title": "Optimized and meta-optimized neural networks for short-term traffic flow prediction: A genetic approach",
    "authors": ["E.I. Vlahogianni", "M.G. Karlaftis", "J.C. Golias"],
    "venue": "Transportation Research Part C",
    "year": 2005
  }, {
    "title": "Short-term traffic forecasting: Where we are and where weâ€™re going",
    "authors": ["E.I. Vlahogianni", "M.G. Karlaftis", "J.C. Golias"],
    "venue": "Transportation Research Part C Emerging Technologies",
    "year": 2014
  }, {
    "title": "Traffic Speed Prediction and Congestion Source Exploration: A Deep Learning Method",
    "authors": ["J. Wang", "Q. Gu", "J. Wu", "G. Liu", "Z. Xiong"],
    "venue": "IEEE International Conference on Data Mining,",
    "year": 2017
  }, {
    "title": "Modeling and Forecasting Vehicular Traffic Flow as a Seasonal ARIMA Process: Theoretical Basis and Empirical Results",
    "authors": ["B.M. Williams", "L.A. Hoel"],
    "venue": "Journal of Transportation Engineering",
    "year": 2003
  }, {
    "title": "Driving with knowledge from the physical world",
    "authors": ["J. Yuan", "Y. Zheng", "X. Xie", "G. Sun"],
    "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2011
  }, {
    "title": "An Improved K -nearest Neighbor Model for Short-term Traffic Flow Prediction",
    "authors": ["L. Zhang", "Q. Liu", "W. Yang", "N. Wei", "D. Dong"],
    "venue": "Procedia - Social and Behavioral Sciences",
    "year": 2013
  }],
  "id": "SP:db34cc2492bb49f099c0ab69a9d7dc5d090c0c2e",
  "authors": [{
    "name": "Zhengchao Zhang",
    "affiliations": []
  }, {
    "name": "Meng Li",
    "affiliations": []
  }, {
    "name": "Xi Lin",
    "affiliations": []
  }, {
    "name": "Yinhai Wang",
    "affiliations": []
  }, {
    "name": "Fang He",
    "affiliations": []
  }],
  "abstractText": "Multistep traffic forecasting on road networks is a crucial task in successful intelligent transportation system applications. To capture the complex non-stationary temporal dynamics and spatial dependency in multistep traffic-condition prediction, we propose a novel deep learning framework named attention graph convolutional sequence-to-sequence model (AGC-Seq2Seq). In the proposed deep learning framework, spatial and temporal dependencies are modeled through the Seq2Seq model and graph convolution network separately, and the attention mechanism along with a newly designed training method based on the Seq2Seq architecture is proposed to overcome the difficulty in multistep prediction and further capture the temporal heterogeneity of traffic pattern. We conduct numerical tests to compare AGC-Seq2Seq with other benchmark models using a real-world dataset. The results indicate that our model yields the best prediction performance in terms of various prediction error measures. Furthermore, the variation of spatiotemporal correlation of traffic conditions ï€ª Corresponding author. Email: fanghe@tsinghua.edu.cn. Preprint submitted to Elsevier (Transportation Research Part C) 2 under different perdition steps and road segments is revealed through sensitivity analyses.",
  "title": "Multistep Speed Prediction on Traffic Networks: A Graph Convolutional Sequence-to-Sequence Learning Approach with Attention Mechanism"
}