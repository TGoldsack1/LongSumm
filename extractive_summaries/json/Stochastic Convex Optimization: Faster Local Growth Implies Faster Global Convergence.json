{
  "sections": [{
    "text": "1Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA 2Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s)."
  }, {
    "heading": "1. Introduction",
    "text": "In this paper, we are interested in solving the following stochastic optimization problem:\nmin w∈K F (w) , Eξ[f(w; ξ)], (1)\nwhere ξ is a random variable, f(w; ξ) is a convex function of w, Eξ[·] is the expectation over ξ and K is a convex domain. We denote by ∂f(w; ξ) a subgradient of f(w; ξ). Let K∗ denote the optimal set of (1) and F∗ denote the optimal value.\nTraditional stochastic subgradient (SSG) method updates the solution according to\nwt+1 = ΠK[wt − ηt∂f(wt; ξt)], (2)\nfor t = 1, . . . , T , where ξt is a sampled value of ξ at t-th iteration, ηt is a step size and ΠK[w] = arg minv∈K ‖w − v‖22 is a projection operator that projects a point into K. Previous studies have shown that under the following assumptions i) ‖∂f(w; ξ)‖2 ≤ G, ii) there exists w∗ ∈ K∗ such that ‖wt −w∗‖2 ≤ B for t = 1, . . . , T 1, and by setting the step size ηt = BG√T in (2), with a high probability 1− δ we have\nF (ŵT )− F∗ ≤ O ( GB(1 + √ log(1/δ))/ √ T ) , (3) where ŵT = ∑T t=1 wt/T . The above convergence implies that in order to obtain an -optimal solution by SSG, i.e., finding a w such that F (w) − F∗ ≤ with a high probability 1 − δ, one needs at least T = O(G2B2(1 + √ log(1/δ))2/ 2) in the worst-case.\nIt is commonly known that the slow convergence of SSG is due to the variance in the stochastic subgradient and the non-smoothness nature of the problem as well, which therefore requires a decreasing step size or a very small step size. Recently, there emerges a stream of studies on various variance reduction techniques to accelerate stochastic gradient method (Roux et al., 2012; Zhang et al., 2013; Johnson & Zhang, 2013; Xiao & Zhang, 2014; Defazio et al.,\n1This holds if we assume the domain K is bounded such that maxw,v∈K ‖w − v‖2 ≤ B or if assume dist(w1,K∗) ≤ B/2 and project every solution wt into K ∩ B(w1, B/2).\n2014). However, they all hinge on the smoothness assumption. The proposed algorithms in this work tackle the issue of variance of stochastic subgradient without the smoothness assumption from another pespective.\nThe main motivation for addressing this problem is from a key observation: a high probability analysis of the SSG method shows that the variance term of the stochastic subgradient is accompanied by an upper bound of distance of intermediate solutions to the target solution. This observation has also been leveraged in previous analysis to design faster convergence for stochastic convex optimization that use a strong or uniform convexity condition (Hazan & Kale, 2011; Juditsky & Nesterov, 2014) or a global growth condition (Ramdas & Singh, 2013) to control the distance of intermediate solutions to the optimal solution by their functional residuals. However, we find these global assumptions are completely unnecessary, which may not only restrict their applications to a broad family of problems but also worsen the convergence rate due to the larger multiplicative growth constant that could be domain-size dependent. In contrast, we develop a new theory only relying on the local growth condition to control the distance of intermediate solutions to the -optimal solution by their functional residuals but achieving a fast global convergence.\nBesides the fundamental difference, the present work also possesses several unique algorithmic contributions compared with previous similar work on stochastic optimization: (i) we have two different ways to control the distance of intermediate solutions to the -optimal solution, one by explicitly imposing a bounded ball constraint and another one by implicitly regularizing the intermediate solutions, where the later one could be more efficient if the projection into the intersection of a bounded ball and the problem domain is complicated; (ii) we develop more practical variants that can be run without knowing the multiplicative growth constant though under a slightly stringent condition; (iii) for problems whose local growth rate is unknown we still develop an improved convergence result of the proposed algorithms comparing with the SSG method. In addition, the present work will demonstrate the improved results and practicability of the proposed algorithms for many problems in machine learning, which is lacking in similar previous work."
  }, {
    "heading": "2. Related Work",
    "text": "The most similar work to the present one is (Ramdas & Singh, 2013), which studied stochastic convex optimization under a global growth condition, which they called Tsybakov noise condition. One major difference from their result is that we achieve the same order of iteration complexity up to a logarithmic factor under only a local growth condition. As observed later on, the multiplicative growth con-\nstant in local growth condition is domain-size independent that is smaller than that in global growth condition, which could be domain-size dependent. Besides, the stochastic optimization algorithm in (Ramdas & Singh, 2013) assume the optimization domain K is bounded, which is removed in this work. In addition, they do not address the issue when the multiplicative constant is unknown and lack study of applicability for machine learning problems. Juditsky & Nesterov (2014) presented primal-dual subgradient and stochastic subgradient methods for solving problems under the uniform convexity assumption (see the definition under Observation 1). As exhibited shortly, the uniform convexity condition covers only a smaller family of problems than the considered local growth condition. However, when the problem is uniform convex, the iteration complexity obtained in this work resembles that in (Juditsky & Nesterov, 2014).\nRecently, there emerge a wave of studies that attempt to improve the convergence of existing algorithms under no strong convexity assumption by considering certain weaker conditions than strong convexity (Necoara et al., 2015; Liu et al., 2015; Zhang & Yin, 2013; Liu & Wright, 2015; Gong & Ye, 2014; Karimi et al., 2016; Zhang, 2016; Qu et al., 2016; Wang & Lin, 2014). Several recent works (Necoara et al., 2015; Karimi et al., 2016; Zhang, 2016) have unified many of these conditions, implying that they are a kind of global growth condition with θ = 1/2. Unlike the present work, most of these developments require certain smoothness assumption except (Qu et al., 2016).\nLuo & Tseng (1992a;b; 1993) pioneered the idea of using local error bound condition to show faster convergence of gradient descent, proximal gradient descent, and many other methods for a family of structured composite problems (e.g., the LASSO problem). Many follow-up works (Hou et al., 2013; Zhou et al., 2015; Zhou & So, 2015) have considered different regularizers (e.g., `1,2 regularizer, nuclear norm regularizer). However, these works only obtained asymptotically faster (i.e., linear) convergence and they hinge on the smoothness on some parts of the problem. Yang & Lin (2016); Xu et al. (2016) have considered the same local growth condition (aka local error bound condition in their work) for developing faster deterministic algorithms for non-smooth optimization. However, they did not address the problem of stochastic convex optimization, which restricts their applicability to largescale problems in machine learning.\nFinally, we note that the improved iteration complexity in this paper does not contradict to the lower bound in (Nemirovsky A.S. & Yudin, 1983; Nesterov, 2004). The bad examples constructed to derive the lower bound for general non-smooth optimization do not satisfy the assumptions made in this work (in particular Assumption 1(b))."
  }, {
    "heading": "3. Preliminaries",
    "text": "Recall the notations K∗ and F∗ that denote the optimal set of (1) and the optimal value, respectively. For the optimization problem in (1), we make the following assumption throughout the paper.\nAssumption 1. For a stochastic optimization problem (1), we assume\n(a) there exist w0 ∈ K and 0 ≥ 0 such that F (w0) − F∗ ≤ 0; (b) K∗ is a non-empty compact set; (c) There exists a constantG such that ‖∂f(w; ξ)‖2 ≤ G.\nRemark: (a) essentially assumes the availability of a lower bound of the optimal objective value, which usually holds for machine learning problems (due to non-negativeness of the objective function). (b) simply assumes the optimal set is closed and bounded. This is a relaxed condition in contrast with most previous work that assume the domain K is bounded. Even if K is unbounded, as long as the function is a proper lower-semicontinuous convex and coercive function defined on a finite dimensional space, K∗ is nonempty and compact (Bolte et al., 2015). Note that any norm-regularized loss function minimization problem on a finite dimensional space in machine learning satisfy this property. (c) is a standard assumption also made in many previous stochastic gradient-based methods. By Jensen’s inequality, we also have ‖∂F (w)‖2 ≤ G.\nFor any w ∈ K, let w∗ denote the closest optimal solution in K∗ to w, i.e., w∗ = arg minv∈K∗ ‖v − w‖22, which is unique. We denote by L the -level set of F (w) and by S the -sublevel set of F (w), respectively, i.e., L = {w ∈ K : F (w) = F∗ + }, S = {w ∈ K : F (w) ≤ F∗ + }. Given K∗ is bounded, it follows from (Rockafellar, 1970, Corollary 8.7.1) that the sublevel set S is bounded for any ≥ 0 and so as the level set L . Let w† denote the closest point in the -sublevel set to w, i.e.,\nw† = arg min v∈S ‖v −w‖22. (4)\nIt is easy to show that w† ∈ L when w /∈ S (using the KKT condition). Let B(w, r) = {u ∈ Rd : ‖u−w‖2 ≤ r} denote an Euclidean ball centered at w with a radius r. Denote by dist(w,K∗) = minv∈K∗ ‖w − v‖2 the distance between w and the set K∗, by ∂0F (w) the projection of 0 onto the nonempty closed convex set ∂F (w), i.e., ‖∂0F (w)‖2 = minv∈∂F (w) ‖v‖2."
  }, {
    "heading": "3.1. Functional Local Growth Rate",
    "text": "We quantify the functional local growth rate by measuring how fast the functional value increase when moving a point away from the optimal solution in the -sublevel set. In particular, a function F (w) has a local growth rate θ ∈\n(0, 1] in the -sublevel set ( 1) if there exists a constant λ > 0 such that:\nλ‖w −w∗‖1/θ2 ≤ F (w)− F∗, ∀w ∈ S , (5)\nwhere w∗ is the closest solution in the optimal set K∗ to w. Note that the local growth rate θ is at most 1. This is due to that F (w) is G-Lipschitz continuous and limw→w∗ ‖w − w∗‖1−α2 = 0 if α < 1. The inequality in (5) can be equivalently written as\n‖w −w∗‖2 ≤ c(F (w)− F∗)θ, ∀w ∈ S , (6)\nwhere c = 1/λθ, which is called as local error bound condition in (Yang & Lin, 2016). In this work, to avoid confusion with earlier work by Luo & Tseng (1992a;b; 1993) who also explored a related but different local error bound condition, we refer to the inequality in (5) or (6) as local growth condition (LGC). If the function F (x) is assumed to satisfy (5) for all w ∈ K, it is referred to as global growth condition (GGC). Note that since we do not assume a bounded K, the GGC might be ill posed. In the following discussions, when compared with GGC we simply assume the domain is bounded.\nBelow, we present several observations mostly from existing work to clarify the relationship between the LGC (6) and previous conditions, and also justify our choice of LGC that covers a much broader family of functions than previous conditions and induces a smaller multiplicative growth constant c than that induced by GGC. Observation 1. Strong convexity or uniform convexity condition implies LGC with θ = 1/2, but not vice versa. F (w) is said to satisfy a uniform convexity condition on K with convexity parameters p ≥ 2 and µ if:\nF (u) ≥ F (v) + ∂F (v)>(u− v) + µ‖u− v‖ p 2\n2 ,∀u,v ∈ K.\nIf we let u = w, v = w∗, and ∂F (v) = 0, we have (5) with θ = 1/p ∈ (0, 1/2]. Clearly LGC covers a broader family of functions than uniform convexity. Observation 2. The weak strong convexity (Necoara et al., 2015), essential strong convexity (Liu et al., 2015), restricted strong convexity (Zhang & Yin, 2013), optimal strong convexity (Liu & Wright, 2015), semi-strong convexity (Gong & Ye, 2014) and other error bound conditions considered in several recent work (Karimi et al., 2016; Zhang, 2016) imply a GGC on the entire optimization domain K with θ = 1/2 for a convex function. Some of these conditions are also equivalent to the GGC with θ = 1/2. We refer the reader to (Necoara et al., 2015), (Karimi et al., 2016) and (Zhang, 2016) for more discussions of these conditions.\nThe third observation shows that LGC could imply faster convergence than that induced by GGC.\nObservation 3. The LGC could induce a smaller constant c in (6) that is domain-size independent than that induced by the GGC on the entire optimization domain K.\nTo illustrate this, we consider a function f(x) = x2 if |x| ≤ 1 and f(x) = |x| if 1 < |x| ≤ s, where s specifies the size of the domain. In the -sublevel set ( < 1), the LGC (6) holds with θ = 1/2 and c = 1. In order to make the inequality |x| ≤ cf(x)1/2 hold for all x ∈ [−s, s], we can see that c = max|x|≤s |x| f(x)1/2 = max|x|≤s √ |x| = √ s. As a result, GGC induces a larger c that depends on the domain size.\nThe next observation shows that Luo-Tseng’s local error bound condition is closely related to the LGC with θ = 1/2. To this end, we first give the definition of Luo-Tseng’s local error bound condition. Let F (w) = h(w) + P (w), where h(w) is a proper closed function with an open domain containing K and is continuously differentiable with a locally Lipschitz continuous gradient on any compact set within dom(h) and P (w) is a proper closed convex function. Such a function F (w) is said to satisfy Luo-Tseng’s local error bound if for any ζ > 0, there exists c, ε > 0 so that ‖w−w∗‖2 ≤ c‖proxP (w−∇h(w))−w‖2, whenever ‖proxP (w − ∇h(w)) − w‖2 ≤ ε and F (w) − F∗ ≤ ζ, where proxP (w) = arg minu∈K 1 2‖u−w‖ 2 2 + P (w).\nObservation 4. If F (w) = h(w)+P (w) is defined above and satisfies the Luo-Tseng’s local error bound condition, it then implies that there exists a sufficiently small ′ > 0 and C > 0 such that ‖w−w∗‖2 ≤ C(F (w)− F∗)1/2 for any w ∈ B(w∗, ′).\nThis observation was established in (Li & Pong, 2016, Theorem 4.1). Note that the LGC condition with = G ′ and θ = 1/2 also implies that ‖w−w∗‖2 ≤ C(F (w)−F∗)1/2 for any w ∈ B(w∗, ′). Nonetheless, Luo-Tseng’s local error bound imposes some smoothness assumption on h(w).\nThe last observation is that the LGC is equivalent to a Kurdyka - Łojasiewicz inequality (KL), which was proved in (Bolte et al., 2015, Theorem 5).\nObservation 5. If F (w) satisfies a KL inequality, i.e., ϕ′(F (w)−F∗)‖∂0F (w)‖2 ≥ 1 for w ∈ {x ∈ K, F (x)− F∗ < } with ϕ(s) = csθ, then LGC (6) holds, and vice versa.\nThe above KL inequality has been established for continuous semi-algebraic and subanalytic functions (Attouch et al., 2013; Bolte et al., 2006; 2015), which cover a broad family of functions therefore justifying the generality of the LGC.\nFinally, we present a key lemma that can leverage the LGC to control the distance of intermediate solutions to an - optimal solution.\nLemma 1. For any w ∈ K and > 0, we have\n‖w −w† ‖2 ≤ dist(w† ,K∗) (F (w)− F (w† )),\nwhere w† ∈ S is the closest point in the -sublevel set to w as defined in (4). Remark: In view of LGC, we can see that ‖w −w† ‖2 ≤ c\n1−θ (F (w)−F (w† )) for any w ∈ K. Yang & Lin (2016) have leveraged this relationship to improve the convergence of the standard subgradient method. In the sequel, we will build on this relationship to further develop novel stochastic optimization algorithms with faster convergence in high probability."
  }, {
    "heading": "4. Main Results",
    "text": "In this section, we will present the proposed accelerated stochastic subgradient (ASSG) methods and establish their improved iteration complexity with a high probability. The key to our development is to control the distance of intermediate solutions to the -optimal solution by their functional residuals that are decreasing as the solutions approach the optimal set. It is this decreasing factor that help mitigate the non-vanishing variance issue in the stochastic subgradient. To formally illustrate this, we consider the following stochastic subgradient update:\nwτ+1 = ΠK∩B(w1,D)[wτ − η∇f(wτ ; ξτ )]. (7)\nLemma 2. Given w1 ∈ K, apply t-iterations of (7). For any fixed w ∈ K ∩ B(w1, D) and δ ∈ (0, 1), with a probability at least 1− δ, the following inequality holds\nF (ŵt)− F (w) ≤ ηG2 2 + ‖w1 −w‖22 2ηt +\n4GD √\n3 log(1δ )√ t ,\nwhere ŵt = ∑t τ=1 wt/t.\nRemark: The proof of the above lemma follows similarly as that of Lemma 10 in (Hazan & Kale, 2011). We note that the last term is due to the variance of the stochastic subgradients. In fact, due to the non-smoothness nature of the problem the variance of the stochastic subgradients cannot be reduced, we therefore propose to address this issue by reducing D in light of the inequality in Lemma 1.\nThe updates in (7) can be also understood as approximately solving the original problem in the neighborhood of w1. In light of this, we will also develop a regularized variant of the proposed method. In the sequel, all omitted proofs can be found in the supplement."
  }, {
    "heading": "4.1. Accelerated Stochastic Subgradient Method: the Constrained variant (ASSG-c)",
    "text": "In this subsection, we present the constrained variant of ASSG that iteratively solves the original problem approx-\nimately in an explicitly constructed local neighborhood of the recent historical solution. The detailed steps are presented in Algorithm 1. We refer to this variant as ASSG-c. The algorithm runs in stages and each stage runs t iterations of updates similar to (7). Thanks to Lemma 1, we gradually decrease the radius Dk in a stage-wise manner. The step size keeps the same during each stage and geometrically decreases between stages. We notice that ASSG-c is similar to the Epoch-GD method by Hazan & Kale (2011) and the (multi-stage) AC-SA method with domain shrinkage by Ghadimi & Lan (2013) for stochastic strongly convex optimization. However, the difference between ASSG and Epoch-GD/AC-SA lies at the initial radius D1 and the number of iterations per-stage, which is due to difference between the strong convexity assumption and Lemma 1. The convergence of ASSG-c is presented in the theorem below.\nTheorem 1. Suppose Assumption 1 holds and F (w) obeys the LGC (6). Given δ ∈ (0, 1), let δ̃ = δ/K, K = dlog2( 0 )e, D1 ≥ c 0 1−θ and t be the smallest integer such that t ≥ max{9, 1728 log(1/δ̃)}G 2D21 20\n. Then ASSG-c guarantees that, with a probability 1− δ, F (wK)− F∗ ≤ 2 . As a result, the iteration complexity of ASSG-c for achieving an 2 -optimal solution with a high probability 1− δ is O(c2G2dlog2( 0 )e log(1/δ)/\n2(1−θ)) provided D1 = O( c 0 (1−θ) ).\nRemark: It is notable that the faster local growth rate θ implies the faster global convergence, i.e., lower iteration complexity. In light of the lower bound presented in (Ramdas & Singh, 2013) under a GGC, our iteration complexity under the LGC is optimal up to at most a logarithmic factor. It is worth mentioning that unlike traditional highprobability analysis of SSG that usually requires the domain to be bounded, the convergence analysis of ASSG does not rely on such a condition. Furthermore, the iteration complexity of ASSG has a better dependence on the quality of the initial solution or the size of domain if it is bounded. In particular, if we let 0 = GB assuming dist(w0,K∗) ≤ B, though this is not necessary in practice, then the iteration complexity of ASSG has only a logarithmic dependence on the distance of the initial solution to the optimal set, while that of SSG has a quadratic dependence on this distance. The above theorem requires a target precision in order to set D1. In subsection 4.3, we alleviate this requirement to make the algorithm more practical."
  }, {
    "heading": "4.2. Accelerated Stochastic Subgradient Method: the Regularized variant (ASSG-r)",
    "text": "One potential issue of ASSG-c is that the projection into the intersection of the problem domain and an Euclidean ball might increase the computational cost per-iteration depending on the problem domain K. To address this issue, we\nAlgorithm 1 ASSG-c(w0,K, t,D1, 0) 1: Input: w0 ∈ K, K, t, 0 and D1 ≥ c 0 1−θ 2: Set η1 = 0/(3G2) 3: for k = 1, . . . ,K do 4: Let wk1 = wk−1 5: for τ = 1, . . . , t− 1 do 6: wkτ+1 = ΠK∩B(wk−1,Dk)[w k τ − ηk∂f(wkτ ; ξkτ )]\n7: end for 8: Let wk = 1t ∑t τ=1 w k τ\n9: Let ηk+1 = ηk/2 and Dk+1 = Dk/2. 10: end for 11: Output: wK present a regularized variant of ASSG. Before delving into the details of ASSG-r, we first present a common strategy that solves the non-strongly convex problem (1) by stochastic strongly convex optimization. The basic idea is from the classical deterministic proximal point algorithm (Rockafellar, 1976) which adds a strongly convex regularizer to the original problem and solve the resulting proximal problem. In particular, we construct a new problem\nmin w∈K\nF̂ (w) = F (w) + 1\n2β ‖w −w1‖22, (8)\nwhere w1 ∈ K is called the regularization reference point. Let ŵ∗ denote the optimal solution to the above problem given w1. It is easy to know F̂ (w) is a 1β -strongly convex function on K. We can employ the stochastic subgradient method suited for strongly convex problems to solve the above problem. The update is given by\nwt+1 = ΠK[w ′ t+1] = arg min\nw∈K ∥∥w −w′t+1∥∥22 , (9) where w′t+1 = wt − ηt(∂f(wt; ξt) + 1β (wt − w1)), and ηt = 2β t\n2. We present a lemma below to bound ‖ŵ∗ − wt‖2 and ‖wt −w1‖2 by the above update, which will be used in the proof of convergence of ASSG-r for solving (1). Lemma 3. For any t ≥ 1, we have ‖ŵ∗ − wt‖2 ≤ 3βG and ‖wt −w1‖2 ≤ 2βG.\nRemark: The lemma implies that the regularization term implicitly imposes a constraint on the intermediate solutions to center around the regularization reference point, which achieves a similar effect as the ball constraint in Algorithm 1.\nRecall that the main iteration of the proximal point algorithm (Rockafellar, 1976) is\nwk ≈ arg min w∈K\nF (w) + 1\n2βk ‖w −wk−1‖22, (10)\nwhere wk approximately solves the minimization problem above with βk changing with k. With the same idea, our\n2The factor 2 in the step size is used for proving the high probability convergence.\nAlgorithm 2 the ASSG-r algorithm for solving (1)\n1: Input: w0 ∈ K, K, t, 0 and β1 ≥ 2c 2 0 2(1−θ) 2: for k = 1, . . . ,K do 3: Let wk1 = wk−1 4: for τ = 1, . . . , t− 1 do 5: Let w′τ+1 = ( 1− 2τ ) wkτ + 2 τw k 1− 2β τ ∂f(w k τ ; ξ k τ ) 6: Let wkτ+1 = ΠK(w ′ τ+1) 7: end for 8: Let wk = 1t ∑t τ=1 w k τ , and βk+1 = βk/2\n9: end for 10: Output: wK\nregularized variant of ASSG generates wk from stage k by solving the minimization problem (10) approximately using (9). The detailed steps are presented in Algorithm 2, which starts from a relatively large value of the parameter β = β1 and gradually decreases β by a constant factor after running a number of t iterations (9) using the solution from the previous stage as the new regularization reference point. Despite of its similarity to the proximal point algorithm, ASSG-r incorporates the LGC into the choices of βk and the number of iterations per-stage and obtains new iteration complexity described below.\nTheorem 2. Suppose Assumption 1 holds and F (w) obeys the LGC (6). Given δ ∈ (0, 1/e), let δ̃ = δ/K, K = dlog2( 0 )e, β1 ≥ 2c2 0 2(1−θ) and t be the smallest integer such that t ≥ max{3, 136β1G 2(1+log(4 log t/δ̃)+log t)\n0 }.\nThen ASSG-r guarantees that, with a probability 1 − δ, F (wK) − F∗ ≤ 2 . As a result, the iteration complexity of ASSG-r for achieving an 2 -optimal solution with a high probability 1 − δ is O(c2G2 log( 0/ ) log(1/δ)/ 2(1−θ)) provided β1 = O( 2c 2 0 2(1−θ) )."
  }, {
    "heading": "4.3. More Practical Variants of ASSG",
    "text": "Readers may have noticed that the presented algorithms require appropriately setting up the initial values of D1 or β1 that depend on unknown c and potentially unknown θ. This subsection is devoted to more practical variants of ASSG. For ease of presentation, we focus on the constrained variant of ASSG.\nWhen c is known, we present the details of a restarting variant of ASSG in Algorithm 3, to which we refer as RASSG. The key idea is to use an increasing sequence of t and another level of restarting for ASSG.\nTheorem 3 (RASSG with unknown c). Let ≤ 0/4, ω = 1, and K = dlog2( 0 )e in Algorithm 3. Suppose D\n(1) 1 is sufficiently large so that there exists ̂1 ∈ [ , 0/2], with which F (·) satisfies a LGC (6) on Ŝ1 with θ ∈ (0, 1) and the constant c, and D(1)1 =\nc 0 ̂1−θ1 . Let δ̂ = δK(K+1) , and t1 = max{9, 1728 log(1/δ̂)} ( GD (1) 1 / 0 )2 . Then\nAlgorithm 3 ASSG with Restarting: RASSG\n1: Input: w(0), K, D(1)1 , t1, 0 and ω ∈ (0, 1] 2: Set (1)0 = 0, η1 = 0/(3G\n2) 3: for s = 1, 2, . . . , S do 4: Let w(s) =ASSG-c(w(s−1),K, ts, D (s) 1 , (s) 0 ) 5: Let ts+1 = ts22(1−θ), D (s+1) 1 = D (s) 1 2\n1−θ, and (s+1) 0 = ω (s) 0\n6: end for 7: Output: w(S)\nwith at most S = dlog2(̂1/ )e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S))−F∗ ≤ 2 . The total number of iterations of RASSG for obtaining 2 -optimal solution is upper bounded by TS = O(dlog2( 0 )e log(1/δ)/ 2(1−θ)).\nRemark: The above theorem requires a slightly stringent LGC condition on Ŝ1 that is induced by the initial value of D1. If the problem satisfies the LGC with θ = 1, we can give a slightly smaller value for θ in order to run Algorithm 3. If the target precision is not specified, we can give it a sufficiently small value ′ (e.g., the machine precision) that only affects K marginally. The corresponding iteration complexity for achieving an -optimal solution is given by O(dlog2( 0 ′ )e log(1/δ)/\n2(1−θ)). The parameter ω ∈ (0, 1] is introduced to increase the practical performance of RASSG, which accounts for decrease of the objective gap of the initial solutions for each call of ASSG-c.\nWhen θ is unknown, we can set θ = 0 and c = Bε with ε ≥ in the LGC (6), where Bε = maxw∈Lε minv∈K∗ ‖w − v‖2 is the maximum distance between the points in the εlevel set Lε and the optimal setK∗. The following theorem states the convergence result.\nTheorem 4 (RASSG with unknown θ). Let θ = 0, ≤ 0/4 , ω = 1, andK = dlog2( 0 )e in Algorithm 3. Assume D\n(1) 1 is sufficiently large so that there exists ̂1 ∈ [ , 0/2]\nrendering that D(1)1 = B̂1 0 ̂1 . Let δ̂ = δK(K+1) , and t1 = max{9, 1728 log(1/δ̂)} ( GD (1) 1 / 0 )2 . Then with at most S = dlog2(̂1/ )e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S)) − F∗ ≤ 2 . The total number of iterations of RASSG for obtaining 2 -optimal solution is upper bounded by TS = O(dlog2( 0 )e log(1/δ) G2B2̂1 2 ).\nRemark: The Lemma 6 in the supplement shows that B is a monotonically decreasing function in terms of , which guarantees the existence of ̂1 given a sufficiently large D\n(1) 1 . The iteration complexity of RASSG could be still better with a smaller factor B̂1 than the B in the iteration complexity of SSG (see (3)), whereB is the domain size or the distance of initial solution to the optimal set."
  }, {
    "heading": "5. Applications in Risk Minimization",
    "text": "In this section, we present some applications of the proposed ASSG to risk minimization in machine learning. Let (xi, yi), i = 1, . . . , n denote a set of pairs of feature vectors and labels that follow a distributionP , where xi ∈ X ⊂ Rd and yi ∈ Y . Many machine learning problems end up solving the regularized empirical risk minimization problem:\nmin w∈Rd\nF (w) = 1\nn n∑ i=1 `(w>xi, yi) + λR(w), (11)\nwhere R(w) is a regularizer, λ is the regularization parameter and `(z, y) is a loss function. Below we will present several examples in machine learning that enjoy faster convergence by the proposed ASSG than by SSG."
  }, {
    "heading": "5.1. Piecewise Linear Minimization",
    "text": "First, we consider some examples of non-smooth and nonstrongly convex problems such that ASSG can achieve linear convergence. In particular, we consider the problem (11) with a piecewise linear loss and `1, `∞ or `1,∞ regularizers.\nPiecewise linear loss includes hinge loss, generalized hinge loss, absolute loss, and -insensitive loss. For particular forms of these loss functions, please refer to (Yang et al., 2014). The epigraph of F (w) defined by sum of a piecewise linear loss function and an `1, `∞ or `1,∞ norm regularizer is a polyhedron. According to the polyhedral error bound condition (Yang & Lin, 2016), for any > 0 there exists a constant 0 < c < ∞ such that dist(w,K∗) ≤ c(F (w)− F∗) for any w ∈ S , meaning that the proposed ASSG has an O(log( 0/ )) iteration complexity for solving such family of problems. Formally, we state the result in the following corollary. Corollary 5. Assume the loss function `(z, y) is piecewise linear, then the problem in (11) with `1, `∞ or `1,∞ norm regularizer satisfy the LGC in (6) with θ = 1. Hence ASSG can have an iteration complexity of O(log(1/δ) log( 0/ )) with a high probability 1− δ."
  }, {
    "heading": "5.2. Piecewise Convex Quadratic Minimization",
    "text": "Next, we consider some examples of piecewise quadratic minimization problems in machine learning and show that ASSG enjoys an iteration complexity of Õ ( 1 ) . We first give an definition of piecewise convex quadratic functions, which is from (Li, 2013). A function g(w) is a real polynomial if there exists k ∈ N+ such that g(w) =∑\n0≤|αj |≤k λj ∏d i=1 w αji i , where λj ∈ R and α j i ∈ N+ ∪\n{0}, αj = (αj1, . . . , α j d), and |αj | = ∑d i=1 α j i . The constant k is called the degree of g. A continuous function F (w) is said to be a piecewise convex polynomial if there exist finitely many polyhedra P1, . . . , Pm with ∪mj=1Pj = Rd such that the restriction of F on each Pj is a convex\npolynomial. Let Fj be the restriction of F on Pj . The degree of a piecewise convex polynomial function F is the maximum of the degree of each Fj . If the degree is 2, the function is referred to as a piecewise convex quadratic function. Note that a piecewise convex quadratic function is not necessarily a smooth function nor a convex function (Li, 2013).\nFor examples of piecewise convex quadratic problems in machine learning, one can consider the problem (11) with a huber loss, squared hinge loss or square loss, and `1, `∞, `1,∞, or huber norm regularizer (Zadorozhnyi et al., 2016). The Huber function is defined as `γ(z) ={\n1 2z 2 if |z| ≤ γ, γ(|z| − 12γ) otherwise, , which is a piecewise convex quadratic function. The huber loss function `(z, y) = `γ(z − y) has been used for robust regression. A Huber regularizer is defined as R(w) = ∑d i=1 `γ(wi).\nIt has been shown that (Li, 2013), if F (w) is convex and piecewise convex quadratic, then it satisfies the LGC (6) with θ = 1/2. The corollary below summarizes the iteration complexity of ASSG for solving these problems. Corollary 6. Assume the loss function `(z, y) is a convex and piecewise convex quadratic, then the problem in (11) with `1, `∞, `1,∞ or huber norm regularizer satisfy the LGC in (6) with θ = 1/2. Hence ASSG can have an iteration complexity of Õ( log(1/δ) ) with a high probability 1− δ."
  }, {
    "heading": "5.3. Structured composite non-smooth problems",
    "text": "Next, we present a corollary of our main result regarding the following structured problem:\nmin w∈Rd F (w) , h(Xw) + P (w). (12)\nCorollary 7. Assume h(u) is a strongly convex function on any compact set and P (w) is polyhedral, then the problem in (12) satisfies the LGC in (6) with θ = 1/2. Hence ASSG can have an iteration complexity of Õ( log(1/δ) ) with a high probability 1− δ. The proof of the first part of Corollary 7 can be found in (Yang & Lin, 2016). One example of h(u) is p-norm error (p ∈ (0, 1)), where h(u) = 1n ∑n i=1 |ui − yi|p. The local strong convexity of the p-norm error (p ∈ (1, 2)) is shown in (Goebel & Rockafellar, 2007).\nFinally, we give an example that satisfies the LGC with intermediate values θ ∈ (0, 1/2). We can consider an `1 constrained `p norm regression (Nyquist, 1983):\nmin ‖w‖1≤s\nF (w) , 1\nn n∑ i=1 (x>i w − yi)p, p ∈ 2N+.\nLiu & Yang (2016) have shown that the problem above satisfies the LGC in (6) with θ = 1p ."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we perform some experiments to demonstrate effectiveness of proposed algorithms. We use very large-scale datasets from libsvm website in experiments, including covtype.binary (n = 581012), real-sim (n = 72309), url (n = 2396130) for classification, million songs (n = 463715), E2006-tfidf (n = 16087), E2006-log1p (n = 16087) for regression. The detailed statistics of these datasets are shown in the supplement.\nWe first compare ASSG with SSG on three tasks: `1 norm regularized hinge loss minimization for linear classification, `1 norm regularized Huber loss minimization for linear regression, and `1 norm regularized p-norm robust regression with a loss function `(w>xi, yi) = |w>xi− yi|p. The regularization parameter λ is set to be 10−4 in all tasks (We also perform the experiments with λ = 10−2 and include the results in the supplement). We set γ = 1 in Huber loss and p = 1.5 in robust regression. In all experiments, we use the constrained variant of ASSG, i.e., ASSG-c. For fairness, we use the same initial solution with all zero entries for all algorithms. We use a decreasing step size proportional to 1/ √ τ (τ is the iteration index) in SSG. The initial step size of SSG is tuned in a wide range to obtain the fastest convergence. The step size of ASSG in the first stage is also tuned around the best initial step size of SSG. The value of D1 in both ASSG and RASSG is set to 100 for all problems. In implementing the RASSG, we restart every 5 stages with t increased by a factor of 1.15, 2 and 2 respectively for hinge loss, Huber loss and robust regression. We tune the parameter ω among {0.3, 0.6, 0.9, 1}. We report the results of ASSG with a fixed number of iterations per-stage t and RASSG with an increasing sequence of t. The results are plotted in Figure 1 (first 6 figures), in which we plot the log difference between the objective value and the smallest obtained objective value (to which we refer as objective gap) versus number of iterations. The\nfigures show that (i) ASSG can quickly converge to a certain level set determined implicitly by t; (ii) RASSG converges much faster than SSG to more accurate solutions; (iii) RASSG can gradually decrease the objective value.\nFinally, we compare RASSG with state-of-art stochastic optimization algorithms for solving a finite-sum problem with a smooth piecewise quadratic loss (e.g., squared hinge loss, huber loss) and an `1 norm regularization. In particular, we compare with SAGA (Defazio et al., 2014) and SVRG++ (Allen-Zhu & Yuan, 2016). We conduct experiments on two high-dimensional datasets url and E2006log1p and fix the regularization parameter λ = 10−4 (We also include the results for λ = 10−2 in the supplement). We use δ = 1 in Huber loss. For RASSG, we start from D1 = 100 and t1 = 103, then restart it every 5 stages with t increased by a factor of 2. We tune the initial step sizes for all algorithms in a wide range and set the values of parameters in SVRG++ followed by (Allen-Zhu & Yuan, 2016). We plot the objective versus the CPU time (second) in Figure 1 (last 2 figures). The results show that RASSG converges faster than other three algorithms for the two tasks. This is not surprising considering that RASSG, SAGA and SVRG++ suffer from an iteration complexity of Õ(1/ ), O(n/ ), and O(n log(1/ ) + 1/ ), respectively."
  }, {
    "heading": "7. Conclusion",
    "text": "In this paper, we have proposed accelerated stochastic subgradient methods for solving general non-strongly convex stochastic optimization under the functional local growth condition. The proposed methods enjoy a lower iteration complexity than vanilla stochastic subgradient method and also a logarithmic dependence on the impact of the initial solution. We have also made an extension by developing a more practical variant. Applications in machine learning have demonstrated the faster convergence of the proposed methods."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). T. Yang would like to thank Lijun Zhang for pointing out (Kakade & Tewari, 2008) for his attention."
  }],
  "year": 2017,
  "references": [{
    "title": "Improved svrg for non-strongly-convex or sum-of-non-convex objectives",
    "authors": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods",
    "authors": ["Attouch", "Hedy", "Bolte", "Jérôme", "Svaiter", "Benar Fux"],
    "venue": "Math. Program.,",
    "year": 2013
  }, {
    "title": "The łojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems",
    "authors": ["Bolte", "Jérôme", "Daniilidis", "Aris", "Lewis", "Adrian"],
    "venue": "SIAM J. on Optimization,",
    "year": 2006
  }, {
    "title": "From error bounds to the complexity of first-order descent methods for convex functions",
    "authors": ["Bolte", "Jérôme", "Nguyen", "Trong Phong", "Peypouquet", "Juan", "Suter", "Bruce"],
    "venue": "CoRR, abs/1510.08234,",
    "year": 2015
  }, {
    "title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
    "authors": ["Defazio", "Aaron", "Bach", "Francis R", "Lacoste-Julien", "Simon"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: Shrinking procedures and optimal algorithms",
    "authors": ["Ghadimi", "Saeed", "Lan", "Guanghui"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Local strong convexity and local lipschitz continuity of the gradient of convex functions",
    "authors": ["R. Goebel", "R.T. Rockafellar"],
    "venue": "Journal of Convex Analysis,",
    "year": 2007
  }, {
    "title": "Linear convergence of variance-reduced projected stochastic gradient without strong convexity",
    "authors": ["Gong", "Pinghua", "Ye", "Jieping"],
    "venue": "CoRR, abs/1406.1102,",
    "year": 2014
  }, {
    "title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
    "authors": ["Hazan", "Elad", "Kale", "Satyen"],
    "venue": "In COLT, pp",
    "year": 2011
  }, {
    "title": "On the linear convergence of the proximal gradient method for trace norm regularization",
    "authors": ["Hou", "Ke", "Zhou", "Zirui", "So", "Anthony Man-Cho", "Luo", "Zhi-Quan"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Deterministic and stochastic primal-dual subgradient algorithms for uniformly convex minimization",
    "authors": ["Juditsky", "Anatoli", "Nesterov", "Yuri"],
    "venue": "Stoch. Syst.,",
    "year": 2014
  }, {
    "title": "On the generalization ability of online strongly convex programming algorithms",
    "authors": ["Kakade", "Sham M", "Tewari", "Ambuj"],
    "venue": "In NIPS, pp",
    "year": 2008
  }, {
    "title": "Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition",
    "authors": ["Karimi", "Hamed", "Nutini", "Julie", "Schmidt", "Mark W"],
    "venue": "In ECMLPKDD,",
    "year": 2016
  }, {
    "title": "Global error bounds for piecewise convex polynomials",
    "authors": ["Li", "Guoyin"],
    "venue": "Math. Program.,",
    "year": 2013
  }, {
    "title": "Calculus of the exponent of kurdyka- łojasiewicz inequality and its applications to linear convergence of first-order methods",
    "authors": ["Li", "Guoyin", "Pong", "Ting Kei"],
    "year": 2016
  }, {
    "title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties",
    "authors": ["Liu", "Ji", "Wright", "Stephen J"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "An asynchronous parallel stochastic coordinate descent algorithm",
    "authors": ["Liu", "Ji", "Wright", "Stephen J", "Ré", "Christopher", "Bittorf", "Victor", "Sridhar", "Srikrishna"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2015
  }, {
    "title": "Adaptive accelerated gradient converging methods under holderian error bound condition",
    "authors": ["Liu", "Mingrui", "Yang", "Tianbao"],
    "year": 2016
  }, {
    "title": "On the convergence of coordinate descent method for convex differentiable minization",
    "authors": ["Luo", "Zhi-Quan", "Tseng", "Paul"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 1992
  }, {
    "title": "On the linear convergence of descent methods for convex essenially smooth minization",
    "authors": ["Luo", "Zhi-Quan", "Tseng", "Paul"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 1992
  }, {
    "title": "Error bounds and convergence analysis of feasible descent methods: a general approach",
    "authors": ["Luo", "Zhi-Quan", "Tseng", "Paul"],
    "venue": "Annals of Operations Research,",
    "year": 1993
  }, {
    "title": "Linear convergence of first order methods for non-strongly convex optimization",
    "authors": ["I. Necoara", "Nesterov", "Yu", "F. Glineur"],
    "venue": "CoRR, abs/1504.06298,",
    "year": 2015
  }, {
    "title": "Problem complexity and method efficiency in optimization. Wiley-Interscience series in discrete mathematics",
    "authors": ["Nemirovsky A.S", "Arkadii Semenovich", "D.B. Yudin"],
    "year": 1983
  }, {
    "title": "Introductory lectures on convex optimization : a basic course",
    "authors": ["Nesterov", "Yurii"],
    "venue": "Applied optimization. Kluwer Academic Publ.,",
    "year": 2004
  }, {
    "title": "The optimal lp norm estimator in linear regression models",
    "authors": ["H. Nyquist"],
    "venue": "Communications in Statistics - Theory and Methods,",
    "year": 1983
  }, {
    "title": "Fast rate analysis of some stochastic optimization algorithms",
    "authors": ["Qu", "Chao", "Xu", "Huan", "Ong", "Chong Jin"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Optimal rates for stochastic convex optimization under tsybakov noise condition",
    "authors": ["Ramdas", "Aaditya", "Singh", "Aarti"],
    "venue": "In ICML, pp",
    "year": 2013
  }, {
    "title": "Monotone operators and the proximal point algorithm",
    "authors": ["Rockafellar", "R. Tyrrell"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 1976
  }, {
    "title": "Convex Analysis. Princeton mathematical series",
    "authors": ["R.T. Rockafellar"],
    "year": 1970
  }, {
    "title": "A stochastic gradient method with an exponential convergence rate for finite training sets",
    "authors": ["Roux", "Nicolas Le", "Schmidt", "Mark W", "Bach", "Francis"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Iteration complexity of feasible descent methods for convex optimization",
    "authors": ["Wang", "Po-Wei", "Lin", "Chih-Jen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "A proximal stochastic gradient method with progressive variance reduction",
    "authors": ["Xiao", "Lin", "Zhang", "Tong"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "Homotopy smoothing for non-smooth problems with lower complexity than O(1",
    "authors": ["Xu", "Yi", "Yan", "Lin", "Qihang", "Yang", "Tianbao"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Rsg: Beating sgd without smoothness and/or strong convexity",
    "authors": ["Yang", "Tianbao", "Lin", "Qihang"],
    "year": 2016
  }, {
    "title": "An efficient primal-dual prox method for non-smooth optimization",
    "authors": ["Yang", "Tianbao", "Mahdavi", "Mehrdad", "Jin", "Rong", "Zhu", "Shenghuo"],
    "venue": "Machine Learning,",
    "year": 2014
  }, {
    "title": "Hubernorm regularization for linear prediction models",
    "authors": ["Zadorozhnyi", "Oleksandr", "Benecke", "Gunthard", "Mandt", "Stephan", "Scheffer", "Tobias", "Kloft", "Marius"],
    "venue": "In ECML-PKDD,",
    "year": 2016
  }, {
    "title": "New analysis of linear convergence of gradient-type methods via unifying error bound conditions",
    "authors": ["Zhang", "Hui"],
    "year": 2016
  }, {
    "title": "Gradient methods for convex minimization: better rates under weaker conditions",
    "authors": ["Zhang", "Hui", "Yin", "Wotao"],
    "venue": "CoRR, abs/1303.4645,",
    "year": 2013
  }, {
    "title": "Linear convergence with condition number independent access of full gradients",
    "authors": ["Zhang", "Lijun", "Mahdavi", "Mehrdad", "Jin", "Rong"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "A unified approach to error bounds for structured convex optimization problems",
    "authors": ["Zhou", "Zirui", "So", "Anthony Man-Cho"],
    "venue": "CoRR, abs/1512.03518,",
    "year": 2015
  }, {
    "title": "L1pnorm regularization: Error bounds and convergence rate analysis of first-order methods",
    "authors": ["Zhou", "Zirui", "Zhang", "Qi", "So", "Anthony Man-Cho"],
    "venue": "In ICML,",
    "year": 2015
  }],
  "id": "SP:890b11c6cd2317f78314f7bba625844bf6c1a693",
  "authors": [{
    "name": "Yi Xu",
    "affiliations": []
  }, {
    "name": "Qihang Lin",
    "affiliations": []
  }, {
    "name": "Tianbao Yang",
    "affiliations": []
  }],
  "abstractText": "In this paper, a new theory is developed for firstorder stochastic convex optimization, showing that the global convergence rate is sufficiently quantified by a local growth rate of the objective function in a neighborhood of the optimal solutions. In particular, if the objective function F (w) in the -sublevel set grows as fast as ‖w − w∗‖ 2 , where w∗ represents the closest optimal solution to w and θ ∈ (0, 1] quantifies the local growth rate, the iteration complexity of first-order stochastic optimization for achieving an -optimal solution can be Õ(1/ 2(1−θ)), which is optimal at most up to a logarithmic factor. To achieve the faster global convergence, we develop two different accelerated stochastic subgradient methods by iteratively solving the original problem approximately in a local region around a historical solution with the size of the local region gradually decreasing as the solution approaches the optimal set. Besides the theoretical improvements, this work also include new contributions towards making the proposed algorithms practical: (i) we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge of multiplicative growth constant and even the growth rate θ; (ii) we consider a broad family of problems in machine learning to demonstrate that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient method. For example, when applied to the `1 regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity. Department of Computer Science, The University of Iowa, Iowa City, IA 52242, USA Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).",
  "title": "Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence"
}