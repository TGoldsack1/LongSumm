{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In many real-world domains, data acquisition is costly. For instance, magnetic resonance imaging (MRI) requires scan times proportional to the number of measurements, which can be significant for patients (Lustig et al., 2008). Geophysical applications like oil drilling require expensive simulation of seismic waves (Qaisar et al., 2013). Such appli-\n1Computer Science Department, Stanford University, CA, USA. Correspondence to: Manik Dhar <dmanik@cs.stanford.edu>, Aditya Grover <adityag@cs.stanford.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ncations, among many others, can benefit significantly from compressed sensing techniques to acquire signals efficiently (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).\nIn compressed sensing, we wish to acquire an n-dimensional signal x ∈ Rn using only m n measurements linear in x. The measurements could potentially be noisy, but even in the absence of any noise we need to impose additional structure on the signal to guarantee unique recovery. Classical results on compressed sensing impose structure by assuming the underlying signal to be approximately l-sparse in some known basis, i.e., the l-largest entries dominate the rest. For instance, images and audio signals are typically sparse in the wavelet and Fourier basis respectively (Mallat, 2008). If the matrix of linear vectors relating the signal and measurements satisfies certain mild conditions, then one can provably recover x with only m = O(l log nl ) measurements using LASSO (Tibshirani, 1996; Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006; Bickel et al., 2009).\nAlternatively, structural assumptions on the signals being sensed can be learned from data, e.g., using a dataset of typical signals (Baraniuk et al., 2010; Peyre, 2010; Chen et al., 2010; Yu & Sapiro, 2011). Particularly relevant to this work, Bora et al. (2017) proposed an approach where structure is provided by a deep generative model learned from data. Specifically, the underlying signal x being sensed is assumed to be close to the range of a deterministic function expressed by a pretrained, latent variable modelG : Rk → Rn such that x ≈ G(z) where z ∈ Rk denote the latent variables. Consequently, the signal x is recovered by optimizing for a latent vector z that minimizes the `2 distance between the measurements corresponding to G(z) and the actual ones. Even though the objective being optimized in this case is non-convex, empirical results suggest that the reconstruction error decreases much faster than LASSO-based recovery as we increase the number of measurements.\nA limitation of the above approach is that the recovered signal is constrained to be in the range of the generator function G. Hence, if the true signal being sensed is not in the range of G, the algorithm cannot drive the reconstruction error to zero even when m ≥ n (even if we ignore error due to measurement noise and non-convex optimization). This is also observed empirically, as the reconstruction error of generative model-based recovery saturates as we keep\nincreasing the number of measurements m. On the other hand, LASSO-based recovery continues to shrink the error with increasing number of measurements, eventually outperforming the generative model-based recovery.\nTo overcome this limitation, we propose a framework that allows recovery of signals with sparse deviations from the set defined by the range of the generator function. The recovered signals have the general form of G(ẑ) + ν̂, where ν̂ ∈ Rn is a sparse vector. This allows the recovery algorithm to consider signals away from the range of the generator function. Similar to LASSO, we relax the hardness in optimizing for sparse vectors by minimizing the `1 norm of the deviations. Unlike LASSO-based recovery, we can exploit the rich structure imposed by a (deep) generative model (at the expense of solving a hard optimization problem if G is non-convex). In fact, we show that LASSO-based recovery is a special case of our framework if the generator function G maps all z to the origin. Unlike generative model-based recovery, the signals recovered by our algorithm are not constrained to be in the range of the generator function.\nOur proposed algorithm, referred to as Sparse-Gen, has desirable theoretical properties and empirical performance. Theoretically, we derive upper bounds on the reconstruction error for an optimal decoder with respect to the proposed model and show that this error vanishes with m = n measurements. We confirm our theory empirically, wherein we find that recovery using Sparse-Gen with variational autoencoders (Kingma & Welling, 2014) as the underlying generative model outperforms both LASSO-based and generative model-based recovery in terms of the reconstruction errors for the same number of measurements for MNIST and Omniglot datasets. Additionally, we observe significant improvements in the more practical and novel task of transfer compressed sensing where a generative model on a data-rich, source domain provides a prior for sensing a data-scarce, target domain."
  }, {
    "heading": "2. Preliminaries",
    "text": "In this section, we review the necessary background and prior work in modeling domain specific structure in compressed sensing. We are interested in solving the following system of equations,\ny = Ax (1)\nwhere x ∈ Rn is the signal of interest being sensed through measurements y ∈ Rm, and A ∈ Rm×n is a measurement matrix. For efficient acquisition of signals, we will design measurement matrices such that m n. However, the system is under-determined whenever rank(A) < n. Hence, unique recovery requires additional assumptions on x. We now discuss two ways to model the structure of x.\nSparsity. Sparsity in a well-chosen basis is natural in many domains. For instance, natural images are sparse in the wavelet basis whereas audio signals exhibit sparsity in the Fourier basis (Mallat, 2008). Hence, it is natural to assume the domain of signals x we are interested in recovering is\nSl(0) = {x : ‖x− 0‖0 ≤ l}. (2)\nThis is the set of l-sparse vectors with the `0 distance measured from the origin. Such assumptions dominate the prior literature in compressed sensing and can be further relaxed to recover approximately sparse signals (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).\nLatent variable generative models. A latent variable model specifies a joint distribution Pθ(x, z) over the observed data x (e.g., images) and a set of latent variables z ∈ Rk (e.g., features). Given a training set of signals {x1, · · · , xM}, we can learn the parameters θ of such a model, e.g., via maximum likelihood. When Pθ(x, z) is parameterized using deep neural networks, such generative models can effectively model complex, high-dimensional signal distributions for modalities such as images and audio (Kingma & Welling, 2014; Goodfellow et al., 2014).\nGiven a pretrained latent variable generative model with parameters θ, we can associate a generative model function G : Rk → Rn mapping a latent vector z to the mean of the conditional distribution Pθ(x|z). Thereafter, the space of signals that can be recovered with such a model is given by the range of the generator function,\nSG = {G(z) : z ∈ Rk}. (3)\nNote that the set is defined with respect to the latent vectors z, and we omit the dependence of G on the parameters θ (which are fixed for a pretrained model) for brevity."
  }, {
    "heading": "2.1. Recovery algorithms",
    "text": "Signal recovery in compressed sensing algorithm typically involves solving an optimization problem consistent with the modeling assumptions on the domain of the signals being sensed.\nSparse vector recovery using LASSO. Under the assumptions of sparsity, the signal x can be recovered by solving an `0 minimization problem (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).\nmin x ‖x‖0\ns.t. Ax = y. (4)\nThe objective above is however NP-hard to optimize, and hence, it is standard to consider a convex relaxation,\nmin x ‖x‖1\ns.t. Ax = y. (5)\nIn practice, it is common to solve the Lagrangian of the above problem. We refer to this method as LASSO-based recovery due to similarities of the objective in Eq. (5) to the LASSO regularization used broadly in machine learning (Tibshirani, 1996). LASSO-based recovery is the predominant technique for recovering sparse signals since it involves solving a tractable convex optimization problem.\nIn order to guarantee unique recovery to the underdetermined system in Eq. (1), the measurement matrix A is designed to satisfy the Restricted Isometry Property (RIP) or the Restricted Eigenvalue Condition (REC) for l-sparse matrices with high probability (Candès & Tao, 2005; Bickel et al., 2009). We define these conditions below. Definition 1. Let Sl(0) ⊂ Rn be the set of l-sparse vectors. For some parameter α ∈ (0, 1), a matrix A ∈ Rm×n is said to satisfy RIP(l, α) if ∀ x ∈ Sl(0),\n(1− α)‖x‖2 ≤ ‖Ax‖2 ≤ (1 + α)‖x‖2.\nDefinition 2. Let Sl(0) ⊂ Rn be the set of l-sparse vectors. For some parameter γ > 0, a matrix A ∈ Rm×n is said to satisfy REC(l, γ) if ∀ x ∈ Sl(0),\n‖Ax‖2 ≥ γ‖x‖2.\nIntuitively, RIP implies that A approximately preserves Euclidean norms for sparse vectors and REC implies that sparse vectors are far from the nullspace of A. Many classes of matrices satisfy these conditions with high probability, including random Gaussian and Bernoulli matrices where every entry of the matrix is sampled from a standard normal and uniform Bernoulli distribution respectively (Baraniuk et al., 2008).\nGenerative model vector recovery using gradient descent. If the signals being sensed are assumed to lie close to the range SG of a generative model function G as defined in Eq. (3) , then we can recover the best approximation to the true signal by `2-minimization over z,\nmin z ‖AG(z)− y‖22. (6)\nThe function G is typically expressed as a deep neural network which makes the overall objective non-convex, but differentiable almost everywhere w.r.t z. In practice, good reconstructions can be recovered by gradient-based optimization methods. We refer to this method proposed by Bora et al. (2017) as generative model-based recovery.\nTo guarantee unique recovery, generative model-based recovery makes two key assumptions. First, the generator functionG is assumed to be L-Lipschitz, i.e., ∀ z1, z2 ∈ Rk,\n‖G(z1)−G(z2)‖2 ≤ L‖z1 − z2‖2.\nSecondly, the measurement matrix A is designed to satisfy the Set-Restricted Eigenvalue Condition (S-REC) with high probability (Bora et al., 2017).\nDefinition 3. Let S ⊆ Rn. For some parameters γ > 0, δ ≥ 0, a matrix A ∈ Rm×n is said to satisfy the SREC(S, γ, δ) if ∀ x1, x2 ∈ S,\n‖A(x1 − x2)‖2 ≥ γ‖x1 − x2‖2 − δ.\nS-REC generalizes REC to an arbitrary set of vectors S as opposed to just considering the set of approximately sparse vectors Sl(0) and allowing an additional slack term δ. In particular, S is chosen to be the range of the generator function G for generative model-based recovery."
  }, {
    "heading": "3. The Sparse-Gen framework",
    "text": "The modeling assumptions based on sparsity and generative modeling discussed in the previous section can be limiting in many cases. On one hand, sparsity assumes a relatively weak prior over the signals being sensed. Empirically, we observe that the recovered signals xL have large reconstruction error ‖xL− x‖22 especially when the number of measurements m is small. On the other hand, generative models imposes a very strong, but rigid prior which works well when the number of measurements is small. However, the performance of the corresponding recovery methods saturates with increasing measurements since the recovered signal xG = G(zG) is constrained to lie in the range of the generator function G. If zG ∈ Rk is the optimum value returned by an optimization procedure for Eq. (6), then the reconstruction error ‖xG − x‖22 is limited by the dimensionality of the latent space and the quality of the generator function.\nTo sidestep the above limitations, we consider a strictly more expressive class of signals by allowing sparse deviations from the range of a generator function. Formally, the domain of the recovered signals is given by,\nSl,G = ∪z∈Dom(G)Sl(G(z)) (7)\nwhere Sl(G(z)) denotes the set of sparse vectors centered on G(z) and z varies over the domain of G (typically Rk). We refer to this modeling assumption and the consequent algorithmic framework for recovery as Sparse-Gen.\nBased on this modeling assumption, we will recover signals of the form G(z) + ν for some ν ∈ Rn that is preferably sparse. Specifically, we consider the optimization of a hybrid objective,\nmin z,ν ‖ν‖0\ns.t. A (G(z) + ν) = y. (8)\nIn the above optimization problem the objective is nonconvex and non-differentiable, while the constraint is nonconvex (for general G), making the above optimization\nproblem hard to solve. To ease the optimization problem, we propose two modifications. First, we relax the `0 minimization to an `1 minimization similar to LASSO.\nmin z,ν ‖ν‖1\ns.t. A (G(z) + ν) = y. (9)\nNext, we square the non-convex constraint on both sides and consider the Lagrangian of the above problem to get the final unconstrained optimization problem for Sparse-Gen,\nmin z,ν ‖ν‖1 + λ‖A (G(z) + ν)− y‖22 (10)\nwhere λ is the Lagrange multiplier.\nThe above optimization problem is non-differentiable w.r.t. ν and non-convex w.r.t. z (if G is non-convex). In practice, it can be solved in practice using gradient descent (since the non-differentiability is only at a finite number of points) or using sequential convex programming (SCP). SCP is an effective heuristic for non-convex problems where the convex portions of the problem are solved using a standard convex optimization technique (Boyd & Vandenberghe, 2004). In the case of Eq. (10), the optimization w.r.t. ν (for fixed z) is a convex optimization problem whereas the non-convexity typically involves differentiable terms (w.r.t. z) if G is a deep neural network. Empirically, we find excellent recovery by standard first order gradient-based methods (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2015).\nUnlike LASSO-based recovery which recovers only sparse signals, Sparse-Gen can impose a stronger domain-specific prior using a generative model. If we fix the generator function to map all z to the origin, we recover LASSO-based recovery as a special case of Sparse-Gen. Additionally, Sparse-Gen is not constrained to recover signals over the range of G, as in the case of generative model-based recovery. In fact, it can recover signals with sparse deviations from the range of G. Note that the sparse deviations can be\ndefined in a basis different from the canonical basis. In such cases, we consider the following optimization problem,\nmin z,ν ‖Bν‖1 + λ‖A (G(z) + ν)− y‖22 (11)\nwhere B is a change of basis matrix that promotes sparsity of the vector Bν. Figure 1 illustrates the differences in modeling assumptions between Sparse-Gen and other frameworks."
  }, {
    "heading": "4. Theoretical Analysis",
    "text": "The proofs for all results in this section are given in the Appendix. Our analysis and experiments account for measurement noise in compressed sensing, i.e.,\ny = Ax+ . (12)\nLet ∆ : Rm → Rn denote an arbitrary decoding function used to recover the true signal x from the measurements y ∈ Rm. Our analysis will upper bound the `2-error in recovery incurred by our proposed framework using mixed norm guarantees (in particular, `2/`1). To this end, we first state some key definitions. Define the least possible `1 error for recovering x under the Sparse-Gen modeling as,\nσSl,G(x) = inf x̂∈Sl,G\n‖x− x̂‖1\nwhere the optimal x̂ is the closest point to x in the allowed domain Sl,G. We now state the main lemma guiding the theoretical analysis. Lemma 1. Given a function G : Rk → Rn and measurement noise with ‖ ‖2 ≤ max, let A be any matrix that satisfies S-REC(S1.5l,G, (1− α), δ) and RIP(2l, α) for some α ∈ (0, 1), l > 0. Then, there exists a decoder ∆ : Rm → Rn such that,\n‖x−∆(Ax+ )‖2 ≤ (2l)−1/2C0σl,G(x) + C1 max + δ′\nfor all x ∈ Rn, where C0 = 2((1+α)(1−α)−1 +1), C1 = 2(1− α)−1, and δ′ = δ(1− α)−1.\nThe above lemma shows that there exists a decoder such that the error in recovery can be upper bounded for measurement matrices satisfying S-REC and RIP. Note that Lemma 1 only guarantees the existence of such a decoder and does not prescribe an optimization algorithm for recovery. Apart from the errors due to the bounded measurement noise max and a scaled slack term appearing in the S-REC condition δ′, the major term in the upper bound corresponds to (up to constants) the minimum possible error incurred by the best possible recovery vector in Sl,G given by σl,G(x). Similar terms appear invariably in the compressed sensing literature and are directly related to the modeling assumptions regarding x (for example, Theorem 8.3 in Cohen et al. (2009)).\nOur next lemma shows that random Gaussian matrices satisfy the S-REC (over the range of Lipschitz generative model functions) and RIP conditions with high probability for G with bounded domain, both of which together are sufficient conditions for Lemma 1 to hold.\nLemma 2. LetG : Bk(r)→ Rn be anL-Lipschitz function where Bk(r) = {z | z ∈ Rk, ‖z‖2 ≤ r} is the `2-norm ball in Rk. For α ∈ (0, 1), if\nm = O\n( 1\nα2\n( k log ( Lr\nδ\n) + l log(n/l) )) then a random matrix A ∈ Rm×n with i.i.d. entries such that Aij ∼ N ( 0, 1m ) satisfies the S-REC(S1.5l,G, 1− α, δ) and RIP(2l, α) with 1− e−Ω(α2m) probability.\nUsing Lemma 1 and Lemma 2, we can bound the error due to decoding with generative models and random Gaussian measurement matrices in the following result.\nTheorem 1. Let G : Bk(r)→ Rn be an L-Lipschitz function. For any α ∈ (0, 1), l > 0, let A ∈ Rm×n be a random Gaussian matrix with\nm = O\n( 1\nα2\n( k log ( Lr\nδ\n) + l log(n/l) )) rows of i.i.d. entries scaled such that Ai,j ∼ N(0, 1/m). Let ∆ be the decoder satisfying Lemma 1. Then, we have with 1− e−Ω(α2m) probability,\n‖x−∆(Ax+ )‖2 ≤ (2l)−1/2C0σl,G(x) + C1 max + δ′\nfor all x ∈ Rn, ‖ ‖2 ≤ max, where C0, C1, γ, δ′ are constants defined in Lemma 1.\nFrom the above lemma, we see that the number of measurements needed to guarantee upper bounds on the reconstruction error of any signal with high probability depends on two terms. The first term includes dependence on the Lipschitz constant L of the generative model function G. A high Lipschitz constant makes recovery harder (by requiring a larger\nnumber of measurements), but only contributes logarithmically. The second term, typical of results in sparse vector recovery, shows a logarithmic growth on the dimensionality n of the signals. Ignoring logarithmic dependences and constants, recovery using Sparse-Gen requires about O(k + l) measurements for recovery. Note that Theorem 1 assumes access to an optimization oracle for decoding. In practice, we consider the solutions returned by gradient-based optimization methods to a non-convex objective defined in Eq. (11) that are not guaranteed to correspond to the optimal decoding in general.\nFinally, we obtain tighter bounds for the special case when G is expressed using a neural network with only ReLU activations. These bounds do not rely explicitly on the Lipschitz constant L or require the domain of G to be bounded.\nTheorem 2. If G : Rk → Rn is a neural network of depth d with only ReLU activations and at most c nodes in each layer, then the guarantees of Theorem 1 hold for\nm = O\n( 1\nα2\n( (k + l)d log c+ (k + l) log(n/l) )) .\nOur theoretical analysis formalizes the key properties of recovering signals using Sparse-Gen. As shown in Lemma 1, there exists a decoder for recovery based on such modeling assumptions that extends recovery guarantees based on vanilla sparse vector recovery and generative model-based recovery. Such recovery requires measurement matrices that satisfy both the RIP and S-REC conditions over the set of vectors that deviate in sparse directions from the range of a generative model function. In Theorems 1-2, we observed that the number of measurements required to guarantee recovery with high probability grow almost linearly (with some logarithmic terms) with the latent space dimensionality k of the generative model and the permissible sparsity l for deviating from the range of the generative model."
  }, {
    "heading": "5. Experimental Evaluation",
    "text": "We evaluated Sparse-Gen for compressed sensing of highdimensional signals from the domain of benchmark image datasets. Specifically, we considered the MNIST dataset of handwritten digits (LeCun et al., 2010) and the OMNIGLOT dataset of handwritten characters (Lake et al., 2015). Both these datasets have the same data dimensionality (28× 28), but significantly different characteristics. The MNIST dataset has fewer classes (10 digits from 0-9) as opposed to Omniglot which shows greater diversity (1623 characters across 50 alphabets). Additional experiments with generative adversarial networks on the CelebA dataset are reported in the Appendix.\nBaselines. We considered methods based on sparse vector recovery using LASSO (Tibshirani, 1996; Candès & Tao,\n2005) and generative model based recovery using variational autoencoders (VAE) (Kingma & Welling, 2014; Bora et al., 2017). For VAE training, we used the standard train/held-out splits of both datasets. Compressed sensing experiments that we report were performed on the entire test set of images. The architecture and other hyperparameter details are given in the Appendix.\nExperimental setup. For the held-out set of instances, we artificially generated measurements y through a random matrix A ∈ Rm×n with entries sampled i.i.d. from a Gaussian with zero mean and standard deviation of 1/m. Measurement noise is sampled from zero mean and diagonal scalar covariance matrix with entries as 0.01. For evaluation, we report the reconstruction error measured as ‖x̂− x‖p where x̂ is the recovered signal and p is a norm of interest, varying the number of measurementsm from 50 to the highest value of 750. We report results for the p = {1, 2,∞} norms.\nWe evaluated sensing of both continuous signals (MNIST) with pixel values in range [0, 1] and discrete signals (Omniglot) with binary pixel values {0, 1}. For all algorithms considered, recovery was performed by optimizing over a continuous space. In the case of sparse recovery methods (including Sparse-Gen) it is possible that unconstrained optimization returns signals outside the domain of interest, in which case they are projected to the required domain by simple clipping, i.e., any signal less than zero is clipped to 0 and similarly any signal greater than one is clipped to 1.\nResults and Discussion. The reconstruction errors for varying number of measurements are given in Figure 2. Consistent with the theory, the strong prior in generative modelbased recovery methods outperforms the LASSO-based methods for sparse vector recovery. In the regime of low measurements, the performance of algorithms that can incorporate the generative model prior dominates over methods modeling sparsity using LASSO. The performance of plain generative model-based methods however saturates with increasing measurements, unlike Sparse-Gen and LASSO which continue to shrink the error. The trends are consistent for both MNIST and Omniglot, although we observe the relative magnitudes of errors in the case of Omniglot are much higher than that of MNIST. This is expected due to the increased diversity and variations of the structure of the signals being sensed in the case of Omniglot. We also observe the trends to be consistent across the various norms considered."
  }, {
    "heading": "5.1. Transfer compressed sensing",
    "text": "One of the primary motivations for compressive sensing is to directly acquire the signals using few measurements. On the contrary, learning a deep generative model requires access to large amounts of training data. In several applications, getting the data for training a generative model might not be feasible. Hence, we test the generative model-based recovery on the novel task of transfer compressed sensing.\nExperimental setup. We train the generative model on a source domain (assumed to be data-rich) and related to a data-hungry target domain we wish to sense. Given the matching dimensions of MNIST and Omniglot, we conduct experiments transferring from MNIST (source) to Omniglot (target) and vice versa.\nResults and Discussion. The reconstruction errors for the norms considered are given in Figure 3. For both the sourcetarget pairs, we observe that the Sparse-Gen consistently performs well. Vanilla generative model-based recovery shows hardly an improvements with increasing measurements. We can qualitatively see this phenomena for transferring from MNIST (source) to Omniglot (target) in Figure 4. With only m = 100 measurements, all models perform poorly and generative model based methods particularly continue to sense images similar to MNIST. On the other hand, there is a noticeable transition at m = 200 measurements for SparseVAE where it adapts better to the domain being sensed than plain generative model-based recovery and achieves lower reconstruction error."
  }, {
    "heading": "6. Related Work",
    "text": "Since the introduction of compressed sensing over a decade ago, there has been a vast body of research studying various extensions and applications (Candès & Tao, 2005; Donoho,\n2006; Candès et al., 2006). This work explores the effect of modeling different structural assumptions on signals in theory and practice.\nThemes around sparsity in a well-chosen basis has driven much of the research in this direction. For instance, the paradigm of model-based compressed sensing accounts for the interdependencies between the dimensions of a sparse data signal (Baraniuk et al., 2010; Duarte & Eldar, 2011; Gilbert et al., 2017). Alternatively, adaptive selection of basis vectors from a dictionary that best capture the structure of the particular signal being sensed has also been explored (Peyre, 2010; Tang et al., 2013). Many of these methods have been extended to recovery of structured tensors (Zhang et al., 2013; 2014). In another prominent line of research involving Bayesian compressed sensing, the sparseness assumption is formalized by placing sparsenesspromoting priors on the signals (Ji et al., 2008; He & Carin, 2009; Babacan et al., 2010; Baron et al., 2010).\nResearch exploring structure beyond sparsity is relatively scarce. Early works in this direction can be traced to Baraniuk & Wakin (2009) who proposed algorithms for recovering signals lying on a smooth manifold. The generative model-based recovery methods consider functions that do not necessarily define manifolds since the range of a generator function could intersect with itself. Yu & Sapiro (2011) coined the term statistical compressed sensing and proposed\nalgorithms for efficient sensing of signals from a mixture of Gaussians. The recent work in deep generative model-based recovery differs in key theoretical aspects as well in the use of a more expressive family of models based on neural networks. A related recent work by Hand & Voroninski (2017) provides theoretical guarantees on the solution recovered for solving non-convex linear inverse problems with deep generative priors. Empirical advances based on well-designed deep neural network architectures that sacrifice many of the theoretical guarantees have been proposed for applications such as MRI (Mardani et al., 2017; 2018). Many recent methods propose to learn mappings of signals to measurements using neural networks, instead of restricting them to be linear, random matrices (Mousavi et al., 2015; Kulkarni et al., 2016; Chang et al., 2017; Lu et al., 2018).\nOur proposed framework bridges the gap between algorithms that model structure using sparsity and enjoy good theoretical properties with advances in deep generative models, in particular their use for compressed sensing."
  }, {
    "heading": "7. Conclusion and Future Work",
    "text": "The use of deep generative models as priors for compressed sensing presents a new outlook on algorithms for inexpen-\nsive data acquisition. In this work, we showed that these priors can be used in conjunction with classical modeling assumptions based on sparsity. Our proposed framework, Sparse-Gen, generalizes both sparse vector recovery and recovery using generative models by allowing for sparse deviations from the range of a generative model function. The benefits of using such modeling assumptions are observed both theoretically and empirically.\nIn the future, we would like to design algorithms that can better model the structure within sparse deviations. Followup work in this direction can benefit from the vast body of prior work in structured sparse vector recovery (Duarte & Eldar, 2011). From a theoretical perspective, a better understanding of the non-convexity resulting from generative model-based recovery can lead to stronger guarantees and consequently better optimization algorithms for recovery. Finally, it would be interesting to extend Sparse-Gen for compressed sensing of other data modalities such as graphs for applications in network tomography and reconstruction (Xu et al., 2011). Real-world graph networks are typically sparse in the canonical basis and can be modeled effectively using deep generative models (Grover et al., 2018), which is consistent with the modeling assumptions of the Sparse-Gen framework."
  }, {
    "heading": "Acknowledgements",
    "text": "We are thankful to Tri Dao, Jonathan Kuck, Daniel Levy, Aditi Raghunathan, and Yang Song for helpful comments on early drafts. This research was supported by Intel Corporation, TRI, a Hellman Faculty Fellowship, ONR, NSF (#1651565, #1522054, #1733686 ) and FLI (#2017-158687). AG is supported by a Microsoft Research PhD Fellowship."
  }],
  "references": [{
    "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
    "authors": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"],
    "venue": "arXiv preprint arXiv:1603.04467,",
    "year": 2016
  }, {
    "title": "Cvxopt: A python package for convex optimization, version 1.1",
    "authors": ["M.S. Andersen", "J. Dahl", "L. Vandenberghe"],
    "venue": "Available at cvxopt. org,",
    "year": 2013
  }, {
    "title": "Bayesian compressive sensing using Laplace priors",
    "authors": ["S.D. Babacan", "R. Molina", "A.K. Katsaggelos"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2010
  }, {
    "title": "A simple proof of the restricted isometry property for random matrices",
    "authors": ["R. Baraniuk", "M. Davenport", "R. DeVore", "M. Wakin"],
    "venue": "Constructive Approximation,",
    "year": 2008
  }, {
    "title": "Random projections of smooth manifolds",
    "authors": ["R.G. Baraniuk", "M.B. Wakin"],
    "venue": "Foundations of computational mathematics,",
    "year": 2009
  }, {
    "title": "Model-based compressive sensing",
    "authors": ["R.G. Baraniuk", "V. Cevher", "M.F. Duarte", "C. Hegde"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1982
  }, {
    "title": "Bayesian compressive sensing via belief propagation",
    "authors": ["D. Baron", "S. Sarvotham", "R.G. Baraniuk"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2010
  }, {
    "title": "Simultaneous analysis of lasso and Dantzig selector",
    "authors": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "Compressed sensing using generative models",
    "authors": ["A. Bora", "A. Jalal", "E. Price", "A.G. Dimakis"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Decoding by linear programming",
    "authors": ["E.J. Candès", "T. Tao"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2005
  }, {
    "title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
    "authors": ["E.J. Candès", "J. Romberg", "T. Tao"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2006
  }, {
    "title": "One network to solve them allsolving linear inverse problems using deep projection models",
    "authors": ["J.R. Chang", "Li", "C.-L", "B. Poczos", "B.V. Kumar", "A.C. Sankaranarayanan"],
    "year": 2017
  }, {
    "title": "Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds",
    "authors": ["M. Chen", "J. Silva", "J. Paisley", "C. Wang", "D. Dunson", "L. Carin"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2010
  }, {
    "title": "Compressed sensing and best k-term approximation",
    "authors": ["A. Cohen", "W. Dahmen", "R. DeVore"],
    "venue": "Journal of the American mathematical society,",
    "year": 2009
  }, {
    "title": "Structured compressed sensing: From theory to applications",
    "authors": ["M.F. Duarte", "Y.C. Eldar"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2011
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Towards understanding the invertibility of convolutional neural networks",
    "authors": ["A.C. Gilbert", "Y. Zhang", "K. Lee", "H. Lee"],
    "venue": "arXiv preprint arXiv:1705.08664,",
    "year": 2017
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Graphite: Iterative generative modeling of graphs",
    "authors": ["A. Grover", "A. Zweig", "S. Ermon"],
    "venue": "arXiv preprint arXiv:1803.10459,",
    "year": 2018
  }, {
    "title": "Global guarantees for enforcing deep generative priors by empirical risk",
    "authors": ["P. Hand", "V. Voroninski"],
    "venue": "arXiv preprint arXiv:1705.07576,",
    "year": 2017
  }, {
    "title": "Exploiting structure in wavelet-based bayesian compressive sensing",
    "authors": ["L. He", "L. Carin"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2009
  }, {
    "title": "Bayesian compressive sensing",
    "authors": ["S. Ji", "Y. Xue", "L. Carin"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2008
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2015
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D. Kingma", "M. Welling"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2014
  }, {
    "title": "Reconnet: Non-iterative reconstruction of images from compressively sensed measurements",
    "authors": ["K. Kulkarni", "S. Lohit", "P. Turaga", "R. Kerviche", "A. Ashok"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Human-level concept learning through probabilistic program induction",
    "authors": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"],
    "year": 2015
  }, {
    "title": "Convcsnet: A convolutional compressive sensing framework based on deep learning",
    "authors": ["X. Lu", "W. Dong", "P. Wang", "G. Shi", "X. Xie"],
    "venue": "arXiv preprint arXiv:1801.10342,",
    "year": 2018
  }, {
    "title": "Compressed sensing MRI",
    "authors": ["M. Lustig", "D.L. Donoho", "J.M. Santos", "J.M. Pauly"],
    "venue": "IEEE signal processing magazine,",
    "year": 2008
  }, {
    "title": "A wavelet tour of signal processing: the sparse way",
    "authors": ["S. Mallat"],
    "venue": "Academic press,",
    "year": 2008
  }, {
    "title": "Deep generative adversarial networks for compressed sensing automates MRI",
    "authors": ["M. Mardani", "E. Gong", "J.Y. Cheng", "S. Vasanawala", "G. Zaharchuk", "M. Alley", "N. Thakur", "S. Han", "W. Dally", "Pauly", "J. M"],
    "venue": "arXiv preprint arXiv:1706.00051,",
    "year": 2017
  }, {
    "title": "Recurrent generative adversarial networks for proximal learning and automated compressive image recovery",
    "authors": ["M. Mardani", "H. Monajemi", "V. Papyan", "S. Vasanawala", "D. Donoho", "J. Pauly"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2018
  }, {
    "title": "A deep learning approach to structured signal recovery",
    "authors": ["A. Mousavi", "A.B. Patel", "R.G. Baraniuk"],
    "venue": "In Annual Allerton Conference on Communication, Control, and Computing,",
    "year": 2015
  }, {
    "title": "Best basis compressed sensing",
    "authors": ["G. Peyre"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2010
  }, {
    "title": "Compressive sensing: From theory to applications, a survey",
    "authors": ["S. Qaisar", "R.M. Bilal", "W. Iqbal", "M. Naureen", "S. Lee"],
    "venue": "Journal of Communications and networks,",
    "year": 2013
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "arXiv preprint arXiv:1511.06434,",
    "year": 2015
  }, {
    "title": "Compressed sensing off the grid",
    "authors": ["G. Tang", "B.N. Bhaskar", "P. Shah", "B. Recht"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2013
  }, {
    "title": "Regression shrinkage and selection via the lasso",
    "authors": ["R. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp",
    "year": 1996
  }, {
    "title": "Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. Coursera: Neural networks for machine learning",
    "authors": ["T. Tieleman", "G. Hinton"],
    "year": 2012
  }, {
    "title": "Compressive sensing over graphs",
    "authors": ["W. Xu", "E. Mallada", "A. Tang"],
    "venue": "In IEEE INFOCOM,",
    "year": 2011
  }, {
    "title": "Statistical compressed sensing of Gaussian mixture models",
    "authors": ["G. Yu", "G. Sapiro"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2011
  }, {
    "title": "Simultaneous rectification and alignment via robust recovery of low-rank tensors",
    "authors": ["X. Zhang", "D. Wang", "Z. Zhou", "Y. Ma"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Hybrid singular value thresholding for tensor completion",
    "authors": ["X. Zhang", "Z. Zhou", "D. Wang", "Y. Ma"],
    "venue": "In AAAI Conference on Artificial Intelligence,",
    "year": 2014
  }],
  "id": "SP:b99675e8ebb90d1663cab4da63a546f55c885cd4",
  "authors": [{
    "name": "Manik Dhar",
    "affiliations": []
  }, {
    "name": "Aditya Grover",
    "affiliations": []
  }, {
    "name": "Stefano Ermon",
    "affiliations": []
  }],
  "abstractText": "In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",
  "title": "Modeling Sparse Deviations for Compressed Sensing using Generative Models"
}