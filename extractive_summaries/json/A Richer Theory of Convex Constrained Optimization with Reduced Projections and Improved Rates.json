{
  "sections": [{
    "text": "1The University of Iowa, Iowa City, IA 52242, USA 2National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.\nThis is the long version of our paper appearing in the Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s)."
  }, {
    "heading": "1. Introduction",
    "text": "In this paper, we aim at solving the following convex constrained optimization problem:\nmin x∈Rd f(x), s.t. c(x) ≤ 0, (1)\nwhere f(x) is a smooth or non-smooth convex function and c(x) is a lower-semicontinuous and convex function. The problem can find applications in machine learning, signal processing, statistics, marketing optimization, and etc. For example, in distance metric learning one needs to learn a positive semi-definite (PSD) matrix such that similar examples are close to each other and dissimilar examples are far from each other (Weinberger et al., 2006; Xing et al., 2003), where the positive semi-definite constraint can be cast into a convex inequality constraint. Another example arising in compressive sensing is to minimize the `1 norm of high-dimensional vector subject to a measurement constraint (Candès & Wakin, 2008). Although general interiorpoint methods can be applied to solve the problem with linear convergence, they suffer from exceedingly high computational cost per-iteration. Another solution is to employ the projected gradient (PG) method (Nesterov, 2004) or the conditional gradient (CG) method (Frank & Wolfe, 1956), where the PG method needs to compute the projection into the constrained domain at each iteration and CG needs to solve a linear optimization problem under the constraint. However, for many constraints (e.g., PSD, quadratic constraints) both projection into the constrained domain and the linear optimization under the constraint are time-consuming, which restrict their capabilities to solving these problems.\nRecently, there emerges a new direction towards addressing the challenge of expensive projection that is to reduce the number of projections. In the seminal paper (Mahdavi et al., 2012), the authors have proposed two algorithms with only one projection at the end of iterations for non-smooth convex and strongly convex optimization, respectively. The idea of both algorithms is to move the constraint function into the objective function and to control the violation of constraint for intermediate solutions. While their developed algorithms enjoy an optimal convergence rate for nonsmooth optimization (i.e., O(1/ 2) iteration complexity)\nand a close-to-optimal convergence rate for strongly convex optimization (i.e., Õ(1/ ) 1), there still lack of theory and algorithms with reduced projections and faster rates for smooth convex optimization and for convex optimization without strong convexity assumptions.\nIn this paper, we make significant contributions by developing a richer theory of convex constrained optimization with reduced projections and faster rates. To be specific,\n• we develop a general framework and theory of optimization with only one projection, where any favorable smooth or non-smooth convex optimization algorithms can be employed to solve the intermediate augmented unconstrained objective function. We discuss in full details the applicability of the proposed algorithms to problems with polyhedral, quadratic or PSD constraints.\n• Applying the general theory to smooth convex optimization 2 with Nesterov’s accelerated gradient methods yields an iteration complexity of O(1/ ) with only one projection. In addition, when equipped with an optimal algorithm for strongly convex optimization the general theory implies the optimal iteration complexity of O(1/ ) for strongly convex optimization with only one projection. For smooth and strongly convex optimization, the general theory implies an iteration complexity of O(1/ β) where β ∈ (1/2, 1) with only one projection and a sufficiently large number of iterations.\n• Building on the general framework and theory, we further develop an improved theory with faster convergence rates for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we show that under a mild local error bound condition, the iteration complexities can be reduced to Õ(1/ 2(1−θ)) for non-smooth optimization and Õ(1/ 1−θ) for smooth optimization, where θ ∈ (0, 1] is a constant in the local error bound condition that characterizes the local growth rate of functional values. To our knowledge, these are the best convergence results with only a logarithmic number of projections for non-strongly convex optimization. We also demonstrate their effectiveness for solving compressive sensing and distance metric learning problems."
  }, {
    "heading": "2. Related Work",
    "text": "The issue of high projection cost in projected gradient descent has received increasing attention in recent years. Most studies are based on the Frank-Wolfe technique that eschews the projection in favor of a linear optimization over the constrained domain (Jaggi, 2013; Hazan & Kale, 2012; Lacoste-Julien et al., 2013; Garber & Hazan, 2015). It happens that for many bounded domains (e.g., bounded\n1where Õ() suppresses a logarithmic factor. 2where the constraint function is assumed to be smooth.\nballs for vectors and matrices, a PSD constraint with a bounded trace norm) the linear optimization over the constrained domain is much cheaper than projection into the constrained domain (Jaggi, 2013). However, there still exist many constraints that render both projection into the constrained domain and linear optimization under the constraint are comparably expensive. Examples include polyhedral constraints, quadratic constraints and a PSD constraint 3.\nTo tackle these complex constraints, the idea of optimization with a reduced number of projections was explored in several studies since (Mahdavi et al., 2012). In a recent paper (Chen et al., 2016), the authors show that for stochastic strongly convex optimization, the optimal convergence rate can be achieved using a logarithmic number of projections. In contrast, the developed theory in this paper implies that only one projection is sufficient to achieve the optimal convergence rate for strongly convex optimization, and a logarithmic number of projections can be used to accelerate convergence rates for non-strongly convex optimization. Cotter et al. (2016) proposed a stochastic algorithm for solving heavily constrained problems with many constraint functions by extending the work of (Mahdavi et al., 2012). Nonetheless, their focus is not to improve the convergence rates. Zhang et al. (2013) studied the smooth and strongly convex optimization and they proposed a stochastic algorithm withO(κ log(T )) projections and proved anO(1/T ) convergence rate, where κ is the condition number and T is the total number of iterations. Nonetheless, if the condition number is high the number of projections could be very large. In addition, their algorithm utilizes the minibatch to avoid frequent projections in stochastic optimization, which is different from the present paper.\nWe note that several recent works also exploit different forms of error bound conditions to improve the convergence (Wang & Lin, 2014; So, 2013; Hou et al., 2013; Zhou et al., 2015; Yang & Lin, 2016; Xu et al., 2016). Most notably, the technique used in our work is closely related to (Yang & Lin, 2016). However, for constrained optimization problems the methods in (Yang & Lin, 2016) still need to conduct projections at each iteration.\nFinally, we comment on the differences between the proposed methods and the classical penalty methods that also move the constraint into the objective using a penalty function (Bertsekas, 1996). The major differences are that (i) the classical penalty methods typically require solving each subproblem exactly while our methods do not require that; and (ii) the classical penalty methods typically guarantee asymptotic convergence while our methods have explicit convergence rates.\n3Indeed, a linear optimization over a PSD constraint is illposed because the PSD domain is unbounded."
  }, {
    "heading": "3. Preliminaries",
    "text": "Let Ω = {x ∈ Rd : c(x) ≤ 0} denote the constrained domain, Ω∗ denote the optimal solution set and f∗ denote the optimal objective value. We denote by ∇f(x) the gradient and by ∂f(x) the subgradient of a smooth or non-smooth function, respectively. When f(x) is a non-smooth function, we consider the problem as non-smooth constrained optimization. When both f(x) and c(x) are smooth, we consider the problem as smooth constrained optimization. A function f(x) is L-smooth if it has a Lipschitz continuous gradient, i.e., ‖∇f(x)−∇f(y)‖ ≤ L‖x− y‖, where ‖ · ‖ denotes the Euclidean norm. A function f(x) is µstrongly convex if it satisfies f(x) ≥ f(y) + ∂f(y)>(x− y) + µ2 ‖x− y‖ 2.\nIn the sequel, dist(x,Ω) denotes the distance of x to a set Ω, i.e., dist(x,Ω) = minu∈Ω ‖x−u‖. Let [s]+ be a hinge operator that is defined as [s]+ = s if s ≥ 0, and [s]+ = 0 if s < 0.\nThroughout the paper, we make the the following assumptions to facilitate the development of our algorithms and theory.\nAssumption 1. For a convex minimization problem (1), we assume (i) there exists a positive value ρ > 0 such that\nmin c(x)=0\nv∈∂c(x),v 6=0\n‖v‖ ≥ ρ, (2)\nor more generally there exists a constant ρ > 0 for any x ∈ Rd, such that x\\ = arg minu∈Rd,c(u)≤0 ‖u − x‖2 satisfies\n‖x\\ − x‖ ≤ [c(x)]+/ρ. (3)\n(ii) there exists a strictly feasible solution such that c(x) < 0; (iii) both f(x) and c(x) are defined everywhere and are Lipschitz continuous with their Lipschitz constants denoted by G and Gc, respectively. We make several remarks about the assumptions. The inequality in (2) is introduced in (Mahdavi et al., 2012), which is to ensure the distance from the final solution before projection to constrained domain Ω is not too large. Note that the inequality in (3) is a more general condition than (2) as seen from the following lemma.\nLemma 1. For any x ∈ Rd, let x\\ = arg minc(u)≤0 ‖u− x‖2. If (2) holds, then (3) holds.\nThe above lemma is implicit in the proof of (Mahdavi et al., 2012). We will provide more discussions about Assumption 1(i) - the key assumption, and exhibit the value of ρ for a number of commonly seen constraints (e.g., polyhedral, quadratic and PSD constraints). To make the presentation more fluent, we postpone these discussions to Section 6. The strict feasibility assumption (ii) allows us to explore the KKT condition of the projection problem shown below.\nAssumption (iii) imposes mild Lipschitz continuity conditions on both f(x) and c(x).\nTraditional projected gradient descent methods need to solve the following projection at each iteration ΠΩ[x] = arg minc(u)≤0 ‖u − x‖2. Conditional gradient methods (a.k.a. the Frank-Wolfe technique) need to solve the following linear optimization at each iteration minu∈Rd,c(u)≤0 u\n>∇f(x). For many constraint functions (see Section 6), solving the projection problem and the linear optimization could be very expensive."
  }, {
    "heading": "4. A General Theory of Optimization with only one projection",
    "text": "In this section, we extend the idea of only one projection proposed in (Mahdavi et al., 2012) to a general theory, and then present optimization algorithms with only one projection for non-smooth and smooth optimization, respectively. To tackle the constraint, we introduce a penalty function hγ(x) parameterized by γ, which obeys the following certificate: there exist constants C ≥ 0 and λ > G/ρ such that\nhγ(x) ≥ λ[c(x)]+,∀x hγ(x) ≤ Cγ, ∀x such that c(x) ≤ 0.\n(4)\nFrom the above condition, it is clear that γ ≥ 0. It is notable that the penalty function hγ(x) will also depend on λ; however it will be set to a constant value, thus the dependence on λ is omitted. We will construct such a penalty function hγ(x) for non-smooth and smooth optimization in next two subsections. We propose to optimize the following augmented objective function\nmin x∈Rd Fγ(x) = f(x) + hγ(x). (5)\nWe can employ any applicable optimization algorithms to optimize Fγ(x) pretending that there is no constraint, and finally obtain a solution x̂T that is not necessarily feasible. In order to obtain a feasible solution, we perform one projection to get x̃T = ΠΩ(x̂T ). The following theorem allows us to convert the convergence of x̂T for Fγ(x) to that of x̃T for f(x).\nTheorem 1. LetA be any iterative optimization algorithm applied to minx Fγ(x) with T iterations, which starts with x1 and returns x̂T as the final solution, such that the following convergence of x̂T holds for any x ∈ Rd\nFγ(x̂T )− Fγ(x) ≤ BT (γ;x,x1), (6)\nwhere BT (γ;x,x1) → 0 when T → ∞. Suppose that Assumption 1 hold, then\nf(x̃T )− f(x∗) ≤ λρ\nλρ−G (Cγ +BT (γ;x∗,x1)), (7)\nwhere x̃T = ΠΩ[x̂T ] and x∗ is an optimal solution to (1).\nRemark: It is worth mentioning that we omit some constant factors in the convergence bound BT (γ;x,x1) that are irrelevant to our discussions. The notationBT (γ;x,x1) emphasizes that it is a function of γ and depends on x1 and a target solution x and it will be referred to as BT . In the next several subsections, we will see that by carefully choosing the penalty function hγ(x) we are able to provide nice convergence for smooth and non-smooth optimization with only one projection. In the above theorem, we assume the optimization algorithm A is deterministic. However, a similar result can be easily extended to a stochastic optimization algorithm A.\nProof. First, we consider c(x̂T ) ≤ 0, which implies that x̂T = x̃T . Due to the certificate of hγ(x), Fγ(x̃T ) ≥ f(x̃T ) and Fγ(x∗) ≤ f(x∗) + Cγ. Hence f(x̃T ) ≤ Fγ(x̂T ) ≤ Fγ(x∗) + BT (γ;x1,x∗) ≤ f(x∗) + Cγ + BT (γ;x1,x∗). Then (7) follows due to λρ/(λρ−G) ≥ 1. Next, we assume c(x̂T ) > 0. Inequality (6) implies that\nf(x̂T ) +λ[c(x̂T )]+ ≤ f(x∗) +Cγ+BT (γ;x∗,x1). (8)\nBy Assumption 1(i), we have [c(x̂T )]+ ≥ ρ‖x̂T − x̃T ‖. Combined with (8) we have\nλρ‖x̂T − x̃T ‖ ≤ f(x∗)− f(x̂T ) + Cγ +BT (γ;x∗,x1) ≤ G‖x̂T − x̃T ‖+ Cγ +BT (γ;x∗,x1),\nwhere the last inequality follows that fact f(x∗)−f(x̂T ) ≤ f(x∗)−f(x̃T ) +f(x̃T )−f(x̂T ) ≤ G‖x̂T − x̃T ‖ because the Lipschitz property and f(x∗) ≤ f(x̃T ). Therefore we have\n‖x̂T − x̃T ‖ ≤ Cγ +BT (γ;x∗,x1, )\nλρ−G .\nFinally, we obtain\nf(x̃T )− f(x∗) ≤ f(x̃T )− f(x̂T ) + f(x̂T )− f(x∗) ≤ G‖x̂T − x̃T ‖+ Cγ +BT (γ;x∗,x1)\n≤ λρ λρ−G (Cγ +BT (γ;x∗,x1))."
  }, {
    "heading": "4.1. Non-smooth Optimization",
    "text": "Since an optimal convergence rate for general non-smooth optimization with only one projection has been attained in (Mahdavi et al., 2012), in this subsection we present an optimal convergence result for strongly convex problems. For non-smooth optimization, we can choose\nh(x) = λ[c(x)]+,\nand hence γ = 0. We will use deterministic subgradient descent as an example to demonstrate the convergence for f(x), though many other optimization algorithms designed for non-smooth optimization are applicable (e.g.,\nthe stochastic subgradient method). The update of subgradient descent method is given by the following\nxt+1 = xt − ηt∂F (xt), t = 1, . . . , T, (9)\nwhere ηt is an appropriate step size. If f(x) is µ-strongly convex, the step size can be set as ηt = 1/(µt) and the final solution can be computed by the α-suffix averaging x̂T = 1αT ∑T t=(1−α)T+1 xt with α > 0 (Rakhlin et al., 2012), or by the polynomial decay averaging with x̂t = (1− s+1s+t )x̂t−1 + s+1 s+txt and s ≥ 1 (Shamir & Zhang, 2013). Both schemes can attain BT = O(1/(µT )) for the convergence of F (x) when f(x) is µ-strongly convex. Combining this with Theorem 1, we have the following convergence result with the proof omitted due to its simplicity.\nCorollary 2. Suppose that Assumption 1 holds and f(x) is µ-strongly convex. Set F (x) = f(x) + λ[c(x)]+ with λ ≥ G/ρ. Let (9) run for T iterations with ηt = 1/(µt). Let x̂T be computed by α-suffix averaging or the polynomial decay averaging. Then with only one projection x̃T = ΠΩ(x̂T ), we achieve\nf(x̃T )− f∗ ≤ λρ λρ−G (G+ λGc)\n2O(1)\nµT .\nRemark: We note that the O(1/(µT )) is also achieved for strongly convex optimization in (Zhang et al., 2013; Chen et al., 2016) but with a logarithmic number of projections. In contrast, Corollary 2 implies only one projection is sufficient to achieve the optimal convergence for strongly convex optimization."
  }, {
    "heading": "4.2. Smooth Optimization",
    "text": "For smooth optimization, we consider both f(x) and c(x) to be smooth 4. Let the smoothness parameter of f(x) and c(x) be Lf and Lc, respectively. In order to ensure the augmented function Fγ(x) to be still a smooth function, we construct the following penalty function\nhγ(x) = γ ln (1 + exp (λc(x)/γ)) . (10)\nThe following proposition shows that hγ(x) is a smooth function and obeys the condition in (4).\nProposition 1. Suppose c(x) is Lc-smooth and GcLipschitz continuous. The penalty function in (10) is a (λLc + λ2G2c 4γ )-smooth function and satisfies (i) hγ(x) ≥ λ[c(x)]+ and (ii) hγ(x) ≤ γ ln 2, ∀x such that c(x) ≤ 0.\nThen Fγ(x) is a smooth function and its smoothness parameter is given by LF = Lf +λLc+ λ2G2c\n4γ . Next, we will\n4it can be extended to when f(x) is non-smooth but its proximal mapping can be easily solved.\nestablish the convergence for f(x) using Nesterov’s optimal accelerated gradient (NAG) methods. The update of one variant of NAG can be written as follows\nxt+1 = yt −∇Fγ(yt)/LF yt+1 = xt+1 + βt+1(xt+1 − xt),\n(11)\nwhere the value of βt can be set to different values depending on whether f(x) is strongly convex or not (see Corollary 3). Previous work have established the convergence of x̂T = xT for Fγ(x), in particular BT = O(LFT 2 ) for smooth non-strongly convex optimization and BT = O ( LF exp(−T √ µ LF ) )\nfor smooth and strongly convex optimization. By combining these results with Theorem 1 and appropriately setting γ, we can achieve the following convergence of x̃T for f(x). Corollary 3. Suppose that Assumption 1 holds, dist(y0,Ω∗) ≤ D, f(x) is Lf -smooth and c(x) is Lc-smooth. Set Fγ(x) = f(x) + hγ(x) with λ > G/ρ and hγ(x) being (10). Let (11) run for T iterations and x̃T = ΠΩ(xT ).\n• If f(x) is convex, we can set γ = λGcD (T+1) √ 2 ln 2 , βt =\nτt−1−1 τt\n, where τt = 1+ √ 1+4τ2t−1 2 with τ0 = 1, and achieve\nf(x̃T )−f∗ ≤ λρ\nλρ−G\n[ λGcD √ 2 ln 2\nT + 1 +\n(Lf + λLc)D 2\n(T + 1)2 ] • If f(x) is µ-strongly convex, we can set γ = 1T 2α with α ∈ (1/2, 1) and βt = √ LF− √ µ√\nLF+ √ µ , and achieve f(x̃T )− f∗ ≤ O ( 1\nT 2α +\n1\nT 4α\n) ,\nas long as T ≥ ( Lf+λLc+λ\n2G2c/4 µ\n) 1 2(1−α)\n(4α lnT ) 1 1−α .\nRemark: The convergence results above indicate an O(1/ ) iteration complexity for smooth optimization and O(1/ 1/(2α)) with α ∈ (1/2, 1) for smooth and strongly convex optimization with only one projection. All omitted proofs can be found in (Yang et al., 2017)."
  }, {
    "heading": "5. Improved Convergence for Non-strongly Convex Optimization",
    "text": "In this section, we will develop improved convergence for non-strongly convex optimization at a price of a logarithmic number of projections by considering an additional condition on the target problem. To facilitate the presentation, we first introduce some notations. The -sublevel set S and -level set L of the problem (1) are denoted by S = {x ∈ Ω : f(x) ≤ f∗ + }, and L = {x ∈ Ω : f(x) = f∗ + }, respectively. Let x† denote the closest point in the -sublevel set S to x ∈ Ω, i.e.,\nx† = arg min u∈Ω ‖u− x‖2, s.t. f(u) ≤ f∗ + . (12)\nLet x∗ denote the closest optimal solution in Ω∗ to x, i.e., x∗ = arg minu∈Ω∗ ‖u− x‖2.\nIn this section, we will make the following additional assumption about the problem (1). Assumption 2. For a convex minimization problem (1), we assume (i) there exist x0 ∈ Ω and 0 ≥ 0 such that f(x0)− minx∈Ω f(x) ≤ 0; (ii) Ω∗ is a non-empty convex compact set; (iii) the optimization problem (1) satisfies a local error bound condition, i.e., there exist θ ∈ (0, 1] and σ > 0 such that for any x ∈ S we have dist(x,Ω∗) ≤ σ(f(x) − f∗)\nθ where Ω∗ denotes the optimal set and f∗ denotes the optimal value. Remark: we would like to remark that the new assumption only imposes mild conditions on the problem. In particular, Assumption 2 (i) supposes there is a lower bound of the optimal value f∗, which usually holds in machine learning applications where the objective function if non-negative; Assumption 2 (ii) ensures that S is also bounded (Rockafellar, 1970), therefore the σ in the local error bound is finite, which can be easily satisfied for a norm regularized or constraint problems; the local error bound condition holds for a broad family of functions (e.g., semi-algebraic functions or real subanalytic functions (Jerome Bolte, 2015; Yang & Lin, 2016)). In Section 7, we will also demonstrate several applications of the improved algorithms proposed in this section by establishing the local error bound condition.\nAlthough the local error bound condition is much weaker than the strong convexity assumption, below we will propose novel algorithms leveraging this condition with faster convergence and only a logarithmic number of projections."
  }, {
    "heading": "5.1. Non-smooth Optimization",
    "text": "To establish an improved convergence for non-smooth optimization, we develop a new algorithm shown in Algorithm 1 based on subgradient descent (GD) method, to which we refer as LoPGD. The algorithm runs for K epochs and each epoch employs GD for minimizing F (x) = f(x) + λ[c(x)]+ with a feasible solution xk−1 ∈ Ω as a starting point and t iterations of updates. At the end of each epoch, the averaged solution x̂k is projected into the constrained domain Ω and the solution xk will be used as the starting point for next epoch. The step size ηk is decreased by half every epoch starting from a given value η1. The theorem below establishes the iteration complexity of LoPGD and also exhibits the values of K, t and η1. To simplify notations, we let p = λρλρ−G and Ḡ = G+ λGc. Theorem 4. Suppose Assumptions 1 and 2 hold. Let η1 = 0 2pḠ2 , K = dlog2( 0/ )e and t = 4σ2p2Ḡ2 2(1−θ) in Algorithm 1, where θ and σ are constants appearing in the local error bound condition. Then f(xK)− f∗ ≤ 2 . Remark: Since the projection is only conducted at the end of each epoch and the total number of epochs is at\nAlgorithm 1 LoPGD 1: INPUT: K ∈ N+ , t ∈ N+, η1 2: Initialization: x0 ∈ Ω, 0 3: for k = 1, 2, . . . ,K do 4: Let xk1 = xk−1 5: for s = 1, 2, . . . , t− 1 do 6: Update xks+1 = x k s − ηk∂F (xks)\n7: end for 8: Let x̂k = ∑t s=1 x k s/t\n9: Let xk = ΠΩ[x̂k] and ηk+1 = ηk/2 10: end for\nAlgorithm 2 LoPNAG 1: INPUT: K ∈ N+ , t1, . . . , tK ∈ N+, γ1 2: Initialization: x0 ∈ Ω, 0 3: for k = 1, 2, . . . ,K do 4: Let yk0 = xk−1 5: for s = 0, 1, 2, . . . , tk − 1 do 6: Update xks+1 = y k s − 1Lk∇Fγk(x k s)\n7: Update yks+1 = x k s+1 + βs+1(x k s+1 − xks) 8: end for 9: Let x̂k = xktk , xk = ΠΩ[x̂k] and γk+1 = γk/2\n10: end for\nmost K = dlog2( 0/ )e, so the total number of projections is only a logarithmic number K. The iteration complexity in Theorem 4 is Õ(1/ 2(1−θ)) that improves the standard result of O(1/ 2) without strong convexity. With θ = 1/2, we can achieve Õ(1/ ) iteration complexity with only O(log(1/ )) projections."
  }, {
    "heading": "5.2. Smooth Optimization",
    "text": "Similar to non-smooth optimization, we also develop a new algorithm based on NAG shown in Algorithm 2, where Fγ(x) is defined using hγ(x) in (10), Lk = LFγk is the smoothness parameter of Fγk and βs = τs−1−1 τs\n, s = 1, . . . , is a sequence with τs updated as in Corollary 3. We refer to this algorithm as LoPNAG. The key idea is to use to a sequence of reducing values for γk instead of using a small value as in Corollary 3, and solve each augmented unconstrained problem Fγk(x) approximately with one projection. The theorem below exhibits the iteration complexity of LoPNAG and reveals the values ofK, γ1 and t1, . . . , tK . To simplify notations, we let L̄ = Lf + λLc.\nTheorem 5. Suppose Assumptions 1 and 2 hold and f(x) is Lf -smooth and c(x) is Lc-smooth. Let γ1 = 06p ln 2 , K = dlog2( 0/ )e and tk = σ\n1−θ max{λGcp √ 18 ln 2, √ 12(Lf + λLc) 0/2k−1}\nin Algorithm 2, where θ and σ are constants appearing in the local error bound condition. Then f(xK)− f∗ ≤ 2 . Remark: It is not difficult to show that the total number of iterations is bounded by Õ(1/ 1−θ), which improves the one in Corollary 3 without strong convexity. If f(x) is a\nsimple non-smooth function whose proximal mapping can be easily computed (e.g., `1 norm), we can replace step 6 in Algorithm 2 by a proximal mapping to handle f(x), which gives the same convergence result in Theorem 5. An example is presented in Section 7 for compressive sensing with θ = 1/2."
  }, {
    "heading": "6. Discussion of Assumption 1 (i)",
    "text": "One might note that a key condition for developing the theory with reduced projections is Assumption 1 (i). Although Mahdavi et al. (2012) has briefly mentioned that the condition can be satisfied for a PSD cone or a Polytope (a bounded polyhedron), their discussion lacks of details in particular on the value of ρ in (2) or (3). Below, we discuss the condition in details about three types of constraints.\nPolyhedral constraints. First, we show that when c(x) is a polyhedral function, i.e., its epigraph is a polyhedron (not necessarily bounded), the inequality (3) is satisfied. To this end, we explore the polyhedral error bound (PEB) condition (Gilpin et al., 2012; Yang & Lin, 2016). In particular, if we consider an optimization problem, minx∈Rd h(x), where the epigraph of h(x) is polyhedron. Let H∗ denote the optimal set and h∗ denote the optimal value of the problem above. The PEB says that there exists ρ > 0 such that for any x ∈ Rd\ndist(x,H∗) ≤ (h(x)− h∗)/ρ. (13)\nTo show that the inequality (3) holds for a polyhedral function c(·), we can consider the optimization problem minx∈Rd [c(x)]+. The optimal set of the above problem is given by H∗ = {x ∈ Rd : c(x) ≤ 0}. For any x such that c(x) > 0, let x\\ = arg minc(u)≤0 ‖u − x‖2 be the closest point in the optimal set to x. Therefore if c(·) is a polyhedral function so does [c(x)]+, by the PEB condition (13) there exists a ρ > 0 such that ‖x − x\\‖ ≤ ([c(x)]+ − minx[c(x)]+)/ρ = [c(x)]+/ρ. Let us consider a concrete example, where the problem has a set of affine inequalities c>i x − bi ≤ 0, i = 1, . . . ,m. There are two methods to encode this into a single constraint function c(x) ≤ 0. The first method is to use c(x) = max1≤i≤m c > i x − bi, which is a polyhedral function and therefore satisfies (3). The second method is to use c(x) = ‖[Cx − b]+‖, where [a]+ = max(0,a) and C = (c1, . . . , cm)\n>. Thus [c(x)]+ = ‖[Cx − b]+‖. The inequality (3) is then guaranteed by Hoffman’s bound and the parameter ρ is given by the minimum non-zero eigenvalue of C>C (Wang & Lin, 2014). Note that the projection onto a polyhedron is a linear constrained quadratic programming problem, and the linear optimization over a polyhedron is a linear programming problem. Both have polynomial time complexity that would be high if m and d are large (Karmarkar, 1984; Kozlov et al., 1980).\nQuadratic constraint. A quadratic constraint can take the form of ‖Ax − y‖2 ≤ τ , where A ∈ Rm×d and y ∈ Rm. Such a constraint appears in compressive sensing (Candès & Wakin, 2008)5, where the goal is to reconstruct a sparse high-dimensional vector x from a small number of noisy measurements y = Ax + ε ∈ Rm with m d. The corresponding optimization problem is\nminx∈Rd ‖x‖1, s.t. ‖Ax− y‖2 ≤ τ. (14)\nwhere τ ≥ ‖ε‖2 is an upper bound on the magnitude of the noise. To check the Assumption 1(i), we note that c(x) = ‖Ax−y‖2−τ and∇c(x) = A>(Ax−y). Let us consider that A has a full row rank 6 and denote by v = Ax − y, then on the boundary c(x) = 0 we have ‖v‖ = √ τ and\n‖A>v‖ ≥ √ τλmin(AA>), where λmin(AA>) > 0 is the minimum eigenvalue of AA> ∈ Rm×m. Therefore the Assumption 1(i) is satisfied with ρ = √ τλmin(AA>). It is notable that the projection and the linear optimization under the quadratic constraint require solving a quadratic programming problem and therefore could be expensive.\nPSD constraint. A PSD constraintX 0 forX ∈ Rd×d can be written as an inequality constraint −λmin(X) ≤ 0, where λmin(X) denotes the minimum eigen-value of X . The subgradient of c(X) = −λmin(X) when λmin(X) = 0 is given by Conv{−uu>|‖u‖ = 1, Xu = 0}, i.e., the convex hull of the outer products of normalized vectors in the null space of the matrix X . In (Yang et al., 2017), we show that if the dimension of the null space of X is r with 1 ≤ r ≤ d, the norm of the subgradient of c(X) on the boundary c(X) = 0 is lower bounded by ρ = 1√\nr ≥ 1√ d .\nFinally, we note that computing a subgradient of [c(X)]+ only needs to compute one eigen-vector corresponding to the smallest eigen-value. In contrast, both projection and linear optimization under a PSD constraint could be very expensive for high-dimensional problems. In particular, the projection onto a PSD domain needs to conduct a singular value decomposition. The linear optimization over a PSD cone is ill-posed due to that PSD cone is not compact (the solution is either 0 or infinity). One may add an artificial constraint on the upper bound of the eigen-values. According to (Jaggi, 2013), the time complexity for solving this linear optimization problem approximately up to an accuracy level ′ is O(Nd1.5/ ′2.5) with N being the number of non-zeros in the gradient and ′ decreasing iteratively required in the Frank-Wolfe method, which could be much more expensive especially for high-dimensional problems and in later iterations than computing the first eigen-pairs at each iteration in our methods.\n5Here we use the square constraint to make it a smooth function so that the proposed algorithms for smooth optimization are applicable by using proximal gradient mapping to handle the `1 norm.\n6which is reasonable because m d."
  }, {
    "heading": "7. Applications",
    "text": ""
  }, {
    "heading": "7.1. Compressive Sensing",
    "text": "We first consider a compressive sensing problem in (14). Becker et al. (2011) proposed an optimization algorithm based on the Nesterov’s smoothing and the Nesterov’s optimal method for the smoothed problem, known as NESTA. It needs to perform the projection into the domain ‖Ax − y‖2 ≤ τ at every iteration and has an iteration complexity of O(1/ ). In contrast, the presented algorithm with only one projection in Section 4.2 using Nesterov’s accelerated proximal gradient method (Beck & Teboulle, 2009) to solve the unconstrained problem enjoys an iteration complexity ofO(1/ ). Moreover, we present a theorem below showing that the problem (14) satisfies the local error bound condition with θ = 1/2, and hence the presented LoPNAG enjoys an Õ(1/ √ ) iteration complexity with only a logarithmic number of projections.\nTheorem 6. Let f(x) = ‖x‖1, c(x) = ‖Ax − y‖2 − τ , Ω∗ denote the optimal set and f∗ be the optimal solution to (14). Assume that there exists x0 such that ‖Ax0−y‖2 < τ and 0 6∈ Ω∗. Then for any > 0, x ∈ Rd such that c(x) ≤ 0 and f(x) ≤ f∗ + , there exists 0 < σ < ∞ such that dist(x,Ω∗) ≤ σ(f(x) − f∗)1/2. Hence, LoPNAG can have an iteration complexity of Õ(1/ √ ) with only O(log(1/ )) projections.\nNext, we demonstrate the effectiveness of the LoPNAG for solving the compressive sensing problem in (14) by comparing with NESTA. We generate a synthetic data for testing. In particular, we generate a random measurement matrix A ∈ Rm×d with m = 1000 and d = 5000. The entries of the matrix A are generated independently with the uniform distribution over the interval [−1,+1]. The vector x∗ ∈ Rd is generated with the same distribution at 100 randomly chosen coordinates. The noise ε ∈ Rm is a dense vector with independent random entries with the uniform distribution over the interval [−ζ, ζ], where ζ is the noise magnitude and is set to 0.01. Finally the vector y was obtained as y = Ax∗ + ε.\nWe use the Matlab package of NESTA 7. For fair comparison, we also use the projection code in the NESTA package for conducting projection. To handle the unknown smoothness parameter in the proposed algorithm, we use the backtracking technique (Beck & Teboulle, 2009). The parameter γ is initially set to 0.001 and decreased by half every 5000 iterations after a projection and the target smoothing parameter in NESTA is set to 10−5. For the value of λ in LoPNAG, we tune it from its theoretical value to several smaller values and choose the one that yields the fastest convergence. We report the results in Table 7.1, which include different number of iterations, the corresponding\n7http://statweb.stanford.edu/˜candes/ nesta/\nnumber of projections, the recovery error of the found solution compared to the underlying true sparse solution, the objective value (i.e., the `1 norm of the found solution) and the running time. Note that each iteration of NESTA requires two projections because it maintains two extra sequence of solutions. From the results, we can see that LoPNAG converges significantly faster than NESTA. Even with only one projection, we are able to obtain a better solution than that of NESTA after running 10000 iterations."
  }, {
    "heading": "7.2. High-dimensional Distance Metric Learning",
    "text": "Consider the following distance metric learning problem:\nmin A 0\n1 2|E| ∑\n(i,j)∈E\n(1−yij−‖xi−xj‖2A)2 + τ‖A‖off1 , (15)\nwhere E denotes all pairs of training examples, yij = 1 indicates xi,xj belong to the same class and yij = −1 indicates they belong to different classes, ‖z‖2A = z>Az and ‖A‖off1 = ∑ i 6=j |Aij |. We note that such a formulation is useful for high dimensional problems due to the `1 regularizer. A similar formulation with different forms of loss function has been adopted in literature (Qi et al., 2009). We consider the square loss because it gives us faster convergence with a logarithmic number of projections by LoPGD. Due to the presence of the non-smooth PSD constraint and the `1 regularizer, Nesterov’s accelerated proximal gradient methods can not be applied efficiently to solving (15) and the augmented unconstrained problem. Nevertheless, we can apply the proposed LoPGD method for solving the problem with a logarithmic number of projections. Regarding the constant θ in the local error bound condition for (15), it still remains an open problem. Nonetheless, a local error bound condition with θ = 0.5 might be established under certain regularity condition of the problem (Zhou & So, 2015; Cui et al., 2017). For example, Cui et al. (2017) provided a direct analysis of a local error bound condition with θ = 0.5 for a class of constrained convex symmetric matrix optimization problems\nregularized by nonsmooth spectral functions (including the indicator function of a PSD constraint). They established sufficient conditions (Theorem 16) for a local error bound condition with θ = 0.5 to hold, which reduces to a regularity condition for (15) depending on the optimal solutions of the problem. A thorough analysis of the regularity condition is much more involved and left as an open problem.\nNext, we demonstrate the empirical performance of LoPGD for solving (15). We use the colon-cancer data available on libsvm web portal, which has 2000 features and 62 examples. Fourty examples are used as training examples to generate 780 pairs to learn the distance metric. The regularization parameter is set to τ = 0.001. We compare LoPGD, gradient descent method with only one projection (referred to as OPGD), and standard projected GD (referred to PGD). The step size in PGD and OPGD is set to η0/ √ t, where t is the iteration index. We use the same tuned initial step size for all algorithms. The number of iterations per-epoch in LoPGD is set to 1000. The penalization parameter λ in both OPGD and LoPGD is tuned and set to 10. In Table 2, we report the objective values, the #of iterations/projections, and running time across the first 8000 iterations. We can see that LoPGD converges dramatically faster than PGD and also much faster than OPGD."
  }, {
    "heading": "8. Conclusion",
    "text": "We have developed a general theory of optimization with only one projection for a family of inequality constrained convex optimization problems. It yields an improved iteration complexity for smooth optimization compared with non-smooth optimization. By exploring the local error bound condition, we further develop new algorithms with a logarithmic number of projections and achieve better convergence for both smooth and non-smooth optimization without strong convexity assumption. Applications in compressive sensing and distance metric learning demonstrate the effectiveness of the proposed improved algorithms."
  }, {
    "heading": "Acknowledgements",
    "text": "We are grateful to all anonymous reviewers for their helpful comments. T. Yang is partially supported by National Science Foundation (IIS-1463988, IIS-1545995). L. Zhang thanks the support from NSFC (61603177) and JiangsuSF (BK20160658)."
  }],
  "year": 2017,
  "references": [{
    "title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
    "authors": ["Beck", "Amir", "Teboulle", "Marc"],
    "venue": "SIAM J. Img. Sci.,",
    "year": 2009
  }, {
    "title": "A fast and accurate first-order method for sparse recovery",
    "authors": ["Becker", "Stephen", "Bobin", "Jérôme", "Candès", "Emmanuel J. Nesta"],
    "venue": "SIAM J. Img. Sci.,",
    "year": 2011
  }, {
    "title": "Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural Computation Series)",
    "authors": ["Bertsekas", "Dimitri P"],
    "venue": "Athena Scientific,",
    "year": 1996
  }, {
    "title": "An introduction to compressive sampling",
    "authors": ["Candès", "Emmanuel J", "Wakin", "Michael B"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2008
  }, {
    "title": "Optimal stochastic strongly convex optimization with a logarithmic number of projections",
    "authors": ["Chen", "Jianhui", "Yang", "Tianbao", "Lin", "Qihang", "Zhang", "Lijun", "Chang", "Yi"],
    "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence (UAI),",
    "year": 2016
  }, {
    "title": "A light touch for heavily constrained SGD",
    "authors": ["Cotter", "Andrew", "Gupta", "Maya R", "Pfeifer", "Jan"],
    "venue": "In Proceedings of the 29th Conference on Learning Theory (COLT),",
    "year": 2016
  }, {
    "title": "Quadratic growth conditions for convex matrix optimization problems associated with spectral functions",
    "authors": ["Cui", "Ying", "Ding", "Chao", "Zhao", "Xinyuan"],
    "year": 2017
  }, {
    "title": "An algorithm for quadratic programming",
    "authors": ["Frank", "Marguerite", "Wolfe", "Philip"],
    "venue": "Naval Research Logistics (NRL),",
    "year": 1956
  }, {
    "title": "Faster rates for the frankwolfe method over strongly-convex sets",
    "authors": ["Garber", "Dan", "Hazan", "Elad"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "First-order algorithm with log(1/epsilon) convergence for epsilon-equilibrium in two-person zero-sum games",
    "authors": ["Gilpin", "Andrew", "Peña", "Javier", "Sandholm", "Tuomas"],
    "venue": "Math. Program.,",
    "year": 2012
  }, {
    "title": "Projection-free online learning",
    "authors": ["Hazan", "Elad", "Kale", "Satyen"],
    "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
    "year": 2012
  }, {
    "title": "On the linear convergence of the proximal gradient method for trace norm regularization",
    "authors": ["Hou", "Ke", "Zhou", "Zirui", "So", "Anthony Man-Cho", "Luo", "Zhi-Quan"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Revisiting frank-wolfe: Projection-free sparse convex optimization",
    "authors": ["Jaggi", "Martin"],
    "venue": "In Proceedings of the International Conference on Machine Learning (ICML), pp",
    "year": 2013
  }, {
    "title": "From error bounds to the complexity of first-order descent methods for convex functions",
    "authors": ["Jerome Bolte", "Trong Phong Nguyen", "Juan Peypouquet Bruce Suter"],
    "venue": "CoRR, abs/1510.08234,",
    "year": 2015
  }, {
    "title": "A new polynomial-time algorithm for linear programming",
    "authors": ["N. Karmarkar"],
    "venue": "In Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing,",
    "year": 1984
  }, {
    "title": "Polynomiale Loesbarkeit der konvexen quadratischen Programmierung",
    "authors": ["M.K. Kozlov", "S.P. Tarasov", "L.G. Khachiyan"],
    "venue": "Zh. Vychisl. Mat. Mat. Fiz.,",
    "year": 1980
  }, {
    "title": "Block-coordinate frank-wolfe optimization for structural svms",
    "authors": ["Lacoste-Julien", "Simon", "Jaggi", "Martin", "Schmidt", "Mark", "Pletscher", "Patrick"],
    "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
    "year": 2013
  }, {
    "title": "Stochastic gradient descent with only one projection",
    "authors": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2012
  }, {
    "title": "Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization",
    "authors": ["Nesterov", "Yurii"],
    "venue": "Kluwer Academic Publishers,",
    "year": 2004
  }, {
    "title": "Making gradient descent optimal for strongly convex stochastic optimization",
    "authors": ["Rakhlin", "Alexander", "Shamir", "Ohad", "Sridharan", "Karthik"],
    "venue": "In Proceedings of the 29th international conference on Machine learning (ICML),",
    "year": 2012
  }, {
    "title": "Convex Analysis. Princeton mathematical series",
    "authors": ["R.T. Rockafellar"],
    "year": 1970
  }, {
    "title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
    "authors": ["Shamir", "Ohad", "Zhang", "Tong"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
    "year": 2013
  }, {
    "title": "Non-asymptotic convergence analysis of inexact gradient methods for machine learning without strong convexity",
    "authors": ["So", "Anthony Man-Cho"],
    "venue": "CoRR, abs/1309.0113,",
    "year": 2013
  }, {
    "title": "Iteration complexity of feasible descent methods for convex optimization",
    "authors": ["Wang", "Po-Wei", "Lin", "Chih-Jen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["Weinberger", "Kilian Q", "Blitzer", "John", "Saul", "Lawrence K"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2006
  }, {
    "title": "Distance metric learning with application to clustering with sideinformation",
    "authors": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2003
  }, {
    "title": "Homotopy smoothing for non-smooth problems with lower complexity than o(1",
    "authors": ["Xu", "Yi", "Yan", "Lin", "Qihang", "Yang", "Tianbao"],
    "venue": "Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Rsg: Beating subgradient method without smoothness and strong convexity",
    "authors": ["Yang", "Tianbao", "Lin", "Qihang"],
    "year": 2016
  }, {
    "title": "A richer theory of convex constrained optimization with reduced projections and improved rates (the long version of icml",
    "authors": ["Yang", "Tianbao", "Lin", "Qihang", "Zhang", "Lijun"],
    "venue": "paper). CoRR,",
    "year": 2017
  }, {
    "title": "O(logt) projections for stochastic optimization of smooth and strongly convex functions",
    "authors": ["Zhang", "Lijun", "Yang", "Tianbao", "Jin", "Rong", "He", "Xiaofei"],
    "venue": "In Proceedings of the International Conference on Machine Learning (ICML),",
    "year": 2013
  }, {
    "title": "A unified approach to error bounds for structured convex optimization problems",
    "authors": ["Zhou", "Zirui", "So", "Anthony Man-Cho"],
    "year": 2015
  }, {
    "title": "L1pnorm regularization: Error bounds and convergence rate analysis of first-order methods",
    "authors": ["Zhou", "Zirui", "Zhang", "Qi", "So", "Anthony Man-Cho"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
    "year": 2015
  }],
  "id": "SP:55de2dfbeb9483531470430b3222e51754033d8f",
  "authors": [{
    "name": "Tianbao Yang",
    "affiliations": []
  }, {
    "name": "Qihang Lin",
    "affiliations": []
  }, {
    "name": "Lijun Zhang",
    "affiliations": []
  }],
  "abstractText": "This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields O(1/ ) iteration complexity, which improves over the O(1/ ) iteration complexity established before for nonsmooth optimization and can be further reduced under strong convexity. Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve an iteration complexity of Õ(1/ 2(1−θ)) for non-smooth optimization and Õ(1/ 1−θ) for smooth optimization, where θ ∈ (0, 1] appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions. Novel applications in solving the constrained `1 minimization problem and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms. The University of Iowa, Iowa City, IA 52242, USA National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>. This is the long version of our paper appearing in the Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).",
  "title": "A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates"
}