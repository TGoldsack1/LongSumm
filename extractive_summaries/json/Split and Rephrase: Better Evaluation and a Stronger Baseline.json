{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 719–724 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n719"
  }, {
    "heading": "1 Introduction",
    "text": "Processing long, complex sentences is challenging. This is true either for humans in various circumstances (Inui et al., 2003; Watanabe et al., 2009; De Belder and Moens, 2010) or in NLP tasks like parsing (Tomita, 1986; McDonald and Nivre, 2011; Jelı́nek, 2014) and machine translation (Chandrasekar et al., 1996; Pouget-Abadie et al., 2014; Koehn and Knowles, 2017). An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing.\nA recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it “Split-andRephrase”. The dataset includes 1,066,115 instances mapping a single complex sentence to a sequence of sentences that express the same meaning, together with RDF triples that describe their semantics. They considered two system setups: a text-to-text setup that does not use the accompany-\ning RDF information, and a semantics-augmented setup that does. They report a BLEU score of 48.9 for their best text-to-text system, and of 78.7 for the best RDF-aware one. We focus on the text-totext setup, which we find to be more challenging and more natural.\nWe begin with vanilla SEQ2SEQ models with attention (Bahdanau et al., 2015) and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method. However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated—the same fact is mentioned in multiple output sentences; and (3) some facts are missing— mentioned in the input but omitted in the output.\nThe model learned to memorize entity-fact pairs instead of learning to split and rephrase. Indeed, feeding the model with examples containing entities alone without any facts about them causes it to output perfectly phrased but unsupported facts (Table 3). Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which—coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of distinct simple sentences—helps to explain the high BLEU score.\nTo aid further research on the task, we propose a more challenging split of the data. We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks (Gu et al., 2016; Merity et al., 2017; See et al., 2017). On the original split, our models outperform the\nbest baseline of Narayan et al. (2017) by up to 8.68 BLEU, without using the RDF triples. On the new split, the vanilla SEQ2SEQ models break completely, while the copy-augmented models perform better. In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal. We report results on this dataset as well. The code and data to reproduce our results are available on Github.1 We encourage future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one."
  }, {
    "heading": "2 Preliminary Experiments",
    "text": "Task Definition In the split-and-rephrase task we are given a complex sentence C, and need to produce a sequence of simple sentences T1, ..., Tn, n ≥ 2, such that the output sentences convey all and only the information in C. As additional supervision, the split-and-rephrase dataset associates each sentence with a set of RDF triples that describe the information in the sentence. Note that the number of simple sentences to generate is not given as part of the input.\nExperimental Details We focus on the task of splitting a complex sentence into several simple ones without access to the corresponding RDF triples in either train or test time. For evaluation we follow Narayan et al. (2017) and compute the averaged individual multi-reference BLEU score for each prediction.2 We split each prediction to\n1https://github.com/biu-nlp/ sprp-acl2018\n2Note that this differs from ”normal” multi-reference BLEU (as implemented in multi-bleu.pl) since the number of references differs among the instances in the test-\nsentences3 and report the average number of simple sentences in each prediction, and the average number of tokens for each simple sentence. We train vanilla sequence-to-sequence models with attention (Bahdanau et al., 2015) as implemented in the OPENNMT-PY toolkit (Klein et al., 2017).4 Our models only differ in the LSTM cell size (128, 256 and 512, respectively). See the supplementary material for training details and hyperparameters. We compare our models to the baselines proposed in Narayan et al. (2017). HYBRIDSIMPL and SEQ2SEQ are text-to-text models, while the other reported baselines additionally use the RDF information.\nResults As shown in Table 2, our 3 models obtain higher BLEU scores then the SEQ2SEQ baseline, with up to 28.35 BLEU improvement, despite being single-layer models vs. the 3-layer models used in Narayan et al. (2017). A possible explanation for this discrepancy is the SEQ2SEQ baseline using a dropout rate of 0.8, while we use 0.3 and only apply it on the LSTM outputs. Our results are also better than the MULTISEQ2SEQ and SPLIT-MULTISEQ2SEQ models, which use explicit RDF information. We also present the macro-average5 number of sim-\nset. 3Using NLTK v3.2.5 https://www.nltk.org/ 4https://github.com/OpenNMT/OpenNMT-py commit d4ab35a 5Since the number of references varies greatly from one complex sentence to another, (min: 1, max: 76,283, median: 16) we avoid bias towards the complex sentences with many references by performing macro average, i.e. we first average the number of simple sentences in each reference among the\nple sentences per complex sentence in the reference rephrasings (REFERENCE) showing that the SPLIT-MULTISEQ2SEQ and SPLIT-SEQ2SEQ baselines may suffer from over-splitting since the reference splits include 2.52 simple sentences on average, while the mentioned models produced 2.84 sentences.\nAnalysis We begin analyzing the results by manually inspecting the model’s predictions on the validation set. This reveals three common kinds of mistakes as demonstrated in Table 3: unsupported facts, repetitions, and missing facts. All the unsupported facts seem to be related to entities mentioned in the source sentence. Inspecting the attention weights (Figure 1) reveals a worrying trend: throughout the prediction, the model focuses heavily on the first word in of the first entity (“A wizard of Mars”) while paying little attention to other cues like “hardcover”, “Diane” and\nreferences of a specific complex sentence, and then average these numbers.\n“the ISBN number”. This explains the abundance of “hallucinated” unsupported facts: rather than learning to split and rephrase, the model learned to identify entities, and spit out a list of facts it had memorized about them. To validate this assumption, we count the number of predicted sentences which appeared as-is in the training data. We find that 1645 out of the 1693 (97.16%) predicted sentences appear verbatim in the training set. Table 1 gives more detailed statistics on the WEBSPLIT dataset.\nTo further illustrate the model’s recognize-andspit strategy, we compose inputs containing an entity string which is duplicated three times, as shown in the bottom two rows of Table 3. As expected, the model predicted perfectly phrased and correct facts about the given entities, although these facts are clearly not supported by the input."
  }, {
    "heading": "3 New Data-split",
    "text": "The original data-split is not suitable for measuring generalization, as it is susceptible to “cheating” by fact memorization. We construct a new train-development-test split to better reflect our expected behavior from a split-and-rephrase model. We split the data into train, development and test sets by randomly dividing the 5,554 distinct complex sentences across the sets, while using the provided RDF information to ensure that:\n1. Every possible RDF relation (e.g., BORNIN, LOCATEDIN) is represented in the training set (and may appear also in the other sets).\n2. Every RDF triplet (a complete fact) is represented only in one of the splits.\nWhile the set of complex sentences is still divided roughly to 80%/10%/10% as in the original split, now there are nearly no simple sentences in\nthe development and test sets that appear verbatim in the train-set. Yet, every relation appearing in the development and test sets is supported by examples in the train set. We believe this split strikes a good balance between challenge and feasibility: to succeed, a model needs to learn to identify relations in the complex sentence, link them to their arguments, and produce a rephrasing of them. However, it is not required to generalize to unseen relations. 6\nThe data split and scripts for creating it are available on Github.7 Statistics describing the data split are detailed in Table 4."
  }, {
    "heading": "4 Copy-augmented Model",
    "text": "To better suit the split-and-rephrase task, we augment the SEQ2SEQ models with a copy mechanism. Such mechanisms have proven to be beneficial in similar tasks like abstractive summarization (Gu et al., 2016; See et al., 2017) and language modeling (Merity et al., 2017). We hypothesize that biasing the model towards copying will improve performance, as many of the words in the simple sentences (mostly corresponding to entities) appear in the complex sentence, as evident by the relatively high BLEU scores for the SOURCE baseline in Table 2.\nCopying is modeled using a “copy switch” probability p(z) computed by a sigmoid over a learned composition of the decoder state, the context vector and the last output embedding. It interpolates the psoftmax distribution over the target vocabulary and a copy distribution pcopy over the source sentence tokens. pcopy is simply the computed attention weights. Once the above distribu-\n6The updated dataset (v1.0, published by Narayan et al. after this work was accepted) follows (2) above, but not (1).\n7https://github.com/biu-nlp/ sprp-acl2018\ntions are computed, the final probability for an output word w is:\np(w) = p(z = 1)pcopy(w) + p(z = 0)psoftmax(w)\nIn case w is not present in the output vocabulary, we set psoftmax(w) = 0. We refer the reader to See et al. (2017) for a detailed discussion regarding the copy mechanism."
  }, {
    "heading": "5 Experiments and Results",
    "text": "Models with larger capacities may have greater representation power, but also a stronger tendency to memorize the training data. We therefore perform experiments with copy-enhanced models of varying LSTM widths (128, 256 and 512). We train the models using the negative log likelihood of p(w) as the objective. Other than the copy mechanism, we keep the settings identical to those in Section 2. We train models on the original split, our proposed data split and the v1.0 split.\nResults Table 5 presents the results. On the original data-split, the COPY512 model outperforms all baselines, improving over the previous best by 8.68 BLEU points. On the new data-split, as expected, the performance degrades for all models, as they are required to generalize to sentences not seen during training. The copy-augmented models perform better than the baselines in this case as well, with a larger relative gap which can be explained by the lower lexical overlap between the train and the test sets in the new split. On the v1.0 split the results are similar to those on our split,\nin spite of it being larger (1,331,515 vs. 886,857 examples), indicating that merely adding data will not solve the task.\nAnalysis We inspect the models’ predictions for the first 20 complex sentences of the original and new validation sets in Table 7. We mark each simple sentence as being “correct” if it contains all and only relevant information, “unsupported” if it contains facts not present in the source, and “repeated” if it repeats information from a previous sentence. We also count missing facts. Figure 2 shows the attention weights of the COPY512 model for the same sentence in Figure 1. Reassuringly, the attention is now distributed more evenly over the input symbols. On the new splits, all models perform catastrophically. Table 6 shows outputs from the COPY512 model when trained on the new split. On the original split, while SEQ2SEQ128 mainly suffers from missing information, perhaps due to insufficient memorization capacity, SEQ2SEQ512 generated the most unsupported sentences, due to overfitting or memorization. The overall number of issues is clearly reduced in the copy-augmented models."
  }, {
    "heading": "6 Conclusions",
    "text": "We demonstrated that a SEQ2SEQ model can obtain high scores on the original split-and-rephrase task while not actually learning to split-andrephrase. We propose a new and more challenging data-split to remedy this, and demonstrate that the cheating SEQ2SEQ models fail miserably on the new split. Augmenting the SEQ2SEQ models with a copy-mechanism improves performance on both data splits, establishing a new competitive baseline for the task. Yet, the split-and-rephrase task (on the new split) is still far from being solved. We strongly encourage future research to evaluate on our proposed split or on the recently released version 1.0 of the dataset, which is larger and also addresses the overlap issues mentioned here."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Shashi Narayan and Jan Botha for their useful comments. The work was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), the Israeli Science Foundation (grant number 1555/15), and the German Research Foundation via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1)."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Motivations and methods for text simplification",
    "authors": ["Raman Chandrasekar", "Christine Doran", "Bangalore Srinivas."],
    "venue": "Proceedings of the 16th conference on Computational linguistics. Association for Computational Linguistics.",
    "year": 1996
  }, {
    "title": "Text simplification for children",
    "authors": ["Jan De Belder", "Marie-Francine Moens."],
    "venue": "Proceedings of the SIGIR workshop on accessible search systems. ACM.",
    "year": 2010
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
    "year": 2016
  }, {
    "title": "Text simplification for reading assistance: a project note",
    "authors": ["Kentaro Inui", "Atsushi Fujita", "Tetsuro Takahashi", "Ryu Iida", "Tomoya Iwakura."],
    "venue": "Proceedings of the second international workshop on Paraphrasing-Volume 16. Association for Computa-",
    "year": 2003
  }, {
    "title": "Improvements to dependency parsing using automatic simplification of data",
    "authors": ["Tomáš Jelı́nek"],
    "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)",
    "year": 2014
  }, {
    "title": "Opennmt: Open-source toolkit for neural machine translation",
    "authors": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush."],
    "venue": "Proceedings of ACL 2017, System Demonstrations. Association for Com-",
    "year": 2017
  }, {
    "title": "Six challenges for neural machine translation",
    "authors": ["Philipp Koehn", "Rebecca Knowles."],
    "venue": "Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics, Vancouver.",
    "year": 2017
  }, {
    "title": "Analyzing and integrating dependency parsers",
    "authors": ["Ryan McDonald", "Joakim Nivre."],
    "venue": "Computational Linguistics 37.",
    "year": 2011
  }, {
    "title": "Pointer sentinel mixture models",
    "authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Split and rephrase",
    "authors": ["Shashi Narayan", "Claire Gardent", "Shay B. Cohen", "Anastasia Shimorina."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation",
    "authors": ["Jean Pouget-Abadie", "Dzmitry Bahdanau", "Bart van Merrienboer", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Get to the point: Summarization with pointer-generator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
    "year": 2017
  }, {
    "title": "Efficient parsing for natural languagea fast algorithm for practical systems",
    "authors": ["Masaru Tomita."],
    "venue": "int. series in engineering and computer science.",
    "year": 1986
  }, {
    "title": "Facilita: reading assistance for low-literacy readers",
    "authors": ["Willian Massami Watanabe", "Arnaldo Candido Junior", "Vinı́cius Rodriguez Uzêda", "Renata Pontin de Mattos Fortes", "Thiago Alexandre Salgueiro Pardo", "Sandra Maria Aluı́sio"],
    "year": 2009
  }],
  "id": "SP:730b6ad1a6a2921cdcd41f6ffcc3db95c07e584e",
  "authors": [{
    "name": "Roee Aharoni",
    "affiliations": []
  }],
  "abstractText": "Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copymechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.",
  "title": "Split and Rephrase: Better Evaluation and a Stronger Baseline"
}