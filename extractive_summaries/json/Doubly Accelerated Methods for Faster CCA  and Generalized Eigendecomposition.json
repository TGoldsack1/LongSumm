{
  "sections": [{
    "text": "√ ε rather than the eigengap."
  }, {
    "heading": "1 Introduction",
    "text": "The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics. Algorithms solving these problems are often used to extract features to compare large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis & Mineiro, 2014), word embeddings (Dhillon et al., 2011), and many others.\nGenEV. Given two symmetric matrices A,B ∈ Rd×d whereB is positive definite. The GenEV problem is to find generalized eigenvectors v1, . . . , vd where each vi satisfies\nvi ∈ arg max v∈Rd ∣∣v>Av∣∣ s.t. { v>Bv = 1 v>Bvj = 0 ∀j ∈ [i− 1]\nThe values λi def = v>i Avi are known as the generalized eigenvalues, and it satisfies |λ1| ≥ · · · |λd|. Following the *Equal contribution . Future version of this paper shall be found at http://arxiv.org/abs/1607.06017. 1Microsoft Research 2Princeton University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntradition of (Wang et al., 2016; Garber & Hazan, 2015), we\nassume without loss of generality that λi ∈ [−1, 1].\nCCA. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY\n>Y , the CCA problem is to find canonical-correlation vectors {(φi, ψi)}ri=1 where r = min{dx, dy} and each pair (φi, ψi) ∈ arg max\nφ∈Rdx ,ψ∈Rdy\n{ φ>Sxyψ } such that { φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈ [i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈ [i− 1]\nThe values σi def = φ>i Sxyψi ≥ 0 are known as the canonical-correlation coefficients, and\n1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisfied.\nIt is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx, Syy} ∈ Rd×d andA = [[0, Sxy]; [S>xy, 0]] ∈ Rd×d for d def = dx+dy; see Lemma 2.3. (This reduction does not always hold if the generalized eigenvectors are computed only approximately.)\nDespite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently. In the breakthrough result of Ma, Lu and Foster (Ma et al., 2015), they proposed to study algorithms to find top k generalized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA). They designed an alternating minimization algorithm whose running time is only linear in the input matrix sparsity and nearly-linear in k. Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate. Unfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with κ and 1/gap, where\n• κ ≥ 1 is the condition number of matrix B in GenEV, or of matrices X>X,Y >Y in CCA; and • gap ∈ [0, 1) is the eigengap λk−λk+1λk in GenEV, or σk−σk+1\nσk in CCA.\nThese parameters are usually not constants and scale with\nthe problem size.\nChallenge 1: Acceleration For many easier scientific computing problems, we are able to design algorithms that have accelerated dependencies on κ and 1/gap. As two concrete examples, k-PCA can be solved with a running time linearly in 1/ √ gap as opposed to 1/gap (Golub & Van Loan, 2012); computing B−1w for a vector w can be solved in time linearly in √ κ as opposed to κ, where κ is the condition number of matrix B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).\nTherefore, can we obtain doubly-accelerated methods for k-GenEV and k-CCA, meaning that the running times linearly scale with both √ κ and 1/ √ gap? Before this paper, for the general case k > 1, the method of Ge et al. (Ge et al., 2016) made acceleration possible for parameter κ, but not for parameter 1/gap (see Table 1).\nChallenge 2: Gap-Freeness Since gap can be even zero in the extreme case, can we design algorithms that do not scale with 1/gap? Recall that this is possible for the easier task of k-PCA. The block Krylov method (Musco & Musco, 2015) runs in time linear in 1/ √ ε as opposed to 1/ √ gap, where ε is the approximation ratio. There is no gap-free result previously known for k-GenEV or k-CCA even for k = 1.\nChallenge 3: Stochasticity For matrix-related problems, one can usually obtain stochastic running times which requires some notations to describe.\nConsider a simple task of computing B−1w for some vector w, where accelerated methods solve it in time linearly in √ κ for κ being the condition number of B. If B = 1 nX >X is given in the form of a covariance matrix where X ∈ Rn×d, then (accelerated) stochastic methods compute B−1w in a time linearly in (1 + √ κ′/n) instead of √ κ, where κ′ = maxi∈[n]{‖Xi‖ 2} λmin(B) ∈ [ κ, nκ ] and Xi is the i-th\nrow of X . (See Lemma 2.6.) Since 1 + √ κ′/n ≤ O( √ κ), stochastic methods are no slower than non-stochastic ones.\nSo, can we obtain a similar but doubly-accelerated stochastic method for k-CCA?1 Note that, if the doublyaccelerated requirement is dropped, this task is easier and indeed possible, see Ge et al. (Ge et al., 2016). However, since their stochastic method is not doubly-accelerated, in certain parameter regimes, it runs even slower than nonstochastic ones (even for k = 1, see Table 2).\nRemark. In general, if designed properly, for worst case running time:\n• Accelerated results are usually better because they are 1 Note that a similar problem can be also asked for k-GenEV when A and B are both given in their covariance matrix forms. We refrain from doing it in this paper for notational simplicity.\nno slower than non-accelerated ones in the worst-case.\n• Gap-free results are better because they imply gapdependent ones.2\n• Stochastic results are usually better because they are no slower than non-stochastic ones in the worst-case."
  }, {
    "heading": "1.1 Our Main Results",
    "text": "We provide algorithms LazyEV and LazyCCA that are doubly-accelerated, gap-free, and stochastic.3\nFor the general k-GenEV problem, our LazyEV can be implemented to run in time4\nÕ (knnz(B)√κ\n√ gap\n+ knnz(A) + k2d √ gap\n) or\nÕ (knnz(B)√κ√\nε + knnz(A) + k2d√ ε ) in the gap-dependent and gap-free cases respectively. Since our running time only linearly depends on √ κ and √ gap (resp. √ ε), our algorithm LazyEV is doubly-accelerated.\nFor the general k-CCA problem, our LazyCCA can be implemented to run in time\nÕ (knnz(X,Y ) · (1 +√κ′/n)+ k2d\n√ gap\n) or\nÕ (knnz(X,Y ) · (1 +√κ′/n)+ k2d\n√ ε ) in the gap-dependent and gap-free cases respectively. Here, nnz(X,Y ) = nnz(X) + nnz(Y ) and κ′ = 2 maxi{‖Xi‖2,‖Yi‖2} λmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector of X or Y . Therefore, our algorithm LazyCCA is doublyaccelerated and stochastic.\nWe fully compare our results with prior work in Table 2 (for k = 1) and Table 1 (for k ≥ 1), and summarize our main contributions:\n• For k > 1, we outperform all relevant prior works (see Table 1). Moreover, no known method was doublyaccelerated even in the non-stochastic setting.\n• For k ≥ 1, we obtain the first gap-free running time. • Even for k = 1, we outperform most of the state-of-\nthe-arts (see Table 2).\nNote that for CCA with k > 1, previous result CCALin only outputs the subspace spanned by the top k correlation vectors but does not identify which vector gives the highest correlation and so on. Our LazyCCA provides per-vector\n2If a method depends on 1/ε then one can choose ε = gap and this translates to a gap-dependent running time.\n3Recalling Footnote 1, for notational simplicity, we only state our k-GenEV result in non-stochastic running time.\n4Throughout the paper, we use the Õ notation to hide polylogarithmic factors with respect to κ, 1/gap, 1/ε, d, n. We use nnz(M) to denote the time needed to multiply M to a vector.\nλk\n∈ [0, 1] and κB = λmax(B)λmin(B) > 1.\nIn CCA, gap = σk−σk+1 σk ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈ [κ, 2nκ], and σk ∈ [0, 1].\nRemark 1. Stochastic methods depend on a modified condition number κ′. The reason κ′ ∈ [κ, 2nκ] is in Fact 2.5. Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\nguarantees on all the top k correlation vectors."
  }, {
    "heading": "1.2 Our Side Results on Doubly-Stochastic Methods",
    "text": "Recall that when considering acceleration, there are two parameters κ and 1/gap. One can also design stochastic methods with respect to both parameters κ and 1/gap, meaning that\nwith a running time proportional to 1 + √ κ′/nc √ gap\ninstead of 1+ √ κ′/n\n√ gap (stochastic) or √ κ√ gap (non-stochastic).\nThe constant c is usually 1/2. We call such methods doubly-stochastic.\nUnfortunately, doubly-stochastic methods are usually slower than stochastic ones. Take 1-CCA as an example. The best stochastic running time (obtained exclusively by\nus) for 1-CCA is nnz(X,Y ) · Õ ( 1+√κ′/n √ gap ) . In contrast, if one uses a doubly-stochastic method —either (Wang et al., 2016) or our LazyCCA— the running time becomes nnz(X,Y ) · Õ ( 1 +\n√ κ′/n1/4√ gap·σ1\n) . Therefore, for 1-CCA,\ndoubly-stochastic methods are faster than stochastic ones\nonly when κ ′\nσ1 ≤ o(n1/2) .\nThe above condition is usually not satisfied. For instance,\n• κ′ is usually around n for most interesting data-sets, cf.\nthe experiments of (Shalev-Shwartz & Zhang, 2014);\n• κ′ is between n1/2 and 100n in all the CCA experiments of (Wang et al., 2016); and\n• by Fact 2.5 it satisfies κ′ ≥ d so κ′ cannot be smaller than o(n1/2) unless d n1/2.5 Even worse, parameter σ1 ∈ [0, 1] is usually much smaller than 1. Note that σ1 is scaling invariant: even if one scales X and Y up by the same factor, σ1 remains unchanged.\nNevertheless, to compare our LazyCCA with all relevant prior works, we obtain doubly-stochastic running times for k-CCA as well. Our running time matches that of (Wang et al., 2016) when k = 1, and no doubly-stochastic running time for k > 1 was known before our work."
  }, {
    "heading": "1.3 Other Related Works",
    "text": "For the easier task of PCA and SVD, the first gap-free result was obtained by Musco and Musco (Musco & Musco, 2015), the first stochastic result was obtained by Shamir (Shamir, 2015), and the first accelerated stochastic result was obtained by Garber et al. (Garber & Hazan, 2015; Garber et al., 2016). The shift-and-invert preconditioning technique of Garber et al. is also used in this paper.\nFor another related problem PCR (principle compo-\n5Note that item (3) κ′ ≥ d may not hold in the more general setting of CCA, see Remark A.1.\nλ1\n∈ [0, 1] and κB = λmax(B)λmin(B) > 1.\nIn CCA, gap = σ1−σ2 σ1 ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈ [κ, 2nκ], and σ1 ∈ [0, 1].\nRemark 1. Stochastic methods depend on modified condition number κ′; the reason κ′ ∈ [κ, 2nκ] is in Def. 2.4. Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).\nRemark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.\nRemark 4. Some CCA methods have a running time dependency on σ1 ∈ [0, 1], and this is intrinsic and cannot be removed. In particular, if we scale the data matrix X and Y , the value σ1 stays the same.\nRemark 5. The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA only). Our LazyEV is faster than theirs by a factor Ω( √ nκ/κ′ × √ 1/σ1). Here, nκ/κ′ ≥ 1/2 and 1/σ1 ≥ 1 are two scaling-invariant quantities usually much greater than 1.\nnent regression), we recently obtained an accelerated method (Allen-Zhu & Li, 2017) as opposed the previously non-accelerated one (Frostig et al., 2016); however, the acceleration techniques there are not relevant to this paper.\nFor GenEV and CCA, many scalable algorithms have been designed recently (Ma et al., 2015; Wang & Livescu, 2015; Michaeli et al., 2015; Witten et al., 2009; Lu & Foster, 2014). However, as summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees. Furthermore, for k > 1, the AppGrad method (Ma et al., 2015) only provides local convergence guarantees and thus requires a warm-start whose\ncomputational complexity is not discussed in their paper.\nFinally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once, therefore requiring k to be known beforehand. As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do."
  }, {
    "heading": "2 Preliminaries",
    "text": "We denote by ‖x‖ or ‖x‖2 the Euclidean norm of vector x. We denote by ‖A‖2, ‖A‖F , and ‖A‖Sq respectively the spectral, Frobenius, and Schatten q-norm of matrix A (for q ≥ 1). We write A B if A,B are symmetric and A−B is positive semi-definite (PSD), and write A B if A,B are symmetric but A − B is positive definite (PD). We denote by λmax(M) and λmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by κM the condition number λmax(M)/λmin(M) of a PSD matrix M .\nThroughout this paper, we use nnz(M) to denote the time to multiply matrix M to any arbitrary vector. For two matricesX,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ), and by Xi or Yi the i-th row vector of X or Y . We also use poly(x1, x2, . . . , xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . . . , xt. Given a column orthonormal matrix U ∈ Rn×k, we denote by U⊥ ∈ Rn×(n−k) the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U ’s columns.\nGiven a PSD matrixB and a vector v, v>Bv is theB-seminorm of v. Two vectors v, w are B-orthogonal if v>Bw = 0. We denote by B−1 the Moore-Penrose pseudoinverse of B if B is not invertible, and by B1/2 the matrix square root of B (satisfying B1/2 0). All occurrences of B−1, B1/2 and B−1/2 are for analysis purpose only. Our final algorithms only require multiplications of B to vectors.\nDefinition 2.1 (GenEV). Given symmetric matrices A,B ∈ Rd×d where B is positive definite. The generalized eigenvectors ofA with respect toB are v1, . . . , vd, where each vi is\nvi ∈ arg max v∈Rd {∣∣v>Av∣∣ s.t. v>Bv = 1 v>Bvj = 0 ∀j ∈ [i− 1] } The generalized eigenvalues λ1, . . . , λd satisfy λi = v>i Avi which can be negative.\nFollowing (Wang et al., 2016; Garber & Hazan, 2015), we assume without loss of generality that λi ∈ [−1, 1].\nDefinition 2.2 (CCA). Given X ∈ Rn×dx , Y ∈ Rn×dy , letting Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY\n>Y , the canonical-correlation vectors are {(φi, ψi)}ri=1 where r = min{dx, dy} and for all i ∈ [r]:\n(φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy\n{ φ>Sxyψ such that\n{ φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈ [i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈ [i− 1] }} The corresponding canonical-correlation coefficients σ1, . . . , σr satisfy σi = φ>i Sxyψi ∈ [0, 1].\nWe emphasize that σi always lies in [0, 1] and is scalinginvariant. When dealing with a CCA problem, we also denote by d = dx + dy .\nLemma 2.3 (CCA to GenEV). Given a CCA problem with matrices X ∈ Rn×dx , Y ∈ Rn×dy , let the canonicalcorrelation vectors and coefficients be {(φi, ψi, σi)}ri=1 where r = min{dx, dy}. Define A = ( 0 Sxy\nS>xy 0\n) and\nB = ( Sxx 0\n0 Syy\n) . Then, the GenEV problem of A with re-\nspect to B has 2r eigenvalues {±σi}ri=1 and corresponding generalized eigenvectors {( φi ψi ) , ( −φi ψi )}n i=1 . The remaining dx + dy − 2r eigenvalues are zeros.\nDefinition 2.4. In CCA, let A and B be as defined in Lemma 2.3. We define condition numbers\nκ def = κB = λmax(B) λmin(B) and κ′ def= 2 maxi{‖Xi‖ 2,‖Yi‖2} λmin(B) . Fact 2.5. κ′ ∈ [κ, 2nκ] and κ′ ≥ d. (See full version.)\nLemma 2.6. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy , let A and B be as defined in Lemma 2.3. For every w ∈ Rd, the Katyusha method (Allen-Zhu, 2017) finds a vector w′ ∈ Rd satisfying ‖w′ −B−1Aw‖ ≤ ε in time\nO ( nnz(X,Y ) · ( 1 + √ κ′/n ) · log κ‖w‖ 2\nε\n) ."
  }, {
    "heading": "3 Leading Eigenvector via Two-Sided Shift-and-Invert",
    "text": "We introduce AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix. AppxPCA± uses the shift-and-invert framework (Garber & Hazan, 2015; Garber et al., 2016), and shall become our building block for the LazyEV and LazyCCA algorithms in the subsequent sections.\nOur pseudo-code Algorithm 1 is a modification of Algorithm 5 in (Garber & Hazan, 2015), and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle A. The main differences between AppxPCA± and (Garber & Hazan, 2015) are two-fold.\nFirst, given a symmetric matrix M , AppxPCA± simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to perform power methods with respect to (λI − M)−1 and (λI + M)−1. This allows us to determine approximately how close λ is to the largest and the smallest eigenvalues of M , and decrease λ accordingly. In the end, AppxPCA± outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed. Second, we provide a multiplicative-error guarantee rather than additive as appeared in (Garber & Hazan, 2015). Without such guarantee, our final running time will depend on 1gap·λmax(M) rather than 1 gap . 6\n6This is why the SI method of (Wang et al., 2016) also uses\nAlgorithm 1 AppxPCA±(A,M, δ×, ε, p)\nInput: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I M I; δ× ∈ (0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the confidence parameter.\n1: ŵ0 ← RanInit(d); s← 0; λ(0) ← 1 + δ×; ŵ0 is a random unit vector, see Def. 3.2 2: m1 ← ⌈ 4 log ( 288dθ p2 )⌉ , m2 ← ⌈ log ( 36dθ p2ε )⌉ ; θ is the parameter of RanInit, see Def. 3.2\n3: ε̃1 ← 164m1 ( δ× 48 )m1 and ε̃2 ← ε8m2 ( δ×48 )m2 4: repeat m1 = TPM(8, 1/32, p) and m2 = TPM(2, ε/4, p), see Lemma B.1 5: s← s+ 1; 6: for t = 1 to m1 do 7: Apply A to find ŵt satisfying\n∥∥ŵt − (λ(s−1)I −M)−1ŵt−1∥∥ ≤ ε̃1; 8: wa ← ŵm1/‖ŵm1‖; wa is roughly (λ(s−1)I −M)−m1 ŵ0 then normalized 9: Apply A to find va satisfying\n∥∥va − (λ(s−1)I −M)−1wa∥∥ ≤ ε̃1; 10: for t = 1 to m1 do 11: Apply A to find ŵt satisfying\n∥∥ŵt − (λ(s−1)I +M)−1ŵt−1∥∥ ≤ ε̃1; 12: wb ← ŵm1/‖ŵm1‖; wb is roughly (λ(s−1)I +M)−m1 ŵ0 then normalized 13: Apply A to find vb satisfying\n∥∥vb − (λ(s−1)I +M)−1wb∥∥ ≤ ε̃1; 14: ∆(s) ← 12 ·\n1 max{w>a va,w>b vb}−ε̃1\nand λ(s) ← λ(s−1) − ∆ (s)\n2 ;\n15: until ∆(s) ≤ δ×λ (s) 12 16: f ← s; 17: if the last w>a va ≥ w>b vb then 18: for t = 1 to m2 do 19: Apply A to find ŵt satisfying\n∥∥ŵt − (λ(f)I −M)−1ŵt−1∥∥ ≤ ε̃2; 20: return (+, w) where w def= ŵm2/‖ŵm2‖. 21: else 22: for t = 1 to m2 do 23: Apply A to find ŵt satisfying\n∥∥ŵt − (λ(f)I +M)−1ŵt−1∥∥ ≤ ε̃2; 24: return (−, w) where w def= ŵm2/‖ŵm2‖. 25: end if\nWe prove in full version the following theorem:\nTheorem 3.1 (AppxPCA±, informal). Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · · ≥ λd ≥ −1 and eigenvectors u1, . . . , ud. Let λ∗ = max{λ1,−λd}. With probability at least 1− p, AppxPCA± produces a pair (sgn,w) satisfying\n• if sgn = +, then w is an approx. positive eigenvector: w>Mw ≥ (\n1− δ× 2\n) λ∗ ∧ ∑\ni∈[d] λi≤(1−δ×/2)λ∗\n(w>ui) 2 ≤ ε\n• if sgn = −, then w is an approx. negative eigenvector: w>Mw ≤ − (\n1−δ× 2\n) λ∗ ∧ ∑\ni∈[d] λi≥−(1−δ×/2)λ∗\n(w>ui) 2 ≤ ε\nThe number of oracle calls toA is Õ(log(1/δ×)), and each time we call A it satisfies\nshift-and-invert but depends on 1 gap·σ1 in Table 2.\n• λmax(λ (s)I−M) λmin(λ(s)I−M) , λmax(λ (s)I+M) λmin(λ(s)I+M) ∈ [1, 96δ× ] and\n• 1 λmin(λ(s)I−M) , 1 λmin(λ(s)I+M) ≤ 48δ×λ∗ .\nWe remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA±, we have allowed this initial vector to be generated from an arbitrary θ-conditioned random vector generator (for later use), defined as follows:\nDefinition 3.2. An algorithm RanInit(d) is a θconditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p ∈ (0, 1), every unit vector u ∈ Rd, with probability at least 1− p, it satisfies (u>w)2 ≤ p\n2θ 9d .\nThis modification is needed in order to obtain our efficient implementations of GenEV and CCA. One can construct a θ-conditioned random vector generator as follows:\nProposition 3.3. Given a PSD matrix B ∈ Rd×d, if we set RanInit(d) def = B\n1/2v (v>Bv)0.5\nwhere v is a random Gaussian vector, then RanInit(d) is a θ-conditioned random vector\ngenerator for θ = κB ."
  }, {
    "heading": "4 LazyEV: Generalized Eigendecomposition",
    "text": "In this section, we construct an algorithm LazyEV that, given symmetric matrix M ∈ Rd×d, computes approximately the k leading eigenvectors ofM that have the largest absolute eigenvalues. Then, for the original k-GenEV problem, we set M = B−1/2AB−1/2 and run LazyEV. This is our plan to find the top k leading generalized eigenvectors of A with respect to B.\nOur algorithm LazyEV is formally stated in Algorithm 2. The algorithm applies k times AppxPCA±, each time computing an approximate leading eigenvector of M with a multiplicative error δ×/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector. We state our main approximation theorem below.\nTheorem 4.1 (informal). Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . . , λd ∈ [−1, 1] and corresponding eigenvectors u1, . . . , ud, and |λ1| ≥ · · · ≥ |λd|. If εpca is sufficiently small,7 LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) ∈ Rd×k which, with probability at least 1− p, satisfies: (a) ‖V >k U‖2 ≤ ε where U = (uj , . . . , ud) and j is the\nsmallest index satisfying |λj | ≤ (1− δ×)λk. (b) For every i ∈ [k], (1−δ×)|λi| ≤ |v>i Mvi| ≤ 11−δ× |λi|.\nAbove, property (a) ensures the k columns of Vk have negligible correlation with the eigenvectors of M whose absolute eigenvalues are ≤ (1− δ×)λk; property (b) ensures the Rayleigh quotients v>i Mvi are all correct up to a 1±δ× error. We in fact have shown two more useful properties in the full version that may be of independent interest.\nThe next theorem states that, if M = B−1/2AB−1/2, our LazyEV can be implemented without the necessity to compute B1/2 or B−1/2.\nTheorem 4.2 (running time). Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose M = B−1/2AB−1/2 and RanInit(d) is defined in Proposition 3.3 with respect to B. Then, the computation of V ← B−1/2LazyEV(A,M, k, δ×, εpca, p) can be implemented to run in time\n• Õ ( knnz(B)+k2d+kΥ√\nδ×\n) where Υ is the time to multiply\nB−1A to a vector, or • Õ ( k √ κBnnz(B)+knnz(A)+k\n2d√ δ×\n) if we use Conjugate gra-\ndient to multiply B−1A to a vector.\n7Meaning εpca ≤ O ( poly(ε, δ×,\n|λ1| |λk+1| , 1 d ) ) . The complete\nspecifications of εpca is included in the full version. Since our final running time only depends on log(1/εpca), we have not attempted to improve the constants in this polynomial dependency.\nChoosing parameter δ× as either gap or ε, our two main theorems above immediately imply the following results for the k-GenEV problem: (proved in full version)\nTheorem 4.3 (gap-dependent GenEV, informal). Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(λi, ui)}di=1, and it satisfies 1 ≥ |λ1| ≥ · · · ≥ |λd|. Then, LazyEV outputs V k ∈ Rd×k satisfying\nV > k BV k = I and ‖V > k BW‖2 ≤ ε\nin time Õ (k√κBnnz(B) + knnz(A) + k2d√\ngap ) Here, W = (uk+1, . . . , ud) and gap =\n|λk|−|λk+1| |λk| .\nTheorem 4.4 (gap-free GenEV, informal). In the same setting as Theorem 4.3, our LazyEV outputs V k = (v1, . . . , vk) ∈ Rd×k satisfying V > k BV k = I and\n∀s ∈ [k] : ∣∣v>s Avs∣∣ ∈ [(1− ε)|λs|, |λs|1− ε]\nin time Õ (k√κBnnz(B) + knnz(A) + k2d√\nε\n) ."
  }, {
    "heading": "5 Ideas Behind Theorems 4.1 and 4.2",
    "text": "In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to approximately find the top eigenvectors ofM . In the full version of this paper, we also discuss how to implement LazyEV without compute B1/2 explicitly, thus proving Theorem 4.2."
  }, {
    "heading": "5.1 Ideas Behind Theorem 4.1",
    "text": "Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M (Allen-Zhu & Li, 2016). That method is called LazySVD and we summarize it as follows.\nAt a high level, LazySVD finds the top k eigenvectors oneby-one and approximately. Starting with M0 = M , in the s-th iteration where s ∈ [k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it vs. Then, LazySVD projects Ms ← (I − vsv>s )Ms−1(I − vsv > s ) and proceeds to the next iteration.\nWhile the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas. Most notably, if vs is an approximate leading eigenvector of Ms−1, then one needs to prove that the small eigenvectors of Ms−1 somehow still “embed” into that of Ms after projection. This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of (Allen-Zhu & Li, 2016).\nAlgorithm 2 LazyEV(A,M, k, δ×, εpca, p)\nInput: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I M I; k ∈ [d], the desired rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a confidence parameter.\n1: M0 ←M ; V0 = []; 2: for s = 1 to k do 3: (∼, v′s)← AppxPCA±(A,Ms−1, δ×/2, εpca, p/k); v′s is an approximate two-sided leading eigenvector of Ms−1 4: vs ← ( (I − Vs−1V >s−1)v′s ) / ∥∥(I − Vs−1V >s−1)v′s∥∥; project v′s to V ⊥s−1 5: Vs ← [Vs−1, vs]; 6: Ms ← (I − vsv>s )Ms−1(I − vsv>s ) we also have Ms = (I − VsV >s )M(I − VsV >s ) 7: end for 8: return Vk.\nIn this paper, to relax the assumption thatM is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make several non-trivial changes. On the algorithm side, LazyEV uses our two-sided shift-andinvert method in Section 3 to find the leading eigenvector of Ms−1 with largest absolute eigenvalue. On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues. For instance:\n• If we perform a projection M ′ ← (I − vv>)M(I − vv>) where v correlates by at most ε with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold µ, then, after the projection, we need to prove that these eigenvectors can be approximately “embedded” into the eigenspace spanned by all eigenvectors of M ′ whose absolute eigenvalues are smaller than µ+ τ . The approximation of this embedding should depend on ε, µ and τ .\nThe full proof of Theorem 4.1 is in the arXiv version. It relies on a few matrix algebraic lemmas (including the aforementioned “embedding lemma”)."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper we propose new iterative methods to solve the generalized eigenvector and the canonical correlation analysis problems. Our methods find the most significant k eigenvectors or correlation vectors, and have running times that linearly scales with k.\nMost importantly, our methods are doubly-accelerated: the running times have square-root dependencies both with respect to the condition number of the matrix (i.e., κ) and with respect to the eigengap (i.e., gap). They are the first doubly-accelerated iterative methods at least for k > 1. They can also be made gap-free, and are the first gap-free iterative methods even for 1-GenEV or 1-CCA.\nAlthough this is a theory paper, we believe that if implemented carefully, our methods can outperform not only previous iterative methods (such as GenELin, AppGrad, CCALin), but also the commercial mathematics libraries for sparse matrices of dimension more than 10, 000. We\nleave it a future work for such careful comparisons."
  }],
  "year": 2017,
  "references": [{
    "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
    "authors": ["Allen-Zhu", "Zeyuan"],
    "venue": "In STOC,",
    "year": 2017
  }, {
    "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent",
    "authors": ["Allen-Zhu", "Zeyuan", "Orecchia", "Lorenzo"],
    "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science,",
    "year": 2017
  }, {
    "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives",
    "authors": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Even faster accelerated coordinate descent using non-uniform sampling",
    "authors": ["Allen-Zhu", "Zeyuan", "Richtárik", "Peter", "Qu", "Zheng", "Yuan", "Yang"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Expander flows, geometric embeddings and graph partitioning",
    "authors": ["Arora", "Sanjeev", "Rao", "Satish", "Vazirani", "Umesh V"],
    "venue": "Journal of the ACM,",
    "year": 2009
  }, {
    "title": "Stability of over-relaxations for the forward-backward algorithm, application to fista",
    "authors": ["Aujol", "J-F", "Dossal", "Ch"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "A survey of preconditioned iterative methods for linear systems of algebraic equations",
    "authors": ["Axelsson", "Owe"],
    "venue": "BIT Numerical Mathematics,",
    "year": 1985
  }, {
    "title": "Multi-view clustering via canonical correlation analysis",
    "authors": ["Chaudhuri", "Kamalika", "Kakade", "Sham M", "Livescu", "Karen", "Sridharan", "Karthik"],
    "venue": "In ICML, pp",
    "year": 2009
  }, {
    "title": "Multi-view learning of word embeddings via cca",
    "authors": ["Dhillon", "Paramveer", "Foster", "Dean P", "Ungar", "Lyle H"],
    "venue": "In NIPS, pp",
    "year": 2011
  }, {
    "title": "Principal Component Projection Without Principal Component Analysis",
    "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Fast and simple PCA via convex optimization",
    "authors": ["Garber", "Dan", "Hazan", "Elad"],
    "venue": "ArXiv e-prints,",
    "year": 2015
  }, {
    "title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation",
    "authors": ["Garber", "Dan", "Hazan", "Elad", "Jin", "Chi", "Kakade", "Sham M", "Musco", "Cameron", "Netrapalli", "Praneeth", "Sidford", "Aaron"],
    "year": 2016
  }, {
    "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis",
    "authors": ["Ge", "Rong", "Jin", "Chi", "Kakade", "Sham M", "Netrapalli", "Praneeth", "Sidford", "Aaron"],
    "year": 2016
  }, {
    "title": "Multi-view regression via canonical correlation analysis",
    "authors": ["Kakade", "Sham M", "Foster", "Dean P"],
    "venue": "In Learning theory,",
    "year": 2007
  }, {
    "title": "Discriminative features via generalized eigenvectors",
    "authors": ["Karampatziakis", "Nikos", "Mineiro", "Paul"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Large scale canonical correlation analysis with iterative least squares",
    "authors": ["Lu", "Yichao", "Foster", "Dean P"],
    "venue": "In NIPS, pp",
    "year": 2014
  }, {
    "title": "Finding linear structure in large datasets with scalable canonical correlation analysis",
    "authors": ["Ma", "Zhuang", "Lu", "Yichao", "Foster", "Dean"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "Nonparametric canonical correlation analysis",
    "authors": ["Michaeli", "Tomer", "Wang", "Weiran", "Livescu", "Karen"],
    "venue": "arXiv preprint,",
    "year": 2015
  }, {
    "title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition",
    "authors": ["Musco", "Cameron", "Christopher"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "A method of solving a convex programming problem with convergence rate O(1/k2)",
    "authors": ["Nesterov", "Yurii"],
    "venue": "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),",
    "year": 1983
  }, {
    "title": "SDCA without Duality, Regularization, and Individual Convexity",
    "authors": ["Shalev-Shwartz", "Shai"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
    "authors": ["Shamir", "Ohad"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "An introduction to the conjugate gradient method without the agonizing pain",
    "authors": ["Shewchuk", "Jonathan Richard"],
    "year": 1994
  }, {
    "title": "Large-scale approximate kernel canonical correlation analysis",
    "authors": ["Wang", "Weiran", "Livescu", "Karen"],
    "venue": "arXiv preprint,",
    "year": 2015
  }, {
    "title": "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis",
    "authors": ["Wang", "Weiran", "Jialei", "Garber", "Dan", "Srebro", "Nathan"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis",
    "authors": ["Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor"],
    "venue": "Biostatistics, pp. kxp008,",
    "year": 2009
  }],
  "id": "SP:30471acf1e119e3e77beaf69027ddad713902fef",
  "authors": [{
    "name": "Zeyuan Allen-Zhu",
    "affiliations": []
  }, {
    "name": "Yuanzhi Li",
    "affiliations": []
  }],
  "abstractText": "We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonicalcorrelation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k. Furthermore, our algorithms are doubly-accelerated: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both kGenEV or k-CCA. We also provide the first gapfree results, which provide running times that depend on 1/ √ ε rather than the eigengap.",
  "title": "Doubly Accelerated Methods for Faster CCA  and Generalized Eigendecomposition"
}