{
  "sections": [{
    "text": "Keywords: Nonconvex optimization · Computational complexity · NP-hardness · Concave penalty · Sparsity"
  }, {
    "heading": "1 Introduction",
    "text": "We study the sparse minimization problem, where the objective is the sum of empirical losses over input data and a sparse penalty function. Such problems commonly arise from empirical risk minimization and variable selection. The role of the penalty function is to induce sparsity in the optimal solution, i.e., to minimize the empirical loss using as few nonzero coefficients as possible.\nProblem 1 Given the loss function ` : R × R 7→ R+, penalty function p : R 7→ R+, and regularization parameter λ > 0, consider the problem\nmin x∈Rd n∑ i=1 ` ( aTi x, bi ) + λ d∑ j=1 p (|xj |) ,\nwhere A = (a1, . . . , an)T ∈ Rn×d, b = (b1, . . . , bn)T ∈ Rn are input data.\n1Princeton University, NJ, USA 2Shanghai University of Finance and Economics, Shanghai, China 3University of Minnesota, MN, USA 4Stanford University, CA, USA. Correspondence to: Mengdi Wang <mengdiw@princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nWe are interested in the computational complexity of Problem 1 under general conditions of the loss function ` and the sparse penalty p. In particular, we focus on the case where ` is a convex loss function and p is a concave penalty with a unique minimizer at 0. Optimization problems with convex ` and concave p are common in sparse regression, compressive sensing, and sparse approximation. A list of applicable examples of ` and p is given in Section 3.\nFor certain special cases of Problem 1, it has been shown that finding an exact solution is strongly NP-hard (Huo & Chen, 2010; Chen et al., 2014). However, these results have not excluded the possibility of the existence of polynomialtime algorithms with small approximation error. (Chen & Wang, 2016) established the hardness of approximately solving Problem 1 when p is the L0 norm.\nIn this paper, we prove that it is strongly NP-hard to approximately solve Problem 1 within certain optimality error. More precisely, we show that there exists a lower bound on the suboptimality error of any polynomial-time deterministic algorithm. Our results apply to a variety of optimization problems in estimation and machine learning. Examples include sparse classification, sparse logistic regression, and many more. The strong NP-hardness of approximation is one of the strongest forms of complexity result for continuous optimization. To our best knowledge, this paper gives the first and strongest set of hardness results for Problem 1 under very general assumptions regarding the loss and penalty functions.\nOur main contributions are three-fold.\n1. We prove the strong NP-hardness for Problem 1 with general loss functions. This is the first results that apply to the broad class of problems including but not limited to: least squares regression, linear model with Laplacian noise, robust regression, Poisson regression, logistic regression, inverse Gaussian models, etc.\n2. We present a general condition on the sparse penalty function p such that Problem 1 is strongly NP-hard. The condition is a slight weaker version of strict concavity. It is satisfied by typical penalty functions such as the Lq norm (q ∈ [0, 1)), clipped L1 norm, SCAD, etc. To the best of our knowledge, this is the most gen-\neral condition on the penalty function in the literature.\n3. We prove that finding an O (λnc1dc2)-optimal solution to Problem 1 is strongly NP-hard, for any c1, c2 ∈ [0, 1) such that c1 + c2 < 1. Here the O(·) hides parameters that depend on the penalty function p, which is to be specified later. It illustrates a gap between the optimization error achieved by any tractable algorithm and the desired statistical precision. Our proof provides a first unified analysis that deals with a broad class of problems taking the form of Problem 1.\nSection 2 summarizes related literatures from optimization, machine learning and statistics. Section 3 presents the key assumptions and illustrates examples of loss and penalty functions that satisfy the assumptions. Section 4 gives the main results. Section 5 discusses the implications of our hardness results. Section 6 provides a proof of the main results in a simplified setting. The full proofs are deferred to the appendix."
  }, {
    "heading": "2 Background and Related Works",
    "text": "Sparse optimization is a powerful machine learning tool for extracting useful information for massive data. In Problem 1, the sparse penalty serves to select the most relevant variables from a large number of variables, in order to avoid overfitting. In recent years, nonconvex choices of p have received much attention; see (Frank & Friedman, 1993; Fan & Li, 2001; Chartrand, 2007; Candes et al., 2008; Fan & Lv, 2010; Xue et al., 2012; Loh & Wainwright, 2013; Wang et al., 2014; Fan et al., 2015).\nWithin the optimization and mathematical programming community, the complexity of Problem 1 has been considered in a number of special cases. (Huo & Chen, 2010) first proved the hardness result for a relaxed family of penalty functions with L2 loss. They show that for the penalties in L0, hard-thresholded (Antoniadis & Fan, 2001) and SCAD (Fan & Li, 2001), the above optimization problem is NPhard. (Chen et al., 2014) showed that the L2-Lp minimization is strongly NP-hard when p ∈ (0, 1). At the same time, (Bian & Chen, 2014) proved the strongly NP-hardness for another class of penalty functions. The preceding existing analyses mainly focused on finding an exact global optimum to Problem 1. For this purpose, they implicitly assumed that all the input and parameters involved in the reduction are rational numbers with a finite numerical representation, otherwise finding a global optimum to a continuous problem would be always intractable. A recent technical report (Chen & Wang, 2016) proves the hardness of obtaining an -optimal solution when p is the L0 norm.\nWithin the theoretical computer science community, there have been several early works on the complexity of sparse\nrecovery, beginning with (Arora et al., 1993). (Amaldi & Kann, 1998) proved that the problem min{‖x‖0 | Ax = b} is not approximable within a factor 2log\n1− d for any > 0. (Natarajan, 1995) showed that, given > 0, A and b, the problem min{‖x‖0 | ‖Ax − b‖2 ≤ } is NP-hard. (Davis et al., 1997) proved a similar result that for some given > 0 and M > 0, it is NP-complete to find a solution x such that ‖x‖0 ≤ M and ‖Ax − b‖ ≤ . More recently, (Foster et al., 2015) studied sparse recovery and sparse linear regression with subgaussian noises. Assuming that the true solution is K-sparse, it showed that no polynomialtime (randomized) algorithm can find aK ·2log1−δ d-sparse solution xwith ‖Ax−b‖22 ≤ dC1n1−C2 with high probability, where δ, C1, C2 are arbitrary positive scalars. Another work (Zhang et al., 2014) showed that under the Gaussian linear model, there exists a gap between the mean square loss that can be achieved by polynomial-time algorithms and the statistically optimal mean squared error. These two works focus on estimation of linear models and impose distributional assumptions regarding the input data. These results on estimation are different in nature with our results on optimization.\nIn contrast, we focus on the optimization problem itself. Our results apply to a variety of loss functions and penalty functions, not limited to linear regression. Moreover, we do not make any distributional assumption regarding the input data.\nThere remain several open questions. First, existing results mainly considered least square problems or Lq minimization problems. Second, existing results focused mainly on the L0 penalty function. The complexity of Problem 1 with general loss function and penalty function is yet to be established. Things get complicated when p is a continuous function instead of the discrete L0 norm function. The complexity for finding an -optimal solution with general ` and p is not fully understood. We will address these questions in this paper."
  }, {
    "heading": "3 Assumptions",
    "text": "In this section, we state the two critical assumptions that lead to the strong NP-hardness results: one for the penalty function p, the other one for the loss function `. We argue that these assumptions are essential and very general. They apply to a broad class of loss functions and penalty functions that are commonly used."
  }, {
    "heading": "3.1 Assumption About Sparse Penalty",
    "text": "Throughout this paper, we make the following assumption regarding the sparse penalty function p(·).\nAssumption 1. The penalty function p(·) satisfies the fol-\nlowing conditions:\n(i) (Monotonicity) p(·) is non-decreasing on [0,+∞).\n(ii) (Concavity) There exists τ > 0 such that p(·) is concave but not linear on [0, τ ].\nIn words, condition (ii) means that the concave penalty p(·) is nonlinear. Assumption 1 is the most general condition on penalty functions in the existing literature of sparse optimization. Below we present a few such examples.\n1. In variable selection problems, the L0 penalization p(t) = I{t 6=0} arises naturally as a penalty for the number of factors selected.\n2. A natural generalization of the L0 penalization is the Lp penalization p(t) = tp where (0 < p < 1). The corresponding minimization problem is called the bridge regression problem (Frank & Friedman, 1993).\n3. To obtain a hard-thresholding estimator, Antoniadis & Fan (2001) use the penalty functions pγ(t) = γ2 − ((γ − t)+)2 with γ > 0, where (x)+ := max{x, 0} denotes the positive part of x.\n4. Any penalty function that belongs to the folded concave penalty family (Fan et al., 2014) satisfies the conditions in Theorem 1. Examples include the SCAD (Fan & Li, 2001) and the MCP (Zhang, 2010a), whose derivatives on (0,+∞) are p′γ(t) = γI{t≤γ}+ (aγ−t)+ a−1 I{t>γ} and p ′ γ(t) = (γ − tb )\n+, respectively, where γ > 0, a > 2 and b > 1.\n5. The conditions in Theorem 1 are also satisfied by the clipped L1 penalty function (Antoniadis & Fan, 2001; Zhang, 2010b) pγ(t) = γ · min(t, γ) with γ > 0. This is a special case of the piecewise linear penalty function:\np(t) = { k1t if 0 ≤ t ≤ a k2t+ (k1 − k2)a if t > a\nwhere 0 ≤ k2 < k1 and a > 0.\n6. Another family of penalty functions which bridges the L0 and L1 penalties are the fraction penalty functions\npγ(t) = (γ + 1)t\nγ + t with γ > 0 (Lv & Fan, 2009).\n7. The family of log-penalty functions:\npγ(t) = 1\nlog(1 + γ) log(1 + γt)\nwith γ > 0, also bridges theL0 andL1 penalties (Candes et al., 2008)."
  }, {
    "heading": "3.2 Assumption About Loss Function",
    "text": "We state our assumption about the loss function `.\nAssumption 2. Let M be an arbitrary constant. For any interval [τ1, τ2] where 0 < τ1 < τ2 < M , there exists k ∈ Z+ and b ∈ Qk such that h(y) = ∑k i=1 `(y, bi) has the following properties:\n(i) h(y) is convex and Lipschitz continuous on [τ1, τ2].\n(ii) h(y) has a unique minimizer y∗ in (τ1, τ2).\n(iii) There exists N ∈ Z+, δ̄ ∈ Q+ and C ∈ Q+ such that when δ ∈ (0, δ̄), we have\nh(y∗ ± δ)− h(y∗) δN ≥ C.\n(iv) h(y∗), {bi}ki=1 can be represented in O(log 1τ2−τ1 ) bits.\nAssumption 2 is a critical, but very general, assumption regarding the loss function `(y, b). Condition (i) requires convexity and Lipschitz continuity within a neighborhood. Conditions (ii), (iii) essentially require that, given an interval [τ1, τ2], one can artificially pick b1, . . . , bk to construct a function h(y) = ∑k i=1 `(y, bi) such that h has its unique minimizer in [τ1, τ2] and has enough curvature near the minimizer. This property ensures that a bound on the minimal value of h(y) can be translated to a meaningful bound on the minimizer y∗. The conditions (i), (ii), (iii) are typical properties that a loss function usually satisfies. Condition (iv) is a technical condition that is used to avoid dealing with infinitely-long irrational numbers. It can be easily verified for almost all common loss functions.\nWe will show that Assumptions 2 is satisfied by a variety of loss functions. An (incomplete) list is given below.\n1. In the least squares regression, the loss function has the form\nn∑ i=1 ( aTi x− bi )2 .\nUsing our notation, the corresponding loss function is `(y, b) = (y − b)2. For all τ1, τ2, we choose an arbitrary b′ ∈ [τ1, τ2]. We can verify that h(y) = `(y, b′) satisfies all the conditions in Assumption 2.\n2. In the linear model with Laplacian noise, the negative log-likelihood function is\nn∑ i=1 ∣∣aTi x− bi∣∣ . So the loss function is `(y, b) = |y−b|. As in the case of least squares regression, the loss function satisfy\nAssumption 2. Similar argument also holds when we consider the Lq loss | · |q with q ≥ 1.\n3. In robust regression, we consider the Huber loss (Huber, 1964) which is a mixture of L1 and L2 norms. The loss function takes the form\nLδ(y, b) =\n{ 1 2 |y − b|\n2 for |y − b| ≤ δ, δ(|y − b| − 12δ) otherwise.\nfor some δ > 0 where y = aTx. We then verify that Assumption 2 is satisfied. For any interval [τ1, τ2], we pick an arbitrary b ∈ [τ1, τ2] and let h(y) = `(y, b). We can see that h(y) satisfies all the conditions in Assumption 2.\n4. In Poisson regression (Cameron & Trivedi, 2013), the negative log-likelihood minimization is\nmin x∈Rd − logL(x;A, b) = min x∈Rd n∑ i=1 (exp(aTi x)−bi·aTi x).\nWe now show that `(y, b) = ey − b · y satisfies Assumption 2. For any interval [τ1, τ2], we choose q and r such that q/r ∈ [eτ1 , eτ2 ]. Note that eτ2 − eτ1 = eτ1+τ2−τ1 − eτ1 ≥ τ2 − τ1. Also, eτ2 is bounded by eM . Thus, q, r can be chosen to be polynomial in d1/(τ2 − τ1)e by letting r = d1/(τ2 − τ1)e and q be some number less than r · eM . Then, we choose k = r and b ∈ Zk such that h(y) = ∑k i=1 `(y, bi) = r · ey − q · y. Let us verify Assumption 2. (i), (iv) are straightforward by our construction. For (ii), note that h(y) take its minimum at ln(q/r) which is inside [τ1, τ2] by our construction. To verify (iii), consider the second order Taylor expansion of h(y) at ln(q/r),\nh(y + δ)− h(y) = r · e y 2 · δ2 + o(δ2) ≥ δ 2 2 + o(δ2),\nWe can see that (iii) is satisfied. Therefore, Assumption 2 is satisfied.\n5. In logistic regression, the negative log-likelihood function minimization is\nmin x∈Rd n∑ i=1 log(1 + exp(aTi x))− n∑ i=1 bi · aTi x.\nWe claim that the loss function `(y, b) = log(1 + exp(y))−b ·y satisfies Assumption 2. By a similar argument as the one in Poisson regression, we can verify that h(y) = ∑r i=1 `(y, bi) = r log(1 + exp(y)) − qy where q/r ∈ [ e τ1\n1+eτ1 , eτ2\n1+eτ2 ] and q, r are polynomial in d1/(τ2−τ1)e satisfies all the conditions in Assumption 2. For (ii), observe that `(y, b) take its minimum\nat y = ln q/r1−q/r . To verify (iii), we consider the second order Taylor expansion at y = ln q/r1−q/r , which is\nh(y + δ)− h(y) = q 2(1 + ey) δ2 + o(δ2)\nwhere y ∈ [τ1, τ2]. Note that ey is bounded by eM , which can be computed beforehand. As a result, (iii) holds as well.\n6. In the mean estimation of inverse Gaussian models (McCullagh, 1984), the negative log-likelihood function minimization is\nmin x∈Rd n∑ i=1 (bi · √ aTi x− 1)2 bi .\nNow we show that the loss function `(y, b) = (b·√y−1)2\nb satisfies Assumption 2. By setting the derivative to be zero with regard to y, we can see that y take its minimum at y = 1/b2. Thus for any [τ1, τ2], we choose b′ = q/r ∈ [1/√τ2, 1/ √ τ1]. We can see that h(y) = `(y, b′) satisfies all the conditions in Assumption 2.\n7. In the estimation of generalized linear model under the exponential distribution (McCullagh, 1984), the negative log-likelihood function minimization is\nmin x∈Rd − logL(x;A, b) = min x∈Rd bi aTi x + log(aTi x).\nBy setting the derivative to 0 with regard to y, we can see that `(y, b) = by + log y has a unique minimizer at y = b. Thus by choosing b′ ∈ [τ1, τ2] appropriately, we can readily show that h(y) = `(y, b′) satisfies all the conditions in Assumption 2.\nTo sum up, the combination of any loss function given in Section 3.1 and any penalty function given in Section 3.2 results in a strongly NP-hard optimization problem."
  }, {
    "heading": "4 Main Results",
    "text": "In this section, we state our main results on the strong NPhardness of Problem 1. We warm up with a preliminary result for a special case of Problem 1.\nTheorem 1 (A Preliminary Result). Let Assumption 1 hold, and let p(·) be twice continuously differentiable in (0,∞). Then the minimization problem\nmin x∈Rn ‖Ax− b‖qq + λ d∑ j=1 p(|xj |), (1)\nis strongly NP-hard.\nThe result shows that many of the penalized least squares problems, e.g., (Fan & Lv, 2010), while enjoying small estimation errors, are hard to compute. It suggests that there does not exist a fully polynomial-time approximation scheme for Problem 1. It has not answered the question: whether one can approximately solve Problem 1 within certain constant error.\nNow we show that it is not even possible to efficiently approximate the global optimal solution of Problem 1, unless P = NP . Given an optimization problem minx∈X f(x), we say that a solution x̄ is -optimal if x̄ ∈ X and f(x̄) ≤ infx∈X f(x) + .\nTheorem 2 (Strong NP-Hardness of Problem 1). Let Assumptions 1 and 2 hold, and let c1, c2 ∈ [0, 1) be arbitrary such that c1 + c2 < 1. Then it is strongly NPhard to find a λ · κ · nc1dc2 -optimal solution of Problem 1, where d is the dimension of variable space and κ = mint∈[τ/2,τ ]{ 2p(t/2)−p(t)t }.\nThe non-approximable error in Theorem 2 involves the constant κ which is determined by the sparse penalty function p. In the case where p is the L0 norm function, we can take κ = 1. In the case of piecewise linear L1 penalty, we have κ = (k1 − k2)/4. In the case of SCAD penalty, we have κ = Θ(γ2).\nAccording to Theorem 2, the non-approximable error λ · κ · nc1dc2 is determined by three factors: (i) properties of the regularization penalty λ ·κ; (ii) data size n; and (iii) dimension or number of variables d. This result illustrates a fundamental gap that can not be closed by any polynomialtime deterministic algorithm. This gap scales up when either the data size or the number of variables increases. In Section 5.1, we will see that this gap is substantially larger than the desired estimation precision in a special case of sparse linear regression.\nTheorems 1 and 2 validate the long-lasting belief that optimization involving nonconvex penalty is hard. More importantly, Theorem 2 provide lower bounds for the optimization error that can be achieved by any polynomial-time algorithm. This is one of the strongest forms of hardness result for continuous optimization."
  }, {
    "heading": "5 An Application and Remarks",
    "text": "In this section, we analyze the strong NP-hardness results in the special case of linear regression with SCAD penalty (Problem 1). We give a few remarks on the implication of our hardness results."
  }, {
    "heading": "5.1 Hardness of Regression with SCAD Penalty",
    "text": "Let us try to understand how significant is the nonapproximable error of Problem 1. We consider the special case of linear models with SCAD penalty. Let the input data (A, b) be generated by the linear model Ax̄ + ε = b, where x̄ is the unknown true sparse coefficients and ε is a zero-mean multivariate subgaussian noise. Given the data size n and variable dimension d, we follow (Fan & Li, 2001) and obtain a special case of Problem 1, given by\nmin x\n1 2 ‖Ax− b‖22 + n d∑ j=1 pγ(|xj |), (2)\nwhere γ = √\nlog d/n. (Fan & Li, 2001) showed that the optimal solution x∗ of problem (2) has a small statistical error, i.e., ‖x̄ − x∗‖22 = O ( n−1/2 + an ) , where an = max{p′λ(|x∗j |) : x∗j 6= 0}. (Fan et al., 2015) further showed that we only need to find a √ n log d-optimal solution to (2) to achieve such a small estimation error.\nHowever, Theorem 2 tells us that it is not possible to compute an d,n-optimal solution for problem (2) in polynomial time, where d,n = λκn1/2d1/3 (by letting c1 = 1/2, c2 = 1/3). In the special case of problem (2), we can verify that λ = n and κ = Ω(γ2) = Ω(log d/n). As a result, we see that\nd,n = Ω(n 1/2d1/3)\n√ n log d,\nfor high values of the dimension d. According to Theorem 2, it is strongly NP-hard to approximately solve problem (2) within the required statistical precision √ n log d. This result illustrates a sharp contrast between statistical properties of sparse estimation and the worst-case computational complexity."
  }, {
    "heading": "5.2 Remarks on the NP-Hardness Results",
    "text": "As illustrated by the preceding analysis, the nonapproximibility of Problem 1 suggests that computing the sparse estimator is hard. The results suggest a fundamental conflict between computation efficiency and estimation accuracy in sparse data analysis. Although the results seem negative, they should not discourage researchers from studying computational perspectives of sparse optimization. We make the following remarks:\n1. Theorems 1, 2 are worst-case complexity results. They suggest that one cannot find a tractable solution to the sparse optimization problems, without making any additional assumption to rule out the worst-case instances.\n2. Our results do not exclude the possibility that, under more stringent modeling and distributional assump-\ntions, the problem would be tractable with high probability or on average.\nIn short, the sparse optimization Problem 1 is fundamentally hard from a purely computational perspective. This paper together with the prior related works provide a complete answer to the computational complexity of sparse optimization."
  }, {
    "heading": "6 Proof of Theorem 1",
    "text": "In this section, we prove Theorem 1. The proof of Theorems 2 is deferred to the appendix which is based on the idea of the proof in this section. We construct a polynomial-time reduction from the 3-partition problem (Garey & Johnson, 1978) to the sparse optimization problem. Given a set S of 3m integers s1, ...s3m, the three partition problem is to determine whether S can be partitioned into m triplets such that the sum of the numbers in each subset is equal. This problem is known to be strongly NPhard (Garey & Johnson, 1978). The main proof idea bears a similar spirit as the works by Huo & Chen (2010), Chen et al. (2014) and Chen & Wang (2016). The proofs of all the lemmas can be found in the appendix.\nWe first illustrate several properties of the penalty function if it satisfies the conditions in Theorem 1.\nLemma 3. If p(t) satisfies the conditions in Theorem 1, then for any l ≥ 2, and any t1, t2, . . . , tl ∈ R, we have p(|t1|) + · · ·+ p(|tl|) ≥ min{p(|t1 + · · ·+ tl|), p(τ)}. Lemma 4. If p(t) satisfies the conditions in Theorem 1, then there exists τ0 ∈ (0, τ) such that p(·) is concave but not linear on [0, τ0] and is twice continuously differentiable on [τ0, τ ]. Furthermore, for any t̃ ∈ (τ0, τ), let δ̄ = min{τ0/3, t̃ − τ0, τ − t̃}. Then for any δ ∈ (0, δ̄) l ≥ 2, and any t1, t2, . . . , tl such that t1 + · · ·+ tl = t̃, we have p(|t1|) + · · ·+ p(|tl|) < p(t̃) + C1δ only if |ti − t̃| < δ for some i while |tj | < δ for all j 6= i, where C1 =\np(τ0/3)+p(2τ0/3)−p(τ0) τ0/3 > 0.\nIn our proof of Theorem 1, we will consider the following function\ngθ,µ(t) := p(|t|) + θ · |t|q + µ · |t− τ̂ |q\nwith θ, µ > 0, where τ̂ is an arbitrary fixed rational number in (τ0, τ). We have the following lemma about gθ,µ(t).\nLemma 5. If p(t) satisfies the conditions in Theorem 1, q > 1, and τ0 satisfies the properties in Lemma 4, then there exist θ > 0 and µ > 0 such that for any θ ≥ θ and µ ≥ µ · θ, the following properties are satisfied:\n1. g′′θ,µ(t) ≥ 1 for any t ∈ [τ0, τ ];\n2. gθ,µ(t) has a unique global minimizer t∗(θ, µ) ∈ (τ0, τ);\n3. Let δ̄ = min{t∗(θ, µ) − τ0, τ − t∗(θ, µ), 1}, then for any δ ∈ (0, δ̄), we have gθ,µ(t) < h(θ, µ) + δ2 only if |t− t∗(θ, µ)| < δ, where h(θ, µ) is the minimal value of gθ,µ(t).\nLemma 6. If p(t) satisfies the conditions in Theorem 1, q = 1, and τ0 satisfies the properties in Lemma 4, then there exist µ̂ > 0 such that for any µ ≥ µ̂, the following properties are satisfied:\n1. g′0,µ(t) < −1 for any t ∈ [τ0, τ̂) and g′0,µ(t) > 1 for any t ∈ (τ̂ , τ ];\n2. g0,µ(t) has a unique global minimizer t∗(0, µ) = τ̂ ∈ (τ0, τ);\n3. Let δ̄ = min{τ̂−τ0, τ− τ̂ , 1}, then for any δ ∈ (0, δ̄), we have g0,µ(t) < h(0, µ) + δ2 only if |t− τ̂ | < δ.\nBy combining the above results, we have the following lemma, which is useful in our proof of Theorem 1.\nLemma 7. Suppose p(t) satisfies the conditions in Theorem 1 and τ0 satisfies the properties in Lemma 4. Let h(θ, µ) and t∗(θ, µ) be as defined in Lemma 5 and Lemma 6 respectively for the case q > 1 and q = 1. Then we can find θ and µ such that for any l ≥ 2, t1, . . . , tl ∈ R,\nl∑ j=1 p(|tj |) + θ · ∣∣∣∣∣∣ l∑ j=1 tj ∣∣∣∣∣∣ q + µ · ∣∣∣∣∣∣ l∑ j=1 tj − τ̂ ∣∣∣∣∣∣ q ≥ h(θ, µ).\nMoreover, let δ̄ = min { τ0 3 , t∗(θ,µ)−τ0 2 , τ−t∗(θ,µ) 2 , 1, C1 } where C1 is defined in Lemma 4, then for any δ ∈ (0, δ̄), we have l∑ j=1 p(|tj |)+θ · ∣∣∣∣∣∣ l∑ j=1 tj ∣∣∣∣∣∣ q +µ · ∣∣∣∣∣∣ l∑ j=1 tj − τ̂ ∣∣∣∣∣∣ q < h(θ, µ)+δ2 (3) holds only if |ti − t∗(θ, µ)| < 2δ for some i while |tj | ≤ δ for all j 6= i.\nProof of Theorem 1. We present a polynomial time reduction to problem (1) from the 3-partition problem. For any given instance of the 3-partition problem with b = (b1, . . . , b3m), we consider the minimization problem minx f(x) in the form of (1) with x = {xij}, 1 ≤ i ≤\n3m, 1 ≤ j ≤ m, where\nf(x) := m∑ j=2 ∣∣∣∣∣ 3m∑ i=1 bixij − 3m∑ i=1 bixi1 ∣∣∣∣∣ q + 3m∑ i=1 ∣∣∣∣∣∣(λθ) 1q m∑ j=1 xij ∣∣∣∣∣∣ q\n+ 3m∑ i=1 ∣∣∣∣∣∣(λµ) 1q  m∑ j=1 xij − τ̂ ∣∣∣∣∣∣ q + λ 3m∑ i=1 m∑ j=1 p(|xij |).\nNote that the lower bounds θ, µ, and µ̂ only depend on the penalty function p(·), we can choose θ ≥ θ and µ ≥ µθ if q > 1, or θ = 0 and µ ≥ µ̂ if q = 1, such that (λθ)\n1/q and (λµ)1/q are both rational numbers. Since τ̂ is also rational, all the coefficients of f(x) are of finite size and independent of the input size of the given 3-partition instance. Therefore, the minimization problem minx f(x) has polynomial size with respect to the given 3-partition instance.\nFor any x, by Lemma 7,\nf(x) ≥0 + λ · 3m∑ i=1 { m∑ j=1 p(|xij |) + θ · ∣∣∣∣∣∣ m∑ j=1 xij ∣∣∣∣∣∣ q\n+ µ · ∣∣∣∣∣∣ m∑ j=1 xij − τ̂ ∣∣∣∣∣∣ q } ≥ 3mλ · h(θ, µ). (4)\nNow we claim that there exists an equitable partition to the 3-partition problem if and only if the optimal value of f(x) is smaller than 3mλ · h(θ, µ) + where is specified later. On one hand, if S can be equally partitioned intom subsets, then we define\nxij = { t∗(θ, µ) if bi belongs to the jth subset; 0 otherwise.\nIt can be easily verified that these xij’s satisfy f(x) = 3mλ · h(θ, µ). Then due to (4), we know that these xij’s provide an optimal solution to f(x) with optimal value 3mλ · h(θ, µ).\nOn the other hand, suppose the optimal value of f(x) is 3mλ · h(θ, µ), and there is a polynomial-time algorithm that solves (1). Then for\nδ = min\n{ τ0\n8 ∑3m i=1 bi , δ̄\n} and = min{λδ2, (τ0/2)q}\nwhere\nδ̄ = min { τ0 3 , t∗(θ, µ)− τ0 2 , τ − t∗(θ, µ) 2 ,\np(τ0/3) + p(2τ0/3)− p(τ0) τ0/3 , 1\n} ,\nwe are able to find a near-optimal solution x such that f(x) < 3mλ · h(θ, µ) + within a polynomial time of log(1/ ) and the size of f(x), which is polynomial with respect to the size of the given 3-partition instance. Now we show that we can find an equitable partition based on this near-optimal solution. By the definition of , f(x) < 3mλ · h(θ, µ) + implies\nm∑ j=1 p(|xij |) + θ ∣∣∣∣∣∣ m∑ j=1 xij ∣∣∣∣∣∣ q + µ · ∣∣∣∣∣∣ m∑ j=1 xij − τ ∣∣∣∣∣∣ q\n<h(θ, µ) + δ2, ∀i = 1, . . . , 3m.\n(5)\nAccording to Lemma 7, for each i = 1, . . . , 3m, (5) implies that there exists k such that |xik − t∗(θ, µ)| < 2δ and |xij | < δ for any j 6= k. Now let\nyij =\n{ t∗(θ, µ) if |xik − t∗(θ, µ)| < 2δ\n0 if |xij | < δ .\nWe define a partition by assigning bi to the jth subset Sj if yij = t\n∗(θ, µ). Note that this partition is well-defined since for each i, by the definition of δ, there exists one and only one yik = t∗(θ, µ) while the others equal 0. Now we show that this is an equitable partition.\nNote that for any j = 1, . . . ,m, the difference between the sum of the j-th subset and the first subset is∣∣∣∣∣∣ ∑ Sj bi − ∑ S1 bi ∣∣∣∣∣∣ = ∣∣∣∣∣ 3m∑ i=1 yij t∗(θ, µ) · bi − 3m∑ i=1 yi1 t∗(θ, µ) · bi ∣∣∣∣∣ = 1\nt∗(θ, µ) ∣∣∣∣∣ 3m∑ i=1 biyij − 3m∑ i=1 biyi1 ∣∣∣∣∣ . By triangle inequality, we have∣∣∣∣∣∣ ∑ Sj bi − ∑ S1 bi ∣∣∣∣∣∣ ≤ 1t∗(θ, µ) ( 3m∑ i=1 bi · |yij − xij |\n+ 3m∑ i=1 bi · |yi1 − xi1|+ ∣∣∣∣∣ 3m∑ i=1 bixij − 3m∑ i=1 bixi1 ∣∣∣∣∣ ) .\nBy the definition of yij , we have |yij − xij | < 2δ for any i, j. for the last term, since f(x) < 3mλ · h(θ, µ) + , we know that∣∣∣∣∣ n∑ i=1 bixij − n∑ i=1 bixi1\n∣∣∣∣∣ < 1/q ≤ τ0/2. Therefore, we have∣∣∣∣∣∣ ∑ Sj bi − ∑ S1 bi ∣∣∣∣∣∣ < 1t∗(θ, µ) ( 4δ n∑ i=1 bi + τ0 2 ) ≤ 1.\nNow since bi’s are all integers, we must have ∑ Sj bi =∑\nS1 bi, which means that the partition is equitable."
  }],
  "year": 2017,
  "references": [{
    "title": "On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems",
    "authors": ["E. Amaldi", "V. Kann"],
    "venue": "Theoretical Computer Science,",
    "year": 1998
  }, {
    "title": "Regularization of wavelet approximations",
    "authors": ["A. Antoniadis", "J. Fan"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2001
  }, {
    "title": "The hardness of approximate optima in lattices, codes, and systems of linear equations",
    "authors": ["S. Arora", "L. Babai", "J. Stern", "Z. Sweedy"],
    "venue": "In Foundations of Computer Science,",
    "year": 1993
  }, {
    "title": "Optimality conditions and complexity for non-lipschitz constrained optimization problems",
    "authors": ["W. Bian", "X. Chen"],
    "year": 2014
  }, {
    "title": "Regression analysis of count data, volume 53",
    "authors": ["A.C. Cameron", "P.K. Trivedi"],
    "venue": "Cambridge university press,",
    "year": 2013
  }, {
    "title": "Enhancing sparsity by reweighted L1 minimization",
    "authors": ["E. Candes", "M. Wakin", "S. Boyd"],
    "venue": "Journal of Fourier Analysis and Applications,",
    "year": 2008
  }, {
    "title": "Exact reconstruction of sparse signals via nonconvex minimization",
    "authors": ["R. Chartrand"],
    "venue": "Signal Processing Letters, IEEE,",
    "year": 2007
  }, {
    "title": "Complexity of unconstrained L2 − Lp minimization",
    "authors": ["X. Chen", "D. Ge", "Z. Wang", "Y. Ye"],
    "venue": "Mathematical Programming,",
    "year": 2014
  }, {
    "title": "Hardness of approximation for sparse optimization with L0 norm",
    "authors": ["Y. Chen", "M. Wang"],
    "venue": "Technical Report,",
    "year": 2016
  }, {
    "title": "Adaptive greedy approximations",
    "authors": ["G. Davis", "S. Mallat", "M. Avellaneda"],
    "venue": "Constructive approximation,",
    "year": 1997
  }, {
    "title": "Variable selection via nonconcave penalized likelihood and its oracle properties",
    "authors": ["J. Fan", "R. Li"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2001
  }, {
    "title": "A selective overview of variable selection in high dimensional feature space",
    "authors": ["J. Fan", "J. Lv"],
    "venue": "Statistica Sinica,",
    "year": 2010
  }, {
    "title": "Strong oracle optimality of folded concave penalized estimation",
    "authors": ["J. Fan", "L. Xue", "H. Zou"],
    "venue": "The Annals of Statistics,",
    "year": 2014
  }, {
    "title": "TAC for sparse learning: Simultaneous control of algorithmic complexity and statistical error",
    "authors": ["J. Fan", "H. Liu", "Q. Sun", "T. Zhang"],
    "venue": "arXiv preprint arXiv:1507.01037,",
    "year": 2015
  }, {
    "title": "Variable selection is hard",
    "authors": ["D. Foster", "H. Karloff", "J. Thaler"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "A statistical view of some chemometrics regression",
    "authors": ["L.E. Frank", "J.H. Friedman"],
    "venue": "tools. Technometrics,",
    "year": 1993
  }, {
    "title": "Strong”NP-completeness results: Motivation, examples, and implications",
    "authors": ["M.R. Garey", "D.S. Johnson"],
    "venue": "Journal of the ACM (JACM),",
    "year": 1978
  }, {
    "title": "Robust estimation of a location parameter",
    "authors": ["P.J. Huber"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1964
  }, {
    "title": "Complexity of penalized likelihood estimation",
    "authors": ["X. Huo", "J. Chen"],
    "venue": "Journal of Statistical Computation and Simulation,",
    "year": 2010
  }, {
    "title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
    "authors": ["Loh", "P.-L", "M.J. Wainwright"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A unified approach to model selection and sparse recovery using regularized least squares",
    "authors": ["J. Lv", "Y. Fan"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "Generalized linear models",
    "authors": ["P. McCullagh"],
    "venue": "European Journal of Operational Research,",
    "year": 1984
  }, {
    "title": "Sparse approximate solutions to linear systems",
    "authors": ["B.K. Natarajan"],
    "venue": "SIAM journal on computing,",
    "year": 1995
  }, {
    "title": "Optimal computational and statistical rates of convergence for sparse nonconvex learning problems",
    "authors": ["Z. Wang", "H. Liu", "T. Zhang"],
    "venue": "Annals of statistics,",
    "year": 2014
  }, {
    "title": "Nonconcave penalized composite conditional likelihood estimation of sparse ising models",
    "authors": ["L. Xue", "H. Zou", "T Cai"],
    "venue": "The Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Nearly unbiased variable selection under minimax concave penalty",
    "authors": ["Zhang", "C.-H"],
    "venue": "The Annals of Statistics,",
    "year": 2010
  }, {
    "title": "Analysis of multi-stage convex relaxation for sparse regularization",
    "authors": ["T. Zhang"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Lower bounds on the performance of polynomial-time algorithms for sparse linear regression",
    "authors": ["Y. Zhang", "M.J. Wainwright", "M.I. Jordan"],
    "venue": "In COLT,",
    "year": 2014
  }],
  "id": "SP:cc981eafc850110c4f47677db3ed7f37bdcdf410",
  "authors": [{
    "name": "Yichen Chen",
    "affiliations": []
  }, {
    "name": "Dongdong Ge",
    "affiliations": []
  }, {
    "name": "Mengdi Wang",
    "affiliations": []
  }, {
    "name": "Zizhuo Wang",
    "affiliations": []
  }, {
    "name": "Yinyu Ye",
    "affiliations": []
  }, {
    "name": "Hao Yin",
    "affiliations": []
  }],
  "abstractText": "Consider the regularized sparse minimization problem, which involves empirical sums of loss functions for n data points (each of dimension d) and a nonconvex sparsity penalty. We prove that finding an O(n1d2)-optimal solution to the regularized sparse optimization problem is strongly NP-hard for any c1, c2 ∈ [0, 1) such that c1 + c2 < 1. The result applies to a broad class of loss functions and sparse penalty functions. It suggests that one cannot even approximately solve the sparse optimization problem in polynomial time, unless P = NP.",
  "title": "Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions"
}