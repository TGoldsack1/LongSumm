{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 648–658 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Several psycholinguistic studies in the last two decades have brought extensive evidence that humans activate a rich array of event knowledge during sentence processing: verbs (e.g. arrest) activate expectations about their typical arguments (e.g. cop, thief ) (McRae et al., 1998; Altmann and Kamide, 1999; Ferretti et al., 2001; McRae et al., 2005; Hare et al., 2009; Matsuki et al., 2011), and nouns activate other nouns typically co-occurring in the same events (Kamide et al., 2003; Bicknell et al., 2010). Subjects are able to determine the plausibility of a noun for a given argument role and quickly use this knowledge to anticipate upcoming linguistic input (McRae and Matsuki, 2009). This phenomenon is referred to in the literature as thematic fit. Thematic fit estimation\nhas been extensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009).\nStarting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”.\nThe present work sets itself among the unsupervised approaches to thematic fit estimation. By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-parsed corpora. The key features of our proposal are a) prototypical representations of verb-specific thematic roles, based on feature weighting and filtering of second order contexts\n1For an overview on constraint-based models, see MacDonald and Seidenberg (2006).\n2We adopt the terminology from Baroni et al. (2014).\n648\n(i.e. contexts that are salient for many of the typical fillers of a given verb-specific thematic role), and b) a similarity measure which computes the Weighted Overlap (WO) between prototypes and candidate fillers.3"
  }, {
    "heading": "2 Related Work",
    "text": "Erk et al. (2010) were, at the best of our knowledge, the first authors to measure the correlation between human-elicited thematic fit ratings and the scores assigned by a syntax-based Distributional Semantic Model (DSM). More specifically, their gold standard consisted of the human judgments collected by McRae et al. (1998) and Padó (2007). The plausibility of each verb-filler pair was computed as the similarity between new candidate nouns and previously attested exemplars for each specific verb-role pairing (as already proposed in Erk (2007)).\nBaroni and Lenci (2010) evaluated their Distributional Memory (henceforth DM)4 framework on the same datasets, adopting an approach to the task that has become dominant in the literature: for each verb role, they built a prototype vector by averaging the dependency-based vectors of its most typical fillers. The higher the similarity of a noun with a role prototype, the higher its plausibility as a filler for that role. Lenci (2011) has later extended the model to account for the dynamic update of the expectations on an argument, depending on how another role is filled. By using the same DM tensor, this study tested an additive and a multiplicative model (Mitchell and Lapata, 2010) to compose and update the expectations on the patient filler of the subject-verb-object triples of the Bicknell dataset (Bicknell et al., 2010).\nThe thematic fit models proposed by Sayeed and Demberg (2014) and Sayeed et al. (2015) are similar to Baroni and Lenci’s, but their DSMs were built by using the roles assigned by the SENNA semantic role labeler (Collobert et al., 2011) to define the feature space. These authors argued that the prototype-based method with dependencies works well when applied to the agent and to the patient role (which are almost always syntactically realized as subjects and objects), but\n3Code: https://github.com/esantus/Thematic Fit 4In this paper, we will make reference to two different models of DM: DepDM and TypeDM. DepDM counts the frequency of dependency links between words (e.g. read, obj, book), while TypeDM uses the variety of surface forms that express the link between words, rather than the link itself.\nthat it might be problematic to apply it to different roles, such as instruments and locations, as the construction of the prototype would have to rely on prepositional complements as typical fillers, and the meaning of prepositions can be ambiguous. Comparing their results with Baroni and Lenci (2010), the authors showed that their system outperforms the syntax-based model DepDM and almost matches the scores of the best performing TypeDM, which uses hand-crafted rules. Moreover, they were the first to evaluate thematic role plausibility for roles other than agent and patient, as they computed the scores also for the instruments and for the locations of the Ferretti datasets (Ferretti et al., 2001).\nGreenberg et al. (2015a,b) further developed the TypeDM and the role-based models, investigating the effects of verb polysemy on human thematic fit judgments and introducing a hierarchical agglomerative clustering algorithm into the prototype creation process. Their goal was to cluster together typical fillers into multiple prototypes, corresponding to different verb senses, and their results showed constant improvements of the performance of the DM-based model.\nFinally, Tilk et al. (2016) presented two neural network architectures for generating probability distributions over selectional preferences for each thematic role. Their models took advantage of supervised training on two role-labeled corpora to optimize the distributional representation for thematic fit modeling, and managed to obtain significant improvements over the other systems on almost all the evaluation datasets. They also evaluated their model on the task of composing and updating verb argument expectations, obtaining a performance comparable to Lenci (2011)."
  }, {
    "heading": "3 Methodology",
    "text": "As pointed out by Sayeed et al. (2016), most works on unsupervised thematic fit estimation vary in the method adopted for constructing the prototypes. The semantic role prototype is usually a vector, obtained by averaging the most typical fillers, and plausibility of new fillers depends on their similarity to the prototype, assessed by means of vector cosine (the standard similarity measure for DSMs; see Turney and Pantel (2010)).\nIts merits notwithstanding, we argue that this method is not optimal for characterizing roles. Distributional vectors are typically built as out-of-\ncontext representations, and they conflate different senses. By building the prototype as the centroid of a cluster of vectors and measuring then the thematic fit with vector cosine, the plausibility score is inevitably affected by many contexts that are irrelevant for the specific verb-argument combination.5 This is likely to be one of the main reasons behind the difficulties of modeling roles other than agent and patient with syntax-based DSMs. We claim that improving the prototype representation might lead to a better characterization of thematic roles, and to a better treatment of polysemy.\nWhen a verb and an argument are composed, humans are intuitively able to select only the part of the potential meaning of the words that is relevant for the concept being expressed (e.g. in The player hit the ball, humans would certainly exclude from the meaning of ball semantic dimensions that are strictly related to its dancing sense). In other words, not all the features of the semantic representations are active, and the composition process makes some features more ‘prominent’, while moving others to the background.6\nAlthough we are not aware of experimental works specifically dedicated to verb-argument composition, a similar idea has been supported in studies on conceptual combinations (Hampton, 1997, 2007): when a head and a modifier are combined, their interaction affects the saliency of the features in the original concepts. For example, in racing car, the most salient properties would be those related to SPEED, whereas in family car SPACE properties would probably be more prominent. Yeh and Barsalou (2006) used a property priming experiment to show how the concept features activated during language comprehension vary across the background situations described by the sentence they occur in. When concepts are combined in a sentence, the features that are relevant for the specific combination are activated and are then easier to verify for human subjects.\nThe same could be true for linguisticallyderived properties of lexical meaning: Simmons et al. (2008) brought neuroimaging evidence of the early activation of word association areas during property generation tasks, and Santos et al. (2011)\n5For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a).\n6An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition.\nshowed that word associates are often among the properties generated for a given concept. Such findings suggest that, while we combine concepts, both embodied simulations and word distributions influence property salience (Barsalou et al., 2008).\nOur model makes the following assumptions:\n• the composition between a verb role representation and an argument shares the same cognitive mechanism underlying conceptual combinations;\n• at least part of semantic representations is derived from, and/or mirrored in, linguistic data.7 Consistently, the process of selecting the relevant features of the concepts being composed corresponds to modify the salience of the dimensions of distributional vectors;\n• thematic fit computation is carried out on the basis of the activation and selection of salient features of a verb thematic role prototype and of the candidate argument filler vectors.\nWe rely on syntax-based DSMs, using dependency relations to approximate verb-specific roles and to identify their most typical fillers: for agents/patients, we extract the most frequent subjects/objects, for instruments we use the prepositional complements introduced by with, and for locations those introduced by either on, at or in.\nAssuming that the linguistic features of distributional vectors correspond to the properties of conceptual composition processes, a candidate filler can be represented as a sorted distributional vector of the filler term, in which the most salient contexts occupy the top positions. Similarly, the abstract representation of a verb-specific role is a sorted prototype-vector, whose features derive from the sum of the most typical filler vectors for that verb-specific role.\nDifferently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by APSyn, a recent proposal by Santus\n7See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008).\net al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific role prototype."
  }, {
    "heading": "3.1 Typical Fillers",
    "text": "The first step of our method consists in identifying the typical fillers of a verb-specific role. Following Baroni and Lenci (2010), we weighted the raw cooccurrences between verbs, syntactic relations and fillers in the TypeDM tensor of DM with Positive Local Mutual Information (PLMI; Evert (2004)).\nGiven the co-occurrence count Ovrf of the verb v, a syntactic relation r and the filler f , we computed the expected count Evrf under the assumption of statistical independence:\nPLMI(v, r, f) = log ( Ov,r,f Ev,r,f ) ∗Ov,r,f (1)\nFrom the ranked list of (v,r,f) tuples, for each slot, we selected as typical fillers the top k lexemes with the highest PLMI scores (see examples in Table 1, Typical Fillers column). In our experiments, we report results for k = {10, 30, 50}."
  }, {
    "heading": "3.2 Role Prototype Vectors",
    "text": "To represent the typical fillers, the candidate fillers and the verb-specific role prototypes (which are obtained by summing their typical filler vectors), we built a syntax-based DSM. This includes relation:word contexts, like sbj:dog, obj:apple, etc..\nContexts were weighted with Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990), Bullinaria and Levy (2012), Levy et al. (2015)). Given a context c and a word w, the PPMI is defined as follows:\nPPMI(w, c) = max(PMI(w, c), 0) (2)\nPMI(w, c) = log\n( P (w, c)\nP (w)P (c)\n) = log ( |w, c|D |w||c| ) (3)\nwhere w is the target word, c is the given context, P(w,c) is the probability of co-occurrence, and D is the collection of observed word-context pairs.8\n8A variant of this DSM weighted with PLMI (which is simply the PPMI multiplied by the word-context frequency) was also built, but because of its lower and inconsistent per-\nThe context c of the prototype vector P representing a thematic role has a value corresponding to the sum of the values of c for each of the k typical fillers used to build P . The contexts of P are then sorted according to their weight. Desirably, the highest-ranking contexts for a role prototype will be those that are more strongly associated with many of its typical fillers. Such second order contexts correspond to the most salient features of the verb-specific thematic role, as they are salient for many role fillers (some examples are reported in Table 1, Top Second Order Contexts column).\nIn summary, we built centroid vectors for our verb-specific thematic roles by means of second order contexts, which are first order dependencybased contexts of the most typical fillers of a verbspecific role. Since we are interested only in the most salient contexts, we ranked the centroid contexts according to their PPMI score, and we took the resulting rank as a distributional characterization of the thematic roles."
  }, {
    "heading": "3.3 Filtering the Contexts",
    "text": "Filtering the prototype dimensions according to syntactic criteria might be useful to improve our role representations. It is, indeed, reasonable to hypothesize that predicates co-occurring with the typical patients of a verb are more relevant for the characterization of its patient role than – let’s say – prepositional complements, as they correspond to other actions that are typically performed on the same patients.\nImagine that apple, pizza, cake etc. are among the most salient fillers for the OBJ slot of to eat, and that OBJ-1:slice-v, OBJ-1:devour-v, SBJ:kidn, INSTRUMENT:fork-n, LOCATION:table-n are some of the most salient contexts of the prototype.9 Things that are typically sliced and/or devoured are more likely to be good fillers for the patient role to eat than things that are simply located on a table or that are patients of actions performed by kids. To test this hypothesis, we evaluated the performance of the system in three different settings, each of which selecting:\nformance we will not discuss it further. Santus et al. (2016c) previously showed that their rank-based measure performs worse on PLMI-weighted vectors, as they are biased towards frequent contexts.\n9Our DSM also makes use of inverse syntactic dependencies: target SYN-1 context means that target is linked to context by the dependency relation SYN (e.g. meal OBJ-1 devour means that meal is OBJ of devour).\n• only predicates in a subject/object relation (SO setting);\n• only prepositional complements (PREP setting);\n• both of them (ALL setting)."
  }, {
    "heading": "3.4 Computing the Thematic Fit",
    "text": "Our hypothesis is that fillers whose salienceranked vector has a large overlap with the prototype representation should have a high thematic fit. Such overlap should take into account not only the number of shared features, but also their respective ranks in the salience-ranked vectors.\nWhen the prototype has been computed and the candidate filler vector has also been sorted, we can measure the Weighted Overlap by adapting APSyn (Santus et al., 2016a,b,c) to our needs:\nWO(wx, wy) = ∑\n∀f (x[1:N]∩y[1:N])\n1\navg(rx(f), ry(f)) (4)\nwhere for every feature f in the intersection between the top N features of the sorted vectors x, x[1:N ], and y, y[1:N ], we sum 1 divided by the average rank of the shared feature in x and y, rx(f) and ry(f) (N is a tunable parameter).\nThis measure assigns the maximum score to vectors sharing exactly the same dimensions, in the same salience ranking. The lower the rank of a shared context in the sorted vector, the smaller its contribution to the thematic fit score. If the feature set intersection is empty, the score will be 0.\nDifferently from cosine similarity, which conflates multiple senses, measuring the Weighted Overlap between prototype and candidate filler can improve the estimation of the thematic fit by favoring the appropriate word senses: for example, for a verb-argument pair like embracev–communism-n, communism-n is likely to intersect and to increase the saliency (through the average rank) only of the second-order features of embrace-v referring to its abstract sense."
  }, {
    "heading": "4 Experiments",
    "text": "Datasets. We tested our method on three popular datasets for thematic fit estimation, namely McRae et al. (1998), Ferretti et al. (2001) and Padó (2007). All the datasets contain human plausibility judgments for verb-role-filler triples. McRae and Padó include scores for agent and patient roles, whereas Ferretti includes instruments and locations (see Table 2 for the coverage of each system for the datasets). Metrics. Performance is evaluated as the Spearman correlation between the scores of the systems and the human plausibility judgments. Fillers. In order to make our results more comparable with previous studies, the typical fillers for each verb role were extracted from the TypeDM tensor of the Distributional Memory framework (see Section 3.1).10 Those were the same fillers used by Baroni and Lenci (2010) and Greenberg et al. (2015b). DSM. Distributional information is derived from the concatenation of two corpora: the British National Corpus (Leech, 1992) and Ukwac (Baroni et al., 2009). Both were parsed with the Maltparser (Nivre and Hall, 2005). From this concatenation, we built a dependency-based DSMs, weighted with PPMI, containing 20,145 targets (i.e. nouns and verbs with frequency above 1000) and 94,860 contexts. The syntactic relations taken into account were: sbj, sbj-1, obj, obj-1, at-1, in-1, on-1, with-1. Settings. To prove our hypotheses and verify the consistency of the system, we tested a large range of settings, varying:\n10http://clic.cimec.unitn.it/dm/\n• the number of fillers used to build the prototype, with the most typical values in the literature ranging between 10 and 50. We report the results for 10, 30 and 50 fillers\n• the types of the dependency relations used for calculating the overlap: we report results for the SO, PREP and ALL settings;\n• the value of N , that is the number of top contexts that we take into account when computing the weighted overlap. Table 3 reports the scores for our best setting, while the performances for other values of N are discussed in the Section 5.\nBaseline and State of the Art. As a baseline, we use the thematic fit model by Baroni and Lenci (2010), with no ranking of the features of the prototypes and with vector cosine as a similarity metric.11 Results are reported for 10, 30 and 50 fillers. For reference, we also report the results of state-of-the-art models, both the unsupervised (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b) and the supervised ones (Tilk et al., 2016)."
  }, {
    "heading": "5 Results",
    "text": "Table 3 describes the performance of the best setting (weight: PPMI; N=2000). In the first three rows, the table shows the scores obtained by our\n11This baseline is equivalent to the approach of Baroni and Lenci (2010), except for the fact that it is applied on a standard dependency-based DSM and not on TypeDM, which combines dependency links and handcrafted lexico-syntactic patterns: see Section 2.\nsystem varying the types of dependency contexts (i.e. ALL, SO, PREP) and the number of fillers considered for the prototype (i.e. 10, 30 and 50). The other rows respectively show i) the scores obtained by calculating the vector cosine between the role prototype vector (i.e. the vector obtained by summing the most typical fillers, with no salience ranking of the dimensions) and the candidate filler vector and ii) the scores reported in the literature for the best unsupervised and supervised models.\nAt a glance, our best scores always outperform the reimplementation of Baroni and Lenci, being mostly competitive with the state of the art models. More precisely, for agents and patients the performance is close to the reported scores for DM, when only predicates are used in the WO calculation, as hypothesized in Section 3.3. The neural network of Tilk and colleagues retains a significant advantage on our models only for the McRae dataset. Our system, however, shows a remarkable improvements on the Ferretti’s datasets, and specifically on Ferretti-Instruments, when only complements are used (see Section 3.3), outperforming even the supervised and more complex model by Tilk et al. (2016), which has access to semantic roles information. Compared to the other unsupervised models, our system has a statistically significant advantage over Baroni and Lenci (2010) on the locations dataset and over Sayeed and Demberg (2014) on the locations and on the instruments dataset (p < 0.05).12\nAt the best of our knowledge, the result for the\n12p-values computed with Fisher’s r-to-z transformation.\ninstruments is the best reported until now in the literature. This is particularly interesting because – as pointed out by Sayeed and Demberg (2014) – instruments and locations are difficult to model for a dependency-based system, given the ambiguity of prepositional phrases (e.g. with does not only encode instruments, but it can also encode other roles, such as in I ate a pizza with Mark). We think this is the main reason behind the different trend observed for the Instruments datasets with respect to the number of the fillers (see Table 3 and Figure 1). Unlike all the other datasets, instrument prototypes built with more fillers tend to be more noisy and therefore to pull down both the vector cosine and WO performance (this is partially true also for locations, where the performances – for cosine and WO with a lower number of contexts – drop with more than 30 fillers: see Figure 1). Systems based on semantic role labeling have an advantage in this sense, as they do not have to deal with prepositional ambiguity.\nOur results show that, by weighting and filtering the features of the role prototype, dependency-based approaches can be successful in modeling roles other than agent and patient, eventually dealing also with the ambiguity of prepositional phrases.\nSettings. Apart from the above-mentioned exceptions, the best scores are obtained building the prototypes with a higher number of fillers, typically with 50, and calculating the WO only with a syntactically-filtered set of contexts. More specifically, Padó and McRae benefit from the calculation of WO using only second order subject-object predicates (i.e. SO), while FerrettiInstruments and Ferretti-Locations benefit from the exclusive use of prepositional complements (i.e. PREP). On the other hand, the opposite setting (e.g. SO for Ferretti-Instruments and FerrettiLocations and PREP for Padó and McRae) leads to much lower scores, whereas the full vectors (i.e.\nALL) tend to have a stable-but-not-excellent performances on all datasets.\nAs briefly mentioned above, in our experiments, we tested both PPMI and PLMI as weighting measures. Table 3 only reports PPMI scores because it performs more regularly than PLMI, whose behaviour is often unpredictable.\nA parameter that has an impact on the performance of our system is the value of N , which is the number of second order contexts that are considered when calculating the WO. We have noticed that the performance of WO is directly related to the growth of N , and this can be noticed in Figure 1, where WO is plotted for the different values of N with every combination of dataset and number of fillers. For space reasons, the plot only contains the performance for the best type of second order contexts for each dataset (i.e. SO for Padó and McRae and COMP for FerrettiLocations and Ferretti-Instruments). As it can be seen in Figure 1, the scores of WO tend to grow with the growth of N in all datasets. Interestingly, they are largely above the competitive baseline in most of the cases, the only exceptions being Padó (where a large N is necessary to outperform the baseline) and Ferretti-Locations with 10 fillers (prepositional ambiguity might have caused the introduction of noisy fillers among the top ones).\nAgent & Patient. In order to further evaluate our system, we have split Padó and McRae datasets into agent and patient subsets. Figure 2 describes the performance of WO and vector cosine baseline while varying N and the number of fillers. The plot shows a clearly better performance of WO for the agent role (i.e. subject), especially when N is equal or over 1000 (note that the value of N has little impact in the agent subset of the McRae dataset). Such advantage, however, is reduced for the patient role (i.e. object). This is particularly interesting because we do not observe large drops in performance for the vector cosine\nbetween agent and patient role (except for Padó, k = 10). The drop is particularly noticeable in Padó, a dataset which has several non-constraining verbs (especially for the patient role: a similar observation was also made by Tilk et al. (2016)). As the constraints on the typical fillers of such verbs are very loose, we hypothesize that it is more difficult to find a set of salient features that are shared by many typical fillers. Therefore, estimations based on the whole vectors turn out to be more reliable. This can be confirmed by looking at the worst correlated words reported in Lexemes column, in Table 4."
  }, {
    "heading": "5.1 Error Analysis",
    "text": "We performed an error analysis to verify – for the best settings of WO in each dataset – the correlation between vector cosine and WO scores (see Table 5), and the peculiarities of the entries with the strongest and the weakest correlation (see Table 4).\nWe found that WO and vector cosine always have a high correlation (i.e. above 0.80), with the highest correlations reported for McRae and Ferretti-Instruments. Looking at Table 4 we can also observe that:\n• the average gold value of the 35 most (4.65) and least (4.56) correlated items does not substantially differ from the average gold value calculated on the full datasets (4.31), meaning that the distribution of likely and unlikely fillers among the best and worst correlated items is similar to the one in the datasets (i.e. no bias can be identified);\n• both measures have difficulties on the same test items (probably because of loose semantic constraints), but report their best performances on different pairs (see Overlap and Lexemes columns);\n• syntactically, vector cosine correlates better with objects, while WO is more balanced between objects and subjects, often showing a preference for the latter (see the distribution in Syntax column)."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper, we have introduced an unsupervised distributional method for modeling predicateargument thematic fit judgments which works purely on syntactic information.\nThe method, inspired by cognitive and psycholinguistic findings, consists in: i) extracting and filtering the most salient second order contexts for each verb-specific role, i.e. the most salient semantic dimensions of typical verb-specific role fillers; and then ii) estimating the thematic fit as a weighted overlap between the top features of the candidate fillers and of the prototypes. Once tested on some popular datasets of thematic fit judgments, our method consistently outperforms a baseline re-implementing the thematic fit model of Baroni and Lenci (2010) and proves to be competitive with state of the art models. It even registered the best performance on the Ferretti-Instruments dataset and it is the second best on the FerrettiLocations, which were known to be particularly hard to model for dependency-based approaches.\nOur method is simple, economic and efficient, it works purely on syntactic dependencies (so it does not require a role-labeled corpus) and achieves good results even with no supervised training. Finally, it offers linguistically and cognitively grounded insights on the process of prototype creation and contextual feature salience, preparing the ground for further speculations and optimizations. For example, future work might aim at identifying strategies for tuning the parameter N to account for the different degrees of selectivity of each verb-specific role. Another possible extension would be the inclusion of a mechanism for updating the role prototypes depending on how the other roles are filled, which would be the key for a more realistic and dynamic model of thematic fit expectations (Lenci, 2011)."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their helpful suggestions.\nThis work has been carried out thanks to the support of the A*MIDEX grant (nANR-11-IDEX0001-02) funded by the French Government “Investissements d’Avenir” program."
  }],
  "year": 2017,
  "references": [{
    "title": "Incremental Interpretation at Verbs: Restricting the Domain of Subsequent Reference",
    "authors": ["Gerry T.M Altmann", "Yuki Kamide."],
    "venue": "Cognition 73(3):247 – 264.",
    "year": 1999
  }, {
    "title": "The WaCky wide web: a Collection of Very Large Linguistically Processed Web-crawled Corpora",
    "authors": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta."],
    "venue": "Language Resources and Evaluation 43(3):209–226.",
    "year": 2009
  }, {
    "title": "Dont Count, Predict! A Systematic Comparison of Context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "Context-predicting Semantic Vectors. In Proceedings of ACL. volume 1.",
    "year": 2014
  }, {
    "title": "Distributional Memory: A General Framework for Corpus-based Semantics",
    "authors": ["Marco Baroni", "Alessandro Lenci."],
    "venue": "Computational Linguistics 36(4):673–721.",
    "year": 2010
  }, {
    "title": "Language and Simulation in Conceptual Processing",
    "authors": ["Lawrence W Barsalou", "Ava Santos", "W Kyle Simmons", "Christine D Wilson."],
    "venue": "Symbols, embodiment, and meaning pages 245–283.",
    "year": 2008
  }, {
    "title": "Effects of Event Knowledge in Processing Verbal Arguments",
    "authors": ["Klinton Bicknell", "Jeffrey L Elman", "Mary Hare", "Ken McRae", "Marta Kutas."],
    "venue": "Journal of Memory and Language 63(4):489–505.",
    "year": 2010
  }, {
    "title": "Extracting Semantic Representations from Word Cooccurrence Statistics: Stop-lists, Stemming, and SVD",
    "authors": ["John A. Bullinaria", "Joseph P. Levy."],
    "venue": "Behavior Research Methods 44(3):890–907.",
    "year": 2012
  }, {
    "title": "Word Association Norms, Mutual Information, and Lexicography",
    "authors": ["Kenneth Ward Church", "Patrick Hanks."],
    "venue": "Computational Linguistics 16(1):22–29.",
    "year": 1990
  }, {
    "title": "Natural Language Processing (Almost) from Scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Improving Zero-shot Learning by Mitigating the Hubness Problem",
    "authors": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni."],
    "venue": "Proceedings of ICLR.",
    "year": 2015
  }, {
    "title": "A Simple, Similarity-based Model for Selectional Preferences",
    "authors": ["Katrin Erk."],
    "venue": "Proceedings of ACL.",
    "year": 2007
  }, {
    "title": "A flexible, corpus-driven model of regular and inverse selectional preferences",
    "authors": ["Katrin Erk", "Sebastian Padó", "Ulrike Padó."],
    "venue": "Computational Linguistics 36:723–763.",
    "year": 2010
  }, {
    "title": "The Statistics of Word Cooccurrences: Word Pairs and Collocations",
    "authors": ["Stefan Evert."],
    "venue": "Ph.D. thesis.",
    "year": 2004
  }, {
    "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
    "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer."],
    "venue": "Proceedings of ACL Workshop on Evaluating Vector Space Representations for NLP.",
    "year": 2016
  }, {
    "title": "Integrating Verbs, Situation Schemas, and Thematic Role Concepts",
    "authors": ["Todd R. Ferretti", "Ken McRae", "Andrea Hatherell."],
    "venue": "Journal of Memory and Language 44(4):516 – 547.",
    "year": 2001
  }, {
    "title": "Verb Polysemy and Frequency Effects in Thematic Fit Modeling",
    "authors": ["Clayton Greenberg", "Vera Demberg", "Asad Sayeed."],
    "venue": "Proceedings of NAACL Workshop on Cognitive Modeling and Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Improving Unsupervised Vector-space Thematic Fit Evaluation via Role-filler Prototype Clustering",
    "authors": ["Clayton Greenberg", "Asad B. Sayeed", "Vera Demberg."],
    "venue": "Proceedings of HLT-NAACL.",
    "year": 2015
  }, {
    "title": "Conceptual Combination",
    "authors": ["James A Hampton."],
    "venue": "Knowledge, Concepts, and Categories pages 133– 159.",
    "year": 1997
  }, {
    "title": "Typicality, Graded Membership, and Vagueness",
    "authors": ["James A. Hampton."],
    "venue": "Cognitive Science 31:355– 384.",
    "year": 2007
  }, {
    "title": "Activating Event Knowledge",
    "authors": ["Mary Hare", "Michael Jones", "Caroline Thomson", "Sarah Kelly", "Ken McRae."],
    "venue": "Cognition 111 2:151–67.",
    "year": 2009
  }, {
    "title": "The Time-course of Prediction in Incremental Sentence Processing: Evidence from Anticipatory Eye Movements",
    "authors": ["Yuki Kamide", "Gerry T.M Altmann", "Sarah L Haywood."],
    "venue": "Journal of Memory and Language 49(1):133 – 156.",
    "year": 2003
  }, {
    "title": "Predication",
    "authors": ["Walter Kintsch."],
    "venue": "Cognitive Science 25:173–202.",
    "year": 2001
  }, {
    "title": "100 Million Words of English: the British National Corpus (BNC)",
    "authors": ["Geoffrey Leech."],
    "venue": "Language Research 28(1):1–13.",
    "year": 1992
  }, {
    "title": "Distributional Semantics in Linguistic and Cognitive Research",
    "authors": ["Alessandro Lenci."],
    "venue": "Italian Journal of Linguistics 20(1):1–31.",
    "year": 2008
  }, {
    "title": "Composing and Updating Verb Argument Expectations: A Distributional Semantic Model",
    "authors": ["Alessandro Lenci."],
    "venue": "Proceedings of ACL Workshop on Cognitive Modeling and Computational Linguistics.",
    "year": 2011
  }, {
    "title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."],
    "venue": "TACL 3:211–225.",
    "year": 2015
  }, {
    "title": "Distance Weighted Cosine Similarity Measure for Text Classification",
    "authors": ["Baoli Li", "Liping Han."],
    "venue": "Intelligent Data Engineering and Automated Learning. Springer Berlin Heidelberg, Berlin, Heidelberg, pages 611–618.",
    "year": 2013
  }, {
    "title": "Constraint Satisfaction Accounts of Lexical and Sentence Comprehension",
    "authors": ["Maryellen C MacDonald", "Mark S Seidenberg."],
    "venue": "Handbook of psycholinguistics 2:581–611.",
    "year": 2006
  }, {
    "title": "Event-based Plausibility Immediately Influences On-line Language Comprehension",
    "authors": ["Kazunaga Matsuki", "Tracy Chow", "Mary Hare", "Jeffrey L Elman", "Christoph Scheepers", "Ken McRae."],
    "venue": "Journal of experimental psychology. Learning, memory, and",
    "year": 2011
  }, {
    "title": "A basis for generating expectancies for verbs from nouns",
    "authors": ["Ken McRae", "Mary Hare", "Jeffrey L. Elman", "Todd Ferretti."],
    "venue": "Memory & Cognition 33(7):1174–1184.",
    "year": 2005
  }, {
    "title": "People Use their Knowledge of Common Events to Understand Language, and Do So as Quickly as Possible",
    "authors": ["Ken McRae", "Kazunaga Matsuki."],
    "venue": "Language and Linguistics Compass 3(6):1417–1429.",
    "year": 2009
  }, {
    "title": "Modeling the Influence of Thematic Fit (and Other Constraints) in Online Sentence Comprehension",
    "authors": ["Ken McRae", "Michael J. Spivey-Knowlton", "Michael K. Tanenhaus."],
    "venue": "Journal of Memory and Language 38(3):283–312.",
    "year": 1998
  }, {
    "title": "Contextual Correlates of Semantic Similarity",
    "authors": ["George A. Miller", "Walter G. Charles."],
    "venue": "Language and Cognitive Processes 6(1):1–28.",
    "year": 1991
  }, {
    "title": "Composition in Distributional Models of Semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive science 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "Maltparser: A Language-independent System for Data-driven Dependency Parsing",
    "authors": ["Joakim Nivre", "Johan Hall."],
    "venue": "Proceedings of Workshop on Treebanks and Linguistic Theories. pages 13–95.",
    "year": 2005
  }, {
    "title": "The Integration of Syntax and Semantic Plausibility in a Wide-coverage Model of Human Sentence Processing",
    "authors": ["Ulrike Padó."],
    "venue": "Ph.D. thesis.",
    "year": 2007
  }, {
    "title": "Property Generation Reflects Word Association and Situated Simulation",
    "authors": ["Ava Santos", "Sergio E. Chaigneau", "W. Kyle Simmons", "Lawrence W. Barsalou."],
    "venue": "Language and Cognition 3(1):83119.",
    "year": 2011
  }, {
    "title": "Testing APSyn against Vector Cosine on Similarity Estimation",
    "authors": ["Enrico Santus", "Emmanuele Chersoni", "Alessandro Lenci", "Chu-Ren Huang", "Philippe Blache."],
    "venue": "Proceedings of PACLIC.",
    "year": 2016
  }, {
    "title": "Unsupervised Measure of Word Similarity: how to Outperform Co-occurrence and Vector Cosine in VSMs",
    "authors": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-Ren Huang."],
    "venue": "Proceedings of the AAAI. AAAI Press, pages 4260–",
    "year": 2016
  }, {
    "title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets",
    "authors": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-Ren Huang."],
    "venue": "Proceedings of LREC.",
    "year": 2016
  }, {
    "title": "Combining Unsupervised Syntactic and Semantic Models of Thematic Fit",
    "authors": ["Asad Sayeed", "Vera Demberg."],
    "venue": "Proceedings of CLIC.",
    "year": 2014
  }, {
    "title": "An Exploration of Semantic Features in an Unsupervised Thematic Fit Evaluation Framework",
    "authors": ["Asad Sayeed", "Vera Demberg", "Pavel Shkadzko."],
    "venue": "Italian Journal of Linguistics.",
    "year": 2015
  }, {
    "title": "Thematic Fit Evaluation: an Aspect of Selectional Preferences",
    "authors": ["Asad Sayeed", "Clayton Greenberg", "Vera Demberg."],
    "venue": "Proceedings of ACL Workshop for Evaluating Vector Space Representations for NLP.",
    "year": 2016
  }, {
    "title": "Evaluation Methods for Unsupervised Word Embeddings",
    "authors": ["Tobias Schnabel", "Igor Labutov", "David M. Mimno", "Thorsten Joachims."],
    "venue": "Proceedings of EMNLP.",
    "year": 2015
  }, {
    "title": "fMRI Evidence for Word Association and Situated Simulation in Conceptual Processing",
    "authors": ["W Kyle Simmons", "Stephan B Hamann", "Carla L Harenski", "Xiaoping P Hu", "Lawrence W Barsalou."],
    "venue": "Journal of Physiology 102 1-3:106–19.",
    "year": 2008
  }, {
    "title": "Event Participant Modelling with Neural Networks",
    "authors": ["Ottokar Tilk", "Vera Demberg", "Asad B. Sayeed", "Dietrich Klakow", "Stefan Thater."],
    "venue": "Proceedings of EMNLP.",
    "year": 2016
  }, {
    "title": "From Frequency to Meaning: Vector Space Models of Semantics",
    "authors": ["Peter D. Turney", "Patrick Pantel."],
    "venue": "Journal of Artificial Intelligence Research 37:141–188.",
    "year": 2010
  }, {
    "title": "A Robust and Extensible Exemplar-Based Model of Thematic Fit",
    "authors": ["Bram Vandekerckhove", "Dominiek Sandra", "Walter Daelemans."],
    "venue": "Proceedings of EACL.",
    "year": 2009
  }, {
    "title": "The Situated Nature of Concepts",
    "authors": ["Wenchi Yeh", "Lawrence W Barsalou."],
    "venue": "The American Journal of Psychology 119:3:349–84.",
    "year": 2006
  }],
  "id": "SP:9268a7e2290a9e0fa905f05dd14c2789d9de7c7b",
  "authors": [{
    "name": "Enrico Santus",
    "affiliations": []
  }, {
    "name": "Emmanuele Chersoni",
    "affiliations": []
  }, {
    "name": "Alessandro Lenci",
    "affiliations": []
  }, {
    "name": "Philippe Blache",
    "affiliations": []
  }],
  "abstractText": "In this paper, we introduce a new distributional method for modeling predicateargument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-theart system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verbspecific semantic roles.",
  "title": "Measuring Thematic Fit with Distributional Feature Overlap"
}