{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1090–1100 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics\nMultilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings."
  }, {
    "heading": "1 Introduction",
    "text": "Topic models provide a high-level view of the main themes of a document collection (Boyd-Graber et al., 2017). Document collections, however, are often not in a single language, driving the development of multilingual topic models. These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis (Vulić et al., 2015), such as detecting cultural differences (Gutiérrez et al., 2016) and bilingual dictionary extraction (Liu et al., 2015).\nMonolingual topic models can be evaluated through likelihood (Wallach et al., 2009b) or coherence (Newman et al., 2010), but topic model evaluation is not well understood in multilingual settings. Our contributions are two-fold. We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2). We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications. This metric\ncorrelates well with human judgments and crosslingual classification results (Sections 5 and 6).\nWe also focus on evaluation in low-resource languages, which lack large parallel corpora, dictionaries, and other tools that are often used in learning and evaluating topic models. To adapt CNPMI to these settings, we create a coherence estimator (Section 3) that extrapolates statistics derived from antiquated, specialized texts like the Bible: often the only resource available for many languages."
  }, {
    "heading": "2 Evaluating Multilingual Coherence",
    "text": "A multilingual topic contains one topic for each language. For a multilingual topic to be meaningful to humans (Figure 1), the meanings should be consistent across the languages, in addition to coherent within each language (i.e., all words in a topic are related).\nThis section describes our approach to evaluating the quality of multilingual topics. After defining the multilingual topic model, we describe topic model evaluation extending standard monolingual approaches to multilingual settings."
  }, {
    "heading": "2.1 Multilingual Topic Modeling",
    "text": "Probabilistic topic models associate each document in a corpus with a distribution over latent topics, while each topic is associated with a distribution over words in the vocabulary. The most widely used topic model, latent Dirichlet allocation (Blei et al., 2003, LDA), can be extended to connect languages. These extensions require additional knowledge to link languages together.\nOne common encoding of multilingual knowledge is document links (indicators that documents are parallel or comparable), used in polylingual topic models (Mimno et al., 2009; Ni et al., 2009). In these models, each document d indexes a tuple of parallel/comparable language-specific documents,\n1090\nd(`), and the language-specific “views” of a document share the document-topic distribution θd. The generative story for the document-links model is:\n1 for each topic k and each language ` do 2 Draw a distribution over words φ`k ∼ Dirichlet(β); 3 for each document tuple d = ( d(1), . . . , d(L) ) do 4 Draw a distribution over topics θd ∼ Dirichlet(α); 5 for each language ` = 1, . . . ,L do 6 for each token t ∈ d(`) do 7 Draw a topic zn ∼ θd; 8 Draw a word wn ∼ φ`z;\nAlternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vulić et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model."
  }, {
    "heading": "2.2 Monolingual Evaluation",
    "text": "Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI). NPMI compares the joint probability of words appearing together Pr(wi,wj) to their probability assuming independence Pr(wi) Pr(wj), normalized by the joint probability:\nNPMI(wi,wj) = log\nPr(wi,wj) Pr(wi) Pr(wj)\nlog Pr(wi,wj) . (1)\nThe word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset.\nThe quality of topic k is the average NPMI of all word pairs (wi,wj) in the topic:\nNPMIk = −1( C 2 ) ∑ i∈W(k,C) ∑ j 6=i NPMI(wi,wj), (2)\nwhere W(k,C) are the C most probable words in the topic-word distribution φk (the number of words is the topic’s cardinality). Higher NPMIk means the topic’s top words are more coupled."
  }, {
    "heading": "2.3 Existing Multilingual Evaluations",
    "text": "While automatic evaluation has been well-studied for monolingual topic models, there are no robust evaluations for multilingual topic models. We first consider two straightforward metrics that could be used for multilingual evaluation, both with limitations. We then propose an extension of NPMI that addresses these limitations.\nInternal Coherence. A simple adaptation of NPMI is to calculate the monolingual NPMI score for each language independently and take the average. We refer this as internal NPMI (INPMI) as it evaluates coherence within a language. However, this metric does not consider whether the topic is coherent across languages—that is, whether a language-specific word distribution φ`1k is related to the corresponding distribution in another language, φ`2k.\nCrosslingual Consistency. Another straightforward measurement is Matching Translation Accuracy (Boyd-Graber and Blei, 2009, MTA), which counts the number of word translations in a topic between two languages using a bilingual dictionary. This metric can measure whether a topic is well-aligned across languages literally, but cannot capture non-literal more holistic similarities across languages."
  }, {
    "heading": "2.4 New Metric: Crosslingual NPMI",
    "text": "We extend NPMI to multilingual models, with a metric we call crosslingual normalized pointwise mutual information (CNPMI). This metric will be the focus of our experiments.\nA multilingually coherent topic means that if wi,`1 in language `1 and wj,`2 in language `2 are in the same topic, they should appear in similar contexts in comparable or parallel corporaR(`1,`2).\nOur adaptation of NPMI is based on the same principles as the monolingual version, but focuses on the co-occurrences of bilingual word pairs. Given a bilingual word pair (wi,`1 ,wj,`2) the co-occurrence of this word pair is the event where word wi,`1 appears in a document in language `1 and the word wj,`2 appears in a comparable or parallel document in language `2.\nThe co-occurrence probability of each bilingual word pair is:\nPr (wi,`1 ,wj,`2) , ∣∣{d : wi,`1 ∈ d(`1),wj,`2 ∈ d(`2) }∣∣ ∣∣R(`1,`2) ∣∣ , (3)\nwhere d = ( d(`1), d(`2) ) is a pair of parallel/comparable documents in the reference corpus R(`1,`2). When one or both words in a bilingual pair do not appear in the reference corpus, the cooccurrence score is zero.\nSimilar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C2 bilingual word pairs,\nCNPMI(`1, `2, k) =\n∑C i,j NPMI (wi,`1 ,wj,`2)\nC2 . (4)\nIt is straightforward to generalize CNPMI from a language pair to multiple languages by averaging CNPMI(`i, `j , k) over all language pairs (`i, `j)."
  }, {
    "heading": "3 Adapting to Low-Resource Languages",
    "text": "CNPMI needs a reference corpus for co-occurrence statistics. Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016). Unfortunately, Wikipedia is often unavailable or not large enough for lowresource languages. It only covers 282 languages,1 and only 249 languages have more than 1,000 pages: many of pages are short or unlinked to\n1 https://meta.wikimedia.org/wiki/List_of_Wikipedias\na high-resource language. Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents.\nAnother option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic. It is good at evaluating topics such as family and religion, but not “modern” topics like biology and Internet. Without reference co-occurrence statistics relevant to these topics, CNPMI will fail to judge topic coherence—it must give the ambiguous answer of zero. Such a score could mean a totally incoherent topic where each word pair never appears together (Topics 6 in Figure 1), or an unjudgeable topic (Topic 5).\nOur goal is to obtain a reliable estimation of topic coherence for low-resource languages when the Bible is the only reference. We propose a model that can correct the drawbacks of a Bible-derived CNPMI. While we assume bilingual topics paired with English, our approach can be applied to any high-resource/low-resource language pair.\nWe take Wikipedia’s CNPMI from high-resource languages as accurate estimations. We then build a coherence estimator on topics from high-resource languages, with the Wikipedia CNPMI as the target output. We use linear regression using the below features. Given a topic in low-resource language, the estimator produces an estimated coherence (Figure 2)."
  }, {
    "heading": "3.1 Estimator Features",
    "text": "The key to the estimator is to find features that capture whether we should trust the Bible. For generality, we focus on features independent of the available resources other than the Bible. This section describes the features, which we split into four groups.\nBase Features (BASE) Our base features include information we can collect from the Bible and the topic model: cardinality C, CNPMI and INPMI, MTA, and topic word coverage (TWC), which counts the percentage of topic words in a topic that appear in a reference corpus.\nCrosslingual Gap (GAP) A low CNPMI score could indicate a topic pair where each language has a monolingually coherent topic but that are not about the same theme (Topic 6 in Figure 1). Thus, we add two features to capture this information\n2The Bible is available in 2,530 languages.\nusing the Bible: mismatch coefficients (MC) and internal comparison coefficients (ICC):\nMC(`1; `2, k) = CNPMI(`1, `2, k)\nINPMI(`1, k) + α , (5)\nICC(`1, `2, k) = INPMI(`1, k) + α\nINPMI(`2, k) + α , (6)\nwhere α is a smoothing factor (α = 0.001 in our experiments). MC recognizes the gap between crosslingual and monolingual coherence, so a higher MC score indicates a gap between coherence within and across languages. Similarly, ICC compares monolingual coherence to tell if both languages are coherent: the closer to 1 the ICC is, the more comparable internal coherence both languages have.\nWord Era (ERA) Because the Bible’s vocabulary is unable to evaluate modern topics, we must tell the model what the modern words are. The word era features are the earliest usage year 3 for each word in a topic. We use both the mean and standard deviation as features.\nMeaning Drift (DRIFT). The meaning of a word can expand and drift over time. For example, in the Bible, “web” appears in Isaiah 59:5:\nThey hatch cockatrice’ eggs, and weave the spider’s web.\n3 https://oxforddictionaries.com/\nThe word “web” could be evaluated correctly in an animal topic. For modern topics, however, Bible fails to capture modern meanings of “web”, as in Topic 5 (Figure 1).\nTo address this meaning drift, we use a method similar to Hamilton et al. (2016). For each English word, we calculate the context vector from Bible and from Wikipedia with a window size of five and calculate the cosine similarity between them as word similarity. Similar context vectors mean that the usage in the Bible is consistent with Wikipedia. We calculate word similarities for all the English topic words in a topic and use the average and standard deviation as features."
  }, {
    "heading": "3.2 Example",
    "text": "In Figure 3, Topic 1 is coherent while Topic 8 is not. From left to right, we incrementally add new feature sets, and show how the estimated topic coherence scores (dashed lines) approach the ideal CNPMI (dotted lines). When only using the BASE features, the estimator gives a higher prediction to Topic 8 than to Topic 1. Their low MTA and TWC prevent accurate evaluations. Adding GAP does not help much. However, ICC(EN, AM, k = 1) is much smaller, which might indicate a large gap of internal coherence between the two languages.\nAdding ERA makes the estimated scores flip between the two topics. Topic 1 has word era of 1823, much older than Topic 8’s word era of 1923, in-\ndicating that Topic 8 includes modern words the Bible lacks (e.g., “computer”). Using all the features, the estimator gives more accurate topic coherence evaluations."
  }, {
    "heading": "4 Experiments: Bible to Wikipedia",
    "text": "We experiment on six languages (Table 1) from three corpora: Romanian (RO) and Swedish (SV) from EuroParl as representative of well-studied and rich-resource languages (Koehn, 2005); Amharic (AM) and Tagalog (TL) from collected news, as lowresource languages (Huang et al., 2002a,b); and Chinese (ZH) and Turkish (TR) from TED Talks 2013 (Tiedemann, 2012), adding language variety to our experiments. Each language is paired with English as a bilingual corpus.\nTypical preprocessing methods (stemming, stop word removal, etc.) are often unavailable for lowresource languages. For a meaningful comparison across languages, we do not apply any stemming or lemmatization strategies, including English, except removing digit numbers and symbols. However, we remove words that appear in more than 30% of documents for each language.\nEach language pair is separately trained using the MALLET (McCallum, 2002) implementation of the polylingual topic model. Each experiment runs five Gibbs sampling chains with 1,000 iterations per chain with twenty topics. The hyperparameters are set to the default values (α = 0.1, β = 0.01), and are optimized every 50 iterations in MALLET using slice sampling (Wallach et al., 2009a)."
  }, {
    "heading": "4.1 Evaluating Multilingual Topics",
    "text": "We use Wikipedia and the Bible as reference corpora for calculating co-occurrence statistics. Different numbers of Wikipedia articles are available for each language pair (Table 1), while the Bible contains a complete set of 1,189 chapters for all of its translations (Christodoulopoulos and Steed-\nAre these two groups of words talking about the same thing?\nman, 2015). We use Wiktionary as the dictionary to calculate MTA."
  }, {
    "heading": "4.2 Training the Estimator",
    "text": "In addition to experimenting on Wikipedia-based CNPMI, we also re-evaluate the topics’ Bible coherence using our estimator. In the following experiments, we use an AdaBoost regressor with linear regression as the coherence estimator (Friedman, 2002; Collins et al., 2000). The estimator takes a topic and low-quality CNPMI score as input and outputs (hopefully) an improved CNPMI score.\nTo make our testing scenario more realistic, we treat one language as our estimator’s test language and train on multilingual topics from the other languages. We use three-fold cross-validation over languages to select the best hyperparameters, including the learning rate and loss function in AdaBoost.R2 (Drucker, 1997)."
  }, {
    "heading": "5 Topic-Level Evaluation",
    "text": "We first study CNPMI at the topic level: does a particular topic make sense? An effective evaluation should be consistent with human judgment of the topics (Chang et al., 2009). In this section, we measure gold-standard human interpretability of multilingual topics to establish which automatic measures of topic interpretability work best."
  }, {
    "heading": "5.1 Task Design",
    "text": "Following monolingual coherence evaluations (Lau et al., 2014), we present topic pairs to bilingual CrowdFlower users. Each task is a topic pair with the top ten topic words (C = 10) for each language. We ask if both languages’ top words in a multilingual topic are talking about the same concept (Figure 4), and make a judgment on a three-point scale—coherent (2 points), somewhat coherent (1 point), and incoherent (0 points). To ensure the users have adequate language competency, we insert several topics that are easily identifiable as incoherent as a qualification test.\nWe randomly select sixty topics from each language pair (360 topics total), and each topic is judged by five users. We take the average of the judgment points and calculate Pearson correlations with the proposed evaluation metrics (Table 2). NPMI-based scores are separately calculated from each reference corpus."
  }, {
    "heading": "5.2 Agreement with Human Judgments",
    "text": "CNPMI (the extended metric) has higher correlations with human judgments than INPMI (the naive adaptation of monolingual NPMI), while MTA (matching translation accuracy) correlations are comparable to CNPMI.\nUnsurprisingly, when using Wikipedia as the reference, the correlations are usually higher than when using the Bible. The Bible’s archaic content limits its ability to estimate human judgments in modern corpora (Section 3).\nNext, we compare CNPMI to two baselines: INPMI and MTA. As expected, CNPMI outperforms INPMI regardless of reference corpus overall, because INPMI only considers monolingual coherence. MTA has higher correlations than CNPMI\nscores from the Bible, because the Bible fails to give accurate estimates due to limited topic coverage. MTA, on the other hand, only depends on dictionaries, which are more comprehensive than the Bible. It is also possible that users are judging coherence based on translations across a topic pair, rather than the overall coherence, which would closely correlate with MTA."
  }, {
    "heading": "5.3 Re-Estimating Topic-Level Coherence",
    "text": "The Bible—by itself—produces CNPMI values that do not correlate well with human judgments (Table 2). After training an estimator (Section 4.2), we calculate Pearson’s correlation between Wikipedia’s CNPMI and the estimated topic coherence score (Table 3). A higher correlation with Wikipedia’s CNPMI means more accurate coherence.\nAs a baseline, the correlation of Bible-based CNPMI without adaptation has negative and nearzero correlations with Wikipedia;4 it does not capture coherence. After training the estimator, the correlations become stronger, indicating the estimated scores are closer to Wikipedia’s CNPMI."
  }, {
    "heading": "5.4 When MTA Falls Short",
    "text": "We analyze MTA from two aspects—the inability to capture semantically-related non-translation topic words, and insensitivity to cardinality—to show why MTA is not an ideal measurement, even though it correlates well with human judgments.\nSemantics We take two examples with EN-ZH (Topic 1) and EN-TL (Topic 2) in Figure 5. Topic 1 has fewer translation pairs than Topic 2, which leads to a lower MTA score for Topic 1. However, all words in Topic 1 talk about art, while it is hard to interpret Topic 2. Wikipedia CNPMI scores reveals\n4Normally one would not estimate CNPMI on rich-resource languages using low-resource languages. For completeness, however, we also include these situations.\nTopic 1 is more coherent. Because our experiments are on datasets with little divergence between the themes discussed across languages, this is uncommon for us but could appear in noisier datasets.\nCardinality Increasing cardinality diminishes a topic’s coherence (Lau and Baldwin, 2016). We vary the cardinality of topics from ten to fifty at intervals of ten (Figure 6). As cardinality increases, more low-probability and irrelevant words appear the topic, which lowers CNPMI scores. However, MTA stays stable or increases with increasing cardinality. Thus, MTA fails to fulfill a critical property of topic model evaluation.\nFinally, MTA requires a comprehensive multilingual dictionary, which may be unavailable for lowresource languages. Additionally, most languages often only have one dictionary, which makes it problematic to use the same resource (a language’s single multilingual dictionary) for training and evaluating models that use a dictionary to build multilingual topics (Hu et al., 2014). Given these concerns, we continue the paper’s focus on CNPMI as a data-driven alternative to MTA. However, for many applications MTA may suffice as a simple, adequate evaluation metric."
  }, {
    "heading": "6 Model-Level Evaluation",
    "text": "While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model’s constituent topics."
  }, {
    "heading": "6.1 Training Knowledge",
    "text": "Adding more knowledge to multilingual topic models improves topics (Hu et al., 2014), so an effective evaluation should reflect this improvement as knowlege is added to the model. For polylingual topic models, this knowledge takes the form of the number of linked documents.\nWe start by experimenting with no multilingual knowledge: no document pairs share a topic distribution θd (but the documents are in the collection as unlinked documents). We then increase the number of document pairs that share θd from 20% of the corpus to 100%. Fixing the topic cardinality at ten, CNPMI captures the improvements in models (Figure 7) through a higher coherence score."
  }, {
    "heading": "6.2 Agreement with Machines",
    "text": "Topic models are often used as a feature extraction technique for downstream machine learning\napplications, and topic model evaluations should reflect whether these features are useful (Ramage et al., 2009). For each model, we apply a document classifier trained on the model parameters to test whether CNPMI is consistent with classification accuracy.\nSpecifically, we want our classifier to transfer information from training on one language to testing on another (Smet et al., 2011; Heyman et al., 2016). We train a classifier on one language’s documents, where each document’s feature vector is the document-topic distribution θd. We apply this to TED Talks, where each document is labeled with multiple categories. We choose the most frequent seven categories across the corpus as labels,5 and only have labeled documents in one side of a bilingual topic model. CNPMI has very strong correlations with classification results, though using the Bible as the reference corpus gives slightly lower correlation—with higher variance— than Wikipedia (Figure 8)."
  }, {
    "heading": "6.3 Re-Estimating Model-Level Coherence",
    "text": "In Section 5.3, we improve Bible-based CNPMI scores for individual topics. Here, we show the estimator also improves model-level coherence. We apply the estimator on the models created in Section 6.2 and calculate the correlation between estimated scores and Wikipedia’s CNPMI (Table 4).\nThe coherence estimator substantially improves scores except for Turkish: the correlation is better before applying the estimator (0.911). We suspect a lack of overlap between topics between Turkish and languages other than Chinese is to blame (Figure 9); the features used by the estimator do not generalize well to other kinds of features; training on many languages pairs would hopefully solve this\n5design, global issues, art, science, technology, business, and culture\nissue. Turkish is also morphologically rich, and our preprocessing completely ignores morphology."
  }, {
    "heading": "6.4 Reference Size",
    "text": "One challenge with low-resource languages is that even if Wikipedia is available, it may have too few documents to accurately calculate coherence. As a final analysis, we examine how the reliability of CNPMI degrades with a smaller reference corpus.\nWe randomly sample 20% to 100% of document pairs from the reference corpora and evaluate the polylingual topic model with all document links (Figure 10), again fixing the cardinality as 10.\nCNPMI is stable across different amounts of ref-\nerence documents, as long as the number of reference documents is sufficiently large. If there are too few reference documents (for example, 20% of Amharic Wikipedia is only 316 documents), then CNPMI degrades."
  }, {
    "heading": "7 Related Work",
    "text": "Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI. Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments. NPMI has been the current gold standard for evaluation and improvements of monolingual topic models (Pecina, 2010; Newman et al., 2011).\nExternal Tasks Another approach is to use a model for predictive tasks: the better the results are on external tasks, the better a topic model is assumed to be. A common task is held-out likelihood (Wallach et al., 2009b; Jagarlamudi and Daumé III, 2010; Fukumasu et al., 2012), but as Chang et al. (2009) show, this does not always reflect human interpretability. Other specific tasks have also been used, such as bilingual dictionary extraction (Liu et al., 2015; Ma and Nasukawa, 2017), cultural difference deteciton (Gutiérrez et al., 2016), and crosslingual document clustering (Vulić et al., 2015).\nRepresentation Learning Topic models are one example of a broad class of techniques of learning representations of documents (Bengio et al., 2013). Other approaches learn respresentations at the word (Klementiev et al., 2012; Vyas and Carpuat, 2016), paragraph (Mogadala and Rettinger, 2016), or corpus level (Søgaard et al., 2015). However, neural representation learning approaches are often data hungry and not adaptable to low-resource languages. The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015)."
  }, {
    "heading": "8 Conclusion",
    "text": "We have provided a comprehensive analysis of topic model evaluation in multilingual settings, including for low-resource languages. While evaluation is an important area of topic model research, no previous work has studied evaluation of multilingual topic models. Our work provided two primary contributions to this area, including a new intrinsic evaluation metric, CNPMI, as well as a model for adapting this metric to low-resource languages without large reference corpora.\nAs the first study on evaluation for multilingual topic models, there is still room for improvement and further applications. For example, human judgment is more difficult to measure than in monolingual settings, and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments. As a measurement of multilingual coherence, we plan to extend CNPMI to high-dimensional representations, e.g., multilingual word embeddings, particularly in low-resource languages (Ruder et al., 2017)."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank the anonymous reviewers for their insightful and constructive comments. Hao has been supported under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Boyd-Graber and Paul were supported by NSF grant IIS-1564275. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors."
  }],
  "year": 2018,
  "references": [{
    "title": "Latent Dirichlet allocation",
    "authors": ["David M. Blei", "Andrew Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research 3:993–1022.",
    "year": 2003
  }, {
    "title": "Normalized (oointwise) mutual information in collocation dxtraction",
    "authors": ["Gerlof Bouma."],
    "venue": "Proceedings of the German Society for Computational Linguistics and Language Technology Conference.",
    "year": 2009
  }, {
    "title": "Multilingual topic models for unaligned text",
    "authors": ["Jordan Boyd-Graber", "David M. Blei."],
    "venue": "Proceedings of Uncertainty in Artificial Intelligence.",
    "year": 2009
  }, {
    "title": "Applications of Topic Models, volume 11 of Foundations and Trends in Information Retrieval",
    "authors": ["Jordan Boyd-Graber", "Yuening Hu", "David Mimno."],
    "venue": "NOW Publishers. http://www.nowpublishers.com/",
    "year": 2017
  }, {
    "title": "Reading tea leaves: how humans interpret topic models",
    "authors": ["Jonathan Chang", "Jordan Boyd-Graber", "Sean Gerrish", "Chong Wang", "David M. Blei."],
    "venue": "Proceedings of Advances in Neural Information Processing Systems.",
    "year": 2009
  }, {
    "title": "A massively parallel corpus: the Bible in 100 languages",
    "authors": ["Christos Christodoulopoulos", "Mark Steedman."],
    "venue": "Language Resources and Evaluation 49(2):375–395.",
    "year": 2015
  }, {
    "title": "Logistic regression, AdaBoost and Bregman distances",
    "authors": ["Michael Collins", "Robert E. Schapire", "Yoram Singer."],
    "venue": "Proceedings of Conference on Learning Theory.",
    "year": 2000
  }, {
    "title": "Improving regressors using boosting techniques",
    "authors": ["Harris Drucker."],
    "venue": "Proceedings of the International Conference of Machine Learning.",
    "year": 1997
  }, {
    "title": "Stochastic gradient boosting",
    "authors": ["Jerome H Friedman."],
    "venue": "Computational Statistics & Data Analysis 38(4):367–378.",
    "year": 2002
  }, {
    "title": "Symmetric correspondence topic models for multilingual text analysis",
    "authors": ["Kosuke Fukumasu", "Koji Eguchi", "Eric P. Xing."],
    "venue": "Proceedings of Advances in Neural Information Processing Systems.",
    "year": 2012
  }, {
    "title": "Detecting cross-cultural differences using a multilingual topic model",
    "authors": ["E. Dario Gutiérrez", "Ekaterina Shutova", "Patricia Lichtenstein", "Gerard de Melo", "Luca Gilardi."],
    "venue": "Transactions of the Association for Computational Linguistics 4:47–60.",
    "year": 2016
  }, {
    "title": "Cultural shift or linguistic drift? Comparing two computational measures of semantic change",
    "authors": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "C-BiLDA: Extracting cross-lingual topics from non-parallel texts by distinguishing shared from unshared content",
    "authors": ["Geert Heyman", "Ivan Vulic", "Marie-Francine Moens."],
    "venue": "Data Mining and Knowledge Discovery 30(5).",
    "year": 2016
  }, {
    "title": "Polylingual tree-based topic models for translation domain adaptation",
    "authors": ["Yuening Hu", "Ke Zhai", "Vladimir Eidelman", "Jordan L. Boyd-Graber."],
    "venue": "Proceedings of the Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "NMSU Amharic language pack from REFLEX V1.1",
    "authors": ["Shudong Huang", "David Graff", "George Doddington"],
    "venue": "Web download file. Philadelphia: Linguistic Data Consortium",
    "year": 2002
  }, {
    "title": "Tagalog language pack from reflex V1.1",
    "authors": ["Shudong Huang", "David Graff", "George Doddington"],
    "venue": "Web download file. Philadelphia: Linguistic Data Consortium",
    "year": 2002
  }, {
    "title": "Extracting multilingual topics from unaligned comparable corpora",
    "authors": ["Jagadeesh Jagarlamudi", "Hal Daumé III."],
    "venue": "Proceedings of the European Conference on Information Retrieval.",
    "year": 2010
  }, {
    "title": "Inducing crosslingual distributed representations of words",
    "authors": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."],
    "venue": "Proceedings of International Conference on Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Europarl: a parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn"],
    "year": 2005
  }, {
    "title": "Bootstrapping translation detection and sentence extraction from comparable corpora",
    "authors": ["Kriste Krstovski", "David A. Smith."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Online multilingual topic models with multilevel hyperpriors",
    "authors": ["Kriste Krstovski", "David A. Smith", "Michael J. Kurtz."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "The sensitivity of topic coherence evaluation to topic cardinality",
    "authors": ["Jey Han Lau", "Timothy Baldwin."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Machine reading tea leaves: automatically evaluating topic coherence and topic model quality",
    "authors": ["Jey Han Lau", "David Newman", "Timothy Baldwin."],
    "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Multilingual topic models for bilingual dictionary extraction",
    "authors": ["Xiaodong Liu", "Kevin Duh", "Yuji Matsumoto."],
    "venue": "ACM Transactions on Asian & Low-Resource Language Information Processing 14(3):11:1–11:22.",
    "year": 2015
  }, {
    "title": "Inverted bilingual topic models for lexicon extraction from non-parallel data",
    "authors": ["Tengfei Ma", "Tetsuya Nasukawa."],
    "venue": "International Joint Conference on Artificial Intelligence.",
    "year": 2017
  }, {
    "title": "MALLET: a machine learning for language toolkit",
    "authors": ["Andrew Kachites McCallum."],
    "venue": "http://mallet.cs.umass.edu.",
    "year": 2002
  }, {
    "title": "Polylingual topic models",
    "authors": ["David M. Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2009
  }, {
    "title": "Optimizing semantic coherence in topic models",
    "authors": ["David M. Mimno", "Hanna M. Wallach", "Edmund M. Talley", "Miriam Leenders", "Andrew McCallum."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2011
  }, {
    "title": "Bilingual word embeddings from parallel and nonparallel corpora for cross-language text classification",
    "authors": ["Aditya Mogadala", "Achim Rettinger."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Improving topic coherence with regularized topic models",
    "authors": ["David Newman", "Edwin V. Bonilla", "Wray L. Buntine."],
    "venue": "Proceedings of Advances in Neural Information Processing Systems.",
    "year": 2011
  }, {
    "title": "Automatic evaluation of topic coherence",
    "authors": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Mining multilingual topics from Wikipedia",
    "authors": ["Xiaochuan Ni", "Jian-Tao Sun", "Jian Hu", "Zheng Chen."],
    "venue": "Proceedings of the World Wide Web Conference.",
    "year": 2009
  }, {
    "title": "Lexical association measures and collocation extraction",
    "authors": ["Pavel Pecina."],
    "venue": "Proceedings of the Language Resources and Evaluation Conference 44(1-2).",
    "year": 2010
  }, {
    "title": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora",
    "authors": ["Daniel Ramage", "David Leo Wright Hall", "Ramesh Nallapati", "Christopher D. Manning."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2009
  }, {
    "title": "The Bible as a parallel corpus: annotating the ’book of 2000 tongues",
    "authors": ["Philip Resnik", "Mari Broman Olsen", "Mona Diab."],
    "venue": "Computers and the Humanities 33(1/2):129–153.",
    "year": 1999
  }, {
    "title": "Exploring the space of topic coherence measures",
    "authors": ["Michael Röder", "Andreas Both", "Alexander Hinneburg."],
    "venue": "Proceedings of ACM International Conference on Web Search and Data Mining.",
    "year": 2015
  }, {
    "title": "A survey of cross-lingual word embedding models",
    "authors": ["Sebastian Ruder", "Ivan Vulić", "Anders Søgaard."],
    "venue": "CoRR abs/1706.04902.",
    "year": 2017
  }, {
    "title": "Evaluation methods for unsupervised word embeddings",
    "authors": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 298–307.",
    "year": 2015
  }, {
    "title": "Knowledge transfer across multilingual corpora via latent topics",
    "authors": ["Wim De Smet", "Jie Tang", "Marie-Francine Moens."],
    "venue": "Pacific-Asia Advances in Knowledge Discovery and Data Mining.",
    "year": 2011
  }, {
    "title": "Inverted indexing for cross-lingual nlp",
    "authors": ["Anders Søgaard", "Željko Agić", "Héctor Martínez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen."],
    "venue": "Proceedings of the Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Parallel data, tools and interfaces in OPUS",
    "authors": ["Jörg Tiedemann."],
    "venue": "Proceedings of the Language Resources and Evaluation Conference.",
    "year": 2012
  }, {
    "title": "Cross-language information retrieval models based on latent topic models trained with documentaligned comparable corpora",
    "authors": ["Ivan Vulić", "Wim De Smet", "Marie-Francine Moens."],
    "venue": "Information Retrieval 16(3):331–368.",
    "year": 2013
  }, {
    "title": "Probabilistic topic modeling in multilingual settings: an overview of its methodology and applications",
    "authors": ["Ivan Vulić", "Wim De Smet", "Jie Tang", "MarieFrancine Moens."],
    "venue": "Information Processing & Management 51(1):111–147.",
    "year": 2015
  }, {
    "title": "Sparse bilingual word representations for cross-lingual lexical entailment",
    "authors": ["Yogarshi Vyas", "Marine Carpuat."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Rethinking LDA: Why priors matter",
    "authors": ["Hanna Wallach", "David Mimno", "Andrew McCallum."],
    "venue": "Proceedings of Advances in Neural Information Processing Systems.",
    "year": 2009
  }, {
    "title": "Evaluation methods for topic models",
    "authors": ["Hanna M. Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David M. Mimno."],
    "venue": "Proceedings of the International Conference of Machine Learning.",
    "year": 2009
  }, {
    "title": "Adapting topic models using lexical associations with tree priors",
    "authors": ["Weiwei Yang", "Jordan L. Boyd-Graber", "Philip Resnik."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing.",
    "year": 2017
  }],
  "id": "SP:e02c16d49438034f9dbe75f74b8dc04dc4887959",
  "authors": [{
    "name": "Shudong Hao",
    "affiliations": []
  }, {
    "name": "Jordan Boyd-Graber",
    "affiliations": []
  }, {
    "name": "Michael J. Paul",
    "affiliations": []
  }],
  "abstractText": "Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.",
  "title": "Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation"
}