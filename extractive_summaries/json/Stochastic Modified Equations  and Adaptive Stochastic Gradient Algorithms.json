{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Stochastic gradient algorithms are often used to solve optimization problems of the form\nmin x∈Rd\nf(x) := 1\nn n∑ i=1 fi(x), (1)\nwhere f, fi : Rd → R for i = 1, . . . , n. In machine learning applications, f is typically the total loss function whereas each fi represents the loss due to the ith training sample. x is a vector of trainable parameters and n is the training sample size, which is typically very large.\nSolving (1) using the standard gradient descent (GD) requires n gradient evaluations per step and is prohibitively expensive when n 1. An alternative, the stochastic gradient descent (SGD), is to replace the full gradient ∇f by a sampled version, serving as an unbiased estimator. In its simplest form, the SGD iteration is written as\nxk+1 = xk − η∇fγk(xk), (2) 1Institute of High Performance Computing, Singapore 2Peking University, Beijing, China 3Beijing Institute of Big Data Research, Beijing, China 4Princeton University, Princeton, NJ, USA. Correspondence to: Qianxiao Li <liqix@ihpc.a-star.edu.sg>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhere k ≥ 0 and {γk} are i.i.d uniform variates taking values in {1, 2, · · · , n}. The step-size η is the learning rate. Unlike GD, SGD samples the full gradient and its computational complexity per iterate is independent of n. For this reason, stochastic gradient algorithms have become increasingly popular in large scale problems.\nMany convergence results are available for SGD and its variants. However, most are upper-bound type results for (strongly) convex objectives, often lacking the precision and generality to characterize the behavior of algorithms in practical settings. This makes it harder to translate theoretical understanding into algorithm analysis and design.\nIn this work, we address this by pursuing a different analytical direction. We derive continuous-time stochastic differential equations (SDE) that can be understood as weak approximations (i.e. approximations in distribution) of stochastic gradient algorithms. These SDEs contain higher order terms that vanish as η → 0, but at finite and small η they offer much needed insight of the algorithms under consideration. In this sense, our framework can be viewed as a stochastic parallel of the method of modified equations in the analysis of classical finite difference methods (Noh & Protter, 1960; Daly, 1963; Hirt, 1968; Warming & Hyett, 1974). For this reason, we refer to these SDEs as stochastic modified equations (SME). Using the SMEs, we can quantify, in a precise and general way, the leadingorder dynamics of the SGD and its variants. Moreover, the continuous-time treatment allows the application of optimal control theory to study the problems of adaptive hyperparameter adjustments. This gives rise to novel adaptive algorithms and perhaps more importantly, a general methodology for understanding and improving stochastic gradient algorithms.\nNotation. We distinguish sequential and dimensional indices by writing a bracket around the latter, e.g. xk,(i) is the ith coordinate of the vector xk, the kth SGD iterate."
  }, {
    "heading": "2. Stochastic Modified Equations",
    "text": "We now introduce the SME approximation. Background materials on SDEs are found in Supplementary Materials\n(SM) B and references therein. First, rewrite the SGD iteration rule (2) as\nxk+1 − xk = −η∇f(xk) + √ ηVk, (3)\nwhere Vk = √ η(∇f(xk)−∇fγk(xk)) is a d-dimensional random vector. Conditioned on xk, Vk has mean 0 and covariance matrix ηΣ(xk) with\nΣ(x) = 1\nn n∑ i=1 (∇f(x)−∇fi(x))(∇f(x)−∇fi(x))T . (4)\nNow, consider the Stochastic differential equation\ndXt = b(Xt)dt+ σ(Xt)dWt, X0 = x0, (5)\nwhose Euler discretization Xk+1 = Xk + ∆tb(Xk) +√ ∆tσ(Xk)Zk, Zk ∼ N (0, I) resembles (3) if we set ∆t = η, b ∼ −∇f and σ ∼ (ηΣ)1/2. Then, we would expect (5) to be an approximation of (2) with the identification t = kη. It is now important to discuss the precise meaning of “an approximation”. The noises that drive the paths of SGD and SDE are independent processes, hence we must understand approximations in the weak sense.\nDefinition 1. Let 0 < η < 1, T > 0 and set N = bT/ηc. Let G denote the set of functions of polynomial growth, i.e. g ∈ G if there exists constants K,κ > 0 such that |g(x)| < K(1 + |x|κ). We say that the SDE (5) is an order α weak approximation to the SGD (2) if for every g ∈ G, there exists C > 0, independent of η, such that for all k = 0, 1, . . . , N ,\n|Eg(Xkη)− Eg(xk)| < Cηα.\nThe definition above is standard in numerical analysis of SDEs (Milstein, 1995; Kloeden & Platen, 2011). Intuitively, weak approximations are close to the original process not in terms of individual sample paths, but their distributions. We now state informally the approximation theorem.\nInformal Statement of Theorem 1. Let T > 0 and define Σ : Rd → Rd×d by (4). Assume f, fi are Lipschitz continuous, have at most linear asymptotic growth and have sufficiently high derivatives belonging to G. Then,\n(i) The stochastic process Xt, t ∈ [0, T ] satisfying\ndXt = −∇f(Xt)dt+ (ηΣ(Xt)) 1 2 dWt, (6)\nis an order 1 weak approximation of the SGD.\n(ii) The stochastic process Xt, t ∈ [0, T ] satisfying\ndXt = −∇(f(Xt) + η4 |∇f(Xt)| 2)dt+ (ηΣ(Xt)) 1 2 dWt\n(7)\nis an order 2 weak approximation of the SGD.\nThe full statement, proof and numerical verification of Thm. 1 is given in SM. C. We hereafter call equations (6) and (7) stochastic modified equations (SME) for the SGD iterations (2). We refer to the second order approximation (7) for exact calculations in Sec. 3 whereas for simplicity, we use the first order approximation (6) when discussing acceleration schemes in Sec. 4, where the order of accuracy is less important.\nThm. 1 allows us to use the SME to deduce distributional properties of the SGD. This result differs from usual convergence studies in that it describes dynamical behavior and is derived without convexity assumptions on f or fi. In the next section, we use the SME to deduce some dynamical properties of the SGD."
  }, {
    "heading": "3. The Dynamics of SGD",
    "text": ""
  }, {
    "heading": "3.1. A Solvable SME",
    "text": "We start with a case where the SME is exactly solvable. Let n = 2, d = 1 and set f(x) = x2 with f1(x) = (x−1)2−1 and f2(x) = (x+ 1)2− 1. Then, the SME (7) for the SGD iterations on this objective is (see SM. D.1)\ndXt = −2(1 + η)Xtdt+ 2 √ ηdWt,\nwithX0 = x0. This is the well-known Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930), which is exactly solvable (see SM. B.3), yielding the Gaussian distribution\nXt ∼ N (x0e−2(1+η)t, η1+η (1− e −4(1+η)t)).\nWe observe that EXt = x0e−2(1+η)t converges exponentially to the optimum x = 0 with rate −2(1 + η) but VarXt = η ( 1− e−4(1+η)t ) /(1 + η) increases from 0 to an asymptotic value of η/(1 + η). The separation t∗ between the descent phase and the fluctuations phase is given by EXt∗ = √ VarXt∗ , whose solution is\nt∗ = 14(1+η) log(1 + η+1 η x 2 0)\nFor t < t∗, descent dominates and when t > t∗, fluctuation dominates. This two-phase behavior is known for convex cases via error bounds (Moulines, 2011; Needell et al., 2014). Using the SME, we obtained a precise characterization of this behavior, including an exact expression for t∗. In Fig. 1, we verify the SME predictions regarding the mean, variance and the two-phase behavior."
  }, {
    "heading": "3.2. Stochastic Asymptotic Expansion",
    "text": "In general, we cannot expect to solve the SME exactly, especially for d > 1. However, observe that the noise terms in the SMEs (6) and (7) are O(η1/2). Hence, we can write Xt as an asymptotic series Xt = X0,t + √ ηX1,t + . . .\nwhere each Xj,t is a stochastic process with initial condition X0,0 = x0 and Xj,0 = 0 for j ≥ 1. We substitute this into the SME and expand in orders of η1/2 and equate the terms of the same order to get equations for Xj,t for j ≥ 0. This procedure is justified rigorously in Freidlin et al. (2012). We obtain to leading order (see SM. B.5),\nXt ∼ N (X0,t, ηSt), (8)\nwhere X0,t solves Ẋ0,t = −∇f(X0,t), X0,0 = x0 and Ṡt = −StHt −HtSt + Σt, where Ht = Hf(X0,t), with Hf denoting the Hessian of f , and Σt = Σ(X0,t), S0 = 0. It is then possible to deduce the dynamics of the SGD. For example, there is generally a transition between descent and fluctuating regimes. St has a steady state (assuming it is asymptotically stable) with |S∞| ∼ |Σ∞|/|H∞|. This means that one should expect a fluctuating regime where the covariance of the SGD is of order O(η|Σ∞|/|H∞|). Preceding this fluctuating regime is a descent regime governed by the gradient flow.\nWe validate our approximations on a non-convex objective. Set d = 2, n = 3 with the sample objectives f1(x) = x2(1), f2(x) = x 2 (2) and f3(x) = δ cos(x(1)/ ) cos(x(2)/ ). In Fig. 2(a), we plot f for = 0.1, δ = 0.2, showing the complex landscape. In Fig. 2(b), we compare the SGD moments |E(xk)| and |Cov(xk)| with predictions of the SME and its asymptotic approximation (8). We observe that our approximations indeed hold for this objective."
  }, {
    "heading": "4. Adaptive Hyper-parameter Adjustment",
    "text": "We showed in the previous section that the SME formulation help us better understand the precise dynamics of the SGD. The natural question is how this can translate to designing practical algorithms. In this section, we exploit the continuous-time nature of our framework to derive adaptive\nlearning rate and momentum parameter adjustment policies. These are particular illustrations of a general methodology to analyze and improve upon SGD variants. We will focus on the one dimensional case d = 1, and subsequently apply the results to high dimensional problems by local diagonal approximations."
  }, {
    "heading": "4.1. Learning Rate",
    "text": ""
  }, {
    "heading": "4.1.1. OPTIMAL CONTROL FORMULATION",
    "text": "1D SGD iterations with learning rate adjustment can be written as\nxk+1 = xk − ηukf ′(xk), (9)\nwhere uk ∈ [0, 1] is the adjustment factor and η is the maximum allowed learning rate. The corresponding SME for (9) is given by (SM. D.1)\ndXt = −utf ′(Xt)dt+ ut √ ηΣ(Xt)dWt, (10)\nwhere ut ∈ [0, 1] is now the continuous time analogue of the adjustment factor uk with the usual identification t = kη. The effect of learning rate adjustment on the dynamics of SGD is clear. Larger uk results in a larger drift term in the SME and hence faster initial descent. However, the same factor is also multiplied to the noise term, causing greater asymptotic fluctuations. The optimal learning rate schedule must balance of these two effects. The problem can therefore be posed as follows: given f, fi, how can we best choose a schedule or policy for adjusting the learning rate in order to minimize Ef at the end of the run? More precisely, this can be cast as an optimal control problem1\nmin u\nEf(XT ) subject to (10),\n1See SM. E for a brief overview of optimal control theory.\nwhere the time-dependent function u is minimized over an admissible control set to be specified. To make headway analytically, we now turn to a simple quadratic objective."
  }, {
    "heading": "4.1.2. OPTIMAL CONTROL OF THE LEARNING RATE",
    "text": "Consider the objective f(x) = 12a(x − b) 2 with a, b ∈ R. Moreover, we assume the fi’s are such that Σ(x) = Σ > 0 is a positive constant. The SME is then\ndXt = −aut(Xt − b)dt+ ut √ ηΣdWt. (11)\nNow, assume u take values in the non-random control set containing all Borel-measurable functions from [0, T ] to [0, 1]. Defining mt = Ef(Xt), and applying Itô formula to (11), we have\nṁt = −2autmt + 12aηΣu 2 t . (12)\nHence, we may now recast the control problem as\nmin u:[0,T ]→[0,1] mT subject to (12).\nThis problem can solved by dynamic programming, using the Hamilton-Jacobi-Bellman equation (Bellman, 1956). We obtain the optimal control policy (SM. E.3)\nu∗t = { 1 a ≤ 0, min(1, 2mtηΣ ) a > 0.\n(13)\nThis policy is of feed-back form since it depends on the current value of the controlled variable mt. Let us interpret the solution. First, if a < 0 we always set the maximum learning rate ut = 1. This makes sense because we have a concave objective where symmetrical fluctuations about any point x results in a lower average value of f(x). Hence, not only do high learning rates improve descent, the high fluctuations that accompany it also lowers Ef . Next, For the convex case a > 0, the solution tells us that when the objective value is large compared to variations in the gradient, we should use the maximum learning rate. When the objective decreases sufficiently, fluctuations will dominate and hence we should lower the learning rate according to the feed-back policy ut = 2mt/ηΣ.\nWith the policy (13), we can solve (12) and plug the solution for mt back into (13) to obtain the annealing schedule\nu∗t =\n{ 1 a ≤ 0 or t ≤ t∗,\n1 1+a(t−t∗) a > 0 and t > t ∗,\nwhere t∗ = (1/2a) log(4m0/ηΣ− 1). Note that by putting a = 2, b = 0,Σ = 4, for small η, this expression agrees with the transition time (8) between descent and fluctuating phases for the SGD dynamics considered in Sec. 3.1. Thus, this annealing schedule says that maximum\nlearning rate should be used for descent phases, whereas ∼ 1/t decay on learning rate should be applied after onset of fluctuations. Our annealing result agree asymptotically with the commonly studied annealing schedules (Moulines, 2011; Shamir & Zhang, 2013), but the difference is that we suggest maximum learning rate before the onset of fluctuations. Of course, the key limitation is that our result is only valid for this particular objective. This naturally brings us to the next question: how does one apply the optimal control results to general objectives?"
  }, {
    "heading": "4.1.3. APPLICATION TO GENERAL OBJECTIVES",
    "text": "Now, we turn to the setting where d > 1 and f, fi are not necessarily quadratic. The most important result in Sec. 4.1.2 is the feed-back control law (13). To apply it, we make a local diagonal-quadratic assumption: we assume that for each x ∈ Rd, there exists a(i), b(i) ∈ R so that f(x) ≈ 12 ∑d i=1 a(i)(x(i) − b(i))2 holds locally in x. We also assume Σ(x) ≈ diag{Σ(1), . . . ,Σ(d)}where each Σ(i) is locally constant. By considering a separate learning rate scale u(i) for each trainable dimension, the control problem decouples to d separate problems of the form considered in Sec. 4.1.2. And hence, we may set u∗(i) element-wise according to the policy (13).\nSince we only assume that the diagonal-quadratic assumption holds locally, the terms a(i), b(i), Σ(i) and m(i) ≈ 12a(i)(x(i) − b(i))\n2 must be updated on the fly. There are potentially many methods for doing so. The approach we take exploits the linear relationship ∇f(i) ≈ a(i)(x(i) − b(i)). Consequently, we may estimate a(i), b(i) via linear regression on the fly: for each dimension, we maintain exponential moving averages (EMA) {gk,(i), g2k,(i), xk,(i), x2k,(i), xgk,(i)} where gk,(i) = ∇fγk(xk)(i). For example, gk+1,(i) = βk,(i)gk,(i) + (1 − βk,(i))gk,(i). The EMA decay parameter βk,(i) controls the effective averaging window size. We adaptively adjust it so that it is small when gradient variations are large, and vice versa. We employ the heuristic βk+1,(i) = (g2k,(i) − g2k,(i))/g2k,(i). This is similar to the approach in Schaul et al. (2013). We also clip each βk+1,(i) to [βmin, βmax] to improve stability. Here, we use [0.9, 0.999] for all experiments, but we checked that performance is insensitive to these values. We can now compute ak,(i), bk,(i) by the ordinary-least-squares formula and Σk,(i) as the variance of the gradients:\nak,(i) = gxk,(i) − gk,(i)xk,(i) x2k,(i) − x2k,(i) ,\nbk,(i) = xk,(i) − gk,(i)\nak,(i) ,\nΣk,(i) = g2k,(i) − g2k,(i). (14)\nAlgorithm 1 controlled SGD (cSGD) Hyper-parameters: η, u0 Initialize x0; β0,(i) = 0.9 ∀i for k = 0 to (#iterations− 1) do\nCompute sample gradient∇fγk(xk) for i = 1 to d do\nUpdate EMA {gk,(i), g2k,(i), xk,(i), x2k,(i), xgk,(i)} with decay parameter βk,(i) Compute ak,(i), bk,(i), Σk,(i) using (14) Compute u∗k,(i) using (15) βk+1,(i) = (g2k,(i) − g2k,(i))/g2k,(i) and clip uk+1,(i) = βk,(i)uk,(i) + (1− βk,(i))u∗k,(i) xk+1,(i) = xk,(i) − ηuk,(i)∇fγk(xk)(i)\nend for end for\nThis allows us to estimate the policy (13) as\nu∗k,(i) = { 1 ak,(i) ≤ 0, min(1, ak,(i)(xk,(i)−bk,(i))2\nηΣk,(i) ) ak,(i) > 0.\n(15) for i = 1, 2, . . . , d. Since quantities are computed from exponentially averaged sources, we should also update our learning rate policy in the same way. The algorithm is summarized in Alg. 1. Due to its optimal control origin, we hereafter call this algorithm the controlled SGD (cSGD)\nRemark 1. Alg. 1 can similarly be applied to mini-batch SGD. Let the batch-size be M , which reduces the covariance by M times and so η in the SME is replaced by η/M . However, at the same time estimating Σk from mini-batch gradient sample variances will underestimate Σ(xk) by a factor ofM . Thus the product ηΣk remains unchanged and Alg. 1 can be applied with no changes.\nRemark 2. The additional overheads in cSGD are from maintaining exponential averages and estimating ak, bk,Σk on the fly with the relevant formulas. These are O(d) operations and hence scalable. Our current rough implementation runs ∼ 40 − 60% slower per epoch than the plain SGD. This is expected to be improved by optimization, parallelization or updating quantities less frequently."
  }, {
    "heading": "4.1.4. PERFORMANCE ON BENCHMARKS",
    "text": "Let us test cSGD on common deep learning benchmarks. We consider three different models. M0: a fully connected neural network with one hidden layer and ReLU activations, trained on the MNIST dataset (LeCun et al., 1998); C0: a fully connected neural network with two hidden layers and Tanh activations, trained on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009); C1: a convolution network with four convolution layers and two fully connected layers also trained on CIFAR-10. Model details\nare found in SM. F.1. In Fig. 3, we compare the performance of cSGD with Adagrad (Duchi et al., 2011) and Adam (Kingma & Ba, 2015) optimizers. We illustrate in particular their sensitivity to different learning rate choices by performing a log-uniform random search over three orders of magnitude. We observe that cSGD is robust to different initial and maximum learning rates (provided the latter is big enough, e.g. we can take η = 1 for all experiments) and changing network structures, while obtaining similar performance to well-tuned versions of the other methods (see also Tab. 1). In particular, notice that the best learning rates found for Adagrad and Adam generally differ for different neural networks. On the other hand, many values can be used for cSGD with little performance loss. For brevity we only show the test accuracies, but the training accuracies have similar behavior (see SM. F.5)."
  }, {
    "heading": "4.2. Momentum Parameter",
    "text": "Another practical way of speeding up the plain SGD is to employ momentum updates - an idea dating back to deterministic optimization (Polyak, 1964; Nesterov, 1983; Qian, 1999). However, the stochastic version has important differences, especially in regimes where sampling noise dominates. Nevertheless, provided that the momentum parameter is well-tuned, the momentum SGD (MSGD) is very effective in speeding up convergence, particularly in early stages of training (Sutskever et al., 2013).\nSelecting an appropriate momentum parameter is important in practice. Typically, generic values (e.g. 0.9, 0.99) are suggested without fully elucidating their effect on the SGD dynamics. In this section, we use the SME framework to analyze the precise dynamics of MSGD and derive effective adaptive momentum parameter adjustment policies."
  }, {
    "heading": "4.2.1. SME FOR MSGD",
    "text": "The SGD with momentum can be written as the following coupled updates\nvk+1 = µvk − ηf ′γk(xk), xk+1 = xk + vk+1. (16)\nThe parameter µ is the momentum parameter taking values in the range 0 ≤ µ ≤ 1. Intuitively, the momentum term vk remembers past update directions and pushes along xk, which may otherwise slow down at e.g. narrow parts of the landscape. The corresponding SME is now a coupled SDE\ndVt = (−η−1(1− µ)Vt − f ′(Xt))dt+ (ηΣ(Xt)) 1 2 dWt,\ndXt = η −1Vtdt. (17)\nThis can be derived by comparing (16) with the Euler discretization scheme of (17) and matching moments. Details can be found in SM. D.3."
  }, {
    "heading": "4.2.2. THE EFFECT OF MOMENTUM",
    "text": "As in Sec. 4.1, we take the prototypical example f(x) = 12a(x − b)\n2 with Σ constant and study the effect of incorporating momentum updates. Define Mt = (Ef(Xt),EV 2t ,EVtf ′(Xt)) ∈ R3. By applying Itô formula to (17), we obtain the ODE system\nṀt = A(µ)Mt+B,\nA(µ) = ( 0 0 a/η 0 −2(1−µ)/η −2 −2 1/η −(1−µ)/η ) , B = ( 0 ηΣ 0 ) . (18)\nIf a < 0, A(µ) has a positive eigenvalue and hence Mt diverges exponentially. Since f is negative, its value must then decrease exponentially for all µ, and the descent rate is maximized at µ = 1. The more interesting case is when a > 0. Instead of solving (18), we observe that all eigenvalues of A(µ) have negative real parts as long as µ < 1. Therefore, Mt has an exponential decay dominated by |Rλ(µ)|, where R denotes real part and λ(µ) = − 1η [(1− µ)− √ (1− µ)2 − 4aη] is the eigenvalue with the least negative real part. Observe that the descent rate |Rλ(µ)| is maximized at\nµopt = max(1− 2 √ aη, 0) (19)\nand when µ > µopt, λ becomes complex. Also, from (18) we have Mt → M∞ = −A(µ)−1B =\n( ηΣ\n4(1−µ) η2Σ 2(1−µ) 0 ) ,\nprovided the steady state is stable. The role of momentum in this problem is now clear. To leading order in η we have λ(µ) ∼ −2a/(1 − µ) for µ ≤ µopt. Hence, any non-zero momentum will improve the initial convergence rate. In fact, the choice µopt is optimal and above it, oscillations set in because of a complex λ. At the same time, increasing momentum also causes increment in eventual fluctuations, since |M∞| = O((1 − µ)−1). In Fig. 4(a), we demonstrate the accuracy of the SME prediction (18) by comparing MSGD iterations. Armed with an intuitive understanding of the effect of momentum, we can now use optimal control to design policies to adapt the momentum parameter."
  }, {
    "heading": "4.2.3. OPTIMAL CONTROL OF THE MOMENTUM PARAMETER",
    "text": "For a < 0, we have discussed previously that µ = 1 maximizes the descent rate and fluctuations generally help de-\ncrease concave functions. Thus, the optimal control is always µ = 1. The non-trivial case is when a > 0. Due to its bi-linearity, directly controlling (18) leads to bangbang type solutions2 that are rarely feed-back laws (Pardalos & Yatsenko, 2010) and thus difficult to apply in practice. Instead, we notice that the descent rate is dominated byRλ(µ), and the leading order asymptotic fluctuations is ηΣ/(4(1− µ)), hence we may consider\nṁt = Rλ(µ)(mt −m∞(µ)) (20)\nwhere mt ∈ R and m∞(µ) = ηΣ/(4(1− µ)) is the leading order estimate of |M∞|. Equation (20) can be understood as the approximate evolution, in an averaged sense, of the magnitude of Mt. Fig. 4(b) shows that (20) is a reasonable approximation of the dynamics of MSGD. This allows us to pose the optimal control problem on the momentum parameter as\nmin µ:[0,T ]→[0,1] mT subject to (20),\nwith µ = µt. Solving this control problem yields the (approximate) feed-back policy (SM. E.4)\nµ∗t = { 1 a ≤ 0, min(µopt,max(0, 1− ηΣ4mt )) a > 0, (21)\nwith µopt given in (19). This says that when far from optimum (mt large), we set µ = µopt which maximizes average descent rate. When mt/ηΣ ∼ √ aη, fluctuations set in and we lower µ.\nAs in Sec. 4.1.3, we turn the control policy above into a generally applicable algorithm by performing local diagonal-quadratic approximations and estimating the relevant quantities on the fly. The resulting algorithm is mostly identical to Alg. 1 except we now use (21) to update µk,(i) and SGD updates are replaced with MSGD updates (see S.M. F.4 for the full algorithm). We refer to this algorithm as the controlled momentum SGD (cMSGD)."
  }, {
    "heading": "4.2.4. PERFORMANCE ON BENCHMARKS",
    "text": "We apply cMSGD to the same three set-ups in Sec. 4.1.4, and compare its performance to the plain Momentum SGD with fixed momentum parameters (MSGD) and the annealing schedule suggested in (Sutskever et al., 2013), with µk = min(1 − 2−1−log2(bk/250c+1), µmax) (MSGD-A). In Fig. 5, we perform a log-uniform search over the hyperparameters µ0, µ and µmax. We see that cMSGD achieves superior performance to MSGD and MSGD-A (see Tab. 1),\n2Bang-bang solutions are control solutions lying on the boundary of the control set and abruptly jumps among the boundary values. For example, in this case it jumps between µ = 0 and µ = 1 repeatedly.\nespecially when the latter has badly tuned µ, µmax. Moreover, it is insensitive to the choice of initial µ0. Just like cSGD, this holds across changing network structures. Further, cMSGD also adapts to other hyper-parameter variations. In Fig. 6, we take tuned µ, µmax (and any µ0) and vary the learning rate η. We observe that cMSGD adapts to the new learning rates whereas the performance of MSGD and MSGD-A deteriorates and µ, µmax must be re-tuned to obtain reasonable accuracy. In fact, it is often the case that MSGD and MSGD-A diverge when η is large, whereas cMSGD remains stable."
  }, {
    "heading": "5. Related Work",
    "text": "Classical bound-type convergence results for SGD and variants include Moulines (2011); Shamir & Zhang (2013);\nBach & Moulines (2013); Needell et al. (2014); Xiao & Zhang (2014); Shalev-Shwartz & Zhang (2014). Our approach differs in that we obtain precise, albeit only distributional, descriptions of the SGD dynamics that hold in non-convex situations.\nIn the vein of continuous approximation to stochastic algorithms, a related body of work is stochastic approximation theory (Kushner & Yin, 2003; Ljung et al., 2012), which establish ODEs as almost sure limits of trajectories of stochastic algorithms. In contrast, we obtain SDEs that are weak limits that approximate not individual sample paths, but their distributions. Other deterministic continuous time approximation methods include Su et al. (2014); Krichene et al. (2015); Wibisono et al. (2016).\nRelated work in SDE approximations of the SGD are Mandt et al. (2015; 2016), where the authors derived the first order SME heuristically. In contrast, we establish a rigorous statement for this type of approximations (Thm. 1). Moreover, we use asymptotic analysis and control theory to translate understanding into practical algo-\nrithms. Outside of the machine learning literature, similar modified equation methods also appear in numerical analysis of SDEs (Zygalakis, 2011) and quantifying uncertainties in ODEs (Conrad et al., 2015).\nThe second half of our work deals with practical problems of adaptive selection of the learning rate and momentum parameter. There is abundant literature on learning rate adjustments, including annealing schedules (Robbins & Monro, 1951; Moulines, 2011; Xu, 2011; Shamir & Zhang, 2013), adaptive per-element adjustments (Duchi et al., 2011; Zeiler, 2012; Tieleman & Hinton, 2012; Kingma & Ba, 2015) and meta-learning (Andrychowicz et al., 2016). Our approach differs in that optimal control theory provides a natural, non-black-box framework for developing dynamic feed-back adjustments, allowing us to obtain adaptive algorithms that are truly robust to changing model settings. Our learning rate adjustment policy is similar to Schaul et al. (2013); Schaul & LeCun (2013) based on one-step optimization, although we arrive at it from control theory. Our method may also be easier to implement because it does not require estimating diagonal Hessians via back-propagation. There is less literature on momentum parameter selection. A heuristic annealing schedule (referred to as MSGD-A earlier) is suggested in Sutskever et al. (2013), based on the original work of Nesterov (1983). The choice of momentum parameter in deterministic problems is discussed in Qian (1999); Nesterov (2013). To the best of our knowledge, a systematic stochastic treatment of adaptive momentum parameter selection for MSGD has not be considered before."
  }, {
    "heading": "6. Conclusion and Outlook",
    "text": "Our main contribution is twofold. First, we propose the SME as a unified framework for quantifying the dynamics of SGD and its variants, beyond the classical convex regime. Tools from stochastic calculus and asymptotic analysis provide precise dynamical description of these algorithms, which help us understand important phenomena, such as descent-fluctuation transitions and the nature of acceleration schemes. Second, we use control theory as a natural framework to derive adaptive adjustment policies for the learning rate and momentum parameter. This translates to robust algorithms that requires little tuning across multiple datasets and model choices.\nAn interesting direction of future work is extending the SME framework to develop adaptive adjustment schemes for other hyper-parameters in SGD variants, such as Polyak-Ruppert Averaging (Polyak & Juditsky, 1992), SVRG (Johnson & Zhang, 2013) and elastic averaging SGD (Zhang et al., 2015). More generally, the SME framework may be a promising methodology for the analysis and design of stochastic gradient algorithms and beyond."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank the anonymous reviewers for their constructive comments. We are also grateful for the many discussions with Dr Sixin Zhang. This work is supported in part by Major Program of NNSFC under grant 91130005, DOE DE-SC0009248, and ONR N00014-13-1-0338."
  }],
  "year": 2017,
  "references": [{
    "title": "Learning to learn by gradient descent by gradient descent",
    "authors": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)",
    "authors": ["Bach", "Francis", "Moulines", "Eric"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Dynamic programming and Lagrange multipliers",
    "authors": ["Bellman", "Richard"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 1956
  }, {
    "title": "Probability measures for numerical solutions of differential equations",
    "authors": ["Conrad", "Patrick R", "Girolami", "Mark", "Särkkä", "Simo", "Stuart", "Andrew", "Zygalakis", "Konstantinos"],
    "venue": "arXiv preprint arXiv:1506.04592,",
    "year": 2015
  }, {
    "title": "The stability properties of a coupled pair of non-linear partial difference equations",
    "authors": ["Daly", "Bart J"],
    "venue": "Mathematics of Computation,",
    "year": 1963
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Random perturbations of dynamical systems, volume 260",
    "authors": ["Freidlin", "Mark I", "Szücs", "Joseph", "Wentzell", "Alexander D"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Heuristic stability theory for finite-difference equations",
    "authors": ["Hirt", "CW"],
    "venue": "Journal of Computational Physics,",
    "year": 1968
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "ICLR,",
    "year": 2015
  }, {
    "title": "Numerical Solution of Stochastic Differential Equations",
    "authors": ["P.E. Kloeden", "E. Platen"],
    "year": 2011
  }, {
    "title": "Accelerated mirror descent in continuous and discrete time",
    "authors": ["Krichene", "Walid", "Bayen", "Alexandre", "Bartlett", "Peter L"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2015
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"],
    "year": 2009
  }, {
    "title": "Stochastic approximation and recursive algorithms and applications, volume 35",
    "authors": ["Kushner", "Harold", "Yin", "G George"],
    "venue": "Springer Science & Business Media,",
    "year": 2003
  }, {
    "title": "The mnist dataset of handwritten digits",
    "authors": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"],
    "venue": "URL http://yann. lecun. com/exdb/mnist,",
    "year": 1998
  }, {
    "title": "Stochastic approximation and optimization of random systems, volume 17",
    "authors": ["Ljung", "Lennart", "Pflug", "Georg Ch", "Walk", "Harro"],
    "year": 2012
  }, {
    "title": "Continuous-time limit of stochastic gradient descent revisited",
    "authors": ["Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M"],
    "venue": "In NIPS-2015,",
    "year": 2015
  }, {
    "title": "A variational analysis of stochastic gradient algorithms",
    "authors": ["Mandt", "Stephan", "Hoffman", "Matthew D", "Blei", "David M"],
    "venue": "arXiv preprint arXiv:1602.02666,",
    "year": 2016
  }, {
    "title": "Numerical integration of stochastic differential equations, volume 313",
    "authors": ["Milstein", "GN"],
    "venue": "Springer Science & Business Media,",
    "year": 1995
  }, {
    "title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
    "authors": ["Moulines", "Eric", "Francis R"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Stochastic gradient descent, weighted sampling, and the randomized algorithm",
    "authors": ["Needell", "Deanna", "Ward", "Rachel", "Srebro", "Nati"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "A method of solving a convex programming problem with convergence rate O(1/k2)",
    "authors": ["Nesterov", "Yurii"],
    "venue": "In Soviet Mathematics Doklady,",
    "year": 1983
  }, {
    "title": "Introductory lectures on convex optimization: A basic course, volume 87",
    "authors": ["Nesterov", "Yurii"],
    "venue": "Springer Science & Business Media,",
    "year": 2013
  }, {
    "title": "Difference methods and the equations of hydrodynamics",
    "authors": ["WF Noh", "Protter", "MH"],
    "venue": "Technical report,",
    "year": 1960
  }, {
    "title": "Optimization and Control of Bilinear Systems: Theory, Algorithms, and Applications, volume 11",
    "authors": ["Pardalos", "Panos M", "Yatsenko", "Vitaliy A"],
    "venue": "Springer Science & Business Media,",
    "year": 2010
  }, {
    "title": "Some methods of speeding up the convergence of iteration methods",
    "authors": ["Polyak", "Boris T"],
    "venue": "USSR Computational Mathematics and Mathematical Physics,",
    "year": 1964
  }, {
    "title": "Acceleration of stochastic approximation by averaging",
    "authors": ["Polyak", "Boris T", "Juditsky", "Anatoli B"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 1992
  }, {
    "title": "On the momentum term in gradient descent learning algorithms",
    "authors": ["Qian", "Ning"],
    "venue": "Neural networks,",
    "year": 1999
  }, {
    "title": "A stochastic approximation method",
    "authors": ["Robbins", "Herbert", "Monro", "Sutton"],
    "venue": "The annals of mathematical statistics,",
    "year": 1951
  }, {
    "title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",
    "authors": ["Schaul", "Tom", "LeCun", "Yann"],
    "venue": "arXiv preprint arXiv:1301.3764,",
    "year": 2013
  }, {
    "title": "No more pesky learning rates",
    "authors": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"],
    "venue": "In ICML (3),",
    "year": 2013
  }, {
    "title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "Mathematical Programming,",
    "year": 2014
  }, {
    "title": "A differential equation for modeling Nesterovs accelerated gradient method: theory and insights",
    "authors": ["Su", "Weijie", "Boyd", "Stephen", "Candes", "Emmanuel"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "On the importance of initialization and momentum in deep learning",
    "authors": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"],
    "venue": "In Proceedings of the 30th international conference on machine learning",
    "year": 2013
  }, {
    "title": "Lecture 6.5 - RMSProp",
    "authors": ["T. Tieleman", "G. Hinton"],
    "venue": "Technical report,",
    "year": 2012
  }, {
    "title": "On the theory of the Brownian motion",
    "authors": ["Uhlenbeck", "George E", "Ornstein", "Leonard S"],
    "venue": "Physical review,",
    "year": 1930
  }, {
    "title": "The modified equation approach to the stability and accuracy analysis of finitedifference methods",
    "authors": ["RF Warming", "Hyett", "BJ"],
    "venue": "Journal of computational physics,",
    "year": 1974
  }, {
    "title": "A variational perspective on accelerated methods in optimization",
    "authors": ["Wibisono", "Andre", "Wilson", "Ashia C", "Jordan", "Michael I"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "A proximal stochastic gradient method with progressive variance reduction",
    "authors": ["Xiao", "Lin", "Zhang", "Tong"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent",
    "authors": ["Xu", "Wei"],
    "venue": "arXiv preprint arXiv:1107.2490,",
    "year": 2011
  }, {
    "title": "ADADELTA: an adaptive learning rate method",
    "authors": ["Zeiler", "Matthew D"],
    "venue": "arXiv preprint arXiv:1212.5701,",
    "year": 2012
  }, {
    "title": "Deep learning with elastic averaging SGD",
    "authors": ["Zhang", "Sixin", "Choromanska", "Anna E", "LeCun", "Yann"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "On the existence and the applications of modified equations for stochastic differential equations",
    "authors": ["Zygalakis", "KC"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2011
  }],
  "id": "SP:2f8eb618406e5ae3fe73b9b4ffe2b346107febaa",
  "authors": [{
    "name": "Qianxiao Li",
    "affiliations": []
  }, {
    "name": "Cheng Tai",
    "affiliations": []
  }],
  "abstractText": "We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.",
  "title": "Stochastic Modified Equations  and Adaptive Stochastic Gradient Algorithms"
}