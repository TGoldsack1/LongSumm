{
  "sections": [{
    "heading": "1 Introduction",
    "text": "In this paper, we focus on the task of Natural Language Inference (NLI), which is known as a significant yet challenging task for natural language understanding. In this task, we are given two sentences which are respectively called premise and hypothesis. The goal is to determine whether the logical relationship between them is entailment, neutral, or contradiction.\nRecently, performance on NLI(Chen et al., 2017b; Gong et al., 2018; Chen et al., 2017c)\n∗corresponding author\nhas been significantly boosted since the release of some high quality large-scale benchmark datasets such as SNLI(Bowman et al., 2015) and MultiNLI(Williams et al., 2017). Table 1 shows some examples in SNLI. Most state-of-the-art works focus on the interaction architectures between the premise and the hypothesis, while they rarely concerned the discourse relations of the sentences, which is a core issue in natural language understanding.\nPeople usually use some certain set of words to express the discourse relation between two sentences1. These words, such as “but” or “and”, are denoted as discourse markers. These discourse markers have deep connections with the intrinsic relations of two sentences and intuitively correspond to the intent of NLI, such as “but” to “contradiction”, “so” to “entailment”, etc.\nVery few NLI works utilize this information revealed by discourse markers. Nie et al. (2017) proposed to use discourse markers to help rep-\n1Here sentences mean either the whole sentences or the main clauses of a compound sentence.\nar X\niv :1\n90 7.\n09 69\n2v 1\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n9\nresent the meanings of the sentences. However, they represent each sentence by a single vector and directly concatenate them to predict the answer, which is too simple and not ideal for the largescale datasets.\nIn this paper, we propose a Discourse Marker Augmented Network for natural language inference, where we transfer the knowledge from the existing supervised task: Discourse Marker Prediction (DMP)(Nie et al., 2017), to an integrated NLI model. We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network. Moreover, because our NLI datasets are manually annotated, each example from the datasets might get several different labels from the annotators although they will finally come to a consensus and also provide a certain label. In consideration of that different confidence level of the final labels should be discriminated, we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model. The contributions of this paper can be summarized as follows.\n• Unlike previous studies, we solve the task of the natural language inference via transferring knowledge from another supervised task. We propose the Discourse Marker Augmented Network to combine the learned encoder of the sentences with the integrated NLI model.\n• According to the property of the datasets, we incorporate reinforcement learning to optimize a new objective function to make full use of the labels’ information.\n• We conduct extensive experiments on two large-scale datasets to show that our method achieves better performance than other stateof-the-art solutions to the problem."
  }, {
    "heading": "2 Task Description",
    "text": ""
  }, {
    "heading": "2.1 Natural Language Inference (NLI)",
    "text": "In the natural language inference tasks, we are given a pair of sentences (P,H), which respectively means the premise and hypothesis. Our goal is to judge whether their logical relationship between their meanings by picking a label from a small set: entailment (The hypothesis is definitely a true description of the premise), neutral\n(The hypothesis might be a true description of the premise), and contradiction (The hypothesis is definitely a false description of the premise)."
  }, {
    "heading": "2.2 Discourse Marker Prediction (DMP)",
    "text": "For DMP, we are given a pair of sentences (S1, S2), which is originally the first half and second half of a complete sentence. The model must predict which discourse marker was used by the author to link the two ideas from a set of candidates."
  }, {
    "heading": "3 Sentence Encoder Model",
    "text": "Following (Nie et al., 2017; Kiros et al., 2015), we use BookCorpus(Zhu et al., 2015) as our training data for discourse marker prediction, which is a dataset of text from unpublished novels, and it is large enough to avoid bias towards any particular domain or application. After preprocessing, we obtain a dataset with the form (S1, S2,m), which means the first half sentence, the last half sentence, and the discourse marker that connected them in the original text. Our goal is to predict them given S1 and S2.\nWe first use Glove(Pennington et al., 2014) to transform {St}2t=1 into vectors word by word and subsequently input them to a bi-directional LSTM:\n−→ hit = −−−−→ LSTM(Glove(Sit)), i = 1, ..., |St| ←− hit = ←−−−− LSTM(Glove(Sit)), i = |St|, ..., 1 (1)\nwhere Glove(w) is the embedding vector of the word w from the Glove lookup table, |St| is the length of the sentence St. We apply max pooling on the concatenation of the hidden states from both directions, which provides regularization and shorter back-propagation paths(Collobert and Weston, 2008), to extract the features of the whole sequences of vectors:\n−→rt = Maxdim([ −→ h1t ; −→ h2t ; ...; −−→ h |St| t ]) ←−rt = Maxdim([ ←− h1t ; ←− h2t ; ...; ←−− h |St| t ])\n(2)\nwhere Maxdim means that the max pooling is performed across each dimension of the concatenated vectors, [; ] denotes concatenation. Moreover, we combine the last hidden state from both directions and the results of max pooling to represent our sentences:\nrt = [ −→rt ;←−rt ; −−→ h |St| t ; ←− h1t ]\n(3)\nwhere rt is the representation vector of the sentence St. To predict the discource marker between S1 and S2, we combine the representations of them with some linear operation:\nr = [r1; r2; r1 + r2; r1 r2] (4)\nwhere is elementwise product. Finally we project r to a vector of label size (the total number of discourse markers in the dataset) and use softmax function to normalize the probability distribution."
  }, {
    "heading": "4 Discourse Marker Augmented Network",
    "text": "As presented in Figure 1, we show how our Discourse Marker Augmented Network incorporates the learned encoder into the NLI model."
  }, {
    "heading": "4.1 Encoding Layer",
    "text": "We denote the premise as P and the hypothesis as H . To encode the words, we use the concatenation of following parts: Word Embedding: Similar to the previous section, we map each word to a vector space by using pre-trained word vectors GloVe. Character Embedding: We apply Convolutional Neural Networks (CNN) over the characters of each word. This approach is proved to be helpful in handling out-of-vocab (OOV) words(Yang et al., 2017). POS and NER tags: We use the part-of-speech (POS) tags and named-entity recognition (NER)\ntags to get syntactic information and entity label of the words. Following (Pan et al., 2017b), we apply the skip-gram model(Mikolov et al., 2013) to train two new lookup tables of POS tags and NER tags respectively. Each word can get its own POS embedding and NER embedding by these lookup tables. This approach represents much better geometrical features than common used one-hot vectors. Exact Match: Inspired by the machine comprehension tasks(Chen et al., 2017a), we want to know whether every word in P is in H (and H in P ). We use three binary features to indicate whether the word can be exactly matched to any question word, which respectively means original form, lowercase and lemma form.\nFor encoding, we pass all sequences of vectors into a bi-directional LSTM and obtain:\npi = BiLSTM(frep(Pi),pi−1), i = 1, ..., n\nuj = BiLSTM(frep(Hj),uj−1), j = 1, ...,m (5) where frep(x) = [Glove(x); Char(x); POS(x); NER(x); EM(x)] is the concatenation of the embedding vectors and the feature vectors of the word x, n = |P |, m = |H|."
  }, {
    "heading": "4.2 Interaction Layer",
    "text": "In this section, we feed the results of the encoding layer and the learned sentence encoder into the attention mechanism, which is responsible for linking and fusing information from the premise and the hypothesis words.\nWe first obtain a similarity matrix A ∈ Rn×m between the premise and hypothesis by\nAij = v > 1 [pi;uj ;pi ◦ uj ; rp; rh] (6)\nwhere v1 is the trainable parameter, rp and rh are sentences representations from the equation (3) learned in the Section 3, which denote the premise and hypothesis respectively. In addition to previous popular similarity matrix, we incorporate the relevance of each word of P (H) to the whole sentence ofH(P ). Now we use A to obtain the attentions and the attended vectors in both directions.\nTo signify the attention of the i-th word of P to every word of H , we use the weighted sum of uj by Ai::\nũi = ∑ j Aij · uj (7)\nwhere ũi is the attention vector of the i-th word of P for the entire H . In the same way, the p̃j is obtained via:\np̃j = ∑ i Aij · pi (8)\nTo model the local inference between aligned word pairs, we integrate the attention vectors with the representation vectors via:\np̂i = f([pi; ũi;pi − ũi;pi ũi]) ûj = f([uj ; p̃j ;uj − p̃j ;uj p̃j ])\n(9)\nwhere f is a 1-layer feed-forward neural network with the ReLU activation function, p̂i and ûj are local inference vectors. Inspired by (Seo et al., 2016) and (Chen et al., 2017b), we use a modeling layer to capture the interaction between the premise and the hypothesis. Specifically, we use bi-directional LSTMs as building blocks:\npMi = BiLSTM(p̂i,p M i−1) uMj = BiLSTM(ûj ,u M j−1)\n(10)\nHere, pMi and u M j are the modeling vectors which contain the crucial information and relationship among the sentences.\nWe compute the representation of the whole sentence by the weighted average of each word:\npM = ∑ i exp(v>2 p M i )∑ i′ exp(v > 2 p M i′ ) pMi\nuM = ∑ j exp(v>3 u M j )∑ j′ exp(v > 3 u M j′ ) uMj\n(11)\nwhere v2,v3 are trainable vectors. We don’t share these parameter vectors in this seemingly parallel strucuture because there is some subtle difference between the premise and hypothesis, which will be discussed later in Section 5."
  }, {
    "heading": "4.3 Output Layer",
    "text": "The NLI task requires the model to predict the logical relation from the given set: entailment, neutral or contradiction. We obtain the probability distribution by a linear function with softmax function:\nd = softmax(W[pM ;uM ;pM uM ; rp rh]) (12)\nwhere W is a trainable parameter. We combine the representations of the sentences computed above with the representations learned from DMP to obtain the final prediction."
  }, {
    "heading": "4.4 Training",
    "text": "As shown in Table 2, many examples from our datasets are labeled by several people, and the choices of the annotators are not always consistent. For instance, when the label number is 3 in SNLI, “total=0” means that no examples have 3 annotators (maybe more or less); “correct=8748” means that there are 8748 examples whose number of correct labels is 3 (the number of annotators maybe 4 or 5, but some provided wrong labels). Although all the labels for each example will be unified to a final (correct) label, diversity of the labels for a single example indicates the low confidence of the result, which is not ideal to only use the final label to optimize the model.\nWe propose a new objective function that combines both the log probabilities of the ground-truth label and a reward defined by the property of the datasets for the reinforcement learning. The most widely used objective function for the natural language inference is to minimize the negative log cross-entropy loss:\nJCE(Θ) = − 1\nN N∑ k log(dkl ) (13)\nwhere Θ are all the parameters to optimize, N is the number of examples in the dataset, dl is the probability of the ground-truth label l.\nHowever, directly using the final label to train the model might be difficult in some situations, where the example is confusing and the labels from the annotators are different. For instance, consider an example from the SNLI dataset:\n• P : “A smiling costumed woman is holding an umbrella.”\n• H: “A happy woman in a fairy costume holds an umbrella.”\nThe final label is neutral, but the original labels from the five annotators are neural, neural, entailment, contradiction, neural, in which case the relation between “smiling” and “happy” might be under different comprehension. The final label’s confidence of this example is obviously lower than an example that all of its labels are the same. To simulate the thought of human being more closely, in this paper, we tackle this problem by using the REINFORCE algorithm(Williams, 1992) to minimize the negative expected reward, which is defined as:\nJRL(Θ) = −El∼π(l|P,H)[R(l, {l∗})] (14)\nwhere π(l|P,H) is the previous action policy that predicts the label given P and H , {l∗} is the set of annotated labels, and\nR(l, {l∗}) = number of l in {l ∗}\n|{l∗}| (15)\nis the reward function defined to measure the distance to all the ideas of the annotators.\nTo avoid of overwriting its earlier results and further stabilize training, we use a linear function to integrate the above two objective functions:\nJ(Θ) = λJCE(Θ) + (1− λ)JRL(Θ) (16)\nwhere λ is a tunable hyperparameter."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Datasets",
    "text": "BookCorpus: We use the dataset from BookCorpus(Zhu et al., 2015) to pre-train our sentence encoder model. We preprocessed and collected discourse markers from BookCorpus as (Nie et al., 2017). We finally curated a dataset of 6527128 pairs of sentences for 8 discourse markers, whose statistics are shown in Table 3. SNLI: Stanford Natural Language Inference(Bowman et al., 2015) is a collection of more than 570k human annotated sentence pairs labeled for entailment, contradiction, and semantic independence. SNLI is two orders of magnitude larger than all other resources of its type. The premise data is extracted from the captions of the Flickr30k corpus(Young et al., 2014), the hypothesis data and the labels are manually annotated. The original SNLI corpus contains also the other category, which includes the sentence pairs lacking consensus among multiple human annotators. We remove this category and use the same split as in (Bowman et al., 2015) and other previous work. MultiNLI: Multi-Genre Natural Language Inference(Williams et al., 2017) is another large-scale corpus for the task of NLI. MultiNLI has 433k sentences pairs and is in the same format as SNLI, but it includes a more diverse range of text, as well as an auxiliary test set for cross-genre transfer evaluation. Half of these selected genres appear in training set while the rest are not, creating in-domain (matched) and cross-domain (mismatched) development/test sets."
  }, {
    "heading": "5.2 Implementation Details",
    "text": "We use the Stanford CoreNLP toolkit(Manning et al., 2014) to tokenize the words and generate POS and NER tags. The word embeddings are initialized by 300d Glove(Pennington et al., 2014), the dimensions of POS and NER embeddings are 30 and 10. The dataset we use to train the embeddings of POS tags and NER tags are the training set given by SNLI. We apply Tensorflow r1.3 as our neural network framework. We set the hidden size as 300 for all the LSTM layers and apply dropout(Srivastava et al., 2014) between layers with an initial ratio of 0.9, the decay rate as 0.97 for every 5000 step. We use the AdaDelta for optimization as described in (Zeiler, 2012) with ρ as 0.95 and as 1e-8. We set our batch size as 36 and the initial learning rate as 0.6. The parameter λ in the objective function is set to be 0.2. For DMP task, we use stochastic gradient descent with initial learning rate as 0.1, and we anneal by half each time the validation accuracy is lower than the previous epoch. The number of epochs is set to be 10, and the feedforward dropout rate\nis 0.2. The learned encoder in subsequent NLI task is trainable. Code is available at https: //github.com/ZJULearning/DMP."
  }, {
    "heading": "5.3 Results",
    "text": "In table 4, we compare our model to other competitive published models on SNLI and MultiNLI. As we can see, our method Discourse Marker Augmented Network (DMAN) clearly outperforms all the baselines and achieves the state-of-the-art results on both datasets.\nThe methods in the top part of the table are sentence encoding based models. Bowman et al. (2016) proposed a simple baseline that uses LSTM to encode the whole sentences and feed them into a MLP classifier to predict the final inference relationship, they achieve an accuracy of 80.6% on SNLI. Nie and Bansal (2017) test their model on both SNLI and MiltiNLI, and achieves competitive results.\nIn the medium part, we show the results of other neural network models. Obviously, the performance of most of the integrated methods are better than the sentence encoding based mod-\nels above. Both DIIN(Gong et al., 2018) and CAFE(Tay et al., 2017) exceed other methods by more than 4% on MultiNLI dataset. However, our DMAN achieves 88.8% on SNLI, 78.9% on matched MultiNLI and 78.2% on mismatched MultiNLI, which are all best results among the baselines.\nWe present the ensemble results on both datasets in the bottom part of the table 4. We build an ensemble model which consists of 10 single models with the same architecture but initialized with different parameters. The performance of our model achieves 89.6% on SNLI, 80.3% on matched MultiNLI and 79.4% on mismatched MultiNLI, which are all state-of-the-art results."
  }, {
    "heading": "5.4 Ablation Analysis",
    "text": "As shown in Table 5, we conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model. Firstly we only use the results of the sentence encoder model to predict the answer, in other words, we represent each sentence by a single vector and use dot product with a linear function to do the classification. The result is obviously not satisfactory, which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large-scale datasets. We then remove the sentence encoder model, which means we don’t use the knowledge transferred from the DMP task and thus the representations rp and rh are set to be zero vectors in the equation (6) and the equation (12). We observe that the performance drops significantly to 87.24%, which is nearly 1.5% to our DMAN model, which indicates that the discourse markers have deep connections with the logical rela-\ntions between two sentences they links. When we remove the character-level embedding and the POS and NER features, the performance drops a lot. We conjecture that those feature tags help the model represent the words as a whole while the char-level embedding can better handle the outof-vocab (OOV) or rare words. The exact match feature also demonstrates its effectiveness in the ablation result. Finally, we ablate the reinforcement learning part, in other words, we only use the original loss function to optimize the model (set λ = 1). The result drops about 0.5%, which proves that it is helpful to utilize all the information from the annotators."
  }, {
    "heading": "5.5 Semantic Analysis",
    "text": "In Figure 2, we show the performance on the three relation labels when the model is pre-trained on different discourse markers sets. In other words, we removed discourse marker from the original set each time and use the rest 7 discourse markers to pre-train the sentence encoder in the DMP task and then train the DMAN. As we can see, there is a sharp decline of accuracy when removing “but”, “because” and “although”. We can intuitively speculate that “but” and “although” have direct connections with the contradiction label (which drops most significantly) while “because” has some links with the entailment label. We observe that some discourse markers such as “if” or “before” contribute much less than other words\nwhich have strong logical hints, although they actually improve the performance of the model. Compared to the other two categories, the “contradiction” label examples seem to benefit the most from the pre-trained sentence encoder."
  }, {
    "heading": "5.6 Visualization",
    "text": "In Figure 3, we also provide a visualized analysis of the hidden representation from similarity matrix A (computed in the equation (6)) in the situations that whether we use the discourse markers or not. We pick a sentence pair whose premise is “3 young man in hoods standing in the middle of a quiet street facing the camera.” and hypothesis is “Three people sit by a busy street bareheaded.” We observe that the values are highly correlated among the synonyms like “people” with “man”, “three” with “3” in both situations. However, words that might have contradictory meanings like “hoods” with “bareheaded”, “quiet” with “busy” perform worse without the discourse markers augmentation, which conforms to the conclusion that the “contradiction” label examples benefit a lot which is observed in the Section 5.5."
  }, {
    "heading": "6 Related Work",
    "text": ""
  }, {
    "heading": "6.1 Discourse Marker Applications",
    "text": "This work is inspired most directly by the DisSent model and Discourse Prediction Task of Nie et al.\n(2017), which introduce the use of the discourse markers information for the pretraining of sentence encoders. They follow (Kiros et al., 2015) to collect a large sentence pairs corpus from BookCorpus(Zhu et al., 2015) and propose a sentence representation based on that. They also apply their pre-trained sentence encoder to a series of natural language understanding tasks such as sentiment analysis, question-type, entailment, and relatedness. However, all those datasets are provided by Conneau et al. (2017) for evaluating sentence embeddings and are almost all small-scale and are not able to support more complex neural network. Moreover, they represent each sentence by a single vector and directly combine them to predict the answer, which is not able to interact among the words level.\nIn closely related work, Jernite et al. (2017) propose a model that also leverage discourse relations. However, they manually group the discourse markers into several categories based on human knowledge and predict the category instead of the explicit discourse marker phrase. However, the size of their dataset is much smaller than that in (Nie et al., 2017), and sometimes there has been disagreement among annotators about what exactly is the correct categorization of discourse relations(Hobbs, 1990).\nUnlike previous works, we insert the sentence encoder into an integrated network to augment the semantic representation for NLI tasks rather than directly combining the sentence embeddings to predict the relations."
  }, {
    "heading": "6.2 Natural Language Inference",
    "text": "Earlier research on the natural language inference was based on small-scale datasets(Marelli et al., 2014), which relied on traditional methods such as shallow methods(Glickman et al., 2005), natural logic methods(MacCartney and Manning, 2007), etc. These datasets are either not large enough to support complex deep neural network models or too easy to challenge natural language.\nLarge and complicated networks have been successful in many natural language processing tasks(Zhu et al., 2017; Chen et al., 2017e; Pan et al., 2017a). Recently, Bowman et al. (2015) released Stanford Natural language Inference (SNLI) dataset, which is a high-quality and large-scale benchmark, thus inspired many significant works(Bowman et al., 2016; Mou et al., 2016;\nVendrov et al., 2016; Conneau et al., 2017; Wang et al., 2017; Gong et al., 2018; McCann et al., 2017; Chen et al., 2017b; Choi et al., 2017; Tay et al., 2017). Most of them focus on the improvement of the interaction architectures and obtain competitive results, while transfer learning from external knowledge is popular as well. Vendrov et al. (2016) incorpated Skipthought(Kiros et al., 2015), which is an unsupervised sequence model that has been proven to generate useful sentence embedding. McCann et al. (2017) proposed to transfer the pre-trained encoder from the neural machine translation (NMT) to the NLI tasks.\nOur method combines a pre-trained sentence encoder from the DMP task with an integrated NLI model to compose a novel framework. Furthermore, unlike previous studies, we make full use of the labels provided by the annotators and employ policy gradient to optimize a new objective function in order to simulate the thought of human being."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we propose Discourse Marker Augmented Network for the task of the natural language inference. We transfer the knowledge learned from the discourse marker prediction task to the NLI task to augment the semantic representation of the model. Moreover, we take the various views of the annotators into consideration and employ reinforcement learning to help optimize the model. The experimental evaluation shows that our model achieves the state-of-the-art results on SNLI and MultiNLI datasets. Future works involve the choice of discourse markers and some other transfer learning sources."
  }, {
    "heading": "8 Acknowledgements",
    "text": "This work was supported in part by the National Nature Science Foundation of China (Grant Nos: 61751307), in part by the grant ZJU Research 083650 of the ZJUI Research Program from Zhejiang University and in part by the National Youth Top-notch Talent Support Program. The experiments are supported by Chengwei Yao in the Experiment Center of the College of Computer Science and Technology, Zhejiang university."
  }],
  "year": 2019,
  "references": [{
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    "year": 2015
  }, {
    "title": "A fast unified model for parsing and sentence understanding",
    "authors": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for",
    "year": 2016
  }, {
    "title": "Reading wikipedia to answer opendomain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "Meeting of the Association for Computational Linguistics (ACL), pages 1870– 1879.",
    "year": 2017
  }, {
    "title": "Natural language inference with external knowledge",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Diana Inkpen."],
    "venue": "arXiv preprint arXiv:1711.04289.",
    "year": 2017
  }, {
    "title": "Enhanced lstm for natural language inference",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), volume 1, pages 1657–",
    "year": 2017
  }, {
    "title": "Recurrent neural network-based sentence encoder with gated attention for natural language inference",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."],
    "venue": "Proceedings of the 2nd Workshop on Evaluating Vector Space",
    "year": 2017
  }, {
    "title": "User personalized satisfaction prediction via multiple instance deep learning",
    "authors": ["Zheqian Chen", "Ben Gao", "Huimin Zhang", "Zhou Zhao", "Haifeng Liu", "Deng Cai."],
    "venue": "International Conference on World Wide Web (WWW), pages 907–915.",
    "year": 2017
  }, {
    "title": "Learning to compose task-specific tree structures",
    "authors": ["Jihun Choi", "Kang Min Yoo", "Sang goo Lee."],
    "venue": "The Association for the Advancement of Artificial Intelligence (AAAI).",
    "year": 2017
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 25th international conference on Machine learning (ICML), pages 160–167. ACM.",
    "year": 2008
  }, {
    "title": "Supervised learning of universal sentence representations from natural language inference data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes"],
    "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natu-",
    "year": 2017
  }, {
    "title": "Web based probabilistic textual entailment",
    "authors": ["Oren Glickman", "Ido Dagan", "Moshe Koppel"],
    "year": 2005
  }, {
    "title": "Natural language inference over interaction space",
    "authors": ["Yichen Gong", "Heng Luo", "Jian Zhang."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2018
  }, {
    "title": "Literature and cognition",
    "authors": ["Jerry R Hobbs."],
    "venue": "21. Center for the Study of Language (CSLI).",
    "year": 1990
  }, {
    "title": "Discourse-based objectives for fast unsupervised sentence representation learning",
    "authors": ["Yacine Jernite", "Samuel R Bowman", "David Sontag."],
    "venue": "arXiv preprint arXiv:1705.00557.",
    "year": 2017
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Advances in neural information processing systems (NIPS), pages 3294–3302.",
    "year": 2015
  }, {
    "title": "Natural logic for textual inference",
    "authors": ["Bill MacCartney", "Christopher D Manning."],
    "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 193–200. Association for Computational Linguistics.",
    "year": 2007
  }, {
    "title": "The stanford corenlp natural language processing toolkit",
    "authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."],
    "venue": "Proceedings of 52nd annual meeting of the association for computational lin-",
    "year": 2014
  }, {
    "title": "A sick cure for the evaluation of compositional distributional semantic models",
    "authors": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"],
    "venue": "In LREC,",
    "year": 2014
  }, {
    "title": "Learned in translation: Contextualized word vectors",
    "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 6297– 6308.",
    "year": 2017
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems (NIPS), pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Natural language inference by tree-based convolution and heuristic matching",
    "authors": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
    "year": 2016
  }, {
    "title": "Dissent: Sentence representation learning from explicit discourse relations",
    "authors": ["Allen Nie", "Erin D Bennett", "Noah D Goodman."],
    "venue": "arXiv preprint arXiv:1710.04334.",
    "year": 2017
  }, {
    "title": "Shortcutstacked sentence encoders for multi-domain inference",
    "authors": ["Yixin Nie", "Mohit Bansal."],
    "venue": "arXiv preprint arXiv:1708.02312.",
    "year": 2017
  }, {
    "title": "Keyword-based query comprehending via multiple optimized-demand augmentation",
    "authors": ["Boyuan Pan", "Hao Li", "Zhou Zhao", "Deng Cai", "Xiaofei He."],
    "venue": "arXiv preprint arXiv:1711.00179.",
    "year": 2017
  }, {
    "title": "Memen: Multi-layer embedding with memory networks for machine comprehension",
    "authors": ["Boyuan Pan", "Hao Li", "Zhou Zhao", "Bin Cao", "Deng Cai", "Xiaofei He."],
    "venue": "arXiv preprint arXiv:1707.09098.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Reasoning about entailment with neural attention",
    "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "arXiv preprint arXiv:1611.01603.",
    "year": 2016
  }, {
    "title": "Reading and thinking: Re-read lstm unit for textual entailment recognition",
    "authors": ["Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Pa-",
    "year": 2016
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "A compare-propagate architecture with alignment factorization for natural language inference",
    "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui."],
    "venue": "arXiv preprint arXiv:1801.00102.",
    "year": 2017
  }, {
    "title": "Order-embeddings of images and language",
    "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "Bilateral multi-perspective matching for natural language sentences",
    "authors": ["Zhiguo Wang", "Wael Hamza", "Radu Florian."],
    "venue": "International Joint Conference on Artificial Intelligence (IJCAI).",
    "year": 2017
  }, {
    "title": "A broad-coverage challenge corpus for sentence understanding through inference",
    "authors": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."],
    "venue": "arXiv preprint arXiv:1704.05426.",
    "year": 2017
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Machine Learning, 8(3-4):229–256.",
    "year": 1992
  }, {
    "title": "Words or characters? fine-grained gating for reading comprehension",
    "authors": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the Association for Computational Linguis-",
    "year": 2014
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701.",
    "year": 2012
  }, {
    "title": "What to do next: modeling user behaviors by time-lstm",
    "authors": ["Yu Zhu", "Hao Li", "Yikang Liao", "Beidou Wang", "Ziyu Guan", "Haifeng Liu", "Deng Cai."],
    "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI), pages",
    "year": 2017
  }, {
    "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
    "authors": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Proceedings of the IEEE in-",
    "year": 2015
  }],
  "id": "SP:48758697a493e9a970c51a6198fbb20133d7ae97",
  "authors": [{
    "name": "Boyuan Pan",
    "affiliations": []
  }, {
    "name": "Yazheng Yang",
    "affiliations": []
  }, {
    "name": "Zhou Zhao",
    "affiliations": []
  }, {
    "name": "Yueting Zhuang",
    "affiliations": []
  }, {
    "name": "Deng Cai",
    "affiliations": []
  }, {
    "name": "Xiaofei He",
    "affiliations": []
  }],
  "abstractText": "Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as “so” or “but” to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.",
  "title": "Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference"
}