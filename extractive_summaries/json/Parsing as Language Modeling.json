{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics\nWe recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%."
  }, {
    "heading": "1 Introduction",
    "text": "Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1. In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.\nIn the remainder of this section we outline the major difference between this and previous work — viewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5)."
  }, {
    "heading": "1.1 Language Modeling",
    "text": "Formally, a language model (LM) is a probability distribution over strings of a language:\nP (x) = P (x1, · · · , xn)\n=\nn∏\nt=1\nP (xt|x1, · · · , xt−1), (1)\nwhere x is a sentence and t indicates a word position. The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well."
  }, {
    "heading": "1.2 Parsing as Language Modeling",
    "text": "A generative parsing model parses a sentence (x) into its phrasal structure (y) according to\nargmax y′∈Y(x)\nP (x,y′),\nwhere Y(x) lists all possible structures of x. If we think of a tree (x,y) as a sequence (z) (Vinyals et\n2331\nal., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows:\nP (x,y) = P (z) = P (z1, · · · , zm)\n= m∏\nt=1\nP (zt|z1, · · · , zt−1), (2)\nwhich is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (zt|z1, · · · , zt−1) for parsing."
  }, {
    "heading": "2 Previous Work",
    "text": "We look here at three neural net (NN) models closest to our research along various dimensions. The first (Zaremba et al., 2014) gives the basic language modeling architecture that we have adopted, while the other two (Vinyals et al., 2015; Dyer et al., 2016) are parsing models that have the current best results in NN parsing."
  }, {
    "heading": "2.1 LSTM-LM",
    "text": "The LSTM-LM of Zaremba et al. (2014) turns (x1, · · · , xt−1) into ht, a hidden state of an LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013), and uses ht to guess xt:\nP (xt|x1, · · · , xt−1) = P (xt|ht) = softmax(Wht)[xt],\nwhere W is a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages (Kim et al., 2016) and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (Jozefowicz et al., 2016). In this paper, we build a parsing model based on the LSTM-LM of Zaremba et al. (2014)."
  }, {
    "heading": "2.2 MTP",
    "text": "Vinyals et al. (2015) observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a\nconditional probability:\nP (y|x) = P (y1, · · · , yl|x)\n= l∏\nt=1\nP (yt|x, y1, · · · , yt−1),\nwhere the conditioning event (x, y1, · · · , yt−1) is modeled by an LSTM encoder and an LSTM decoder. The encoder maps x into he, a set of vectors that represents x, and the decoder obtains a summary vector (h′t) which is concatenation of the decoder’s hidden state (hdt ) and weighted sum of word representations ( ∑n i=1 αih e i ) with an alignment vector (α). Finally the decoder predicts yt given h′t. Inspired by MTP, our model processes sequential trees."
  }, {
    "heading": "2.3 RNNG",
    "text": "Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016):\nP (x,y) = P (a) =\nm∏\nt=1\nP (at|a1, · · · , at−1), (3)\nwhere a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2). RNNG and our model differ in how they compute the conditioning event (z1, · · · , zt−1): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM’s hidden state as shown in the next section."
  }, {
    "heading": "3 Model",
    "text": "Our model, the model of Zaremba et al. (2014) applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:\nP (x,y) = P (z) = m∏\nt=1\nP (zt|z1, · · · , zt−1)\n=\nm∏\nt=1\nP (zt|ht)\n= m∏\nt=1\nsoftmax(Wht)[zt],\nwhere ht is a hidden state of an LSTM. Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies\nargmax y′∈Y ′(x)\nP (x,y′). (4)"
  }, {
    "heading": "3.1 Hyper-parameters",
    "text": "The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50. We initialize starting states with previous minibatch’s last hidden states (Sutskever, 2013). The forget gate bias is initialized to be one (Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05). Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is bigger than 20 (Pascanu et al., 2013). The learning rate is 0.25 · 0.85max( −15, 0) where is an epoch number. For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012)."
  }, {
    "heading": "4 Experiments",
    "text": "We describe datasets we use for evaluation, detail training and development processes.1"
  }, {
    "heading": "4.1 Data",
    "text": "We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014). We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et\n1The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.\n2We use the reimplementation by Huang et al. (2010)."
  }, {
    "heading": "10 94.0 91.2 39.8",
    "text": ""
  }, {
    "heading": "100 96.3 91.7 39.9",
    "text": ""
  }, {
    "heading": "500 97.0 91.8 40.0",
    "text": "al., 2015) because in preliminary experiments Charniak parser (Charniak, 2000) performed better when trained on all of 24 million trees than when trained on resampled two million trees.\nGiven x, we produce Y ′(x), 50-best trees, with Charniak parser and find y with LSTM-LM as Dyer et al. (2016) do with their discriminative and generative models.3"
  }, {
    "heading": "4.2 Training and Development",
    "text": ""
  }, {
    "heading": "4.2.1 Supervision",
    "text": "We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data. Both perplexity and F1 of LSTM-LM (G) improve and then plateau (Figure 2). Perplexity, the\n3Dyer et al. (2016)’s discriminative model performs comparably to Charniak (89.8 vs. 89.7).\nmodel’s training objective, nicely correlates with F1, what we care about. Training takes 12 hours (37 epochs) on a Titan X. We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51o). As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser. We address this problem in Section 5.3."
  }, {
    "heading": "4.2.2 Semi-supervision",
    "text": "We unk words that appear at most once in the training (21,755 types). We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization. We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTMLM (GS) achieves 92.5 F1 on the development."
  }, {
    "heading": "5 Results",
    "text": ""
  }, {
    "heading": "5.1 Supervision",
    "text": "As shown in Table 2, with 92.6 F1 LSTM-LM (G) outperforms an ensemble of five MTPs (Vinyals et al., 2015) and RNNG (Dyer et al., 2016), both of which are trained on the WSJ only."
  }, {
    "heading": "5.2 Semi-supervision",
    "text": "We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models\n4The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.\ntrained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016). We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers’ parsing performance along with their training data is reported in Table 3. LSTM-LM (GS) outperforms all the other parsers with 93.1 F1."
  }, {
    "heading": "5.3 Improved Semi-supervision",
    "text": "Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x). To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y ′(x). As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x). A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1. When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016). Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014)."
  }, {
    "heading": "6 Conclusion",
    "text": "The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016). We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016). We also wish to develop a complete parsing model using the LSTMLM framework."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science\n5Version 3.3.0. 6We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl. Following the convention, we ignore punctuation.\nat Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees."
  }],
  "year": 2016,
  "references": [{
    "title": "Globally normalized transition-based neural networks",
    "authors": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-",
    "year": 2016
  }, {
    "title": "A maximum-entropy-inspired parser",
    "authors": ["Eugene Charniak."],
    "venue": "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2000
  }, {
    "title": "Syntactic parse fusion",
    "authors": ["Do Kook Choe", "David McClosky", "Eugene Charniak."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Recurrent neural network grammars",
    "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
    "year": 2016
  }, {
    "title": "Learning precise timing with lstm recurrent networks",
    "authors": ["Felix A Gers", "Nicol N Schraudolph", "Jürgen Schmidhuber."],
    "venue": "The Journal of Machine Learning Research.",
    "year": 2003
  }, {
    "title": "Generating sequences with recurrent neural networks",
    "authors": ["Alex Graves."],
    "venue": "arXiv preprint arXiv:1308.0850.",
    "year": 2013
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["Michael U. Gutmann", "Aapo Hyvärinen."],
    "venue": "The Journal of Machine Learning Research.",
    "year": 2012
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation.",
    "year": 1997
  }, {
    "title": "Self-training with products of latent variable grammars",
    "authors": ["Zhongqiang Huang", "Mary Harper", "Slav Petrov."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2010
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning.",
    "year": 2015
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."],
    "venue": "arXiv preprint arXiv:1602.02410.",
    "year": 2016
  }, {
    "title": "Character-aware neural language models",
    "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "An empirical comparison of parsing methods for stanford dependencies",
    "authors": ["Lingpeng Kong", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1404.4314.",
    "year": 2014
  }, {
    "title": "Ambiguity-aware ensemble training for semisupervised dependency parsing",
    "authors": ["Zhenghua Li", "Min Zhang", "Wenliang Chen."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Computational linguistics.",
    "year": 1993
  }, {
    "title": "Effective self-training for parsing",
    "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL.",
    "year": 2006
  }, {
    "title": "Hierarchical probabilistic neural network language model",
    "authors": ["Frederic Morin", "Yoshua Bengio."],
    "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics.",
    "year": 2005
  }, {
    "title": "English gigaword fifth edition",
    "authors": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."],
    "venue": "Linguistic Data Consortium, LDC2011T07.",
    "year": 2011
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning.",
    "year": 2013
  }, {
    "title": "Products of random latent variable grammars",
    "authors": ["Slav Petrov."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Dropout improves recurrent neural networks for handwriting recognition",
    "authors": ["Vu Pham", "Théodore Bluche", "Christopher Kermorvant", "Jérôme Louradour."],
    "venue": "2014 14th International Conference on Frontiers in Handwriting Recognition.",
    "year": 2014
  }, {
    "title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing",
    "authors": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Training recurrent neural networks",
    "authors": ["Ilya Sutskever."],
    "venue": "Ph.D. thesis, University of Toronto.",
    "year": 2013
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "Advances in Neural Information Processing Systems 28.",
    "year": 2015
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."],
    "venue": "arXiv preprint arXiv:1409.2329.",
    "year": 2014
  }, {
    "title": "Fast and accurate shift-reduce constituent parsing",
    "authors": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.",
    "year": 2013
  }],
  "id": "SP:a538974cbebcae6c1c86d77e4c25b86848723322",
  "authors": [{
    "name": "Do Kook Choe",
    "affiliations": []
  }, {
    "name": "Eugene Charniak",
    "affiliations": []
  }],
  "abstractText": "We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing — 93.8 F1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.",
  "title": "Parsing as Language Modeling"
}