{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Robust statistics was founded in the seminal works of (Tukey, 1960) and (Huber, 1964). The overarching motto is that any model (especially a parametric one) is only approximately valid, and that any estimator designed for a particular distribution that is to be used in practice must also be stable in the presence of model misspecification. The standard setup is to assume that the samples we are\n*Equal contribution 1University of Southern California, Los Angeles, California, USA 2Massachusetts Institute of Technology, Cambridge, Massachusetts, USA 3University of California, San Diego, La Jolla, California, USA. Correspondence to: Ilias Diakonikolas <diakonik@usc.edu>, Gautam Kamath <g@csail.mit.edu>, Daniel M. Kane <dakane@cs.ucsd.edu>, Jerry Li <jerryzli@mit.edu>, Ankur Moitra <moitra@mit.edu>, Alistair Stewart <alistais@usc.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ngiven come from a nice distribution, but that an adversary has the power to arbitrarily corrupt a constant fraction of the observed data. After several decades of work, the robust statistics community has discovered a myriad of estimators that are provably robust. An important feature of this line of work is that it can tolerate a constant fraction of corruptions independent of the dimension and that there are estimators for both the location (e.g., the mean) and scale (e.g., the covariance). See (Huber & Ronchetti, 2009) and (Hampel et al., 1986) for further background.\nIt turns out that there are vast gaps in our understanding of robustness, when computational considerations are taken into account. In one dimension, robustness and computational efficiency are in perfect harmony. The empirical mean and empirical variance are not robust, because a single corruption can arbitrarily bias these estimates, but alternatives such as the median and the interquartile range are straightforward to compute and are provably robust.\nBut in high dimensions, there is a striking tension between robustness and computational efficiency. Let us consider estimators for location. The Tukey median (Tukey, 1960) is a natural generalization of the one-dimensional median to high-dimensions. It is known that it behaves well (i.e., it needs few samples) when estimating the mean for various symmetric distributions (Donoho & Gasko, 1992; Chen et al., 2016). However, it is hard to compute in general (Johnson & Preparata, 1978; Amaldi & Kann, 1995) and the many heuristics for computing it degrade badly in the quality of their approximation as the dimension scales (Clarkson et al., 1993; Chan, 2004; Miller & Sheehy, 2010). The same issues plague estimators for scale. The minimum volume ellipsoid (Rousseeuw, 1985) is a natural generalization of the one-dimensional interquartile range and is provably robust in high-dimensions, but is also hard to compute. And once again, heuristics for computing it (Van Aelst & Rousseeuw, 2009; Rousseeuw & Struyf, 1998) work poorly in high dimensions.\nThe fact that robustness in high dimensions seems to come\nCollectively, the authors were supported by NSF CCF1652862, CCF-1551875, CCF-1617730, CCF-1650733, CCF1553288, CCF-1453261, ONR N00014-12-1-0999, three Sloan Research Fellowships, two Google Faculty Research Awards, an NSF fellowship, the MIT NEC Corporation, and a USC startup grant.\nat such a steep price has long been a point of consternation within robust statistics. In a 1997 retrospective on the development of robust statistics, Huber laments: “It is one thing to design a theoretical algorithm whose purpose is to prove [large fractions of corruptions can be tolerated] and quite another thing to design a practical version that can be used not merely on small, but also on medium sized regression problems, with a 2000 by 50 matrix or so. This last requirement would seem to exclude all of the recently proposed [techniques].”\nThe goal of this paper is to answer Huber’s call to action and design estimators for both the mean and covariance that are highly practical, provably robust, and work in high-dimensions. Such estimators make the promise of robust statistics – estimators that work in high-dimensions and limit the error induced by outliers – much closer to a reality.\nFirst, we make some remarks to dispel some common misconceptions. There has been a considerable amount of recent work on robust principal component analysis, much of it making use of semidefinite programming. Some of these works can tolerate a constant fraction of corruptions (Candès et al., 2011), however require that the locations of the corruptions are evenly spread throughout the dataset so that no individual sample is entirely corrupted. In contrast, the usual models in robust statistics are quite rigid in what they require and they do this for good reason. A common scenario that is used to motivate robust statistical methods is if two studies are mixed together, and one subpopulation does not fit the model. Then one wants estimators that work without assuming anything at all about these outliers.\nThere have also been semidefinite programming methods proposed for robust principal component analysis with outliers (Xu et al., 2010). These methods assume that the uncorrupted matrix is rank r and that the fraction of outliers is at most 1/r, which again degrades badly as the rank of the matrix increases. Moreover, any method that uses semidefinite programming will have difficulty scaling to the sizes of the problems we consider here. For sake of comparison – even with state-of-the-art interior point methods – it is not currently feasible to solve the types of semidefinite programs that have been proposed when the matrices have dimension larger than a hundred."
  }, {
    "heading": "1.1. Robustness in a Generative Model",
    "text": "Recent works in theoretical computer science have sought to circumvent the usual difficulties of designing efficient and robust algorithms by instead working in a generative model. The starting point for our paper is the work of Diakonikolas et al. (2016a) who gave an efficient algorithm for the problem of agnostically learning a Gaussian: Given a polynomial number of samples from a high-dimensional\nGaussian N (µ,Σ), where an adversary has arbitrarily corrupted an ε-fraction, find a set of parametersN ′(µ̂, Σ̂) that satisfy dTV (N ,N ′) ≤ Õ(ε)1.\nTotal variation distance is the natural metric to use to measure closeness of the parameters, since a (1−ε)-fraction of the observed samples came from a Gaussian. (Diakonikolas et al., 2016a) gave an algorithm for the above problem (note that the guarantees are dimension independent), whose running time and sample complexity are polynomial in the dimension d and 1/ε. (Lai et al., 2016) independently gave an algorithm for the unknown mean case that achieves dTV (N ,N ′) ≤ Õ(ε √ log d), and in the unknown covariance case achieves guarantees in a weaker metric that is not affine invariant. A crucial feature is that both algorithms work even when the moments of the underlying distribution satisfy certain conditions, and thus are not necessarily brittle to the modeling assumption that the inliers come from a Gaussian distribution.\nA more conceptual way to view such work is as a proofof-concept that the Tukey median and minimum volume ellipsoid can be computed efficiently in a natural family of distributional models. This follows because not only would these be good estimates for the mean and covariance in the above model, but in fact any estimates that are good must also be close to them. Thus, these works fit into the emerging research direction of circumventing worst-case lower bounds by going beyond worst-case analysis.\nSince the dissemination of the aforementioned works (Diakonikolas et al., 2016a; Lai et al., 2016), there has been a flurry of research activity on computationally efficient robust estimation in a variety of high-dimensional settings, including studying graphical models (Diakonikolas et al., 2016b), understanding the computation-robustness tradeoff for statistical query algorithms (Diakonikolas et al., 2016c), tolerating much more noise by allowing the algorithm to output a list of candidate hypotheses (Charikar et al., 2017), and developing robust algorithms under sparsity assumptions (Li, 2017; Du et al., 2017), and more (Diakonikolas et al., 2017; Steinhardt et al., 2017)."
  }, {
    "heading": "1.2. Our Results",
    "text": "Our goal in this work is to show that high-dimensional robust estimation can be highly practical. However, there are two major obstacles to achieving this. First, the sample complexity and running time of the algorithms in (Diakonikolas et al., 2016a) is prohibitively large for highdimensional applications. We just would not be able to store as many samples as we would need, in order to com-\n1We use the notation Õ(·) to hide factors which are polylogarithmic in the argument – in particular, we note that this bound does not depend on the dimension.\npute accurate estimates, in high-dimensional applications.\nOur first main contribution is to show nearly-tight bounds on the sample complexity of the filtering-based algorithm of (Diakonikolas et al., 2016a). Roughly speaking, we accomplish this with a new definition of the good set which straightforwardly plugs into the existing analysis, showing that one can estimate the mean with Õ(d/ε2) samples (when the covariance is known) and the covariance with Õ(d2/ε2) samples. Both of these bounds are informationtheoretically optimal, up to logarithmic factors.\nOur second main contribution is to vastly improve the fraction of adversarial corruptions that can be tolerated in applications. The fraction of errors that the algorithms of (Diakonikolas et al., 2016a) can tolerate is indeed a constant that is independent of the dimension, but it is very small both in theory and in practice – a naive implementation of the algorithm did not remove any outliers in many realistic scenarios. We avoid this by giving new ways to empirically tune the threshold for where to remove points from the sample set.\nFinally, we show that the same bounds on the error guarantee continue to work even when the underlying distribution is sub-Gaussian. This theoretically confirms that the robustness guarantees of such algorithms are in fact not overly brittle to the distributional assumptions. In fact, the filtering algorithm of (Diakonikolas et al., 2016a) is easily shown to be robust under much weaker distributional assumptions, while retaining near-optimal sample and error guarantees. As an example, we show that it yields a near sample-optimal efficient estimator for robustly estimating the mean of a distribution, under the assumption that its covariance is bounded. Even in this regime, the filtering algorithm guarantees optimal error, up to a constant factor. Furthermore we empirically corroborate this finding by showing that the algorithm works well on real world data, as we describe below.\nNow we come to the task of testing out our algorithms. To the best of our knowledge, there have been no experimental evaluations of the performance of the myriad of approaches to robust estimation. It remains mostly a mystery which ones perform well in high-dimensions, and which do not. To test out our algorithms, we design a synthetic experiment where a (1 − ε)-fraction of the samples come from a Gaussian and the rest are noise and sampled from another distribution (in many cases, Bernoulli). This gives us a baseline to compare how well various algorithms recover µ and Σ, and how their performance degrades based on the dimension. Our plots show a predictable and yet striking phenomenon: All earlier approaches have error rates that scale polynomially with the dimension and ours is a constant that is almost indistinguishable from the error that comes from sample noise alone. Moreover, our algorithms\nare able to scale to hundreds of dimensions.\nBut are algorithms for agnostically learning a Gaussian unduly sensitive to the distributional assumptions they make? We are able to give an intriguing visual demonstration of our techniques on real data. The famous study of (Novembre et al., 2008) showed that performing principal component analysis on a matrix of genetic data recovers a map of Europe. More precisely, the top two singular vectors define a projection into the plane and when the groups of individuals are color-coded with where they are from, we recover familiar country boundaries that corresponds to the map of Europe. The conclusion from their study was that genes mirror geography. Given that one of the most important applications of robust estimation ought to be in exploratory data analysis, we ask: To what extent can we recover the map of Europe in the presence of noise? We show that when a small number of corrupted samples are added to the dataset, the picture becomes entirely distorted (and this continues to hold even for many other methods that have been proposed). In contrast, when we run our algorithm, we are able to once again recover the map of Europe. Thus, even when some fraction of the data has been corrupted (e.g., medical studies were pooled together even though the subpopulations studied were different), it is still possible to perform principal component analysis and recover qualitatively similar conclusions as if there were no noise at all!"
  }, {
    "heading": "2. Formal Framework",
    "text": "Notation. For a vector v, we will let ‖v‖2 denote its Euclidean norm. IfM is a matrix, we will let ‖M‖2 denote its spectral norm and ‖M‖F denote its Frobenius norm. We will write X ∈u S to denote that X is drawn from the empirical distribution defined by S.\nRobust Estimation. We consider the following powerful model of robust estimation that generalizes many other existing models, including Huber’s contamination model: Definition 2.1. Given ε > 0 and a distribution family D, the adversary operates as follows: The algorithm specifies some number of samples m. The adversary generates m samples X1, X2, . . . , Xm from some (unknown) D ∈ D. It then draws m′ from an appropriate distribution. This distribution is allowed to depend on X1, X2, . . . , Xm, but when marginalized over the m samples satisfies m′ ∼ Bin(ε,m). The adversary is allowed to inspect the samples, removesm′ of them, and replaces them with arbitrary points. The set of m points is then given to the algorithm.\nIn summary, the adversary is allowed to inspect the samples before corrupting them, both by adding corrupted points and deleting uncorrupted points. In contrast, in Huber’s model the adversary is oblivious to the samples and is only allowed to add corrupted points.\nWe remark that there are no computational restrictions on the adversary. The goal is to return the parameters of a distribution D̂ in D that are close to the true parameters in an appropriate metric. For the case of the mean, our metric will be the Euclidean distance. For the covariance, we will use the Mahalanobis distance, i.e., ‖Σ−1/2Σ̂Σ−1/2 − I‖F . This is a strong affine invariant distance that implies corresponding bounds in total variation distance.\nWe will use the following terminology:\nDefinition 2.2. We say that a set of samples is ε-corrupted if it is generated by the process described in Definition 2.1."
  }, {
    "heading": "3. Nearly Sample-Optimal Efficient Robust Learning",
    "text": "In this section, we present near sample-optimal efficient robust estimators for the mean and the covariance of highdimensional distributions under various structural assumptions of varying strength. Our estimators rely on the filtering technique introduced in (Diakonikolas et al., 2016a).\nThis paper gave two algorithmic techniques: the first one was a spectral technique to iteratively remove outliers from the dataset (filtering), and the second one was a soft-outlier removal method relying on convex programming. The filtering technique seemed amenable to practical implementation (as it only uses simple eigenvalue computations), but the corresponding sample complexity bounds given in (Diakonikolas et al., 2016a) are polynomially worse than the information-theoretic minimum. On the other hand, the convex programming technique of Diakonikolas et al. (2016a) achieved better sample complexity bounds (e.g., near sample-optimal for robust mean estimation), but relied on the ellipsoid method, which seemed to preclude a practically efficient implementation.\nIn this work, we achieve the best of both worlds: we give a better analysis of the filter, giving sample-optimal bounds (up to logarithmic factors) for both the mean and the covariance. Moreover, we show that the filtering technique easily extends to much weaker distributional assumptions (e.g., under bounded second moments). Roughly speaking, the filtering technique follows a general iterative recipe: (1) via spectral methods, find some univariate test which is violated by the corrupted points, (2) find some concrete tail bound violated by the corrupted points, and (3) discard all points which violate this tail bound.\nWe start with sub-gaussian distributions. Recall that if P is sub-gaussian on Rd with mean vector µ and parameter ν > 0, then for any unit vector v ∈ Rd we have that PrX∼P [|v · (X − µ)| ≥ t] ≤ exp(−t2/2ν).\nTheorem 3.1. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix\nI , and ε > 0. Let S be an ε-corrupted set of samples from G of size Ω((d/ε2) poly log(d/ε)). There exists an efficient algorithm that, on input S and ε > 0, returns a mean vector µ̂ so that with probability at least 9/10 we have ‖µ̂− µG‖2 = O(ε √ log(1/ε)).\nDiakonikolas et al. (2016a) gave algorithms for robustly estimating the mean of a Gaussian distribution with known covariance and for robustly estimating the mean of a binary product distribution. The main motivation for considering these specific distribution families is that robustly estimating the mean within Euclidean distance immediately implies total variation distance bounds for these families. The above theorem establishes that these guarantees hold in a more general setting with near sample-optimal bounds. Under a bounded second moment assumption, we show: Theorem 3.2. Let P be a distribution on Rd with unknown mean vector µP and unknown covariance matrix ΣP σ2I . Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d). There exists an efficient algorithm that, on input S and ε > 0, with probability 9/10 outputs µ̂ with ‖µ̂− µP ‖2 ≤ O( √ εσ).\nThe sample size above is optimal, up to a logarithmic factor, and the error guarantee is easily seen to be the best possible up to a constant factor. The main difference between the filtering algorithm establishing the above theorem and the filtering algorithm for the sub-gaussian case is how we choose the threshold for the filter. Instead of looking for a violation of a concentration inequality, here we will choose a threshold at random. In this case, randomly choosing a threshold weighted towards higher thresholds suffices to throw out more corrupted samples than uncorrupted samples in expectation. Although it is possible to reject many good samples this way, we show that the algorithm still only rejects a total of O(ε) samples with high probability.\nFinally, estimating the covariance of a Gaussian: Theorem 3.3. LetG ∼ N (0,Σ) be a Gaussian in d dimensions, and let ε > 0. Let S be an ε-corrupted set of samples from G of size Ω((d2/ε2) poly log(d/ε)). There exists an efficient algorithm that, given S and ε, returns the parameters of a Gaussian distribution G′ ∼ N (0, Σ̂) so that with probability at least 9/10, it holds ‖I−Σ−1/2Σ̂Σ−1/2‖F = O(ε log(1/ε)).\nWe now provide a high-level description of the main ingredient which yields these improved sample complexity bounds. The initial analysis of Diakonikolas et al. (2016a) established sample complexity bounds which were suboptimal by polynomial factors because it insisted that the set of good samples (i.e., before the corruption) satisfied very tight tail bounds. To some degree such bounds are necessary, as when we perform our filtering procedure, we need to ensure that not too many good samples are thrown\naway. However, the old analysis required that fairly strong tail bounds hold uniformly. The idea for the improvement is as follows: If the errors are sufficient to cause the variance of some polynomial p (linear in the unknown mean case or quadratic in the unknown covariance case) to increase by more than ε, it must be the case that for some T , roughly an ε/T 2 fraction of samples are error points with |p(x)| > T . As long as we can ensure that less than an ε/T 2 fraction of our good sample points have |p(x)| > T , this will suffice for our filtering procedure to work. For small values of T , these are much weaker tail bounds than were needed previously and can be achieved with a smaller number of samples. For large values of T , these tail bounds are comparable to those used in previous work (Diakonikolas et al., 2016a) , but in such cases we can take advantage of the fact that |p(G)| > T only with very small probability, again allowing us to reduce the sample complexity. The details are deferred to the supplementary material."
  }, {
    "heading": "4. Filtering",
    "text": "We now describe the filtering technique more rigorously, as well as some additional practical heuristics."
  }, {
    "heading": "4.1. Robust Mean Estimation",
    "text": "We first consider mean estimation. The algorithms which achieve Theorems 3.1 and 3.2 both follow the general recipe in Procedure 1. We must specify three parameter functions:\n• Thres(ε) is a threshold function—we terminate if the covariance has spectral norm bounded by Thres(ε).\n• Tail(T, d, ε, δ, τ) is an univariate tail bound, which would only be violated by a τ fraction of points if they were uncorrupted, but is violated by many more of the current set of points.\n• δ(ε, s) is a slack function, which we require for technical reasons.\nGiven these objects, our filter is fairly easy to state: first, we compute the empirical covariance. Then, we check if the spectral norm of the empirical covariance exceeds Thres(ε). If it does not, we output the empirical mean with the current set of data points. Otherwise, we project onto the top eigenvector of the empirical covariance, and throw away all points which violate Tail(T, d, ε, δ, τ), for some choice of slack function δ.\nSub-gaussian case To instantiate this algorithm for the subgaussian case, we take Thres(ε) = O(ε log 1/ε), δ(ε, s) = 3 √ ε(s− 1), and Tail(T, d, ε, δ, τ) = 8 exp(−T 2/2ν) + 8 εT 2 log(d log(d/ετ)) , where ν is the subgaussian parameter. See the supplementary material for details.\nProcedure 1 Filter-based algorithm template for robust mean estimation\n1: Input: An ε-corrupted set of samples S, Thres(ε),Tail(T, d, ε, δ, τ), δ(ε, s) 2: Compute the sample mean µS ′\n= EX∈uS′ [X], covariance Σ, approximations for the largest absolute eigenvalue and eigenvector of Σ, λ∗ := ‖Σ‖2, and v∗.\n3: if ‖Σ‖2 ≤ Thres(ε) then 4: return µS ′ . 5: end if 6: Let δ = δ(ε, ‖Σ‖2). 7: Find T > 0 such that\nPr X∈uS′\n[ |v∗ · (X − µS ′ )| > T + δ ] > Tail(T, d, ε, δ, τ).\n8: return {x ∈ S′ : |v∗ · (x− µS′)| ≤ T + δ}.\nSecond moment case To instantiate this algorithm for the second moment case, we take Thres(ε) = 9, δ = 0, and we take Tail to be a random rescaling of the largest deviation in the data set, in the direction v∗. See the supplementary material for details."
  }, {
    "heading": "4.2. Robust Covariance Estimation",
    "text": "Our algorithm for robust covariance follows the exact recipe outlined above, with one key difference—we check for deviations in the empirical fourth moment tensor. Intuitively, just as in the robust mean setting, we used degree-2 information to detect outliers for the mean (the degree-1 moment), here we use degree-4 information to detect outliers for the covariance (the degree-2 moment).\nThis corresponds to finding a normalized degree-2 polynomial whose empirical variance is too large. Filtering along this polynomial with an appropriate choice of Thres(ε), δ(ε, s), and Tail gives the desired bounds. See the supplementary material for more details."
  }, {
    "heading": "4.3. Better Univariate Tests",
    "text": "In the algorithms described above for robust mean estimation, after projecting onto one dimension, we center the points at the empirical mean along this direction. This is theoretically sufficient, however, introduces additional constant factors since the empirical mean along this direction may be corrupted. Instead, one can use a robust estimate for the mean in one direction. Namely, it is well known that the median is a provably robust estimator for the mean for symmetric distributions (Huber & Ronchetti, 2009; Hampel et al., 1986), and under certain models it is in fact optimal in terms of its resilience to noise (Dvoretzky et al., 1956; Massart, 1990; Chen, 1998; Daskalakis & Kamath, 2014; Diakonikolas et al., 2017). By centering the points\nat the median instead of the mean, we are able to achieve better error in practice."
  }, {
    "heading": "4.4. Adaptive Tail Bounding",
    "text": "In our empirical evaluation, we found that it was important to find an appropriate choice of Tail, to achieve good error rates, especially for robust covariance estimation. Concretely, in this setting, our tail bound is given by Tail(T, d, ε, δ, τ) = C1 exp(−C2T ) + Tail2(T, d, ε, δ, τ), for some function Tail2, and constants C1, C2. We found that for reasonable settings, the term that dominated was always the first term on the RHS, and that Tail2 is less significant. Thus, we focused on optimizing the first term.\nWe found that depending on the setting, it was useful to change the constant C2. In particular, in low dimensions, we could be more stringent, and enforce a stronger tail bound (which corresponds to a higher C2), but in higher dimensions, we must be more lax with the tail bound. To do this in a principled manner, we introduced a heuristic we call adaptive tail bounding. Our goal is to find a choice of C2 which throws away roughly an ε-fraction of points. The heuristic is fairly simple: we start with some initial guess for C2. We then run our filter with this C2. If we throw away too many data points, we increase our C2, and retry. If we throw away too few, then we decrease our C2 and retry. Since increasing C2 strictly decreases the number of points thrown away, and vice versa, we binary search over our choice of C2 until we reach something close to our target accuracy. In our current implementation, we stop when the fraction of points we throw away is between ε/2 and 3ε/2, or if we’ve binary searched for too long. We found that this heuristic drastically improves our accuracy, and allows our algorithm to scale fairly smoothly from low to high dimension."
  }, {
    "heading": "5. Experiments",
    "text": "We performed an empirical evaluation of the above algorithms on synthetic and real data sets with and without\nsynthetic noise. All experiments were done on a laptop computer with a 2.7 GHz Intel Core i5 CPU and 8 GB of RAM. The focus of this evaluation was on statistical accuracy, not time efficiency. In all synthetic trials, our algorithm consistently had the smallest error, sometimes orders of magnitude better than any other algorithms. In the semi-synthetic benchmark, our algorithm also (arguably) performs the best, though this is subjective. While we did not optimize our code for runtime, it is always comparable to (and often better than) the effective alternatives."
  }, {
    "heading": "5.1. Synthetic Data",
    "text": "Experiments with synthetic data allow us to verify the error guarantees and the sample complexity rates proven in Section 3. In both cases, the experiments validate the accuracy and usefulness of our algorithm, almost exactly matching the best rate without noise.\nUnknown mean The results of our synthetic mean experiment are shown in Figure 1. In the synthetic mean experiment, we set ε = 0.1, and for dimension d = [100, 150, . . . , 400], we generate n = 10dε2 samples, where a (1 − ε)-fraction come from N (µ, I), and an ε fraction come from a noise distribution. Our goal is to produce an estimator which minimizes the `2 error the estimator has to the truth. As a baseline, we compute the error that is achieved by only the uncorrupted sample points. This error will be used as the gold standard for comparison, since in the presence of error, this is roughly the best one could do\neven if all the noise points were identified exactly.2\nOn this data, we compared the performance of our Filter algorithm to that of (1) the empirical mean of all the points, (2) a trivial pruning procedure, (3) the geometric median of the data, (4) a RANSAC-based mean estimation algorithm, and (5) a recently proposed robust estimator for the mean due to (Lai et al., 2016), which we will call LRVMean. For (5), we use the implementation available in their Github.3 In Figure 1, the x-axis indicates the dimension of the experiment, and the y-axis measures the `2 error of our estimated mean minus the `2 error of the empirical mean of the true samples from the Gaussian, i.e., the excess error induced over the sampling error.\nWe tried various noise distributions, and found that the same qualitative pattern arose for all of them. In the reported experiment, our noise distribution was a mixture of two binary product distributions, where one had a couple of large coordinates (see the supplementary material for a detailed description). For all (nontrivial) error distributions we tried, we observed that indeed the empirical mean, pruning, geometric median, and RANSAC all have error which diverges as d grows, as the theory predicts. On the other hand, both our algorithm and LRVMean have markedly smaller error as a function of dimension. Indeed, our algorithm’s error is almost identical to that of the empirical mean of the uncorrupted sample points.\nUnknown covariance See Figure 2 for the results of our synthetic covariance experiment. Our setup is similar to that for the synthetic mean. Since both our algorithm and LRVCov require access to fourth moments, we ran into issues with limited memory on machines. This limitation prevented us from performing experiments at the same dimensionality as the unknown mean setting, and we could not use as many samples. We fix ε = 0.05. For dimension d = [10, 20, . . . , 100], we generate 0.5dε2 samples, where a (1 − ε)-fraction come from N (0,Σ), and the rest come from a noise distribution. We measure distance in the natural affine invariant way, namely, the Mahalanobis distance induced by Σ to the identity: err(Σ̂) = ‖Σ−1/2Σ̂Σ−1/2 − I‖F . As before, we use the empirical error of only the uncorrupted data points as a benchmark.\nOn this corrupted data, we compared the performance of our Filter algorithm to that of (1) the empirical covariance of all the points, (2) a trivial pruning procedure, (3) a RANSAC-based minimal volume ellipsoid (MVE) algorithm, and (5) a recently proposed robust estimator for the covariance due to (Lai et al., 2016), which we will call\n2We note that it is possible that an estimator may achieve slightly better error than this baseline.\n3https://github.com/kal2000/AgnosticMean\\ AndCovarianceCode\nLRVCov. For (5), we again obtained the implementation from their Github repository.\nWe tried various choices of Σ and noise distribution. Figure 2 shows two choices of Σ and noise. Again, the x-axis indicates the dimension of the experiment and the y-axis indicates the estimator’s excess Mahalanobis error over the sampling error. In the left figure, we set Σ = I , and our noise points are simply all located at the all-zeros vector. In the right figure, we set Σ = I+10e1eT1 , where e1 is the first basis vector, and our noise distribution is a somewhat more complicated distribution, which is similarly spiked, but in a different, random, direction. We formally define this distribution in the supplementary material. For all choices of Σ and noise we tried, the qualitative behavior of our algorithm and LRVCov was unchanged. Namely, we seem to match the empirical error without noise up to a very small slack, for all dimensions. On the other hand, the performance of empirical mean, pruning, and RANSAC varies widely with the noise distribution. The performance of all these algorithms degrades substantially with dimension, and their error gets worse as we increase the skew of the underlying data. The performance of LRVCov is the most similar to ours, but again is worse by a large constant factor. In particular, our excess risk was on the order of 10−4 for large d, for both experiments, whereas the excess risk achieved by LRVCov was in all cases a constant between 0.1 and 2.\nThese experiments demonstrate that our statistical guarantees are in fact quite strong. As our excess error is almost zero (and orders of magnitude smaller than other approaches), this suggests that our sample complexity is indeed near-optimal, since we match the rate without noise, and that the constants and logarithmic factors in the theoretical recovery guarantee are often small or non-existent."
  }, {
    "heading": "5.2. Semi-synthetic Data",
    "text": "To demonstrate the efficacy of our method on real data, we revisit the famous study of Novembre et al. (2008). In this study, the authors investigated data collected as part of the POPRES project. This dataset consists of the genotyping of thousands of individuals using the Affymetrix 500K single nucleotide polymorphism (SNP) chip. The authors pruned the dataset to obtain the genetic data of over 1387 European individuals, annotated by their country of origin. Using principal components analysis, they produce a twodimensional summary of the genetic variation, which bears a striking resemblance to the map of Europe.\nOur experimental setup is as follows. While the original dataset is very high dimensional, we use a 20 dimensional version of the dataset as found in the authors’ GitHub4. We\n4https://github.com/NovembreLab/Novembre_ etal_2008_misc\nfirst randomly rotate the data, as then 20 dimensional data was diagonalized, and the high dimensional data does not follow such structure. We then add an additional ε1−ε fraction of points (so that they make up an ε-fraction of the final points). These added points were discrete points, following a simple product distribution (described in the supplementary materials). We used a number of methods to obtain a covariance matrix for this dataset, and we projected the data onto the top two singular vectors of this matrix. In Figure 3, we compare our techniques to pruning. In particular, our output was able to more or less reproduce the map of Europe, whereas pruning fails to. In the supplementary material, we also compare our result with a number of other techniques, including those we tested against in the unknown covariance experiments, and other robust PCA techniques. The only alternative algorithm which was able to produce meaningful output was LRVCov, which produced output that was similar to ours, but which produced a map which was somewhat more skewed. We believe that our algorithm produces the best picture.\nIn Figure 3, we also display the actual points which were output by our algorithm’s Filter. While it manages to remove most of the noise points, it also seems to remove some of the true data points, particularly those from Eastern Europe and Turkey. We attribute this to a lack of samples from these regions, and thus one could consider them as outliers to a dataset consisting of Western European individuals. For instance, Turkey had 4 data points, so it seems quite reasonable that any robust algorithm would naturally consider these points outliers.\nWe view our experiments as a proof of concept demonstration that our techniques can be useful in real world exploratory data analysis tasks, particularly those in highdimensions. Our experiments reveal that a minimal amount of noise can completely disrupt a data analyst’s ability to notice an interesting phenomenon, thus limiting us to only very well-curated data sets. But with robust methods, this noise does not interfere with scientific discovery, and we can still recover interesting patterns which otherwise would have been obscured by noise."
  }],
  "year": 2017,
  "references": [{
    "title": "The complexity and approximability of finding maximum feasible subsystems of linear relations",
    "authors": ["E. Amaldi", "V. Kann"],
    "venue": "Theoretical Computer Science,",
    "year": 1995
  }, {
    "title": "Robust principal component analysis",
    "authors": ["E.J. Candès", "X. Li", "Y. Ma", "J. Wright"],
    "venue": "J. ACM,",
    "year": 2011
  }, {
    "title": "An optimal randomized algorithm for maximum tukey depth",
    "authors": ["T.M. Chan"],
    "venue": "In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
    "year": 2004
  }, {
    "title": "Learning from untrusted data",
    "authors": ["M. Charikar", "J. Steinhardt", "G. Valiant"],
    "venue": "In Proceedings of STOC’17,",
    "year": 2017
  }, {
    "title": "A general decision theory for huber’s ε-contamination model",
    "authors": ["M. Chen", "C. Gao", "Z. Ren"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2016
  }, {
    "title": "A note on bias robustness of the median",
    "authors": ["Z. Chen"],
    "venue": "Statistics & probability letters,",
    "year": 1998
  }, {
    "title": "Approximating center points with iterated radon points",
    "authors": ["K.L. Clarkson", "D. Eppstein", "G.L. Miller", "C. Sturtivant", "Teng", "S.-H"],
    "venue": "In Proceedings of the Ninth Annual Symposium on Computational Geometry,",
    "year": 1993
  }, {
    "title": "Faster and sample nearoptimal algorithms for proper learning mixtures of gaussians",
    "authors": ["C. Daskalakis", "G. Kamath"],
    "venue": "In Proceedings of The 27th Conference on Learning Theory, COLT",
    "year": 2014
  }, {
    "title": "Robust estimators in high dimensions without the computational intractability",
    "authors": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"],
    "venue": "In Proceedings of FOCS’16,",
    "year": 2016
  }, {
    "title": "Robust learning of fixed-structure bayesian networks. CoRR, abs/1606.07384, 2016b",
    "authors": ["I. Diakonikolas", "D.M. Kane", "A. Stewart"],
    "venue": "URL https://arxiv. org/abs/1606.07384",
    "year": 2016
  }, {
    "title": "Statistical query lower bounds for robust estimation of highdimensional gaussians and gaussian mixtures",
    "authors": ["I. Diakonikolas", "D.M. Kane", "A. Stewart"],
    "venue": "CoRR, abs/1611.03473,",
    "year": 2016
  }, {
    "title": "Robustly learning a gaussian: Getting optimal error",
    "authors": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"],
    "venue": "efficiently. CoRR,",
    "year": 2017
  }, {
    "title": "Breakdown properties of location estimates based on halfspace depth and projected outlyingness",
    "authors": ["D.L. Donoho", "M. Gasko"],
    "venue": "Ann. Statist., 20(4):1803–1827,",
    "year": 1992
  }, {
    "title": "Computationally efficient robust estimation of sparse functionals",
    "authors": ["S.S. Du", "S. Balakrishnan", "A. Singh"],
    "venue": "In Proceedings of COLT’17,",
    "year": 2017
  }, {
    "title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
    "authors": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"],
    "venue": "Ann. Mathematical Statistics,",
    "year": 1956
  }, {
    "title": "Robust statistics. The approach based on influence functions",
    "authors": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"],
    "year": 1986
  }, {
    "title": "Robust estimation of a location parameter",
    "authors": ["P.J. Huber"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1964
  }, {
    "title": "The densest hemisphere problem",
    "authors": ["D.S. Johnson", "F.P. Preparata"],
    "venue": "Theoretical Computer Science,",
    "year": 1978
  }, {
    "title": "Agnostic estimation of mean and covariance",
    "authors": ["K.A. Lai", "A.B. Rao", "S. Vempala"],
    "venue": "In Proceedings of FOCS’16,",
    "year": 2016
  }, {
    "title": "Robust sparse estimation tasks in high dimensions",
    "authors": ["J. Li"],
    "venue": "In Proceedings of COLT’17,",
    "year": 2017
  }, {
    "title": "The tight constant in the Dvoretzky-KieferWolfowitz inequality",
    "authors": ["P. Massart"],
    "venue": "Annals of Probability,",
    "year": 1990
  }, {
    "title": "Approximate centerpoints with proofs",
    "authors": ["G.L. Miller", "D. Sheehy"],
    "venue": "Comput. Geom.,",
    "year": 2010
  }, {
    "title": "Genes mirror geography within europe",
    "authors": ["J. Novembre", "T. Johnson", "K. Bryc", "Z. Kutalik", "A.R. Boyko", "A. Auton", "A. Indap", "K.S. King", "S. Bergmann", "Nelson", "M. R"],
    "year": 2008
  }, {
    "title": "Multivariate estimation with high breakdown point",
    "authors": ["P. Rousseeuw"],
    "venue": "Mathematical Statistics and Applications, pp",
    "year": 1985
  }, {
    "title": "Computing location depth and regression depth in higher dimensions",
    "authors": ["P.J. Rousseeuw", "A. Struyf"],
    "venue": "Statistics and Computing,",
    "year": 1998
  }, {
    "title": "Resilience: A criterion for learning in the presence of arbitrary outliers",
    "authors": ["J. Steinhardt", "M. Charikar", "G. Valiant"],
    "year": 2017
  }, {
    "title": "A survey of sampling from contaminated distributions",
    "authors": ["J.W. Tukey"],
    "venue": "Contributions to probability and statistics,",
    "year": 1960
  }, {
    "title": "Minimum volume ellipsoid",
    "authors": ["S. Van Aelst", "P. Rousseeuw"],
    "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,",
    "year": 2009
  }, {
    "title": "Robust pca via outlier pursuit",
    "authors": ["H. Xu", "C. Caramanis", "S. Sanghavi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }],
  "id": "SP:ea4ca2521dbf1308b19a5018cb95283438c07050",
  "authors": [{
    "name": "Ilias Diakonikolas",
    "affiliations": []
  }, {
    "name": "Gautam Kamath",
    "affiliations": []
  }, {
    "name": "Daniel M. Kane",
    "affiliations": []
  }, {
    "name": "Jerry Li",
    "affiliations": []
  }, {
    "name": "Ankur Moitra",
    "affiliations": []
  }, {
    "name": "Alistair Stewart",
    "affiliations": []
  }],
  "abstractText": "Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.",
  "title": "Being Robust (in High Dimensions) Can Be Practical"
}