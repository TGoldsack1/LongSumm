{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Support vector machines (SVM) is an established algorithm for classification with two categories (Vapnik, 1998; Smola and Schlkopf, 1998; Steinwart and Christmann, 2008; Friedman et al., 2009). The method finds the maximum margin separating hyperplane; it finds the hyperplane dividing the input space (perhaps after mapping the data to a higher dimensional space) into two categories and maximizing the minimum distance from a point to the hyperplane. SVM can also be adapted to allow for imperfect classification, in which case we speak of soft margin SVM.\nGiven the success of SVM at binary classification, many 1Harris School of Public Policy, University of Chicago, Chicago, IL, USA. Correspondence to: Guillaume A. Pouliot <guillaumepouliot@uchicago.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nattempts have been made at extending the methodology to accommodate classification with K > 2 categories (Sun et al., 2017; Dogan et al., 2016; Lopez et al., 2016; Kumar et al., 2017, survey available in Ma and Guo, 2014). Lee, Lin and Wahba (2004) propose what is arguably the natural multicategory generalization of binary SVM. For instance, their multicategory SVM (MSVM) is Fisher consistent (i.e., the classification rule it produces converges to the Bayes rule), which is a key property and motivation for the use standard SVM. Furthermore, it encompasses standard SVM as a special case.\nHowever, the method has not been widely used in application, nor has it been studied from a statistical perspective, the way SVM has been. Amongst the machine learning community, MSVM has not gathered popularity commensurate to that of SVM. Likewise, three major publications (Jiang et al., 2008; Koo et al., 2008; Li et al., 2011) have established Donsker theorems for SVM, and none have done so for MSVM.\nInterestingly, computation and statistical analysis of MSVM are hindered by the same obstable. The optimization problem which MSVM consists of is done under a sum-to-zero constraint on the vector argument. This makes both the numerical optimization task and the statistical asymptotic analysis of the estimator more challenging. The numerical optimization is substantially slowed down by the equality constraint1 as detailed in Table 1. Likewise, standard methods for deriving Donsker theorems and limit distribution theory do not apply to such constrained vectors of random variables.2\nIn a separate strain of literature, Mroueh, Poggio, Rosasco and Slotine (2012) have proposed the simplex-cone SVM (SC-SVM), a multicategory classifier developped within the vector reproducing kernel Hilbert space set-up. The SCSVM optimization program is computationally tractable, and in particular does away with the equality constraint slowing down the primal and dual formulations of MSVM (Lee\n1In fact, a “hack” sometimes used is to ignore the equality constraint in the primal or dual formulation. This can result in arbitrarily large distorsions of the optimal solution.\n2For instance, the covariance matrix of a vector of random variables constrained to sum to zero is not positive definite.\net al., 2004). Nevertheless, its use has remained marginal, arguably due to more limited interpretability, e.g., the notion of distance is captured via angles and the nesting of binary SVM as a special case is not entirely straightforward.\nAs our main contribution, we show that MSVM and SCSVM are in fact exactly equivalent. As a direct consequence, we deliver faster computations for MSVM. Simulations such as those presented in Table 1 display speed gains or an order of magnitude. Furthermore, the equivalence with an unconstrained estimator allows for statistical analysis of MSVM. As a second contribution, we deliver a Donsker theorem for MSVM, as well as an asymptotic covariance formula with sample analog. A third contribution is to use the asymptotic analysis result to propose a statistically efficient, inverse-variance weighted modification of Onevs-Rest. Finally, as a fourth contribution, we show that the asymptotic analysis allows us to partially answer, with analytic characterizations, the open question relating to the very competitive performance of the seemingly more naive One-vs-Rest method against MSVM.\nThe fourth contribution is important because it provides analytical substance to a long-standing open question. To be sure, the different attempts at developping a multicategory generalization of binary SVM can be understood as subscribing to one of two broad approaches. The first approach consists in doing multicategory classification using the standard, binary SVM. For instance, the popular One-vs-Rest approach works as follows: to predict the category of a point in a test set3 (i.e. out of sample), run K binary SVMs where the first category is one of the original K categories, and the second category is the union of the remaining K 1 categories. The predicted category is the one that was picked against all others with the greatest “confidence”. In practice, the confidence criteria used is the distance of the test point to the separating hyperplane (we show in Subsection 3.1 that even this can be improved according to statistical considerations). The second approach consists in generalizing the standard SVM to develop a single machine which implements multicategory classification solving a single, joint optimization problem. Many such algorithms have been suggested (Weston and Watkins, 1999; Crammer and Singer, 2002; Lee et al., 2004). Intuition would suggest that joint optimization makes for a more statistically efficient procedure, and for superior out-of-sample prediction performance. However, in a quite counterintuitive turn of events, it has been widely observed in practice that multicategory classification with binary machines offers a performance (for instance, in out-of-sample classification) which is competitive with, and sometimes superior to, that of single-machine multicategory SVM algorithms. This phenomenon is widely acknowledged (Rifkin and Klautau, 2004) but very little\n3Or the fitted category of a point in the training set.\ntheory has been put forth to explain it.\nWe make some progress towards an analytical characterization of the comparative performance, and are able to suggest an explanation as to the competitive, and sometimes superior, empirical performance of One-vs-Rest compared to MSVM. We argue that, in some respect, One-vs-Rest makes a more efficient use of the information contained in the data.\nThe remainder of the paper is organized as follows. Section 2 defines both MSVM and SC-SVM, and contains the proof of the equivalence between the two methods. Section 3 gives the Donsker theorem for MSVM, and describes how the asymptotic distribution may be used for more efficient classification. Section 4 suggests an analytical explanation for the surprisingly competitive performance of One-vs-Rest classifiers versus MSVM. Section 5 discusses and concludes."
  }, {
    "heading": "2. Equivalence",
    "text": "The multicategory SVM (MSVM) of Lee et al. (2004) is arguably the more elegant and natural generalization of SVM to multicategory data. However, its implementation, even for moderate size data sets, is complicated by the presence of a sum constraint on the vector argument.\nThe simplex encoding of Mroueh et al. (2012) is relieved of the linear constraint on the vector argument. However, we believe the simplex encoding is not more widely used because it is not known what standard encoding it corresponds to, making it challenging for practitioners to carry out interpretable classification analysis. The following result resolves both issues, making it of practical interest for analysts and researchers using multicategory classification methods.\nWe define MSVM and SC-SVM, and establish their equivalence. The presentation is done with finite-dimensional kernels for ease of exposition. Remark 3 details the generalization to infinite-dimensional kernels in reproducing kernel Hilbert spaces.\nWith K categories, data is of the form (x i , y i ) 2 Rp ⇥ {1, ...,K}, i = 1, ..., N . When carrying out multicategory classification, different choices of encodings of the category variables y\ni lead to optimization problems that are differently formulated and implemented.\nFor their multicategory SVM (MSVM), Lee et al. (2004) encode y\ni associated with category k 2 {1, ...,K} as a Ktuple with 1 in the kth entry and 1\nK 1 in every other entry. For instance,\n”yi in category 2” , yi = ✓\n1 K 1 , 1, 1 K 1 , · · · , 1 K 1\n◆\n.\nThe loss function they suggest is then based on the difference between the decision function and the encoded y\ni\n’s.\nSpecifically, in the case of finite-dimensional feature maps, they suggest minimizing\n1\nn\nn\nX\ni=1\nL(y i ) · [Wx i + b y i\n]+ +\n2\n|||W |||, (1)\nwhere |||W ||| = trace(WTW ), and L(y i ) = 1 K e yi is a vector that has 0 in the kth entry when y i\ndesignates category k, and a 1 in every other entry. Importantly, the decision function is constrained to sum to zero, i.e. 1T\nk (Wx+ b) = 0, 8 x. The function [·]+applies pointwise to its vector argument.\nMroueh et al. (2012) preconize an encoding that does away with the sum-to-zero constraint. The loss function they suggest is based on the inner product between the decision function and their encoding of y\ni ’s. Likewise in the finite-dimensional case, the penalized minimization problem entailed by their loss function is\n1\nn\nn\nX\ni=1\nX\ny 0 6=y\n\n1 K 1 + D c y 0 , ˜Wx i + ˜b E\n+\n+\n˜\n2\n||| ˜W |||,\n(2)\nwhere c y is a unit vector in RK 1 which encodes the response; it is a row of a simplex coding matrix, which is the key building block of their construction.\nA simplex coding matrix (Mroueh et al., 2012; Pires et al., 2013) is a matrix C 2 RK⇥(K 1) such that its rows c\nk\nsatisfy (i) kc k k22 = 1; (ii) cTi cj = 1 K 1 for i 6= j ; and (iii) P K\nk=1 ck = 0K 1. It encodes the responses as unit vectors in RK 1 having maximal equal angle with each other. Further note that, because its domain is a (K 1)- dimensional subspace of RK , any given C has a unique inverse operator ˜C defined on the image {x 2 RK : 1T\nK x = 0}.\nFor a given choice of simplex encoding defined by C, the operator C : RK 1 ! RK can be thought of as mapping decision functions and encoded y’s from the unrestricted simplex encoding space to the standard, restricted encoding space used by Lee et al. (2004).\nA natural question is then: if f(x) = Wx+ b and ˜f(x) = ˜Wx + ˜b are optimal solutions to (1) and (2), respectively, are ˜C (Wx+ b) and C( ˜Wx+˜b) then optimal solutions to (2) and (1), respectively? We show that this is in fact the case. That is, both problems are exactly equivalent.\nWe now show the problems are equivalent. The equivalence\nof the loss functions is straighforward. Indeed,\nP\ny0 6=y\nh\nfy0(x) + 1\nK 1\ni\n+ =\nP\ny0 6=yi\n\n⇣\nC\n˜ f(x)\n⌘\ny0 +\n1 K 1\n+\n=\nP\ny0 6=yi\nhD\ncy0 , ˜ f(x)\nE\n+ 1 K 1\ni\n+ ,\n(3)\nwhich is exactly the SC-SVM loss of Mroueh et al. (2004). Writing out f and ˜f as linear functions, the identity becomes\nP\ny0 6=y\nh\n!y0x+ by0 + 1\nK 1\ni\n+\n=\nP\ny0 6=yi\nhD\ncy0 , ˜ Wx+ ˜ b\nE\n+ 1 K 1\ni\n+\n(4)\nwith f(x) = Wx + b and ˜f(x) = ˜Wx + ˜b, and ! y 0 is the (y0)th row of W .\nEquality (up to a change of tuning parameter) of the penalty relies on the key observation of this exercise, which is that CTC is the diagonal matrix K\nK 1IK 1. It then immediately follows that\nK 1 K trace\n⇣\n˜\nW\nT ˜\nW\n⌘\n= K 1 K trace W T C T CW\n= trace W T W .\n(5)\nIn conclusion, we have\n1\nn\nn X\ni=1\nL(yi) · [Wxi + b yi]+ +\n2\n|||W |||\n=\n1\nn\nn X\ni=1\nX\ny0 6=y\n\n1 K 1 + D c 0 y, ˜ Wx+ ˜ b E\n+\n+ (K 1) 2K ||| ˜W |||,\n(6) as desired.\nWe now prove the key linear algebra result. There are other ways (see remarks below) to prove this result. However, it is desirable to establish the equivalence between the more practical encoding and the more interpretable one in an intuitive way. The geometric proof given below accomplishes this by establishing the equivalence through a volume preservation argument.\nPROPOSITION\nLet C 2 RK⇥(K 1) be a simplex coding matrix. Then its columns are orthogonal and have norm q K\nK 1 .\nProof\nThe key observation (Gantmacher, 1959, vol. 1, p.251) is that\nV = p G, (7)\nwhere V = V (C) is the volume of the parallelipiped spanned by the columns of C, and G = G(C) is the Grammian of C. The Grammian (defined below) extends the notion of volume to objects determined by more vectors than the space they are embedded in has dimensions.\nLet C·i denote the ith column of C, and recall that |||C||| denotes the sum of the squared entries of C. Note that V  kC·1k · · · · · C·(K 1)\n, which holds with equality if and only if all columns are mutually orthogonal. Further note that\nkC·1k·· · ·· C·(K 1) \nr\n|||C||| K 1\n! K 1\n=\n✓\nK\nK 1\n◆ K 1 2\nwhich holds with equality if and only if kC·ik = q\nK K 1 , i = 1, ...,K 1. Hence, if G = ⇣ K K 1 ⌘ K 1 ,\nit must be that the statement of the proposition is true.\nWe compute the Grammian. By Gantmacher (1959),\nG(C) = K X\ni=1\ndet 2 (C i·) , (8)\nwhere C i· is C with the ith row removed. Noting that C i·CT i· is a circulant matrix and using the relevant determinant formula, we find that\nP\nK i=1 det C i·CT i·\n= K · det\n0\nB\n@\n1 1 K 1\n. . . 1\nK 1 1\n1\nC\nA\n= K · Q K 2 j=0\n⇣\n1 1 K 1 P K 2 m=1\n⇣\ne 2⇡ij K 1\n⌘\nm\n⌘\n= K · ⇣\n1 K 2 K 1\n⌘\n· Q K 2 j=1\n⇣\n1 1 K 1 P K 2 m=1\n⇣\ne 2⇡ij K 1\n⌘\nm\n⌘\n= K · ⇣\n1 K 1\n⌘\n· Q K 2 j=1\n⇣\n1 + 1 K 1\n⌘\n= K · ⇣\n1 K 1\n⌘ · ⇣ K\nK 1\n⌘ K 2\n=\n⇣\nK\nK 1\n⌘ K 1 ,\nwhich proves the claim.\nNote that we have used the orthogonality of the complex exponential basis,\nn 1 X\nm=0\ne 2⇡ijm n =\n⇢\nn, j mod n = 0 0, o.w. .\n⇤ The immediate implication of the above argument and proposition is that we may compute MSVM using the equivalent, unconstrained representation of SC-SVM. In Table 1, we display “clock-on-the-wall” computation times. Collected simulations suggest gains of an order of magnitude.\nRemark 1 The result of the Proposition holds for a more general simplex matrix C 2 RK⇥D, 0 < D < K, having rows of equal norm and maximal equal angle between them.\nRemark 2 A different argument of a more algebraic geometry flavor can be given, which suggests the choice of a canonical C. Given K, there exists a simplex coding matrix C such that pairwise coordinate projections (i.e. projections on a plane spanned by two distinct standard basis vectors) yield equidistant points around a circle (“a pie with equal sized slices”). This is trivial for K = 3, and geometrically obvious for K = 4. Call such a simplex coding matrix a canonical coding matrix. From this geometric observation, the orthogonality of the columns readily follows: for any two disctinct columns of C, say C·i, C·j , i 6= j, we have that\nhC·i, C·ji = K X\nt=1\ncos\n✓\nt⇡\nK/2\n◆\nsin\n✓\nt⇡\nK/2\n◆\n=\n1\n2\nK\nX\nt=1\nsin\n✓\nt⇡\nK/4\n◆\n= 0.\nThe length of the columns can be established as in the proof of the Proposition. Furthermore, and somewhat surprisingly, we can go the other way and construct C from the condition on its pairwise coordinate projections (Chan, 2013).\nRemark 3 The equivalence of MSVM and SC-SVM immediately generalizes to the infinite-dimensional kernel case. The representer theorem yields that f\nj (x) = b j +\nP\nn i=1 aijK(xi, x) for j = 1, ...,K with sum-to-zero constraint. Then (3) holds in the same notation. Letting A denote the matrix with (i, j) entry a\nij and K the matrix with (i, j) entry K(x\ni , x j ), the penalty equivalence follows from observing that\ntrace(ATKA) = trace(C ˜ATK ˜ACT )\n= trace(CTC ˜ATK ˜A) = K\nK 1trace( ˜ATK ˜A).\nWe then get, again, equality of the objective functions up to the tuning parameter."
  }, {
    "heading": "3. Donsker Theorem",
    "text": "By considering MSVM as penalized M -estimator, one can in principle work out its asymptotic distribution. In (2), under simplex encoding, MSVM is phrased as an unconstrained M -estimator, and the asymptotic distribution for the estimated parameters –and thus separating hyperplane– can be obtained using standard empirical process theory. The expression for the covariance matrices presented below are novel and of practical use. To the best of my knowledge, if a practitioner wants to compute the asymptotic covariance matrix of SVM or MSVM –which is essential in order to know where extrapolation is reliable– this article is the only resource displaying worked out expressions with sample analogs.4\nOne readily obtains (Van der Vaart, 2008) a standard central limit theorem result of the form p n ⇣ ˆ ˜ ⇥\nn\n˜⇥⇤ ⌘\nd! N(0, H 1Multi⌦MultiH 1 Multi), (9)\nwhere ˜⇥ = (vec( ˜W )T ,˜b)T , the information matrix ⌦Multi is\nE\n0\n@\nX\ny0 6=y\nc T y01\nnD\ncy0 , ˜ f\nE\nã o\n1\nA\n0\n@\nX\ny0 6=y\ncy01 nD cy0 , ˜ f E\nã o\n1\nA\n⌦ ⇣ (x T , 1) T (x T , 1) ⌘ ,\nand the Hessian HMulti is\nEy\n2\n4\nX\ny0 6=y\n⇣\nc T y0cy0\n⌘\np\n⇣ D\ncy0 , ˜ b\nE ã ⌘\n⌦E h (x T , 1) T (x T , 1) D\ncy0 , ˜ f\nE = ã, y ii .\nBoth are evaluated at ˜⇥⇤, ˜f = ˜f(x), and ã = 1 K 1 , and p = p hcy0 ,W̃x+b̃i|y is the density of D c y 0 , ˜f E\nconditional on y. Derivations are given in the online appendix.\n4Koo et al. (2008) and Jiang et al. (2008) do not provide expressions with sample analogs."
  }, {
    "heading": "3.1. Efficient Classifiers",
    "text": "SVM are most commonly used for classification and prediction tasks. Accordingly, the most immediate practical use for an estimate of the variance of the separating hyperplane is the construction of a more accurate classifier.\nConsider the One-vs-Rest method, for instance. The Onevs-Rest method fits K hyperplanes, which in the linear case are defined by (!\ni , b i ) 2 Rp+1, and categorizes a point by attributing it to the category in which it is the “deepest”. That is,\nŷnew = argmax k\nn\n!̂T k xnew +ˆbk o .\nHowever, studentized distances yield more sensible and reliable classifications by accounting for the comparative uncertainty of the hyperplanes when categorizing a given point. Naturally, a point being ”deeper” with respect to a classifying hyperplane –in terms of the length of the line from the point to the hyperplane and normal to the hyperplane– should make one more confident in the classification if it occurs in a section of the space where the hyperplane has lower variance. In sections with high variance, the distance could be much smaller in resamplings of the data. Accordingly, we suggest the following efficient categorization rule\nŷ⇤new = argmax k\n(\n!̂T k xnew +ˆbk p\n(xTnew, 1)⌃k(x T new, 1)\n)\n,\nwhere ⌃ k is the asymptotic variance of (!̂ k ,ˆb k ), or a consistent estimate. An analog modification can be applied to make the MSVM procedure more efficient."
  }, {
    "heading": "4. Efficiency of One-vs-Rest",
    "text": "Explaining the surprisingly competitive performance of the naive One-vs-Rest approach, comparatively to the more sophisticated MSVM approach, is an important open question. The phenomenon is detailed and documented empirically in Rifkin and Klautau (2004) and is well established in the machine learning folklore. However, there are practically no theoretical results in the way of an explanation. In this section, we consider this question from the asymptotic statistics perspective and argue that the competitive performance of One-vs-Rest may be explained by a more efficient use of information.\nThe idea is to consider the full One-vs-Rest method as a single M -estimator and to artificially impose a sum-to-zero constraint on the decision function. I can use the simplex encoding and obtain the (joint) asymptotic variance of the K separating hyperplanes in the form H 11vsR⌦1vsRH 1 1vsR.\nNote that I pick the geometric margin to be 1 K 1 , rather than 1 in the standard form for binary (and thus One-vs-\nRest) SVM. The loss function for One-vs-Rest in simplex encoding is\nK\nX\nk=1\n1{y = k} ·  1\nK 1 D c k , ˜Wx+˜b E\n+\n(10)\n+1{y 6= k} ·  1\nK 1 + D c k , ˜Wx+˜b E\n+\n!\nwhich is minimized in ˜W and ˜b. The first summand penalizes classification for which the point x is not sufficiently far from the hyperplane within the true category. This is where we speak of using the information from a point’s “own” category. The second summand penalizes classifications for which the point x is not sufficiently far from the hyperplane away from the wrong category. This is where we speak of using the information from “other” categories.\nThe sum-to-zero constraint is added for analytical reasons; we need it to make the covariance matrices comparable. It will be apparent that the analytical conclusion is robust to this modification.\nThe information matrix ⌦1vsR is\nE\n⇣\nc T y 1{ã\nD\ncy, ˜ f\nE 0} ⌘⇣ cy1{ã D cy, ˜ f E 0} ⌘\n⌦(xT , 1)T (xT , 1)\n2E(cTy 1{ã D cy, ˜ f E 0})( X\ny0 6=y\ncy01{ã+ D ck0 , ˜ f E 0})\n⌦(xT , 1)T (xT , 1)\n+E(\nX\ny0 6=y\nc T y01{ã+\nD\nck, ˜ f\nE\n0})( X\ny0 6=y\ncy01{ã+ D ck0 , ˜ f E 0})\n⌦(xT , 1)T (xT , 1),\nand the Hessian H1vsR is\nEy\nh\n(c T y cy)\n⇣\np\n⇣D\ncy, ˜ b\nE ã ⌘⌘\n⌦E h (x T , 1) T (x T , 1) D\nck, ˜ f\nE\n= ã, y\nii\n+Ey\n2\n4\nX\ny0 6=y\n(c T y0cy0)\n⇣\np\n⇣ D\ncy0 , ˜ b\nE ã ⌘⌘\n⌦E h (x T , 1) T (x T , 1) D\ncy0 , ˜ f\nE = ã, y ii .\nWe get instructive comparisons. First of all, HMulti < H1vsR. That is, the one-vs-rest problem has more “curvature” than the MSVM. Indeed, it is clear from inspection5\n5Note the addition of a y = y0 positive summand.\nthat this comes from the one-vs-rest procedure using information from the “own category”, while MSVM doesn’t as it only uses information with respect to “other” categories.\nIt is clear from the comparison of the loss functions (2) and (10), corresponding to SC-SVM and the simplex encoding of One-vs-Rest, respectively, that both penalize for an observation that falls within the half-space assigned to an “other” category, but only One-vs-Rest rewards for points falling within their true, “own” category. It was not clear, however, if the rewarding a point for being in its “own” category is still informative and not redundant when it is already penalized if it is in any “other” category. However, in spite of imposing an additional constraint on the solution space of the One-vs-Rest problem, we do find from inspection of the Hessian that the additional information from rewarding classification of points within their “own” category is informative and not redundant. Although this was not obvious a priori, it is revealed by the statistical asymptotic analysis.\nFurthermore, in the special case of a separable data generating process (DGP), that is in the case in which 1{ã D\nc y\n, ˜f E\n0} = 0 a.s., we get that ⌦Multi = ⌦1vsR and both procedures have the same target hyperplane. Therefore, One-vs-Rest is strictly more statistically efficient than multicategory when the DGP is separable. In this specific case, this translates into smaller expected prediction error. We have displayed a case, that of perfect seprarability, where One-vs-Rest (with simplex encoding) provably dominates MSVM.\nIn non-separable cases, this dominance may not hold. In fact, we expect MSVM to outperform One-vs-Rest in some cases due to the more efficient gathering, by joint optimization, of the information from the “other” categories."
  }, {
    "heading": "5. Discussion and Conclusion",
    "text": "We established rigorously, and with a proof conveying geometric intuition, the equivalence of MSVM and SC-SVM. This provides a formulation of the optimization problem for computing MSVM which is relieved of the sum-to-zero constraint that bogged down computations in the implementations as suggested in Lee et al. (2004). Our hope is that availablity of faster computations for MSVM will encourage applied researchers and analysts to employ MSVM in multicategory classification tasks. We gave the first central limit theorem for MSVM, along with an asymptotic covariance formula having a sample analog, which is a new result even for binary SVM. The variance formula allows for the construction of studentized decision functions for One-vs-Rest procedures, improving their accuracy and statistical efficiency. These make for more reliable classification, especially for extrapolation. We gave an analytical characterization of the surprisingly good performance of the\nOne-vs-Rest procedure, comparatively to MSVM, using the asymptotic distribution of estimators. We hope this line of study fosters further research."
  }, {
    "heading": "Acknowledgements",
    "text": "I would like to thank Lorenzo Rosasco for introducing me to the material studied in this article and for his support throughout this project. Different angles for studying SVM methods as M -estimators arose in stimulating conversation with Isaiah Andrews. Jann Spiess read an early draft of the article and made helpful comments. Jules MarchandGagnon collaborated on a cousin project and contributed insights which this article bears the mark of."
  }, {
    "heading": "Appendix: Construction of the Simplex Encoding Matrix C",
    "text": "It is straightforward to build a function that takes K and outputs the simplex encoding matrix C. We give intuitive pseudocode for the general K case. A code file is available in the online appendix.\nThe construction relies on mapping vectors in spherical coordinates to their representation in Cartesian coordinates. The function StoC : RK 2 ! RK 1 does just that.\nStoC function( 1, 2, ..., K 2){ v1 = cos( 1)\nv2 = sin( 1) cos( 2)\nv3 = sin( 1) sin( 2) cos( 3)\n... v K 2 = sin( 1) · · · sin( K 3) cos( K 2) v K 1 = sin( 1) · · · sin( K 3) sin( K 2)\nv = (v1, ..., vK 1)\nv\n}\nUsing the StoC function, it is now easy to construct the simplex encoding matrix. This can be done with the function\nC : K 7! RK⇥(K 1) , which we now describe.\nC function(K){ C1,· = StoC(0, 0, ..., 0)\nC2,· = StoC\n✓\nacos\n✓\n1 K 1\n◆\n, 0, ..., 0\n◆\nC3,· = StoC\n✓\nacos\n✓\n1 K 1\n◆\n, acos\n✓\n1 K 2\n◆\n, 0, ..., 0\n◆\n...\nC K 1,· = StoC\n✓\nacos\n✓\n1 K 1\n◆\n, acos\n✓\n1 K 2\n◆\n,\n..., acos\n✓\n1 3\n◆\n, acos\n✓\n1 2\n◆◆\nC K,· = StoC\n✓\nacos\n✓\n1 K 1\n◆\n, acos\n✓\n1 K 2\n◆\n,\n..., acos\n✓\n1 3\n◆ , 2 · acos ✓\n1 2\n◆◆\nC\n}"
  }],
  "year": 2018,
  "references": [{
    "title": "Grobner Bases over fields with Valuations and Tropical Curves by Coordinate Projections",
    "authors": ["A.J. Chan"],
    "venue": "PhD Thesis, University of Warwick,",
    "year": 2013
  }, {
    "title": "On the algorithmic implementation of multiclass kernel-basd vector machines",
    "authors": ["K. Crammer", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2001
  }, {
    "title": "A unified view on multi-class support vector classification",
    "authors": ["Dogan", "Urn", "Tobias Glasmachers", "Christian Igel"],
    "venue": "Journal of Machine Learning Research",
    "year": 2016
  }, {
    "title": "The elements of statistical learning",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "venue": "Springer, Berlin: Springer series in statistics,",
    "year": 2009
  }, {
    "title": "The Theory of Matrices, Chelsea",
    "authors": ["F.R. Gantmacher"],
    "venue": "New York,",
    "year": 1959
  }, {
    "title": "Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers",
    "authors": ["B. Jiang", "X. Zhang", "T. Cai"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "All-in-one multicategory least squares nonparallel hyperplanes support vector machine",
    "authors": ["Kumar", "Deepak", "Manoj Thakur"],
    "venue": "Pattern Recognition Letters",
    "year": 2017
  }, {
    "title": "Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data",
    "authors": ["Y. Lee", "L. Yin", "G. Wahba"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2004
  }, {
    "title": "Principal Support Vector Machines for Linear and Nonlinear Sufficient Dimension Reduction",
    "authors": ["B.B. Li", "A. Artemiou", "L. Li"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "A novel multi-class SVM model using second-order cone constraints",
    "authors": ["Lopez", "Julio", "Sebastin Maldonado", "Miguel Carrasco"],
    "venue": "Applied Intelligence",
    "year": 2016
  }, {
    "title": "Support vector machines applications",
    "authors": ["Ma", "Yunqian", "Guodong Guo", "eds"],
    "year": 2014
  }, {
    "title": "Multiclass Learning with Simplex Coding",
    "authors": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-J.E. Slotine"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Cost-sensitive multiclass classification risk bounds",
    "authors": ["B.A. Pires", "C. Szepesvari", "M. Ghavamzadeh"],
    "venue": "Proceedings of The 30th International Conference on Machine Learning",
    "year": 2013
  }, {
    "title": "In defense of one-vs-all classification",
    "authors": ["R. Rifkin", "A. Klautau"],
    "venue": "The Journal of Machine Learning Research",
    "year": 2004
  }, {
    "title": "Learning with kernels",
    "authors": ["A.J. Smola", "B. Schlkopf"],
    "venue": "GMDForschungszentrum Informationstechnik,",
    "year": 1998
  }, {
    "title": "Support vector machines",
    "authors": ["I. Steinwart", "A. Christmann"],
    "venue": "Springer Science & Business Media,",
    "year": 2008
  }, {
    "title": "Anglebased multicategory distance-weighted SVM",
    "authors": ["Sun", "Hui", "Bruce A. Craig", "Lingsong Zhang"],
    "venue": "The Journal of Machine Learning Research",
    "year": 2017
  }, {
    "title": "Statistical learning theory",
    "authors": ["V.N. Vapnik"],
    "venue": "Vol. 1. New York: Wiley,",
    "year": 1998
  }, {
    "title": "Support vector machines for multi-class pattern recognition",
    "authors": ["J. Weston", "C. Watkins"],
    "venue": "In ESANN,",
    "year": 1999
  }],
  "id": "SP:e8dbe395841662a927bcc01188920e20cd310f32",
  "authors": [{
    "name": "Guillaume A. Pouliot",
    "affiliations": []
  }],
  "abstractText": "The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary support vector machines (SVM). However, its use has been limited by computational difficulties. The simplex-cone SVM (SCSVM) of Mroueh et al. (2012) is a computationally efficient multicategory classifier, but its use has been limited by a seemingly opaque interpretation. We show that MSVM and SCSVM are in fact exactly equivalent, and provide a bijection between their tuning parameters. MSVM may then be entertained as both a natural and computationally efficient multicategory extension of SVM. We further provide a Donsker theorem for finite-dimensional kernel MSVM and partially answer the open question pertaining to the very competitive performance of One-vs-Rest methods against MSVM. Furthermore, we use the derived asymptotic covariance formula to develop an inverse-variance weighted classification rule which improves on the One-vs-Rest approach.",
  "title": "Equivalence of Multicategory SVM and Simplex Cone SVM:  Fast Computations and Statistical Theory"
}