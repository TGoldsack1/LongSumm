{
  "sections": [{
    "heading": "1. Introduction",
    "text": "It has long been known that solutions obtained from optimization methods can demonstrate striking sensitivity to the parameters of the problem (Bertsimas et al., 2011). Robust optimization, in contrast, is a paradigm in the mathematical programming community with the aim of safeguarding the solutions from the changes in the underlying parameters.\nIn this paper, we consider submodular maximization, a very well studied discrete optimization problem defined over a finite set of items (e.g., images, videos, blog posts, sensors, etc). Submodularity formalizes the notion of diminishing returns, stating (informally) that selecting an item earlier results in a higher utility than selecting it later. This notion has found far-reaching applications in machine learning\n1Department of Computer Science, Yale University, New Haven, Connecticut, USA 2Google Research, Zurich, Switzerland. Correspondence to: Ehsan Kazemi <ehsan.kazemi@yale.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n(Bach et al., 2013), web search and mining (Borodin et al., 2017), social network (Kempe et al., 2003), crowdsourcing (Singla et al., 2016), and user modeling (Yue & Guestrin, 2011), to name a few. However, almost all the existing methods for submodular maximization, ranging from centralized (Nemhauser et al., 1978; Feldman et al., 2017) to streaming (Badanidiyuru et al., 2014; Feldman et al., 2018), to distributed (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015), rely on greedy selection of elements. As a result, the returned solution of such methods are remarkably sensitive to even a single deletion from the set of items.\nThe need for efficient deletion-robust optimization methods is wide-spread across many data-driven applications. With access to big and massive data (usually generated by millions of users), along with strong machine learning techniques, many service providers have been able to exploit these new resources in order to improve the accuracy of their data analytics. At the same time, it has been observed that many such inference tasks may leak very sensitive information about the data providers (i.e., personally identifiable information, protected health information, legal or financial data, etc). Similarly these algorithms can encode hidden biases that disproportionately and adversely impact members with certain characteristics (e.g., gender and race).\nIn order to reduce the effect of information extraction on privacy and fairness, one needs to be able to remove sensitive data points (e.g., geolocations) or discard sensitive data features (e.g., skin color) from the dataset without incurring too much loss in performance. For instance, Article 17 of European “General Data Protection Regulation” states obligations with respect to providing individuals with the “Right to erasure (or Right to be forgotten)”. By exercising this right, individuals may enforce the service providers to delete their personal data or put restrictions from using part of it. Similarly, Title VII of the Civil Rights Act of American anti-discrimination law prohibits employment discrimination against certain characteristics (such as color and sex). Thus, to obtain fairer machine learning algorithms, we need to reduce the bias inherent in the training examples due to the lack of certain types of information, not being representative, or reflecting historical biases. This can be done by either removing protected attributes from training data\n(Zemel et al., 2013) or train them separately for different protected groups (Chayes, 2017), among other procedures. Unfortunately, sensitive features or biased data usually are not known a priori and we might be aware of their existence just after training our models (Beutel et al., 2017). Retraining a machine learning model from scratch, after removing sensitive features and biased data, is quite expensive for large datasets. Deletion-robust submodular maximization can save a lot of time and computational resources in these scenarios. In this paper, we provide a computationally feasible way of rerunning the algorithms should some attributes or data points be discarded.\nMost existing submodular maximization methods, often used for data extraction (Mirzasoleiman et al., 2013) and informative subset selection (Wei et al., 2015), do not provide such guarantees. In this paper, we develop the first scalable and memory-efficient algorithms for maximizing a submodular function subject to a cardinality constraint that are robust against any number of adversarial deletions. This is in sharp contrast to previous methods that could only handle a fixed number of deletions (Orlin et al., 2016; Bogunovic et al., 2017) or otherwise their memory requirement scales multiplicatively with the number of deletions (Mirzasoleiman et al., 2017).\nOur contributions: For a monotone submodular function with a cardinality constraint k, we develop the following randomized algorithms that are robust against any d deletions: 1. Centralized: We propose ROBUST-CENTRALIZED that achieves (1/2 )-approximation guarantee (in expectation) with the memory requirement O (k + d log k/ 2). Note that the memory complexity is only a logarithmic factor (e.g., log k) away from a trivial lower bound O(k + d). 2. Streaming: We propose ROBUST-STREAMING that achieves (1/2 )-approximation guarantee (in expectation) with the memory requirement O k log k/ + d log2 k/ 3 .\n3. Distributed: We propose ROBUST-DISTRIBUTED that achieves (0.218 )-approximation guarantee (in expectation) with the memory requirement O (m(k + d log k/ 2)), where m is the number of machines. We also introduce COMPACT-DISTRIBUTED, a variant of ROBUSTDISTRIBUTED, where its memory requirement is independent of number of machines.\nTable 1 compares our proposed methods with previous algorithms. The proofs of all the theoretical results are deferred to the Supplementary Material."
  }, {
    "heading": "2. Related Work",
    "text": "Monotone submodular maximization under cardinality constraints is studied extensively in centralized, streaming and distributed scenarios. The classical result of Nemhauser et al. (1978) proves that the simple GREEDY algorithm that starts with an empty set and iteratively adds elements with\nthe highest marginal gain provides (1 1/e)-approximation guarantee. To scale to large datasets, several streaming algorithms with constant factor approximations have recently been proposed (Badanidiyuru et al., 2014; Kumar et al., 2015; Buchbinder et al., 2015). Also, different distributed submodular maximization algorithms have been developed lately (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015).\nKrause et al. (2008) introduced the robust formulation of the classical cardinality constrained submodular maximization for the first time and gave a bi-criterion approximation to the problem of max|A|k mini2{1,··· ,`} fi(A), where fi is normalized monotone submodular for every i. Note that submodular maximization of function f that is robust to the deletion of d items can be modeled as a special case of this problem: max|A|k min|D|d f(A \\D). Krause et al. (2008) guaranteed a robust solution by returning a set whose size is k(1 + ⇥(log(dk log n)). There are two main drawbacks with this approach when applied to deletions: first, the size of final solution is logarithmically larger than k, and second, the running time is exponential in d. Orlin et al. (2016) designed a centralized algorithm that outputs a set of cardinality k in a polynomial time. Their algorithm is robust to the deletion of only o( p k) elements. Bogunovic et al. (2017) further improved the result of Orlin et al. (2016) to o(k) deletions. The approximation guarantees for both of these algorithms are 0.387. The aforementioned methods try to construct a solution without allowing to update the answer after deletion. In contrast, Mirzasoleiman et al. (2017) developed a streaming algorithm which is robust to the deletion of any number of d elements. They keep a set of size O(kd log k/ ), and after each deletion they find a feasible solution of size at most k from this set. They also improved the approximation guarantee to 1/2 . The main drawback of this algorithm is the memory requirement, which is quite impractical for large values of d and k; e.g., for k = O( p n) and d = O( p n) the memory requirement is even larger than n. Independently and concurrently with our work, Mitrovic et al. (2017) presented a robust to deletion streaming algorithm. Also, there are several recent works on robust optimization of non-submodular functions (Bogunovic et al., 2018; Tzoumas et al., 2018).\nSubmodular maximization has been widely used in classical machine learning and data mining applications, including extracting representative elements with exemplar based clustering (Krause & Gomes, 2010), data summarization through active set selection (Herbrich et al., 2003; Seeger, 2004), feature selection (Krause & Guestrin, 2005) and document summarization (Lin & Bilmes, 2011)."
  }, {
    "heading": "3. Problem Definition",
    "text": "Assume we have a set function f : 2V ! R 0. We define the marginal gain of an element e 2 V to the set A ✓ V\nby f (e|A) = f(A [ {e}) f(A). The function f is submodular if for all A ✓ B ✓ V and e 2 V \\ B, we have f (e|A) f (e|B). A submodular function f is monotone if for every A ✓ B ✓ V , we have f(A)  f(B).\nIn many submodular optimization applications, a subset of items of the ground set V may be removed at different points in time. For this reason, we require to find solutions which are robust to the deletion. Indeed, the goal is to maximize a submodular function f over a set V of items under a cardinality constraint k, where it is robust to the deletion of any subset D ⇢ V of size |D|  d. More precisely, we are interested in solving the following problem for each possible (and unknown a priori) instance of D:\nS ⇤ = argmax\nS✓V \\D,|S|k f(S). (1)\nWe also define OPT = f(S⇤). The most straightforward approach to this problem is to solve Eq. (1) for each instance of D. Unfortunately, solving Eq. (1), for large datasets, is computationally prohibitive. Also, deletion of elements from the set V can happen at different stages in real time applications. This makes the problem even harder. Our solution to this problem is to maintain a small set A ⇢ V, called a core-set of V, where for each set D we can efficiently find a subset B ✓ A \\D that provides an acceptable approximation for Eq. (1). Note that set A is constructed without knowing set D. For this reason, next we define the notion of (↵, d)-robust randomized core-set. Definition 1. A random subset of A ✓ V is an (↵, d)-robust randomized core-set for a set V, if for any subset D ✓ V of size |D|  d, there exists a B ✓ A \\D, |B|  k such that\nE[f(B)] ↵ · max S✓V \\D,|S|k f(S),\nwhere expectation is taken over the randomization of set A."
  }, {
    "heading": "4. Robustness and Cardinality Constraint",
    "text": "In this section, we present three fast and scalable randomized algorithms. These algorithms solve the problem of robust submodular maximization in centralized, streaming and distributed scenarios. Our algorithms provide, in expectation, constant factor approximation guarantees, where they\nare robust to the (even adversarial) deletion of any d items from the set V. In our setting, an adversary might try to find a set of inputs for which our algorithms fail to provide good results. In order to make the optimization robust to the adversarial deletions, we introduce randomness in the selection process. We assume that the adversary does not have access to the random bits of the randomized algorithms.\nThe proposed algorithms are designed based on a general idea that the elements are chosen randomly from a large enough pool of similar items. This idea is useful because the adversary is not aware of the random bits of the algorithms, which makes the deletion probability of elements we have chosen negligible. Therefore, we can bound the expected value of each selected set.\nOur solution consists of two steps. In the first step, we find a small core-set of elements (in comparison to the whole dataset). We prove that after the deletion of at most d arbitrary elements, we can still find a good approximation for the optimization problem in this small set. In the second step, we choose at most k elements from the core-set we have found in the first step. We prove a constant approximation factor for our algorithm in expectation. This guarantees that the core-set is (↵, d)-robust randomized for a constant ↵ and arbitrary d.\nIn the optimization procedure, we use a thresholding idea to select elements. Similar ideas have been used previously for designing streaming algorithms (Badanidiyuru et al., 2014; Buchbinder et al., 2015; Chekuri et al., 2015). In those algorithms, when an element of the stream arrives, if this element has sufficiently large marginal value it is kept; otherwise it is discarded. In the robust submodular maximization, we keep a large enough pool of elements with sufficient marginal values before adding or discarding them. We randomly pick an element when the size of pool is at least d/✏. Thus the element picked at each step is deleted with a probability at most ✏. This is true because the size of deleted items is at most d. To guarantee the quality of the chosen elements after the deletion (i.e., we want the expected value of f over the set of picked elements does\nnot change a lot after deletion), not only they should have been picked from a large pool of elements, the elements of pool should have almost the same marginal gains. To explain, in more details, why we need this property consider the example in Appendix A."
  }, {
    "heading": "4.1. Centralized Algorithm",
    "text": "In this section we outline a centralized algorithm, called ROBUST-CORESET-CENTRALIZED, to find an (↵, d)-robust core-set. We also present the ROBUST-CENTRALIZED algorithm which is able to find a good solution from the core-set.\nBadanidiyuru et al. (2014) showed that one way to obtain a constant factor approximation to the classical submodular maximization problem is to use a thresholding idea. They proved that choosing elements with marginal gain at least ⌧ ⇤ = OPT2k from a stream until a maximum of k elements are chosen returns a set with an approximation factor of 1/2. The main problem with this primary idea is that the value of OPT is not known by the algorithm. Badanidiyuru et al. (2014) pointed out that, from the submodularity of f , we have 0  OPT  k 0 where 0 is the largest value in set {f({e})|e 2 V }. By dividing the range [ 0, k 0] into intervals of [⌧i, ⌧i+1) (where ⌧i+1/⌧i is close to 1) it is possible to find a good enough approximation for OPT.\nWe should first note that due to the deletion process, the relevant maximum singleton value is not 0 anymore, and it is 00 = maxe2V \\D f({e}). The algorithm is unaware of set D, therefore 00 could be anywhere in the range [ d, 0] where d is the (d+ 1)-th largest value in the set {f({e})|e 2 V }. The lower bound of d is implied by the fact that at most d elements will be deleted. So ⌧⇤ = OPT2k could fall anywhere in the range [ d/2k, 0]. Unlike the deletion free case, the upper and lower limits of this range do not differ only by a multiplicative factor of k, thus a naive approach makes us try arbitrarily large number of different choices to find a good estimate of ⌧⇤. We resolve this issue by the following observation.\nWe reserve a set B of elements that might be valuable after the deletion process. Let Vd be the (d + 1) largest singleton value elements, i.e., the top d + 1 elements in the set {f({e})|e 2 V }. We preserve all elements of Vd for the next round by inserting them to B. This way, we do not have to worry about thresholds above d as all elements that might have marginal value above d to any set should be in set Vd and they are added to B. Therefore, we consider all thresholds in the set T = {(1 + ✏)i| d2k  (1 + ✏)\ni  d}. Starting from the largest ⌧ 2 T to the smallest, we iteratively construct two sets A⌧ and B⌧ . At the end of the algorithm, the set B is defined as the union of Vd and [⌧2TB⌧ . We output set B, along with all sets {A⌧}⌧2T, as the core-set.\nWe initialize A⌧ to ?. We let B⌧ to be the set of elements whose marginal values to the set [⌧ 0 ⌧A⌧ 0 is in the range\nAlgorithm 1 ROBUST-CORESET-CENTRALIZED\n1: d the (d+ 1)-th largest value of {f({e})|e 2 V } 2: Vd all the d+ 1 elements with the largest values in\nset {f({e})|e 2 V } 3: T = {(1 + ✏)i| d2(1+✏)k  (1 + ✏)\ni  d} 4: For each ⌧ 2 T : {A⌧} ? and {B⌧} ? 5: V V \\ Vd 6: for ⌧ 2 T from the highest to the lowest do 7: while |B⌧ | d/✏ for B⌧ = {e 2 V : ⌧ \nf (e|[⌧ 0 ⌧ A⌧ 0) < (1+ ✏)⌧} and |[⌧ 0 ⌧ A⌧ 0 | < k do\n8: Randomly pick an element e from B⌧ and add it to A⌧ , i.e., A⌧ A⌧ [ {e}\n9: V V \\ (A⌧ [B⌧ ) 10: B {[B⌧} [ Vd 11: Return {A⌧}, B\n[⌧, (1 + ✏)⌧). We note that this is a dynamic definition and whenever we add an element to any of A⌧ sets, the related B⌧ set might change as well. Elements in the set B⌧ are similar to each other in terms of their marginal values. Without deletions, we can choose any element from B⌧ and add it to our solution. However, if B⌧ has only a few elements, the adversary can delete all of them, and we will be left with an arbitrary poor solution. To make the selection process robust, we select a random element from B⌧ and add it to A⌧ only if there are at least d/✏ elements in B⌧ . This way even if all the deleted elements are from the set B⌧ , the probability of each selected element being deleted is at most ✏. We also know that all elements added to A⌧ have similar marginal values and are interchangeable. We keep adding elements to A⌧ until either [⌧ 0 ⌧A⌧ 0 has k elements or the size of set B⌧ becomes smaller than d/✏. At this stage, we keep both sets A⌧ and B⌧ as a part of the output core-set. We also remove them from the ground set V and move on to the next lower threshold. The pseudo code of ROBUSTCORESET-CENTRALIZED is given in Algorithm 1.\nThe sets {A⌧} and B are the outputs (core-set) of ROBUSTCORESET-CENTRALIZED. In Appendix B , we show how ROBUST-CENTRALIZED (with pseudo code given in Algorithm 2) returns a solution for submodular maximization problem after the deletion of set D.\nTheorem 1. For any > 0, by setting ✏ = 2 3 , ROBUSTCORESET-CENTRALIZED and ROBUST-CENTRALIZED satisfy the following properties:\n• ROBUST-CENTRALIZED outputs a set S such that |S|  k and E[f(S)] (1/2 ) · OPT.\n• ROBUST-CORESET-CENTRALIZED outputs at most O (k + d log k/ 2) elements as the core-set.\n• The query complexities of ROBUST-CORESETCENTRALIZED and ROBUST-CENTRALIZED are O ((k + log k/ )|V |) and O ((k + d log k/ 2)(log k/ )).\nAlgorithm 2 ROBUST-CENTRALIZED\n1: Input: {A0 ⌧ } and B0 {A0 ⌧ and B0 contain ele-\nments of A⌧ and B (outputs of ROBUST-CORESETCENTRALIZED) after deletion.}\n2: Output: Set S of cardinality at most k 3: 00 the largest value of {f({e})|e 2 {[A0⌧} [B0} 4: T0 = {(1 + ✏)i| 0 0\n2(1+✏)k  (1 + ✏) i  00}\n5: for ⌧ 2 T0 from the highest to the lowest do 6: S⌧ S ⌧ 02T0,⌧ 0 ⌧ A 0 ⌧ 0 7: for all e 2 B0 do 8: if f (e|S⌧ ) ⌧ and |S⌧ | < k then 9: S⌧ S⌧ [ e\n10: Return argmaxS⌧ f(S⌧ )"
  }, {
    "heading": "4.2. Streaming Algorithm",
    "text": "In many applications, the dataset does not fit in the main memory of a single machine or even the data itself arrives as a stream. So it is not possible to use centralized algorithms which need random access to the whole data. In this section, we present a streaming algorithm with a limited available memory. We first use the thresholding idea of Section 4.1 in order to find a core-set for V. Then we show that it is possible to find a good solution from this core-set when deletion happens. Recall that for ROBUST-CORESETCENTRALIZED, the maximum singleton element and the thresholds are fixed while in the streaming setting, they may change as new elements arrive. To apply ideas of the centralized algorithm, we should overcome the following challenges: (i) it is not possible to make several passes over the data for different thresholds (i.e., we cannot start from the largest possible marginal gain to the lowest), and (ii) the value of 0 and d are not known a priori.\nWe show that it is possible to maintain a good approximation of OPT even with a single pass over the data. From now on, let 0 and d, respectively, denote the largest and the (d+ 1)-th largest singleton values in the stream of data at time step t. First, note that d  OPT and the marginal gain of all the currently received elements is at most 0. Therefore, it is enough to consider thresholds in the range [ d2k , 0]. A new threshold is instantiated when the maximum singleton element is changed. These new (increasing) thresholds are between the current maximum and the previous one. Therefore, all the elements with marginal gains larger than the new threshold will appear after its instantiation.\nROBUST-CORESET-STREAMING, for each threshold ⌧ , keeps two sets A⌧ and B⌧ = [⌧ 0 ⌧B⌧,⌧ 0 . All the elements with marginal gains at least ⌧ to set A⌧ are good enough to be picked by this instance of the algorithm. In order to make the selected elements robust to deletions, we should put all good enough elements in different B⌧,⌧ 0 sets, with thresholds ⌧ 0 in the range [⌧, 0], based on their marginal values. Whenever a set B⌧,⌧ 0 becomes large, we pick one\nAlgorithm 3 ROBUST-CORESET-STREAMING\n1: T = {(1 + e)i|i 2 Z} 2: For each ⌧, ⌧ 0 2 T : {A⌧} ? and {B⌧,⌧ 0} ? 3: for every arriving element et do 4: d the (d + 1)-th largest element of {f{e1}, · · · , {f{et}} 5: 0 the largest element of {f{e1}, · · · , {f{et}} 6: Tt = {(1 + ✏)i| d2(1+✏)k  (1 + ✏)\ni  d} 7: Delete all A⌧ and B⌧,⌧ 0 such the ⌧ or ⌧ 0 /2 Tt 8: for ⌧ 2 Tt do 9: if |A⌧ | < k and ⌧  f (e|A⌧ ) then\n10: Add et to B⌧,⌧ 0 such that for ⌧ 0  f (et|A⌧ ) < ⌧ 0(1 + ✏) 11: while 9⌧ 00 such that |B⌧,⌧ 00 | d/✏ do 12: Randomly pick an element e from B⌧,⌧ 00 and add it to A⌧ , i.e., A⌧ A⌧ [ {e} 13: For all e 2\nS ⌧ 002Ti,⌧ 00 ⌧ B⌧,⌧ 00 recompute\nf (e|A⌧ ) and re-place them in correct bins 14: for ⌧ 2 Tn do 15: B⌧ S ⌧ 02Tn,⌧ 0 ⌧ B⌧,⌧ 0 16: Return {A⌧}, {B⌧}\nelement of it randomly to add to A⌧ . This ensures that an element is picked from a large pool of almost similar elements. Formally, all the elements with a marginal gain in the range [⌧ 0, ⌧ 0(1 + ✏)) are added to the set B⌧,⌧ 0 . When the size of a B⌧,⌧ 0 is at least d/✏, we randomly pick an element from B⌧,⌧ 0 and add it to A⌧ . Adding an element to A⌧ may decrease the marginal gains of elements in B⌧,⌧ 0 sets. So we recompute their marginal gains and put them in the right B⌧,⌧ 00 set (they are kept if their marginal gains are at least ⌧ , otherwise they are discarded). These changes may make another set large, so we keep adding elements to A⌧ while we find a large B⌧,⌧ 00 set. This process continues until a maximum of k elements are added to A⌧ or the stream of data ends. Note that there are at most d elements with marginal gains in the range ( d, 0]; we can simply keep these elements (refer to it as set Vd). For all d < ⌧  0, we have A⌧ = ?, because there is no pool of size at least d/✏ elements to pick from it. Also, for B⌧,⌧ 0 sets, we do not need to cover the range ( d, 0] with too many thresholds. Indeed, when d changes (it can only increase), we can update the set Vd and locate the removed elements from Vd into a correct B⌧,⌧ 0 . Therefore, it is sufficient to consider only thresholds in the range[ d2k , d]. The pseudo code of ROBUST-CORESET-STREAMING is given in Algorithm 3.\nIn Appendix D, we introduce another algorithm (called ROBUST-STREAMING) such that after deletion of any set D from the core-set finds a solution with an expected approximation guarantee of 1 3✏2 to the optimum solution.\nTheorem 2. For any > 0, by setting ✏ = 2 3 , ROBUSTCORESET-STREAMING and ROBUST-STREAMING satisfy the following properties:\nAlgorithm 4 ROBUST-DISTRIBUTED 1: for e 2 V do 2: Assign e to a machine i chosen uniformly at random; 3: Let Vi be the elements assigned to machine i 4: Run ROBUST-CORESET-CENTRALIZED (Algorithm 1)\non each machine to obtain {Ai ⌧ } and Bi\n5: Run ROBUST-CENTRALIZED (Algorithm 2) on each {Ai\n⌧ 0} and Bi0 to get the set Si of cardinality at most k from each machine {{Ai\n⌧ 0} and Bi0 are elements of {Ai\n⌧ } and Bi after deletion of set D.}\n6: S argmaxSi{f(Si)} 7: T GREEDY({ S i S ⌧2Ti A i ⌧ 0} S { S i B\ni0}) 8: Return argmax{f(T ), f(S)}\n• ROBUST-STREAMING outputs a set S such that |S|  k and E[f(S)] (1/2 ) · OPT.\n• ROBUST-CORESET-STREAMING makes one pass over the dataset.\n• ROBUST-CORESET-STREAMING outputs at most O k log k/ + d log2 k/ 3 elements as the core-set.\n• The query complexities of ROBUST-CORESETSTREAMING and ROBUST-STREAMING are O |V | log k/ + dk log2 k/ 3 and O d log3 k/ 4 ."
  }, {
    "heading": "4.3. Distributed Algorithm",
    "text": "In this section, build upon ideas from (Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015), we present a robust distributed submodular maximization algorithm, called ROBUST-DISTRIBUTED. We prove that our distributed algorithm finds an (↵, d)-robust randomized core-set with a constant ↵ and any arbitrary d.\nROBUST-DISTRIBUTED is a two-round distributed algorithm within a MapReduce framework. It first randomly partitions dataset between m machines. Each machine i runs ROBUST-CORESET-CENTRALIZED on its data and passes the result (i.e., sets {Ai\n⌧ } and Bi) to a central machine. Af-\nter the deletion of the set D, this single central machine runs m instances of ROBUST-CENTRALIZED on the outputs received from each machine i and finds solutions Si. In addition, it runs the classical GREEDY on the union of sets received from all machines (i.e., union of all sets {Ai\n⌧ 0} and B\ni0) to find another solution T . The final solution is the best answer among T and sets Si. ROBUST-DISTRIBUTED is outlined in Algorithm 4. Theorem 3. For any > 0, by setting ✏ = /2, ROBUSTDISTRIBUTED outputs a set S, |S|  k such that E[f(S)] ↵ /(↵+ ) · OPT, where ↵ = 1/3 and = 1 1/e. This results in an approximation factor of 0.218 . Corollary 1. Running ROBUST-CORESET-CENTRALIZED on the output of ROBUST-DISTRIBUTED produces a compact core-set of size O (k + d log k/ 2). Also, ROBUSTCENTRALIZED finds a solution with (0.109 )-\napproximation guarantee from this compact core-set. We refer to this version of our distributed algorithm as COMPACTDISTRIBUTED.\nThe main motivation of COMPACT-DISTRIBUTED is that the memory complexity does not increase with the number of machines m (while it still provides a constant factor approximation)."
  }, {
    "heading": "5. Experimental Results",
    "text": "In this section, we extensively evaluate the performance of our algorithms on several publicly available real-world datasets. We consider algorithms that can be robust to the deletion of any number of items and return k elements after deletion. Note that both OSU (Orlin et al., 2016) and PRO-GREEDY (Bogunovic et al., 2017) are robust to the deletion of only o(k) items. For this reason, we compare our proposed methods with three other baselines: (i) ROBUST (Mirzasoleiman et al., 2017), (ii) STAR-T-GREEDY (Mitrovic et al., 2017), and (iii) the stochastic greedy algorithm (Mirzasoleiman et al., 2015) (SG), where we first obtain a solution S of size r = 6k (we set r > k to make the solution robust to deletion), and then we report GREEDY(S \\D) as the final answer.\nIn our experiments, we evaluate the effect of three parameters: (i) d where an algorithm is designed to be robust to d deletions; (ii) cardinality constraint k of the final solution; and (iii) number of deleted elements r. The objective value of all algorithms are normalized to the utility obtained from a classical greedy algorithm that knows the set of deleted items D beforehand. Note that we are able to guarantee the performance of our algorithms (also this is true for ROBUST (Mirzasoleiman et al., 2017) and STAR-T-GREEDY (Mitrovic et al., 2017)) only when the number of deletions r is less than d. While the theoretical improvements of our algorithms for larger values of d is more significant (see Table 1), for a fair comparison, we used the experimental setting of Mirzasoleiman et al. (2017). In these experiments, we also evaluate the effect of larger number of deletions, i.e., where r d. We observe, even though our algorithms are not designed for such higher number of deletions, they demonstrate a gracefully robust behavior."
  }, {
    "heading": "5.1. Location Privacy",
    "text": "In a wide range of applications, data can be represented as a kernel matrix K, which encodes the similarity between different items in the database. In order to find a representative set S of cardinality k, a common objective function is\nf(S) = log det(I + ↵KS,S), (2)\nwhere KS,S is the principal sub-matrix of K indexed by S and ↵ > 0 is a regularization parameter (Herbrich et al., 2003; Seeger, 2004; Krause & Guestrin, 2005). This function is monotone submodular.\nIn this section, we analyze a dataset of 10,000 geolocations.\nEach data entry is longitude and latitude coordinates of Uber pickups in Manhattan, New York in April 2014 (UberDataset). Our goal is to find k representative samples using the objective function described in Eq. (2). The similarity of two location samples i and j is defined by a Gaussian kernel Ki,j = exp( d2i,j/h2), where the distance di,j (in meters) is calculated from the coordinates and h is set to 5000. We set d = 5, i.e., we make algorithms (theoretically) robust to deletion of at most five elements. To compare the effect of deletions on the performance of algorithms, we use two strategies to choose the deleted items: (i) classical greedy algorithm, and (ii) the stochastic greedy algorithm.\nIn the first experiment, we study the effect of deleting different number of items on the normalized objective values. To refer to an algorithm with a specific deletion strategy, we use the name of algorithm followed by the deletion strategy, e.g., Rob-Stream-G refers to ROBUST-STREAMING where the deleted items are picked by greedy strategy. From Fig. 1a, we observe that ROBUST-STREAMING and ROBUST-CENTRALIZED are more robust to deletion than ROBUST and SG. The effect of deleting by greedy strategy on the performance of algorithms is more pronounced than SG strategy. It can be seen that, even by deleting more than d = 5 items, our algorithms maintain their performance. Also, SG (which is not designed to be robust to deletions) shows the worst performance.\nOther than normalized objective values, the memory requirement of each algorithm is quite important. Indeed, we are interested in deletion-robust algorithms that do not keep many items. Fig. 1b compares the memory complexity of algorithms. We observe that ROBUST-CENTRALIZED needs to keep the least number of items. For ROBUST algorithm, the memory complexity increases super linear in k (it is O(k log k)), which makes it quite impractical for large values of k and d. Also, we observe ROBUST-STREAMING outperforms STAR-T-GREEDY in both objective function and memory requirement. To sum-up, we observe that our algorithms provide the best of two worlds: while their normalized objective values are clearly better than other baselines, they need to keep much fewer number of items."
  }, {
    "heading": "5.2. Submodular Feature Selection",
    "text": "One of the challenges in learning from high dimensional data is to select a subset of relevant features in a computationally feasible way. For this reason, the quality of a subset of features S can be captured by the mutual information between attributes in S and the class variable Y (Krause & Guestrin, 2005). More specifically,\nI(Y ;XS) = X\ny2Y\nX\nx2XS\np(x, y) log2\n✓ p(x, y)\np(x)p(y)\n◆ ,\nwhere XS is a random variable that represents the set S of k features. The joint distribution on (Y,X1, · · · , Xk),\nunder the Naive Bayes assumption, is defined by p(y, x1, · · · , xk) = p(y) Q k\ni=1 p(xi|y). This assumption makes the computation of joint distribution tractable. In our experiments, we estimate each p(xi|y) by counting frequencies in the dataset. In the feature selection problem, the goal is to choose k features such that maximizing f(S) = I(Y ;XS). It is known that the function f(S) = I(Y ;XS), under the Naive Bayes assumption, is monotone submodular (Krause & Guestrin, 2005).\nIn this section and Appendix G, we use this feature selection method on two real datasets. We first show that our algorithms, after the deletion of sensitive features (i.e., features that might cause unfairness in the final classifier) provide results with near optimal quality (based on mutual information). Second, we demonstrate that classifiers that are trained on these selected features perform very well.\nIn the first experiment, we use the Adult Income dataset from UCI Repository (Blake & Merz, 1998). This dataset contains information about 32,561 individuals and whether income of those individuals is over 50K a year. We extract 113 binary features from this dataset. The goal of the classification task is to predict the income status of 16,281 test cases. For the deletions, we remove sensitive features that might result in the unfairness, e.g., features about sex, race, nationality, marital status and relationship status. Fig. 1c compares algorithms based on different number of deletions for k = 5 and k = 10. We observe that for both values of k, ROBUST-CENTRALIZED considerably outperforms ROBUST (Mirzasoleiman et al., 2017) and SG. Also, the performance of ROBUST is better than SG.\nTo further investigate the effect of deletions, we compare accuracy of different classifiers, where each is trained on the features found by our algorithms and baselines. We train two type of classifiers: (i) Naive Bayes (Zhang, 2004) and (ii) SVM (Smola & Schölkopf, 2004). From Table 2, we observe that a SVM classifier, which is trained over all features, results in an accuracy of 83.0%. If we use a greedy algorithm to find the best 5 features and train SVM classifier on those features, the accuracy will drop to 79.6% (clearly there is a trade off between the number of features and accuracy). After deleting 10 features that might result in unfairness in classification (e.g., race and sex), we again use the greedy algorithm to find the best five features (referred to as GREEDYD). The accuracy in this case is 79.3%. Interestingly, we observe that the accuracies of classifiers which are trained on the features found by ROBUST-CENTRALIZED and ROBUST-STREAMING drop only by 0.2%. Also, for Naive Bayes classifier, we do not observe any decrease on the accuracy when we train on the features found by our algorithms. Finally, both Centralized (22) and Streaming (29) algorithms need to keep fewer number of items than ROBUST (39) and STAR-T-GREEDY (50)."
  }, {
    "heading": "5.3. Large Data Summarization",
    "text": "To evaluate the performance of ROBUST-DISTRIBUTED on large datasets, we consider the Census1990 dataset from UCI Repository (Blake & Merz, 1998). This dataset consists of 2,458,285 data points with 68 features. We are going to find k representative samples from this large dataset. We apply the set selection objective function described in Eq. (2). The similarity between two entries x and x0 is defined by 1 kx x\n0kp 68 , where kx x0k is the Euclidean distance between feature vectors of x and x0.\nWe randomly split the dataset into m = 12 partitions. For each instance of ROBUST-CORESET-CENTRALIZED, we set d = 25 with an ✏ = 0.1. As a baseline, we consider a distributed version of stochastic greedy algorithm (refer to it as SG-DISTRIBUTED). For this algorithm, we first run stochastic greedy on each partitions to select Si = 6k items. After deletion of D, we report f(GREEDY([Si \\D)) as the final result. Also, we normalize the utility of functions to the objective value of an instance of SG-DISTRIBUTED that knows the set of deleted items D in advance. For deletions, we propose four different strategies: D1 randomly deletes 50% of items, D2 randomly deletes 80% of items, D3 deletes all men in the dataset, and D4 deletes all women.\nWe investigate the effect of different deletion strategies for\ntwo values of k 2 {50, 100}. In Figs. 2a and 2b, we observe that ROBUST-DISTRIBUTED clearly outperforms SGDISTRIBUTED in all cases. Furthermore, we observe that the objective value of ROBUST-DISTRIBUTED in all scenarios is even better than our reference function for normalization (normalized objective values are larger than 1). Each machine on average stores 209.3 (for k = 50) and 348.3 (for k = 100) items. The standard deviations of memory complexities are 36.9 and 26.5, respectively. To conclude, ROBUST-DISTRIBUTED enables us to robustly summarize a dataset of size 2,458,285 with storing only ⇡4500 items. Our experimental results confirm that this core-set is robust to the deletion of even 80% of items."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we considered the problem of deletion-robust submodular maximization. We provided the first scalable and memory-efficient solutions in different optimization settings, namely, centralized, streaming, and distributed models of computation. We rigorously proved that our methods enjoy constant factor approximations with respect to the optimum algorithm that is also aware of the deleted set of elements. We showcased the effectiveness of our algorithms on real-word problems where part of data should be deleted due to privacy and fairness constraints."
  }, {
    "heading": "Acknowledgements",
    "text": "Amin Karbasi was supported by a DARPA Young Faculty Award (D16AP00046) and a AFOSR Young Investigator Award (FA9550-18-1-0160). Ehsan Kazemi was supported by the Swiss National Science Foundation (Early Postdoc.Mobility) under grant number 168574."
  }],
  "year": 2018,
  "references": [{
    "title": "The power of randomization: Distributed submodular maximization on massive datasets",
    "authors": ["R. Barbosa", "A. Ene", "H. Nguyen", "J. Ward"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Theory and applications of robust optimization",
    "authors": ["D. Bertsimas", "D.B. Brown", "C. Caramanis"],
    "venue": "SIAM review,",
    "year": 2011
  }, {
    "title": "Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations",
    "authors": ["A. Beutel", "J. Chen", "Z. Zhao", "E.H. Chi"],
    "venue": "arXiv preprint arXiv:1707.00075,",
    "year": 2017
  }, {
    "title": "Uci repository of machine learning databases [http://www",
    "authors": ["C.L. Blake", "C.J. Merz"],
    "venue": "ics. uci. edu/ ̃ mlearn/mlrepository. html]. irvine, ca: University of california. Department of Information and Computer Science,",
    "year": 1998
  }, {
    "title": "Robust Submodular Maximization: A Non-Uniform Partitioning Approach",
    "authors": ["I. Bogunovic", "S. Mitrovic", "J. Scarlett", "V. Cevher"],
    "year": 2017
  }, {
    "title": "Robust Maximization of Non-Submodular Objectives",
    "authors": ["I. Bogunovic", "J. Zhao", "V. Cevher"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "Max-Sum Diversification, Monotone Submodular Functions, and Dynamic Updates",
    "authors": ["A. Borodin", "A. Jain", "H.C. Lee", "Y. Ye"],
    "venue": "ACM Transactions on Algorithms (TALG),",
    "year": 2017
  }, {
    "title": "How Machine Learning Advances Will Improve the Fairness of Algorithms, 2017. URL https://www.huffingtonpost.com/entry/ how-to-shorten-a-website-link_us_ 579bb7aee4b07066ba1ea7dd",
    "authors": ["J.T. Chayes"],
    "year": 2017
  }, {
    "title": "Streaming algorithms for submodular function maximization",
    "authors": ["C. Chekuri", "S. Gupta", "K. Quanrud"],
    "venue": "In International Colloquium on Automata, Languages, and Programming,",
    "year": 2015
  }, {
    "title": "Greed Is Good: Near-Optimal Submodular Maximization via Greedy Optimization",
    "authors": ["M. Feldman", "C. Harshaw", "A. Karbasi"],
    "venue": "In Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "Do Less, Get More: Streaming Submodular Maximization with Subsampling",
    "authors": ["M. Feldman", "A. Karbasi", "E. Kazemi"],
    "venue": "CoRR, abs/1802.07098,",
    "year": 2018
  }, {
    "title": "Fast sparse Gaussian process methods: The informative vector machine",
    "authors": ["R. Herbrich", "N.D. Lawrence", "M. Seeger"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2003
  }, {
    "title": "Ridge regression: Biased estimation for nonorthogonal problems",
    "authors": ["A.E. Hoerl", "R.W. Kennard"],
    "year": 1970
  }, {
    "title": "Maximizing the spread of influence through a social network",
    "authors": ["D. Kempe", "J. Kleinberg", "É. Tardos"],
    "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2003
  }, {
    "title": "Budgeted nonparametric learning from data streams",
    "authors": ["A. Krause", "R.G. Gomes"],
    "venue": "In Proceedings of the 27th International Conference on Machine Learning",
    "year": 2010
  }, {
    "title": "Near-optimal Nonmyopic Value of Information in Graphical Models",
    "authors": ["A. Krause", "C. Guestrin"],
    "venue": "Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence,",
    "year": 2005
  }, {
    "title": "Robust submodular observation selection",
    "authors": ["A. Krause", "H.B. McMahan", "C. Guestrin", "A. Gupta"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "Fast greedy algorithms in mapreduce and streaming",
    "authors": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"],
    "venue": "ACM Transactions on Parallel Computing,",
    "year": 2015
  }, {
    "title": "A Class of Submodular Functions for Document Summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "authors": ["H. Lin", "J. Bilmes"],
    "year": 2011
  }, {
    "title": "Randomized composable core-sets for distributed submodular maximization",
    "authors": ["V. Mirrokni", "M. Zadimoghaddam"],
    "venue": "In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,",
    "year": 2015
  }, {
    "title": "Distributed submodular maximization: Identifying representative elements in massive data",
    "authors": ["B. Mirzasoleiman", "A. Karbasi", "R. Sarkar", "A. Krause"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Lazier Than Lazy Greedy",
    "authors": ["B. Mirzasoleiman", "A. Badanidiyuru", "A. Karbasi", "J. Vondrák", "A. Krause"],
    "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "DeletionRobust Submodular Maximization: Data Summarization with “the Right to be Forgotten",
    "authors": ["B. Mirzasoleiman", "A. Karbasi", "A. Krause"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach",
    "authors": ["S. Mitrovic", "I. Bogunovic", "A. Norouzi-Fard", "J.M. Tarnawski", "V. Cevher"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "An analysis of approximations for maximizing submodular set functionsâĂŤi",
    "authors": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "Robust monotone submodular function maximization",
    "authors": ["J.B. Orlin", "A.S. Schulz", "R. Udwani"],
    "venue": "In International Conference on Integer Programming and Combinatorial Optimization,",
    "year": 2016
  }, {
    "title": "Greedy forward selection in the informative vector machine",
    "authors": ["M. Seeger"],
    "venue": "Technical report, Technical report, University of California at Berkeley,",
    "year": 2004
  }, {
    "title": "Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization",
    "authors": ["A. Singla", "S. Tschiatschek", "A. Krause"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "A tutorial on support vector regression",
    "authors": ["A.J. Smola", "B. Schölkopf"],
    "venue": "Statistics and computing,",
    "year": 2004
  }, {
    "title": "Resilient Non-Submodular Maximization over Matroid Constraints",
    "authors": ["V. Tzoumas", "A. Jadbabaie", "G.J. Pappas"],
    "venue": "arXiv preprint arXiv:1804.01013,",
    "year": 2018
  }, {
    "title": "Submodularity in data subset selection and active learning",
    "authors": ["K. Wei", "R. Iyer", "J. Bilmes"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "Linear submodular bandits and their application to diversified retrieval",
    "authors": ["Y. Yue", "C. Guestrin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Learning fair representations",
    "authors": ["R. Zemel", "Y. Wu", "K. Swersky", "T. Pitassi", "C. Dwork"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning",
    "year": 2013
  }, {
    "title": "The Optimality of Naive Bayes",
    "authors": ["H. Zhang"],
    "venue": "In Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference,",
    "year": 2004
  }],
  "id": "SP:62599f741551c1f8a12336714f6cd7d4a7004e54",
  "authors": [{
    "name": "Ehsan Kazemi",
    "affiliations": []
  }, {
    "name": "Morteza Zadimoghaddam",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against any number of adversarial deletions. We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints; (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our experiments show that our solution is robust against even 80% of data deletion.",
  "title": "Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints"
}