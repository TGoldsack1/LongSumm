{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Recently there have been increasing interests in learning and inference with implicit distributions, i.e., distributions defined by a sampling process but without tractable densities. Popular examples include Generative adversarial networks (GAN) (Goodfellow et al., 2014; Mohamed & Lakshminarayanan, 2016). Compared to the explicit likelihoods (e.g., Gaussian) in other deep generative models such as variational autoencoders (VAE) (Kingma & Welling, 2013), implicit distributions are shown able to capture the complex data manifold that lies in a high dimensional space,\n1Dept. of Comp. Sci. & Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., THBI Lab, Tsinghua University 2Dept. of Comp. Sci., University of Toronto. Correspondence to: Jiaxin Shi <shijx15@mails.tsinghua.edu.cn>, Jun Zhu <dcszj@tsinghua.edu.cn>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nleading to more realistic samples generated by GAN than other models. Besides, as the constraint of requiring an explicit density is removed, implicit distributions are treated as more flexible variants of variational families for approximate inference (Ranganath et al., 2016; Liu & Feng, 2016; Mescheder et al., 2017; Tran et al., 2017; Huszár, 2017; Li & Turner, 2018; Shi et al., 2018).\nDespite that it is appealing to use flexible implicit distributions, which capture complex correlations and manifold structures, deploying them in practical scenes is still challenging. This is because most learning and inference algorithms require optimizing some divergences between two distributions, which often rely on evaluating the densities of them. However, the density of an implicit distribution is intractable and we only have access to its samples. Previous works have explored two directions to solve the problem. One is to first approximate the optimization objective with the samples and then use the approximation to guide the learning procedure. Many works in this direction are based on the fact that the density ratio between two distributions can be estimated from their samples, by a probabilistic classifier (also known as the discriminator) trained in an adversarial game (Donahue et al., 2016; Dumoulin et al., 2016; Mescheder et al., 2017; Tran et al., 2017; Huszár, 2017), or by kernel-based estimators (Shi et al., 2018). The other direction is to estimate the gradients instead of the objective. Li & Turner (2018) propose the Stein gradient estimator for the log density of an implicit distribution. It is based on a ridge regression that inverts a generalized version of Stein’s identity (Gorham & Mackey, 2015; Liu et al., 2016). As argued in their work, this approach is more direct and avoids probable arbitrarily diverse gradients provided by the approximate objective. In this paper, we focus on the latter direction.\nThough the Stein gradient estimator has been shown to be a fast and easy way to obtain gradient estimates for implicit models. It is still limited by its simple formulation, i.e., the unjustified choice of both the test function (the kernel feature mapping) and the regularization scheme (the Frobeniusnorm regularization used in ridge regression). The problem has deeper implications. For instance, no theoretical results have been established for the estimator. Moreover, there is no principled way to obtain gradient estimates at positions out of the sample points.\nIn this paper, we develop a novel gradient estimator for implicit distributions, which is called the Spectral Stein Gradient Estimator (SSGE). To approximate the gradient function of the log density (i.e., ∇x log q(x)), SSGE expands it in terms of the eigenfunctions of a kernel-based operator. These eigenfunctions are orthogonal with respect to the underlying distribution. By setting the test functions in the Stein’s identity to these eigenfunctions, we can take advantage of their orthogonality to obtain a simple solution. The eigenfunctions in the solution are then approximated by the Nyström method (Nyström, 1930; Baker, 1997; Williams & Seeger, 2001). Unlike the Stein gradient estimator (Li & Turner, 2018), our approach allows for a direct and principled out-of-sample extension. Moreover, we provide theoretical analysis on the error bound of SSGE and discuss the bias-variance tradeoff in practice. We also discuss its effectiveness in reducing the curse of dimensionality by drawing connections to kernel PCA."
  }, {
    "heading": "2. Background",
    "text": "In this section we briefly introduce the Nyström method and the Stein gradient estimator."
  }, {
    "heading": "2.1. The Nyström Method",
    "text": "The Nyström method originates as a method for approximating the solution of Fredholm integral equations of the second kind (Nyström, 1930; Baker, 1997). It was used by Williams & Seeger (2001) for estimating extensions of eigenvectors in Gaussian process regression. Specifically, the following equation for finding the eigenfunctions {ψj}j≥1, ψj ∈ L2(X , q)1 of the covariance kernel k(x,y) w.r.t. the probability measure q is considered:∫\nk(x,y)ψ(y)q(y)dy = µψ(x). (1)\nAnd there is a constraint that the eigenfunctions {ψj}j≥1 are orthonormal under q:∫\nψi(x)ψj(x)q(x)dx = δij , (2)\nwhere δij = 1[i = j]. Approximating the left side of eq. (1) with its unbiased Monte Carlo estimate using i.i.d. samples {x1, . . . ,xM} from q and applying the equation to these samples, we obtain\n1\nM Kψ ≈ µψ, (3)\nwhere K is the Gram matrix: Kij = k(xi,xj), and ψ =[ ψ(x1), . . . , ψ(xM ) ]> . This is an eigenvalue problem for K. We compute the eigenvectors u1, . . . ,uJ with the J\n1L2(X , q) denotes the space of all square-integrable functions w.r.t. q.\nlargest eigenvalues λ1 ≥ · · · ≥ λJ for K. Now we have the solutions of eq. (3) by comparing against Kuj = λjuj :\nψj(x m) ≈ √ Mujm, m = 1, . . . ,M, (4)\nµj ≈ λj M . (5)\nNote that the scaling factor in eq. (4) is due to the empirical constraint translated from eq. (2): 1 M ∑M m=1 ψi(x m)ψj(x m) ≈ δij . Baker (1997) shows that for a fixed kernel k, λjM converges to µj in the limit as M →∞. Plugging these solutions back into eq. (1), we get the Nyström formula for approximating the value of the jth eigenfunction at any point x:\nψj(x) ≈ ψ̂j(x) = √ M\nλj M∑ m=1 ujmk(x,x m). (6)\nThe Nyström method has been shown to be a thread linking many dimension reduction methods such as kernel PCA (Schölkopf et al., 1998), multidimensional scaling (MDS) (Borg & Groenen, 2005), local linear embedding (LLE) (Roweis & Saul, 2000), Laplacian eigenmaps (Belkin & Niyogi, 2003), and spectral clustering (Weiss, 1999), unifying their out-of-sample extensions (Bengio et al., 2004a;b; Burges et al., 2010). We will discuss these connections later."
  }, {
    "heading": "2.2. Stein’s Identity and Stein Gradient Estimator",
    "text": "Recent developments in Stein discrepancy and its kernelized extensions (Gorham & Mackey, 2015; Chwialkowski et al., 2016; Liu et al., 2016; Liu & Wang, 2016) have renewed the interests in Stein’s method, which is a classic tool in statistics. Central to these works is an equation that generalizes the original Stein’s identity (Stein, 1981), shown in the following theorem.\nTheorem 1 (Gorham & Mackey 2015; Liu et al. 2016). Assume that q(x) is a continuous differentiable probability density supported onX ⊂ Rd. h : X → Rd′ is a smooth vectorvalued function h(x) = [h1(x), h2(x), . . . , hd′(x)]\n>, and ∀i ∈ 1, . . . , d′, hi is in the Stein class of q, i.e.,∫\nx∈X ∇x (hi(x)q(x)) dx = 0. (7)\nThen the following identity holds:\nEq[h(x)∇x log q(x)> +∇xh(x)] = 0. (8)\nThe condition (7) can be easily checked using integration by parts or divergence theorem. Specifically, when X = Rd, eq. (7) holds if lim‖x‖→∞ h(x)q(x) = 0; when X is a compact subset of Rd with piecewise smooth boundary ∂X , eq. (7) holds if h(x)q(x) = 0,∀x ∈ ∂X . Here h(x) is\ncalled the test function. We can check that for a RBF kernel k, and for any fixed x, k(x, ·) and k(·,x) are in the Stein class of continuous differentiable densities supported on Rd.\nBecause the expectation in eq. (8) can be approximated by Monte Carlo estimates, the identity connects ∇x log q(x) and the samples from q. Inspired by this, Li & Turner (2018) propose the Stein gradient estimator, which inverts eq. (8) to obtain estimates of ∇x log q(x) at the sample points. Below we briefly review their method. Specifically, consider M i.i.d. samples x1:M drawn from q(x). We define two matrices H = [ h(x1), · · · ,h(xM )\n] ∈ Rd′×M and G =[\n∇x1 log q(x1), · · · ,∇xM log q(xM ) ]> ∈ RM×d. Monte\nCarlo sampling with eq. (8) shows that\n− 1 M HG ≈ ∇xh, (9)\nwhere ∇xh = 1M ∑M m=1∇xmh(xm) ∈ Rd\n′×d, ∇xmh(xm) = [∇xmh1(xm), . . . ,∇xmhd′(xm)]>. Equation (9) inspires the following ridge regression problem:\nargmin Ĝ∈RM×d\n‖∇xh + 1\nM HĜ‖2F +\nη\nM2 ‖Ĝ‖2F ,\nwhere ‖ · ‖2F denotes the Frobenius norm of a matrix and η > 0 is the regularization coefficient. It has an analytic solution that\nĜStein = −M(K + ηI)−1H>∇xh, (10)\nwhere K = H>H. By noticing that Kij = h(xi)>h(xj) and applying the kernel trick, we have Kij = k(xi,xj), where k : Rd × Rd → R is a positive definite kernel. Similarly we can show (H>∇xh)ij = 1 M ∑M m=1∇xmj k(xi,xm). With the kernel trick, the above derivation implicitly set the test function to be the feature mapping h : Rd → H,h(x) = k(x, ·), where H is the Reproducing Kernel Hilbert Space (RKHS) induced by k.\nThough introducing the kernel trick enhances the expressiveness of the Stein gradient estimator, the choice of the test function is not well justified. It is unclear whether eq. (8) holds when the test function is set to the kernel feature mapping h(x) = k(x, ·), which maps from Rd to H instead of the common Rd′ . Li & Turner (2018) show that their approach is equivalent to minimizing a regularized version of the V-statistics of kernel Stein discrepancy (Liu et al., 2016), which has proved to be effective in testing goodness-of-fit. However, it is still unclear whether the test power is sufficient due to the added Frobenius-norm regularization term. Besides, eq. (10) only gives the gradient estimates at the sample points. For out-of-sample prediction at a test point, two choices are proposed in Li & Turner (2018): one is by adding the test point to the sample points and re-compute eq. (10), which could be computationally\ndemanding. We will refer to this out-of-sample extension as Stein+. The other choice is to refit a parametric estimator (a linear combination of RBF kernels), which is cheaper but less accurate. Both approaches assume that the test points are also sampled from q. However, they are unjustified when the assumption is not satisfied. Since the latter is an approximation to the former, we will only compare with Stein+ in the experiments for out-of-sample predictions."
  }, {
    "heading": "3. Method",
    "text": "In this section we derive a gradient estimator for implicit distributions called the Spectral Stein Gradient Estimator (SSGE). Unlike the previous Stein gradient estimator that only provides estimates at the sample points, SSGE directly estimates the gradient function and thus allows simple and principled out-of-sample predictions. We also provide theoretical analysis on the error bound of SSGE."
  }, {
    "heading": "3.1. Spectral Stein Gradient Estimator",
    "text": "To begin with, let x denote a d-dimensional vector in Rd. Consider an implicit distribution q(x) supported on X ⊂ Rd, from which we observe M i.i.d. samples x1:M . We denote the target gradient function to estimate by g : X → Rd: g(x) = ∇x log q(x). The ith component of the gradient is gi(x) = ∇xi log q(x). We assume g1, . . . , gd ∈ L2(X , q). As introduced in section 2.1, {ψj}j≥1 form an orthonormal basis of L2(X , q). So we can expand gi(x) into the following spectral series:\ngi(x) = ∞∑ j=1 βijψj(x). (11)\nBelow we will show how to estimate the coefficients βij . According to Theorem 1, we have the following proposition. Proposition 1. If k(·, ·) has continuous second order partial derivatives, and both k(x, ·) and k(·,x) are in the Stein class of q, the following set of equations hold true:\nEq[ψj(x)g(x) +∇xψj(x)] = 0, j = 1, 2 . . . ,∞. (12)\nProof. We only need to prove that ψj(x) is in the Stein class of q. See Appendix A for details.\nSubstituting eq. (11) into eq. (12) and using the orthonormality of {ψj}j≥1, we can show that\nβij = −Eq∇xiψj(x). To estimate βij , we need an approximation of ∇xiψj(x). The key observation is that derivatives can be taken w.r.t. both sides of eq. (1):\nµj∇xiψj(x) = ∇xi ∫ k(x,y)ψj(y)q(y)dy\n= ∫ ∇xik(x,y)ψj(y)q(y)dy.\n(13)\nMonte-Carlo sampling with eq. (13), we have an estimate of ∇xiψj(x):\n∇̂xiψj(x) ≈ 1\nµjM M∑ m=1 ∇xik(x,xm)ψj(xm). (14)\nSubstituting eqs. (4) and (5) into eq. (14) and comparing with eq. (6), we can show\n∇̂xiψj(x) ≈ ∇xi ψ̂j(x). (15) Perhaps surprisingly, Equation (15) indicates that∇xi ψ̂j(x) is a good approximation to ∇xiψj(x)2. In fact, as we shall see in Theorem 2, the error introduced by Nyström approximation is negligible with high probability as M →∞. Now truncating the series expansion to the first J terms and plugging in the Nyström approximations of {ψj}Jj=1, we get our estimator:\nĝi(x) = J∑ j=1 β̂ijψ̂j(x), (16)\nβ̂ij = − 1\nM M∑ m=1 ∇xi ψ̂j(xm), (17)\nwhere ψ̂j is the Nyström approximation of ψj as in Section 2.1. We use RBF kernels in all experiments.\nComputational Cost The spectral gradient estimator ĝi(x) depends on β̂ij and ψ̂j(x). To compute them, the computational bottleneck lies in computing the Gram matrix and its eigendecomposition, which have complexity O(M2d) and O(M3), respectively. Therefore the computational cost of constructing the estimator isO(M3+M2d). For prediction, given x ∈ Rd, evaluating ĝi(x) has cost O(M(d+ J)). In comparison, the Stein gradient estimator directly approximates gradients at the sample points, which involves computing the Gram matrix and its inverse. Thus the overall complexity is also O(M3 + M2d). Note that SSGE only requires the J largest eigenvalues and corresponding eigenvectors, efficient algorithms (Parlett & Scott, 1979) might be applied to further reduce its complexity."
  }, {
    "heading": "3.2. Theoretical Results",
    "text": "Following the derivation in Section 3.1, we analyze the theoretical properties of the resulting estimator in eqs. (16) and (17). To be clear, we formally restate the assumptions that have been made in the derivation. Assumption 1. k(x, ·) and k(·,x) are in the Stein class of q. Assumption 2. gi(x) ∈ L2(X , q), i = 1, . . . , d, i.e.,∫\ngi(x) 2q(x) dx = ∞∑ j=1 β2ij ≤ C <∞.\n2This does not hold for general functions.\nAssumption 3. µ1 > µ2 > · · · > µJ > 0.\nNote that Assumption 1 holds for RBF kernels. Assumption 2 is necessary for gi(x)’s being possible to be expanded into the spectral series. We need Assumption 3 since our derivation is based on several well-studied bounds of Nyström approximation, i.e., Lemmas 4 and 5 in Appendix B (Sinha & Belkin, 2009; Izbicki et al., 2014). Note that when this assumption does not hold, we could proceed as in Rosasco et al. (2010) (Theorem 12) and derive the error bound in a similar way.\nTheorem 2 (Error Bound of SSGE, proof in Appendix B). Given the above assumptions, the error ∫ |ĝi(x) − gi(x)|2q(x) dx is bounded by\nJ2 ( Op ( 1\nM\n) +Op ( C\nµJ∆2JM\n)) +\nJOp\n( C\nµJ∆2JM\n) + ‖gi‖2HO(µJ),\n(18)\nwhere ∆J = min1≤j≤J |µj − µj+1|, Op is the Big O notation in probability.\nThe first three terms in eq. (18) are the sample errors caused by the Nyström approximation, which we call the estimation error. It is negligible with high probability asM →∞. The last term is caused by the bias introduced by the truncation, which we call the approximation error. From the bound we can observe a tradeoff between the estimation error and the approximation error. As an illustration, one may set J to be as large as possible to reduce the magnitude of µJ and thus reduce the approximation error, but it will increase the estimation error at a rate of Op ( J2\nµJ\n) .\nFor RBF kernels and their corresponding RKHS, smoother target functions tend to have smaller ‖gi‖2H, and thus a tighter bound. In general, this indicates that choosing the appropriate kernel which is suitable to the target gradient function can improve the performance of the gradient estimator (by leading to a smaller ‖gi‖2H). Hyperparameter Selection When RBF kernels are used, SSGE has two free parameters: The kernel bandwidth σ, and the number of eigenfunctions (J) used in the estimate. For σ, we use the median heuristic, i.e., we set it to be the median of pairwise distances between all samples, which turns out to work well in all experiments. Below we discuss the criterion for selecting J , which is usually harder.\nAs the performance of the gradient estimator directly influences the task where it is used. The optimal choice for tuning J is to apply cross-validation on the specific task. However, since J is a discrete parameter, one has to manually set a continuous interval and bin it so that the commonly used black-box hyperparameter-search methods (e.g., Bayesian\noptimization) are applicable. However, this approach does not take the magnitude of eigenvalues into consideration. Observing this, we propose that, instead of directly tuning J , we could tune a threshold r̄ for the percentage of remaining eigenvalues:\nJ = argmax J′ rJ′ , s.t. rJ′ = ∑J′ j=1 λj∑M j′=1 λj′ , rJ′ ≤ r̄.\nNote that searching r̄ may still not be easy due to the nonsmooth validation surface. But in experiments we found that r̄ values in [0.95, 0.99] usually work well."
  }, {
    "heading": "3.3. Gradient Estimation for Entropy",
    "text": "Above we have derived a gradient estimator for the log density of an implicit distribution. Now we discuss a useful extension of it. Consider the situation where we need to optimize the entropy H(q) = −Eq log q of an implicit distribution qφ(x) w.r.t. its parameters φ. When x is continuous and can be reparameterized (Kingma & Welling, 2013), e.g., x = f( ;φ), ∼ N (0, I), it can be shown (see Appendix C) that\n∇φH(q) = −E ∇x log qφ(f( ;φ))∇φf( ;φ), (19)\nwhere∇x log qφ(f( ;φ)) can be easily estimated by SSGE. As we shall see in experiments, eq. (19) can be used for variational inference with implicit distributions."
  }, {
    "heading": "4. Related Work",
    "text": "Our work is closely related to other works on implicit distributions. Apart from the density-ratio based approaches introduced in Section 1, we discuss two more directions.\nNonparametric Inference Nonparametric variational inference (VI) methods such as PMD (Dai et al., 2016) and SVGD (Liu & Wang, 2016) remove the need of parametric families, they keep a set of particles and gradually adjust them towards the true posterior. These particles can be viewed as samples from an implicit distribution. Instead of directly computing gradients in the sample space like us, SVGD performs functional gradient descent to transform the implicit distribution towards the true posterior. Though elegant, SVGD is limited to KL-divergence based VI problems, while our approach is generally applicable wherever gradient estimates are needed for the log density of implicit distributions.\nKernel Exponential Families and Score Matching Previous to our work, the problem of estimating gradient functions of intractable log densities has been worked on by Strathmann et al. (2015); Sutherland et al. (2018). They identified the problem when developing Hamiltonian Monte Carlo (HMC) under the settings where higher-order information of the target distribution is unavailable. To address\nit, a kernel exponential family (Sriperumbudur et al., 2017) is fit to samples along the Markov Chain trajectory by score matching (Hyvärinen, 2005), and then serves as a surrogate of the target distribution to provide gradient estimates for HMC. We will compare to them in Section 5.2."
  }, {
    "heading": "5. Experiments",
    "text": "We evaluate the proposed approach on both toy problems and real-world examples. The latter includes applications of SSGE to two widely used inference methods: Hamiltonian Monte Carlo and variational inference. Code is available at https://github.com/thjashin/ spectral-stein-grad. Implementations are based on ZhuSuan (Shi et al., 2017)."
  }, {
    "heading": "5.1. Toy Experiment",
    "text": "As a simple example, we experiment with estimating the gradient function of a 1-D standard Gaussian distribution. The target log density is log q(x) = − 12 log 2π − 12x2, and the true gradient function is ∇x log q(x) = −x. We draw M = 100 i.i.d. samples from q for use in the estimation. In Figure 1 we plot the gradients estimates produced by the Stein gradient estimator, its out-of-sample extension (Stein+) (see Section 2.2), and our approach (SSGE). Since the original Stein estimator only gives estimates at the sample points, we plot them as individual points (in red). For the regularization coefficient η in eq. (10), we searched it in {0.001, 0.01, 0.1, 1, 10, 100} and plot the best result at η = 0.1 3. For SSGE, we set J = 6. We can see that despite all three estimators produce rather good approximation where the samples are taken densely (e.g., in [−2, 2]), the gradient function estimated by SSGE is notably better at the places where samples are less dense."
  }, {
    "heading": "5.2. Gradient-free Hamiltonian Monte Carlo",
    "text": "In this experiment we investigate the usefulness of SSGE in constructing a gradient-free HMC sampler. We follow the\n3The criterion for selecting η is unclear in Li & Turner (2018).\nsettings in Sejdinovic et al. (2014); Strathmann et al. (2015) and consider a Gaussian Process classification problem on the UCI Glass dataset. The goal is to infer the posterior over hyperparameters under a fully Bayesian treatment. Specifically, consider a Gaussian process whose joint distribution is over latent variables f , labels y, and hyperparameters θ:\np(f ,y,θ) = p(θ)p(f |θ)p(y|f), (20)\nwhere f |θ ∼ N(0,Kθ). Kθ is the Gram matrix formed by the data points x1:N ∈ RD:\n(Kθ)ij = exp\n{ −\nD∑ d=1 |xi,d − xj,d|2 2`2d\n} , (21)\nwhere we define θd = log `2d. The problem to consider is a binary classification between window and non-window glasses, so the likelihood is given by a logistic classifier: p(yi|fi) = 11+exp(−yifi) , yi ∈ {−1, 1}. The posterior over θ is highly nonlinear, as shown in Figure 2a. As pointed out in previous works (Murray & Adams, 2010; Sejdinovic et al., 2014), sampling from the posterior of θ is challenging, e.g., Gibbs sampling often gets stuck due to p(θ|f ,y) is very sharp. One way to address this problem is the pseudomarginal MCMC (Andrieu et al., 2009), through which a markov chain can be simulated to directly sample from p(θ|y). Since the likelihood p(y|θ) is intractable, pseudomarginal MCMC replaces it with an unbiased Monte Carlo estimate using importance sampling:\np̂(y|θ) = 1 K K∑ i=1 p(y|f i)p(f i|θ) q(f i) , f1:K ∼ q(f). (22)\nIn practice q(f) is chosen to be the Laplace approximation of p(f |y,θ). As for any pseudo-marginal MCMC scheme, the gradient information of the posterior is not available and HMC is not suitable. We have to resort to gradientfree MCMC methods. As mentioned in Section 4, kernel adaptive Metropolis samplers (Sejdinovic et al., 2014) were\ndeveloped and then extended to kernel HMC (KMC) (Strathmann et al., 2015). In this experiment we compare the performance of KMC and HMC with gradients estimated by SSGE and Stein+.\nTo begin, we run 20 randomly initialized adaptiveMetropolis samplers for 30k iterations, with the first 10k samples discarded. We then keep every 400-th sample in each of the chains, and combine them to get 1k samples. These samples are treated as the ground truth. Similar to Sutherland et al. (2018), our experiment assumes the idealized scenario where a burn-in period for collecting a sufficient number of samples has completed. This is to remove all the other factors that could have an effect on the comparison of acceptance ratios, which then only depends on the accuracy of the gradient estimation of potentials, i.e., −∇θ log p(θ|y). So we fit all three estimators on a random subset of M = 200 of these samples and repeated 10 times. For each fitted estimator, we start from a random initial point from the posterior sketch and run HMC samplers with the gradient estimates for 5k iterations. To be fair in comparing the acceptance ratios, no adaptation of HMC parameters can be used. So we randomly uses between 1 and 10 leapfrog steps of size chosen uniformly in [0.01, 0.1], and a standard Gaussian momentum. The kernel bandwidths (σ) in all three estimators are determined by median heuristics (i.e., set to the median of the pairwise distances between the M data points). For Stein+, η = 0.001. For SSGE, r̄ = 0.95.\nThe average acceptance ratios over 10 runs are plotted in Figure 2b. We can see that SSGE clearly outperforms Stein+ and is even better than the KMC algorithm, which is specially designed as a gradient-free HMC algorithm. Though KMC is carefully designed, its gradients are estimated by first fitting a kernel exponential family as a surrogate and then taking derivatives through it, while SSGE is arguably more direct."
  }, {
    "heading": "5.3. Variational Inference with Implicit Distributions",
    "text": "As introduced in Section 1, there have been increasing interests in constructing flexible variational posteriors with implicit distributions. Specifically, for a latent-variable model p(z,x) where x and z denote observed and latent variables, respectively, Variational Inference (VI) approximates the posterior p(z|x) by maximizing the following evidence lower bound (ELBO):\nL(x;φ) = Eqφ(z) log p(z,x)− Eqφ(z) log qφ(z), (23) where qφ(z) is called the variational distribution. The second term of eq. (23) is the entropy of q, which is intractable for implicit distributions. As shown in Section 3.3, SSGE can be used here for estimating gradients of the entropy term, thus allowing VI with implicit distributions. Below we conduct experiments on two examples: Bayesian Neural Networks (BNN) and Variational Autoencoders (VAE).\nNote that the original Stein gradient estimator can also be used here. In experiments, we find that despite lack of theoretical evidences, the performance of a well-tuned Stein gradient estimator is very close to SSGE when no out-ofsample predictions are required. As the emphasis of this paper is on SSGE, we only focus on verifying the accuracy of SSGE below."
  }, {
    "heading": "5.3.1. BAYESIAN NEURAL NETWORKS",
    "text": "We evaluate the predictive ability of BNNs with implicit variational posteriors trained by SSGE. To visually access the quality of uncertainty, we choose a 1-D regression problem (Hernández-Lobato & Adams, 2015; Louizos & Welling, 2016). Specifically, 20 inputs are randomly sampled from [−4, 4], then the target value y is computed with y = x3 + n, n ∼ N (0, 9). We use a BNN with 1 hidden layer and 20 units to model the normalized inputs and targets. We also set the variance of the observation noise to the true value. We compare SSGE with implicit posteriors, Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) and Bayes-by-backprop (BBB) (Blundell et al., 2015). To better demonstrate SSGE’s gradient estimation effect, we also test SSGE with a factorized posterior, in comparison to BBB.\nWe keep 20 chains and run 100k iterations for HMC. All other methods are trained with 100 samples for 20k iterations using Adam optimizer (Kingma & Ba, 2014). For SSGE, we set J = 100. The implicit posteriors we use for weights in both layers are standard normal distributions transformed by fully connected networks with one hidden layer of 100 units.\nAs shown in Fig. 3, HMC, as the golden standard, smoothly fits the training data and outputs sensible uncertainty esti-\nmation. HMC not only produces large uncertainty outside the data region, its predictive variance also varies even in regions with training points. This kind of interpolation behavior is hard to be captured by factorized Gaussian posteriors, as shown in Fig. 3c and 3d. SSGE with implicit posteriors also has big predictive variances beyond training points, which implies that the BNN trained by SSGE is not overfitting, although it underestimates the uncertainty in the middle region. Also, we observe that SSGE can have similar interpolation behaviors as HMC (see the rightmost two points in Fig. 3b). Besides, SSGE with a factorized posterior has a similar prediction with BBB. Given the network and the variational posterior are both the same, we can attribute this similarity to accurate gradient estimation by SSGE."
  }, {
    "heading": "5.3.2. VARIATIONAL AUTOENCODERS",
    "text": "From the above example we see that SSGE enables variational posteriors parameterized by implicit distributions. To demonstrate that it scales to larger models and datasets, we adopt the settings in Shi et al. (2018) and train a deep convolutional VAE with implicit variational posteriors (Implicit VAE for short) on the CelebA dataset. As in their work, the latent dimension is 32, and the network structure of the decoder is chosen to be the same as DCGAN (Radford et al., 2015). The observation likelihoods are Gaussian distributions with trainable data-independent variances. The implicit posterior is a deep convolutional net symmetric to the decoder, with Gaussian noises injected into hidden layers. Full details of the model structures can be found in Shi et al. (2018).\nTo examine how accurate the gradient estimates provided by SSGE are, we conduct experiments under three different settings: a plain VAE with normal variational posteriors, an Implicit VAE trained with the entropy term removed from the ELBO, and an Implicit VAE using SSGE’s gradient estimates for the entropy. For SSGE, we set M = 100, and r̄ = 0.99. In Figures 4a to 4c we show samples randomly generated from the trained models. We can see that without the entropy term, the Implicit VAE tends to overfit and produces visually bad generations, while if we retain the entropy term and use SSGE to estimate its gradients, the Implicit VAE can produce realistic samples. To quantitatively measure the sample quality, we compare the Fréchet Inception Distance (FID) (Heusel et al., 2017) between real data and random generations from the models. The results are shown in Figure 4d. We can see that the Implicit VAE trained by SSGE converges faster and produces samples with slightly better quality than the plain VAE. This is probably due to that implicit posteriors are less likely to overfit (Shi et al., 2018), and SSGE gives accurate gradients for optimizing them. Besides the CelebA experiments, we also tested the models on MNIST dataset and evaluated the test log likelihoods. See Appendix D for details."
  }, {
    "heading": "6. Discussion",
    "text": "Connection to Kernel PCA As mentioned in Section 2.1, the Nyström approximation is closely related to Kernel PCA (Schölkopf et al., 1998) (KPCA), which is a powerful method for nonlinear dimension reduction. In KPCA, the input data is first projected to a (usually high-dimensional) feature space, where PCA is then applied. The operations in the feature space are handled by the kernel trick. We briefly review the method below.\nGiven a positive definite kernel k : X ×X → R, we denote the induced RKHS byH and its corresponding feature map by φ : X → H. Let the data to be analyzed be {xi}Mi=1, xi ∈ X . To simplify the derivation, we first assume the data to be centered in the feature space. Then the covariance matrix is formed as C = 1M ∑M i=1 φ(x\ni)φ(xi)>. In general, PCA requires to solve the following eigenvalue problem4:\nCv = µv. (24)\nA key observation of KPCA is that the eigenvectors lie in the span of the feature vectors, since from eq. (24) we have\nv = 1\nµM M∑ i=1 [ φ(xi)>v ] φ(xi) = M∑ i=1 αiφ(x i). (25)\nHere we use α = [α1, . . . , αM ]> to represent the coefficients. This implies that instead of directly dealing with eq. (24) we can consider a set of n projected equations: φ(xi)>Cv = µφ(xi)>v, i = 1, . . . ,M . Plugging eq. (25) here and replacing φ(xi)>φ(xj) with k(xi,xj) (the kernel trick), we get 1MKKα = µKα, which turns out an eigenvalue problem for K: 1MKα = µα. Note that this is exactly the same eigenvalue problem solved in eq. (3). As above, we denote by u1, . . . ,uJ the eigenvectors of K that correspond to the J largest eigenvalues λ1 ≥ · · · ≥ λJ , and we have µj = λj M . To determine the αs, we set the eigenvectors v to have unit lengths: v>v = α>Kα = λα>α = 1. So the\n4We reuse some notations from the above sections (e.g., the eigenvalue µ), and as we shall see, they are closely related.\nαs should be normalized to have length 1λ : αj = 1√ λj uj . For a new data point x, KPCA computes the embedding ξ(x) (the projection onto the first J eigenvectors) as ξ(x) = [φ(x)>v1, . . . , φ(x) >vJ ] > = [α>1 kx, . . . ,α > J kx]\n>, where kx = [k(x,x1), . . . , k(x,xM )]>. It was pointed out by Williams & Seeger (2000) to be equivalent to using the well understood Nyström approximation. We can see this by noticing that each component of ξ(x) is identical to eq. (6) up to a scaling factor:\nξj(x) = α > j kx = √ λj M ψ̂j(x). (26)\nLooking back at eq. (16), we can see that SSGE estimates the gradients by a linear estimator with KPCA embeddings as input features. As KPCA embeddings are known to automatically adapt to the geometry of the samples, given a suitable kernel is chosen, it can reduce the curse of dimensionality when the estimator is applied to high dimensional spaces, which helps explain the effectiveness of SSGE.\nConnection to Manifold-modeling Dimension Reduction Methods It has been pointed out in previous works (Williams, 2001; Ham et al., 2004; Bengio et al., 2004a;b) that many successful manifold-modeling dimension reduction methods (e.g., MDS, LLE, Laplacian eigenmaps, and Spectral clustering) can be viewed as KPCA with different ways of constructing data-dependent kernels. We believe it is a promising direction to learn a better kernel from a dataset of samples that could improve the manifold modeling behavior of KPCA embeddings, thus further improving the gradient estimator."
  }, {
    "heading": "7. Conclusion",
    "text": "We propose the Spectral Stein Gradient Estimator (SSGE) for implicit distributions. Unlike previous methods, SSGE directly estimates the gradient function and thus has a principled out-of-sample extension. Future work may include learning kernels or eigenfunctions in the estimator, as indicated by the error bound as well as the connection to dimension reduction methods."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank anonymous reviewers for insightful feedbacks, and thank the meta-reviewer and Chang Liu for comments on improving Theorem 1. This work was supported by NSFC Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, Siemens and Intel."
  }],
  "year": 2018,
  "references": [{
    "title": "The pseudo-marginal approach for efficient monte carlo computations",
    "authors": ["C. Andrieu", "Roberts", "G. O"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "The Numerical Treatment of Integral Equations",
    "authors": ["C.T. Baker"],
    "year": 1997
  }, {
    "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
    "authors": ["M. Belkin", "P. Niyogi"],
    "venue": "Neural computation,",
    "year": 2003
  }, {
    "title": "Learning eigenfunctions links spectral embedding and kernel PCA",
    "authors": ["Y. Bengio", "O. Delalleau", "N.L. Roux", "Paiement", "J.-F", "P. Vincent", "M. Ouimet"],
    "venue": "Neural computation,",
    "year": 2004
  }, {
    "title": "Out-of-sample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering",
    "authors": ["Y. Bengio", "Paiement", "J.-f", "P. Vincent", "O. Delalleau", "N.L. Roux", "M. Ouimet"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2004
  }, {
    "title": "Weight uncertainty in neural networks",
    "authors": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Modern multidimensional scaling: Theory and applications",
    "authors": ["I. Borg", "P.J. Groenen"],
    "venue": "Springer Science & Business Media,",
    "year": 2005
  }, {
    "title": "A kernel test of goodness of fit",
    "authors": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Provable bayesian inference via particle mirror descent",
    "authors": ["B. Dai", "N. He", "H. Dai", "L. Song"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Adversarial feature learning",
    "authors": ["J. Donahue", "P. Krähenbühl", "T. Darrell"],
    "venue": "arXiv preprint arXiv:1605.09782,",
    "year": 2016
  }, {
    "title": "Adversarially learned inference",
    "authors": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"],
    "venue": "arXiv preprint arXiv:1606.00704,",
    "year": 2016
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Measuring sample quality with stein’s method",
    "authors": ["J. Gorham", "L. Mackey"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2015
  }, {
    "title": "A kernel view of the dimensionality reduction of manifolds",
    "authors": ["J. Ham", "D.D. Lee", "S. Mika", "B. Schölkopf"],
    "venue": "In International Conference on Machine Learning, pp",
    "year": 2004
  }, {
    "title": "Probabilistic backpropagation for scalable learning of bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R. Adams"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Gans trained by a two time-scale update rule converge to a nash equilibrium",
    "authors": ["M. Heusel", "H. Ramsauer", "T. Unterthiner", "B. Nessler", "G. Klambauer", "S. Hochreiter"],
    "venue": "arXiv preprint arXiv:1706.08500,",
    "year": 2017
  }, {
    "title": "Variational inference using implicit distributions",
    "authors": ["F. Huszár"],
    "venue": "arXiv preprint arXiv:1702.08235,",
    "year": 2017
  }, {
    "title": "Estimation of non-normalized statistical models by score matching",
    "authors": ["A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "High-dimensional density ratio estimation with extensions to approximate likelihood computation",
    "authors": ["R. Izbicki", "A. Lee", "C. Schafer"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Gradient estimators for implicit models",
    "authors": ["Y. Li", "R.E. Turner"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "Two methods for wild variational inference",
    "authors": ["Q. Liu", "Y. Feng"],
    "venue": "arXiv preprint arXiv:1612.00081,",
    "year": 2016
  }, {
    "title": "Stein variational gradient descent: A general purpose bayesian inference algorithm",
    "authors": ["Q. Liu", "D. Wang"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "A kernelized stein discrepancy for goodness-of-fit tests",
    "authors": ["Q. Liu", "J. Lee", "M. Jordan"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Structured and efficient variational deep learning with matrix gaussian posteriors",
    "authors": ["C. Louizos", "M. Welling"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks",
    "authors": ["L. Mescheder", "S. Nowozin", "A. Geiger"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Learning in implicit generative models",
    "authors": ["S. Mohamed", "B. Lakshminarayanan"],
    "venue": "arXiv preprint arXiv:1610.03483,",
    "year": 2016
  }, {
    "title": "Slice sampling covariance hyperparameters of latent gaussian models",
    "authors": ["I. Murray", "R.P. Adams"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Mcmc using hamiltonian dynamics",
    "authors": ["Neal", "R. M"],
    "venue": "Handbook of Markov Chain Monte Carlo,",
    "year": 2011
  }, {
    "title": "Über die praktische auflösung von integralgleichungen mit anwendungen auf randwertaufgaben",
    "authors": ["E.J. Nyström"],
    "venue": "Acta Mathematica,",
    "year": 1930
  }, {
    "title": "The lanczos algorithm with selective orthogonalization",
    "authors": ["B.N. Parlett", "D.S. Scott"],
    "venue": "Mathematics of computation,",
    "year": 1979
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "arXiv preprint arXiv:1511.06434,",
    "year": 2015
  }, {
    "title": "Operator variational inference",
    "authors": ["R. Ranganath", "D. Tran", "J. Altosaar", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "On learning with integral operators",
    "authors": ["L. Rosasco", "M. Belkin", "E.D. Vito"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Nonlinear dimensionality reduction by locally linear embedding",
    "authors": ["S.T. Roweis", "L.K. Saul"],
    "venue": "Science,",
    "year": 2000
  }, {
    "title": "Nonlinear component analysis as a kernel eigenvalue problem",
    "authors": ["B. Schölkopf", "A. Smola", "Müller", "K.-R"],
    "venue": "Neural computation,",
    "year": 1998
  }, {
    "title": "Kernel adaptive metropolis-hastings",
    "authors": ["D. Sejdinovic", "H. Strathmann", "M.L. Garcia", "C. Andrieu", "A. Gretton"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "ZhuSuan: A library for Bayesian deep learning",
    "authors": ["J. Shi", "J. Chen", "J. Zhu", "S. Sun", "Y. Luo", "Y. Gu", "Y. Zhou"],
    "venue": "arXiv preprint arXiv:1709.05870,",
    "year": 2017
  }, {
    "title": "Kernel implicit variational inference",
    "authors": ["J. Shi", "S. Sun", "J. Zhu"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "Semi-supervised learning using sparse eigenfunction bases",
    "authors": ["K. Sinha", "M. Belkin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Density estimation in infinite dimensional exponential families",
    "authors": ["B. Sriperumbudur", "K. Fukumizu", "A. Gretton", "A. Hyvärinen", "R. Kumar"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "Estimation of the mean of a multivariate normal distribution",
    "authors": ["C.M. Stein"],
    "venue": "The Annals of Statistics,",
    "year": 1981
  }, {
    "title": "Gradient-free hamiltonian monte carlo with efficient kernel exponential families",
    "authors": ["H. Strathmann", "D. Sejdinovic", "S. Livingstone", "Z. Szabo", "A. Gretton"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Efficient and principled score estimation with nystrm kernel exponential families",
    "authors": ["D. Sutherland", "H. Strathmann", "M. Arbel", "A. Gretton"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2018
  }, {
    "title": "Hierarchical implicit models and likelihood-free variational inference",
    "authors": ["D. Tran", "R. Ranganath", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Segmentation using eigenvectors: a unifying view",
    "authors": ["Y. Weiss"],
    "venue": "In The proceedings of the seventh IEEE international conference on computer vision,",
    "year": 1999
  }, {
    "title": "The effect of the input density distribution on kernel-based classifiers",
    "authors": ["C. Williams", "M. Seeger"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2000
  }, {
    "title": "On a connection between kernel PCA and metric multidimensional scaling",
    "authors": ["C.K. Williams"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2001
  }, {
    "title": "Using the Nyström method to speed up kernel machines",
    "authors": ["C.K. Williams", "M. Seeger"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2001
  }],
  "id": "SP:b1e669189d0dc1508654aae9adce35b8a50f151f",
  "authors": [{
    "name": "Jiaxin Shi",
    "affiliations": []
  }, {
    "name": "Shengyang Sun",
    "affiliations": []
  }, {
    "name": "Jun Zhu",
    "affiliations": []
  }],
  "abstractText": "Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein’s identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nyström method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-ofsample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nyström method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.",
  "title": "A Spectral Approach to Gradient Estimation for Implicit Distributions"
}