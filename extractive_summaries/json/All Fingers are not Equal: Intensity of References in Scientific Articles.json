{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1348–1358, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "With more than one hundred thousand new scholarly articles being published each year, there is a rapid growth in the number of citations for the relevant scientific articles. In this context, we highlight the following interesting facts about the process of citing scientific articles: (i) the most commonly cited paper by Gerard Salton, titled “A Vector Space Model for Information Retrieval” (alleged to have been published in 1975) does not actually exist in reality (Dubin, 2004), (ii) the scientific authors read only 20% of the works they cite (Simkin and Roychowdhury, 2003), (iii) one third of the refer-\nences in a paper are redundant and 40% are perfunctory (Moravcsik and Murugesan, 1975), (iv) 62.7% of the references could not be attributed a specific function (definition, tool etc.) (Teufel et al., 2006). Despite these facts, the existing bibliographic metrics consider that all citations are equally significant.\nIn this paper, we would emphasize the fact that all the references of a paper are not equally influential. For instance, we believe that for our current paper, (Wan and Liu, 2014) is more influential reference than (Garfield, 2006), although the former has received lower citations (9) than the latter (1650) so far1. Therefore the influence of a cited paper completely depends upon the context of the citing paper, not the overall citation count of the cited paper. We further took the opinion of the original authors of few selective papers and realized that around 16% of the references in a paper are highly influential, and the rest are trivial (Section 4). This motivates us to design a prediction model, GraLap to automatically label the influence of a cited paper with respect to a citing paper. Here, we label paper-reference pairs rather than references alone, because a reference that is influential for one citing paper may not be influential with equal extent for another citing paper.\nWe experiment with ACL Anthology Network (AAN) dataset and show that GraLap along with the novel feature set, quite efficiently, predicts the intensity of references of papers, which achieves (Pearson) correlation of 0.90 with the human annotations. Finally, we present four interesting appli-\n1The statistics are taken from Google Scholar on June 2, 2016.\n1348\ncations to show the efficacy of considering unequal intensity of references, compared to the uniform intensity.\nThe contributions of the paper are four-fold: (i) we acquire a rich annotated dataset where paperreference pairs are labeled based on the influence scores (Section 4), which is perhaps the first goldstandard for this kind of task; (ii) we propose a graph-based label propagation model GraLap for semi-supervised learning which has tremendous potential for any task where the training set is less in number and labels are non-uniformly distributed (Section 3); (iii) we propose a diverse set of features (Section 3.3); most of them turn out to be quite effective to fit into the prediction model and yield improved results (Section 5); (iv) we present four applications to show how incorporating the reference intensity enhances the performance of several stateof-the-art systems (Section 6)."
  }, {
    "heading": "2 Defining Intensity of References",
    "text": "All the references of a paper usually do not carry equal intensity/strength with respect to the citing paper because some papers have influenced the research more than others. To pin down this intuition, here we discretize the reference intensity by numerical values within the range of 1 to 5, (5: most influential, 1: least influential). The appropriate definitions of different labels of reference intensity are presented in Figure 1, which are also the basis of building the annotated dataset (see Section 4):\nNote that “reference intensity” and “reference similarity” are two different aspects. It might happen that two similar reference are used with different intensity levels in a citing paper – while one is just mentioned somewhere in the paper and other is used as a baseline. Here, we address the former problem as a semi-supervised learning problem with clues taken from content of the citing and cited papers."
  }, {
    "heading": "3 Reference Intensity Prediction Model",
    "text": "In this section, we formally define the problem and introduce our prediction model."
  }, {
    "heading": "3.1 Problem Definition",
    "text": "We are given a set of papers P = {P1, P2, ..., PM} and a sets of references R = {R1, R2, ..., RM}, where Ri corresponds to the set of references (or cited papers) of Pi. There is a set of papers PL ∈ P whose references RL ∈ R are already labeled by ` ∈ L = {1, ..., 5} (each reference is labeled with exactly one value). Our objective is to define a predictive function f that labels the references RU ∈ {R \\ RL} of the papers PU ∈ {P \\ PL} whose reference intensities are unknown, i.e., f : (P,R, PL, RL, PU , RL) −→ L.\nSince the size of the annotated (labeled) data is much smaller than unlabeled data (|PL| |PU |), we consider it as a semi-supervised learning problem.\nDefinition 1. (Semi-supervised Learning) Given a set of entries X and a set of possible labels YL, let us assume that (x1, y1), (x2, y2),..., (xl, yl) be the set of labeled data where xi is a data point and yi ∈ YL is its corresponding label. We assume that at least one instance of each class label\nis present in the labeled dataset. Let (xl+1, yl+1), (xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled data points where YU = {yl+1, yl+2, ...yl+u} are unknown. Each entry x ∈ X is represented by a set of features {f1, f2, ..., fD}. The problem is to determine the unknown labels using X and YL.\n3.2 GraLap: A Prediction Model We propose GraLap, a variant of label propagation (LP) model proposed by (Zhu et al., 2003) where a node in the graph propagates its associated label to its neighbors based on the proximity. We intend to assign same label to the vertices which are closely connected. However unlike the traditional LP model where the original values of the labels continue to fade as the algorithm progresses, we systematically handle this problem in GraLap. Additionally, we follow a post-processing in order to handle “classimbalance problem”. Graph Creation. The algorithm starts with the creation of a fully connected weighted graph G = (X,E) where nodes are data points and the weight wij of each edge eij ∈ E is determined by the radial basis function as follows:\nwij = exp\n( − ∑D\nd=1(x d i − xdj )2 σ2\n) (1)\nThe weight is controlled by a parameter σ. Later in this section, we shall discuss how σ is selected. Each node is allowed to propagate its label to its neighbors through edges (the more the edge weight, the easy to propagate). Transition Matrix. We create a probabilistic transition matrix T|X|×|X|, where each entry Tij indicates the probability of jumping from j to i based on the following: Tij = P (j → i) = wij∑|X|\nk=1 wkj .\nLabel Matrix. Here, we allow a soft label (interpreted as a distribution of labels) to be associated with each node. We then define a label matrix Y|X|×|L|, where ith row indicates the label distribution for node xi. Initially, Y contains only the values of the labeled data; others are zero. Label Propagation Algorithm. This algorithm works as follows:\nAfter initializing Y and T , the algorithm starts by disseminating the label from one node to its neighbors (including self-loop) in one step (Step 3). Then we normalize each entry of Y by the sum of its cor-\n1: Initialize T and Y 2: while (Y does not converge) do 3: Y ← TY 4: Normalize rows of Y , yij =\nyij∑ k yik\n5: Reassign original labels to XL\nresponding row in order to maintain the interpretation of label probability (Step 4). Step 5 is crucial; here we want the labeled sources XL to be persistent. During the iterations, the initial labeled nodes XL may fade away with other labels. Therefore we forcefully restore their actual label by setting yil = 1 (if xi ∈ XL is originally labeled as l), and other entries (∀j 6=lyij) by zero. We keep on “pushing” the labels from the labeled data points which in turn pushes the class boundary through high density data points and settles in low density space. In this way, our approach intelligently uses the unlabeled data in the intermediate steps of the learning. Assigning Final Labels. Once YU is computed, one may take the most likely label from the label distribution for each unlabeled data. However, this approach does not guarantee the label proportion observed in the annotated data (which in this case is not well-separated as shown in Section 4). Therefore, we adopt a label-based normalization technique. Assume that the label proportions in the labeled data are c1, ..., c|L| (s.t. ∑|L| i=1 ci = 1). In case of YU , we try to balance the label proportion observed in the ground-truth. The label mass is the column sum of YU , denoted by YU.1 , ..., YU.|L| , each of which is scaled in such a way that YU.1 : ... : YU.|L| = c1 : ... : c|L|. The label of an unlabeled data point is finalized as the label with maximum value in the row of Y . Convergence. Here we briefly show that our algorithm is guaranteed to converge. Let us combine Steps 3 and 4 as Y ← T̂ Y , where T̂ = Tij/ ∑ k Tik. Y is composed of YLl×|L| and YUu×|L| , where YU never changes because of the reassignment. We can split T̂ at the boundary of labeled and unlabeled data as follows:\nF̂ = [ T̂ll T̂lu T̂ul T̂uu ]\nTherefore, YU ← T̂uuYU+ T̂ulYL, which can lead to YU = limn→∞ T̂nuuY 0 + [ ∑n i=1 T̂ (i−1) uu ]T̂ulYL, where Y 0 is the shape of Y at iteration 0. We need\nto show T̂nuuijY 0 ← 0. By construction, T̂ij ≥ 0, and since T̂ is row-normalized, and T̂uu is a part of T̂ , it leads to the following condition: ∃γ < 1, ∑u\nj=1 T̂uuij ≤ γ, ∀i = 1, ..., u. So, ∑\nj\nT̂nuuij = ∑\nj\n∑\nk\nT̂ (n−1) uuik T̂uukj\n= ∑\nk\nT̂ (n−1) uuik\n∑\nj\nT̂uuik\n≤ ∑\nk\nT̂ (n−1) uuik γ\n≤ γn\nTherefore, the sum of each row in T̂nuuij converges to zero, which indicates T̂nuuijY\n0 ← 0. Selection of σ. Assuming a spatial representation of data points, we construct a minimum spanning tree using Kruskal’s algorithm (Kruskal, 1956) with distance between two nodes measured by Euclidean distance. Initially, no nodes are connected. We keep on adding edges in increasing order of distance. We choose the distance (say, df ) of the first edge which connects two components with different labeled points in them. We consider df as a heuristic to the minimum distance between two classes, and arbitrarily set σ = d0/3, following 3σ rule of normal distribution (Pukelsheim, 1994)."
  }, {
    "heading": "3.3 Features for Learning Model",
    "text": "We use a wide range of features that suitably represent a paper-reference pair (Pi, Rij), indicating Pi refers to Pj through reference Rij . These features can be grouped into six general classes. 3.3.1 Context-based Features (CF)\nThe “reference context” of Rij in Pi is defined by three-sentence window (sentence where Rij occurs and its immediate previous and next sentences). For multiple occurrences, we calculate its average score. We refer to “reference sentence” to indicate the sentence where Rij appears. (i) CF:Alone. It indicates whether Rij is mentioned alone in the reference context or together with other references. (ii) CF:First. When Rij is grouped with others, this feature indicates whether it is mentioned first (e.g., “[2]” is first in “[2,4,6]”).\nNext four features are based on the occurrence of words in the corresponding lists created manually (see Table 1) to understand different aspects.\n(iii) CF:Relevant. It indicates whether Rij is explicitly mentioned as relevant in the reference context (Rel in Table 1). (iv) CF:Recent. It tells whether the reference context indicates that Rij is new (Rec in Table 1). (v) CF:Extreme. It implies that Rij is extreme in some way (Ext in Table 1). (vi) CF:Comp. It indicates whether the reference context makes some kind of comparison with Rij (Comp in Table 1).\nNote we do not consider any sentiment-based features as suggested by (Zhu et al., 2015). 3.3.2 Similarity-based Features (SF)\nIt is natural that the high degree of semantic similarity between the contents of Pi and Pj indicates the influence of Pj in Pi. We assume that although the full text of Pi is given, we do not have access to the full text of Pj (may be due to the subscription charge or the unavailability of the older papers). Therefore, we consider only the title of Pj as a proxy of its full text. Then we calculate the cosine-similarity2 between the title (T) of Pj and (i) SF:TTitle. the title, (ii) SF:TAbs. the abstract, SF:TIntro. the introduction, (iv) SF:TConcl. the conclusion, and (v) SF:TRest. the rest of the sections (sections other than abstract, introduction and conclusion) of Pi.\nWe further assume that the “reference context” (RC) of Pj in Pi might provide an alternate way of summarizing the usage of the reference. Therefore, we take the same similarity based approach mentioned above, but replace the title of Pj with its RC and obtain five more features: (vi) SF:RCTitle, (vii) SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and (x) SF:RCRest. If a reference appears multiple times in a citing paper, we consider the aggregation of all RCs together."
  }, {
    "heading": "3.3.3 Frequency-based Feature (FF)",
    "text": "The underlying assumption of these features is that a reference which occurs more frequently in a citing paper is more influential than a single occurrence (Singh et al., 2015). We count the frequency of Rij in (i) FF:Whole. the entire content, (ii) FF:Intro. the introduction, (iii) FF:Rel. the related work, (iv) FF:Rest. the rest of the sections (as\n2We use the vector space based model (Turney and Pantel, 2010) after stemming the words using Porter stammer (Porter, 1997).\nmentioned in Section 3.3.2) of Pi. We also introduce (v) FF:Sec. to measure the fraction of different sections of Pi where Rij occurs (assuming that appearance of Rij in different sections is more influential). These features are further normalized using the number of sentences in Pi in order to avoid unnecessary bias on the size of the paper."
  }, {
    "heading": "3.3.4 Position-based Features (PF)",
    "text": "Position of a reference in a paper might be a predictive clue to measure the influence (Zhu et al., 2015). Intuitively, the earlier the reference appears in the paper, the more important it seems to us. For the first two features, we divide the entire paper into two parts equally based on the sentence count and then see whether Rij appears (i) PF:Begin. in the beginning or (ii) PF:End. in the end of Pi. Importantly, if Rij appears multiple times in Pi, we consider the fraction of times it occurs in each part.\nFor the other two features, we take the entire paper, consider sentences as atomic units, and measure position of the sentences where Rij appears, including (iii) PF:Mean. mean position of appearance, (iv) PF:Std. standard deviation of different appearances. These features are normalized by the total length (number of sentences) of Pi. , thus ranging from 0 (indicating beginning of Pi) to 1 (indicating the end of Pi)."
  }, {
    "heading": "3.3.5 Linguistic Features (LF)",
    "text": "The linguistic evidences around the context ofRij sometimes provide clues to understand the intrinsic influence of Pj on Pi. Here we consider word level and structural features. (i) LF:NGram. Different levels of n-grams (1- grams, 2-grams and 3-grams) are extracted from the reference context to see the effect of different word combination (Athar and Teufel, 2012).\n(ii) LF:POS. Part-of-speech (POS) tags of the words in the reference sentence are used as features (Jochim and Schütze, 2012). (iii) LF:Tense. The main verb of the reference sentence is used as a feature (Teufel et al., 2006). (iv) LF:Modal. The presence of modal verbs (e.g., “can”, “may”) often indicates the strength of the claims. Hence, we check the presence of the modal verbs in the reference sentence. (v) LF:MainV. We use the main-verb of the reference sentence as a direct feature in the model. (vi) LF:hasBut. We check the presence of conjunction “but”, which is another clue to show less confidence on the cited paper. (vii) LF:DepRel. Following (Athar and Teufel, 2012) we use all the dependencies present in the reference context, as given by the dependency parser (Marneffe et al., 2006). (viii) LF:POSP. (Dong and Schfer, 2011) use seven regular expression patterns of POS tags to capture syntactic information; then seven boolean features mark the presence of these patterns. We also utilize the same regular expressions as shown below 3 with the examples (the empty parenthesis in each example indicates the presence of a reference token Rij in the corresponding sentence; while few examples are complete sentences, few are not):\n• “.*\\\\(\\\\) VV[DPZN].*”: Chen () showed that cohesion is held in the vast majority of cases for English-French.\n• “.*(VHP|VHZ) VV.*”: while Cherry and Lin () have shown it to be a strong feature for word alignment...\n• “.*VH(D|G|N|P|Z) (RB )*VBN.*”: Inducing features for taggers by clustering has been tried by several researchers ().\n• “.*MD (RB )*VB(RB )* VVN.*”: For example, the likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair ().\n3The meaning of each POS tag can be found in http://nlp.stanford.edu/software/tagger. shtml(Toutanova and Manning, 2000).\n• “[ IW.]*VB(D|P|Z) (RB )*VV[ND].*”: Our experimental set-up is modeled after the human evaluation presented in ().\n• “(RB )*PP (RB )*V.*”: We use CRF () to perform this tagging. • “.*VVG (NP )*(CC )*(NP ).*”: Following (), we provide the an-\nnotators with only short sentences: those with source sentences between 10 and 25 tokens long.\nThese are all considered as Boolean features. For each feature, we take all the possible evidences from all paper-reference pairs and prepare a vector. Then for each pair, we check the presence (absence) of tokens for the corresponding feature and mark the vector accordingly (which in turn produces a set of Boolean features)."
  }, {
    "heading": "3.3.6 Miscellaneous Features (MS)",
    "text": "This group provides other factors to explain why is a paper being cited. (i) MS:GCount. To answer whether a highly-cited paper has more academic influence on the citing paper than the one which is less cited, we measure the number of other papers (except Pi) citing Pj . (ii) MS:SelfC. To see the effect of self-citation, we check whether at least one author is common in both Pi and Pj . (iii) MG:Time. The fact that older papers are rarely cited, may not stipulate that these are less influential. Therefore, we measure the difference of the publication years of Pi and Pj . (iv) MG:CoCite. It measures the co-citation counts of Pi and Pj defined by\n|Ri∩Rj | |Ri∪Rj | , which in turn an-\nswers the significance of reference-based similarity driving the academic influence (Small, 1973).\nFollowing (Witten and Frank, 2005), we further make one step normalization and divide each feature by its maximum value in all the entires."
  }, {
    "heading": "4 Dataset and Annotation",
    "text": "We use the AAN dataset (Radev et al., 2009) which is an assemblage of papers included in ACL related venues. The texts are preprocessed where sentences, paragraphs and sections are properly separated using different markers. The filtered dataset contains 12,843 papers (on average 6.21 references per paper) and 11,092 unique authors.\nNext we use Parscit (Councill et al., 2008) to identify the reference contexts from the dataset and then extract the section headings from all the papers. Then each section heading is mapped into one\nof the following broad categories using the method proposed by (Liakata et al., 2012): Abstract, Introduction, Related Work, Conclusion and Rest. Dataset Labeling. The hardest challenge in this task is that there is no publicly available dataset where references are annotated with the intensity value. Therefore, we constructed our own annotated dataset in two different ways. (i) Expert Annotation: we requested members of our research group4 to participate in this survey. To facilitate the labeling process, we designed a portal where all the papers present in our dataset are enlisted in a drop-down menu. Upon selecting a paper, its corresponding references were shown with five possible intensity values. The citing and cited papers are also linked to the original texts so that the annotators can read the original papers. A total of 20 researchers participated and they were asked to label as many paperreference pairs as they could based on the definitions of the intensity provided in Section 2. The annotation process went on for one month. Out of total 1640 pairs annotated, 1270 pairs were taken such that each pair was annotated by at least two annotators, and the final intensity value of the pair was considered to be the average of the scores. The Pearson correlation and Kendell’s τ among the annotators are 0.787 and 0.712 respectively. (ii) Author Annotation: we believe that the authors of a paper are the best experts to judge the intensity of references present in the paper. With this intension, we launched a survey where we requested the authors whose papers are present in our dataset with significant numbers. We designed a web portal in similar fashion mentioned earlier; but each author was only shown her own papers in the drop-down menu. Out of 35 requests, 22 authors responded and total 196 pairs are annotated. This time we made sure that each paper-reference pair was annotated by only one author. The percentages of labels in the overall annotated dataset are as follows: 1: 9%, 2: 74%, 3: 9%, 4: 3%, 5: 4%."
  }, {
    "heading": "5 Experimental Results",
    "text": "In this section, we start with analyzing the importance of the feature sets in predicting the reference\n4All were researchers with the age between 25-45 working on document summarization, sentiment analysis, and text mining in NLP.\nintensity, followed by the detailed results. Feature Analysis. In order to determine which features highly determine the gold-standard labeling, we measure the Pearson correlation between various features and the ground-truth labels. Figure 2(a) shows the average correlation for each feature group, and in each group the rank of features based on the correlation is shown in Figure 2(b). Frequencybased features (FF) turn out to be the best, among which FF:Rest is mostly correlated. This set of features is convenient and can be easily computed. Both CF and LF seem to be equally important. However, PF tends to be less important in this task.\nResults of Predictive Models. For the purpose of evaluation, we report the average results after 10- fold cross-validation. Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in (Wan and Liu, 2014), (iii) SVR+O: SVR model with our feature set, (iv) C4.5SSL: C4.5 semisupervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set (Zhu et al., 2003). Three metrics are used to compare the results of the competing models with the annotated labels: Root Mean Square Error (RMSE), Pearson’s correlation coeffi-\ncient (ρ), and coefficient of determination (R2)5. Table 2 shows the performance of the competing models. We incrementally include each feature set into GraLap greedily on the basis of ranking shown in Figure 2(a). We observe that GraLap with only FF outperforms SVR+O with 41% improvement of ρ. As expected, the inclusion of PF into the model improves the model marginally. However, the overall performance of GraLap is significantly higher than any of the baselines (p < 0.01)."
  }, {
    "heading": "6 Applications of Reference Intensity",
    "text": "In this section, we provide four different applications to show the use of measuring the intensity of references. To this end, we consider all the labeled entries for training and run GraLap to predict the intensity of rest of the paper-reference pairs."
  }, {
    "heading": "6.1 Discovering Influential Articles",
    "text": "Influential papers in a particular area are often discovered by considering equal weights to all the citations of a paper. We anticipate that considering the reference intensity would perhaps return more meaningful results. To show this, Here we use the following measures individually to compute the influence of a paper: (i) RawCite: total number of citations per paper, (ii) RawPR: we construct a citation network (nodes: papers, links: citations), and measure PageRank (Page et al., 1998) of each node n: PR(n) = 1−qN + q ∑ m∈M(n) PR(m) |L(m)| ; where, q, the damping factor, is set to 0.85, N is the total number of nodes, M(n) is the set of nodes that have edges to n, and L(m) is the set of nodes that m has an edge to, (iii) InfCite: the weighted version of RawCite, measured by the sum of intensities of all citations of a paper, (iv) InfPR: the weighted version of RawPR: PR(n) = 1−qN + q ∑\nm∈M(n) Inf(m→n)PR(m)∑\na∈L(m)Inf(m→a) , where Inf indicates\nthe influence of a reference. We rank all the articles based on these four measures separately. Table 3(a) shows the Spearman’s rank correlation between pair-wise measures. As expected, (i) and (ii) have high correlation (same for (iii) and (iv)), whereas across two types of measures the correlation is less. Further, in order to know which mea-\n5The less (resp. more) the value of RMSE and R2 (resp. ρ), the better the performance of the models.\nsure is more relevant, we conduct a subjective study where we select top ten papers from each measure and invite the experts (not authors) who annotated the dataset, to make a binary decision whether a recommended paper is relevant. 6. The average pairwise inter-annotator’s agreement (based on Cohen’s kappa (Cohen, 1960)) is 0.71. Table 3(b) presents that out of 10 recommendations of InfPR, 7 (5) papers are marked as influential by majority (all) of the annotators, which is followed by InfCite. These results indeed show the utility of measuring reference intensity for discovering influential papers. Top three papers based on InfPR from the entire dataset are shown in Table 4."
  }, {
    "heading": "6.2 Identifying Influential Authors",
    "text": "H-index, a measure of impact/influence of an author, considers each citation with equal weight (Hirsch, 2005). Here we incorporate the notion of reference intensity into it and define hif-index.\nDefinition 2. An author A with a set of papers P (A) has an hif-index equals to h, if h is the largest value such that |{p ∈ P (A)|Inf(p) ≥ h}| ≥ h; where Inf(p) is the sum of intensities of all citations of p.\nWe consider 37 ACL fellows as the list of goldstandard influential authors. For comparative evaluation, we consider the total number of papers (TotP), total number of citations (TotC) and average citations per paper (AvgC) as three competing measures along with h-index and hif-index. We arrange all the authors in our dataset in decreasing order of each measure. Figure 3(a) shows the Spearman’s rank correlation among the common elements across pair-wise rankings. Figure 3(b) shows the Precision@k for five competing measures at identifying ACL fellows. We observe that hif-index performs significantly well with an overall precision of 0.54, followed by AvgC (0.37),\n6We choose papers from the area of “sentiment analysis” on which experts agree on evaluating the papers.\nh-index (0.35), TotC (0.32) and TotP (0.34). This result is an encouraging evidence that the referenceintensity could improve the identification of the influential authors. Top three authors based on hif-index are shown in Table 4."
  }, {
    "heading": "6.3 Effect on Recommendation System",
    "text": "Here we show the effectiveness of referenceintensity by applying it to a real paper recommendation system. To this end, we consider FeRoSA7 (Chakraborty et al., 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facetwise recommendations with each facet representing the purpose of recommendation (Chakraborty et al., 2016). The methodology is based on random walk with restarts (RWR) initiated from a query paper. The model is built on AAN dataset and considers both the citation links and the content information to produce the most relevant results. Instead of using the unweighted citation network, here we use the weighted network with each edge labeled by the intensity score. The final recommendation of FeRoSA is obtained by performing RWR with the transition probability proportional to the edge-weight (we call it Inf-FeRoSA). We observe that Inf-FeRoSA achieves an average precision of 0.81 at top 10 recommendations, which is 14% higher then FeRoSA while considering the flat version and 12.34% higher than FeRoSA while considering the faceted version."
  }, {
    "heading": "6.4 Detecting Citation Stacking",
    "text": "Recently, Thomson Reuters began screening for journals that exchange large number of anomalous citations with other journals in a cartel-like arrangement, often known as “citation stacking” (Jump, 2013; Hardcastle, 2015). This sort of citation stacking is much more pernicious and difficult to detect.\n7www.ferosa.org\nWe anticipate that this behavior can be detected by the reference intensity. Since the AAN dataset does not have journal information, we use DBLP dataset (Singh et al., 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al., 2014)). From this dataset, we extract all the features mentioned in Section 3.3 except the ones that require full text, and run our model using the existing annotated dataset as training instances. We measure the traditional impact factor (IF ) of the journals and impact factor after considering the reference intensity (IFif ). Figure 4(a) shows that there are few journals whose IFif significantly deviates (3σ from the mean) from IF ; out of the suspected journals 70% suffer from the effect of self-journal citations as well (shown in Figure 4(b)), example including Expert Systems with Applications (current IF of 2.53). One of the future work directions would be to predict such journals as early as possible after their first appearance."
  }, {
    "heading": "7 Related Work",
    "text": "Although the citation count based metrics are widely accepted (Garfield, 2006; Hirsch, 2010), the belief that mere counting of citations is dubious has also been a subject of study (Chubin and Moitra, 1975). (Garfield, 1964) was the first who explained the reasons of citing a paper. (Pham and Hoffmann, 2003) introduced a method for the rapid development of complex rule bases for classifying text segments.\n(Dong and Schfer, 2011) focused on a less manual approach by learning domain-insensitive features from textual, physical, and syntactic aspects To address concerns about h-index, different alternative measures are proposed (Waltman and van Eck, 2012). However they too could benefit from filtering or weighting references with a model of influence. Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al., 2011). Recently, (Wan and Liu, 2014) proposed a SVR based approach to measure the intensity of citations. Our methodology differs from this approach in at lease four significant ways: (i) they used six very shallow level features; whereas we consider features from different dimensions, (ii) they labeled the dataset by the help of independent annotators; here we additionally ask the authors of the citing papers to identify the influential references which is very realistic (Gilbert, 1977); (iii) they adopted SVR for labeling, which does not perform well for small training instances; here we propose GraLap , designed specifically for small training instances; (iv) four applications of reference intensity mentioned here are completely new and can trigger further to reassessing the existing bibliometrics."
  }, {
    "heading": "8 Conclusion",
    "text": "We argued that the equal weight of all references might not be a good idea not only to gauge success of a research, but also to track follow-up work or recommending research papers. The annotated dataset would have tremendous potential to be utilized for other research. Moreover, GraLap can be used for any semi-supervised learning problem. Each application mentioned here needs separate attention. In future, we shall look into more linguistic evidences to improve our model."
  }],
  "year": 2016,
  "references": [{
    "title": "Contextenhanced citation sentiment detection",
    "authors": ["Awais Athar", "Simone Teufel."],
    "venue": "NAACL, pages 597–601, Stroudsburg, PA, USA. ACL.",
    "year": 2012
  }, {
    "title": "Positive and negative aspects of citation indices and journal impact factors",
    "authors": ["Alexandru T. Balaban."],
    "venue": "Scientometrics, 92(2):241–247.",
    "year": 2012
  }, {
    "title": "A maximum entropy approach to natural language processing",
    "authors": ["Adam L. Berger", "Vincent J. Della Pietra", "Stephen A. Della Pietra."],
    "venue": "Comput. Linguist., 22(1):39–71, March.",
    "year": 1996
  }, {
    "title": "Towards a stratified learning approach to predict future citation counts",
    "authors": ["Tanmoy Chakraborty", "Suhansanu Kumar", "Pawan Goyal", "Niloy Ganguly", "Animesh Mukherjee."],
    "venue": "Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries,",
    "year": 2014
  }, {
    "title": "Advances in Knowledge Discovery",
    "authors": ["Tanmoy Chakraborty", "Amrith Krishna", "Mayank Singh", "Niloy Ganguly", "Pawan Goyal", "Animesh Mukherjee"],
    "year": 2016
  }, {
    "title": "An unsupervised method for detecting grammatical errors",
    "authors": ["Martin Chodorow", "Claudia Leacock."],
    "venue": "NAACL, pages 140–147, Stroudsburg, PA, USA. Association for Computational Linguistics.",
    "year": 2000
  }, {
    "title": "Content-Analysis of References Adjunct or Alternative to Citation Counting",
    "authors": ["D.E. Chubin", "S.D. Moitra."],
    "venue": "Social studies of science, 5(4):423–441.",
    "year": 1975
  }, {
    "title": "A Coefficient of Agreement for Nominal Scales",
    "authors": ["J. Cohen."],
    "venue": "Educational and Psychological Measurement, 20(1):37–41.",
    "year": 1960
  }, {
    "title": "Parscit: an open-source crf reference string parsing package",
    "authors": ["Isaac G Councill", "C Lee Giles", "Min-Yen Kan."],
    "venue": "LREC, pages 28–30, Marrakech, Morocco.",
    "year": 2008
  }, {
    "title": "Applying weighted pagerank to author citation networks",
    "authors": ["Ying Ding."],
    "venue": "JASIST, 62(2):236–245.",
    "year": 2011
  }, {
    "title": "Ensemble-style self-training on citation classification",
    "authors": ["Cailing Dong", "Ulrich Schfer."],
    "venue": "IJCNLP, pages 623–631. ACL, 11.",
    "year": 2011
  }, {
    "title": "The most influential paper gerard salton never wrote",
    "authors": ["David Dubin."],
    "venue": "Library Trends, 52(4):748–764.",
    "year": 2004
  }, {
    "title": "Can citation indexing be automated? Statistical association methods for mechanized documentation",
    "authors": ["Eugene Garfield"],
    "venue": "Symposium proceedings,",
    "year": 1964
  }, {
    "title": "The History and Meaning of the Journal Impact Factor",
    "authors": ["Eugene Garfield."],
    "venue": "JAMA, 295(1):90–93.",
    "year": 2006
  }, {
    "title": "Referencing as persuasion",
    "authors": ["G.N. Gilbert."],
    "venue": "Social Studies of Science, 7(1):113–122.",
    "year": 1977
  }, {
    "title": "Citations, self-citations, and citation stacking, http://editorresources",
    "authors": ["James Hardcastle"],
    "year": 2015
  }, {
    "title": "An index to quantify an individual’s scientific research output",
    "authors": ["J.E. Hirsch."],
    "venue": "PNAS, 102(46):16569– 16572.",
    "year": 2005
  }, {
    "title": "An index to quantify an individual’s scientific research output that takes into account the effect of multiple coauthorship",
    "authors": ["J.E. Hirsch."],
    "venue": "Scientometrics, 85(3):741–754, December.",
    "year": 2010
  }, {
    "title": "Counting citations in texts rather than reference lists to improve the accuracy of assessing scientific contribution",
    "authors": ["Wen-Ru Hou", "Ming Li", "Deng-Ke Niu."],
    "venue": "BioEssays, 33(10):724–727.",
    "year": 2011
  }, {
    "title": "Towards a generic and flexible citation classifier based on a faceted classification scheme",
    "authors": ["Charles Jochim", "Hinrich Schütze."],
    "venue": "COLING, pages 1343–1358, Bombay, India.",
    "year": 2012
  }, {
    "title": "On the Shortest Spanning Subtree of a Graph and the Traveling Salesman Problem",
    "authors": ["J.B. Kruskal."],
    "venue": "Proceedings of the American Mathematical Society, volume 7, pages 48–50.",
    "year": 1956
  }, {
    "title": "Automatic recognition of conceptualization zones in scientific articles and two life science applications",
    "authors": ["Maria Liakata", "Shyamasree Saha", "Simon Dobnik", "Colin R. Batchelor", "Dietrich Rebholz-Schuhmann."],
    "venue": "Bioinformatics, 28(7):991–1000.",
    "year": 2012
  }, {
    "title": "Generating typed dependency parses from phrase structure parses",
    "authors": ["M. Marneffe", "B. Maccartney", "C. Manning."],
    "venue": "LREC, pages 449–454, Genoa, Italy, May. European Language Resources Association (ELRA).",
    "year": 2006
  }, {
    "title": "Some results on the function and quality of citations",
    "authors": ["M.J. Moravcsik", "P. Murugesan."],
    "venue": "Social studies of science, 5(1):86–92.",
    "year": 1975
  }, {
    "title": "The pagerank citation ranking: Bringing order to the web",
    "authors": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd."],
    "venue": "WWW, pages 161–172, Brisbane, Australia.",
    "year": 1998
  }, {
    "title": "A new approach for scientific citation classification using cue phrases",
    "authors": ["Son Bao Pham", "Achim Hoffmann."],
    "venue": "Tamas Domonkos Gedeon and Lance Chun Che Fung, editors, Advances in Artificial Intelligence: 16th Australian Conference on AI, pages 759–",
    "year": 2003
  }, {
    "title": "Readings in information retrieval",
    "authors": ["M.F. Porter."],
    "venue": "chapter An Algorithm for Suffix Stripping, pages 313– 316. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.",
    "year": 1997
  }, {
    "title": "The Three Sigma Rule",
    "authors": ["Friedrich Pukelsheim."],
    "venue": "The American Statistician, 48(2):88–91.",
    "year": 1994
  }, {
    "title": "Lexical semantic techniques for corpus analysis",
    "authors": ["James Pustejovsky", "Peter Anick", "Sabine Bergler."],
    "venue": "Comput. Linguist., 19(2):331–358, June.",
    "year": 1993
  }, {
    "title": "C4.5: Programs for Machine Learning",
    "authors": ["J. Ross Quinlan"],
    "year": 1993
  }, {
    "title": "The acl anthology network corpus",
    "authors": ["Dragomir R. Radev", "Pradeep Muthukrishnan", "Vahed Qazvinian."],
    "venue": "Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, NLPIR4DL, pages 54–61, Stroudsburg, PA,",
    "year": 2009
  }, {
    "title": "The role of citation context in predicting long-term citation profiles: An experimental study based on a massive bibliographic text dataset",
    "authors": ["Mayank Singh", "Vikas Patidar", "Suhansanu Kumar", "Tanmoy Chakraborty", "Animesh Mukherjee", "Pawan Goyal."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Co-citation in the scientific literature: A new measure of the relationship between two documents",
    "authors": ["Henry Small."],
    "venue": "JASIST, 24(4):265–269.",
    "year": 1973
  }, {
    "title": "Automatic classification of citation function",
    "authors": ["Simone Teufel", "Advaith Siddharthan", "Dan Tidhar."],
    "venue": "EMNLP, pages 103–110, Stroudsburg, PA, USA. ACL.",
    "year": 2006
  }, {
    "title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger",
    "authors": ["Kristina Toutanova", "Christopher D. Manning."],
    "venue": "EMNLP, pages 63– 70, Stroudsburg, PA, USA. ACL.",
    "year": 2000
  }, {
    "title": "From frequency to meaning: Vector space models of semantics",
    "authors": ["Peter D. Turney", "Patrick Pantel."],
    "venue": "J. Artif. Int. Res., 37(1):141–188, January.",
    "year": 2010
  }, {
    "title": "The inconsistency of the h-index",
    "authors": ["Ludo Waltman", "Nees Jan van Eck."],
    "venue": "JASIST, 63(2):406–415, February.",
    "year": 2012
  }, {
    "title": "Are all literature citations equally important? automatic citation strength estimation and its applications",
    "authors": ["Xiaojun Wan", "Fang Liu."],
    "venue": "JASIST, 65(9):1929– 1938.",
    "year": 2014
  }, {
    "title": "Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems)",
    "authors": ["Ian H. Witten", "Eibe Frank."],
    "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.",
    "year": 2005
  }, {
    "title": "Weighted citation: An indicator of an article’s prestige",
    "authors": ["Erjia Yan", "Ying Ding."],
    "venue": "JASIST, 61(8):1635– 1643.",
    "year": 2010
  }, {
    "title": "Semi-supervised learning using gaussian fields and harmonic functions",
    "authors": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty."],
    "venue": "ICML, pages 912–919, Washington D.C.",
    "year": 2003
  }, {
    "title": "Measuring academic influence: Not all citations are equal",
    "authors": ["Xiaodan Zhu", "Peter Turney", "Daniel Lemire", "Andr Vellino."],
    "venue": "JASIST, 66(2):408–427.",
    "year": 2015
  }],
  "id": "SP:347f38c18d6025ab930b23b6ecdd43a3e16d3860",
  "authors": [{
    "name": "Tanmoy Chakraborty",
    "affiliations": []
  }],
  "abstractText": "Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semisupervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.",
  "title": "All Fingers are not Equal: Intensity of References in Scientific Articles"
}