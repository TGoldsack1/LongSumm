{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 217–223 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2034"
  }, {
    "heading": "1 Introduction",
    "text": "Understanding complex compositional language in context is a challenge shared by many tasks. Visual question answering and robot instruction systems require reasoning about sets of objects, quantities, comparisons, and spatial relations; for example, when instructing home assistance or assembly-line robots to manipulate objects in cluttered environments. This reasoning requires robust language understanding, and is only partially addressed by existing datasets. VQA (Antol et al., 2015), while lexically and visually diverse, includes relatively short sentences with limited coverage of such phenomena. CLEVR (Johnson et al., 2016) and SHAPES (Andreas et al., 2016b), in contrast, display complex compositional structure, but include only synthetic language.\nIn this paper, we introduce the Cornell Natural Language Visual Reasoning (NLVR) corpus and task. We define the binary prediction task of judging if a statement is true for an image or not, and introduce a corpus of annotated pairs of natural language statements and synthetic images.\nCollecting this kind of language presents two challenges. First, we must design environments to\nsupport such descriptions. We use simple visual environments displaying objects with complex visual relations between them. Figure 1 shows two generated images. The second challenge is eliciting complex descriptions displaying a range of syntactic and semantic phenomena. We use a twostage crowdsourcing process. In the first stage, we present sets of images and ask workers to write descriptive statements that distinguish them. Using synthetic images with abstract shapes allows us to control the potential distinctions between them; for example, by discouraging simple statements about object existence. In the second stage, we ask workers to label the truth value for the sentences and images generated in the first stage.\nOur data includes 92,244 sentence-image pairs with 3,962 unique sentences. We include both images and the structured representation used to generate them to support research using both raw visual information and structured data. Figure 1 shows two examples. To assess the difficulty of NLVR, we experiment with multiple baselines. The best model using images achieves an accuracy of 66.12, demonstrating remaining challenges\n217\nin the data. We also analyze the language in our data for presence of certain linguistic phenomena, and compare this analysis with related datasets. The data and leaderboard are available at http://lic.nlp.cornell.edu/nlvr."
  }, {
    "heading": "2 Related Work and Datasets",
    "text": "Several datasets have been created to study visual reasoning and language. VQA (Antol et al., 2015; Zitnick and Parikh, 2013) includes crowdsourced questions and answers for photographs and abstract scenes, and has been studied extensively (e.g., Lu et al., 2016; Xu and Saenko, 2016; Zhou et al., 2015; Chen et al., 2015a; Andreas et al., 2016b,a; Ray et al., 2016). In contrast to VQA, we use synthetic images and emphasize representing a broad range of language phenomena. Our motivation is similar to that of SHAPES (Andreas et al., 2016b) and CLEVR (Johnson et al., 2016). Both datasets also use synthetic images and emphasize representing diverse spatial language. However, unlike our approach, they include only automatically generated language.\nVisual reasoning has also been addressed in instructional language corpora (e.g., MacMahon et al., 2006; Chen and Mooney, 2011; Bisk et al., 2016), where executable instructions are grounded in manipulable environments. The language we observe is similar to the type of language studied for understanding and generation of referential expressions (Mitchell et al., 2010; Matuszek et al., 2012; FitzGerald et al., 2013).\nOur task is related to caption generation, which has been studied extensively (e.g., Pedersoli et al., 2016; Carrara et al., 2016; Chen et al., 2016) with MSCOCO (Chen et al., 2015b) and Flickr30K (Young et al., 2014; Plummer et al., 2015). In contrast to caption generation, our task does not require approximate metrics like BLEU.\nSeveral existing datasets focus on natural language querying of structured representations, including GeoQuery (Zelle, 1995) and WikiTables (Pasupat and Liang, 2015). Our work is complementary to these resources. While our corpus was collected using images, we also provide structured representations. When used with these representations, our corpus is similar to WikiTables, where questions are paired with small web tables. Instead of web tables, we use object sets and focus on visual language."
  }, {
    "heading": "3 Task",
    "text": "Statements in our data are grounded in synthetic images rendered from structured representations. Given an example, the task is to determine whether a statement is true or false for the image or structured representation. While we describe the image, the structured representation is equivalent. We provide examples of the structured representation in the supplementary material. Images are divided into three boxes. Figure 1 shows two images. Each box contains 1-8 objects. Each object has four properties: position (x/y coordinates), color (black, blue, yellow), shape (triangle, square, circle), and size (small, medium, large). Objects within a box cannot overlap and must be contained entirely in the box. We distinguish between images containing scattered objects and images containing only squares arranged in towers up to four blocks tall. The top image in Figure 1 is a tower example; the bottom is a scatter example.\nThis design encourages compositional language with complex visual reasoning. We divide the image into boxes to encourage set theoretic reasoning within and between boxes. We also use a relatively limited number of values for each property. While a large number of properties provides a more diverse image, it is likely to result in descriptions that refer to property differences. We find that the limited number of properties elicits descriptions with rich compositional structure."
  }, {
    "heading": "4 Data Collection",
    "text": "We generate images following the structure described in Section 3, and collect grounded natural language descriptions. Data is collected in two phases: sentence writing and validation. During sentence writing, workers are asked to write contrasting descriptions about a set of images. To validate sentences, the description is paired with each of the images. We execute the collection process four times to collect training, development, and two test sets (Test-P and Test-U). We retain one test set as unreleased (Test-U). Generating Images We generate images by rendering a randomly sampled structured representation. The number of objects in each box and their properties are sampled uniformly. We generate an equal number of scatter and tower images. To generate the sets of images presented to annotators, we generate two images independently, a third image by using the set of objects in the first im-\nage and randomly re-shuffling them between the boxes, and a fourth image by re-shuffling the objects in the second image. For images with towers, we constrain the re-shuffling to form towers.\nPhase 1 – Sentence Writing Each writing task presents an annotator with four images. Figure 2 shows the sentence writing prompt, including the set of constraints, which is shown for all writing tasks. The constraints force the worker to contrast two pairs by referring to similarities and differences between the images, but not to refer to the position of the image in the prompt, or of each box in each image. These constraints are placed to elicit more set-theoretic language, and to allow us to divide the result of each task into four examples, pairing the annotator’s sentence with each of the four images it was presented with.\nPhase 2 – Validation In the second phase, we pair each sentence with the four images used to generate it. We re-label all sentence-image pairs as true or false, correcting for any violations of the constraints in the first phase. We do not use the original position of the image as any part of the final label to neutralize any ordering effect. In practice, 8.2% of examples had a different label than inferred from their original position in\nthe first phase. During validation, boxes are randomly permuted to ensure the last constraint was followed. We allow workers to annotate a sentence as nonsensical with regard to the image, and instruct annotators to ignore grammar errors. Post-processing We prune pairs when their majority class is nonsensical. When collecting multiple annotations for a pair, we prune pairs if the gap between the classes is less than two votes."
  }, {
    "heading": "5 Data Statistics and Analysis",
    "text": "We use the crowdsourcing platform Upwork,1 and select ten annotators using a small set of example questions. We collect 3,974 task instances and 28,723 total validation judgments at a total cost of $5,526. From these 3,974 task instances we extract 15,896 sentence-image pairs. We prune 522 pairs in post-processing. For the training set we collect a single validation annotation for each sentence-image pair; for the rest of the data we collect five annotations each. Finally, we generate six sentence-image pairs from each sample by permuting the boxes. The validation step ensures this permutation does not change the label. Table 1 shows the number of sentences and pairs, including permutations, for each split.\nWe merge the development and test splits to calculate agreement statistics. We calculate Krippendorf’s α and Fleiss’ κ (Cocos et al., 2015) on both the full and pruned datasets. To calculate Fleiss’ κ, we randomly permute the five annotations to be assigned to five “raters” and compute average kappa from 100 iterations. Before pruning, we observe α = 0.768 and κ = 0.709, indicating substantial agreement (Landis and Koch, 1977). Pruning improves agreement to α = 0.831 (indicating almost-perfect agreement) and κ = 0.808.\nWe analyze 200 development sentences to identify the distribution of semantic phenomena and syntactic ambiguity (Table 2). For comparison, we apply this analysis to 200 abstract-image and 200 real-image sentences from VQA (Antol et al., 2015). The difference in the distribution illustrates the complexity of our data. The mean sentence\n1http://upwork.com\nlength in our data is 11.22 tokens and the vocabulary size is 262. In Figure 3, we compare sentence length distribution to VQA, MSCOCO (Chen et al., 2015b), and CLEVR (Johnson et al., 2016). Our sentences are generally longer than VQA and more similar in length to MSCOCO. However, our task is more similar to VQA, where context is used to understand language, rather than to generate."
  }, {
    "heading": "6 Methods",
    "text": "We evaluate multiple methods on the rendered images and structured representations. Hyperparameters and initialization details are described in the supplementary material.\n2We say a statement or question uses presupposition when it assumes the truth value of some proposition in order for its entire truth value to be defined. In this example, an image which does not have three black items will have no defined truth value for this statement."
  }, {
    "heading": "6.1 Majority Class and Single Modality",
    "text": "We use image- and text-only models to measure how well biases in our data can be used to solve the task. If the model is able to do well on the text- or image-only baselines, this implies our data does not require the two modalities. Antol et al. (2015) performed a similar analysis of VQA with the questions only to gauge how and if background knowledge of the domain could aid performance.\nMajority Assign the most common label (true) to all examples.\nText Only Encode the sentence with a recurrent neural network (RNN; Elman, 1990) with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) and a binary softmax computed from the final output.\nImage Only Encode the image with a convolutional neural network (CNN) with three layers. The CNN output is used by a three-layer perceptron with a softmax on the final layer.3"
  }, {
    "heading": "6.2 Structured Representation",
    "text": "We use the structured representations described in Sections 3 and 4.\n3We also experimented using the ImageNet-trained Inception v4 model (Szegedy et al., 2017), but found it did not improve performance, possibly due to the difference between our images and ImageNet.\nMaxEnt Train a MaxEnt classifier. We use the text and structured representation to compute property- and count-based features. Propertybased features trigger when some property (e.g., an object is touching a wall) is true in the structure. We create features by crossing triggered properties with each n-grams from the sentence, up to n = 6. Count-based features trigger when a count we observe in the image (e.g., the number of black triangles) is present in the sentence. We generate features combining the type of item counted (e.g., black triangles) with the n-grams surrounding the count in the sentence, up to n = 6. We provide details in the supplementary material. MLP Train a single-layer perceptron with a softmax layer. The input to the perceptron is the mean of the feature embeddings. We use the same feature set as the MaxEnt model. Image Features+RNN Compute features from the structure representation only, and encode the text with an LSTM RNN. The two representations are concatenated, and used as input to a two-layer perceptron and a softmax layer."
  }, {
    "heading": "6.3 Image Representation",
    "text": "CNN+RNN Concatenate the CNN and RNN representations (Section 6.1) and apply a multilayer perceptron with a softmax. NMN The neural module networks approach of Andreas et al. (2016b). We experiment with the default maximum leaves of two, and with allowing for more expressive representations with a maximum leaves of five. We observe higher development accuracy with the trees using maximum leaves of five (63.06% vs. 62.4% with the default of two), which we use in our experiments."
  }, {
    "heading": "7 Results",
    "text": "We run each experiment ten times and report mean accuracy as well as standard deviation for randomly initialized models. Table 3 shows our re-\nsults. NMN is the best performing model using images. Table 2 shows the NMN accuracy for each category in our qualitative analysis sample. While the number of sentences in some categories is relatively small, we observe a higher number of failures in sentences that include negations and coordinations. For models using the structured representation, the MaxEnt model provides the best performance. When ablating count-based features from the MaxEnt model, development accuracy decreases from 68.04 to 57.7. This indicates counting is an important aspect of the problem."
  }, {
    "heading": "8 Discussion",
    "text": "We introduce the Cornell Natural Language Visual Reasoning dataset and task. The data includes complex compositional language grounded in images and structured representations. The task requires addressing challenges in visual and set-theoretic reasoning. We experiment with multiple systems and, in general, observe relatively low performance. Together with our qualitative analysis, this exemplifies the complexity of the data. We release our annotated training and development sets, and create two test sets. The public test set will be released along with its annotation. Computing results on the unreleased test data will require submitting trained models. Procedures for submitting models and the task leader board are available at http://lic.nlp.cornell.edu/nlvr."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was supported by a Microsoft Research Women’s Fellowship, a Google Faculty Award, and an Amazon Web Services Cloud Credits for Research Grant. We thank the Cornell and University of Washington NLP groups for their support and helpful comments. We thank the anonymous reviewers for their feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Learning to compose neural networks for question answering",
    "authors": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Neural module networks",
    "authors": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."],
    "venue": "Conference on Computer Vision and Pattern Recognition.",
    "year": 2016
  }, {
    "title": "VQA: Visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."],
    "venue": "International Journal of Computer Vision.",
    "year": 2015
  }, {
    "title": "Natural language communication with robots",
    "authors": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
    "year": 2016
  }, {
    "title": "Picture it in your mind: Generating high level visual representations from textual descriptions",
    "authors": ["Fabio Carrara", "Andrea Esuli", "Tiziano Fagni", "Fabrizio Falchi", "Alejandro Moreo."],
    "venue": "CoRR abs/1606.07287.",
    "year": 2016
  }, {
    "title": "Learning to interpret natural language navigation instructions from observations",
    "authors": ["David L. Chen", "Raymond J. Mooney."],
    "venue": "Proceedings of the National Conference on Artificial Intelligence.",
    "year": 2011
  }, {
    "title": "ABC-CNN: An attention based convolutional neural network for visual question answering",
    "authors": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ramakant Nevatia."],
    "venue": "CoRR abs/1511.05960.",
    "year": 2015
  }, {
    "title": "Bootstrap, review, decode: Using out-ofdomain textual data to improve image captioning",
    "authors": ["Wenhu Chen", "Aurélien Lucchi", "Thomas Hofmann."],
    "venue": "CoRR abs/1611.05321.",
    "year": 2016
  }, {
    "title": "Microsoft COCO captions: Data collection and evaluation server",
    "authors": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C. Lawrence Zitnick."],
    "venue": "CoRR abs/1504.00325.",
    "year": 2015
  }, {
    "title": "Effectively crowdsourcing radiology report annotations",
    "authors": ["Anne Cocos", "Aaron Masino", "Ting Qian", "Ellie Pavlick", "Chris Callison-Burch."],
    "venue": "Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis.",
    "year": 2015
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L. Elman."],
    "venue": "Cognitive Science 14:179–211.",
    "year": 1990
  }, {
    "title": "Learning distributions over logical forms for referring expression generation",
    "authors": ["Nicholas FitzGerald", "Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9:1735– 1780.",
    "year": 1997
  }, {
    "title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
    "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross B. Girshick."],
    "venue": "CoRR abs/1612.06890.",
    "year": 2016
  }, {
    "title": "The measurement of observer agreement for categorical data",
    "authors": ["J. Richard Landis", "Gary Koch."],
    "venue": "Biometrics 33 1:159–74.",
    "year": 1977
  }, {
    "title": "Hierarchical question-image coattention for visual question answering",
    "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."],
    "venue": "Neural Information Processing Systems.",
    "year": 2016
  }, {
    "title": "Walk the talk: Connecting language, knowledge, action in route instructions",
    "authors": ["Matthew MacMahon", "Brian Stankiewics", "Benjamin Kuipers."],
    "venue": "Proceedings of the National Conference on Artificial Intelligence.",
    "year": 2006
  }, {
    "title": "A joint model of language and perception for grounded attribute learning",
    "authors": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."],
    "venue": "Proceedings of the International Conference on Machine Learning.",
    "year": 2012
  }, {
    "title": "Natural reference to objects in a visual domain",
    "authors": ["Margaret Mitchell", "Kees van Deemter", "Ehud Reiter."],
    "venue": "Proceedings of the 6th International Natural Language Generation Conference. http://aclweb.org/anthology/W10-4210.",
    "year": 2010
  }, {
    "title": "Compositional semantic parsing on semi-structured tables",
    "authors": ["Panupong Pasupat", "Percy Liang."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-",
    "year": 2015
  }, {
    "title": "Areas of attention for image captioning",
    "authors": ["Marco Pedersoli", "Thomas Lucas", "Cordelia Schmid", "Jakob Verbeek."],
    "venue": "CoRR abs/1612.01033.",
    "year": 2016
  }, {
    "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
    "authors": ["Bryan A. Plummer", "Liwei Wang", "Chris M. Cervantes", "Juan C. Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik."],
    "venue": "The IEEE International Con-",
    "year": 2015
  }, {
    "title": "Question relevance in VQA: Identifying non-visual and false-premise",
    "authors": ["Arijit Ray", "Gordon Christie", "Mohit Bansal", "Dhruv Batra", "Devi Parikh"],
    "year": 2016
  }, {
    "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
    "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alexander A. Alemi."],
    "venue": "Association for the Advancement of Artificial Intelligence.",
    "year": 2017
  }, {
    "title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
    "authors": ["Huijuan Xu", "Kate Saenko."],
    "venue": "European Conference on Computer Vision.",
    "year": 2016
  }, {
    "title": "From image descriptions to visual denotations",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the Association of Computational Linguistics http://aclweb.org/anthology/Q14-1006.",
    "year": 2014
  }, {
    "title": "Using inductive logic programming to automate the construction of natural language parsers",
    "authors": ["John M. Zelle."],
    "venue": "Ph.D. thesis, University of Texas, Austin.",
    "year": 1995
  }, {
    "title": "Simple baseline for visual question answering",
    "authors": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."],
    "venue": "CoRR abs/1512.02167.",
    "year": 2015
  }, {
    "title": "Bringing semantics into focus using visual abstraction",
    "authors": ["C. Lawrence Zitnick", "Devi Parikh."],
    "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition.",
    "year": 2013
  }],
  "id": "SP:c208f738fba3e3598018e6fdfbc72c6fbb7ba9d0",
  "authors": [{
    "name": "Alane Suhr",
    "affiliations": []
  }, {
    "name": "Mike Lewis",
    "affiliations": []
  }, {
    "name": "James Yeh",
    "affiliations": []
  }, {
    "name": "Yoav Artzi",
    "affiliations": []
  }],
  "abstractText": "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
  "title": "A Corpus of Natural Language for Visual Reasoning"
}