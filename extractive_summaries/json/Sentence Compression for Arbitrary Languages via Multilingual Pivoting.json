{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2453"
  }, {
    "heading": "1 Introduction",
    "text": "Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001).\nThe bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how\n1Publicly available for download at https://github. com/Jmallins/MOSS\nthe compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015).\nNeural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g., part-of-speech tags, parse trees). In order to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller. For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products. Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences). Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles. More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts).\n2Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million.\nSince large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015). As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions.\nOur work addresses the paucity of data for sentence compression models. We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements). We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016). Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.We also demonstrate that models trained on multilingual data perform well out-of-domain.\nAlthough our approach does not employ compression corpora for training, for evaluation purposes, we create MOSS, a new Multilingual Compression dataset for English, French, and German. MOSS is a parallel corpus containing documents from the European parliament proceedings, TED talks, news commentaries, and the EU bookshop. Each document is written in English, French, and German, and compressed by native speakers of the respective language who process a document at a time. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents.\nThere has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing.\nOur contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is available in the bilingual parallel data; and the release of a multilingual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling."
  }, {
    "heading": "2 Pivot-based Neural Compression",
    "text": "In our pivot-based sentence compression model an input sequence is first translated into a foreign language, and then back into the source language. Unlike previous paraphrasing pivoting models (Mallinson et al., 2017), we parameterize our translation models with a length feature, which allows us to produce compressed output. We define two models, performing compression in one step or alternatively in two steps which affords more flexibility in model output."
  }, {
    "heading": "2.1 NMT Background",
    "text": "In the neural encoder-decoder framework for MT (Bahdanau et al., 2015; Sutskever et al., 2014), an encoder takes in a source X = (x1, ...,xTx) of length Tx and the decoder generates a target sequence (y1, ...,yTy) of length Ty. Let hi be the hidden state of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, hi = [ −→ hi ; ←− hi ]. We deviate from previous work (Bahdanau et al., 2015; Sutskever et al., 2014) in that we initialize the decoder with the average of the hidden states, following Sennrich et al. (2017):\ns0 = tanh(Winit ∑Txi=1 hi\nTx ) (1)\nwhere Winit is a learnt parameter. Our decoder is a conditional recurrent neural network, specifically a gated recurrent unit (GRU, Cho et al., 2014) with attention, which we denote as cGRUatt . cGRUatt takes as input the previous hidden state s j−1, the source annotations C = h1, ...,hTx , and the previously decoded symbol y j−1 in order to update its hidden state s j, which is used to decode symbol y j at position j:\ns j = cGRUatt(s j−1,y j−1,C) (2)\ncGRUatt consists of three components. The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an intermediate representation s′j. The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s′j in order to compute the context vector c j:\nc j = AT T (C,s′j) = Tx\n∑ i αi jhi (3)\nαi j = exp(ei j)\n∑Txk=1 exp(ek j) (4)\nei j = f (s′j,hi) (5)\nWhere αi j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.\nFinally, we generate s j, the hidden state of cGRUatt , by using the intermediate representation s′j and the context vector c j. Given s j, y j−1, and c j the output probability p(y j|s j,y j−1,c j) is computed using a feedforward neural network with a softmax activation. We define the probability of sequence y as:\nP(y|x;θ) = Ty\n∏ j=1 p(y j|s j,y j−1,c j) (6)"
  }, {
    "heading": "2.2 Length Control",
    "text": "To be able to produce compressed sentences, we parameterize our model with a length vector which allows to control the output length. Our approach is similar to the LenInit model of Kikuchi et al. (2016), however we use a GRU instead of an LSTM. The hidden state of the decoder consists of the average of the encoder’s hidden states but also a length vector LV , a learnt parameter, which is scaled by the desired target length Ty′ . We therefore rewrite Equation (1) as follows:\ns′0 = tanh ( Winit [∑Txi=1 hi\nTx ;LV ·Ty′\n]) (7)\nAs such we now define our model as:\nP(y|x,Ty′ ;θ) (8)\nDuring training, the target length is set to Ty′ = Ty. However, at test time, the target length generally varies according to the domain, genre, and language at hand. We determine the target length experimentally based on a small validation set."
  }, {
    "heading": "2.3 Pivoting",
    "text": "Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016).\nWe use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form. We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1, ..., fK}. The probability of generating compression y = y1...yTy is decomposed as:\nP(y|x) = Fx\n∑ f\nP(y| f ; −→ θ ) ·P( f |x; ←− θ ) (9)\nwhich we approximate as the tokenwise weighted average of the pivots:\nP(y|x)≈ Ty\n∏ j=1\nFx ∑ f P(y j|y< j, f )P( f |x) (10)\nwhere y< j = y1, ...y j . To ensure a probability distribution, we normalize the K-best list Fx, such that the translation probabilities sum to one. We use beam search to decode tokens by conditioning on multiple pivoting sentences. The results with the best decoding scores are considered candidate compressions.\nTo ensure the model produces compressed output, we extend the pivoting approach in two ways. In single step compression, one of the translation\nmodels is parameterized with length information:\nP(y|x,Ty′) ≈ F\n∑ f\nP(y| f ,Ty′ ; −→ θ ) · P( f |x; ←− θ )\nIn dual-step compression, we parameterize both translation models with length information:\nP(y|x,Ty′ ,Ty′′)≈ F\n∑ f\nP(y| f ,Ty′ ; −→ θ )·P( f |x,Ty′′ ; ←− θ )\nWe find that dual-compression performs better when the system is expected to drastically compress the source sentence (e.g., in a headline generation task). Imposing a high compression ratio from the start tends to produce unintelligible text. The model attempts to reduce the length of the source at all costs, even at the expense of being semantically faithful to the input. Performing two moderate compressions in succession reduces both length and content conservatively and as a result produces more meaningful text.\nIn Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions. We show the output of a single-step compression model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation (8):\nTy′ = Tx ·CR (11)\nThe figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source\n3The term refers to the percentage of words retained from the source sentence in the compression.\nlanguage (Mallinson et al., 2017). We can see that the majority of sentences are generated with length close to the desired compression rate."
  }, {
    "heading": "3 The MOSS Dataset",
    "text": "For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French. The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre. We sampled five documents each from:\n1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment).\n2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues.\n3. The EU bookshop corpus (Skadiņš et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel.\n4. The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs).\nWe obtained compressions using the Crowdflower platform. Crowdworkers were given instructions that explained the task and defined sentence compression with the aid of examples. They\nwere asked to compress while preserving the most important information, ensuring the sentences remained grammatical and meaning preserving. Annotators were encouraged to use any rewriting operations that seemed appropriate, e.g., to delete words, add new words, substitute them, or reorder them. Annotation proceeded on a document-bydocument basis, line-by-line. Crowdworkers compressed the first twenty lines of each document and we elicited five compression per document. Example compressions are shown in Table 1.\nTable 2 presents various statistics on our corpus. As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between. We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances. Overall, French speakers seem more conservative when shortening sentences compared to English and German. In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED). We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure com-\nmonly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change a long sentence to shorter output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences. We observe that crowdworkers perform a fair amount of rewriting across corpora and languages. The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions."
  }, {
    "heading": "4 Experimental Setup",
    "text": "Neural Machine Translation Training Nematus (Sennrich et al., 2017) was used as the machine translation system for all our experiments. We generally used the default settings and training procedures as specified within Nematus. All networks have a hidden layer size of 1,000, and an embedding layer size of 512. In addition, layer normalization (Ba et al., 2016) was used. During training, we used ADAM (Kingma and Ba, 2014), a minibatch size of 80, and the training set was reshuffled between epochs. We also employed early stopping.\nWe used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions.\nWe experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2.\n4BLEU scores were calculated using mteval-v13a.pl.\nComparison Systems We compared our model against ABS, a sequence-to-sequence attentionbased model, developed by Rush et al. (2015). This model was trained on a monolingual dataset extracted from the Annotated English Gigaword corpus (Napoles et al., 2011). The dataset consists of approximately 4 million pairs of the first sentence from each source document and its headline. We also trained LenInit (Kikuchi et al., 2016) on the same corpus which is conceptually similar to ABS but additionally controls the output length using a length embedding vector (as described in Section 2.2).5 Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale. An obvious workaround is to translate Gigaword to French and German and then train compression models on the translated data. As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German. Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set."
  }, {
    "heading": "5 Results",
    "text": "MOSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006). These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples.\nTable 3(a) reports results on English with a model which controls the output length (L) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in a single step (see Section 2.3). Dual-step compres-\n5We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016).\n6We add a begin-of-sentence marker at the start of the candidate and reference sentences.\nsion obtained inferior results which we omit for the sake of brevity. As can be seen, models which use a single pivot are better than those using multiple ones (German is better than French; see SPde vs SP f r). More pivots might introduce noise at the expense of translation quality.\nOverall, pivot-based models outperform ABS and LenInit. This is perhaps to be expected since these models are tested on out of domain data with different vocabulary and writing conventions; MOSS does not contain any newspaper articles. Unfortunately, it is not possible to train ABS and LenInt on in-domain data as compression data only exists for the headlines-first sentences pairs. As an upper bound, we also report how well humans agree with each other, treating one (randomly selected) reference as system output and computing how it agrees with the rest (row Gold in Table 3). All models lag significantly behind human performance on this task.\nTables 3(b) and 3(c) report results on French and German, respectively. For these languages, we obtained best results with English as pivot, using a single-step compression model. ABS and LenInit perform poorly when trained directly on translations of Gigaword into French and German; their performance improves considerably when they are trained on the Gigaword and used to compress English translations of French or German (ABSen, LenIniten). Again, we observe that our models (SPL ,en, MPL ,en) outperform the comparison systems across all metrics and that using a single pivot yields better compressions. Example compressions are given in Table 4 where we show output produced by ABS and SP for each language (see the supplementary material for more examples). Finally, notice that automatic scores for the prefix baseline across languages are misleadingly high, since it simply repeats the source sentence up to a fixed length without performing any rewriting.\nWe also elicited human judgments through the Crowdflower platform. We asked crowdworkers to rate the grammaticality of the target compressions and whether they preserved the most important information from the source. In both cases, they used a five-point rating scale where a high number indicates better performance. We randomly selected 25 sentences from each corpus from the test portion of MOSS, i.e., 100 long-short sentence pairs per language. We compared compressions generated by our model (SPL ), with ABS models for the three languages, the prefix baseline, and (randomly selected) gold-standard reference (Ref) compressions from MOSS. All systems used the length parameter to allow comparisons with approximately the same compression rates. We collected five ratings per compression. Our results are summarized in Table 5. We show mean ratings for grammaticality (Gram), importance (Imp) and their combination (column Avg). Across languages our model (SPL) significantly (p < 0.05) outperforms comparison systems (Pfix, ABS) on both dimensions of grammaticality and importance (significance tests were performed using a student t-test). All systems are significantly worse (p < 0.05) than the human reference compressions.\nFinally, in Table 6 we analyze the output of our best model (SPL ) using the same statistics we applied to the human compressions (see Table 2). As can be seen, the model generally compressess more aggressively and applies more ed-\nits than the crowdworkers (both compression rates and TER scores are higher for all three languages). Although the rate of deletions is similar to humans, insertions, substitutions and shifts happen to a greater extent for our model, indicating that it performs a good amount of paraphrasing.\nDUC-2004 Evaluation Besides MOSS, we evaluated our model on the benchmark DUC-2004 task-1 dataset. In this task, the aim is to create a very short summary (75 bytes) for a document. The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries. We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary. To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.\nOur results are shown in Table 7. To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE-L).9 We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants. We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword. ABS+ applies minimum error rate (MERT) training as a copy-\n7Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35\n8Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70\n9We used ROUGE version 1.5.5 with the original DUC-2004 ROUGE parameters.\ning mechanism. LenEmb and LenInit include a length parameter (Kikuchi et al., 2016), whereas RAS uses a specialized recurrent neural network architecture (Elman, 1990). We also report how well DUC-2004 abstractors agree with each other (row Gold in Table 7). Example compressions are given in Table 8, where we show output produced by SPL ,de and a human reference (see the supplementary material for further examples).\nUsing automatic metrics we see that our model generally performs worse compared to these systems and that German is the best pivot for English. Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is interesting to see that our model is able to compress out-of-domain. We do not have access to headline-first sentence pairs, while all comparison systems do. We also elicited human judgments on the compressions of 100 lead sentences whose documents were randomly selected from the DUC-2004 test set. We compared the prefix baseline, our model (SPL ,de), ABS+ (Rush et al., 2015), LenEmb (Kikuchi et al., 2016), Topiary (Zajic et al., 2004), and a randomly selected reference. Topiary came top in almost all measures in the DUC-2004 evaluation; it first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords. Crowdworkers rated grammaticality and importance, using a five-point scale; we collected five ratings per compression.\nAs shown in Table 9 ABS+ has the lead with our system following suit. In terms of grammaticality, ABS+ and SPL ,de are not significantly different from the gold standard or from each other (Pfix, Topiary, and LenEmb are significantly worse than Gold; p < 0.05). In terms of importance, pairwise differences between systems and the gold standard are not significant. Overall, we observe that SPL ,de performs comparably to ABS+ even though it was\nnot trained on any compression specific data. Inspection of system output reveals that our model performs more paraphrasing than comparison systems (a conclusion also confirmed by the statistics in Table 6)."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper we have shown that multilingual corpora can be used to bootstrap compression models across languages and text genres. Our approach adapts existing neural machine translation machinery to the compression task coupled with methods which decode the output to a desired length. An interesting direction for future work would be to train our model using reinforcement learning (Ranzato et al., 2016; Zhang and Lapata, 2017) in order to control the compression output more directly. Moreover, although we do not use any direct supervision in our experiments, it would be interesting to incorporate it as a means of domain adaptation (Cheng et al., 2016).\nAcknowledgments The authors gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1; Mallinson) and the European Research Council (award number 681760; Lapata)."
  }],
  "year": 2018,
  "references": [{
    "title": "Layer normalization",
    "authors": ["Lei Jimmy Ba", "Ryan Kiros", "Geoffrey E. Hinton."],
    "venue": "CoRR, abs/1607.06450.",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations, San Diego, California.",
    "year": 2015
  }, {
    "title": "Jointly learning to extract and compress",
    "authors": ["Taylor Berg-Kirkpatrick", "Dan Gillick", "Dan Klein."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 481–490, Portland, Ore-",
    "year": 2011
  }, {
    "title": "Wit3: Web inventory of transcribed and translated talks",
    "authors": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."],
    "venue": "Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261–268, Trento, Italy.",
    "year": 2012
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740–750, Doha, Qatar.",
    "year": 2014
  }, {
    "title": "Semisupervised learning for neural machine translation",
    "authors": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2016
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Abstractive sentence summarization with attentive recurrent neural networks",
    "authors": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
    "year": 2016
  }, {
    "title": "Models for sentence compression: A comparison across domains, training requirements and evaluation measures",
    "authors": ["James Clarke", "Mirella Lapata."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th An-",
    "year": 2006
  }, {
    "title": "Global inference for sentence compression: An integer linear programming approach",
    "authors": ["James Clarke", "Mirella Lapata."],
    "venue": "Journal of Artificial Intelligence Research, 31:273–381.",
    "year": 2008
  }, {
    "title": "Sentence compression as tree transduction",
    "authors": ["Trevor Cohn", "Mirella Lapata."],
    "venue": "Journal of Artificial Intelligence Research, 34:637–674.",
    "year": 2009
  }, {
    "title": "An abstractive approach to sentence compression",
    "authors": ["Trevor Cohn", "Mirella Lapata."],
    "venue": "ACM Transactions on Intelligent Systems and Technology, 4(3):1–",
    "year": 2013
  }, {
    "title": "Text Compaction for Display on Very Small Screens",
    "authors": ["Simon Corston-Oliver."],
    "venue": "Proceedings of the NAACL Workshop on Automatic Summarization, pages 89–98, Pittsburgh, Pennsylvania.",
    "year": 2001
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L. Elman."],
    "venue": "Cognitive Science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "Sentence compression by deletion with LSTMs",
    "authors": ["Katja Filippova", "Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2015
  }, {
    "title": "Overcoming the lack of parallel data in sentence compression",
    "authors": ["Katja Filippova", "Yasemin Altun."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA.",
    "year": 2013
  }, {
    "title": "Zero-resource translation with multi-lingual neural machine translation",
    "authors": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
    "year": 2016
  }, {
    "title": "Lexicalized Markov grammars for sentence compression",
    "authors": ["Michel Galley", "Kathleen McKeown."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings",
    "year": 2007
  }, {
    "title": "Flexible Japanese sentence compression by relaxing unit constraints",
    "authors": ["Jun Harashima", "Sadao Kurohashi."],
    "venue": "Proceedings of COLING 2012, pages 1097–1112, Mumbai, India.",
    "year": 2012
  }, {
    "title": "Japanese sentence compression with a large training dataset",
    "authors": ["Shun Hasegawa", "Yuta Kikuchi", "Hiroya Takamura", "Manabu Okumura."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
    "year": 2017
  }, {
    "title": "A syntax-free approach to japanese sentence compression",
    "authors": ["Tsutomu Hirao", "Jun Suzuki", "Hideki Isozaki."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language",
    "year": 2009
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Speech summarization: an approach through word extraction and a method for evaluation",
    "authors": ["Chiori Hori", "Sadaoki Furui."],
    "venue": "IEICE Transactions on Information and Systems, E87-D(1):15–25.",
    "year": 2004
  }, {
    "title": "Sentence Reduction for Automatic Text Summarization",
    "authors": ["Hongyan Jing."],
    "venue": "Proceedings of the 6th ANLP, pages 310–315, Seattle,WA.",
    "year": 2000
  }, {
    "title": "The proper place of men and machines in language translation",
    "authors": ["Martin Kay."],
    "venue": "Machine Translation, 12(1-2):3–23.",
    "year": 1997
  }, {
    "title": "Controlling output length in neural encoder-decoders",
    "authors": ["Yuta Kikuchi", "Graham Neubig", "Ryohei Sasano", "Hiroya Takamura", "Manabu Okumura."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Summarization beyond sentence extraction: a probabilistic approach to sentence compression",
    "authors": ["Kevin Knight", "Daniel Marcu."],
    "venue": "Artificial Intelligence, 139(1):91–107.",
    "year": 2002
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Proceedings of the 10th Machine Translation Summit, pages 70–86, Phuket, Thailand.",
    "year": 2005
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain.",
    "year": 2004
  }, {
    "title": "A french human reference corpus for multidocument summarization and sentence compression",
    "authors": ["Claude de Loupy", "Marie Guégan", "Christelle Ayache", "Somara Seng", "Juan-Manuel Torres Moreno."],
    "venue": "Proceedings of the 7th International Con-",
    "year": 2010
  }, {
    "title": "Sentence compression for automatic subtitling",
    "authors": ["Juhani Luotolahti", "Filip Ginter."],
    "venue": "Proceedings of the 20th Nordic Conference for Computational Linguistics, pages 135–143, Vilnius, Lithuania.",
    "year": 2015
  }, {
    "title": "Mutiple alternative sentence compressions for automatic text summarization",
    "authors": ["Nitin Madnani", "David Zajic", "Bonnie Dorr", "Necip Fazil Ayan", "Jimmy Lin."],
    "venue": "Proceedings of the 2007 Document Understanding Conference, Rochester, NY, USA.",
    "year": 2007
  }, {
    "title": "Paraphrasing revisited with neural machine translation",
    "authors": ["Jonathan Mallinson", "Rico Sennrich", "Mirella Lapata."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Pa-",
    "year": 2017
  }, {
    "title": "Discriminative sentence compression with soft syntactic constraints",
    "authors": ["Ryan McDonald."],
    "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics, pages 297–304, Trento, Italy.",
    "year": 2006
  }, {
    "title": "Paraphrastic sentence compression with a character-based metric: Tightening without deletion",
    "authors": ["Courtney Napoles", "Chris Callison-Burch", "Juri Ganitkevitch", "Benjamin Van Durme."],
    "venue": "Proceedings of the Workshop on Monolingual Text-To-Text Generation,",
    "year": 2011
  }, {
    "title": "Sequence level training with recurrent neural networks",
    "authors": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba."],
    "venue": "Proceedings of the International Conference on Learning Representations, San Juan, Puerto Rico.",
    "year": 2016
  }, {
    "title": "Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar",
    "authors": ["Stefan Riezler", "Tracy H. King", "Richard Crouch", "Annie Zaenen."],
    "venue": "Proceedings of the HLT/NAACL, pages 118–125,",
    "year": 2003
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal.",
    "year": 2015
  }, {
    "title": "Nematus: a toolkit for neural machine",
    "authors": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"],
    "year": 2017
  }, {
    "title": "Improving neural machine translation models with monolingual data",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational",
    "year": 2016
  }, {
    "title": "Billions of parallel words for free: Building and using the EU bookshop corpus",
    "authors": ["Raivis Skadiņš", "Jörg Tiedemann", "Roberts Rozis", "Daiga Deksne."],
    "venue": "Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC’14),",
    "year": 2014
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of the 7th Conference of the Association for Machine Translation of the Americas, pages",
    "year": 2006
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in Neural Information Processing Systems 27, pages 3104–3112. Curran Associates, Inc.",
    "year": 2014
  }, {
    "title": "A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs",
    "authors": ["Kristina Toutanova", "Chris Brockett", "Ke M. Tran", "Saleema Amershi."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
    "year": 2016
  }, {
    "title": "Supervised and unsupervised learning for sentence compression",
    "authors": ["Jenine Turner", "Eugene Charniak."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 290–297, Ann Arbor, Michigan.",
    "year": 2005
  }, {
    "title": "A comparison of pivot methods for phrase-based statistical machine translation",
    "authors": ["Masao Utiyama", "Hitoshi Isahara."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2007
  }, {
    "title": "Sentence compression for automated subtitling: A hybrid approach",
    "authors": ["Vincent Vandeghinste", "Yi Pan."],
    "venue": "Proceedings of the ACL Workshop on Text Summarization, pages 89–95, Barcelona, Spain.",
    "year": 2004
  }, {
    "title": "Automatic generation of story highlights",
    "authors": ["Kristian Woodsend", "Mirella Lapata."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565–574, Uppsala, Sweden.",
    "year": 2010
  }, {
    "title": "Pivot language approach for phrase-based statistical machine translation",
    "authors": ["Hua Wu", "Haifeng Wang."],
    "venue": "Machine Translation, 21(3):165–181.",
    "year": 2007
  }, {
    "title": "Bbn/umd at duc-2004: Topiary",
    "authors": ["David Zajic", "Bonnie Dorr", "Richard Schwartz."],
    "venue": "In Proceedings of the NAACL Workshop on Document Understanding, pages 112–119, Boston, Massachusetts.",
    "year": 2004
  }, {
    "title": "Sentence simplification with deep reinforcement learning",
    "authors": ["Xingxing Zhang", "Mirella Lapata."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 595–605, Copenhagen, Denmark.",
    "year": 2017
  }],
  "id": "SP:bcb83e9af0b8841409145d48c256661732013af4",
  "authors": [{
    "name": "Jonathan Mallinson",
    "affiliations": []
  }, {
    "name": "Rico Sennrich",
    "affiliations": []
  }, {
    "name": "Mirella Lapata",
    "affiliations": []
  }],
  "abstractText": "In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 MOSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.",
  "title": "Sentence Compression for Arbitrary Languages via Multilingual Pivoting"
}