{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider the problem of estimating the parameters θ ∈ RM of an unnormalised statistical model φ(u;θ) : X 7→ R+ from observed data X = {x1, . . . ,xN}, where the\n*Equal contribution 1UMIC, RWTH Aachen University, Aachen, Germany (affiliated with KTH Royal Institute of Technology and University of Edinburgh during project timespan) 2School of Informatics, University of Edinburgh, Edinburgh, United Kingdom. Correspondence to: Ciwan Ceylan <ceylan@vision.rwthaachen.de>, Michael Gutmann <michael.gutmann@ed.ac.uk>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nxi ∈ X are independently sampled from the unknown data distribution pd. Unnormalised models output non-negative numbers but do not integrate or sum to one, i.e. they are statistical models that are defined up to the partition function Z(θ) = ∫ φ(u;θ) du. Unnormalised models are widely used, e.g. to model images (Köster & Hyvärinen, 2010; Gutmann & Hyvärinen, 2013), natural language (Mnih & Teh, 2012; Zoph et al., 2016), or memory (Hopfield, 1982).\nIf the partition function Z(θ) can be evaluated analytically in closed form, the unnormalised model φ(u;θ) can be easily converted to a (normalised) statistical model p(u;θ) = φ(u;θ)/Z(θ) that can be estimated by maximising the likelihood. However, for most unnormalised models the integral defining the partition function is analytically intractable and computationally expensive to approximate.\nSeveral methods have been proposed in the literature to estimate unnormalised models including Monte Carlo maximum likelihood (Geyer, 1994), contrastive divergence (Hinton, 2002), score matching (Hyvärinen, 2005), and noisecontrastive estimation (Gutmann & Hyvärinen, 2010; 2012) and its generalisations (Pihlaja et al., 2010; Gutmann & Hirayama, 2011). The basic idea of noise-contrastive estimation (NCE) is to formulate the density estimation problem as a classification problem where the model is trained to distinguish between the observed data and some reference (noise) data. NCE is used in several application domains (Mnih & Teh, 2012; Chen et al., 2015; Tschiatschek et al., 2016) and similar “learning by comparison” ideas are employed for learning with generative latent variable models (Gutmann et al., 2014; Goodfellow et al., 2014).\nIn NCE, the choice of the auxiliary noise distribution is left to the user. While simple distributions, e.g. uniform or Gaussian distributions, have successfully been used (Gutmann & Hyvärinen, 2012; Mnih & Teh, 2012), the estimation performance of NCE depends on the distribution chosen and more tailored distributions were found to typically yield better results, see e.g. (Ji et al., 2016). Intuitively, the noise samples in NCE ought to resemble the observed data in order for the classification problem not to be too easy. To alleviate the burden on the user to generate such noise, we here propose conditional noise-contrastive estimation that semiautomatically generates the noise based on the observed data.\nThe rest of the paper is structured as follows. In Section 2, we present the theory of conditional noise-contrastive estimation (CNCE), establish basic properties, and prove that a limiting case yields score matching. In Section 3, we validate the theory on synthetic data and compare the estimation performance of CNCE with NCE. In Section 4, we apply CNCE to real data and show that it can handle complex models by estimating a four-layer neural network model of natural images, and Section 5 concludes the paper."
  }, {
    "heading": "2. Conditional noise-contrastive estimation",
    "text": "Conditional noise-contrastive estimation (CNCE) turns an unsupervised estimation problem into a supervised learning problem by training the model to distinguish between data and noise samples. This is the same high-level approach as NCE takes, but in contrast to NCE, the novel idea of CNCE is to generate the noise samples with the aid of the observed data samples. Therefore, unlike NCE, CNCE does not assume the noise samples to be generated independently of the data samples, but rather to be drawn from a conditional noise distribution pc. The generated noise samples are paired with the data samples, with κ noise samples yij ∈ Y, j = 1, . . . , κ per observed data point xi. Thus, a total of N · κ noise samples yij ∼ pc(yij |xi) are generated from pc. We denote the collection of all noise samples by Y. In what follows, we assume that X = Y, but this assumption can be relaxed to X ⊆ Y (see Supplementary Materials A). In any case, we denote the union of X and Y by U.\nWe derive the loss function for CNCE in analogy to the derivation of the loss function for NCE. We divide all pairs of data and noise samples into two classes, Cα and Cβ, of equal size. Class Cα is formed by tuples (u1,u2) with u1 ∈ X and u2 ∈ Y, whileCβ is formed by tuples (u1,u2) with u1 ∈ Y and u2 ∈ X. Consequently, the probability distributions for the classes Cα and Cβ are given by\npα(u1,u2) = pd(u1)pc(u2|u1), (1) pβ(u1,u2) = pd(u2)pc(u1|u2), (2)\nwhere pd denotes the distribution of the xi. The class conditional distributions can be obtained by Bayes’ rule,\npCα|u(u1,u2) = pα(u1,u2)\npα(u1,u2) + pβ(u1,u2) (3)\n= 1\n1 + pd(u2)pc(u1|u2)pd(u1)pc(u2|u1) , (4)\npCβ|u(u1,u2) = 1\n1 + pd(u1)pc(u2|u1)pd(u2)pc(u1|u2) . (5)\nThe prior class probabilities cancel because there are equally many samples in each class.\nBy replacing pd(·) with the model φ( · ;θ)/Z(θ), the partition functions cancel and the following parametrised ver-\nsions of the class conditional distributions are obtained\npCα|u(u1,u2;θ) = 1\n1 + φ(u2;θ)pc(u1|u2)φ(u1;θ)pc(u2|u1) , (6)\npCβ|u(u1,u2;θ) = 1\n1 + φ(u1;θ)pc(u2|u1)φ(u2;θ)pc(u1|u2) . (7)\nThe CNCE loss function is now formed as the negative log likelihood over the conditional class probabilities, in the same manner as in NCE (Gutmann & Hyvärinen, 2012),\nJN (θ) = 2\nκN κ∑ j=1 N∑ i=1 log [1 + exp(−G(xi,yij ;θ))] , (8)\nG(u1,u2;θ) = log φ(u1;θ)pc(u2|u1) φ(u2;θ)pc(u1|u2) . (9)\nThe CNCE loss function JN is the sample version of J (θ) = 2Exy log (1 + exp(−G(x,y;θ))), which is obtained by taking both N and κ to the∞ limit. To further develop the theory, it is helpful to write J (θ) as a functional of G, which gives\nJ̃ [G] = 2Exy log (1 + exp(−G(x,y))) . (10)\nWe then obtain the following theorem:\nTheorem (Nonparametric estimation). LetG : U×U→ R be a function of the form\nG(u1,u2) = f(u1)− f(u2) + log pc(u2|u1) pc(u1|u2) , (11)\nwhere f is a function from U to R. Under the assumption X = Y, J̃ attains a unique minimum at\nG∗(u1,u2) = log pd(u1)pc(u2|u1) pd(u2)pc(u1|u2)\n(12)\nfor (u1,u2) ∈ X×X with pd(u1) > 0 and pc(u1|u2) > 0.\nThe proof of a more general version is given in Supplementary Materials A. The theorem shows that in the limit of large N and κ, the optimal function f equals log pd up to an additive constant. For parametrisations that are flexible enough so thatG(u1,u2;θ∗) = G∗(u1,u2) for some value θ∗, the theorem together with the definition of G(u1,u2;θ) in (9) implies that φ(u;θ∗) ∝ pd(u). We have here the proportionality sign because the normalising constant is not estimated in CNCE.\nWhile the theorem above concerns nonparametric estimation, and hence does not take into account how G is parametrised, it forms the basis for a consistency proof of CNCE. A standard approach is to identify conditions under which JN (θ) converges uniformly in probability to J (θ) and then to appeal to e.g. Theorem 5.7 of (van der Vaart,\n1998). A similar approach where the Kullback-Leibler divergence takes the role of J can be used to prove consistency of maximum likelihood estimation. The conditions for uniform convergence are typically fairly technical and we here forego this endeavour and instead provide empirical evidence for consistency in Section 3.\nThe generic CNCE algorithm generally takes two steps: obtain the noise samples by sampling from the conditional noise distribution pc, and then minimise the loss function JN over the parameters θ. The user decides the trade-off between precision and computational expenditures via κ and also needs to provide pc.\nThere are two advantages to choosing pc over choosing the noise distribution in NCE. First, the observed data samples can be leveraged for sampling the noise, meaning that a resemblance to pd is easier to achieve than it would be for NCE. Indeed, all simulations in the paper were performed with the simple Gaussian specified below. Second, if pc is known to be symmetric, i.e. pc(u1|u2) = pc(u2|u1), it does not need to be evaluated because the densities cancel out in Equation (9).\nA simple symmetric choice of pc when x and y ∈ RD is\npc(y|x; ε) = N (y;x, ε21), yij = xi + εξij . (13)\nHere 1 is the identity matrix, ξij ∈ RD is a multivariate standard normal random variable and ε ∈ [0,∞) a scalar parameter that corresponds to the standard deviation of each dimension, and which therefore controls the similarity between Y and X. It is here assumed that the data have been standardised (Murphy, 2012, Chaper 4) so that the empirical variances of the data are one for each dimension. Otherwise, different values of ε ought to be used for each dimension.\nCNCE is also applicable to discrete random variables, e.g. by using a multinoulli distribution over y conditioned on x, and non-negative data (see Supplementary Materials C).\nIn our simulations, we adjust ε using simple heuristics so that the gradients of the loss function are not too small. This typically occurs when ε is too large so that the noise and data are easily distinguishable, but also when ε is too small. It can be verified that the loss function attains the value 2 log(2) for ε = 0 independent of the model and θ. In brief, the heuristic algorithm starts with a small ε that is incremented until the value of the loss function is sufficiently far away from 2 log(2).\nWhile small ε cause the gradients to be small in absolute terms, the following theorem shows that the loss function remains meaningful and that CNCE then corresponds to score matching (Hyvärinen, 2005).\nTheorem (Connection to score matching). Assume that φ(u;θ) is an unnormalised probability density and that\nfθ(u) = log φ(u;θ) is twice differentiable. If y = x + εξ where ξ is a vector of uncorrelated random variables of mean zero and variance one that are independent from x and have a symmetric density, then\nJ (θ) =ε 2 2 Ex [∑\ni\n∂2fθ(x)\n∂x2i +\n1 2 ||∇xfθ(x)||22 ] + 2 log(2) +O(ε3). (14)\nThe term in the brackets is the loss function that is minimised in score matching (Hyvärinen, 2005). The theorem is proved in Supplementary Materials B. Note that pc in (13) fulfills the conditions in the theorem.\nThe theorem can be understood as follows: Score matching consists in finding parameter values so that the slope of the model pdf matches the slope of the data pdf. For symmetric conditional noise distributions pc, the nonlinearity G in Equation (9) equals G(u1,u2;θ) = log φ(u1;θ) − log φ(u2;θ) = fθ(u1) − fθ(u2). From (12), we know that at the optimum of J (θ), G(u1,u2;θ) matches log pd(u1) − log pd(u2). The values which the arguments u1 and u2 take during the minimisation are determined by the conditional noise distribution. For small ε, the arguments are always close to each other, so that G(u1,u2;θ) is approximately proportional to a directional derivative of fθ(u) = log φ(u;θ) along a random direction. This means that for small ε, J (θ) is minimised when the slope of the model pdf matches the slope of the data pdf, as in score matching."
  }, {
    "heading": "3. Empirical validation of the theory",
    "text": "We here validate consistency and compare CNCE with NCE on synthetic data. The models below were used in unnormalised form for CNCE and NCE. For the results with MLE, the models were first normalised. Additional results for nonnegative and discrete data are provided in Supplementary Materials C."
  }, {
    "heading": "3.1. Models",
    "text": "The Gaussian model is an unnormalised multivariate Gaussian model in five dimensions with zero mean and parametrised precision matrix Λ. As the precision matrix is symmetric, the Gaussian model has 15 parameters,\nlog φ(u;Λ) = −1 2 uTΛu, u ∈ R5. (15)\nThe estimation error was measured as the Euclidean distance between the true and estimated parameters.\nThe ICA model is commonly used in signal possessing for blind source separation (Hyvärinen & Oja, 2000). Assuming equally many sources as data dimensions, D = 4, and a\nLaplacian distribution for the sources, the unnormalised ICA model is\nlog φ(u;B) = − √ 2 D∑ j=1 |bj · u|, u ∈ R4. (16)\nThe model is parametrised by the demixing matrix B and has D2 = 16 free parameters. The (normalised) ICA model can be estimated using MLE (Hyvärinen & Oja, 2000, 4.4.1). The estimation error was calculated as the Euclidean distance between true and estimated parameter vector after accounting for the sign and order ambiguity of the ICA model (Hyvärinen & Oja, 2000, 2.2) in the same manner as in (Gutmann & Hyvärinen, 2012).\nBoth the Gaussian and the ICA model were previously used to validate the consistency of NCE, and a Gaussian noise distribution achieved good estimation performance (Gutmann & Hyvärinen, 2012). In order to investigate the potential benefit of the adaptive noise of CNCE, we used the following more challenging “ring model” where the data lie in lower dimensional manifold.\nThe Ring model is given by\nlog φ(u;µr, γr) = − γr 2 (‖u‖2 − µr)2, u ∈ R5. (17)\nThe model is best understood in polar coordinates: the angular components are uniformly distributed and the radial direction is Gaussian with mean µr and precision γr. The mean is assumed known, and the task is to estimate the precision parameter γr. Figure 1 shows the (normalised) pdf for the ring model in two dimensions, as well as the NCE noise and the CNCE noise generated according to Equation (13). As often done in NCE, a Gaussian noise is chosen to match the mean and covariance of the data distribution. Because of the manifold structure of the data, the NCE noise is concentrated in areas where the data distribution takes small values, which is in contrast to the CNCE noise that well covers the data manifold."
  }, {
    "heading": "3.2. Results",
    "text": "Figures 2a and 2b show the estimation error as a function of the number of data points N . For both the Gaussian and ICA models, the CNCE error decreases linearly in the loglog domain as the sample size increases, which indicates convergence in quadratic mean, and hence consistency. Furthermore, as the number of noise-per-data points κ grows, the error appears to approach the MLE error.\nThe MLE of the ICA model had a tendency to get stuck in local minima for a small part of the estimations (13 out of 100). Consequently, the 0.9 quantile for MLE in Figure 2b shows a high and relatively constant error corresponding to such local minima. While this also occurred for CNCE, it is not visible in Figure 2b as it occurred less often (7/100 simulations).\nAs shown in Figure 2c, NCE performs better than CNCE for the Gaussian model given the same number of noise and data samples. For the ICA model, they are roughly onpar for sufficiently many data samples, see Figure 2d. An advantage for NCE on these models may not be surprising given that the NCE noise distribution already covers the data distribution very well. Furthermore, Figures 2e and 2f show that the difference between NCE and CNCE decreases as ratio of noise to data samples increases.\nFigure 3 shows the results for the ring model using κ = 10. CNCE achieves about one order of magnitude lower estimation error compared to NCE. With reference to Figure 1, this vast improvement over NCE can be understood as follows: For the noise distribution used in NCE, the majority of the noise samples end up inside the ring where the data sample probability is low, so that they are not useful for learning (the classification problem is too easy, with the noise not providing enough contrast). CNCE, on the other hand, automatically generates suitably contrastive noise on (or close to) the data manifold, which facilitates learning."
  }, {
    "heading": "4. Neural image model",
    "text": "To show that CNCE can be used to estimate complex unnormalised models, we used it for unsupervised deep learning and estimated a four-layer feed-forward neural network model from natural images. The model extends the two- and three-layer models of natural images previously estimated with NCE (Gutmann & Hyvärinen, 2012; 2013). We here focus on the learned features. In Supplementary Materials D, we present a qualitative comparison with NCE.\nThe data X are image patches of size 32× 32 px, sampled from 11 different monochrome images depicting wild life scenes (van Hateren & van der Schaaf, 1998) in the same manner as (Gutmann & Hyvärinen, 2013). Figure 4a shows examples of the extracted image patches. The sampled image patches were vectorised and both the ensemble mean and local mean (DC component) were subtracted. The resulting data were then whitened and their dimensionality reduced to D = 600 by principal component analysis (Murphy, 2012, Chapter 12.2), retaining 98% of the variance. We denote the data (random vector) after preprocessing by u(1)."
  }, {
    "heading": "4.1. Model specification",
    "text": "The unnormalised image model φ defined below consist of a “structured” part φ̃ that models the non-Gaussianity of the natural image data and a Gaussian part that accounts for the covariance structure. In the PCA space, the model is\nlog φ(u(1);θ) = log φ̃(u(1);θ)− 1 2 u(1) · u(1), (18)\nwhere · denotes the inner product between two vectors. This corresponds to a model for images defined in the subspace spanned by the first D principle component directions.\nThe Gaussian term in (18) tends to mask the non-Gaussian structure that we are primarily interested in. In order to better learn about the non-Gaussian properties of natural images, we define the conditional noise distribution as\nlog pc(u2|u1) = log p̃c(u2|u1)− 1\n2 u2 · u2 + const, (19)\nwhere p̃c is the Gaussian noise distribution in (13). With this choice, the two Gaussian terms of the model and noise cancel in the nonlinearity G(u1,u2;θ), so that\nG(u1,u2;θ) = log φ̃(u1;θ)p̃c(u2|u1) φ̃(u2;θ)p̃c(u1|u2) . (20)\nDue to the cancelling, φ̃ in Equation (18) is considered the effective model and p̃c the effective conditional noise distribution. Examples of noise patches sampled from p̃c are shown in Figure 4b.\nWe next define the (effective) model φ̃ via a four layers deep, fully connected, feed-forward neural network. The general idea is that we iterate between feature extraction and pooling layers (Gutmann & Hyvärinen, 2013). Unlike in many image models, we here do not impose translation invariance by using convolutional networks; neither do we fix the pooling layers but learn them from data. The input and output dimensions of each layer are provided in Supplementary Materials D.\nThe preprocessed image patches u(1) are first passed through a gain-control stage where they are centred and rescaled to cancel out some effects of the lighting conditions (Gutmann & Hyvärinen, 2012),\nũ(u) = √ D − 1 u− 〈u〉\n‖u− 〈u〉‖2 , 〈u〉 = 1 D D∑ k=1 uk. (21)\nThen they are passed through a feature extraction and a pooling layer,\nz (1) j = w (1) j · ũ(u (1)), (22)\nz (2) j = log ( q (2) j · ( z(1) )2 + 1 ) . (23)\nBoth the features w(1)j and pooling weights q (2) j are free parameters; we thus learn which 1st layer outputs to pool together. The pooling weights are restricted to be nonnegative, which we enforce by writing them as q(2)j = (w (2) j )\n2, with element-wise squaring. The log nonlinearity counteracts the squaring, leading to an approximation of the max operation (Gutmann & Hyvärinen, 2013).\nWe then repeat this processing block of gain control, feature extraction, and pooling: The outputs z(2)j of the 2\nnd layer are passed through the same gain control stage as the image patches, i.e. whitening, dimensionality reduction and rescaling, in line with previous work (Gutmann & Hyvärinen, 2013), followed by feature extraction and pooling,\nz (3) j = w (3) j · ũ (3), z (4) j = q (4) j · z (3). (24)\nThe pooling weights q(4)j are restricted to be non-negative, which is enforced as for the second layer. We here work with a simpler pooling model than in Equation (23). An output z (4) j of the pooling layer is large if q (4) j pools over units that are concurrently active, which is related to detecting sign congruency (Gutmann & Hyvärinen, 2009).\nThe unnormalised model φ̃ is then given by the total activation of the units in each layer, which means that the overall population activity indicates how likely an input is. Following (Gutmann & Hyvärinen, 2012; 2013) we used\nlog φ̃(L)(u(1);θ) = K(L)∑ j=1 fth ( z (L) j + b (L) j ) (25)\nfor L = 2, 3, 4 where fth is a smooth rectifying linear unit1 and b(L)j threshold parameters that are also learned from the data. The thresholding causes only strongly active units to contribute to log φ̃(L)(u(1);θ), which is related to sparse coding (Gutmann & Hyvärinen, 2012). In the case L = 1, the outputs z(1)j were passed through the additional nonlinearity log((·)2 + 1) prior to thresholding. This corresponds to computing the 2nd layer outputs with the 2nd layer weights fixed to correspond together to the identity matrix.\nWe learned the weights hierarchically one layer at a time, e.g. after learning of the 1st layer weights, we kept them fixed and learned the second layer weight vector w(2)j etc."
  }, {
    "heading": "4.2. Estimation results",
    "text": "The learned features, i.e receptive fields (RFs) of the 1st layer neurons, can be visualised as images. The learned 2nd layer weight vectors are sparse and the non-zero weights indicate over which 1st layer units the pooling happens. In Figure 5, we visualise randomly selected 2nd layer units,\n1fth(u) = 0.25 log(cosh(2u)) + 0.5u+ 0.17\nand the 1st layer units that they pool together. The 1st layer has learned Gabor features (Hyvärinen et al., 2009, Chapter 3) and the 2nd layer tends to pool these features according to frequency, orientation and locality, in line with previous models of natural images (Hyvärinen et al., 2009).\nTo visualise the learned weights on the 3rd layer, we followed (Gutmann & Hyvärinen, 2013) and visualised them as space-orientation receptive fields. That is, we probed the learned neural network with Gabor stimuli at different locations, orientations, and frequency, and visualised the response of the 3rd layer units as a polar plot. The polar plot is centred on the probing location, and the maximal radius is an indicator of the envelope and hence spatial frequency of the Gabor stimulus (larger circles correspond to lower spatial frequencies). We visualised the pooling on the 4th layer as for the 2nd layer by indicating the pooling strength with bars underneath the space-orientation receptive fields.\nFigure 6 shows examples of the learned 3rd and 4th layer units as well natural image inputs that elicit strong responses for the 4th layer units shown. The learned 3rd layer units detect longer straight or bended contours, which is largely in line with previous findings (Gutmann & Hyvärinen, 2013). The learned 4th layer unit on the top in the figure (unit 4) has learned to pool together 3rd layer units that share the same spatial orientation preference but are tuned to different spatial frequencies. This is line with previous modelling results (Hyvärinen et al., 2005) where similar pooling emerged in a model with more restrictive assumptions. The learned 4th layer unit shown on the bottom (unit 19) is tuned to vertical and horizontal low-frequency structure that bend around the southwest corner, which corresponds to a low-frequency corner detector. The full set of learned units is shown in the same way in Supplementary Materials D. Overall, the results show that CNCE both yields results that are in line with previous work and further finds novel and intuitively reasonable pooling patterns on the newly considered fourth layer."
  }, {
    "heading": "5. Conclusions",
    "text": "In this paper, we addressed the problem of density estimation for unnormalised models where the normalising partition function cannot be computed. We proposed a new method that follows the principles of noise-contrastive estimation and “learning by comparison”. In contrast to noisecontrastive estimation (NCE), in the proposed conditional noise-contrastive estimation (CNCE), the contrastive noise is allowed to depend on the data.\nThe main advantage of allowing the noise distribution to depend on the data is that the information in the data can be leveraged to produce, with rather simple conditional noise distributions as for example a Gaussian, noise samples that are well adapted to a wide range of different data and model\n2 4 6 8 10 12 14 16 18 20 22\ntypes. A second advantage is that for symmetric conditional noise distributions, a closed form expression for the conditional noise is not needed, which both enables a wider choice of distributions and has computational benefits. If the value of the normalisation constant is not of interest, a third advantage of the proposed approach is that the intractable partition function cancels out. Unlike in noise-contrastive estimation, there is thus never a need to introduce an additional parameter for the scaling of the model.\nWe provided theoretical and empirical arguments that CNCE provides a consistent estimator and proved that score matching emerges as a limiting case. As score matching makes more stringent assumptions but does not rely on sampling, it is an open question whether we can use this result to e.g. devise a hybrid approach where parts of the model are automatically estimated with the more suitable method.\nWe further found that the relative performances of NCE and CNCE are model dependent, but that CNCE has an\nadvantage in the important case where the data lie in a lower dimensional manifold.\nAn inherent limitation of empirical comparisons, and hence also those performed here, is that the results depend on the models and noise distributions used. However, given the adaptive nature of CNCE, simple Gaussian conditional noise distributions are likely widely useful, as exemplified by our results on unsupervised deep learning of a neural image model.\nThe proposed method further allows one to iteratively adapt the conditional noise distribution to make the classification task successively more challenging, as it was done in some simulations for NCE (Gutmann & Hyvärinen, 2010), and generally for learning in generative latent variable models (Gutmann et al., 2014; Goodfellow et al., 2014). This is an interesting direction of future work on CNCE."
  }, {
    "heading": "Acknowledgements",
    "text": "MUG would like to thank Jun-ichiro Hirayama at ATR and RIKEN AIP, Japan, for helpful discussions. We thank the anonymous reviewers for their insightful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition",
    "authors": ["X. Chen", "X. Liu", "M.J.F. Gales", "P.C. Woodland"],
    "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,",
    "year": 2015
  }, {
    "title": "On the convergence of Monte Carlo maximum likelihood calculations",
    "authors": ["C.J. Geyer"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
    "year": 1994
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Bregman divergence as general framework to estimate unnormalized statistical models",
    "authors": ["M. Gutmann", "J. Hirayama"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence,",
    "year": 2011
  }, {
    "title": "Learning features by contrasting natural images with noise",
    "authors": ["M. Gutmann", "A. Hyvärinen"],
    "venue": "In Proceedings of the International Conference on Artificial Neural Networks,",
    "year": 2009
  }, {
    "title": "Likelihood-free inference via classification",
    "authors": ["M. Gutmann", "R. Dutta", "S. Kaski", "J. Corander"],
    "venue": "arXiv preprint arXiv:1407.4981,",
    "year": 2014
  }, {
    "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "A three-layer model of natural image statistics",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "Journal of Physiology-Paris,",
    "year": 2013
  }, {
    "title": "Training products of experts by minimizing contrastive divergence",
    "authors": ["G.E. Hinton"],
    "venue": "Neural computation,",
    "year": 2002
  }, {
    "title": "Neural networks and physical systems with emergent collective computational abilities",
    "authors": ["J.J. Hopfield"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 1982
  }, {
    "title": "Estimation of non-normalized statistical models using score matching",
    "authors": ["A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Independent component analysis: algorithms and applications",
    "authors": ["A. Hyvärinen", "E. Oja"],
    "venue": "Neural networks,",
    "year": 2000
  }, {
    "title": "Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in v2",
    "authors": ["A. Hyvärinen", "M. Gutmann", "P.O. Hoyer"],
    "venue": "BMC Neuroscience,",
    "year": 2005
  }, {
    "title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies",
    "authors": ["S. Ji", "S. Vishwanathan", "N. Satish", "M. Anderson", "P. Dubey"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "A two-layer model of natural stimuli estimated with score matching",
    "authors": ["U. Köster", "A. Hyvärinen"],
    "venue": "Neural Computation,",
    "year": 2010
  }, {
    "title": "A fast and simple algorithm for training neural probabilistic language models",
    "authors": ["A. Mnih", "Y.W. Teh"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2012
  }, {
    "title": "Machine Learning: A Probabilistic Perspective",
    "authors": ["K.P. Murphy"],
    "year": 2012
  }, {
    "title": "A family of computationally efficient and simple estimators for unnormalized statistical models",
    "authors": ["M. Pihlaja", "M. Gutmann", "A. Hyvärinen"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence,",
    "year": 2010
  }, {
    "title": "Learning probabilistic submodular diversity models via noise contrastive estimation",
    "authors": ["S. Tschiatschek", "J. Djolonga", "A. Krause"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Asymptotic Statistics",
    "authors": ["A. van der Vaart"],
    "year": 1998
  }, {
    "title": "Independent component filters of natural images compared with simple cells in primary visual cortex",
    "authors": ["J.H. van Hateren", "A. van der Schaaf"],
    "venue": "Proceedings of the Royal Society of London B: Biological Sciences,",
    "year": 1998
  }, {
    "title": "Simple, fast noise-contrastive estimation for large RNN vocabularies",
    "authors": ["B. Zoph", "A. Vaswani", "J. May", "K. Knight"],
    "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2016
  }],
  "id": "SP:eded36d59a0b913072375ae548cb0ac5e91f5dc5",
  "authors": [{
    "name": "Ciwan Ceylan",
    "affiliations": []
  }, {
    "name": "Michael U. Gutmann",
    "affiliations": []
  }],
  "abstractText": "Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semiautomated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.",
  "title": "Conditional Noise-Contrastive Estimation of Unnormalised Models"
}