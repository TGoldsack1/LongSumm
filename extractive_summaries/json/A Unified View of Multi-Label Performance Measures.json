{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Multi-label classification aims to build classification models for objects assigned with multiple labels simultaneously, which is a common learning paradigm in real-world applications. In text categorization, a document may be associated with a range of topics, such as science, entertainment, and news (Schapire & Singer, 2000); in image classification, an image can have both field and mountain tags (Boutell et al., 2004); in music information retrieval, a piece of music can convey various messages such as classic, piano and passionate (Turnbull et al., 2008).\nIn traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal. In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as\n1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\neach instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake of the following two cases is more serious: one instance with three incorrect labels vs. three instances each with one incorrect label. Therefore, a number of performance measures focusing on different aspects have been proposed, such as Hamming loss, ranking loss, one-error, average precision, coverage (Schapire & Singer, 2000), micro-F1 and macro-F1 (Tsoumakas et al., 2011).\nMulti-label learning algorithms usually perform differently on different measures; however, there are only a few studies about multi-label performance measures. Dembczynski et al. (2010) showed that Hamming loss and subset 0/1 loss could not be optimized at the same time. Gao & Zhou (2013) proposed to study the Bayes consistency of surrogate losses for multi-label learning; they proved that none of convex surrogate loss is consistent with ranking loss, and gave a consistent surrogate loss function for Hamming loss in deterministic case. There are a number of studies about F-measure, mostly focusing on single-label tasks, including multi-label learning as application. For example, Ye et al. (2012) gave justifications and connections about F-measure optimization using decision theoretic approaches (DTA) and empirical utility maximization approaches (EUM). Later, Waegeman et al. (2014) studied the F-measure optimality of inference algorithms from the DTA perspective. Koyejo et al. (2015) devoted to study of EUM optimal multi-label classifiers. These theoretical studies offer much insight, though lacking a unified understanding of relation among a variety of multi-label performance measures. Moreover, some performance measures which have been popularly used in evaluation (Zhang & Wu, 2015) have not been theoretically studied.\nIn this paper, we try to disclose some shared properties among different measures and establish a unified understanding for multi-label performance evaluation. We propose a margin view to revisit eleven commonly used multi-label performance measures, including Hamming loss, ranking loss, one-error, coverage, average precision, macro-, micro- and instance-averaging F-measures and AUCs. Specifically, we propose the concepts of labelwise margin and instance-wise margin, based on which the corresponding effectiveness of multi-label classifiers is defined and then used as bridge to connect different perfor-\nmance measures. Our theoretical results show that by maximizing instance-wise margin, macro-AUC, macro-F1 and Hamming loss are to be optimized, whereas by maximizing label-wise margin, the other eight performance measures except micro-AUC are to be optimized. Inspired by the theoretical findings, we design the LIMO (Labelwise and Instance-wise Margins Optimization) approach to maximize both the two margins. Experiments validate our theoretical findings and demonstrate a flexible way to optimize different measures through one approach by different parameter settings.\nThe rest of the paper is organized as follows. Section 2 introduces the notation and definitions of eleven multi-label performance measures. Section 3 proposes the label-wise and instance-wise margins, and presents our theoretical results. Section 4 presents the LIMO approach. Section 5 reports the results of experiments. Finally, Section 6 concludes and indicates several future issues."
  }, {
    "heading": "2. Preliminaries",
    "text": ""
  }, {
    "heading": "2.1. Notation",
    "text": "Assume that xi ∈ Rd×1 is a real value instance vector, yi ∈ {0, 1}l×1 is a label vector for xi. m denotes the number of training samples. Therefore yij (i ∈ {1, . . . ,m}, j ∈ {1, . . . , l}) means the jth label of the ith instance, and yij = 1 or 0 means the jth label is relevant or irrelevant. The instance matrix is X ∈ Rm×d and the label matrix is Y ∈ {0, 1}m×l. H : Rd → {0, 1}l is the multi-label classifier, which consists of l models, one for a label, so H = {h1, . . . , hl} and hj(xi) denotes the prediction of yij . Moreover, F : Rd → Rl is the multi-label predictor and the predicted value can be regarded as the confidence of relevance. Similarly, F can be decomposed as {f1, . . . , fl} where fj(xi) denotes the predicted value of yij .\nH can be induced from F via thresholding functions. For example, hj(xi) = [[fj(xi) > t(xi)]] uses a thresholding function based on the instance xi and outputs 1 if predicted value is higher than the threshold. [[π]] returns 1 if predicate π holds, and 0 otherwise.\nFor simplification, we use Y i· to denote the ith row vector and Y ·j to denote the jth column vector of the label matrix. Furthermore, Y +i· (or Y − i· ) denotes the index set of relevant (or irrelevant) labels of Y i·. Formally, Y +i· = {j |yij = 1} and Y −i· = {j |yij = 0}. In terms of jth column of label matrix, Y +·j = {i |yij = 1} denotes the index set of positive instances of the jth label and Y −·j = {i |yij = 0} denotes the set of negative instances similarly. We use | · | to denote the cardinality of a set, thus, the number of relevant labels of xi is |Y +i· |."
  }, {
    "heading": "2.2. Multi-label Performance Measures",
    "text": "Table 1 summarizes the eleven multi-label performance measures commonly used in previous studies. The first five measures (Hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire & Singer (2000) and a multitude of works, e.g., Huang et al. (2012) and Zhang & Wu (2015). The next six measures are extensions of F-measure and AUC (the Area Under the ROC Curve) in multi-label classification via different averaging strategies. These F-measures are popluar both in algorithm evaluation (Liu & Tsang, 2015) and theoretical analysis (Koyejo et al., 2015). AUCs are used for algorithm evaluation such as in Lampert (2011), Pham et al. (2015) and Zhang & Wu (2015).\nSome of these measures are defined on classifier H , and they care about the binary classification performance. While some of these measures are defined on predictor F , and they usually measure the ranking performance of the predictor. We have noticed that some performance measures on ranking are ill-defined when F is a constant function. For example, if F outputs 1 for all labels, then oneerror(F ) = 0, coverage(F ) = 0 and various AUCs will be 1, which are the optimal values respectively. In multi-label learning community, there is often an underlying assumption that a total ranking can be induced from continuous real-value predictions, which is common in practical cases. In this paper, we still stick to the convention in previous works and assume that no tie happens in continuous prediction to solve this definition flaw."
  }, {
    "heading": "3. Theoretical Results",
    "text": "Here we define two new concepts: label-wise margin and instance-wise margin.\nDefinition 1. Given a multi-label predictor F : Rd → Rl and F = {f1, . . . , fl}, a training set (X,Y ), the labelwise margin on instance xi is defined as:\nγlabeli = min u,v {fu(xi)− fv(xi) | (u, v) ∈ Y +i· × Y − i· }.\nY +i· × Y − i· is the set of all the (relevant, irrelevant) label index pairs of instance i.\nDefinition 2. Given a multi-label predictor F : Rd → Rl and F = {f1, . . . , fl}, a training set (X,Y ), the instancewise margin on label Y ·j is defined as:\nγinstj = min a,b {fj(xa)− fj(xb) | (a, b) ∈ Y +·j × Y − ·j }.\nY +·j × Y − ·j is the set of all the (positive, negative) instance index pairs of label j.\nLabel-wise margin and instance-wise margin describe the discriminative ability of F . The larger the label-wise mar-\ngin, the easier to distinguish relevant and irrelevant labels of an instance. Meanwhile, the larger the instance-wise margin, the easier for F to distinguish positive and negative instances of a particular label. Therefore, we want to maximize label-wise/instance-wise margin to get better performance.\nAlthough we prefer maximizing these two margins, with respect to performance measures, the objective can be relaxed. We define three properties a predictor F can have: label-wise effective, instance-wise effective and double effective.\nDefinition 3. If all the label-wise margins ofF on a dataset D = (X,Y ) are positive, this predictor F is label-wise effective on D.\nDefinition 4. If all the instance-wise margins of F on a dataset D = (X,Y ) are positive, this predictor F is instance-wise effective on D.\nDefinition 5. If all the label-wise margins and instancewise margins of F on a dataset D = (X,Y ) are positive, this predictor F is double effective on D.\nRoughly speaking, label-wise effective means F can exactly distinguish relevant and irrelevant labels of each instance and instance-wise effective means F can exactly distinguish positive and negative instances of every label. Not surprisingly, double effective F has the strongest ability in distinguishing.\nIn the next two subsections, we use the effectiveness to an-\nalyze different performance measures, and summarize the analysis results in Section 3.3."
  }, {
    "heading": "3.1. Performance Measures on Ranking",
    "text": "Several multi-label performance measures can be empirically optimized according to the following theorems:\nTheorem 1. If a multi-label predictorF is label-wise effective on D, then ranking loss, one-error, coverage, average precision and instance-AUC are optimized on the dataset.\nProof. (a) Ranking loss: From the definition of labelwise effective, for every pair (u, v) ∈ Y +i· × Y − i· , we have fu(xi) > fv(xi). Therefore, the reversed set Sirank (in Table 1 ranking loss) is empty and the cardinality of the set is zero, which implies the cardinality sum of all reversed sets rloss(F ) = 0. Ranking loss is optimized.\n(b) One-error: For a label-wise effective F , because labelwise margin is positive on an instance xi, we have:\nmax u fu(xi) > max v\nfv(xi),∀u ∈ Y +i· ,∀v ∈ Y − i· .\nThen ∀xi, argmaxF (xi) ∈ Y +i· .\nThus, [[argmaxF (xi) /∈ Y +i· ]] = 0 for every instance xi, and one-error(F ) = 0. One-error is optimized.\n(c) Coverage: When F is label-wise effective, the maximum rank of a relevant label is less than the minimum rank of an irrelevant label, which means:\nmax u∈Y +i· rankF (xi, u) < min v∈Y −i· rankF (xi, v), (1)\nmax u∈Y +i·\nrankF (xi, u) = |Y +i· |.\nTherefore, coverage can be calculated as:\ncoverage(F ) = 1\nm ∑m i=1 [|Y +i· | − 1].\nWhich is the optimal value of coverage.\n(d) Average precision: Assume that j is a relevant label of instance i, it follows from Equation (1) that:\nrankF (xi, j) = |{k ∈ Y +i· |rankF (xi, k) ≤ rankF (xi, j)}|\nSince rankF (xi, j) is exactly the definition of Sijprecision, avgprec(F ) = 1, i.e, average precision is optimized.\n(e) Instance-AUC: Because of label-wise effective, for an instance xi, we have:\nfu(xi) > fv(xi),∀(u, v) ∈ Y +i· × Y − i· .\nTherefore, the size of the correct ordered prediction value pair on instance i is:\n|{(u, v) ∈ Y +i· × Y − i· |fu(xi) ≥ fv(xi)}| = |Y + i· ||Y − i· |.\nSo instance-AUC(F ) = 1 and instance-AUC is optimized.\nSimilar to the proof of instance-AUC, we can prove the result of macro-AUC:\nTheorem 2. If a multi-label predictor F is instance-wise effective on D, then macro-AUC is optimized.\nProof. Because of instance-wise effective, for a label vector Y ·j , we have:\nfj(xa) > fj(xb),∀(a, b) ∈ Y +·j × Y − ·j .\nTherefore, the size of the correct ordered prediction value pair on label j is:\n{(a, b) ∈ Y +·j × Y − ·j |fj(xa) ≥ fj(xb)} = |Y + ·j ||Y − ·j |.\nSo macro-AUC(F ) = 1 and macro-AUC is optimized.\nMicro-AUC sees the label matrix as a whole and cannot be optimized by instance-wise effective F or label-wise effective F . However, the double effective F is much more powerful. We now prove the following result of micro-AUC.\nTheorem 3. If a multi-label predictor F is double effective on D, then as the number of instances grows, micro-AUC is optimized.\nProof. We first prove a result of random variables Ai, B,C. If n random variablesA1, A2, · · · , An are drawn from uniform distribution U(0, 1), for a random constant a, the event that at least one Ai is smaller than a is:\nPr[∃Ai, Ai ≤ a] = 1− (1− a)n.\nAnother random variable B is uniformly distributed in (0,min{Ai}), and the probability that a random variable C ∼ U(0, 1) is bigger than B is:\nPr[C > B] ≥Pr[(C ≥ a) ∧ (∃Ai, Ai ≤ a)]\n=(1− a 2 )[1− (1− a)n]. (2)\nFor any small a, we can choose a large enough n to make Equation (2) close to 1.\nGiven a label matrix Y ∈ {0, 1}m×l and the corresponding prediction matrix F ∈ (0, 1)m×l, because predictor F is double effective, the prediction matrix satisfies the following conditions:\nFij > Fiu if Yij = 1 ∧ Yiu = 0, Fij > Fvj if Yij = 1 ∧ Yvj = 0.\nTo force the value in F is in (0, 1), we further assume a uniform distribution Fij ∼ U(0, 1) when Yij = 1.\nIf Yij = 0, then Fij should be less than Fiu if Yiu = 1 and Fvj if Yvj = 1. Suppose that the minimum value b is defined as:\nb = min { min v {Fvj |Yvj = 1},min u {Fiu|Yiu = 1} } .\nThen Fij is drawn from U(0, b). And we can choose a small constant value a > b.\nAccording to Equation (2), the probability that a random pair (i, j, u, v) to be a correct micro pair is:\nPmicro = Pr[Fij > Fuv|Yij = 1, Yuv = 0]\n≥ (1− a 2 )[1− (1− a)n],\nwhere n = k\nml (m+ l − 2)\nIn the practical case, the number of labels is proportional to the number of instances: k ∝ m. We assume k = pm where p is a constant smaller than l.\nlim m→∞ n = lim m→∞\np l (m+ l − 2) =∞,\nlim m→∞ |Smicro| |( ∑m i=1 |Y + i· |) · ( ∑m i=1 |Y − i· |)| = lim m→∞ Pmicro = 1.\nTherefore, micro-AUC is to be optimized as the number of instances grows.\nWith the above analysis, we can conclude that a label-wise effective F can optimize ranking loss, one-error, coverage, average precision, instance-AUC, micro-AUC and an instance-wise effective F can optimize macro-AUC. For micro-AUC, a double effective F can optimize it as the number of instances increases."
  }, {
    "heading": "3.2. Performance Measures on Classification",
    "text": "As mentioned in Section 2.2, there are some measures evaluating classifier H instead of predictor F . There are many thresholding or binarization strategies (Fan & Lin, 2007; Fürnkranz et al., 2008; Read et al., 2011). For simplicity, we focus on two main strategies: thresholding on each instance and thresholding on each label.\nA label-wise effective F can be equipped with a thresholding function based on each instance such as t(xi) and construct the H by hj(xi) = [[fj(xi) > t(xi)]]. However, using t(xi) on an instance-wise effective F is unreasonable since the predicted values on different labels may not be comparable. In a word, we should use suitable threshold function on different effective F s, i.e., t(xi) on each instance for label-wise effective F , and tj on each label\nfor instance-wise effective F . It is reasonable to use either t(xi) or tj for double effective F .\nTo formally analyze the performance measures on classification, we define the threshold error:\nDefinition 6. Given a descending ordered real-value sequence x1, x2, . . . , xk with an optimal cut number c∗, where c∗ ∈ N and 1 ≤ c∗ ≤ k. For a real value threshold t ∈ (xk − 1, x1 + 1), the threshold error = | argmini(xi)− c∗| where xi > t.\nIntuitively, the threshold error counts how many items are incorrectly classified on a descending ordered sequence where the correct answer is c∗. Based on the threshold error, we propose the following theorems about performance measures on classification.\nTheorem 4. For a label-wise effective F , if the thresholding function makes at most i error on each instance i, the micro-F1, instance-F1 and Hamming loss are bounded as follows:\nmicro-F1(H) = instance-F1(H)\n≥ 1 m m∑ i=1 min {2(|Y +i· | − i) 2|Y +i· | − i , 2|Y +i· | 2|Y +i· |+ i } ,\nhloss(H) ≤ 1 ml ∑m i=1 i.\nThe main idea of the above theorem is that, given the threshold error and the number of relevant labels, we can compute the gap between the worst possible and the perfect contingency table. Hence the F-measure is based on the contingency table, the lower bound can be deduced. The detailed proof of Theorem 4 is in Appendix A.1.\nSimilar to Theorem 4, we can prove the results for labelwise effective F :\nTheorem 5. For an instance-wise effective F , if the thresholding function makes at most j error on each label j, then the macro-F1 and Hamming loss are bounded as follows:\nmacro-F1(H) ≥ 1 l l∑ j=1 min {2(|Y +·j | − j) 2|Y +·j | − j , 2|Y +·j | 2|Y +·j |+ j } ,\nhloss(H) ≤ 1 ml ∑l j=1 j .\nThe detailed proof of Theorem 5 is in Appendix A.2.\nWith the above analysis, we can conclude that a labelwise effective F can optimize instance-F1 and micro-F1, an instance-wise effective F can optimize macro-F1. Both the two effective F s can optimize Hamming loss. For a double effective F , because it enjoys both the properties, it can optimize all the above mentioned performance measures if proper thresholds are used."
  }, {
    "heading": "3.3. Summary",
    "text": "Table 2 summarizes our theoretical results in Section 3.1 and 3.2. Each row shows the results of one multi-label performance measure. Note that double effective is a special case of label-wise effective and instance-wise effective and thus, if one performance measure is optimized by either label-wise or instance-wise effective predictor, it will also be optimized by double effective predictor.\nIn the light of the analysis, the performance on different performance measures through optimizing margins can be expected. For example, if one maximizes instance-wise margin on each label, s/he will get good performance on macro-AUC but may suffer higher loss on ranking loss, coverage and some other measures where ‘7’ marked in the inst-wise column. If one tries to maximize the label-wise margin but pay no attention to instance-wise margin, s/he may perform well on average precision but poor on macroF1 (e.g., Elisseeff & Weston (2002)). Maximzing both the label-wise margin and instance-wise margins to get a double effective F is expected to be the best choice."
  }, {
    "heading": "4. The LIMO Approach",
    "text": "The above analysis reveals that maximizing different margins will optimize different measures, and if possible, double effective F is prefered since it enjoys the benefits of maximizing both the label-wise margin and the instancewise margin. Therefore, we propose the LIMO approach. LIMO is a single approach which can optimize both the two margins, and it can also be degenerated to optimize either margin seperately via parameter setting."
  }, {
    "heading": "4.1. Formulation",
    "text": "Suppose that F is a linear predictor, which means F (X) = W TX where W = [w1,w2, · · · ,wl]. We propose the following formulation:\nargmin W ,ξ l∑ i=1 ||wi||2 + λ1 m∑ i=1 ∑ (u,v) ξuvi + λ2 l∑ j=1 ∑ (a,b) ξjab\ns.t. w>u xi −w>v xi > 1− ξuvi , ξuvi ≥ 0, for i = 1, · · · ,m and (u, v) ∈ Y +i· × Y − i· ,\nw>j xa −w>j xb > 1− ξ j ab, ξ j ab ≥ 0,\nfor j = 1, · · · , l and (a, b) ∈ Y +·j × Y − ·j .\n(3)\nHere ξuvi and ξ j ab are the slack variables, and λ1, λ2 are the trade-off parameters. When both λ1 and λ2 are positive, both label-wise and instance-wise margins are considered. If we set λ1 = 0 (or λ2 = 0), then only the instance-wise (or label-wise) margin is considered. In this paper, if the approach only considers instance-wise (or label-wise) margin , we call the approach as LIMO-inst (or LIMO-label). And LIMO considers both the two margins.\nAlgorithm 1 LIMO Input:\nData matrix X ∈ Rm×d, label matrix Y ∈ {0, 1}m×l, step size η, trade-off parameters λ1, λ2, and the maximium iteration number T .\nProcedure: 1: Initialize W 0 with N(0, 1/ √ d) random values.\n2: Compute the weight vector cinst of each instance, cinsti = |Y + i· ||Y − i· |/ ∑m i=1 |Y + i· ||Y − i· |. 3: Compute the weight vector clabel of each label, clabelj = |Y + ·j ||Y − ·j |/ ∑l j=1 |Y + ·j ||Y − ·j |. 4: for t = 1, 2, · · · , T do 5: Random sample an instance xti using weight c\ninst, 6: Random sample a positive label yiu and a negative label yiv of instance xti. 7: if 1−w>u xti +w>v xti > 0 then 8: wtu = w t−1 u − η(−λ1xti +wt−1u ). 9: wtv = w t−1 v − η(λ1xti +wt−1v ). 10: end if 11: Random sample index j of label using weight clabel. 12: Random sample a positive instance xta and a negative instance xtb on label j. 13: if 1−w>j xta +w>j xtb > 0 then 14: wtj = w t−1 j − η(λ2(xtb − xta) +w t−1 j ). 15: end if 16: end for 17: W = 1T ∑T t=1 W\nt. Output:\nMulti-label linear model W ."
  }, {
    "heading": "4.2. Algorithm",
    "text": "The objective Equation (3) is difficult to solve directly because of the large number of constraints and slack variables. For a training set with m instances and l labels, the number of constraints will be O(m2l + ml2), which may exceed memory limit in real-world applicaitons.\nIn order to deal with the computational problem, we solve Equation (3) by stochastic gradient descent (SGD) with fixed step size and the default averaging technique in Shalev-Shwartz & Ben-David (2014, Chapter 14.3). The key point of SGD is to find out a random vector, whose expected value at each iteration equals the gradient direction. We randomly sample two kinds of triplets and use them to compute the correct direction. At each iteration t, we sample a triplet (xti, yiu, yiv) where yiu is relevant and yiv is irrelevant, and a triplet (j,xta,x t b) where x t a is a positive instance and xtb is a negative instance both on label j. Then we use the two triplets to compute the random gradient vector for SGD. The detailed algorithm is presented in Algorithm 1 and the proof that the random vector is an unbiased estimation of the gradient direction is available in Appendix A.3.\nAfter the training procedure, we can use the linear model to predict continuous confidence values on the training data, then choose the best threshold value by optimizing a specific classification measure."
  }, {
    "heading": "5. Experiments",
    "text": "We conduct experiments with LIMO on both synthetic and benchmark data. Note that the main purpose of our work is to study multi-label performance measures from the aspect of margin optimization, and thus, the goal of our experiments is to validate our theoretical findings rather than claim that LIMO is superior, although its performance is really highly competitive."
  }, {
    "heading": "5.1. Synthetic Data",
    "text": "We conduct experiments on synthetic data with 4 labels. 2000 data points are randomly generated from a (−1,+1)2 square, and the labels are assigned as in Figure 1. 50% data are held out for testing. The synthetic data is designed to simulate a typical real-world circumstance. The number of co-occurrent labels varies, the regions of each label are different and the data cannot be perfectly seperated by a linear learner.\nTo demonstrate the relationship between margins and performance measures, we degenerate LIMO to only consider either margin by setting the trade-off parameter λ1 or λ2 to zero. LIMO-inst sets λ1 = 0 and LIMO-label sets λ2 = 0. The other parameter is set to 100 and LIMO sets\nFigure 1. Input space consists of four regions with different assignments of the label set {A,B,C,D}. The center point is with coordinate (0, 0).\nABCD\nABD\nBCA\n-1 0 1 -1\n0\n1\nλ1 = λ2 = 100. Ten replications of the experiment are conducted and the average results are reported. Because the range of performance measure coverage is not [0, 1], while some performance measures are better when higher, and some are better when lower, we rescale all the performance values into relative values for clearer visualization. The best one is rescaled to 1 and the worst one is rescaled to 0. Figure 2 shows the relative results, where the originally worst performance value is given on the right. instance F1 macro F1 micro F1\nHamming loss\n1.0 0.8 0.6 0.4 0.2 0.0\nLIMO-inst-t LIMO-inst-t(x) LIMO-label-t LIMO-label-t(x) LIMO-t\nLIMO-t(x) 0.804\n0.852\n0.837\n0.188\nrelative value\na b s o lu te w o rs t v a lu e\n* *\n*\n*\n*\nThe results shown in Figure 2 support our theoretical findings in Table 2. For example, micro-AUC is considered to be optimized by double effective F but not the other two, therefore LIMO (the red circle) gets the best relative value. For some measures proved to be optimized by label-wise margin such as ranking loss, average precision, coverage and instance-AUC, LIMO-label beats LIMO-inst. While for macro-AUC, LIMO-inst wins. For one-error, all three versions of LIMO do extremely well and get less than 0.001 absolute value, which may be the reason why the relative values are unexpected.\nmicro-AUC\ninstance-AUC\nmacro-AUC\ncoverage\none-error\navg. precision\nranking loss\n1.0 0.8 0.6 0.4 0.2 0.0\nLIMO-inst LIMO-label LIMO\n0.854\n0.973\n0.828\n1.575\n0.001\n0.992\n0.027\nrelative value\na b s o lu te w o rs t v a lu e\nFigure 3 shows the relative performance on classification. We use two types of thresholding discussed in Section 3.2:\nthreshold function based on each instance or each label (denoted by -t(x) or -t in the legend). The thresholds are estimated on training data. This figure exactly shows our theoretical results: LIMO-label equipped with t(x) can optimize instance-F1 and micro-F1; LIMO-inst equipped with t can optimize macro-F1. By considering both label-wise margin and instance-wise margin, LIMO works well on all four classificaiton measures."
  }, {
    "heading": "5.2. Benchmark Data",
    "text": "We conduct experiments on eleven multi-label performance measures to further show that optimizing the labelwise or the instance-wise margin can lead to different results, as revealed in our theoretical analysis.\nFive benchmark multi-label datasets1 are used in our experiments. We choose them because they denote different domains: (i) A music dataset CAL500, (ii) an email dataset enron, (iii) a clinical text dataset medical, (iv) an image dataset corel5k, (v) a tagging dataset bibtex. We randomly split each dataset into two parts, i.e., 70% for training and 30% for testing. The experiments are repeated ten times, and the averaged results are reported.\nBecause our algorithm optimizes a linear model, three linear methods called Binary Relevance (BR) (Zhang & Zhou, 2014), ML-kNN (Zhang & Zhou, 2007) and GFM (Waegeman et al., 2014) are provided for fair comparison. As in experiments on synthetic data, we degenerate LIMO (λ1 = λ2 = 1) to LIMO-inst (λ1 = 0, λ2 = 1) and LIMOlabel (λ1 = 1, λ2 = 0). The step size of SGD is set to 0.01. For BR, L2-regularized SVM (Chang & Lin, 2011) with C=1 is used as base learner. For ML-kNN and GFM, the number of nearest neighbors is 10. Suitable thresholds discussed in Section 3.2 are used for classification measures. We take the default parameter settings recommended by authors of the compared methods respectively. Because on one hand, we believe the parameter settings recommended by their authors are meaningful, on the other hand, it is hard to say which parameter setting is better in terms of eleven performance measures.\nBecause some measures are better when higher, and some measures are better when lower, to demonstrate the results more clearly, we compute the average rank of each approach over all datasets on a specific measure. For example, when we want to examine how LIMO performs on ranking loss, we first compute the ranks on each dataset: LIMO ranks 1st on CAL500, enron, bibtex and ranks 2nd on medical, corel5k. Then the average rank of LIMO on ranking loss is (1+1+1+2+2)/5=1.4. Figure 4 shows the average ranks. Due to the space limit, the detailed results used to compute the ranks are provided in Appendix B.2.\n1http://mulan.sourceforge.net/datasets-mlc.html\nThe results in Figure 4 are consistent with our theoretical findings. LIMO-inst (the square) performs well on marcoF1 and macro-AUC, while LIMO-label (the triangle) performs well on other performance measures. LIMO (the circle) almost ranks top on every performance measure.\nThe experiments on synthetic and benchmark data support our theoretical analysis. Although different performance measures focus on different aspects, they share the common property which is formalized in our work as labelwise margin and instance-wise margin. In practice, it is recommended to use higher weight (λ1/λ2) on specific margin to optimize the required performance measure. LIMO with nonlinear predictors may perform better, which needs a novel optimization algorithm."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we establish a unified view for a variety of multi-label performance measures. Based on the proposed concepts of label-wise/instance-wise margins, we prove that some performance measures are to be optimized by label-wise effective classifiers, whereas some by instancewise effective classifiers. Inspired by the theoretical findings, we design the LIMO approach which can be adjusted to label-wise/instance-wise effective via different parameter settings.\nOur work discloses that there are some shared properties among different subsets of multi-label performance measures. This explains why some measures seem to be redundant in experiments, and suggests that in future empirical studies, rather than randomly grasp a set of measures for evaluation, it is more informative to evaluate using measures with different properties, such as some measures optimized by label-wise effective predictors and some optimized by instance-wise effective predictors. In the future, it is encouraging to study the asymptotic properties of these performance measures when the two margins are suboptimal. The margin view also sheds a light for the design of novel multi-label algorithms."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by the NSFC (61333014), 973 Program (2014CB340501), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Authors want to thank reviewers for helpful comments, and thank Sheng-Jun Huang, Xiu-Shen Wei, Miao Xu for reading a draft."
  }],
  "year": 2017,
  "references": [{
    "title": "Learning multi-label scene classification",
    "authors": ["Boutell", "Matthew R", "Luo", "Jiebo", "Shen", "Xipeng", "Brown", "Christopher M"],
    "venue": "Pattern Recognition,",
    "year": 2004
  }, {
    "title": "LIBSVM: A library for support vector machines",
    "authors": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"],
    "venue": "ACM Trans. Intelligent Systems and Technology,",
    "year": 2011
  }, {
    "title": "Regret analysis for performance metrics in multi-label classification: the case of hamming and subset zero-one loss",
    "authors": ["Dembczynski", "Krzysztof", "Waegeman", "Willem", "Cheng", "Weiwei", "Hüllermeier", "Eyke"],
    "venue": "In ECML/PKDD,",
    "year": 2010
  }, {
    "title": "A kernel method for multi-labelled classification",
    "authors": ["Elisseeff", "André", "Weston", "Jason"],
    "venue": "In NIPS, pp",
    "year": 2002
  }, {
    "title": "A study on threshold selection for multi-label classification",
    "authors": ["Fan", "Rong-En", "Lin", "Chih-Jen"],
    "venue": "National Taiwan University,",
    "year": 2007
  }, {
    "title": "Multilabel classification via calibrated label ranking",
    "authors": ["Fürnkranz", "Johannes", "Hüllermeier", "Eyke", "Mencı́a", "Eneldo Loza", "Brinker", "Klaus"],
    "venue": "Machine Learning,",
    "year": 2008
  }, {
    "title": "On the consistency of multilabel learning",
    "authors": ["Gao", "Wei", "Zhou", "Zhi-Hua"],
    "venue": "Artificial Intelligence,",
    "year": 2013
  }, {
    "title": "Multilabel hypothesis reuse",
    "authors": ["Huang", "Sheng-Jun", "Yu", "Yang", "Zhou", "Zhi-Hua"],
    "venue": "In ACM SIGKDD,",
    "year": 2012
  }, {
    "title": "Consistent multilabel classification",
    "authors": ["Koyejo", "Oluwasanmi", "Natarajan", "Nagarajan", "Ravikumar", "Pradeep", "Dhillon", "Inderjit S"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Maximum margin multi-label structured prediction",
    "authors": ["Lampert", "Christoph H"],
    "venue": "In NIPS, pp",
    "year": 2011
  }, {
    "title": "On the optimality of classifier chain for multi-label classification",
    "authors": ["Liu", "Weiwei", "Tsang", "Ivor W"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Multi-instance multi-label learning in the presence of novel class instances",
    "authors": ["Pham", "Anh T", "Raich", "Raviv", "Fern", "Xiaoli Z", "Arriaga", "Jesús Pérez"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Classifier chains for multi-label classification",
    "authors": ["Read", "Jesse", "Pfahringer", "Bernhard", "Holmes", "Geoff", "Frank", "Eibe"],
    "venue": "Machine Learning,",
    "year": 2011
  }, {
    "title": "Boostexter: A boosting-based system for text categorization",
    "authors": ["Schapire", "Robert E", "Singer", "Yoram"],
    "venue": "Machine Learning,",
    "year": 2000
  }, {
    "title": "Understanding Machine Learning: From Theory to Algorithms",
    "authors": ["Shalev-Shwartz", "Shai", "Ben-David"],
    "year": 2014
  }, {
    "title": "Random k-labelsets for multilabel classification",
    "authors": ["Tsoumakas", "Grigorios", "Katakis", "Ioannis", "Vlahavas", "Ioannis P"],
    "venue": "IEEE Trans. Knowledge and Data Engineering,",
    "year": 2011
  }, {
    "title": "Semantic annotation and retrieval of music and sound effects",
    "authors": ["Turnbull", "Douglas", "Barrington", "Luke", "Torres", "David A", "Lanckriet", "Gert R. G"],
    "venue": "IEEE Trans. Audio, Speech & Language Processing,",
    "year": 2008
  }, {
    "title": "On the bayes-optimality of F-measure maximizers",
    "authors": ["Waegeman", "Willem", "Dembczynski", "Krzysztof", "Jachnik", "Arkadiusz", "Cheng", "Weiwei", "Hüllermeier", "Eyke"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Optimizing F-measure: A tale of two approaches",
    "authors": ["Ye", "Nan", "Chai", "Kian Ming Adam", "Lee", "Wee Sun", "Chieu", "Hai Leong"],
    "venue": "In ICML,",
    "year": 2012
  }, {
    "title": "LIFT: Multi-label learning with label-specific features",
    "authors": ["Zhang", "Min-Ling", "Wu", "Lei"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }, {
    "title": "ML-KNN: A lazy learning approach to multi-label learning",
    "authors": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"],
    "venue": "Pattern Recognition,",
    "year": 2007
  }, {
    "title": "A review on multilabel learning algorithms",
    "authors": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"],
    "venue": "IEEE Trans. Knowledge and Data Engineering,",
    "year": 2014
  }],
  "id": "SP:469f77bd2287a2edb4f4f9ac13f121431fa50b1e",
  "authors": [{
    "name": "Xi-Zhu Wu",
    "affiliations": []
  }, {
    "name": "Zhi-Hua Zhou",
    "affiliations": []
  }],
  "abstractText": "Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than singlelabel setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures are to be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results validate our theoretical findings.",
  "title": "A Unified View of Multi-Label Performance Measures"
}