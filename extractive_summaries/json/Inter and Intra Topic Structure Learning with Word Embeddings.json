{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Significant research effort has been devoted to developing advanced text analysis technologies. Probabilistic topic models such as Latent Dirichlet Allocation (LDA), are popular approaches for this task, which discover latent topics from text collections. One preferred property of probabilistic topic models is interpretability: one can explain that a document is composed of topics and a topic is described by words. Although widely used, most variations of standard vanilla topic models (e.g., LDA) assume topics are independent and there are no structures among them. This limits those models’ ability to explore any hierarchical thematic structures. Therefore, it is interesting to develop a model that is capable of exploring topic structures and yields not only improved modeling accuracy but also better\n1Faculty of Information Technology, Monash University, Australia 2McCombs School of Business, University of Texas at Austin. Correspondence to: Lan Du <lan.du@monash.edu>, Mingyuan Zhou <mingyuan.zhou@mccombs.utexas.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ninterpretability.\nOne popular direction to explore topic structure is using the hierarchical/deep representation of text data, such as the nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015), Deep Poisson Factor Analysis (DPFA) (Gan et al., 2015), and Gamma Belief Network (GBN) (Zhou et al., 2016; Cong et al., 2017). In general, these models assume that topics in the higher layers of a hierarchy are more general/abstract than those in the lower layers. Therefore, by revealing hierarchical correlations between topics, topic hierarchies provide an intuitive way to understand text data.\nIn addition to topic hierarchies, we are also interested in analyzing the fine-grained thematic structure within each individual topic. As we know, in conventional models, topics are discovered locally from the word co-occurrences in a corpus. So we refer those topics as local topics. Due to the limitation of the context of a target corpus, some local topics may be hard to interpret because of the following two effects: (1) They can mix the words which co-occur locally in the target corpus but are less semantically related in general; (2) Local topics can be dominated by specialized words, which are less interpretable without extra knowledge. For example, we show four example topics of our experiments in Table 1, where we can see: Topic 1 is composed of the words from both the “scientific publication” and “biology” aspects; Topic 2 is a mixture of “sports” and “music”; Topics 3 and 4 are very specific topics about “singer” and “video game” respectively. We humans are able to understand those local topics in the above way because we are equipped with the global semantics of the words, making us go beyond the local context of the target corpus. Therefore, we are motivated to propose a model which is able to automatically analyze the fine-grained thematic structures of local topics, further improving the interpretability of topic modeling.\nFortunately, word embeddings such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), and FastText (Bojanowski et al., 2017) can be used as an accessible source of global semantic information for topic models. Learned from large corpora, word embeddings encode the semantics of words with their locations in a space, where more related words are closer to each other. For example in Topic 1, according to the distances of word embeddings, the words “biology, cell, psychology, bioinformatics” should be in one cluster and “journal, science, research, international, scientific” should be in the other. Therefore, if a topic model can leverage the information in word embeddings, it may discover the fine-grained thematic structures of local topics. Furthermore, it has been demonstrated that conventional topic models suffer from data sparsity, resulting in a large performance degradation on some shorter internetgenerated documents like tweets, product reviews, and news headlines (Zuo et al., 2016; Zhao et al., 2017c). In this case, word embeddings can also serve as complementary information to alleviate the sparsity issue in topic models.\nIn this paper, we propose a novel deep structured topic model, named the Word Embeddings Deep Topic Model, WEDTM1, which improves the interpretability of topic models by discovering topic hierarchies (i.e., inter topic structure) and fine-grained interpretations of local topics (i.e., intra topic structure). Specifically, the proposed model adapts a multi-layer Gamma Belief Network which generates deep representations of topics as well as documents. Moreover, WEDTM is able to split a local topic into a set of sub-topics, each of which captures one finegrained thematic aspect of the local topic, in a way that each sub-topic is informed by word embeddings. WEDTM has the following key properties: (1) Better interpretability with topic hierarchies and sub-topics informed by word embeddings. (2) The state-of-the-art perplexity, document classification, and topic coherence performance, especially for sparse text data. (3) A straightforward Gibbs sampling algorithm facilitated by fully local conjugacy under data augmentation."
  }, {
    "heading": "2. Related Work",
    "text": "Deep/hierarchical topic models: Several approaches have been developed to learn hierarchical representations of documents and topics. The Pachinko Allocation model (PAM) (Li & McCallum, 2006) assumes the topic structure is modeled by a directed acyclic graph (DAG), which is document specific. nCRP (Blei et al., 2010) models topic hierarchies by introducing a tree structure prior constructed with multiple CRPs. Paisley et al. (2015); Kim et al. (2012); Ahmed et al. (2013) further extend nCRP by either softening its constraints or applying it to different problems respec-\n1https://github.com/ethanhezhao/WEDTM\ntively. Poisson Factor Analysis (PFA) (Zhou et al., 2012) is a nonnegative matrix factorization model with Poisson link, which is a popular alternative to LDA, for topic modeling. The details of the close relationships between PFA and LDA can be found in Zhou (2018). There are several deep extensions to PFA for documents, such as DPFA (Gan et al., 2015), DPFM (Henao et al., 2015), and GBN (Zhou et al., 2016). Among them, GBN factorizes the factor score matrix (topic weights of documents) in PFA with nonnegative gamma-distributed hidden units connected by the weights drawn from the Dirichlet distribution. From a modeling perspective, GBN is related to PAM, while GBN assumes there is a corpus-level topic hierarchy shared by all the documents. As reported by Cong et al. (2017), GBN outperforms other hierarchical models including nHDP, DPFA, and DPFM. Despite having the attractive properties, these deep models barely consider intra topic structures or the sparsity issue associated with internet-generated corpora.\nWord embedding topic models: Recently, there is a growing interest in applying word embeddings to topic models, especially for sparse data. For example, WF-LDA (Petterson et al., 2010) extends LDA to model word features with the logistic-normal transform, where word embeddings are used as word features in Zhao et al. (2017b). LF-LDA (Nguyen et al., 2015) integrates word embeddings into LDA by replacing the topic-word Dirichlet multinomial component with a mixture of a Dirichlet multinomial component and a word embedding component. Due to the non-conjugacy in WF-LDA and LF-LDA, part of the inference has to be done by MAP optimization. Instead of generating tokens, Gaussian LDA (GLDA) (Das et al., 2015) directly generates word embeddings with the Gaussian distribution. The model proposed in Xun et al. (2017) further extends GLDA by modeling topic correlations. MetaLDA (Zhao et al., 2017c; 2018a) is a conjugate topic model that incorporates both document and word meta information. However, in MetaLDA, word embeddings have to be binarized, which can lose useful information. WEI-FTM (Zhao et al., 2017b) is a focused topic model where a topic focuses on a subset of words, informed by word embeddings. To our knowledge, topic hierarchies and sub-topics are not considered in most of the existing word embedding models."
  }, {
    "heading": "3. The Proposed Model",
    "text": "Based on the PFA framework, WEDTM is a hierarchical model with two major components: one for discovering the inter topic hierarchies and the other for discovering intra topic structures (i.e., sub-topics) informed by word embeddings. The two components are connected by the bottom-layer topics, detailed as follows. Assume that each document j is presented as a word count vector x(1)j ∈ NV0 , where V is the size of the vocabulary; the pre-trained L\ndimensional real-valued embeddings for each word v ∈ {1, · · · , V } are stored in a L-dimensional vector fv ∈ RL. Now we consider WEDTM with T hidden layers, where the t-th layer is with Kt topics and kt is the index of each topic. In the bottom layer (t = 1), there are K1 (local) topics, each of which is associated with S sub-topics. To assist clarity, we split the generative process of the model into three parts, shown as follows:\nGenerating documents  θ (1) j ∼ Gam [ Φ(2)θ (2) j , p (2) j /(1− p (2) j ) ] , φ (1) k1 ∼ Dir (βk1) ,\nx (1) j ∼ Pois\n( Φ(1)θ\n(1) j\n) ,\nInter structure  θ (T ) j ∼ Gam ( r, 1/c (T+1) j ) , · · · θ (t) j ∼ Gam ( Φ(t+1)θ (t+1) j , 1/c (t+1) j ) (t < T ), φ (t) kt ∼ Dir (η01) (t > 1),\n· · ·\nIntra structure  w<s>k1 ∼ N [0, diag(1/σ <s>)] , α<s>k1 ∼ Gam(α <s> 0 /S, 1/c <s> 0 ), β<s>vk1 ∼ Gam ( α<s>k1 , e f>v w <s> k1 ) ,\nβvk1 := ∑S s β <s> vk1 ,\nwhere (t) is the index of the layer that a variable belongs to and <s> is the index of sub-topic s. To complete the model, we impose the following priors on the latent variables:\nrkT ∼ Gam(γ0/KT , 1/c0), γ0 ∼ Gam(a0, 1/b0), p(t)j ∼ Beta(a0, b0),\nc (t) j ∼ Gam(e0, 1/f0), α <s> 0 ∼ Gam(e0, 1/f0),\nc<s>0 ∼ Gam(e0, 1/f0), σ<s>l ∼ Gam(a0, 1/b0).\nWe first take a look at the bottom layer of the model, i.e., the process of generating the documents, which follows a PFA framework. In this part, WEDTM models the word counts x(1)j in a document by a Poisson (Pois) distribution and factorizes the Poisson parameters into a product of the factor loadings Φ(1) ∈ RV×K1+ and hidden units θ (1) j . θ (1) j is the first-layer latent representation (unnormalized topic weights) of document j, each element of which is drawn from a gamma (Gam) distribution2. The k1-th column of Φ(1), φ(1)k1 ∈ R V + is the word distribution of topic k1, drawn from a Dirichlet (Dir) distribution. We then explain the component for discovering inter topic hierarchies, which is similar to the structure of GBN (Zhou et al., 2016). Specifically, the shape parameter of θ(1)j is factorized into θ (2) j and Φ(2) ∈ RK1×K2+ , where θ (2) j is the second-layer latent\n2The first and second parameters of the gamma distribution are the shape and scale respectively.\nrepresentation of document j and φ(2)k2 ∈ R K1 + models the correlations between topic k2 and all the first-layer topics. Note that strictly speaking, k2 is not a “real” topic as it is not a distribution over words. But it can be interpreted with words by Φ(1)φ(2)k2 . By repeating this construction, we are able to build a deep structure to discover topic hierarchies.\nNow we explain how sub-topics are discovered for the bottom-layer topics with the help of word embeddings. First of all, WEDTM applies individual asymmetric Dirichlet parameters βk1 ∈ RV+ for each bottom-layer (local) topic φ\n(1) k1 . We further construct βvk1 = ∑S s β <s> vk1\n, where β<s>vk1 models how strongly word v is associated with sub-topic s in local topic k1. For each sub-topic s, we introduce an Ldimensional sub-topic embedding: w<s>k1 ∈ R\nL. As β<s>vk1 is gamma distributed, its scale parameter is constructed by the dot product of the embeddings of sub-topic s and word v through the exponential function.\nThe basic idea of our model is summarized as follows:\n1. In terms of sub-topics, we assume each (local, bottomlayer) topic is associated with several sub-topics, in a way that the sub-topics contribute to the prior of the local topic via a sum model (Zhou, 2016). Therefore, if a word dominates in one or more sub-topics, it is likely that the word will still dominate in the local topic. With this construction, a sub-topic is expected to capture one fine-grained thematic aspect of the local topic and each sub-topic can be directly interpreted with words via β<s>k1 ∈ R V + .\n2. To leverage word embeddings to inform the learning of sub-topics, we introduce the sub-topic embedding for each of them, w<s>k1 , which directly interacts with the word embeddings. Therefore, sub-topic embeddings are learned with both the local context of the target corpus and the global information of word embeddings. According to our model construction, the probability density function of βvk1 is the convolution of S covariance-dependent gamma distributions (Zhou, 2016). Therefore, if the sub-topic embeddings of s and word embeddings of v are close, the dot product of them will be large, giving a large expectation of β<s>vk1 . The large expectation means that v has a large weight in sub-topic s of k. Finally, β<s>vk1 further contributes\nto the local topic’s prior βvk1 , informing φ (1) vk1\nof the local topic.\n3. It is also noteworthy the special case of WEDTM, where S = 1, meaning that there are no sub-topics and each local topic k1 is associated with one topic embedding vector wk1 . Consequently, in WEDTM, there are three latent variables capturing the weights between the words and local topic k1: eF >wk1 (F ∈ RL×V is\nthe embeddings of all the words), βk1 , and φ (1) k1\n, each of which is a vector over words. It is interesting to analyze the connections and differences of them. eF >wk1 is the prior of βk1 , while βk1 is the prior of φ (1) k1 . So eF >wk1 is the closest one to the word embeddings, i.e., the global semantic information, while φ(1)k1 is the closest one to the data, i.e., the local document context of the target corpus. Therefore, unlike conventional topic models with φ(1)k1 only, the three variables of WEDTM give three different views to the same topic, from global to local, respectively. We qualitatively show this interesting comparison in Section 5.4.\n4. The last but not least, word embeddings in WEDTM can be viewed to serve as the prior/complementary information to assist the learning of the whole model, which is important especially for sparse data."
  }, {
    "heading": "4. Inference",
    "text": "Unlike many other word embeddings topic models, the fully local conjugacy of WEDTM facilitates the derivation of an effective Gibbs sampling algorithm. As the sampling for the latent variables in the process of generating documents and modeling inter topic structure are similar to GBN, the details can be found in Zhou et al. (2016). Here we focus on the sampling of the latent variables for modeling intra topic structure.\nAssume that sampled by Eq. (28) in Appendix B of Zhou et al. (2016), the latent count for the bottom-layer local topics are x(1)vjk1 , which counts how many words v in document j are allocated with local topic k1.\nSample β<s>vk1 . We first sample: ( h<1>vk1 , · · · , h <S> vk1 ) ∼ Mult ( hvk1 ,\nβ<1>vk1 βvk1 , · · · , β<S>vk1 βvk1\n) ,\n(1)\nwhere hvk1 ∼ CRT ( x (1) v·k1 , βvk1 ) (Zhou & Carin, 2015;\nZhao et al., 2017a), and x(1)v·k1 := ∑ j x (1) vjk1 3. Then:\nβ<s>vk1 ∼ Gam(α<s>k1 + h <s> vk1 , 1)\ne−π <s> vk1 + log 1qk1\n, (2)\nwhere qk1 ∼ Beta(β·k1 , x (1) ··k1) (Zhao et al., 2018b) and we define π<s>vk1 := f > v w <s> k1 .\n3We hereafter use · of a dimension to denote the sum over that dimension.\nSample α<s>k . We first sample g <s> vk1 ∼ CRT ( h<s>vk1 , α <s> k1 ) , then:\nα<s>k1 ∼ Gam(α<s>0 /S + g <s> ·k1 , 1) c<s>0 + log ( 1 + eπ\n<s> vk1 log 1qk1 ) . (3) It is noteworthy that the hierarchical construction onα<s>k1 is closely related to the gamma-negative binomial process and can be considered as a (truncated) gamma process (Zhou & Carin, 2015; Zhou, 2016) with an intrinsic shrinkage mechanism on S. It means that the model is able to automatically learn the number of effective sub-topics.\nSample w<s>k1 .\nw<s>k1 ∼ N (µ <s> k1 ,Σ<s>k1 ),\nµ<s>k1 =\nΣ<s>k1 [ V∑ v ( h<s>vk1 − α <s> k1 2 − ω<s>vk1 log log 1 qk1 ) fv ] ,\nΣ<s>k1 =\n[ diag(1/σ<s>) +\nV∑ v ω<s>vk1 fv(fv) >\n]−1 ,(4)\nwhere ω<s>vk1 ∼ PG ( h<s>vk1 + α <s> k1 , π<s>vk1 + log log 1 qk1 ) and PG denotes the Pólya gamma distribution (Polson et al., 2013). To sample from PG, we use an accurate and efficient approximate sampler in Zhou (2016).\nOmitted derivations, details, and the overall algorithm are in the supplementary materials."
  }, {
    "heading": "5. Experiments",
    "text": "We evaluate the proposed WEDTM by comparing it with several recent advances including deep topic models and word embedding topic models. The experiments were conducted on four real-world datasets including both regular and sparse texts. We report perplexity, document classification accuracy, and topic coherence scores. We also qualitatively analyze the topic hierarchies and sub-topics."
  }, {
    "heading": "5.1. Experimental Settings",
    "text": "In the experiments, we used a regular text dataset (20NG) and three sparse text datasets (WS, TMN, Twitter), the details of which are as follows: 1. 20NG, 20 Newsgroup, consists of 18,774 articles with 20 categories. Following Zhou et al. (2016), we used the 2000 most frequent terms after removing stopwords. The average document length is 76. 2. WS, Web Snippets, contains 12,237 web search snippets with 8 categories, used by Li et al. (2016); Zhao et al. (2017c;b). The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average. 3. TMN, Tag My\n(a) WS\n50 100 200 -200\n-150\n-100\n-50\n0\n50\nWEDTM-1 WEDTM-2 WEDTM-3 GBN-1 GBN-2 GBN-3\nMetaLDA WEI-FTM\nGBN-1: 704, 595, 548\n(b) TMN\n50 100 200 -250\n-200\n-150\n-100\n-50\n0\n50\nGBN-1: 1498, 1299, 1190\n(c) Twitter\n50 100 200 -50\n-40\n-30\n-20\n-10\n0\n10\nGBN-1: 461, 349, 305\n(d) 20NG\n100 200 400 -20\n-10\n0\n10\n20\nGBN-1: 631, 522, 479\n(e) WS\n20% 40% 60% 80% -600\n-500\n-400\n-300\n-200\n-100\n0\n100\nWEDTM-1 WEDTM-2 WEDTM-3 GBN-1 GBN-2 GBN-3\nMetaLDA WEI-FTM\nGBN-1: 1786, 994, 728, 595\n(f) TMN\n20% 40% 60% 80% -800\n-600\n-400\n-200\n0\n200\nGBN-1: 3140, 1925, 1495, 1299\n(g) Twitter\n20% 40% 60% 80% -150\n-100\n-50\n0\n50\nGBN-1: 699, 446, 382, 349\n(h) 20NG\n20% 40% 60% 80% -25\n-20\n-15\n-10\n-5\n0\n5\nGBN-1: 704, 595, 549, 522\nFigure 1. (a)-(d): Relative per-heldout-word perplexity6 (the lower the better) with the varied K1 and fixed proportion (80%) of training words of each document. (e)-(h): Relative per-heldout-word perplexity6 with the varied proportion of training words of each document and fixed K1 (100 on WS, TMN, and Twitter; 200 for 20NG). The error bars indicate the standard deviations of 5 random trials. The number attached to WEDTM and GBN indicates the number of layers (i.e., T ) used.\nNews, consists of 32,597 RSS news snippets from Tag My News with 7 categories, used by Nguyen et al. (2015); Zhao et al. (2017c;b). Each snippet contains a title and a short description. There are 13,370 tokens in the vocabulary and the average length of a snippet is 18. 4. Twitter, was extracted in 2011 and 2012 microblog tracks at Text REtrieval Conference (TREC)4 and preprocessed in Yin & Wang (2014). It has 11,109 tweets in total. The vocabulary size is 6,344 and a tweet contains 21 words on average.\nWe compared WEDTM with: 1. GBN (Zhou et al., 2016), the state-of-the-art deep topic model. 2. MetaLDA (Zhao et al., 2017c; 2018a), the state-of-the-art topic model with binary meta information about document and/or word. Word embeddings need to be binarized before used in the model. 3. WEI-FTM (Zhao et al., 2017b), the state-of-the-art focused topic model that incorporates real-valued word embeddings.\nIt is noteworthy that GBN was reported (Cong et al., 2017) to have better performance than other deep (hierarchical) topic models such as nHDP (Paisley et al., 2015), DPFA (Gan et al., 2015), and DPFM (Henao et al., 2015). MetaLDA and WEI-FTM were reported to perform better than other word embedding topic models including WFLDA (Petterson et al., 2010) and GPUDMM (Li et al., 2016) as well as short text topic models like PTM (Zuo et al.,\n4http://trec.nist.gov/data/microblog.html\n2016). Therefore, we considered the three above competitors to WEDTM.\nOriginally MetaLDA (when no document meta information is provided) and WEI-FTM follow the LDA framework, where the topic distribution for document j is θj ∼ Dir(α01) and α0 is a hyperparameter (usually set to 0.1). For a fair comparison, we replaced this part with the PFA framework with the gamma-negative binomial process (Zhou & Carin, 2015), which is equivalent to GBN when T = 1 and closely related to the hierarchical Dirichlet Process LDA (HDPLDA) (Teh et al., 2012).\nFor all the models, we used 50-dimensional GloVe word embeddings pre-trained on Wikipedia5. Except for MetaLDA, where we followed the paper to binarise the word embeddings, the other three models used the original realvalued embeddings. The hyperparameter settings we used for WEDTM and GBN are a0 = b0 = 0.01, e0 = f0 = 1.0, η0 = 0.05. For MetaLDA and WEI-FTM, we collected 1000 MCMC samples after 1000 burnins; for GBN and WEDTM, we collected 1000 for T = 1 and 500 for T > 1 MCMC samples after 1000 for T = 1 and 500 for T > 1 burnins, to estimate the posterior mean. Due to the shrinkage effect of WEDTM on S, discussed in Section 4, we set S = 5 which is large enough for all the topics.\n5https://nlp.stanford.edu/projects/glove/"
  }, {
    "heading": "5.2. Perplexity",
    "text": "Perplexity is a measure that is widely used (Wallach et al., 2009) to evaluate the modeling accuracy of topic models. Here we randomly chose a certain proportion of the word tokens in each document as training and used the remaining ones to calculate per-heldout-word perplexity. Figure 1 shows the relative perplexity6 results of all the models on all the datasets, where we varied the number of bottomlayer topics as well as the proportion of training words. The proposed WEDTM performs significantly better than the others, especially on sparse data. There are several interesting remarks of the results: (1) The perplexity advantage of WEDTM over GBN becomes obvious when the corpus becomes sparse (e.g., WS/TMN/Twitter V.S. 20NG and 20% V.S. 80% training words). It shows that using word embeddings as the prior information benefits the model. (2) In general, increasing the depth of the model leads to better perplexity. However, when the data are too sparse (e.g. WS with 20% training words), the single-layer WEDTM and GBN perform better than their multi-layer counterparts. (3) Although MetaLDA and WEI-FTM leverage word embed-\n6We subtracted the score of GBN with only one layer (GBN-1) from the score of each model. The lines plot the differences. So GBN-1 is the horizontal line on “0”. The absolute score of GBN-1 is given below each figure.\ndings as well, the proposed WEDTM outperforms them significantly. Perhaps the way that WEDTM incorporates word embeddings is more effective."
  }, {
    "heading": "5.3. Document Classification",
    "text": "We consider the multi-class classification task for predicting the categories for test documents to evaluate the quality of the latent document representation (unnormalized topic weights) extracted by these models.7 In this experiment, following Zhou et al. (2016), we ran the topic models on the training documents and trained a L2 regularized logistic regression using the LIBLINEAR package (Fan et al., 2008) with the latent representation θ(1)j as features. After training, we used the trained topic models to extract the latent representations of the test documents and the trained logistic regression to predict the categories. For all the datasets, we randomly selected 80% documents for training and used the remaining 20% for testing. Figure 2 shows the relative document classification accuracy6 results for all the models. It can be observed that with word embeddings, WEDTM outperforms GBN significantly, the best on TMN and 20NG, and the second-best on WS. Again, we see a similar phe-\n7The results of Twitter are not reported because each document of it is associated with multiple categories.\nnomenon: word embeddings help more on the sparser data and increasing the network depth improves the accuracy."
  }, {
    "heading": "5.4. Topic Coherence",
    "text": "Topic coherence is another popular evaluation of topic models (Zuo et al., 2016; Zhao et al., 2017b;b). It measures the semantic coherence in the most significant words (top words) in a topic. Here we used the Normalized Pointwise Mutual Information (NPMI) (Aletras & Stevenson, 2013; Lau et al., 2014) to calculate topic coherence score of the top 10 words of each topic and report the average score of all the topics.8\nTo compare with the other models, in this experiment, we set S = 1 for WEDTM. Recall that in WEDTM, from global to local, there are three ways to interpret a topic. Here we evaluate NPMI for two of them: eF\n>wk1 and φ(1)k1 . Figure 3 shows the NPMI scores for all the models on WS, TMN, and Twitter. It is not surprising to see that the top words generated by eF\n>wk1 in WEDTM always gain the highest NPMI scores, meaning that the topics are more coherent. This is because the topic embeddings in WEDTM directly interact with word embeddings. Moreover, if we just compare the topics generated by φ(1)k1 , WEDTM also gives more coherent topics than the other models. This demonstrates that the proposed model is able to discover more interpretable topics."
  }, {
    "heading": "5.5. Qualitative Analysis",
    "text": "As one of the most appealing properties of WEDTM is its interpretability, we conducted the extensive qualitative evaluation of the quality of the topics discovered by WEDTM, including topic embeddings, sub-topics, and topic hierarchies. More qualitative analysis including topic hierarchy visualization and synthetic document generation is shown in the supplementary materials.\nDemonstration of topic embeddings: We demonstrate that WEDTM discovers more coherent topics by comparing with those of GBN in Table 2. Here we set S = 1 as well. This demonstration further explains the numerical results in Figure 3. It is also interesting to compare the local interpretation (φ(1)k ) and global interpretation (topic embeddings) of the same topic in WEDTM. For example, in the fifth set, the local interpretation (5.b) is about “networks and security,” while the global interpretation (5.c) generalizes it with more general words related to “communications.” We can also observe that although the local interpretation of WEDTM is not as close to word embeddings as the global interpretation, as informed by the global interpretation, the\n8We used the Palmetto package with a large Wikipedia dump to compute NPMI (http://palmetto.aksw.org).\nlocal interpretation of WEDTM’s topics is still considerably more coherent than those in GBN.\nDemonstration of sub-topics: In Figure 4, We show the sub-topics discovered by WEDTM for the topics used as examples at the beginning of the paper (Table 1). It can be observed that the intra topic structures with sub-topics clearly help to explain the local topics. For example, WEDTM successfully splits Topic 1 into sub-topics related to “journal” and “biology,” and Topic 2 into “music” and “sports”. Moreover, with the help of word embeddings, WEDTM discovers general sub-topics for specific topics. For example, Topic 3 and 4 are more interpretable with the sub-topics of “singer” and “game” respectively. The experiment also empirically demonstrates the shrinkage mechanism of the model: for most topics, the effective sub-topics are less than the maximum number S = 5.\nDemonstration of topic hierarchies: Figure 5 shows an example that jointly demonstrates the inter and intra structures of WEDTM. The tree is a cluster of topics related to “health,” where the topic hierarchies are discovered by ranking {Φ(t)}t, the leaf nodes are the topics in the bottom layer, and each bottom-layer topic is associated with a set of sub-topics. In WEDTM, the inter topic structures are revealed in the form of topic hierarchies while the intra topic structures are revealed in the form of sub-topics. Combining the two kinds of topic structures in this way gives a better view of the target corpus, which may further benefit other text analysis tasks."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we have proposed WEDTM, a deep topic model that leverages word embeddings to discover inter topic structures with topic hierarchies and intra topic structures with sub-topics. Moreover, with the introduction to sub-topic embeddings, each sub-topic can be informed by the global information in word embeddings, so as to discover a fine-grained thematic aspect of a local topic. With topic embeddings, WEDTM provides different views to a topic, from global to local, which further improves the interpretability of the model. As a fully conjugate model, the inference of WEDTM can be done by a straightforward Gibbs sampling algorithm. Extensive experiments have shown that WEDTM achieves the state-of-the-art performance on perplexity, document classification, and topic quality. In addition, with topic hierarchies, sub-topics, and topic embeddings, the model can discover more interpretable structured topics, which helps to get better understandings of text data. Given the local conjugacy, it is possible to derive more scalable inference algorithms for WEDTM, such as stochastic variational inference and stochastic gradient MCMC, which is a good subject for future work."
  }],
  "year": 2018,
  "references": [{
    "title": "Nested Chinese restaurant franchise process: Applications to user tracking and document modeling",
    "authors": ["A. Ahmed", "L. Hong", "A. Smola"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Evaluating topic coherence using distributional semantics",
    "authors": ["N. Aletras", "M. Stevenson"],
    "venue": "In Proc. of the 10th International Conference on Computational Semantics,",
    "year": 2013
  }, {
    "title": "The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies",
    "authors": ["D. Blei", "T. Griffiths", "M. Jordan"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2010
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"],
    "venue": "TACL,",
    "year": 2017
  }, {
    "title": "Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC",
    "authors": ["Y. Cong", "B. Chen", "H. Liu", "M. Zhou"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Gaussian LDA for topic models with word embeddings",
    "authors": ["R. Das", "M. Zaheer", "C. Dyer"],
    "venue": "In ACL, pp",
    "year": 2015
  }, {
    "title": "LIBLINEAR: A library for large linear classification",
    "authors": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"],
    "venue": "JMLR, 9:1871–1874,",
    "year": 2008
  }, {
    "title": "Scalable deep Poisson factor analysis for topic modeling",
    "authors": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Deep Poisson factor modeling",
    "authors": ["R. Henao", "Z. Gan", "J. Lu", "L. Carin"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Modeling topic hierarchies with the recursive Chinese restaurant process",
    "authors": ["J.H. Kim", "D. Kim", "S. Kim", "A. Oh"],
    "venue": "In CIKM,",
    "year": 2012
  }, {
    "title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
    "authors": ["J.H. Lau", "D. Newman", "T. Baldwin"],
    "venue": "In EACL,",
    "year": 2014
  }, {
    "title": "Topic modeling for short texts with auxiliary word embeddings",
    "authors": ["C. Li", "H. Wang", "Z. Zhang", "A. Sun", "Z. Ma"],
    "venue": "In SIGIR,",
    "year": 2016
  }, {
    "title": "Pachinko allocation: DAGstructured mixture models of topic correlations",
    "authors": ["W. Li", "A. McCallum"],
    "venue": "In ICML, pp",
    "year": 2006
  }, {
    "title": "Distributed representations of words and phrases and their compositionally",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Improving topic models with latent feature word representations",
    "authors": ["D.Q. Nguyen", "R. Billingsley", "L. Du", "M. Johnson"],
    "venue": "TACL, 3:299–313,",
    "year": 2015
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C. Manning"],
    "venue": "In EMNLP,",
    "year": 2014
  }, {
    "title": "Word features for Latent Dirichlet Allocation",
    "authors": ["J. Petterson", "W. Buntine", "S.M. Narayanamurthy", "T.S. Caetano", "A.J. Smola"],
    "venue": "In NIPS,",
    "year": 1921
  }, {
    "title": "Bayesian inference for logistic models using Pólya–Gamma latent variables",
    "authors": ["N. Polson", "J. Scott", "J. Windle"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2013
  }, {
    "title": "Hierarchical Dirichlet processes",
    "authors": ["Y.W. Teh", "M. Jordan", "M. Beal", "D. Blei"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2012
  }, {
    "title": "Rethinking LDA: Why priors matter",
    "authors": ["H.M. Wallach", "D.M. Mimno", "A. McCallum"],
    "venue": "In NIPS, pp. 1973–1981,",
    "year": 2009
  }, {
    "title": "A correlated topic model using word embeddings",
    "authors": ["G. Xun", "Y. Li", "W.X. Zhao", "J. Gao", "A. Zhang"],
    "venue": "In IJCAI,",
    "year": 2017
  }, {
    "title": "A Dirichlet multinomial mixture modelbased approach for short text clustering",
    "authors": ["J. Yin", "J. Wang"],
    "venue": "In SIGKDD,",
    "year": 2014
  }, {
    "title": "Leveraging node attributes for incomplete relational data",
    "authors": ["H. Zhao", "L. Du", "W. Buntine"],
    "venue": "In ICML, pp",
    "year": 2017
  }, {
    "title": "A word embeddings informed focused topic model",
    "authors": ["H. Zhao", "L. Du", "W. Buntine"],
    "venue": "In ACML, pp",
    "year": 2017
  }, {
    "title": "MetaLDA: A topic model that efficiently incorporates meta information",
    "authors": ["H. Zhao", "L. Du", "W. Buntine", "G. Liu"],
    "venue": "In ICDM,",
    "year": 2017
  }, {
    "title": "Leveraging external information in topic modelling",
    "authors": ["H. Zhao", "L. Du", "W. Buntine", "G. Liu"],
    "venue": "Knowledge and Information Systems,",
    "year": 2018
  }, {
    "title": "Bayesian multilabel learning with sparse features and labels, and label co-occurrences",
    "authors": ["H. Zhao", "P. Rai", "L. Du", "W. Buntine"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "Softplus regressions and convex polytopes",
    "authors": ["M. Zhou"],
    "venue": "arXiv preprint arXiv:1608.06383,",
    "year": 2016
  }, {
    "title": "Nonparametric Bayesian negative binomial factor analysis",
    "authors": ["M. Zhou"],
    "venue": "Bayesian Analysis,",
    "year": 2018
  }, {
    "title": "Negative binomial process count and mixture modeling",
    "authors": ["M. Zhou", "L. Carin"],
    "venue": "TPAMI, 37(2):307–320,",
    "year": 2015
  }, {
    "title": "Betanegative binomial process and Poisson factor analysis",
    "authors": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"],
    "venue": "In AISTATS,",
    "year": 2012
  }, {
    "title": "Topic modeling of short texts: A pseudodocument view",
    "authors": ["Y. Zuo", "J. Wu", "H. Zhang", "H. Lin", "F. Wang", "K. Xu", "H. Xiong"],
    "venue": "In SIGKDD,",
    "year": 2016
  }],
  "id": "SP:060a7ecf67ae5febedb1c22e29b9f8a880cf4f23",
  "authors": [{
    "name": "He Zhao",
    "affiliations": []
  }, {
    "name": "Lan Du",
    "affiliations": []
  }, {
    "name": "Wray Buntine",
    "affiliations": []
  }, {
    "name": "Mingyaun Zhou",
    "affiliations": []
  }],
  "abstractText": "One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.",
  "title": "Inter and Intra Topic Structure Learning with Word Embeddings"
}