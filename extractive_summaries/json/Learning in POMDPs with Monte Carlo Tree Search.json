{
  "sections": [{
    "heading": "1 Introduction",
    "text": "The Partially Observable Markov Decision Processes (POMDP) [8] is a general model for sequential decision making in stochastic and partially observable environments, which are ubiquitous in real-world problems. A key shortcoming of POMDP methods is the assumption that the dynamics of the environment are known a priori. In real-world applications, however, it may be impossible to obtain a complete and accurate description of the system. Instead, we may have uncertain prior knowledge about the model. When lacking a model, a prior can be incorporated into the POMDP problem in a principled way, as demonstrated by the Bayes-Adaptive POMDP framework [13].\nThe BA-POMDP framework provides a Bayesian approach to decision making by maintaining a probability distribution over possible models as the agent acts in an online reinforcement learning setting [5, 19]. This method casts the Bayesian reinforcement learning problem into a POMDP planning problem where the hidden model of the environment is part of the state space. Unfortunately, this planning problem becomes very large, with a countably infinite state space over all possible models, and as such, current solution methods are not scalable or perform poorly [13].\nOnline and sample-based planning has shown promising performance on non-trivial POMDP problems [12]. Online methods reduce the complexity by considering the relevant (i.e., reachable) states only, and sample-based approaches tackle the complexity issues through approximations in the form of simulated interactions with the environment. Here we modify one of those methods, Partial Observable Monte-Carlo Planning (POMCP) [15] and extend it to the Bayes-Adaptive case, leading to a novel approach: BA-POMCP.\nIn particular, we improve the sampling approach by exploiting the structure of the BA-POMDP resulting in root sampling and expected models methods. We also present an approach for more efficient model representation, which we call linking states. Lastly, we prove the correctness of our improvements, showing that they converge to the true BA-POMDP solution. As a result, we present methods that significantly improve the scalability of learning in BAPOMDPs, making them practical for larger problems."
  }, {
    "heading": "2 Background",
    "text": "First, we discuss POMDPs and BA-POMDPs in respectively Section 2.1 and 2.2."
  }, {
    "heading": "2.1 POMDPs",
    "text": "Formally, a POMDP is described by a tuple (S, A, Z, D, R, γ, h), where S is the set of states of the environment; A is the set of actions; Z is the set of observations; D is the ‘dynamics function’ that describes the dynamics of the ∗This is an extended version of a paper that was published at ICML’2017.\nar X\niv :1\n80 6.\n05 63\n1v 1\n[ cs\n.A I]\n1 4\nJu n\n20 18\nsystem in the form of transition probabilities D(s′,z|s,a);1 R is the immediate reward function R(s, a) that describes the reward of selecting a in s; γ ∈ [0,1) is the discount factor; and h is the horizon of an episode in the system.\nThe goal of the agent in a POMDP is to maximize the expected cumulative (discounted) reward, also called the expected return. The agent has no direct access to the system’s state, so it can only rely on the action-observation history ht = 〈a0,z1, . . . ,at−1,zt〉 up to the current step t. It can use this history to maintain a probability distribution over the state, also called a belief, b(s). A solution to a POMDP is then a mapping from a belief b to an action a, which is called a policy π. Solution methods aim to find an optimal policy, a mapping from a belief to an action with the highest possible expected return.\nThe agent maintains its belief during execution through belief updates. A belief update calculates the posterior probability of the state s′ given the previous belief over s and action-observation pair 〈a,z〉: b′(s) = P (s′|b(s), a, z). This operation is infeasible for large spaces because it enumerates over the entire state space. A common approximation method is to represent the belief with a (unweighted) particle filter [17]. A particle filter is a collection of K particles (states). Each particle represents a probability of 1K ; if a specific state x occurs n times in a particle filter, then P (x) = nK . The precision of the filter is determined by the number of particles K. To update such a belief after execution of action a and observation z, a standard approach is to utilize rejection sampling: the agent repeatedly samples a state s from its belief, then simulates the execution of a on s through D, and receives a (simulated) new state s′sim and observation zsim. s\n′ is added to the new belief only when zsim equals z, and rejected otherwise. This process repeats until the new belief contains K particles.\nPartially Observable Monte-Carlo Planning (POMCP) [15], is a scalable method which extends Monte Carlo tree search (MCTS) to solve POMDPs. POMCP is one of the leading algorithms for solving general POMDPs. At each time step, the algorithm performs online planning by incrementally building a lookahead tree that contains Q(h,a), where h is the action-observation history-path to reach that node. It samples hidden states s at the root node (called ‘root sampling’) and uses that state to sample a trajectory that first traverses the lookahead tree and then performs a (random) rollout. The return of this trajectory is used to update the statistics for all visited nodes. These statistics include the number of times an action has been taken at a history (N(h,a)) and estimated value of being in that node (Q(h,a)), based on an average over the returns.\nBecause this lookahead tree can be very large, the search is directed to the relevant parts by selecting the actions inside the tree that maximize the ‘upper confidence bounds’ [2]: U(h,a) = Q(h, a) + c √ log(N(h) + 1)/N(h,a). Here, N(h) is the number of times the history has been visited. At the end of each simulation, the discounted accumulated return is used to update the estimated value of all the nodes in the tree that have been visited during that simulation. POMCP terminates after some criteria has been met, typically defined by a maximum number of simulations or allocated time. The agent then picks the action with the highest estimated value (maxaQ(b,a)). POMCP can be shown to converge to an -optimal value function. Moreover, the method has demonstrated good performance in large domains with a limited number of simulations. The extension of POMCP that is used in this work is discussed in Section 3."
  }, {
    "heading": "2.2 BA-POMDPs",
    "text": "Most research concerning POMDPs has considered the task of planning: given a full specification of the model, determine an optimal policy (e.g., [8, 14]). However, in many real-world applications, the model is not (perfectly) known in advance, which means that the agent has to learn about its environment during execution. This is the task considered in reinforcement learning (RL) [16].\nA fundamental RL problem is the difficulty of deciding whether to select actions in order to learn a better model of the environment, or to exploit the current knowledge about the rewards and effects of actions. In recent years, Bayesian RL methods have become popular because they can provide a principled solution to this exploration/exploitation tradeoff [19, 5, 6, 10, 18].\nIn particular, we consider the framework of Bayes-Adaptive POMDPs [11, 13]. BA-POMDPs use Dirichlet distributions to model uncertainty over transitions and observations2 (typically assuming the reward function is chosen by the designer and is known). In particular, if the agent could observe both states and observations, it could maintain a vector χ with the counts of the occurrences for all 〈s, a, s′, z〉 tuples. We write χs′zsa for the number of times that 〈s,a〉 is followed by 〈s′,z〉.\nWhile the agent cannot observe the states and has uncertainty about the actual count vector, this uncertainty can be represented using regular POMDP formalisms. That is, the count vector is included as part of the hidden state of a specific POMDP, called a BA-POMDP. Formally, a BA-POMDP is a tuple 〈S̄, A, D̄, R̄, Z, γ, h〉 with some modified\n1 This formulation allows for easier notation and generalizes the typical formulation with separate transition T and observation functions O: D = 〈T,O〉. In our experiments, we do employ this typical factorization.\n2 [11, 13] follow the standard T & O POMDP representations, but we use our combined D formalism.\nAlgorithm 1 BA-POMCP(b̄,num sims)\n1: //b̄ is an augmented belief (e.g., particle filter) 2: h0 ← () . The empty history (i.e., now) 3: for i← 1 . . . num sims do 4: //First, we root sample an (augmented) state: 5: s̄← SAMPLE(b̄) . reference to a particle 6: s̄′ ← COPY(s̄) 7: SIMULATE(s̄′, 0, h0) 8: end for 9: a← GREEDYACTIONSELECTION(h0)\n10: return a\ncomponents in comparison to the POMDP. While the observation and action space remain unchanged, the state (space) of the BA-POMDP now includes Dirichlet parameters: s̄ = 〈s, χ〉, which we will refer to as augmented states. The reward model remains the same, since it is assumed to be known, R̄(〈s′,χ′),a) = R(s,a). The dynamics functions, D̄, however, is described in terms of the counts in s̄, and is defined as follows\nDχ(s ′,z|s, a) , E[D(s′,z|s, a)|χ] = χ s′z sa∑\ns′z χ s′z sa\n. (1)\nThese expectations can now be used to define the transitions for the BA-POMDP. If we let δs ′z sa denote a vector of the length of χ containing all zeros except for the position corresponding to 〈s,a,s′,z〉 (where it has a one), and if we let Ia,b denote the Kronecker delta that indicates (is 1 when) a = b, then we can define D̄ as D̄(s′,χ′,z|s,χ, a) = Dχ(s\n′,z|s, a)Iχ′,χ+δs′zsa . Remember that these counts are not observed by the agent, since that would require observations of the state. The agent can only maintain belief over these count vectors. Still, when interacting with the environment, the ratio of the true—but unknown—count vectors will converge to coincide with the true transition and observation probabilities in expectation. It is important to realize, however, that this convergence of count vector ratios does not directly imply learnability by the agent: even though the ratio of the count vectors of the true hidden state will converge, the agent’s belief over count vectors might not.\nBA-POMDPs are infinite state POMDP models and thus extremely difficult to solve. Ross et al. [13] introduced a technique to convert such models to finite models, but these are still very large. Therefore, Ross et al. propose a simple lookahead planner to solve BA-POMDPs in an online manner. This approach approximates the expected values associated with each action at the belief by applying a lookahead search of depth d. This method will function as the comparison baseline in our experiments, as no other BA-POMDP solution methods have been proposed."
  }, {
    "heading": "3 BA-POMDPs via Sample-based Planning",
    "text": "Powerful methods, such as POMCP [15], have significantly improved the scalability of POMDP solution methods. At the same time the most practical solution method for BA-POMDPs, the aforementioned lookahead algorithm, is quite limited in dealing with larger problems. POMDP methods have rarely been applied to BA-POMDPs [1], and no systematic investigation of their performance has been conducted. In this paper, we aim to address this void, by extending POMCP to BA-POMDPs, in an algorithm that we refer to as BA-POMCP. Moreover, we propose a number of novel adaptations to BA-POMCP that exploit the structure of the BA-POMDP. In this section, we first lay out the basic adaptation of POMCP to BA-POMDPs and then describe the proposed modifications that improve its efficiency.\nBA-POMCP BA-POMCP, just like POMCP, constructs a lookahead tree through simulated experiences (Algorithm 1). In BA-POMDPs, however, the dynamics of the system are inaccessible during simulations, and the belief is a probability distribution over augmented states. BA-POMCP, as a result, must sample augmented states from the belief b̄, and use copies of those states (s̄ = 〈s,χ〉) for each simulation (Algorithm 2). We will refer to this as root sampling of the state (line 6). The copy is necessary, as otherwise the STEP function in Algorithm 2 would alter the belief b̄. It is also expensive, for χ grows with the state, action and observation space, to |S|2 × |A| × |Ω| parameters. 3 In practice, this operation becomes a bottleneck to the runtime of BA-POMCP in larger domains.\n3It is |S|2 × |A|+ |S| × |A| × |Ω| when assuming D is factored in T & O\nAlgorithm 2 SIMULATE(s̄, d, h) 1: if ISTERMINAL(h) || d == max depth then 2: return 0 3: end if 4: //Action selection uses statistics stored at node h: 5: a← UCBACTIONSELECTION(h) 6: R← R(s̄,a) 7: z ← STEP(s̄, a) //modifies s̄ to sampled next state 8: h′ ← (h,a,z) 9: if h′ ∈ Tree then 10: r ← R+ γ SIMULATE(s̄, d+ 1, h′) 11: else 12: CONSTRUCTNODE(h′) //Initializes statistics 13: r ← R+ γ ROLLOUT(s̄, d+ 1, h′) 14: end if 15: //Update statistics: 16: N(h,a)← N(h,a) + 1 17: Q(h,a)← N(h,a)−1\nN(h,a) Q(h,a) + 1 N(h,a) r\n18: return r\nAlgorithm 3 BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: Dsa ∼ χsa 2: 〈s′,z〉 ∼ Dsa 3: //In place updating of s̄ = 〈s, χ〉 4: χs ′z sa ← χs ′z sa + 1\n5: s← s′ 6: return z\nTo apply POMCP on BA-POMDPs, where the dynamics are unknown, we modify the STEP function, proposing several variants. The most straightforward one, BA-POMCP-STEP is employed in what we refer to as ‘BA-POMCP’. This method, shown in Algorithm 3, is similar to BA-MCP [7]: essentially, it samples a dynamic model Dsa which specifies probabilities Pr(s′,z|s,a) and subsequently samples an actual next state and observation from that distribution. Note that the underlying states and observations are all represented simply as an index, and hence the assignment on line 5 is not problematic. However, the cost of the model sampling operation in line 1 is.\nRoot Sampling of the Model BA-MCP [7] addresses the fully observable BRL problem by using POMCP on an augmented state s̄ = 〈s, T 〉, consisting of the observable state, as well as the hidden true transition function T . Application of POMCP’s root sampling of state in this case leads to ‘root sampling of a transition function’: Since the true transition model T does not change during the simulation, one is sampled at the root and used during the entire simulation. In the BA-POMCP case, root sampling of a state s̄ = 〈s, χ〉 does not lead to a same interpretation: no model, but counts are root sampled and they do change over time.\nWe use this as inspiration to introduce a similar, but clearly different, (since this is not root sampling of state) technique called root sampling of the model (which we will refer to as just ‘root sampling’). The idea is simple: every time we root sample a state s̄ = 〈s, χ〉 ∼ b̄ at the beginning of a simulation (line 5 in Algorithm 1), we directly sample a Ḋ ∼ Dir(χ), which we will refer to as the root-sampled model Ḋ and it is used for the rest of the simulation.\nWe denote this root sampling in BA-POMCP as ‘R-BA-POMCP’. The approach is formalized by R-BA-POMCPSTEP (Algorithm 4). Note that no count updates take place (cf. line 4 in Algorithm 3). This highlights an important advantage of this technique: since the counts are not used in the remainder of the simulation, the copy of counts (as part of line 6 of Algorithm 1) can be avoided altogether. Since this copy operation is costly, especially in larger domains, where the number of states, action and observations and the number of counts is large, this can lead to significant savings. Finally, we point out that, similar to what Guez et al. [7] propose, Ḋ can be constructed lazily: the part of the model Ḋ is only sampled when it becomes necessary.\nThe transition probabilities during R-BA-POMCP differ from those in BA-POMCP, and it is not obvious that a policy based on R-BA-POMCP maintains the same guarantees. We prove in Section 4 that the solution of R-BAPOMCP in the limit converges to that of BA-POMCP.\nAlgorithm 4 R-BA-POMCP-STEP (s̄ = 〈s, χ〉, a) 1: //Sample from the root sampled model 2: s′,z ∼ Ḋs,a 3: s← s′ 4: return z\nAlgorithm 5 E-BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: //Sample from the Expected model 2: s′,z ∼ Dχ(·, ·|s,a) 3: χs ′z sa ← χs ′z sa + 1\n4: s← s′ 5: return z\nExpected models during simulations The second, complementary, adaptation modifies the way models are sampled from the root-sampled counts in STEP. This version samples the transitions from the expected dynamics Dχ given in (1), rather than from a sampled dynamics function D ∼ Dir(χ). The latter operation is relatively costly, while constructing Dχ is very cheap. In fact, this operation is so cheap, that it is more efficient to (re-)calculate it on the fly rather than to actually store Dχ. This approach is shown in Algorithm 5.\nLinking States Lastly, we propose a specialized data structure to encode the augmented BA-POMDP states. The structure aims to optimize for the complexity of the count-copy operation in line 6 of Algorithm 1 while allowing modifications to s̄. The linking state sl is a tuple of a system state, a pointer (or link) to an unmodifiable set of counts χ and a set of updated counts 〈s, l, δ〉. l is a pointer to some set of counts χ, which remain unchanged during count updates (such as in the STEP function), and instead are stored in the set of updated counts, δ, as shown in Algorithm 6. The consequence is that the linking state copy-operation can safely perform a shallow copy of the counts χ, and must only consider δ, which is assumed to be much smaller.\nLinking states can be used during the (rejection-sample-based) belief update at the beginning of each real time step. While the root-sampled augmented states (including δ in linking states) are typically deleted at the end of each simulation during L-BA-POMCP, each belief update potentially increases the size of δ of each particle. Theoretically, the number of updated counts represented in δ increases and the size of δ may (eventually) grow similar to the size of χ. Therefore, at some point, it is necessary to construct a new χ′ that combines χ and δ (after which δ can be safely emptied). We define a new parameter for the maximum size of δ, λ, and condition to merge only if the size of δ exceeds λ. We noticed that, in practice, the number of merges is much smaller than the amount of copies in BA-POMCP. We also observed in our experiments that it is often the case that a specific (small) set of transitions are notably more popular than others and that δ grows quite slowly."
  }, {
    "heading": "4 Theoretical Analysis",
    "text": "Here, we analyze the proposed root sampling of the dynamics function and expected transition techniques, and demonstrate they converge to the solution of the BA-POMDP. These main steps of this proof are similar to those in [15]. We point out however, that the technicalities of proving the components are far more involved.\nThe convergence guarantees of the original POMCP method are based on showing that, for an arbitrary rollout policy π, the POMDP rollout distribution (the distribution over full histories when performing root sampling of state) is equal to the derived MDP rollout distribution (the distribution over full histories when sampling in the belief MDP). Given that these are identical it is easy to see that the statistics maintained in the search tree will converge to the same number in expectation. As such, we will show a similar result here for expected transitions (‘expected’ for short) and root sampling of the dynamics function (‘root sampling’ below).\nWe define H0 as the full history (also including states) at the root of simulation, Hd as the full history of a node at depth d in the simulation tree, and χ(Hd) as the counts induced by Hd. We then define the rollout distributions:\nDefinition 1. The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π. It is given by\nPπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (2)\nwith Pπ(H0) = b0(〈s0,χ0〉) the belief ‘now’ (at the root of the online planning).\nAlgorithm 6 L-BA-POMCP-STEP(sl = 〈s, l, δ〉, a) 1: D ∼ 〈l, δ〉 2: s′,z ∼ Ds,a 3: s← s′ 4: δs ′z sa ← δs ′z sa + 1\n5: return z\nNote that there are two expectations in the above definition: ‘expected transitions’ mean that transitions for a history Hd are sampled from Dχ(Hd). The other ‘expected’ is the expectation of those samples (and it is easy to see that this will converge to the expected transition probabilities Dχ(Hd)(sd+1,zd+1|as,sd)). For root sampling of the dynamics model, this is less straightforward, and we give the definition in terms of the empirical distribution:\nDefinition 2. The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by\nP̃πK(Hd) , 1\nKd Kd∑ i=1 I{ Hd=H (i) d\n}, where\n• K is the number of simulations that comprise the empirical distribution, • Kd is the number of simulations that reach depth d (not all simulations might be equally long), • H(i)d is the history specified by the i-th particle at stage d.\nNow, our main theoretical result is that these distributions are the same in the limit of the number of simulations:\nTheorem 3. The full-history RS-BA-POMDP rollout distribution (Def. 2) converges in probability to the quantity of Def. 1:\n∀Hd P̃πKd(Hd) p→ Pπ(Hd). (3)\nProof. The proof is listed in appendix A.\nCorollary 4. Given suitably chosen exploration constant (e.g., c > Rmax1−γ ), BA-POMCP with root-sampling of dynamics function converges in probability to the expected transition solution.\nProof. Since Theorem 3 guarantees the distributions over histories are the same in the limit, they will converge to the same values maintained in the tree.\nFinally, we see that these are solutions for the BA-POMDP:\nCorollary 5. BA-POMCP with expected transitions sampling, as well as with root sampling of dynamics function converge to an -optimal value function of a BA-POMDP: V (〈s,χ〉 ,h) p→ V ∗ (〈s,χ〉 ,h), where =\nprecision 1−γ .\nProof. A BA-POMDP is a POMDP, so the analysis from Silver and Veness [15] applies to the BA-POMDP, which means that the stated guarantees hold for BA-POMCP. The BA-POMDP is stated in terms of expected transitions, so the theoretical guarantees extend to the expected transition BA-POMCP, which in turn via corollary 4 implies that the theoretical guarantees extend to RS-BA-POMCP.\nFinally, we note that linking states does not affect they way that sampling is performed at all:\nProposition 6. Linking states does not affect convergence of BA-POMCP."
  }, {
    "heading": "5 Empirical Evaluation",
    "text": "Experimental setup In this section, we evaluate our algorithms on a small toy problem, the well-known Tiger problem [3] and test scalability on a larger domain: the Partially Observable Sysadmin (POSysadmin) problem. In POSysadmin, the agent acts as a system administrator with the task of maintaining a network of n computers. Computers are either ‘working’ or ‘failing’, which can be deterministically resolved by ‘rebooting’ the computer. The agent does not know the state of any computer, but can ‘ping’ any individual computer. At each step, any of the computers can ‘fail’ with some probability f . This leads to a state space of size 2n, an action space of 2n+ 1, where the agent can ‘ping’ or ‘reboot’ any of the computers, or ‘do nothing’, and an observation space of 3 ({NULL, failing, working}). The ‘ping’ action has a cost of 1 associated with it, while rebooting a computer costs 20 and switches the computer to ‘working’. Lastly, each ‘failing’ computer has a cost of 10 at each time step.\nWe conducted an empirical evaluation with aimed for 3 goals: The first goal attempts to support the claims made in Section 4 and show that the adaptations to BA-POMCP do not decrease the quality of the resulting policies. Second, we investigate the runtime of those modifications to demonstrate their contribution to the efficiency of BA-POMCP. The last part contains experiments that directly compare the performance per action selection time with the baseline approach of Ross et al. [13]. For brevity, Table 1 describes the default parameters for the following experiments. It will be explicitly mentioned whenever different values are used.\nBA-POMCP variants Section 4 proves that the solutions of the proposed modifications (root-sampling (R-), expected models (E-) and linking states (L-)) in the limit converge to the solution of BA-POMCP. Here, we investigate the behaviour of these methods in practice. For the Tiger problem, the agent’s initial belief over the transition model is correct (i.e., counts that correspond to the true probabilities with high confidence), but it provides an uncertain belief that underestimates the reliability of the observations. Specifically, it assigns 5 counts to hearing the correct observation and 3 counts to incorrect: the agent initially beliefs it will hear correctly with a probability of 62.5%. The experiment is run for with 100, 1000 & 1000 simulations and all combinations of BA-POMCP adaptations.\nFigure 1a plots the average return over 10000 runs for a learning period of 100 episodes for Tiger. The key\nobservation here is two-fold. First, all methods improve over time through refining their knowledge about D. Second, there are three distinct clusters of lines, each grouped by the number of simulations. This shows that all 3 variants (R/L/E-BA-POMCP) lead to the same results.\nWe repeat this investigation with the (3-computer) POSysadmin problems, where we allow 100 simulations per time step. In this configuration, the network was fully connected with a failure probability f = 0.1. The (deterministic) observation function is assumed known a priori, but the prior over the transition function is noisy as follows: for each count c, we take the true probability of that transition (called p) and (randomly) either subtract or add .15. Note that we do not allow transitions with non-zero probability to fall below 0 by setting those counts to 0.001. Each Dirichlet distribution is then normalized the counts to sum to 20. With 3 computers, this results in |S| × |A| = 8× 7 = 56 noisy Dirichlet distributions of |S| = 8 parameters.\nFigure 1b shows how each method is able to increase its performance over time for POSysadmin. Again, the proposed modifications do not seem to alter the solution quality for a specific number of simulations.\nBA-POMCP scalability While the previous experiments indicate that the three adaptations produce equally good policies, they do not support any of the efficiency claims made in Section 3. Here, we compare the scalability of BA-POMCP on the POSysadmin problem. The proposed BA-POMCP variants are repeatedly run for 100 episodes on instances of POSysadmin of increasing network size (3 to 10 computers), and we measure the average action selection time required for 1000 simulations. Note that the experiments are capped to allow up to 5 seconds per action selection, demonstrating the problem size that a specific method can perform 1000 simulations in under 5 seconds.\nFigure 2 shows that BA-POMCP takes less than 0.5 seconds to perform 1000 simulations on an augmented state with approximately 150 parameters (3 computers), but is quickly unable to solve larger problems, as it requires more than 4 seconds to plan for a BA-POMDP with 200000 counts. BA-POMCP versions with a single adaptation are able to solve the same problems twice as fast, while combinations are able to solve much larger problems with up to 5 million parameters (10 computers). This implies not only that each individual adaptation is able to speed up BA-POMCP, but also that they complement one another.\nPerformance The previous experiments first show that the adaptations do not decrease the policy quality of BAPOMCP and second that the modified BA-POMCP methods improve scalability. Here we put those thoughts together and directly consider the performance relative to the action selection time. In these experiments we take the average return over multiple repeats of 100 episodes and plot them according to the time required to reach such performance. Here BA-POMCP is also directly compared to the baseline lookahead planner by Ross et al. [13].\nFirst, we apply lookahead with depth 1&2 on the Tiger problem under the same circumstance as the first experiment for increasing number of particles (25, 50, 100, 200 & 500), which determines the runtime. The resulting average episode return is plotted against the action selection time in Figure 3a.\nThe results show that most methods reach near optimal performance after 0.5 seconds action selection time. R-BAPOMCP and E-R-BA-POMCP perform worse than their counterparts BA-POMCP and E-BAPOMCP, which suggests that root sampling of the dynamics actually slows down BA-POMCP slightly. This phenomenon is due to the fact that the Tiger problem is so small, that the overhead of copying the augmented state and re-sampling of dynamics (during\n(a) The average return over 100 episodes per action selection time of on the Tiger problem\n(b) The average return over 100 episodes per action selection time of BA-POMCP on the POSysadmin problem\nSTEP function) that root sampling avoids is negligible and does overcome the additional complexity of root sampling. Also note that, even though the Tiger problem is so trivial that a lookahead of depth 1 suffices to solve the POMDP problem optimally, BA-POMCP still consistently outperforms this baseline.\nThe last experiment shows BA-POMCP and lookahead on the POSysadmin domain with 6 computers (which contains 55744 counts) with a failure rate of 0.05. The agent was provided with an accurate belief χ.4 The results are shown in Figure 3b.\nWe were unable to get lookahead search to solve this problem: the single instance which returned results in a reasonable amount of time (the single dot in the lower right corner) was with a lookahead depth of 1 (which is insufficient for this domain) with just 50 particles. BA-POMCP, however, was able to perform up to 4096 simulations within 5 seconds and reach an average return of approximately −198, utilizing a belief of 1000 particles. The best performing method, L-R-E-BA-POMCP requires less than 2 seconds for similar results, and is able to reach approximately −190 in less than 3 seconds. Finally, we see that each of the individual modifications outperform the original BA-POMCP, where Expected models seems to be the biggest contributor."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper provides a scalable framework for learning in Bayes-Adaptive POMDPs. BA-POMDPs give a principled way of balancing exploration and exploiting in RL for POMDPs, but previous solution methods have not scaled to non-trivial domains. We extended the Monte Carlo Tree Search method POMCP to BA-POMDPs and described three modifications—Root Sampling, Linking States and Expected Dynamics models— to take advantage of BA-POMDP structure. We proved convergence of the techniques and demonstrated that our methods can generate high-quality solutions on significantly larger problems than previous methods in the literature."
  }, {
    "heading": "Acknowledgements",
    "text": "Research supported by NSF grant #1664923 and NWO Innovational Research Incentives Scheme Veni #639.021.336.\n4 We do not use the same prior as in the first BA-POMCP variants experiments since this gives uninformative results due to the fact that solution methods convergence to the optimal policy with respect to the (noisy) belief, which is different from the one with respect to the true model."
  }, {
    "heading": "A Proof of Theorem 3",
    "text": "While RS-BA-POMCP is potentially more efficient, it is not directly clear whether it still converges to an -optimal value function. Here we show that the method is sound by showing that, when using root sampling of the model, the distribution over full histories (including states, actions and observations) will converge in probability to the same distribution when not using this additional root sampling step."
  }, {
    "heading": "Notation",
    "text": "We will give an concise itemized description of the used notation."
  }, {
    "heading": "Action-observation histories.",
    "text": "• hd is an action-observation history at depth d of a simulation. • hd = (a0,z1, . . . ,ad−1,zd).\n‘Full’ histories. In addition to actions and observations, full histories also include the states. • H0 is the (unknown) full history (of real experience) at the root of the simulation: i.e., if there have been k steps of\n‘real’ experience H0 = (s−k,a−k,s−k+1,z−k−1, . . . ,a−1,s0,z0). • Hd is a full history (of simulated experience) at depth d in the lookahead tree: Hd = (H0,a0,s1,z1,a1,s2,z2, . . . ,ad−1,sd,zd) =\n(Hd−1,ad−1,sd,zd) = 〈H0,s0:d,hd〉. • H(i)d is the full history at depth d corresponding to simulation i. • In our proof, we will also need to indicate if a particular full history Hd is consistent with a full history at the root\nof simulation:\nCons(H0,Hd) = { 1 if Hd is consistent with the full history at the root H0 , 0 otherwise.\nDynamics Function. We fold transition and observations function into one: • D denotes the dynamics model. • Dstztst−1at−1 = D s′z sa = Dst−1,at−1(st,zt) = D(st,zt|st−1,at−1) = Pr(st,zt|st−1,at−1).\n• Dsa denotes the vector: 〈 Ds 1z1 sa , . . . ,D s|S|z|Z| sa 〉 ."
  }, {
    "heading": "Counts.",
    "text": "• χs′zsa denotes how often 〈s′,z〉 occurred after 〈s,a〉. • χsa is the vector of counts for 〈s,a〉. • χ = 〈χs1a1 , . . . ,χs|S|a|A|〉 is the total collection of all such count vectors. • χ(Hd) denotes the vector of counts at simulated full history Hd. • If χ0 = χ(H0) is the count vector at the root of simulation, we have that χ(Hd) = χ0 + ∆(Hd), with ∆(Hd) the\nvector of counts of all (s,a,s′,z) quadruples occurring in Hd since the root of simulation (after H0)."
  }, {
    "heading": "Dirichlet distributions.",
    "text": "• Let x = 〈x1 . . . xK〉 ∈ ∆K and α = 〈α1 . . . αK〉 be a count vector, then we write Dir(x|α) = Pr(x;α) = B(α) ∏K i=1 x αi−1 i , with B(α) = Γ( ∑ i αi)∏\ni Γ(αi) the Dirichlet normalization constant, with Γ the gamma function.\n• So, in translated in terms of dynamics function and counts, we have:\n– for a particular s,a: Dir(Dsa|χsa) = Pr(Dsa;χsa) = B(χsa) ∏ 〈s′,z〉∈S×Z ( Ds ′z sa )χs′zsa −1 .\n– we will also abuse notation and write Dir(D|χ) = ∏ 〈s,a〉Dir(Dsa|χsa)."
  }, {
    "heading": "Var.",
    "text": "• ẋ denotes a root sampled quantity x. • I{condition} is the indicator function which is 1 iff condition is true and 0 otherwise."
  }, {
    "heading": "Definitions",
    "text": "Definition 7. The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π. It is given by\nPπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (4)\nwith Pπ(H0) = b0(〈s0,χ0〉) the belief ‘now’ (at the root of the online planning).\nDefinition 8. The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by\nP̃πK(Hd) , 1\nKd Kd∑ i=1 I{ Hd=H (i) d\n}, where\n• K is the number of simulations that comprise the empirical distribution. • Kd is the number of simulations that reach depth d (not all simulations might be equally long). • H(i)d is the history specified by the i-th particle at stage d.\nRemark: throughout this proof we assume that there is only 1 initial count vector at the root. Or put better: we assume that there is one unique H0 at which all simulations start. However, for ‘real’ steps t > 0 we could be in different Hrealt all corresponding to the same observed real history h real t . In this case, root sampling from the belief can be thought of root sampling the initial full history H0 ∼ b(Hrealt ). As such, our proof shows convergence in probability of\n∀H0∀Hd P̃πKd(Hd|H0) p→ Pπ(Hd|H0).\nfor each such sampled H0. It is clear that that directly implies that ∀Hd P̃πKd(Hd) = EH0 [ P̃πKd(Hd|H0) ] p→ EH0 [Pπ(Hd|H0)] = Pπ(Hd).\nIn the below, we omit the explicit conditioning on H0."
  }, {
    "heading": "Proof of Main Theorem",
    "text": "The proof depends on a lemma that follows below.\nTheorem 9. The full-history RS-BA-POMDP rollout distribution (Def. 8) converges in probability to full-history BAPOMDP rollout distribution (Def. 7):\n∀Hd P̃πKd(Hd) p→ Pπ(Hd). (5)\nProof. For ease of notation we prove this for stage d+ 1. Note that a history Hd+1 = (Hd,ad,sd+1,zd+1), only differs from Hd in that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple, implying that χ(Hd+1) only differs from χ(Hd) in the counts χsdad for sdad. Therefore, the expression for P̃ π Kd\n(Hd) derived in Lemma 10 below (cf. equation (23)) can be written in recursive form as\nP̃π(Hd+1) = Cons(H0,Hd) d∏ t=0 π(at|ht) ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd+1))\n= Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd) ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) B(χsa(Hd)) B(χsa(Hd+1)) = Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd) ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) ∏ 〈s,a〉 B(χsa(Hd)) B(χsa(Hd+1))\n =\nCons(H0,Hd) d−1∏ t=0 π(at|ht) ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) π(ad|hd) B(χsdad(Hd)) B(χsdad(Hd+1))\n= P̃π(Hd)π(ad|hd) B(χsdad(Hd))\nB(χsdad(Hd+1))\nwith base case P̃π(H0) = 1, and\nB(χsdad(Hd))\nB(χsdad(Hd+1)) =\nB(χsdad(H0)) B(χsdad(Hd+1)) · B(χsdad(Hd)) B(χsdad(H0)) = B(χsdad(H0))/B(χsdad(Hd+1)) B(χsdad(H0))/B(χsdad(Hd)) (6)\nthe result of dividing out the contribution of the old counts for sdad and multiplying in the new contribution. Now, we investigate these terms more closely.\nAgain remember that the sole difference between Hd+1 = (Hd,ad,sd+1,zd+1) and Hd is that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple. Let us write T = ∑ (s′,z) χ s′z sdad\n(Hd) for the total of the counts for sd,ad and N = χsd+1zd+1sdad (Hd) for the number of counts for that such a transition was to (sd+1zd+1). Because Hd+1 only has 1 extra transition, we also know that for this history, the total counts is one higher: ∑ (s′,z) χ s′z sdad (Hd+1) = T + 1 and since that transition was to (sd+1zd+1) the counts χ sd+1zd+1 sdad (Hd+1) = N + 1. Now let us expand the term from (6):\nB(χsdad(Hd))\nB(χsdad(Hd+1)) =\nΓ(T )/ ∏ s′z Γ(χ s′z sdad (Hd))\nΓ(T + 1)/ ∏ s′z Γ(χ s′z sdad (Hd+1))\n= Γ(T )\nΓ(T + 1)\n∏ s′z Γ(χ s′z sdad\n(Hd+1))∏ s′z Γ(χ s′ sdad (Hd))\n= Γ(T )\nΓ(T + 1)\nΓ(χ sd+1zd+1 sdad (Hd+1)) ∏ s′z 6=(sd+1zd+1) Γ(χ s′z sdad (Hd+1))\nΓ(χ sd+1zd+1 sdad (Hd)) ∏ s′z 6=(sd+1zd+1) Γ(χ s′z sdad (Hd))\n= Γ(T )\nΓ(T + 1)\nΓ(χ sd+1zd+1 sdad (Hd+1))\nΓ(χ sd+1zd+1 sdad (Hd))\n= Γ(T )\nΓ(T + 1)\nΓ(N + 1)\nΓ(N)\nNow, the gamma function has the property that Γ(x+ 1) = xΓ(x) [4], which means that we get\n= Γ(T )\nTΓ(T )\nNΓ(N) Γ(N) = N T .\nTherefore we get B(χsdad(Hd))\nB(χsdad(Hd+1)) =\nχ sd+1zd+1 sdad (Hd)∑\n(s′,z) χ s′z sdad (Hd)\nand thus\nP̃π(Hd+1) = P̃ π(Hd)π(ad|hd)\nχ sd+1zd+1 sdad (Hd)∑\n(s′,z) χ s′z sdad (Hd) . (7)\nthe r.h.s. of this equation is identical to (4) except for the difference in between P̃π(Hd) and Pπ(Hd). This can be resolved by forward induction with base step: P̃π(H0) = b0(〈s0,χ0,ψ0〉) = Pπ(H0), and the induction step (show P̃π(Hd+1) = P\nπ(Hd+1) given P̃π(Hd) = Pπ(Hd)) directly following from (4) and (7). Therefore we can conclude that ∀d P̃π(Hd) = Pπ(Hd).\nSince Lemma 10 establishes that ∀Hd P̃πKd(Hd) p→ P̃π(Hd), we directly have\n∀Hd P̃πKd(Hd) p→ Pπ(Hd),\nthus proving the result.\nThe proof depends on the following lemma:\nLemma 10. The full-history RS-BA-POMDP rollout distribution converges in probability to the following quantity:\n∀Hd P̃πKd(Hd) p→ b0(s0) [ d∏ t=1 π(at−1|ht−0) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)  (8) with B(α) = Γ(α1+...·+αk)Γ(α1)·...·Γ(αk) the normalization term of a Dirichlet distribution with parametric vector α.\nProof. Via the weak law of large numbers, we have that the empirical mean of a random variable converges in probability to its expectation.\n∀Hd P̃πKd(Hd) p→ 1 Kd Kd∑ i=1 I{ Hd=H (i) d } p→ E [ I{ Hd=H (i) d }] This expectation can be rewritten as follows\nE [ I{ Hd=H (i) d }] = ∑ H\n(i) d\nP̃π ( H\n(i) d ) I{ Hd=H (i) d } = P̃π (Hd) (9)\nwhere P̃π(Hd) denotes the (true, non-empirical) probability that the RS-BA-POMDP rollout generates full historyHd. This is an expectation over the root sampled model Ḋ:\nP̃π(Hd) = ∫ P̃π ( Hd|Ḋ ) Dir(Ḋ|χ̇)dḊ (10)\n= ∫ [ Cons(H0,Hd)\nd∏ t=1 Ḋ(st,zt|st−1,at−1)π(at−1|ht−1)\n] Dir(Ḋ|χ̇)dḊ (11)\n= Cons(H0,Hd) [ d∏ t=1 π(at−1|ht−1) ](∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ] Dir(Ḋ|χ̇)dḊ ) (12)\nWhere Cons(H0,Hd) is a term that indicates whether (takes value 1 if) Hd is consistent with the full history at the root H0.5\n5An earlier version of this proof ([9]) contained a term b0(s0) instead of Cons(H0,Hd), which fails to recognize that this proof assumes H0 to be fixed. See also the remark on page 11.\nNow we can exploit the fact that only the Dirichlet for the transitions specified by Hd matter.∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ] Dir(Ḋ|χ0)dḊ (13) ={split up the integral over one big vector into integrals over smaller vectors}∫ · · · ∫ [ d∏\nt=1\nḊst,ztst−1,at−1 ]∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  dḊs1a1 . . . dḊs|S|a|A| (14) ={reorder the transition probabilities: ∆sas ′z χ (Hd)is the number of occurences of (s,a,s\n′,z)in Hd}∫ · · · ∫ ∏\n〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )∆sas′zχ (Hd)∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  dḊs1a1 . . . dḊs|S|a|A| (15) = ∫ · · · ∫ ∏\n〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )∆sas′zχ (Hd)∏ 〈s,a〉 B(χ̇sa) ∏ 〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1 dḊs1a1 . . . dḊs|S|a|A| (16) = ∫ · · · ∫ ∏\n〈s,a〉  ∏ 〈s′,z〉 ( Ḋs ′z sa )∆sas′zχ (Hd)B(χ̇sa) ∏ 〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1 dḊs1a1 . . . dḊs|S|a|A| (17) = ∫ · · · ∫ ∏\n〈s,a〉 B(χ̇sa)  ∏ 〈s′,z〉 ( Ḋs ′z sa )∆sas′zχ (Hd) ∏ 〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1 dḊs1a1 . . . dḊs|S|a|A| (18) = ∫ · · · ∫ ∏\n〈s,a〉 B(χ̇sa) ∏ 〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1+∆sas′zχ (Hd) dḊs1a1 . . . dḊs|S|a|A| (19) Now we reverse the order of integration and multiplication, which is possible since the different s,a pairs over which we integrate are disjoint.6 We obtain:\n= ∏ 〈s,a〉 B(χsa(H0)) ∫ ∏ 〈s′,z〉 ( Ḋsa(s ′,z) )χsas′z0 +∆sas′zχ (Hd)−1 dḊsa (20)\n={since we integrate over the entire vector Ḋsa, the integral equals 1/B(χsa(H0) + ∆saχ (Hd))}∏ 〈s,a〉 B(χsa(H0)) 1 B(χsa(H0) + ∆saχ (Hd)) (21)\n= ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) (22)\nTherefore\nP̃π(Hd) = Cons(H0,Hd) [ d−1∏ t=0 π(at|ht) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd))  , (23) proving (8).\n6E.g, consider two sets A1 = { a (1) 1 ,a (2) 1 } and A2 = { a (1) 2 ,a (2) 2 ,a (3) 2 } . Equation (19) is of the same form as\n∑ a1∈A1 ∑ a2∈A2 2∏ i=1 ai = ∑ a1∈A1 ∑ a2∈A2 a1a2 = a (1) 1 a (1) 2 + a (1) 1 a (2) 2 + a (1) 1 a (3) 2 + a (2) 1 a (1) 2 + a (2) 1 a (2) 2 + a (2) 1 a (3) 2\n= a (1) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) + a (2) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) = ( a (1) 1 + a (2) 1 )( a (1) 2 + a (2) 2 + a (3) 2 ) =\n ∑ a1∈A1 a1  ∑ a2∈A2 a2  = 2∏ i=1 ∑ ai∈Ai ai"
  }],
  "year": 2018,
  "references": [{
    "title": "Scalable planning and learning for multiagent POMDPs",
    "authors": ["C. Amato", "F.A. Oliehoek"],
    "venue": "In Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Finite-time analysis of the multiarmed bandit problem",
    "authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"],
    "venue": "Machine learning,",
    "year": 2002
  }, {
    "title": "Acting optimally in partially observable stochastic domains",
    "authors": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"],
    "venue": "In Proc. of the AAAI Conf. on Artificial Intelligence,",
    "year": 1994
  }, {
    "title": "Optimal Statistical Decisions",
    "authors": ["M.H. DeGroot"],
    "venue": "Wiley-Interscience,",
    "year": 2004
  }, {
    "title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
    "authors": ["M.O. Duff"],
    "venue": "PhD thesis, University of Massachusetts Amherst,",
    "year": 2002
  }, {
    "title": "Reinforcement learning with kernels and Gaussian processes",
    "authors": ["Y. Engel", "S. Mannor", "R. Meir"],
    "venue": "In Proceedings of the ICML Workshop on Rich Representations for Reinforcement Learning,",
    "year": 2005
  }, {
    "title": "Efficient Bayes-adaptive reinforcement learning using sample-based search",
    "authors": ["A. Guez", "D. Silver", "P. Dayan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Planning and acting in partially observable stochastic domains",
    "authors": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"],
    "venue": "Artificial intelligence,",
    "year": 1998
  }, {
    "title": "Best response bayesian reinforcement learning for multiagent systems with state uncertainty",
    "authors": ["F.A. Oliehoek", "C. Amato"],
    "venue": "In AAMAS Workshop on Multiagent Sequential Decision Making Under Uncertainty,",
    "year": 2014
  }, {
    "title": "Model-based Bayesian reinforcement learning in partially observable domains",
    "authors": ["P. Poupart", "N.A. Vlassis"],
    "venue": "In International Symposium on Artificial Intelligence and Mathematics,",
    "year": 2008
  }, {
    "title": "Bayes-Adaptive POMDPs",
    "authors": ["S. Ross", "B. Chaib-draa", "J. Pineau"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2007
  }, {
    "title": "Online planning algorithms for POMDPs",
    "authors": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-Draa"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2008
  }, {
    "title": "A Bayesian approach for learning and planning in partially observable Markov decision processes",
    "authors": ["S. Ross", "J. Pineau", "B. Chaib-draa", "P. Kreitmann"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "A survey of point-based POMDP solvers",
    "authors": ["G. Shani", "J. Pineau", "R. Kaplow"],
    "venue": "Autonomous Agents and Multi-Agent Systems,",
    "year": 2012
  }, {
    "title": "Monte-Carlo planning in large POMDPs",
    "authors": ["D. Silver", "J. Veness"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Reinforcement Learning: An Introduction",
    "authors": ["R.S. Sutton", "A.G. Barto"],
    "year": 1998
  }, {
    "title": "Monte Carlo POMDPs",
    "authors": ["S. Thrun"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1999
  }, {
    "title": "Bayesian reinforcement learning",
    "authors": ["N. Vlassis", "M. Ghavamzadeh", "S. Mannor", "P. Poupart"],
    "venue": "In Reinforcement Learning,",
    "year": 2012
  }, {
    "title": "Exploration control in reinforcement learning using optimistic model selection",
    "authors": ["J.L. Wyatt"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2001
  }],
  "id": "SP:614b197a11eba3f25337b0c9b9cb409ae6230238",
  "authors": [{
    "name": "Sammie Katt",
    "affiliations": []
  }, {
    "name": "Frans A. Oliehoek",
    "affiliations": []
  }, {
    "name": "Christopher Amato",
    "affiliations": []
  }],
  "abstractText": "The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BAPOMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BAPOMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",
  "title": "Learning in POMDPs with Monte Carlo Tree Search∗"
}