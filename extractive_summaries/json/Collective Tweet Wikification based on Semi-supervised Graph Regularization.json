{
  "sections": [{
    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 380–390, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "With millions of tweets posted daily, Twitter enables both individuals and organizations to disseminate information, from current affairs to breaking news in a timely fashion. In this work, we study the wikification (Disambiguation to Wikipedia) task (Mihalcea and Csomai, 2007) for tweets, which aims to automatically identify each concept mention in a tweet, and link it to a\nconcept referent in a knowledge base (KB) (e.g., Wikipedia). For example, as shown in Figure 1, Hawks is an identified mention, and its correct referent concept in Wikipedia is Atlanta Hawks. An end-to-end wikification system needs to solve two sub-problems: (i) concept mention detection, (ii) concept mention disambiguation.\nWikification is a particularly useful task for short messages such as tweets because it allows a reader to easily grasp the related topics and enriched information from the KB. From a systemto-system perspective, wikification has demonstrated its usefulness in a variety of applications, including coreference resolution (Ratinov and Roth, 2012) and classification (Vitale et al., 2012).\nSufficient labeled data is crucial for supervised models. However, manual wikification annotation for short documents is challenging and timeconsuming (Cassidy et al., 2012). The challenges are: (i) unlinkability, a valid concept may not exist in the KB. (ii) ambiguity, it is impossible to determine the correct concept due to the dearth of information within a single tweet or multiple correct answer. For instance, it would be difficult to determine the correct referent concept for “Gators” in t1 in Figure 1. Linking “UCONN” in t3 to University of Connecticut may also be acceptable since Connecticut Huskies is the athletic team of the university. (iii) prominence, it is challenging to select a set of linkable mentions that are important and relevant. It is not tricky to select “Fans”, “slump”, and “Hawks” as linkable mentions, but other mentions such as “stay up” and “stay positive” are not prominent. Therefore, it is challenging to create sufficient high quality labeled tweets for supervised models and worth considering semi-supervised learning with the exploration of unlabeled data.\n380\nHowever, when selecting semi-supervised learning frameworks, we noticed another unique challenge that tweets pose to wikification due to their informal writing style, shortness and noisiness. The context of a single tweet usually cannot provide enough information for prominent mention detection and similarity computing for disambiguation. Therefore, a collective inference model over multiple tweets in the semi-supervised setting is desirable. For instance, the four tweets in Figure 1 are posted by the same author within a short time period. If we perform collective inference over them we can reliably link ambiguous mentions such as “Gators”, “Hawks”, and “Bucks” to basketball teams instead of other concepts such as the county Bucks County.\nIn order to address these unique challenges for wikification for the short tweets, we employ graph-based semi-supervised learning algorithms (Zhu et al., 2003; Smola and Kondor, 2003; Blum et al., 2004; Zhou et al., 2004; Talukdar and Crammer, 2009) for collective inference by exploiting the manifold (cluster) structure in both unlabeled and labeled data. These approaches normally assume label smoothness over a defined graph, where the nodes represent a set of labeled and unlabeled instances, and the weighted edges reflect the closeness of each pair of instances. In order to construct a semantic-rich graph capturing the similarity between mentions and concepts for the model, we introduce three novel fine-grained relations based on a set of local features, social networks and meta paths.\nThe main contributions of this paper are summarized as follows:\n• To the best of our knowledge, this is the first\neffort to explore graph-based semi-supervised learning algorithms for the wikification task. • We propose a novel semi-supervised graph reg-\nularization model performing collective inference for joint mention detection and disambiguation. Our approach takes advantage of three proposed principles to incorporate both local and global evidence from multiple tweets. • We propose a meta path-based unified frame-\nwork to detect both explicitly and implicitly relevant mentions."
  }, {
    "heading": "2 Preliminaries",
    "text": "Concept and Concept Mention We define a concept c as a Wikipedia article (e.g., Atlanta Hawks), and a concept mentionm as an n-gram from a specific tweet. Each concept has a set of textual representation fields (Meij et al., 2012), including title (the title of the article), sentence (the first sentence of the article), paragraph (the first paragraph of the article), content (the entire content of the article), and anchor (the set of all anchor texts with incoming links to the article).\nWikipedia Lexicon Construction We first construct an offline lexicon with each entry as 〈m, {c1, ..., ck}〉, where {c1, ..., ck} is the set of possible referent concepts for the mention m. Following the previous work (Bunescu, 2006; Cucerzan, 2007; Hachey et al., 2013), we extract the possible mentions for a given concept c using the following resources: the title of c; the aliases appearing in the introduction and infoboxes of c (e.g., The Evergreen State is an alias of Washington state); the titles of pages redirecting to c (e.g., State of Washington is a redirecting page of Washington (state)); the titles of the disambigua-\ntion pages containing c; and all the anchor texts appearing in at least 5 pages with hyperlinks to c (e.g., WA is a mention for the concept Washington (state) in the text “401 5th Ave N [[Seattle]], [[Washington (state)—WA]] 98109 USA”. We also propose three heuristic rules to extract mentions (i.e., different combinations of the family name and given name for a person, the headquarters of an organization, and the city name for a sports team).\nConcept Mention Extraction Based on the constructed lexicon, we then consider all n-grams of size≤ n (n=7 in this paper) as concept mention candidates if their entries in the lexicon are not empty. We first segment @usernames and #hashtags into regular tokens (e.g., @amandapalmer is segmented as amanda palmer and #WorldWaterDay is split as World Water Day) using the approach proposed by (Wang et al., 2011). Segmentation assists finding concept candidates for these non-regular mentions."
  }, {
    "heading": "3 Principles and Approach Overview",
    "text": ""
  }, {
    "heading": "3.1 Principles",
    "text": "A single tweet may not provide enough evidence to identify prominent mentions and infer their correct referent concepts due to the lack of contextual information. To tackle this problem, we propose to incorporate global evidence from multiple tweets and performing collective inference for both mention identification and disambiguation. We first introduce the following three principles that our approach relies on.\nPrinciple 1 (Local compatibility): Two pairs of 〈m, c〉 with strong local compatibility tend to\nhave similar labels. Mentions and their correct referent concepts usually tend to share a set of characteristics such as string similarity betweenm and c (e.g., 〈Chicago, Chicago〉 and 〈Facebook, Facebook〉). We define the local compatibility to model such set of characteristics.\nPrinciple 2 (Coreference): Two coreferential mentions should be linked to the same concept. For example, if we know “nc” and “North Carolina” are coreferential, then they should both be linked to North Carolina.\nPrinciple 3 (Semantic Relatedness): Two highly semantically-related mentions are more likely to be linked to two highly semanticallyrelated concepts. For instance, when “Sweet 16” and “Hawks” often appear together within relevant contexts, they can be reliably linked to two baseketball-related concepts NCAA Men’s Division I Basketball Championship and Atlanta Hawks, respectively."
  }, {
    "heading": "3.2 Approach Overview",
    "text": "Given a set of tweets 〈t1, ..., t|T |〉, our system first generates a set of candidate concept mentions, and then extracts a set of candidate concept referents for each mention based on the Wikipedia lexicon. Given a pair of mention and its candidate referent concept 〈m, c〉, the remaining task of wikification is to assign either a positive label if m should be selected as a prominently linkable mention and c is its correct referent concept, or otherwise a negative label. The label assignment is obtained by our semi-supervised graph regularization framework based on a relational graph, which is constructed from local compatibility, coreference, and semantic relatedness relations. The overview of our approach is as illustrated in Figure 2."
  }, {
    "heading": "4 Relational Graph Construction",
    "text": "We first construct the relational graphG = 〈V,E〉, where V = {v1, ..., vn} is a set of nodes and E = {e1, ..., em} is a set of edges. Each vi = 〈mi, ci〉 represents a tuple of mention mi and its referent concept candidate ci. An edge is added between two nodes vi and vj if there is a proposed relation based on the three principles described in section 3.1."
  }, {
    "heading": "4.1 Local Compatibility",
    "text": "We first compute local compatibility (Principle 1) by considering a set of novel local features to cap-\nture the importance and relevance of a mention m to a tweet t, as well as the correctness of its linkage to a concept c. We have designed a number of features which are similar to those commonly used in wikification and entity linking work (Meij et al., 2012; Guo et al., 2013; Mihalcea and Csomai, 2007).\nMention Features We define the following features based on information from mentions. • IDFf (m) = log( |C|df(m)), where |C| is the total number of concepts in Wikipedia and df(m) is the total number of concepts in whichm occurs, and f indicates the field property, including title, content, and anchor. • Keyphraseness(m) = |Ca(m)|df(m) to measure\nhow likely m is used as an anchor in Wikipedia, where Ca(m) is the set of concepts where m appears as an anchor.\n• LinkProb(m) = ∑\nc∈Ca(m) count(m,c)∑ c∈C count(m,c)\n, where count(m, c) indicates the number of occurrence of m in c. • SNIL(m) and SNCL(m) to count the number\nof concepts that are equal to or contain a sub-ngram of m, respectively (Meij et al., 2012).\nConcept Features The concept features are solely based on Wikipedia, including the number of incoming and outgoing links for c, and the number of words and characters in c.\nMention + Concept Features This set of features considers information from both mentions and concepts:\n• prior popularity prior(m, c) = count(m,c)∑ c′ count(m,c′)\n, where count(m, c) measures the frequency of the anchor links from m to c in Wikipedia. • TFf (m, c) = countf (m,c)|f | to measure the rela-\ntive frequency of m in each field representation f of c, normalized by the length of f . The fields include title, sentence, paragraph, content and anchor. • NCT (m, c), TCN(m, c), and TEN(m, c) to\nmeasure whether m contains the title of c, whether the title of c contains m, and whether m equals to the title of c, respectively.\nContext Features This set of features include (i) Context Capitalization features, which indicate whether the current mention, the token before, and the token after are capitalized. (ii) tf-idf based features, which include the dot product of two word\nvectors vc and vt, and the average tf-idf value of common items in vc and vt, where vc and vt are the top 100 tf-idf word vectors in c and t.\nLocal Compatibility Computation For each node vi = 〈mi, ci〉, we collect its local features as a feature vector Fi = 〈f1, f2, ..., fd〉. To avoid features with large numerical values that dominate other features, the value of each feature is re-scaled using feature standardization approach. The cosine similarity is then adopted to compute the local compatibility of two nodes and construct a k nearest neighbor (kNN) graph, where each node is connected to its k nearest neighboring nodes. We compute the weight matrix that represents the local compatibility relation as:\nW locij = { cosine(Fi, Fj) j ∈ kNN(i) 0 Otherwise"
  }, {
    "heading": "4.2 Meta Path",
    "text": "In this subsection, we introduce the concept meta path which will be used to detect coreference (section 4.3) and semantic relatedness relations (section 4.4).\nA meta-path is a path defined over a network and composed of a sequence of relations between different object types (Sun et al., 2011b). In our experimental setting, we can construct a natural Twitter network summarized by the network schema in Figure 3. The network contains four types of objects: Mention (M), Tweet (T), User (U), and Hashtag (H). Tweets and mentions are connected by links “contain” and “contained by” (denoted as “contain−1”); and other linked relationships can be described similarly.\nWe then define the following five types of meta paths to connect two mentions as:\n• “M - T - M”, • “M - T - U - T - M”, • “M - T - H - T - M”, • “M - T - U - T - M - T - H - T - M”, • “M - T - H - T - M - T - U - T - M”.\nEach meta path represents one particular semantic relation. For instance, the first three paths are basic ones expressing the explicit relations that two mentions are from the same tweet, posted by the same user, and share the same #hashtag, respectively. The last two paths are concatenated ones which are constructed by concatenating the first three simple paths to express the implicit relations that two mentions co-occur with a third mention sharing either the same authorship or #hashtag. Such complicated paths can be exploited to detect more semantically-related mentions from wider contexts. For example, the relational link between “narita airport” and “Japan” would be missed without using the path “narita airport - t1 - u1 - t2 - american - t3 - h1 - t4 - Japan” since they don’t directly share any authorships or #hashtags."
  }, {
    "heading": "4.3 Coreference",
    "text": "A coreference relation (Principle 2) usually occurs across multiple tweets due to the highly redundant information in Twitter. To ensure high precision, we propose a simple yet effective approach utilizing the rich social network relations in Twitter.\nWe consider two mentions mi and mj coreferential if mi and mj share the same surface form or one is an abbreviation of the other, and at least one meta path exists betweenmi andmj . Then we define the weight matrix representing the coreferential relation as:\nW corefij =  1.0 if mi and mj are coreferential,\nand ci = cj 0 Otherwise"
  }, {
    "heading": "4.4 Semantic Relatedness",
    "text": "Ensuring topical coherence (Principle 3) has been beneficial for wikification on formal texts (e.g., News) by linking a set of semantically-related mentions to a set of semantically-related concepts simultaneously (Han et al., 2011; Ratinov et al., 2011; Cheng and Roth, 2013). However, the shortness of a single tweet means that it may not provide enough topical clues. Therefore, it is important to extend this evidence to capture semantic relatedness information from multiple tweets.\nWe define the semantic relatedness score between two mentions as SR(mi,mj) = 1.0 if at least one meta path exists between mi and mj , otherwise SR(mi,mj) = 0. In order to compute the semantic relatedness of two concepts ci and cj , we adopt the approach proposed by (Milne and\nWitten, 2008a):\nSR(ci, cj) = 1− log max(|Ci|, |Cj |)− log |Ci ∩ Cj |log(|C|)− log min(|Ci|, |Cj |) ,\nwhere |C| is the total number of concepts in Wikipedia, and Ci and Cj are the set of concepts that have links to ci and cj , respectively.\nThen we compute a weight matrix representing the semantic relatedness relation as:\nW relij = { SR(Ni, Nj) if SR(Ni, Nj) ≥ δ 0 Otherwise\nwhere SR(Ni, Nj) = SR(mi,mj) × SR(ci, cj) and δ = 0.3, which is optimized from a development set."
  }, {
    "heading": "4.5 The Combined Relational Graph",
    "text": "Based on the above three weight matricesW loc, W coref , and W rel, we first obtain their corresponding transition matrices P loc, P coref , and P rel, respectively. The entry Pij of the transition matrix P for a weight matrix W is computed as Pij =\nWij∑ k Wik\nsuch that ∑\nk Pik = 1. Then we obtain the combined graph G with weight matrix W , where Wij = αP locij + βP coref ij + γP rel ij . α, β, and γ are three coefficients between 0 and 1 with the constraint that α+ β + γ = 1. They control the contributions of these three relations in our semi-supervised graph regularization model. We choose transition matrix to avoid the domination of one relation over others. An example graph of G is shown in Figure 4. Compared to the referent graph which considers each mention or concept as a node in previous graph-based re-ranking approaches (Han et al., 2011; Shen et al., 2013), our\nnovel graph representation has two advantages: (i) It can easily incorporate more features related to both mentions and concepts. (ii) It is more appropriate for our graph-based semi-supervised model since it is difficult to assign labels to a pair of mention and concept in the referent graph."
  }, {
    "heading": "5 Semi-supervised Graph Regularization",
    "text": "Given the constructed relational graph with the weighted matrix W and the label vector Y of all nodes, we assume the first l nodes are labeled as Yl and the remaining u nodes (u = n− l) are initialized with labels Y 0u . Then our goal is to refine Y 0u and obtain the final label vector Yu.\nIntuitively, if two nodes are strongly connected, they tend to hold the same label. We propose a novel semi-supervised graph regularization framework based on the graph-based semi-supervised learning algorithm (Zhu et al., 2003): Q(Y) = µ n∑\ni=l+1\n(yi−y0i )2 + 1 2 ∑ i,j Wij(yi−yj)2.\nThe first term is a loss function that incorporates the initial labels of unlabeled examples into the model. In our method, we adopt prior popularity (section 4.1) to initialize the labels of the unlabeled examples. The second term is a regularizer that smoothes the refined labels over the constructed graph. µ is a regularization parameter that controls the trade-off between initial labels and the consistency of labels on the graph. The goal of the proposed framework is to ensure that the refined labels of unlabeled nodes are consistent with their strongly connected nodes, as well as not too far away from their initial labels.\nThe above optimization problem can be solved directly since Q(Y) is convex (Zhu et al., 2003; Zhou et al., 2004). Let I be an identity matrix and DW be a diagonal matrix with entries Dii =∑\nj Wij . We can split the weighted matrix W into four blocks as W = [ Wll Wlu Wul Wuu ] , where Wmn is anm×nmatrix. Dw is split similarly. We assume that the vector of the labeled examples Yl is fixed, so we only need to infer the refined label vector of the unlabeled examples Yu. In order to minimize Q(Y), we need to find Y ∗u such that ∂Q\n∂Yu ∣∣∣∣ Yu=Y ∗u = (Duu + µIuu)Yu −WuuYu −\nWulYl − µY 0u = 0.\nTherefore, a closed form solution can be derived as Y ∗u = (Duu + µIuu −Wuu)−1(WulYl + µY 0u ).\nHowever, for practical application to a largescale data set, an iterative solution would be more efficient to solve the optimization problem. Let Y tu be the refined labels after the t\nth iteration, the iterative solution can be derived as:\nY t+1u = (Duu+µIuu) −1(WuuY tu+WulYl+µY 0 u ).\nThe iterative solution is more efficient since (Duu + µIuu) is a diagonal matrix and its inverse is very easy to compute."
  }, {
    "heading": "6 Experiments",
    "text": "In this section we compare our approach with state-of-the-art methods as shown in Table 1."
  }, {
    "heading": "6.1 Data and Scoring Metric",
    "text": "For our experiments we use a public data set (Meij et al., 2012) including 502 tweets posted by 28 verified users. The data set was annotated by two annotators. We randomly sample 102 tweets for development and the remaining for evaluation. We use a Wikipedia dump on May 3, 2013 as our knowledge base, which includes 30 million pages. For computational efficiency, we also filter some mention candidates by applying the preprocessing approach proposed in (Ferragina and Scaiella, 2010), and remove all the concepts with prior popularity less than 2% from an mention’s concept set for each mention, similar to (Guo et al., 2013).\nA mention and concept pair 〈m, c〉 is judged as correct if and only if m is linkable and c is the correct referent concept for m. To evaluate the performance of a wikification system, we use the standard precision, recall and F1 measures."
  }, {
    "heading": "6.2 Experimental Results",
    "text": "The overall performance of various approaches is shown in Table 2. The results of the supervised method proposed by (Meij et al., 2012) are obtained from 5-fold cross validation. For our semi-supervised setting, we experimentally sample 200 tweets for training and use the remaining set as unlabeled and testing sets. In our semisupervised regularization model, the matrix W loc is constructed by a kNN graph (k = 20). The regularization parameter µ is empirically set to 0.1, and the coefficients α, β, and γ are learnt from the development set by considering all the combina-\ntions of values from 0 to 1 at 0.1 intervals1. In order to randomize the experiments and make the comparison fair, we conduct 20 test runs for each method and report the average scores across the 20 trials.\nThe relatively low performance of the baseline system TagMe demonstrates that only relying on prior popularity and topical information within a single tweet is not enough for an end-to-end wikification system for the short tweets. As an example, it is difficult to obtain topical clues in order to link the mention “Clinton” to Hillary Rodham Clinton by relying on the single tweet “wolfblitzercnn: Behind the scenes on Clinton’s Mideast trip #cnn”. Therefore, the system mistakenly links it to the most popular concept Bill Clinton.\nIn comparision with the supervised baseline proposed by (Meij et al., 2012), our model SSRegu1 relying on local compatibility already achieves comparable performance with 50% of labeled data. This is because that our model performs collective inference by making use of the manifold (cluster) structure of both labeled and unlabeled data, and that the local compatibility relation is detected with high precision2 (89.4%). For example, the following three pairs of mentions and concepts 〈pelosi, Nancy Pelosi〉, 〈obama, Barack Obama〉, and 〈gaddafi, Muam-\n1These three coefficients are slightly different with different training data, a sample of them is: α = 0.4, β = 0.5, and γ = 0.1\n2Here we define precision as the percentage of links that holds the same label.\nmar Gaddafi〉 have strong local compatibility with each other since they share many similar characteristics captured by the local features such as string similarity between the mention and the concept. Suppose the first pair is labeled, then its positive label will be propagated to other unlabeled nodes through the local compatibility relation, and correctly predict the labels of other nodes.\nIncorporating coreferential or semantic relatedness relation into SSRegu1 provides further gains, demonstrating the effectiveness of these two relations. For instance, “wh” is correctly linked to White House by incorporating evidence from its coreferential mention “white house”. The coreferential relation (Principle 2) is demonstrated to be more beneficial than the semantic relatedness relation (Principle 3) because the former is detected with much higher precision (99.7%) than the latter (65.4%).\nOur full model SSRegu123 achieves significant improvement over the supervised baseline (5% absolute F1 gain with 95.0% confidence level by the Wilcoxon Matched-Pairs Signed-Ranks Test), showing that incorporating global evidence from multiple tweets with fine-grained relations is beneficial. For instance, the supervised baseline fails to link “UCONN” and “Bucks” in our examples to Connecticut Huskies and Milwaukee Bucks, respectively. Our full model corrects these two wrong links by propagating evidence through the semantic links as shown in Figure 4 to obtain mutual ranking improvement. The best performance of our full model also illustrates that the three relations complement each other.\nWe also study the disambiguation performance for the annotated mentions, as shown in Table 3. We can easily see that our proposed approach using 50% labeled data achieves similar performance with the state-of-the-art supervised model with 100% labeled data. When the mentions are given, the unpervised approach TagMe has already\nachieved reasonable performance. In fact, mention detection actually is the performance bottleneck of a tweet wikification system (Guo et al., 2013). Our system performs better in identifying the prominent mention."
  }, {
    "heading": "6.3 Effect of Concatenated Meta Paths",
    "text": "In this work, we propose a unified framework utilizing meta path-based semantic relations to explore richer relevant context. Beyond the basic meta paths, we introduce concatenated ones by concatenating the basic ones. The performance of the system without using the concatenated meta paths is shown in Table 4. In comparison with the system based on all defined meta paths, we can clearly see that the systems using concatenated ones outperform those relying on the simple ones. This is because the concatenated meta paths can incorporate more relevant information with implicit relations into the models by increasing 1.6% coreference links and 9.3% semantic relatedness links. For example, the mention “narita airport” is correctly disambiguated to the concept “Narita International Airport” with higher confidence since its semantic relatedness relation with “Japan” is detected with the concatenated meta path as described in section 4.2."
  }, {
    "heading": "6.4 Effect of Labeled Data Size",
    "text": "In previous experiments, we experimentally set the number of labeled tweets to be 200 for overall performance comparision with the baselines. In this subsection, we study the effect of labeled data size on our full model. We randomly sample 100 tweets as testing data, and randomly select 50, 100, 150, 200, 250, and 300 tweets as labeled data. 20 test runs are conducted and the average results are reported across the 20 trials, as shown in Figure 5. We find that as the size of the labeled data increases, our proposed model achieves better performance. It is encouraging to see that our approach, with only 31.3% labeled tweets (125 out of 400), already achieves a performance that is comparable to the state-of-the-art supervised model trained from 100% labeled tweets."
  }, {
    "heading": "6.5 Parameter Analysis",
    "text": "In previous experiments, we empirically set the parameter µ = 0.1. µ is the regularization parameter that controls the trade-off between initial labels and the consistency of labels on the graph. When µ increases, the model tends to trust more in the initial labels. Figure 6 shows the performance of our models by varying µ from 0.02 to 50. We can easily see that the system performce is stable when µ < 0.4. However, when µ ≥ 0.4, the system performance dramatically decreases, showing that prior popularity is not enough for an end-toend wikification system."
  }, {
    "heading": "7 Related Work",
    "text": "The task of linking concept mentions to a knowledge base has received increased attentions over the past several years, from the linking of concept mentions in a single text (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Milne and Witten, 2008a; Kulkarni et al., 2009; He et al., 2011; Ratinov et al., 2011; Cassidy et al., 2012; Cheng and Roth, 2013), to the linking of a cluster of corefer-\nent named entity mentions spread throughout different documents (Entity Linking) (McNamee and Dang, 2009; Ji et al., 2010; Zhang et al., 2010; Ji et al., 2011; Zhang et al., 2011; Han and Sun, 2011; Han et al., 2011; Gottipati and Jiang, 2011; He et al., 2013; Li et al., 2013; Guo et al., 2013; Shen et al., 2013; Liu et al., 2013).\nA significant portion of recent work considers the two sub-problems mention detection and mention disambiguation separately and focus on the latter by first defining candidate concepts for a deemed mention based on anchor links. Mention disambiguation is then formulated as a ranking problem, either by resolving one mention at each time (non-collective approaches), or by disambiguating a set of relevant mentions simultaneously (collective approaches). Non-collective methods usually rely on prior popularity and context similarity with supervised models (Mihalcea and Csomai, 2007; Milne and Witten, 2008b; Han and Sun, 2011), while collective approaches further leverage the global coherence between concepts normally through supervised or graph-based re-ranking models (Cucerzan, 2007; Milne and Witten, 2008b; Han and Zhao, 2009; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Ferragina and Scaiella, 2010; Fernandez et al., 2010; Radford et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Han et al., 2011; Ratinov et al., 2011; Chen and Ji, 2011; Kozareva et al., 2011; Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Especially note that when applying the collective methods to short messages from social media, evidence from other messages usually needs to be considered (Cassidy et al., 2012; Shen et al., 2013; Liu et al., 2013). Our method is a collective approach with the following novel advancements: (i) A novel graph representation with fine-grained relations, (ii) A unified framework based on meta paths to explore richer relevant context, (iii) Joint identification and linking of mentions under semi-supervised setting.\nTwo most similar methods to ours were proposed by (Meij et al., 2012; Guo et al., 2013) by performing joint detection and disambiguation of mentions. (Meij et al., 2012) studied several supervised machine learning models, but without considering any global evidence either from a single tweet or other relevant tweets. (Guo et al., 2013) explored second order entity-to-entity relations but did not incorporate evidence from multi-\nple tweets. This work is also related to graph-based semisupervised learning (Zhu et al., 2003; Smola and Kondor, 2003; Zhou et al., 2004; Talukdar and Crammer, 2009), which has been successfully applied in many Natural Language Processing tasks (Niu et al., 2005; Chen et al., 2006). We introduce a novel graph that incorporates three fine-grained relations. Our work is further related to meta path-based heterogeneous information network analysis (Sun et al., 2011b; Sun et al., 2011a; Kong et al., 2012; Huang et al., 2013), which has demonstrated advantages over homogeneous information network analysis without differentiating object types and relational links."
  }, {
    "heading": "8 Conclusions",
    "text": "We have introduced a novel semi-supervised graph regularization framework for wikification to simultaneously tackle the unique challenges of annotation and information shortage in short tweets. To the best of our knowledge, this is the first work to explore the semi-supervised collective inference model to jointly perform mention detection and disambiguation. By studying three novel finegrained relations, detecting semantically-related information with semantic meta paths, and exploiting the data manifolds in both unlabeled and labeled data for collective inference, our work can dramatically save annotation cost and achieve better performance, thus shed light on the challenging wikification task for tweets."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), U.S. NSF CAREER Award under Grant IIS-0953149, U.S. DARPA Award No. FA8750-13-2-0041 in the Deep Exploration and Filtering of Text (DEFT) Program, IBM Faculty Award, Google Research Award and RPI faculty start-up grant. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on."
  }],
  "year": 2014,
  "references": [{
    "title": "Semi-supervised learning using randomized mincuts",
    "authors": ["A. Blum", "J. Lafferty", "M. Rwebangira", "R. Reddy."],
    "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML ’04.",
    "year": 2004
  }, {
    "title": "Using encyclopedic knowledge for named entity disambiguation",
    "authors": ["Razvan Bunescu."],
    "venue": "EACL, pages 9–16.",
    "year": 2006
  }, {
    "title": "Analysis and enhancement of wikification for microblogs with context expansion",
    "authors": ["T. Cassidy", "H. Ji", "L. Ratinov", "A. Zubiaga", "H. Huang."],
    "venue": "Proceedings of COLING 2012.",
    "year": 2012
  }, {
    "title": "Collaborative ranking: A case study on entity linking",
    "authors": ["Z. Chen", "H. Ji."],
    "venue": "Proc. EMNLP2011.",
    "year": 2011
  }, {
    "title": "Relation extraction using label propagation based semisupervised learning",
    "authors": ["J. Chen", "D. Ji", "C Tan", "Z. Niu."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for",
    "year": 2006
  }, {
    "title": "Relational inference for wikification",
    "authors": ["X. Cheng", "D. Roth."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Large-scale named entity disambiguation based on wikipedia data",
    "authors": ["Silviu Cucerzan."],
    "venue": "EMNLPCoNLL 2007.",
    "year": 2007
  }, {
    "title": "Tac entity linking by performing full-document entity extraction and disambiguation",
    "authors": ["S. Cucerzan."],
    "venue": "Proc. TAC 2011 Workshop.",
    "year": 2011
  }, {
    "title": "Webtlab: A cooccurence-based approach to kbp 2010 entity-linking task",
    "authors": ["N. Fernandez", "J.A. Fisteus", "L. Sanchez", "E. Martin."],
    "venue": "Proc. TAC 2010 Workshop.",
    "year": 2010
  }, {
    "title": "Tagme: on-thefly annotation of short text fragments (by wikipedia entities)",
    "authors": ["P. Ferragina", "U. Scaiella."],
    "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM ’10.",
    "year": 2010
  }, {
    "title": "Linking entities to a knowledge base with query expansion",
    "authors": ["S. Gottipati", "J. Jiang."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2011
  }, {
    "title": "A graphbased method for entity linking",
    "authors": ["Y. Guo", "W. Che", "T. Liu", "S. Li."],
    "venue": "Proc. IJCNLP2011.",
    "year": 2011
  }, {
    "title": "To link or not to link? a study on end-to-end tweet entity linking",
    "authors": ["S. Guo", "M. Chang", "E. Kiciman."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2013
  }, {
    "title": "Evaluating entity linking with wikipedia",
    "authors": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J. Curran."],
    "venue": "Artif. Intell.",
    "year": 2013
  }, {
    "title": "A generative entity-mention model for linking entities with knowledge base",
    "authors": ["X. Han", "L. Sun."],
    "venue": "Proc. ACL2011.",
    "year": 2011
  }, {
    "title": "Named entity disambiguation by leveraging wikipedia semantic knowledge",
    "authors": ["X. Han", "J. Zhao."],
    "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, CIKM 2009.",
    "year": 2009
  }, {
    "title": "Collective entity linking in web text: A graph-based method",
    "authors": ["X. Han", "L. Sun", "J. Zhao."],
    "venue": "Proc. SIGIR2011.",
    "year": 2011
  }, {
    "title": "Generating links to background knowledge: A case study using narrative radiology reports",
    "authors": ["J. He", "M. de Rijke", "M. Sevenster", "R. van Ommering", "Y. Qian."],
    "venue": "Proceedings of the 20th ACM international conference on Information and knowledge",
    "year": 2011
  }, {
    "title": "Efficient collective entity linking with stacking",
    "authors": ["Z. He", "S. Liu", "Y. Song", "M. Li", "M. Zhou", "H. Wang."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Resolving entity morphs in censored data",
    "authors": ["H. Huang", "Z. Wen", "D. Yu", "H. Ji", "Y. Sun", "J. Han", "H. Li."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
    "year": 2013
  }, {
    "title": "Overview of the tac 2010 knowledge base population track",
    "authors": ["H. Ji", "R. Grishman", "H.T. Dang", "K. Griffitt", "J. Ellis."],
    "venue": "Text Analysis Conference (TAC) 2010.",
    "year": 2010
  }, {
    "title": "Overview of the tac 2011 knowledge base population track",
    "authors": ["H. Ji", "R. Grishman", "H.T. Dang."],
    "venue": "Text Analysis Conference (TAC) 2011.",
    "year": 2011
  }, {
    "title": "Meta path-based collective classification in heterogeneous information networks",
    "authors": ["X. Kong", "P. Yu", "Y. Ding", "J. Wild."],
    "venue": "Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12.",
    "year": 2012
  }, {
    "title": "Class label enhancement via related instances",
    "authors": ["Z. Kozareva", "K. Voevodski", "S. Teng."],
    "venue": "Proc. EMNLP2011.",
    "year": 2011
  }, {
    "title": "Collective annotation of wikipedia entities in web text",
    "authors": ["S. Kulkarni", "A. Singh", "G. Ramakrishnan", "S. Chakrabarti."],
    "venue": "KDD.",
    "year": 2009
  }, {
    "title": "Mining evidences for named entity disambiguation",
    "authors": ["Y. Li", "C. Wang", "F. Han", "J. Han", "D. Roth", "X. Yan."],
    "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’13.",
    "year": 2013
  }, {
    "title": "Entity linking for tweets",
    "authors": ["X. Liu", "Y. Li", "H. Wu", "M. Zhou", "F. Wei", "Y. Lu."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
    "year": 2013
  }, {
    "title": "Overview of the tac 2009 knowledge base population track",
    "authors": ["P. McNamee", "H.T. Dang."],
    "venue": "Text Analysis Conference (TAC) 2009.",
    "year": 2009
  }, {
    "title": "Adding semantics to microblog posts",
    "authors": ["E. Meij", "W. Weerkamp", "M. de Rijke."],
    "venue": "Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12.",
    "year": 2012
  }, {
    "title": "Wikify!: linking documents to encyclopedic knowledge",
    "authors": ["R. Mihalcea", "A. Csomai."],
    "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07.",
    "year": 2007
  }, {
    "title": "Learning to link with wikipedia",
    "authors": ["D. Milne", "I.H. Witten."],
    "venue": "An effective, low-cost measure of semantic relatedness obtained from wikipedia links. the Wikipedia and AI Workshop of AAAI.",
    "year": 2008
  }, {
    "title": "Learning to link with wikipedia",
    "authors": ["D. Milne", "I.H. Witten."],
    "venue": "Proceeding of the 17th ACM conference on Information and knowledge management, pages 509–518. ACM.",
    "year": 2008
  }, {
    "title": "Word sense disambiguation using label propagation based semisupervised learning",
    "authors": ["Z. Niu", "D. Ji", "C. Tan."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).",
    "year": 2005
  }, {
    "title": "Entity extraction via ensemble semantics",
    "authors": ["M. Pennacchiotti", "P. Pantel."],
    "venue": "Proc. EMNLP2009.",
    "year": 2009
  }, {
    "title": "Cmcrc at tac10: Documentlevel entity linking with graph-based re-ranking",
    "authors": ["W. Radford", "B. Hachey", "J. Nothman", "M. Honnibal", "J.R. Curran."],
    "venue": "Proc. TAC 2010 Workshop.",
    "year": 2010
  }, {
    "title": "Learning-based multisieve co-reference resolution with knowledge",
    "authors": ["L. Ratinov", "D. Roth."],
    "venue": "EMNLP.",
    "year": 2012
  }, {
    "title": "Local and global algorithms for disambiguation to wikipedia",
    "authors": ["L. Ratinov", "D. Roth", "D. Downey", "M. Anderson."],
    "venue": "Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).",
    "year": 2011
  }, {
    "title": "Linking named entities in tweets with knowledge base via user interest modeling",
    "authors": ["W. Shen", "J. Wang", "P. Luo", "M. Wang."],
    "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’13.",
    "year": 2013
  }, {
    "title": "Kernels and regularization on graphs",
    "authors": ["A. Smola", "R. Kondor."],
    "venue": "COLT.",
    "year": 2003
  }, {
    "title": "Co-author relationship prediction in heterogeneous bibliographic networks",
    "authors": ["Y. Sun", "R. Barber", "M. Gupta", "C. Aggarwal", "J. Han."],
    "venue": "Proceedings of the 2011 International Conference on Advances in Social Networks Analysis and Mining, ASONAM",
    "year": 2011
  }, {
    "title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks",
    "authors": ["Y. Sun", "J. Han", "X. Yan", "P. Yu", "T. Wu."],
    "venue": "PVLDB, 4(11).",
    "year": 2011
  }, {
    "title": "New regularized algorithms for transductive learning",
    "authors": ["P. Talukdar", "K. Crammer."],
    "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD ’09.",
    "year": 2009
  }, {
    "title": "Classification of short texts by deploying topical annotations",
    "authors": ["D. Vitale", "P. Ferragina", "U. Scaiella."],
    "venue": "ECIR, pages 376–387.",
    "year": 2012
  }, {
    "title": "Web scale nlp: A case study on url word breaking",
    "authors": ["K. Wang", "C. Thrasher", "B. Hsu."],
    "venue": "Proceedings of the 20th International Conference on World Wide Web, WWW ’11.",
    "year": 2011
  }, {
    "title": "Entity linking leveraging automatically generated annotation",
    "authors": ["W. Zhang", "J. Su", "C. Tan", "W. Wang."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010).",
    "year": 2010
  }, {
    "title": "A wikipedia-lda model for entity linking with batch size changing",
    "authors": ["W. Zhang", "J. Su", "C.L. Tan."],
    "venue": "Proc. IJCNLP2011.",
    "year": 2011
  }, {
    "title": "Learning with local and global consistency",
    "authors": ["D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Schölkopf."],
    "venue": "Advances in Neural Information Processing Systems 16.",
    "year": 2004
  }, {
    "title": "Semisupervised learning using gaussian fields and harmonic functions",
    "authors": ["X. Zhu", "Z. Ghahramani", "J. Lafferty."],
    "venue": "ICML.",
    "year": 2003
  }],
  "id": "SP:8c9636125b555c2e482f78510d98c8dc85261f07",
  "authors": [{
    "name": "Hongzhao Huang",
    "affiliations": []
  }, {
    "name": "Yunbo Cao",
    "affiliations": []
  }, {
    "name": "Xiaojiang Huang",
    "affiliations": []
  }, {
    "name": "Heng Ji",
    "affiliations": []
  }, {
    "name": "Chin-Yew Lin",
    "affiliations": []
  }],
  "abstractText": "Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base (e.g., Wikipedia). Due to the shortness of a tweet, a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time. In addition, it is challenging to generate sufficient high quality labeled data for supervised models with low cost. To tackle these challenges, we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations. In order to identify semanticallyrelated mentions for collective inference, we detect meta path-based semantic relations through social networks. Compared to the state-of-the-art supervised model trained from 100% labeled data, our proposed approach achieves comparable performance with 31% labeled data and obtains 5% absolute F1 gain with 50% labeled data.",
  "title": "Collective Tweet Wikification based on Semi-supervised Graph Regularization"
}