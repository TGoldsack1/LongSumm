{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2015 Conference, pages 227–231, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Automated text summarization is an active field of research in various communities, including Information Retrieval, Natural Language Processing, and Text Mining.\nSome authors reduce summarization to the maximum coverage problem (Takamura and Okumura, 2009; Gillick and Favre, 2009) which, despite positive results, is known as NPhard (Khuller et al., 1999). Because linear programming (LP) helps to find an accurate approximated solution to this problem it has recently become very popular in the summarization field (Gillick and Favre, 2009; Woodsend and Lapata, 2010; Hitoshi Nishikawa and Kikui, 2010; Makino et al., 2011).\nTrying to solve a trade-off between summary quality and time complexity, we propose a summarization model solving the approximated maximum coverage problem by linear programming in\npolynomial time. We measure information coverage by an objective function and strive to obtain a summary that preserves its optimal value as much as possible. Three objective functions considering different metrics of information are introduced and evaluated. The main achievement of our method is a text representation model expanding a classic vector space model (Salton et al., 1975) to hyperplane and half-spaces and making it possible to represent an exponential number of extracts without computing them explicitly. This model also enables us to find the optimal extract by simple optimizing an objective function in polynomial time, using linear programming over rationals. For the first time, the frequent sequence mining was integrated with the maximal coverage approach in order to obtain a summary that best describes the summarized document. One of the introduced objective functions implements this idea.\nOur method ranks and extracts significant sentences into a summary, without any need in morphological text analysis. It was applied for both single-document (MSS) and multi-document (MMS) MultiLing 2015 summarization tasks, in three languages–English, Hebrew, and Arabic. In this paper we present experimental results in comparison with other systems that participated in the same tasks, using the same languages."
  }, {
    "heading": "2 Preprocessing and definitions",
    "text": "We are given a document or a set of related documents in UTF-8 encoding. Documents are split into sentences S1, ..., Sn. All sentences undergo tokenization, stop-word removal, and stemming. For some languages, stemming may be very basic or absent, and a list of stop-words may be unavailable. All these factors affect summarization quality.\nUnique stemmed words are called terms and are denoted by T1, ..., Tm. Every sentence is modeled as a sequence of terms from T1, ..., Tm where each\n227\nterm may appear zero or more times in a sentence. We are also given the desired number of words for a summary, denoted by MaxWords .\nThe goal of extractive summarization is to find a subset of sentences S1, ..., Sn that has no more than MaxWords words and conveys as much information as possible about the documents. Because it is difficult, or even impossible, to know what humans consider to be the best summary, we approximate the human decision process by optimizing certain objective functions over representation of input documents constructed according to our model. The number of words in a summary, sentences, and terms, are represented as constraints in our model."
  }, {
    "heading": "3 Polytope model",
    "text": ""
  }, {
    "heading": "3.1 Definitions",
    "text": "In the polytope model (Litvak and Vanetik, 2014) a document is viewed as an integer sentence-term matrix A = (aij), where aij denotes the number of appearances of term Tj in sentence Si. A row i of matrix A is used to define a linear constraint for sentence Si as follows:\nm∑ j=1 aijxij ≤ m∑ j=1 aij (1)\nEquation (1) also defines the lower half-space in Rmn corresponding to sentence Si. Together with additional constraints, such as a bound MaxWords on the number of words in the summary, we obtain a system of linear inequalities that describes the intersection of corresponding lower half-spaces of Rmn, forming a closed convex polyhedron called a polytope: ∑m j=1 aijxij ≤ ∑m j=1 aij , ∀i = 1..n\n0 ≤ xij ≤ 1, ∀i = 1..n, j = 1..m∑n i=1 ∑m j=1 aijxij ≤ MaxWords\n(2)\nAll possible extractive summaries are represented by vertices of the polytope defined in (2).\nIt remains only to define an objective function which optimum on the polytope boundary will define the summary we seek. Because such an optimum may be achieved not on a polytope vertex but rather on one of polytope faces (because we use linear programming over rationals), we need only to locate the vertex of a polytope closest to the point of optimum. This task is done by finding distances from the optimum to every one of the sentence hyperplanes and selecting those with\nminimal distance to the point of optimum. If there are too many candidate sentences, we give preference to those closest to the beginning of the document.\nThe main advantage of this model is the relatively low number of constraints (comparable with the number of terms and sentences in a document) and both the theoretical and practical polynomial running times of LP over rationals (Karmarkar, 1984)."
  }, {
    "heading": "3.2 Objective functions",
    "text": "In this section, we describe the objective functions we used in our system. Humans identify good summaries immediately, but specifying summary quality as a linear function of terms, sentences, and their parameters is highly nontrivial. In most cases, additional parameters, variables, and constraints must be added to the model."
  }, {
    "heading": "3.3 Maximal sentence relevance",
    "text": "The first objective function maximizes relevance of sentences chosen for a summary, while minimizing pairwise redundancy between them.\nWe define relevance cosrel i of a sentence Si as a cosine similarity between the sentence, viewed as a weighted vector of its terms, and the document. Relevance values are completely determined by the text and are not affected by choice of a summary. Every sentence Si is represented by a sentence variable:\nsi = ∑m j=1 aijxij/ ∑m j=1 aij (3)\nFormally, variable si represents the hyperplane bounding the lower half-space of Rmn related to sentence Si and bounding the polytope. Clearly, si assumes values in range [0, 1], where 0 means that the sentence is completely omitted from the summary and 1 means that the sentence is definitely chosen for the summary. Relevance of all sentences in the summary is described by the expression\nn∑ i=1 cosrel isi (4)\nRedundancy needs to be modeled and computed for every pair of sentences separately. We use additional redundancy variables red ij for every pair Si, Sj of sentences where i < j. Every one of these variables is 0 − 1 bounded and achieves a value of 1 only if both sentences are chosen for\nthe summary with the help of these constraints: 0 ≤ red ij ≤ 1, 0 ≤ i < j ≤ n red ij ≤ si, red ij ≤ sj si + sj − red ij ≤ 1 (5)\nThe numerical redundancy coefficient for sentences Si and Sj is their cosine similarity as term vectors, which we compute directly from the text and denote by cosred ij . The objective function we use to maximize relevance of the chosen sentences while minimizing redundancy is\nmax n∑\ni=1\ncosrel isi − n∑\ni=1 n∑ j=1 cosred ijred ij (6)"
  }, {
    "heading": "3.4 Sum of bigrams",
    "text": "The second proposed objective function maximizes the weighted sum of bigrams (consecutive term pairs appearing in sentences), where the weight of a bigram denotes its importance.\nThe importance count ij of a bigram (Ti, Tj) is computed as the number of its appearances in the document. It is quite possible that this bigram appears twice in one sentence, and once in another, and i = j is possible as well.\nIn order to represent bigrams, we introduce new bigram variables bgij for i, j = 1..m, covering all possible term pairs. An appearance of a bigram in sentence Sk is modeled by a 0 − 1 bounded variable bgkij , and c k ij denotes the number of times this bigram appears in sentence Sk. A bigram is represented by a normalized sum of its appearances in various sentences as follows:{\n0 ≤ bgkij ≤ 1, ∀i, j, k bgij = ∑n k=1 c k ijbg k ij/ ∑n k=1 c k ij\n(7)\nAdditionally, the appearance bgkij of a bigram in sentence Sk is tied to terms Ti and Tj composing it, with the help of variables xki and xkj denoting appearances of these terms in Sk:\nbgkij ≤ xki bgkij ≤ xkj xki + xkj − bgkij ≤ 1\n(8)\nThe constraints in (8) express the fact that a bigram cannot appear without the terms composing it, and appearance of both terms causes, in turn, the appearance of a bigram. Our objective function is:\nmax : m∑\ni=1 m∑ j=1 count ijbgij (9)"
  }, {
    "heading": "3.5 Maximal relevance with frequent itemsets",
    "text": "The third proposed objective function modifies the model so that only the most important terms are taken into account.\nLet us view each sentence Si as a sequence (Ti1, . . . , Tin) of terms, and the order of terms preserves the original word order of a sentence. Source documents are viewed as a database of sentences. Database size is n. Let s = (Ti1, . . . , Tik) be a sequence of terms of size k. Support of s in the database is the ratio of sentences containing this sequence, to the database size n.\nGiven a user-defined support bound S ∈ [0, 1], a term sequence s is frequent if support(s) ≥ S . Frequent term sequences can be computed by a multitude of existing algorithms, such as Apriori (Agrawal et al., 1994), FreeSpan (Han et al., 2000), GSP (Zaki, 2001), etc.\nIn order to modify the generic model described in (2), we first find all frequent sequences in the documents and store them in set F . Then we sort F first by decreasing sequence size and then by decreasing support, and finally we keep only top B sequences for a user-defined boundary B.\nWe modify the general model (2) by representing sentences as sums of their frequent sequences from F . Let F = {f1, . . . , fk}, sorted by decreasing size and then by decreasing support. A sentence Si is said to contain fj if it contains it as a term sequence and no part of fj in Si is covered by sequences f1, . . . , fj−1.\nLet count ij denote the number of times sentence Si contains frequent term sequence fj . Variables fij denote the appearance of sequence fj in sentence Si. We replace the polytope (2) by:{ ∑k\nj=1 count ijfij ≤ ∑k\nj=1 count ij , ∀i = 1..n 0 ≤ fij ≤ 1, ∀i = 1..n, j = 1..k\n(10) We add variables describing the relevance of each sentence by introducing sentence variables:\nsi = ∑k j=1 countijfij/ ∑k j=1 countij (11)\nDefining a boundary on the length of a summary now requires an additional constraint because frequent sequences do not contain all the terms in the sentences. Summary size is bounded as follows:\nn∑ i=1 lengthisi ≤ MaxWords (12)\nHere, lengthi is the exact word count of sentence Si.\nRelevance freqrel i of a sentence Si is defined as a cosine similarity between the vector of terms in Si covered by members of F , and the entire document. The difference between this approach and the one described in Section 3.3 is that only frequent terms are taken into account when computing sentence-document similarity. The resulting objective function maximizes relevance of chosen sentences while minimizing redundancy defined in (5):\nmax n∑\ni=1\nfreqrel isi − n∑\ni=1 n∑ j=1 cosred ijred ij (13)"
  }, {
    "heading": "4 Experiments",
    "text": "Tables 4, 4, and 1 contain the summarized results of automated evaluations for MultiLing 2015, single-document summarization (MSS) task for English, Hebrew, and Arabic corpora, respectively. The quality of the summaries is measured by ROUGE-1 (Recall, Precision, and Fmeasure).(Lin, 2004) We also demonstrate the absolute ranks of each submission–P-Rank, R-Rank, and F-Rank–when their scores are sorted by Precision, Recall, and F-measure, respectively. Only the best submissions (in terms of F-measure) for each participated system are presented and sorted in descending order of their F-measure scores. Two systems–Oracles and Lead–were used as topline and baseline summarizers, respectively. Oracles compute summaries for each article using the combinatorial covering algorithm in (Davis et al., 2012)–sentences were selected from a text to maximally cover the tokens in the human summary, using as few sentences as possible until its size exceeded the human summary, at which point it was truncated. Because Oracles can actually “see” the human summaries, it is considered as the optimal algorithm and its scores are the best scores that extractive approaches can achieve. Lead simply extracts the leading substring of the body text of the articles having the same length as the human summary of the article.\nBelow we summarize the comparative results for our summarizer (denoted in the following tables by Poly) in both tasks, in terms of Rouge-1, F-measure. For comparisons, we consider the best result out of 3 functions: coverage of frequent sequences for English and coverage of meaningful words for Hebrew and Arabic. English: 4th places out of 9 participants in both MSS and MMS tasks. Hebrew: 3rd place out of 7 and out of 9 partici-\npants in MSS and MMS tasks, respectively; and the highest recall score in MMS task. Arabic: 5th place out of 7 systems in MSS task, and 4th place out of 9 participants and the highest recall score in MMS task. As can be seen, the best performance for our summarizer has been achieved on the dataset of Hebrew documents. For example, only the top-line Oracles and the supervised MUSE summarizers outperformed our system in MSS task. Poly also outperformed Gillick (2009) model using ILP. The average running time for Poly is 500 ms per document."
  }, {
    "heading": "5 Conclusions and Future Work",
    "text": "In this paper we present an extractive summarization system based on a linear programming model. We represent the document as a set of intersecting hyperplanes. Every possible summary of a document is represented as the intersection of two or more hyperlanes. We consider the summary to be the best if the optimal value of the objective function is achieved during summarization. We introduce multiple objective functions describing the relevance of a sentence in terms of information coverage. The results obtained by automatic evaluation show that the introduced approach performs quite well for Hebrew and English. Only top-line and supervised summarizers outperform Poly on the Hebrew corpus. It is worth noting that our system is unsupervised and does not require annotated data, and it has polynomial running time."
  }],
  "year": 2015,
  "references": [{
    "title": "Fast algorithms for mining association rules",
    "authors": ["Rakesh Agrawal", "Ramakrishnan Srikant"],
    "venue": "In Proc. 20th int. conf. very large data bases, VLDB,",
    "year": 1994
  }, {
    "title": "OCCAMS – An Optimal Combinatorial Covering Algorithm for Multi-document Summarization",
    "authors": ["S.T. Davis", "J.M. Conroy", "J.D. Schlesinger."],
    "venue": "Proceedings of the IEEE 12th International Conference on Data Mining Workshops, pages 454–463.",
    "year": 2012
  }, {
    "title": "A Scalable Global Model for Summarization",
    "authors": ["Dan Gillick", "Benoit Favre."],
    "venue": "Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18.",
    "year": 2009
  }, {
    "title": "Freespan: frequent pattern-projected sequential pattern mining",
    "authors": ["Jiawei Han", "Jian Pei", "Behzad Mortazavi-Asl", "Qiming Chen", "Umeshwar Dayal", "Mei-Chun Hsu."],
    "venue": "Proceedings of the sixth ACM SIGKDD international conference on Knowl-",
    "year": 2000
  }, {
    "title": "Opinion Summarization with Integer Linear Programming Formulation for Sentence Extraction and Ordering",
    "authors": ["Yoshihiro Matsuo Hitoshi Nishikawa", "Takaaki Hasegawa", "Genichiro Kikui."],
    "venue": "Coling 2010: Poster Volume, pages",
    "year": 2010
  }, {
    "title": "New polynomial-time algorithm for linear programming",
    "authors": ["N. Karmarkar."],
    "venue": "Combinatorica, 4:373– 395.",
    "year": 1984
  }, {
    "title": "The budgeted maximum coverage problem",
    "authors": ["Samir Khuller", "Anna Moss", "Joseph (Seffi) Naor"],
    "venue": "Information Precessing Letters,",
    "year": 1999
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004), pages 25–26.",
    "year": 2004
  }, {
    "title": "Efficient summarization with polytopes",
    "authors": ["Marina Litvak", "Natalia Vanetik."],
    "venue": "Innovative Document Summarization Techniques: Revolutionizing Knowledge Understanding: Revolutionizing Knowledge Understanding, page 54.",
    "year": 2014
  }, {
    "title": "Balanced coverage of aspects for text summarization",
    "authors": ["Takuya Makino", "Hiroya Takamura", "Manabu Okumura."],
    "venue": "TAC ’11: Proceedings of Text Analysis Conference.",
    "year": 2011
  }, {
    "title": "A vectorspace model for information retrieval",
    "authors": ["G. Salton", "C. Yang", "A. Wong."],
    "venue": "Communications of the ACM, 18.",
    "year": 1975
  }, {
    "title": "Text summarization model based on maximum coverage problem and its variant",
    "authors": ["Hiroya Takamura", "Manabu Okumura."],
    "venue": "EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,",
    "year": 2009
  }, {
    "title": "Automatic Generation of Story Highlights",
    "authors": ["Kristian Woodsend", "Mirella Lapata."],
    "venue": "ACL ’10: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565– 574.",
    "year": 2010
  }, {
    "title": "Spade: An efficient algorithm for mining frequent sequences",
    "authors": ["Mohammed J Zaki."],
    "venue": "Machine learning, 42(1-2):31–60. 231",
    "year": 2001
  }],
  "id": "SP:c9e097d9e8ae2bd6d4dfaf27417aab6be131f444",
  "authors": [{
    "name": "Natalia Vanetik",
    "affiliations": []
  }, {
    "name": "Marina Litvak",
    "affiliations": []
  }],
  "abstractText": "The problem of extractive text summarization for a collection of documents is defined as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way. In this paper we describe the linear programming-based global optimization model to rank and extract the most relevant sentences to a summary. We introduce three different objective functions being optimized. These functions define a relevance of a sentence that is being maximized, in different manners, such as: coverage of meaningful words of a document, coverage of its bigrams, or coverage of frequent sequences of words. We supply here an overview of our system’s participation in the MultiLing contest of SIGDial 2015.",
  "title": "Multilingual Summarization with Polytope Model"
}