{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Convolutional neural networks (CNNs) (LeCun et al., 1998) have proven successful across many tasks including image classification (LeCun et al., 1998; Krizhevsky et al., 2012), face recognition (Lawrence et al., 1997), speech recognition (Hinton et al., 2012), text classification (Wang et al., 2012), and game playing (Mnih et al., 2015; Silver et al., 2016). There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity—each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing—the same filter is applied to each patch.\nHowever, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard (Blum\n1Stanford University, CA, USA 2University of California, Berkeley, CA, USA. Correspondence to: Yuchen Zhang <zhangyuc@cs.stanford.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n& Rivest, 1992). In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation (Bottou, 1998). This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper (Fahlman, 1988)), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.\nIn this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying two-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by the paper (Zhang et al., 2016a), which put forth a relaxation for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Relaxing this low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs.\nOn the theoretical front, we prove an oracle inequality on the generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite data—a quantity to which we refer as the oracle risk—plus a model complexity term that decays to zero polynomially in the sample size. Our results suggest that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network (Zhang et al., 2016a), highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. Finally, we apply CCNNs to the MNIST handwritten digit dataset\nas well as four variation datasets (VariationsMNIST), and find that it achieves state-of-the-art accuracy.\nRelated work. With the empirical success of deep neural networks, there has been an increasing interest in understanding its connection to convex optimization. Bengio et al. (2005) showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. Aslan et al. (2013; 2014) propose a method for learning multi-layer latent-variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning fullyconnected neural networks.\nPast work has studied learning translation-invariant features without backpropagation. Mairal et al. (2014) present convolutional kernel networks. They propose a translationinvariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we provide for CCNNs in this paper, even for learning one convolutional layer. The ScatNet method (Bruna & Mallat, 2013) uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, in contrast to CCNNs. Daniely et al. (2016) show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from random initialization.\nNotation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let ‖A‖∗ be its nuclear norm, ‖A‖2 be its spectral norm (i.e., maximal singular value), and ‖A‖F be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that ∑∞ `=1 v 2 ` < ∞. For any vectors u, v ∈ `2(N),\nthe inner product 〈u, v〉 := ∑∞ `=1 uivi and the `2-norm\n‖u‖2 := √ 〈u, u〉 are well defined."
  }, {
    "heading": "2. Background and problem setup",
    "text": "In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem."
  }, {
    "heading": "2.1. Convolutional neural networks",
    "text": "At a high level, a two-layer CNN1 is a function that maps an input vector x ∈ Rd0 (e.g., an image) to an output vector in y ∈ Rd2 (e.g., classification scores for d2 classes). This mapping is formed in the following manner:\n1Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4.\n• First, we extract a collection of P vectors {zp(x)}Pp=1 of the full input vector x. Each vector zp(x) ∈ Rd1 is referred to as a patch, and these patches may depend on overlapping components of x. • Second, given some choice of activation function σ : R→ R and a collection of weight vectors {wj}rj=1 in Rd1 , we define the functions\nhj(z) := σ(w > j z) for each patch z ∈ Rd1 . (1)\nEach function hj (for j ∈ [r]) is known as a filter, and note that the same filters are applied to each patch—this corresponds to the parameter sharing of a CNN. • Third, for each patch index p ∈ [P ], filter index j ∈ [r], and output coordinate k ∈ [d2], we introduce a coefficient αk,j,p ∈ R that governs the contribution of the filter hj on patch zp(x) to output fk(x). The final form of the CNN is given by f(x) : = (f1(x), . . . , fd2(x)), where the kth component is given by\nfk(x) := r∑ j=1 P∑ p=1 αk,j,phj(zp(x)). (2)\nTaking the patch functions {zp}Pp=1 and activation function σ as fixed, the parameters of the CNN are the filter vectors w := {wj ∈ Rd1 : j ∈ [r]} along with the collection of coefficient vectors α := {αk,j ∈ RP : k ∈ [d2], j ∈ [r]}. We assume that all patch vectors zp(x) ∈ Rd1 are contained in the unit `2-ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant γ > 0 to every patch zp(x) and multiplying 1/γ to the filter vectors w, this assumption holds without changing the the output of the network.\nGiven some positive radii B1 and B2, we consider the model class\nFcnn(B1, B2) := { f of the form (2) : max\nj∈[r] ‖wj‖2 ≤ B1\nand max k∈[d2],j∈[r]\n‖αk,j‖2 ≤ B2 } . (3)\nWhen the radii (B1, B2) are clear from context, we adopt Fcnn as a convenient shorthand."
  }, {
    "heading": "2.2. Empirical risk minimization.",
    "text": "Given an input-output pair (x, y) and a CNN f , we let L(f(x); y) denote the loss incurred when the output y is predicted via f(x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2, . . . , d2}. For example, given a vector f(x) = (f1(x), . . . , fd2(y)) ∈ Rd2 of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f(x); y) := −fy(x) +\nlog (∑d2 y′=1 exp(fy′(x)) ) .\nGiven n training examples {(xi, yi)}ni=1, we would like to compute an empirical risk minimizer:\nf̂cnn ∈ arg min f∈Fcnn n∑ i=1 L(f(xi); yi). (4)\nRecalling that functions f ∈ Fcnn depend on the parameters w and α in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class Fcnn for which empirical risk minimization is convex."
  }, {
    "heading": "3. Convexifying CNNs",
    "text": "We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting."
  }, {
    "heading": "3.1. Linear activation functions: low rank relaxations",
    "text": "In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function σ(t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = 〈zp(x), wj〉. For each x ∈ Rd0 , we first define the P × d1-dimensional matrix\nZ(x) := z1(x) >\n... zP (x) >  . (5) We also define the P -dimensional vector αk,j := (αk,j,1, . . . , αk,j,P )\n>. With this notation, we can rewrite equation (2) for the kth output as\nfk(x) = r∑ j=1 P∑ p=1 αk,j,p〈zp(x), wj〉 = r∑ j=1 α>k,jZ(x)wj\n= tr ( Z(x) ( r∑ j=1 wjα > k,j )) = tr(Z(x)Ak), (6)\nwhere in the final step, we have defined the d1 × P - dimensional matrix Ak := ∑r j=1 wjα > k,j . Observe that fk now depends linearly on the matrix parameter Ak. Moreover, the matrixAk has rank at most r, due to the parameter\nsharing of CNNs. See Figure 1 for a graphical illustration of this model structure.\nLetting A := (A1, . . . , Ad2) be a concatenation of these matrices across all d2 output coordinates, we can then define a function fA : Rd1 → Rd2 of the form\nfA(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2)). (7)\nNote that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define Fcnn(B1, B2) to be the set of functions fA which satisfies: (C1) maxj∈[r] ‖wj‖2 ≤ B1, maxk∈[d2],j∈[r] ‖αk,j‖2 ≤ B2; and (C2) rank(A) = r. This is simply an alternative formulation of our original class of CNNs defined in equation (3). Notice that if the filter weights wj are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns. The column space of matrix A contains the convolution parameters {wj}, and the row space of A contains to the output parameters {αk,j}.\nThe matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation is based on the nuclear norm ‖A‖∗ corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as ‖A‖∗ ≤ B1B2r √ d2. Consequently, if we define the function class\nFccnn := { fA : ‖A‖∗ ≤ B1B2r √ d2 } , (8)\nthen we are guaranteed that Fccnn ⊇ Fcnn.\nWe propose to minimize the empirical risk (4) over Fccnn instead of Fcnn; doing so defines a convex optimization problem over this richer class of functions\nf̂ccnn := arg min fA∈Fccnn n∑ i=1 L(fA(xi); yi). (9)\nIn Section 3.3, we describe iterative algorithms that can be used to solve this convex problem in the more general setting of nonlinear activation functions."
  }, {
    "heading": "3.2. Nonlinear activations: RKHS filters",
    "text": "For nonlinear activation functions σ, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we will show, this relaxation allows us to reduce the problem to the linear activation case.\nLet K : Rd1 × Rd1 → R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaus-\nsian RBF kernel) and a sufficiently smooth activation function σ, we are able to show that the filter h : z 7→ σ(〈w, z〉) is contained in the RKHS induced by the kernel functionK. See Section 3.4 for the choice of the kernel function and the activation function. Let S := {zp(xi) : p ∈ [P ], i ∈ [n]} be the set of patches in the training dataset. The representer theorem then implies that for any patch zp(xi) ∈ S, the function value can be represented by\nh(zp(xi)) = ∑\n(i′,p′)∈[n]×[P ]\nci′,p′K(zp(xi), zp′(xi′)) (10)\nfor some coefficients {ci′,p′}(i′,p′)∈[n]×[P ]. Filters of the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7→ K(z, zp′(xi′)). Such filters are parametrized by a finite set of coefficients {ci′,p′}(i′,p′)∈[n]×[P ], which can be estimated via empirical risk minimization.\nLet K ∈ RnP×nP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair (i, p) ∈ [n] × [P ]. The entry at row (i, p) and column (i′, p′) of matrix K is equal to K(zp(xi), zp′(xi′)). So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ> of the kernel matrix, where Q ∈ RnP×m; one example is the Cholesky factorization with m = nP . We can interpret each row Q(i,p) ∈ Rm as a feature vector in place of the original zp(xi) ∈ Rd1 , and rewrite equation (10) as\nh(zp(xi)) = 〈Q(i,p), w〉 where w := ∑ (i′,p′) ci′,p′Q(i′,p′).\nIn order to learn the filter h, it suffices to learn the mdimensional vector w. To do this, define patch matrices Z(xi) ∈ RP×m for each i ∈ [n] so that its p-th row is Q(i,p). Then the problem reduces to learning a linear filter with coefficient vector w. Carrying out all of Sec-\ntion 3.1, solving the ERM gives us a parameter matrix A ∈ Rm×Pd2 . The only difference is that `2-norm constraint (C1) needs to be adapted to the norm of the RKHS. See Appendix B for details.\nAt test time, given a new input x ∈ Rd0 , we can compute a patch matrix Z(x) ∈ RP×m as follows: • The p-th row of this matrix is the feature vector for\npatch p, which is equal to Q†v(zp(x)) ∈ Rm, where for any patch z, the vector v(z) is defined as a nP - dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). We note that if x is an instance xi in the training set, then the vector Q†v(zp(x)) is exactly equal to Q(i,p). Thus the mapping Z(x) applies to both training and testing.\n• We can then compute the predictor fk(x) = tr(Z(x)Ak) via equation (6). Note that we do not explicitly need to compute the filter values hj(zp(x)) to compute the output under the CCNN.\nRetrieving filters. When we learn multi-layer CCNNs (Section 4), we need to compute the filters hj explicitly in order to form the inputs to the next layer. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrixA, we compute a rankr approximation A ≈ Û V̂ >. Then set the j-th filter hj to the mapping\nz 7→ 〈Ûj , Q†v(z)〉 for any patch z ∈ Rd1 , (11)\nwhere Ûj ∈ Rm is the j-th column of matrix Û , and Q†v(z) represents the feature vector for patch z.2 The matrix V̂ > encodes parameters of the output layer, thus\n2If z is a patch in the training set, namely z = zp(xi), then we have equation Q†v(z) = Q(i,p)\nAlgorithm 1 Learning two-layer CCNNs Input: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r. 1. Construct a kernel matrix K ∈ RnP×nP such that the entry\nat column (i, p) and row (i′, p′) is equal to K(zp(xi), zp′(xi′)). Compute a factorization K = QQ> or an approximation K ≈ QQ>, where Q ∈ RnP×m.\n2. For each xi, construct patch matrix Z(xi) ∈ RP×m whose p-th row is the (i, p)-th row of Q, where Z(·) is defined in Section 3.2. 3. Solve the following optimization problem to obtain a matrix Â = (Â1, . . . , Âd2):\nÂ ∈ argmin ‖A‖∗≤R L̃(A) where (12)\nL̃(A) := n∑\ni=1\nL (( tr(Z(xi)A1), . . . , tr(Z(xi)Ad2) ) ; yi ) .\n4. Compute a rank-r approximation Û V̂ > ≈ Â where Û ∈ Rm×r and V̂ ∈ RPd2×r .\nOutput: predictor f̂ccnn(x) := ( tr(Z(x)Â1), . . . , tr(Z(x)Âd2) ) and the convolutional layer output H(x) := Û>Z(x)>.\ndoesn’t appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rank-r approximation of the matrix A is not unique. The heuristic we suggest is to form the singular value decomposition A = UΛV >, then define Û to be the first r columns of U .\nWhen we apply all of the r filters to all patches of an input x ∈ Rd0 , the resulting output is H(x) := Û>Z(x)> — this is an r×P matrix whose element at row j and column p is equal to hj(zp(x))."
  }, {
    "heading": "3.3. Algorithm",
    "text": "The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest approach is via projected gradient descent: At iteration t, using a step size ηt > 0, we form the new matrix At+1 based on the previous iterate At according to:\nAt+1 = ΠR ( At − ηt ∇AL̃(At) ) . (13)\nHere ∇AL̃ denotes the gradient of the objective function defined in (12), and ΠR denotes the Euclidean projection onto the nuclear norm ball {A : ‖A‖∗ ≤ R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the `1-ball. This latter projection step can be carried out efficiently by the algorithm of Duchi et al. (2008). There are other efficient optimiza-\ntion algorithms (Duchi et al., 2011; Xiao & Zhang, 2014) for solving the problem (12). All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples.\nThe computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation (Drineas & Mahoney, 2005) or random feature approximation (Rahimi & Recht, 2007); both are randomized methods to obtain a tall-and-thin matrix Q ∈ RnP×m such thatK ≈ QQ>. Typically, the parameterm is chosen to be much smaller than nP . In order to compute the matrix Q, the Nyström approximation method takes O(m2nP ) time. The random feature approximation takesO(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform (Le et al., 2013). The time complexity of project gradient descent also scales with m rather than with nP ."
  }, {
    "heading": "3.4. Theoretical results",
    "text": "In this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d2 = 1.3\nThe learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function of the following form: z 7→ q(〈w, z〉), where q is an arbitrary polynomial function and w ∈ Rd1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel:\nK(z, z′) := 1 2− 〈z, z′〉 , ‖z‖2 ≤ 1, ‖z′‖2 ≤ 1. (14)\nThis kernel was studied by Shalev-Shwartz et al. (2011) for learning halfspaces, and by Zhang et al. (2016a) for learning fully-connected neural networks. We also consider the Gaussian RBF kernel:\nK(z, z′) := e−γ‖z−z ′‖22 , ‖z‖2 = ‖z′‖2 = 1. (15)\nAs shown by Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis.\nLet f̂ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of f̂ccnn is comparable to that of the best CNN model. In particular, we consider the following types of activation functions σ:\n3We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case.\n(a) arbitrary polynomial functions (e.g., used by Chen & Manning (2014); Livni et al. (2014)).\n(b) sinusoid activation function σ(t) := sin(t) (e.g., used by Sopena et al. (1999); Isa et al. (2010)).\n(c) erf function σerf(t) := 2/ √ π ∫ t 0 e−z 2\ndz, which represents a close approximation to the sigmoid function (Zhang et al., 2016a).\n(d) a smoothed hinge loss σsh(t) := ∫ t −∞ 1 2 (σerf(z) +\n1)dz, which represents a close approximation to the ReLU function (Zhang et al., 2016a).\nTo understand how these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: σ(t) = ∑∞ j=0 ajt\nj , and note that the smoothness of these functions are characterized by the rate of their coefficients {aj}∞j=0 converging to zero. If σ is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function σ. If σ is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {aj}∞j=0 converge quickly enough to zero (the criterion depends on the concrete choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: thus, (a), (b), (c), and (d) are all are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, so only (a) and (b) are valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough to be contained in the RKHS.\nWe are ready to state the main theoretical result. In the theorem statement, we use K(X) ∈ RP×P to denote the random kernel matrix obtained from an input vector X ∈ Rd0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)).\nTheorem 1. Assume that the loss function L(·; y) is LLipchitz continuous for every y ∈ [d2] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function σ, there is a constantCσ(B1) such that by choosing hyper-parameterR := Cσ(B1)B2r in Algorithm 1, the expected generalization error is at most\nEX,Y [L(f̂ccnn(X);Y )] ≤ inf f∈Fcnn EX,Y [L(f(X);Y )]\n+ c LCσ(B1)B2r √ log(nP ) EX [‖K(X)‖2]√\nn , (16)\nwhere c > 0 is a universal constant.\nProof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that con-\ntains the class of CNNs. This function class is defined as:\nFccnn := { x 7→ r∗∑ j=1 P∑ p=1 αj,phj(zp(x)) : r ∗ <∞ (17)\nand r∗∑ j=1 ‖αj‖2‖hj‖H ≤ Cσ(B1)B2d2 } . (18)\nwhere ‖·‖H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the `2-norm bounds on the weight vectors are replaced by a single constraint on ‖αj‖2 and ‖hj‖H. We prove the following property for the predictor f̂ccnn: it must be an empirical risk minimizer ofFccnn. This property holds even though equation (18) defines a non-parametric function class Fccnn, while Algorithm 1 optimizes f̂ccnn in a parametric function class.\nSecond, we characterize the Rademacher complexity of this new function class Fccnn, proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory (Bartlett & Mendelson, 2003), we conclude that the generalization loss of f̂ccnn converges to the least possible generalization error of Fccnn. The latter loss is bounded by the generalization loss of CNNs (because Fcnn ⊆ Fccnn), which establishes the theorem. See the full version of this paper (Zhang et al., 2016b) for a rigorous proof of Theorem 1.\nRemark on activation functions. It is worth noting that the quantity Cσ(B1) depends on the activation function σ, and more precisely, depends on the convergence rate of the polynomial expansion of σ. Appendix A shows that if σ is a polynomial function of degree `, then Cσ(B1) = O(B`1). If σ is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity Cσ(B1) will be exponential in B1. From an algorithmic perspective, we don’t need to know the activation function for executing Algorithm 1. From a theoretical perspective, however, the choice of σ is relevant from the point of Theorem 1 to compare f̂ccnn with the best CNN, whose representation power is characterized by the choice of σ. Therefore, if a CNN with a low-degree polynomial σ performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification.\nRemark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights wj,p for each filter index j and patch index p. With this change,\nthe new CNN output (2) is\nf(x) = r∑ j=1 P∑ p=1 αj,pσ(w > j,pzp(x)),\nwhere αj,p ∈ R and wj,p ∈ Rd1 . Note that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by Zhang et al. (2016a). Their paper shows that under the norm constraints ‖wj‖2 ≤ B′1 and∑r j=1 ∑P p=1 |αj,p| ≤ B′2, the excess risk of the recur-\nsive kernel method is at most O(LCσ(B′1)B′2 √ Kmax/n), where Kmax = maxz:‖z‖2≤1K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class Fcnn, we have B′1 = B1 and B′2 = B2r √ P . Thus, the expected risk of the estimated f̂ is bounded by:\nEX,Y [L(f̂(X);Y )] ≤ inf f∈Fcnn EX,Y [L(f(X);Y )]\n+ c LCσ(B1)B2r √ PKmax√\nn . (19)\nComparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of √ P Kmax versus √ E[‖K(X)‖2]. Since the matrix K(X) is P -dimensional, we have\n‖K(X)‖2 ≤ max p∈[P ] ∑ q∈[P ] |K(zp(X), zq(X))| ≤ P Kmax.\nThis demonstrates that √ P Kmax is always greater than√\nE[‖K(X)‖2]. In general, the first term can be up to factor of √ P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference is intuitive given that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesn’t rigorously show that one method is better than the other, it gives intuition for understanding the importance of parameter sharing."
  }, {
    "heading": "4. Learning multi-layer CCNNs",
    "text": "In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs.\nAverage pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P × r to dimensions P ′ × r with P ′ < P . For the CCNN model, if we apply average pooling after\nAlgorithm 2 Learning multi-layer CCNNs Input:Data {(xi, yi)}ni=1, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . ,m: • Train a two-layer network by Algorithm 1, taking {(Hs−1(xi), yi)}ni=1 as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and f̂s be the predictor. Output: Predictor f̂m and the top layer output Hm.\nthe convolutional layer, then the k-th output of the CCNN model becomes tr(GZ(x)Ak) where G ∈ RP\n′×P is the pooling matrix. Thus, performing a pooling operation requires only replacing every matrix Z(xi) in problem (12) by the pooled matrix GZ(xi). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P ′-fold.\nProcessing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x ∈ RC×d0 . The c-th row of matrix x, denoted by x[c] ∈ Rd0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel:\nzp(x) := (zp(x[1]), . . . , zp(x[C])) ∈ RCd1 . Then we construct the feature matrix Z(x) using the concatenated patch vectors {zp(x)}Pp=1. From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form σ( ∑C c=1〈wc, zp(x[c])〉), parametrized by the vectors {wc}Cc=1.\nMulti-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs, summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as input—note that this consists of r channels (one from each previous filter); thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we compare the CCNN approach with other methods on the MNIST dataset and more challenging variations (VariationsMNIST), including adding white noise (rand), random rotation (rot), random image back-\nground (img) or combining the last two (img+rot). For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations (VariationsMNIST).\nFor the CCNN method and the baseline CNN method, we train two-layer and three-layer models respectively. The models with k convolutional layers are denoted by CCNNk and CNN-k. Each convolutional layer is constructed on 5 × 5 patches with unit stride, followed by 2 × 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use Gaussian kernel for the CCNN. The feature matrix Z(x) is constructed via random feature approximation (Rahimi & Recht, 2007) with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening (Coates et al., 2010). The convex optimization problem is solved by projected SGD with mini-batches of size 50. Code and reproducible experiments are available on the CodaLab platform4.\nAs a baseline approach, the CNN models are activated by the ReLU function σ(t) = max{0, t} or the quadratic function σ(t) = t2. We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening (Srivastava et al., 2014). We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVMrbf ) and a fully connected neural network with one hidden layer (NN1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) (Sohn & Lee, 2012), the stacked denoising auto-encoder with\n4http://worksheets.codalab.org/ worksheets/0x1468d91a878044fba86a5446f52aacde/\nthree hidden layers (SDAE-3) (Vincent et al., 2010), the ScatNet-2 model (Bruna & Mallat, 2013) and the PCANet2 model (Chan et al., 2015).\nTable 1 shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the errors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of local filters and parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets. Further adding a third convolutional layer doesn’t notibly improve the performance on these datasets.\nIn Section 3.4, we showed that if the activation function σ is a polynomial function, then the CCNN (which does not depend on σ) requires lower sample complexity to match the performance of the best possible CNN using σ. More precisely, if σ is a degree-` polynomial, then Cσ(B) in the upper bound will be controlled by O(B`). This motivates us to study the performance of low-degree polynomial activations. Table 1 shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good.\nAcknowledgements. MJW and YZ were partially supported by the Office of Naval Research Grant DOD ONRN00014 and the NSF Grant NSF-DMS-1612948. PL and YZ were partially supported by the Microsoft Faculty Fellowship."
  }],
  "year": 2017,
  "references": [{
    "title": "Convex two-layer modeling",
    "authors": ["Aslan", "Özlem", "Cheng", "Hao", "Zhang", "Xinhua", "Schuurmans", "Dale"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Convex deep learning via normalized kernels",
    "authors": ["Aslan", "Özlem", "Zhang", "Xinhua", "Schuurmans", "Dale"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Rademacher and Gaussian complexities: Risk bounds and structural results",
    "authors": ["Bartlett", "Peter L", "Mendelson", "Shahar"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "Convex neural networks",
    "authors": ["Bengio", "Yoshua", "Roux", "Nicolas L", "Vincent", "Pascal", "Delalleau", "Olivier", "Marcotte", "Patrice"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2005
  }, {
    "title": "Training a 3-node neural network is NP-complete",
    "authors": ["Blum", "Avrim L", "Rivest", "Ronald L"],
    "venue": "Neural Networks,",
    "year": 1992
  }, {
    "title": "Online learning and stochastic approximations",
    "authors": ["Bottou", "Léon"],
    "venue": "On-line learning in neural networks,",
    "year": 1998
  }, {
    "title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence",
    "authors": ["Bruna", "Joan", "Mallat", "Stéphane"],
    "venue": "IEEE Transactions on,",
    "year": 2013
  }, {
    "title": "Pcanet: A simple deep learning baseline for image classification",
    "authors": ["Chan", "Tsung-Han", "Jia", "Kui", "Gao", "Shenghua", "Lu", "Jiwen", "Zeng", "Zinan", "Ma", "Yi"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2015
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Chen", "Danqi", "Manning", "Christopher D"],
    "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2014
  }, {
    "title": "An analysis of single-layer networks in unsupervised feature learning",
    "authors": ["Coates", "Adam", "Lee", "Honglak", "Ng", "Andrew Y"],
    "venue": "Ann Arbor,",
    "year": 2010
  }, {
    "title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
    "authors": ["Daniely", "Amit", "Frostig", "Roy", "Singer", "Yoram"],
    "venue": "arXiv preprint arXiv:1602.05897,",
    "year": 2016
  }, {
    "title": "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
    "authors": ["Drineas", "Petros", "Mahoney", "Michael W"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Efficient projections onto the `1-ball for learning in high dimensions",
    "authors": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"],
    "venue": "In Proceedings of the 25th International Conference on Machine Learning,",
    "year": 2008
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "An empirical study of learning speed in back-propagation networks",
    "authors": ["Fahlman", "Scott E"],
    "venue": "Journal of Heuristics,",
    "year": 1988
  }, {
    "title": "Suitable mlp network activation functions for breast cancer and thyroid disease detection",
    "authors": ["IS Isa", "Z Saad", "S Omar", "MK Osman", "KA Ahmad", "Sakim", "HA Mat"],
    "venue": "In 2010 Second International Conference on Computational Intelligence, Modelling and Simulation,",
    "year": 2010
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Face recognition: A convolutional neuralnetwork approach",
    "authors": ["Lawrence", "Steve", "Giles", "C Lee", "Tsoi", "Ah Chung", "Back", "Andrew D"],
    "venue": "Neural Networks, IEEE Transactions on,",
    "year": 1997
  }, {
    "title": "Fastfoodapproximating kernel expansions in loglinear time",
    "authors": ["Le", "Quoc", "Sarlós", "Tamás", "Smola", "Alex"],
    "venue": "In Proceedings of the International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "On the computational efficiency of training neural networks",
    "authors": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Convolutional kernel networks",
    "authors": ["Mairal", "Julien", "Koniusz", "Piotr", "Harchaoui", "Zaid", "Schmid", "Cordelia"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Random features for large-scale kernel machines",
    "authors": ["Rahimi", "Ali", "Recht", "Benjamin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2007
  }, {
    "title": "Learning kernel-based halfspaces with the 01 loss",
    "authors": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Sridharan", "Karthik"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Learning invariant representations with local transformations",
    "authors": ["Sohn", "Kihyuk", "Lee", "Honglak"],
    "venue": "In Proceedings of the 29th International Conference on Machine Learning",
    "year": 2012
  }, {
    "title": "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems",
    "authors": ["Sopena", "Josep M", "Romero", "Enrique", "Alquezar", "Rene"],
    "venue": "In ICANN",
    "year": 1999
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "End-to-end text recognition with convolutional neural networks",
    "authors": ["Wang", "Tao", "Wu", "David J", "Coates", "Andrew", "Ng", "Andrew Y"],
    "venue": "In Pattern Recognition (ICPR),",
    "year": 2012
  }, {
    "title": "A proximal stochastic gradient method with progressive variance reduction",
    "authors": ["Xiao", "Lin", "Zhang", "Tong"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "`1regularized neural networks are improperly learnable in polynomial time",
    "authors": ["Zhang", "Yuchen", "Lee", "Jason D", "Jordan", "Michael I"],
    "venue": "In Proceedings on the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Convexified convolutional neural networks. CoRR, abs/1609.01000, 2016b. URL http://arxiv.org/ abs/1609.01000",
    "authors": ["Zhang", "Yuchen", "Liang", "Percy", "Wainwright", "Martin J"],
    "year": 2016
  }],
  "id": "SP:49fe0c4e086289cc2cd6582e99e239f89ec1f5f0",
  "authors": [{
    "name": "Yuchen Zhang",
    "affiliations": []
  }, {
    "name": "Percy Liang",
    "affiliations": []
  }, {
    "name": "Martin J. Wainwright",
    "affiliations": []
  }],
  "abstractText": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented in terms of a lowrank matrix, and the rank constraint can be relaxed so as to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layerwise manner. Empirically, we find that CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.",
  "title": "Convexified Convolutional Neural Networks"
}