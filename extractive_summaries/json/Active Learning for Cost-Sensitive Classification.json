{
  "sections": [{
    "text": "Keywords Active learning · Non-myopic · Cost-sensitive · Unequal misclassification costs · Misclassification loss · Imbalanced data · Uncertainty sampling · Error reduction\nEditors: João Gama, Indrė Žliobaitė, Alípio M. Jorge, and Concha Bielza.\nB Georg Krempl georg.krempl@ovgu.de; georg.krempl@iti.cs.uni-magdeburg.de\nDaniel Kottke daniel.kottke@iti.cs.uni-magdeburg.de\nVincent Lemaire vincent.lemaire@orange.com\n1 KMD Lab, University Magdeburg, Magdeburg, Germany\n2 Orange Labs, Lannion, France"
  }, {
    "heading": "1 Introduction",
    "text": "The volume of automatically generated data increases constantly (Gantz and Reinsel 2012) but human annotation capacities remain limited. Learning from large pools or fast streams of unlabelled data, yet scarce and expensive labelled data, requires the development of fast and efficient active learning algorithms (Gopalkrishnan et al. 2012; Krempl et al. 2014). Such algorithms actively construct a training set, rather than passively processing a given one. Their objective is to select among the unlabelled instances (candidates) the ones for labelling that are deemed tomaximise the classification performance the most (Settles 2012), thus focusing annotation efforts to the most valuable candidates. While fast active learning itself poses a challenge, an even bigger one is cost-sensitivity1 (Liu et al. 2009), where misclassification costs differ between classes, as for example in the diagnosis of rare but dangerous ailments in patients (Attenberg and Ertekin 2013), where classifying a sick patient as healthy incurs more severe consequences than classifying a healthy one as sick.\nWe address these challenges by presenting a novel, fast probabilistic active learning approach, which is suitable for binary classification, both under equal and non-equal misclassification costs. This probabilistic (Krempl et al. 2014a, b) active learning approach computes the expected performance gain, thereby considering both a candidate’s label realisation and the true posterior of the positive class in the candidate’s neighbourhood. The latter directly incorporates the likelihoods of different possible posteriors under the already labelled data. This advances most state-of-the-art decision-theoretic active learning literature (e.g. Freytag et al. 2013; Garnett et al. 2012), which considers solely the most likely (or most pessimistic) posterior.\nWe make three important contributions beyond (Krempl et al. 2014a, b) and other works: First, we optimise label selection for the minimisation of misclassification loss, a costsensitive performance measure (Hand 2009). Second, we derive a fast, closed-form solution for calculating the probabilistic gain of an instance. We show that this yields a myopic active learning approach with the same linear asymptotic computational time as uncertainty sampling, which is one of the fastest available approaches (Settles 2012, p. 64). Third, we propose a non-myopic extension of our optimised probabilistic active learning (OPAL) approach, where the myopic, isolated view on the value of each label is exchanged for considering the possible remaining number of similar label acquisitions under a given budget. We show that this provides an advantage in particular in cost-sensitive settings, while it requires solely additional time of a factor O(m · logm) of the budget size. In practical applications, neither this budget size, which for example results from limited human annotation capacities, nor the misclassification costs, which for example are determined by economic consequences as in the German Credit dataset (Elkan 2001), do constitute tunable parameters. Our approach is simple to implement, and neither requires maintaining an evaluation set, nor self-labelling. Experimental evaluation shows its competitiveness in classification performance and speed, compared to other cost-sensitive and cost-insensitive active learning approaches.\nThe rest of this paper is organised as follows: first, Sect. 2 provides the necessary background anddiscusses the relatedwork.OurOPAL-approach is presented inSect. 3. The results of its experimental evaluation are reported in Sect. 4, comparing it to several active learning strategies, including error-reduction and uncertainty sampling approaches specialised for cost-sensitive settings.\n1 Some authors use cost-sensitive for differing label acquisition costs between candidates (Liu et al. 2009)."
  }, {
    "heading": "2 Background and related work",
    "text": "Our work addresses cost-sensitive active learning for binary classification. Given the existing overviews on the various existing cost-insensitive approaches in recent surveys such as Settles (2012), Fu et al. (2012), Cohn (2010) and Settles (2009), we focus on the most related approaches and start with cost-insensitive approaches before moving to cost-sensitive ones.\nIn expected error reduction (ER) (RoyandMcCallum2001;Cohnet al. 1996), the expected error upon incorporating a candidate into the training set is calculated. This is done by simulating for each of its possible label realisations the classifier update, and calculating the resulting error on an evaluation set (e.g. using the set of already labelled instances). In contrast to earlier work (Cohn et al. 1996), the error reduction approach inRoy andMcCallum (2001) is usable with any classifier, and directly optimises a user-specified classification performance measure. Nevertheless, ER requires reliable estimates for the true posteriors (Chapelle 2005), which are difficult to obtain in early learning stages, where solely few labels are available. Therefore, several regularisation approaches, such as Beta priors, have been explored (Chapelle 2005). This family of approaches is known (see e.g. Settles 2012) to yield good results, but it is slow, requiring an asymptotic runtime of O(|V| · |U |), where V is the evaluation set and U the pool of candidates.\nA fast active learning approach is uncertainty sampling (US) (Lewis andGale 1994),which has an asymptotic time complexity of O(|U |) and is usable on fast data streams (Zliobaitė et al. 2013). It employs so-called uncertainty measures as proxies for a candidate’s impact on the classification performance, and the candidate with the highest uncertainty is selected for labelling. In the seminal work of Lewis and Gale (1994), a probabilistic classifier is used on a candidate to compute the posterior of its most likely class. The absolute difference between this posterior estimate and 0.5 is used as uncertainty measure (lower values denoting higher uncertainty). In addition to this confidence-based uncertainty measure, other measures are common as well (Settles 2012), like entropy or the margin between a candidate and the decision boundary. Similar to the issue of the true posterior above, a known drawback (Zhu et al. 2010) ofUS is that these proxies donot consider the number of similar instances onwhich the posterior estimates are made or the decision boundaries are drawn. The reported results of empirical evaluations are somewhat inconclusive, with some authors [e.g. Chapelle (2005) or Schein and Ungar (2007)] reporting for US on some data sets even worse performance than random sampling.\nOur recently (Krempl et al. 2014a, b) proposed probabilistic active learning (PAL) approach combines the qualities of uncertainty sampling and error reduction, namely being fast and optimising directly a performance measure. Following a smoothness assumption (Chapelle et al. 2006), our approach uses probabilistic estimates for summarising the labelled information in a candidate’s neighbourhood and evaluating the impact of acquiring a label therein. This impact is expressed by the expected performance gain (the so-called probabilistic gain), measured in terms of an user-defined point classification performance measure (Parker 2011) like accuracy. Expectation is not only done over the possible realisations of a candidate’s label as in error reduction, but also over the true posterior in the candidate’s neighbourhood. PAL then selects the candidate that in expectation improves the classification performance the most within its neighbourhood. PAL runs in the same asymptotic time O(|U |) as uncertainty sampling and showed good results in cost-insensitive classification experiments (Krempl et al. 2014b), yet its suitability for cost-sensitive applications is an open question.\nCost-sensitive learning (Liu et al. 2009) is a particular challenging task for active learning algorithms, where misclassification costs are not equal among different classes. This occurs for example in fraud detection (Elkan 2001), where positives are rare, but misclassifying them (i.e. producing a false negative) is more costly than misclassifying a negative instance as positive. The objective is then to minimise the misclassification loss (Hand 2009), i.e. the cost-weighted sum of false positives and false negatives. A related, yet different problem is that of skewed or imbalanced class prior distributions (seeHe andMa (2013) for an overview), where one class is far less frequent than the other. This latter problem is addressed in passive classification (where labels are known) by resampling (Chawla et al. 2002; Attenberg and Ertekin 2013), i.e. oversampling the minority or undersampling the majority class. In Attenberg and Ertekin (2013), active-learning-based approaches for resampling are reviewed. However, while resampling strategies are useful for creating a balanced training sample, they do not directly address the former problem of cost-sensitive classification itself. Furthermore, the reported empirical results in Elkan (2001), Liu (2009) suggest that their suitability for cost-sensitive classification is highly classifier dependent. Thus, we focus on approaches that directly address cost-sensitive classification.\nIn passive classification, unequal misclassification costs are addressed by using classification rules that minimise the conditional risk (Domingos 1999). A corresponding active learning strategy is to use cost-sensitive measures for label selection. This is done in the costsensitive variant of ER (Margineantu 2005), where misclassification loss is used as error measure, and varying label acquisition costs between instances are considered. Nevertheless, it inherits the slow runtime of ER and its issues associated with the ignorance of the true posterior. For query-by-committee approaches, which use the disagreement between an ensemble of classifiers as a proxy for a candidate’s value, Tomanek and Hahn (2009) proposes a class-weighted, vote entropy-based measure as disagreement metric. However, this approach is specific for natural language processing, where active selection is between given conglomerates of labels.\nUncertainty measures can be made cost-sensitive by weighting posterior estimates with class-specificmisclassification costs (Liu et al. 2009).However, an active learning component might induce a sampling bias, such that with additional labels the posterior estimates deviate further from the true posterior (Liu et al. 2009). This poses a problem especially in costsensitive classification tasks, where reliable posterior estimates are required to determine the misclassification-loss optimal decision boundary. Liu et al. (2009) addresses this by proposing a so-called cost-sensitive uncertainty sampling approach that performs self-labelling of all remaining unlabelled instances after each label request. This aims at de-biasing the training sample for a cost-sensitive classifier, but also increases the asymptotic time complexity to O(|U |2). To the best of our knowledge, no direct empirical comparison between the approaches of Margineantu (2005) and Liu et al. (2009) has been published yet. Furthermore, they share another shortcoming in addition to requiring time-consuming steps: they are myopic, as they evaluate the impact of the next label acquisition without considering the remaining labelling budget. Nevertheless, as already stated in early works on active learning (Roy and McCallum 2001), the optimal query may very well depend on this remaining budget, which defines how many additional label requests will follow. Thus, extending active learning approaches to become non-myopic (also called far-sighted) is considered relevant (Zhao et al. 2012; Vijayanarasimhan et al. 2010). Vijayanarasimhan et al. (2010) proposed a far-sighted cost-sensitive active learning method for support vector machines that chooses a set of instances out of the pool of unlabelled candidates incorporating the individual labelling costs into the SVM’s objective function. Zhao et al. (2012) select a set of instances greedily based on expected entropy reduction. They fur-\nthermore suggest to be near-optimal and define a stopping criterion for the active learning process."
  }, {
    "heading": "3 Optimised probabilistic active learning (OPAL)",
    "text": "Weaddress fast active learning for binary classification in a cost-sensitive environment, where the costs τ of a false positive classification potentially differ from that of a false negative one (1 − τ ). The objective of our approach is to select from the pool of unlabelled candidates the one that reduces the misclassification loss (Hand 2009) the most, once it is labelled and incorporated into the training set.\nIn the next Sect. 3.1, we provide the detailed modelling and derivation of our probabilistic performance gain estimate (GOPAL), the pseudo-code2 and a numeric example. This is followed by a discussion of OPAL’s properties (Sect. 3.2), in particular under varying misclassification cost ratios and budgets. For convenience, we summarise in Table 1 the notation that is subsequently used.\n3.1 Modelling and derivation of GOPAL\nOur approach follows a smoothness assumption [see ch. 1.2.1, p. 7 in Chapelle et al. (2006)], such that neighbouring positions in the feature space are assumed to have similar posteriors. Given a labelling candidate (x, ·) from a pool of unlabelled instances U , and a set L of already labelled instances (x, y), our approach needs to assess how well its neighbourhood has been explored, i.e. to count the number of already labelled instances that are similar to the candidate, in order to further assess the value of additional labels therein. Estimates on the posterior probabilities Pr(y|x) are not sufficient, as their normalisation cancels out the absolute number of labels, keeping solely information on the proportion of each class. Therefore, we resort to the unnormalised values. That is, we use the absolute frequencies for the number of labels of each class in the candidate’s neighbourhood.\nWe differentiate between two neighbourhood concepts: The first, disjoint one applies to categorical or pre-clustered data. Such data allows to count the number LC(x,L) of labelled instances that are similar to the candidate w.r.t. their features (or assigned cluster). These label counts for x are summarised by its label statistics ls = (n, p̂), a tuple consisting of the absolute number n of labels in a candidate’s neighbourhood, and the share p̂ of positives therein. The second concept of smooth, continuous neighbourhoods corresponds for example to numerical data, where the influence of instances increaseswith the similarity of their features. In analogy to counts in the first case, we use frequency estimates in this second case. Using probabilistic classifiers that are modified to return unnormalised estimates for the absolute frequencies is one option. We recommend to use generative probabilistic classifiers like Naive Bayes rather than discriminiative ones like logistic regression. The information on the labelled data kept by the former by modelling Pr(X, Y ) and Pr(X) allows to compute the label statistics directly. Furthermore, generative classifiers converge with fewer labels, as shown in Ng and Jordan (2001), which is important in active learning contexts. If these classifiers are not available, we propose to use Gaussian kernel frequency estimation (here, Σ is the bandwidth matrix):\nLC(x,L) ≈ KFE(x,L) = ∑\nxi∈L exp\n( −1 2 · (x − xi )′Σ−1(x − xi ) )\n(1)\n2 Implementations are available on our companion website: http://kmd.cs.ovgu.de/res/opal.\nTable 1 Used symbols and notation\nSymbol Description Reference\nInput data\nx Feature vector of an instance p. 5 y ∈ {0, 1} Class label of an instance (0=neg., 1=pos.) p. 5 U = {(x, ·)} Pool of unlabelled instances p. 5 L = {(x, y)} Pool of labelled instances p. 5 Variables imposed by the application domain τ ∈ [0, 1] Cost of each false positive classification p. 5, p. 9, Eq. 16 m ≥ 0 Budget for the candidate’s neighbourhood p. 7 Variables within the neighbourhood of a candidate (x, ·) dx ≥ 0 Density weight (w.r.t. all instances in U ∪ L) p. 6, Eq. 2 gx Density weighted optimised probabilistic gain p. 11 ls = (n, p̂) Label statistics with: p. 5 n Total number of already obtained labels\np̂ Share of positives therein (a posterior estimate) k ∈ {0, . . . ,m} Number of positives among future label realisations p. 7 p ∈ [0, 1] True posterior probability of the positive class p. 7 Functions L(p|ls) Likelihood of a possible true posterior p. 7, Eq. 7 ωp(Slabel) ∈ [0, 1] Normalised likelihood of a possible true posterior p. 8, Eqs. 10–12 Γ (z) Legendre’s gamma function, see pp. 206–208 in Press\net al. (1992) p. 7\nIML (n, p̂, τ,m, k) Integral, proportional to the expected performance p. 11, Eq. 32 GOPAL(n, p̂, τ,m) Optimised probabilistic gain, i.e. a candidate’s p. 11, Eq. 35 ∈ [−1, 1] Exp. average misclassification loss reduction\nBased on this, we derive the total number of labels n = LC(x,L) and the share of positives therein p̂ = LC(x,L+)/LC(x,L), where L+ is the subset of labelled positive instances. The tuple ls = (n, p̂) constitutes the label statistics of x’s neighbourhood. Using Eq. 1, we also derive the density in the candidate’s neighbourhood\ndx = LC(x,L ∪ U)|L ∪ U | (2)\nThis serves as a weight for the importance of the classification performance within this neighbourhood, as compared to other regions in feature space. Therefore, we later weight the average misclassification loss reduction by this density-weight. It is useful to precompute dx for all candidates in the pool, as L ∪ U is static in the pool-based active learning scenario."
  }, {
    "heading": "3.1.1 A non-myopic, cost-sensitive probabilistic gain",
    "text": "Following our recently (Krempl et al. 2014a, b) proposed probabilistic active learning approach, we use a candidate’s label statistics ls to compute its probabilistic gain, which\ncorresponds to the expected gain in classification performance from acquiring the candidate’s label. This is done by first modelling both, the candidate’s label realisation y and the true posterior p of the positive class in its neighbourhood, as random variables, and computing the expectation over both variables simultaneously, using the normalised likelihood given the label statistics. The resulting probabilistic gain is weighted by the feature density dx at its position, and the candidate with highest density-weighted probabilistic gain is selected.\nHowever, in contrast to (Krempl et al. 2014a, b), our optimised probabilistic active learning (OPAL) offers three advantages for fast, cost-sensitive applications: first, it quantifies a candidate’s probabilistic gain (its label’s value) in terms of misclassification loss reduction, which is a cost-sensitive measure. Second, it uses a closed-form solution for computing the probabilistic gain, making it faster. Third, it is non-myopic, considering the effect of more than one label acquisition at once. Thus, given a budget that allows to acquire m labels at once within the neighbourhood, we compute the expectation over a set y1, y2, . . . , ym of label realisations, rather than on a single label realisation y. However, the ordering of labels is irrelevant in this additional training set. By counting the number of positive realisations k in the possible sets, m + 1 different cases (k = 0, 1, . . . ,m) are distinguishable, and the number of positive realisations is a binomial-distributed random variable K ∼ Binm,p . Thus, we perform the expectation over its realisation k and over the true posterior p, rather than over y and p as in Krempl et al. (2014b).\nThe true posterior in this neighbourhood is unknown, but for its possible values, the likelihoods are calculable by using the frequency estimates from the label statistics. For this, we model the unknown true posterior in this neighbourhood by a Beta-distributed random variable P. Its realisation p is itself the parameter of the Bernoulli distribution that controls the label realisation y ∈ {0, 1} of any instance within the neighbourhood. Consequently, for any set of m label realisations in the neighbourhood, the number k of positives therein is the realisation of a Binomial-distributed random variable K :\nP ∼ Betan· p̂+1,n·(1− p̂)+1 (3) Y ∼ Bernoullip = Ber p (4) K ∼ Binomialm,p = Binm,p (5)\nWe will denote the probability (density) functions (pdf’s) of the above distributions by Betaα,β(), Ber p(), and Binm,p(), respectively. For computing the binomial coefficient in Binm,p(k) = ( m k ) · pk · (1 − p)m−k , as well as in the subsequent equations below, we use the generalised binomial coefficient for non-integer arguments, and the gamma function Γ (z) as defined by Legendre3:\n( m k ) = Γ (m + 1) Γ (k + 1) · Γ (m − k + 1) (6)\nThe true posterior’s Beta distribution above is the result of its normalised likelihood, given the already observed labels, as we will show below. According to Eq. 5, the likelihood of a true posterior p given the data summarised in ls corresponds to the probability mass function Binn,p(n p̂):\n3 See for example pages 206–208 in Press et al. (1992).\nL(p|ls) = L(p|(n, p̂)) = Binn,p(n p̂) (7)\n= Γ (n + 1) · p n· p̂ · (1 − p)n·(1− p̂)\nΓ (n · p̂ + 1) · Γ (n · (1 − p̂) + 1) (8)\nFollowing a Bayesian approach, we consider a prior g(p) for P, and obtain the normalised likelihood\nωls(p) = L(p|ls)g(p)∫ 1 0 L(ψ |ls)g(ψ)dψ\n(9)\nThe choice of a suitable prior g(p) depends on our a priori information about the class prior distribution Pr(Y = +) in the data. Without any a priori information, we chose a uniform prior for P, i.e. g(p) ∼ U (0, 1). As a result, g(p) is a constant function, and the integral in the denominator sums up to (1 + n)−1, yielding (1 + n) as normalising constant:\nωls(p) = (1 + n) · L(p|ls) (10) Expanding this using Eq. 7, and setting (1+ n) · Γ (n + 1) = Γ (n + 2), we obtain precisely the probability function of the Beta-distribution:\nωls(p) = Γ (n + 2) · p n· p̂ · (1 − p)n·(1− p̂)\nΓ (n · p̂ + 1) · Γ (n · (1 − p̂) + 1) (11)\n= Γ (α + β) Γ (α) · Γ (β) · p α−1 · (1 − p)β−1 = Betaα,β(p) (12)\nHere, we rewrite in the last step the arguments of the Γ -functions and obtain α = n · p̂ + 1 and β = n · (1 − p̂) + 1. These parameters for the Beta-distribution have a correspondence in the positive and negative labels in the candidate’s neighbourhood: with each positive label therein, α increases by one, while with each negative, β increases by one. Thus, the normalised likelihood expressed by the Beta-distribution is a uniform distribution if no labels are available. However, if labels are available, its peak around p̂ becomes more and more distinct with an increase in the number of available labels.\nIn contrast to Krempl et al. (2014b), we do the expectation in OPAL over k and p, rather than over y and p, yielding the candidate’s probabilistic gain (GOPAL), defining the expected change of the performance measure for its neighbourhood in average per additional label:\nGOPAL(ls, τ,m) = 1 m\n· Ep [ Ek [ gainp(ls, k,m) ] ] (13)\n= 1 m\n· ∫ 1\n0 Betaα,β(p) ·\n∑\n0≤k≤m Binm,p(k) · gainp(ls, k,m) dp (14)\nHere, gainp(ls, k,m) is the performance gain within the neighbourhood with label statistics ls and true posterior p, given that k among m additional label realisations are positive. Using a point performance measure perf p( p̂) that calculates the classification performance under a posterior estimate p̂ and a true posterior p, the performance gain is written as difference between future and current performance:\ngainp(ls, k,m) = perf p ( n p̂ + k n + m ) − perf p( p̂) (15)\nWe address active learning for cost-sensitive binary classification tasks, where τ ∈ [0; 1] indicates the cost for each false positive instance, and 1 − τ is the corresponding cost for\neach false negative one, assuming zero costs for correct classifications. A point performance measure (Parker 2011) for this setting is misclassification loss (Hand 2009), which is the product of themisclassification costmatrix and the confusionmatrix.Within a neighbourhood with true posterior p and a classification rule classifying a share of q instances therein as positive,4 the misclassification loss is\nMLoss(p, q) = p · (1 − q) · costFN + (1 − p) · q · costFP = (16) p · (1 − q) · (1 − τ) + (1 − p) · q · τ = q · (τ − p) + p · (1 − τ) (17)\nThus, given p and τ , the misclassification loss is a linear function of q ∈ [0; 1]. It has a positive slope for p < τ , and a negative for p > τ . Due to the positive slope, it is optimal to set q = 0 in the former case (and q = 1 in the latter), in order to minimise the loss function. Thus, the cost-optimal decision is:\nq∗ = ⎧ ⎨\n⎩ 0 p < τ 1 − τ p = τ 1 p > τ\n(18)\nOne could argue that in the case τ = p the choice of q∗ is an arbitrary one, as the first factor q · (τ − p) is zero, meaning equal loss of p2 = τ 2 for all choices of q∗. However, in order to obtain a consistent classification rule, one should specify the assignment q∗ = 1− τ at ties, rather than simply replacing one strict inequality condition in Eq. 18 with a non-strict one. This is illustrated when studying the classification under extreme values for τ . For example, if τ = 0, false positives do not cost anything, while false negatives are very expensive. A cost-optimal classification rule should thus classify every instance as positive, i.e. q∗ should be one for all possible p. While cases of p ∈ ]0; 1] are covered by the third clause in Eq. 18, the second clause must return q∗ = 1 for cases of p = 0. Vice versa, if τ = 1 this second clause must return q∗ = 0. Thus, it should be set to 1 − τ (or 1 − p, equivalently). As the true posterior p is not directly observable, the counted observed share p̂ of positives is used as proxy instead. Thus, ties might occur frequently enough to consider this as relevant.\nGiven p and τ , using negated misclassification loss (Eq. 16) under cost-optimal classification (Eq. 18), we derive a performance measure suited for Eq. 15:\nper f p,τ ( p̂) = −ML p,τ ( p̂) = − ⎧ ⎨\n⎩ p · (1 − τ) p̂ < τ τ · (1 − τ) p̂ = τ τ · (1 − p) p̂ > τ\n(19)\nIntentionally,we do not include the density of the neighbourhood here, tomeasure the effect of this misclassification loss reduction for the whole data set, because we intend to separate data set specific information from this value. Of course, ML p,τ ( p̂) should have been multiplied with the neighbourhood’s density dx , but this factor can be delivered to the very left side of the whole formula.\nPlugging this into Eq. 13 yields the probabilistic misclassification loss reduction\nGOPAL(ls, τ,m) = 1m · ∫ 1 0 Betaα,β (p) m∑\nk=0 Binm,p(k)\n( ML p,τ ( p̂) − ML p,τ ( n p̂ + k n + m )) dp (20)\n4 Classification within a neighbourhood is assumed to be indifferently of the precise location within the neighbourhood, i.e. we assume conditional independence of the posterior given the neighbourhood of an instance."
  }, {
    "heading": "3.1.2 Derivation of the closed-form solution",
    "text": "For deriving a closed-form solution, we split Eq. 20 it into a term for to the expected current performance Ecur and another for the expected future performance E f ut :\nGOPAL(ls, τ,m) = 1 m · (Ecur − E f ut )\n(21)\nThe first term Ecur , where m = k = 0 and Bin0,p(0) = 1, is simplified to:\nEcur = ∫ 1\n0 Betaα,β(p) · ML p,τ ( p̂) dp (22)\nExpanding the Beta-distributed probability Betaα,β(p) by Eq. 12 and themisclassification loss by the case-by-case formula in Eq. 18, and integrating out yields:\nEcur = ∫ 1\n0\nΓ (n + 2) · pn· p̂ · (1 − p)n·(1− p̂) Γ (n · p̂ + 1) · Γ (n · (1 − p̂) + 1) ·\n⎧ ⎨\n⎩ p · (1 − τ) dp p̂ < τ τ · (1 − τ) dp p̂ = τ τ · (1 − p) dp p̂ > τ\n(23)\n= (n + 1) · (\nn n · p̂\n) ·\n⎧ ⎪⎪⎪⎨\n⎪⎪⎪⎩\n(1 − τ) · Γ (1+n−n p̂)Γ (2+n p̂) Γ (3+n) p̂ < τ (τ − τ 2) · Γ (1+n−n p̂)Γ (1+n p̂) Γ (2+n) p̂ = τ τ · Γ (2+n−n p̂)Γ (1+n p̂) Γ (3+n) p̂ > τ\n(24)\nThe second term, E f ut , in Eq. 21 is:\nE f ut = ∫ 1\n0 Betaα,β(p)\nm∑ k=0 Binm,p(k) · ML p,τ ( n p̂ + k n + m ) dp (25)\n= m∑ k=0 · ∫ 1 0 Betaα,β(p) · Binm,p(k) · ML p,τ ( n p̂ + k n + m ) dp (26)\nAs above, we expand the terms therein, which now include the Binomial-distributed\nprobability Binm,p(k) = ( m k ) · pk · (1 − p)m−k :\nE f ut = m∑\nk=0\n∫ 1\n0\nΓ (n + 2) · pn· p̂ · (1 − p)n·(1− p̂) Γ (n · p̂ + 1) · Γ (n · (1 − p̂) + 1) · (27)\n· ( m k ) · pk · (1 − p)m−k · ML p,τ ( n p̂ + k n + m ) dp (28)\n= (n + 1) · (\nn n · p̂\n) · m∑\nk=0 ·IML(n, p̂, τ,m, k) (29)\nwhere IML is a function of n, p̂, τ , m, and k, containing the integral and proportional to the expected performance, which is integrated out as follows:\nIML(n, p̂, τ,m, k) = ∫ 1\n0\n( m k ) · pn· p̂+k · (1 − p)n+m−n· p̂−k · ML p,τ ( n p̂ + k n + m ) dp (30)\n= ( m k ) · ∫ 1 0 pn· p̂+k · (1− p)n+m−n· p̂−k ·\n⎧ ⎪⎪⎪⎨\n⎪⎪⎪⎩ p · (1 − τ)dp n p̂+kn+m < τ (τ − τ 2)dp n p̂+kn+m = τ τ · (1 − p)dp n p̂+kn+m > τ\n(31)\n= ( m k ) ·\n⎧ ⎪⎪⎪⎨\n⎪⎪⎪⎩\n(1 − τ) · Γ (1−k+m+n−n p̂)Γ (2+k+n p̂) Γ (3+m+n) n p̂+k n+m < τ (τ − τ 2) · Γ (1−k+m+n−n p̂)Γ (1+k+n p̂) Γ (2+m+n) n p̂+k n+m = τ τ · Γ (2−k+m+n−n p̂)Γ (1+k+n p̂) Γ (3+m+n) n p̂+k n+m > τ\n(32)\nUsing this in Eqs. 24 and 29, we obtain\nEcur = (n + 1) · (\nn n · p̂\n) · IML(n, p̂, τ, 0, 0) (33)\nE f ut = (n + 1) · (\nn n · p̂\n) · m∑\nk=0 ·IML (n, p̂, τ,m, k) (34)\nand Eq. 35 to compute the GOPAL in the candidate’s neighbourhood:\nGOPAL(n, p̂, τ,m) = (n + 1)m · ( n n · p̂\n) · ⎛ ⎝IML (n, p̂, τ, 0, 0) − m∑\nk=0 IML (n, p̂, τ,m, k)\n⎞\n⎠ (35)"
  }, {
    "heading": "3.1.3 Pseudocode and numeric examples",
    "text": "The pseudocode for OPAL in pool-based active learning is given in Fig. 1. Lines 2–7 iterate over each labelling candidate (x, ·) in the pool U . First (line 3), a candidate’s label statistics ls = (nx , px ) are computed according to Eq. 1. Second (line 4), the density weight dx is estimated, i.e. the proportion of all labelled and unlabelled instances within the candidate’s neighbourhood divided by those in all neighbourhoods, seeEq. 2. In line 5, the optimalm∗x that maximises theGOPAL (seeEq. 35) is found, using a logarithmic searchoverm′ = 1, 2, . . . ,m. Density-weighting this maximal GOPAL (line 6) yields gx . , and the candidate maximising the density-weighted probabilistic gain is returned (line 8).\nThe GOPAL, visualised in Fig. 2, corresponds to the expected average reduction in misclassification loss in each subsequent classification5 in the candidate’s neighbourhood. For equal misclassification costs, theGOPAL is proportional to the expected average gain in accuracy and is highest for candidates close to the decision boundary (where p̂ ≈ τ ), like the points D and E (compared to B and C) in Fig. 2. For a very small number n of already obtained similar labels, GOPAL approximates random sampling as n → 0, corresponding to the barely available information. Nevertheless, as n increases (compared to the remaining budget m), the equations above are dominated by the observed posterior p̂. Thus, the difference between expected future IML(n, p̂, τ,m, k) and current performance IML(n, p̂, τ, 0, 0) converges towards zero, making candidates in well-explored regions (e.g. A) less valuable than those in unexplored ones (e.g. D, E). In the lower subplot in Fig. 2, the points (D, E) show the importance of the density-weighting: Point E is in a less explored but also sparser area than D, thus E has a higher GOPAL (0.0817 vs. 0.0737) but a 6.5-times lower density weight, as improving the performance in its region will effect 6.5 times fewer future classifications. Thus, the density-weighted probabilistic gain of E (0.00164) is lower than that of D (0.00982). In contrast, US neither incorporates the amount of available information (e.g.A vs. D), nor the importance of neighbourhoods (e.g.D vs. E). Note that for unequal misclassification costs, the GOPAL is not symmetric around τ , but rather favours sampling instances from the regions where potentially a more costly error is made. That is, if false positive costs are relatively low compared to false negative ones (e.g. τ = 0.1), misclassification of positives (as false negatives) is expensive compared to the misclassification of negatives. Accordingly, the probabilistic gain is higher in regions where currently instances are classified as negative, as the possible error therein is more expensive. Therefore, our cost-sensitive approach will favour sampling in these regions. A further discussion of the properties of GOPAL is provided in Sect. 3.2.2, where Fig. 3 on page 14 illustrates the shape of this function.\n3.2 Properties of GOPAL\nWe now briefly discuss the asymptotic (with respect to data set size) computational time complexity of OPAL in Sect. 3.2.1, comparing it to related algorithms for active learning of\n5 Assuming cost-optimal classification (Domingos 1999), see Eq. 18 in Sect. 3.1.\nbinary, incremental classifiers, before illustrating the effect of the myopic extension on the probabilistic gain in Sect. 3.2.2."
  }, {
    "heading": "3.2.1 Computational complexity",
    "text": "For the non-myopic selection of a candidate from a pool U of labelling candidates, OPAL iterates first over all candidates in the pool (lines 2–7). Each iteration consists of (1) querying label statistics, (2) querying density weights, (3) determining the locally optimal budget, and (4) computing the density-weighted probabilistic gain. The first step requires absolute frequency estimates of labels in the candidate’s neighbourhood, similar to the relative frequency estimates needed by entropy or confidence uncertainty measures. These are obtained in constant time by probabilistic classifiers. The second step requires density estimates over all instances, that is over labelled L and unlabelled U ones. Precomputing these density estimates once for all later calls of OPAL leads to constant query time, as in the pool-based setting the union L ∪ U is constant. The third step requires a logarithmic search over all possible m′ ∈ 1, 2, . . . ,m, where for eachm′ theGOPAL is calculated. The latter is done in O(m) time, due to the closed-form solution obtained for Eq. 35. Thus, the third step requires O(m log(m)) time. The fourth step computes the density-weighted probabilistic gain gx , requiring constant time. For the subsequent selection of the best candidate in line 8, the maximum of gx as well as the index of the corresponding candidate are kept. Thus, the iteration over the pool is determining the overall asymptotic time complexity of O(|U | ·m log(m)) for the non-myopic OPAL, where m is the remaining labelling budget that is in general much smaller than |U |. OPAL’s myopic counterpart requires asymptotically linear time O(|U |), as m = 1.\nIn comparison, uncertainty sampling also requires asymptotically linear time O(|U |), whereas error reduction as discussed in Settles (2012) requires O(|U | · |V|) time, where |V| ≈ |U |, as V needs to be a representative sample of the data."
  }, {
    "heading": "3.2.2 Effect of the non-myopic extension",
    "text": "Besides of being faster and cost-sensitive, OPAL extends PAL by adding the ability of acting non-myopic. The first two properties are the result from using a closed-form solution and misclassification loss as performance measure, and have already been discussed. Thus, we will now focus on the third one, which allows OPAL to consider a budgetmwhen computing the probabilistic gain of a candidate.\nWe illustrate the usefulness and effect of this extension on the probabilistic gain function in Fig. 3. In this figure, the first two columns of plots show the probabilistic gain function (in terms of average misclassification loss reduction) for different label statistics, i.e. combinations of different numbers of already obtained labels n and observed posteriors p̂ = P̂r(+|x). The first column shows the myopic probabilistic gain (m = 1), the second the non-myopic one (m = m∗), where m ∈ {1, 2, . . . , 21} is chosen such that the probabilistic gain is maximal. The third column corresponds to the difference (in a logarithmic scale) between the two probabilistic gains. The three rows correspond to the different misclassification costs τ = 0.1, 0.25, and 0.5. The plots for τ = 0.75 (and τ = 0.9) are not shown, as they are reflection symmetric to those of τ = 0.25 (and τ = 0.1, respectively).\nGiven equal misclassification costs (τ = 0.5, third row) and a candidate in a neighbourhood, where already two labels (all positive) have been acquired (n = 2, p̂ = 1). In this neighbourhood, a single additional label can not alter the classification decision, thus the probabilistic gain in a myopic setting is zero. This is seen in the left-bottom plot, where the\nprobabilistic gain is zero for n = 2, p̂ = 1 (the corner in the uttermost back). However, if more than one label can be acquired in this neighbourhood, these labels might change the classification. Thus, under a non-myopic setting and equal misclassification costs, the probabilistic gain should be positive. Indeed, the probabilistic gain is GOPAL = 0.0274 for the optimal acquisition of three labels (m∗ = 3). Correspondingly, the flanks in the centred plots are flatter than in the left ones.\nFor unequal misclassification costs (e.g. in first row with τ = 0.1, meaning false positives are cheap compared to false negatives), the expected gain from a single additional label might even be negative. This corresponds to situations, where just sufficiently positive labels were acquired within a neighbourhood to classify instances therein as positive (i.e. p̂ = P̂r(+|x) > τ ). In the myopic setting, the realisation of the single additional label is binary. If it is positive, it does not alter the classification. If it is negative, it inverts the classification. The latter results in a wrong classification if the true posterior p is actually greater τ , which\nis very likely, given that the share of positives p̂ among the already seen labels was greater τ . Therefore, in the left-upper plot, the myopic probabilistic gain is negative for n = 1 and p̂ ∈ [0.1, 0.2], with a negative peak at GOPAL = −0.12 for p̂ = 0.199.\nIn contrast to the myopic setting with its binary label realisation, the non-myopic setting uses a rational number: the share of positives among the realisation of additional labels. Thus, the effect of this special case decreases with increasing m, as shown in the uppercentre plot. Nevertheless, for extreme misclassification cost inequalities (e.g. τ = 0.1 or τ = 0.9), the probabilistic gain remains non-positive for all neighbourhoods with p̂ > τ , which are therefore not selected for label requests. In contrast, for moderate misclassification cost inequalities (e.g. τ = 0.25 or τ = 0.75), it becomes positive for some neighbourhoods therein, namely those having an observed posterior p̂ close to τ . As a consequence, the effect of the non-myopic extension might be more important for situations with moderate misclassification cost inequalities, than for those with either equal misclassification costs or extreme unequal ones. However, this requires empirical evaluation, which we provide in Sect. 4.3.\nConcerning the probabilistic gain function’s mode, our numerical experiments indicate it to be an unimodal function of m for a given combination of n, p̂ and τ ."
  }, {
    "heading": "4 Experimental evaluation",
    "text": "We expect our new cost-sensitive method OPAL to perform at least equally well in terms of resulting classification performance as other (cost-sensitive) active learning approaches, while being faster than other cost-sensitive approaches. Furthermore, we expect OPAL to be better than PAL towards the end of the learning process, through its non-myopic extension. Therefore, we designed a framework that ensures a fair evaluation of our contributions.\nIn the first subsection, we describe our evaluation setting, the active learning approaches used in the comparison, the data sets and our framework. In the second subsection, we present and discuss the results of the experimental evaluation. There, we first assess the usefulness of our cost-sensitive extension. Then, we show that OPAL is in most cases superior, both to its myopic counterpart PAL and to other active learning approaches, while having the same asymptotic time complexity as uncertainty sampling."
  }, {
    "heading": "4.1 Evaluation settings",
    "text": ""
  }, {
    "heading": "4.1.1 Active learning algorithms",
    "text": "For experimental evaluation, we use the fast version of OPAL described in Sect. 3, which applies a logarithmic search for determining the optimal budget. In pretests, there was no significant difference in classification performance between this approach and another variant of OPAL doing exhaustive search. In addition, we use a cost-sensitive, myopic PAL (Krempl et al. 2014b) with the presented speed optimisation (here denoted as csPAL). This is the equivalent to OPAL with a fixed budget of m = 1.\nFurthermore, we use a cost-sensitive variant of Uncertainty Sampling (Liu et al. 2009) (denoted as U.S.) and Certainty Sampling (Ferdowsi et al. 2011) (denoted as C.S.), which both optimise confidence6 (posterior difference to 0.5). Here, the posterior probabilities are calculated from a cost-weighted frequency estimation. Liu et al. (2009) proposed to use a\n6 We tested confidence- and entropy-based uncertainty measures in pretests and used the one with the best performance over all data sets for the final evaluation.\nself-training approach for uncertainty sampling, such that posterior estimates are optimised for confidence calculation. We denote this extension as U.S. st.\nFor error reduction, we use the cost-sensitive algorithm proposed by Margineantu (2005) (denoted as Marg) and the non-cost-sensitive version by Chapelle (2005) (denoted as Chap). As a non-myopic representative, we use the method by Zhao et al. (2012) (denoted as Zhao). As the latter originally needs initial labels, we use a beta-correction for the classifier predictions of 0.001 (like for Chap). This simulates that in each evaluation neighbourhood an equal number of positives and negatives has been seen. In our framework, we always use 40 labels to be acquired by the active learners. Therefore, we disabled the automatic stopping criterion (otherwise learning often stopped far too early). Furthermore, we use random selection (denoted as Rand) as a baseline."
  }, {
    "heading": "4.1.2 Data sets",
    "text": "In our experiments, we used 3 synthetic and 5 real data sets (from Asuncion and Newman (2013)). Each attribute was scaled to a [0; 1]-range, because we use Gaussian Kernel Frequency estimates with a fixed and pre-tuned bandwidth sigma (see Sect. 1). The main characteristics (number of instances, number of attributes), such as training and test set size and the bandwidth σ of the Gaussian Kernel, are given in Table 2.\nTwo of the synthetic data sets are based on the generator used in Chapelle (2005). They consist of 4x4 clusters, arranged in a checker-board formation (on a 2 dimensional feature space). While the clusters are low-density-separated in Che (as in Chapelle (2005)), they are adjoined in Che2. The third synthetic data set (Sim) consists of two normal distributed, overlapping clusters in a two dimensional space.We used this very simple example as a proof of concept for active learning methods.\nThe real-world data sets are Seeds (See), Vertebral (Ver), Mammographic mass (Mam), Yeast (YeaU) and Abalone (Aba), see Asuncion and Newman (2013). As show in Table 2, balanced as well as unbalanced class distributions occur. Categorical features (as in Mam) have been dichotomised into multiple binary features. In Mam, instances with missing values have been removed. For the multi-class dataset See, we classified Kama and Canadian vs. Rose; for Ver, we used normal vs. abnormal; for Aba, we used trees with rings <10 vs.\nrings≥10; for YeaU, we usedMIT vs. the rest. For better comparison to Chapelle (2005) and Krempl et al. (2014b), we used a Parzen window classifier, which is a generative probabilistic classifier as discussed in Sect. 3.1. However, for some cost-ratios this classifier is not able to discriminate in the data, thus it is classifying all instances into the more expensive class. This is detectable in the bandwidth tuning curves (see Fig. 1), when the best σ -value (the one with lowest misclassification loss) is the maximal, uttermost left one. We reported the results on those data-set/cost-ratio-combinations for completeness, but emphasise that the classification performance on such ill-posed learning problems is not meaningful."
  }, {
    "heading": "4.1.3 Framework",
    "text": "Our framework decouples classification and active learning. All runs behave exactly the same except for the active sampling component, which decides the instance whose label should be acquired next. Thus, we use exactly the same frequency estimates (see Eq. 1) and the same classification algorithm (a Parzen window classifier (Chapelle 2005)) with the identical parameters for any active learning strategy during the calculation. Furthermore, we decoupled the classification and evaluation process to ensure, that every active learning method just differs in the set of labelled instances. To get more significant results, we used a cross-validation with random sub-samplings in 100 runs. The training and test set sizes are listed in Table 2.\nHere, active learning startswithout initial labels on the unlabelled training sample, and finishes after 40 label acquisitions (steps).We implemented the framework inOctave/MATLAB, whichwas parallelised to run on a cluster. Every run uses the same pre-tuned, data set-specific bandwidth, and each of the 40 steps is evaluated on the same, dedicated (labelled) test sample with the same cost-sensitive Parzen window classifier, recording misclassification loss and speed.\nThe presented learning curves show the arithmetic mean of the misclassification loss over all 100 runs for a given combination of data set, algorithm and cost-ratio."
  }, {
    "heading": "4.2 Relevance of the cost-sensitivity",
    "text": "From OPAL’s theoretical characteristics, we expect OPAL to choose the best instances, with respect to a given cost-ratio τ . To evaluate the relevance of this cost-sensitivity empirically, we run OPAL for its GOPAL calculation with 5 different τ values (τ ∈ {.1, .25, .5, .75, .9}), and evaluate its misclassification loss regarding the true cost-ratio τ ∗. If the cost-sensitivity is meaningful and relevant, the curve with τ = τ ∗ should have the best performance compared to all other τ -values.\nFigure 4 shows a selection of data sets (columns) and the evaluation cost-ratios τ ∗ (rows). Each plot shows the learning curves with the classification performance in terms of misclassification loss on its y-axis and the steps of its learning process in the number of already requested labels on the x-axis. The 5 different variants of OPALwith the varying τ are printed in different colours while the correct one is plotted in bold. The results of the other data sets are given in the appendix (see Sect. 1; Fig. 8).\nThe first interesting fact is that the correct usage of τ = τ ∗ leads to a converging curve, while some wrong ones lead to diverging curves. Thus, ignoring the application-specific cost-ratio results in a low classification performance on these data sets. Furthermore, the curves for neighbouring τ -values behave similarly, especially the curves for τ = 0.25 and τ = 0.1, respectively τ = 0.75 and τ = 0.9.\nComparing the level and velocity of the misclassification curves, the bold ones are mostly superior. The single exception occurs on Che, whereOPAL selects optimal labels for τ = 0.5 (that is one instance of each cluster), so it performs very well for the other evaluation costratios too. This is due to the very special characteristics (well-separated clusters) of this data set. When the separation between clusters is reduced, as in Che2, the effect vanishes. The other exceptions, like in YeaU for τ ∗ = .1, occur all on ill-posed learning problems (see Fig. 1), where the results are not meaningful.\nSummarising the results, the use of the cost-sensitive extension is beneficial, although there are some exceptional cases, where τ = τ ∗ achieves better performance due to special structure in the data. It is noteworthy that in a real-world application, τ is not a tunable parameter but rather imposed by the application domain. The results show that ignoring this application-specific cost-ratio will in most cases result in a non-optimal classification performance."
  }, {
    "heading": "4.3 Comparison between OPAL and other active learning strategies",
    "text": "This subsection assess whether (1) OPAL’s non-myopic extension is beneficial for a given labelling budget, (2) OPAL’s performance is superior or at least equal to other active learning approaches, and (3) its time complexity increases solely linearly with training set size (like uncertainty sampling).\nFor comparison, we use learning curves measuring the performance in terms of misclassification loss as before. The plots for all data sets and algorithms are given in Figs. 5 and 6. The best active learning method is the one, that has a fast-converging, low final misclassification loss level. Furthermore, we give a numerical value (see Table 3) for comparing two algorithms directly over all 8 data sets. It is computed as the portion of OPAL being better than the compared algorithm on all 100 runs of all data sets (thus on 800 pairs) for a given cost-ratio (τ ∗) and labelling step. We also report the results of one-sided Wilcoxon signed-rank tests with significance level 0.001 on these pairs, by indicating a significantly better performance of OPAL by ∗, and a significantly worse performance by †.\n(1) OPAL’s non-myopic extension is beneficial Here, we compare the non-myopic OPAL and the myopic csPAL. Both algorithms just differ in their available budget size. While OPAL chooses the best GOPAL for a given m-vector, csPAL just considers the very next possible label (m = 1).\nAs already discussed in Sect. 3.2.2, the learning curves for τ ∗ = 0.5 of both algorithms are quite similar. Furthermore, the tables state that OPAL is at most in 4% better than csPAL. This value might confuse, but one must be reminded, that this is just the portion of OPAL being better. Because there is no significance of being worse, we can derive that OPAL and csPAL behave quite similar (have the same values). However, if the number of already obtained labels is very high, the myopic GOPAL used in csPAL will get zero, resulting in a random-sampling-like behaviour. In contrast, if m is sufficiently large, meaning that still several more labels will be acquired, the non-myopic variant in OPAL is advantageous, as it will still perform a differentiated selection.\nFor τ ∗ = 0.5, the non-myopic variant is slightly advantageous in terms of final misclassification loss (e.g. Mam for τ ∗ = 0.75, YeaU for τ ∗ = 0.9 or See) or at least performs equally well. Interestingly, while for extreme cost-ratios (τ = 0.1 or τ = 0.9) the non-myopic variant is still advantageous over its myopic counterpart (in 44 or 48% of the cases, see Table 3), the difference is not as big as it is for moderately unequal cost-ratios (τ = 0.25 or τ = 0.75). However, this is in accordance to the theoretical observationsmade in Sect. 3.2.2, where a strongest effect for moderately unequal misclassification cost ratios was predicted.\nFurthermore, we observe that csPAL converges faster (see e.g. Table 3 for 10 label acquisitions). However, this again matches with the theoretical discussion in Sect. 3.2.2, because we set the budget initially to m = 40 (and not to 10), thus OPAL has optimised its learning path for 40 label acquisitions. Evaluating its performance after less steps is therefore slightly\nmalicious. However, the results indicate that (1) setting the budget correctly to the remaining one is beneficial for final classification performance, and (2) a faster learning is achievable by setting m to low values, if one is willing to forfeit long-term performance.\n(2) OPAL’s performance is superior Measuring the overall performance of active learning methods over different data sets and cost-ratios is complex, due to weighting and measuring the characteristics of learning curves, which ideally converge fast to a low final misclassification loss level. Therefore, Table 3 provides a summary of the total percentage ofwins ofOPAL\nagainst each other approach. The learning curves show that OPAL is better than U.S. (with and without self-training) after 6 label requests (when learning becomes meaningful) in most cases. Explanations according to Settles (2012, pp. 19–20) are that US (a) ignores the extend of exploration in a neighbourhood, (b) relies on a hypothesis biased by its sampling, and (c) is fairly myopic. We can not confirm a superiority of C.S. (Ferdowsi et al. 2011) compared to any other (even random) active learning approach in our experiments. The cost-sensitive error reduction method Marg performs worse than expected. It is outperformed by its costinsensitive counterpart Chap, maybe due to solely using labelled instances for evaluation, as opposed to the self-labelling used by Chap. Chap and the non-myopic, cost-insensitive error\nreduction method Zhao sometimes achieve competitive results (esp. on YeaU), but only at very high computational costs, which prevented them to be completed on Aba. Using the numbers of Table 3, Rand is surprisingly the best competitor. All in all, we can argue that OPAL outperforms all other tested algorithms with high significance (see Table 3) and has a good trade-off between fast convergence and low final misclassification loss value. Although such an evaluation is not in the scope of this paper, the results on YeaU indicate that OPAL is also suitable for unbalanced data sets.\n(3) OPAL’s runtime in comparison with training set size To experimentally verify OPAL’s time complexity (cmp. Sect. 3.2.1), we measured the time in seconds per run used by each active learning process and summed it over all 40 label acquisitions in Table 4 (all differences w.r.t. OPAL are significant at level 0.001). Obviously, Rand is fastest, followed by U.S., C.S., csPAL and OPAL, which slow down constantly with increasing training set size. Our myopic approach csPAL is just slightly slower than U.S., due to its more complex value calculation. OPAL takes about 7 times longer than csPAL, because it computes the GOPAL more often when searching for the optimal budget over m = 1, 2, . . . , 40. In contrast, the execution times of Marg, Chap, and Zhao explode on bigger data sets, taking for a single cost ratio\nTable 4 Average execution time (in s), rows ordered in ascending data set size\nData OPAL csPAL U.S. U.S. st C.S. Marg Chap Zhao Rand\nSee 1.867 0.254 0.206 0.468 0.162 43.535 51.87 254.8 0.015\nChe 1.905 0.249 0.201 0.452 0.183 54.897 56.60 319.9 0.016\nChe2 1.968 0.261 0.199 0.510 0.198 66.282 69.68 440.7 0.015\nVer 1.987 0.269 0.202 0.653 0.207 71.126 78.66 451.7 0.015\nMam 2.580 0.353 0.268 3.913 0.277 192.86 280.1 1577 0.016\nSim 2.827 0.335 0.239 2.422 0.202 242.98 302.6 1641 0.016\nYeaU 2.993 0.379 0.272 9.318 0.260 285.51 499.9 3050 0.017\nAba 7.000 1.001 0.703 136.1 0.706 NaN NaN NaN 0.023\nAll differences w.r.t. OPAL are significant (level 0.001, one-sided Wilcoxon signed-rank test)\nmore than 8 (Marg), 13 (Chap), or 84 (Zhao) hours. Their calculations on Aba were aborted after one week."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we addressed the problem of fast, non-myopic active learning for binary classification in cost-sensitive applications. In such applications, unlabelled data is abundant but annotation capacities are limited and require an efficient allocation between labelling candidates. Furthermore, the costs of misclassifications differ between classes, and ultimately the optimal candidate given a remaining labelling budget should be chosen.\nWe proposed a novel approach, OPAL, that optimises probabilistic active learning for such situations. Given the misclassification cost ratio and remaining budget, which are predetermined by the application, our approach follows a smoothness assumption and computes the expected misclassification loss reduction within a candidate’s neighbourhood. For this expectation over the true posterior in the neighbourhood and over the subsequent label realisations therein, we derived a fast, closed-form solution. This allows to select the candidate that reduces the expected misclassification loss in its neighbourhood the most. We have shown that for a myopic setting, our approach runs in asymptotically linear time in the size of the candidate pool. For the non-myopic setting, we have shown that an additional factor that is solely O(m · logm) in the budget size is required. Furthermore, we have illustrated the effect of the non-myopic extension, indicating its usefulness for unequal misclassification costs. This is confirmed in experimental evaluations on several synthetic and real-world data sets, where our approach has shown comparable or better classification performance than several uncertainty sampling- or error-reduction-based active learning strategies, both in cost-sensitive and cost-insensitive settings.\nOur fast approach requires no tunable parameters, yet it is simple to implement, and it neither requires an evaluation sample, nor self-labelling. Thus, its natural extension to data streams has not missed our attention. However, this remains to be done in further research.\nAcknowledgments We would like to thank our colleagues from the University Magdeburg and the KMD Lab, in particular Myra Spiliopoulou, Pawel Matuszyk and Christian Braune. Furthermore, we thank the anonymous reviewers for their helpful comments and suggestions.\nAppendix\nBandwidth (σ ) tuning\nThe plots in Fig. 7 below show the misclassification loss (y-axis) on the test set, given full label information on the training sets of size 40, for different σ -bandwidths of the Parzen window classifier. The maximal bandwidth (leftmost values of the x-axis) corresponds to a non-discriminating classifier, which simply classifies any instance into the class with higher misclassification cost. Thus, ideally each cost-ratio/data-set-combination exhibits a unique minimum that is smaller than this rightmost value.Combinationswithmonotonically decreasing misclassification loss curves indicate ill-posed learning problems.\nCost-sensitivity (cont.)\nIn Fig. 8, we continue the results from Fig. 4 on the relevance of the cost-sensitiveness, for details see Sect. 4.2.\nDetailed derivation of Eq. 24 from Eq. 22\nStarting with Eq. 22, we apply Eq. 12, the misclassification loss def. from Eq. 16 and expand the latter by the cost-optimal classification rule from Eq. 18:\nEcur = ∫ 1\n0 Betaα,β(p) · ML p,τ ( p̂) dp (36)\n= ∫ 1\n0\nΓ (α + β) Γ (α)Γ (β) pα−1(1 − p)β−1 · q(τ − p) + p(1 − τ) dp (37)\n= ∫ 1\n0\nΓ (α + β) Γ (α)Γ (β)\npα−1(1 − p)β−1 · ⎧ ⎨\n⎩ 0(τ − p) + p(1 − τ) p̂ < τ (1 − τ)(τ − p) + p(1 − τ) p̂ = τ 1(τ − p) + p(1 − τ) p̂ > τ dp\n(38)\n= ∫ 1\n0\nΓ (α + β) Γ (α)Γ (β)\npα−1(1 − p)β−1 · ⎧ ⎨\n⎩ p(1 − τ) p̂ < τ (1 − τ)τ p̂ = τ τ(1 − p) p̂ > τ dp (39)\nFollowing Eq. 12, we set α = n p̂ + 1 and β = n(1 − p̂) + 1, and obtain Eq. 24. Detailed derivation of Eq. 32 from Eq. 31\nUsing the definition of the Beta Integral\n∫ 1\n0 xa(1 − x)b dx = Beta(a + 1, b + 1) = Γ (a + 1)Γ (b + 1) Γ (a + b + 2) (40)\nand setting\na = ⎧ ⎨\n⎩ n p̂ + k + 1 n p̂+kn+m < τ n p̂ + k n p̂+kn+m ≥ τ\n(41)\nb = ⎧ ⎨\n⎩ n + m − n p̂ − k n p̂+kn+m ≤ τ n + m − n p̂ − k + 1 n p̂+kn+m > τ\n(42)\nwe can express the first factors in the integral in Eq. 31 and thus derive Eq. 32:\nIML(n, p̂, τ,m, k) = ( m k ) · ∫ 1 0 pn· p̂+k · (1 − p)n+m−n· p̂−k ·\n⎧ ⎪⎪⎪⎨\n⎪⎪⎪⎩ p · (1 − τ)dp n p̂+kn+m < τ (τ − τ 2)dp n p̂+kn+m = τ τ · (1 − p)dp n p̂+kn+m > τ\n(43)\n= ( m k ) ·\n⎧ ⎪⎪⎪⎨\n⎪⎪⎪⎩\n(1 − τ) · Γ (1−k+m+n−n p̂)Γ (2+k+n p̂) Γ (3+m+n) n p̂+k n+m < τ (τ − τ 2) · Γ (1−k+m+n−n p̂)Γ (1+k+n p̂) Γ (2+m+n) n p̂+k n+m = τ τ · Γ (2−k+m+n−n p̂)Γ (1+k+n p̂) Γ (3+m+n) n p̂+k n+m > τ\n(44)"
  }],
  "year": 2015,
  "references": [{
    "title": "Imbalanced learning: Foundations, algorithms, and applications, chap",
    "authors": ["J. Attenberg", "S. Ertekin"],
    "venue": "Class Imbalance and Active Learning,",
    "year": 2013
  }, {
    "title": "Active learning for parzen window classifier",
    "authors": ["O. Chapelle"],
    "venue": "In Proceedings of the tenth international workshop on artificial intelligence and statistics,",
    "year": 2005
  }, {
    "title": "Semi-supervised learning",
    "authors": ["O. Chapelle", "B. Schölkopf", "A. Zien"],
    "year": 2006
  }, {
    "title": "Smote: Synthetic minority oversampling technique",
    "authors": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"],
    "venue": "Journal Artificial Intelligence Research (JAIR),",
    "year": 2002
  }, {
    "title": "Encyclopedia of machine learning (pp. 10–14)",
    "authors": ["D. Cohn"],
    "venue": "Active learning",
    "year": 2010
  }, {
    "title": "Active learning with statistical models",
    "authors": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 1996
  }, {
    "title": "Metacost: A general method for making classifiers cost-sensitive",
    "authors": ["P. Domingos"],
    "venue": "Proceedings of the fifth ACM SIGKDD international conference on knowledge discovery and data mining,",
    "year": 1999
  }, {
    "title": "The foundations of cost-sensitive learning",
    "authors": ["C. Elkan"],
    "venue": "In B. Nebel (Ed.) Proceedings of the seventeenth international joint conference on artificial intelligence,",
    "year": 2001
  }, {
    "title": "An online strategy for safe active learning. In ICML workshop on combining learning strategies to reduce label cost",
    "authors": ["Z. Ferdowsi", "R. Ghani", "M. Kumar"],
    "year": 2011
  }, {
    "title": "Labeling examples that matter: Relevance-based active learningwith gaussian processes",
    "authors": ["A. Freytag", "E. Rodner", "P. Bodesheim", "J. Denzler"],
    "venue": "InGerman conference on computer vision (GCPR),",
    "year": 2013
  }, {
    "title": "A survey on instance selection for active",
    "authors": ["Y. Fu", "X. Zhu", "B. Li"],
    "venue": "learning.Knowledge and Information Systems,",
    "year": 2012
  }, {
    "title": "The digital universe in 2020: Big data, bigger digital shadows, and biggest growth in the far east",
    "authors": ["J. Gantz", "D. Reinsel"],
    "venue": "http://estonia.emc.com/collateral/analyst-reports/",
    "year": 2012
  }, {
    "title": "Bayesian optimal active search and surveying",
    "authors": ["R. Garnett", "Y. Krishnamurthy", "X. Xiong", "J.G. Schneider", "R. Mann"],
    "venue": "In Proceedings of the 29th international conference on machine learning (ICML",
    "year": 2012
  }, {
    "title": "Big data, big business: Bridging the gap. In Proceedings of the 1st international workshop on big data (pp. 7–11). Streams and heterogeneous source mining: Algorithms, systems, programmingmodels and applications, BigMine’12 NewYork, NY: ACM",
    "authors": ["V. Gopalkrishnan", "D. Steier", "H. Lewis", "J. Guszcza"],
    "year": 2012
  }, {
    "title": "Measuring classifier performance: A coherent alternative to the area under the roc curve",
    "authors": ["D.J. Hand"],
    "venue": "Machine Learning,",
    "year": 2009
  }, {
    "title": "Imbalanced learning: Foundations, algorithms, and applications",
    "authors": ["H. He", "Y. Ma"],
    "year": 2013
  }, {
    "title": "Probabilistic active learning: A short proposition",
    "authors": ["G. Krempl", "D. Kottke", "M. Spiliopoulou"],
    "venue": "In Proceedings of the 21st European conference on artificial intelligence (ECAI2014), August 18–22,",
    "year": 2014
  }, {
    "title": "Probabilistic active learning: Towards combining versatility, optimality and efficiency",
    "authors": ["G. Krempl", "D. Kottke", "M. Spiliopoulou"],
    "venue": "In Proceedings of the 17th international conference on discovery science (DS),",
    "year": 2014
  }, {
    "title": "Open challenges for data stream mining research",
    "authors": ["G. Krempl", "I. Zliobaitė", "D. Brzeziński", "E. Hüllermeier", "M. Last", "V. Lemaire", "T. Noack", "A. Shaker", "S. Sievi", "M. Spiliopoulou", "J. Stefanowski"],
    "venue": "SIGKDD Explorations. Special Issue on Big Data (to appear)",
    "year": 2014
  }, {
    "title": "A sequential algorithm for training text classifiers",
    "authors": ["D.D. Lewis", "W.A. Gale"],
    "venue": "In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval (pp. 3–12)",
    "year": 1994
  }, {
    "title": "A self-training approach to cost sensitive uncertainty",
    "authors": ["A. Liu", "G. Jun", "J. Ghosh"],
    "venue": "sampling.Machine Learning,",
    "year": 2009
  }, {
    "title": "Spatially cost-sensitive active learning",
    "authors": ["A. Liu", "G. Jun", "J. Ghosh"],
    "venue": "In Proceedings of the SIAM international conference on data mining,",
    "year": 2009
  }, {
    "title": "Active cost-sensitive learning",
    "authors": ["D.D. Margineantu"],
    "venue": "In Proceedings of the 19th international joint conference on Artificial intelligence,",
    "year": 2005
  }, {
    "title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes",
    "authors": ["A.Y. Ng", "M.I. Jordan"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2001
  }, {
    "title": "An analysis of performance measures for binary classifiers",
    "authors": ["C. Parker"],
    "venue": "In Proceedings of the 11th IEEE international conference on data mining (ICDM2011),",
    "year": 2011
  }, {
    "title": "Numerical recipes in Fortran 77: The art of scientific computing (2nd ed.)",
    "authors": ["W.H. Press", "B.P. Flannery", "S.A. Teukolsky", "W.T. Vetterling"],
    "year": 1992
  }, {
    "title": "Toward optimal active learning through sampling estimation of error reduction",
    "authors": ["N. Roy", "A. McCallum"],
    "venue": "InProceedings of the 18th international conferenceonmachine learning,",
    "year": 2001
  }, {
    "title": "Active learning for logistic regression",
    "authors": ["A.I. Schein", "L.H. Ungar"],
    "venue": "An evaluation.Machine Learning,",
    "year": 2007
  }, {
    "title": "Active learning literature survey",
    "authors": ["B. Settles"],
    "venue": "Computer Sciences Technical Report 1648,",
    "year": 2009
  }, {
    "title": "Active learning. No. 18 in synthesis lectures on artificial intelligence and machine learning. San Rafael",
    "authors": ["B. Settles"],
    "year": 2012
  }, {
    "title": "Far-sighted active learning on a budget for image and video recognition",
    "authors": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2010
  }, {
    "title": "A near-optimal non-myopic active learningmethod",
    "authors": ["Y. Zhao", "G. Yang", "X. Xu", "Q. Ji"],
    "venue": "InProceedings of the 21st international conference on pattern recognition,",
    "year": 2012
  }, {
    "title": "Active learning with sampling by uncertainty and density for data annotations",
    "authors": ["J. Zhu", "H. Wang", "B.K. Tsou", "M.Y. Ma"],
    "venue": "IEEE Transactions on Audio, Speech and Language Processing,",
    "year": 2010
  }, {
    "title": "Active learning with drifting streaming data",
    "authors": ["I. Zliobaitė", "A. Bifet", "B. Pfahringer", "G. Holmes"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
    "year": 2013
  }],
  "id": "SP:038b36e3104f23b57a443329379262c18d451c56",
  "authors": [{
    "name": "Georg Krempl",
    "affiliations": []
  }, {
    "name": "Daniel Kottke",
    "affiliations": []
  }, {
    "name": "Vincent Lemaire",
    "affiliations": []
  }],
  "abstractText": "In contrast to ever increasing volumes of automatically generated data, human annotation capacities remain limited. Thus, fast active learning approaches that allow the efficient allocation of annotation efforts gain in importance. Furthermore, cost-sensitive applications such as fraud detection pose the additional challenge of differing misclassification costs between classes. Unfortunately, the few existing cost-sensitive active learning approaches rely on time-consuming steps, such as performing self-labelling or tedious evaluations over samples. We propose a fast, non-myopic, and cost-sensitive probabilistic active learning approach for binary classification. Our approach computes the expected reduction in misclassification loss in a labelling candidate’s neighbourhood. We derive and use a closedform solution for this expectation, which considers the possible values of the true posterior of the positive class at the candidate’s position, its possible label realisations, and the given labelling budget. The resulting myopic algorithm runs in the same linear asymptotic time as uncertainty sampling, while its non-myopic counterpart requires an additional factor of O(m · logm) in the budget size. The experimental evaluation on several synthetic and realworld data sets shows competitive or better classification performance and runtime, compared to several uncertainty samplingand error-reduction-based active learning strategies, both in cost-sensitive and cost-insensitive settings.",
  "title": "Optimised probabilistic active learning (OPAL)"
}