{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In this paper, we study reinforcement learning (RL) problems where the agent receives rich sensory observations from the environment, forms complex contexts from sensorimotor streams, uses function approximation to generalize to unseen contexts, and must perform systematic exploration to learn efficiently. Such problems are at the core of empirical RL research (e.g., Mnih et al., 2015; Bellemare et al., 2016), yet no existing theory provides rigorous and satisfactory guarantees in a general setting. This situation motivates an important question: how can we solve RL problems where exploration is critical and the agent receives rich observations, in a sample-efficient manner?\nTo answer the question, we propose a new formulation, Contextual Decision Processes (CDPs), to capture a large class of sequential decision-making problems: CDPs gen-\n1University of Michigan, Ann Arbor 2University of Massachusetts, Amherst 3Microsoft Research, New York. Correspondence to: Nan Jiang <nanjiang@umich.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\neralize MDPs where the state forms the context (Ex. 1) and POMDPs where the history forms the context (Ex. 2), and can be much more concise than alternative formulations based on sufficient statistics (e.g., Hutter, 2005). We define CDPs in Section 2, and the learning goal is to find a nearoptimal policy for a CDP with the help of a value-function approximator in a sample-efficient manner.1\nA structural assumption: When the context space is very large or infinite, as is common in practice, lower bounds that are exponential in the problem horizon preclude efficient learning in CDPs, even when simple function approximators are used. However, RL problems arising in applications are often far more benign than the pathological lower bound instances, and we identify a structural assumption capturing this intuition. As our first major contribution, we define a notion of Bellman factorization (Definition 5) in Section 3, and focus on problems with low Bellman rank.\nAt a high level, Bellman rank is an algebraic dimension capturing the interplay between the CDP and the valuefunction approximator that we show is small for many previously-studied settings. For example, every MDP with a tabular value-function has Bellman rank bounded by the rank of its transition matrix, which is at most the number of states but can be considerably smaller. For a POMDP with reactive value-functions, the Bellman rank is at most the number of hidden states and has no dependence on the observation space. We provide other instances of low Bellman rank including Linear Quadratic Regulators and Predictive State Representations. Overall, CDPs with a small Bellman rank yield a unified framework for a large class of sequential decision making problems.\nA new algorithm: Our second contribution is a new algorithm for episodic RL called OLIVE (Optimism Led Iterative Value-function Elimination), detailed in Section 4.1. OLIVE iteratively refines a space of candidate Q-value functions F . At each iteration, it chooses a value function f using an optimistic criterion and collects trajectories from the corresponding greedy policy πf . If πf attains a high-value, the algorithm terminates and outputs f . Other-\n1Throughout the paper, by sample-efficient we mean a number of trajectories that is polynomial in the problem horizon, number of actions, Bellman rank (to be introduced), and polylogarithmic in the number of candidate value-functions.\nwise, it eliminates all g ∈ F which violate certain Bellman equations under trajectories generated by πf and performs the next iteration with this refined class of functions.\nA PAC guarantee: We prove that OLIVE performs sample-efficient learning in CDPs with a small Bellman rank (Section 4.2). Concretely, when the Q?function for the CDP is contained in F , OLIVE requires Õ(M2H3K log(N/δ)/ 2) trajectories to find an - suboptimal policy,2 where M is the Bellman rank, H is the length of a trajectory, K is the number of actions, N is the cardinality of F , and δ is the failure probability.\nImportantly, the sample complexity bound has a logarithmic dependence on N , enabling powerful function approximation, and no direct dependence on the size of the context space, which can be very large or infinite. As many existing models, including the ones highlighted in Table 1, have low Bellman rank, the result immediately implies sampleefficient learning in all of these settings.3\nThe main PAC-guarantee can be extended in several ways, discussed in Appendix A. Specifically, OLIVE is robust to the failure of our assumptions, can adapt to unknown Bellman rank, and can handle infinite function classes with bounded statistical complexity. These extensions demonstrate that the Bellman rank robustly captures the difficulty of exploration in sequential-decision making problems.\nTo summarize, this work advances our understanding of RL with complex observations where long-term planning and exploration are critical. While OLIVE represents an exponential advance in statistical efficiency, its computational complexity, which is polynomial in N , is intractable for the powerful function classes of interest. This computational issue must be addressed before we can empirically evaluate the effectiveness of the proposed algorithm. We discuss this and other future directions in Section 6.\nRelated work. There is rich theoretical literature on RL in tabular settings, including MDPs (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003; Strehl et al., 2006) and POMDPs (Azizzadenesheli et al., 2016) with small state\n2A logarithmic dependence on a norm parameter ζ is omitted here, as ζ is polynomial in most cases.\n3Our algorithm requires discrete action spaces and does not immediately apply to LQRs; see more discussion in Section 3.\nand observation spaces, with an emphasis on sophisticated exploration to find near-optimal policies in a sampleefficient manner. While there have been extensions to large state spaces (Kakade et al., 2003; Jong & Stone, 2007; Pazis & Parr, 2016), these approaches fail to be a good fit for practical scenarios where the environment is typically perceived through complex observations such as image, text, or audio signals. Alternatively, Monte Carlo Tree Search (MCTS) methods can handle large state spaces, but only at the cost of exponential dependence on the planning horizon (Kearns et al., 2002; Kocsis & Szepesvári, 2006).\nClosest to our work are the results of Wen & Van Roy (2013) and Krishnamurthy et al. (2016), which also obtain sample complexity independent of the number of unique contexts, but only under deterministic dynamics and other special structures. In contrast, we study a much broader class of problems with relatively mild conditions.\nOn the empirical side, recent successes on both the Atari platform (Mnih et al., 2015; Wang et al., 2015) and Go (Silver et al., 2016) have sparked a flurry of research interest. These approaches leverage advances in deep learning for powerful function approximation, but typically use simple strategies, such as -greedy, for exploration. Better exploration strategies, such as pseudo-counts in Bellemare et al. (2016), and combining MCTS with function approximation (e.g., Silver et al. (2016)), typically require strong domain knowledge and large amounts of data to be successful.\nHallak et al. (2015) have proposed Contextual MDPs, where each context parametrizes an MDP. In contrast, our use of contexts is in analogy with contextual bandits (Langford & Zhang, 2008), and is similar to state features in RL."
  }, {
    "heading": "2. Contextual Decision Processes (CDPs)",
    "text": "In this section, we introduce a new model, called a Contextual Decision Process, as a unified framework for reinforcement learning with rich observations."
  }, {
    "heading": "2.1. Model and Examples",
    "text": "CDPs make minimal assumptions to capture a general class of RL problems and are defined as follows.\nDefinition 1 (Contextual Decision Process (CDP)). A\n(finite-horizon) CDP is defined as a tuple (X ,A, H, P ), where X is the context space, A is the action space, and H is the horizon of the problem. P = (P∅, P+) is the system descriptor, where P∅ ∈ ∆(X ) is a distribution over initial contexts, that is x1 ∼ P∅, and P+ : (X × A × R)∗ × X × A → ∆(R × X ) elicits the next reward and context from the interactions so far x1, a1, r1, . . . , xh, ah:\n(rh, xh+1) ∼ P+(x1, a1, r1, . . . , xh, ah).\nIn a CDP, the agent interacts with the environment in episodes. In an episode, the agent observes a context x1, takes action a1, receives reward r1 and observes x2, repeating H times. A policy π : X → A specifies the agent’s decision-making strategy, i.e. ah = π(xh), ∀h ∈ [H], and induces a distribution over trajectories (x1, a1, r1, . . . , xH , aH , rH , xH+1) via the system descriptor P . The value of a policy, V π , is defined as\nV π = EP [∑H h=1 rH ∣∣∣ a1:H ∼ π] , (1) where a1:H ∼ π abbreviates for a1 = π(x1), . . . , aH = π(xH). Throughout, the expectation is always taken over contexts and rewards drawn according to the system descriptor P , so we suppress the subscript P . The goal of the agent is to find a policy π that attains the largest value.\nCDPs capture classical RL models, like MDPs and POMDPs, with appropriately chosen contexts:\nExample 1 (MDPs with states as contexts). Consider a finite-horizon MDP (S,A, H,Γ1,Γ, R), where S is the state space, A is the action space, H is the horizon, Γ1 ∈ ∆(S) is the initial state distribution, Γ : S × A → ∆(S) is the state transition function, R : S × A → ∆([0, 1]) is the reward function, and an episode takes the form of (s1, a1, r1, . . . , sH , aH , rH). We can convert the MDP to a CDP (X ,A, H, P ) by letting X = S × [H] and xh = (sh, h), which allows the set of policies {X → A} to contain the optimal policy (Puterman, 1994). The system descriptor is P = (P∅, P+), where P∅(x1) = Γ1(s1), and P+(rh, xh+1 |x1, a1, r1, . . . , xh, ah) = R(rh|sh, ah) Γ(sh+1|sh, ah).\nAs above, the system descriptor for a model is usually obvious and we omit its specification in the following examples. Turning to POMDPs, it might seem that a CDP limits the agent’s decision-making strategies to memoryless (or reactive) policies, as we only consider policies in {X → A}. This is not true. We clarify this issue by showing that we can use the history as context, and the induced CDP suffers no loss in the ability to represent optimal policies.\nExample 2 (POMDPs with histories as contexts). Consider a finite-horizon POMDP with a hidden state space S, an observation spaceO, and an emission processDs specifying a distribution overO. We can convert the POMDP to\na CDP (X ,A, H, P ) by lettingX = (O×A×R)∗×O and xh = (o1, a1, r1, . . . , oh) is the observed history at level h.\nOur next example considers a POMDP where the context can be substantially more concise than the full history. As will be formalized in Section 2.2, all we need is that the context can express a good value function, which is significantly weaker than requiring it be a sufficient statistic (unlike e.g., Hutter 2005). Therefore, it is important to separate the context in a CDP from any precise notion of state in the process, and instead keep it as a modeling choice.\nExample 3 (POMDPs with sliding windows of observations as contexts). Sometimes partial observability can be resolved by using a small history: for example, in Atari games, it is common to keep track of the last 4 images (Mnih et al., 2015). In this case, we can represent the problem as a CDP by letting xh = (oh−3, oh−2, oh−1, oh).\nWe hope the above examples demonstrate the generality and flexibility of the CDP framework. Finally, we introduce a regularity assumption on the rewards.\nAssumption 1 (Boundedness of rewards). We assume that regardless of how actions are chosen, for any h = 1, . . . ,H , rh ≥ 0 and ∑H h=1 rh ≤ 1 almost surely.4"
  }, {
    "heading": "2.2. Value-based RL and Function Approximation",
    "text": "A CDP makes no assumptions on the cardinality of the context space, which makes it critical to generalize across contexts, since an agent might not observe the same context twice. Hence, we consider value-based RL with function approximation. That is, the agent is given a set of functions F ⊆ X × A → [0, 1] and uses it to approximate an action-value function (or Q-function). To avoid imposing boundary-conditions, we set f(xH+1) ≡ 0 w.l.o.g. For ease of presentation, we assume that F is finite with |F| = N < ∞ throughout the paper. In Appendix A.3 we allow infinite function classes with bounded complexity.\nAs in typical value-based RL, the goal is to identify f ∈ F which respects a particular set of Bellman equations and achieves a high value with its greedy policy πf (x) = argmaxa∈A f(x, a). We next set up the appropriate extensions of Bellman equations to CDPs and the optimal value V ?F through a series of definitions. Unlike MDPs, these involve both the CDP and function approximator F . Definition 2 (Average Bellman error). Given a policy π : X → A and a function f : X × A → [0, 1], the average Bellman error of f under π at level h is defined as\nE(f, π, h) = E [ f(xh,ah)− rh − f(xh+1, ah+1)∣∣ a1:h−1 ∼ π, ah:h+1 ∼ πf ]. (2)\n4The bound of 1 is w.l.o.g. More generally, we may simply replace with /R in all the sample complexity results when the bound isR. See more discussion in Kakade (2003, Section 2.2.3).\nThe average Bellman error measures the self-consistency of f between its predictions at levels h and h+ 1, when all the previous actions are taken according to some policy π. We now define a set of Bellman equations.\nDefinition 3 (Bellman equations and validity of f ). Given an (f, π, h) triple, a Bellman equation posits E(f, π, h) = 0. We say f ∈ F is valid if the Bellman equation on (f, πf ′ , h) holds for every f ′ ∈ F , h ∈ [H].\nNote that the validity assumption only considers roll-ins according to the greedy policies πf , which is the natural policy class given F . In MDPs, each Bellman equation can be viewed as the linear combination of the standard Bellman optimality equations for Q?,5 where the coefficients are the probabilities with which the roll-in policy π visits each state. This leads to the following consequence.\nFact 1 (Q? is always valid). Given an MDP and a space of functions F : S × [H]×A → [0, 1], if Q? ∈ F , then in the corresponding CDP with X = S × [H], Q? is valid.\nWhile Q? satisfies the Bellman equations and yields the optimal policy π? = πQ? , there can be other functions which also satisfy the equations while yielding suboptimal policies. For instance, if f(x, πf (x)) correctly predicts the long-term reward of πf , then f is always valid. Since validity alone does not imply that we get a good policy, it is natural to search for a valid value function which also induces a high-value policy. We formalize this goal next.\nDefinition 4 (Optimal value). Define\nf? = argmax f∈F : f is valid V πf , and V ?F = V πf? .\nFact 2. In the setting of Fact 1, we have f? = Q? ∈ F , and V ?F = V ?, which is the optimal long-term value.\nDefinition 4 implicitly assumes that there is at least one valid f ∈ F . This is weaker than the realizability assumption made in the value-based RL literature, that F contains Q? of an MDP (e.g., Krishnamurthy et al., 2016) (see Facts 1 and 2). While some works only require Q? to be approximately captured (e.g., Antos et al., 2008), our algorithm can also be adapted to work with an approximate notion of validity as discussed in Appendix A.4."
  }, {
    "heading": "3. Bellman Factorization and Bellman Rank",
    "text": "CDPs are general models for sequential decision making, but are there efficient RL algorithms for them?\nUnfortunately, without further assumptions, learning in CDPs is generally hard, since they subsume MDPs and\n5We refer the readers who are not familiar with the definition of Q? to standard texts, such as (Sutton & Barto, 1998).\nPOMDPs with arbitrarily large state/observation spaces. Formally, the sample complexity of learning CDPs in the worst-case is Ω(KH) when K = |A|, even when the complexity of the function class, measured by log |F|, is small. The result is due to Krishnamurthy et al. (2016) and is included in Appendix F.1 for completeness.\nOf course the lower bound instances are quite pathological and devoid of any structure that is often present in real problems. To capture these realistic scenarios, we propose a new complexity measure and restrict our attention to settings where this measure is low. As we will see, this measure is naturally small for many existing models, and, when it is small, efficient reinforcement learning is possible.\nThe complexity measure we propose is a structural characterization of the set of Bellman equations induced by the CDP and the class F (recall Definitions 2 and 3), that we need to check to find valid functions. Checking validity by enumeration is statistically intractable for large F , since it requires Ω(|F|) samples to perform all roll-ins. However, observe that the Bellman equations are structured in tabular MDPs: the average Bellman error under any roll-in policy is a stochastic combination of the single-state errors, and checking the single-state errors (which is tractable) is sufficient to guarantee validity. This observation hints toward a more general phenomenon: whenever the collection of Bellman errors across all roll-in policies can be concisely represented, we may be able to check the validity of all functions in a tractable way.\nThis intuition motivates a new complexity measure that we call the Bellman rank. Define the Bellman error matrices, one for each h, to be |F|× |F| matrices where the (f, f ′)th entry is the Bellman error E(f, πf ′ , h). Informally, the Bellman rank for a CDP and a given value-function class F is a uniform upper bound on the rank of these H Bellman error matrices.\nDefinition 5 (Bellman factorization and Bellman rank). We say that a CDP (X ,A, H, P ) and F ⊂ X ×A → [0, 1] admit Bellman factorization with Bellman rank M and norm parameter ζ, if there exists νh : F → RM , ξh : F → RM for each h ∈ [H], such that for any f, f ′ ∈ F , h ∈ [H],\nE(f, πf ′ , h) = 〈νh(f ′), ξh(f)〉, (3)\nand ‖νh(f ′)‖2 · ‖ξh(f)‖2 ≤ ζ <∞.\nThe exact factorization in Eq. (3) can be relaxed to an approximate version as is discussed in Appendix A.4. Unlike rank-based notions in PSRs (Littman et al., 2001) and multiplicity automata (Schützenberger, 1961), Bellman rank depends both on the process and the class F . In the remainder of this section we showcase the generality of Definition 5 by describing a number of common RL settings that have a small Bellman rank. Throughout, we see how\nthe Bellman rank captures the process-specific structures that allow for efficient exploration. Proofs of all claims in this section are deferred to Appendix B.\nWe start with the tabular MDP setting, and show that the Bellman rank is at most the number of states.\nProposition 1 (Bellman rank bounded by number of states in MDPs). Consider the setting of Example 1 with the corresponding CDP. With any class F , this model admits a Bellman factorization with M = |S| and ζ = 2 √ M .\nThe MDP example is particularly simple as each coordinate of the M -dimensional space corresponds to a state, which is observable. Our next few examples show that this is not necessary, and that the Bellman factorization can be based on latent properties of the process. We next consider large MDPs whose transition dynamics have a low-rank structure. A closely related setting has been considered by Barreto et al. (2011; 2014) where the low-rank structure is exploited to speed up MDP planning, but no sample-efficient RL algorithms were previously known for this setting.\nProposition 2 (Bellman rank in low-rank MDPs, informally). Consider the setting of Example 1 with a transition matrix Γ having rank at most M . The induced CDP along with any F ⊂ X × A → [0, 1] admits a Bellman factorization with Bellman rank M .\nThe next example considers POMDPs with large observations spaces and reactive value functions, where the Bellman rank is at most the number of hidden states.\nProposition 3 (Bellman rank bounded by hidden states in reactive POMDPs). Consider the setting of Example 3 with |S| < ∞ and a sliding window of size 1. Given any F ⊂ X ×A → [0, 1], this model admits a Bellman factorization with M = |S| and ζ = 2 √ M .\nPropositions 2 and 3 can be proved under a unified model that generalizes POMDPs by allowing the transition and reward functions to depend on the observation (Figure 1). This model captures the experimental settings considered in state-of-the-art empirical RL work, where agents act in a grid-world (|S| is small) and receives complex and rich observations such as raw pixel images (|O| is large); see e.g., Johnson et al. (2016). The model also subsumes and generalizes the setting of Krishnamurthy et al. (2016) which requires deterministic transitions in the underlying MDP.\nNext, we consider Predictive State Representations (PSRs), which are models of partially observable systems with parameters grounded in observable quantities (Littman et al., 2001). Similar to the case of POMDPs, we can bound the Bellman rank in terms of the rank of the PSR6 when the candidate value functions are reactive.\n6Every POMDP has an equivalent PSR whose rank is bounded by the number of hidden states (Singh et al., 2004).\nProposition 4 (Bellman rank in PSRs, informally). Consider a partially observable system with observation space O and the induced CDP (X ,A, H, P ) with xh = (oh, h). If the linear dimension of the system (i.e., rank of its PSR model) is at most L, then given any F : X × A → [0, 1], the Bellman rank is bounded by LK.\nThe last example considers a class of linear control problems called Linear Quadratic Regulators (LQRs). We show that the Bellman rank in LQRs is bounded by the dimension of the state space. Unlike previous examples, here we crucially use structure of the quadratic value functions, which is the form Q? takes. Exploration in this class of problems has been previously considered by Osband & Van Roy (2014). Note that the algorithm to be introduced in the next section does not directly apply to LQRs due to the continuous action space, and adaptations that exploit the structure of the action space may be needed.\nProposition 5 (Bellman rank in LQRs, informally). An LQR can be viewed as an MDP with continuous state space Rd and action space RK , where the dynamics are described by some linear equations. Given the function class F which consists of non-stationary quadratic functions of the state, the Bellman rank is bounded by d2 + 1."
  }, {
    "heading": "4. Algorithm and Main Results",
    "text": "In this section we present our algorithm for learning CDPs that have a Bellman factorization with small Bellman rank, along with the main sample complexity guarantee. To aid presentation and help convey the main ideas, we make three simplifying assumptions. We assume that (1) the agent knows the Bellman rank M and the corresponding norm bound, (2) the function class F is finite with cardinality N , and (3) the validity and Bellman factorization conditions (Definitions 3 and 5) hold exactly. We relax these assumptions in Section 5 and Appendix A.\nWe are interested in designing an algorithm for PAC Learning CDPs. We say that an algorithm PAC learns a\nCDP if given F , two parameters , δ ∈ (0, 1), and access to the CDP, the algorithm outputs a policy π̂ with V π̂ ≥ V ?F − with probability at least 1 − δ. The sample complexity is the number of episodes needed to achieve such a guarantee, and is typically expressed in terms of , δ, and other relevant parameters. The goal is to design an algorithm with sample complexity that is Poly(M,K,H, 1/ , log(N), log(1/δ)) where M is the Bellman rank, K is the number of actions, and H is the time horizon. Importantly, the bound allows no dependence on the number of unique contexts |X |."
  }, {
    "heading": "4.1. Algorithm",
    "text": "Pseudocode for our algorithm, OLIVE (Optimism Led Iterative Value-function Elimination), is displayed in Algorithm 1. Theorem 1 describes how to set the parameters nest, neval, n, and φ. For brevity, we introduce a shorthand for empirical Bellman errors given a tuple (x, a, r, x′):\nσ(f, x, a, r, x′) := f(x, a)− r − f(x′, πf (x′)). (4)\nAt a high level, the algorithm aims to eliminate functions f ∈ F that fail to satisfy the validity condition in Definition 3. This is done by Lines 13 and 14 inside the loop of the algorithm. Line 13 uses importance weighting to get an unbiased estimate of E(f, πt, ht), the average Bellman error for function f on roll-in policy πt at time ht. Thus, Line 14 eliminates functions that have high average Bellman error under πt and hence are not valid.\nThe other major component of the algorithm involves choosing the roll-in policy πt and level ht on which to do the learning step. At iteration t, we choose the roll-in policy πt optimistically, by choosing ft that predicts the highest value at the starting context distribution and setting πt = πft . To pick ht, we compute ft’s average Bellman error on its own roll-in distribution (Line 7), and set ht to be any level for which this average Bellman error is high (See Line 11). As we will show, these choices ensure that substantial learning happens on each iteration, guaranteeing that the algorithm uses polynomially many episodes.\nThe last component is the termination criterion. The algorithm terminates if ft has small average Bellman error on its own roll-in distribution at all levels. This criteria guarantees that πt is near optimal.\nComputationally, the algorithm requires enumeration of the value-function class, which we expect to be extremely large or infinite in practice. A computationally efficient implementation is essential for a practical algorithm, which remains an open question. We focus on the sample efficiency of the algorithm in this paper.\nIntuition for OLIVE. To convey intuition, it is helpful to ignore any sampling effects by replacing all empirical esti-\nmates with population values and set to 0. The first important fact is that the algorithm never eliminates a valid function, since the learning step in Line 14 only eliminates a function f if we can find a distribution on which it has a large average Bellman error. If f is valid, then E(f, π, h) = 0 for all π, h, so f is never eliminated.\nThe second fact is that if a function f is valid, then its predicted value is exactly the value achieved by the greedy policy πf , that is Vf = E[f(x1, πf (x1))] = V πf . This is based on the following lemma.\nLemma 1 (Value-function error decomposition). Define Vf = E[f(x1, πf (x1))]. Then ∀f : X ×A → [0, 1],\nVf − V πf = H∑ h=1 E(f, πf , h). (5)\nTherefore, since ft is chosen optimistically as the maximizer of the value prediction among the surviving functions, and since we never eliminate valid functions, if OLIVE terminates, it must output a policy with value V ?F . In the analysis, we incorporate sampling effects to derive robust versions of these facts so the algorithm always outputs a policy that is at most -suboptimal.\nThe more challenging component is bounding the number of iterations of the algorithm, which is critical for obtaining a polynomial sample complexity bound. This argument crucially relies on the Bellman factorization (Definition 5), which enables us to embed the distributions over contexts for any roll-in policy intoM dimensions and measure progress in this low-dimensional space.\nFor now, fix some h and focus on the iterations where ht = h. If we ignore sampling effects we can set φ = 0. By using the Bellman factorization to write E(f, πft , h) as an inner product, we can think of the learning step in Line 14 as introducing a homogeneous linear constraint on the set of ξh(f) vectors: 〈νh(ft), ξh(f)〉 = 0. Now, if we execute the learning step at h again in a later iteration t′, we have 〈νh(ft′), ξh(ft′)〉 6= 0 from Line 11. Importantly, this means that νh(ft′) must be linearly independent from previous νh(ft) since 〈νh(ft), ξh(ft′)〉 = 0. Since every time ht = h, the number of linearly independent constraints increases by 1, the number of iterations where ht = h is at most M , the dimension of the space. Thus the Bellman rank (times H) upper-bounds the number of iterations.\nThe above heuristic reasoning, despite relying on the brittle notion of linear independence, can be made robust. With sampling effects, rather than homogeneous linear equalities, the learning step for level h introduces linear inequality constraints to the ξh(f) vectors. But if f ′ is a surviving function that forces us to train at h, it means that 〈νh(f ′), ξh(f ′)〉 is very large, while 〈νh(·), ξh(f ′)〉 is very small for all previous νh(·) vectors used in the learning\nAlgorithm 1 OLIVE (F ,M, ζ, , δ) – Optimism Led Iterative Value-function Elimination\n1: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x(i)1 } nest i=1. 2: Estimate the predicted value for each f ∈ F : V̂f = 1nest ∑nest i=1 f(x (i) 1 , πf (x (i) 1 )). 3: F0 ← F . 4: for t = 1, 2, . . . do 5: Choose policy ft = argmaxf∈Ft−1 V̂f , πt = πft . 6: Collect neval trajectories by following πt (i.e. a (i) h = πt(x (i) h ) for all h and i = 1, . . . , neval).\n7: Estimate ∀h ∈ [H], Ẽ(ft, πt, h) := 1neval ∑neval i=1 σ(f, x (i) h , a (i) h , r (i) h , x (i) h+1).\n8: if ∑H h=1 Ẽ(ft, πt, h) ≤ 5 /8 then\n9: Terminate and output πt. 10: end if 11: Pick ht ∈ [H] such that Ẽ(ft, πt, ht) ≥ 5 /(8H). 12: Collect n trajectories where a(i)h = πt(x (i) h ) for all h 6= ht and a (i) ht is drawn uniformly at random.\n13: Estimate ∀f ∈ F , Ê(f, πt, ht) := 1n ∑n i=1 1[a (i) ht =πf (x (i) ht )] 1/K σ(f, x (i) ht , a (i) ht , r (i) ht , x (i) ht+1 ). (see Eq. (4))\n14: Learn Ft = { f ∈ Ft−1 : ∣∣∣Ê(f, πt, ht)∣∣∣ ≤ φ} . 15: end for\nstep. Intuitively this means that the new νh(f ′) vector is quite different from all of the previous ones. Our proof uses a volumetric argument to show that this suffices to guarantee substantial learning takes place. In more detail, we track the volume of an enclosing ellipsoid of the surviving ξh(f) functions and show that each time we learn at level h this volume shrinks multiplicatively, which results in an iteration complexity that is linear in MH .\nThe optimistic choice for ft is critical for driving the agent’s exploration. With this choice, if ft is valid, then the algorithm terminates correctly, and if ft is not valid, then substantial progress is made. Thus the agent does not get stuck exploring with many valid but suboptimal functions, which could result in exponential sample complexity."
  }, {
    "heading": "4.2. Sample Complexity",
    "text": "We now turn to the main result, which guarantees that OLIVE PAC-learns Contextual Decision Processes with polynomial sample complexity.\nTheorem 1. For any , δ ∈ (0, 1), any CDP and function classF that admit a Bellman factorization with parameters M and ζ, run OLIVE with the following parameters:\nφ = 12H √ M , nest =\n32\n2 log(6N/δ),\nneval = 288H2\n2 log\n( 12H2M log(6H √ Mζ/ )\nδ\n) ,\nn = 4608H2MK\n2 log\n( 12NHM log(6H √ Mζ/ )\nδ\n) .\nThen, with probability at least 1−δ, OLIVE returns a policy\nπ̂ that satisfies V π̂ ≥ V ?F − (recall Definition 3 for V ?F ), and the number of episodes required is at most7\nÕ ( M2H3K\n2 log(Nζ/δ)\n) . (6)\nThus, if a CDP and function class F admit a Bellman factorization with small Bellman rank and F contains valid functions, OLIVE is guaranteed to find a near optimal valid function using only polynomially many episodes. To our knowledge, this is the most general polynomial sample complexity bound for RL with rich observations and function approximation, as many popular models are shown to admit small Bellman rank (see Section 3, Table 1). The result also certifies that the notion of Bellman factorization, which is quite general, is sufficient for efficient exploration and learning in sequential decision making problems.\nIt is worth briefly comparing this result with prior work.\n1. The most closely related result is the recent work of Krishnamurthy et al. (2016), who also consider episodic RL with infinite observation spaces and function approximation. The model studied there is a CDP with Bellman rankM , so our result applies as is to that setting. Importantly, we eliminate the need for deterministic transitions in that work, while improving the dependence on H and , although with worse scaling in M . We emphasize that our result applies to a much more general class of models.\n2. Several works provide sample complexity bounds for fitted value/policy iteration methods (e.g., Munos\n7We use Õ(·) notation to suppress poly-logarithmic dependence on everything except N and δ.\n(2003); Antos et al. (2008); Munos & Szepesvári (2008)). While these results are relevant, they do not address the exploration issue, which is our main focus, and circumvent it by impliciting assuming an exploratory policy for data collection.\n3. Ng & Jordan (2000) proposed a policy search method for POMDPs called PEGASUS, with a sample complexity that scales polynomially with the statistical complexity of the policy class and the horizon. Despite the powerful result, the algorithm requires careful control over the random numbers that determine the state transitions. While the assumption can hold for certain simulated environments, the scope of applications is relatively limited.\n4. Since CDPs include small-state MDPs (Kearns & Singh, 2002; Brafman & Tennenholtz, 2003; Strehl et al., 2006), the algorithm can be applied as is to these problems. Unfortunately, our sample complexity is polynomially worse than the state of the art Õ(Mpoly(H)K 2 log(1/δ)) bounds for PAC-learning MDPs (Dann & Brunskill, 2015). On the other hand, the algorithm also applies to MDPs with infinite state spaces with Bellman factorizations, which cannot be handled by tabular approaches.\n5. Finally, Contextual Decision Processes also encompass contextual bandits, where the sample complexity is Θ(K log(N)/ 2) (Agarwal et al., 2014). As contextual bandits have M = H = 1, OLIVE achieves the optimal sample complexity in this case.\nTurning briefly to lower bounds, since the CDP setting with Bellman factorization is new, general lower bounds for the broad class do not exist. However, we can use MDP lower bounds for guidance on the question of optimality, since the small-state MDPs in Example 1 are a special case. While no existing MDP lower bounds apply as is (because formulations vary), in Appendix F.2 we adapt ideas from Auer et al. (2002) to obtain a Ω(MKH/ 2) sample complexity lower bound for learning the MDPs in Example 1.\nIn comparison, the sample complexity in Theorem 1 is worse in M,H , and log(N) factors, but of course the small-state MDP is a significantly simpler special case. We leave as future work the question of optimal sample complexity for learning CDPs with low Bellman rank."
  }, {
    "heading": "5. Extensions",
    "text": "The basic result presented here is quite robust and admits many extensions, some of which we briefly describe here; the details are deferred to Appendix A.\n1. Handling infinite function classes with dependence on VC-dimension like quantities. This result uses a\ncontext-value function class G ⊂ X → [0, 1] and a policy class Π ⊂ X → A instead of a context-action value class as in OLIVE, with sample complexity depending on the pseudo-dimension of G and the Natarajan dimension of Π. These are standard measures for regression and multi-class classification, and several natural classes have known bounds.\n2. Competing with approximately valid value-functions with inexact Bellman factorization. For this result, we extend the definition of validity and V ?F (Defs. 3 and 4) to allow small but non-zero Bellman errors, and also only require that the Bellman error matrices have a low rank approximation with small `∞ error. 3. Adapting to unknown Bellman rank. Here we run OLIVE with choices of M growing at a doubling schedule and show that the PAC-guarantee is preserved without loss in sample complexity."
  }, {
    "heading": "6. Discussion",
    "text": "In this paper, we presented a new model for RL with rich observations, called Contextual Decision Processes, and a structural property, the Bellman factorization, of these models that enables sample-efficient learning. The unified approach allows us to address several settings of practical interest that have largely eluded RL theory to date. Our work also elicits several further questions:\n1. Can we obtain a computationally efficient algorithm for some form of this setting? Prior related work (for instance in contextual bandits (Dudik et al., 2011; Agarwal et al., 2014)) used supervised learning oracles for computationally efficient approaches. Is there a suitable oracle for this setting?\n2. The sample complexity depends polynomially on the cardinality of the action space. Can we extend the results to handle large or continuous action spaces (e.g., by incorporating concepts such as Eluder dimension (Russo & Van Roy, 2013))?\n3. Can we address sample-efficient RL given only a policy class rather than a value function class? Empirical approaches often rely on policy gradients, which are subject to local optima. Are there parallel results to this work, without access to value functions?\nResolutions to these questions are important for further connecting RL theory with practice."
  }, {
    "heading": "Acknowledgements",
    "text": "Part of this work was completed while NJ and AK were at Microsoft Research. NJ was partially supported by Rackham Predoctoral Fellowship in University of Michigan."
  }],
  "year": 2017,
  "references": [{
    "title": "Taming the monster: A fast and simple algorithm for contextual bandits",
    "authors": ["Agarwal", "Alekh", "Hsu", "Daniel", "Kale", "Satyen", "Langford", "John", "Li", "Lihong", "Schapire", "Robert E"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Optimal Control: Linear Quadratic Methods",
    "authors": ["Anderson", "Brian D.O", "Moore", "John B"],
    "venue": "Courier Corporation,",
    "year": 2007
  }, {
    "title": "Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path",
    "authors": ["Antos", "András", "Szepesvári", "Csaba", "Munos", "Rémi"],
    "venue": "Machine Learning,",
    "year": 2008
  }, {
    "title": "The nonstochastic multiarmed bandit problem",
    "authors": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"],
    "venue": "SIAM Journal on Computing,",
    "year": 2002
  }, {
    "title": "Reinforcement learning of POMDPs using spectral methods",
    "authors": ["Azizzadenesheli", "Kamyar", "Lazaric", "Alessandro", "Anandkumar", "Animashree"],
    "venue": "Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Policy iteration based on stochastic factorization",
    "authors": ["Barreto", "André da Motta Salles", "Pineau", "Joelle", "Precup", "Doina"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2014
  }, {
    "title": "Reinforcement learning using kernel-based stochastic factorization",
    "authors": ["Barreto", "Andre S", "Precup", "Doina", "Pineau", "Joelle"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Unifying count-based exploration and intrinsic motivation",
    "authors": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Characterizations of learnability for classes of {0",
    "authors": ["Ben-David", "Shai", "Cesa-Bianchi", "Nicolo", "Long", "Philip M"],
    "venue": "n}valued functions. In Conference on Learning Theory,",
    "year": 1992
  }, {
    "title": "The ellipsoid method: A survey",
    "authors": ["Bland", "Robert G", "Goldfarb", "Donald", "Todd", "Michael J"],
    "venue": "Operations research,",
    "year": 1981
  }, {
    "title": "Closing the learning-planning loop with predictive state representations",
    "authors": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"],
    "venue": "International Journal of Robotics Research,",
    "year": 2011
  }, {
    "title": "R-max – a general polynomial time algorithm for near-optimal reinforcement learning",
    "authors": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "Sample complexity of episodic fixed-horizon reinforcement learning",
    "authors": ["Dann", "Christoph", "Brunskill", "Emma"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Efficient optimal learning for contextual bandits",
    "authors": ["Dudik", "Miroslav", "Hsu", "Daniel", "Kale", "Satyen", "Karampatziakis", "Nikos", "Langford", "John", "Reyzin", "Lev", "Zhang", "Tong"],
    "venue": "In Uncertainty in Artificial Intelligence,",
    "year": 2011
  }, {
    "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
    "authors": ["Haussler", "David"],
    "venue": "Information and computation,",
    "year": 1992
  }, {
    "title": "Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-Chervonenkis dimension",
    "authors": ["Haussler", "David"],
    "venue": "Journal of Combinatorial Theory, Series A,",
    "year": 1995
  }, {
    "title": "A generalization of Sauer’s lemma",
    "authors": ["Haussler", "David", "Long", "Philip M"],
    "venue": "Journal of Combinatorial Theory, Series A,",
    "year": 1995
  }, {
    "title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability",
    "authors": ["Hutter", "Marcus"],
    "year": 2005
  }, {
    "title": "The Malmo Platform for artificial intelligence experimentation",
    "authors": ["Johnson", "Matthew", "Hofmann", "Katja", "Hutton", "Tim", "Bignell", "David"],
    "venue": "In International Joint Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Model-based exploration in continuous state spaces",
    "authors": ["Jong", "Nicholas K", "Stone", "Peter"],
    "venue": "In Abstraction, Reformulation, and Approximation,",
    "year": 2007
  }, {
    "title": "On the sample complexity of reinforcement learning",
    "authors": ["Kakade", "Sham"],
    "venue": "PhD thesis,",
    "year": 2003
  }, {
    "title": "Exploration in metric state spaces",
    "authors": ["Kakade", "Sham", "Kearns", "Michael", "Langford", "John"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2003
  }, {
    "title": "Near-optimal reinforcement learning in polynomial time",
    "authors": ["Kearns", "Michael", "Singh", "Satinder"],
    "venue": "Machine Learning,",
    "year": 2002
  }, {
    "title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes",
    "authors": ["Kearns", "Michael", "Mansour", "Yishay", "Ng", "Andrew Y"],
    "venue": "Machine Learning,",
    "year": 2002
  }, {
    "title": "Bandit based MonteCarlo planning",
    "authors": ["Kocsis", "Levente", "Szepesvári", "Csaba"],
    "venue": "In European Conference on Machine Learning,",
    "year": 2006
  }, {
    "title": "PAC reinforcement learning with rich observations",
    "authors": ["Krishnamurthy", "Akshay", "Agarwal", "Alekh", "Langford", "John"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "The epoch-greedy algorithm for multi-armed bandits with side information",
    "authors": ["Langford", "John", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2008
  }, {
    "title": "A unifying framework for computational reinforcement learning theory",
    "authors": ["Li", "Lihong"],
    "venue": "PhD thesis, Rutgers, The State University of New Jersey,",
    "year": 2009
  }, {
    "title": "Predictive representations of state",
    "authors": ["Littman", "Michael L", "Sutton", "Richard S", "Singh", "Satinder"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2001
  }, {
    "title": "Human-level control through deep reinforcement learning",
    "authors": ["Shane", "Hassabis", "Demis"],
    "venue": "Nature,",
    "year": 2015
  }, {
    "title": "Error bounds for approximate policy iteration",
    "authors": ["Munos", "Rémi"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2003
  }, {
    "title": "Finite-time bounds for fitted value iteration",
    "authors": ["Munos", "Rémi", "Szepesvári", "Csaba"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "On learning sets and functions",
    "authors": ["Natarajan", "Balas K"],
    "venue": "Machine Learning,",
    "year": 1989
  }, {
    "title": "Pegasus: A policy search method for large MDPs and POMDPs",
    "authors": ["Ng", "Andrew Y", "Jordan", "Michael"],
    "venue": "In Uncertainty in Artificial Intelligence,",
    "year": 2000
  }, {
    "title": "Model-based reinforcement learning and the eluder dimension",
    "authors": ["Osband", "Ian", "Van Roy", "Benjamin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Some extensions of an inequality of Vapnik and Chervonenkis",
    "authors": ["Panchenko", "Dmitriy"],
    "venue": "Electronic Communications in Probability,",
    "year": 2002
  }, {
    "title": "Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates",
    "authors": ["Pazis", "Jason", "Parr", "Ronald"],
    "venue": "In Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Convergence of Stochastic Processes",
    "authors": ["Pollard", "David"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Eluder dimension and the sample complexity of optimistic exploration",
    "authors": ["Russo", "Dan", "Van Roy", "Benjamin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "On the definition of a family of automata",
    "authors": ["M.P. Schützenberger"],
    "venue": "Information and Control,",
    "year": 1961
  }, {
    "title": "Mastering the game of Go with deep neural networks and tree search",
    "authors": ["Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"],
    "year": 2016
  }, {
    "title": "An upper bound on the loss from approximate optimal-value functions",
    "authors": ["Singh", "Satinder", "Yee", "Richard C"],
    "venue": "Machine Learning,",
    "year": 1994
  }, {
    "title": "Predictive state representations: A new theory for modeling dynamical systems",
    "authors": ["Singh", "Satinder", "James", "Michael R", "Rudary", "Matthew R"],
    "venue": "In Uncertainty in Artificial Intelligence,",
    "year": 2004
  }, {
    "title": "PAC model-free reinforcement learning",
    "authors": ["Strehl", "Alexander L", "Li", "Lihong", "Wiewiora", "Eric", "Langford", "John", "Littman", "Michael L"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2006
  }, {
    "title": "Reinforcement Learning: An Introduction",
    "authors": ["Sutton", "Richard S", "Barto", "Andrew G"],
    "year": 1998
  }, {
    "title": "On minimum volume ellipsoids containing part of a given ellipsoid",
    "authors": ["Todd", "Michael J"],
    "venue": "Mathematics of Operations Research,",
    "year": 1982
  }, {
    "title": "On Khachiyan’s algorithm for the computation of minimum-volume enclosing ellipsoids",
    "authors": ["Todd", "Michael J", "Yıldırım", "E Alper"],
    "venue": "Discrete Applied Mathematics,",
    "year": 2007
  }, {
    "title": "Dueling network architectures for deep reinforcement learning",
    "authors": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Efficient exploration and value function generalization in deterministic systems",
    "authors": ["Wen", "Zheng", "Van Roy", "Benjamin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }],
  "id": "SP:630fb1478b02c18d18ef3d0329b8e5318d3819cb",
  "authors": [{
    "name": "Nan Jiang",
    "affiliations": []
  }, {
    "name": "Akshay Krishnamurthy",
    "affiliations": []
  }, {
    "name": "Alekh Agarwal",
    "affiliations": []
  }, {
    "name": "John Langford",
    "affiliations": []
  }, {
    "name": "Robert E. Schapire",
    "affiliations": []
  }],
  "abstractText": "This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.",
  "title": "Contextual Decision Processes with low Bellman rank are PAC-Learnable"
}