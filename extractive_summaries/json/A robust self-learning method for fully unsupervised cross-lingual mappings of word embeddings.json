{
  "sections": [{
    "text": "ar X\niv :1\n91 2.\n01 70\n6v 2\n[ cs\n.L G\n] 3\nM ar\n2 02\n0\nIn this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. We show that the reproduction of their method is indeed feasible with some minor assumptions. We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper. In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters. We then propose key recommendations that apply to any research project in order to deliver fully reproducible research. Keywords: Machine Learning, Unsupervised Learning, Word Alignment, Cross-Lingual Word Embeddings, Reproducibility"
  }, {
    "heading": "1. Introduction",
    "text": "The cross-lingual mapping of word embeddings is a prob-\nlem that has been studied more thoroughly with the rise of distributed representations induced from neural network architectures (Mikolov et al., 2013b). The goal of this task is to induce automatically a word-to-word translation dictionary D ∈ R|Vs|×|Vt| where D[i, j] = 1 means that the i-th word from the source vocabulary Vs is a translation of the j-th word in the target vocabulary Vt. The data used to learn the mapping from two different languages is two sets of word embeddings Xs and Xt corresponding to the vectors of the source and the target language respectively. The mapping from one language space to the other is usually done with a projection matrix Wt which projects the embeddings of the target language in the same space as the source language (or vice versa), i.e. Xs ≈ XtWt. There are several methods available to achieve such a mapping depending on the resources in hand. Given a dataset of parallel word-aligned data, supervised mapping-based approaches are amongst the most popular to date (Mikolov et al., 2013a; Dinu and Baroni, 2014; Gouws and Søgaard, 2015). Several unsupervised methods (Yang et al., 2018; Conneau et al., 2017) based on Generative Adversarial Neural Networks (GANs) (Goodfellow et al., 2014) have also been proposed in the case where no seed dictionary is available. The method that we reproduce is the work of Artetxe et al. (2018b) (referenced as ”the authors”). It also falls in the unsupervised setting but is based on distances between nearest neighbors to build the initial seed dictionary. We refer the reader to the survey of Ruder et al. (2019) for more details, which provides an extensive overview of the different methods for learning cross-lingual mappings between two different word embedding spaces. On another hand, delivering reproducible research is too often an underestimated concern. The fast pace of the research community makes the verification of every submit-\n†Authors contributed equally to this work.\nted paper barely possible, especially in a boiling community such as natural language processing. Fortunately, we see the rise of different challenges1,2,3 that emphasize the importance of supporting the proposed results with an official code implementation as well as the corresponding dataset. It has even been mandatory to provide the source code and the detailed procedure to obtain the same results like the ones claimed in the paper in the NeurIPS Reproducibility Challenge4. In light of this quest for reproducibility, we hereby propose to reproduce the paper from Artetxe et al. (2018b) in the context of REPROLANG 2020 by also providing the stammering of a methodology for delivering reproducible experiments. Even though this algorithm relies on a stochastic component, we can reproduce the results issued from this algorithm and further analyze its behavior. It is therefore important to provide a clean, readable codebase that supports a clear and concise paper. We begin in section 2. with the problem statement and section 3. with the presentation of the analyzed algorithm. We then present what we reproduced from the original paper’s results as well as how we obtained our results in section 4. We provide recommendations regarding the techniques used to obtain these results and their applicability to other research projects in section 5. and close the analysis with an assessment of the algorithm’s robustness in section 6. We also publish our code online as required 5."
  }, {
    "heading": "2. Problem Statement",
    "text": "Word vectors, often called word embeddings, are distributed representations derived from a textual corpus (Mikolov et al., 2013b; Pennington et al., 2014). The dimension d of these representations often spans from 100 to\n1 https://reproducibility-challenge.github.io/iclr_2019/ 2 https://aaai.org/Conferences/AAAI-19/ws19workshops/#ws16 3 http://rescience.github.io/ 4 https://reproducibility-challenge.github.io/neurips2019/ 5 https://gitlab.com/nicolasgarneau/vecmap/\nusing this commit: b1abbd26\n1,000. One key outcome of learning these word representations is that it associates words in a vocabulary V with a similar meaning (appearing in a similar context) with similar distribution vectors. This set of representations, often called the embedding matrix X ∈ R|V|×d serves as the input for many models in natural language understanding applications such as text classification and machine translation. The paper on which we conduct our reproducibility experiment tackles the task of word vectors space alignment. Given two sets of source and target embedding matrices Xs ∈ R|Vs|×d and Xt ∈ R|Vt|×d induced from two different textual corpora, one tries to find the mappings Ws ∈ Rd×d and Wt ∈ Rd×d such that XsWs ≈ XtWt. The vocabularies Vs and Vt can be of different sizes. Usually, only a subset of the n most frequent words is used for the alignment. The work of the authors strictly focuses on the task of unsupervised bilingual dictionary induction, hence aligning the word vector space of a source language with one of a target language without word-aligned data. Essentially, the original paper’s approach tries to find a good initial solution D0 by aligning word vectors from the source and the target language that have a similar distribution. Their motivation, referred to in the literature as the isometry assumption, is that monolingual word embedding spaces are approximately isomorphic (Vulić et al., 2019). They demonstrated for example that the vector of the word “two” in English has a similar distribution as the word vector “due” in Italian and will be different from the distribution of the word “cane”, also in Italian. Once the initial dictionary D0 is induced from the unsupervised procedure, a self-supervised iteration loop is invoked to refine the mapping from the source to the target language. The whole algorithm is further analyzed in the following section."
  }, {
    "heading": "3. The Proposed Algorithm",
    "text": "In this section, we detail the four different steps of the algorithm proposed by Artetxe et al. (2018b)."
  }, {
    "heading": "3.1. Step 1: Embedding Normalization",
    "text": "To directly quote the original paper, the first step of the proposed method is to length normalize the word embeddings Xs and Xt, then mean center each dimension and finally, length normalize again."
  }, {
    "heading": "3.2. Step 2: Fully Unsupervised Initialization",
    "text": "The next step is a component introduced by the authors in the original paper: the unsupervised seed dictionary initialization. To build the initial dictionary D0, we begin by applying multiple transformations to both Xs and Xt, described as follows. Given one language’s embedding matrix X, we start by computing its Singular Value Decomposition, namely USVT = X. We then compute√ M = USUT where M = XXT = US2UT corresponds to the similarity matrix for the given language’s embedding matrix. Each row (word embedding) of the yielded matrices √ Ms and √ Mt is then sorted independently of other rows and we apply the embedding normalization described in subsection 3.1. A similarity matrix between the two sets of languages is computed K = √ Ms √ Mt T . It is\nimportant to note that before the above steps, a vocabulary cutoff of n = 4, 000 is applied, yielding Xs,Xt ∈ Rn×d. Finally, D0 is built by applying Cross-domain Similarity Local Scaling (CSLS) (Conneau et al., 2017) retrieval onK and bidirectional dictionary induction D0 = DXs→Xt + DXs←Xt . Further details on both CSLS and the bidirectional dictionary induction can be found in ??"
  }, {
    "heading": "3.3. Step 3: Robust Self-Learning",
    "text": "The initial dictionary D0 is rarely a good solution in itself. To overcome this, the authors proposed a self-learning algorithm that iteratively refines the previously induced dictionary. Hartmann et al. (2019) specifically demonstrated that without this algorithm, the unsupervised dictionary induction is worse than vanilla GAN methods such as Conneau et al. (2017). The algorithm comprises two main steps done iteratively until convergence. The first step is to compute the optimal orthogonal mapping maximizing the objective function 6\nargmax Ws,Wt\n∑\ni\n∑\nj\nDij\n( (Xs[i, ∗]·Ws)·(Xt[j, ∗]·Wt) ) (1)\nfor the current dictionary Dt at iteration t. The second step is then to apply nearest neighbor retrieval over the similarity matrix of the mapped embeddings XsWsW T t Xt to yield the next seed dictionary Dt+1 for the next iteration. In order to make the self-learning more robust and achieve better performance, the authors also propose four improvements to the above algorithm: stochasticity in the dictionary induction, a frequency-based vocabulary cutoff, usage of the CSLS instead of the nearest neighbor to compute the optimal dictionary and usage of a bidirectional approach in the dictionary induction. The frequency-based vocabulary cutoff only retains the top n = 20, 000 most frequent words from both embedding matrices. Done after the unsupervised seed dictionary initialization and before the first self-learning step, the objective of this step is to increase the computing efficiency and reduce the complexity of the optimization problem. The proposed value of n was given without much explanation, only saying that it was working well in practice. We chose to further analyze the impact of different values of n in subsection 6.2. The authors proposed a stochastic feature that may be vital for the convergence of the algorithm with some languages. They randomly keep some elements in the similarity matrix yielded at the end of each iteration with a probability of p while the others are ignored. This encourages the exploration of the search space and allows the dictionary to greatly vary between two iterations when p is small. As the algorithm starts to converge, the value of p gradually grows to the maximum value of 1. The initial value of p is set to p0 = 0.1 and it is multiplied by pfactor = 2 every time the objective function (1) didn’t improve of more than a delta value of ǫ = 10−6 in the last 50 iterations. We also chose to further analyze the impact of the (p0, pfactor) combination in subsection 6.2.\n6Here X[i, ∗] denotes the i-th row of the matrix X\nTypically, to compute the optimal dictionary over the mapped embeddings, we use the nearest neighbor retrieval from the source language into the target language but Dinu and Baroni (2014) showed that this approach suffers from the hubness problem. This phenomenon, where many points are universal neighbours to many other points, is an intrinsic problem of high-dimensional spaces (Radovanović et al., 2010). The authors adopted the CSLS introduced by Conneau et al. (2017) which specifically tackles this problem. This approach’s idea is to penalize universal neighbors by subtracting each word’s average cosine similarity to its k nearest neighbors in the other language from the cosine similarity result between words from the target and source languages. Since the value used by the authors is k = 10, as per Conneau et al. (2017)’s recommendation, we again chose to further analyze the impact of different values of k in subsection 6.2. to grasp a better understanding of its impact.\nThe authors also proposed to use a bidirectional approach for the dictionary induction. This improvement is based on the intuition that, when the dictionary is induced from the source into the target language, some of the words may not be present or some may occur numerous times. The authors claim that those target words occurring multiple times may cause a problem of local optima since they may act as an aggregation hub, making it much more difficult to escape from that undesired solution. The bidirectional approach thus uses the concatenation of both mappings, source to target and target to source as the dictionary D, namely D = DXs→Xt +DXs←Xt ."
  }, {
    "heading": "3.4. Step 4: Symmetric Re-Weighting",
    "text": "As explained in Artetxe et al. (2018a), re-weighting the parameters of the target language’s embeddings according to their cross-correlation is beneficial and greatly improved the quality of the induced dictionary. They also showed that using re-weighting and self-learning didn’t seem to work well together since it provokes an accentuation of the local optima problem and discourages the exploration of other possible better regions, which is most of the problem addressed by the four improvements proposed by the authors in the self-learning step. As a result, this step is done only once after the self-learning loop converged. However, unlike Artetxe et al. (2018a) which applied the re-weighting on either the source or the target language, the authors applied the re-weighting to both languages. Using the symmetric approach improves the performance of the system, but it’s not clear why they chose to use a symmetric reweighting instead of a target only re-weighting as proposed in Artetxe et al. (2018a)."
  }, {
    "heading": "4. Reproducing the Results",
    "text": "We focused on reproducing the results of the entire ablation study of Table 4, as well as the “Proposed Method” line from Tables 2 and 3 of the original paper of Artetxe et al. (2018b). The results comprise four different languages, Deutsch (DE), Spanish (ES), Finnish (FI) and Italian (IT). Since we did not have access to the dataset of Zhang et al. (2017), we could not reproduce the results of\nTable 1. We thus discuss in this section the results we obtained for the proposed method and the ablation study with some issues we faced."
  }, {
    "heading": "4.1. Original Results",
    "text": "To reproduce the results of the original paper, we directly used their publicly available codebase7, instead of completely reimplementing their algorithm on our side. As reported in Table 1, we were able to reproduce the original results with their codebase within a negligible difference, most likely due to the stochastic nature of the dictionary induction of the algorithm. Like in the original paper, we provide the best and average (avg) accuracy for every language pair as well as its average runtime (t). We performed 25 runs per target language and, instead of listing the number of successful runs (where accuracy > 5 %), we present the success rate (s). We can see that, as expected, we have a success rate of 1.0. The execution time highly differs from the original paper. It is important to note that even by using the same hardware as the authors (Nvidia Titan Xp GPU), the average runtime for each language is 2 to 4 times longer than the actual runtime reported in the paper. It is an important factor when comes the time to reproduce the results if we have a limited amount of resources at hand. Another thing to keep in mind when using this algorithm is that the frequency-based vocabulary cutoff assumes that the word vectors have been saved in the embedding text file ordered by their frequency in the training corpus. While this is the default behavior of the Fasttext library (Mikolov et al., 2018), it may not always be the case."
  }, {
    "heading": "4.2. Ablation Study",
    "text": "Our reproduction results of the ablation study in the original paper are reported in Table 2. Amongst other things, we note that the accuracy results we obtained are all within the 95 % confidence interval given by our 25 runs, with the only exceptions being the unsupervised initialization ablation and the runtimes. Regarding the unsupervised initialization ablation, we initially faced the challenge of having to reproduce the random seed dictionary initialization that was mentioned in the paper yet missing in the code. We therefore explored two very plausible approaches to random initializations: one where each word of the smallest language is randomly assigned a word from the biggest language (referred to as ‘Random Complete’) and one where a cutoff is done on both languages before the random pairing (referred to as ‘Random Cutoff ’). The first thing to note is that both our tested random initializations reach convergence between 10 and 30 % of the time, in contrast with the authors’ 0 % success rate. Also, when runs beginning with random initialization are successful, the final performance of the algorithm is the same as the one with the full system. This hints that the initial seed dictionary used, whether obtained by unsupervised or random initialization, only affects the difficulty of the optimization problem but not the retrieved solution. Our results also showed great differences in the algorithm’s runtimes, even though we used the same graphics card as\n7https://github.com/artetxem/vecmap\nthe original paper. We also point out that not all ablation study configurations can be run with the same compute power, i.e. when removing the vocabulary cutoff parameter, the matrices no longer fit inside a GPU’s memory and we have to prepare additional RAM space and CPU resources to run the script. However, when we attempted to reproduce the frequency-based vocabulary cutoff ablation where n is set to 100k, we were unable to obtain a single run to complete even after 3 days of computations. This is why we left this line dashed out in Table 2."
  }, {
    "heading": "4.3. Reproduction Issues",
    "text": "While it may seem trivial to use an official implementation to reproduce the results of a paper, the reality is that it often requires a good amount of human effort to run a complete reproduction of the results. The latter is what happened with us when following the given instructions to obtain the reported results. While the code did execute and complete when using the ACL 2018 setting, we had accuracies below the ones expected for each language pair (5 to 7 % below). It is only after the further analysis that we found that the provided setting did not include the CSLS procedure, explaining the different results. After eventually managing to reproduce the reported full algorithm results, we hit another breaking point: the ablation study was not included in the source code. While almost all ablations (except the random initialization, as per subsection 4.2.) could be run from the provided implementation, no script was given to sequentially execute all ablation tests and report the results. We propose some key recommendations on how to address these issues and facilitate reproducibility and reusability when providing an official implementation with a paper in the section 5."
  }, {
    "heading": "5. Recommendations",
    "text": "Reproducing the model and the results of an original paper can be quite a hassle. In this section, we provide a general framework applicable to any Machine Learning project that will help researchers deliver highly reproducible experiments. We begin with minor recommendations regarding the source code provided by the authors. We then propose a way to host the dataset and a tool that handles the download and the upload of it. Since another very important thing to consider when running experiments is to log them all, we hereby propose to automate the logging of the experimentations as well as the gathering of the results. These steps considerably facilitate the automatic generation of tables and graphs as was required for this challenge. Finally, we recommend to use a 100 % reproducible environment to\nrun the experiments, hence to Dockerize the whole project (Cito and Gall, 2016; Hartmann et al., 2019)."
  }, {
    "heading": "5.1. Codebase Recommendations",
    "text": "One thing that every research codebase should have is a list of the external libraries needed to execute the code. In the case of a Python project, it should have a requirements.txt file. This file holds all the python dependencies needed to run the project’s main script. We thus prepared such a file in our codebase since the original codebase was missing one.\nWhen running experiments, it is important to reduce the number of actions a human needs to perform in order to obtain the final results. We then made the training and the evaluation of the algorithm, originally in two separate files, into one single script. This also removes the writing of the mapped embeddings on disk which vastly reduces the amount of disk space needed.\nIn the same line of ideas, the default hyperparameters of the algorithm should be the ones that reproduce the main results of the paper (Table 2 in Artetxe et al. (2018b)). This is why we proposed to explicitly code not only the full algorithm but also every ablation configuration as Experiment classes within our codebase. This abstraction in our source code enables us to easily provide scripts that reproduce our ablation study as well as the hyperparameter grid search conducted in subsection 6.2.\nCoordination between a paper’s key sections and its official implementation is also a concern we wish to raise. When reading a scientific paper, if one wishes to have a closer look at the implementation of a particular algorithm step or section, one should be able to do so without having to understand the entire codebase. This is why we created an exact correspondence between step names in the original paper like ’CSLS Retrieval’ and ’Robust Self-Learning’ and function names in our source code. We argue this name mapping should be easy to implement at the end of the delivery of a research project and that it contributes significantly towards easier reusability of the delivered implementation."
  }, {
    "heading": "5.2. Dataset Handling",
    "text": "Properly handling the benchmark dataset is often an underestimated point. In an iterative and collaborative setting, it is important to efficiently host (when possible) and version the data. We thus recommend a tool designed to handle those two elements flawlessly; Data Version Control (DVC). Similar to standard Version Control Systems (VCS)\nlike Git8, DVC tracks the different state of the dataset during development as well as in between the processing steps before obtaining the final results of the model. While there is no need in our particular reproducibility challenge context to track the different states of the dataset over time, it definitely requires an efficient collaboration environment, hence our choice to use the Python DVC library 9 with Amazon S3 as the remote repository. DVC was designed with large data files in mind, meaning gigabytes or even hundreds of gigabytes in file size. In our case, the original dataset takes up to 6 Gigabytes. The previous way of retrieving the dataset over the network with a standard 20 Mbits/sec internet connexion took up to an hour to complete (including uncompressing the data). Using DVC reduced the retrieval time of the dataset to 3 minutes over the network with the same internet connexion. While retrieving the dataset may seem like a one-time effort during the development of the model, when comes the time to distribute the computation over several machines, one can save valuable time. We also made the dataset available as a public archive10 since it was required by the challenge."
  }, {
    "heading": "5.3. Automatic Experiment Logging",
    "text": "When doing research, it is easy to enter the experiment’s hurry loop; as soon as we have an idea, we code it and we launch our main script without committing the modifications. Grossly keeping track of an architecture and its corresponding results in our head or a spreadsheet is good for nothing when it comes to the time to retrieve and analyze\n8 https://git-scm.com/\n9https://dvc.org 10https://vecmap-submission.s3.amazonaws.com/dataset.tar.gz\npast experiments. We thus propose to automate the process of logging as well as retrieving the results of every experiment in order to reduce the risk of losing experiment information. To this end, we used a flexible yet emergent tool that beautifully solves this problem; MLflow11,12. MLflow provides a model agnostic Python API that lets you track not only the results of a given configuration but also the associated source code, the dataset used, and much more. It has been of great use for the automatic generation of tables and graphs in this actual paper as it is required by the challenge. We also believe it is vital to use such a framework for any scientific team doing serious research to reduce the overhead and stress of manually logging and keeping the information about experimentations, especially considering the low effort it requires to setup."
  }, {
    "heading": "5.4. Dockerization",
    "text": "Docker13 is a software that provides an abstraction of the system libraries, tools, and runtime. A Docker container is essentially a lightweight executable package that can run on every14 environment. In this project, we did face a dependencies problem between the Cupy python library and its associated CUDA drivers. In fact, even with a Docker container and the nvidia-docker15 library, we had to make\n11https://mlflow.org 12One can find numerous alternatives such as Sacred,\nComet.ml or Weights and Biases for example. 13 https://www.docker.com/\n14As long as the environment provides the necessary hardware\nspecifications. 15ht ps:// ithub.com/NVIDIA/nvidia-docker\nsure that the Cupy compiled library matched the actual host’s CUDA drivers. This issue brings the reproducibility of the project at stake when the hardware of the host’s machine differs from the original one. We thus assume that the host machine running our codebase within our provided docker image16 has the requirements such as the CUDA drivers to fulfill the experiments."
  }, {
    "heading": "6. Assessing the Algorithm’s Robustness",
    "text": "Vulić et al. (2019) showed that completely unsupervised word translation approaches tend to fail when language pairs are distant. They however identify Artetxe et al. (2018b)’s algorithm as the current most robust among completely unsupervised approaches. Hence, in order to assess ourselves the algorithm’s robustness, we decided to apply it on other languages that have fewer similarities with the English language. We also conduct a grid search over key hyperparameters which enlightens us on the stability of the whole procedure."
  }, {
    "heading": "6.1. More Languages",
    "text": "We carefully selected four new languages that are characterized by very different roots than the one used in the original paper. We used Estonian (ET) which is a language that gets its root from Finno-Ugric, the same as Finnish. We also selected Persian (FA), Latvian (LV) and Vietnamese (VI). We can see in Table 3 that the results on Estonian corroborate the results from the initial paper where the stochastic dictionary induction step is crucial for proper convergence. We observed similar behavior for the Persian language. Interestingly, even with the full system, the algorithm poorly performs on Latvian and Vietnamese languages. We conducted the same ablation study as with the original languages. We can see that the algorithm does not converge without an unsupervised initialization and without the stochastic procedure. It also struggles to converge on three languages out of four when the CSLS component is turned off. These results clearly show that the proposed method may become brittle when the target language shares fewer commonalities with the source language."
  }, {
    "heading": "6.2. Robustness to Hyperparameters",
    "text": "In order to correctly assess the algorithm’s robustness to variations in one key hyperparameter’s values, we conducted experiments where we fixed all of the parameter values to the default ones and only varied the tested hyperparameter, ensuring adequate conclusions could be drawn. The key parameters we chose to examine are (1) the number of considered neighbors in the CSLS procedure, (2) the number of retained words in the frequency-based vocabulary cutoff and (3) the initial value of p and its growing factor in the stochastic dictionary induction. We then assess each hyperparameter’s impact on both the performance and the execution time (in terms of the number of iterations and/or iteration duration) of the algorithm in order to provide well-informed recommendations.\n16registry.gitlab.com/nicolasgarneau/vecmap\nCSLS We conducted experiments where we varied the k number of neighbors considered in the CSLS procedure from 1 to 20, with results reported on Figure 1. For all language pairs, we denote a variation of approximately 1 % between the highest and lowest accuracy obtained over the evaluated range. These variations are however well in between the 95 % confidence interval region for most of the tested values, suggesting the correlation between the performance and the number of neighbors considered in CSLS is loose. Furthermore, when taking into account that iteration duration only slightly grows with the growth of k, the author’s suggested universal value of k = 10 neighbors considered in the CSLS retrieval procedure appears like a legitimate compromise.\nFrequency-based vocabulary cutoff For this experiment, we only retained the n most frequent words of both languages before launching the self-learning iterative procedure (subsection 3.3.), with values of n ranging from 10k to 30k, with increments of 1k. When increasing the value of n, our results show the system’s accuracy decreasing on Spanish, increasing on both Finnish and Deutsch and attaining a stable range for Italian. While the accuracy difference is very different from one target language to another, the variation on all the tested range is between 1 and 2 %. Regarding the iteration duration’s correlation with the number of retained words, our experiments show a quadratic growth of an iteration’s duration as well as an overall increase in the number of iterations before convergence when increasing n, in line with the original paper’s conclusion. Pairing the highly languagedependent impact of this hyperparameter on the algorithm’s performance with its major impact on its execution time, we conclude that the number of most frequent words retained before the self-learning procedure should be the target of careful finetuning for each target language.\nStochastic dictionary induction For the tests on the stochastic dictionary induction, we considered a linear space of 5 values between 0.05 and 0.3 for the initial keep probability (p0) and a linear space of 4 values between 1.5 and 3 for p’s growth factor (pfactor) and ran tests for each of the 20 total combinations. Our results only show a slight performance difference between all tested value pairs, with all language pairs only varying for less than 1 % and three of the four language pairs varying for less than 0.5 %. One important to note however is that making the algorithm greedier (with a higher value of p0) does not lead to any performance loss: the best performances are rather found when using those high p0 values. Considering the number of iterations only decreases when p0 grows, it appears the original paper’s p0 = 0.1 value only increases the number of iterations without a significant impact on performance. No such conclusion can be drawn for the pfactor hyperparameter, which appears very weakly correlated to overall performance."
  }, {
    "heading": "7. Conclusion",
    "text": "In this paper, we studied the reproducibility of the model proposed by Artetxe et al. (2018b). We found out that their\nEnglish-Deutsch\nEnglish-Spanish\nEnglish-Finnish\nEnglish-Italian\nEnglish-Deutsch\nEnglish-Spanish\nEnglish-Finnish\nEnglish-Italian\nmethod of mapping embeddings between two languages is robust when the languages share commonalities. Otherwise, the approach struggles to learn proper mapping. We also assessed the robustness of the hyperparameters of the algorithm in many languages.\nWe introduced several recommendations regarding the guidelines every researcher should follow in order to deliver fully reproducible research. It is often said that replicability (reproducing the results of a model from a new implementation) is more complicated than reproducibility (reproducing the results from an existing implementation). However, we found out that reproducing the results may become an issue if there are hardware or time constraints as we faced during our experimentations. Indeed, we were able to perform a grid-search on the hyperparameter and\nvalidate the robustness of the algorithm thanks to the 64 GPUs we had in hand, otherwise, it would have taken months to run. That being said, reproducibility is an issue when hardware and time constraints come into play."
  }, {
    "heading": "8. Acknowledgements",
    "text": "This research was enabled in part by support provided by Calcul Québec (https://www.calculquebec.ca/) and Compute Canada (www.computecanada.ca). We also acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Finally, we wish to thank Anders Søgaard for his precious advice and the reviewers for their insightful comments regarding our work and methodology.\nEnglish-Deutsch\nEnglish-Spanish"
  }, {
    "heading": "9. References",
    "text": "Artetxe, M., Labaka, G., and Agirre, E. (2018a). Gener-\nalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In AAAI Conference on Artificial Intelligence, pages 5012–5019.\nArtetxe, M., Labaka, G., and Agirre, E. (2018b). A robust\nself-learning method for fully unsupervised cross-lingual mappings of word embeddings. In ACL, pages 789–798.\nCito, J. and Gall, H. C. (2016). Using docker containers\nto improve reproducibility in software engineering research. In IEEE/ACM, pages 906–907.\nConneau, A., Lample, G., Ranzato, M., Denoyer, L., and\nJégou, H. (2017). Word translation without parallel data. ArXiv, 1710.04087.\nDinu, G. and Baroni, M. (2014). Improving zero-shot\nlearning by mitigating the hubness problem. ArXiv, 1412.6568.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A. C., and Ben-\ngio, Y. (2014). Generative adversarial networks. ArXiv, 1406.2661.\nGouws, S. and Søgaard, A. (2015). Simple task-specific\nbilingual word embeddings. In HLT-NAACL.\nHartmann, M., Kementchedjhieva, Y., and Søgaard, A.\n(2019). Comparing unsupervised word translation methods step by step. In NeurIPS.\nMikolov, T., Le, Q. V., and Sutskever, I. (2013a). Exploit-\ning similarities among languages for machine translation. ArXiv, 1309.4168.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013b). Distributed representations of words and phrases and their compositionality. In NIPS.\nMikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and\nJoulin, A. (2018). Advances in pre-training distributed word representations. In LREC.\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In EMNLP.\nRadovanović, M., Nanopoulos, A., and Ivanović, M.\n(2010). Hubs in space: Popular nearest neighbors in high-dimensional data. JMLR, 11:2487–2531, December. Ruder, S., Vuli’c, I., and Sogaard, A. (2019). A survey of\ncross-lingual word embedding models. Journal of Artificial Intelligence Research, 65:569–631, Aug. Vulić, I., Glavaš, G., Reichart, R., and Korhonen, A.\n(2019). Do we really need fully unsupervised crosslingual embeddings? In EMNLP-IJCNLP, pages 4398– 4409. Yang, P., Luo, F., Wu, S., Xu, J., Zhang, D., and Sun, X.\n(2018). Learning unsupervised word mapping by maximizing mean discrepancy. ArXiv, 1811.00275. Zhang, M., Liu, Y., Luan, H., and Sun, M. (2017). Adver-\nsarial training for unsupervised bilingual lexicon induction. In ACL, pages 1959–1970."
  }],
  "year": 2020,
  "references": [{
    "title": "Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations",
    "authors": ["M. Artetxe", "G. Labaka", "E. Agirre"],
    "venue": "In AAAI Conference on Artificial Intelligence,",
    "year": 2018
  }, {
    "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
    "authors": ["M. Artetxe", "G. Labaka", "E. Agirre"],
    "venue": "In ACL,",
    "year": 2018
  }, {
    "title": "Using docker containers to improve reproducibility in software engineering research",
    "authors": ["J. Cito", "H.C. Gall"],
    "venue": "In IEEE/ACM,",
    "year": 2016
  }, {
    "title": "Word translation without parallel data",
    "authors": ["A. Conneau", "G. Lample", "M. Ranzato", "L. Denoyer", "H. Jégou"],
    "year": 2017
  }, {
    "title": "Improving zero-shot learning by mitigating the hubness problem",
    "authors": ["G. Dinu", "M. Baroni"],
    "year": 2014
  }, {
    "title": "Generative adversarial networks. ArXiv, 1406.2661",
    "authors": ["Y. gio"],
    "year": 2014
  }, {
    "title": "Simple task-specific bilingual word embeddings",
    "authors": ["S. Gouws", "A. Søgaard"],
    "venue": "In HLT-NAACL",
    "year": 2015
  }, {
    "title": "Comparing unsupervised word translation methods step by step. In NeurIPS",
    "authors": ["M. Hartmann", "Y. Kementchedjhieva", "A. Søgaard"],
    "year": 2019
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["T. Mikolov", "Q.V. Le", "I. Sutskever"],
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "year": 2013
  }, {
    "title": "Advances in pre-training distributed word representations",
    "authors": ["T. Mikolov", "E. Grave", "P. Bojanowski", "C. Puhrsch", "A. Joulin"],
    "year": 2018
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C.D. Manning"],
    "year": 2014
  }, {
    "title": "A survey of cross-lingual word embedding models",
    "authors": ["S. Ruder", "I. Vuli’c", "A. Sogaard"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2019
  }, {
    "title": "Do we really need fully unsupervised crosslingual embeddings",
    "authors": ["I. Vulić", "G. Glavaš", "R. Reichart", "A. Korhonen"],
    "venue": "In EMNLP-IJCNLP,",
    "year": 2019
  }, {
    "title": "Learning unsupervised word mapping by maximizing mean discrepancy",
    "authors": ["P. Yang", "F. Luo", "S. Wu", "J. Xu", "D. Zhang", "X. Sun"],
    "year": 2018
  }, {
    "title": "Adversarial training for unsupervised bilingual lexicon induction",
    "authors": ["M. Zhang", "Y. Liu", "H. Luan", "M. Sun"],
    "venue": "In ACL,",
    "year": 2017
  }],
  "id": "SP:a580381e9f29c6b3de87748dda443cef0733139d",
  "authors": [{
    "name": "Nicolas Garneau",
    "affiliations": []
  }, {
    "name": "Mathieu Godbout",
    "affiliations": []
  }, {
    "name": "David Beauchemin",
    "affiliations": []
  }, {
    "name": "Audrey Durand",
    "affiliations": []
  }, {
    "name": "Luc Lamontagne",
    "affiliations": []
  }],
  "abstractText": "In this paper, we reproduce the experiments of Artetxe et al. (2018b) regarding the robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. We show that the reproduction of their method is indeed feasible with some minor assumptions. We further investigate the robustness of their model by introducing four new languages that are less similar to English than the ones proposed by the original paper. In order to assess the stability of their model, we also conduct a grid search over sensible hyperparameters. We then propose key recommendations that apply to any research project in order to deliver fully reproducible research.",
  "title": "A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well"
}