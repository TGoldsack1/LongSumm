{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The problem of aligning sequences is well studied in the literature across many domains, e.g., machine translation (Brown et al., 1993; Dyer et al., 2013; Bahdanau et al., 2014), speech recognition (Graves et al., 2006; 2013), handwriting recognition (Graves et al., 2009; Graves & Schmidhuber, 2009), alignment of books with movies made based on them (Zhu et al., 2015) and more. The alignment is often done sequentially, one step at a time. We propose a neural network architecture, capable of aligning two input sequences globally and at once.\nWe focus on the alignment of source code and its translation to the compiled object code. During compilation, source code typically written in a human-readable high level programming language, such as C, C++ and Java, is transformed by the compiler to object code. Every object code statement stems from a specific location in the source code, and, therefore, there is a statement-level alignment\n1The School of Computer Science, Tel Aviv University 2Facebook AI Research. Correspondence to: Dor Levy <dor.levy@cs.tau.ac.il>, Lior Wolf <wolf@cs.tau.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nbetween source code and object code.\nAs far as we know, statement-by-statement alignment of source- and object-code is not treated in the literature. It is challenging, since the per-statement outcome of the compilation process also depends on other statements of the source code. In addition, this outcome is produced in increasing levels of sophistication that are determined by the compiler’s optimization flags.\nOur compound deep neural network combines one embedding and one RNN per input sequence, a CNN applied to a grid of sequence representation pairs and multiple softmax layers. Training is performed using both real-world and synthetic data that we created for this purpose. The real-world data consists of 53,000 short functions from 90 open-source projects of the GNU project. Three levels of compiler optimization are used and the ground truth alignment labels are extracted from the compiler’s output.\nOur experiments1 show that the neural network presented is able to predict the alignment considerably more accurately than the literature baselines. Moreover, our method is general and transcends the problem of aligning sequences. We demonstrate it by using exactly the same architecture for learning the Traveling Salesman Problem."
  }, {
    "heading": "1.1. Our Contributions",
    "text": "We propose a novel network architecture and challenge it with a difficult alignment problem, which has unique characteristics: the input sequences’ representations are not per token, but per statement (a subsequence of tokens). The alignment is predicted by our architecture not sequentially (e.g., by employing attention), but by considering the entire grid of potential alignments at once. This is done using an architecture that combines a top-level CNN with LSTMs (Hochreiter & Schmidhuber, 1997).\nWhile neural networks have been shown to be capable of aligning sequences in the domain of NLP, where a sentence in one natural (human) language is aligned with its translation (Bahdanau et al., 2014), the current domain has additional challenges. First, each source or object-code statement contains both an operation (a reserved C key-\n1Our code and data are publicly available at: https:// github.com/DorLevyML/learn-align\nword or an opcode) and potentially multiple parameters, and are, therefore, typically more complex than natural language words. Second, highly optimized compilation means that the alignment is highly non-monotonous. Third, the alignment is very often partial, since not all source-code statements are aligned with the object-code statements. Finally, the meaning of each code statement is completely context dependent, since, for example, the variables are reused within multiple statements. In natural languages, the context helps to resolve ambiguities. However, a direct dictionary based alignment already provides a moderately accurate result. In the current domain, the alignment process has to depend entirely on the context."
  }, {
    "heading": "1.2. Related Work",
    "text": "Extensive work was done on the problem of predicting the alignment and computing its probability given a pair consisting of a source sentence and a candidate target sentence (Brown et al., 1993; Dyer et al., 2013). The alignment probability is then used for re-ranking the translation candidates in the translation pipeline.\nBahdanau et al. (2014) propose an architecture for jointly aligning and translating between two languages. The encoder of the source language is based on a bidirectional RNN. During the decoding process, in which the new sentence in the target language is created, an RNN is used to predict one word at a time. This RNN pools as one of its inputs, a weighted combination of the representations of the various words in the source language. The weights of this combination are pseudo-probabilities that represent the similarity of the predicted word in the translated sentence to each of the words in the source sentence. In contrast to our work, the model described in (Bahdanau et al., 2014) implicitly aligns an input sequence to an output sequence, as part of the translation process, while our model explicitly aligns two input sequences. Note that most human languages are relatively similar and are constructed by similar rules. It is unlikely that the same translation architectures could successfully and accurately translate, for example, C code to object code.\nOur work is close in concept to Pointer Networks (Vinyals et al., 2015), where the proposed architecture outputs discrete tokens corresponding to positions in the input sequence. The input sequence is first encoded by an LSTM to a representation sequence. A second LSTM, at each time step, then points to a location in the input sequence through an attention mechanism and given the previously pointed value of the input sequence. Similarly, our architecture also points to locations in an input sequence. However, in contrast to Vinyals et al. (2015), our architecture receives two input sequences and points to locations on a grid formed by the two.\nThe approach that is most closely related to ours is MatchLSTM (Wang & Jiang, 2015). This architecture is used to determine, given a premise sentence and a hypothesis sentence, whether the hypothesis can be inferred from the premise. The Match-LSTM is designed to do so by matching of the hypothesis and premise word-by-word. First, the two sentences are processed using two LSTM networks. A third LSTM then processes sequentially the hypothesis representation sequence and for every word in the hypothesis sentence produces a match score for all words in the premise representation sequence using an attention mechanism. Finally, after the third LSTM is done processing the hypothesis sentence, its last hidden state is used to produce a single prediction for the relation between the hypothesis and the premise. Although our work is aimed to align two sequences, our proposed architecture is far from Match-LSTM. While Match-LSTM matches sequentially every word in the hypothesis sentence to all words in the premise, our architecture represents all the statement pairs as a grid and aligns all of them globally and combined, using a CNN. Another difference is that the alignment produced by Match-LSTM is only implicit, since the goal of the architecture is to predict the relation between the two sequences. In our architecture, the alignment is explicit and fully supervised during training.\nAnother aspect in which our architecture differs from the ones proposed by Bahdanau et al. (2014); Vinyals et al. (2015); Wang & Jiang (2015), is that in order to align statements, it does not learn representations that correspond to tokens in the input sequences, but learns representations that correspond to segments in the input sequence – each segment being a mini-sequence of tokens that corresponds to one statement.\nSequence processing with CNNs The use of CNNs for sequence processing tasks has been expanding recently. Such tasks include sequence encoding (Zhang et al., 2015; Lee et al., 2016), sentiment prediction (Blunsom et al., 2014), document summarization (Denil et al., 2014) and translation (Gehring et al., 2017). One reason is the computational efficiency of CNNs compared to RNNs, which leads to faster computations both on GPU and CPU. Another reason is their ability to capture translation invariant features in text, as shown by, e.g., Allamanis et al. (2016), who use a convolutional attention mechanism in order to generate extreme summarization of source code functions.\nNeural networks and source code tasks Neural networks have been shown to be useful in tasks involving source code. For example, In (Zaremba & Sutskever, 2014) a sequence-to-sequence LSTM learns to execute simple class of python programs only from seeing input-output pairs. In Allamanis et al. (2016), a model learns to generate meaningful summaries to short functions written in Java."
  }, {
    "heading": "2. The Code Alignment Problem",
    "text": "We consider source code written in the C programming language, in which statements are generally separated by a semicolon (;). The compiler translates the source code to object code. For example, the GCC compiler (Stallman et al., 2009) is used. We view the object code as assembly, where each statement contains an opcode and its operands. Since the source code is translated to object code during compilation, there is a well-defined alignment between them, which is known to the compiler. GCC outputs this information when it runs with a debug configuration."
  }, {
    "heading": "2.1. Problem Formalization",
    "text": "In the GCC alignment output, the statement level alignment between source- and object-code is a many-to-one map from object code statements to source code statements: while every object-code statement is aligned to some source-code statement, not all source-code statements are covered. This is due to optimization performed by the compiler. Our definition of a statement is slightly modified, due to the convention used in the GCC alignment output. A C statement can be one of the following: (i) a simple statement in C containing one command ending with a semicolon; (ii) curly parentheses ({,}); (iii) the signature of a function; (iv) one of if(EXP1),\nfor(EXP1;EXP2;EXP3), or while(EXP1), including the corresponding expressions; (v) else or do. Note that the following code\n1 do 2 { 3 a += 4 ; 4 } 5 w h i l e ( i < 5 0 0 ) ;\ncontains five statements (as numbered) since the do, the {, the }, and the while are all separate statements.\nThe object code statements follow the conventional definition, as shown, for example, in assembly code listings. Each statement contains a single opcode such as mov, jne, or pop, and its operands.\nAn example is shown in Fig. 1, which depicts both the source code of a single C language function, which contains M = 10 statements, and the compiled object code of this function, which contains N = 14 statements. The alignment between the two is shown graphically by using a matrix of size N ×M . Each column (row) of this matrix corresponds to one source (object) code statement. The matrix (i, j) element encodes the probability of aligning object-code statement i ∈ 1, . . . , N with the source-code statement j ∈ 1, . . . ,M . Since the matrix is the ground truth label, all probabilities are either 0 (black) or 1 (white). In other words, each row i is a one-hot vector showing the alignment of one object-code statement i.\nAs can be seen in the figure, the first opcode push corresponds to the function’s statement {, that opens the function’s block. As expected, there are also many opcodes that implement the for statement, which comprises comparing, incrementing and jumping.\nThe matrix representation is the target value of the neural alignment network. The network will output the rows of the alignment matrix as vectors of pseudo probabilities. We can view the resulting prediction matrix as a softalignment. In order to obtain hard alignments, we simply take the index of the maximal element in each row.\nAnother dimension in which we challenge our alignment network, is compilation optimization, which drastically changes the object code based on the level of optimization used (see Fig. 1 of supplementary material). This optimization makes the object code more efficient and can render it shorter (more common) or longer than the code without optimization, see supplementary material."
  }, {
    "heading": "3. The Neural Alignment Network",
    "text": "Each statement is treated as a sequence of tokens, where the last token of each such sequence is always the end-ofstatement (EOS) token. A function is given by concatenating all such sequences to one sequence.\nWe employ a compound deep neural network for predicting the alignment, as explained in Sec. 3.2. It consists of four parts: the first part is used for representing each source code statement j as a vector vj . The second part does the same for the object code, resulting in a representation vector ui. The third part processes using a convolutional neural network pairs of vector representations, one of each type, as a multi-channel grid, and produces an alignment score s(i, j). It is not a probability. However, the higher the alignment value, the more likely the two statements are to be aligned. The alignment scores are fed to the top-most part of the network, which computes the pseudo probabilities pij of aligning object code statement i to the source code statement j. Specifically, the fourth part considers for an object-code statement i, all source-code statements j = 1, 2, . . . ,M the alignment score, and employs the softmax function: pij =\nexp(s(ui,vj))∑M k=1 exp(s(ui,vk)) ."
  }, {
    "heading": "3.1. Encoding the Input Statements",
    "text": "Our model incorporates two LSTM networks to encode the sequences, one for each sequence domain: source code and object code. Therefore, we first embed each token in the input sequences in a high-dimensional space. We use different embeddings for source code and for object code, since each is composed of a different vocabulary. The vocabularies are hybrid, in the sense that they consist of both words and characters.\nThe source code vocabulary is a hybrid of characters and the C language reserved words. A C reserved word is embedded to a single vector, while variable names, arguments and numeric values are decomposed to character by character sequences. The vocabulary contains the C language reserved words as atomic units, EOS, and the following single character elements: (i) alphanumeric characters including all letters and digits; (ii) the operators +, -, /, *, &, |, ˆ, ∼, ?; and (iii) the following punctuation marks: (, ), [, ], {, }, <, >, =, !, ,, ’, \", ;, #, \\. Let ε(α) denote the embedding of a C token α to a vector. Then the C code string if (a5<42), for example, is decomposed to the following sequence: ε(if), ε((), ε(a), ε(5), ε(<), ε(4), ε(2), ε()), ε(EOS).\nSimilarly, the object code vocabulary is also a hybrid, and contains opcodes, registers and characters of numeric values and is based on the assembly representation of the object code. The opcode of each statement is one out of dozens of possible values. The operands are either regis-\nters or numeric values. The vocabulary also includes the punctuation marks of the assembly language and, therefore, contains the following types of elements: (i) the various opcodes; (ii) the various registers; (iii) hexadecimal digits; (iv) the symbols (,),x,-,:; and (v) EOS, which ends every statement. Let ε′(β) denote the embedding of an object code token β to a vector. Then the following assembly string mov %eax,-0x8(%rbp), for example, is decomposed to the following sequence: ε′(mov), ε′(%eax), ε′(-), ε′(0), ε′(x), ε′(8), ε′((), ε′(%rbp), ε′()), ε′(EOS)."
  }, {
    "heading": "3.2. Neural Network Architecture",
    "text": "The network architecture is depicted in Fig. 2(a). The input sequences introduce many complex and long-range dependencies. Therefore, the network employs two LSTM encoders: one for creating a representation of the source code statements and one is used for representing the object code statements. In all of our experiments, the LSTMs have one layer and 128 cells.\nRecall that each statement in the input sequences is a sequence of tokens. However, for alignment, only a single vector representation is required per statement. In order to obtain a single vector representation per statement, we sample the representation sequences output by the encoders only at time steps corresponding to EOS’s. It should be noted that information from other statements is not lost, since each RNN activation is affected by other activations in the sequence. Moreover, since EOS is ubiquitous, its representation must be based on its context. Otherwise, it is meaningless. During training, the network learns to create meaningful representations at the location of the EOS inputs.\nThe result of the LSTM encoders areM representation vectors output by the source-code encoding LSTM, denoted by {vj}j∈(1,...,M), and N representation vectors output by the object-code encoding LSTM, denoted by {ui}i∈(1,...,N). The statement representation vectors are then assembled in anN×M grid, such that the (i, j) element is [ui; vj ], where ; denotes vector concatenation. Since each encoder LSTM has 128 cells, the vector [ui; vj ] has 256 channels.\nIn order to transform the statement representation pairs to alignment scores, we employ a decoding Convolutional Neural Network (CNN) over the 256-channel grid. The decoding CNN has five convolutional layers, each with 32 5 × 5 filters followed by ReLU non-linearities, except for the last layer which consists of one 5× 5 filter and no nonlinearities. The CNN output is, therefore, a single channel N ×M grid, s(i, j), representing the alignment score of object code statement i and source code statement j.\nIn the many-to-one alignment problem, the network’s output for each row should contain pseudo probabilities.\nTherefore, we add a softmax layer on top of the list of alignment scores computed for each object-code statement i: s(ui, v1), s(ui, v2), . . . , s(ui, vM ), i.e., there are N softmax layers, each converting M alignment scores to a vector of probabilities {pij}j∈(1,...,M) for each row i ∈ (1, . . . , N).\nDuring training, the Negative Log Likelihood (NLL) loss is used. Let A be the set of N object-code to source-code alignments (i, j). The training loss for a single training sample is given by 1N ∑ (i,j)∈A−log(pij) , i.e., the loss is the mean of NLL values of all N rows."
  }, {
    "heading": "3.3. Local Grid Decoder",
    "text": "For comparison, we also consider a model that performs decoding directly over the statements grid. In this model, the decoder consists only of a single layer network s attached to each one of the NM pairs of object code and source code statement representations (ui and vj). The same network weights are shared between all NM pairs and are trained jointly. This network is given by:\ns(ui, vj) = v T tanh(Woui +Wsvj)\nwhere v, Wo and Ws are the network’s weights. We consider another, simpler version of the Local Grid Decoder, where s(ui, vj) = uTi · vj , i.e., an inner product operation is employed, instead of the single layer network."
  }, {
    "heading": "4. Literature Baseline Methods",
    "text": "In this section, we describe the baseline methods that we compare to our architecture. Our architecture and all baselines use LSTM encoders to encode the input sequences, and softmax layers on top of the decoder output in order to produce an alignment probability, as explained in Sec. 3.2. The architectures differ only in the decoder part that produces alignment scores, i.e., in the model s(i, j). A profound difference between our architecture and the baselines, is that while our architecture predicts the alignment over the whole statements grid at once, the baselines predict the alignment sequentially."
  }, {
    "heading": "4.1. Pointer Network",
    "text": "This baseline adapts the Pointer Network (Ptr-Net) architecture proposed by (Vinyals et al., 2015) in two ways. PtrNet is designed to solve the task of producing a sequence of pointers to an input sequence. The Ptr-Net architecture employs an encoder LSTM to represent the input sequence as a sequence of hidden states ej . A second decoder LSTM then produces hidden states that are used to point to locations in the input sequence via an attention mechanism. Denote the hidden states of the decoder as di. The attention mechanism is then given by:\nuij = v T tanh(W1ej +W2di) j ∈ (1, . . . , n)\npi = softmax(u i)\nwhere n is the input sequence length and pi is the soft prediction at time step i of the decoder LSTM. The input to the decoder LSTM at time step i is argmax\nj (ui−1j ), i.e., the\ninput token ”pointed” by the attention mechanism at the previous step. Thus, the output of the decoder LSTM can be considered as a sequence of pointers to locations at the input sequence.\nSince in the alignment problem we need to align each object code statement to one of the source code statements, we adapt Ptr-Net to produce ”pointers” to the source code statements sequence for every object code statement. The adaptation is not trivial: our problem presents two input sequences, while Ptr-Net is originally designed to handle one. We create two such adaptations, Ptr1 and Ptr2, which are depicted in Fig. 2(b) and (c), respectively.\nIn Ptr1, we employ a Ptr-Net decoder at each time step i over the sequence of object code statement representations ui. The decoder is an LSTM network, whose hid-\nden state hi is fed to an attention model employed over the whole sequence of source code statement representations vj : s(i, j) = vT tanh(Wsvj +Whhi).\nThe outputs s(i, j) of the attention model are used as the alignment scores that will be fed later to the softmax layers. The Ptr-Net decoder receives at each time step i, the source code statement representation that the attention model ”pointed” to at the previous step i − 1, i.e., vpi−1 where pi = argmax\nj (s(i, j)).\nFinally, in order to condition the output of the pointer decoder at the current object code statement representation ui, the input of the pointer decoder LSTM is the concatenation of ui and vpi−1 :\nhi = LSTM([ui; vpi−1 ], hi−1, ci, ci−1),\nwhere ci is the contents of the LSTM memory cells at time step i. At the first time step i = 1, the value of vp0 is the all-0 vector, and h0 is initialized with the last hidden state of the source-code statements encoding LSTM.\nIt should be noted, that at each step, the Ptr-Net decoder sees the current object code statement and the previous ”pointed” source code statement. It means that the LSTM sees the source code statement that is aligned to the previous object code statement. A wiser adaptation would present the Ptr-Net decoder LSTM with the explicit alignment decision, i.e., the previous ”pointed” source code statement and the previous object code statement, such that the input is a pair of two statements that were predicted to align. Thus, in the second adaptation of Ptr-Net to our problem, which we call Ptr2, the input to the Ptr-Net decoder LSTM is the concatenation of ui−1 and vpi−1 :\nhi = LSTM([ui−1; vpi−1 ], hi−1, ci, ci−1).\nThe current object code statement representation ui is then fed directly to the attention model, in addition to the PtrNet decoder output and the source code statement representation: s(i, j) = vT tanh(Woui +Wsvj +Whhi)."
  }, {
    "heading": "4.2. Match-LSTM",
    "text": "This baseline uses the matching scores of the Match-LSTM architecture (Wang & Jiang, 2015). The architecture receives as inputs two sentences, a premise and a hypothesis. First, the two sentences are processed using two LSTM networks, to produce the hidden representation sequences vj and ui for the premise and hypothesis, respectively. Next, attention ai vectors are computed over the premise representation sequence as follows: ai = ∑M k=1 αijvj , where\nαij are the attention weights and are given by\nαij = exp(s(ui, vj))∑M k=1 exp(s(ui, vk))\ns(i, j) = vT tanh(Woui +Wsvj +Whhi−1),\nwhere hi is the hidden state of the third LSTM that processes the hypothesis representation sequence together with the attention vector computed over the whole premise sequence: hi = LSTM([ui; ai], hi−1, ci, ci−1).\nFor further details about the Match-LSTM architecture, see (Wang & Jiang, 2015). In order to adapt Match-LSTM to our problem, we simply substitute the premise (hypothesis) representation sequence with the source (object) code statements representation sequence, and use the matching scores s(i, j) as the alignment scores."
  }, {
    "heading": "5. Evaluation",
    "text": "Data collection We employ both synthetic C functions generated randomly and human-written C functions from real-world projects. In order to generate random C functions, we used pyfuzz, an open-source random program generator for python (Myint, 2013), and modified it so it will output short functions written in C rather than python. For the real-world human-written data set, we used over 53,000 short functions from 90 open-source projects, that are part of the GNU project and are written in C. Among them are grep, nano, etc. Before compilation, we ran only the preprocessor of GCC, in order to clean the sources of non-code text, such as comments, macros, #ifdef commands and more.\nIn order to compile the source code with optimizations, we use the GCC compiler (Stallman et al., 2009) with the optimization levels -O1, -O2 or -O3. Each level turns on additional optimization flags. Each of the datasets of generated and human-written C functions has three parts, each compiled using one of the three mentioned optimization levels. After compilation of the human-written projects, some functions contained object code from other, inline functions. These functions were excluded from the dataset in order to introduce the network with pure translation\npairs, i.e., source code and object code that has originated entirely from it. In addition, we tell GCC to output debugging information that includes the statement-level alignment between each C function and the object code compiled from it. Therefore, each sample in the resulting dataset consists of source code, object code compiled at some optimization level and the statement-by-statement alignment between them. Tab. 1 reports the statistics of the code alignment datasets.\nTraining procedure For each data set, we train one network for all optimization levels. The length of all functions has been limited to 450 tokens. The training set of synthetic functions contains 120,000 samples. The validation and the test sets contain 15,000 samples each. The training, validation and test sets of human-written functions contain 42,391, 5,474 and 5,253 samples, respectively. During training, we use batches of 32 samples each.\nThe weights of the LSTM and attention networks are initialized uniformly in [−1.0, 1.0]. The CNN filter weights are initialized using truncated normal distribution with a standard deviation of 0.1. The biases of the LSTM and CNN networks are initialized to 0.0, except for the biases of the LSTM forget gates, which are initialized to 1.0 in order to encourage memorization at the beginning of training (Józefowicz et al., 2015). The Adam learning rate scheme (Kingma & Ba, 2015) is used, with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, and = 1e− 08."
  }, {
    "heading": "5.1. Alignment Results",
    "text": "Our proposed network and the baseline methods are trained and evaluated over the datasets of synthetic and humanwritten code. Tab. 2 and Tab. 3 present the resulting accuracy, which is computed per object-code statement as follows. First, the network predicts pseudo-probabilities of aligning source code statements to each object code statement. Second, in order to obtain hard alignments, we take the index of the maximal element in each row of the predicted soft alignment matrix. Third, for every object code statement, we count a true alignment only if the aligned source code statement is the ground truth alignment. The accuracy is reported separately for the three optimization levels and for all of them combined. As can be seen in Tab. 2, all models excel over synthetic code, reaching almost perfect alignment accuracy with a slight advantage to our Convolutional and Local Grid Decoders. Tab. 3 shows that the GNU code is more challenging to all methods. Our proposed Grid Decoder models outperform all baseline methods, and the Convolutional Grid Decoder is superior by a substantial margin over the local and inner product alternatives. The Ptr1, Ptr2 and Match-LSTM baselines reach about the same performance. It is an expected result, since these models are very similar: they all employ a decoding LSTM and an attention mechanism, with only small differences in performing the sequential processing of the encoded representation sequences."
  }, {
    "heading": "5.2. Traveling Salesman Problem (TSP)",
    "text": "We perform an additional experiment based on the TSP benchmark presented in (Vinyals et al., 2015) in order to di-\nrectly compare with the Pointer Network architecture (PtrNet), where it was already tested. The input of the TSP problem is a randomly ordered sequence of 2D points. The output is a sequence of all the points reordered, such that the route length (sum of distance between adjacent points) is minimal. For our method, we consider the connectivity matrix of the cycle graph in lieu of the alignment matrix. As reported in (Vinyals et al., 2015), overfitting was observed here. Therefore, we performed the following data augmentation process. For each sample in the training set, the IDs of the 2D points are permuted randomly and independently of the other samples. It is equivalent to randomly shuffling the order of the points in the sample sequence. The IDs in the label are then permuted accordingly, to represent the same target route. During training, the process was repeated at the beginning of every epoch, and independently of past epochs. Fig. 5 depicts an example route and its connectivity matrix before and after permutation of the node IDs. The results are presented in Tab. 4, along with the optimal and approximated results (see (Vinyals et al., 2015) for further details). As can be seen, our method is comparable to the original Ptr-Net model for both n = 5 and n = 10."
  }, {
    "heading": "6. Summary",
    "text": "We present a neural network architecture for aligning two sequences. We challenge our network with aligning source code to its compiled object code, sequences that in some aspects are more complex than human language sentences. Our experiments demonstrate that the proposed architecture is successful in predicting the alignment. On this task, the network outperforms multiple literature baselines such as Pointer Networks and Match-LSTM, suggesting that a global, CNN-based approach to alignment is better than the sequential, RNN-based approach.\nOur model can be used for alignment of any two sequences with a many-to-one map between them, and extended to other graph problems, as demonstrated for TSP."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank Ofir Press, Dotan Kaufman and Shimi Salant for useful advice and insightful discussions. This work was supported by a 2016 ICRC grant."
  }],
  "year": 2017,
  "references": [{
    "title": "A convolutional attention network for extreme summarization of source code",
    "authors": ["M. Allamanis", "H. Peng", "C. Sutton"],
    "venue": "arXiv preprint arXiv:1602.03001,",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "venue": "arXiv preprint,",
    "year": 2014
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["P. Blunsom", "E. Grefenstette", "N. Kalchbrenner"],
    "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics,",
    "year": 2014
  }, {
    "title": "The mathematics of statistical machine translation",
    "authors": ["P.F. Brown", "S.A. Della-Pietra", "V.J. Della-Pietra", "R.L. Mercer"],
    "venue": "Computational Linguistics,",
    "year": 1993
  }, {
    "title": "Modelling, visualising and summarising documents with a single convolutional neural network",
    "authors": ["M. Denil", "A. Demiraj", "N. Kalchbrenner", "P. Blunsom", "N. de Freitas"],
    "venue": "arXiv preprint arXiv:1406.3830,",
    "year": 2014
  }, {
    "title": "A simple, fast, and effective reparameterization of IBM model 2. In HLT-NAACL, pp. 644–648",
    "authors": ["C. Dyer", "V. Chahuneau", "N.A. Smith"],
    "venue": "The Association for Computational Linguistics,",
    "year": 2013
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["J. Gehring", "M. Auli", "D. Grangier", "D. Yarats", "Y.N. Dauphin"],
    "venue": "arXiv preprint arXiv:1705.03122,",
    "year": 2017
  }, {
    "title": "Offline handwriting recognition with multidimensional recurrent neural networks",
    "authors": ["A. Graves", "J. Schmidhuber"],
    "venue": "In NIPS, pp",
    "year": 2009
  }, {
    "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
    "authors": ["A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber"],
    "venue": "In ICML,",
    "year": 2006
  }, {
    "title": "A novel connectionist system for unconstrained handwriting recognition",
    "authors": ["A. Graves", "M. Liwicki", "S. Fernndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2009
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["A. Graves", "A. r. Mohamed", "G. Hinton"],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural Comput.,",
    "year": 1997
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["R. Józefowicz", "W. Zaremba", "I. Sutskever"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Fully character-level neural machine translation without explicit segmentation",
    "authors": ["J. Lee", "K. Cho", "T. Hofmann"],
    "venue": "arXiv preprint arXiv:1610.03017,",
    "year": 2016
  }, {
    "title": "Pyfuzz: Random program generator for python",
    "authors": ["S. Myint"],
    "venue": "https://github.com/myint/pyfuzz,",
    "year": 2013
  }, {
    "title": "Using The GNU Compiler Collection: A GNU Manual For GCC Version 4.3.3",
    "authors": ["Stallman", "R. M"],
    "year": 2009
  }, {
    "title": "Learning natural language inference with LSTM",
    "authors": ["S. Wang", "J. Jiang"],
    "venue": "arXiv preprint arXiv:1512.08849,",
    "year": 2015
  }, {
    "title": "Learning to execute",
    "authors": ["W. Zaremba", "I. Sutskever"],
    "venue": "arXiv preprint arXiv:1410.4615,",
    "year": 2014
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["X. Zhang", "J. Zhao", "Y. LeCun"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
    "authors": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"],
    "venue": "In ICCV,",
    "year": 2015
  }],
  "id": "SP:4bad0167868e8b24b74ff33b0a7058217364df1e",
  "authors": [{
    "name": "Dor Levy",
    "affiliations": []
  }, {
    "name": "Lior Wolf",
    "affiliations": []
  }],
  "abstractText": "We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences – one being the translation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.",
  "title": "Learning to Align the Source Code to the Compiled Object Code"
}