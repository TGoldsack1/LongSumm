{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2018 Conference, pages 391–399, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics\n391"
  }, {
    "heading": "1 Introduction",
    "text": "The language understanding (LU) module is a key component of dialogue system (DS), parsing user’s utterances into corresponding semantic concepts (or semantic slots 1). For example, the utterance “Show me flights from Boston to New York” can be parsed into (from city=Boston, to city=New York) (Pieraccini et al., 1992). Typically, the LU is seen as a plain slot filling task.\n∗The corresponding author is Kai Yu. 1Slot and concept are equal in LU. They will be mixed in\nthe rest of this paper to some extent.\nWith sufficient in-domain data and deep learning models (e.g. recurrent neural networks, bidirectional long-short term memory network), statistical methods have achieved satisfactory performance in the slot filling task recently (Kurata et al., 2016; Vu, 2016; Liu and Lane, 2016).\nHowever, retrieving sufficient in-domain data for training LU model (Tur et al., 2010) is unrealistic, especially when the semantic slot extends or dialogue domain changes. The ability of LU approaches to cope with changed domains and limited data is a key to the deployment of commercial dialogue systems (e.g. Apple Siri, Amazon Alexa, Google Home, Microsoft Cortana etc).\nIn this paper, we investigate substructure of semantic slots to find out slot relations and promote data reuse. We represent semantic slots with a hierarchical structure based on atomic concept tuple, as shown in Figure 1. Each semantic slot is composed of different atomic concepts, e.g. slot “from city” can be defined as a tuple of atoms [“from location”,“city name”],\nand “date of birth” can be defined as [“date”,“birth”].\nUnlike the traditional slot definition on a plain level, modeling on the atomic concepts helps identify linguistic patterns of related slots by atom sharing, and even decrease the required amount of training data. For example, the training and test sets are unmatched in Figure 2, whereas the patterns of atomic concepts (e.g. “from”, “to”, “city”) can be shared.\nIn this paper, we investigate the slot filling task switching from plain slots to hierarchical structures by proposing the novel atomic concept tuples which are constructed manually. For comparison, we also introduce a competitive method which automatically learns slot representation from the word sequence of each slot name. Our methods are applied to value set mismatch and domain adaptation problems on ATIS (Hemphill et al., 1995) and DSTC 2&3 (Henderson et al., 2013) respectively. As shown in the experimental results, the slot-filling based on concept transfer learning is effective in solving the value set mismatch and domain adaptation problems. The concept transfer learning method especially achieves state-of-theart performance (F1-score 96.08%) on the ATIS task.\nThe rest of the paper is organized as follows. The next section is about the relation to prior work. The atomic concept tuple is introduced in section 3. The proposed concept transfer learning is then described in section 4. Section 5 describes a competitive method with slot embedding derived from the literal descriptions of slot names. In section 6, the proposed approach is evaluated on the value set mismatch and domain adaptation problems. Finally, our conclusions are presented in section 7."
  }, {
    "heading": "2 Related Work",
    "text": "Slot Filling in LU Zettlemoyer and Collins (2007) proposed a grammar induction method by learning a Probabilistic Combinatory Categorial Grammar (PCCG) from logical-form annotations. As a\ngrammar-based method, PCCG is close to a hierarchical concepts structure in grammar generation and combination. But this grammar-based method does not possess high generalization capability for atomic concept sharing, and heavily depends on a well-defined lexicon set.\nRecent research on statistical slot filling in LU has been focused on the Recurrent Neural Network (RNN) and its extensions. At first, RNN outperformed CRF (Conditional Random Field) on the ATIS dataset (Yao et al., 2013; Mesnil et al., 2013). Long-short term memory network (LSTM) was introduced to obtain a marginal improvement over RNN (Yao et al., 2014). After that, many RNN variations were proposed: encoder-labeler model (Kurata et al., 2016), attention model (Liu and Lane, 2016; Zhu and Yu, 2017) etc. However, these work only predicted the plain semantic slot, not the structure of atomic concepts.\nDomain Adaptation in LU For the domain adaptation in LU, Zhu et al. (2014) proposed generating spoken language surface forms by using patterns of the source domain and the ontology of the target domain. With regard to the unsupervised LU, Heck and Hakkani-Tur (2012) exploited the structure of semantic knowledge graphs from the web to create natural language surface forms of entity-relation-entity portions of knowledge graphs. For the zero-shot learning of LU, Ferreira et al. (2015); Yazdani and Henderson (2015) proposed a model to calculate similarity scores between an input sentence and semantic items. In this paper, we focus on the extension of slots with limited seed data."
  }, {
    "heading": "3 Atomic Concept Tuples",
    "text": "Although concept definition is one of the most crucial problems of LU, there is no unified surface form for the domain ontology. Even for the same semantic slot, names of this slot may be quite different. For example, the city where the flight departs may be called “from city”, “depart city” or “from loc.city name”. Ontology definitions from different groups may be similar but not consistent, which is not convenient for data reuse. Meanwhile, semantic slots defined in traditional LU systems are on a plain level, while there is no structure to indicate their relation.\nTo solve this problem, we propose to use atomic concepts to represent the semantic slots. Atomic concepts are exploited to break down the slots. We\nrepresent the semantic slots as atomic concept tuples (Figure 1 is an example). The semantic slot composed of these atomic concepts can keep a unified resource for concept definition and extend the semantic knowledge flexibly.\nWe propose a criteria to construct atomic concept manually. For a given vocabulary C of the atomic concepts, a semantic slot s can be represented by a tuple [c1, c2, ..., ck], where ci ∈ C is in the i-th dimension and k is tuple length. In particular, a “null” atom is introduced for each dimension. Table 1 illustrates an example of slot representation on the ATIS task. To avoid a scratch concept branch, we make a constraint:\nCi ∩ Cj = {null}, 1 ≤ i 6= j ≤ k\nwhere Ci (1 ≤ i ≤ k) denotes all possible atomic concepts which exist in dimension i (i.e. ci ∈ Ci). The concept tuple is ordered.\nIn general, atomic concepts can be classified into two categories, one is value-aware and the other is context-aware. The principle for defining slot as a concept branch is: lower dimension less context-aware. For example, “city name” and “airport name” depend on rare context (valueaware). They should be located in the first dimension. “from location” depends on the context like a pattern of “a flight leaves [city name]”, which should be in the second dimension. The atomic concept tuple shows the inner relation between different semantic slots explicitly.\nTherefore, the procedure of constructing atomic concept tuples for slots can be divided into the following steps.\n• Firstly, we build a vocabularyC of the atomic concepts for all the slots. By analyzing the conceptual intersection of different slots, we can split the slots into smaller ones which are called atomic concepts. After that, each slot is represented as a set of atomic concepts which are not ordered.\n• Secondly, we gather the atoms into different groups. Atomic concepts from the same group should be mutually exclusive. Therefore we can investigate the inner relation and outer relation of these groups.\n• Finally, each group is associated with one dimension (Ci) of the atomic concept tuple. The groups are ordered depending on whether they are value-aware or contextaware."
  }, {
    "heading": "4 Concept Transfer Learning",
    "text": "The slot filling is typically considered as a sequence labelling problem. In this paper, we only consider the sequence-labelling based slot filling task. The input (word) sequence is denoted by w = (w1, w2, ..., wN ), and the output (slot tag) sequence is denoted by s = (s1, s2, ..., sN ). Since a slot may be mapped to several continuous words, we follow the popular in/out/begin (IOB) representation (e.g. an example in Figure 3).\nThe typical slot filling task predicts a plain slot sequence given a word sequence, dubbed as plain slot-filling (PS).\nIn this paper, the popular bidirectional LSTMRNN (BLSTM) is used to model the sequence labeling problem (Graves, 2012). It can be exploited to capture both past and future features for a specific time frame. The BLSTM reads the input sentence w and generates N hidden states hi = ←− hi ⊕ −→ hi , i ∈ {1, .., N}:\n←− hi = b( ←−− hi+1, ewi); −→ hi = f( −−→ hi−1, ewi)\nwhere ←− hi is the hidden vector of the backward pass in BLSTM and −→ hi is the hidden vector of the forward pass in BLSTM at time i, b and f are LSTM units of the backward and forward passes respectively, ew denotes the word embedding for each word w, and ⊕ denotes the vector concatenation operation. We write the entire operation as a mapping BLSTMΘw (Θw refers to the parameters):\n(h1...hN ) = BLSTMΘw(w1...wN ) (1)\nTherefore, the plain slot filling defines a distribution over slot tag sequences given an input word\nsequence:\np(s|w) = N∏ i=1 p(si|hi)\n= N∏ i=1 softmax(Wo · hi)T δsi\n(2)\nwhere the matrix Wo (output layer) consists of the vector representations of each slot tag, the symbol δd is a Kronecker delta with a dimension for each slot tag, and the softmax function is used to estimate the probability distribution over all possible plain slots."
  }, {
    "heading": "4.1 Atomic-Concepts Based Slot Filling",
    "text": "The slot is indicated as an atomic concept tuple based on hierarchical concept structure. Slot filling is considered as a concept-tuple labelling task.\n(a) Atomic concept independent Slot filling can be transferred to a multi-task sequence labelling problem, regarding these atomic concepts independently (i.e. AC). Each task predicts one atomic concept by a respective output layer. Thus, the slot filling problem can be formulated as\np(s|w) = N∏ i=1 [p(IOBi|hi) k∏ j=1 p(cij |hi)]\nwhere the semantic slot si is represented by an atomic concept branch [ci1, ci2, ..., cik], and IOBi is the IOB schema tag at time i. As illustrated in Figure 4(a), the semantic slot “from city” can be represented as [“city name”,“from loc”]. The\nprediction of IOB is regarded as another task specifically. All tasks share the same parameters except for the output layers.\n(b) Atomic concept dependent Atomic concepts can also be regarded dependently (i.e. ACD) so that atomic concept prediction depends on the former predicted results. The slot filling problem can be formulated as\np(s|w)\n= N∏ i=1 [p(IOBi|hi)p(ci1|hi) k∏ j=2 p(cij |hi, ci,1:j−1)]\nwhere ci,1:j−1 = (ci,1, ..., ci,j−1) is the predicted result of former atomic concepts of slot tag si, indicating a structured multi-task learning framework.\nIn this paper, we make some simplifications on concept dependence. We predict atomic concept only based on the last atomic concept, as shown in Figure 4(b)."
  }, {
    "heading": "4.2 Training and Decoding",
    "text": "Since our approach is a structured multi-task learning problem, the model loss is summed over each task during training. For the domain adaptation, we firstly gather training data from the source domain and seed data from the target domain to be a union set. Subsequently, the union data is fed into the slot filling model.\nDuring the decoding stage, we combine predicted atomic concepts with probability multiplication. The evaluation is made on the top-best hypothesis. Although the atomic-concepts based slot\nfilling may predict an unseen slot. We didn’t perform any post-processing but considered the unseen slot as a wrong prediction."
  }, {
    "heading": "5 Literal Description of Slot Name",
    "text": "In the section, we introduce a competitive system which uses the literal description of the slot as an input of the slot filling model. The literal description of slot used in this paper is the word sequence of each slot name, which can be obtained automatically. As the names of relative slots may include the same or similar word, the word sequence of slot name can also help reveal the relation between different slots. Therefore, it is very meaningful to compare this method with the atomic concept tuples involving human knowledge.\nThe architecture of this competitive system is illustrated in Figure 5. First, it assumes that each slot name is a meaningful natural language description so that the slot filling task is tractable from the input word sequence and slot name. Second, another BLSTM model is applied to derive softmax embedding from the slot names. In this method, we also split the slot filling task into IOB tag prediction and slot name prediction. In other words, the slot tag si is broken down into IOBi and slot name SNi, e.g. the slot tag “B-from city” is split into “B” and “from city”. The details are indicated below.\nWith the BLSTM applied on the input sequence, we have hidden vectors hi, i ∈ {1, .., N} as shown in Eqn. (1). This model redefines the distribution\nover slot tag sequences given an input word sequence, compared with Eqn. (2):\np(s|w) = N∏ i=1 p(IOBi|hi)p(SNi|hi)\nwhere p(IOBi|hi) predicts the IOB tag and p(SNi|hi) makes a prediction for the slot name. We define\np(SNi|hi) = softmax(W · hi)T δSNi\nwhere W ∈ RA×B is a matrix, hi ∈ RB is a vector, A is the number of all different slot names. The matrix W consists of the embedding of each slot name (i.e. each row vector of W with length B).\nTo capture the slot relation within different slot names, we apply another BLSTM model (as shown in the orange dotted circle of Figure 5) onto the word sequence (literal description) of each slot name. For the j-th slot name (j ∈ {1, .., A}) with a word sequence xj = (xj1, ..., x j Nj ), we have\n←− vjn = lstm b( ←−− vjn+1, exjn); −→ vjn = lstm f ( −−→ vjn−1, exjn)\nwhere ←− vjn is the hidden vector of the backward pass and −→ vjn is the hidden vector of the forward pass at time n (n ∈ {1, .., Nj}), ex denotes the word embedding for each word x. We take the tails of both backward and forward pass as the slot embedding, i.e.\nWj = ←− vj1 ⊕ −→ vjNj\nwhere Wj is the j-th row vector of matrix W . The relative slots using the same or similar word in slot naming will be close in the space of slot embedding inherently. Therefore, this method is a competitive system to the atomic concept tuples. We will show the comparison in the following section."
  }, {
    "heading": "6 Experiments",
    "text": "We evaluate our atomic-concept methods on two tasks: value set mismatch and domain adaptation.\nValue set mismatch task evaluates the generalization capability of different slot filling models. In a language understanding (LU) system, each slot has a value set with all possible values which can be assigned to it. Since the semantically annotated data is always limited, only a part of values\nis seen in the training data. Will the slot filling model perform well on the unseen values? To answer this question, we synthesize a test set by the values mismatched with the training set of ATIS corpus. Our methods may take advantages of the prior knowledge about slot relations based on the atomic concepts and the literal descriptions of slot names.\nDomain adaptation task evaluates the adaptation capability of our methods when they meet new slots in the target domain. In this task, a seed training set of the target domain is provided. However, it is very limited: 1) some new slots may not be covered; 2) not all contexts are covered for each new slot. The atomic-concepts based method would alleviate this problem. Each slot is defined as a tuple of atomic concepts in our method. Therefore, it is possible to learn an unseen slot of the target domain if its atomic concepts exist in the data of the source domain and the seed data of the target domain. It is also possible to see more contexts for a new slot if its atomic concepts exist in the source domain which has much more data."
  }, {
    "heading": "6.1 Value Set Mismatch",
    "text": "ATIS corpus has been widely used as a benchmark by the LU community. The training data consists of 4978 sentences and the test data consists of 893 sentences.\nIn this task, we perform an adaptation for unmatched training and test sets, in which there are many unseen slot-value pairs in the test set (Figure 2 is an example). It is a common problem in the development of commercial dialogue system since it is impossible to collect data covering all possible slot-value pairs. We simulate this problem on the ATIS dataset (Hemphill et al., 1995) by creating an unmatched test set (ATIS X test).\nATIS X test is synthesized from the standard ATIS test set by randomly replacing the value of each slot with an unseen one. The unseen value sets are collected from the training set according to bottom-level concepts (e.g. “city name”, “airport name”). For example, if the value set of “from city” is {“New York”, “Boston”} and the value set of “to city” is {“Boston”}, then the unseen value for “to city” is “New York”. The test sentence “Flights to [xx:to city]” can be replaced to “Flights to [New York:to city]”. Finally, the ATIS X test gets the same sentence number to the standard ATIS test set."
  }, {
    "heading": "6.1.1 Experimental Settings",
    "text": "We randomly selected 80% of the training data for model training and the remaining 20% for validation. We deal with unseen words in the test set by marking any words with only one single occurrence in the training set as 〈unk〉. We also converted sequences of numbers to the string DIGIT, e.g. 1990 is converted to DIGIT*4 (Zhang and Wang, 2016). Regarding BLSTM model, we set the dimension of word embeddings to 100 and the number of hidden units to 100. For training, the network parameters are randomly initialized in accordance with the uniform distribution (-0.2, 0.2). Stochastic gradient descent (SGD) is used for updating parameters. The dropout with a probability of 0.5 is applied to the non-recurrent connections during the training stage.\nWe try different learning rates by grid-search in range of [0.008, 0.04]. We keep the learning rate for 100 epochs and save the parameters that give the best performance on the validation set. Finally, we report the F1-score of the semantic slots on the test set with parameters that have achieved the best F1-score on the validation set. The F1-score is calculated using CoNLL evaluation script. 2"
  }, {
    "heading": "6.1.2 Experimental Results and Analysis",
    "text": "Table 2 summarizes the recently published results on the ATIS slot filling task and compares them with the results of our proposed methods on the standard ATIS test set. We can see that RNN outperforms CRF because of the ability to capture long-term dependencies. LSTM beats RNN by solving the problem of vanishing or exploding gradients. BLSTM further improves the result by considering both the past and future features. Encoder-decoder achieves the state-of-theart performance by modeling the label dependencies. Encoder-labeler is a similar method to the Encoder-decoder. These systems are designed to predict the plain semantic slots traditionally.\nCompared with the published results, our method outperforms the previously published F1score, illustrated in Table 2. AC gets a marginal improvement (+0.15%) over PS by predicting the atomic concepts independently instead of the plain slots. Moreover, ACD predicts the atomic concepts dependently, gains 0.50% (significant level 95%) over the AC. Worth to mention that ACD achieves a new state-of-the-art performance of the\n2http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nstandard slot-tagging task on the ATIS dataset, with only the lexicon features 3.\nOur methods are also tested on the ATIS X test to measure the ability of generalization. For comparison, we also apply dictionary features (ngram indication) of value sets (e.g. some kind of gazetteers) collected from training data into the PS model (i.e. PS+dict-feats in Table 2). From Table 2, we can see that: 1) The plain slot filling models (PS, Encoder-decoder) are not on par with other models. 2) The atomic-concepts based slot filling gets a slight improvement over the PS with dict-feats, considering the concepts independently (AC). 3) The atomic-concepts based slot fillings (ACD gains a large margin over AC, considering the concepts dependently. 4) The method based on slot name embedding (described in Section 5) achieves a slight improvement than AC, which implies that it is possible to reveal the relationship between slots automatically.\n3There are other published results that achieved better performance by using Name Entity features, e.g. Mesnil et al. (2013) got 96.24% F1-score. The NE features are manually annotated and strong information. So it would be more meaningful to use only lexicon features. Meanwhile, several other works can obtain competitive results by using the intent classification as another task for joint training, e.g. Liu and Lane (2016) achieved 95.98% F1-score. In this paper, we consider the slot filling task only.\nCase study: As illustrated in Table 3, the plain slot filling (PS) predicts the label of “late” wrongly, whereas the atomic-concepts based slot fillings (i.e. AC and ACD) get the accurate annotation. The word of “late” is never covered by the slot “period of day” in the training set. It is hard for the plain slot filling (PS) to predict an unseen mapping correctly. Luckily, the “late” is covered by the family of the slot “period of day” in the training set, e.g. “arrive time.period of day”. Therefore, AC and ACD can learn this by modeling the atomic concepts separately."
  }, {
    "heading": "6.2 Domain Adaptation",
    "text": "Our methods are also evaluated on the DSTC 2&3 task (Henderson et al., 2013) which is considered to be a realistic domain adaptation problem.\nDSTC 2 (source domain) comprises of dialogues from the restaurant information domain in Cambridge. We use the dstc2 train set (1612 dialogues) for training and the dstc2 dev (506 dialogues) for validation.\nDSTC 3 (target domain) introduces the tourist information domain about restaurant, pubs and coffee shops in Cambridge, which is an extension of DSTC 2. We use seed data dstc3 seed (only 11 dialogues) as the training set of the target domain.\nDSTC3 S test: In this paper, we focus on three new semantic slots: “has tv, has internet, children allowed”. 4 They only exist in the DSTC 3 dataset and have few appearances in the seed data. A test set is chosen for specific evaluation on these new semantic slots, by gathering all the sentences (688 sentences) whose annotation contains these three slots and randomly selecting 1000 sentences irrelevant to these three slots from the dstc3 test set. This test set is named as DSTC3 S test (1688 sentences).\nThe union of a slot and action is taken as a plain semantic slot (e.g. “confirm.food=Chinese”), since each slot is tied with an action (e.g. “inform”, “deny” and “confirm”) in DSTC 2&3. The slot and action are taken as atomic concepts. For the slot filling task, only the semantic annotation with aligned information is kept, e.g. the semantic tuple “request(phone)” is ignored. We use transcripts as input, and make slot-value alignment by\n4For each slot of “has tv, has internet, children allowed”, the semantic annotation “request(slot)” is replaced with “confirm(slot=True)”. Then we have the slot-tagging format, e.g. ”does it have [television:confirm.has tv]”.\nstring matching simply."
  }, {
    "heading": "6.2.1 Experimental Results and Analysis",
    "text": "The experimental settings are similar to the ATIS’s, whereas the seed data in DSTC 3 is also used for validation.\nThe performance of our methods in the DSTC 2&3 task is illustrated in Table 4. We can see that: 1) By incorporating the data of the source domain (dstc2 train), PS and AC achieve improvements respectively. 2) AC gains more than PS by modeling the plain semantic slot as atomic concepts. The atomic concepts promote the associated slots to share input features for the same atoms. 3) The atomic-concepts based slot filling considering the concepts dependently (ACD) gains little (0.17%) over AC considering the concepts independently. It may be due to the small size of dstc3 seed.\nCase study: Several cases from these models (trained on the union set of dstc2 train and dstc3 seed) are also chosen to explain why the atomic-concepts based slot filling outperforms the typical plain slot filling, as shown in Table 5. From the above part of Table 5, we can see PS predicts a wrong slot. Because the grammar “does it have [something]” is only for the plain slot “confirm.hastv” in the seed data. From the below part of Table 5, we can see that only ACD which considers the concepts dependently predicts the right slot. Since “confirm.childrenallowed” never exists in the seed data, PS can’t learn patterns about it. Limited by the quantity of the seed data, AC also doesn’t extract the semantics correctly."
  }, {
    "heading": "7 Conclusion",
    "text": "To address data sparsity problem of language understanding (LU) task, we present a novel method of concept definition based on well-defined atomic concepts. We present the concept transfer learning for slot filling on the atomic concept level to solve the problem of adaptive LU. The experiments on the ATIS and DSTC 2&3 datasets show our method obtains promising results and outperforms the traditional slot filling, due to the knowledge sharing of atomic concepts.\nThe atomic concepts are constructed manually in this paper. In future work, we want to explore more flexible concept definition for concept transfer learning of LU. Moreover, we also propose a competitive method based on slot name embedding which can be extracted from the literal description of the slot name automatically. The experimental result shows that it lays foundation for finding a more flexible concept definition method for adaptive LU."
  }, {
    "heading": "Acknowledgments",
    "text": "This work has been supported by the China NSFC project (No. 61573241), Shanghai International Science and Technology Cooperation Fund (No. 16550720300) and the JiangSu NSFC project (BE2016078). Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University. We also thank Tianfan Fu for comments that greatly improved the manuscript."
  }],
  "year": 2018,
  "references": [{
    "title": "Zero-shot semantic parser for spoken language understanding",
    "authors": ["Emmanuel Ferreira", "Bassam Jabaian", "Fabrice Lefvre."],
    "venue": "16th Annual Conference of the International Speech Communication Association (InterSpeech).",
    "year": 2015
  }, {
    "title": "Supervised Sequence Labelling with Recurrent Neural Networks",
    "authors": ["Alex Graves."],
    "venue": "Springer Berlin Heidelberg.",
    "year": 2012
  }, {
    "title": "Exploiting the semantic web for unsupervised spoken language understanding",
    "authors": ["L Heck", "D Hakkani-Tur."],
    "venue": "Spoken Language Technology Workshop, pages 228–233.",
    "year": 2012
  }, {
    "title": "The atis spoken language systems pilot corpus",
    "authors": ["Charles T Hemphill", "John J Godfrey", "George R Doddington."],
    "venue": "Proceedings of the Darpa Speech and Natural Language Workshop, pages 96– 101.",
    "year": 1995
  }, {
    "title": "Dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Jason Williams"],
    "year": 2013
  }, {
    "title": "Leveraging sentence-level information with encoder lstm for semantic slot filling",
    "authors": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou", "Mo Yu."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2077–2083,",
    "year": 2016
  }, {
    "title": "Attention-based recurrent neural network models for joint intent detection and slot filling",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "17th Annual Conference of the International Speech Communication Association (InterSpeech).",
    "year": 2016
  }, {
    "title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding",
    "authors": ["Grégoire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio."],
    "venue": "INTERSPEECH, pages 3771–3775.",
    "year": 2013
  }, {
    "title": "A speech understanding system based on statistical representation of semantics",
    "authors": ["Roberto Pieraccini", "Evelyne Tzoukermann", "Zakhar Gorelov", "J-L Gauvain", "Esther Levin", "C-H Lee", "Jay G Wilpon."],
    "venue": "Acoustics, Speech, and Signal Processing, 1992.",
    "year": 1992
  }, {
    "title": "What is left to be understood in atis? In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 19–24",
    "authors": ["Gokhan Tur", "Dilek Hakkani-Tür", "Larry Heck."],
    "venue": "IEEE.",
    "year": 2010
  }, {
    "title": "Sequential convolutional neural networks for slot filling in spoken language understanding",
    "authors": ["Ngoc Thang Vu."],
    "venue": "17th Annual Conference of the International Speech Communication Association (InterSpeech).",
    "year": 2016
  }, {
    "title": "Spoken language understanding using long short-term memory neural networks",
    "authors": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."],
    "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), pages 189–194. IEEE.",
    "year": 2014
  }, {
    "title": "Recurrent neural networks for language understanding",
    "authors": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."],
    "venue": "INTERSPEECH, pages 2524–2528.",
    "year": 2013
  }, {
    "title": "A model of zero-shot learning of spoken language understanding",
    "authors": ["Majid Yazdani", "James Henderson."],
    "venue": "Conference on Empirical Methods in Natural Language Processing, pages 244–249.",
    "year": 2015
  }, {
    "title": "Online learning of relaxed ccg grammars for parsing to logical form",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Compu-",
    "year": 2007
  }, {
    "title": "Neural models for sequence chunking",
    "authors": ["Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou."],
    "venue": "AAAI, pages 3365–3371.",
    "year": 2017
  }, {
    "title": "A joint model of intent determination and slot filling for spoken language understanding",
    "authors": ["Xiaodong Zhang", "Houfeng Wang."],
    "venue": "the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16).",
    "year": 2016
  }, {
    "title": "Semantic parser enhancement for dialogue domain extension with little data",
    "authors": ["Su Zhu", "Lu Chen", "Kai Sun", "Da Zheng", "Kai Yu."],
    "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 336–341. IEEE.",
    "year": 2014
  }, {
    "title": "Encoder-decoder with focus-mechanism for sequence labelling based spoken language understanding",
    "authors": ["Su Zhu", "Kai Yu."],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pages 5675–5679.",
    "year": 2017
  }],
  "id": "SP:cdb5bb81ca35ae1a62e07686b8c45b64a02ce1c5",
  "authors": [{
    "name": "Su Zhu",
    "affiliations": []
  }, {
    "name": "Kai Yu",
    "affiliations": []
  }],
  "abstractText": "Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of atomic concepts. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-ofthe-art performance (F1-score 96.08%) on ATIS by only using lexicon features.",
  "title": "Concept Transfer Learning for Adaptive Language Understanding"
}