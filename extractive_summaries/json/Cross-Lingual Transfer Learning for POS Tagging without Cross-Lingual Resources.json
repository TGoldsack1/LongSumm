{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).\nGiven insufficient training examples, we can improve the POS tagging performance by cross-\nlingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).\nGiven no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.\nYang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages. Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.\nIn this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings. Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-\n2832\nlows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information. The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.\nEvaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different."
  }, {
    "heading": "2 Model",
    "text": "Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model. The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1. Given an input\nword sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation. These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.\nFor the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.\nThe outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences. The loss function of the POS tagging can be formulate as:\nLp = − S∑\ni=1 N∑ j=1 pi,j log (p̂i,j) , (1)\nwhere S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag. In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.\nLanguage-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016). First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014). The encoder is with three convolution filters whose sizes are 3, 4, and 5. For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector. Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer. The discriminator is implemented\n1We also tried isolated character-level modules but the overall performance was worse.\nas a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.\nSince the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic. The loss function of the language classifier is formulated as:\nLa = − S∑\ni=1\nli log l̂i, (2)\nwhere S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging. Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.\nBidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word. For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”. This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017). We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module. It can be formulated as:\nLl = − S∑\ni=1 N∑ j=1 log (P (wj+1|fj)) +\nlog (P (wj−1|bj)) , (3)\nwhere fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.\nAll the three loss functions are added to be optimized altogether as:\nL = ws (Lp + λLa + λLl) , (4)\nwhere λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016). ws is used to give different weights to the source language and the target language. Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language. For the source language, instead, it is set as the size of the target train set divided by the size of the source train set."
  }, {
    "heading": "3 Experiments",
    "text": "For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016). We used English as the source language, which is with 12,543 training sentences.2 We chose datasets with 1k to 14k training sentences. The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.\nTable 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages. The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels. Training with only the train sets in the target languages (c) showed 91.61% on average. When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average. Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.\nWith transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated. They showed better average accuracies than target only cases, but they showed mixed results. However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%. For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of\n2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.\nother transfer learning models. For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models. Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results. This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.\nTable 2 shows the results when using 320 taglabeled training sentences. In this case, transfer learning methods still show better accuracies than target-only approaches on average. However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results. In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.\nWhen training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average. However, not using the common BLSTM\nin transfer learning models showed better performance than using it on average.3 The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs. In this case, just having private BLSTMs without the common BLSTM can show better performance. We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results. For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%. This is expected since transfer learning is effective when the target train set is small.\nAn extension of this work is utilizing multiple languages as the source languages. Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other. For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources. When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-\n3The results in detail are shown in the first authors dissertation Kim (2017).\nformance than the results of using English as a single source language. Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language. We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4\nImplementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set. The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model. Each word vector is 128 dimensional and each character vector is 100 dimensional. They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010). For stable training, we use gradient clipping, where the threshold is set to 5. The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200. Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328\n4The extended work in detail are shown in Kim (2017). 5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.\ndimensional. The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014). For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016). For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0. Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector."
  }, {
    "heading": "4 Conclusion",
    "text": "We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations. Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases. Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their helpful comments. All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987)."
  }],
  "year": 2017,
  "references": [{
    "title": "Wasserstein GAN",
    "authors": ["Martin Arjovsky", "Soumith Chintala", "Léon Bottou."],
    "venue": "International Conference on Machine Learning (ICML).",
    "year": 2017
  }, {
    "title": "Cross-lingual transfer of correlations between parts of speech and gaze features",
    "authors": ["Maria Barrett", "Frank Keller", "Anders Søgaard."],
    "venue": "Proceedings of COLING, pages 1330–1339.",
    "year": 2016
  }, {
    "title": "Domain separation networks",
    "authors": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."],
    "venue": "Advances in Neural Information Processing Systems 29 (NIPS), pages 343–351.",
    "year": 2016
  }, {
    "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
    "authors": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie."],
    "venue": "arXiv:1606.01614.",
    "year": 2016
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learing Research (JMLR), 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Simpler unsupervised POS tagging with bilingual projections",
    "authors": ["Long Duong", "Paul Cook", "Steven Bird", "Pavel Pecina."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 634–639.",
    "year": 2013
  }, {
    "title": "A theoretically grounded application of dropout in recurrent neural networks",
    "authors": ["Yarin Gal", "Zoubin Ghahramani."],
    "venue": "Advances in Neural Information Processing Systems 29 (NIPS), pages 1019–1027.",
    "year": 2016
  }, {
    "title": "Domain-adversarial training of neural networks",
    "authors": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Franois Laviolette", "Mario Marchand", "Victor Lempitsky."],
    "venue": "Journal of Machine Learning Research",
    "year": 2016
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 249–256.",
    "year": 2010
  }, {
    "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "A resource-light approach to Russian morphology: Tagging Russian using Czech resources",
    "authors": ["Jiri Hana", "Anna Feldman", "Chris Brew."],
    "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2004
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 2342–2350.",
    "year": 2015
  }, {
    "title": "Linguistic Knowledge Transfer for Enriching Vector Representations",
    "authors": ["Joo-Kyung Kim."],
    "venue": "Ph.D. thesis, The Ohio State University.",
    "year": 2017
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751.",
    "year": 2014
  }, {
    "title": "Part-of-speech taggers for lowresource languages using CCA features",
    "authors": ["Young-Bum Kim", "Benjamin Snyder", "Ruhi Sarikaya."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2015
  }, {
    "title": "Frustratingly easy neural domain adaptation",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of COLING, pages 387–396.",
    "year": 2016
  }, {
    "title": "New transfer learning techniques for disparate label sets",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "ADAM: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Lei Ba."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Finding function in form: Compositional character models for open vocabulary word representation",
    "authors": ["Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernández Astudillo", "Silvio Amir", "Chris Dyer", "Alan W. Black", "Isabel Trancoso"],
    "year": 2015
  }, {
    "title": "Rectifier nonlinearities improve neural network acoustic models",
    "authors": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng."],
    "venue": "ICML 2013 Workshop on Deep Learning for Audio, Speech, and Language Processing.",
    "year": 2013
  }, {
    "title": "Universal dependencies 1.4",
    "authors": ["Larraitz Uria", "Gertjan van Noord", "Viktor Varga", "Veronika Vincze", "Lars Wallin", "Jing Xian Wang", "Jonathan North Washington", "Mats Wirén", "Zdeněk Žabokrtský", "Amir Zeldes", "Daniel Zeman", "Hanzhi Zhu"],
    "year": 2016
  }, {
    "title": "Ohio supercomputer center",
    "authors": ["Ohio Supercomputer Center."],
    "venue": "http://osc.edu/ark: /19495/f5s1ph73.",
    "year": 1987
  }, {
    "title": "A survey on transfer learning",
    "authors": ["Sinno Jialin Pan", "Qiang Yang."],
    "venue": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 22(10):1345–1359.",
    "year": 2010
  }, {
    "title": "Dropout improves recurrent neural networks for handwriting recognition",
    "authors": ["Vu Pham", "Théodore Bluche", "Christopher Kermorvant", "Jérôme Louradour"],
    "year": 2014
  }, {
    "title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
    "authors": ["Barbara Plank", "Anders Søgaard", "Yoav Goldberg."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-",
    "year": 2016
  }, {
    "title": "Semi-supervised multitask learning for sequence labeling",
    "authors": ["Marek Rei."],
    "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "Token and type constraints for cross-lingual part-of-speech tagging",
    "authors": ["Oscar Täckström", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL), 1:1–12.",
    "year": 2013
  }, {
    "title": "Cross-lingual part-of-speech tagging through ambiguous learning",
    "authors": ["Guillaume Wisniewski", "Nicolas Pécheux", "Souhir Gahbiche-Braham", "François Yvon."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language",
    "year": 2014
  }, {
    "title": "Transfer learning for sequence tagging with hierarchical recurrent networks",
    "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Ten pairs to tag – multilingual POS tagging via coarse mapping between embeddings",
    "authors": ["Yuan Zhang", "David Gaddy", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for",
    "year": 2016
  }],
  "id": "SP:afd5a4fd403ac325f9e6b2028acfa3efa10dd141",
  "authors": [{
    "name": "Joo-Kyung Kim",
    "affiliations": []
  }, {
    "name": "Young-Bum Kim",
    "affiliations": []
  }, {
    "name": "Ruhi Sarikaya",
    "affiliations": []
  }, {
    "name": "Eric Fosler-Lussier",
    "affiliations": []
  }],
  "abstractText": "Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",
  "title": "Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources"
}