{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Interpretability is an extremely important criterion when a machine learning model is applied in areas such as medicine, financial markets, and criminal justice (e.g., see the discussion paper by Lipton ((Lipton, 2016)), as well as references therein). Many complex models, such as random forests, kernel methods, and deep neural networks, have been developed and employed to optimize prediction accuracy, which can compromise their ease of interpretation.\nIn this paper, we focus on instancewise feature selection as a specific approach for model interpretation. Given a machine learning model, instancewise feature selection asks for the importance score of each feature on the prediction of a given instance, and the relative importance of each feature is allowed to vary across instances. Thus, the importance scores can act as an explanation for the specific instance, indicating which features are the key for the model to make its prediction on that instance. A related concept in machine learning\n1University of California, Berkeley 2Work done partially during an internship at Ant Financial 3Georgia Institute of Technology 4Ant Financial 5The Voleon Group. Correspondence to: Jianbo Chen <jianbochen@berkeley.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nis feature selection, which selects a subset of features that are useful to build a good predictor for a specified response variable (Guyon & Elisseeff, 2003). While feature selection produces a global importance of features with respect to the entire labeled data set, instancewise feature selection measures feature importance locally for each instance labeled by the model.\nExisting work on interpreting models approach the problem from two directions. The first line of work computes the gradient of the output of the correct class with respect to the input vector for the given model, and uses it as a saliency map for masking the input (Simonyan et al., 2013; Springenberg et al., 2014). The gradient is computed using a Parzen window approximation of the original classifier if the original one is not available (Baehrens et al., 2010). Another line of research approximates the model to be interpreted via a locally additive model in order to explain the difference between the model output and some “reference” output in terms of the difference between the input and some “reference” input (Bach et al., 2015; Kindermans et al., 2016; Ribeiro et al., 2016; Lundberg & Lee, 2017; Shrikumar et al., 2017; Sundararajan et al., 2017). Ribeiro et al. (2016) proposed the LIME, methods which randomly draws instances from a density centered at the sample to be explained, and fits a sparse linear model to predict the model outputs for these instances. Shrikumar et al. (2017) presented DeepLIFT, a method designed specifically for neural networks, which decomposes the output of a neural network on a specific input by backpropagating the contribution back to every feature of the input. Lundberg & Lee (2017) used Shapley values to quantify the importance of features of a given input, and proposed a sampling based method “kernel SHAP” for approximating Shapley values. (Sundararajan et al., 2017) proposed Integrated Gradients (IG), which constructs the additive model by cumulating the gradients along the line between the input and the reference point. Essentially, the two directions both approximate the model locally via an additive model, with different definitions of locality. While the first one considers infinitesimal regions on the decision surface and takes the first-order term in the Taylor expansion as the additive model, the second one considers the finite difference between an input vector and a reference vector.\nIn this paper, our approach to instancewise feature selection is via mutual information, a conceptually different perspective from existing approaches. We define an “explainer,” or instancewise feature selector, as a model which returns a distribution over the subset of features given the input vector. For a given instance, an ideal explainer should assign the highest probability to the subset of features that are most informative for the associated model response. This motivates us to maximize the mutual information between the selected subset of features and the response variable with respect to the instancewise feature selector. Direct estimation of mutual information and discrete feature subset sampling are intractable; accordingly, we derive a tractable method by first applying a variational lower bound for mutual information, and then developing a continuous reparametrization of the sampling distribution.\nAt a high level, the primary differences between our approach and past work are the following. First, our framework globally learns a local explainer, and therefore takes the distribution of inputs into consideration. Second, our framework removes the constraint of local feature additivity on an explainer. These distinctions enable our framework to yield a more efficient, flexible, and natural approach for instancewise feature selection. In summary, our contributions in this work are as follows (see also Table 1 for systematic comparisons):\n• We propose an information-based framework for instancewise feature selection. • We introduce a learning-based method for instancewise feature selection, which is both efficient and modelagnostic.\nFurthermore, we show that the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metric and human evaluation on Amazon Mechanical Turk."
  }, {
    "heading": "2. A framework",
    "text": "We now lay out the primary ingredients of our general approach. While our framework is generic and can be applied to both classification and regression models, the current discussion is restricted to classification models. We assume\none has access to the output of a model as a conditional distribution, Pm(· | x), of the response variable Y given the realization of the input random variable X = x ∈ Rd."
  }, {
    "heading": "2.1. Mutual information",
    "text": "Our method is derived from considering the mutual information between a particular pair of random vectors, so we begin by providing some basic background. Given two random vectors X and Y , the mutual information I(X;Y ) is a measure of dependence between them; intuitively, it corresponds to how much knowledge of one random vector reduces the uncertainty about the other. More precisely, the mutual information is given by the Kullback-Leibler divergence of the product of marginal distributions of X and Y from the joint distribution of X and Y (Cover & Thomas, 2012); it takes the form\nI(X;Y ) = EX,Y [ log pXY (X,Y )\npX(X)pY (Y )\n] ,\nwhere pXY and pX , pY are the joint and marginal probability densities if X,Y are continuous, or the joint and marginal probability mass functions if they are discrete. The expectation is taken with respect to the joint distribution of X and Y . One can show the mutual information is nonnegative and symmetric in two random variables. The mutual information has been a popular criteria in feature selection, where one selects the subset of features that approximately maximizes the mutual information between the response variable and the selected features (Gao et al., 2016; Peng et al., 2005). Here we propose to use mutual information as a criteria for instancewise feature selection."
  }, {
    "heading": "2.2. How to construct explanations",
    "text": "We now describe how to construct explanations using mutual information. In our specific setting, the pair (X,Y ) are characterized by the marginal distribution X ∼ PX(·), and a family of conditional distributions of the form (Y | x) ∼ Pm(· | x). For a given positive integer k, let ℘k = {S ⊂ 2d | |S| = k} be the set of all subsets of size k. An explainer E of size k is a mapping from the feature space Rd to the power set ℘k; we allow the mapping to be randomized, meaning that we can also think of E as mapping x to a conditional distribution P(S | x) over S ∈ ℘k. Given the chosen subset S = E(x), we use xS to denote the sub-vector formed by the chosen features. We view the choice of the number of explaining features k as\nbest left in the hands of the user, but it can also be tuned as a hyper-parameter.\nWe have thus defined a new random vector XS ∈ Rk; see Figure 1 for a probabilistic graphical model representing its construction. We formulate instancewise feature selection as seeking explainer that optimizes the criterion\nmax E\nI(XS ;Y ) subject to S ∼ E(X). (1)\nIn words, we aim to maximize the mutual information between the response variable from the model and the selected features, as a function of the choice of selection rule.\nIt turns out that a global optimum of Problem (1) has a natural information-theoretic interpretation: it corresponds to the minimization of the expected length of encoded message for the model Pm(Y | x) using Pm(Y |xS), where the latter corresponds to the conditional distribution of Y upon observing the selected sub-vector. More concretely, we have the following:\nTheorem 1. Letting Em[· | x] denote the expectation over Pm(· | x), define\nE∗(x) := argmin S\nEm [ log\n1\nPm(Y | xS) ∣∣∣ x] . (2) Then E∗ is a global optimum of Problem (1). Conversely, any global optimum of Problem (1) degenerates to E∗ almost surely over the marginal distribution PX . The proof of Theorem 1 is left to the supplementary materials. In practice, the above global optimum is obtained only if the explanation family E is sufficiently large. In the case when Pm(Y |xS) is unknown or computationally expensive to estimate accurately, we can choose to restrict E to suitably controlled families so as to prevent overfitting."
  }, {
    "heading": "3. Proposed method",
    "text": "A direct solution to Problem (1) is not possible, so that we need to approach it by a variational approximation. In particular, we derive a lower bound on the mutual information, and we approximate the model conditional distribution Pm by a suitably rich family of functions."
  }, {
    "heading": "3.1. Obtaining a tractable variational formulation",
    "text": "We now describe the steps taken to obtain a tractable variational formulation.\nA variational lower bound: Mutual information between XS and Y can be expressed in terms of the conditional distribution of Y given XS :\nI(XS ;Y ) = E [ log\nPm(XS , Y ) P(XS)Pm(Y )\n] = E [ log\nPm(Y |XS) Pm(Y ) ] = E [ logPm(Y |XS) ] + Const.\n= EXES|XEY |XS [ logPm(Y |XS) ] + Const.\nFor a generic model, it is impossible to compute expectations under the conditional distribution Pm(· | xS). Hence we introduce a variational family for approximation:\nQ : = { Q | Q = {xS → QS(Y |xS), S ∈ ℘k} } . (3)\nNote each member Q of the family Q is a collection of conditional distributions QS(Y |xS), one for each choice of k-sized feature subset S. For any Q, an application of Jensen’s inequality yields the lower bound\nEY |XS [logPm(Y |XS)] ≥ ∫ Pm(Y |XS) logQS(Y |XS)\n= EY |XS [logQS(Y |XS)], where equality holds if and only if Pm(Y |XS) and QS(Y |XS) are equal in distribution. We have thus obtained a variational lower bound of the mutual information I(XS ;Y ). Problem (1) can thus be relaxed as maximizing the variational lower bound, over both the explanation E and the conditional distribution Q:\nmax E,Q\nE [ logQS(Y | XS) ] such that S ∼ E(X). (4)\nFor generic choices Q and E , it is still difficult to solve the variational approximation (4). In order to obtain a tractable method, we need to restrict both Q and E to suitable families over which it is efficient to perform optimization.\nA single neural network for parametrizing Q: Recall that Q = {QS(· | xS), S ∈ ℘k} is a collection of conditional distributions with cardinality |Q| = ( d k ) . We assume X is a continuous random vector, and Pm(Y | x) is continuous with respect to x. Then we introduce a single neural network function gα : Rd × [c]→ [0, 1] for parametrizing Q, where [c] = {0, 1, . . . , c− 1} denotes the set of possible classes, and α denotes the learnable parameters. We define QS(Y |xS) : = gα(x̃S , Y ), where x̃S ∈ Rd is transformed from x by replacing entries not in S with zeros:\n(x̃S)i = { xi, i ∈ S, 0, i /∈ S.\nWhen X contains discrete features, we embed each discrete feature with a vector, and the vector representing a specific feature is set to zero simultaneously when the corresponding feature is not in S."
  }, {
    "heading": "3.2. Continuous relaxation of subset sampling",
    "text": "Direct estimation of the objective function in equation (4) requires summing over ( d k ) combinations of feature subsets after the variational approximation. Several tricks exist for tackling this issue, like REINFORCE-type Algorithms (Williams, 1992), or weighted sum of features parametrized by deterministic functions of X . (A similar concept to the second trick is the “soft attention” structure in vision (Ba et al., 2014) and NLP (Bahdanau et al., 2014) where the weight of each feature is parametrized by\na function of the respective feature itself.) We employ an alternative approach generalized from Concrete Relaxation (Gumbel-softmax trick) (Jang et al., 2017; Maddison et al., 2014; 2016), which empirically has a lower variance than REINFORCE and encourages discreteness (Raffel et al., 2017).\nThe Gumbel-softmax trick uses the concrete distribution as a continuous differentiable approximation to a categorical distribution. In particular, suppose we want to approximate a categorical random variable represented as a one-hot vector in Rd with category probability p1, p2, . . . , pd. The random perturbation for each category is independently generated from a Gumbel(0, 1) distribution:\nGi = − log(− log ui), ui ∼ Uniform(0, 1). We add the random perturbation to the log probability of each category and take a temperature-dependent softmax over the d-dimensional vector:\nCi = exp{(log pi +Gi)/τ}∑d j=1 exp{(log pj +Gj)/τ} .\nThe resulting random vector C = (C1, . . . , Cd) is called a Concrete random vector, which we denote by\nC ∼ Concrete(log p1, . . . , log pd).\nWe apply the Gumbel-softmax trick to approximate weighted subset sampling. We would like to sample a subset S of k distinct features out of the d dimensions. The sampling scheme for S can be equivalently viewed as sampling a k-hot random vector Z from Ddk : = {z ∈ {0, 1}d |∑ zi = k}, with each entry of z being one if it is in the selected subset S and being zero otherwise. An importance score which depends on the input vector is assigned for each feature. Concretely, we define wθ : Rd → Rd that maps the input to a d-dimensional vector, with the ith entry of wθ(X) representing the importance score of the ith feature.\nWe start with approximating sampling k distinct features out of d features by the sampling scheme below: Sample a single feature out of d features independently for k times. Discard the overlapping features and keep the rest. Such a scheme samples at most k features, and is easier to approximate by a continuous relaxation. We further approximate the above scheme by independently sampling k independent Concrete random vectors, and then we define a d-dimensional random vector V that is the elementwise maximum of C1, C2, . . . , Ck:"
  }, {
    "heading": "Cj ∼ Concrete(wθ(X)) i.i.d. for j = 1, 2, . . . , k,",
    "text": "V = (V1, V2, . . . , Vd), Vi = max\nj Cji .\nThe random vector V is then used to approximate the k-hot random vector Z during training.\nWe write V = V (θ, ζ) as V is a function of θ and a collection of auxiliary random variables ζ sampled independently\nfrom the Gumbel distribution. Then we use the elementwise product V (θ, ζ) X between V andX as an approximation of X̃S ."
  }, {
    "heading": "3.3. The final objective and its optimization",
    "text": "After having applied the continuous approximation of feature subset sampling, we have reduced Problem (4) to the following:\nmax θ,α\nEX,Y,ζ [ log gα(V (θ, ζ) X,Y ) ] , (5)\nwhere gα denotes the neural network used to approximate the model conditional distribution, and the quantity θ is used to parametrize the explainer. In the case of classification with c classes, we can write\nEX,ζ [ c∑ y=1 [Pm(y | X) log gα(V (θ, ζ) X, y) ] . (6)\nNote that the expectation operator EX,ζ does not depend on the parameters (α, θ), so that during the training stage, we can apply stochastic gradient methods to jointly optimize the pair (α, θ). In each update, we sample a mini-batch of unlabeled data with their class distributions from the model to be explained, and the auxiliary random variables ζ, and we then compute a Monte Carlo estimate of the gradient of the objective function (6)."
  }, {
    "heading": "3.4. The explaining stage",
    "text": "During the explaining stage, the learned explainer maps each sample X to a weight vector wθ(X) of dimension d, each entry representing the importance of the corresponding feature for the specific sample X . In order to provide a deterministic explanation for a given sample, we rank features according to the weight vector, and the k features with the largest weights are picked as the explaining features.\nFor each sample, only a single forward pass through the neural network parametrizing the explainer is required to yield explanation. Thus our algorithm is much more efficient in the explaining stage compared to other model-agnostic explainers like LIME or Kernel SHAP which require thousands of evaluations of the original model per sample."
  }, {
    "heading": "4. Experiments",
    "text": "We carry out experiments on both synthetic and real data sets. For all experiments, we use RMSprop (Maddison et al., 2016) with the default hyperparameters for optimization. We also fix the step size to be 0.001 across experiments. The temperature for Gumbel-softmax approximation is fixed to be 0.1. Codes for reproducing the key results are available online at https://github.com/Jianbo-Lab/ L2X."
  }, {
    "heading": "4.1. Synthetic Data",
    "text": "We begin with experiments on four synthetic data sets:\n• 2-dimensional XOR as binary classification. The input vector X is generated from a 10-dimensional standard Gaussian. The response variable Y is generated from P (Y = 1|X) ∝ exp{X1X2}. • Orange Skin. The input vector X is generated from a 10- dimensional standard Gaussian. The response variable Y is generated from P (Y = 1|X) ∝ exp{ ∑4 i=1X 2 i − 4}. • Nonlinear additive model. Generate X from a 10-dimensional standard Gaussian. The response variable Y is generated from P (Y = 1|X) ∝ exp{−100 sin(2X1) + 2|X2|+X3 + exp{−X4}}. • Switch feature. GenerateX1 from a mixture of two Gaussians centered at ±3 respectively with equal probability. If X1 is generated from the Gaussian centered at 3, the 2−5th dimensions are used to generate Y like the orange skin model. Otherwise, the 6− 9th dimensions are used to generate Y from the nonlinear additive model.\nThe first three data sets are modified from commonly used data sets in the feature selection literature (Chen et al., 2017). The fourth data set is designed specifically for instancewise feature selection. Every sample in the first data set has the first two dimensions as true features, where each dimension itself is independent of the response variable Y but the combination of them has a joint effect on Y . In the second data set, the samples with positive labels centered around a sphere in a four-dimensional space. The sufficient statistic is formed by an additive model of the first four features. The response variable in the third data set is generated from a nonlinear additive model using the first four features. The last data set switches important features (roughly) based on the sign of the first feature. The 1− 5 features are true for samples with X1 generated from the Gaussian centered at −3, and the 1, 6− 9 features are true otherwise.\nWe compare our method L2X (for “Learning to Explain”) with several strong existing algorithms for instancewise\nfeature selection, including Saliency (Simonyan et al., 2013), DeepLIFT (Shrikumar et al., 2017), SHAP (Lundberg & Lee, 2017), LIME (Ribeiro et al., 2016). Saliency refers to the method that computes the gradient of the selected class with respect to the input feature and uses the absolute values as importance scores. SHAP refers to Kernel SHAP. The number of samples used for explaining each instance for LIME and SHAP is set as default for all experiments. We also compare with a method that ranks features by the input feature times the gradient of the selected class with respect to the input feature. Shrikumar et al. (2017) showed it is equivalent to LRP (Bach et al., 2015) when activations are piecewise linear, and used it in Shrikumar et al. (2017) as a strong baseline. We call it “Taylor” as it is the first-order Taylor approximation of the model.\nOur experimental setup is as follows. For each data set, we train a neural network model with three hidden dense layers. We can safely assume the neural network has successfully captured the important features, and ignored noise features, based on its error rate. Then we use Taylor, Saliency, DeepLIFT, SHAP, LIME, and L2X for instancewise feature selection on the trained neural network models. For L2X, the explainer is a neural network composed of two hidden layers. The variational family is composed of three hidden layers. All layers are linear with dimension 200. The number of desired features k is set to the number of true features.\nThe underlying true features are known for each sample, and hence the median ranks of selected features for each sample in a validation data set are reported as a performance metric, the box plots of which have been plotted in Figure 3. We observe that L2X outperforms all other methods on nonlinear additive and feature switching data sets. On the XOR model, DeepLIFT, SHAP and L2X achieve the best performance. On the orange skin model, all algorithms have near optimal performance, with L2X and LIME achieving the most stable performance across samples.\nWe also report the clock time of each method in Figure 2, where all experiments were performed on a single NVidia Tesla k80 GPU, coded in TensorFlow. Across all the four data sets, SHAP and LIME are the least efficient as they require multiple evaluations of the model. DeepLIFT, Taylor and Saliency requires a backward pass of the model. DeepLIFT is the slowest among the three, probably due to the fact that backpropagation of gradients for Taylor and Saliency are built-in operations of TensorFlow, while backpropagation in DeepLIFT is implemented with high-level operations in TensorFlow. Our method L2X is the most efficient in the explanation stage as it only requires a forward pass of the subset sampler. It is much more efficient compared to SHAP and LIME even after the training time has been taken into consideration, when a moderate number\nof samples (10,000) need to be explained. As the scale of the data to be explained increases, the training of L2X accounts for a smaller proportion of the over-all time. Thus the relative efficiency of L2X to other algorithms increases with the size of a data set."
  }, {
    "heading": "4.2. IMDB",
    "text": "The Large Movie Review Dataset (IMDB) is a dataset of movie reviews for sentiment classification (Maas et al., 2011). It contains 50, 000 labeled movie reviews, with a\nsplit of 25, 000 for training and 25, 000 for testing. The average document length is 231 words, and 10.7 sentences. We use L2X to study two popular classes of models for sentiment analysis on the IMDB data set."
  }, {
    "heading": "4.2.1. EXPLAINING A CNN MODEL WITH KEY WORDS",
    "text": "Convolutional neural networks (CNN) have shown excellent performance for sentiment analysis (Kim, 2014; Zhang & Wallace, 2015). We use a simple CNN model on Keras (Chollet et al., 2015) for the IMDB data set, which\nis composed of a word embedding of dimension 50, a 1-D convolutional layer of kernel size 3 with 250 filters, a maxpooling layer and a dense layer of dimension 250 as hidden layers. Both the convolutional and the dense layers are followed by ReLU as nonlinearity, and Dropout (Srivastava et al., 2014) as regularization. Each review is padded/cut to 400 words. The CNN model achieves 90% accuracy on the test data, close to the state-of-the-art performance (around 94%). We would like to find out which k words make the most influence on the decision of the model in a specific review. The number of key words is fixed to be k = 10 for all the experiments.\nThe explainer of L2X is composed of a global component and a local component (See Figure 2 in Yang et al. (2018)). The input is initially fed into a common embedding layer followed by a convolutional layer with 100 filters. Then the local component processes the common output using two convolutional layers with 50 filters, and the global component processes the common output using a max-pooling layer followed by a 100-dimensional dense layer. Then we concatenate the global and local outputs corresponding to each feature, and process them through one convolutional layer with 50 filters, followed by a Dropout layer (Srivastava et al., 2014). Finally a convolutional network with kernel size 1 is used to yield the output. All previous convolutional layers are of kernel size 3, and ReLU is used as nonlinearity. The variational family is composed of an word embedding layer of the same size, followed by an average pooling and a 250-dimensional dense layer. Each entry of the output vector V from the explainer is multiplied with the embedding of the respective word in the variational family. We use both automatic metrics and human annotators to validate the effectiveness of L2X.\nPost-hoc accuracy. We introduce post-hoc accuracy for quantitatively validating the effectiveness of our method.\nEach model explainer outputs a subset of features XS for each specific sample X . We use Pm(y | X̃S) to approximate Pm(y | XS). That is, we feed in the sample X to the model with unselected words masked by zero paddings. Then we compute the accuracy of using Pm(y | X̃S) to predict samples in the test data set labeled by Pm(y | X), which we call post-hoc accuracy as it is computed after instancewise feature selection.\nHuman accuracy. When designing human experiments, we assume that the key words convey an attitude toward a movie, and can thus be used by a human to infer the review sentiment. This assumption has been partially validated given the aligned outcomes provided by post-hoc accuracy and by human judges, because the alignment implies the consistency between the sentiment judgement based on selected words from the original model and that from humans. Based on this assumption, we ask humans on Amazon Mechanical Turk (AMT) to infer the sentiment of a review given the ten key words selected by each explainer. The words adjacent to each other, like “not good at all,” keep their adjacency on the AMT interface if they are selected simultaneously. The reviews from different explainers have been mixed randomly, and the final sentiment of each review is averaged over the results of multiple human annotators. We measure whether the labels from human based on selected words align with the labels provided by the model, in terms of the average accuracy over 500 reviews in the test data set. Some reviews are labeled as “neutral” based on selected words, which is because the selected key words do not contain sentiment, or the selected key words contain comparable numbers of positive and negative words. Thus these reviews are neither put in the positive nor in the negative class when we compute accuracy. We call this metric human accuracy.\nThe result is reported in Table 4. We observe that the model\nprediction based on only ten words selected by L2X align with the original prediction for over 90% of the data. The human judgement given ten words also aligns with the model prediction for 84.4% of the data. The human accuracy is even higher than that based on the original review, which is 83.3% (Yang et al., 2018). This indicates the selected words by L2X can serve as key words for human to understand the model behavior. Table 2 shows the results of our model on four examples."
  }, {
    "heading": "4.2.2. EXPLAINING HIERARCHICAL LSTM",
    "text": "Another competitive class of models in sentiment analysis uses hierarchical LSTM (Hochreiter & Schmidhuber, 1997; Li et al., 2015). We build a simple hierarchical LSTM by putting one layer of LSTM on top of word embeddings, which yields a representation vector for each sentence, and then using another LSTM to encoder all sentence vectors. The output representation vector by the second LSTM is passed to the class distribution via a linear layer. Both the two LSTMs and the word embedding are of dimension 100. The word embedding is pretrained on a large corpus (Mikolov et al., 2013). Each review is padded to contain 15 sentences. The hierarchical LSTM model gets around 90% accuracy on the test data. We take each sentence as a single feature group, and study which sentence is the most important in each review for the model.\nThe explainer of L2X is composed of a 100-dimensional word embedding followed by a convolutional layer and a max pooling layer to encode each sentence. The encoded sentence vectors are fed through three convolutional layers and a dense layer to get sampling weights for each sentence. The variational family also encodes each sentence with a convolutional layer and a max pooling layer. The encoding vectors are weighted by the output of the subset sampler, and passed through an average pooling layer and a dense layer to the class probability. All convolutional layers are of filter size 150 and kernel size 3. In this setting, L2X can be interpreted as a hard attention model (Xu et al., 2015) that employs the Gumbel-softmax trick.\nComparison is carried out with the same metrics. For human accuracy, one selected sentence for each review is shown to human annotators. The other experimental setups are kept the same as above. We observe that post-hoc accuracy reaches 84.4% with one sentence selected by L2X, and human judgements using one sentence align with the original model prediction for 77.4% of data. Table 3 shows the explanations from our model on four examples."
  }, {
    "heading": "4.3. MNIST",
    "text": "The MNIST data set contains 28×28 images of handwritten digits (LeCun et al., 1998). We form a subset of the MNIST data set by choosing images of digits 3 and 8, with 11, 982\nimages for training and 1, 984 images for testing. Then we train a simple neural network for binary classification over the subset, which achieves accuracy 99.7% on the test data set. The neural network is composed of two convolutional layers of kernel size 5 and a dense linear layer at last. The two convolutional layers contains 8 and 16 filters respectively, and both are followed by a max pooling layer of pool size 2. We try to explain each sample image with k = 4 image patches on the neural network model, where each patch contains 4 × 4 pixels, obtained by dividing each 28 × 28 image into 7 × 7 patches. We use patches instead of raw pixels as features for better visualization.\nWe parametrize the explainer and the variational family with three-layer and two-layer convolutional networks respectively, with max pooling added after each hidden layer. The 7× 7 vector sampled from the explainer is upsampled (with repetition) to size 28 × 28 and multiplied with the input raw pixels.\nWe use only the post-hoc accuracy for experiment, with results shown in Table 4. The predictions based on 4 patches selected by L2X out of 49 align with those from original images for 95.8% of data. Randomly selected examples with explanations are shown in Figure 4. We observe that L2X captures most of the informative patches, in particular those containing patterns that can distinguish 3 and 8."
  }, {
    "heading": "5. Conclusion",
    "text": "We have proposed a framework for instancewise feature selection via mutual information, and a method L2X which seeks a variational approximation of the mutual information, and makes use of a Gumbel-softmax relaxation of discrete subset sampling during training. To our best knowledge, L2X is the first method to realize real-time interpretation of a black-box model. We have shown the efficiency and the capacity of L2X for instancewise feature selection on both synthetic and real data sets."
  }, {
    "heading": "Acknowledgements",
    "text": "L.S. was also supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS. We thank Nilesh Tripuraneni for comments about the Gumbel trick."
  }],
  "year": 2018,
  "references": [{
    "title": "Multiple object recognition with visual attention",
    "authors": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1412.7755,",
    "year": 2014
  }, {
    "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
    "authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "Müller", "K.-R", "W. Samek"],
    "venue": "PloS one,",
    "year": 2015
  }, {
    "title": "How to explain individual classification decisions",
    "authors": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K. Hansen", "MÃžller", "K.-R"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "venue": "arXiv e-prints,",
    "year": 2014
  }, {
    "title": "Kernel feature selection via conditional covariance minimization",
    "authors": ["J. Chen", "M. Stern", "M.J. Wainwright", "M.I. Jordan"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Elements of information theory",
    "authors": ["T.M. Cover", "J.A. Thomas"],
    "year": 2012
  }, {
    "title": "Variational information maximization for feature selection",
    "authors": ["S. Gao", "G. Ver Steeg", "A. Galstyan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "An introduction to variable and feature selection",
    "authors": ["I. Guyon", "A. Elisseeff"],
    "venue": "Journal of machine learning research,",
    "year": 2003
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["E. Jang", "S. Gu", "B. Poole"],
    "year": 2017
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Y. Kim"],
    "venue": "arXiv preprint arXiv:1408.5882,",
    "year": 2014
  }, {
    "title": "Investigating the influence of noise and distractors on the interpretation of neural networks",
    "authors": ["Kindermans", "P.-J", "K. Schütt", "Müller", "K.-R", "S. Dähne"],
    "venue": "arXiv preprint arXiv:1611.07270,",
    "year": 2016
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["J. Li", "Luong", "M.-T", "D. Jurafsky"],
    "venue": "arXiv preprint arXiv:1506.01057,",
    "year": 2015
  }, {
    "title": "The mythos of model interpretability",
    "authors": ["Z.C. Lipton"],
    "venue": "arXiv preprint arXiv:1606.03490,",
    "year": 2016
  }, {
    "title": "A unified approach to interpreting model predictions",
    "authors": ["S.M. Lundberg", "Lee", "S.-I"],
    "year": 2017
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables",
    "authors": ["C.J. Maddison", "A. Mnih", "Y.W. Teh"],
    "venue": "arXiv preprint arXiv:1611.00712,",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2013
  }, {
    "title": "Feature selection based on mutual information criteria of max-dependency, maxrelevance, and min-redundancy",
    "authors": ["H. Peng", "F. Long", "C. Ding"],
    "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
    "year": 2005
  }, {
    "title": "Online and linear-time attention by enforcing monotonic alignments",
    "authors": ["C. Raffel", "T. Luong", "P.J. Liu", "R.J. Weiss", "D. Eck"],
    "venue": "arXiv preprint arXiv:1704.00784,",
    "year": 2017
  }, {
    "title": "Why should i trust you?: Explaining the predictions of any classifier",
    "authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
    "authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1312.6034,",
    "year": 2013
  }, {
    "title": "Striving for simplicity: The all convolutional net",
    "authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"],
    "venue": "arXiv preprint arXiv:1412.6806,",
    "year": 2014
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "Axiomatic attribution for deep networks",
    "authors": ["M. Sundararajan", "A. Taly", "Q. Yan"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["R.J. Williams"],
    "venue": "Machine learning,",
    "year": 1992
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Greedy attack and gumbel attack: Generating adversarial examples for discrete data",
    "authors": ["P. Yang", "J. Chen", "Hsieh", "C.-J", "Wang", "J.-L", "M.I. Jordan"],
    "venue": "arXiv preprint arXiv:1805.12316,",
    "year": 2018
  }, {
    "title": "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
    "authors": ["Y. Zhang", "B. Wallace"],
    "venue": "arXiv preprint arXiv:1510.03820,",
    "year": 2015
  }],
  "id": "SP:d4dd944476f1991501f70aea17d498191f22317a",
  "authors": [{
    "name": "Jianbo Chen",
    "affiliations": []
  }, {
    "name": "Le Song",
    "affiliations": []
  }, {
    "name": "Martin J. Wainwright",
    "affiliations": []
  }, {
    "name": "Michael I. Jordan",
    "affiliations": []
  }],
  "abstractText": "We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.",
  "title": "Learning to Explain: An Information-Theoretic Perspective  on Model Interpretation"
}