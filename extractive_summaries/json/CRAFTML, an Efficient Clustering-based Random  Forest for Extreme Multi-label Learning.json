{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Multi-label classification has received a tremendous attention in the last decade and recently, stimulated by real-life applications involving large datasets (e.g. image (Partalas et al., 2015) and text (Deng et al., 2009) annotation and product recommendation (McAuley et al., 2015)), it has been extended to problems where the number of labels can exceed one million (Agrawal et al., 2013). In this new context, called eXtreme Multi-label Learning (XML), most of the classical algorithms face scalability issues (Weston et al., 2013) or performance degradation. In an attempt to overcome these challenges, researchers have recently explored three directions: (i) using optimization tricks such as pri-\n1Computer Science Laboratory of Nantes (LS2N), France 2Orange Labs Lannion, France. Correspondence to: Wissam Siblini <wissam.siblini@univ-nantes.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmal/dual conversion or sparsification (Yen et al., 2016) and parallelization on supercomputers (Babbar & Schölkopf, 2017; Yen et al., 2017), (ii) reducing the data dimensionality for solving a smaller size problem with a classical approach (Yu et al., 2014; Bhatia et al., 2015) or (iii) hierarchically partitioning the initial problem into small scale sub-problems (Jasinska et al., 2016; Jain et al., 2016). The tree-based decomposition of the third approach has several advantages. It can exploit a hierarchical structuration of the problem either hidden in the mass of data or available for users like the hierarchy of the Wikipedia Article Categories (Partalas et al., 2015) or the ImageNet tags (Deng et al., 2009). Moreover, by decomposing learning into subtasks, it reduces the learning/prediction complexity and opens the way to parallelization. Finally, its sequence of successive decisions allows a great expressivity.\nMotivated by those properties, we here present a novel fast and accurate tree-based approach called CRAFTML (Clustering-based RAndom Forest of predictive Trees for extreme Multi-label Learning). Similarly to PFastReXML (Jain et al., 2016) which is among the best tree-based approaches for XML, CRAFTML is a forest of decision trees trained with the supervision of the labels where the splitting conditions are based on all the features. But CRAFTML has two major differences with PFastReXML: (i) it exploits a random forest strategy which not only randomly reduces both the feature and the label spaces to obtain diversity but also replaces random selections with random projections to preserve more information; (ii) it uses a novel lowcomplexity splitting strategy which avoids the resolution of a multi-objective optimization problem at each node. Numerical experiments on nine datasets from the XML literature show that CRAFTML outperforms the existing XML tree-based approaches with a lower training time and a smaller memory size. Moreover, it has relevant advantages over the most accurate methods today (DISMEC (Babbar & Schölkopf, 2017) and PPDSparse (Yen et al., 2017)): its model size is smaller and its training/prediction complexities are much lower. Experiments confirm that CRAFTML remains competitive with these approaches which both run on one hundred-core machines. In addition, with a parallelized implementation on a five-core machine, the training time of CRAFTML becomes lower than the training time\nof DISMEC for all datasets and close to that of PPDSparse for the large dataset Amazon-670K with 490k instances and 670k labels. By taking advantage of the data sparsity and the hash-based dimensionality reduction, and by implementing a very fast partitioning strategy, CRAFTML leads to a high scalability and a very good tradeoff between accuracy, computing resources and execution speed.\nThe rest of the paper is organized as follows. Section 2 briefly recalls previous recent works in extreme multi-label learning. Section 3 describes our new proposal CRAFTML. Section 4 analyzes its temporal and spatial complexities and discusses the choice of the hyperparameter values. Section 5 compares the performances of CRAFTML with the best performers from the recent literature."
  }, {
    "heading": "2. Related Works",
    "text": "Due to the increasing interest of multi-label classification in the last decade, many algorithms have been proposed (Tsoumakas & Katakis, 2006; Zhang & Zhou, 2014). Several numerical experimentations have highlighted the performances of the multi-label k-nearest neighbors (ML-kNN) (Zhang & Zhou, 2007) and of the multi-label random forest (RF-PCT) (Kocev et al., 2007). However, they cannot scale up anymore with the new dimensionality orders (105 to 107) of the eXtreme Multi-label Learning. Three different strategies are today developed to tackle the scaling issue: optimization tricks and parallelization, dimensionality reduction and hierarchical decomposition.\nOptimization Tricks and Parallelization (PDSparse (Yen et al., 2016), PPDSparse (Yen et al., 2017), DISMEC (Babbar & Schölkopf, 2017)). PDSparse resorts to a sparse linear model regularized with an elastic net to minimize the ranking loss measure. It cleverly exploits a primal-dual conversion of the optimization problem and the sparsity to scale-up to XML data. It has been recently extended to a parallel model called PPDSparse. DISMEC is based on a regularized one-vs-rest large margin linear model. By exploiting the independence between the submodels associated to each label, computations are accelerated with a parallelization of the training stage. And the memory consumption is reduced with a parameter thresholding which leads to a sparse model.\nDimensionality Reduction (WSABIE (Weston et al., 2011), LEML (Yu et al., 2014), SLEEC (Bhatia et al., 2015), AnnexML (Tagami, 2017)). Generally speaking, dimensionality reduction offers a synthetized representation of the data with a lower number of variables (features, labels, or both). LEML and WSABIE were among the first dimensionality reduction approaches applied to XML. They create a low-rank reduced version of the data, but they miss\nthe information brought by the long tail distribution of the labels, and the performances of the classifiers are debased (Bhatia et al., 2015). To overcome this difficulty, SLEEC and AnnexML train a set of local low-rank embeddings on a partition of the feature space to cover a global high-rank embedding of good quality. In SLEEC, the partition is deduced from an unsupervised clustering of the items. In AnnexML, the partition aims at gathering in a same class items which share tail labels. Only AnnexML and SLEEC are retained in our numerical experiments as they significantly outperform WSABIE and LEML.\nTree-based Methods (LPSR (Weston et al., 2013), FastXML (Prabhu & Varma, 2014), its extension PFastReXML (Jain et al., 2016), PLT (Jasinska et al., 2016)). Roughly speaking, the tree-based methods transform the initial large-scale problem into a series of small-scale subproblems by hierarchically partitioning the instance set or the label set. These different subsets are associated to the nodes of a tree. The initial whole set associated to the root is partitioned into a fixed number k of subsets which are themselves associated to k child nodes of the root. The decomposition process is repeated until a stop condition is checked on the subsets. In each node, two optimization problems are raised: (i) computing a partition for a given criterion, and (ii) defining a condition or building a classifier on the feature space to decide to which subset(s) of the partition an instance is assigned. In the prediction phase, the input instance follows a path from the root to a leaf (instance tree) or several leaves (label tree) determined by the successive local decisions. For a label tree, the labels associated to the reached leaves are those predicted with a non-zero probability. For an instance tree, the prediction is given by a local classifier trained on the leaf instances.\nTwo pioneering approaches RF-PCT and HOMER (Tsoumakas et al., 2008) presented in the last decade have highlighted the interest of the tree-based strategy for multilabel learning. In the XML literature, three recent tree-based approaches have been proposed: LPSR and FastXML respectively based on a single k-ary instance tree and a forest of binary instance trees, and PLT based on a label tree (Bengio et al., 2010; Deng et al., 2011). The partitioning criterion of LPSR aims at regrouping in a same subset the instances which share common features and labels. And the assignation of an instance to a subset depends on its proximity to the subset feature centroid. The partitioning criterion of FastXML is a nDCG-based label ranking loss which tends to regroup instances with common labels. And instances are assigned to one of the k = 2 subsets with a sparse splitting hyperplane on the feature space. FastXML has been very recently extended to a new version refered to as PFastReXML in order to better take into account the tail labels. PLT recursively builds label subsets which regroup labels\nthat occur in the same instances and stops its decomposition when the subsets contain a single label. A multi-label classifier is trained to estimate the probabilities of following the different root-to-leaf paths in the tree conditionally to the input instance features. Path ending on leaves associated to relevant labels are expected to have a high probability.\nThese approaches have obtained competitive results in numerical experiments. But we believe that there is still room for improvement by exploring two directions: using very fast partitioning strategies and exploiting tree feature/label randomization. Indeed, to build the tree nodes, the current XML approaches resort to complex optimization processes which can be replaced with simpler operations with lower complexities. Moreover, tree diversity, implemented with random feature selection, contributed to the success of random forests (Breiman, 2001) and RF-PCT in multi-label learning. In the XML literature, this has been tested with feature random selection in the MLRF approach (Agrawal et al., 2013) but the obtained predictive performances are limited compared to FastXML for one main reason: its partitioning strategy (Prabhu & Varma, 2014). A projection is able to preserve more information than a selection for a same ratio of compression and a joint random projection of features and labels is more promising to deal with the extreme number of labels. To address these shortcomings, we have introduced a novel tree-based approach called CRAFTML."
  }, {
    "heading": "3. CRAFTML",
    "text": "In the following, we consider a training set of n instances, each described by a vector x ∈ Rdx of dx features and a vector y ∈ {0, 1}dy of dy labels. The feature (resp. label) matrix X (resp. Y ) of the set is the n× dx (resp. n× dy) matrix where each row corresponds to the feature (resp. label) vector of an instance."
  }, {
    "heading": "3.1. The Method Main Steps",
    "text": "CRAFTML computes a forest F of mF k-ary instance trees whose construction follows the common scheme of the instance tree-based methods (see Algorithm 1) recalled in Section 2. The stop condition of the recursive partitioning of a tree is classical: (i) the cardinality of the node’s instance subset is lower than a given threshold nleaf , (ii) all the instances have the same features, or (iii) all the instances have the same labels. Once a tree has been trained, its leaves store the average label vector of their associated instances. Similarly to FastXML the nodes partitioning objective of CRAFTML is to regroup instances with common labels in a same subset but its computation is very different. Our goal was to develop a strategy driven by two constraints: partition computation must be based on randomly projected instances to ensure diversity and must perform low complexity operations for scalability. Therefore, the node training\nAlgorithm 1 trainTree Input: Training set with a feature matrix X and a label matrix Y . Initialize node v v.isLeaf←testStopCondition(X,Y ) if v.isLeaf = false then\nv.classif← trainNodeClassifier(X,Y ) (Xchildi , Ychildi)i=0,..,k−1 ← split(v.classif, X, Y ) for i from 0 to k − 1 do v.childi ←trainTree(Xchildi , Ychildi)\nend for else\nv.ŷ ←computeMeanLabelVector(Y ) end if Output: node v\nAlgorithm 2 trainNodeClassifier Input: feature matrix (Xv) and label matrix (Yv) of the instance set of the node v. Xs, Ys ← sampleRows(Xv, Yv, ns) X ′s ← XsPx # random feature projection Y ′s ← YsPy # random label projection c← k-means(Y ′s , k) # c ∈ {0, ..., k − 1}min(nv,ns) for i from 0 to k − 1 do\n(classif)i,. ← computeCentroid({(X ′s)j,.|cj = i}) end for Output: Classifier classif (∈ Rk×d′x ).\nc is a vector where the j th component cj denotes the cluster index of the j th instance associated to (X ′s)j,. and (Y ′ s )j,..\nstage in CRAFTML is decomposed into three consecutive steps (see Algorithm 2):\n1. a random projection into lower dimensional spaces of the label and feature vectors of the node’s instances.\n2. a k-means based partitioning of the instances into k temporary subsets from their projected labels.\n3. The training of a simple multi-class classifier to assign each instance to its relevant temporary subset (i.e. cluster index computed at step 2) from its feature vector. The instances are partitioned into k final subsets (child nodes) by the classifier (”split” in Algorithm 1).\nIn the prediction phase, for each tree, the input instance follows a root-to-leaf path determined by the successive decisions of the classifier and the provided prediction is the average label vector stored in the leaf reached. The forest aggregates the tree predictions with the average operator.\nLet us specify that contrary to classical random forests which use bootstraps, each tree of CRAFTML is trained\non the full initial dataset. In XML, instance samples can miss a large number of labels and we want each tree to consider every label. The main similarities and differences between CRAFTML and the other state-of-the-art tree-based approaches are summarized in Table 1."
  }, {
    "heading": "3.2. Node Training in Detail",
    "text": "We here detail the three steps of the instance partitioning process in each node v of a tree T of a forest F .\nStep 1: Random Projection of the Instances of v The feature and label vectors x and y of each instance of v are projected into a space with a lower dimensionality: x′ = xPx and y′ = yPy where Px (resp Py) is a random projection matrix of Rdx×d′x (resp. Rdy×d ′ y ) and d′x (resp. d′y) is the dimension of the reduced feature (resp. label) space. The projection matrices are different from one tree to another. To optimize memory, the coefficients of the projection matrices are not stored but generated with a seed when needed. We have tested two random projections: a projection generated from a standard Gaussian distribution and a sparse orthogonal projection hereafter referred to as the hashing trick (Weinberger et al., 2009) which has only one non-zero parameter with a value of −1 or +1 on each row. Comparisons have led us to favor the hashing trick. It led to slightly better performances. Moreover, through the sparsity of its projection, it is much faster, and as the resulting projected vector has at the most as many non-zero elements as the original one, other components of the forest can be accelerated. In addition, we have explored the impact of the diversity of the projection matrices between the nodes of a same tree T . We have considered four combinations: SxSy, SxDy, DxSy and DxDy where Sx (resp. Sy) is the case where the feature (resp. label) projections are the Same in each node of T and Dx (resp. Dy) is the case where the feature (resp. label) projections are Different from one node to another. Comparisons presented in Section 4.2 have led us to favor SxSy.\nStep 2: Instance Partitioning into k Temporary Subsets Let Ys be the label matrix of a sample drawn without re-\nplacement of size at most ns of the instance set associated to v. The sample is partitioned with a spherical k-means (Loyd’s algorithm) applied on YsPy. The cosine metric of the spherical k-means is fast to compute and is welladapted to sparse data. The cluster centroids are initialized with the k-means++ strategy (Arthur & Vassilvitskii, 2007) which improves cluster stability and algorithm performances against a random initialization.\nStep 3: Assigning a Subset from the Projected Features The multi-class classifier is very simple: in each temporary subset, we compute the centroid of the instance projected feature vectors. In the prediction phase, the classifier assigns the subset whose centroid is the closest to the input instance projected feature vector. We have also tested a one-vs-rest linear model but the resulting forest was less accurate and much slower. Two metrics (cosine and classical Euclidean) have been compared too and, for similar reasons as in the k-means, the cosine is better."
  }, {
    "heading": "4. Algorithm Analysis and Parameter Setting",
    "text": "In this section, we provide bounds for the time and memory complexities of CRAFTML. Then, we analyze the impact of the hyperparameters which govern its performances, and we recommend a parameter setting adapted to XML data."
  }, {
    "heading": "4.1. Time and Memory Complexities",
    "text": "In the following, we denote by sx (resp. sy) the average number of non-zero elements in the feature (resp. label) vectors of the instances. In practice, thanks to the hashing trick, the projected feature and label vectors have less than respectively sx and sy non-zero elements on average. For a node v of a tree T , nv denotes the number of instances of the subset associated to v. The number i of iterations of the spherical k-means is fixed a priori.\nLemma 1. For a node v of a tree T , the time complexity Cv is bounded by O ( nv ×C ) where C = k× (i× sy + sx) is the complexity per instance.\nProof. The time complexity of a node v is the sum of the complexities of the spherical k-means initialized with a kmeans++, of the training of the multi-class classifier and of its predictions on the instances of the subset associated to v. The k-means algorithm is applied on the projected labels and its complexity is bounded by the sum of the complexity O(i × nv × sy × k) of the Loyd’s algorithm and the complexity O(nv × sy × k) of the k-means++. The classifier training is based on the computation of the feature centroid for each cluster which leads to a total complexity O(nv × sx). For its predictions, the classifier computes the cosine distance to the k centroids for each instance, which requires O(nv × k × sx) operations.\nIn practice, the complexity of the node training is lower than the bound given in Lemma 1. Indeed, the k-means and the classifier training are applied on a sample of the node instance set with a size equal to min(nv, ns) ≤ nv . In addition, in our experiments, CRAFTML already reaches its best performances with only i = 2 iterations in the k-means.\nLet us now consider a strictly k-ary tree T and denote by lT its number of leaves, by mT = lT−1k−1 its number of nodes\nand by nT = ∑\nv∈T nv mT\nthe average number of instances in its nodes.\nProposition 1. If the tree T is balanced, its training time complexity CT is bounded by O ( logk(\nn nleaf\n) × n × C ) .\nOtherwise, CT is equal to O ( lT−1 k−1 × nT × C ) .\nProof. If T is a balanced tree, the jth level Tj of T has kj nodes and for each node v ∈ Tj , nv = nkj . From Lemma 1, the complexity of the jth level is therefore O ( n kj ×k j ×C ) which is independent of j. Due to the stop condition, the number of levels is bounded by O(logk( nnleaf )). Finally the product of a level complexity with the number of levels gives the complexity CT for the tree.\nIf T is an unbalanced tree, the training complexity of T which is the sum of the complexities of its nodes equals to O (∑ v∈T nv ×C ) . Substituting ∑ v∈T nv by mT × nT in the last formula ends the proof.\nWith the ratio Ck−1 = k×(i×sy+sx)\nk−1 , the contribution of k in the time complexity vanishes for the unbalanced tree. Furthermore, this time depends on the number of leaves lT and the average number of instance in the nodes nT . In practice, lT is between nnleaf for the best case and n for the worst case. In our experiments we have observed on the XML datasets that, when fixing nleaf = 10 and k = 2, we obtain lT = n2.83±0.41 and nT = 24.32± 2.61.\nWe can note that by exploiting the data sparsity (sx and sy) the time complexity is independent of the projection dimensions d′x and d ′ y. In XML datasets where instances are very sparse, sx and sy are much smaller than dx and dy. In addition, the complexity of the random projection which is negligible compared to the other operations is not considered here. For instance, in a tree T , the complexity of the fastest combination SxSy chosen in our experiments is equal to O ( n × (sx + sy) × Cgen ) where Cgen is the complexity of generating a coefficient of the projection matrices1. It is also important to stress that, in practice, the bound CT is above reality due to the instance sampling in each node v which reduces the complexities of the spherical k-means and of the classifier training.\n1In our experiments, the coefficients are generated with MurmurHash3 (Appleby, 2008) which is among the fastest hash function and which produces good quality random distributions.\nProposition 2. The memory complexity of a tree T is bounded by O ( n× sy +mT × k × d′x ) .\nProof. Each leaf of T stores the mean label vector of its associated instances. In the best case all the nlT instances have the same labels and the vector contains sy non-zero elements on average. In the worst case the nlT instances have different labels and the vector approximately contains sy×n lT\nnon-zero elements on average. Consequently, the memory required for the leaves is bounded by O ( lT× sy×nlT ) . Each node of T stores a multi-class classifier represented by the k feature vectors of the centroids of dimension d′x; the required memory is mT × k × d′x.\nIn practice, the bound determined in Proposition 2 is significantly greater than the required memory for two reasons. Firstly, the instances regrouped in a same leaf with the clustering share many common labels and the worst case does not occur. Secondly, the centroids stored in the nodes are sparse especially for nodes distant from the root because they are computed on a small subset of similar vectors."
  }, {
    "heading": "4.2. Performance Analysis",
    "text": "In this section, we explore the respective effect of five hyperparameters (d′x, d ′ y, mF , nleaf , ns) on the predictive performances of CRAFTML for five real world datasets from the XML repository2: four common datasets from the multi-label learning literature (Bibtex, Mediamill, Delicious, EURLex-4K) with a maximum of 5000 features and 3993 labels and a large one (Wiki10-31K) with 101938 features and 30938 labels (see Table 2 for details). The quality of the results is measured with the precision at 1 (P@1), 3 (P@3) and 5 (P@5) which are defined in the repository and classically used in XML numerical experiments. To avoid test set overfitting here, we restrict ourselves to the training part of the datasets: a validation set with twenty percent of the instances is used for evaluation.\nImpact of the Projection Figures 1 and 2 show the impact of the projection dimensions and of the diversity strategies on the performances of CRAFTML with mF = 50. Performances significantly increase with d′x, less with d ′ y and both evolutions reach a plateau which varies with the dataset characteristics. When the two dimensions d′x and d ′ y are large enough (> 500), the effect of the random projections is substancial (comparison with vs without projection in Figure 1). Moreover, in this case the four combinations of presence/absence of node diversity SxSy, SxDy, DxSy and DxDy lead to close behaviors (Figure 2).\n2http://manikvarma.org/downloads/XC/ XMLRepository.html\nImpact of the Number of Trees The contribution of the nodes to the forest memory complexity given by Proposition 2 linearly depends on the product mF × d′x. This raises a question: are the best performances obtained with a few trees with a large dimension d′x or with a large set of trees with a small dimension? Intuitively several trees are required to benefit from the aggregation effect but the dimension must be reasonable to preserve the accuracy of each tree. Figure 3 confirms this intuition. It shows that the optimum is reached for a correct balance between mF and d′x whose value varies with the dataset.\nImpact of the Tree Depth The tree depth is controlled by the stop condition and especially by the minimal number of instances per leaf nleaf . For each tree a large nleaf is\nrequired to avoid overfitting the training data (left chart on Figure 4), but for a forest, a limited set of instances per leaf creates more diversity between the trees (right chart on Figure 4). This phenomenon has already been described in the random forest literature (Breiman, 2001).\nImpact of the Sample Size in the Spherical k-means For the dataset with the largest number of instance n = 1.7M (WikiLSHTC-325K) the performances improve with the sample size until they reach a plateau for ns = 20000. The impacts on P@1, on P@3 and on P@5 are almost identical."
  }, {
    "heading": "4.3. Parameter Setting",
    "text": "Experiments have shown that CRAFTML requires a dozen deep trees and large dimensions (d′x > 5000 and d ′ y > 5000) for the projected spaces. To limit size effects in the experimental comparisons, the chosen number of trees and stop condition are the same as for FastXML: mF = 50 and nleaf = 10 (Prabhu & Varma, 2014). As shown in Section 4.1, the label projection dimension d′y does not impact the time and memory complexities and it has consequently been fixed to an arbitrary high value: d′y = min(dy, 10000). The feature projection dimension d′x has also no effect on time and a very limited one on memory in practice. CRAFTML reaches the plateau of performances for each dataset for a sample size ns = 20000 and a dimension d′x = min(dx, 10000). And its measured model size is low (Table 3). Moreover, for the chosen values of mF , nleaf , d′x and d ′ y , CRAFTML obtains close performances with the four variations SxSy, SxDy, DxSy and DxDy. We use SxSy which is the fastest combination."
  }, {
    "heading": "5. Experimental Comparisons",
    "text": "In this section we compare CRAFTML with the nine best methods of the state-of-the-art in extreme multi-label learning presented in Section 2: four tree-based methods (FastXML, PFastReXML, LPSR, PLT) and five others with a distinction between those adapted to single-core\nmachines (SLEEC, AnnexML, PDSparse) and those especially designed for parallel implementations (DISMEC, PPDSparse). The numerical experiments are carried on nine XML datasets from different application domains: Bibtex, Mediamill, Delicious, EURLex-4K, Wiki10-31K, Delicious200K, AmazonCat-13K, WikiLSHTC-325K, Amazon670K. The instance, feature and label cardinalities are reported in the first column of Table 2 and additional details are available in the XML repository. Results considered for the comparisons are extracted from the last published results 3. The hyperparameters chosen for CRAFTML are those described in the previous section. Table 2 presents the results of the predictive performances and Table 3 the time consumption and the model size."
  }, {
    "heading": "5.1. Comparison with Tree-based Methods",
    "text": "CRAFTML outperforms the best tree-based methods in most cases. For the datasets WikiLSHTC-325K and Amazon-670K, the domination of PFastreXML may be partly explained by the fact that it is trained with label propensities that are computed with additional external information (label hierarchy of Wikipedia and Amazon) (Jain\n3from the XML repository for PFastReXML, FastXML, LPSRNB, SLEEC, and DISMEC, from (Jasinska et al., 2016) for PLT, from (Yen et al., 2017) for PDSparse, PPDSparse and the remaining results of DISMEC, and from (Tagami, 2017) for AnnexML\net al., 2016). Comparisons of computing times are sensitive as the approaches have been developed with different languages (Java for CRAFTML) and times have been measured on different machines. Nevertheless Table 3 confirms that CRAFTML is very competitive. Compared with the other tree-based methods its observed training time is lower on average and its model size is smaller. These measures are consistent with the theoretical results: due to the sampling strategy and the dimensionality reduction resulting from the random projections, the training time and memory complexities of CRAFTML are the lowest ones. Its predictive time is higher than the others but its complexity is equivalent."
  }, {
    "heading": "5.2. Comparison with other Single-core Implementations (SLEEC, PDSparse, AnnexML)",
    "text": "The performances of CRAFTML are better than those of PDSparse except for WikiLSHTC-325K. They are substantially equivalent to those of SLEEC but there is a slight domination of CRAFTML on the four largest datasets. Moreover, CRAFTML is faster than SLEEC and than PDSparse except for the dataset AmazonCat-13K. The specificities of this dataset -i.e. a small number of labels and a large number of instances- favor PDSparse. The CRAFTML model size is lower than that of SLEEC -except for WikiLSHTC325K-, but it is greater than that of PDSparse: 1.2 times for EURLex-4K, 1.98 times for WikiLSHTC-325K, 93 times\nfor Delicious-200K and 45 times for Amazon-13K. With a threshold applied on its parameter after training, the final PDSparse model size is very low, but PDSparse requires a large amount of memory during training; e.g. it cannot be trained on the dataset Amazon-670K with 100Gb of memory (Yen et al., 2017). The comparison with AnnexML is more sensitive as its training time and model size are not published for a single-core implementation. The published performances show that CRAFTML is close to AnnexML except for Amazon-670K and WikiLSHTC-325K. But for the latter, the training time of AnnexML (4 hours) only mentioned for a 24-core implementation suggests that CRAFTML is faster (1.5 hour on a 5-core machine)."
  }, {
    "heading": "5.3. Comparison with Parallel Implementations (DISMEC, PPDSparse)",
    "text": "The results of the linear models DISMEC and PPDSparse reported in Tables 2 and 3 have been obtained on a one hundred-core machine. Let us recall that the DISMEC model has been specifically designed for parallelization and is inapplicable on a single-core machine. When comparing performances of these two approaches with those obtained by CRAFTML on a single-core machine the conclusions are mixed and depend on the dataset. The datasets WikiLSHTC325K and Amazon-670K seem to favor the two linear model based approaches over every tree-based methods. For most datasets, the size of CRAFTML model is lower than that of DISMEC and PPDSparse. Surprisingly, the prediction time of CRAFTML obtained on a single-core machine is often lower than those of DISMEC and PPDSparse. Its training time is also lower than that of DISMEC for the large datasets and similar for the smallest ones but higher than that of PPDSparse. In addition, we have measured the time gains of CRAFTML with a five-core machine. In\nthis case, the training time of CRAFTML is lower than that of DISMEC for all the datasets and that of PPDSparse for Delicious-200K, and it becomes close to the training time of PPDSparse for Amazon-670K. Consequently, even with five cores only, CRAFTML is competitive with the best parallelized approaches. Most importantly, its acceleration factor of about four between a single-core and a five-core implementation and its lower training/prediction time complexity allows us to hope to be as fast or even faster than PPDSparse on a comparable supercomputer."
  }, {
    "heading": "6. Conclusion",
    "text": "Our new XML multi-label learning method CRAFTML outperforms the other tree-based methods with a single core computer and it is competitive with the state-of-the-art PPDSparse even with a restricted parallel implementation. Contrary to most of the current XML methods, CRAFTML does not rely on a complex optimization scheme. It combines simple and fast learning blocks (e.g. k-mean clustering, basic multi-class classifier) which leaves room for extensions to reach performances required by current societal and technical challenges (Kambatla et al., 2014). With the growing dimensionality of data, machine learning increasingly resorts to supercomputers. But this access is far from being available everywhere today and its cost will set limits in the future. Consequently, cheap and scalable machine learning algorithms are required to favor the democratization of the numerous real-world applications which still rely on standard computation. However, cloud computing (Hashem et al., 2015) and the increasing development of supercomputers (Dean et al., 2018) also need methods that fully exploit the available computation resources by being, in particular, easily parallelizable."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the reviewers whose comments helped us to improve and clarify this manuscript."
  }],
  "year": 2018,
  "references": [{
    "title": "Multilabel learning with millions of labels: Recommending advertiser bid phrases for web pages",
    "authors": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"],
    "venue": "In Proceedings of the 22nd international conference on World Wide Web,",
    "year": 2013
  }, {
    "title": "Murmurhash 2.0 - https://sites",
    "authors": ["A. Appleby"],
    "venue": "google.com/site/murmurhash/,",
    "year": 2008
  }, {
    "title": "k-means++: The advantages of careful seeding",
    "authors": ["D. Arthur", "S. Vassilvitskii"],
    "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
    "year": 2007
  }, {
    "title": "Dismec: Distributed sparse machines for extreme multi-label classification",
    "authors": ["R. Babbar", "B. Schölkopf"],
    "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,",
    "year": 2017
  }, {
    "title": "Label embedding trees for large multi-class tasks",
    "authors": ["S. Bengio", "J. Weston", "D. Grangier"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Sparse local embeddings for extreme multi-label classification",
    "authors": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2015
  }, {
    "title": "A new golden age in computer architecture: Empowering the machine learning revolution",
    "authors": ["J. Dean", "D. Patterson", "C. Young"],
    "venue": "IEEE Micro,",
    "year": 2018
  }, {
    "title": "Imagenet: A large-scale hierarchical image database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition.,",
    "year": 2009
  }, {
    "title": "Fast and balanced: Efficient label tree learning for large scale object recognition",
    "authors": ["J. Deng", "S. Satheesh", "A.C. Berg", "F. Li"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "The rise of big data on cloud computing: Review and open research issues",
    "authors": ["I.A.T. Hashem", "I. Yaqoob", "N.B. Anuar", "S. Mokhtar", "A. Gani", "S.U. Khan"],
    "venue": "Information Systems,",
    "year": 2015
  }, {
    "title": "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
    "authors": ["H. Jain", "Y. Prabhu", "M. Varma"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "Extreme f-measure maximization using sparse probability estimates",
    "authors": ["K. Jasinska", "K. Dembczynski", "R. Busa-Fekete", "K. Pfannschmidt", "T. Klerx", "E. Hullermeier"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Trends in big data analytics",
    "authors": ["K. Kambatla", "G. Kollias", "V. Kumar", "A. Grama"],
    "venue": "Journal of Parallel and Distributed Computing,",
    "year": 2014
  }, {
    "title": "Ensembles of multi-objective decision trees",
    "authors": ["D. Kocev", "C. Vens", "J. Struyf", "S. Džeroski"],
    "venue": "Machine Learning: ECML",
    "year": 2007
  }, {
    "title": "Inferring networks of substitutable and complementary products",
    "authors": ["J. McAuley", "R. Pandey", "J. Leskovec"],
    "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2015
  }, {
    "title": "Lshtc: A benchmark for largescale text classification",
    "authors": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "Amini", "M.-R", "P. Galinari"],
    "venue": "arXiv preprint arXiv:1503.08581,",
    "year": 2015
  }, {
    "title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning",
    "authors": ["Y. Prabhu", "M. Varma"],
    "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2014
  }, {
    "title": "Annexml: Approximate nearest neighbor search for extreme multi-label classification",
    "authors": ["Y. Tagami"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2017
  }, {
    "title": "Multi-label classification: An overview",
    "authors": ["G. Tsoumakas", "I. Katakis"],
    "venue": "International Journal of Data Warehousing and Mining,",
    "year": 2006
  }, {
    "title": "Effective and efficient multilabel classification in domains with large number of labels",
    "authors": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"],
    "venue": "In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data",
    "year": 2008
  }, {
    "title": "Feature hashing for large scale multitask learning",
    "authors": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2009
  }, {
    "title": "Wsabie: Scaling up to large vocabulary image annotation",
    "authors": ["J. Weston", "S. Bengio", "N. Usunier"],
    "venue": "In International Joint Conference on Artificial Intelligence,",
    "year": 2011
  }, {
    "title": "Label partitioning for sublinear ranking",
    "authors": ["J. Weston", "A. Makadia", "H. Yee"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification",
    "authors": ["Yen", "I.E.-H", "X. Huang", "P. Ravikumar", "K. Zhong", "I. Dhillon"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Large-scale multi-label learning with missing labels",
    "authors": ["Yu", "H.-F", "P. Jain", "P. Kar", "I. Dhillon"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Ml-knn: A lazy learning approach to multi-label learning",
    "authors": ["Zhang", "M.-L", "Zhou", "Z.-H"],
    "venue": "Pattern recognition,",
    "year": 2007
  }, {
    "title": "A review on multi-label learning algorithms",
    "authors": ["Zhang", "M.-L", "Zhou", "Z.-H"],
    "venue": "IEEE transactions on knowledge and data engineering,",
    "year": 2014
  }],
  "id": "SP:ea8380650754c485afa585cf7576a445a5a0f6c6",
  "authors": [{
    "name": "Wissam Siblini",
    "affiliations": []
  }, {
    "name": "Pascale Kuntz",
    "affiliations": []
  }, {
    "name": "Frank Meyer",
    "affiliations": []
  }],
  "abstractText": "Extreme Multi-label Learning (XML) considers large sets of items described by a number of labels that can exceed one million. Tree-based methods, which hierarchically partition the problem into small scale sub-problems, are particularly promising in this context to reduce the learning/prediction complexity and to open the way to parallelization. However, the current best approaches do not exploit tree randomization which has shown its efficiency in random forests and they resort to complex partitioning strategies. To overcome these limits, we here introduce a new random forest based algorithm with a very fast partitioning approach called CRAFTML. Experimental comparisons on nine datasets from the XML literature show that it outperforms the other tree-based approaches. Moreover with a parallelized implementation reduced to five cores, it is competitive with the best state-of-the-art methods which run on one hundred-core machines.",
  "title": "CRAFTML, an Efficient Clustering-based Random  Forest for Extreme Multi-label Learning "
}