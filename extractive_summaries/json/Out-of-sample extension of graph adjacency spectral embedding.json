{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Given a graph G = (V,E) on n vertices with adjacency matrix A ∈ {0, 1}n×n, the problem of graph embedding is to map the vertices of G to some d-dimensional vector space S in such a way that geometry in S reflects the topology of G. For example, we may ask that vertices with high conductance in G be assigned to nearby vectors in S. This is a special case of the problem of dimensionality reduction, well-studied in machine learning and related disciplines (van der Maaten et al., 2009). When applied to graph data, each vertex in G is described by an n-dimensional binary\n1Department of Statistics, University of Michigan, USA. 2School of Mathematics and Physics, University of Queensland, Australia. 3International Computer Science Institute, Berkeley, USA. 4Department of Statistics, University of California at Berkeley, USA. 5Department of Applied Mathematics and Statistics, Johns Hopkins University, USA. Correspondence to: Keith Levin <klevin@umich.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nvector, namely its corresponding column (or row) in adjacency matrix A ∈ {0, 1}n×n, and we wish to associate with each vertex v ∈ V a lower-dimensional representation, say xv ∈ S. The two most commonly-used approaches for graph embeddings are the graph Laplacian embedding and its variants (Belkin & Niyogi, 2003; Coifman & Lafon, 2006) and the adjacency spectral embedding (ASE, Sussman et al., 2012). Both of these embedding procedures produce low-dimensional representations of the vertices in a graph G, and the question of “which embedding is preferable?” is dependent on the downstream task. Indeed, one can show that neither embedding dominates the other for the purposes of vertex classification; see, for example, Section 4.3 of Tang & Priebe (to appear). In addition, the results in Section 4.3 of Tang & Priebe (to appear) suggest that ASE performs better than the Laplacian eigenmaps embedding for graphs that exhibit a core-periphery structure. Such structures are ubiquitous in real networks, such as those arising in social and biological sciences (Jeub et al., 2015; Leskovec et al., 2009).\nThe ASE and Laplacian embedding differ in that the latter has received far more attention, especially with respect to questions of limit objects (Hein et al., 2005) and out-ofsample extensions (Bengio et al., 2003). The aim of this paper is to establish theoretical foundations for the latter of these two problems in the case of the adjacency spectral embedding."
  }, {
    "heading": "2. Background and Notation",
    "text": "In the standard out-of-sample (OOS) extension, we are presented with training dataD = {z1, z2, . . . , zn} ⊆ X , where X is the set of possible observations. The data D give rise to a symmetric matrix M = [K(zi, zj)] ∈ Rn×n, where K : X × X → R≥0 is a kernel function that measures similarity between elements of X , so that K(y, z) is large if y, z ∈ X are similar, and is small otherwise. Suppose that we have computed an embedding of the data D. Let us denote this embedding by X ∈ Rn×d, so that the embedding of zi ∈ D is given by the i-th row of X . Suppose that we are given an additional observation z ∈ X , not necessarily included in D, and we wish to embed z under the same scheme as was used to produce X . A naı̈ve approach would be to discard the old embedding X , consider the augmented\ncollection D = D ∪ {z} and construct a new embedding X̃ ∈ R(n+1)×d. However, in many applications, it is infeasible to compute this embedding again from scratch, either because of computational constraints or because the similarities {K(zi, zj) : zi, zj ∈ D} may no longer be available after X has been computed. Thus, the OOS problem is to embed z using only the available embedding X which was initially learned from D and the similarities {K(zi, z)}ni=1.\nAs an example, consider the Laplacian eigenmaps embedding (Belkin & Niyogi, 2003; Belkin et al., 2006). Given a graph G = (V,E) with adjacency matrix A ∈ Rn×n, the d-dimensional normalized Laplacian of G is the matrix L = D−1/2AD−1/2, where D ∈ Rn×n is the diagonal degree matrix, i.e., dii = ∑ j Aij is the degree of the vertex i (Luxburg, 2007; Vishnoi, 2013). The d-dimensional normalized Laplacian eigenmaps embedding of G is given by the rows of the matrix UL ∈ Rn×d, whose columns are the d orthonormal eigenvectors corresponding to the top d eigenvalues of L, excepting the trivial eigenvalue 1. We note that some authors (see, for example, Chung, 1997) use I −D−1/2AD−1/2 to be the normalized graph Laplacian, but since this matrix has the same eigenspace as our L, results concerning the eigenvectors of either of these matrices are equivalent. Suppose that a vertex v is added to graph G, to form graph G̃ with adjacency matrix\nÃ = [ A ~a ~aT 0 ] , (1)\nwhere ~a ∈ {0, 1}n. A naı̈ve approach to embedding G̃ would be to compute the top eigenvectors of the graph Laplacian of G̃ as before. However, the OOS extension problem requires that we only use the information available in UL and ~a to compute an embedding of the new vertex v.\nBengio et al. (2003) presented out-of-sample extensions for multidimensional scaling (MDS, Torgerson, 1952; Borg & Groenen, 2005), spectral clustering (Weiss, 1999; Ng et al., 2002), Laplacian eigenmaps (Belkin & Niyogi, 2003) and ISOMAP (Tenenbaum et al., 2000). These OOS extensions were based on a least-squares formulation of the embedding problem, arising from the fact that the in-sample embeddings are given by functions of the eigenvalues and eigenfunctions. Trosset & Priebe (2008) considered a different OOS extension for MDS. Rather than following the approach of Bengio et al. (2003), Trosset & Priebe (2008) cast the MDS OOS extension as a simple modification of the in-sample MDS optimization problem.\nLet {(λt, vt)}nt=1 be the eigen-pairs of the matrix M , constructed from some suitably-chosen similarity function, K, defined on pairs of observations in D ×D. In general, OOS extensions for eigenvector-based embeddings can be derived as in Bengio et al. (2003) as the solution of a least-squares\nproblem\nmin f(x)∈Rd n∑ i=1\n( K(x, xi)− 1\nn d∑ t=1 λtft(xi)ft(x)\n)2 ,\nwhere {xi}ni=1 are the in-sample observations, and ft(xi) = [vt]i is ith component of vt. Belkin et al. (2006) presented a slightly different approach that incorporates regularization in both the intrinsic geometry of the data distribution and the geometry of the similarity function K. Their approach applies to Laplacian eigenmaps as well as to regularized least squares and SVM. The authors also introduced a Laplacian SVM, in which a Laplacian penalty term is added to the standard SVM objective function. Belkin et al. (2006) showed that all of these embeddings have OOS extensions that arise as the solution of a generalized eigenvalue problem. We refer the interested reader to Levin et al. (2015) for a practical application of this OOS extension. More recent approaches to OOS extension have avoided altogether the need to solve a least squares or eigenvalue problem by, instead, training a neural net to learn the embedding directly (see, for example, Quispe et al., 2016; Jansen et al., 2017).\nThe only existing work to date on the ASE OOS extension of which we are aware appears in Tang et al. (2013a). The authors considered the OOS extension for ASE applied to latent position graphs (see, for example Hoff et al., 2002), in which each vertex is associated with an element of a vector space and edge probabilities are given by a suitably-chosen inner product. The authors introduced a least-squares OOS extension for embeddings of latent position graphs and proved a theorem, analogous to our Theorem 1, for the error of this extension about the true latent position. Theorem 1 simplifies the proof of the result due to Tang et al. (2013a) for the case of random dot product graphs (see Definition 1 below).\nOf crucial importance in assessing OOS extensions, but largely missing from the existing literature, is an investigation of how the OOS estimate compares with the insample embedding. That is, for an out-of-sample observation z ∈ X , how well does its OOS embedding X̂z ∈ Rd, approximate the embedding that would be obtained by considering the full sample D = D ∪ {z}? In this paper, we address this question in the context of the adjacency spectral embedding. In particular, we show in our main results, Theorems 1 and 2, that two different approaches to the ASE OOS extension recover the in-sample embedding at a rate that is, in a certain sense, optimal (see the discussion at the end of Section 4). We conjecture that analogous rate results can be obtained for other OOS extensions such as those presented in Bengio et al. (2003)."
  }, {
    "heading": "2.1. Notation",
    "text": "We pause briefly to establish notational conventions for this paper. For a matrix B ∈ Rn1×n2 , we let σi(B) denote the i-th singular value of B, so that σ1(B) ≥ σ2(B) ≥ · · · ≥ σk(B) ≥ 0, where k = min{n1, n2}. For positive integer n, we let [n] = {1, 2, . . . , n}. Throughout this paper, n will index the number of vertices in a hollow graph G, the observed data, and we let c > 0 denote a positive constant, not depending on n, whose value may change from line to line. For an event E, we let Ec denote its complement. We will say that event En, indexed so as to depend on n, occurs with high probability, and write En w.h.p. , if for some constant > 0, it holds for all suitably large n that Pr[Ecn] ≤ n−(1+ ). In this paper, we will show Pr[Ec] ≤ cn−2 any time we wish to show that event E occurs with high probability. All our results involve showing that some event En occurs w.h.p., and we note that in all such cases, the Borel-Cantelli Lemma implies that with probability 1, the event Ecn occurs for at most finitely many n. That is, all our finite-sample results can be easily altered to yield corresponding asymptotic results, as well. For a function f : Z≥0 → R≥0 and a sequence of random variables {Zn}, we will write Zn = O(f(n)) if there exists a constant C and a number n0 such that Zn ≤ Cf(n) for all n ≥ n0, and write Zn = O(f(n)) a.s. if the event Zn ≤ Cf(n) occurs a.s.a.a. For a vector x ∈ Rd, we use the unadorned norm ‖x‖ to denote the Euclidean norm of x. For a matrix M ∈ Rn×d, we use the unadorned norm ‖M‖ to denote the operator norm\n‖M‖ = max x∈Rd:‖x‖=1 ‖Mx‖\nand we use ‖ · ‖2→∞ to denote the matrix operator norm\n‖M‖2→∞ = max x:‖x‖=1 ‖Mx‖∞ = max i∈[n] ‖Mi,·‖,\nwhich can be proven via the Cauchy-Schwarz inequality (Horn & Johnson, 2013). This latter operator norm will be especially useful for us, in that a bound on ‖M‖2→∞ gives a uniform bound on the rows of matrix M ."
  }, {
    "heading": "2.2. Roadmap",
    "text": "The remainder of this paper is structured as follows. In Section 3, we present two OOS extensions of the ASE. In Section 4, we prove convergence of these two OOS extensions when applied to random dot product graphs. In Section 5, we explore the empirical performance of the two extensions presented in Section 3, and we conclude with a brief discussion in Section 6."
  }, {
    "heading": "3. Out-of-sample Embedding for ASE",
    "text": "Given a graph G encoded by adjacency matrix A ∈ {0, 1}n×n, the adjacency spectral embedding (ASE) pro-\nduces a d-dimensional embedding of the vertices ofG, given by the rows of the n-by-d matrix\nX̂ = UAS 1/2 A , (2)\nwhere UA ∈ Rn×d is a matrix with orthonormal columns given by the d eigenvectors corresponding to the top d eigenvalues of A, which we collect in the diagonal matrix SA ∈ Rd×d. We note that in general, one would be better-suited to consider the matrix [ATA]1/2, so that all eigenvalues are guaranteed to be nonnegative, but we will see that in the random dot product graph, the model that is the focus of this paper, the top d eigenvalues of A are positive with high probability (see, for example, either Lemma 1 in Athreya et al. (2016) or Observation 2 in Levin et al. (2017), or refer to the technical report, (Levin et al., 2018)).\nThe random dot product graph (RDPG, Young & Scheinerman, 2007) is an edge-independent random graph model in which the graph structure arises from the geometry of a set of latent positions, i.e., vectors associated to the vertices of the graph. As such, the adjacency spectral embedding is particularly well-suited to this model.\nDefinition 1. (Random Dot Product Graph) Let F be a distribution on Rd such that xT y ∈ [0, 1] whenever x, y ∈ suppF , and let X1, X2, . . . , Xn be drawn i.i.d. from F . Collect these n random points in the rows of a matrix X ∈ Rn×d. Suppose that (symmetric) adjacency matrix A ∈ {0, 1}n×n is distributed in such a way that\nPr[A|X] = ∏\n1≤i<j≤n\n(XTi Xj) Aij (1−XTi Xj)1−Aij . (3)\nWhen this is the case, we write (A,X) ∼ RDPG(F, n). If G is the random graph corresponding to adjacency matrix A, we say that G is a random dot product graph with latent positions X1, X2, . . . , Xn, where Xi is the latent position corresponding to the i-th vertex.\nA number of results exist showing that the adjacency spectral embedding yields consistent estimates of the latent positions in a random dot product graph (Sussman et al., 2012; Tang et al., 2013b) and recovers community structure in the stochastic block model (Lyzinski et al., 2014). We note an inherent nonidentifiability in the random dot product graph, arising from the fact that for any orthogonal matrix W ∈ Rd×d, the latent positions X ∈ Rn×d and XW ∈ Rd×d give rise to the same distribution over graphs, since XXT = (XW )(XW )T = E[A | X]. Owing to this nonidentifiability, we can only hope to recover the latent positions in X up to some orthogonal rotation. The reader may notice that the RDPG as defined has the limitation that it can only capture graphs with positive semi-definite expected value. This limitation can be overcome by extending the RDPG to the generalized RDPG (Rubin-Delanchy\net al., 2017). The results stated in the present work can, for the most part, be extended to this generalized model, but we restrict ourselves here to the RDPG as it appears in Definition 1 for the sake of simplicity.\nSuppose that, given adjacency matrix A, we compute embedding\nX̂ = [X̂1X̂2 . . . X̂n] T ,\nwhere X̂i ∈ Rd denotes the embedding of the i-th vertex. Now suppose we add a vertex v with latent position w̄ ∈ Rd to the original graph G, obtaining an augmented graph G̃ = ([n] ∪ {v}, E ∪ Ev), where Ev denotes the set of edges between v and the vertices of G. One would like to embed vertex v according to the same distribution as the original n vertices and obtain an estimate of w̄. Let the binary vector ~a ∈ {0, 1}n encode the edges Ev incident upon vertex v, with entries ai = (~a)i ∼ Bernoulli(XTi w̄). The augmented graph G̃ then has the adjacency matrix as in (1). As discussed earlier, the natural approach to embedding vertex v is to simply re-embed the whole matrix G̃ by computing the ASE of Ã. Suppose that we wish to avoid such a computation, for example due to resource constraints. The problem then becomes one of embedding the new vertex v based solely on the information present in X̂ and ~a. Two natural approaches to such an OOS extension suggest themselves."
  }, {
    "heading": "3.1. Linear Least Squares OOS Extension",
    "text": "A natural approach to OOS embedding, pursued by, for example, Bengio et al. (2003), is to embed vertex v as the least-squares solution to X̂w = ~a. That is, we embed the vertex v as the vector ŵLS solving\nmin w∈Rd n∑ i=1 ( ai − X̂Ti w )2 , (4)\nwhere ai denotes the i-th component of the binary vector ~a encoding the edges between v and the original n vertices. We will denote the solution to the least-squares optimization in Equation (4) by ŵLS, and term this the linear least squares out-of-sample (LLS OOS) embedding."
  }, {
    "heading": "3.2. Maximum Likelihood OOS Extension",
    "text": "A more principled approach to OOS extension, but perhaps more involved computationally, is to consider the following maximum-likelihood formulation. The entries of the vector ~a are distributed independently as ai ∼ Bernoulli(XTi w̄), where w̄ denotes the true latent position of OOS vertex v. Since we do not have access to the latent positions {Xi}ni=1, we use instead their estimates {X̂i}ni=1. This yields the following objective:\nmax w∈Rd n∑ i=1 ai log X̂ T i w + (1− ai) log ( 1− X̂Ti w ) . (5)\nUnfortunately, this optimization problem may fail to achieve its optimum inside the support of F . Indeed, it may not even have a finite solution. Thus, we will instead settle for solving the following constrained modification of Equation (5),\nmax w∈T̂ n∑ i=1 ai log X̂ T i w + (1− ai) log ( 1− X̂Ti w ) , (6)\nwhere T̂ = {w ∈ Rd : ≤ X̂Ti w ≤ 1 − , i ∈ [n]}, and > 0 is a small constant. We note that this is based only on the edges incident on the OOS vertex rather than on the full data Ã, and uses the spectral estimates {X̂i}ni=1 rather than the true latent positions {Xi}ni=1. Despite both of these facts, we will term the extension given by Equation (6) as the maximum-likelihood out-of-sample (ML OOS) extension, and we will let ŵML denote its solution."
  }, {
    "heading": "4. Main Results",
    "text": "Our main results show that both the linear least-squares and maximum-likelihood OOS extensions in Equations (4) and (6) recover the true latent position w̄ of v. Further, both OOS extensions converge to w̄ at the same asymptotic rate (i.e., up to a constant) as we would have obtained, had we computed the ASE of Ã in (1) directly. This rate is given by Lemma 2.5 from Lyzinski et al. (2014), which we state here in a slightly adapted form. The lemma states, in essence, that the ASE recovers the latent positions with error of order n−1/2 log n, uniformly over the n vertices. We remind the reader that ‖M‖2→∞ denotes the 2-to-∞ operator norm, ‖M‖2→∞ = maxx:‖x‖=1 ‖Mx‖∞. Lemma 1 (Adapted from Lyzinski et al. (2014), Lemma 2.5). Let X = [X1, X2, . . . , Xn]T ∈ Rn×d be the matrix of latent positions of an RDPG, and let X̂ ∈ Rn×d denote the matrix of estimated latent positions yielded by ASE as in (2). Then with probability at least 1− cn−2, there exists orthogonal matrix W ∈ Rd×d such that\n‖X̂ −XW‖2→∞ ≤ c log n\nn1/2 .\nThat is, it holds with high probability that for all i ∈ [n],\n‖X̂i −WTXi‖ ≤ c log n\nn1/2 .\nIn what follows, we let A ∈ {0, 1}n×n denote the random adjacency matrix of an RDPGG, and letX1, X2, . . . , Xn ∈ Rd denote its latent positions, collected in matrix X = [X1, X2, . . . , Xn]\nT ∈ Rn×d. That is, (A,X) ∼ RDPG(F, n). We use X̂ = [X̂1, X̂2, . . . , X̂n]T ∈ Rn×d to denote the matrix whose rows are the estimated latent positions, obtained via ASE as in (2). We let w̄ denote the true latent position of the OOS vertex v.\nTheorem 1. With notation as above, let ŵLS denote the least-squares estimate of w̄, i.e., the solution to (4). Then there exists an orthogonal matrix W ∈ Rd×d such that\n‖WŵLS − w̄‖ ≤ cn−1/2 log n w.h.p.\nProof. The proof of this result relies upon a classic result for solutions of perturbed linear systems to establish that with high probability, ‖WŵLS − wLS‖ ≤ cn−1/2 log n, where W ∈ Rd×d is the orthogonal matrix guaranteed by Lemma 1 andwLS is the LS estimate based on the true latent positions {Xi} rather than on the estimates {X̂i}. A basic Hoeffding inequality to show that with high probability, ‖wLS − w̄‖ ≤ cn−1/2 log n, where again W ∈ Rd×d is the orthogonal matrix in Lemma 1. A triangle inequality applied to ‖WŵLS− w̄‖ combined with a union bound over the two high-probability events just described yields the result. A detailed version of this proof can be found in the technical report (Levin et al., 2018).\nAs mentioned in Section 3, we would like to consider a maximum-likelihood OOS extension based on the likelihood ˆ̀(w) = ∑n i=1 ai log X̂ T i w+ (1−ai) log(1− X̂Ti w). Toward this end, we would ideally like to use the solution to the optimization problem\narg max w∈Rd\nˆ̀(w),\nbut to ensure a sensible solution, we instead consider\nŵML = arg max w∈T̂\nˆ̀(w), (7)\nwhere we remind the reader that T̂ = {w ∈ Rd : ≤ X̂Ti w ≤ 1 − , i = 1, 2, . . . , n}. Theorem 2 shows that ŵML recovers the true latent position of the OOS vertex, up to rotation, with error decaying at the same rate as that obtained in Theorem 1 for the LS OOS extension.\nTheorem 2. With notation as above, let ŵML be the estimate defined in Equation (7), and let > 0 be such that x, y ∈ suppF implies < xT y < 1 − . Denote the true latent position of the OOS vertex v by w̄ ∈ suppF . Then for all n suitably large, there exists an orthogonal matrix W ∈ Rd×d such that with high probability,\n‖WŵML − w̄‖ ≤ cn−1/2 log n w.h.p.,\nand this matrix W is the same one guaranteed by Lemma 1.\nProof. By a standard argument from convex optimization, alongside the definition of T̂ , one can thow that for suitably large n,\n‖WŵML − w̄‖ ≤ c‖∇ˆ̀(WT w̄)‖\nn w.h.p.\nBy the triangle inequality one can then show that\n‖∇ˆ̀(WT w̄)‖ ≤ c √ n log n w.h.p.\nA detailed proof can be found in (Levin et al., 2018).\nRemark 1. Given our in-sample embedding X̂ and the vector of edge indicators ~a, we can think of the OOS extension as an estimate of w̄, the latent position of the OOS vertex v. Lemma 1 implies that if we took the naı̈ve approach of applying ASE to the adjacency matrix Ã in (1), our estimate would have error of order at most O(n−1/2 log n). Theorems 1 and 2 imply that the OOS estimate obtains the same asymptotic estimation error, without recomputing the embedding of Ã.\nIn addition to the bounds in Theorems 1 and 2, we can show that the least-squares OOS extension satisfies a stronger property, namely the following central limit theorem.\nTheorem 3. Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG. Let w̄ ∈ suppF and ŵLS ∈ Rd be, respectively, the latent position and the least-squares embedding from (4) of an OOS vertex v. There exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that\n√ n(V Tn ŵLS − w̄) L−→ N (0,Σw̄),\nwhere Σw̄ ∈ Rd×d is given by\nΣw̄ = ∆ −1E [ XT1 w̄(1−XT1 w̄)X1XT1 ] ∆−1, (8)\nand ∆ = EX1XT1 .\nProof. This theorem follows from an adaptation of Theorem 1 in (Levin et al., 2017) A detailed proof can be found in (Levin et al., 2018).\nIf the OOS vertex is distributed according to F , we have the following corollary by integrating w̄ with respect to F .\nCorollary 1. Let (A,X) ∼ RDPG(F, n) be a ddimensional RDPG, and let w̄ be distributed according to F , independent of (A,X). Then there exists a sequence of orthogonal d× d matrices {Vn}∞n=1 such that\n√ n(V Tn ŵLS − w̄) L−→ ∫ N (0,Σw)dF (w),\nwhere Σw is defined as in Equation (8) above.\nWe conjecture that a CLT analogous to Theorem 3 holds for the ML OOS extension."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we briefly explore our results through simulations. We leave a more thorough experimental examination of our results, particularly as they apply to realworld data, for future work. We first give a brief exploration of how quickly the asymptotic distribution in Theorem 3 becomes a good approximation. Toward this end, let us consider a simple mixture of point masses, F = Fλ,x1,x2 = λδx1 +(1−λ)δx2 , where x1, x2 ∈ R2 and λ ∈ (0, 1). This corresponds to a two-block stochastic block model (Holland et al., 1983), in which the block probability matrix is given by [\nxT1 x1 x T 1 x2 xT1 x2 x T 2 x2\n] .\nCorollary 1 implies that if all latent positions (including the OOS vertex) are drawn according to F , then the OOS estimate should be distributed as a mixture of normals centered at x1 and x2, with respective mixing coefficients λ and 1− λ.\nTo assess how well the asymptotic distribution predicted by Theorem 3 and Corollary 1 holds, we generate RDPGs with latent positions drawn i.i.d. from distribution F = Fλ,x1,x2 defined above, with\nλ = 0.4, x1 = (0.2, 0.7) T , and x2 = (0.65, 0.3)T .\nFor each trial, we draw n+ 1 independent latent positions from F , and generate a binary adjacency matrix from these latent positions. We let the (n+1)-th vertex be the OOS vertex. Retaining the subgraph induced by the first n vertices, we obtain an estimate X̂ ∈ Rn×2 via ASE, from which we obtain an estimate for the OOS vertex via the LS OOS extension as defined in (4). We remind the reader that for each RDPG draw, we initially recover the latent positions\nonly up to a rotation. Thus, for each trial, we compute a Procrustes alignment (Gower & Dijksterhuis, 2004) of the in-sample estimates X̂ to their true latent positions. This yields a rotation matrix R, which we apply to the OOS estimate. Thus, the OOS estimates are sensibly comparable across trials. Figure 1 shows the empirical distribution of the OOS embeddings of 100 independent RDPG draws, for n = 50 (left), n = 100 (center) and n = 500 (right) in-sample vertices. Each cross is the location of the OOS estimate for a single draw from the RDPG with latent position distribution F , colored according to true latent position. OOS estimates with true latent position x1 are plotted as blue crosses, while OOS estimates with true latent position x2 are plotted as red crosses. The true latent positions x1 and x2 are plotted as solid circles, colored accordingly. The plot includes contours for the two normals centered at x1 and x2 predicted by Theorem 3 and Corollary 1, with the ellipses indicating the isoclines corresponding to one and two (generalized) standard deviations.\nExamining Figure 1, we see that even with only 100 vertices, the mixture of normal distributions predicted by Theorem 3 holds quite well, with the exception of a few gross outliers from the blue cluster. With n = 500 vertices, the approximation is particularly good. Indeed, the n = 500 case appears to be slightly under-dispersed, possibly due to the Procrustes alignment. It is natural to wonder whether a similarly good fit is exhibited by the ML-based OOS extension. We conjectured at the end of Section 4 that a CLT similar to that in Theorem 3 would also hold for the ML-based OOS extension as defined in Equation (7). Figure 2 shows the empirical distribution of 100 independent OOS estimates, under the same experimental setup as Figure 1, but using the ML OOS extension rather than the linear least-squares extension. The plot supports our conjecture that the ML-based OOS estimates are also approximately normally distributed\nabout the true latent positions.\nFigure 1 suggests that we may be confident in applying the large-sample approximation suggested by Theorem 3 and Corollary 1. Applying this approximation allows us to investigate the trade-offs between computational cost and classification accuracy, to which we now turn our attention. The mixture distribution Fλ,x1,x2 above suggests a task in which, given an adjacency matrix A, we wish to classify the vertices according to which of two clusters or communities they belong. That is, we will view two vertices as belonging to the same community if their latent positions are the same (Holland et al., 1983, i.e., the latent positions specify an SBM,). More generally, one may view the task of recovering vertex block memberships in a stochastic block model as a clustering problem. Lyzinski et al. (2014) showed that applying ASE to such a graph, followed by k-means clustering of the estimated latent positions, correctly recovers community memberships of all the vertices (i.e., correctly assigns all vertices to their true latent positions) with high probability.\nFor concreteness, let us consider a still simpler mixture model, F = Fλ,p,q = λδp+ (1−λ)δq , where 0 < p < q < 1, and draw an RDPG (Ã,X) ∼ RDPG(F, n+m), taking the first n vertices to be in-sample, with induced adjacency matrix A ∈ Rn×n. That is, we draw the full matrix\nÃ = [ A B BT C ] ,\nwhere C ∈ Rm×m is the adjacency matrix of the subgraph induced by them OOS vertices andB ∈ Rn×m encodes the edges between the in-sample vertices and the OOS vertices. The latent positions p and q encode a community structure in the graph Ã, and, as alluded to above, a common task in network statistics is to recover this community structure.\nLet w̄(1), w̄(2), . . . , w̄(m) ∈ {p, q} denote the true latent positions of the m OOS vertices, with respective least-squares OOS estimates ŵ(1)LS , ŵ (2) LS , . . . , ŵ (m) LS , each obtained from the in-sample ASE X̂ ∈ Rn of A. We note that one could devise a different OOS embedding procedure that makes use of the subgraph C induced by these m OOS vertices, but we leave the development of such a method to future work. Corollary 1 implies that each ŵ(t)LS for t ∈ [m] is marginally (approximately) distributed as\nŵ (t) LS ∼ λN (p, (n+ 1) −1σ2p) + (1−λ)N (q, (n+ 1)−1σ2q ),\nwhere σ2p = ∆ −2 (λp2(1− p2)p2 + (1− λ)pq(1− pq)q2) ,\nσ2q = ∆ −2 (λpq(1− pq)p2 + (1− λ)q2(1− q2)q2) ,\nand ∆ = λp2 + (1− λ)q2.\nClassifying the t-th OOS vertex based on ŵ(t)LS via likelihood ratio thus has (approximate) probability of error\nηn,p,q = λ(1− Φ (√\nn+ 1(xn+1,p,q − p) σp ) + (1− λ)Φ (√ n+ 1(xn+1,p,q − q)\nσq\n) ,\nwhere Φ denotes the cdf of the standard normal and xn,p,q is the value of x solving\nλσ−1p exp{n(x− p)2/(2σ2p)} = (1− λ)σ−1q exp{n(x− q)2/(2σ2q )},\nand hence our overall error rate when classifying the m OOS vertices will grow as mηn+1,p,q .\nAs discussed previously, the OOS extension allows us to avoid the expense of computing the ASE of the full matrix\nÃ = [ A B BT C ] .\nThe LLS OOS extension is computationally inexpensive, requiring only the computation of the matrix-vector product S−1/2A U T A~a, with a time complexity O(d\n2n) (assuming one does not precompute the product S−1/2A U T A ). The eigenvalue computation required for embedding Ã is far more expensive than the LLS OOS extension. Nonetheless, if one were intent on reducing the OOS classification error ηn+1,p,q, one might consider paying the computational expense of embedding Ã to obtain estimates w̃(1), w̃(2), . . . , w̃(m) of the m OOS vertices. That is, we obtain estimates for the m OOS vertices by making them insample vertices, at the expense of solving an eigenproblem on the (m + n)-by-(m + n) adjacency matrix. Of course, the entire motivation of our approach is that the in-sample matrix A may not be available. Nonetheless, a comparison against this baseline, in which all data is used to compute our embeddings, is instructive.\nTheorem 1 in Athreya et al. (2016) implies that the w̃(t) estimates based on embedding the full matrix Ã are (approximately) marginally distributed as\nw̃(t) ∼ λN (p, (n+m)−1σ2p)+(1−λ)N (q, (n+m)−1σ2q ),\nwith classification error\nηn+m,p,q = λΦ\n( p− xn+m,p,q\nσp ) + (1− λ)Φ ( xn+m,p,q − q\nσq\n) ,\nwhere xn+m,p,q is the value of x solving\nλσ−1p exp{(m+ n)(x− p)2/(2σ2p)} = (1− λ)σ−1q exp{(m+ n)(x− q)2/(2σ2q )},\nand it can be checked that ηn+m,q,p < ηn,q,p when m > 1. Thus, at the cost of computing the ASE of Ã, we may obtain a better estimate. How much does this additional computation improve classification the OOS vertices? Figure 3 explores this question.\nFigure 3 compares the error rates of the in-sample and OOS estimates as a function of m and n in the model just described, with λ = 0.4, p = 0.6 and q = 0.61. The plot depicts the ratio of the (approximate) in-sample classification error η(n+m),p,q to the (approximate) OOS classification error η(n+1),p,q , as a function of the number of OOS vertices m, for differently-sized in-sample graphs, n = 100, 1000, and 10000. We see that over several magnitudes of graph\nsize, the in-sample embedding does not improve appreciably over the OOS embedding except when multiple hundreds of OOS vertices are available. When hundreds or thousands of OOS vertices are available simultaneously, we see in the right-hand side of Figure 3 that the in-sample embedding classification error may improve upon the OOS classification error by a large multiplicative factor. Whether or not this improvement is worth the additional computational expense will, depend upon the available resources and desired accuracy, but this suggests that the additional expense associated with performing a second ASE computation is only worthwhile in the event that hundreds or thousands of OOS vertices are available simultaneously. This surfeit of OOS vertices is rather divorced from the typical setting of OOS extension problems, where one typically wishes to embed at most a few previously unseen observations."
  }, {
    "heading": "6. Discussion and Conclusion",
    "text": "We have presented a theoretical investigation of two OOS extensions of the ASE, one based on a linear least squares estimate and the other based on a plug-in maximum-likelihood estimate. We have also proven a central limit theorem for the LLS-based extension, and simulation suggests that this CLT is a good approximation even with just a few hundred vertices. We conjecture that a similar CLT holds for the MLbased OOS extension, a conjecture supported by similar simulation data. Finally, we have given a brief illustration of how this OOS extension and the approximation it introduces might be weighed against the computational expense of recomputing a full graph embedding by examining how vertex classification error depends on the size of the set of OOS vertices. We leave a more thorough exploration of this trade-off for future work."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors would like to thank the reviewers for their helpful comments, which markedly improved the quality of this paper. Keith Levin was partially supported by NSF grant DMS-1646108. Farbod Roosta-Khorasani was partially supported by the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923). Carey E. Priebe was supported by the DARPA D3M program through contract FA8750-17-2-0112. Farbod RoostaKhorasani and Michael Mahoney also gratefully acknowledge support from DARPA D3M."
  }],
  "year": 2018,
  "references": [{
    "title": "A limit theorem for scaled eigenvectors of random dot product graphs",
    "authors": ["A. Athreya", "V. Lyzinski", "D.J. Marchette", "C.E. Priebe", "D.L. Sussman", "M. Tang"],
    "venue": "Sankhya A,",
    "year": 2016
  }, {
    "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
    "authors": ["M. Belkin", "P. Niyogi"],
    "venue": "Neural Computation,",
    "year": 2003
  }, {
    "title": "Manifold Regularization: A Geometric Framework for Learning from Examples",
    "authors": ["M. Belkin", "P. Niyogi", "V. Sindhwani"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2006
  }, {
    "title": "Out-of-sample extensions for LLE, ISOMAP, MDS, eigenmaps, and spectral clustering",
    "authors": ["Y. Bengio", "J. Paiement", "P. Vincent", "O. Delalleau", "N.L. Roux", "M. Ouimet"],
    "venue": "In NIPS,",
    "year": 2003
  }, {
    "title": "Modern multidimensional scaling: Theory and applications",
    "authors": ["I. Borg", "P.J.F. Groenen"],
    "venue": "Springer Science & Business Media,",
    "year": 2005
  }, {
    "title": "Spectral Graph Theory. Number 92 in Conference Board of the Mathematical Sciences Regional Conference Series in Mathematics",
    "authors": ["F. Chung"],
    "venue": "American Mathematical Society,",
    "year": 1997
  }, {
    "title": "Procrustes Problems. Number 30 in Oxford Statistical Science Series",
    "authors": ["J.C. Gower", "G.B. Dijksterhuis"],
    "year": 2004
  }, {
    "title": "From graphs to manifolds – weak and strong pointwise consistency of graph Laplacians",
    "authors": ["M. Hein", "Audibert", "J.-Y", "U. von Luxburg"],
    "venue": "In Proceedings of the 18th Annual Conference on Learning Theory, pp. 470–485,",
    "year": 2005
  }, {
    "title": "Latent space approaches to social network analysis",
    "authors": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2002
  }, {
    "title": "Matrix Analysis",
    "authors": ["R.A. Horn", "C.R. Johnson"],
    "year": 2013
  }, {
    "title": "Scalable out-of-sample extension of graph embeddings using deep neural networks",
    "authors": ["A. Jansen", "G. Sell", "V. Lyzinski"],
    "venue": "Pattern Recognition Letters,",
    "year": 2017
  }, {
    "title": "Think locally, act locally: The detection of small, medium-sized, and large communities in large networks",
    "authors": ["L.G.S. Jeub", "P. Balachandran", "M.A. Porter", "P.J. Mucha", "M.W. Mahoney"],
    "venue": "Physical Review E,",
    "year": 2015
  }, {
    "title": "Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters",
    "authors": ["J. Leskovec", "K.J. Lang", "A. Dasgupta", "M.W. Mahoney"],
    "venue": "Internet Mathematics,",
    "year": 2009
  }, {
    "title": "Segmental acoustic indexing for zero resource keyword search",
    "authors": ["K. Levin", "A. Jansen", "B. Van Durme"],
    "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2015
  }, {
    "title": "A central limit theorem for an omnibus embedding of random dot product graphs",
    "authors": ["K. Levin", "A. Athreya", "M. Tang", "V. Lyzinski", "C.E. Priebe"],
    "venue": "Retrieved from arXiv,",
    "year": 2017
  }, {
    "title": "Out-of-sample extension of graph adjacency spectral embedding",
    "authors": ["K. Levin", "F. Roosta-Khorasani", "M.W. Mahoney", "C.E. Priebe"],
    "venue": "Retrieved from arXiv,",
    "year": 2018
  }, {
    "title": "A tutorial on spectral clustering",
    "authors": ["U.V. Luxburg"],
    "venue": "Statistics and computing,",
    "year": 2007
  }, {
    "title": "Perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding",
    "authors": ["V. Lyzinski", "D.L. Sussman", "M. Tang", "A. Athreya", "C.E. Priebe"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2014
  }, {
    "title": "On spectral clustering: Analysis and an algorithm",
    "authors": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2002
  }, {
    "title": "Extreme learning machine for out-of-sample extension in laplacian eigenmaps",
    "authors": ["A.M. Quispe", "C. Petitjean", "L. Heutte"],
    "venue": "Pattern Recognition Letters,",
    "year": 2016
  }, {
    "title": "The generalised random dot product graph. Retrieved from arXiv, 2017",
    "authors": ["P. Rubin-Delanchy", "C.E. Priebe", "M. Tang"],
    "venue": "URL https://arxiv.org/abs/",
    "year": 2017
  }, {
    "title": "A consistent adjacency spectral embedding for stochastic blockmodel graphs",
    "authors": ["D.L. Sussman", "M. Tang", "D.E. Fishkind", "C.E. Priebe"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2012
  }, {
    "title": "Out-of-sample extension of latent position graphs. Retrieved from arXiv, 2013a. URL https://arxiv.org/abs/ 1305.4893",
    "authors": ["M. Tang", "Y. Park", "C.E. Priebe"],
    "year": 2013
  }, {
    "title": "Universally consistent vertex classification for latent position graphs",
    "authors": ["M. Tang", "D.L. Sussman", "C.E. Priebe"],
    "venue": "The Annals of Statistics,",
    "year": 2013
  }, {
    "title": "A global geometric framework for nonlinear dimensionality reduction",
    "authors": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"],
    "year": 2000
  }, {
    "title": "The out-of-sample problem for classical multidimensional scaling",
    "authors": ["M.W. Trosset", "C.E. Priebe"],
    "venue": "Computational statistics and data analysis,",
    "year": 2008
  }, {
    "title": "Dimensionality reduction: A comparative review",
    "authors": ["L.J.P. van der Maaten", "E.O. Postma", "H.J. van den Herik"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Segmentation using eigenvectors: a unifying view",
    "authors": ["Y. Weiss"],
    "venue": "In Proc. IEEE International Conference on Computer Vision, pp",
    "year": 1999
  }, {
    "title": "Random dot product graph models for social networks",
    "authors": ["S. Young", "E. Scheinerman"],
    "venue": "In Proceedings of the 5th International Conference on Algorithms and Models for the Web-graph,",
    "year": 2007
  }],
  "id": "SP:c7a8e22ce5cab453894e3b87b9c1be0f7c4e88ce",
  "authors": [{
    "name": "Keith Levin",
    "affiliations": []
  }, {
    "name": "Farbod Roosta-Khorasani",
    "affiliations": []
  }, {
    "name": "Michael W. Mahoney",
    "affiliations": []
  }, {
    "name": "Carey E. Priebe",
    "affiliations": []
  }],
  "abstractText": "Many popular dimensionality reduction procedures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to observations not seen in the initial training sample. In this work, we consider the problem of obtaining an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding the vertices of a graph into Euclidean space. We present two different approaches to this problem, one based on a least-squares objective and the other based on a maximum-likelihood formulation. We show that if the graph of interest is drawn according to a certain latent position model called a random dot product graph, then both of these out-of-sample extensions estimate the true latent position of the out-of-sample vertex with the same error rate. Further, we prove a central limit theorem for the least-squares-based extension, showing that the estimate is asymptotically normal about the truth in the large-graph limit.",
  "title": "Out-of-sample extension of graph adjacency spectral embedding"
}