{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The technological feasibility of ubiquitous Text To Speech (TTS), in which talking avatars of everyone we know would interact with us as a form of asynchronous communication, depends on the ability to sample individual speakers in a casual way. In the past year, Neural TTS systems have shifted from high quality single speaker systems to multi-speaker systems (see Sec. 2). However, most of these systems rely on the availability of training samples of all speakers during training.\nOne exception is the VoiceLoop system (Taigman et al., 2018), which was shown to be able to fit a new speaker from relatively few samples. This ability, however, is limited by three factors: (i) The obtained quality of a training-naive voice is lower than the quality obtained for speakers that participated in the training process. (ii) The ability of the system to capture the identity of a new speaker, heavily depends on the amount of data that is available for the fitting process. (iii) The fitting requires both the voice sample and the transcript.\n1Facebook AI Research 2Tel Aviv University. Correspondence to: Eliya Nachmani <eliyan@fb.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nIn this work, we propose a TTS network that is designed to fit a new voice based on a limited amount of data and without the transcript of the new speaker. While the current multi-speaker TTS methods rely on a speaker embedding that is stored in one or more Look Up Tables (LUTs), our method incorporates a fitting network Ns, which is trained jointly with the other networks of the TTS system.\nDuring training, instead of retrieving the speaker embedding for the speaker of the training sample from the LUT, we apply the network Ns to the sample’s target audio. The embedding thus obtained is used in the process of generating the same audio. Multiple losses are added in order to ensure that the embedding of the reconstructed audio is similar to the embedding of the target sample and that the embeddings are well-separated.\nWhile our approach is general and can be applied to any neural TTS system, we focus our experiments on the VoiceLoop system, since it is the only multispeaker system for which an official implementation was released. The official implementation of Char2Wav (Sotelo et al., 2017) is multispeaker, but the method was published as a single speaker method and is not competitive with other multispeaker systems. DeepVoice3 has a community implementation (Park, 2018). However, at this time, the generated voice contains noticeable artifacts.\nIn addition to proposing the new method, we also investigate the underlying principles of voice fitting by constructing a single network that does not model individual speakers, but which is made to mimic multiple speakers by performing priming. These experiments demonstrate that mimicking a new speaker does not require an optimization process and can be based on even a short sample."
  }, {
    "heading": "2. Previous Work",
    "text": "A common view of the TTS literature divides the existing methods into four top-level groups: rule-based methods, concatenative systems, statistical-parametric, which includes many successful HMM based methods, and the emerging neural models.\nWhile there have been attempts to create concatenative systems that rely on relatively little data (Jin et al., 2017), or on automatic filtering of the training set used for an HMM\nsystem (Baljekar & Black), concatenative and statisticalparametric systems still require clean and well transcribed samples. These samples are of a minimal length of tens of minutes and sometimes require the reading of specific sentences in order to cover the entire space of combinations of consecutive phonemes.\nThe situation is not yet materially different for neural speech systems. A critical look at the current neural methods reveals that the most striking voice qualities are obtained on single speaker models, trained on hours of carefully transcribed samples and that the literature on multispeaker systems does not focus on de-novo speakers.\nThe recent neural TTS systems include the Deep Voice systems DV1 (Arik et al., 2017b), DV2 (Arik et al., 2017a) and DV3 (Ping et al., 2018), WaveNet (Oord et al., 2016) and Parallel WaveNet (van den Oord et al., 2017), Char2Wav (Sotelo et al., 2017), Tacotron (Wang et al., 2017) and Tacotron2 (Shen et al., 2017), and VoiceLoop (Taigman et al., 2018). One can sort these systems according to several axes, which are of relevance to our work: the input of the system, the output of the system, the underlying architecture, the ability to work with multiple speakers, the nature of the embedding of multiple speakers, and the ability to fit new speakers.\nThe literature contains three types of features, with some systems tested on multiple types: (i) raw letters (Tacotron, Tacotron2, DV3), (ii) phonemes (Char2Wav, VoiceLoop, DV3), and (iii) linguistic features, including duration and pitch of phonemes (WaveNet, Parallel WaveNet, DV1, DV2). The latter requires a dedicated system to extract these features.\nWith regards to output, there are also a few options in the literature: Tacotron creates spectrograms, which are inverted using the Griffin-Lim method. Char2Wav and Voice Loop produce World Vocoder Features (Morise et al., 2016). The WaveNet and Parallel WaveNet systems create raw audio. DV1, DV2, DV3 and Tacotron2 employ WaveNet as neural vocoder to transform compact representations, such as spectrograms or vocoder features to raw audio.\nThere is a variability in the literature also for the underlying method. Wavenet employs dilated convolutions. The Tacotron method employs multiple RNNs, convolutions and a highway network (Srivastava et al., 2015). Recently, Tacotron2 simplified the latter by replacing highway networks with RNNs and predicting a residual to improve the system output. DV1 and DV2 employ bidirectional RNNs, multilayer fully connected networks and residual connections. DV3 simplified the previous systems by using a convolutional sequence to sequence architecture (Gehring et al., 2017) and incorporating the key-value attention mechanism of (Vaswani et al., 2017). Finally, the VoiceLoop method\nis based on a shifting buffer, which is updated in a FIFO manner.\nIn all three categories above, we follow VoiceLoop: our method relies on phonemes, produces world vocoder features, and shares large parts of our architecture with it, including the shifting buffer-based RNN. The reliance on World Vocoder is convenient, especially considering the landscape of open-source WaveNet implementations with regards to quality and efficiency. However, it upper-bounds the obtained quality.\nOnly three published neural systems are multi-speaker: DV2, DV3, and VoiceLoop1. All three systems were tested on the VCTK dataset, which contains over a hundred speakers. DV2 was also applied to an internal dataset of audiobooks with 447 speakers and DV3 to LibriSpeech (Panayotov et al., 2015) with 2484 speakers. The VoiceLoop system was demonstrated on a dataset of four “in-the-wild” speakers collected from YouTube videos of public speeches. Our method is evaluated, among other datasets, on the VoxCeleb dataset (Nagrani et al., 2017), which is of lower quality and more uncontrolled and heterogeneous than any other existing corpus.\nAs mentioned, out of these systems, the only one that was demonstrated to fit new speakers, which were not encountered during training, is the VoiceLoop system. Unlike our system, this is done by an optimization process in which the embedding of a new speaker is searched by repeating the training process for her samples, while fixing all weights, except for the embedding of that speaker. Therefore, a lengthy backpropagation fitting phase is needed in order to obtain text to speech in a new voice. We show that this process is both unnecessary and leads to suboptimal performance."
  }, {
    "heading": "2.1. The VoiceLoop Architecture",
    "text": "At the heart of VoiceLoop is a buffer St, which serves as a differentiable memory and is used by all of the model’s networks. At every time step, a new representation vector ut is inserted into the buffer, and the first inserted vector is discarded.\nThe VoiceLoop model is composed out of three shallow fully connected networks (Nu, Na, No), two LUTs (LUTp, LUTs), and two projection matrices (Fu and Fo). Nu creates the new vector ut, Na updates the attention mechanism, andNo generates the next audio frame, ot, which is encoded as vocoder features. The input to the VoiceLoop system is a sequence of phonemes s1, s2, . . . , sl, which are converted to a sequence of embedding vectors, given by LUTp and stored as the columns of a matrix E. LUTs and the projection matrices allow the multi-speaker behavior, and each\n1Wavenet was shown to produce mumbling of multiple speakers, but was not demonstrated as a multi-speaker TTS system\nspeaker is represented by an embedding vector z, which is stored in this LUT.\nEach forward pass runs three sequential steps. In the first step, the context vector is computed. The Graves monotonic attention mechanism (Graves, 2013) is used: the attention networkNa receives the current buffer and outputs the priors of the Gaussian Mixture Model, shifts of the means of the Gaussians, and their log-variances. The current context vector ct is then computed as a weighted sum of the columns of the input sequence embedding matrix E.\nIn the next step, a representation vector ut is added to the buffer at the first location St[1], shifting all other buffer locations to the right: St[i+1] = St−1[i] for i = 1, . . . , k− 1. ut is computed by Nu using the buffer St−1, the sum of the context vector ct and the projection of the speakers embedding Fuz, and the previous output ot−1.\nThis output vector ot is generated by the third and last step by the network No, whose inputs are the buffer St and the projection of the user by Fo.\nTraining is done by minimizing the MSE of the output vocoder features. Since this procedure assumes that the generated output is perfectly aligned with the target, teacherforcing is used. In other words, during training, the network receives the correct output of the previous frame in lieu of the predicted target ot−1.\nIn this work, we remove the speaker-embedding LUT (LUTs) and employ an additional network that transforms an input audio clip to the representation z. By doing so, we are able to have the task of fitting as part of the method and not as an afterthought. This enables efficient fitting from a short sample and without the transcribed text."
  }, {
    "heading": "3. The Voice Constancy Phenomenon",
    "text": "The neural multispeaker systems in the literature (Arik et al., 2017a; Ping et al., 2018; Taigman et al., 2018) are based on embedding the speaker’s voice in some vector space. This form of embedding opens the way to fitting a new speaker, using backpropagation, as was shown in (Taigman et al., 2018).\nIn this section, we aim to show that speaker embedding is not the only possible way to construct a multi-speaker system and that the fitting process does not require an optimization step. While the technique we present here is detached from fitting method that is the focus of our paper, it provides important insights on the behavior of multi-speaker voice generating systems as well as on the length of data required in order to capture the voice of new speakers. Namely, it shows that a single network can be trained for all speakers in the training set, without conditioning on the speaker, and that a new speaker can be captured from a very short sample.\nThis is demonstrated by training a single network, in which all identity related elements are removed. i.e., We remove the speaker embedding LUTs and the two projection matrices Fu and Fo. While training, we make no use of the identity information in any way, creating a network that is agnostic to the speaker.\nIn order to generate a specific voice, we make use of the property that we call voice-constancy. Namely, that a network that generates a sample in a certain voice would continue to employ that voice in subsequent frames. This property is an outcome of the training process, in which the input regarding the speaker is not given to the network and is only evident as part of the samples of the previous time steps, provided during training, due to the teacher-forcing procedure.\nIn order to speak in a certain voice we, therefore, use priming (Graves et al., 2014). We play a short sample, typically of 300 frames (1500 milliseconds), using a teacher forcing procedure. We then let the generation process continue without resetting the buffer. In other words, we continue the voice generation using the primed buffer and the new input text. The results obtained using the priming-based approach are presented in Sec. 5."
  }, {
    "heading": "4. The Fitting Sub-Network",
    "text": "The priming-based method presented in the previous section is simple to implement and moderately effective. However, we were not able to make it work better than the baseline VoiceLoop system. In addition, it requires, during fitting, the textual transcription as well as the audio sample. While an automatic speech recognition system can be used to extract this transcript, it is still limiting for several reasons: (i) speech recognition (i.e., speech to text) systems do not exist for most of the world’s languages. (ii) the accuracy of these systems is limited, especially in uncontrolled settings. (iii) for the existing speech recognition systems, aligning the produced transcript to the audio, as well as extracting the phonemes requires addition preprocessing steps.\nWe modify the VoiceLoop system, described in Sec. 2.1, by incorporating a fitting network Ns, which given an audio sample y = y1, y2, . . . , ym produces an embedding vector z. This embedding vector is then used in the VoiceLoop networks as the speaker’s embedding.\nNs receives as input a tensor of size 1 × m × do, which is length of the audio times the size of the vocoder feature vector, set to 63 for the World Vocoder features (Morise et al., 2016). The network has five convolutional layers of 3 × 3 filters, each with 32 channels. Batch normalization is performed after each convolutional layer, followed by a ReLU activation function. Following the convolutional layers, average pooling over time is performed, followed\nby two fully-connected layers, of size 256 each, with ReLU activations. Finally, an affine projection followed by an L2 normalization is performed in order to obtain the embedding vector z."
  }, {
    "heading": "4.1. The Loss Term",
    "text": "The VoiceLoop system is trained using the MSE loss, given an audio sample y:\nLMSE = 1\ndo ∑ y l∑ t=1 ‖yt − ot‖2 (1)\nwhere both the ground truth yt and the network’s output ot are vectors in Rdo . Since teacher forcing is used, the two sequences are of the same length. Note that in our method, ot is both a function of y, via Ns and of a sequence of phonemes s, as depicted in Fig. 1.\nWe add two additional losses. Given three voice samples: y1,y2,y3 such that y1,y2 are from the same speaker, and y3 is not, we would like the computed embedding of the first two samples to be similar to each other, while different from that of the third. A contrastive loss term with a margin ∆ is used:\nLcontrast = 1\n2 ∑ y1,y2,y3 (‖Ns(y1)−Ns(y2)‖2\n+ max(0,∆− ‖Ns(y2)−Ns(y3)‖)2), (2)\nwhere the sampling of a triplet is done by organizing each batch, such that it contains a sequence of pair of samples from the same speaker, and joining the first sample of the next pair to each pair. In all our experiments, we use a margin of ∆ = 1.\nIn addition, in order to require speaker constancy between the input audio clip and the generated audio clip, we add a third loss term. Given an input audio y, we compute the embedding using Ns, run VoiceLoop with this embedding, obtaining an output audio o = o1, o2, . . . , ol, to which we apply Ns again. The loss term is then defined as:\nLcycle = ∑ y ‖Ns(y)−Ns(o)‖2 (3)\nThe overall loss is a weighted combination of the three losses:\nL = LMSE + αLcontrast + βLcycle, (4)\nwhere in all of our experiments, we set α = β = 10."
  }, {
    "heading": "4.2. Training Details",
    "text": "Similar to Taigman et al. (2018), we train our networks in two phases. The first phase employs data with larger\namounts of added i.i.d white noise, while the subsequent phase is trained on longer sequences to which less noise was applied. Specifically, during the first phase, a noise SD equal to 4.0 is added to the sequences of ground truth vocoder features (y) and these sequence are cropped to a length of 100. A batch size equal to 256 is used for exactly 90 epochs. Phase 2 of the training process employs noise SD of 2.0, and sequence lengths that are trimmed at 1000 vocoder features. The batch size is reduced to 30, in order to fit longer sequences in memory. This phase is run until convergence.\nThe various parameters follow the implementation released by Taigman et al. (2018). A few modifications are done to the attention mechanism. First, a tanh activation function is applied to the output of the network Na. Second, we found that a slight improvement is obtained if, during inference only, the mixture component with the maximal prior is selected, instead of employing the weighted sum to obtain the next shift in the attention position.\nWe also added a balanced mini-batch mechanism to speed up the training iterations. This is done by splitting the training dataset into four equally sized partitions based on the audio sample length and sampling each mini-batch from one of the splits. As a result, each sample in a specific mini-batch requires less padding, which in-turn reduces the total amount of computation required from the model."
  }, {
    "heading": "5. Experiments",
    "text": "We focus on multispeaker TTS and especially on fitting. Since VoiceLoop is the only open implementation of a multispeaker contribution, and since it is the only contribution to demonstrate fitting to new speakers, we employ it as our baseline. We also evaluate the priming based method of Sec. 3 and also perform an ablation analysis to demonstrate the contribution of the various components of our loss.\nEvaluation Metrics A comparison between generative methods in a non-deterministic setting is always a challenge. However, following the literature, we employ a sufficient number of tools in order to demonstrate the gap in performance. In order to evaluate the quality of the generated audio, we employ both the Mean Opinion Scores (MOS) and the Mel Cepstral Distortion (MCD) scores. To evaluate the identifiability of the generated voice, we employ either multiclass classification on a network trained on the VCTK identities, or the ROC statistics obtained for the same/notsame task using the embedding layer of the same speaker identification network.\nThe MOS measure is obtained using the crowdMOS toolkit by (P. Ribeiro et al., 2011) and Amazon Mechanical Turk (AMT). The samples were presented at a fixed framerate of\n16kHz and at least 20 raters participated in each such experiment, with 95% confidence intervals. All AMT experiments were restricted to North American raters.\nMCD is an automatic method of testing compatibility between the spectra of two audio sequences, which is limited to specific aspects of the quality. Since the sequences generated are not aligned, the MCD DTW is used, in which Dynamic Time Warping aligns the sequences prior to comparison.\nBeyond quality, the generated voices need to comply with the target voice. Following (Arik et al., 2017a; Taigman et al., 2018), this is evaluated using a speaker recognition network. We train offline a network with the same architecture of Ns for this purpose on the ground-truth training set of the VCTK speakers. For VCTK experiments, the network is tested on the generated sequences and the obtained accuracy is reported.\nFor datasets other than VCTK, we consider pairs of samples and compute distances using this identification network. Specifically, for each pair, we compute the cosine distance between the activations of the last layer prior to the classifi-\ncation, which is of dimensionality 256. As “same” pairs, we collect one previously unused real-voice sample of speaker A and one generated sample, using a voice that was fitted on another sample of speaker A. For the “not-same” pair, the process is identical, except that the second sample is collected from speaker B. We then compute the ROC curve, using all same pairs and a large sample of not-same pairs, and report the Area Under Curve (AUC).\nDatasets The VCTK dataset (Veaux et al., 2017) contains 109 speakers. In order to make our experiments compatible with the results reported by (Taigman et al., 2018), we employ their subset of 85 speakers for training our network and another subset of 16 speakers for the fitting experiments. The remaining eight speakers, which were left out for validation, are not used in our experiments. Among the samples of each speaker, we use the existing splits of train and test.\nThe LibriSpeech dataset (Panayotov et al., 2015) is a corpus of 360 hours of voice that was compiled out of audio books from the LibriVox collection of free public domain audiobooks. Due to the expected training time, we focus in this submission on a subset we call “Libri-15GB”, which is\ncomprised of the first 15GB of the dataset, when sorting the speakers alphabetically . The fitting experiments are done on speakers from the rest of the dataset (“Libri-rest”).\nThe VoxCeleb dataset (Nagrani et al., 2017) is a compilation of YouTube urls and time stamps, which were obtained using an automatic pipeline, which consists of video-based active speaker identification and face verification. The dataset is collected for the task of identification based on voice, and the quality of many of the audio clips is not high. Our system does not need the transcript for fitting. However, the baseline VoiceLoop method does. For this purpose, we employ the automatic transcript by YouTube. This transcript is also used to cut the dataset into individual sentences."
  }, {
    "heading": "5.1. Evaluation of Trained Voices",
    "text": "We first evaluate the quality of the trained model on the trained identities. While this is not the focus of this work, it is important to validate that our approach, which forgoes the speaker LUT and replaces it with a network trained on short voice clips, does not result in a degradation in performance.\nTab. 1 depicts the MOS values obtained for generated test samples of the VCTK85 dataset, as well as the Libri15GB subset. As can be seen, the quality obtained with our model is higher than that of VoiceLoop. Similar conclusions can be drawn from Tab. 2, which shows the MCD scores. Note that as reported also in (Ping et al., 2018), LibriSpeech results are lower than VCTK results. This probably stems from the added prosody in audiobooks and from the inability of the systems to model long term interactions within paragraphs due to the training procedure.\nIn addition, in Tab. 3, we evaluate the identification accuracy for the VCTK85 generated voices as well as the AUC obtained on the same/not-same identification experiment on Libri15GB. As shown, our method, despite not using an explicit per-speaker embedding during training, presents a similar level of identifiability to that of VoiceLoop."
  }, {
    "heading": "5.2. Fitting Experiments",
    "text": "In the next set of experiments, we evaluate our ability to capture new voices, unseen during training. For both our method and the baseline VoiceLoop method, and across all fitted datasets, models which were trained on VCTK85 were used.\nIn Tab. 4, we present the MOS values obtained for the fitted voices. In this experiment, all of the training samples of each new speaker were used (of course, each new speaker was fitted individually). As can be seen, our method shows a significant gap over the baseline VoiceLoop method for the 16 new speakers in VCTK16. A similar gap is shown for VCTK16 in Tab. 6 for the automatic MCD and the identity classification accuracy.\nWe do not present MOS results for VoiceLoop model fitted on Libri-rest and VoxCeleb dataset since the quality of the generated samples was not suitable for MOS experimentation. However, We do include identification results for the fitted VoiceLoop results as baselines on LibriSpeech and VoxCeleb.\nIdentification results for LibriSpeech are shown in Fig. 2 as ROC curves. As can be seen, our method outperforms VoiceLoop. The identification results for the fitted voices of VoxCeleb is not high, even for the original voice clips, due to a high intra-speaker variability and low audio quality. We, therefore, report these on subsets of the speakers which are stratified by quality. Specifically, we measure the inter-sample distances for each speaker and compare it to the average inter-speaker distance. We then threshold and employ only classes with a ratio (of the latter over the former) higher than a parameter. In Fig. 3, we present the AUC obtained in the same not same discrimination task as a function of this parameter. As can be seen, the identifiability obtained by our method is relatively high, when compar-\ning to the score obtained by the ground truth samples. The AUC without filtering is 0.72 for the ground truth, 0.62 for VoiceLoop and 0.67 for our method\nThe ability to fit a new speaker is expected to depend on the length of the available sample. We, therefore, perform experiments exploring the quality of the fitted voices are a function of the maximal sample’s length. Note that the average length is lower than the maximal length: if the length threshold is set at 15 min and there are only 12 min of that speaker, then only 12 min are used.\nThe results are presented in Tab. 5. As can be seen, our method is preferable across all lengths to VoiceLoop, in which the fitting procedure is a much lengthier process that involves backpropagation. Note that quality-wise (but not with regards to identification), the best results for our method are obtained when using a short sample of the speaker. This could be a consequence of the length of the sample used for Ns during training, which, as detailed in Sec 4.2, is 0.5 sec in the first 90 epochs and up to 5 sec (but 3.1 sec on average) in the subsequent epochs. The Priming based method, presented in Sec. 3, shows relatively good quality but is not as identifiable."
  }, {
    "heading": "5.3. Ablation Analysis",
    "text": "Using the automatic MCD and top-1 identification accuracy scores, we also compare with simplified versions of our method in which some of the losses are removed. Specifically, we compare with a version of our method in which Lcycle is removed and another version in which Lcontrast is\nremoved. As can be seen in Tab. 6 removing each of these terms leads to a significant loss of accuracy for the fitted voices, and in the case of Lcontrast also for the trained voices.\nIn order to further visualize this, Fig. 4 presents 2D TSNE plots for the embedding obtained from the VCTK16 samples with and without the Lcycle term. Both these results are obtained withoutLcontrast, which pushes male and females so far that the TSNE plot is uninformative without zooming. As can be seen, Lcycle contributes significantly to the separation of unseen voices in the embedding space."
  }, {
    "heading": "5.4. Audio Samples",
    "text": "Various samples can be found on the project’s webpage https://ytaigman.github.io/fitspk/ index.html. These include generated samples of VCTK16 voices segregated by the length of the sample used for fitting and samples generated after fitting voices from both VoxCeleb and LibriSpeech-rest."
  }, {
    "heading": "6. Conclusions",
    "text": "By demonstrating the ability to fit, in a feed-forward manner on even very short samples from uncontrolled (“in-thewild”) datasets, we brought neural TTS systems significantly closer to fulfilling their promise. The advancement we have made, although shown in the context of a specific architecture, is widely applicable and one can draw a few conclusions, which are unintuitive and even surprising.\nFirst, identifiable voices can be captured from short sam-\nples and without transcript. Machines are, therefore, able to mimic voices much more easily than what was previously believed. This is further demonstrated by presenting a single, simplified, network that given a second of speech can replicate its speaker approximately well. This capability is based on a priming operator, and relies on a phenomenon we identify, the voice constancy property.\nSecond, by training on VCTK85 and then fitting on datasets with different characteristics and many more speakers, is becomes apparent that it is sufficient to train on a small population of 85 speakers in order to capture much of the variation in the general population.\nThird, we demonstrate that a dynamic embedding, which is captured on-the-fly, is able to at least match learned embeddings. This is surprising, and as our ablation analysis shows, stems from the losses we incorporate into the problem.\nOne of the losses employed, Lcycle, opens the way for training a TTS system in a semi-supervised way, in which some speakers are transcribed and some are not. This is because it does not require that the audio generated by the system is identical to the input audio, only that the speaker identity is preserved. This way, the same TTS system can be trained on many languages at once, including languages without suitable transcribed corpora. This is left for future work."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank Ailing Zhang and Shubho Sengupta for helping with multigpu optimization. This work was carried out in partial fulfillment of the requirements for the Ph.D. degree of the first author."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep voice 2: Multispeaker neural text-to-speech",
    "authors": ["S. Arik", "G. Diamos", "A. Gibiansky", "J. Miller", "K. Peng", "W. Ping", "J. Raiman", "Y. Zhou"],
    "venue": "In Neural Information Processing Systems (NIPS),",
    "year": 2017
  }, {
    "title": "Deep voice: Real-time neural text-tospeech",
    "authors": ["S.O. Arik", "M. Chrzanowski", "A. Coates", "G. Diamos", "A. Gibiansky", "Y. Kang", "X. Li", "J. Miller", "J. Raiman", "S Sengupta"],
    "venue": "In Proc. of the 34th International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Convolutional Sequence to Sequence Learning",
    "authors": ["J. Gehring", "M. Auli", "D. Grangier", "D. Yarats", "Y.N. Dauphin"],
    "venue": "In Proc. of ICML,",
    "year": 2017
  }, {
    "title": "Generating sequences with recurrent neural networks",
    "authors": ["A. Graves"],
    "venue": "arXiv preprint arXiv:1308.0850,",
    "year": 2013
  }, {
    "title": "Voco: Text-based insertion and replacement in audio narration",
    "authors": ["Z. Jin", "G.J. Mysore", "S. Diverdi", "J. Lu", "A. Finkelstein"],
    "venue": "In SIGGRAPH,",
    "year": 2017
  }, {
    "title": "World: A vocoderbased high-quality speech synthesis system for real-time applications",
    "authors": ["M. Morise", "F. Yokomori", "K. Ozawa"],
    "venue": "IEICE TRANSACTIONS on Information and Systems,",
    "year": 2016
  }, {
    "title": "Voxceleb: a large-scale speaker identification dataset",
    "authors": ["A. Nagrani", "J.S. Chung", "A. Zisserman"],
    "venue": "In INTERSPEECH,",
    "year": 2017
  }, {
    "title": "Wavenet: A generative model for raw audio",
    "authors": ["Oord", "A. v. d", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1609.03499,",
    "year": 2016
  }, {
    "title": "CROWDMOS: an approach for crowdsourcing mean opinion score studies",
    "authors": ["F.P. Ribeiro", "D. Florencio", "C. Zhang", "M. Seltzer"],
    "venue": "In ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing Proceedings, pp. 2416–2419,",
    "year": 2011
  }, {
    "title": "Librispeech: An ASR corpus based on public domain audio books. dataset licensed under (cc by 4.0)",
    "authors": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"],
    "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,",
    "year": 2015
  }, {
    "title": "Tensorflow implementation of deep voice 3, 2018",
    "authors": ["K. Park"],
    "venue": "URL https://github.com/Kyubyong/ deepvoice3",
    "year": 2018
  }, {
    "title": "Deep voice 3: 2000-speaker neural text-to-speech",
    "authors": ["W. Ping", "K. Peng", "A. Gibiansky", "S.Ö. Arik", "A. Kannan", "S. Narang", "J. Raiman", "J. Miller"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2018
  }, {
    "title": "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions",
    "authors": ["J. Shen", "R. Pang", "R.J. Weiss", "M. Schuster", "N. Jaitly", "Z. Yang", "Z. Chen", "Y. Zhang", "Y. Wang", "R SkerryRyan"],
    "venue": "arXiv preprint arXiv:1712.05884,",
    "year": 2017
  }, {
    "title": "Char2wav: End-to-end speech synthesis",
    "authors": ["J. Sotelo", "S. Mehri", "K. Kumar", "J.F. Santos", "K. Kastner", "A. Courville", "Y. Bengio"],
    "venue": "In ICLR workshop,",
    "year": 2017
  }, {
    "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
    "authors": ["Y. Taigman", "L. Wolf", "A. Polyak", "E. Nachmani"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2018
  }, {
    "title": "Parallel wavenet: Fast high-fidelity speech synthesis",
    "authors": ["D. Hassabis"],
    "venue": "arXiv preprint arXiv:1711.10433,",
    "year": 2017
  }, {
    "title": "u., and Polosukhin, I. Attention is all you need",
    "authors": ["A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "L. Kaiser"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit. dataset licensed under (odc-by",
    "authors": ["C. Veaux", "J. Yamagishi", "K MacDonald"],
    "year": 2017
  }, {
    "title": "Tacotron: A fully end-to-end text-to-speech synthesis model",
    "authors": ["Y. Wang", "R. Skerry-Ryan", "D. Stanton", "Y. Wu", "R.J. Weiss", "N. Jaitly", "Z. Yang", "Y. Xiao", "Z. Chen", "S Bengio"],
    "venue": "arXiv preprint arXiv:1703.10135,",
    "year": 2017
  }],
  "id": "SP:17475418c35030e096f219555abb5e5a4ff84160",
  "authors": [{
    "name": "Eliya Nachmani",
    "affiliations": []
  }, {
    "name": "Adam Polyak",
    "affiliations": []
  }, {
    "name": "Yaniv Taigman",
    "affiliations": []
  }, {
    "name": "Lior Wolf",
    "affiliations": []
  }],
  "abstractText": "Learning-based Text To Speech systems have the potential to generalize from one speaker to the next and thus require a relatively short sample of any new voice. However, this promise is currently largely unrealized. We present a method that is designed to capture a new speaker from a short untranscribed audio sample. This is done by employing an additional network that given an audio sample, places the speaker in the embedding space. This network is trained as part of the speech synthesis system using various consistency losses. Our results demonstrate a greatly improved performance on both the dataset speakers, and, more importantly, when fitting new voices, even from very short samples.",
  "title": "Fitting New Speakers Based on a Short Untranscribed Sample"
}