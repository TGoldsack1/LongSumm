{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2454–2464 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "A task-oriented spoken dialogue system (SDS) is a system that can continuously interact with a human to accomplish a predefined task through speech. Dialogue manager, which maintains the dialogue state and decides how to respond, is the\ncore of an SDS. In this paper, we focus on the dialogue policy.\nAt the early research, the spoken dialogue systems assume observable dialogue states. Dialogue policy is simply a set of hand-crafted mapping rules from state to machine action. This is referred to as rule-based policy, which often has acceptable performance but has no ability of self-adaption. Nowadays rule-based policy is popular in commercial dialogue systems.\nHowever, in real world scenarios, unpredictable user behavior, inevitable automatic speech recognition, and spoken language understanding errors make it difficult to maintain the true dialogue state and make the decision. Hence, in recent years, there is a research trend towards statistical dialogue management. A well-founded theory for this is the partially observable Markov decision process (POMDP) (Kaelbling et al., 1998), which can provide robustness to errors from the input module and automatic policy optimization by reinforcement learning. Most POMDP based policy learning research is usually carried out using either user simulator or employed users (Williams and Young, 2007; Young et al., 2010). The trained policy is not guaranteed to work well in real world scenarios. Therefore, on-line policy training has been of great interest (Gašić et al., 2011). Recently, Chen et al. (2017) proposed two qualitative metrics 1 to measure on-line policy learning: safety and efficiency. Safety reflects whether the initial policy can satisfy the quality-of-service requirement in real-world scenarios during the online policy learning period. Efficiency reflects how long it takes for the on-line policy training algorithm to reach a satisfactory performance level.\nMost traditional RL-based policy training suf-\n1The quantitative evaluation metrics of safety and efficiency are proposed in section 4.\n2454\nfers poor initial performance, i.e. causes the safety problem. In light of above, Chen et al. (2017) proposed a safe and efficient on-line policy optimization framework, i.e. companion teaching (CT), in which a human teacher is added in the classic POMDP. The teacher has two missions: one is to show example actions, another is to act as a critic to give the student extra reward which can make the learning of policy more efficient. The example actions not only make the learning safer but also can be directly used by the training of the student policy. However, there are costs to the teaching of a human teacher.\nBased on CT, companion learning (CL) framework is proposed to integrate rule-based policy and RL-based policy, resulting in safe and efficient on-line policy learning. Here, the rule-based policy acts as a virtual teacher which replaces the human teacher in CT. There are a few differences between these two kinds of teachers. First, because it has no marginal cost when it’s deployed, the rule teacher can be consulted at any time if needed. On the other hand, the rule policy is not as good as the human teacher, therefore it’s important to determine when and how much the student policy depends on the rule teacher. Here, we propose an agent-aware dropout Deep Q-Network (AADDQN) as the student statistical policy, which provides (1) two separate experience replay pools for student and teacher, (2) an uncertainty estimated by dropout which can be used to control the timing of consultation and learning.\nIn summary, our main contributions are threefolds: (1) Companion learning (CL) framework was proposed to integrate rule-based policy and RL-based policy. (2) An agent-aware dropout Deep Q-Network (AAD-DQN) was proposed as the statistical student policy. (3) Compared with other companion teaching approaches (Chen et al., 2017) as well as supervised pre-training using static dialogue corpus (Fatemi et al., 2016), CL with AAD-DQN can achieve better performance."
  }, {
    "heading": "2 Related Work",
    "text": "Most previous studies of on-line policy learning have been focused on the efficiency issue, such as Gaussian Process Reinforcement Learning (GPRL) (Gašić et al., 2010). In GPRL, the kernel function defines prior correlations of the objective function given different belief states, which can significantly speed up the policy learning (Gašić\nand Young, 2014). Alternative methods include Kalman temporal difference reinforcement learning (Pietquin et al., 2011).\nMore recently, deep reinforcement learning (DRL) (Mnih et al., 2015) is applied in dialogue policy optimization, including deep Q-Network (DQN) (Cuayáhuitl et al., 2015; Fatemi et al., 2016; Zhao and Eskenazi, 2016; Lipton et al., 2016) and policy gradient (PG) methods, e.g. REINFORCE (Williams and Zweig, 2016; Su et al., 2016; Williams et al., 2017), Advantage ActorCritic (A2C) (Fatemi et al., 2016). In order to speed up the learning of DQN, Lipton et al. (2016) proposed an efficient exploration technique based on Thompson sample from a Bayesian neural network. Furthermore, they showed that using a few successful dialogues generated by a rulebased policy to pre-fill the replay buffer can benefit the learning at the beginning. To improve the efficiency of PG methods, policy network is initialized with supervised learning (SL) before RL training (Williams and Zweig, 2016; Williams et al., 2017; Su et al., 2016, 2017; Fatemi et al., 2016), which is similar to the idea in (Silver et al., 2016). However, combining RL with SL for dialogue policy optimization is not new. Henderson et al. (2008) were among the first to prove the benefits of combining supervised and reinforcement learning. In the experiments, we will compare CL with these pre-training methods.\nAlthough the improvement of efficiency can benefit the safety of learning process, no matter how efficient the algorithm is, an unsafe on-line learned policy can lead to bad user experience at the beginning of learning period and consequently fail to attract sufficient real users to continuously improve the policy. Therefore, it is important to address the safety issue. There are few works about the safety issue of on-line dialogue policy optimization. Williams (2008) proposed a method for integrating business rules and POMDPs. The rules act as the action mask, i.e. the rules nominate a set of one or more actions, and the POMDP chooses the optimal action."
  }, {
    "heading": "3 Proposed Framework",
    "text": ""
  }, {
    "heading": "3.1 Companion Learning for On-line Policy Optimization",
    "text": "In the CL framework, there are two agents: one is the student policy, another is the teacher policy. Here, teacher policy is the extra part com-\npared with the classic statistical dialogue manager architecture (Young et al., 2013). The goal of online policy training is to optimize the student policy from data via interaction with users in real scenarios. The teacher guides the policy learning at each turn as a companion of the dialogue policy, hence, referred to as companion learning 2. The CL framework is described in Figure 1(a).\nAt each turn, the input module (ASR and SLU) receives an acoustic input signal from the human user and the dialogue state tracker keeps the dialogue state up-to-date. The dialogue state is then transmitted to both the student policy and the teacher policy. The student policy first generates a candidate action astut and when it needs help from the teacher policy, it sends astut with some auxiliary information which will be transmitted to the teacher. The teacher policy can then help the student policy with one of the following ways or both:\n• Example Action (EA): The teacher generates an action ateat instead of a stu t according\nto its policy. It corresponds to the left switch in Figure 1(a).\n• Critic Advice (CA): The teacher will not explicitly show an action. Instead, it gives an extra reward rintt to the student policy. It corresponds to the right switch in Figure 1(a).\nThe action from control module is then transmitted to the output module, which generates the nature text and audio. At each turn, an extrinsic reward signal rextt will be given to the student policy by\n2The name companion learning has another potential meaning that the agents can learn from each other, i.e. the rules guide the RL training, and the optimised RL policy can provide some intuition for the revision of rules. We will give some preliminary discussions about this point in section 5.3.\nthe environment, i.e. the user. The extrinsic reward rextt with the extra intrinsic reward r int t will be used to update the policy parameters θ using reinforcement learning algorithms.\nIn the CL framework, there are two things that matter: one is when to consult the teacher, another is how to use the teacher’s experiences. In this paper, an agent-aware dropout DQN (AAD-DQN) is proposed. As shown in Figure 1(b), the certainty information during the interaction is used to define a companion function, which controls how often to sample the teacher’s experiences for updating parameters during the training phase (left), and when to use EA or CA teaching method during decision phase (right).\nThe rest of this section is organized as follows. The next subsection introduces the agentaware experience replay in DQN. The definition of certainty in DQN and the companion function are presented in subsection 3.3. The rule-based teacher policy is described in subsection 3.4."
  }, {
    "heading": "3.2 Agent-Aware Experience Replay in DQN",
    "text": "A Deep Q-Network (DQN) is a multi-layer neural network which maps a belief state bt to the Q values of the possible actions at at that state, Q(bt, at; θ), where θ is the weight vector of the neural network. Neural networks for the approximation of value functions have long been investigated (Lin, 1993). However, these methods were previously quite unstable (Mnih et al., 2013). In DQN, Mnih et al. (2013, 2015) proposed two techniques to overcome this instability, namely experience replay and the use of a target network.\nAt every turn, the transition including the previous belief state bt, previous action at, corresponding reward rt and current belief state bt+1 is put in a finite pool (Lin, 1993). In this pa-\nper, two pools Dstu and Dtea are used to store the student’s experiences and the teacher’s experiences respectively as shown in Figure 1(b). When the teaching method EA is used in the t-th turn, at = ateat and the transition is put in Dtea, otherwise at = astut and the transition is put in Dstu . When CA is used, rt = rextt + r int t , otherwise rt = rextt . Once any of the pool has reached its predefined maximum size, adding a new transition results in deleting the oldest transition in the pool. During training, a pool is first selected from Dtea andDstu. The probability of selectingDtea is ptea, i.e. D ∼ Ber(Dtea,Dstu; ptea) 3.Then a minibatch of transitions is uniformly sampled from the selected pool, i.e. (bt, at, rt,bt+1) ∼ U(D). We call this agent-aware experience replay.\nExcept for the experience replay, a target network with weight vector θ− is used. This target network is similar to the Q-network except that its weights are only copied every K steps from the Q-network, and remain fixed during all the other steps. The loss function for the Q-network at each iteration takes the following form:\nL(θ) = ED∼Ber(Dtea,Dstu;ptea), (bt,at,rt,bt+1)∼U(D)[( rt + γmax\nat+1 Q(bt+1, at+1; θ−)−Q(bt, at; θ) )2] (1)\nwhere γ ∈ [0, 1] is the discount factor. The probability ptea controls how often the student learns from the teacher’s experiences. As the learning goes on, the probability will decrease. More details will be described in the next section."
  }, {
    "heading": "3.3 Companion Strategy",
    "text": "It’s important for the student to estimate an appropriate point to end the reliance on the teacher. If the reliance is ended too early, the student itself may not reach an acceptable performance, resulting in the sharp drop of performance, which is the safety problem. However, if the student always relies on the teacher, it’s hard to improve its performance to surpass the teacher’s performance, which is the efficiency problem.\nWe get some inspirations from the studying process of a call center service agent. Consider how a new call center service agent gets started. At first, an experienced call center agent tells him some basic rules and the new agent works by often consulting these rules. His confidence about\n3Ber is short for Bernoulli.\nhow to make decisions gradually increases during the continuous practice. Eventually, he is so confident about his own decisions that he no longer needs any consultation to these rules and even explores some better response ways through interaction with users which are not initially included in the rules. Similarly, we can use the uncertainty/certainty of the Q-network to determine the teaching time.\nThere are several methods to estimate the uncertainty/certainty in deep neural networks, e.g. Bayesian neural networks (Blundell et al., 2015), dropout (Gal and Ghahramani, 2016), bootstrap (Osband et al., 2016) . Here we use the dropout to estimate the certainty of Q-Network. We call this Q-network DropoutQNetwork. Dropout is a technique used to avoid over-fitting in neural networks. It was introduced several years ago by (Hinton et al., 2012) and studied more extensively in (Srivastava et al., 2014). When dropout is used in training, the elements of the output of each hidden layer h is randomly set to zero with probability p, i.e. h′ = h z 4 where z is binary vector and each element zi ∼ Ber(1−p). h′ is scaled by\n1 1−p and then fed to the next layer. At test time the dropout is disabled, i.e. the output of each hidden layer h is directly fed to the next layer. Although dropout was suggested as an ad-hoc technique, recently it was theoretically proven that the dropout training in deep neural networks is an approximate Bayesian inference in deep Gaussian processes (Gal and Ghahramani, 2016). Therefore, a direct result of this theory gives us tools to model uncertainty with dropout neural networks. To obtain the uncertainty, similar with that at train phrase the dropout is enabled at test phrase. For each input instance (i.e. dialogue belief state) bt, performing N stochastic forward passes through the network and averaging the output qi , [qi1, · · · , qiM ] to get the mean and the variance. Generally, the variance can be utilized to measure the uncertainty of output. However, it’s not a normalized criteria, and it’s hard to set a threshold below which we should be confident with the output.\nInstead, we proposed a novel method to measure the certainty of the decision of student policy at t-th turn. For each stochastic forward passes, the action ati = arg maxj qij is regarded as a vote. After N passes 5, there is a committee\n4Here is the element-wise product. 5The N forward passes can be done in parallel, e.g. the\n{at1, · · · , atN} consisting of N votes. The action astut that should be taken in the belief state bt is the one with the largest percentage of the votes, and the corresponding percentage is defined as certainty ct. The process is described in Algorithm 1.\nAlgorithm 1 The Decision Procedure of Student Policy πstu(bt, N) Require:\nThe repeat times N and the belief state bt 1: Initial the probability vector p =\n[p1, · · · , pM ] with zero vector, where M is the number of actions.\n2: for i = 1, N do 3: qi← DropoutQNetwork(bt) 4: ati ← arg maxj qij 5: p[ati]← p[ati] + 1/N 6: end for 7: ct ← maxj pj 8: astut ← arg maxj pj 9: return astut , ct\nAt the end of e-th dialogue, the average certainty of all turns is computed, i.e. Ce = 1 Te ∑Te t=0 ct, where Te is the number of turns in e-th dialogue. Generally, the variance of Ce between successive dialogues is high. In order to the smooth the estimation, here we use the moving average of Ce in previous W dialogues to represent the certainty of student at current dialogue, i.e.\nCe = 1 W e−1∑ i=e−W Ci. (2)\nAs the training goes on, Ce grows until it converges. If Ce in all successive W dialogues are greater then a threshold Cth as shown in Figure 2, it’s assumed that the student reaches a point where it is confident enough with its own decision steadily. Therefore, the teaching, both EA and CA, should be ended from now on.\nBefore the end of the teaching, CA is done in all turns. However, if EA is always done, the disappearance of the teacher may cause a dramatic change in the hybrid decision policy, which results in a sharp drop of performance. To deal with this issue, a monotonically increasing function of the relative certainty Ptea(∆Ce) is proposed to control the frequency of EA teaching.\ndialogue state can be repeated N times to form a mini-batch, then one forward is executed to get N outputs simultaneously.\n∆Ce represents the distance between Ce and Cth, i.e. ∆Ce = max(0, Cth − Ce). The effect of Ptea(∆Ce) is that the closer Ce is to Cth, the more unlikely EA teaching is executed. Besides controlling how often the student directly consult the teacher, another mission of Ptea(∆Ce) is to control how often the teacher’s experiences are replayed, i.e. the probability ptea described in section 3.2. Implementation details of Ptea(∆Ce) are described in Appendix C.\nThe full procedure of companion learning with logic rules is described in Algorithm 2."
  }, {
    "heading": "3.4 Teacher Policy: Logic Rules",
    "text": "Rule-based policy is popular in commercial dialogue systems (Williams, 2008). The policy, i.e. the dialogue plan/flow, is designed by a domain expert. His knowledge of task domain and business rules is encoded in the rules. There are many methods to represent the decision rules, e.g. propositional logic, first-order logic, decision tree. Here, we use the ordered propositional logic rules, which can be easily translated into IF-THEN rules. When making the decision, these rules are executed in pre-defined order. If the conditions of any rule are satisfied, the decision process will be terminated and the output is the corresponding action. In this paper, three hand-crafted logic rules, R1, R2, and R3 , were used as the teacher:\n• R1: confirm the most likely value in slots where the most likely value has probability between 0.1 and 0.66;\n• R2: offer a restaurant if there is at least one slot in which the belief of most likely value is more than the belief of special value “none”;\n6This threshold is the best one we have tried.\nAlgorithm 2 Companion Learning with Logic Rules Require:\nThe number of stochastic forward pass N , the maximal extra reward δ > 0. 1: Initialize the parameters θ of student policy 2: Initialize replay pools Dtea and Dstu with {},\ncertainty memory C with {}, teaching with True.\n3: for e = 1, E do 4: Update the dialogue belief state b0 5: Initialize the average certainty Ce ← 0 6: if teaching is True then 7: teaching, ptea ← Companion(C) 8: end if 9: for t = 0, Te do\n10: Set intrinsic reward rintt ← 0 11: Get system action and the corresponding certainty, i.e. astut , ct ← πstu(bt, N) 12: Ce ← Ce + ct 13: Get action from the rule-based policy, i.e. ateat ← πtea(bt) 14: EA ∼ Ber(ptea) 15: if teaching is True and EA is True then 16: at ← ateat 17: else 18: at ← astut 19: end if 20: if teaching is True then 21: rintt ← (2× 1{at = ateat } − 1)δ 22: end if 23: Ce ← 1TeCe, and store Ce in C 24: Give the action at to the environment,\nobserve the extrinsic reward rextt and update the dialogue belief state bt+1\n25: rt ← rintt + rextt 26: if EA is True then 27: Store {bt, at, rt,bt+1} in Dtea 28: else 29: Store {bt, at, rt,bt+1} in Dstu 30: end if 31: Update the parameters θ of\nDropoutQNetwork according to the equation (1).\n32: end for 33: end for 34: return θ\nAlgorithm 3 Companion Function Companion(C) Require:\nThe average certainty memory C at e-th dialogue and the moving window size W . 1: Initialize teaching with False, ptea with 0 2: for i = 0,W do 3: Compute the moving average certainty\nCe−i in (e-i)-th dialogue with equation (2).\n4: if Ce−i < Cth then 5: teaching ← True 6: break 7: end if 8: end for 9: if teaching is True then\n10: ∆Ce ← max(0, Cth − Ce) 11: ptea ← Ptea(∆Ce) 12: end if 13: return teaching, ptea\n• R3: request values for a slot which is uniformly selected from a pre-defined slot list.\nThe corresponding pseudo-codes are presented in Appendix B."
  }, {
    "heading": "4 Evaluation Metrics of On-line Policy Optimization",
    "text": "Most previous work on the evaluation of RL-based dialogue policy optimization focuses on the final performance (FP) when the system converges to a steady level. However, for on-line policy optimization, it’s important to measure the learning process. Except for FP, we proposed two quantitative metrics: safety loss and efficiency loss."
  }, {
    "heading": "4.1 Safety Loss",
    "text": "In the on-line training process, unless the performance of the system reaches the acceptable performance Sa, the interaction between users and the system will be unsafe and causes trouble to continuing training. So the safety of the system is defined to be the system’s ability to maintain performance above the acceptable performance Sa.\nWe quantify the safety loss of the system by summing up the performance gap between the acceptable performance and the system performance Se in every episode during the on-line learning. Suppose there are E dialogues, then\nL1 = E∑\ne=1 max(0, Sa − Se). The safety loss has an\nintuitive interpretation as the area of the region below the threshold and above training curve. This metric is similar to the integral of absolute error (IAE) (Shinners, 1998) metric commonly adopted in the evaluation of control systems (Gaing, 2004; Jesus and Tenreiro MacHado, 2008)."
  }, {
    "heading": "4.2 Efficiency Loss",
    "text": "Another important issue of on-line learning is efficiency. The efficiency indicates the speed at which the system reaches a specific performance level. In reality, we can tolerate a system to make mistakes at the beginning but it should improve at a significant speed until reaching the ideal performance Si. Therefore, later failures should weight more than early failures to evaluate efficiency. Similar to the integral of time multiplied by absolute error (ITAE) (Shinners, 1998) metric, we propose a metric efficiency loss. We multiply the performance gap between ideal performance and current performance with the episode index, thus giving later failure greater penalty. Specifically,\nL2 = E∑\ne=1 max(0, Si − Se)e.\nMore illustrations about safety loss and efficiency loss are given in Appendix D."
  }, {
    "heading": "5 Experiments",
    "text": "Our experiments have three objectives: (1) Comparing our proposed dropout DQN in Algorithm 1 with some baselines when there is no teacher. (2) Comparing CL with other two baselines when the teacher gets involved, and investigating the benefits of our proposed agent-aware experience replay. (3) Visually analyzing the differences in behaviors between the rule-based teacher policy and the optimized student policy.\nAn agenda-based user simulator (Schatzmann et al., 2007a) with error model (Schatzmann et al., 2007b) was implemented to emulate the behavior of the human user, and a rule-based policy with 0.695 success rate described in section 3.2 was used as the teacher in our experiments. The purpose of the user’s interacting with SDS is to find restaurant information in the Cambridge (UK) area (Henderson and Thomson, 2014). This domain has 7 slots of which 4 can be used by the system to constrain the database search. The summary action space consists of 16 summary actions. More details are described in Appendix A.\nFor reward, at each turn, an extrinsic reward of\n−0.05 is given to the student policy. At the end of the dialogue, a reward of +1 is given for dialogue success. The maximal extra reward δ is 0.05.\nFor each set-up, 10000 dialogues are used for training, the moving dialogue success rate is recorded with a window size of 1000. The final results are the average of 40 runs."
  }, {
    "heading": "5.1 Policy Learning without Teaching",
    "text": "In this section, four policies without teaching are compared:\n• DQN: A vanilla deep Q-Network (Mnih et al., 2015) which has two hidden layers, each with 128 nodes.\n• A2C: An advantage actor-critic policy which consists of an actor network and a critic network (Fatemi et al., 2016).\n• Dropout DQN 1 and Dropout DQN 32: They both have a dropout layer after each hidden layer. The dropout rate is 0.2. Their difference is that the number of stochastic forward pass N of Dropout DQN 32 in Algorithm 1 is 32, while that of Dropout DQN 1 is 1. Dropout DQN 1 makes decision according to one output of Q-network similar to that of vanilla DQN. Dropout DQN 1 was first proposed in (Gal and Ghahramani, 2016), and was confirmed that Dropout DQN 1 can obtain more efficient exploration.\nThe learning curves are described in Figure 3 and the evaluation results are described in Table 1. Comparing Dropout DQN 1 with DQN in figure 3, the improvement of efficiency caused by\ndropout can be observed as claimed in (Gal and Ghahramani, 2016). However, Dropout DQN 1 seems to suffer premature and sub-optimal convergence, while our proposed Dropout DQN 32, whose decision is based on multi votes (algorithm 1), can result in improvement of efficiency and better final performance. Moreover, Dropout DQN 32 also performs much better than the policy gradient method A2C.\nFor the following experiments, the times of stochastic forward pass N in Algorithm 1 is 32."
  }, {
    "heading": "5.2 Policy Learning with Teaching",
    "text": "In this section, four methods of teaching by the rule-based policy are compared:\n• EA: 500 dialogues are taught with EA at the beginning (Chen et al., 2017).\n• A2C PreTrain: At the beginning, 500 dialogue are collected with rule-based policy. These examples are used to pre-train the actor network with supervised learning. After the pre-training, the policy is continuously optimized with the A2C algorithm (Fatemi et al., 2016).\n• CL AAD: Full CL with AAD-DQN described in section 3.\n• CL D: CL without agent-aware experience repay, i.e. the teacher’s experiences and student’s experiences are put in one pool and are uniformly sampled for the experience replay in equation (1).\nAs can be seen in Figure 4, there is a big dip in the performance of A2C PreTrain. One possible explanation is that because the rule-based policy is sub-optimal, the pre-training makes the student policy reach a local minimum point. The rltraining should first make it escape from the local\nminimum point, which results in a temporary loss in performance.\nComparing CL methods (CL D and CL AAD) with EA in Figure 4 and in Table 1, we can conclude that CL can significantly boost the safety of learning process. Moreover, except for safety, CL AAD can boost the efficiency, which benefits from the agent-aware experience replay."
  }, {
    "heading": "5.3 Comparison of Optimized Student Policy and Rule-based Teacher Policy",
    "text": "To interpret what the student has learnt, we further compare the rules and an optimized student policy with 76.7% success rate. The rule-based policy is used to collect 5000 dialogues, while in each turn the decision made by the student policy is also recorded. Figure 5 is a confusion matrix. The x-axis denotes the student’s decision and the y-axis denotes the rules’ decision. The numbers on the left are the statistics for each action in 5000 dialogues. Each element in the matrix denotes the normalized number of turns when the rule chooses the action in the corresponding line, the student chooses the action in the corresponding column.\nAs is shown in Figure 5, offer and confirm are two action types used most frequently. In more than half of turns when the rule-based pol-\nicy chooses offer 1, the student policy will choose a different action. Furthermore, from the element in line offer 1 and column request area, we can find that in this situation the student policy prefers the action request area. Inspired by this disagreement, we designed a new rule:\n• R4: request values for slot area when there is only one other slot constraint for the database query.\nSimilarly, as can be seen in Figure 5, in a considerable proportion of turns when the rule-based policy chooses confirm area, confirm pricerange, or confirm name, the student policy will choose the action offer 2, which may mean that for slots area, pricerange, or name, when there are values for database query, the system should offer a restaurant instead of confirming the slot-value constraints. Therefore, the rule R1 in section 3.2 was revised as follows:\n• R1*: For slot food, confirm the most likely value has the probability between 0.1 and 0.6; For slot area, pricerange and name, confirm the most likely value, the belief of which is smaller than the belief of the special value “none” and is larger than 0.1.\nTable 2 is the evaluation results of different ordered rules. The rule R4 can significantly boost the success rate (comparing line 2 with line 1),\nwhile the rule R1* can both boost the success rate and decrease the dialogue length (comparing line 3 with line 1). The combination of R4 and R1* takes respective advantages (comparing line 4 with line 1, line 2 and line 3). The performance of final order rules is comparable to the performance of optimized student policy.\nIt is worth noting that the primary rules R1, R2, and R3 in section 3.2 don’t distinguish between different slots. However, the new rules R4 and R1* are all slot-specific, which it is difficult to design at the beginning."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper has proposed a companion learning framework to unify rule-based policy and RLbased policy. Here, the rule-based policy acts as a teacher, which either directly shows example action or gives an extra reward. Based on the uncertainty estimated using a dropout Q-Network, a companion strategy is proposed to control when the student policy directly consults rules and how often the student policy learns from the teacher’s experiences. Simulation experiments showed that our proposed framework can significantly improve both safety and efficiency of on-line policy optimization. Additionally, we visually analyzed the differences in behaviors between the rule-based teacher policy and the optimized student policy, which gave us some inspirations to refine the rules."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by the Shanghai Sailing Program No. 16YF1405300, the China NSFC projects (No. 61573241 and No. 61603252) and the Interdisciplinary Program (14JCZ03) of Shanghai Jiao Tong University in China. Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University."
  }],
  "year": 2017,
  "references": [{
    "title": "Weight uncertainty in neural network",
    "authors": ["Charles Blundell", "Julien Cornebise", "Koray Kavukcuoglu", "Daan Wierstra."],
    "venue": "International Conference on Machine Learning, pages 1613– 1622.",
    "year": 2015
  }, {
    "title": "On-line dialogue policy learning with companion teaching",
    "authors": ["Lu Chen", "Runzhe Yang", "Cheng Chang", "Zihao Ye", "Xiang Zhou", "Kai Yu."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
    "year": 2017
  }, {
    "title": "Strategic dialogue management via deep reinforcement learning",
    "authors": ["Heriberto Cuayáhuitl", "Simon Keizer", "Oliver Lemon."],
    "venue": "NIPS Deep Reinforcement Learning Workshop.",
    "year": 2015
  }, {
    "title": "Policy networks with two-stage training for dialogue systems",
    "authors": ["Mehdi Fatemi", "Layla El Asri", "Hannes Schulz", "Jing He", "Kaheer Suleman."],
    "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 101–110.",
    "year": 2016
  }, {
    "title": "A Particle Swarm Optimization Approach for Optimum Design of PID Controller in AVR System",
    "authors": ["Zwe-Lee L Gaing."],
    "venue": "IEEE Transactions on Energy Conversion, 19(2):384–391.",
    "year": 2004
  }, {
    "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
    "authors": ["Yarin Gal", "Zoubin Ghahramani."],
    "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML-16).",
    "year": 2016
  }, {
    "title": "Gaussian processes for fast policy optimisation of POMDP-based dialogue",
    "authors": ["Milica Gašić", "Filip Jurčı́ček", "Simon Keizer", "François Mairesse", "Blaise Thomson", "Kai Yu", "Steve Young"],
    "year": 2010
  }, {
    "title": "On-line policy optimisation of spoken dialogue systems via live interaction with human subjects",
    "authors": ["Milica Gašić", "Filip Jurčı́ček", "Blaise Thomson", "Kai Yu", "Steve Young"],
    "venue": "In Automatic Speech Recognition and Understanding (ASRU),",
    "year": 2011
  }, {
    "title": "Gaussian processes for pomdp-based dialogue manager optimization",
    "authors": ["Milica Gašić", "Steve Young."],
    "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(1):28–40.",
    "year": 2014
  }, {
    "title": "Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets",
    "authors": ["James Henderson", "Oliver Lemon", "Kallirroi Georgila."],
    "venue": "Computational Linguistics, 34(4):487–511.",
    "year": 2008
  }, {
    "title": "The second dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson."],
    "venue": "SIGDIAL, volume 263, Stroudsburg, PA, USA. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Improving neural networks by preventing coadaptation of feature detectors",
    "authors": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1207.0580.",
    "year": 2012
  }, {
    "title": "Fractional control of heat diffusion systems",
    "authors": ["Isabel S. Jesus", "J.A. Tenreiro MacHado."],
    "venue": "Nonlinear Dynamics, 54(3):263–282.",
    "year": 2008
  }, {
    "title": "Planning and acting in partially observable stochastic domains",
    "authors": ["Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra."],
    "venue": "Artificial Intelligence, 101(1-2):99–134.",
    "year": 1998
  }, {
    "title": "Reinforcement learning for robots using neural networks",
    "authors": ["Long-Ji Lin."],
    "venue": "Ph.D. thesis, Fujitsu Laboratories Ltd.",
    "year": 1993
  }, {
    "title": "Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking",
    "authors": ["Zachary C Lipton", "Jianfeng Gao", "Lihong Li", "Xiujun Li", "Faisal Ahmed", "Li Deng."],
    "venue": "arXiv preprint arXiv:1608.05081.",
    "year": 2016
  }, {
    "title": "Playing atari with deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."],
    "venue": "arXiv preprint arXiv:1312.5602.",
    "year": 2013
  }, {
    "title": "Human-level control through deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"],
    "year": 2015
  }, {
    "title": "Deep exploration via bootstrapped dqn",
    "authors": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy."],
    "venue": "Advances in Neural Information Processing Systems, pages 4026–4034.",
    "year": 2016
  }, {
    "title": "Sample Efficient On-line Learning of Optimal Dialogue Policies with Kalman Temporal Differences",
    "authors": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan."],
    "venue": "IJCAI, pages 1878–1883.",
    "year": 2011
  }, {
    "title": "Agenda-based user simulation for bootstrapping a pomdp dialogue system",
    "authors": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young."],
    "venue": "NAACL, pages 149–152, Morristown, NJ, USA. Association for Computational Linguis-",
    "year": 2007
  }, {
    "title": "Error simulation for training statistical dialogue systems",
    "authors": ["Jost Schatzmann", "Blaise Thomson", "Steve Young."],
    "venue": "Automatic Speech Recognition & Understanding, 2007. ASRU. IEEE Workshop on, pages 526–531. IEEE.",
    "year": 2007
  }, {
    "title": "Modern control system theory and design",
    "authors": ["Stanley M Shinners."],
    "venue": "John Wiley & Sons.",
    "year": 1998
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"],
    "year": 2016
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Sample-efficient actor-critic reinforcement learning with supervised data for dialogue management",
    "authors": ["Pei-Hao Su", "Pawel Budzianowski", "Stefan Ultes", "Milica Gasic", "Steve Young."],
    "venue": "Proceedings of the 18th Annual Meeting of the Special Interest",
    "year": 2017
  }, {
    "title": "Continuously learning neural dialogue management",
    "authors": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina RojasBarahona", "Stefan Ultes", "David Vandyke", "TsungHsien Wen", "Steve Young."],
    "venue": "arXiv preprint arXiv:1606.02689.",
    "year": 2016
  }, {
    "title": "The best of both worlds: unifying conventional dialog systems and pomdps",
    "authors": ["Jason D Williams."],
    "venue": "INTERSPEECH, pages 1173–1176.",
    "year": 2008
  }, {
    "title": "Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning",
    "authors": ["Jason D Williams", "Kavosh Asadi", "Geoffrey Zweig."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
    "year": 2017
  }, {
    "title": "Partially observable markov decision processes for spoken dialog systems",
    "authors": ["Jason D Williams", "Steve Young."],
    "venue": "Computer Speech and Language, 21(2):393–422.",
    "year": 2007
  }, {
    "title": "End-toend LSTM-based dialog control optimized with supervised and reinforcement learning",
    "authors": ["Jason D Williams", "Geoffrey Zweig."],
    "venue": "CoRR.",
    "year": 2016
  }, {
    "title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management",
    "authors": ["Steve Young", "Milica Gašić", "Simon Keizer", "François Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."],
    "venue": "Computer Speech and Lan-",
    "year": 2010
  }, {
    "title": "Pomdp-based statistical spoken dialog systems: A review",
    "authors": ["Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D Williams."],
    "venue": "Proceedings of the IEEE, 101(5):1160–1179.",
    "year": 2013
  }, {
    "title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning",
    "authors": ["Tiancheng Zhao", "Maxine Eskenazi."],
    "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    "year": 2016
  }],
  "id": "SP:f65c3407751797140e9e67d5711617aebbce3902",
  "authors": [{
    "name": "Lu Chen",
    "affiliations": []
  }, {
    "name": "Xiang Zhou",
    "affiliations": []
  }, {
    "name": "Cheng Chang",
    "affiliations": []
  }, {
    "name": "Runzhe Yang",
    "affiliations": []
  }, {
    "name": "Kai Yu",
    "affiliations": []
  }],
  "abstractText": "Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a predefined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher’s experiences. AADDQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safety and efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.",
  "title": "Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning"
}