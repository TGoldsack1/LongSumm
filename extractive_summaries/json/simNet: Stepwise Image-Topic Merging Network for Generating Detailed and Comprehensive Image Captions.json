{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 137–149 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n137"
  }, {
    "heading": "1 Introduction",
    "text": "Image captioning attracts considerable attention in both natural language processing and computer vision. The task aims to generate a description in natural language grounded on the input image. It is a very challenging yet interesting task. On the one hand, it has to identify the objects in the image, associate the objects, and express them in a fluent sentence, each of which is a difficult subtask. On the other hand, it combines two important fields in artificial intelligence, namely, natural language processing and computer vision. More importantly, it has a wide range of applications, including text-based image retrieval, helping visually impaired people see (Wu et al., 2017), humanrobot interaction (Das et al., 2017), etc.\nModels based on the encoder-decoder framework have shown success in image captioning. According to the pivot representation, they can be\n∗Equal Contributions 1 The code is available at https://github.com/\nlancopku/simNet\nroughly categorized into models based on visual information (Vinyals et al., 2015; Chen and Zitnick, 2015; Mao et al., 2014; Karpathy and Li, 2015, 2017), and models based on conceptual information (Fang et al., 2015; You et al., 2016; Wu et al., 2016). The later explicitly provides the visual words (e.g. dog, sit, red) to the decoder instead of the image features, and is more effective in image captioning according to the evaluation on benchmark datasets. However, the models based on conceptual information have a major drawback that it is hard for the model to associate the details with the specific objects in the image, because the visual words are inherently unordered in semantics. Figure 1 shows an example. For semantic attention, although open is provided as a visual word, due to the insufficient use of visual information, the model gets confused about what objects open should be associated with and thus discards open in the caption. The model may even associate the details incorrectly, which is the case\nfor the position of the dog. In contrast, models based on the visual information often are accurate in details but have difficulty in describing the image comprehensively and tend to only describe a subregion.\nIn this work, we get the best of both worlds and integrate visual attention and semantic attention for generating captions that are both detailed and comprehensive. We propose a Stepwise ImageTopic Merging Network as the decoder to guide the information flow between the image and the extracted topics. At each time step, the decoder first extracts focal information from the image. Then, it decides which topics are most probable for the time step. Finally, it attends differently to the visual information and the conceptual information to generate the output word. Hence, the model can efficiently merge the two kinds of information, leading to outstanding results in image captioning.\nOverall, the main contributions of this work are:\n• We propose a novel approach that can effectively merge the information in the image and the topics to generate cohesive captions that are both detailed and comprehensive. We refine and combine two previous competing attention mechanisms, namely visual attention and semantic attention, with an importancebased merging gate that effectively combines\nand balances the two kinds of information.\n• The proposed approach outperforms the state-of-the-art methods substantially on two benchmark datasets, Flickr30k and COCO, in terms of SPICE, which correlates the best with human judgments. Systematic analysis shows that the merging gate contributes the most to the overall improvement."
  }, {
    "heading": "2 Related Work",
    "text": "A large number of systems have been proposed for image captioning. Neural models based on the encoder-decoder framework have been attracting increased attention in the last few years in several multi-discipline tasks, such as neural image/video captioning (NIC) and visual question answering (VQA) (Vinyals et al., 2015; Karpathy and Li, 2015; Venugopalan et al., 2015; Zhao et al., 2016; Zhang et al., 2017). State-of-theart neural approaches (Anderson et al., 2018; Liu et al., 2018; Lu et al., 2018) incorporate the attention mechanism in machine translation (Bahdanau et al., 2014) to generate grounded image captions. Based on what they attend to, the models can be categorized into visual attention models and semantic attention models.\nVisual attention models pay attention to the image features generated by CNNs. CNNs are typically pre-trained on the image recognition task to extract general visual signals (Xu et al., 2015; Chen et al., 2017; Lu et al., 2017). The visual attention is expected to find the most relevant image regions in generating the caption. Most recently, image features based on predicted bounding boxes are used (Anderson et al., 2018; Lu et al., 2018). The advantages are that the attention no longer needs to find the relevant generic regions by itself but instead find relevant bounding boxes that are object orientated and can serve as semantic guides. However, the drawback is that predicting bounding boxes is difficult, which requires large datasets (Krishna et al., 2017) and complex models (Ren et al., 2015, 2017a).\nSemantic attention models pay attention to a predicted set of semantic concepts (Fang et al., 2015; You et al., 2016; Wu et al., 2016). The semantic concepts are the most frequent words in the captions, and the extractor can be trained using various methods but typically is only trained on the given image captioning dataset. This kind\nof approach can be seen as the extension of the earlier template-based slotting-filling approaches (Farhadi et al., 2010; Kulkarni et al., 2013).\nHowever, few work studies how to combine the two kinds of attention models to take advantage of both of them. On the one hand, due to the limited number of visual features, it is hard to provide comprehensive information to the decoder. On the other hand, the extracted semantic concepts are unordered, making it hard for the decoder to portray the details of the objects correctly.\nThis work focuses on combining the visual attention and the semantic attention efficiently to address their drawbacks and make use of their merits. The visual attention is designed to focus on the attributes and the relationships of the objects, while the semantic attention only includes words that are objects so that the extracted topics could be more accurate. The combination is controlled by the importance-based merging mechanism that decides at each time step which kind of information should be relied on. The goal is to generate image captions that are both detailed and comprehensive."
  }, {
    "heading": "3 Approach",
    "text": "Our proposed model consists of an image encoder, a topic extractor, and a stepwise merging decoder. Figure 3 shows a sketch. We first briefly introduce the image encoder and the topic extractor. Then, we introduce the proposed stepwise image-topic merging decoder in detail."
  }, {
    "heading": "3.1 Image Encoder",
    "text": "For an input image, the image encoder expresses the image as a series of visual feature vectors V = {v1,v2, . . . ,vk},vi ∈ Rg. Each feature corresponds to a different perspective of the image. The visual features serve as descriptive guides of the objects in the image for the decoder. We use a\nResNet152 (He et al., 2016), which is commonly used in image captioning, to generate the visual features. The output of the last convolutional layer is used as the visual information:\nV =W V,ICNN(I) (1)\nwhere I is the input image, and W V,I shrinks the last dimension of the output.2"
  }, {
    "heading": "3.2 Topic Extractor",
    "text": "Typically, identifying an object requires a combination of visual features, and considering the limited capacity of the visual features, it is hard for the conventional decoder to describe the objects in the image comprehensively. An advance in image captioning is to provide the decoder with the semantic concepts in the image directly so that the decoder is equipped with an overall perspective of the image. The semantic concepts can be objects (e.g. person, car), attributes (e.g. off, electric), and relationships (e.g. using, sitting). We only use the words that are objects in this work, the reason of which is explained later. We call such words topics. The topic extractor concludes a list of candidate topic embeddings T = {w1,w2, . . . ,wm},wi ∈ Re from the image, where e is the dimension of the topic word embeddings. Following common practice (Fang et al., 2015; You et al., 2016), we adopt the weakly-supervised approach of Multiple Instance Learning (Zhang et al., 2006) to build a topic extractor. Due to limited space, please refer to Fang et al. (2015) for detailed explanation.\nDifferent from existing work that uses all the most frequent words in the captions as valid semantic concepts or visual words, we only include the object words (nouns) in the topic word list. Existing work relies on attribute words and rela-\n2For conciseness, all the bias terms of linear transformations in this paper are omitted.\ntionship words to provide visual information to the decoder. However, it not only complicates the extracting procedure but also contributes little to the generation. For an image containing many objects, the decoder is likely to combine the attributes with the objects arbitrarily, as such words are specific to certain objects but are provided to the decoder unordered. In contrast, our model has visual information as additional input and we expect that the decoder should refer to the image for such kind of information instead of the extracted concepts."
  }, {
    "heading": "3.3 Stepwise Image-Topic Merging Decoder",
    "text": "The essential component of the decoder is the proposed stepwise image-topic merging network. The decoder is based on an LSTM (Hochreiter and Schmidhuber, 1997). At each time step, it combines the textual caption, the attentive visual information, and the attentive conceptual information as the context for generating an output word. The goal is achieved by three modules, the visual attention, the topic attention, and the merging gate.\nVisual Attention as Output The visual attention attends to attracting parts of the image based on the state of the LSTM decoder. In existing work (Xu et al., 2015), only the previous hidden state ht−1 ∈ Rd of the LSTM is used in computation of the visual attention:\nZt = tanh(W Z,V V ⊕W Z,hht−1) (2) αt = softmax(Ztwα,Z) (3)\nwhere W Z,V ∈ Rk×g,W Z,h ∈ Rk×d,wα,Z ∈ Rk are the learnable parameters. We denote the matrix-vector addition as ⊕, which is calculated by adding the vector to each column of the matrix. αt ∈ Rk is the attentive weights of V and the attentive visual input zt ∈ Rg is calculated as\nzt = V αt (4)\nThe visual input zt and the embedding of the previous output word yt−1 are the input of the LSTM.\nht = LSTM( [ zt yt−1 ] ,ht−1) (5)\nHowever, there is a noticeable drawback that the previous output word yt−1, which is a much stronger indicator than the previous hidden state ht−1, is not used in the attention. As zt is used as the input, we call it input attention. To overcome that drawback, we add another attention that incorporates the current hidden state ht, which is\nbased on the last generated word yt−1:\nZ̃t = tanh(W̃ Z,V V ⊕ W̃ Z,hht) (6)\nα̃t = softmax(Z̃tw̃α,Z) (7)\nz̃t = V α̃t (8)\nThe procedure resembles the input attention, and we call it output attention. It is worth mentioning that the output attention is essentially the same with the spatial visual attention proposed by Lu et al. (2017). However, they did not see it from the input-output point of view nor combine it with the input attention.\nThe attentive visual output is further transformed to rt = tanh(W s,zz̃t),W s,z ∈ Re×g, which is of the same dimension as the topic word embedding to simplify the following procedure.\nTopic Attention In an image caption, different parts concern different topics. In the existing work (You et al., 2016), the conceptual information is attended based on the previous output word:\nβt = softmax(T TUyt−1) (9)\nwhereU ∈ Re×e,βt ∈ Rm. The profound issue is that this approach neglects the visual information. It should be beneficial to provide the attentive visual information when selecting topics. The hidden state of the LSTM contains both the information of previous words and the attentive input visual information. Therefore, the model attends to the topics based on the hidden state of the LSTM:\nQt = tanh(WQ,TT ⊕WQ,hht) (10) βt = softmax(Qtwβ,Q) (11)\nwhere WQ,T ∈ Rm×e,WQ,h ∈ Rm×d,wβ,Q ∈ Rm are the parameters to be learned. βt ∈ Rm is the weight of the topics, from which the attentive conceptual output qt ∈ Re is calculated:\nqt = Tβt (12)\nThe topic attention qt and the hidden state ht are combined as the contextual information st:\nst = tanh(W s,qqt +W s,hht) (13)\nwhere W s,q ∈ Re×e,W s,h ∈ Re×d are learnable parameters.\nMerging Gate We have prepared both the visual information rt and the contextual information st. It is not reasonable to treat the two kinds of information equally when the decoder generates different types of words. For example, when generating descriptive words (e.g., behind, red), rt should matter more than st. However, when generating\nobject words (e.g., people, table), st is more important. We introduce a novel score-based merging mechanism to make the model adaptively learn to adjust the balance:\nγt = σ(S(st)− S(rt)) (14) ct = γtst + (1− γt)rt (15)\nwhere σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual attention, and S is the scoring function. The scoring function needs to evaluate the importance of the topic attention. Noticing that Eq. (10) and Eq. (11) have a similar purpose, we define S similarly:\nS(st) = tanh(W S,hht +W S,sst) ·wS (16) S(rt) = tanh(W S,hht +W S,rrt) ·wS (17) where · denotes dot product of vectors, W S,s ∈ Rm×e,W S,r ∈ Rm×e are the parameters to be learned, and W S,h,ws share the weights of WQ,h,wβ,Q from Eq. (10) and Eq. (11), respectively.\nFinally, the output word is generated by:\nyt ∼ pt = softmax(W p,cct) (18) where each value of pt ∈ R|D| is a probability indicating how likely the corresponding word in vocabulary D is the current output word. The whole model is trained using maximum log likelihood and the loss function is the cross entropy loss.\nIn all, our proposed approach encourages the model to take advantage of all the available information. The adaptive merging mechanism makes the model weigh the information elaborately."
  }, {
    "heading": "4 Experiment",
    "text": "We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach."
  }, {
    "heading": "4.1 Datasets and Metrics",
    "text": "There are several datasets containing images and their captions. We report results on the popular Microsoft COCO (Chen et al., 2015) dataset and the Flickr30k (Young et al., 2014) dataset. They contain 123,287 images and 31,000 images, respectively, and each image is annotated with 5 sentences. We report results using the widely-used publicly-available splits in the work of Karpathy and Li (2015). There are 5,000 images each in the validation set and the test set for COCO, 1,000 images for Flickr30k.\nWe report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE. SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems. They both incorporate the consensus of a set of references for an example. BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation. ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization. In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015)."
  }, {
    "heading": "4.2 Settings",
    "text": "Following common practice, the CNN used is the ResNet152 model (He et al., 2016) pre-trained on ImageNet.3 There are 2048 7 × 7 feature maps, and we project them into 512 feature maps, i.e. g is 512. The word embedding size e is 256 and the hidden size d of the LSTM is 512. We only keep caption words that occur at least 5 times in the training set, resulting in 10,132 words for COCO and 7,544 for Flickr30k. We use the topic extractor pre-trained by Fang et al. (2015) for 1,000 concepts on COCO. We only use 568 manuallyannotated object words as topics. For an image, only the top 5 topics are selected, which means m is 5. The same topic extractor is used for Flickr30k, as COCO provides adequate generality. The caption words and the topic words share the same embeddings. In training, we first train the model without visual attention (freezing the CNN parameters) for 20 epochs with the batch size of 80. The learning rate for the LSTM is 0.0004. Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponentially decays with the number of epochs so that it is halved every 50 epochs. We also use momen-\n3We use the pre-trained model from torchvision.\ntum of 0.8 and weight decay of 0.999. We use Adam (Kingma and Ba, 2014) for parameter optimization. For fair comparison, we adopt early stop based on CIDEr within maximum 50 epochs."
  }, {
    "heading": "4.3 Results",
    "text": "We compare our approach with various representative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-theart on the two datasets in comparable settings. Table 1 shows the result on Flickr30k. As we can see, our model outperforms the comparable systems in terms of all of the metrics except BLEU-4. Moreover, our model overpasses the state-of-theart with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments (Anderson et al., 2016).\nTable 2 shows the results on COCO. Among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of BLEU-4. Most encouragingly, our model is also competitive with Up-Down (Ander-\nson et al., 2018), which uses much larger dataset, Visual Genome (Krishna et al., 2017), with dense annotations to train the object detector, and directly optimizes CIDEr. Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR. Breakdown of SPICE Fscores over various subcategories (see Table 3) shows that our model is in dominant lead in almost all subcategories. It proves the effectiveness of our approach and indicates that our model is quite data efficient.\nFor the methods that directly optimize CIDEr, it is intuitive that CIDEr can improve significantly. The similar improvement of BLEU-4 is evidence that optimizing CIDEr leads to more ngram matching. However, it comes to our notice that the improvements of SPICE, METEOR, and ROUGE-L are far less significant, which suggests there may be a gaming situation where the n-gram matching is wrongfully exploited by the model in reinforcement learning. As shown by Liu et al. (2017), it is most reasonable to jointly optimize\nall the metrics at the same time. We also evaluate the proposed model on the COCO evaluation server, the results of which are shown in Appendix A.1, due to limited space."
  }, {
    "heading": "5 Analysis",
    "text": "In this section, we analyze the contribution of each component in the proposed approach, and give examples to show the strength and the potential improvements of the model. The analysis is conducted on the test set of COCO.\nTopic Extraction The motivation of using objects as topics is that they are easier to identify so that the generation suffers less from erroneous predictions. This can be proved by the F-score of the identified topics in the test set, which is shown in Table 4. Using top-5 object words is at least as good as using top-10 all words. However, using top-10 all words introduces more erroneous visual words to the generation. As shown in Ta-\nble 5, when extracting all words, providing more words to the model indeed increases the captioning performance. However, even when top-20 all words are used, the performance is still far behind using only top-5 object words and seems to reach the performance ceiling. It proves that for semantic attention, it is also important to limit the absolute number of incorrect visual words instead of merely the precision or the recall. It is also interesting to check whether using other kind of words can reach the same effect. Unfortunately, in our experiments, only using verbs or adjectives as semantic concepts works poorly.\nTo examine the contributions of the submodules in our model, we conduct a series of experiments. The results are summarized in Table 3. To help with the understanding of the differences, we also report the breakdown of SPICE F-scores.\nVisual Attention Our input attention achieves similar results to previous work (Xu et al., 2015),\nif not better. Using only the output attention is much more effective than using only the input attention, with substantial improvements in all metrics, showing the impact of information gap caused by delayed input in attention. Combining the input attention and the output attention can further improve the results, especially in color and size descriptions.\nTopic Attention As expected, compared with visual attention, the topic attention is better at identifying objects but worse at identifying attributes. We also apply the merging gate to the topic attention, but it now merges qt and ht instead of st and rt. With the merging gate, the model can balance the information in caption text and extracted topics, resulting in better overall scores. While it overpasses the conventional visual attention, it lags behind the output attention.\nMerging Gate Combing the visual attention and the topic attention directly indeed results in a huge boost in performance, which confirms our motivation. However, directly combining them also causes lower scores in attributes, color, count, and size, showing that the advantages are not fully made use of. The most dramatic improvements come from applying the merging gate to the combined attention, showing that the proposed balance mechanism can adaptively combine the two kinds of information and is essential to the overall performance. The average merging gate value summarized in Figure 4 suggests the same.\nWe give some examples in the left plot of Figure 5 to illustrate the differences between the models more intuitively. From the examples, it is clear that the proposed simNet generates the best captions in that more objects are described and many informative and detailed attributes are included, such as the quantity and the color.\nVisualization Figure 6 shows the visualization of the topic attention and the visual attention with running examples. As we can see, the topic attention is active when generating a phrase containing the related topic. For example, bathroom is always most attended when generating a bathroom. The merging gate learns to direct the information flow efficiently. When generating words such as on and a, it gives lower weight to the topic attention and prefers the visual attention. As to the visual attention, the output attention is much more focused than the input attention. As we hypothesized, the conventional input attention lacks the information of the last generated word and does not know what to look for exactly. For example, when generating bathroom, the input attention does not know the previous generated word is a, and it loses its focus, while the output attention is relatively more concentrated. Moreover, the merging gate learns to overcome the erroneous topics, as shown in the second example. When generating chair, the topic attention is focused on a wrong object bed, while the visual attention attends correctly to the chair, and especially the output attention attends to the armrest. The merging gate effectively remedies\nthe misleading information from the topic attention and outputs a lower weight, resulting in the model correctly generating the word chair.\nError Analysis We conduct error analysis using the proposed (full) model on the test set to provide insights on how the model may be improved. We find 123 out of 1000 generated captions that are not satisfactory. There are mainly three types of errors, i.e. distance (32, 26%), movement (22, 18%), and object (60, 49%), with 9 (7%) other errors. Distance error takes place when there is a lot of objects and the model cannot grasp the foreground and the background relationship. Movement error means that the model fails to describe whether the objects are moving. Those two kinds of errors are hard to eliminate, as they are fundamental problems of computer vision waiting to be resolved. Object error happens when there are incorrect extracted topics, and the merging gate regards the topic as grounded in the image. In the given example, the incorrect topic is garden. The tricky part is that the topic is seemingly correct according to the image features or otherwise the proposed model will choose other topics. A more powerful topic extractor may help with the problem but it is unlikely to be completely avoided."
  }, {
    "heading": "6 Conclusions",
    "text": "We propose the stepwise image-topic merging network to sequentially and adaptively merge the visual and the conceptual information for improved image captioning. To our knowledge, we are the first to combine the visual and the semantic attention to achieve substantial improvements. We introduce the stepwise merging mechanism to efficiently guide the two kinds of information when generating the caption. The experimental results demonstrate the effectiveness of the proposed approach, which substantially outperforms the stateof-the-art image captioning methods in terms of SPICE on COCO and Flickr30k datasets. Quantitative and qualitative analysis show that the generated captions are both detailed and comprehensive in comparison with the existing methods."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported in part by National Natural Science Foundation of China (No. 61673028). We thank all the anonymous reviewers for their constructive comments and suggestions. Xu Sun is the corresponding author of this paper."
  }, {
    "heading": "A Supplementary Material",
    "text": "A.1 Results on COCO Evaluation Server Table 6 shows the performance on the online COCO evaluation server4. We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation. The SPICE metrics are only available at the leaderboard on the COCO dataset website5, which, unfortunately, has not been updated for more than a year. Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data. The three techniques typically result in orthogonal improvements (Lu et al., 2017; Rennie et al., 2017; Anderson et al., 2018). Moreover, the SPICE results are missing, in which the proposed model has the most advantage. Nonetheless, our model is second only to Up-Down (Anderson et al., 2018) and surpasses almost all the other models in published work, especially when 40 references are considered.\n4https://competitions.codalab.org/ competitions/3221\n5http://cocodataset.org/ #captions-leaderboard"
  }],
  "year": 2018,
  "references": [{
    "title": "SPICE: semantic propositional image caption evaluation",
    "authors": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."],
    "venue": "Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Pro-",
    "year": 2016
  }, {
    "title": "Bottom-up and top-down attention for image captioning and VQA",
    "authors": ["Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang."],
    "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2018
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "CoRR, abs/1409.0473.",
    "year": 2014
  }, {
    "title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
    "authors": ["Satanjeev Banerjee", "Alon Lavie."],
    "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
    "year": 2005
  }, {
    "title": "Temporal-difference learning with sampling baseline for image captioning",
    "authors": ["Hui Chen", "Guiguang Ding", "Sicheng Zhao", "Jungong Han."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, Febru-",
    "year": 2018
  }, {
    "title": "SCACNN: spatial and channel-wise attention in convolutional networks for image captioning",
    "authors": ["Long Chen", "Hanwang Zhang", "Jun Xiao", "Liqiang Nie", "Jian Shao", "Wei Liu", "Tat-Seng Chua."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recog-",
    "year": 2017
  }, {
    "title": "Microsoft COCO captions: Data collection and evaluation server",
    "authors": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C. Lawrence Zitnick."],
    "venue": "CoRR, abs/1504.00325.",
    "year": 2015
  }, {
    "title": "Mind’s eye: A recurrent visual representation for image caption generation",
    "authors": ["Xinlei Chen", "C. Lawrence Zitnick."],
    "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 2422–",
    "year": 2015
  }, {
    "title": "Visual dialog",
    "authors": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
    "year": 2017
  }, {
    "title": "From captions to visual concepts and back",
    "authors": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Kumar Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig"],
    "year": 2015
  }, {
    "title": "Every picture tells a story: Generating sentences from images",
    "authors": ["Ali Farhadi", "Seyyed Mohammad Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David A. Forsyth."],
    "venue": "Computer Vision",
    "year": 2010
  }, {
    "title": "Semantic compositional networks for visual captioning",
    "authors": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
    "year": 2017
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Fei-Fei Li."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3128–3137. IEEE Computer",
    "year": 2015
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Fei-Fei Li."],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664–676.",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Visual genome: Connecting language and vision",
    "authors": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li"],
    "year": 2017
  }, {
    "title": "ROUGE: a package for auto",
    "authors": ["Chin-Yew Lin"],
    "year": 2004
  }, {
    "title": "Improved image caption",
    "authors": ["Kevin Murphy"],
    "year": 2017
  }, {
    "title": "Show, tell and discrim",
    "authors": ["Xiaogang Wang"],
    "year": 2018
  }, {
    "title": "Neural baby talk",
    "authors": ["Parikh."],
    "venue": "2018 IEEE Con-",
    "year": 2018
  }, {
    "title": "A hierarchical end-to-end model for",
    "authors": ["Ren"],
    "year": 2018
  }, {
    "title": "Deep captioning with multimodal recurrent neural networks (m-RNN)",
    "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille."],
    "venue": "CoRR, abs/1412.6632.",
    "year": 2014
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadel-",
    "year": 2002
  }, {
    "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
    "authors": ["Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-",
    "year": 2015
  }, {
    "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
    "authors": ["Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun."],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137–1149.",
    "year": 2017
  }, {
    "title": "Deep reinforcement learningbased image captioning with embedding reward",
    "authors": ["Zhou Ren", "Xiaoyu Wang", "Ning Zhang", "Xutao Lv", "Li-Jia Li."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
    "year": 2017
  }, {
    "title": "Self-critical sequence training for image captioning",
    "authors": ["Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July",
    "year": 2017
  }, {
    "title": "CIDEr: consensus-based image description evaluation",
    "authors": ["Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages",
    "year": 2015
  }, {
    "title": "Sequence to sequence - video to text",
    "authors": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond J. Mooney", "Trevor Darrell", "Kate Saenko."],
    "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, De-",
    "year": 2015
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages",
    "year": 2015
  }, {
    "title": "Skeleton key: Image captioning by skeleton-attribute decomposition",
    "authors": ["Yufei Wang", "Zhe Lin", "Xiaohui Shen", "Scott Cohen", "Garrison W. Cottrell."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
    "year": 2017
  }, {
    "title": "What value do explicit high level concepts have in vision to language problems",
    "authors": ["Qi Wu", "Chunhua Shen", "Lingqiao Liu", "Anthony R. Dick", "Anton van den Hengel"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Automatic alt-text: Computergenerated image descriptions for blind users on a social network service",
    "authors": ["Shaomei Wu", "Jeffrey Wieland", "Omid Farivar", "Julie Schiller."],
    "venue": "Proceedings of the 2017 ACM Conference on Computer Supported Coopera-",
    "year": 2017
  }, {
    "title": "Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach",
    "authors": ["Jingjing Xu", "Xu Sun", "Qi Zeng", "Xiaodong Zhang", "Xuancheng Ren", "Houfeng Wang", "Wenjie Li."],
    "venue": "Proceedings of the 56th Annual Meeting of the Associa-",
    "year": 2018
  }, {
    "title": "A skeleton-based model for promoting coherence among sentences in narrative story generation",
    "authors": ["Jingjing Xu", "Yi Zhang", "Qi Zeng", "Xuancheng Ren", "Xiaoyan Cai", "Xu Sun."],
    "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural",
    "year": 2018
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "Proceedings of the 32nd International",
    "year": 2015
  }, {
    "title": "Boosting image captioning with attributes",
    "authors": ["Ting Yao", "Yingwei Pan", "Yehao Li", "Zhaofan Qiu", "Tao Mei."],
    "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 2229, 2017, pages 4904–4912. IEEE Computer Soci-",
    "year": 2017
  }, {
    "title": "Image captioning with semantic attention",
    "authors": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4651–",
    "year": 2016
  }, {
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the Association for Computational Linguis-",
    "year": 2014
  }, {
    "title": "Multiple instance boosting for object detection",
    "authors": ["Cha Zhang", "John C. Platt", "Paul A. Viola."],
    "venue": "Y. Weiss, B. Schölkopf, and J. C. Platt, editors, Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems,",
    "year": 2006
  }, {
    "title": "Visual translation embedding network for visual relation detection",
    "authors": ["Hanwang Zhang", "Zawlin Kyaw", "Shih-Fu Chang", "Tat-Seng Chua."],
    "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,",
    "year": 2017
  }, {
    "title": "Partial multi-modal sparse coding via adaptive similarity structure regularization",
    "authors": ["Zhou Zhao", "Hanqing Lu", "Deng Cai", "Xiaofei He", "Yueting Zhuang."],
    "venue": "Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, Amsterdam,",
    "year": 2016
  }, {
    "title": "2018) and surpasses almost all the other models in published work, especially when 40 references are considered",
    "authors": ["derson"],
    "year": 2018
  }],
  "id": "SP:091433bc8791bb66797b519811834a8a53af622d",
  "authors": [{
    "name": "Fenglin Liu",
    "affiliations": []
  }, {
    "name": "Xuancheng Ren",
    "affiliations": []
  }, {
    "name": "Yuanxin Liu",
    "affiliations": []
  }, {
    "name": "Houfeng Wang",
    "affiliations": []
  }, {
    "name": "Xu Sun",
    "affiliations": []
  }],
  "abstractText": "The encode-decoder framework has shown recent success in image captioning. Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.1",
  "title": "simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions"
}