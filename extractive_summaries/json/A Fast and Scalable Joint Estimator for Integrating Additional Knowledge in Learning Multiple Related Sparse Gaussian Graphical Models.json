{
  "sections": [{
    "text": "We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large K) under a highdimensional (large p) situation. In this paper, we propose a novel Joint Elementary Estimator incorporating additional Knowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledgespecific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(p5K4) to O(p2K4). We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot) as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and two real-world data, JEEK outperforms the speed of the state-ofarts significantly while achieving the same level of prediction accuracy.\n1Department of Computer Science, University of Virginia, http://www.jointnets.org/ . Correspondence to: Beilun Wang <bw4mw@virginia.edu>, Yanjun Qi <yanjun@virginia.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s)."
  }, {
    "heading": "1 Introduction",
    "text": "Technology revolutions in the past decade have collected large-scale heterogeneous samples from many scientific domains. For instance, genomic technologies have delivered petabytes of molecular measurements across more than hundreds of types of cells and tissues from national projects like ENCODE (Consortium et al., 2012) and TCGA (Network et al., 2011). Neuroimaging technologies have generated petabytes of functional magnetic resonance imaging (fMRI) datasets across thousands of human subjects (shared publicly through projects like openfMRI (Poldrack et al., 2013). Given such data, understanding and quantifying variable graphs from heterogeneous samples about multiple contexts is a fundamental analysis task.\nSuch variable graphs can significantly simplify networkdriven studies about diseases (Ideker & Krogan, 2012), can help understand the neural characteristics underlying clinical disorders (Uddin et al., 2013) and can allow for understanding genetic or neural pathways and systems. The number of contexts (denoted as K) that those applications need to consider grows extremely fast, ranging from tens (e.g., cancer types in TCGA (Network et al., 2011)) to thousands (e.g., number of subjects in openfMRI (Poldrack et al., 2013)). The number of variables (denoted as p) ranges from hundreds (e.g., number of brain regions) to tens of thousands (e.g., number of human genes).\nThe above data analysis problem can be formulated as jointly estimating K conditional dependency graphs G(1), G(2), . . . , G(K) on a single set of p variables based on heterogeneous samples accumulated from K distinct contexts. For homogeneous data samples from a given i-th context, one typical approach is the sparse Gaussian Graphical Model (sGGM) (Lauritzen, 1996; Yuan & Lin, 2007). sGGM assumes samples are independently and identically drawn from Np(µ(i),Σ(i)), a multivariate Gaussian distribution with mean vector µ(i) and covariance matrix Σ(i). The graph structure G(i) is encoded by the sparsity pattern of the inverse covariance matrix, also named precision matrix, Ω(i). Ω(i) := (Σ(i))−1. Ω(i)jk = 0 if and only if in G(i) an edge does not connect j-th node and k-th node (i.e., conditional independent). sGGM imposes an `1 penalty on the parameter Ω(i) to achieve a consistent estimation\nunder high-dimensional situations. When handling heterogeneous data samples, rather than estimating sGGM of each condition separately, a multi-task formulation that jointly estimatesK different but related sGGMs can lead to a better generalization(Caruana, 1997).\nPrevious studies for joint estimation of multiple sGGMs roughly fall into four categories: (Danaher et al., 2013; Mohan et al., 2013; Chiquet et al., 2011; Honorio & Samaras, 2010; Guo et al., 2011; Zhang & Wang, 2012; Zhang & Schneider, 2010; Zhu et al., 2014): (1) The first group seeks to optimize a sparsity regularized data likelihood function plus an extra penalty functionR′ to enforce structural similarity among multiple estimated networks. Joint graphical lasso (JGL) (Danaher et al., 2013) proposed an alternating direction method of multipliers (ADMM) based optimization algorithm to work with two regularization functions (`1 + R′). (2) The second category tries to recover the support of Ω(i) using sparsity penalized regressions in a column by column fashion. Recently (Monti et al., 2015) proposed to learn population and subject-specific brain connectivity networks via a so-called “Mixed Neighborhood Selection” (MSN) method in this category. (3) The third type of methods seeks to minimize the joint sparsity of the target precision matrices under matrix inversion constraints. One recent study, named SIMULE (Shared and Individual parts of MULtiple graphs Explicitly) (Wang et al., 2017b), automatically infers both specific edge patterns that are unique to each context and shared interactions preserved among all the contexts (i.e. by modeling each precision matrix as Ω(i) = Ω(i)I + ΩS) via the constrained `1 minimization. Following the CLIME estimator (Pang et al., 2014), the constrained `1 convex formulation can also be solved column by column via linear programming. However, all three categories of aforementioned estimators are difficult to scale up when the dimension p or the number of tasks K are large because they cannot avoid expensive steps like SVD (Danaher et al., 2013) for JGL, linear programming for SIMULE or running multiple iterations of p expensive penalized regressions in MNS. (4) The last category extends the so-called ”Elementary Estimator” graphical model (EE-GM) formulation (Yang et al., 2014b) to revise JGL’s penalized likelihood into a constrained convex program that minimizes (`1 + R′). One proposed estimator FASJEM (Wang et al., 2017a) is solved in an entry-wise manner and group-entry-wise manner that largely outperforms the speed of its JGL counterparts. More details of the related works are in Section (5).\nOne significant caveat of state-of-the-art joint sGGM estimators is the fact that little attention has been paid to incorporating existing knowledge of the nodes or knowledge of the relationships among nodes in the models. In addition to the samples themselves, additional information is widely available in real-world applications. In fact, incorporating the\nknowledge is of great scientific interest. A prime example is when estimating the functional brain connectivity networks among brain regions based on fMRI samples, the spatial position of the regions are readily available. Neuroscientists have gathered considerable knowledge regarding the spatial and anatomical evidence underlying brain connectivity (e.g., short edges and certain anatomical regions are more likely to be connected (Watts & Strogatz, 1998)). Another important example is the problem of identifying gene-gene interactions from patients’ gene expression profiles across multiple cancer types. Learning the statistical dependencies among genes from such heterogeneous datasets can help to understand how such dependencies vary from normal to abnormal and help to discover contributing markers that influence or cause the diseases. Besides the patient samples, state-ofthe-art bio-databases like HPRD (Prasad et al., 2009) have collected a significant amount of information about direct physical interactions among corresponding proteins, regulatory gene pairs or signaling relationships collected from high-qualify bio-experiments.\nAlthough being strong evidence of structural patterns we aim to discover, this type of information has rarely been considered in the joint sGGM formulation of such samples. To the authors’ best knowledge, only one study named as WSIMULE tried to extend the constrained `1 minimization in SIMULE into weighted `1 for considering spatial information of brain regions in the joint discovery of heterogeneous neural connectivity graphs (Singh et al., 2017). This method was designed just for the neuroimaging samples and has O(p5K4) time cost, making it not scalable for large-scale settings (more details in Section 3).\nThis paper aims to fill this gap by adding additional knowledge most effectively into scalable and fast joint sGGM estimations. We propose a novel model, namely Joint Elementary Estimator incorporating additional Knowledge (JEEK), that presents a principled and scalable strategy to include additional knowledge when estimating multiple related sGGMs jointly. Briefly speaking, this paper makes the following contributions:\n• Novel approach: JEEK presents a new way of integrating additional knowledge in learning multi-task sGGMs in a scalable way. (Section 3) • Fast optimization: We optimize JEEK through an entrywise and group-entry-wise manner that can dramatically improve the time complexity to O(p2K4). (Section 3.4) • Convergence rate: We theoretically prove the convergence rate of JEEK asO(log(Kp)/ntot). This rate shows the benefit of joint estimation and achieves the same convergence rate as the state-of-the-art that are much harder to compute. (Section 4) • Evaluation: We evaluate JEEK using several synthetic datasets and two real-world data, one from neuroscience and one from genomics. It outperforms state-of-the-art\nbaselines significantly regarding the speed. (Section 6)\nJEEK provides the flexibility of using (K + 1) different weight matrices representing the extra knowledge. We try to showcase a few possible designs of the weight matrices in Section S:5, including (but not limited to):\n• Spatial or anatomy knowledge about brain regions; • Knowledge of known co-hub nodes or perturbed nodes; • Known group information about nodes, such as genes\nbelonging to the same biological pathway or cellular location; • Using existing known edges as the knowledge, like the known protein interaction databases for discovering gene networks (a semi-supervised setting for such estimations).\nWe sincerely believe the scalability and flexibility provided by JEEK can make structure learning of joint sGGM feasible in many real-world tasks.\nAtt: Due to space limitations, we have put details of certain contents (e.g., proofs) in the appendix. Notations with “S:” as the prefix in the numbering mean the corresponding contents are in the appendix. For example, full proofs are in Section (S:3).\nNotations: math notations we use are described in Section (S:1). ntot = K∑ i=1 ni is the total number of data samples."
  }, {
    "heading": "2 Background",
    "text": "Sparse Gaussian graphical model (sGGM):The classic formulation of estimating sparse Gaussian Graphical model (Yuan & Lin, 2007) from a single given condition (single sGGM) is the “graphical lasso” estimator (GLasso) (Yuan & Lin, 2007; Banerjee et al., 2008). It solves the following `1 penalized maximum likelihood estimation (MLE) problem:\nargmin Ω>0\n− log det(Ω)+ < Ω, Σ̂ > +λn||Ω||1 (2.1)\nM-Estimator with Decomposable Regularizer in High-Dimensional Situations: Recently the seminal study (Negahban et al., 2009) proposed a unified framework for highdimensional analysis of the following general formulation: M-estimators with decomposable regularizers:\nargmin θ L(θ) + λnR(θ) (2.2)\nwhere R(·) represents a decomposable regularization function and L(·) represents a loss function (e.g., the negative log-likelihood function in sGGM L(Ω) = − log det(Ω)+ < Ω, Σ̂ >). Here λn > 0 is the tuning parameter.\nElementary Estimators (EE): Using the analysis framework from (Negahban et al., 2009), recent studies (Yang\net al., 2014a;b;c) propose a new category of estimators named “Elementary estimator” (EE) with the following general formulation:\nargmin θ R(θ) subject to:R∗(θ − θ̂n) ≤ λn (2.3)\nWhereR∗(·) is the dual norm ofR(·),\nR∗(v) := sup u6=0\n< u, v >\nR(u) = sup R(u)≤1 < u, v > . (2.4)\nThe solution of Eq. (2.3) achieves the near optimal convergence rate as Eq. (2.2) when satisfying certain conditions. R(·) represents a decomposable regularization function (e.g., `1-norm) andR∗(·) is the dual norm ofR(·) (e.g., `∞-norm is the dual norm of `1-norm). λn is a regularization parameter.\nThe basic motivation of Eq. (2.3) is to build simpler and possibly fast estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized MLE. θ̂n needs to be carefully constructed, well-defined and closedform for the purpose of simpler computations. The formulation defined by Eq. (2.3) is to ensure its solution having the desired structure defined by R(·). For cases of highdimensional estimation of linear regression models, θ̂n can be the classical ridge estimator that itself is closed-form and with strong statistical convergence guarantees in highdimensional situations.\nEE-sGGM:(Yang et al., 2014b) proposed elementary estimators for graphical models (GM) of exponential families, in which θ̂n represents so-called proxy of backward mapping for the target GM (more details in Section S:4). The key idea (summarized in the upper row of Figure 1) is to investigate the vanilla MLE and where it breaks down for estimating a graphical model of exponential families in the case of high-dimensions (Yang et al., 2014b). Essentially the vanilla graphical model MLE can be expressed as a backward mapping that computes the model parameters from some given moments in an exponential family distribution. For instance, in the case of learning Gaussian GM (GGM) with vanilla MLE, the backward mapping is Σ̂−1 that estimates Ω from the sample covariance matrix (moment) Σ̂. We introduce the details of backward mapping in Section S:4.\nHowever, even though this backward mapping has a simple closed form for GGM, the backward mapping is normally not well-defined in high-dimensional settings. When given the sample covariance Σ̂, we cannot just compute the vanilla MLE solution as [Σ̂]−1 for GGM since Σ̂ is rankdeficient when p > n. Therefore Yang et al. (Yang et al., 2014b) used carefully constructed proxy backward maps as θ̂n = [Tv(Σ̂)] −1 that is both available in closed-form, and\nwell-defined in high-dimensional settings for GGMs. We introduce the details of [Tv(Σ̂)]−1 and its statistical property in Section S:4. Now Eq. (2.3) becomes the following closed-form estimator for learning sparse Gaussian graphical models (Yang et al., 2014b):\nargmin Ω ||Ω||1,,off\nsubject to:||Ω− [Tv(Σ̂)]−1||∞,off ≤ λn (2.5)\nEq. (2.5) is a special case of Eq. (2.3), in whichR(·) is the off-diagonal `1-norm and the precision matrix Ω is the θ we search for. When R(·) is the `1-norm, the solution of Eq. (2.3) (and Eq. (2.5)) just needs to perform entry-wise thresholding operations on θ̂n to ensure the desired sparsity structure of its final solution."
  }, {
    "heading": "3 Proposed Method: JEEK",
    "text": "In applications of Gaussian graphical models, we typically have more information than just the data samples themselves. This paper aims to propose a simple, scalable and theoretically-guaranteed joint estimator for estimating multiple sGGMs with additional knowledge in large-scale situations."
  }, {
    "heading": "3.1 A Joint EE (JEE) Formulation",
    "text": "We first propose to jointly estimate multiple related sGGMs from K data blocks using the following formulation:\nargmin Ω(1),Ω(2),...,Ω(K) K∑ i=1 L(Ω(i)) + λnR(Ω(1),Ω(2), . . . ,Ω(K))\n(3.1)\nwhere Ω(i) denotes the precision matrix for i-th task. L(Ω) = − log det(Ω)+ < Ω, Σ̂ > describes the negative log-likelihood function in sGGM. Ω(i) 0 means that Ω(i) needs to be a positive definite matrix. R(·) represents a decomposable regularization function enforcing sparsity and structure assumptions (details in Section (3.2)).\nFor ease of notation, we denote that Ωtot = (Ω(1),Ω(2), . . . ,Ω(K)) and Σtot = (Σ(1),Σ(2), . . . ,Σ(K)).\nΩtot and Σtot are both p × Kp matrices (i.e., Kp2 parameters to estimate). Now define an inverse function as inv(Atot) := (A(1) −1 , A(2) −1 , . . . , A(K) −1 ), where Atot is a given p ×Kp matrix with the same structure as Σtot. Then we rewrite Eq. (3.1) into the following form:\nargmin Ωtot\nL(Ωtot) + λnR(Ωtot) (3.2)\nNow connecting Eq. (3.2) to Eq. (2.2) and Eq. (2.3), we propose the following joint elementary estimator (JEE) for learning multiple sGGMs:\nargmin Ωtot\nR(Ωtot)\nsubject to: R∗(Ωtot − Ω̂totntot) ≤ λn (3.3)\nThe fundamental component in Eq. (2.3) for the single context sGGM was to use a well-defined proxy function to approximate the vanilla MLE solution (named as the backward mapping for exponential family distributions) (Yang et al., 2014b). The proposed proxy θ̂n = [Tv(Σ̂)]−1 is both well-defined under high-dimensional situations and also has a simple closed-form. Following a similar idea, when learning multiple sGGMs, we propose to use inv(Tv(Σ̂tot)) for Ω̂totntot and get the following joint elementary estimator:\nargmin Ωtot\nR(Ωtot)\nSubject to: R∗(Ωtot − inv(Tv(Σ̂tot))) ≤ λn (3.4)"
  }, {
    "heading": "3.2 Knowledge as Weight (KW-Norm)",
    "text": "The main goal of this paper is to design a principled strategy to incorporate existing knowledge (other than samples or structured assumptions) into the multi-sGGM formulation. We consider two factors in such a design:\n(1) When learning multiple sGGMs jointly from real-world applications, it is often of great scientific interests to model and learn context-specific graph variations explicitly, because such variations can “fingerprint” important markers in domains like cognition (Ideker & Krogan, 2012) or pathology (Kelly et al., 2012). Therefore we design to share parameters between different contexts. Mathematically, we model Ω(i) as two parts:\nΩ(i) = Ω (i) I + ΩS (3.5)\nwhere Ω(i)I is the individual precision matrix for context i and ΩS is the shared precision matrix between contexts. Again, for ease of notation we denote ΩtotI = (Ω\n(1) I ,Ω (2) I , . . . ,Ω (K) I ) and Ω tot S = (ΩS ,ΩS , . . . ,ΩS).\n(2) We represent additional knowledge as positive weight matrices from Rp×p. More specifically, we represent\nthe knowledge of the task-specific graph as weight matrix {W (i)} and WS representing existing knowledge of the shared network. The positive matrix-based representation is a powerful and flexible strategy that can describe many possible forms of existing knowledge. In Section (S:5), we provide a few different designs of {W (i)} and WS for real-world applications. In total, we have weight matrices {W (1)I ,W (2) I , . . . ,W (K) I ,WS} to represent additional knowledge. To simplify notations, we denote W totI = (W (1) I ,W (2), . . . ,W (K) I ) and W tot S = (WS ,WS , . . . ,WS).\nNow we propose the following knowledge as weight norm (kw-norm) combining the above two:\nR(Ωtot) = ||W totI ◦ ΩtotI ||1 + ||W totS ◦ ΩtotS ||1 (3.6)\nHere the Hadamard product ◦ is the element-wise product between two matrices i.e. [A ◦B]ij = AijBij . The kw-norm( Eq. (3.6)) has the following three properties:\n• (i) kw-norm is a norm function if and only if any entries in W totI and W tot S do not equal to 0. • (ii) If the condition in (i) holds, kw-norm is a decomposable norm. • (iii) If the condition in (i) holds, the dual norm of kwnorm isR∗(u) = max(||W totI ◦ u||∞, ||W totS ◦ u||∞).\nSection S:3.1 provides proofs of the above claims."
  }, {
    "heading": "3.3 JEE with Knowledge (JEEK)",
    "text": "Plugging Eq. (3.6) to Eq. (3.4), we obtain the following formulation of JEEK for learning multiple related sGGMs from heterogereous samples:\nargmin ΩtotI ,Ω tot S\n||W totI ◦ ΩtotI ||1 + ||W totS ◦ ΩtotS ||\nSubject to: ||W totI ◦ (Ωtot − inv(Tv(Σ̂tot)))||∞ ≤ λn ||W totS ◦ (Ωtot − inv(Tv(Σ̂tot)))||∞ ≤ λn Ωtot = ΩtotS + Ω tot I\n(3.7)\nIn Section 4, we theoretically prove that the statistical convergence rate of JEEK achieves the same sharp convergence rate as the state-of-the-art estimators for multi-task sGGMs. Our proofs are inspired by the unified framework of the high-dimensional statistics (Negahban et al., 2009)."
  }, {
    "heading": "3.4 Solution of JEEK:",
    "text": "A huge computational advantage of JEEK (Eq. (3.7)) is that it can be decomposed into p × p independent small linear programming problems. To simplify notations, we denote Ω(i)I j,k (the {j, k}-th entry of Ω (i)) as ai. Similarly\nAlgorithm 1. Joint Elementary Estimator with additional knowledge (JEEK) for Multi-task sGGMs Input: Data sample matrix X(i) ( i = 1 toK), regularization hyperparameter λn, Knowledge weight matrices {W (i)I ,WS} and LP(.) (a linear programming solver) Output: {Ω(i)} ( i = 1 toK)\n1: for i = 1 toK do 2: Initialize Σ̂(i) = 1ni−1 ∑ni s=1(X (i) s, −µ̂ (i))(X(i)s, −µ̂ (i))T (the sample\ncovariance matrix of X(i)) 3: Initialize Ω(i) = 0p×p 4: Calculate the proxy backward mapping [Tv(Σ̂(i))]−1 5: end for 6: for j = 1 to p do 7: for k = 1 to j do 8: ci = [Tv(Σ̂(i))]−1j,k 9: wi = W (i)j,k 10: ws = WSj,k 11: ai, b = LP(wi, ws, ci, λn) where i = 1, . . . , K and LP(.) solves Eq. (3.8) 12: for i = 1 toK do 13: Ω(i)j,k = Ω(i)k,j = ai + b 14: Ω(i)I j,k = ai 15: ΩSj,k = b 16: end for 17: end for 18: end for\nwe denote ΩSj,k as b and [Tv(Σ̂(i))] −1 j,k be ci. Similarly we denote W (i)j,k = wi and W S j,k = ws. ”A group of entries” means a set of parameters {a1, . . . , aK , b} for certain j, k. In order to estimate {a1, . . . , aK , b}, JEEK (Eq. (3.7)) can be decomposed into the following formulation for a certain j, k :\nargmin ai,b ∑ i |wiai|+K|wsb|\nSubject to: |ai + b− ci| ≤ λn\nmin(wi, ws) ,\ni = 1, . . . ,K\n(3.8)\nEq. (3.8) can be easily converted into a linear programming form of Eq. (S:1–1) with only K + 1 variables. The time complexity of Eq. (3.8) is O(K4). Considering JEEK has a total p(p − 1)/2 of such subproblems to solve, the computational complexity of JEEK (Eq. (3.7)) is therefore O(p2K4). We summarize the optimization algorithm of JEEK in Algorithm 1 (details in Section (S:1.2))."
  }, {
    "heading": "4 Theoretical Analysis",
    "text": "KW-Norm:We presented the three properties of kw-norm in Section 3.2. The proofs of these three properties are included in Section (S:3.1).\nTheoretical error bounds of Proxy Backward Mapping: (Yang et al., 2014b) proved that when (p ≥ n), the proxy backward mapping [Tv(Σ̂)]−1 used by EE-sGGM achieves the sharp convergence rate to its truth (i.e., by proving\n||Tv(Σ̂))−1 − Σ∗−1||∞ = O( √ log p n )). The proof was extended from the previous study (Rothman et al., 2009) that\ndevised Tv(Σ̂) for estimating covariance matrix consistently in high-dimensional situations. See detailed proofs in Section S:4.3. To derive the statistical error bound of JEEK, we need to assume that inv(Tv(Σ̂tot)) are well-defined. This is ensured by assuming that the true Ω(i) ∗ satisfy the conditions defined in Section (S:3.1).\nTheoretical error bounds of JEEK:We now use the highdimensional analysis framework from (Negahban et al., 2009), three properties of kw-norm, and error bounds of backward mapping from (Rothman et al., 2009; Yang et al., 2014b) to derive the statistical convergence rates of JEEK. Detailed proofs of the following theorems are in Section 4 .\nBefore providing the theorem, we need to define the structural assumption, the IS-Sparsity, we assume for the parameter truth. (IS-Sparsity): The ’true’ parameter of Ωtot∗ can be decomposed into two clear structures–{ΩtotI ∗ and ΩtotS ∗}. ΩtotI ∗ is exactly sparse with ki non-zero entries indexed by a support set SI and ΩtotS ∗ is exactly sparse with ks\nnon-zero entries indexed by a support set SS . SI ⋂ SS = ∅.\nAll other elements equal to 0 (in (SI ⋃ SS) c).\nTheorem 4.1. Consider Ωtot whose true parameter Ωtot∗ satisfies the (IS-Sparsity) assumption. Suppose we compute the solution of Eq. (3.7) with a bounded λn such that λn ≥ max(||W totI ◦ (Ωtot\n∗− inv(Tv(Σ̂tot)))||∞, ||W totS ◦ (Ωtot\n∗ − inv(Tv(Σ̂tot)))||∞), then the estimated solution Ω̂tot satisfies the following error bounds:\n||Ω̂tot − Ωtot∗||F ≤ 4 √ ki + ksλn max(||W totI ◦ (Ω̂tot − Ωtot ∗ )||∞, ||W totS ◦ (Ω̂tot − Ωtot\n∗||∞) ≤ 2λn\n||W totI ◦ (Ω̂totI − ΩtotI ∗ )||1 + ||W totS ◦ (Ω̂totS − ΩtotS ∗ )||1\n≤ 8(ki + ks)λn (4.1)\nProof. See detailed proof in Section S:3.2\nTheorem (4.1) provides a general bound for any selection of λn. The bound of λn is controlled by the distance between Ωtot\n∗ and inv(Tv(Σ̂tot)). We then extend Theorem (4.1) to derive the statistical convergence rate of JEEK. This gives us the following corollary: Corollary 4.2. Suppose the high-dimensional setting, i.e., p > max(ni). Let v := a √ log(Kp) ntot . Then for\nλn := 8κ1a κ2 √ log(Kp) ntot\nand ntot > c logKp, with a probability of at least 1− 2C1 exp(−C2Kp log(Kp)), the estimated optimal solution Ω̂tot has the following error bound:\n||Ω̂tot−Ωtot∗||F\n≤ 16κ1amax j,k (W totI j,k,W tot S j,k)\nκ2\n√ (ki + ks) log(Kp)\nntot (4.2)\nwhere a, c, κ1 and κ2 are constants.\nProof. See detailed proof in Section S:3.2.2 (especially from Eq. (S:3–11) to Eq. (S:3–19)).\nBayesian View of JEEK:In Section (S:2) we provide a direct Bayesian interpretation of JEEK through the perspective of hierarchical Bayesian modeling. Our hierarchical Bayesian interpretation nicely explains the assumptions we make in JEEK."
  }, {
    "heading": "5 Connecting to Relevant Studies",
    "text": "JEEK is closely related to a few state-of-the-art studies summarized in Table 1. We compare the time complexity and functional properties of JEEK versus these studies.\nNAK: (Bu & Lederer, 2017)For the single task sGGM, one recent study (Bu & Lederer, 2017) (following ideas from (Shimamura et al., 2007)) proposed to integrating Additional Knowledge (NAK)into estimation of graphical models through a weighted Neighbourhood selection formulation (NAK) as: β̂j = argmin\nβ,βj=0\n1 2 ||X j−Xβ||22 + ||rj ◦β||1.\nNAK is designed for estimating brain connectivity networks from homogeneous samples and incorporate distance knowledge as weight vectors. 1 In experiments, we compare JEEK to NAK (by running NAK R package K times) on multiple synthetic datasets of simulated samples about brain regions. The data simulation strategy was suggested by (Bu & Lederer, 2017). Same as the NAK (Bu & Lederer, 2017), we use the spatial distance among brain regions as additional knowledge in JEEK.\nW-SIMULE: (Singh et al., 2017)Like JEEK, one recent study (Singh et al., 2017) of multi-sGGMs (following ideas from (Wang et al., 2017b)) also assumed that Ω(i) = Ω\n(i) I + ΩS and incorporated spatial distance knowl-\nedge in their convex formulation for joint discovery of heterogeneous neural connectivity graphs. This study, with name W-SIMULE (Weighted model for Shared and Individual parts of MULtiple graphs Explicitly) uses a weighted constrained `1 minimization:\nargmin Ω (i) I ,ΩS\n∑ i ||W ◦ Ω(i)I ||1 + K||W ◦ ΩS ||1 (5.1)\nSubject to: ||Σ(i)(Ω(i)I + ΩS)− I||∞ ≤ λn, i = 1, . . . , K\n1Here β̂j indicates the sparsity of j-th column of a single Ω̂. Namely, β̂jk = 0 if and only if Ω̂k,j = 0. rj is a weight vector as the additional knowledge The NAK formulation can be solved by a classic Lasso solver like glmnet.\nW-SIMULE simply includes the additional knowledge as a weight matrix W . 2\nDifferent from W-SIMULE, JEEK separates the knowledge of individual context and the shared using different weight matrices. While W-SIMULE also minimizes a weighted `1 norm, its constraint optimization term is entirely different from JEEK. The formulation difference makes the optimization of JEEK much faster and more scalable than WSIMULE (Section (6)). We have provided a complete theoretical analysis of error bounds of JEEK, while W-SIMULE provided no theoretical results. Empirically, we compare JEEK with W-SIMULE R package from (Singh et al., 2017) in the experiments.\nJGL: (Danaher et al., 2013): Regularized MLE based multi-sGGMs Studies mostly follow the so called joint graphical lasso (JGL) formulation as Eq. (5.2):\nargmin Ω(i) 0 K∑ i=1 (−L(Ω(i)) + λn K∑ i=1 ||Ω(i)||1\n+ λ ′ nR ′ (Ω (1) ,Ω (2) , . . . ,Ω (K) )\n(5.2)\nR′(·) is the second penalty function for enforcing some structural assumption of group property among the multiple graphs. One caveat of JGL is that R′(·) cannot model explicit additional knowledge. For instance,it can not incorporate the information of a few known hub nodes shared by the contexts. In experiments, we compare JEEK to JGLco-hub and JGL-perturb-hub toolbox provided by (Mohan et al., 2013).\nFASJEM: (Wang et al., 2017a) One very recent study extended JGL using so-called Elementary superpositionstructured moment estimator formulation as Eq. (5.3):\nargmin Ωtot\n||Ωtot||1 + R′(Ωtot)\ns.t.||Ωtot − inv(Tv(Σ̂tot))||∞ ≤ λn R′∗(Ωtot − inv(Tv(Σ̂tot))) ≤ λn\n(5.3)\nFASJEM is much faster and more scalable than the JGL estimators. However like JGL estimators it can not model additional knowledge and its optimization needs to be carefully re-designed for differentR′(·). 3\n2It can be solved by any linear programming solver and can be column-wise paralleled. However, it is very slow when p > 200 due to the expensive computation cost O(K4p5).\n3FASJEM extends JGL into multiple independent group-entry wise optimization just like JEEK. HereR\n′∗(·) is the dual norm of R′(·). Because (Wang et al., 2017a) only designs the optimization of two cases (group,2 and group,inf), we can not use it as a baseline.\nBoth NAK and W-SIMULE only explored the formulation for estimating neural connectivity graphs using spatial information as additional knowledge. Differently our experiments (Section (6)) extend the weight-as-knowledge formulation on weights as distance, as shared hub knowledge, as perturbed hub knowledge, and as nodes’ grouping information (e.g., multiple genes are known to be in the same pathway). This has largely extends the previous studies in showing the real-world adaptivity of the proposed formulation. JEEK elegantly formulates existing knowledge based on the problem at hand and avoid the need to design knowledge-specific optimization."
  }, {
    "heading": "6 Experiments",
    "text": "We empirically evaluate JEEK and baselines on four types of datasets, including two groups of synthetic data, one realworld fMRI dataset for brain connectivity estimation and one real-world genomics dataset for estimating interaction among regulatory genes (results in Section (6.2)). In order to incorporating various types of knowledge, we provide five different designs of the weight matrices in Section S:5. Details of experimental setup, metrics and hyper-parameter tuning are included in Section (S:6.1). Baselines used in our experiments have been explained in details by Section (5). We also use JEEK with no additional knowledge (JEEKNK) as a baseline.\nJEEK is available as the R package ’jeek’ in CRAN."
  }, {
    "heading": "6.1 Experiment: Simulated Samples with Known Hubs as Knowledge",
    "text": "Inspired the JGL-co-hub and JGL-perturb-hub toolbox (JGL-node) provided by (Mohan et al., 2013), we empirically show JEEK’s ability to model known co-hub or perturbed-hub nodes as knowledge when estimating multiple sGGMs. We generate multiple simulated Gaussian datasets through the random graph model (Rothman et al., 2008) to simulate both the co-hub and perturbed-hub graph structures (details in S:7.1). We use JGL-node package, W-SIMULE and JEEK-NK as baselines for this set of experiments. The weights in {W totI ,W totS } are designed using the strategy proposed in Section (S:5).\nWe use AUC score (to reflect the consistency and variance of a method’s performance when varying its important hyperparameter) and computational time cost to compare JEEK with baselines. We compare all methods on many simulated cases by varying p from the set {100, 200, 300, 400, 500}"
  }, {
    "heading": "100 200 300 400 500",
    "text": "and the number of tasks K from the set {2, 3, 4}. In Figure 2 and Figure S:1(a)(b), JEEK consistently achieves higher AUC-scores than the baselines JGL, JEEK-NK and W-SIMULE for all cases. JEEK is more than 10 times faster than the baselines on average. In Figure 2, for each p > 300 case (with n = p/2), W-SIMULE takes more than one month and JGL takes more than one day. Therefore we can not show them with p > 300."
  }, {
    "heading": "6.2 Experiment: Gene Interaction Network from Real-World Genomics Data",
    "text": "Next, we apply JEEK and the baselines on one real-world biomedical data about gene expression profiles across two different cell types. We explored two different types of knowledge: (1) Known edges and (2) Known group about genes. Figure S:1(c) shows that JEEK has lower time cost and recovers more interactions than baselines (higher number of matched edges to the existing bio-databases.). More results are in Appendix Section (S:7.2) and the design of weight matrices for this case is in Section (S:5)."
  }, {
    "heading": "6.3 Experiment: Simulated Data about Brain Connectivity with Distance as Knowledge",
    "text": "Following (Bu & Lederer, 2017), we use one known Euclidean distance between human brain regions as additional knowledge W and use it to generate multiple simulated datasets (details in Section S:7.3). We compare JEEK with the baselines regarding (a) Scalability (computational time cost), and (b) effectiveness (F1-score, because NAK package does not allow AUC calculation). For each simulation case, the computation time for each estimator is the summation of a method’s execution time over all values of λn. Figure S:2(a)(b) show clearly that JEEK outperforms its baselines. JEEK has a consistently higher F1-Score and is almost 6 times faster than W-SIMULE in the high dimensional case. JEEK performs better than JEEK-NK, confirming the advantage of integrating additional distance knowledge. While NAK is fast, its F1-Score is nearly 0 and hence, not useful for multi-sGGM structure learning."
  }, {
    "heading": "6.4 Experiment: Functional Connectivity Estimation from Real-World Brain fMRI Data",
    "text": "We evaluate JEEK and relevant baselines for a classification task on one real-world publicly available resting-state fMRI dataset: ABIDE(Di Martino et al., 2014). The ABIDE data aims to understand human brain connectivity and how it reflects neural disorders (Van Essen et al., 2013). ABIDE includes two groups of human subjects: autism and control, and therefore we formulate it as K = 2 graph estimation. We utilize the spatial distance between human brain regions as additional knowledge for estimating functional connectivity edges among brain regions. We use Linear Discriminant Analysis (LDA) for a downstream classification task aiming to assess the ability of a graph estimator to learn the differential patterns of the connectome structures. (Details of the ABIDE dataset, baselines, design of the additional knowledge W matrix, cross-validation and LDA classification method are in Section (S:7.4).)\nFigure S:2(c) compares JEEK and three baselines: JEEKNK, W-SIMULE and W-SIMULE with no additional knowledge (W-SIMULE-NK). JEEK yields a classification accuracy of 58.62% for distinguishing the autism subjects versus the control subjects, clearly outperforming JEEK-NK and W-SIMULE-NK. JEEK is roughly 7 times faster than the W-SIMULE estimators, locating at the top left region in Figure S:2(c) (higher classification accuracy and lower time cost). We also experimented with variations of theW matrix and found the classification results are fairly robust to the variations of W (Section (S:7.4))."
  }, {
    "heading": "7 Conclusions",
    "text": "We propose a novel method, JEEK, to incorporate additional knowledge in estimating multi-sGGMs. JEEK achieves the same asymptotic convergence rate as the state-of-the-art. Our experiments has showcased using weights for describing pairwise knowledge among brain regions, for shared hub knowledge, for perturbed hub knowledge, for describing group information among nodes (e.g., genes known to be in the same pathway), and for using known interaction edges as the knowledge."
  }],
  "year": 2018,
  "references": [{
    "title": "Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data",
    "authors": ["O. Banerjee", "L. El Ghaoui", "A. d’Aspremont"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "Integrating additional knowledge into estimation of graphical models",
    "authors": ["Y. Bu", "J. Lederer"],
    "venue": "arXiv preprint arXiv:1704.02739,",
    "year": 2017
  }, {
    "title": "Multitask learning",
    "authors": ["R. Caruana"],
    "venue": "Machine learning,",
    "year": 1997
  }, {
    "title": "Inferring multiple graphical structures",
    "authors": ["J. Chiquet", "Y. Grandvalet", "C. Ambroise"],
    "venue": "Statistics and Computing,",
    "year": 2011
  }, {
    "title": "An integrated encyclopedia of dna elements in the human genome",
    "authors": ["Consortium", "E. P"],
    "year": 2012
  }, {
    "title": "The joint graphical lasso for inverse covariance estimation across multiple classes",
    "authors": ["P. Danaher", "P. Wang", "D.M. Witten"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2013
  }, {
    "title": "Joint estimation of multiple graphical models",
    "authors": ["J. Guo", "E. Levina", "G. Michailidis", "J. Zhu"],
    "venue": "Biometrika, pp. asq060,",
    "year": 2011
  }, {
    "title": "Multi-task learning of gaussian graphical models",
    "authors": ["J. Honorio", "D. Samaras"],
    "venue": "In Proceedings of the 27th International Conference on Machine Learning",
    "year": 2010
  }, {
    "title": "Differential network biology",
    "authors": ["T. Ideker", "N.J. Krogan"],
    "venue": "Molecular systems biology,",
    "year": 2012
  }, {
    "title": "Characterizing variation in the functional connectome: promise and pitfalls",
    "authors": ["C. Kelly", "B.B. Biswal", "R.C. Craddock", "F.X. Castellanos", "M.P. Milham"],
    "venue": "Trends in cognitive sciences,",
    "year": 2012
  }, {
    "title": "Node-based learning of multiple gaussian graphical models",
    "authors": ["K. Mohan", "P. London", "M. Fazel", "Lee", "S.-I", "D. Witten"],
    "venue": "arXiv preprint arXiv:1303.5145,",
    "year": 2013
  }, {
    "title": "Learning population and subject-specific brain connectivity networks via mixed neighborhood selection",
    "authors": ["R.P. Monti", "C. Anagnostopoulos", "G. Montana"],
    "venue": "arXiv preprint arXiv:1512.01947,",
    "year": 2015
  }, {
    "title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
    "authors": ["S. Negahban", "B. Yu", "M.J. Wainwright", "P.K. Ravikumar"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Integrated genomic analyses of ovarian carcinoma",
    "authors": ["Network", "C.G.A. R"],
    "venue": "Nature, 474(7353):609–615,",
    "year": 2011
  }, {
    "title": "The fastclime package for linear programming and large-scale precision matrix estimation in r",
    "authors": ["H. Pang", "H. Liu", "R. Vanderbei"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Toward open sharing of task-based fmri data: the openfmri project",
    "authors": ["R.A. Poldrack", "D.M. Barch", "J. Mitchell", "T. Wager", "A.D. Wagner", "J.T. Devlin", "C. Cumba", "O. Koyejo", "M. Milham"],
    "venue": "Frontiers in neuroinformatics,",
    "year": 2013
  }, {
    "title": "Human protein reference database?2009 update",
    "authors": ["T.K. Prasad", "R. Goel", "K. Kandasamy", "S. Keerthikumar", "S. Kumar", "S. Mathivanan", "D. Telikicherla", "R. Raju", "B. Shafreen", "A Venugopal"],
    "venue": "Nucleic acids research,",
    "year": 2009
  }, {
    "title": "Sparse permutation invariant covariance estimation",
    "authors": ["A.J. Rothman", "P.J. Bickel", "E. Levina", "J Zhu"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2008
  }, {
    "title": "Generalized thresholding of large covariance matrices",
    "authors": ["A.J. Rothman", "E. Levina", "J. Zhu"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2009
  }, {
    "title": "Weighted lasso in graphical gaussian modeling for large gene network estimation based on microarray data",
    "authors": ["T. Shimamura", "S. Imoto", "R. Yamaguchi", "S. Miyano"],
    "venue": "In Genome Informatics",
    "year": 2007
  }, {
    "title": "A constrained, weightedl1 minimization approach for joint discovery of heterogeneous neural connectivity graphs",
    "authors": ["C. Singh", "B. Wang", "Y. Qi"],
    "venue": "arXiv preprint arXiv:1709.04090,",
    "year": 2017
  }, {
    "title": "Salience network–based classification and prediction of symptom severity in children with autism",
    "authors": ["L.Q. Uddin", "K. Supekar", "C.J. Lynch", "A. Khouzam", "J. Phillips", "C. Feinstein", "S. Ryali", "V. Menon"],
    "venue": "JAMA psychiatry,",
    "year": 2013
  }, {
    "title": "A fast and scalable joint estimator for learning multiple related sparse gaussian graphical models",
    "authors": ["B. Wang", "J. Gao", "Y. Qi"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "A constrained l1 minimization approach for estimating multiple sparse gaussian or nonparanormal graphical models",
    "authors": ["B. Wang", "R. Singh", "Y. Qi"],
    "venue": "Machine Learning,",
    "year": 2017
  }, {
    "title": "Collective dynamics of ‘small-world’networks",
    "authors": ["D.J. Watts", "S.H. Strogatz"],
    "year": 1998
  }, {
    "title": "Elementary estimators for high-dimensional linear regression",
    "authors": ["E. Yang", "A. Lozano", "P. Ravikumar"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Elementary estimators for graphical models",
    "authors": ["E. Yang", "A.C. Lozano", "P. Ravikumar"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Elementary estimators for sparse covariance matrices and other structured moments",
    "authors": ["E. Yang", "A.C. Lozano", "P. Ravikumar"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Model selection and estimation in the gaussian graphical model",
    "authors": ["M. Yuan", "Y. Lin"],
    "year": 2007
  }, {
    "title": "Learning structural changes of gaussian graphical models in controlled experiments",
    "authors": ["B. Zhang", "Y. Wang"],
    "venue": "arXiv preprint arXiv:1203.3532,",
    "year": 2012
  }, {
    "title": "Learning multiple tasks with a sparse matrix-normal penalty",
    "authors": ["Y. Zhang", "J.G. Schneider"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Structural pursuit over multiple undirected graphs",
    "authors": ["Y. Zhu", "X. Shen", "W. Pan"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2014
  }],
  "id": "SP:4d9580d2e2afbac534cbd86b9dbdc31a9c3f4940",
  "authors": [{
    "name": "Beilun Wang",
    "affiliations": []
  }, {
    "name": "Arshdeep Sekhon",
    "affiliations": []
  }, {
    "name": "Yanjun Qi",
    "affiliations": []
  }],
  "abstractText": "We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large K) under a highdimensional (large p) situation. In this paper, we propose a novel Joint Elementary Estimator incorporating additional Knowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledgespecific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(pK) to O(pK). We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot) as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and two real-world data, JEEK outperforms the speed of the state-ofarts significantly while achieving the same level of prediction accuracy. Department of Computer Science, University of Virginia, http://www.jointnets.org/ . Correspondence to: Beilun Wang <bw4mw@virginia.edu>, Yanjun Qi <yanjun@virginia.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",
  "title": "A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models"
}