{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2016 Conference, pages 31–41, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Irony and sarcasm in dialogue constitute a highly creative use of language signaled by a large range of situational, semantic, pragmatic and lexical cues. Previous work draws attention to the use of both hyperbole and rhetorical questions in conversation as distinct types of lexico-syntactic cues defining diverse classes of sarcasm (Gibbs, 2000).\nTheoretical models posit that a single semantic basis underlies sarcasm’s diversity of form, namely “a contrast” between expected and experienced events, giving rise to a contrast between what is said and a literal description of the actual situation (Colston and O’Brien, 2000; Partington, 2007). This semantic characterization has not been straightforward to operationalize computationally for sarcasm in dialogue. Riloff et al.\n(2013) operationalize this notion for sarcasm in tweets, achieving good results. Joshi et al. (2015) develop several incongruity features to capture it, but although they improve performance on tweets, their features do not yield improvements for dialogue.\nPrevious work on the Internet Argument Corpus (IAC) 1.0 dataset aimed to develop a highprecision classifier for sarcasm in order to bootstrap a much larger corpus (Lukin and Walker, 2013), but was only able to obtain a precision of just 0.62, with a best F of 0.57, not high enough for bootstrapping (Riloff and Wiebe, 2003; Thelen and Riloff, 2002). Justo et al. (2014) experimented with the same corpus, using supervised learning, and achieved a best precision of 0.66 and a best F of 0.70. Joshi et al. (2015)’s explicit congruity features achieve precision around 0.70 and best F of 0.64 on a subset of IAC 1.0.\nWe decided that we need a larger and more diverse corpus of sarcasm in dialogue. It is difficult to efficiently gather sarcastic data, because only about 12% of the utterances in written online debate forums dialogue are sarcastic (Walker et al., 2012a), and it is difficult to achieve high reliability for sarcasm annotation (Filatova, 2012; Swanson et al., 2014; González-Ibáñez et al., 2011; Wallace et al., 2014). Thus, our contributions are:\n• We develop a new larger corpus, using several methods that filter non-sarcastic utterances to skew the distribution toward/in favor of sarcastic utterances. We put filtered data out for annotation, and are able to achieve high annotation reliability.\n• We present a novel operationalization of both rhetorical questions and hyperbole to develop subcorpora to explore the differences between them and general sarcasm.\n• We show that our new corpus is of high quality by applying supervised machine learning with simple features to explore how different\n31\ncorpus properties affect classification results. We achieve a highest precision of 0.73 and a highest F of 0.74 on the new corpus with basic n-gram and Word2Vec features, showcasing the quality of the corpus, and improving on previous work.\n• We apply a weakly-supervised learner to characterize linguistic patterns in each corpus, and describe the differences across generic sarcasm, rhetorical questions and hyperbole in terms of the patterns learned.\n• We show for the first time that it is straightforward to develop very high precision classifiers for NOT-SARCASTIC utterances across our rhetorical questions and hyperbole subtypes, due to the nature of these utterances in debate forum dialogue."
  }, {
    "heading": "2 Creating a Diverse Sarcasm Corpus",
    "text": "There has been relatively little theoretical work on sarcasm in dialogue that has had access to a large corpus of naturally occurring examples. Gibbs (2000) analyzes a corpus of 62 conversations between friends and argues that a robust theory of verbal irony must account for the large diversity in form. He defines several subtypes, including rhetorical questions and hyperbole:\n• Rhetorical Questions: asking a question that implies a humorous or critical assertion • Hyperbole: expressing a non-literal meaning\nby exaggerating the reality of a situation\nOther categories of irony defined by Gibbs (2000) include understatements, jocularity, and sarcasm (which he defines as a critical/mocking form of irony). Other work has also tackled jocularity and humor, using different approaches for data aggregation, including filtering by Twitter hashtags, or analyzing laugh-tracks from recordings (Reyes et al., 2012; Bertero and Fung, 2016).\nPrevious work has not, however, attempted to operationalize these subtypes in any concrete way. Here we describe our methods for creating a corpus for generic sarcasm (Gen) (Sec. 2.1), rhetorical questions (RQ), and hyperbole (Hyp) (Sec. 2.2) using data from the Internet Argument Corpus (IAC 2.0).1 Table 1 provides examples of SARCASTIC and NOT-SARCASTIC posts from the corpus we create. Table 2 summarizes the final composition of our sarcasm corpus.\n1The IAC 2.0 is available at https://nlds.soe.ucsc.edu/iac2, and our sarcasm corpus will be released at https://nlds.soe.ucsc.edu/sarcasm2."
  }, {
    "heading": "2.1 Generic Dataset (Gen)",
    "text": "We first replicated the pattern-extraction experiments of Lukin and Walker (2013) on their dataset using AutoSlog-TS (Riloff, 1996), a weaklysupervised pattern learner that extracts lexicosyntactic patterns associated with the input data. We set up the learner to extract patterns for both SARCASTIC and NOT-SARCASTIC utterances. Our first discovery is that we can classify NOTSARCASTIC posts with very high precision, ranging between 80-90%.2\nBecause our main goal is to build a larger, more diverse corpus of sarcasm, we use the highprecision NOT-SARCASTIC patterns extracted by AutoSlog-TS to create a “not-sarcastic” filter. We did this by randomly selecting a new set of 30K posts (restricting to posts with between 10 and 150 words) from IAC 2.0 (Abbott et al., 2016), and applying the high-precision NOT-SARCASTIC\n2We delay a detailed discussion of the characteristics of this NOT-SARCASTIC classifier, and the patterns that we learn, until Sec. 4 where we describe AutoSlog-TS and the linguistic characteristics of the whole corpus.\npatterns from AutoSlog-TS to filter out any posts that contain at least one NOT-SARCASTIC cue. We end up filtering out two-thirds of the pool, only keeping posts that did not contain any of our highprecision NOT-SARCASTIC cues. We acknowledge that this may also filter out sarcastic posts, but we expect it to increase the ratio of sarcastic posts in the remaining pool.\nWe put out the remaining 11,040 posts on Mechanical Turk. As in Lukin and Walker (2013), we present the posts in “quote-response” pairs, where the response post to be annotated is presented in the context of its “dialogic parent”, another post earlier in the thread, or a quote from another post earlier in the thread (Walker et al., 2012b). In the task instructions, annotators are presented with a definition of sarcasm, followed by one example of a quote-response pair that clearly contains sarcasm, and one pair that clearly does not. Each task consists of 20 quote-response pairs that follow the instructions. Figure 1 shows the instructions and layout of a single quote-response pair presented to annotators. As in Lukin and Walker (2013) and Walker et al. (2012b), annotators are asked a binary question: Is any part of the response to this quote sarcastic?.\nTo help filter out unreliable annotators, we create a qualifier consisting of a set of 20 manuallyselected quote-response pairs (10 that should receive a SARCASTIC label and 10 that should receive a NOT-SARCASTIC label). A Turker must pass the qualifier with a score above 70% to participate in our sarcasm annotations tasks.\nOur baseline ratio of sarcasm in online debate forums dialogue is the estimated 12% sarcastic posts in the IAC, which was found previously by Walker et al. by gathering annotations for sarcasm, agreement, emotional language, attacks, and nastiness from a subset of around 20K posts from the IAC across various topics (Walker et al., 2012a). Similarly, in his study of recorded conversation among friends, Gibbs cites 8% sarcastic utterances among all conversational turns (Gibbs, 2000).\nWe choose a conservative threshold: a post is only added to the sarcastic set if at least 6 out of 9 annotators labeled it sarcastic. Of the 11,040 posts we put out for annotation, we thus obtain 2,220 new posts, giving us a ratio of about 20% sarcasm – significantly higher than our baseline of 12%. We choose this conservative threshold to ensure the quality of our annotations, and we leave aside posts that 5 out of 9 annotators label as sarcastic for future work – noting that we can get even higher ratios of sarcasm by including them (up to 31%). The percentage agreement between\neach annotator and the majority vote is 80%. We then expand this set, using only 3 highlyreliable Turkers (based on our first round of annotations), giving them an exclusive sarcasm qualification to do additional HITs. We gain an additional 1,040 posts for each class when using majority agreement (at least 2 out of 3 sarcasm labels) for the additional set (to add to the 2,220 original posts). The average percent agreement with the majority vote is 89% for these three annotators. We supplement our sarcastic data with 2,360 notsarcastic posts from the original data by (Lukin and Walker, 2013) that follow our 150-word length restriction, and complete the set with 900 posts that were filtered out by our NOT-SARCASTIC filter3 – resulting in a total of 3,260 posts per class (6,520 total posts).\nRows 1 and 2 of Table 1 show examples of posts that are labeled sarcastic in our final generic sarcasm set. Using our filtering method, we are able to reduce the number of posts annotated from our original 30K to around 11K, achieving a percentage of 20% sarcastic posts, even though we choose\n3We use these unbiased not-sarcastic data sources to avoid using posts coming from the sarcasm-skewed distribution.\nto use a conservative threshold of at least 6 out of 9 sarcasm labels. Since the number of posts being annotated is only a third of the original set size, this method reduces annotation effort, time, and cost, and helps us shift the distribution of sarcasm to more efficiently expand our dataset than would otherwise be possible."
  }, {
    "heading": "2.2 Rhetorical Questions and Hyperbole",
    "text": "The goal of collecting additional corpora for rhetorical questions and hyperbole is to increase the diversity of the corpus, and to allow us to explore the semantic differences between SARCASTIC and NOT-SARCASTIC utterances when particular lexico-syntactic cues are held constant. We hypothesize that identifying surface-level cues that are instantiated in both sarcastic and not sarcastic posts will force learning models to find deeper semantic cues to distinguish between the classes.\nUsing a combination of findings in the theoretical literature, and observations of sarcasm patterns in our generic set, we developed a regex pattern matcher that runs against the 400K unannotated posts in the IAC 2.0 database and retrieves matching posts, only pulling posts that have parent posts and a maximum of 150 words. Table 3 only shows a small subset of the “more successful” regex patterns we defined for each class.\nCue annotation experiments. After running a large number of retrieval experiments with our regex pattern matcher, we select batches of the resulting posts that mix different cue classes to put out for annotation, in such a way as to not allow the annotators to determine what regex cues were used. We then successively put out various batches for annotation by 5 of our highly-qualified annotators, in order to determine what percentage of\nposts with these cues are sarcastic. Table 3 summarizes the results for a sample set of cues, showing the number of posts found containing the cue, the subset that we put out for annotation, and the percentage of posts labeled sarcastic in the annotation experiments. For example, for the hyperbolic cue “wow”, 977 utterances with the cue were found, 153 were annotated, and 44% of those were found to be sarcastic (i.e. 56% were found to be not-sarcastic). Posts with the cue “oh wait” had the highest sarcasm ratio, at 87%. It is the distinction between the sarcastic and notsarcastic instances that we are specifically interested in. We describe the corpus collection process for each subclass below.\nIt is important to note that using particular cues (regex) to retrieve sarcastic posts does not result in posts whose only cue is the regex pattern. We demonstrate this quantitatively in Sec. 4. Sarcasm is characterized by multiple lexical and morphosyntactic cues: these include the use of intensifiers, elongated words, quotations, false politeness, negative evaluations, emoticons, and tag questions inter alia. Table 4 shows how sarcastic utterances often contain combinations of multiple indicators, each playing a role in the overall sarcastic tone of the post.\nRhetorical Questions. There is no previous work on distinguishing sarcastic from non-sarcastic uses of rhetorical questions (RQs). RQs are syntactically formulated as a question, but function as an indirect assertion (Frank, 1990). The polarity of the question implies an assertion of the opposite polarity, e.g. Can you read? implies You can’t read. RQs are prevalent in persuasive discourse, and are frequently used ironically (Schaffer, 2005; Ilie, 1994; Gibbs, 2000). Previous work focuses on their formal semantic properties (Han, 1997), or distinguishing RQs from standard questions (Bhattasali et al., 2015).\nWe hypothesized that we could find RQs in abundance by searching for questions in the middle of a post, that are followed by a statement, using the assumption that questions followed by a statement are unlikely to be standard information-\nseeking questions. We test this assumption by randomly extracting 100 potential RQs as per our definition and putting them out on Mechanical Turk to 3 annotators, asking them whether or not the questions (displayed with their following statement) were rhetorical. According to majority vote, 75% of the posts were rhetorical.\nWe thus use this “middle of post” heuristic to obviate the need to gather manual annotations for RQs, and developed regex patterns to find RQs that were more likely to be sarcastic. A sample of the patterns, number of matches in the corpus, the numbers we had annotated, and the percent that are sarcastic after annotation are summarized in Table 3.\nWe extract 357 posts following the intermediate question-answer pairs heuristic from our generic (Gen) corpus. We then supplement these with posts containing RQ cues from our cue-annotation experiments: posts that received 3 out of 5 sarcastic labels in the experiments were considered sarcastic, and posts that received 2 or fewer sarcastic labels were considered not-sarcastic. Our final rhetorical questions corpus consists of 851 posts per class (1,702 total posts). Table 5 shows some examples of rhetorical questions and selfanswering from our corpus. Hyperbole. Hyperbole (Hyp) has been studied as an independent form of figurative language, that can coincide with ironic intent (McCarthy and Carter, 2004; Cano Mora, 2009), and previous computational work on sarcasm typically includes features to capture hyperbole (Reyes et al., 2013). Kreuz and Roberts (1995) describe a standard frame for hyperbole in English where an adverb modifies an extreme, positive adjective, e.g. “That was absolutely amazing!” or “That was simply the most incredible dining experience in my entire life.”\nColston and O’Brien (2000) provide a theoretical framework that explains why hyperbole is so strongly associated with sarcasm. Hyperbole exaggerates the literal situation, introducing a discrepancy between the “truth” and what is said, as a matter of degree. A key observation is that this is\na type of contrast (Colston and Keller, 1998; Colston and O’Brien, 2000). In their framework:\n• An event or situation evokes a scale; • An event can be placed on that scale; • The utterance about the event contrasts with\nactual scale placement.\nFig. 2 illustrates that the scales that can be evoked range from negative to positive, undesirable to desirable, unexpected to expected and certain to uncertain. Hyperbole moves the strength of an assertion further up or down the scale from the literal meaning, the degree of movement corresponds to the degree of contrast. Depending on what they modify, adverbial intensifiers like totally, absolutely, incredibly shift the strength of the assertion to extreme negative or positive.\nTable 6 shows examples of hyperbole from our corpus, showcasing the effect that intensifiers have in terms of strengthening the emotional evaluation of the response. To construct a balanced corpus of sarcastic and not-sarcastic utterances with hyperbole, we developed a number of patterns based on the literature and our observations of the generic corpus. The patterns, number matches on the whole corpus, the numbers we had annotated and the percent that are sarcastic after annotation are summarized in Table 3. Again, we extract a small subset of examples from our Gen corpus (30 per\nclass), and supplement them with posts that contain our hyperbole cues (considering them sarcastic if they received at least 3/5 sarcastic labels, notsarcastic otherwise). The final hyperbole dataset consists of 582 posts per class (1,164 posts in total).\nTo recap, Table 2 summarizes the total number of posts for each subset of our final corpus."
  }, {
    "heading": "3 Learning Experiments",
    "text": "Our primary goal is not to optimize classification results, but to explore how results vary across different subcorpora and corpus properties. We also aim to demonstrate that the quality of our corpus makes it more straightforward to achieve high classification performance. We apply both supervised learning using SVM (from Scikit-Learn (Pedregosa et al., 2011)) and weakly-supervised linguistic pattern learning using AutoSlog-TS (Riloff, 1996). These reveal different aspects of the corpus.\nSupervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit (Pedregosa et al., 2011). We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not (Socher et al., 2013; Li et al., 2016). The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or “!!!”), and emoticons. We use GoogleNews Word2Vec features (Mikolov et al., 2013).4\nTable 7 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation. The data is balanced evenly between the SARCASTIC and NOTSARCASTIC classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCASTIC class, while n-grams give the best NOT-SARCASTIC F-measure of 0.73. Both of these results are higher F than previously reported for classifying sarcasm in dialogue, and we might expect that feature engineering could yield even greater performance.\n4We test our own custom 300-dimensional embeddings created for the dialogic domain using the Gensim library (Řehůřek and Sojka, 2010), and a very large corpus of user-generated dialogue. While this custom model works well for other tasks on IAC 2.0, it did not work well for sarcasm classification, so we do not discuss it further.\nOn the RQ corpus, n-grams provide the best F-measure for SARCASTIC at 0.70 and NOTSARCASTIC at 0.71. Although W2V performs well, the n-gram model includes features involving repeated punctuation and emoticons, which the W2V model excludes. Punctuation and emoticons are often used as distinctive feature of sarcasm (i.e. “Oh, really?!?!”, [emoticon-rolleyes]).\nFor the Hyp corpus, the best F-measure for both the SARCASTIC and NOT-SARCASTIC classes again comes from n-grams, with F-measures of 0.65 and 0.68 respectively. It is interesting to note that the overall results of the Hyp data are lower than those for Gen and RQs, likely due to the smaller size of the Hyp dataset.\nTo examine the effect of dataset size, we com-\npare F-measure (using the same 10-fold crossvalidation setup) for each dataset while holding the number of posts per class constant. Figure 3 shows the performance of each of the Gen, RQ, and Hyp datasets at intervals of 100 posts per class (up to the maximum size of 582 posts per class for Hyp, and 851 posts per class for RQ). From the graph, we can see that as a general trend, the datasets benefit from larger dataset sizes. Interestingly, the results for the RQ dataset are very comparable to those of Gen. The Gen dataset eventually gets the highest sarcastic F-measure (0.74) at its full dataset size of 3,260 posts per class.\nWeakly-Supervised Learning. AutoSlog-TS is a weakly supervised pattern learner that only requires training documents labeled broadly as SARCASTIC or NOT-SARCASTIC. AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Table 8 lists each pattern template and the right-hand side illustrates a specific lexicosyntactic pattern (in bold) that represents an instantiation of each general pattern template for learning sarcastic patterns in our data.5 In addition to these 17 templates, we added patterns to AutoSlog for adjective-noun, adverb-adjective and adjective-adjective, because these patterns are frequent in hyperbolic sarcastic utterances.\nThe examples in Table 8 show that Colston’s notion of contrast shows up in many learned patterns, and that the source of the contrast is highly variable. For example, Row 1 implies a contrast with a set of people who are not your mother. Row 5 contrasts what you were asked with what you’ve (just) done. Row 10 contrasts chapter 12 and chapter 13 (Hirschberg, 1985). Row 11 contrasts what I am allowed vs. what you have to do.\nAutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(SARCASTIC | p) and P(NOT-SARCASTIC | p), along with the pattern’s overall frequency. We define two tuning parameters for each class: θf , the frequency with which a pattern occurs, θp, the probability with which a pattern is associated with the given class. We do a grid-search, testing the performance of our patterns thresholds from θf = {2-6} in intervals of 1, θp={0.60-0.85} in intervals of 0.05. Once we extract the subset of patterns passing our thresholds, we search for these patterns in the posts in our development set, classifying a post as a given class if it contains θn={1,\n5The examples are shown as general expressions for readability, but the actual patterns must match the syntactic constraints associated with the pattern template.\n2, 3} of the thresholded patterns. For more detail, see (Riloff, 1996; Oraby et al., 2015).\nAn advantage of AutoSlog-TS is that it supports systematic exploration of recall and precision tradeoffs, by selecting pattern sets using different parameters. The parameters have to be tuned on a training set, so we divide each dataset into 80% training and 20% test. Figure 4 shows the precision (x-axis) vs. recall (y-axis) tradeoffs on the test set, when optimizing our three parameters for precision. Interestingly, the subcorpora for RQ and Hyp can get higher precision than is possible for Gen. When precision is fixed at 0.75, the recall for RQ is 0.07 and the recall for Hyp is 0.08. This recall is low, but given that each retrieved post provides multiple cues, and that datasets on the web are huge, these P values make it possible to bootstrap these two classes in future."
  }, {
    "heading": "4 Linguistic Analysis",
    "text": "Here we aim to provide a linguistic characterization of the differences between the sarcastic and the not-sarcastic classes. We use the AutoSlog-TS pattern learner to generate patterns automatically, and the Stanford dependency parser to examine relationships between arguments (Riloff, 1996; Manning et al., 2014). Table 10 shows the number of sarcastic patterns we extract with AutoSlog-TS, with a frequency of at least 2 and a probability of at least 0.75 for each corpus. We learn many novel lexico-syntactic cue patterns that are not the regex that we search for. We discuss specific novel learned patterns for each class below.\nGeneric Sarcasm. We first examine the different patterns learned on the Gen dataset. Table 9 show examples of extracted patterns for each class. We\nobserve that the NOT-SARCASTIC patterns appear to capture technical and scientific language, while the SARCASTIC patterns tend to capture subjective language that is not topic-specific. We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method. Instead, such cues co-occur with the cues we search for, expanding our pattern inventory as we show in Table 10.\nRhetorical Questions. We notice that while the NOT-SARCASTIC patterns generated for RQs are similar to the topic-specific NOT-SARCASTIC patterns we find in the general dataset, there are some interesting features of the SARCASTIC patterns that are more unique to the RQs.\nMany of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser (Manning et al., 2014) for the questions in the RQ dataset. Table 11 shows a few examples of the relations we extract. Hyperbole. One common pattern for hyperbole\ninvolves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole. Table 12 illustrates some of the new adverb adjective patterns that are frequent, high-precision indicators of sarcasm.\nWe learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table 13. Interestingly, many of these instantiate the observations of Cano Mora (2009) on hyperbole and its related semantic fields: creating contrast by exclusion, e.g. no limit and no way, or by expanding a predicated class, e.g. everyone knows. Many of them are also contrastive. Table 12 shows just a few examples, such as though it in no way and so much knowledge."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "We have developed a large scale, highly diverse corpus of sarcasm using a combination of linguistic analysis and crowd-sourced annotation. We use filtering methods to skew the distribution of sarcasm in posts to be annotated to 20-31%, much higher than the estimated 12% distribution of sarcasm in online debate forums. We note that when\nusing Mechanical Turk for sarcasm annotation, it is possible that the level of agreement signals how lexically-signaled the sarcasm is, so we settle on a conservative threshold (at least 6 out of 9 annotators agreeing that a post is sarcastic) to ensure the quality of our annotations.\nWe operationalize lexico-syntactic cues prevalent in sarcasm, finding cues that are highly indicative of sarcasm, with ratios up to 87%. Our final corpus consists of data representing generic sarcasm, rhetorical questions, and hyperbole.\nWe conduct supervised learning experiments to highlight the quality of our corpus, achieving a best F of 0.74 using very simple feature sets. We use weakly-supervised learning to show that we can also achieve high precision (albeit with a low recall) for our rhetorical questions and hyperbole datasets; much higher than the best precision that is possible for the Generic dataset. These high precision values may be used for bootstrapping these two classes in the future.\nWe also present qualitative analysis of the different characteristics of rhetorical questions and hyperbole in sarcastic acts, and of the distinctions between sarcastic/not-sarcastic cues in generic sarcasm data. Our analysis shows that the forms of sarcasm and its underlying semantic contrast in dialogue are highly diverse.\nIn future work, we will focus on feature engineering to improve results on the task of sarcasm classification for both our generic data and subclasses. We will also begin to explore evaluation on real-world data distributions, where the ratio of sarcastic/not-sarcastic posts is inherently unbalanced. As we continue our analysis of the generic and fine-grained categories of sarcasm, we aim to better characterize and model the great diversity of sarcasm in dialogue."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was funded by NSF CISE RI 1302668, under the Robust Intelligence Program."
  }],
  "year": 2016,
  "references": [{
    "title": "Internet argument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it",
    "authors": ["Robert Abbott", "Brian Ecker", "Pranav Anand", "Marilyn Walker"],
    "venue": "In Language Resources and Evaluation Conference,",
    "year": 2016
  }, {
    "title": "A long shortterm memory framework for detecting humor in dialogues",
    "authors": ["Dario Bertero", "Pascale Fung."],
    "venue": "North American Association of Computational Linguistics Conference, NAACL-16.",
    "year": 2016
  }, {
    "title": "Automatic identification of rhetorical questions",
    "authors": ["Shohini Bhattasali", "Jeremy Cytryn", "Elana Feldman", "Joonsuk Park."],
    "venue": "Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
    "year": 2015
  }, {
    "title": "All or nothing: a semantic analysis of hyperbole",
    "authors": ["Laura Cano Mora."],
    "venue": "Revista de Lingüı́stica y Lenguas Aplicadas, pages 25–35.",
    "year": 2009
  }, {
    "title": "You’ll never believe this: Irony and hyperbole in expressing surprise",
    "authors": ["Herbert L. Colston", "Shauna B. Keller."],
    "venue": "Journal of psycholinguistic research, 27(4):499–513.",
    "year": 1998
  }, {
    "title": "Contrast and pragmatics in figurative language: Anything understatement can do, irony can do better",
    "authors": ["Herbert L. Colston", "Jennifer O’Brien"],
    "venue": "Journal of Pragmatics,",
    "year": 2000
  }, {
    "title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing",
    "authors": ["Elena Filatova."],
    "venue": "Language Resources and Evaluation Conference, LREC2012.",
    "year": 2012
  }, {
    "title": "You call that a rhetorical question?: Forms and functions of rhetorical questions in conversation",
    "authors": ["Jane Frank."],
    "venue": "Journal of Pragmatics, 14(5):723–738.",
    "year": 1990
  }, {
    "title": "Irony in talk among friends",
    "authors": ["Raymond W. Gibbs."],
    "venue": "Metaphor and Symbol, 15(1):5–27.",
    "year": 2000
  }, {
    "title": "Identifying sarcasm in twitter: a closer look",
    "authors": ["Roberto González-Ibáñez", "Smaranda Muresan", "Nina Wacholder."],
    "venue": "Proc. of the 49th Annual Meeting of the Association for Computational Linguistics, volume 2, pages 581–586.",
    "year": 2011
  }, {
    "title": "Deriving the interpretation of rhetorical questions",
    "authors": ["Chung-hye Han."],
    "venue": "The Proc. of the Sixteenth West Coast Conference on Formal Linguistics, WCCFL16.",
    "year": 1997
  }, {
    "title": "A Theory of Scalar Implicature",
    "authors": ["Julia Hirschberg."],
    "venue": "Ph.D. thesis, University of Pennsylvania, Computer and Information Science.",
    "year": 1985
  }, {
    "title": "What else can I tell you?: a pragmatic study of English rhetorical questions as discursive and argumentative acts",
    "authors": ["Cornelia Ilie."],
    "venue": "Acta Universitatis Stockholmiensis: Stockholm studies in English. Almqvist & Wiksell International.",
    "year": 1994
  }, {
    "title": "Harnessing context incongruity for sarcasm detection",
    "authors": ["Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, volume 2, pages 757–762.",
    "year": 2015
  }, {
    "title": "Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web",
    "authors": ["Raquel Justo", "Thomas Corcoran", "Stephanie M Lukin", "Marilyn Walker", "M Inés Torres."],
    "venue": "Knowledge-Based Systems.",
    "year": 2014
  }, {
    "title": "Two cues for verbal irony: Hyperbole and the ironic tone of voice",
    "authors": ["Roger J. Kreuz", "Richard M. Roberts."],
    "venue": "Metaphor and Symbolic Activity, 10(1):21–31.",
    "year": 1995
  }, {
    "title": "Visualizing and understanding neural models in nlp",
    "authors": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."],
    "venue": "North American Association of Computational Linguistics Conference, NAACL-16.",
    "year": 2016
  }, {
    "title": "Really? well",
    "authors": ["Stephanie Lukin", "Marilyn Walker."],
    "venue": "apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. NAACL 2013, page 30.",
    "year": 2013
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."],
    "venue": "ACL (System Demonstrations), pages 55–60.",
    "year": 2014
  }, {
    "title": "there’s millions of them”: hyperbole in everyday conversation",
    "authors": ["Michael McCarthy", "Ronald Carter."],
    "venue": "Journal of Pragmatics, 36(2):149–184.",
    "year": 2004
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "And thats a fact: Distinguishing factual and emotional argumentation in online dialogue",
    "authors": ["Shereen Oraby", "Lena Reed", "Ryan Compton", "Ellen Riloff", "Marilyn Walker", "Steve Whittaker."],
    "venue": "2nd Workshop on Argument Mining, NAACL HLT 2015, page",
    "year": 2015
  }, {
    "title": "Irony and reversal of evaluation",
    "authors": ["Alan Partington."],
    "venue": "Journal of Pragmatics, 39(9):1547–1569.",
    "year": 2007
  }, {
    "title": "Scikitlearn: Machine learning in Python",
    "authors": ["Matthieu Perrot", "Edouard Duchesnay."],
    "venue": "Journal of Machine Learning Research, 12:2825–2830.",
    "year": 2011
  }, {
    "title": "Software Framework for Topic Modelling with Large Corpora",
    "authors": ["Radim Řehůřek", "Petr Sojka."],
    "venue": "Proc. of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50.",
    "year": 2010
  }, {
    "title": "A multidimensional approach for detecting irony in twitter",
    "authors": ["Antonio Reyes", "Paolo Rosso", "Tony Veale."],
    "venue": "Data Knowl. Eng., 47(1):239–268, March.",
    "year": 2013
  }, {
    "title": "From humor recognition to irony detection: The figurative language of social media",
    "authors": ["Antonio Reyes", "Paolo Rosso", "Davide Buscaldi."],
    "venue": "Data Knowl. Eng., 74:1–12.",
    "year": 2012
  }, {
    "title": "Learning extraction patterns for subjective expressions",
    "authors": ["Ellen Riloff", "Janyce Wiebe."],
    "venue": "Proc. of the 2003 conference on Empirical methods in natural language processing-Volume 10, pages 105–112. Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "Sarcasm as contrast between a positive sentiment and negative situation",
    "authors": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."],
    "venue": "Proc. of the 2013 Conference on Empirical Methods in Natural Lan-",
    "year": 2013
  }, {
    "title": "Automatically generating extraction patterns from untagged text",
    "authors": ["Ellen Riloff."],
    "venue": "Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044–1049.",
    "year": 1996
  }, {
    "title": "Can rhetorical questions function as retorts? : Is the pope catholic",
    "authors": ["Deborah Schaffer"],
    "venue": "Journal of Pragmatics,",
    "year": 2005
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proc. of the 2013 Conference on Empiri-",
    "year": 2013
  }, {
    "title": "Getting reliable annotations for sarcasm in online dialogues",
    "authors": ["Reid Swanson", "Stephanie Lukin", "Luke Eisenberg", "Thomas Chase Corcoran", "Marilyn A Walker."],
    "venue": "Language Resources and Evaluation Conference, LREC 2014.",
    "year": 2014
  }, {
    "title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts",
    "authors": ["Michael Thelen", "Ellen Riloff."],
    "venue": "Proc. of the ACL-02 Conference on Empirical Methods In Natural Language Processing, pages 214–221.",
    "year": 2002
  }, {
    "title": "A corpus for research on deliberation and debate",
    "authors": ["Marilyn Walker", "Pranav Anand", "Robert Abbott", "Jean E. Fox Tree."],
    "venue": "Language Resources and Evaluation Conference, LREC2012, pages 812– 817.",
    "year": 2012
  }, {
    "title": "That’s your evidence?: Classifying stance in online political debate",
    "authors": ["Marilyn A. Walker", "Pranav Anand", "Rob Abbott", "Jean E Fox Tree", "Craig Martell", "Joseph King."],
    "venue": "Decision Support Systems, 53(4):719–729.",
    "year": 2012
  }, {
    "title": "Humans require context to infer ironic intent (so computers probably do, too)",
    "authors": ["Byron C. Wallace", "Do Kook Choe", "Laura Kertz", "Eugene Charniak"],
    "year": 2014
  }],
  "id": "SP:d54a928b0236b29c66bfb061cffb8c93bc403826",
  "authors": [{
    "name": "Shereen Oraby",
    "affiliations": []
  }, {
    "name": "Vrindavan Harrison",
    "affiliations": []
  }, {
    "name": "Lena Reed",
    "affiliations": []
  }, {
    "name": "Ernesto Hernandez",
    "affiliations": []
  }, {
    "name": "Ellen Riloff",
    "affiliations": []
  }, {
    "name": "Marilyn Walker",
    "affiliations": []
  }],
  "abstractText": "The use of irony and sarcasm in social media allows us to study them at scale for the first time. However, their diversity has made it difficult to construct a high-quality corpus of sarcasm in dialogue. Here, we describe the process of creating a largescale, highly-diverse corpus of online debate forums dialogue, and our novel methods for operationalizing classes of sarcasm in the form of rhetorical questions and hyperbole. We show that we can use lexico-syntactic cues to reliably retrieve sarcastic utterances with high accuracy. To demonstrate the properties and quality of our corpus, we conduct supervised learning experiments with simple features, and show that we achieve both higher precision and F than previous work on sarcasm in debate forums dialogue. We apply a weakly-supervised linguistic pattern learner and qualitatively analyze the linguistic differences in each class.",
  "title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue"
}