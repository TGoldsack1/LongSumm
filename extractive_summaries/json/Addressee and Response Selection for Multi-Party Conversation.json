{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2133–2143, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Short text conversation (STC) has been gaining popularity: given an input message, predict an appropriate response in a single-round, two-party conversation (Wang et al., 2013; Shang et al., 2015). Modeling STC is simpler than modeling a complete conversation, but instantly helps applications such as chat-bots and automatic short-message replies (Ji et al., 2014).\nBeyond two-party conversations, there is also a need for modeling multi-party conversation, a form of conversation with several interlocutors conversing with each other (Traum, 2003; Dignum and Vreeswijk, 2003; Uthus and Aha, 2013). For example, in the Ubuntu Internet Relay Chat (IRC), sev-\neral users cooperate to find a solution for a technical issue contributed by another user. Each agent might have one part of the solution, and these pieces have to be combined through conversation in order to come up with the whole solution.\nA unique issue of such multi-party conversations is addressing, a behavior whereby interlocutors indicate to whom they are speaking (Jovanović and Akker, 2004; Akker and Traum, 2009). In faceto-face communication, the basic clue for specifying addressees is turning one’s face toward the addressee. In contrast, in voice-only or textbased communication, the explicit declaration of addressee’s names is more common.\nIn this work, we tackle addressee and response selection for multi-party conversation: given a context, predict an addressee and response. As Figure 1 shows, a system is required to select an addressee from the agents appearing in the previous context and a response from a fixed set of candidate responses (Section 3).\n2133\nThe key challenge for predicting appropriate addressees and responses is to jointly capture who is talking about what at each time step in a context. For jointly modeling the speaker-utterance information, we present two modeling frameworks: 1) static modeling and 2) dynamic modeling (Section 5). While speakers are represented as fixed vectors in the static modeling, they are represented as hidden state vectors that dynamically change with time steps in the dynamic modeling. In practice, our models trained for the task can be applied to retrieval-based conversation systems, which retrieves candidate responses from a large-scale repository with the matching model and returns the highest scoring one with the ranking model (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015). Our trained models work as the ranking model and allow the conversation system to produce addressees as well as responses.\nTo evaluate the trained models, we provide a corpus and dataset. By exploiting Ubuntu IRC Logs1, we build a large-scale multi-party conversation corpus, and create a dataset from it (Section 6). Our experiments on the dataset show the models instantiated by the static and dynamic modeling outperform a strong baseline. In particular, the model based on the dynamic modeling robustly predicts appropriate addressees and responses even if the number of interlocutors in a conversation increases.2\nWe make three contributions in this work:\n1. We formalize the task of addressee and response selection for multi-party conversation.\n2. We present modeling frameworks and the performance benchmarks for the task.\n3. We build a large-scale multi-party conversation corpus and dataset for the task."
  }, {
    "heading": "2 Related Work",
    "text": "This work follows in the footsteps of Ritter et al. (2011), who tackled the response generation problem: given a context, generate an appropriate response. While previous response generation ap-\n1http://irclogs.ubuntu.com/ 2Our code, corpus, and dataset are publicly available at\nhttps://github.com/hiroki13/response-ranking\nproaches utilize statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003), they apply statistical machine translation based techniques without such heuristics, which leads to recent work utilizing the SMT-based techniques with neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016).\nAs another popular approach, retrieval-based techniques are used to retrieve candidate responses from a repository and return the highest scoring one with the ranking model (Ji et al., 2014; Wang et al., 2015; Hu et al., 2014; Wang et al., 2013; Lu and Li, 2013). Stemming from this approach, the next utterance classification (NUC) task has been proposed, in which a system is required to select an appropriate response from a fixed set of candidates (Lowe et al., 2015; Kadlec et al., 2015). The NUC is regarded as focusing on the ranking problem of retrieval-based system, since it omits the candidate retrieving step. The merit of NUC is that it allows us to easily evaluate the model performance on the basis of accuracy.\nOur proposed addressee and response selection task is an extension of the NUC. We generalize the task by integrating the addressee detection, which has been regarded as a problematic issue in multiparty conversation (Traum, 2003; Jovanović and Akker, 2004; Uthus and Aha, 2013). Basically, the addressee detection has been tackled in the spoken/multimodal dialog system research, and the models largely rely on acoustic signal or gaze information (Jovanović et al., 2006; Akker and Traum, 2009; Ravuri and Stolcke, 2014). This current work is different from such previous work in that our models predict addressees with only textual information.\nFor predicting addressees or responses, how the context is encoded is crucial. In single-round conversation, a system is expected to encode only one utterance as a context (Ritter et al., 2011; Wang et al., 2013). In contrast, in multi-turn conversation, a system is expected to encode multiple utterances (Shang et al., 2015; Lowe et al., 2015). Very recently, individual personalities have been encoded as distributed embeddings used for response generation in two-party conversation (Li et al., 2016). Our work is different from that work in that our proposed personality-independent representation allows us to handle new agents unseen in the training data."
  }, {
    "heading": "3 Addressee and Response Selection",
    "text": "We propose and formalize the task of addressee and response selection (ARS) for multi-party conversation. The ARS task assumes the situation where a responding agent gives a response to an addressee following a context.3"
  }, {
    "heading": "Notation",
    "text": "Table 1 shows the notations for the formalization. We denote vectors with bold lower-case (e.g. xt, h), matrices with bold upper-case (e.g. W, Ha), scalars with italic lower-case or upper-case (e.g. am, Q), and sets with bold italic lower-case or cursive uppercase (e.g. x, C) letters."
  }, {
    "heading": "Formalization",
    "text": "Given an input conversational situation x, an addressee a and a response r are predicted:\nGIVEN : x = (ares, C, R)\nPREDICT : a, r\nwhere ares is a responding agent, C is a context and R is a set of candidate responses. The context C is a sequence of previous utterances up to the current time step T :\nC = (ua1,1, · · · ,uaT ,T )\nwhere uat,t is an utterance given by an agent at at a time step t. Each utterance uat,t is a sequence of Nt tokens:\nuat,t = (wat,t,1, · · · , wat,t,Nt)\nwhere wat,t,n is a token index in the vocabulary V . 3In actual situations, responses can be addressed to multiple agents. In this work, we assume the situation where one specific agent can be the addressee of a response.\nTo predict an addressee a as a target output, we select an agent from a set of the agents appearing in a context A(C). Note that a ground-truth addressee is always included in A(C). To predict an appropriate response r, we select a response from a set of candidate responses R, which consists of Q candidates:\nR = {r1, · · · , rQ}\nrq = (wq,1, · · · , wq,Nq)\nwhere rq is a candidate response, which consists of Nq tokens, and wq,n is an token index in the vocabulary V ."
  }, {
    "heading": "4 Dual Encoder Models",
    "text": "Our proposed models are extensions of the dual encoder (DE) model in (Lowe et al., 2015). The DE model consists of two recurrent neural networks (RNN) that respectively compute the vector representation of an input context and candidate response.\nA generic RNN, with input xt ∈ Rdw and recurrent state ht ∈ Rdh , is defined as:\nht = f(ht−1,xt) = π(Whht−1 + Wxxt) (1)\nwhere π is a non-linear function, Wx ∈ Rdh×dw is a parameter matrix for xt, Wh ∈ Rdh×dh is a parameter matrix for ht−1, and the recurrence is seeded with the 0 vector, i.e. h0 = 0. The recurrent state ht acts as a compact summary of the inputs seen up to time step t.\nIn the DE model, each word vector of the context C and the response rq is consumed by each RNN, and is then summarized into the context vector hc ∈ Rdh and the response vector hq ∈ Rdh . Using these vectors, the model calculates the probability that the given candidate response is the groundtruth response given the context as follows:\nPr(y(rq) = 1|C, rq) = σ(hTc W hq) (2)\nwhere y is a binary function mapping from rq to {0, 1}, in which 1 represents the ground-truth sample and 0 represents the false one, σ is the logistic sigmoid function, and W ∈ Rdh×dh is a parameter matrix. As extensions of this model, we propose our multi-party encoder models."
  }, {
    "heading": "5 Multi-Party Encoder Models",
    "text": "For capturing multi-party conversational streams, we jointly encode who is speaking what at each time step. Each agent and its utterance are integrated into the hidden states of an RNN.\nWe present two multi-party modeling frameworks: (i) static modeling and (ii) dynamic modeling, both of which jointly utilize agent and utterance representation for encoding multiple-party conversation. What distinguishes the models is that while the agent representation in the static modeling framework is fixed, the one in the dynamic modeling framework changes along with each time step t in a conversation.\nModeling Frameworks\nAs an instance of the static modeling, we propose a static model to capture the speaking-orders of agents in conversation. As an instance of the dynamic modeling, we propose a dynamic model using an RNN to track agent states. Note that the agent representations are independent of each personality (unique user). The personality-independent representation allows us to handle new agents unseen in the training data.\nFormally, similar to Eq. 2, both of the models calculate the probability that the addressee ap or response rq is the ground-truth given the input x:\nPr(y(ap) = 1|x) = σ ([ares ; hc]T Wa ap) (3)\nPr(y(rq) = 1|x) = σ ([ares ; hc]T Wr hq) (4)\nwhere y is a binary function mapping from ap or rq to {0, 1}, in which 1 represents the ground-truth sample and 0 represents the false one. The function σ is the logistic sigmoid function. ares ∈ Rda is a responding agent vector, ap ∈ Rda is a candidate addressee vector, hc ∈ Rdh is a context vector, hq ∈ Rdh is a candidate response vector. These vectors are respectively defined in each model. Wa ∈ R(da+dh)×dh is a parameter matrix for the addressee selection probability, and Wr ∈ R(da+dh)×dh is a parameter matrix for the response selection probability. These model parameters are learned during training.\nOn the basis of Eqs. 3 and 4, a resulting addressee\nand response are selected as follows:\nâ = argmax ap∈A(C)\nPr(y(ap) = 1|x) (5)\nr̂ = argmax rq∈R\nPr(y(rq) = 1|x) (6)\nwhere â is the highest probability addressee of a set of agents in the context A(C), and r̂ is the highest probability response of a set of candidate responses R."
  }, {
    "heading": "5.1 A Static Model",
    "text": "In the static model, agent matrix A is defined for the agent vectors in Eqs. 3 and 4. This agent matrix can be defined arbitrarily. We define the agent matrix A on the basis of agents’ speaking orders. Intuitively, the agents that spoke in recent time steps are more likely to be an addressee. Our static model captures such property.\nThe static model is shown in Figure 2. First, agents in the context A(C) and a responding agent ares are sorted in descending order based on each latest speaking time. Then the order is assigned as an agent index am ∈ (1, · · · , |A(C)|) to each agent. In the table shown in Figure 2, the responding agent (represented as SYSTEM) has the agent index 1 because he spoke at the most recent time step t = 6. Similarly, User 1 has the index 2 because he spoke at the second most recent time step t = 5, and User 2 has the index 3 because he spoke at the third t = 3.\nEach speaking-order index am is associated with the am-th column of the agent matrix A:\nam = A[∗, am]\nSimilarly, a responding agent vector ares and a candidate addressee vector ap in Eqs. 3 and 4 are respectively extracted from A, i.e. ares = A[∗, ares] and ap = A[∗, ap].\nConsuming the agent vectors, an RNN updates its hidden state. For example, at the time step t = 1 in Figure 2, the agent vector a1 of User 1 is extracted from A on the basis of agent index 2 and then consumed by the RNN. Then, the RNN consumes each word vector w of User 1’s utterance. By consuming the agent vector before word vectors, the model can capture which agent speaks the utterance. The last state of the RNN is regarded as hc. As the transition function f of RNN (Eq. 1), we use the Gated Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014).\nFor the candidate response vector hq, each word vector (wq,1, · · · ,wq,Nq) in the response rq is summarized with the RNN. Using these vectors ares, ap, hc, and hq, we predict a next addressee and response with the Eqs. 3 and 4."
  }, {
    "heading": "5.2 A Dynamic Model",
    "text": "In the static model, agent representation A is a fixed matrix that does not change in a conversational stream. In contrast, in the dynamic model, agent representation At tracks each agent’s hidden state which dynamically changes with time steps t.\nFigure 3 shows the overview of the dynamic model. Initially, we set a zero matrix as initial agent state A0, and each column vector of the agent matrix corresponds to an agent hidden state vector. Then, each agent state is updated by consuming the utter-\nance vector at each time step. Note that the states of the agents that are not speaking at the time are updated by zero vectors.\nFormally, each column of At corresponds to an agent state vector:\nam,t = At[∗, am]\nwhere an agent state vector am,t of an agent am at a time step t is the am-th column of the agent matrix At.\nEach vector of the matrix is updated at each time step, as shown in Figure 3. An agent state vector am,t ∈ Rda for each agent am at each time step t is recurrently computed:\nam,t = g(am,t−1,um,t), am,0 = 0\nwhere um,t ∈ Rdw is a summary vector of an utterance of an agent am and computed with an RNN. As the transition function g, we use the GRU. For example, at a time step t = 2 in Figure 3, the agent state vector a1,2 is influenced by its utterance vector u1,2 and updated from the previous state a1,1.\nThe agent matrix updated up to the time step T is denoted as AT , which is max-pooled and used as a summarized context vector:\nhc = max i AT [i]\nThe agent matrix AT is also used for a responding agent vector ares and a candidate addressee vector ap, i.e. ares = AT [∗, ares] and ap = AT [∗, ap]. rq is summarized into a response vector hq in the same way as the static model."
  }, {
    "heading": "5.3 Learning",
    "text": "We train the model parameters by minimizing the joint loss function:\nL(θ) = α La(θ) + (1 − α) Lr(θ) + λ\n2 ||θ||2\nwhere La is the loss function for the addressee selection, Lr is the loss function for the response selection, α is the hyper-parameter for the interpolation, and λ is the hyper-parameter for the L2 weight decay.\nFor addressee and response selection, we use the cross-entropy loss functions:\nLa(θ) = − ∑\nn\n[ log Pr(y(a+) = 1|x)\n+ log (1 − Pr(y(a−) = 1|x) ] Lr(θ) = − ∑\nn\n[ log Pr(y(r+) = 1|x)\n+ log (1 − Pr(y(r−) = 1|x) ]\nwhere x is the input set for the task, i.e. x = (ares, C, R), a+ is a ground-truth addressee, a− is a false addressee, r+ is a ground-truth response, and r− is a false response. As a false addressee a−, we pick up and use the addressee with the highest probability from the set of candidate addressees except the ground-truth one (A(C) \\ a+). As a false response, we randomly pick up and use a response from the set of candidate responses except the ground-truth one (R \\ r+)."
  }, {
    "heading": "6 Corpus and Dataset",
    "text": "Our goal is to provide a multi-party conversation corpus/dataset that can be used over a wide range of conversation research, such as turn-taking modeling (Raux and Eskenazi, 2009) and disentanglement modeling (Elsner and Charniak, 2010), as well as for the ARS task. Figure 4 shows the flow of the corpus and dataset creation process. We firstly crawl Ubuntu IRC Logs and preprocess the obtained logs.\nThen, from the logs, we extract and add addressee information to the corpus. In the final step, we set candidate responses and labels as the dataset. Table 2 shows the statistics of the corpus and dataset."
  }, {
    "heading": "6.1 Ubuntu IRC Logs",
    "text": "The Ubuntu IRC Logs is a collection of logs from Ubuntu-related chat rooms. In each chat room, a number of users chat on and discuss various topics, mainly related to technical support with Ubuntu issues.\nThe logs are put together into one file per day for each room. Each file corresponds to a document D. In a document, one line corresponds to one log given by a user. Each log consists of three items (Time, UserID, Utterance). Using such information, we create a multi-party conversation corpus."
  }, {
    "heading": "6.2 The Multi-Party Conversation Corpus",
    "text": "To pick up only the documents written in English, we use a language detection library (Nakatani, 2010). Then, we remove the system logs from each document and leave only user logs. For segmenting the words in each utterance, we use a word tokenizer (TreebankWordTokenizer) of the Natural Language Toolkit4. Using the preprocessed documents, we create a corpus, whose row consists of the three items (UserID, Addressee, Utterance).\nFirst, the IDs of the users in a document are collected into the user ID list by referring to the UserID in each log. Then, as the addressee user ID, we extract the first word of each utterance. In the Ubuntu IRC Logs, users follow the name mention convention (Uthus and Aha, 2013), in which they express\n4http://www.nltk.org/\ntheir addressee by mentioning the addressee’s user ID at the beginning of the utterance. By exploiting the name mentions, if the first word of each utterance is identical to a user ID in the user ID list, we extract the addressee ID and then create a table consisting of (UsetID, Addressee, Utterance). In the case that addressee IDs are not explicitly mentioned at the beginning of the utterance, we do not extract anything."
  }, {
    "heading": "6.3 The ARS Dataset",
    "text": "By exploiting the corpus, we create a dataset for the ARS task. If the line of the corpus includes an addressee ID, we regard it as a sample for the task. As the ground truth addressees and responses, we straightforwardly use the obtained addressee IDs and the preprocessed utterances.\nAs false responses, we sample utterances elsewhere within a document. This document-within sampling method makes the response selection task more difficult than the random sampling method5. One reason for this is that common or similar topics in a document are often discussed and the used words tend to be similar, which makes the wordbased features for the task less effective. We partitioned the dataset randomly into a training set (90%), a development set (5%) and a test set (5%)."
  }, {
    "heading": "7 Experiments",
    "text": "We provide performance benchmarks of our learning architectures on the addressee and response selection (ARS) task for multi-party conversation."
  }, {
    "heading": "7.1 Experimental Setup",
    "text": ""
  }, {
    "heading": "Datasets",
    "text": "We use the created dataset for the experiments. The number of candidate responses RES-CAND (|R|) is set to 2 or 10."
  }, {
    "heading": "Evaluation Metrics",
    "text": "We evaluate performance by accuracies on three aspects: addressee-response pair selection (ADR-RES), addressee selection (ADR), and response selection (RES). In the addressee-response pair selection, we regard the answer as correct if both the addressee and the response are correctly\n5Lowe et al. (2015) adopted the random sampling method.\nselected. In the addressee/response selection, we regard the answer as correct if the addressee/response is correctly selected."
  }, {
    "heading": "Optimization",
    "text": "The models are trained by backpropagation through time (Werbos, 1990; Graves and Schmidhuber, 2005). For the backpropagation, we use stochastic gradient descent (SGD) with a mini-batch training method. The mini-batch size is set to 128. The hyper-parameter α for the interpolation between the two loss functions (Section 5.3) is set to 0.5. For the L2 weight decay, the hyper-parameter λ is selected from {0.001, 0.0005, 0.0001}.\nParameters of the models are randomly initialized over a uniform distribution with support [−0.01, 0.01]. To update parameters, we use Adam (Kingma and Ba, 2014) with the default setting suggested by the authors. As the word embeddings, we used the 300 dimension vectors pre-trained by GloVe6 (Pennington et al., 2014). To avoid overfitting, the word vectors are fixed across all experiments. The hidden dimensions of parameters are set to dw = 300 and dh = 50 in the both models, and da is set to 300 in the static model and 50 in the dynamic model.\nTo identify the best training epoch and model configuration, we use the early stopping method (Yao et al., 2007). In this method, if the best accuracy of ADR-RES on the development set has not been updated for consecutive 5 epochs, training is stopped and the best performing model is picked up. The max epochs is set to 30, which is sufficient for convergence."
  }, {
    "heading": "Implementation Details",
    "text": "For computational efficiency, we limit the length of a context C as CT−Nc+1:T = (uT−Nc+1, · · · , uT ), where Nc, called context window, is the number of utterances prior to a time step t. We set Nc to {5, 10, 15}. In addition, we truncate the utterances and responses at a maximum of 20 words. For batch processing, we zero-pad them so that the number of words is constant. Out-of-vocabulary words are replaced with <unk>, whose vector is the averaged vector over all word vectors.\n6http://nlp.stanford.edu/projects/glove/"
  }, {
    "heading": "Baseline Model",
    "text": "We set a baseline using the term frequency-inverse document frequency (TF-IDF) retrieval model for the response selection (Lowe et al., 2015). We firstly compute two TF-IDF vectors, one for a context window and one for a candidate response. Then, we compute a cosine similarity for these vectors, and select the highest scoring candidate response as a result. For the addressee selection, we adopt a rulebased method: to determine the agent that gives an utterance most recently except a responding agent, which captures the tendency that agents often respond to the other that spoke immediately before."
  }, {
    "heading": "7.2 Results",
    "text": ""
  }, {
    "heading": "Overall Performance",
    "text": "Table 3 shows the empirical benchmark results. The dynamic model achieves the best results in all the metrics. The static model outperforms the baseline, but is inferior to the dynamic model.\nIn addressee selection (ADR), the baseline model achieves around 55% in accuracy. This means that if you select the agents that spoke most recently as an addressee, the half of them are correct. Compared with the baseline, our proposed models achieve better results, which suggests that the models can select the correct addressees that spoke at more previous time steps. In particular, the dynamic model achieves 68% in accuracy, which is 7 point higher than the accuracy of static model.\nIn response selection (RES), our models outperform the baseline. Compared with the static model,\nthe dynamic model achieves around 0.5 point higher in accuracy."
  }, {
    "heading": "Effects of the Context Window",
    "text": "In response selection, a performance boost of our proposed models is observed for the context window Nc = 10 over Nc = 5. Comparing the results of the models with the context window Nc = 10 and Nc = 15, the performance is improved but relatively small, which suggests that the performance almost reaches the convergence. In addressee selection, the performance improvements of the static model with the broader context window is limited. In contrast, in the dynamic model, a steady performance boost is observed, yielding an increase of over 5 points between Nc = 15 and Nc = 5,"
  }, {
    "heading": "Effects of the Sample Size",
    "text": "Figure 5 shows the accuracy curves of addresseeresponse selection (ADR-RES) for different training sample sizes. We use 1/2, 1/4, and 1/8 of the whole training samples for training. The results show that as the amount of the data increases, the performance of our models are improved and gradually approaches the convergence. Remarkably, the performance of the dynamic models using the 1/8 samples is comparable to that of the static model using the whole samples."
  }, {
    "heading": "Effects of the Number of Participants",
    "text": "To shed light on the relationship between the model performance and the number of agents in multi-party conversation, we investigate the effect of the number of agents participating in each context. Table 4 compares the performance of the models for different numbers of agents in a context.\nIn addressee selection, the performance of all models gradually gets worse as the number of agents in the context increases. However, compared with the baseline, our proposed models suppress the performance degradation. In particular, the dynamic model predicts correct addressees most robustly.\nIn response selection, unexpectedly, the performance of all the models gets better as the number of agents increases. Detailed investigation on the interaction between the number of agents and the response selection complexity is an interesting line of future work."
  }, {
    "heading": "8 Conclusion",
    "text": "We proposed addressee and response selection for multi-party conversation. Firstly, we provided the formal definition of the task, and then created a corpus and dataset. To present benchmark results, we proposed two modeling frameworks, which jointly model speakers and their utterances in a context. Experimental results showed that our models of the frameworks outperform a baseline.\nOur future objective to tackle the task of predicting whether to respond to a particular utterance. In this work, we assume that the situations where there is a specific addressee that needs an appropriate response and a system is required to respond. In actual multi-party conversation, however, a system sometimes has to wait and listen to the conversation that other participants are engaging in without needless interruption. Hence, the prediction of whether to respond in a multi-party conversation would be an important next challenge."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Graham Neubig, Yuya Taguchi, Ryosuke Kohita, Ander Martinez, the members of the NAIST Computational Linguistics Laboratory, the members of IBM Research - Tokyo, Long Duong, and the reviewers for their helpful comments."
  }],
  "year": 2016,
  "references": [{
    "title": "A comparison of addressee detection methods for multiparty conversations",
    "authors": ["Rieks Akker", "David Traum."],
    "venue": "Workshop on the Semantics and Pragmatics of Dialogue.",
    "year": 2009
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of EMNLP, pages",
    "year": 2014
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv: 1412.3555.",
    "year": 2014
  }, {
    "title": "Towards a testbed for multi-party dialogues",
    "authors": ["Frank PM Dignum", "Gerard AW Vreeswijk."],
    "venue": "Advances in Agent Communication, pages 212–230.",
    "year": 2003
  }, {
    "title": "Disentangling chat",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "Computational Linguistics, pages 389– 409.",
    "year": 2010
  }, {
    "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "Convolutional neural network architectures for matching natural language sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "Proceedings of NIPS, pages 2042–2050.",
    "year": 2014
  }, {
    "title": "An information retrieval approach to short text conversation",
    "authors": ["Zongcheng Ji", "Zhengdong Lu", "Hang Li."],
    "venue": "arXiv preprint arXiv: 1408.6988.",
    "year": 2014
  }, {
    "title": "Towards automatic addressee identification in multiparty dialogues",
    "authors": ["Natasa Jovanović", "op den Rieks Akker."],
    "venue": "Proceedings of SIGDIAL.",
    "year": 2004
  }, {
    "title": "Addressee identification in face-to-face meetings",
    "authors": ["Natasa Jovanović", "op den Rieks Akker", "Anton Nijholt"],
    "venue": "In Proceedings of EACL",
    "year": 2006
  }, {
    "title": "Improved deep learning baselines for ubuntu corpus dialogs",
    "authors": ["Rudolf Kadlec", "Martin Schmid", "Jan Kleindiest."],
    "venue": "arXiv preprint arXiv: 1510.03753.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Lei Ba."],
    "venue": "arXiv preprint arXiv: 1412.6980.",
    "year": 2014
  }, {
    "title": "A stochastic model of human-machine interaction for learning dialog strategies",
    "authors": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."],
    "venue": "IEEE Transactions on Speech and Audio Processing, pages 11–23.",
    "year": 2000
  }, {
    "title": "A persona-based neural conversation model",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of ACL.",
    "year": 2016
  }, {
    "title": "The ubuntu dialogue corpus: A large",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian V. Serban", "Joelle Pineau"],
    "year": 2015
  }, {
    "title": "A deep architecture for matching short texts",
    "authors": ["Zhengdong Lu", "Hang Li."],
    "venue": "Proceedings of NIPS, pages 1367–1375.",
    "year": 2013
  }, {
    "title": "Language detection library for java",
    "authors": ["Shuyo Nakatani"],
    "year": 2010
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of EMNLP, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "A finite-state turn-taking model for spoken dialog systems",
    "authors": ["Antoine Raux", "Maxine Eskenazi."],
    "venue": "Proceedings of NAACL, pages 629–637.",
    "year": 2009
  }, {
    "title": "Neural network models for lexical addressee detection",
    "authors": ["Suman V Ravuri", "Andreas Stolcke."],
    "venue": "Proceedings of INTERSPEECH, pages 298–302.",
    "year": 2014
  }, {
    "title": "Data-driven response generation in social media",
    "authors": ["Alan Ritter", "Colin Cherry", "William B. Dolan."],
    "venue": "Proceedings of EMNL, pages 583–593.",
    "year": 2011
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."],
    "venue": "Proceedings of AAAI, pages 3776–3783.",
    "year": 2016
  }, {
    "title": "Neural responding machine for short-text conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "Proceedings of ACL/IJCNLP, pages 1577–1586.",
    "year": 2015
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of",
    "year": 2015
  }, {
    "title": "Issues in multiparty dialogues",
    "authors": ["David Traum."],
    "venue": "Advances in Agent communication, pages 201–211.",
    "year": 2003
  }, {
    "title": "Multiparticipant chat analysis: A survey",
    "authors": ["David C Uthus", "David W Aha."],
    "venue": "Artificial Intelligence, pages 106–121.",
    "year": 2013
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "V. Quoc Le."],
    "venue": "arXiv preprint arXiv: 1506.05869.",
    "year": 2015
  }, {
    "title": "A trainable generator for recommendations in multimodal dialog",
    "authors": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent."],
    "venue": "Proceedings of INTERSPEECH. Citeseer.",
    "year": 2003
  }, {
    "title": "A dataset for research on short-text conversations",
    "authors": ["Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen."],
    "venue": "Proceedings of EMNLP, pages 935–945.",
    "year": 2013
  }, {
    "title": "Syntax-based deep matching of short texts",
    "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."],
    "venue": "Proceedings of IJCAI, pages 1354–1361.",
    "year": 2015
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["Paul J Werbos."],
    "venue": "Proceedings of the IEEE, 78(10):1550–1560.",
    "year": 1990
  }, {
    "title": "On early stopping in gradient descent learning",
    "authors": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto."],
    "venue": "Constructive Approximation, 26(2):289–315.",
    "year": 2007
  }, {
    "title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management",
    "authors": ["Steve Young", "Milica Gašić", "Simon Keizer", "François Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."],
    "venue": "Computer Speech & Language, pages",
    "year": 2010
  }],
  "id": "SP:811e014002d1e4d1e185fc236cf9e3fafe2aade5",
  "authors": [{
    "name": "Hiroki Ouchi",
    "affiliations": []
  }, {
    "name": "Yuta Tsuboi",
    "affiliations": []
  }],
  "abstractText": "To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents. In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say. The key challenge of this task is to jointly model who is talking about what in a previous context. For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling. To show benchmark results of our frameworks, we created a multi-party conversation corpus. Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents.",
  "title": "Addressee and Response Selection for Multi-Party Conversation"
}