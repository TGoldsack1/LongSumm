{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Replicating results in deep learning research is often hard. This harms their usefulness to industry, leads to a waste of effort by other researchers, and limits the scientific value of such results.\nOne reason is that many papers provide information insufficient for replication. Details of the experimental setup can significantly influence the results (Henderson et al., 2018; Fokkens et al., 2013; Raeder et al., 2010), so the details should be provided at least in appendices, ideally alongside the source code, as was strongly emphasized e.g. by Ince et al. (2012).\nHowever, an important second factor hinders replicability: most deep learning training methods are inherently stochastic. This randomness usually comes from random data ordering in stochastic gradient descent and from random parameter initialization, though there can be additional sources of randomness such as dropout or gradient noise. Consequently, even if we fix the model architecture and the experimental setup (including the hyperparameters), we obtain a different result each time we run an experiment. Statistical techniques are needed to handle this variability. However,\n1 IBM Watson, Prague AI Research & Development Lab. RK has since moved to Deepmind. Correspondence to: Ondrej Bajgar < ondrej@bajgar.org >.\nOriginally published in the Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nin deep learning research, they are heavily underused. What is usually done instead?\nMost empirical deep learning papers simply report the performance of the best single model (sometimes calling it just \"single model\" performance). We will later show this is the case at least for some sub-domains. Given the result stochasticity, such method is statistically flawed. The best model performance is not robust under experiment replication, and its expected value improves with an increasing number of experiments performed, among other problems. Since many deep learning publications largely ignore these issues, we dedicate the first part of this article to explaining them in some detail, and later run experiments to quantify them.\nAppropriate statistical techniques are hence necessary for evaluating (and comparing) the performance of machine learning (ML) architectures. Some well-developed methods exist for such comparisons (a great introduction is given for instance by Cohen (1995)). However, most existing methods focus on comparing the mean performance. This may be one of the reasons why statistical methods are being underused, since mean may be unattractive to researchers in certain situations.\nThere are multiple possible reasons for this. The one that we do consider sound1 is that when deploying models in practice, it is often natural to train multiple instances of a model and then deploy the best one to production based on a validation set evaluation.2 Underperforming models can be discarded, so the final deployed model does come from\n1Other reasons why researchers resort to the best performance as opposed to the mean may come from the current highly competitive atmosphere in the field with (possibly excessive) focus on performance on standard datasets (see Church (2017) or Sculley et al. (2018) for further discussion), which may motivate researchers to publish only their best results. Also, statistically sound estimation of performance does require repeatedly re-running experiments, which does incur additional cost, which researchers may prefer to invest in additional model tuning, especially in the present situation where reviewers seem not to require statistically sound evaluation of models and on the other hand may favour high-performing models. Of course, these motives should instead give place to effort to do good science, as opposed to a race on standard benchmarks.\n2In some applications there is focus on speed of training and on reducing computational costs – there it does make sense to focus on the performance of the typical model as opposed to the best out of n, so the use of mean or median is appropriate.\nthe higher tier of the model performance population, and the use of mean may be inappropriate.\nHence, rather than to completely abandon reporting the performance of the best model, we propose a way to fix its flaws. We do this by estimating the expected best-out-of-n (Boon) performance by running more than n experiments, which gives the estimate statistical validity if a sufficient number of experiments are run. We discuss how this measure relates to the performance distribution of the model, and we also give a method to empirically estimate Boon.\nThe paper proceeds as follows: First, we give a high-level explanation of why reporting performance of the best single model is problematic. We also give some evidence that it is widely used in the deep learning community, which is why this explanation may be needed. We proceed by presenting Boon as a way to fix the above problems. We then give some experimental evidence for the flaws of best-singlemodel reporting and show that Boon does not suffer from them. We wrap up by discussing the place of Boon in a ML researcher’s toolbox alongside traditional measures such as mean or median."
  }, {
    "heading": "2. Best Single Model Performance",
    "text": "In articles presenting new deep learning architectures, the performance is often reported as the score of the “best single model” or simply “single model”. In practice, this usually means that the researchers train multiple instances of the proposed architecture (often with different sets of hyperparameters), evaluate these instances on some validation set, and select the best-performing model. This best model is evaluated on a test set, and the resulting test score is then reported as the metric characterizing the architecture and used for comparing it to previous models. If the score is better than those reported in previous work, the architecture is presented as superior. This practice results in several issues:\nPopulation variance Since results of experiments are stochastic, the performance of a single model is just a single instance drawn from a possibly disparate population. If others train the model on their own, they get another sample from the architecture’s performance distribution, which may substantially differ from the one listed in the original paper. Such paper thus gives insufficient information about what to expect from the new architecture, which should be one of the article’s main purposes.\nOne may object that the result published in the paper is not chosen from the population at random – it is selected using a validation result. However, the correlation between the validation and test results is generally imperfect; in fact, in some of our experiments, it is almost zero, as we show in Section 4. Furthermore, if we indeed do have a strong\ncorrelation, we get another problem:\nExpectation of best result increases with the number of experiments Simply put, the more samples from a population we draw, the more extreme the best among them is likely to be. In other words, the expected value of the best result depends on the number of experiments that the researchers run. There are three closely related problems with this: Firstly, this makes the number of experiments run an important explanatory variable; however, this variable is usually unreported, which is a severe methodological flaw in itself. It also leads to the second problem: since each research team runs a different number of experiments, the results are not directly comparable. Thirdly, this motivates researchers to run more experiments and gives an advantage to those who are able to do so. This pushes publishing quantitative results towards a race in computational power rather than a fair comparison of architectures themselves.\nBest model performance is not a meaningful characteristic of the performance distribution Even if we knew the underlying theoretical performance distribution – that is, if we had perfect information about the architecture’s performance – it would not be clear what we would mean by \"best model performance\" without specifying the size of the pool from which we are choosing the best model. Imagine some architecture having a Gaussian performance distribution. Asking what is the best possible performance does not make sense in such a case, since the support of the distribution is unbounded. Even for capped metrics such as accuracy, where the performance distribution necessarily has bounded support, the best (possible) model3 may be so unlikely, that it would be of no practical importance. Hence, best model performance is not a meaningful characteristic of the performance distribution.\nGenerality / Falsifiability Finally, there is the question of what the authors are trying to express. Using “best single model performance”, they are essentially claiming: “There once existed an instance of our model that once achieved a result X on dataset Y”. Such fact is not of that much interest to the scientific community, which would rather need to know how the architecture behaves generally. Relatedly, a frequently given characteristic of science is falsifiability of theories (Popper, 1959). A theory claiming that there are invisible unicorns running among us is not science, since we cannot think of any potential empirical evidence that could prove the theory false. Similarly, any number of replication experiments that produce substantially worse results cannot prove the above performance claim wrong. If, for instance, a confidence interval were given, replications could\n3In the sense of validation performance being at or near the supremum of the validation performance distribution’s support.\nvery quickly show the published result at least extremely implausible, if not false.\nWe will quantify the former two problems for two concrete architectures in Section 4."
  }, {
    "heading": "2.1. Prevalence",
    "text": "Despite all these problems, reporting the performance of the best model is still the main way of reporting results in some areas of ML, especially in empirically oriented deep learning papers, and, alarmingly, such practice seems to be tolerated by reviewers even at prime conferences. For instance, what concerns models published on the popular Children’s Book Test dataset for reading comprehension (on which we run experiments later), none of the (more than ten) papers used any form of statistical testing or confidence intervals, and most reported only performance of the best single model without even mentioning the total number of experiments run. These include papers published at NIPS (Hermann et al., 2015), ICLR (Hill et al., 2016; Munkhdalai & Yu, 2017), ACL (Chen et al., 2016; Dhingra et al., 2017; Cui et al., 2017), or EMNLP (Trischler et al., 2016).\nThe same is true for the recently popular SQuAD dataset: for instance, none of the four papers (Yang et al., 2017; Wang & Jiang, 2017; Seo et al., 2017; Xiong et al., 2017) that published results on this dataset at ICLR 2017 has used any statistical testing or confidence intervals nor published mean (or otherwise aggregated) results across multiple runs.\nLet us look more generally at the example of ICLR 2017 (chosen as a deep-learning-focused conference featuring many empirical results – as a rough guide, 174 out of 194 ICLR papers have \"experiment\" in a (sub-)section heading). Only 11 papers mention terms related to hypothesis testing4, and 11 contain the string \"confidence interval\". Further details can be found in Appendix B.\nWhile this is a rough and limited survey, it does suggest that while deep learning research is to a large extent an empirical science, statistical methods are often underused.\n3. Expected Best-out-of-n (Boon) Performance\nThe issues outlined above point to desiderata for a more suitable method of reporting an architecture’s performance. It should provide information about general behaviour of the architecture under specified conditions, well characterizing\n4This was checked by searching for any of the following strings: \"hypothesis test\", \"p-value\", \"t-test\" \"confidence level\", \"significance level\", \"ANOVA\", \"analysis of variance\", \"Wilcoxon\", \"sign test\".\nthe associated random performance distribution. It should also be invariant under the number of experiments run and be as robust as possible to random noise.\nGiven these requirements, traditional statistical measures, such as mean or median, probably come to mind of many readers. They do indeed fix the above issues; still, they express only the performance of a typical member of the population. However, in many ML applications, it may be the best model from a pool that is of interest. When practitioners are choosing a model for deployment, they train multiple models and deploy the best-performing one 5. This gives some justification to reporting the performance of the best model and gives us a reason to attempt to fix its problems rather than completely dismiss it. Such corrected best-model measure would be more informative than mean or median in these outlined situations.\nA natural way to improve comparability between models, each evaluated in a different number of experiments, is to normalize the results to the expected result if the number of experiments were the same, say n, which can be easily estimated if we run m experiments, m ≥ n. The greater the number of experiments m, the more robust the estimate of the expected best, which also helps us eliminate the problem of statistical robustness. We are proposing the expected best-out-of-n performance, Boon, to be used where the performance of the best model from a pool seems as an appropriate measure.\nLet us first examine how the expected best-out-of-n (Boon) performance relates to the (theoretical) performance distribution we are trying to characterize; we will then proceed with empirical estimation, which is of value in practice. The calculations are not particularly innovative from statistical point of view and are close to many standard results from the field of Order Statistics (see for instance Arnold et al. (2008) for more context).\n3.1. Boon of a probability distribution\nShowing how to calculate Boon of a known theoretical probability distribution will serve two purposes: Firstly, since we are proposing Boon as a way to characterize the performance distribution, this will make the relation between Boon and the performance distribution explicit. Secondly, in some cases we may be able to make an assumption about the family to which the theoretical distribution belongs (e.g.\n5This would usually be the case when a model is trained once and then deployed for longer-term usage, which may be the case for instance for Machine Translation systems. In other cases, when it is practical to train only as single model instance due to hardware constraints (either because training is extremely costly, or because it needs to be done repeatedly, e.g. for individual customers), we may indeed be interested in a typical model and hence in mean or median performance.\nwe could assume it is approximately Gaussian). The analytic calculation below will allow us to leverage this information when empirically estimating Boon by deducing a parametric estimator, which may be especially useful when our sample size m is small thanks to its lower variance due to added prior information."
  }, {
    "heading": "3.1.1. SINGLE EVALUATION",
    "text": "Let us first look at the simpler case of validation performance (that is, the case where we are choosing the best model with respect to the metric we are reporting) as it is easier to grasp: How do we calculate an expected best Boon(P)6 of independent identically distributed (i.i.d.) random variables X1, ..., Xn with probability distribution P (the performance distribution of an architecture) with a probability density function (p.d.f.) f and a cumulative distribution function (c.d.f.) F ? In the case where best means maximal (minimum can be calculated similarly), the maximum max{X1, ..., Xn} has a c.d.f. equal to\nFmax(x) = P[max{X1, ..., Xn} ≤ x] = P[X1 ≤ x, ...,Xn ≤ x] = Fn(x) (1)\nusing the independence of the Xis in the last step.\nIn the case of a continuous distribution, we can obtain the p.d.f. of the maximum by simply differentiating with respect to x:\nfmax(x) = d\ndx Fmax(x) = nf(x)F\nn−1(x)\nUsing the p.d.f., we can now calculate the expected value of the maximum as\nBoon(P) = ∫ ∞ −∞ xfmax(x)dx = ∫ ∞ −∞ xnf(x)Fn−1(x)dx\n(2)\nWe can get a precise numerical estimate of the above integrals in any major numerical computation package such as numpy. For illustration, for the standard normal distribution we have Boo5 (N (0, 1)) ≈ 1.163, Boo10 (N (0, 1)) ≈ 1.539. More generally, Boon ( N (µ, σ2) ) can then be expressed as µ + σBoon (N (0, 1)). Thanks to this form we can get numerical estimates of Boon ( N (µ, σ2) ) just by estimating the two usual parameters of the Gaussian, Boon (N (0, 1)) becoming just a constant coefficient if we fix n. The full details of calculation for the Gaussian distribution can be found in Appendix A.\nIn the case of a discrete performance distribution, which will be useful for empirical estimation below, we get a prob-\n6Using the overline to distinguish from the double validationtest evaluation case later.\nability mass function\nP[max{X1, . . . , Xn} = m] = P[max{X1, . . . , Xn} ≤ m]−P[max{X1, . . . , Xn} < m]\n(3)\nso if pj is the probability weight associated with value xj , i.e. P[Xi = xj ] = pj for all i, this gives us Boon(P) = ∑ i  ∑ j: xj≤xi pj n −  ∑ j: xj<xi pj nxi . (4)"
  }, {
    "heading": "3.1.2. VALIDATION-TEST EVALUATION",
    "text": "In the previous part, we were choosing the best model with respect to the metric whose expectation we were calculating. Hence, that method can be used to calculate the expected best validation performance of n models. In practice, the best model is usually chosen with respect to the validation performance, while the primary interest is in the corresponding test performance. To calculate the expected test performance of the best-validation model, we need to substitute the direct value of x in Equations 2 and 4, with the expectation of the test performance Xtest conditional on the validation performance xval,\nEtv (xval) := E [Xtest|Xval = xval]\nyielding an expression for the expected test performance of the best-validation model chosen from a pool of size n\nBoon(P) = ∫ Etv(xval)dP\nbest val (xval) =∫ ∞\n−∞ Etv(xval)nfval(xval)F\nn−1 val (xval)dxval (5)\nwhere Pbestval is the marginal probability distribution of the best-out-of-n validation performance. Similar simple substitution can be done in the discrete case.\nExpanding the expression for the bivariate Gaussian distribution having marginal test performance with mean µtest, standard deviation σtest, and test-validation correlation ρ as in Appendix A.2 yields a convenient expression\nBoon = µtest + ρ σtest Boon (N (0, 1)) , (6)\nwhich can again be used for parametric estimation."
  }, {
    "heading": "3.2. Empirical estimation",
    "text": "We usually do not know the exact performance distribution of the model; we only have samples from this distribution – the results of our experiments. In such case, we can estimate the expected maximum empirically, and in fact it is the\nempirical estimates that are likely to be used in practice to compare models.\nTo get a non-parametric estimator, for which we do not make any assumption about the family of the performance distribution, we take the empirical distribution arising from our sample as an approximation of the architecture’s true performance distribution, similarly to Bootstrap methods. The empirical performance distribution P̂ assigns a probability weight of 1m to each of our m samples. We approximate Boon of the true performance distribution by Boon of this empirical distribution. For the uniform empirical distribution, all the pi in Equation 4 are equal to 1m . Hence, if we rank our samples from worst-validation to best-validation as (xvalidi1 , x test i1 ), . . . , (xvalidim , x test im ), we get\nB̂oon ( (xvalid1 , x test 1 ), . . . , (x valid m , x test m ) )\n= m∑ j=1 (( j m )n − ( j − 1 m )n) xtestij . (7)\nThis is in fact a weighted average of the test results. In case of a tie in validation results, i.e. if xvalidij+1 = xvalidij+2 = ... = x valid ij+k\n, one should assign an equal weight of(( j+1 m )n − ( jm)n + · · ·+ ( j+km )n − ( j+k−1m )n) /k =(( j+k m )n − ( j m )n) /k to the corresponding test samples7.\nThis estimator does not make any assumption about the performance distribution from which our observations are drawn. If we do use such an assumption (e.g. we know that the performance distribution of our architecture usually belongs to a certain family, e.g. Gaussian), we can add information to our estimator and possibly get an even better estimate (i.e. one with lower sampling error). For the Gaussian distribution, we can use the standard estimators of the parameters in Equation 6 to get a parametric estimator\nµ̂test + ρ̂ σ̂test Boon (N (0, 1))\nwhere µ̂test, ρ̂ and σ̂test are standard estimators of mean, correlation, and standard deviation respectively. A similar parametric estimator could be calculated for other distributions."
  }, {
    "heading": "3.3. Choice of n",
    "text": "Boon eliminates the problem of dependence on the number of experiments run, m. However, we still need to choose n, the number of experiments to which we normalize. This is similar to the choice one is facing when using a quantile – should one use the 75% one, the 95% one, or some other?\nThe choice of n most useful to a reader is when n is the number of candidate models that a practitioner would train\n7You can get an optimized Python implementation of the nonparametric estimator via pip install boon.\nbefore choosing the best one for some target application. Such number will differ domain to domain and will heavily depend on the computational cost of training the specific architecture on the specific domain. The n of interest may differ for each reader – ideally, researchers should characterize the architecture’s performance distribution as fully as possible and the readers may be able to deduce the value of Boon for whichever n they choose (up to a limit).\nLeaving an additional degree of freedom in the choice of metric creates a risk of cherry picking. However, in many areas of machine learning, there already are many available metrics. Still, the main reporting metric seems to quickly converge on each task. The first published paper makes a choice; the subsequent ones usually follow suit (else they risk a suspicion that the architecture is not competitive on the previous metric). We believe similar convergence is likely for Boon on each task.\nIn our experiments, we decided to use n = 5 – the AS Reader model which we use for our experiments takes about 2 hours to train on a single GPU, so someone replicating the Boo5 performance could expect to achieve it overnight, which seems a to be a reasonable requirement."
  }, {
    "heading": "3.4. Accounting for estimator uncertainty",
    "text": "Even Boon is just a single number whose estimate can be noisy. Hence, with Boon, as well as with mean and other ways of aggregating results of a wider population, we should always use appropriate statistical methods when trying to compare the quantitative performance of a new model against a baseline. This can be done using significance testing (such as the t-test), or with the help of confidence intervals, which seems to be the method preferred by a significant part of the scientific community (e.g. Gardner & Altman (1986) or Berrar & Lozano (2013)), since it allows us to disentangle the effect size from the uncertainty associated with noise and sample size.\nFor some theoretical distributions, there exist ways to calculate the hypothesis test or confidence interval analytically (e.g. using the t-test or standard normal quantiles for the Gaussian). However, in cases where the family of the performance distribution or of the estimator is not known, we need to resort to computational methods - usually Monte Carlo (if we do know at least the family of the performance distribution) or the Bootstrap (Efron, 1979) (if we do not). A brief description of how to calculate confidence intervals using the Bootstrap is provided in the Appendix."
  }, {
    "heading": "4. Experiment Results",
    "text": "Note: The data and code for their analysis can be found at http://gitlab.com/obajgar/boon, along with Python functions you can use to calculate Boon.\nWe have run several experiments to quantify the scope of the problems outlined in Section 2. We just briefly summarize the main results here for illustration; a more detailed description of the experiments and analysis in the form of an iPython notebook can be found in the Gitlab repository or in Appendix C.\nPerformance variation To estimate the random variation of results, we repeatedly8 trained models from two domains of deep learning: the ResNet (He et al., 2016) on the CIFAR100 dataset (Krizhevsky & Hinton, 2009) to represent Image Recognition and the Attention Sum Reader (AS Reader) (Kadlec et al., 2016) on the Children’s Book Test Common Nouns (CBT CN) (Hill et al., 2016) to represent Reading Comprehension. Each of these trainings generated a pair of a validation and test performances. The resulting empirical performance distributions are illustrated in Figure 1.\nIf we fix all hyperparameters, the interquartile ranges of the models’ accuracies are 0.98(±0.09)%9 and 1.20(±0.12)% (absolute). This is comparable to the median differences between published results on these datasets: 0.86% and 1.15% respectively10. Hence, random variation in performance cannot be considered negligible as is now often done. Furthermore, if we allow the hyperparameters to vary (in our case by random search), the result variance further increases, which further amplifies the outlined effects. In the\n8Specifically, 74 times for Resnet, 370 times for the AS Reader with fixed hyperparameters, and 197 times for the AS Reader with random hyperparameters.\n9± standard deviation computed using 99999 Bootstrap samples.\n10We looked at results of successive architectures evaluated on the two tasks as listed in (Huang et al., 2017; Munkhdalai & Yu, 2017). We sorted the results with respect to the test performance and then calculated the differences between successive models. From these we calculated the median. Full details can be found in the Gitlab repository.\ncase of the AS Reader the interquartile range increased to 2.9(±0.3)% when we randomly picked hyperparameters from a range applicable to training the model on a single GPU. However, note that the problem of result incomensurability due to hyperparameter optimization is not the focus of this work. The method that we present here is still applicable to the problem in the case of random hyperparameter sampling for which we include results, however we aim to compensate mainly for randomness due to parameter initialization and data shuffling – which is significant in itself, as we have just demonstrated.\nSeveral other articles confirm significant variation in model performance due to different random seeds: e.g van den Berg et al. (2016) in Speech Recognition, Henderson et al. (2018) in Deep Reinforcement Learning, or Reimers & Gurevych (2017) in Named Entity Recognition. They all agree that reporting performance scores of single models is insufficient to characterize architecture performance.\nEstimator variance Figure 2 shows the 95% confidence intervals of best single model results compared to the Boo5 performance for a range of result-pool sizes m. This is shown for the cases of both strong and weak test-validation correlation. In both cases Boo5 is significantly less noisy than the best-single-model result. In fact in the case of random hyperparameter search, Boon shows even smaller variation than the mean (due to the negative skew of the performance distribution).\nBest-model performance improves with the number of experiments We also mentioned that if only the performance of the best model is reported, the more experiments are run, the better the expected result. Figure 2b illustrates that this effect can indeed be fairly strong, if the validation performance is a good predictor of the test performance, as is the case of the AS Reader with random hyperparameter\n10 20 30 40 50 # experiments\n0.675\n0.680\n0.685\n0.690\n0.695\nte st\na cc\nur ac\ny\nbest single boo5 mean\n(a) Resnet (fixed hyperparameters; low test-validation correlation)\n0 20 40 60 80 100 # experiments\n0.58\n0.60\n0.62\n0.64\n0.66\nte st\na cc\nur ac\ny\nbest single boo5 mean\n(b) AS Reader (random hyperparameter sampling; high testvalidation correlation)\nFigure 2: Averages and 95% confidence intervals of test performance for three ways of aggregating results of multiple experiments for various numbers of experiments run. Each confidence interval was constructed using smoothed11 Bootstrap sampling from our pool of 75 for Resnet and 197 experiments for the AS Reader with fixed and random hyperparameters respectively. Since we strongly encourage researchers to provide confidence intervals for their results, we provide and overview of how to construct them using the Bootstrap in Appendix D.\nsearch, where the expectation of the best single model performance increases from 61.3% if we train it once, to 63.3% if we train it 5 times, to 63.5% for 20 times. This effect is nicely explained in more detail e.g. by Jensen & Cohen (2000). It gives a further argument for refraining from using this method and certainly also for publishing the number of experiments run, which is often not done. Boon is not subject to this effect.\nValidation-test correlation However, note that the assumption that validation performance is a good predictor of test performance is sometimes not true. In the two cases with fixed hyperparameters that we looked at, the Spearman correlation between validation and test results was only 0.10 and 0.18 respectively for the two models. The correlation significantly increases if we allow the hyperparameters to vary – to 0.83 for the AS Reader. These results are also illustrated in Figure 1c. Larger validation sets are also likely to improve this correlation, which can be understood as the degree of generalization from validation to test. Note that the problem of increasing expected performance mentioned above is relevant only in the case of higher correlation between validation and test results. The effect becomes very strong in the case where the performance we are reporting is also used for choosing the best model, which emphasizes the need for honest separation of validation and test data.\n11While Boon and mean could be sampled using vanilla Bootstrap, best-validation result is influenced only by a single value from the sample and hence uses only few values from the upper tier of our result pool, which makes our pool size insufficient. Hence we use Gaussian kernel smoothing (Scott, 1992) to expand our result pool."
  }, {
    "heading": "5. Discussion",
    "text": "Boon does fix the main flaws of reporting the best single model performance. However, let us have a look at some of its limitations.\nHyperparameter tuning This work does not fully compensate for improved expected results due to hyperparameter tuning, nor was it its primary aim. Boon is appropriate in the case of random hyperparameter sampling, where the performances in different runs are independent. However, this is not the case for more advanced hyperparameter optimization methods. The primary focus of this work was on tackling variability due to random initialization, data shuffling, and similar sources, which we have shown to be significant in itself. Compensation for more advanced hyperparameter tuning (and ensuring the comparability of models in that case) is certainly a worthwhile area for future research.\nMean, median, and other alternatives We do not claim our method to be strictly superior to traditional ways of aggregating results, such as mean or quantiles. However, we have outlined a case where using Boon is justified – situations where a final model to be deployed can be chosen from a pool of trained candidates. In such case, Boon is easily interpretable and more informative than a performance of a typical model, expressed by mean or median. Hence, we think Boon is a useful addition to the methodological toolbox along existing methods.\nJust a single number Boon is still just a single number whose ability to characterize the performance distribution is limited by its single dimension. Paper authors should try to characterise the performance distribution as fully as possible, which may involve a histogram, mean, standard deviation, ideally along a dataset containing the results of all experiments, from which an interested reader may be able to deduce whichever characteristic she finds interesting. Unfortunately, such characterization is usually lacking.\nHowever, alongside this detailed characterization, describing an architecture’s performance by a single number still has its appeal, especially for the purpose of comparison among architectures and choosing the best one according to some criterion (in fact, each quantitative score can be understood as a proxy for ordering architectures with respect to such criterion of interest, such as expected performance of the best model out of n). We have explained why, in some cases, Boon may be useful for such purpose.\nComputational cost Some may deem Boon impractical due to its requirement to train architectures many times, which may be very expensive in some cases. However, result stochasticity needs to be addressed to produce reliable results, and it is hard to imagine a general method to do so without repeated evaluation12. Researchers should focus on architectures which they can evaluate properly given their resources. However, the main target of our criticism is not projects whose resources are stretched by a single training; it is projects that do have the necessary resources for multiple evaluations but use them to produce better-looking results rather than results that are more informative and robust."
  }, {
    "heading": "6. Conclusion",
    "text": "Reporting just the best single model performance is not statistically sound. This practice in machine learning research should change if the research is to have lasting value. Reviewers can play an important role in bringing this change.\nStill, asking for the performance of a best model out of n can have valid reasons. For the situations where the best-model performance is indeed a good metric, we are suggesting Boon as a way to evaluate it properly.\nAppendix\nA. Boon of the Gaussian Distribution A.1. Single evaluation dataset\nIn the simpler case in which the best model is chosen with respect to the same dataset on which the performance is\n12Though different methods can substantially differ in the number of evaluations that they require.\nthen reported, we can substitute the p.d.f. and c.d.f. of the Gaussian distribution into Equation 2 to get\nBoon(N (µ, σ2)) =∫ ∞ −∞ xn 1√ 2πσ2 e (x−µ)2 2σ2 Φn−1 ( x− µ σ ) dx (8)\nwhere Φ is the c.d.f. of a standard normal random variable. Substituting z = x−µσ , dx = σdz, yields∫ ∞\n−∞ n(µ+ σz) 1√ 2πσ2 e z2 2 Φn−1 (z)σdz =\n= µ ∫∞ −∞ n 1√ 2π e z2 2 Φn−1 (z) dz+\n+σ ∫ ∞ −∞ nz 1√ 2π e z2 2 Φn−1 (z) dz = µ+σBoon (N (0, 1))\n(9)\n(the first integrand has the form of the p.d.f. found above and hence integrates to one) so the expected maximum is neatly expressed in terms of a maximum of a standard normal and is linearly proportional to both the mean and the standard deviation. Once n is fixed for comparison purposes, Boon (N (0, 1)) is just a constant, e.g. Boo5 (N (0, 1)) ≈ 1.163, Boo10 (N (0, 1)) ≈ 1.539.\nA.2. Test-validation evaluation\nLet us turn to the case of reporting the expected test set performance of a best-validation model. If we model the validation and test performances by a Bivariate Normal Distribution with valid-test correlation ρ, means µval, µtest, and variances σ2val, σ 2 test, then given a validation performance xval, the test performance is distributed normally with conditional expectation\nEtv(xval) = µtest + ρ σtest σval (xval − µval)\nwhich gives\nBoon(N (µ, σ2)) =∫ ∞ −∞ ( µtest + ρ σtest σval (xval − µval) ) n·\n· 1√ 2πσ2val e (x−µval) 2 2σ2val Φn−1 ( x− µval σval ) dx . (10)\nUsing the same two tricks as above, this can be simplified to\nBoon(N (µ, σ2)) = µtest +ρ σtest Boon (N (0, 1)) (11)\nwhere Boon (N (0, 1)) is the single-evaluation expected maximum of the standard normal distribution as defined above."
  }],
  "year": 2018,
  "references": [{
    "title": "Significance tests or confidence intervals: which are preferable for the comparison of classifiers",
    "authors": ["D. Berrar", "J.A. Lozano"],
    "venue": "Journal of Experimental & Theoretical Artificial Intelligence,",
    "year": 2013
  }, {
    "title": "A Thorough Examination of the CNN / Daily Mail Reading Comprehension",
    "authors": ["D. Chen", "J. Bolton", "C.D. Manning"],
    "year": 2016
  }, {
    "title": "Emerging trends: I did it, I did it, I did it",
    "authors": ["K.W. Church"],
    "venue": "Natural Language Engineering,",
    "year": 2017
  }, {
    "title": "Empirical methods for artificial intelligence",
    "authors": ["P.R. Cohen"],
    "venue": "MIT press Cambridge,",
    "year": 1995
  }, {
    "title": "Attention-over-Attention Neural Networks for Reading Comprehension",
    "authors": ["Y. Cui", "Z. Chen", "S. Wei", "S. Wang", "T. Liu", "G. Hu"],
    "year": 2017
  }, {
    "title": "Gated-Attention Readers for Text Comprehension",
    "authors": ["B. Dhingra", "H. Liu", "W.W. Cohen", "R. Salakhutdinov"],
    "venue": "In ACL,",
    "year": 2017
  }, {
    "title": "Bootstrap methods: Another look at the jackknife",
    "authors": ["B. Efron"],
    "venue": "The Annals of Statistics, pp",
    "year": 1979
  }, {
    "title": "Offspring from reproduction problems: What replication failure teaches us",
    "authors": ["A. Fokkens", "M. Van Erp", "M. Postma", "T. Pedersen", "P. Vossen", "N. Freire"],
    "venue": "In ACL,",
    "year": 2013
  }, {
    "title": "Confidence intervals rather than p values: estimation rather than hypothesis testing",
    "authors": ["M.J. Gardner", "D.G. Altman"],
    "venue": "British Medical Journal,",
    "year": 1986
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "year": 2016
  }, {
    "title": "Deep reinforcement learning that matters",
    "authors": ["P. Henderson", "R. Islam", "P. Bachman", "J. Pineau", "D. Precup", "D. Meger"],
    "venue": "In AAAI,",
    "year": 2018
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "The goldilocks principle: Reading children’s books with explicit memory representations",
    "authors": ["F. Hill", "A. Bordes", "S. Chopra", "J. Weston"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Densely connected convolutional networks",
    "authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"],
    "year": 2017
  }, {
    "title": "The case for open computer programs",
    "authors": ["D.C. Ince", "L. Hatton", "J. Graham-Cumming"],
    "year": 2012
  }, {
    "title": "Multiple comparisons in induction algorithms",
    "authors": ["D.D. Jensen", "P.R. Cohen"],
    "venue": "Machine Learning,",
    "year": 2000
  }, {
    "title": "Text Understanding with the Attention Sum Reader Network",
    "authors": ["R. Kadlec", "M. Schmid", "O. Bajgar", "J. Kleindienst"],
    "venue": "In ACL,",
    "year": 2016
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "year": 2009
  }, {
    "title": "Reasoning with memory augmented neural networks for language comprehension",
    "authors": ["T. Munkhdalai", "H. Yu"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "The logic of scientific discovery",
    "authors": ["K.R. Popper"],
    "year": 1959
  }, {
    "title": "Consequences of variability in classifier performance estimates",
    "authors": ["T. Raeder", "T.R. Hoens", "N.V. Chawla"],
    "venue": "In IEEE 10th International Conference on Data Mining (ICDM), pp",
    "year": 2010
  }, {
    "title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
    "authors": ["N. Reimers", "I. Gurevych"],
    "venue": "In EMNLP,",
    "year": 2017
  }, {
    "title": "Multivariate density estimation: Theory, practice, and visualization",
    "authors": ["D.W. Scott"],
    "year": 1992
  }, {
    "title": "Winner’s curse? on pace, progress, and empirical rigor",
    "authors": ["D. Sculley", "J. Snoek", "A. Wiltschko", "A. Rahimi"],
    "venue": "In ICLR Workshop,",
    "year": 2018
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Natural Language Comprehension with the EpiReader",
    "authors": ["A. Trischler", "Z. Ye", "X. Yuan", "K. Suleman"],
    "venue": "In EMNLP,",
    "year": 2016
  }, {
    "title": "Training variance and performance evaluation of neural networks",
    "authors": ["E. van den Berg", "B. Ramabhadran", "M. Picheny"],
    "venue": "in speech. arXiv,",
    "year": 2016
  }, {
    "title": "Machine comprehension using matchlstm and answer pointer",
    "authors": ["S. Wang", "J. Jiang"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Dynamic coattention networks for question answering",
    "authors": ["C. Xiong", "V. Zhong", "R. Socher"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Words or characters? fine-grained gating for reading comprehension",
    "authors": ["Z. Yang", "B. Dhingra", "Y. Yuan", "J. Hu", "W.W. Cohen", "R. Salakhutdinov"],
    "venue": "In ICLR,",
    "year": 2017
  }],
  "id": "SP:a444891ae91c9b87a432c8373b8be050c24aa47f",
  "authors": [{
    "name": "Ondrej Bajgar",
    "affiliations": []
  }, {
    "name": "Rudolf Kadlec",
    "affiliations": []
  }, {
    "name": "Jan Kleindienst",
    "affiliations": []
  }],
  "abstractText": "We point out important problems with the common practice of using the best single model performance for comparing deep learning architectures, and we propose a method that corrects these flaws. Each time a model is trained, one gets a different result due to random factors in the training process, which include random parameter initialization and random data shuffling. Reporting the best single model performance does not appropriately address this stochasticity. We propose a normalized expected best-out-of-n performance (Boon) as a way to correct these problems.",
  "title": "A Boo(n) for Evaluating Architecture Performance"
}