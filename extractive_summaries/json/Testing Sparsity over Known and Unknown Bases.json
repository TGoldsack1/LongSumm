{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Property testing is the study of algorithms that query their input a small number of times and distinguish between whether their input satisfies a given property or is “far” from satisfying that property. The quest for efficient testing algorithms was initiated by (Blum et al., 1993) and (Babai et al., 1991) and later explicitly formulated by (Rubinfeld & Sudan, 1996) and (Goldreich et al., 1998). Property testing can be viewed as a relaxation of the traditional notion of a decision problem, where the relaxation is quantified in terms of a distance parameter. There has been extensive work in this area over the last couple of decades; see, for instance, the surveys (Ron, 2008) and (Rubinfeld & Shapira, 2006) for some different perspectives.\n*Equal contribution 1Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India. Correspondence to: Siddharth Barman <barman@iisc.ac.in>, Arnab Bhattacharyya <arnabb@iisc.ac.in>, Suprovat Ghoshal <suprovat@iisc.ac.in>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nAs evident from these surveys, research in property testing has largely focused on properties of combinatorial and algebraic structures, such as bipartiteness of graphs, linearity of Boolean functions on the hypercube, membership in errorcorrecting codes or representability of functions as concise Boolean formulae. In this work, we study the question of testing properties of continuous structures, specifically properties of vectors and matrices over the reals.\nOur computational model is a natural extension of the standard property testing framework by allowing queries to be linear measurements of the input. Let P ⊂ Rd be a property of real vectors. Let dist : Rd → R>0 be a “distance” function such that dist(x) = 0 for all x ∈ P . We say that an algorithm A is a tester for P with respect to dist and with parameters ε, δ > 0 if for any input y ∈ Rn, the algorithm A observes My where M ∈ Rq×d is a randomized matrix and has the following guarantee:\n(i) If y ∈ P , PrM[A(My) accepts] > 1− δ.\n(ii) If dist(y) > ε, PrM[A(My) accepts] 6 δ.\nWe call each inner product between the rows of M and y a (linear) query, and the number of rows q = q(ε, δ) is the query complexity of the tester. The running time of the tester A is its running time on the outcome of its queries. As typical in property testing, we do not count the time needed to evaluate the queries. If P ⊂ Rd×p is a property of real matrices with an associated distance function dist : Rd×p → R>0, testing is defined similarly: given an input matrix Y ∈ Rd×p, the algorithm observes MY for a random matrix M ∈ Rq×d with analogous completeness and soundness properties. A linear projection of an input vector or matrix to a low-dimensional space is also called a linear sketch or a linear measurement. The technique of obtaining small linear sketches of high-dimensional vectors has been used to great effect in algorithms for streaming (e.g., (Alon et al., 1996; McGregor, 2014)) and numerical linear algebra (see (Woodruff, 2014) for an excellent survey). Because GPUs are specially designed to optimize matrix-vector computation, many modern optimization and learning algorithms work with linear sketches of their input.\nWe focus on testing whether a vector is sparse with respect\nto some basis.1 A vector x is said to be k-sparse if it has at most k nonzero coordinates. Sparsity is a structural characteristic of signals of interest in a diverse range of applications. It is a pervasive concept throughout modern statistics and machine learning, and algorithms to solve inverse problems under sparsity constraints are among the most successful stories of the optimization community (see the book (Hastie et al., 2015)). The natural property testing question we consider is whether there exists a solution to a linear inverse problem under a sparsity constraint.\nThere are two settings in which we investigate the sparsity testing problem.\n(a) In the first setting, the basis is not known in advance. For input vectors y1,y2, . . . ,yp ∈ Rd, the property to test is whether there exists a matrix A ∈ Rd×m and k-sparse unit vectors x1,x2, . . .xp ∈ Rm such that yi = Axi for all i ∈ [p]. Note that m is specified as a parameter and could be much larger than d (the overcomplete case). In this setting, we restrict the unknown A to be a (ε, k)-RIP matrix which means that (1 − ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ for any ksparse x. This is a standard assumption made in many related works (see Section 1.2 for details).\nIn this setting, we design an efficient tester for this property that projects the inputs toO(ε−2 log p) dimensions and, informally speaking, rejects if for all (ε, k)-RIP matrices A, there is some yi such that yi −Axi has large norm for all “approximately sparse” xi.\n(b) In the second setting, a design matrix A ∈ Rd×m is known explicitly, and the property to test is whether a given input vector y ∈ Rd equals Ax for a k-sparse vector x ∈ Rm. For instance, A can be the Fourier basis or an overcomplete dictionary in an image processing application. We approach this problem in full generality, without putting any restriction on the structure of A.\nInformally, our main result in this setting is that for any design matrix A, there exists a tester projecting the input y toO(k logm) dimensions that rejects if y−Ax has large norm for any O(k)-sparse x. The running time of the tester is polynomial inm. As we describe in Section 1.2, previous work in numerical linear algebra yields a tester with the same query complexity and with qualitatively similar soundness guarantees but which requires running time exponential in m or assumptions about the matrix A.\nRemark 1.1 (Problem Formulation). Note that the settings considered in the known and unknown design matrix settings\n1With slight abuse of notation, we use the term basis to denote the set of columns of a design matrix. The columns might not be linearly independent.\nare quite different from each other. In particular, for the known design setting, the input is a single vector. However, given a single input vector y ∈ Rd, the analogous unknown design testing question would be moot, since one can always consider the vector y to be the design matrix A, in which it trivially admits a 1-sparse representation. For the same reason, unknown design testing is interesting only when the number of vectors p exceeds m.\nIn both of the above tests, the measurement matrix is a random matrix with iid gaussian entries, chosen so as to preserve norms and certain other geometric properties upon dimensionality reduction.2 In particular, our testers are oblivious to the input. It is a very interesting open question as to whether non-oblivious testers can strengthen the above results."
  }, {
    "heading": "1.1. Our Results",
    "text": "We now present our results more formally. For integer m > 0, let Sm−1 = {x ∈ Rm : ‖x‖ = 1}, and let Spmk = {x ∈ Sm−1 : ‖x‖0 6 k}.3\nTheorem 1.2 (Unknown Design Matrix). Fix ε, δ ∈ (0, 1) and positive integers d, k,m and p, such that (k/m)1/8 < ε < 1100 and k > 10 log 1 ε . There exists a tester with query complexity O(ε−2 log (p/δ)) which, given as input vectors y1,y2, . . . ,yp ∈ Rd, has the following behavior (where Y is the matrix having y1,y2, . . . ,yp as columns):\n– Completeness: If Y admits a decomposition Y = AX, where A ∈ Rd×m satisfies (ε, k)-RIP and X ∈ Rm×p with each column of X in Spmk , then the tester accepts with probability > 1− δ.\n– Soundness: Suppose Y does not admit a decomposition Y = A(X + Z) + W with\n1. The design matrix A ∈ Rd×m being (ε, k)-RIP, with ‖ai‖ = 1 for every i ∈ [m]. 2. The coefficient matrix X ∈ Rm×p being column wise `-sparse, where ` = O(k/ε4).\n3. The error matrices Z ∈ Rm×p and W ∈ Rd×p satisfying\n‖zi‖∞ 6 ε2, ‖wi‖2 6 O(ε1/4) for all i ∈ [p].\nThen the tester rejects with probability > 1− δ. 2If evaluating the queries efficiently was an objective, one could also use sparse dimension reduction matrices (Dasgupta et al., 2010; Kane & Nelson, 2014; Bourgain et al., 2015), but we do not pursue this direction here.\n3Here, ‖x‖0 denotes the the sparsity of the vector, ‖x‖0 := |{i ∈ [m] | xi 6= 0}|. Without any subscript, ‖ · ‖ denotes the `2-norm: ‖x‖ := √∑ i x 2 i .\nThe contrapositive of the soundness guarantee from the above theorem states that if the tester accepts, then matrix Y admits a factorization of the form Y = A(X+Z)+W, with error matrices Z and W having `∞ and `2 error bounds. The matrix X+Z is a sparse matrix with `∞-based thresholding, and W is an additive `2-error term.4\nTheorem 1.3 (Known Design Matrix). Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m such that ‖ai‖ = 1 for every i ∈ [m]. There exists a tester with query complexity O(kε−2 log(m/δ)) that behaves as follows for an input vector y ∈ Rd:\n– Completeness: If y = Ax for some x ∈ Spmk , then the tester accepts with probability 1.\n– Soundness: If ‖Ax− y‖2 > ε for every x : ‖x‖0 6 K, then the tester rejects with probability > 1 − δ. Here, K = O(k/ε2).\nThe running time of the tester is poly(m, k, 1/ε).\nA different way of stating the result is that the tester, using O(kε−2 log(m/δ)) linear queries, accepts with probability 1 if y = Ax for a k-sparse x ∈ Rm and rejects with probability 1− δ if ‖Ax− y‖ > ε‖x‖ for every O(k/ε2)sparse x. To complement this result, we show that a better tradeoff between the sparsity and reconstruction error is likely to be impossible.\nTheorem 1.4 (Hardness). Assume SAT does not have nO(log logn)-time algorithms, and let η be any constant less than 1. Then, there does not exist a polynomial time algorithm that, given input A ∈ Rd×m (where ‖ai‖ = 1 for every i ∈ [m]), y ∈ Rd and ε > 0, distinguishes with constant probability between the following two cases: (i) y = Ax for a k-sparse x, and (ii) ‖y −Ax‖ > ε‖x‖η for every (k/ε2)-sparse x.\nNote that the above hardness applies to any polynomial time algorithm, not just sketching algorithms.\nWe also give tolerant variants of these testers (Theorems H.1 and H.2) which can handle bounded noise for the completeness case. Moreover, the tester for the known design case can be converted into a new sketching algorithm for sparse recovery (Theorem D.1).\nFinally, we also give an algorithm for testing dimensionality, which is based on similar techniques.\nTheorem 1.5 (Testing Dimensionality). Fix ε, δ ∈ (0, 1), positive integers d, k and p, where k > 10ε2 log d. There exists a tester with query complexity O(p log δ−1), which\n4Theorem 1.2 can be restated in terms of incoherent (instead of RIP) design matrices as well. This follows from the fact that the incoherence and RIP constants of a matrix are order-wise equivalent. This observation is formalized in Appendix F.\ngives as input vectors y1, . . . ,yp ⊂ Sd−1, has the following behavior:\n– Completeness: If rank(Y ) 6 k, then the tester accepts with probability > 1− δ.\n– Soundness: If rankε(Y ) > k′, then the tester rejects with probability > 1− δ. Here, k′ = 20k/ε2\nThe soundness criteria in the above Theorem is stated in terms of the ε-approximate rank of a matrix (see Definition E.1). This is a well-studied relaxation of the standard definition of rank, and has applications in approximation algorithms, communication complexity and learning theory (see (Alon et al., 2013) and references therein)."
  }, {
    "heading": "1.2. Related Work",
    "text": "Although, to the best of our knowledge, the testing problems we consider have not been explicitly investigated before, there are several related areas of study that frame our results in their proper context.\nUnknown Design setting. In the setting of the unknown design matrix, the question of recovering the design matrix and the sparse representation (as opposed to our problem of testing their existence) is called the dictionary learning or sparse coding problem. The first work to give a dictionary learning algorithm with provable guarantees was (Spielman et al., 2012) where the dictionary was restricted to be square. For the more common overcomplete setting, (Arora et al., 2014) and (Agarwal et al., 2014) independently gave algorithms with provable guarantees for dictionaries satisfying incoherence and RIP respectively. All of these (as well as other more recent) works assume distributions from which the input samples are generated in an i.i.d fashion. In contrast, our work is in the agnostic setting and hence, is incomparable with these results.\nIt is known that the dictionary learning problem is NP-hard, even for square dictionaries (Razaviyayn et al., 2014; Tillmann, 2015). In fact, (Tillmann, 2015) shows that unless SAT has a quasi-polynomial time algorithm, it is impossible, given Y ∈ Rd×p, to approximate in polynomial time the minimum k upto a factor 2log\n1−ε d (for any ε > 0) such that Y = AX where each column of X ∈ Rd×p is k-sparse. This motivates our bicriteria relaxation of both the sparsity as well as the additive error in Theorem 1.2.\nKnown Design setting. Some results about testing sparsity in the known design setting are implicit in recent work on streaming algorithms and oblivious subspace embeddings. Of particular interest are the following results:\nTheorem 1.6 (Implicit in (Kane et al., 2010)). Fix ε ∈ (0, 1), positive integers m, k and an invertible matrix A ∈\nRm×m. Then, there is a tester with query complexity O(ε−2 log(m)) that, for an input y ∈ Rm, accepts with probability at least 2/3 if y = Ax for some k-sparse x ∈ Zm, and rejects with probability 2/3 if y 6= Ax for all (1 + ε)k-sparse x ∈ Zm. The running time of the algorithm is poly(m, 1/ε). Theorem 1.7 (Implicit in prior work, see (Woodruff, 2014)). Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m. Then, there is a tester with query complexity O(kε−2 log(m/δ)) that, for an input vector y ∈ Rd, accepts with probability 1 if y = Ax for some k-sparse x and rejects with probability at least 1− δ if ‖y −Ax‖ > ε for all k-sparse x. The running time of the tester is the time required to solve the following optimization problem:\nx̂ = arg min x′∈K ‖SAx′ − Sy‖ = arg min x′∈K ‖S(Ax′ − y)‖\n(1) where S ∈ Rq×d is a random sketch matrix (where q d) and K = {x : ‖x‖0 6 k}\nDetailed descriptions of the algorithms and proof sketches for the above Theorems are given in Section B.4. The algorithms from the above theorems come with significant limitations. In particular, the guarantees for Theorem 1.6 hold only when the design matrix is invertible. On the other hand, the running time for the algorithm in Theorem 1.7 is the cost of solving the optimization problem in Equation (1), which is known to be NP-hard for general matrices.\nThe problem of testing sparsity has also been studied in non-sketching settings as well, where the algorithm is allowed access to the entire input. In particular, (Natarajan, 1995) gave a bicriteria-approximation algorithm, where the blowup in the sparsity is proportional to ‖A†‖22 (which can be large if A is ill conditioned).\nTesting Dimensionality. In (Czumaj et al., 2000), some problems in computational geometry were studied from the property testing perspective, but the problems involved only discrete structures. (Krauthgamer & Sasson, 2003) studied the problem of testing dimensionality, but their notion of farness from being low-dimensional is different from ours5. (Chierichetti et al., 2017) gave approximation algorithms for computing approximate rank of the matrix, in the setting where the algorithms have full access to the input."
  }, {
    "heading": "1.3. Discussion",
    "text": "A standard approach to designing a testing algorithm for a property P is the following: we identify an alternative property P ′ which can be tested efficiently and exactly, while satisfying the following:\n5In their setup, a sequence of vectors y1, . . . ,yp is ε-far from being d-dimensional if at least εp vectors need to be removed to make it be of dimension d\n(i) Completeness: If an instance satisfies P , then it satisfies P ′.\n(ii) Soundness: If an instance satisfies P ′, the it is close to satisfying P .\nIn other words, we reduce the property testing problem to that of finding a efficiently testable property P ′, which can be interpreted as a surrogate for property P . The inherent geometric nature of the problems looked at in this paper motivate us to look for P ′s which are based around convex geometry and high dimensional probability.\nFor the unknown design setting, we are intuitively looking for a P ′ based on a quantity ω that robustly captures sparsity and is easily computable using linear queries, in the sense that ω is small when the input vectors have a sparse coding and large when they are “far” from any sparse coding. Moreover, ω needs to be invariant with respect to isometries and nearly invariant with respect to near-isometries. A natural and widely-used measure of structure that satisfies the above mentioned properties is the gaussian width.\nDefinition 1.8. The gaussian width of a set S ⊆ Rd is: ω(S) = Eg[supv∈S〈g,v〉] where g ∈ Rd is a random vector drawn from N(0, 1)d, i.e., a vector of independent standard normal variables.\nThe gaussian width of S measures how well on average the vectors in S correlate with a randomly chosen direction. It is invariant under orthogonal transformations of S as the distribution of g is spherically symmetric. It is a well-studied quantity in high-dimensional geometry ((Vershynin, 2015; Mendelson & Vershynin, 2002)), optimization ((Chandrasekaran et al., 2012; Amelunxen et al., 2013)) and statistical learning theory ((Bartlett & Mendelson, 2002)). The following bounds are well-known.\nLemma 1.9 (See, for example, (Rudelson & Vershynin, 2008; Vershynin, 2015)).\n(i) If S is a finite subset of Sd−1, then ω(S) 6 √ 2 log |S|.\n(ii) ω(Sd−1) 6 √ d\n(iii) If S ⊆ Sd−1 is of dimension k, then ω(S) 6 √ k.\n(iv) ω(Spdk) 6 2 √ 3k log(d/k) when d/k > 2 and k > 4.\nIn the context of Theorems 1.2 and 1.5, one can observe that whenever a given set satisfies sparsity or dimensionality constraints, the gaussian width of such sets are small (points (iii) and (iv) from the above Lemma). Therefore, one can hope to test dimensionality or sparsity by computing an empirical estimate of the gaussian width and comparing the estimate to the results in Lemma 1.9. While completeness of such testers would follow directly from concentration of measure, establishing soundness would require us to show\nthat approximate converses of points (iii) and (iv) hold as well i.e., whenever the gaussian width of the set S is small, it can be approximated by sets which are approximately sparse in some design matrix (or have low rank).\nFor the soundness direction of Theorem 1.2, the above arguments are made precise using Lemma 3.3 and Theorem 3.2, which show that small gaussian width sets can be approximated by random projections of sparse vectors and vectors with small `∞-norm. For Theorem 1.5, we use lemma E.2 which shows that sets with small gaussian width have small approximate rank.\nFor the known design setting, we are looking for aP ′, which would ensure that if a given point y ∈ Rd satisfies P ′, then it is close to having a sparse representation in the matrix A. Towards this end, the approximate Carathéodory’s theorem states that if a point y ∈ Rd belonging to the convex-hull of A, then it is close to another point which admits a sparse representation. On the other hand, if a unit vector x ∈ Sd−1 ∩Rd+ were k-sparse to begin with , then it can be seen that the corresponding y = Ax would belong to the convex hull of √ k · A. These observations taken together, seem to suggest that one can take P ′ to be membership in the convex-hull of √ k ·A. This intuition is made precise in the analysis of the tester in Section 4."
  }, {
    "heading": "1.4. Organization",
    "text": "Section 2 introduces notations and preliminaries used in the rest of the paper. In Sections 3 and 4, we design and analyze the testers for the unknown and known basis setting respectively. Section 5 contains empirical results which supplement Section 3. In Section B we prove additional lemmas used in the proof of Theorem 3.2, and in Section A we prove Theorem 3.2. In Section C, we prove Theorem C.1, a stronger version of Theorem 1.4. In Section D, we show that Theorem 1.3 yields a sketching algorithm for sparse recovery. In Section E, we design and analyze the dimensionality tester. In Section G, we describe the results for testing sparsity in the known case implicit in previous work. Finally, in Section H, we give noise tolerant testers for the known and unknown basis settings."
  }, {
    "heading": "2. Preliminaries",
    "text": "Given S ⊂ Rd, we shall use conv(S) to denote the convex hull of S. For a vector x ∈ Rd, we use ‖ · ‖p to denote its `p-norm, and we will drop the indexing when p = 2. We denote the `2-distance of the point x to the set S by dist(x, S). We recall the definition of ε-isometry:\nDefinition 2.1. Given sets S ⊂ Rm and S′ ⊂ Rn (for some m,n ∈ N), we say that S′ is an ε-isometry of S, if there exists a mapping ψ : S 7→ S′ which satisfies the following\nproperty:\n∀x,y ∈ S : (1−ε)‖x−y‖ 6 ‖ψ(x)−ψ(y)‖ 6 (1+ε)‖x−y‖\nFor the unknown design setting, we shall require the notion of Restricted Isometry Property, which is defined as follows:\nDefinition 2.2 ((ε, k)-RIP). A matrix A ∈ Rd×m satisfies (ε, k)-RIP, if for every x ∈ Spmk the following holds:\n(1− ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ (2)\nWe use the following version of Gordon’s Theorem repeatedly in this work.\nTheorem 2.3 (Gordon’s Theorem (Gordon, 1985)). Given S ⊂ SD−1 and a random gaussian matrix G ∼ 1√ d′ N(0, 1)d ′×D, we have\nE G [ max x∈S ‖Gx‖2 ] 6 1 + ω(S)√ d′\nIt directly implies the following generalization of the Johnson-Lindenstrauss lemma.\nTheorem 2.4 (Generalized Johnson-Lindenstrauss lemma). Let S ⊆ Sn−1. Then there exists linear transformation Φ : Rn 7→ Rd′ , for d′ = O ( ω(S)2\nε2\n) , such that Φ is an\nε-isometry on S. Moreover, Φ ∼ 1√ d′ N(0, 1)d ′×n is an ε-isometry on S with high probability.\nIt can be easily verified that the quantity maxx∈S ‖Gx‖2 is 1-Lipschitz with respect to G. Therefore, using Gaussian concentration for Lipschitz functions, we get the following corollary :\nCorollary 2.5. Let S and G be as in Theorem 2.3. Then for all ε > 0, we have\nPr G ( max x∈S ‖Gx‖2 > 1 + ( 1 + ε )ω(S)√ d′ ) 6 exp ( −O(εω(S))2\n) The following lemma gives concentration for the gaussian width:\nLemma 2.6 (Concentration on the gaussian width (Boucheron et al., 2013)). Let S ⊂ Rd. Let W = supv∈S〈g,v〉 where g is drawn from N(0, 1)d. Then:\nPr[|W −EW | > u] < 2e− u2 2σ2\nwhere σ2 = supv∈S ( ‖v‖22 ) . Notice that the bound is dimension independent.\nLastly, we shall use the `2-variant of the approximate Carathéodory’s Theorem:\nTheorem 2.7. (Theorem 0.1.2 (Vershynin, 2016) ) Given X = {w1, . . . ,wp} where ‖wi‖ 6 1 for every i ∈ [p]. Then for every choice z ∈ conv ( X )\nand k ∈ N, there exists wi1 ,wi2 , . . . ,wik such that∥∥∥∥1k ∑\nj∈[k]\nwij − z ∥∥∥∥ 6 2√k (3)"
  }, {
    "heading": "2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector",
    "text": "We record here simple lemmas bounding the number of linear queries needed to estimate the gaussian width of a set and the length of a vector.\nLemma 2.8 (Estimating Gaussian Width using linear queries). For any u > 4, ε ∈ (0, 1/2) and δ > 0, there is a randomized algorithm that given a set S ⊆ Rd and ‖v‖ ∈ [1 ± ε] for all v ∈ S, computes ω̂ such that ω(S)− u 6 ω̂ 6 ω(S) + u with probability at least 1− δ. The algorithm makes O(log(1/δ) · |S|) linear queries to S.\nProof. By Lemma 2.6, for a random g ∼ N(0, 1)d, supv∈S〈g,v〉 is away from ω(S) by u with probability at most 2e−16/4.5 < 0.1. By the Chernoff bound, the median of O(log δ−1) trials will satisfy the conditions required of ω̂ with probability at least 1− δ. Lemma 2.9 (Estimating norm using linear queries). Given ε ∈ (0, 1/2) and δ > 0, for any vector x ∈ Rd , only O(ε−2 log δ−1) linear queries to x suffice to decide whether ‖x‖ ∈ [1− ε, 1 + ε] with success probability 1− δ.\nProof. It is easy to verify that Eg∼N(0,1)d [〈g,x〉2] = ‖x‖2. Therefore, it can be estimated to a multiplicative error of (1 ± ε/2) by taking the average of the squares of linear measurements using O ( 1 ε2 log 1 δ ) -queries. For the case ‖x‖2 6 2, a multiplicative error (1± ε/2) implies an additive error of ε. Furthermore, when ‖x‖2 > 2, a multiplicative error of (1± ε/2) implies that L > 2(1− ε/2) > 1 + ε for ε < 1/2."
  }, {
    "heading": "3. Analysis for Unknown Design setting",
    "text": "In this section, we prove Theorem 1.2. Let S denote the set {y1, . . . ,yp}. Our testing algorithm is shown in Algorithm 1.\nThe number of linear queries made by the tester is O(pε−2 log(p/δ)) in Line 1 and O(p log δ−1) in Line 2."
  }, {
    "heading": "3.1. Completeness",
    "text": "Assume that for each i ∈ [p], yi = Axi for a matrix A ∈ Rd×m satisfying (ε, k)-RIP and xi ∈ Spmk . By definition\nAlgorithm 1 SparseTestUnknown 1: Use Lemma 2.9 to decide with probability at least 1− δ/2 if there exists yi such that ‖yi‖ 6∈ [1− 2ε, 1 + 2ε]. Reject if so.\n2: Use Lemma 2.8 to obtain ω̂, an estimate of ω(S) within additive error √ 3k log(m/k) with probability at least\n1− δ/2. 3: Accept if ω̂ 6 4 √ 3k log(m/k), else reject.\nof RIP, we know that 1− ε 6 ‖yi‖ 6 1 + ε, so that Line 1 of the algorithm will pass with probability at least 1− δ/2. From Lemma 1.9, we know that ω({x1, . . .xp}) 6 2 √\n3k log(m/k). Lemma 3.1 shows that the gaussian width of S is approximately the same; its proof, deferred to the appendix (Section B.4), uses Slepian’s Lemma (Lemma B.3).\nLemma 3.1. LetX ⊂ Sm−1 be a finite set, and let S ⊂ Rd be an ε-isometric embedding of X . Then\n(1− ε)ω(X) 6 ω(S) 6 (1 + ε)ω(X) (4)\nHence, the gaussian width of y1, . . . ,yp is at most 2(1 + ε) √\n3k log(m/k). Taking into account the additive error in Line 2, we see that with probability at least 1 − δ/2, ω̂ 6 (3 + 2ε) √ 3k log(m/k) 6 4 √ 3k log(m/k). Hence, the tester accepts with probability at least 1− δ."
  }, {
    "heading": "3.2. Soundness",
    "text": "As mentioned before, in order to prove soundness we need to show that whenever the gaussian width of the set S is small, it is close to some sparse point-set. Let ω∗ = 4 √ 3k log mk . We shall break the analysis into two cases: Case (i) { ω∗ > (ε/C)2 √ d } : For this case, we use the\nfact random projection of discretized sparse point-sets (Definition A.1) form an appropriated cover of S. This is formalized in the following theorem, which in a sense shows an approximate inverse of Gordon’s Theorem for sparse vectors:\nTheorem 3.2. Given ε > 0 and integers C, d, k and m, let n = O ( k ε2 log(m/k) ) . Suppose m > k/ε8. Let Φ : Rm 7→ Rn be drawn from 1√ n N(0, 1)n×m. Then, for ` = O(kε−4), with high probability, the set Φnorm(Ŝp m\n` ) is an O(ε 1/4)-cover of Sn−1, where\nΦnorm(x) = Φ(x)/‖Φ(x)‖2.\nThe proof of the above Theorem is deferred to Section A. From the choice of parameters we have d 6 C\n′k ε2 log m k\nTherefore, using the above Theorem we know that there\nexists (ε, k)-RIP matrix Φ ∈ Rd×m such that Φnorm ( Spm` ) is an O(ε1/4)-cover of Sd−1 (and therefore it is a ε1/4cover of S). Therefore, there exists X ∈ Rm×p such that Y = Φ(X) + E where the columns of X and E satisfy the respective ‖ · ‖0 and ‖ · ‖2-upper bounds respectively. Case (ii) { ω∗ 6 (ε/C)2 √ d } : For this case, we use the\nfollowing result on the concentration of `∞-norm: Lemma 3.3. Given S ⊂ Sd−1, we have\nPr R∼Od\n[ max\ny∈R(S) ‖y‖∞ 6 C\nω(S)\nd1/2\n] > 1\n2\nwhere Od is the orthogonal group in Rd i.e., R is a uniform random rotation.\nAlthough this concentration bound is known, for completeness we give a proof in the appendix (Section B.7). From the above lemma, it follows that there exists R ∈ Od such that for any z ∈ Z := R(S) we have ‖z‖∞ 6 ε2 and therefore Y = R−1Z. Furthermore, since R is orthogonal, therefore the matrix R−1 is also orthogonal, and therefore it satisfies (ε, k)-RIP.\nTo complete the proof, we observe that even though the given factorization has inner dimension d, we can trivially extend it to one with inner dimension m. This can be done by constructing Φ = [ R−1 G ] with G ∼ 1√ d N(0, 1)d×m−d. Since ω∗ d, from Theorem 2.4 it follows that with high probability G (and consequently Φ) will satisfy (ε, k)-RIP. Finally, we construct Ẑ ∈ Rm×n by padding Z with m − d rows of zeros. Therefore, by construction Y = Φ · Ẑ, where for every i ∈ [p] we have ‖zi‖∞ 6 ε2. Hence the claim follows."
  }, {
    "heading": "4. Analysis for the Known Design setting",
    "text": "In this section, we describe and analyze the tester for the known design matrix case. The algorithm itself is a simple convex-hull membership test, which can be solved using a linear program.\nAlgorithm 2 SparseTest-KnownDesign 1: Set n = 100klog mδ , sample projection matrix Φ ∼\n1√ n N(0, 1)n×d\n2: Observe linear sketch ỹ = Φ(y) 3: Let A± = A ∪ −A 4: Accept iff ỹ ∈ √ k · conv ( Φ(A±) ) We shall now prove the completeness and soundness guarantees of the above tester. The running time bound follows because convex hull membership reduces to linear programming.\nCompleteness Let y = Ax where A ∈ Rd×m is an arbitrary matrix with ‖ai‖ = 1 for every i ∈ [m]. Furthermore ‖x‖2 = 1 and ‖x‖0 6 k. Therefore, by Cauchy-Schwartz we have ‖x‖1 6 √ k‖x‖2 = √ k. Hence, it follows that\ny ∈ √ k · conv(A±). Since Φ : Rm 7→ Rd is a linear trans-\nformation, we have Φ(y) ∈ √ k · conv(Φ(A±)). Therefore, the tester accepts with probability 1.\nSoundness Consider the set Aε/√k which is the set of all (2k/ε2)-uniform convex combinations of √ k(A±) i.e.,\nAε/ √ k = { ∑ vi∈Ω ε2 2k vi : multiset Ω ∈ (√ k.A± )2k/ε2} (5)\nThen, from the approximate Carathéodory theorem, it follows that Aε/√k is an ε-cover of √ k · conv ( A± ) . Furthermore, |Aε/√k| 6 (2m) 2k/ε2 . By our choice of n, with\nprobability at least 1 − δ/2, the set Φ ( {y} ∪ Aε/√k ) is ε-isometric to {y} ∪Aε/√k.\nLet Ãε/√k = Φ ( Aε/ √ k ) . Again, by the approximate Carathéodory’s theorem, the set Ãε/√k is an ε-cover of\nΦ (√ k · conv(A±) ) . Now suppose the test accepts y with probability at least δ. Then, with probability at least δ/2, the test accepts and the above ε-isometry conditions hold simultaneously. Then,\nỹ ∈ √ k · conv ( Φ(A±) ) 1⇒ dist ( ỹ, Ãε/ √ k ) 6 ε\n2⇒ dist ( y, Aε/ √ k ) 6 ε(1− ε)−1 6 2ε\n⇒ dist ( y, √ k · conv(A±) ) 6 2ε\nwhere step 1 follows from the ε-cover guarantee of Ãε/√k, step 2 follows from the ε-isometry guarantee. Invoking the approximate Carathéodory theorem, we get that there exists ŷ = Ax̂ ∈ √ k · conv(±A) such that ‖x̂‖0 6 O(k/ε2) and ‖ŷ − y‖ 6 O(ε). This completes the soundness direction."
  }, {
    "heading": "5. Experimental Results",
    "text": "Our algorithm for the unknown design setting is based on the principle that the property of sparse representability in some basis admits an approximate characterization in terms of gaussian width. This section provides experimental evidence which supplements our theoretical results. For the empirical study, we use the classic Barbara image (which is of size 512× 512 pixels). Specifically, we consider 9 subimages of size 100 × 100 pixels each (see Figure 1). For each such sub-image, we compute a matrix representation (by the standard technique of subdividing the images into patches, see, e.g., (Elad & Aharon, 2006)). In particular,\neach sub-image is represented as a matrix Y of dimension 64 × 8649. Then, for each matrix Y corresponding to a sub-image, we estimate the gaussian width of the `2-column normalized matrix. In addition, setting the number of atoms m = 100 and sparsity k = 10, we run the k-SVD algorithm for 50 iterations and record the reconstruction error.6\nFigure 2 shows the comparison between gaussian width and reconstruction error, in which we observe that there is an approximate correlation between the two quantities. In particular, for sub-images 2,7 and 8—which mostly consist of background—both the gaussian width and the reconstruction error is small. On the other hand, images 3, 6 and 9, which consist of intricate patterns and objects, have large gaussian width as well as large reconstruction error. Consequently, we can deduce that for sub-images with large gaussian width, in order to achieve low reconstruction error, one would have consider a larger number of atoms m or larger sparsity k."
  }, {
    "heading": "6. Conclusion and Open Questions",
    "text": "In this paper, we studied the problem of testing sparsity with respect to unknown and known bases. While the optimization variants of these problems (namely Dictionary Learning and Sparse Recovery) are known to be NP-hard in the worst case, our results show that under appropriate relaxations, these problems admit efficient property testing algorithms. Future work include designing testing algorithms for sparsity over an unknown basis with stronger\n6For a matrix Y ∈ Rd×n approximated by overcomplete basis A and coefficient matrix X, the reconstruction error is equal to ‖Y −AX‖2F /(n.d).\nguarantees or developing impossibility results. We also hope that this paper leads to study of property testing of other widely studied hypotheses in machine learning such as nonnegative rank and VC-dimension."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank David Woodruff for showing us the sketching-based tester described in Section 1.2."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning sparsely used overcomplete dictionaries",
    "authors": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"],
    "venue": "In Proc. 27th Annual ACM Workshop on Computational Learning Theory, pp",
    "year": 2014
  }, {
    "title": "The space complexity of approximating the frequency moments",
    "authors": ["N. Alon", "Y. Matias", "M. Szegedy"],
    "venue": "In Proc. 28th Annual ACM Symposium on the Theory of Computing,",
    "year": 1996
  }, {
    "title": "The approximate rank of a matrix and its algorithmic applications: approximate rank",
    "authors": ["N. Alon", "T. Lee", "A. Shraibman", "S. Vempala"],
    "venue": "In Proc. 45th Annual ACM Symposium on the Theory of Computing,",
    "year": 2013
  }, {
    "title": "Living on the edge: A geometric theory of phase transitions in convex optimization",
    "authors": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"],
    "venue": "CoRR, abs/1303.6672,",
    "year": 2013
  }, {
    "title": "Non-deterministic exponential time has two-prover interactive protocols",
    "authors": ["L. Babai", "L. Fortnow", "C. Lund"],
    "venue": "Computational Complexity,",
    "year": 1991
  }, {
    "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
    "authors": ["P.L. Bartlett", "S. Mendelson"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "Sparse approximation via generating point sets",
    "authors": ["A. Blum", "S. Har-Peled", "B. Raichel"],
    "venue": "In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms,",
    "year": 2016
  }, {
    "title": "Selftesting/correcting with applications to numerical problems",
    "authors": ["M. Blum", "M. Luby", "R. Rubinfeld"],
    "venue": "J. Comp. Sys. Sci.,",
    "year": 1993
  }, {
    "title": "Concentration Inequalities: A Nonasymptotic Theory of Independence",
    "authors": ["S. Boucheron", "G. Lugosi", "P. Massart"],
    "venue": "OUP Oxford,",
    "year": 2013
  }, {
    "title": "Toward a unified theory of sparse dimensionality reduction in euclidean space",
    "authors": ["J. Bourgain", "S. Dirksen", "J. Nelson"],
    "venue": "Geometric and Functional Analysis,",
    "year": 2015
  }, {
    "title": "The convex geometry of linear inverse problems",
    "authors": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2012
  }, {
    "title": "Algorithms for $\\ell p$ low-rank approximation",
    "authors": ["F. Chierichetti", "S. Gollapudi", "R. Kumar", "S. Lattanzi", "R. Panigrahy", "D.P. Woodruff"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Property testing in computational geometry",
    "authors": ["A. Czumaj", "C. Sohler", "M. Ziegler"],
    "venue": "In Proc. 8th European Symposium on Algorithms,",
    "year": 2000
  }, {
    "title": "A sparse JohnsonLindenstrauss transform",
    "authors": ["A. Dasgupta", "R. Kumar", "T. Sarlós"],
    "venue": "In Proc. 42nd Annual ACM Symposium on the Theory of Computing,",
    "year": 2010
  }, {
    "title": "Image denoising via sparse and redundant representations over learned dictionaries",
    "authors": ["M. Elad", "M. Aharon"],
    "venue": "IEEE Transactions on Image processing,",
    "year": 2006
  }, {
    "title": "Variable selection is hard",
    "authors": ["D. Foster", "H. Karloff", "J. Thaler"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2015
  }, {
    "title": "Property testing and its connection to learning and approximation",
    "authors": ["O. Goldreich", "S. Goldwasser", "D. Ron"],
    "venue": "J. ACM,",
    "year": 1998
  }, {
    "title": "Some inequalities for gaussian processes and applications",
    "authors": ["Y. Gordon"],
    "venue": "Israel Journal of Mathematics,",
    "year": 1985
  }, {
    "title": "Statistical learning with sparsity: the lasso and generalizations",
    "authors": ["T. Hastie", "R. Tibshirani", "M. Wainwright"],
    "venue": "CRC press,",
    "year": 2015
  }, {
    "title": "Sparser johnson-lindenstrauss transforms",
    "authors": ["D.M. Kane", "J. Nelson"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2014
  }, {
    "title": "An optimal algorithm for the distinct elements problem",
    "authors": ["D.M. Kane", "J. Nelson", "D.P. Woodruff"],
    "venue": "In Proc. 29th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of database systems,",
    "year": 2010
  }, {
    "title": "Property testing of data dimensionality",
    "authors": ["R. Krauthgamer", "O. Sasson"],
    "venue": "In Proc. 14th ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2003
  }, {
    "title": "Graph stream algorithms: a survey",
    "authors": ["A. McGregor"],
    "venue": "ACM SIGMOD Record,",
    "year": 2014
  }, {
    "title": "Entropy, combinatorial dimensions and random averages",
    "authors": ["S. Mendelson", "R. Vershynin"],
    "venue": "In Computational Learning Theory, 15th Annual Conference on Computational Learning Theory, COLT",
    "year": 2002
  }, {
    "title": "Sparse approximate solutions to linear systems",
    "authors": ["B.K. Natarajan"],
    "venue": "SIAM journal on computing,",
    "year": 1995
  }, {
    "title": "Dictionary learning for sparse representation: Complexity and algorithms",
    "authors": ["M. Razaviyayn", "Tseng", "H.-W", "Luo", "Z.-Q"],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2014
  }, {
    "title": "Property testing: A learning theory perspective",
    "authors": ["D. Ron"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2008
  }, {
    "title": "Sublinear time algorithms",
    "authors": ["R. Rubinfeld", "A. Shapira"],
    "venue": "In Proc. International Congress of Mathematicians 2006,",
    "year": 2006
  }, {
    "title": "Robust characterizations of polynomials with applications to program testing",
    "authors": ["R. Rubinfeld", "M. Sudan"],
    "venue": "SIAM J. on Comput.,",
    "year": 1996
  }, {
    "title": "On sparse reconstruction from fourier and gaussian measurements",
    "authors": ["M. Rudelson", "R. Vershynin"],
    "venue": "Communications on Pure and Applied Mathematics,",
    "year": 2008
  }, {
    "title": "The one-sided barrier problem for gaussian noise",
    "authors": ["D. Slepian"],
    "venue": "The Bell System Technical Journal,",
    "year": 1962
  }, {
    "title": "Exact recovery of sparsely-used dictionaries",
    "authors": ["D.A. Spielman", "H. Wang", "J. Wright"],
    "venue": "In Proc. 25th Annual ACM Workshop on Computational Learning Theory, pp",
    "year": 2012
  }, {
    "title": "On the computational intractability of exact and approximate dictionary learning",
    "authors": ["A.M. Tillmann"],
    "venue": "IEEE Signal Processing Letters,",
    "year": 2015
  }, {
    "title": "Lectures in geometric functional analysis",
    "authors": ["R. Vershynin"],
    "venue": "Preprint, University of Michigan,",
    "year": 2011
  }, {
    "title": "Estimation in high dimensions: a geometric perspective",
    "authors": ["R. Vershynin"],
    "venue": "In Sampling Theory, a Renaissance,",
    "year": 2015
  }, {
    "title": "High dimensional probability, 2016",
    "authors": ["R. Vershynin"],
    "year": 2016
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["D.P. Woodruff"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }],
  "id": "SP:97e5b98168c1cb784806b70ca70db8c9635d5dca",
  "authors": [{
    "name": "Siddharth Barman",
    "affiliations": []
  }, {
    "name": "Arnab Bhattacharyya",
    "affiliations": []
  }, {
    "name": "Suprovat Ghoshal",
    "affiliations": []
  }],
  "abstractText": "Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications. In this work, we describe property testing algorithms for sparsity that observe a lowdimensional projection of the input. We consider two settings. In the first setting, we test sparsity with respect to an unknown basis: given input vectors y1, . . . ,yp ∈ R whose concatenation as columns forms Y ∈ Rd×p, does Y = AX for matrices A ∈ Rd×m and X ∈ Rm×p such that each column of X is k-sparse, or is Y “far” from having such a decomposition? In the second setting, we test sparsity with respect to a known basis: for a fixed design matrix A ∈ Rd×m, given input vector y ∈ R, is y = Ax for some ksparse vector x or is y “far” from having such a decomposition? We analyze our algorithms using tools from high-dimensional geometry and probability.",
  "title": "Testing Sparsity over Known and Unknown Bases"
}