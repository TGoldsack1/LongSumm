{
  "sections": [{
    "heading": "1. Introduction",
    "text": "How does depth help? This central question of deep learning still eludes full theoretical understanding. The general consensus is that there is a trade-off: increasing depth improves expressiveness, but complicates optimization. Superior expressiveness of deeper networks, long suspected, is now confirmed by theory, albeit for fairly limited learning problems (Eldan & Shamir, 2015; Raghu et al., 2016; Lee et al., 2017; Cohen et al., 2017; Daniely, 2017; Arora et al., 2018). Difficulties in optimizing deeper networks have also been long clear – the signal held by a gradient gets buried as it propagates through many layers. This is known as the “vanishing/exploding gradient problem”. Modern techniques such as batch normalization (Ioffe & Szegedy, 2015) and residual connections (He et al., 2015) have somewhat alleviated these difficulties in practice.\n1Department of Computer Science, Princeton University, Princeton, NJ, USA 2School of Mathematics, Institute for Advanced Study, Princeton, NJ, USA 3Google Brain, USA. Correspondence to: Nadav Cohen <cohennadav@ias.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nGiven the longstanding consensus on expressiveness vs. optimization trade-offs, this paper conveys a rather counterintuitive message: increasing depth can accelerate optimization. The effect is shown, via first-cut theoretical and empirical analyses, to resemble a combination of two wellknown tools in the field of optimization: momentum, which led to provable acceleration bounds (Nesterov, 1983); and adaptive regularization, a more recent technique proven to accelerate by Duchi et al. (2011) in their proposal of the AdaGrad algorithm. Explicit mergers of both techniques are quite popular in deep learning (Kingma & Ba, 2014; Tieleman & Hinton, 2012). It is thus intriguing that merely introducing depth, with no other modification, can have a similar effect, but implicitly.\nThere is an obvious hurdle in isolating the effect of depth on optimization: if increasing depth leads to faster training on a given dataset, how can one tell whether the improvement arose from a true acceleration phenomenon, or simply due to better representational power (the shallower network was unable to attain the same training loss)? We respond to this hurdle by focusing on linear neural networks (cf. Saxe et al. (2013); Goodfellow et al. (2016); Hardt & Ma (2016); Kawaguchi (2016)). With these models, adding layers does not alter expressiveness; it manifests itself only in the replacement of a matrix parameter by a product of matrices – an overparameterization.\nWe provide a new analysis of linear neural network optimization via direct treatment of the differential equations associated with gradient descent when training arbitrarily deep networks on arbitrary loss functions. We find that the overparameterization introduced by depth leads gradient descent to operate as if it were training a shallow (single layer) network, while employing a particular preconditioning scheme. The preconditioning promotes movement along directions already taken by the optimization, and can be seen as an acceleration procedure that combines momentum with adaptive learning rates. Even on simple convex problems such as linear regression with `p loss, p > 2, overparameterization via depth can significantly speed up training. Surprisingly, in some of our experiments, not only did overparameterization outperform naı̈ve gradient descent, but it was also faster than two well-known acceleration methods – AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012).\nIn addition to purely linear networks, we also demonstrate (empirically) the implicit acceleration of overparameterization on a non-linear model, by replacing hidden layers with depth-2 linear networks. The implicit acceleration of overparametrization is different from standard regularization – we prove its effect cannot be attained via gradients of any fixed regularizer.\nBoth our theoretical analysis and our empirical evaluation indicate that acceleration via overparameterization need not be computationally expensive. From an optimization perspective, overparameterizing using wide or narrow networks has the same effect – it is only the depth that matters."
  }, {
    "heading": "2. Related Work",
    "text": "Theoretical study of optimization in deep learning is a highly active area of research. Works along this line typically analyze critical points (local minima, saddles) in the landscape of the training objective, either for linear networks (see for example Kawaguchi (2016); Hardt & Ma (2016) or Baldi & Hornik (1989) for a classic account), or for specific non-linear networks under different restrictive assumptions (cf. Choromanska et al. (2015); Haeffele & Vidal (2015); Soudry & Carmon (2016); Safran & Shamir (2017)). Other works characterize other aspects of objective landscapes, for example Safran & Shamir (2016) showed that under certain conditions a monotonically descending path from initialization to global optimum exists (in compliance with the empirical observations of Goodfellow et al. (2014)).\nThe dynamics of optimization was studied in Fukumizu (1998) and Saxe et al. (2013), for linear networks. Like ours, these works analyze gradient descent through its corresponding differential equations. Fukumizu (1998) focuses on linear regression with `2 loss, and does not consider the effect of varying depth – only a two (single hidden) layer network is analyzed. Saxe et al. (2013) also focuses on `2 regression, but considers any depth beyond two (inclusive), ultimately concluding that increasing depth can slow down optimization, albeit by a modest amount. In contrast to these two works, our analysis applies to a general loss function, and any depth including one. Intriguingly, we find that for `p regression, acceleration by depth is revealed only when p > 2. This explains why the conclusion reached in Saxe et al. (2013) differs from ours.\nTurning to general optimization, accelerated gradient (momentum) methods were introduced in Nesterov (1983), and later studied in numerous works (see Wibisono et al. (2016) for a short review). Such methods effectively accumulate gradients throughout the entire optimization path, using the collected history to determine the step at a current point in time. Use of preconditioners to speed up optimization is also a well-known technique. Indeed, the classic Newton’s method can be seen as preconditioning based on second\nderivatives. Adaptive preconditioning with only first-order (gradient) information was popularized by the BFGS algorithm and its variants (cf. Nocedal (1980)). Relevant theoretical guarantees, in the context of regret minimization, were given in Hazan et al. (2007); Duchi et al. (2011). In terms of combining momentum and adaptive preconditioning, Adam (Kingma & Ba, 2014) is a popular approach, particularly for optimization of deep networks."
  }, {
    "heading": "3. Warmup: `p Regression",
    "text": "We begin with a simple yet striking example of the effect being studied. For linear regression with `p loss, we will see how even the slightest overparameterization can have an immense effect on optimization. Specifically, we will see that simple gradient descent on an objective overparameterized by a single scalar, corresponds to a form of accelerated gradient descent on the original objective.\nConsider the objective for a scalar linear regression problem with `p loss (p – even positive integer): L(w) = E(x,y)∼S [ 1 p (x >w − y)p ] x ∈ Rd here are instances, y ∈ R are continuous labels, S is a finite collection of labeled instances (training set), and w ∈ Rd is a learned parameter vector. Suppose now that we apply a simple overparameterization, replacing the parameter vector w by a vector w1 ∈ Rd times a scalar w2 ∈ R:\nL(w1, w2) = E(x,y)∼S [ 1 p (x >w1w2 − y)p ] Obviously the overparameterization does not affect the expressiveness of the linear model. How does it affect optimization? What happens to gradient descent on this nonconvex objective?\nObservation 1. Gradient descent over L(w1, w2), with fixed small learning rate and near-zero initialization, is equivalent to gradient descent over L(w) with particular adaptive learning rate and momentum terms.\nTo see this, consider the gradients of L(w) andL(w1, w2): ∇w := E(x,y)∼S [ (x>w − y)p−1x ] ∇w1 := E(x,y)∼S [ (x>w1w2 − y)p−1w2x\n] ∇w2 := E(x,y)∼S [ (x>w1w2 − y)p−1w>1 x\n] Gradient descent over L(w1, w2) with learning rate η > 0:\nw (t+1) 1 ← [ w (t) 1 −η∇w(t)1 , w (t+1) 2 ← [ w (t) 2 −η∇w(t)2\nThe dynamics of the underlying parameter w = w1w2 are: w(t+1) = w (t+1) 1 w (t+1) 2\n←[ (w(t)1 −η∇w(t)1 )(w (t) 2 −η∇w(t)2 ) = w (t) 1 w (t) 2 − ηw\n(t) 2 ∇w(t)1 − η∇w(t)2 w (t) 1 +O(η2)\n= w(t) − η(w(t)2 )2∇w(t) − η(w (t) 2 ) −1∇ w (t) 2 w(t) +O(η2)\nη is assumed to be small, thus we neglect O(η2). Denoting ρ(t):=η(w(t)2 )\n2 ∈R and γ(t):=η(w(t)2 )−1∇w(t)2 ∈R, this gives:\nw(t+1) ←[ w(t) − ρ(t)∇w(t) − γ(t)w(t) Since by assumption w1 and w2 are initialized near zero, w will initialize near zero as well. This implies that at every iteration t, w(t) is a weighted combination of past gradients. There thus exist µ(t,τ) ∈ R such that:\nw(t+1) ← [ w(t) − ρ(t)∇w(t) − ∑t−1\nτ=1 µ(t,τ)∇w(τ)\nWe conclude that the dynamics governing the underlying parameter w correspond to gradient descent with a momentum term, where both the learning rate (ρ(t)) and momentum coefficients (µ(t,τ)) are time-varying and adaptive."
  }, {
    "heading": "4. Linear Neural Networks",
    "text": "Let X := Rd be a space of objects (e.g. images or word embeddings) that we would like to infer something about, and let Y := Rk be the space of possible inferences. Suppose we are given a training set {(x(i),y(i))}mi=1 ⊂ X × Y , along with a (point-wise) loss function l : Y × Y → R≥0. For example, y(i) could hold continuous values with l(·) being the `2 loss: l(ŷ,y) = 12 ‖ŷ − y‖ 2 2; or it could hold one-hot vectors representing categories with l(·) being the softmax-cross-entropy loss: l(ŷ,y) = − ∑k r=1 yr log(e ŷr/ ∑k r′=1 e\nŷr′ ), where yr and ŷr stand for coordinate r of y and ŷ respectively. For a predictor φ, i.e. a mapping from X to Y , the overall training loss is L(φ) := 1m ∑m i=1 l(φ(x\n(i)),y(i)). If φ comes from some parametric family Φ := {φθ : X → Y|θ ∈ Θ}, we view the corresponding training loss as a function of the parameters, i.e. we consider LΦ(θ) := 1m ∑m i=1 l(φθ(x\n(i)),y(i)). For example, if the parametric family in question is the class of (directly parameterized) linear predictors:\nΦlin := {x 7→Wx|W ∈ Rk,d} (1) the respective training loss is a function from Rk,d to R≥0.\nIn our context, a depth-N (N ≥ 2) linear neural network, with hidden widths n1, n2, . . . , nN−1∈N, is the following parametric family of linear predictors: Φn1...nN−1 := {x 7→WNWN−1· · ·W1x|Wj∈Rnj ,nj−1 , j=1...N}, where by definition n0 := d and nN := k. As customary, we refer to each Wj , j=1...N , as the weight matrix of layer j. For simplicity of presentation, we hereinafter omit from our notation the hidden widths n1...nN−1, and simply write ΦN instead of Φn1...nN−1 (n1. . .nN−1 will be specified explicitly if not clear by context). That is, we denote:\nΦN := (2) {x 7→WNWN−1· · ·W1x|Wj ∈ Rnj ,nj−1 , j=1...N}\nFor completeness, we regard a depth-1 network as the family of directly parameterized linear predictors, i.e. we set Φ1 := Φlin (see Equation 1).\nThe training loss that corresponds to a depth-N linear network – LΦ N\n(W1, ...,WN ), is a function from Rn1,n0×· · ·×RnN ,nN−1 to R≥0. For brevity, we will denote this function by LN (·). Our focus lies on the behavior\nof gradient descent when minimizing LN (·). More specifically, we are interested in the dependence of this behavior on N , and in particular, in the possibility of increasing N leading to acceleration. Notice that for any N ≥ 2 we have:\nLN (W1, ...,WN ) = L 1(WNWN−1· · ·W1) (3)\nand so the sole difference between the training loss of a depth-N network and that of a depth-1 network (classic linear model) lies in the replacement of a matrix parameter by a product of N matrices. This implies that if increasing N can indeed accelerate convergence, it is not an outcome of any phenomenon other than favorable properties of depthinduced overparameterization for optimization."
  }, {
    "heading": "5. Implicit Dynamics of Gradient Descent",
    "text": "In this section we present a new result for linear neural networks, tying the dynamics of gradient descent on LN (·) – the training loss corresponding to a depth-N network, to those on L1(·) – training loss of a depth-1 network (classic linear model). Specifically, we show that gradient descent on LN (·), a complicated and seemingly pointless overparameterization, can be directly rewritten as a particular preconditioning scheme over gradient descent on L1(·).\nWhen applied to LN (·), gradient descent takes on the following form:\nW (t+1) j ← [ (1− ηλ)W (t) j − η\n∂LN ∂Wj (W (t) 1 , . . . ,W (t) N ) (4)\n, j = 1. . .N\nη > 0 here is a learning rate, and λ ≥ 0 is an optional weight decay coefficient. For simplicity, we regard both η and λ as fixed (no dependence on t). Define the underlying end-to-end weight matrix:\nWe := WNWN−1 · · ·W1 (5)\nGiven that LN (W1, . . . ,WN ) = L1(We) (Equation 3), we view We as an optimized weight matrix for L1(·), whose dynamics are governed by Equation 4. Our interest then boils down to the study of these dynamics for different choices of N . For N = 1 they are (trivially) equivalent to standard gradient descent over L1(·). We will characterize the dynamics for N ≥ 2.\nTo be able to derive, in our general setting, an explicit update rule for the end-to-end weight matrix We (Equation 5), we introduce an assumption by which the learning rate is small, i.e. η2 ≈ 0. Formally, this amounts to translating Equation 4 to the following set of differential equations:\nẆj(t) = −ηλWj(t)− η ∂LN\n∂Wj (W1(t), . . . ,WN (t)) (6)\n, j = 1. . .N\nwhere t is now a continuous time index, and Ẇj(t) stands for the derivative of Wj with respect to time. The use of differential equations, for both theoretical analysis and algorithm design, has a long and rich history in optimization\nresearch (see Helmke & Moore (2012) for an overview). When step sizes (learning rates) are taken to be small, trajectories of discrete optimization algorithms converge to smooth curves modeled by continuous-time differential equations, paving way to the well-established theory of the latter (cf. Boyce et al. (1969)). This approach has led to numerous interesting findings, including recent results in the context of acceleration methods (e.g. Su et al. (2014); Wibisono et al. (2016)).\nWith the continuous formulation in place, we turn to express the dynamics of the end-to-end matrix We:\nTheorem 1. Assume the weight matrices W1. . .WN follow the dynamics of continuous gradient descent (Equation 6). Assume also that their initial values (time t0) satisfy, for j = 1. . .N − 1:\nW>j+1(t0)Wj+1(t0) = Wj(t0)W > j (t0) (7)\nThen, the end-to-end weight matrix We (Equation 5) is governed by the following differential equation: Ẇe(t) = −ηλN ·We(t) (8) −η ∑N\nj=1\n[ We(t)W > e (t) ] j−1 N ·\ndL1 dW (We(t)) · [ W>e (t)We(t) ]N−j N\nwhere [·] j−1 N and [·] N−j N , j = 1 . . . N , are fractional power operators defined over positive semidefinite matrices.\nProof. (sketch – full details in Appendix A.1) If λ= 0 (no weight decay) then one can easily show that W>j+1(t)Ẇj+1(t) = Ẇj(t)W > j (t) throughout optimization. Taking the transpose of this equation and adding to itself, followed by integration over time, imply that the difference between W>j+1(t)Wj+1(t) and Wj(t)W > j (t) is constant. This difference is zero at initialization (Equation 7), thus will remain zero throughout, i.e.:\nW>j+1(t)Wj+1(t) = Wj(t)W > j (t) , ∀t ≥ t0 (9)\nA slightly more delicate treatment shows that this is true even if λ > 0, i.e. with weight decay included.\nEquation 9 implies alignment of the (left and right) singular spaces of Wj(t) and Wj+1(t), simplifying the product Wj+1(t)Wj(t). Successive application of this simplification allows a clean computation for the product of all layers (that is, We), leading to the explicit form presented in theorem statement (Equation 8).\nTranslating the continuous dynamics of Equation 8 back to discrete time, we obtain the sought-after update rule for the end-to-end weight matrix: W (t+1)e ← [ (1− ηλN)W (t)e (10) −η ∑N\nj=1\n[ W (t)e (W (t) e ) > ] j−1 N ·\ndL1 dW (W (t) e ) · [ (W (t)e ) >W (t)e ]N−j N\nThis update rule relies on two assumptions: first, that the\nlearning rate η is small enough for discrete updates to approximate continuous ones; and second, that weights are initialized on par with Equation 7, which will approximately be the case if initialization values are close enough to zero. It is customary in deep learning for both learning rate and weight initializations to be small, but nonetheless above assumptions are only met to a certain extent. We support their applicability by showing empirically (Section 8) that the end-to-end update rule (Equation 10) indeed provides an accurate description for the dynamics of We.\nA close look at Equation 10 reveals that the dynamics of the end-to-end weight matrix We are similar to gradient descent over L1(·) – training loss corresponding to a depth-1 network (classic linear model). The only difference (besides the scaling by N of the weight decay coefficient λ) is that the gradient dL 1\ndW (We) is subject to a transformation before being used. Namely, for j = 1. . .N , it is multiplied from the left by [WeW>e ] j−1 N and from the right by [W>e We] N−j N , followed by summation over j. Clearly, when N = 1 (depth-1 network) this transformation reduces to identity, and as expected, We precisely adheres to gradient descent over L1(·). When N ≥ 2 the dynamics of We are less interpretable. We arrange it as a vector to gain more insight:\nClaim 1. For an arbitrary matrix A, denote by vec(A) its arrangement as a vector in column-first order. Then, the end-to-end update rule in Equation 10 can be written as:\nvec(W (t+1)e )← [ (1− ηλN) · vec(W (t)e ) (11) −η · P\nW (t) e vec\n( dL1 dW (W (t) e ) )\nwhere P W (t) e is a positive semidefinite preconditioning matrix that depends on W (t)e . Namely, if we denote the singular values of W (t)e ∈ Rk,d by σ1 . . . σmax{k,d} ∈ R≥0 (by definition σr = 0 if r > min{k, d}), and corresponding left and right singular vectors by u1 . . .uk ∈ Rk and v1 . . .vd ∈ Rd respectively, the eigenvectors of PW (t)e are:\nvec(urv > r′) , r = 1 . . . k , r ′ = 1 . . . d\nwith corresponding eigenvalues:∑N j=1 σ 2N−jN r σ 2 j−1N r′ , r = 1 . . . k , r ′ = 1 . . . d\nProof. The result readily follows from the properties of the Kronecker product – see Appendix A.2 for details.\nClaim 1 implies that in the end-to-end update rule of Equation 10, the transformation applied to the gradient dL 1\ndW (We) is essentially a preconditioning, whose eigendirections and eigenvalues depend on the singular value decomposition of We. The eigendirections are the rank-1 matrices urv>r′ , where ur and vr′ are left and right (respectively) singular vectors of We. The eigenvalue of urv>r′ is ∑N j=1 σ 2(N−j)/N r σ 2(j−1)/N r′ , where σr and σr′ are the singular values of We corresponding to ur and vr′ (respectively). When N ≥ 2, an increase in σr or σr′ leads to\nan increase in the eigenvalue corresponding to the eigendirection urv>r′ . Qualitatively, this implies that the preconditioning favors directions that correspond to singular vectors whose presence in We is stronger. We conclude that the effect of overparameterization, i.e. of replacing a classic linear model (depth-1 network) by a depth-N linear network, boils down to modifying gradient descent by promoting movement along directions that fall in line with the current location in parameter space. A-priori, such a preference may seem peculiar – why should an optimization algorithm be sensitive to its location in parameter space? Indeed, we generally expect sensible algorithms to be translation invariant, i.e. be oblivious to parameter value. However, if one takes into account the common practice in deep learning of initializing weights near zero, the location in parameter space can also be regarded as the overall movement made by the algorithm. We thus interpret our findings as indicating that overparameterization promotes movement along directions already taken by the optimization, and therefore can be seen as a form of acceleration. This intuitive interpretation will become more concrete in the subsection that follows.\nA final point to make, is that the end-to-end update rule (Equation 10 or 11), which obviously depends on N – number of layers in the deep linear network, does not depend on the hidden widths n1 . . . nN−1 (see Section 4). This implies that from an optimization perspective, overparameterizing using wide or narrow networks has the same effect – it is only the depth that matters. Consequently, the acceleration of overparameterization can be attained at a minimal computational price, as we demonstrate empirically in Section 8."
  }, {
    "heading": "5.1. Single Output Case",
    "text": "To facilitate a straightforward presentation of our findings, we hereinafter focus on the special case where the optimized models have a single output, i.e. where k = 1. This corresponds, for example, to a binary (two-class) classification problem, or to the prediction of a numeric scalar property (regression). It admits a particularly simple form for the end-to-end update rule of Equation 10: Claim 2. Assume k = 1, i.e. We ∈ R1,d. Then, the end-toend update rule in Equation 10 can be written as follows: W (t+1)e ← [ (1− ηλN) ·W (t)e (12)\n−η‖W (t)e ‖ 2− 2N 2 ·\n( dL1\ndW (W (t) e )+\n(N − 1) · Pr W (t) e\n{ dL1 dW (W (t) e ) })\nwhere ‖·‖2− 2 N\n2 stands for Euclidean norm raised to the power of 2− 2N , and PrW {·}, W ∈ R\n1,d, is defined to be the projection operator onto the direction of W :\nPrW : R1,d → R1,d (13)\nPrW {V } :=\n{ W ‖W‖2\nV > · W‖W‖2 , W 6= 0 0 , W = 0\nProof. The result follows from the definition of a fractional power operator over matrices – see Appendix A.3.\nClaim 2 implies that in the single output case, the effect of overparameterization (replacing classic linear model by depth-N linear network) on gradient descent is twofold: first, it leads to an adaptive learning rate schedule, by introducing the multiplicative factor ‖We‖2−2/N2 ; and second, it amplifies (by N ) the projection of the gradient on the direction of We. Recall that we view We not only as the optimized parameter, but also as the overall movement made in optimization (initialization is assumed to be near zero). Accordingly, the adaptive learning rate schedule can be seen as gaining confidence (increasing step sizes) when optimization moves farther away from initialization, and the gradient projection amplification can be thought of as a certain type of momentum that favors movement along the azimuth taken so far. These effects bear potential to accelerate convergence, as we illustrate qualitatively in Section 7, and demonstrate empirically in Section 8."
  }, {
    "heading": "6. Overparametrization Effects Cannot Be Attained via Regularization",
    "text": "Adding a regularizer to the objective is a standard approach for improving optimization (though lately the term regularization is typically associated with generalization). For example, AdaGrad was originally invented to compete with the best regularizer from a particular family. The next theorem shows (for single output case) that the effects of overparameterization cannot be attained by adding a regularization term to the original training loss, or via any similar modification. This is not obvious a-priori, as unlike many acceleration methods that explicitly maintain memory of past gradients, updates under overparametrization are by definition the gradients of something. The assumptions in the theorem are minimal and also necessary, as one must rule-out the trivial counter-example of a constant training loss.\nTheorem 2. Assume dL 1\ndW does not vanish at W = 0, and is continuous on some neighborhood around this point. For a given N ∈ N, N > 2,1 define:\nF (W ) := (14)\n‖W‖2− 2 N 2 · ( dL1 dW (W ) + (N−1) · PrW { dL1 dW (W ) })\nwhere PrW {·} is the projection given in Equation 13. Then, there exists no function (of W ) whose gradient field is F (·). Proof. (sketch – full details in Appendix A.4) The proof uses the fundamental theorem for line integrals, which states that the integral of ∇g for any differentiable function g amounts to 0 along every closed curve. Overparametrization changes gradient descent’s behavior: instead of following the original gradient dL 1\ndW , it follows some other direction F (·) (see Equations 12 and 14) that\n1 For the result to hold with N = 2, additional assumptions on L1(·) are required; otherwise any non-zero linear function L1(W ) = WU> serves as a counter-example – it leads to a vector field F (·) that is the gradient of W 7→ ‖W‖2 ·WU >.\nis a function of the original gradient as well as the current point W . We think of this change as a transformation that maps one vector field φ(·) to another – Fφ(·): Fφ(W ) ={ ‖W‖2− 2 N ( φ(W )+(N−1) 〈 φ(W ), W‖W‖ 〉 W ‖W‖ ) ,W 6=0\n0 ,W=0\nNotice that for φ = dL 1\ndW , we get exactly the vector field F (·) defined in theorem statement. The mapping φ 7→ Fφ is linear. Moreover, because of the linearity of line integrals, for any curve Γ, the functional φ 7→ ∫ Γ Fφ – a mapping of vector fields to scalars, is linear as well.\nWe show that F (·) contradicts the fundamental theorem for line integrals. To do so, we construct a closed curve Γ=Γr,R for which the linear functional φ 7→ ∮ Γ Fφ does not vanish at φ=dL 1\ndW . Let e := dL1 dW (W=0)/‖ dL1\ndW (W=0)‖, which is well-defined since by assumption dL 1\ndW (W=0) 6= 0. For r < Rwe define Γr,R := Γ1r,R → Γ2r,R → Γ3r,R → Γ4r,R as illustrated in Figure 1. With the definition of Γr,R in place, we decompose dL 1\ndW into a constant vector field κ≡ dL 1\ndW (W=0) plus a residual ξ. We explicitly compute the line integrals along Γ1r,R . . .Γ 4 r,R for Fκ, and derive bounds for Fξ. This, along with the linearity of the functional φ 7→ ∫ Γ Fφ, provides a lower bound on the line integral of F (·) over Γr,R. We show the lower bound is positive as r,R→ 0, thus F (·) indeed contradicts the fundamental theorem for line integrals."
  }, {
    "heading": "7. Illustration of Acceleration",
    "text": "To this end, we showed that overparameterization (use of depth-N linear network in place of classic linear model) induces on gradient descent a particular preconditioning scheme (Equation 10 in general and 12 in the single output case), which can be interpreted as introducing some forms of momentum and adaptive learning rate. We now illustrate qualitatively, on a very simple hypothetical learning problem, the potential of these to accelerate optimization.\nConsider the task of linear regression, assigning to vectors in R2 labels in R. Suppose that our training set consists of two points in R2 × R: ([1, 0]>, y1) and ([0, 1]>, y2). Assume also that the loss function of interest is `p, p ∈ 2N: `p(ŷ, y) = 1 p (ŷ − y)\np. Denoting the learned parameter by w = [w1, w2] >, the overall training loss can be written as:2\n2 We omit the averaging constant 1 2 for conciseness.\nL(w1, w2) = 1 p (w1 − y1) p + 1p (w2 − y2) p\nWith fixed learning rate η > 0 (weight decay omitted for simplicity), gradient descent over L(·) gives:\nw (t+1) i ← [ w (t) i − η(w (t) i − yi) p−1 , i = 1, 2\nChanging variables per ∆i = wi − yi, we have: ∆\n(t+1) i ←[ ∆ (t) i ( 1− η(∆(t)i ) p−2) , i = 1, 2 (15) Assuming the original weights w1 and w2 are initialized near zero, ∆1 and ∆2 start off at −y1 and −y2 respectively, and will eventually reach the optimum ∆∗1 = ∆ ∗ 2 = 0 if the learning rate is small enough to prevent divergence: η < 2\nyp−2i , i = 1, 2\nSuppose now that the problem is ill-conditioned, in the sense that y1 y2. If p = 2 this has no effect on the bound for η.3 If p > 2 the learning rate is determined by y1, leading ∆2 to converge very slowly. In a sense, ∆2 will suffer from the fact that there is no “communication” between the coordinates (this will actually be the case not just with gradient descent, but with most algorithms typically used in large-scale settings – AdaGrad, Adam, etc.).\nNow consider the scenario where we optimize L(·) via overparameterization, i.e. with the update rule in Equation 12 (single output). In this case the coordinates are coupled, and as ∆1 gets small (w1 gets close to y1), the learning rate is effectively scaled by y2− 2 N\n1 (in addition to a scaling by N in coordinate 1 only), allowing (if y1>1) faster convergence of ∆2. We thus have the luxury of temporarily slowing down ∆2 to ensure that ∆1 does not diverge, with the latter speeding up the former as it reaches safe grounds. In Appendix B we consider a special case and formalize this intuition, deriving a concrete bound for the acceleration of overparameterization."
  }, {
    "heading": "8. Experiments",
    "text": "Our analysis (Section 5) suggests that overparameterization – replacement of a classic linear model by a deep linear network, induces on gradient descent a certain preconditioning scheme. We qualitatively argued (Section 7) that in some cases, this preconditioning may accelerate convergence. In this section we put these claims to the test, through a series of empirical evaluations based on TensorFlow toolbox (Abadi et al. (2016)). For conciseness, many of the details behind our implementation are deferred to Appendix C.\nWe begin by evaluating our analytically-derived preconditioning scheme – the end-to-end update rule in Equation 10. Our objective in this experiment is to ensure that our analysis, continuous in nature and based on a particular assumption on weight initialization (Equation 7), is indeed applicable to practical scenarios. We focus on the single output\n3 Optimal learning rate for gradient descent on quadratic objective does not depend on current parameter value (cf. Goh (2017)).\ncase, where the update-rule takes on a particularly simple (and efficiently implementable) form – Equation 12. The dataset chosen was UCI Machine Learning Repository’s “Gas Sensor Array Drift at Different Concentrations” (Vergara et al., 2012; Rodriguez-Lujan et al., 2014). Specifically, we used the dataset’s “Ethanol” problem – a scalar regression task with 2565 examples, each comprising 128 features (one of the largest numeric regression tasks in the repository). As training objectives, we tried both `2 and `4 losses. Figure 2 shows convergence (training objective per iteration) of gradient descent optimizing depth-2 and depth3 linear networks, against optimization of a single layer model using the respective preconditioning schemes (Equation 12 with N = 2, 3). As can be seen, the preconditioning schemes reliably emulate deep network optimization, suggesting that, at least in some cases, our analysis indeed captures practical dynamics.\nAlongside the validity of the end-to-end update rule, Figure 2 also demonstrates the negligible effect of network width on convergence, in accordance with our analysis (see Section 5). Specifically, it shows that in the evaluated setting, hidden layers of size 1 (scalars) suffice in order for the essence of overparameterization to fully emerge. Unless otherwise indicated, all results reported hereinafter are based on this configuration, i.e. on scalar hidden layers. The computational toll associated with overparameterization will thus be virtually non-existent.\nAs a final observation on Figure 2, notice that it exhibits faster convergence with a deeper network. This however does not serve as evidence in favor of acceleration by depth, as we did not set learning rates optimally per model (simply used the common choice of 10−3). To conduct a fair comparison between the networks, and more importantly, between them and a classic single layer model, multiple learning rates were tried, and the one giving fastest convergence was taken on a per-model basis. Figure 3 shows the results of this experiment. As can be seen, convergence of deeper\nnetworks is (slightly) slower in the case of `2 loss. This falls in line with the findings of Saxe et al. (2013). In stark contrast, and on par with our qualitative analysis in Section 7, is the fact that with `4 loss adding depth significantly accelerated convergence. To the best of our knowledge, this provides first empirical evidence to the fact that depth, even without any gain in expressiveness, and despite introducing non-convexity to a formerly convex problem, can lead to favorable optimization.\nIn light of the speedup observed with `4 loss, it is natural to ask how the implicit acceleration of depth compares against explicit methods for acceleration and adaptive learning. Figure 4-left shows convergence of a depth-3 network (optimized with gradient descent) against that of a single layer model optimized with AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012). The displayed curves correspond to optimal learning rates, chosen individually via grid search. Quite surprisingly, we find that in this specific setting, overparameterizing, thereby turning a convex problem non-convex, is a more effective optimization strategy than carefully designed algorithms tailored for convex problems. We note that this was not observed with all algorithms – for example Adam (Kingma & Ba, 2014) was considerably faster than overparameterization. However, when introducing overparameterization simultaneously with Adam (a setting we did not theoretically analyze), further acceleration is attained – see Figure 4-right. This suggests that at least in some cases, not only plain gradient descent benefits from depth, but also more elaborate algorithms commonly employed in state of the art applications.\nAn immediate question arises at this point. If depth indeed accelerates convergence, why not add as many layers as one can computationally afford? The reason, which is actually apparent in our analysis, is the so-called vanishing gradient problem. When training a very deep network (large N ), while initializing weights to be small, the end-to-end matrix We (Equation 5) is extremely close to zero, severely attenuating gradients in the preconditioning scheme (Equation 10). A possible approach for alleviating this issue is to initialize weights to be larger, yet small enough such that the\nend-to-end matrix does not “explode”. The choice of identity (or near identity) initialization leads to what is known as linear residual networks (Hardt & Ma, 2016), akin to the successful residual networks architecture (He et al., 2015) commonly employed in deep learning. Notice that identity initialization satisfies the condition in Equation 7, rendering the end-to-end update rule (Equation 10) applicable. Figure 5-left shows convergence, under gradient descent, of a single layer model against deeper networks than those evaluated before – depths 4 and 8. As can be seen, with standard, near-zero initialization, the depth-4 network starts making visible progress only after about 65K iterations, whereas the depth-8 network seems stuck even after 100K iterations. In contrast, under identity initialization, both networks immediately make progress, and again depth serves as an implicit accelerator.\nAs a final sanity test, we evaluate the effect of overparameterization on optimization in a non-idealized (yet simple) deep learning setting. Specifically, we experiment with the convolutional network tutorial for MNIST built into TensorFlow,4 which includes convolution, pooling and dense layers, ReLU non-linearities, stochastic gradient descent with momentum, and dropout (Srivastava et al., 2014). We introduced overparameterization by simply placing two matrices in succession instead of the matrix in each dense layer. Here, as opposed to previous experiments, widths of the newly formed hidden layers were not set to 1, but rather to the minimal values that do not deteriorate expressiveness (see Appendix C). Overall, with an addition of roughly 15% in number of parameters, optimization has accelerated considerably – see Figure 5-right. The displayed results were obtained with the hyperparameter settings hardcoded into the tutorial. We have tried alternative settings (varying learning rates and standard deviations of initializations – see\n4 https://github.com/tensorflow/models/ tree/master/tutorials/image/mnist\nAppendix C), and in all cases observed an outcome similar to that in Figure 5-right – overparameterization led to significant speedup. Nevertheless, as reported above for linear networks, it is likely that for non-linear networks the effect of depth on optimization is mixed – some settings accelerate by it, while others do not. Comprehensive characterization of the cases in which depth accelerates optimization warrants much further study. We hope our work will spur interest in this avenue of research."
  }, {
    "heading": "9. Conclusion",
    "text": "Through theory and experiments, we demonstrated that overparameterizing a neural network by increasing its depth can accelerate optimization, even on very simple problems.\nOur analysis of linear neural networks, the subject of various recent studies, yielded a new result: for these models, overparameterization by depth can be understood as a preconditioning scheme with a closed form description (Theorem 1 and the claims thereafter). The preconditioning may be interpreted as a combination between certain forms of adaptive learning rate and momentum. Given that it depends on network depth but not on width, acceleration by overparameterization can be attained at a minimal computational price, as we demonstrate empirically in Section 8.\nClearly, complete theoretical analysis for non-linear networks will be challenging. Empirically however, we showed that the trivial idea of replacing an internal weight matrix by a product of two can significantly accelerate optimization, with absolutely no effect on expressiveness (Figure 5-right).\nThe fact that gradient descent over classic convex problems such as linear regression with `p loss, p > 2, can accelerate from transitioning to a non-convex overparameterized objective, does not coincide with conventional wisdom, and provides food for thought. Can this effect be rigorously quantified, similarly to analyses of explicit acceleration methods such as momentum or adaptive regularization (AdaGrad)?"
  }, {
    "heading": "Acknowledgments",
    "text": "Sanjeev Arora’s work is supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla Research, Amazon Research, DARPA and SRC. Elad Hazan’s work is supported by NSF grant 1523815 and Google Brain. Nadav Cohen is a member of the Zuckerman Israeli Postdoctoral Scholars Program, and is supported by Eric and Wendy Schmidt."
  }],
  "year": 2018,
  "references": [{
    "title": "Tensorflow: A system for large-scale machine learning",
    "authors": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M Isard"],
    "venue": "In OSDI,",
    "year": 2016
  }, {
    "title": "Understanding deep neural networks with rectified linear units",
    "authors": ["R. Arora", "A. Basu", "P. Mianjy", "A. Mukherjee"],
    "venue": "International Conference on Learning Representations (ICLR),",
    "year": 2018
  }, {
    "title": "Neural networks and principal component analysis: Learning from examples without local minima",
    "authors": ["P. Baldi", "K. Hornik"],
    "venue": "Neural networks,",
    "year": 1989
  }, {
    "title": "Elementary differential equations and boundary value problems, volume 9",
    "authors": ["W.E. Boyce", "R.C. DiPrima", "C.W. Haines"],
    "year": 1969
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2015
  }, {
    "title": "Analysis and design of convolutional networks via hierarchical tensor decompositions",
    "authors": ["N. Cohen", "O. Sharir", "Y. Levine", "R. Tamari", "D. Yakira", "A. Shashua"],
    "venue": "arXiv preprint arXiv:1705.02302,",
    "year": 2017
  }, {
    "title": "Depth separation for neural networks",
    "authors": ["A. Daniely"],
    "venue": "arXiv preprint arXiv:1702.08489,",
    "year": 2017
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "The power of depth for feedforward neural networks",
    "authors": ["R. Eldan", "O. Shamir"],
    "venue": "arXiv preprint arXiv:1512.03965,",
    "year": 2015
  }, {
    "title": "Effect of batch learning in multilayer neural networks",
    "authors": ["K. Fukumizu"],
    "venue": "Gen, 1(04):1E–03,",
    "year": 1998
  }, {
    "title": "Why momentum really works. Distill, 2017",
    "authors": ["G. Goh"],
    "venue": "doi: 10. 23915/distill.00006. URL http://distill.pub/2017/ momentum",
    "year": 2017
  }, {
    "title": "Qualitatively characterizing neural network optimization problems",
    "authors": ["I.J. Goodfellow", "O. Vinyals", "A.M. Saxe"],
    "venue": "arXiv preprint arXiv:1412.6544,",
    "year": 2014
  }, {
    "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond",
    "authors": ["B.D. Haeffele", "R. Vidal"],
    "venue": "CoRR abs/1202.2745,",
    "year": 2015
  }, {
    "title": "Identity matters in deep learning",
    "authors": ["M. Hardt", "T. Ma"],
    "venue": "arXiv preprint arXiv:1611.04231,",
    "year": 2016
  }, {
    "title": "Logarithmic regret algorithms for online convex optimization",
    "authors": ["E. Hazan", "A. Agarwal", "S. Kale"],
    "venue": "Mach. Learn.,",
    "year": 2007
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "arXiv preprint arXiv:1512.03385,",
    "year": 2015
  }, {
    "title": "Optimization and dynamical systems",
    "authors": ["U. Helmke", "J.B. Moore"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International conference on machine learning,",
    "year": 2015
  }, {
    "title": "SciPy: Open source scientific tools for Python, 2001",
    "authors": ["E. Jones", "T. Oliphant", "P Peterson"],
    "venue": "URL http://www.scipy. org/. [Online; accessed ¡today¿]",
    "year": 2001
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "On the ability of neural nets to express distributions",
    "authors": ["H. Lee", "R. Ge", "A. Risteski", "T. Ma", "S. Arora"],
    "venue": "arXiv preprint arXiv:1702.07028,",
    "year": 2017
  }, {
    "title": "A method of solving a convex programming problem with convergence rate o (1/k2)",
    "authors": ["Y. Nesterov"],
    "venue": "In Soviet Mathematics Doklady,",
    "year": 1983
  }, {
    "title": "Updating quasi-newton matrices with limited storage",
    "authors": ["J. Nocedal"],
    "venue": "Mathematics of Computation,",
    "year": 1980
  }, {
    "title": "On the expressive power of deep neural networks",
    "authors": ["M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. SohlDickstein"],
    "venue": "arXiv preprint arXiv:1606.05336,",
    "year": 2016
  }, {
    "title": "On the calibration of sensor arrays for pattern recognition using the minimal number of experiments",
    "authors": ["I. Rodriguez-Lujan", "J. Fonollosa", "A. Vergara", "M. Homer", "R. Huerta"],
    "venue": "Chemometrics and Intelligent Laboratory Systems,",
    "year": 2014
  }, {
    "title": "On the quality of the initial basin in overspecified neural networks",
    "authors": ["I. Safran", "O. Shamir"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Spurious local minima are common in two-layer relu neural networks",
    "authors": ["I. Safran", "O. Shamir"],
    "venue": "arXiv preprint arXiv:1712.08968,",
    "year": 2017
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"],
    "venue": "arXiv preprint arXiv:1312.6120,",
    "year": 2013
  }, {
    "title": "No bad local minima: Data independent training error guarantees for multilayer neural networks",
    "authors": ["D. Soudry", "Y. Carmon"],
    "venue": "arXiv preprint arXiv:1605.08361,",
    "year": 2016
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "A differential equation for modeling nesterovs accelerated gradient method: Theory and insights",
    "authors": ["W. Su", "S. Boyd", "E. Candes"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
    "authors": ["T. Tieleman", "G. Hinton"],
    "venue": "COURSERA: Neural networks for machine learning,",
    "year": 2012
  }, {
    "title": "Chemical gas sensor drift compensation using classifier ensembles",
    "authors": ["A. Vergara", "S. Vembu", "T. Ayhan", "M.A. Ryan", "M.L. Homer", "R. Huerta"],
    "venue": "Sensors and Actuators B: Chemical,",
    "year": 2012
  }, {
    "title": "A variational perspective on accelerated methods in optimization",
    "authors": ["A. Wibisono", "A.C. Wilson", "M.I. Jordan"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["M.D. Zeiler"],
    "venue": "arXiv preprint arXiv:1212.5701,",
    "year": 2012
  }],
  "id": "SP:f986e71af471bdd84884be2dae69690ecb578da9",
  "authors": [{
    "name": "Sanjeev Arora",
    "affiliations": []
  }, {
    "name": "Nadav Cohen",
    "affiliations": []
  }, {
    "name": "Elad Hazan",
    "affiliations": []
  }],
  "abstractText": "Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization – linear neural networks, a wellstudied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with `p loss, p > 2, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.",
  "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"
}