{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Keyphrases are short phrases that indicate the core information of a document. As shown in Figure 1, the keyphrase generation (KG) problem focuses on automatically producing a keyphrase set (a set of keyphrases) for the given document. Because of the condensed expression, keyphrases can benefit various downstream applications including opinion mining (Berend, 2011; Wilson et al., 2005), doc-\n1Our code is available at https://github.com/ Chen-Wang-CUHK/ExHiRD-DKG.\nument clustering (Hulth and Megyesi, 2006), and text summarization (Wang and Cardie, 2013).\nKeyphrases of a document can be categorized into two groups: present keyphrase that appears in the document and absent keyphrase that does not appear in the document. Recent generative methods for KG apply the attentional encoderdecoder framework (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to predict both present and absent keyphrases. To generate multiple keyphrases for an input document, these methods first use beam search to generate a huge number of keyphrases (e.g., 200) and then pick the top N ranked keyphrases as the final prediction. Thus, in other words, these methods can only predict a fixed number of keyphrases for all documents.\nHowever, in a practical situation, the appropriate number of keyphrases varies according to the content of the input document. To simultaneously predict keyphrases and determine the suitable number of keyphrases, Yuan et al. (2018) adopts a sequential decoding method with greedy search to generate one sequence consisting of the predicted keyphrases and separators. For example, the produced sequence may be “hemodynamics [sep] erectile dysfunction [sep] ...”, where “[sep]” is the sep-\nar X\niv :2\n00 4.\n08 51\n1v 1\n[ cs\n.C L\n] 1\n8 A\npr 2\n02 0\narator. After producing an ending token, the decoding process terminates. The final keyphrase predictions are obtained after splitting the sequence by separators. However, there are two drawbacks to this method. First, the sequential decoding method ignores the hierarchical compositionality existing in a keyphrase set (a keyphrase set is composed of multiple keyphrases and each keyphrase consists of multiple words). In this work, we examine the hypothesis that a generative model can predict more accurate keyphrases by incorporating the knowledge of the hierarchical compositionality in the decoder architecture. Second, the sequential decoding method tends to generate duplicated keyphrases. It is simple to design specific post-processing rules to remove the repeated keyphrases, but generating and then removing repeated keyphrases wastes time and computing resources. To address these two limitations, we propose a novel exclusive hierarchical decoding framework for KG, which includes a hierarchical decoding process and an exclusion mechanism.\nOur hierarchical decoding process is designed to explicitly model the hierarchical compositionality of a keyphrase set. It is composed of phrase-level decoding (PD) and word-level decoding (WD). A PD step determines which aspect of the document to summarize based on both the document content and the aspects summarized by previouslygenerated keyphrases. The hidden representation of the captured aspect is employed to initialize the WD process. Then, a new WD process is conducted under the PD step to generate a new keyphrase word by word. Both PD and WD repeat until meeting the stop conditions. In our method, both PD and WD attend the document content to gather contextual information. Moreover, the attention score of each WD step is rescaled by the corresponding PD attention score. The purpose of the attention rescaling is to indicate which aspect is focused on by the current PD step.\nWe also propose two kinds of exclusion mechanisms (i.e., a soft one and a hard one) to avoid generating duplicated keyphrases. Either the soft one or the hard one is used in our hierarchical decoding process. Both of them are used in the WD process of our hierarchical decoding. Besides, both of them collect the previously-generated K keyphrases, where K is a predefined window size. The soft exclusion mechanism is incorporated in the training stage, where an exclusive loss is em-\nployed to encourage the model to generate a different first word of the current keyphrase with the first words of the collected K keyphrases. However, the hard exclusion mechanism is used in the inference stage, where an exclusive search is used to force WD to produce a different first word with the first words of the collected K keyphrases. Our motivation is from the statistical observation that in 85% of the documents on the largest KG benchmark, the keyphrases of each individual document have different first words. Moreover, since a keyphrase is usually composed of only two or three words, the predicted first word significantly affects the prediction of the following keyphrase words. Thus, our exclusion mechanisms can boost the diversity of the generated keyphrases. In addition, generating fewer duplications will also improve the chance to produce correct keyphrases that have not been predicted yet.\nWe conduct extensive experiments on four popular real-world benchmarks. Empirical results demonstrate the effectiveness of our hierarchical decoding process. Besides, both the soft and the hard exclusion mechanisms significantly reduce the number of duplicated keyphrases. Furthermore, after employing the hard exclusion mechanism, our model consistently outperforms all the SOTA sequential decoding baselines on the four benchmarks.\nWe summarize our main contributions as follows: (1) to our best knowledge, we are the first to design a hierarchical decoding process for the keyphrase generation problem; (2) we propose two novel exclusion mechanisms to avoid generating duplicated keyphrases as well as improve the generation accuracy; and (3) our method consistently outperforms all the SOTA sequential decoding methods on multiple benchmarks under the new setting."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Keyphrase Extraction",
    "text": "Most of the traditional extractive methods (Witten et al., 1999; Mihalcea and Tarau, 2004) focus on extracting present keyphrases from the input document and follow a two-step framework. They first extract plenty of keyphrase candidates by handcrafted rules (Medelyan et al., 2009). Then, they score and rank these candidates based on either unsupervised methods (Mihalcea and Tarau, 2004) or supervised learning methods (Nguyen and Kan, 2007; Hulth, 2003). Recently, neural-based se-\nquence labeling methods (Gollapalli et al., 2017; Luan et al., 2017; Zhang et al., 2016) are also explored in keyphrase extraction problem. However, these extractive methods cannot predict absent keyphrase which is also an essential part of a keyphrase set."
  }, {
    "heading": "2.2 Keyphrase Generation",
    "text": "To produce both present and absent keyphrases, Meng et al. (2017) introduced a generative model, CopyRNN, which is based on an attentional encoder-decoder framework (Bahdanau et al., 2014) incorporating with a copy mechanism (Gu et al., 2016). A wide range of extensions of CopyRNN are recently proposed (Chen et al., 2018, 2019b; Ye and Wang, 2018; Chen et al., 2019a; Zhao and Zhang, 2019). All of them rely on beam search to over-generate lots of keyphrases with large beam size and then select the top N (e.g., five or ten) ranked ones as the final prediction. That means these over-generated methods will always predict N keyphrases for any input documents. Nevertheless, in a real situation, the keyphrase number should be determined by the document content and may vary among different documents.\nTo this end, Yuan et al. (2018) introduced a new setting that the KG model should predict multiple keyphrases and simultaneously decide the suitable keyphrase number for the given document. Two models with a sequential decoding process, catSeq and catSeqD, are proposed in Yuan et al. (2018). The catSeq is also an attentional encoder-decoder model (Bahdanau et al., 2014) with copy mechanism (See et al., 2017), but adopting new training and inference setup to fit the new setting. The catSeqD is an extension of catSeq with orthogonal regularization (Bousmalis et al., 2016) and target encoding. Lately, Chan et al. (2019) proposed a reinforcement learning based fine-tuning method, which fine-tunes the pre-trained models with adaptive rewards for generating more sufficient and accurate keyphrases. We follow the same setting with Yuan et al. (2018) and propose an exclusive hierarchical decoding method for the KG problem. To the best of our knowledge, this is the first time the hierarchical decoding is explored in the KG problem. Different from the hierarchical decoding in other areas (Fan et al., 2018; Yarats and Lewis, 2018; Tan et al., 2017; Chen and Zhuge, 2018), we rescale the attention score of each WD step with the corresponding PD attention score to provide aspect\nguidance when generating keyphrases. Moreover, either a soft or a hard exclusion mechanism is innovatively incorporated in the decoding process to improve generation diversity."
  }, {
    "heading": "3 Notations and Problem Definition",
    "text": "We denote vectors and matrices with bold lowercase and uppercase letters respectively. Sets are denoted with calligraphy letters. We use W to represent a parameter matrix.\nWe define the keyphrase generation problem as follows. The input is a document x, the output is a keyphrase set Y = {yi}i=1,...,|Y|, where |Y| is the keyphrase number of x. Both the x and each yi are sequences of words, i.e., x = [x1, ..., xlx ] and yi = [yi1, ..., y\ni lyi ], where lx and lyi are the word\nnumbers of x and yi correspondingly."
  }, {
    "heading": "4 Our Methodology",
    "text": "We first encode each word of the document into a hidden state and then employ our exclusive hierarchical decoding shown in Figure 2 to produce keyphrases for the given document. Our hierarchical decoding process consists of phrase-level decoding (PD) and word-level decoding (WD). Each PD step decides an appropriate aspect to summarize based on both the context of the document and the aspects summarized by previous PD steps. Then, the hidden representation of the captured aspect is employed to initialize the WD process to generate a new keyphrase word by word. The WD process terminates when producing a “[eowd]” token. If the WD process output a “[eopd]” token, the whole hierarchical decoding process stops. Both PD and WD attend the document content. The PD attention score is used to re-weight the WD attention score to provide aspect guidance. To improve the diversity of the predicted keyphrases, we incorporate either an exclusive loss when training (i.e., the soft exclusion mechanism) or an exclusive search mechanism when inference (i.e., the hard exclusion mechanism)."
  }, {
    "heading": "4.1 Sequential Encoder",
    "text": "To obtain the context-aware representation of each document word, we employ a two-layered bidirectional GRU (Cho et al., 2014) as the document encoder: mk = BiGRU(exk ,\n−→mk−1,←−mk+1), where k = 1, 2, ..., lx and exk is the embedding vector of xk with de dimensions. mk = [ −→mk;←−mk] ∈ Rd\nis the encoded context-aware representation of xk. Here, “[· ; ·]” means concatenation."
  }, {
    "heading": "4.2 Hierarchical Decoder",
    "text": "Our hierarchical decoding process is controlled by the hierarchical decoder, which utilizes a phraselevel decoder and a word-level decoder to handle the PD process and the WD process respectively. We present our hierarchical decoder first and then introduce the exclusion mechanisms. In our decoders, all the hidden states and attentional vectors are d-dimensional vectors."
  }, {
    "heading": "4.2.1 Phrase-level Decoder",
    "text": "We adopt a unidirectional GRU layer as our phraselevel decoder. After the WD process under last PD step is finished, the phrase-level decoder will update its hidden state as follows:\nhi = −−→ GRU1(h̃i−1,end,hi−1), (1)\nwhere h̃i−1,end is the attentional vector for the ending WD step under the (i-1)-th PD step (e.g., h̃2,2 in Figure 2(b)). hi is regarded as the hidden representation of the captured aspect at the i-th PD step. h0 is initialized as the document representation [−→mlx ;\n←−m1]. h̃0,end is initialized with zeros. In PD-Attention process, the PD attentional score βi = [βi,1, βi,2, . . . , βi,lx ] is computed from the following attention mechanism employing hi\nas the query vector:\nβi,k = exp(si,k)/ lx∑ n=1 exp(si,n), (2) si,n = (hi) TW1mn. (3)"
  }, {
    "heading": "4.2.2 Word-level Decoder",
    "text": "We choose another unidirectional GRU layer to conduct word-level decoding. Under the i-th PD step, the word-level decoder updates its hidden state first:\nhi,j = −−→ GRU2([h̃i,j−1; eyij−1 ],hi,j−1), (4)\nwhere h̃i,j−1 is the WD attentional vector of the (j-1)-th WD step and eyij−1 is the de-dimensional embedding vector of the yij−1 token. We define hi,0 = −−→ GRU2([0; es],hi), where hi is the current hidden state of the phrase-level decoder, 0 is a zero vector, and es is the embedding of the start token. Then, the WD attentional vector is computed:\nh̃i,j = tanh(W2[hi,j ; ai,j ]), (5)\nai,j = lx∑ k=1 ᾱ(i,j),kmk, (6)\nᾱ(i,j),k = α(i,j),k × βi,k∑lx n=1 α(i,j),n × βi,n , (7)\nwhere α(i,j),k is the original WD attention score which is computed similar to βi,k except that a\nnew parameter matrix is used and hi,j is employed as the query vector. The purpose of the rescaling operation in Eq. (7) is to indicate the focused aspect of the current PD step for each WD step.\nFinally, the h̃i,j is utilized to predict the probability distribution of current keyword with the copy mechanism (See et al., 2017):\nP ij = (1− gij)P ij,V + gijP ij,X , (8)\nwhere gij = sigmoid(w T g h̃i,j + bg) ∈ R is the copy gate. P ij,V = softmax(W3h̃i,j + bV) ∈ R|V| is the probability distribution over a predefined vocabulary V . P ij,X = ∑ k:xk=y i j ᾱ(i,j),k ∈ R|X | is the copying probability distribution over X which is a set of all the words that appeared in the document. P ij ∈ R|V∪X | is the final predicted probability distribution. Finally, greedy search is applied to produce the current token.\nThe WD process terminates when producing a “[eowd]” token. The whole hierarchical decoding process ends if the word-level decoder produces a “[eopd]” token at the 0-th step, i.e., yi0 is predicted as “[eopd]”."
  }, {
    "heading": "4.3 Training",
    "text": "A standard negative log-likelihood loss is employed as the generation loss to train our hierarchical decoding model:\nLg = − |Ȳ|∑ i=1 lȳi∑ j=0 logP ij (ȳ i j |x; Ȳi−1; ȳij−1), (9)\nwhere Ȳi−1 = ȳ1, . . . , ȳi−1 are the target keyphrases of previously-finished PD steps and ȳij−1 = ȳ i 0, . . . , ȳ i j−1 are target keyphrase words of previous WD steps under the i-th PD step. When training, each original target keyphrase is extended with a “[neopd]” token and a “[eowd]” token, i.e., ȳi = [“[neopd]”, yi1, . . . , y\ni lyi , “[eowd]”]. Besides,\na “[eopd]” token is also incorporated into the targets to indicate the ending of whole decoding process. Teacher forcing is employed when training."
  }, {
    "heading": "4.4 Soft and Hard Exclusion Mechanisms",
    "text": "To alleviate the duplication generation problem, we propose a soft and a hard exclusion mechanisms. Either of them can be incorporated into our hierarchical decoding process to form one kind of exclusive hierarchical decoding method. Soft Exclusion Mechanism. An exclusive loss (EL) is introduced in the training stage as shown\nAlgorithm 1 Training with Exclusive Loss Require: The window size KEL. The target keyphrases\n[ȳ1, . . . , ȳi, . . . , ȳ|Ȳ|]. The predicted probability distribution P ij for the j-th WD step under the i-th PD step where i = 1, . . . , |Ȳ| and j = 0, 1, . . . , lȳi . 1: Firstly, the exclusive loss of the j-th WD step under the i-th PD step is computed as follows. 2: KEL ← min{KEL, i− 1} 3: if KEL > 0 and j == 1 then 4: Li,jEL = ∑i−1 idx=i−KEL,ȳidxj 6=ȳ i j − log(1− P ij (ȳidxj )) 5: else 6: Li,jEL = 0.0 7: end if 8: Secondly, the exclusive loss for the whole decoding pro-\ncess is calculated as LEL = ∑ i,j L i,j EL.\n9: Finally, the joint loss L = Lg +LEL is employed to train the model.\nAlgorithm 2 Inference with Exclusive Search Require: The window size KES . The first words of\npreviously-predicted keyphrases [y11 , . . . , y i−1 1 ]. The current WD step index j. The predicted probability distribution P ij for current WD step.\n1: KES ← min{KES , i− 1} 2: if KES > 0 and j == 1 then 3: for idx = i−KES , i−KES + 1, . . . , i− 1 do 4: P ij (y idx j )← 0.0 5: end for 6: end if 7: Return yij = arg max(P i j ) as the predicted word for\ncurrent WD step.\nin Algorithm 1. “j == 1” in line “3” means the current WD step is predicting the first word of a keyphrase. In short, the exclusive loss punishes the model for the tendency to generate the same first word of the current keyphrase with the first words of previously-generated keyphrases within the window size KEL.\nHard Exclusion Mechanism. An exclusive search (ES) is introduced in the inference stage as shown in Algorithm 2. The exclusive search mechanism forces the word-level decoding to predict a different first word with the first words of previously-predicted keyphrases within the window size KES .\nSince a keyphrase usually has only two or three words, the first word significantly affects the prediction of the following words. Therefore, both the soft and the hard exclusion mechanisms can improve the diversity of generated keyphrases."
  }, {
    "heading": "5 Experiment Setup",
    "text": "Our model implementations are based on the OpenNMT system (Klein et al., 2017) using PyTorch (Paszke et al., 2017). Experiments of all\nmodels are repeated with three different random seeds and the averaged results are reported."
  }, {
    "heading": "5.1 Datasets",
    "text": "We employ four scientific article benchmark datasets to evaluate our models, including KP20k (Meng et al., 2017), Inspec (Hulth, 2003), Krapivin (Krapivin et al., 2009), and SemEval (Kim et al., 2010). Following previous work (Yuan et al., 2018; Chen et al., 2019a), we use the training set of KP20k to train all the models. After removing the duplicated data, we maintain 509,818 data samples in the training set, 20,000 in the validation set, and 20,000 in the testing set. After training, we test all the models on the testing datasets of these four benchmarks. The dataset statistics are shown in Table 1."
  }, {
    "heading": "5.2 Baselines",
    "text": "We focus on the comparisons with state-of-the-art decoding methods and choose the following generation models under the new setting as our baselines:\n• Transformer (Vaswani et al., 2017). A transformer-based sequence to sequence model incorporating with copy mechanism.\n• catSeq (Yuan et al., 2018). An RNN-based attentional encoder-decoder model with copy mechanism. Both the encoding and decoding are sequential.\n• catSeqD (Yuan et al., 2018). An extension of catSeq which incorporates orthogonal regularization (Bousmalis et al., 2016) and target encoding into the sequential decoding process to improve the generation diversity and accuracy.\n• catSeqCorr (Chan et al., 2019). Another extension of catSeq, which incorporates the sequential decoding with coverage (See et al., 2017) and review mechanisms to boost the generation diversity and accuracy. This method is adjusted from Chen et al. (2018) to fit the new setting.\nIn this paper, we propose two novel models that are denoted as follows:\n• ExHiRD-s. Our Exclusive HieRarchical Decoding model with the soft exclusion mechanism. In experiments, the window size KEL is selected as 4 after tuning on the KP20k validation dataset.\n• ExHiRD-h. Our Exclusive HieRarchical Decoding model with the hard exclusion mechanism. In experiments, the values of the window size KES are selected as 4, 1, 1, 1 for Inspec, Krapivin, SemEval, and KP20k respectively after tuning on the corresponding validation datasets.\nWe choose the bilinear attention from Luong et al. (2015) and the copy mechanism from See et al. (2017) for all the models."
  }, {
    "heading": "5.3 Evaluation Metrics",
    "text": "We engage F1@M which is recently proposed in Yuan et al. (2018) as one of our evaluation metrics. F1@M compares all the predicted keyphrases by the model with ground-truth keyphrases, which means it does not use a fixed cutoff for the predictions. Therefore, it considers the number of predictions.\nWe also use F1@5 as another evaluation metric. When the number of predictions is less than five, we randomly append incorrect keyphrases until it obtains five predictions instead of directly using the original predictions. If we do not adopt such an appending operation, F1@5 will become the same with F1@M when the prediction number is less than five.\nThe macro-averaged F1@M and F1@5 scores are reported. When determining whether two keyphrases are identical, all the keyphrases are stemmed first. Besides, all the duplicated keyphrases are removed after stemming."
  }, {
    "heading": "5.4 Implementation Details",
    "text": "Following previous work (Meng et al., 2017; Yuan et al., 2018; Chen et al., 2019a; Chan et al., 2019), we lowercase the characters, tokenize the sequences, and replace digits with “<digit>” token. Similar to Yuan et al. (2018), when training, the present keyphrase targets are sorted according to the orders of their first occurrences in the document. Then, the absent keyphrase targets are put at the end of the sorted present keyphrase targets. We use “<p start>” and “<a start>” as the\n“[neopd]” token of present and absent keyphrases respectively. “;” is employed as the “[eowd]” token for both present and absent keyphrases. “</s>” is used as the “[eopd]” token.\nThe vocabulary with 50,000 tokens is shared between the encoder and decoder. We set de as 100 and d as 300. The hidden states of the encoder layers are initialized as zeros. In the training stage, we randomly initialize all the trainable parameters including the embedding using a uniform distribution in [−0.1, 0.1]. We set batch size as 10, max gradient norm as 1.0, and initial learning rate as 0.001. We do not use dropout. Adam (Kingma and Ba, 2014) is used as our optimizer. The learning rate decays to half if the perplexity on KP20k validation set stops decreasing. Early stopping is applied when training. When inference, we set the minimum phrase-level decoding step as 1 and the maximum as 20."
  }, {
    "heading": "6 Results and Analysis",
    "text": ""
  }, {
    "heading": "6.1 Present and Absent Keyphrase Predictions",
    "text": "We show the present and absent keyphrase prediction results in Table 2 and Table 3 correspondingly. As indicated in these two tables, both the ExHiRD-s model and the ExHiRD-h outperform the state-of-the-art baselines on most of the metrics, which demonstrates the effectiveness of our exclusive hierarchical decoding methods. Besides, the ExHiRD-h model consistently achieves the best results on both present and absent keyphrase pre-\ndiction in all the datasets2."
  }, {
    "heading": "6.2 Duplication Ratio of Predicted Keyphrases",
    "text": "In this section, we study the model capability of avoiding producing duplicated keyphrases. Duplication ratio is denoted as “DupRatio” and defined as follows:\nDupRatio = # duplications # predictions , (10)\nwhere # means “the number of”. For instance, the DupRatio is 0.5 (3/6) for [A, A, B, B, A, C].\nWe report the average DupRatio per document in Table 4. From this table, we observe that our ExHiRD-s and ExHiRD-h consistently and significantly reduce the duplication ratios on all datasets. Moreover, we also find that our ExHiRDh model achieves the lowest duplication ratios on all datasets.\n2We also tried to simultaneously incorporate the soft and the hard exclusion mechanisms into our hierarchical decoding model, but it still underperforms ExHiRD-h."
  }, {
    "heading": "6.3 Number of Predicted Keyphrases",
    "text": "We also study the average number of unique keyphrase predictions per document. Duplicated keyphrases are removed. The results are shown in Table 5. One main finding is that all the models generate an insufficient number of unique keyphrases on most datasets, especially for predicting absent keyphrases. We also observe that our methods can improve the number of unique keyphrases by a large margin, which is extremely beneficial to solve the problem of insufficient generation. Correspondingly, it also leads to overgenerate more keyphrases than the ground-truth for the cases that do not have this problem, such as the present keyphrase predictions on Krapivin and KP20k datasets. We leave solving the overgeneration of present keyphrases on Krapivin and KP20k as our future work."
  }, {
    "heading": "6.4 ExHiRD-h: Ablation Study",
    "text": "Since our ExHiRD-h model achieves the best performance on almost all of the metrics, we select it as our final model and probe it more subtly in the following sections. In order to understand the effects of each component of ExHiRD-h, we conduct an ablation study on it and report the results on the SemEval dataset in Table 6.\nWe observe that both our hierarchical decoding process and exclusive search mechanism are help-\nful to generate more accurate present and absent keyphrases. Besides, we also find that the significant performance margins on the duplication ratio and the keyphrase numbers are mainly from the exclusive search mechanism."
  }, {
    "heading": "6.5 ExHiRD-h: Window Size of Exclusive Search",
    "text": "For a more comprehensive understanding of our exclusive search mechanism in our ExHiRD-h model, we also study the effects of the window size KES . We conduct the experiments on KP20k dataset and list the results in Table 7.\nWe note that a larger window size KES leads to a lower DupRatio as we anticipated. It is because the exclusive search can observe more previouslygenerated keyphrases to avoid generating duplicated keyphrases when KES is larger. When KES is “all”, the DupRatio is not absolute zero because we stem keyphrases when determining whether they are duplicated. Besides, we also find that larger KES leads to better F1@5 scores. The reason is that for F1@5 scores, we append incorrect keyphrases to obtain five predictions when the number of predictions is less than five. A larger KES leads to predict more unique keyphrases, append less absolutely incorrect keyphrases and improve the chance to output more accurate keyphrases. However, generating more unique keyphrases may also lead to more incorrect predictions, which will degrade the F1@M scores since F1@M considers all the unique predictions without a fixed cutoff."
  }, {
    "heading": "6.6 ExHiRD-h: Incorporate Baselines with Exclusive Search",
    "text": "Our exclusive search is a general method that can be easily applied to other models. In this section, we study the effects of our exclusive search on other baseline models. We show the experimental results on KP20k dataset in Table 8.\nFrom this table, we note that the effects of exclusive search on baselines are similar to the effects on our hierarchical decoding. We also see our ExHiRD-h still achieves the best performance on most of the metrics, even if baselines are also incorporated with exclusive search, which exhibits the superiority of our hierarchical decoding again."
  }, {
    "heading": "7 ExHiRD-h: Case Study",
    "text": "We display a prediction example in Figure 3. Our ExHiRD-h model generates more accurate keyphrases for the document comparing to the four baselines. Besides, we also observe much less repeated keyphrases are generated by our ExHiRDh. For instance, all the baselines produce the keyphrase “debugging” at least three times. However, our ExHiRD-h only generates it once, which demonstrates that our proposed method is more powerful in avoiding duplicated keyphrases."
  }, {
    "heading": "8 Conclusion and Future Work",
    "text": "In this paper, we propose an exclusive hierarchical decoding framework for keyphrase generation. Unlike previous sequential decoding methods, our hierarchical decoding consists of a phrase-level decoding process to capture the current aspect to summarize and a word-level decoding process to generate keyphrases based on the captured aspect. Besides, we also propose a soft and a hard exclusion mechanisms to enhance the diversity of the generated keyphrases. Extensive experimental results demonstrate the effectiveness of our meth-\nods. One interesting future direction is to explore whether the beam search is helpful to our model."
  }, {
    "heading": "Acknowledgments",
    "text": "The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2300174 (Collaborative Research Fund, No. C5026-18GF)). We would like to thank our colleagues for their comments."
  }],
  "year": 2020,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR 2014.",
    "year": 2014
  }, {
    "title": "Opinion expression mining by exploiting keyphrase extraction",
    "authors": ["Gábor Berend."],
    "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1162–1170, Chiang Mai, Thailand. Asian Federation of Natural Language",
    "year": 2011
  }, {
    "title": "Domain separation networks",
    "authors": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."],
    "venue": "NeurIPS 2016, pages 343–351.",
    "year": 2016
  }, {
    "title": "Neural keyphrase generation via reinforcement learning with adaptive rewards",
    "authors": ["Hou Pong Chan", "Wang Chen", "Lu Wang", "Irwin King."],
    "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2163–2174,",
    "year": 2019
  }, {
    "title": "Abstractive textimage summarization using multi-modal attentional hierarchical RNN",
    "authors": ["Jingqiang Chen", "Hai Zhuge."],
    "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4046–4056, Brussels, Belgium.",
    "year": 2018
  }, {
    "title": "Keyphrase generation with correlation constraints",
    "authors": ["Jun Chen", "Xiaoming Zhang", "Yu Wu", "Zhao Yan", "Zhoujun Li."],
    "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4057–4066, Brussels, Belgium.",
    "year": 2018
  }, {
    "title": "An integrated approach for keyphrase generation via exploring the power of retrieval and extraction",
    "authors": ["Wang Chen", "Hou Pong Chan", "Piji Li", "Lidong Bing", "Irwin King."],
    "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
    "year": 2019
  }, {
    "title": "Title-guided encoding for keyphrase generation",
    "authors": ["Wang Chen", "Yifan Gao", "Jiani Zhang", "Irwin King", "Michael R. Lyu."],
    "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial",
    "year": 2019
  }, {
    "title": "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Hierarchical neural story generation",
    "authors": ["Angela Fan", "Mike Lewis", "Yann Dauphin."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
    "year": 2018
  }, {
    "title": "Incorporating expert knowledge into keyphrase extraction",
    "authors": ["Sujatha Das Gollapalli", "Xiaoli Li", "Peng Yang."],
    "venue": "AAAI 2017, pages 3180–3187.",
    "year": 2017
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
    "year": 2016
  }, {
    "title": "Improved automatic keyword extraction given more linguistic knowledge",
    "authors": ["Anette Hulth."],
    "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216– 223.",
    "year": 2003
  }, {
    "title": "A study on automatically extracted keywords in text categorization",
    "authors": ["Anette Hulth", "Beáta B. Megyesi."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational",
    "year": 2006
  }, {
    "title": "SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
    "authors": ["Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin."],
    "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21–26, Uppsala, Swe-",
    "year": 2010
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "OpenNMT: Opensource toolkit for neural machine translation",
    "authors": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush."],
    "venue": "Proceedings of ACL 2017, System Demonstrations, pages 67–72, Vancouver, Canada. Association for",
    "year": 2017
  }, {
    "title": "Large dataset for keyphrases extraction",
    "authors": ["Mikalai Krapivin", "Aliaksandr Autaeu", "Maurizio Marchese."],
    "venue": "Technical report, University of Trento.",
    "year": 2009
  }, {
    "title": "Scientific information extraction with semisupervised neural tagging",
    "authors": ["Yi Luan", "Mari Ostendorf", "Hannaneh Hajishirzi."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2641–2651, Copen-",
    "year": 2017
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lis-",
    "year": 2015
  }, {
    "title": "Human-competitive tagging using automatic keyphrase extraction",
    "authors": ["Olena Medelyan", "Eibe Frank", "Ian H. Witten."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318–1327, Singapore. As-",
    "year": 2009
  }, {
    "title": "Deep keyphrase generation",
    "authors": ["Rui Meng", "Sanqiang Zhao", "Shuguang Han", "Daqing He", "Peter Brusilovsky", "Yu Chi."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2017
  }, {
    "title": "TextRank: Bringing order into text",
    "authors": ["Rada Mihalcea", "Paul Tarau."],
    "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
    "year": 2004
  }, {
    "title": "Keyphrase extraction in scientific publications",
    "authors": ["Thuy Dung Nguyen", "Min-Yen Kan."],
    "venue": "ICADL 2007, pages 317–326.",
    "year": 2007
  }, {
    "title": "Automatic differentiation in pytorch",
    "authors": ["Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer."],
    "venue": "NIPS-W.",
    "year": 2017
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
    "year": 2017
  }, {
    "title": "Abstractive document summarization with a graphbased attentional neural model",
    "authors": ["Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
    "year": 2017
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
    "year": 2017
  }, {
    "title": "Domainindependent abstract generation for focused meeting summarization",
    "authors": ["Lu Wang", "Claire Cardie."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1395–1405,",
    "year": 2013
  }, {
    "title": "Recognizing contextual polarity in phraselevel sentiment analysis",
    "authors": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."],
    "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Process-",
    "year": 2005
  }, {
    "title": "KEA: practical automatic keyphrase extraction",
    "authors": ["Ian H. Witten", "Gordon W. Paynter", "Eibe Frank", "Carl Gutwin", "Craig G. Nevill-Manning."],
    "venue": "Proceedings of the Fourth ACM conference on Digital Libraries 1999, pages 254–255.",
    "year": 1999
  }, {
    "title": "Hierarchical text generation and planning for strategic dialogue",
    "authors": ["Denis Yarats", "Mike Lewis."],
    "venue": "ICML 2018, pages 5587–5595.",
    "year": 2018
  }, {
    "title": "Semi-supervised learning for neural keyphrase generation",
    "authors": ["Hai Ye", "Lu Wang."],
    "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4142–4153, Brussels, Belgium. Association for Computational Lin-",
    "year": 2018
  }, {
    "title": "One size does not fit all: Generating and evaluating variable number of keyphrases",
    "authors": ["Xingdi Yuan", "Tong Wang", "Rui Meng", "Khushboo Thaker", "Peter Brusilovsky", "Daqing He", "Adam Trischler."],
    "venue": "CoRR, abs/1810.05241.",
    "year": 2018
  }, {
    "title": "Keyphrase extraction using deep recurrent neural networks on twitter",
    "authors": ["Qi Zhang", "Yang Wang", "Yeyun Gong", "Xuanjing Huang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 836–845,",
    "year": 2016
  }, {
    "title": "Incorporating linguistic constraints into keyphrase generation",
    "authors": ["Jing Zhao", "Yuxiang Zhang."],
    "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5224–5233, Florence, Italy. Association for Compu-",
    "year": 2019
  }],
  "id": "SP:ba46ece6feba34c408d081a8dce66f0ecf4b7a60",
  "authors": [{
    "name": "Wang Chen",
    "affiliations": []
  }, {
    "name": "Hou Pong Chan",
    "affiliations": []
  }, {
    "name": "Piji Li",
    "affiliations": []
  }, {
    "name": "Irwin King",
    "affiliations": []
  }],
  "abstractText": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previouslypredicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases1.",
  "title": "Exclusive Hierarchical Decoding for Deep Keyphrase Generation"
}