{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Machine learning is increasingly applied in security-critical domains such as automotive systems, healthcare, finance and robotics. To ensure safe deployment in these applications, there is an increasing need to design machine-learning algorithms that are robust in the presence of adversarial attacks.\nA realistic attack paradigm that has received a lot of recent attention (Goodfellow et al., 2014; Papernot et al., 2016a; Szegedy et al., 2013; Papernot et al., 2017b) is test-time\n*Equal contribution 1University of California, San Diego 2University of Wisconsin-Madison. Correspondence to: Kamalika Chaudhuri <kamalika@cs.ucsd.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1Code available at: https://github.com/ EricYizhenWang/robust_nn_icml\nattacks via adversarial examples. Here, an adversary has the ability to provide modified test inputs to an alreadytrained classifier, but cannot modify the training process in any way. Their goal is to perturb legitimate test inputs by a “small amount” in order to force the classifier to report an incorrect label. An example is an adversary that replaces a stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize it as an yield sign. This attack is undetectable to the human eye if the perturbation is small enough.\nPrior work has considered adversarial examples in the context of linear classifiers (Lowd and Meek, 2005), kernel SVMs (Biggio et al., 2013) and neural networks (Szegedy et al., 2013; Goodfellow et al., 2014; Papernot et al., 2017b; 2016a; Moosavi-Dezfooli et al., 2016). However, most of this work has either been empirical, or has focussed on developing theoretically motivated attacks and defenses. Consequently, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood.\nThis work develops a theoretical framework for robust learning in order to understand the effects of distributional properties and finite samples on robustness. Building on traditional bias-variance theory (Friedman et al., 2000), we posit that a classification algorithm may be robust to adversarial examples due to three reasons. First, it may be distributionally robust, in the sense that the output classifier is robust as the number of training samples grow to infinity. Second, even the output of a distributionally robust classification algorithm may be vulnerable due to too few training samples – this is characterized by finite sample robustness. Finally, different training algorithms might result in classifiers with different degrees of robustness, which we call algorithmic robustness. These quantities are analogous to bias, variance and algorithmic effects respectively.\nNext, we analyze a simple non-parametric classification algorithm: k-nearest neighbors in our framework. Our analysis demonstrates that large sample robustness properties of this algorithm depend very much on k.\nSpecifically, we identify two distinct regimes for k with vastly different robustness properties. When k is constant, we show that k-nearest neighbors has zero robustness in the\nlarge sample limit in regions where p(y = 1|x) lies in (0, 1). This is in contrast with accuracy, which may be quite high in these regions. For k = Ω( √ dn log n), where d is the data dimension and n is the sample size, we show that the robustness region of k-nearest neighbors approaches that of the Bayes Optimal classifier in the large sample limit. This is again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known for a much slower growing k (Devroye et al., 1994; Chaudhuri and Dasgupta, 2014). Since k = Ω( √ dn log n) is too high to use in practice with nearest neighbors, we next propose a novel robust version of the 1-nearest neighbor classifier that operates on a modified training set. We provably show that in the large sample limit, this algorithm has superior robustness to standard 1-nearest neighbors for data distributions with certain properties.\nFinally, we validate our theoretical results by empirically evaluating our algorithm on three datasets against several popular attacks. Our experiments demonstrate that our algorithm performs better than or about as well as both standard 1-nearest neighbors and nearest neighbors with adversarial training – a popular and effective defense mechanism. This suggests that although our performance guarantees hold in the large sample limit, our algorithm may have good robustness properties even for realistic training data sizes."
  }, {
    "heading": "1.1. Related Work",
    "text": "Adversarial examples have recently received a great deal of attention (Goodfellow et al., 2014; Biggio et al., 2013; Papernot et al., 2016a; Szegedy et al., 2013; Papernot et al., 2017b). Most of the work, however, has been empirical, and has focussed on developing increasingly sophisticated attacks and defenses."
  }, {
    "heading": "1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES",
    "text": "Prior theoretical work on adversarial examples falls into two categories – analysis and theory-inspired defenses. Work on analysis includes (Fawzi et al., 2016), which analyzes the robustness of linear and quadratic classifiers under random and semi-random perturbations. (Hein and Andriushchenko, 2017) provides robustness guarantees on linear and kernel classifiers trained on a given data set. (Gilmer et al., 2018) shows that linear classifiers for high dimensional datasets may have inherent robustness-accuracy trade-offs.\nWork on theory-inspired defenses include (Mądry et al., 2017; Kolter and Wong, 2017; Aman Sinha, 2018), who provide defense mechanisms for adversarial examples in neural networks that are relaxations of certain principled optimization objectives. (Katz et al., 2017) shows how to use program verification to certify robustness of neural networks around given inputs for small neural networks.\nOur work differs from these in two important ways. First, unlike most prior work which looks at a given training dataset, we consider effects of the data distribution and number of samples, and analyze robustness properties in the large sample limit. Second, unlike prior work which largely focuses on parametric methods such as neural networks, our focus is on a canonical non-parametric method – the nearest neighbors classifier."
  }, {
    "heading": "1.1.2. RELATED WORK ON NEAREST NEIGHBORS",
    "text": "There has been a body of work on the convergence and consistency of nearest-neighbor classifiers and their many variants (Cover and Hart, 1967; Stone, 1977; Kulkarni and Posner, 1995; Devroye and Wagner, 1977; Chaudhuri and Dasgupta, 2014; Kontorovich and Weiss, 2015); all these works however consider accuracy and not robustness.\nIn the asymptotic regime, (Cover and Hart, 1967) shows that the accuracy of 1-nearest neighbors converges in the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the expected error rate of the Bayes Optimal classifier. This implies that even 1-nearest neighbor may achieve relatively high accuracy even when p(y = 1|x) is not 0 or 1. In contrast, we show that 1-nearest neighbor is inherently nonrobust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.\nFor larger k, the accuracy of k-nearest neighbors is known to converge to that of the Bayes Optimal classifier if kn → ∞ and kn/n → 0 as the sample size n → ∞. We show that the robustness also converges to that of the Bayes Optimal classifier when kn grows at a much higher rate – fast enough to ensure uniform convergence. Whether this high rate is necessary remains an intriguing open question.\nFinite sample rates on the accuracy of nearest neighbors are known to depend heavily on properties of the data distribution, and there is no distribution free rate as in parametric methods (Devroye and Wagner, 1977). (Chaudhuri and Dasgupta, 2014) provides a clean characterization of the finite sample rates of nearest neighbors as a function of natural interiors of the classes. Here we build on their results by defining a stricter, more robust version of interiors and providing bounds as functions of these new robust quantities."
  }, {
    "heading": "1.1.3. OTHER RELATED WORK",
    "text": "(Amsaleg et al., 2016) provides a method for generating adversarial examples for nearest neighbors, and shows that the effectiveness of attacks grow with intrinsic dimensionality. Finally, (Papernot et al., 2016b; 2017b) provides black-box attacks on substitute classifiers; their experiments show that attacks from other types of substitute classifiers are not successful on nearest neighbors; our experiments corroborate these results.\n2. The Setting and Definitions"
  }, {
    "heading": "2.1. The Basic Setup",
    "text": "We consider test-time attacks in a white box setting, where the adversary has full knowledge of the training process – namely, the type of classifier used, the training data and any parameters – but cannot modify training in any way.\nGiven an input x, the adversary’s goal is to perturb it so as to force the trained classifier f to report a different label than f(x). The amount of perturbation is measured by an application-specific metric d, and is constrained to be within a radius r. Our analysis can be extended to any metric, but for this paper we assume that d is the Euclidean distance for mathematical simplicity; we also focus on binary classification, and leave extensions to multiclass for future work.\nFinally, we assume that unlabeled instances are drawn from an instance space X , and their labels are drawn from the label space {0, 1}. There is an underlying data distribution D that generates labeled examples; the marginal over X of D is µ and the conditional distribution of labels given x is denoted by η."
  }, {
    "heading": "2.2. Robustness and astuteness",
    "text": "We begin by defining robustness, which for a classifier f at input x is measured by the robustness radius. Definition 2.1 (Robustness Radius). The robustness radius of a classifier f at an instance x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x� to which f assigns a label different from f(x):\nρ(f, x) = inf r {∃x� ∈ X ∩B(x, r) s.t f(x) �= f(x�)}\nObserve that the robustness radius measures a classifier’s local robustness. A classifier f with robustness radius r at x guarantees that no adversarial example of x with norm of perturbation less than r can be created using any attack method. A plausible way to extend this into a global notion is to require a lower bound on the robustness radius everywhere; however, only the constant classifier will satisfy this condition. Instead, we consider robustness around meaningful instances, that we model as examples drawn from the underlying data distribution. Definition 2.2 (Robustness with respect to a Distribution). The robustness of a classifier f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ), is the fraction of instances drawn from µ for which the robustness radius is greater than or equal to r.\nR(f, r, µ) = Pr x∼µ\n(ρ(f, x) ≥ r)\nFinally, observe that we are interested in classifiers that are both robust and accurate. This leads to the notion of\nastuteness, which measures the fraction of instances on which a classifier is both accurate and robust.\nDefinition 2.3 (astuteness). The astuteness of a classifier f with respect to a data distribution D and a radius r is the fraction of examples on which it is accurate and has robustness radius at least r; formally,\nAstD(f, r) = Pr (x,y)∼D\n(ρ(f, x) ≥ r, f(x) = y),\nObserve that astuteness is analogous to classification accuracy, and we argue that it is a more appropriate metric if we are concerned with both robustness and accuracy. Unlike accuracy, astuteness cannot be directly empirically measured unless we have a way to certify a lower bound on the robustness radius. In this work, we will prove bounds on the astuteness of classifiers, and in our experiments, we will approximate it by measuring resistance to standard attacks."
  }, {
    "heading": "2.3. Sources of Robustness",
    "text": "There are three plausible reasons why classifiers lack robustness – distributional, finite sample and algorithmic. These sources are analogous to bias, variance, and algorithmic effects respectively in standard bias-variance theory.\nDistributional robustness measures the effect of the data distribution on robustness when an infinitely large number of samples are used to train the classifier. Formally, if Sn is a training sample of size n drawn from D and A(Sn, ·) is a classifier obtained by applying the training procedure A on Sn, then the distributional robustness at radius r is limn→∞ ESn∼D[R(A(Sn, ·), r, µ)]. In contrast, for finite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ) for finite n – usually by putting high probability bounds over the training set. Thus, finite sample robustness depends on the training set size n, and quantifies how it changes with sample size. Finally, robustness also depends on the training algorithm itself; for example, some variants of nearest neighbors may have higher robustness than nearest neighbors itself.\n2.4. Nearest Neighbor and Bayes Optimal Classifiers\nGiven a training set Sn = {(X1, Y1), . . . , (Xn, Yn)} and a test example x, we use the notation X(i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of X(i)(x).\nGiven a test example x, the k-nearest neighbor classifier Ak(Sn, x) outputs:\n= 1, if Y (1)(x) + . . .+ Y (k)(x) ≥ k/2 = 0, otherwise.\nThe Bayes optimal classifier g over a data distribution D\nhas the following classification rule:\ng(x) = � 1 if η(x) = Pr(y = 1|x) ≥ 1/2; 0 otherwise. (1)"
  }, {
    "heading": "3. Robustness of Nearest Neighbors",
    "text": "How robust is the k-nearest neighbor classifier? We show that it depends on the value of k. Specifically, we identify two distinct regimes – constant k and k = Ω( √ dn log n) where d is the data dimension – and show that nearest neighbors has different robustness properties in the two."
  }, {
    "heading": "3.1. Low k Regime",
    "text": "In this region, k is a constant that does not depend on the training set size n. Provided certain regularity conditions hold, we show that k-nearest neighbors is inherently nonrobust in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes 0 in the large sample limit. Theorem 3.1. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean metric in a neighborhood of x. Then, for fixed k, ρ(Ak(Sn, ·), x) converges in probability to 0.\nRemarks. Observe that Theorem 3.1 implies that the distributional robustness (and hence astuteness) in a region where η(x) ∈ (0, 1) is 0. This is in contrast with accuracy; for 1-NN, the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the Bayes Optimal classifier, and thus may be quite high.\nThe proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect to an embedded manifold will give the same result, but will result in a more complex proof.\nIn the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the interior of the region where η ∈ {0, 1}, and provide finite sample rates for this case."
  }, {
    "heading": "3.2. High k Regime",
    "text": "Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor classifiers converge to the Bayes Optimal, provided k is set properly. We next show that if k is Ω( √ dn log n), the regions of robustness and the astuteness of the k nearest neighbor classifiers also approach the corresponding quantities for the Bayes Optimal classifier as n → ∞. Thus, if the Bayes Optimal classifier is robust, then so is k-nearest neighbors in the large sample limit. The main intuition is that k = Ω( √ dn log n) is large enough for uniform convergence – where, with high probability, all\nEuclidean balls with k examples have the property that the empirical averages of their labels are close to their expectations. This guarantees that for any x, the k-nearest neighbor reports the same label as the Bayes Optimal classifier for all x� close to x. Thus, if the Bayes Optimal classifier is robust, so is nearest neighbors."
  }, {
    "heading": "3.2.1. DEFINITIONS",
    "text": "We begin with some definitions that we can use to characterize the robustness of the Bayes Optimal classifier. Following (Chaudhuri and Dasgupta, 2014), we use the notation Bo(x, r) to denote an open ball and B(x, r) to denote a closed ball of radius r around x. We define the probability radius of a ball around x as:\nrp(x) = inf{r | µ(B(x, r)) ≥ p}\nWe next define the r-robust (p,Δ)-strict interiors as follows:\nX+r,Δ,p = {x ∈ supp(µ) | ∀x� ∈ Bo(x, r), ∀x�� ∈ B(x�, rp(x�)), η(x��) > 1/2 +Δ} X−r,Δ,p = {x ∈ supp(µ) | ∀x� ∈ Bo(x, r), ∀x�� ∈ B(x�, rp(x�)), η(x��) < 1/2−Δ}\nWhat is the significance of these interiors? Let x� be an instance such that all x�� ∈ B(x�, rp(x�)) have η(x��) > 1/2 +Δ. If p ≈ kn , then the k points x�� closest to x� have η(x��) > 1/2 + Δ. Provided the average of the labels of these points is close to expectation, which happens when k is large relative to 1/Δ, k-nearest neighbor outputs label 1 on x�. When x is in the r-robust (p,Δ)-strict interior region X+r,Δ,p, this is true for all x� within distance r of x, which means that k-nearest neighbors will be robust at x. Thus, the r-robust (p,Δ)-strict interior is the region where we natually expect k-nearest neighbor to have robustness radius r, when k is large relative to 1Δ and p ≈ kn . Readers familiar with (Chaudhuri and Dasgupta, 2014) will observe that the set of all x� for which ∀x�� ∈ B(x�, rp(x�)), η(x��) > 1/2 +Δ forms a stricter version of the (p,Δ)-interiors of the 1 region that was defined in this work; these x� also represent the region where k-nearest neighbors are accurate when k ≈ max(np, 1/Δ2). The rrobust (p,Δ)-strict interior is thus a somewhat stricter and more robust version of this definition."
  }, {
    "heading": "3.2.2. MAIN RESULTS",
    "text": "We begin by characterizing where the Bayes Optimal classifier is robust.\nTheorem 3.2. The Bayes Optimal classifier has robustness radius r at x ∈ X+r,0,0 ∪ X−r,0,0. Moreover, its astuteness is E[η(x)1(x ∈ X+r,0,0)] + E[(1− η(x))1(x ∈ X−r,0,0)].\nThe proof is in the Appendix, along with analogous results for astuteness. The following theorem, along with a similar result for astuteness, proved in the Appendix, characterizes robustness in the large k regime.\nTheorem 3.3. For any n, pick a δ and a Δn → 0. There exist constant C1 and C2 such that if\nkn ≥ C1 √ dn log n+n log(1/δn)\nΔn , and pn ≥ knn (1 +\nC2\n� d log n+log(1/δ)\nkn ), then, with probability ≥ 1− 3δ, kn-\nNN has robustness radius r in x ∈ X+r,Δn,pn ∪ X − r,Δn,pn .\nRemarks. Some remarks are in order. First, observe that as n → ∞, Δn and pn tend to 0; thus, provided certain continuity conditions hold, X+r,Δn,pn ∪X − r,Δn,pn\napproaches X+r,0,0 ∪ X−r,0,0, the robustness region of the Bayes Optimal classifier.\nSecond, observe that as r-robust strict interiors extend the definition of interiors in (Chaudhuri and Dasgupta, 2014), Theorem 3.3 is a robustness analogue of Theorem 5 in this work. Unlike the latter, Theorem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open question for future work."
  }, {
    "heading": "4. A Robust 1-NN Algorithm",
    "text": "Section 3 shows that nearest neighbors is robust for k as large as Ω( √ dn log n). However, this k is too high to use in practice – high values of k require even higher sample sizes (Chaudhuri and Dasgupta, 2014), and lead to higher running times. Thus a natural question is whether we can find a more robust version of the algorithm for smaller k. In this section, we provide a more robust version of 1-nearest neighbors, and analytically demonstrate its robustness.\nOur algorithm is motivated by the observation that 1-nearest neighbor is robust when oppositely labeled points are far apart, and when test points lie close to training data. Most training datasets however contain nearby points that are oppositely labeled; thus, we propose to remove a subset of training points to enforce this property.\nWhich points should we remove? A plausible approach is to keep the largest subset where oppositely labeled points are far apart; however, this subset has poor stability properties even for large n. Therefore, we propose to keep all points x such that: (a) we are highly confident about the label of x and its nearby points and (b) all points close to x have the same label. Given that all such x are kept, we remove as few points as possible, and execute nearest neighbors on the remaining dataset.\nThe following definition characterizes data where oppositely labeled points are far apart.\nDefinition 4.1 (r-separated set). A set A = {(x1, y1), . . . , (xm, ym)} of labeled examples is said to be r-separated if for all pairs (xi, yi), (xj , yj) ∈ A, �xi − xj� ≤ r implies yi = yj .\nThe full algorithm is described in Algorithm 1 and Algorithm 2. Given confidence parameters Δ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability ≥ 1− δ, the empirical majority of kn labels agrees with the majority in expectation, provided the latter is at least Δ away from 12 .\nAlgorithm 2 is used to determine whether an xi should be kept. Let f(xi) be the output of Algorithm 2 on xi. If yi = f(xi) and if for all xj ∈ B(xi, r), f(xi) = f(xj) = yi, then we mark xi as red. Finally, we compute the largest r-separated subset of the training data that includes all the red points; this reduces to a constrained matching problem as in (Kontorovich and Weiss, 2015). The resulting set, returned by Algorithm 1, is our new training set. We observe that this set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are far apart. Moreover, we keep all (xi, yi) when we are confident about the label of xi and its nearby points. Observe that our final procedure is a 1-NN algorithm, even though kn neighbors are used to determine if a point should be retained in the training set."
  }, {
    "heading": "4.1. Performance Guarantees",
    "text": "The following theorem establishes performance guarantees for Algorithm 1. Theorem 4.2. Pick a Δn and δ, and set kn = 3 log(2n/δ)/Δ2n. Pick a margin parameter τ . Then, there exist constants C and C0 such that the following hold. If we\nset pn = knn (1 + C � d log n+log(1/δ) kn ), and define the set:\nXR = � x ���x ∈ X+r+τ,Δn,pn ∪ X − r+τ,Δn,pn ,\nµ(B(x, τ)) ≥ 2C0 n (d log n+ log(1/δ))\n�\nThen, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, Δn and δ has robustness radius at least r − 2τ on XR.\nRemarks. The proof is in the Appendix, along with an analogous result for astuteness. Observe that XR is roughly the high density subset of the r + τ -robust strict interior X+r+τ,Δn,pn ∪ X − r+τ,Δn,pn\n. Since η(x) is constrained to be greater than 12 + Δn or less than 1 2 − Δn in this region, as opposed to 0 or 1, this is an improvement over standard nearest neighbors when the data distribution has a large high density region that intersects with the interiors.\nA second observation is that as τ is an arbitrary constant, we can set to it be quite small and still satisfy the condition on µ(B(x, τ)) for a large fraction of x’s when n is very large. This means that in the large sample limit, r − 2τ may be close to r and XR may be close to the high density subset of X+r,Δn,pn ∪ X − r,Δn,pn for a lot of smooth distributions.\nAlgorithm 1 Robust_1NN(Sn, r, Δ, δ, x) for (xi, yi) ∈ Sn do\nf(xi) = Confident-Label(Sn,Δ, δ, xi) end for SRED = ∅ for (xi, yi) ∈ Sn do\nif f(xi) = yi and f(xi) = f(xj) for all xj such that �xi − xj� ≤ r and (xj , yj) ∈ Sn then SRED = SRED\n�{(xi, yi)} end if\nend for Let S� be the largest r-separated subset of Sn that contains all points in SRED. return new training set S�\nAlgorithm 2 Confident-Label(Sn, Δ, δ, x) kn = 3 log(2n/δ)/Δ 2\nȳ = (1/kn) �kn i=1 Y (i)(x) if ȳ ∈ [ 12 −Δ, 12 +Δ] then return ⊥ else return 12sgn(ȳ − 12 ) + 12 end if"
  }, {
    "heading": "5. Experiments",
    "text": "The results in Section 4 assume large sample limits. Thus, a natural question is how well Algorithm 1 performs with more reasonable amounts of training data. We now empirically investigate this question.\nSince there are no general methods that certify robustness at an input, we assess robustness by measuring how our algorithm performs against a suite of standard attack methods. Specifically, we consider the following questions:\n1. How does our algorithm perform against popular white box and black box attacks compared with standard baselines?\n2. How is performance affected when we change the training set size relative to the data dimension?\nThese questions are considered in the context of three datasets with varying training set sizes relative to the dimension, as well as two standard white box attacks and black box attacks with two kinds of substitute classifiers."
  }, {
    "heading": "5.1. Methodology",
    "text": "Data. We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with differing data sizes relative to dimension. Halfmoon is a popular 2-dimensional synthetic data set for non-linear classification. We use a training set of size 2000 and a test set of size 1000 generated with standard deviation σ = 0.2. The MNIST 1v7 data set is a subset of the 784- dimensional MNIST data. For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images of each digit. Finally, for the Abalone dataset (Lichman, 2013), our classification task is to distinguish whether an abalone is older than 12.5 years based on 7 physical measurements. For training, we use 500 and for test, 100 samples. In addition, a validation set with the same size as the test set is generated for each experiment for parameter tuning.\nBaselines. We compare Algorithm 1, denoted by RobustNN, against three baselines. The first is the standard 1-nearest neighbor algorithm, denoted by StandardNN. We use two forms of adversarially-trained nearest neighbors - ATNN and ATNN-all. Let S be the training set used by standard nearest neighbors. In ATNN, we augment S by creating, for each (x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and adding (xadv, y). The ATNN classifier is 1-nearest neighbor on this augmented data. In ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods in the experiment, and add them all to S. ATNN-all is the nearest neighbor classifier on this augmented data. For example, for white box Direct Attacks in Section 5.2, ATNN includes adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial examples generated by both Direct and Kernel Substitute Attacks.\nObserve that all algorithms except StandardNN have parameters to tune. RobustNN has three input parameters – Δ, δ and a defense radius r which is an approximation to the robustness radius. For simplicity, we set Δ = 0.45, δ = 0.1 and tune r on the validation set; this can be viewed as tuning the parameter τ in Theorem 4.2. For ATNN and ATNN-all, the methods that generate the augmenting adversarial examples need a perturbation magnitude r; we call this the defense radius. To be fair to all algorithms, we tune the defense radius for each. We consider the adversary with the highest attack perturbation magnitude in the experiment, and select the defense radius that yields the highest validation accuracy against this adversary."
  }, {
    "heading": "5.2. White-box Attacks and Results",
    "text": "To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white box and black box. For white-box attacks, the adversary knows all details about the classifier under attack, including its training data, the\ntraining algorithm and any hyperparameters."
  }, {
    "heading": "5.2.1. ATTACK METHODS",
    "text": "We consider two white-box attacks – direct attack (Amsaleg et al., 2016) and Kernel Substitute Attack (Papernot et al., 2016b).\nDirect Attack. This attack takes as input a test example x, an attack radius r, and a training dataset S (which may be an augmented or reduced dataset). It finds an x� ∈ S that is closest to x but has a different label, and returns the adversarial example xadv = x+ r x−x �\n||x−x�||2 .\nKernel Substitute Attack. This method attacks a substitute kernel classifier trained on the same training set. For a test input �x, a set of training points Z with one-hot labels Y , a\nkernel classifier f predicts the class probability as:\nf : �x →\n� e−||�x−�z|| 2 2/c � �z∈X�\n�z∈X e −||�x−�z||22/c\n· Y\nThe adversary trains a kernel classifier on the training set of the corresponding nearest neighbors, and then generates adversarial examples against this kernel classifier. The advantage is that the prediction of the kernel classifier is differentiable, which allows the use of standard gradientbased attack methods. For our experiments, we use the popular fast-gradient-sign method (FSGM). The parameter c is tuned to yield the most effective attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone."
  }, {
    "heading": "5.2.2. RESULTS",
    "text": "Figure 1 shows the results. We see that RobustNN outperforms all baselines for Halfmoon and Abalone for all attack radii. For MNIST, for low attack radii, RobustNN’s classification accuracy is slightly lower than the others, while it outperforms the others for large attack radii. Additionally, as is to be expected, the Direct Attack results in lower general accuracy than the Kernel Substitute Attack.\nThese results suggest that our algorithm mostly outperforms the baselines StandardNN, ATNN and ATNN-all. As predicted by theory, the performance gain is higher when the training set size is large relative to the dimension – which is the setting where nearest neighbors work well in general. It has superior performance for Halfmoon and Abalone, where the training set size is large to medium relative to dimension. In contrast, in the sparse dataset MNIST, our algorithm has slightly lower classification accuracy for small attack radii, and higher otherwise."
  }, {
    "heading": "5.3. Black-box Attacks and Results",
    "text": "(Papernot et al., 2017b) has observed that some defense methods that work by masking gradients remain highly amenable to black box attacks. In this attack, the adversary is unaware of the target classifier’s nature, parameters or training data, but has access to a seed dataset drawn from the same distribution which they use to train and attack a substitute classifier. To establish robustness properties of Algorithm 1, we therefore validate it against black box attacks based on two types of substitute classifiers."
  }, {
    "heading": "5.3.1. ATTACK METHODS",
    "text": "We use two types of substitute classifiers – kernel classifiers and neural networks. The adversary trains the substitute classifier using the method of (Papernot et al., 2017b) and uses the adversarial examples against the substitute to attack the target classifier.\nKernel Classifier. The kernel classifier substitute is the same as the one in Section 5.2, but trained using the seed data and the method of (Papernot et al., 2017b).\nNeural Networks. The neural network for MNIST is the ConvNet in (Papernot et al., 2017a)’s tutorial. For Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.\nProcedure. To train the substitute classifier, the adversary uses the method of (Papernot et al., 2016b) to augment the seed data for two rounds; labels are obtained by querying the target classifier. Adversarial examples against the substitutes are created by FGSM, following (Papernot et al., 2016b). As a sanity check, we verify the performance of the substitute classifiers on the original training and test sets. Details are in\nTable 1 in the Appendix. Sanity checks on the performance of the substitute classifiers are presented in Table 1 in the Appendix."
  }, {
    "heading": "5.3.2. RESULTS",
    "text": "Figure 2 shows the results. For all algorithms, black box attacks are less effective than white box, which corroborates the results of (Papernot et al., 2016b), who observed that black-box attacks are less successful against nearest neighbors. We also find that the kernel substitute attack is more effective than the neural network substitute, which is expected as kernel classifiers have similar structure to nearest neighbors. Finally, for Halfmoon and Abalone, our algorithm outperforms the baselines for both attacks; however, for MNIST neural network substitute, our algorithm does not perform as well for small attack radii. This again confirms the theoretical predictions that our algorithm’s performance is better when the training set is large relative to the data dimension – the setting in which nearest neighbors work well in general."
  }, {
    "heading": "5.4. Discussion",
    "text": "The results show that our algorithm performs either better than or about the same as standard baselines against popular white box and black box attacks. As expected from our theoretical results, it performs better for denser datasets which have high or medium amounts of training data relative to the dimension, and either slightly worse or better for sparser datasets, depending on the attack radius. Since nonparametric methods such as nearest neighbors are mostly used for dense data, this suggests that our algorithm has good robustness properties even with reasonable amounts of training data."
  }, {
    "heading": "6. Conclusion",
    "text": "We introduce a novel theoretical framework for learning robust to adversarial examples, and introduce notions of distributional and finite-sample robustness. We use these notions to analyze a non-parametric classifier, k-nearest neighbors, and introduce a novel modified 1-nearest neighbor algorithm that has good robustness properties in the large sample limit. Experiments show that these properties are still retained for reasonable data sizes.\nMany open questions remain. The first is to close the gap in analysis of k-nearest neighbors for k in between our two regimes. The second is to develop nearest neighbor algorithms with better robustness guarantees. Finally, we believe that our work is a first step towards a comprehensive analysis of how the size of training data affects robustness; we believe that an important line of future work is to carry out similar analyses for other supervised learning methods."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank NSF under IIS 1253942 for support. This work was also partially supported by ARO under contract number W911NF-1-0405. We thank all anonymous reviewers for their constructive comments."
  }],
  "references": [{
    "title": "Certifiable distributional robustness with principled adversarial training",
    "authors": ["Aman Sinha", "J.D. Hongseok Namkoong"],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "The vulnerability of learning to adversarial perturbation increases with intrinsic dimensionality",
    "authors": ["L. Amsaleg", "J. Bailey", "S. Erfani", "T. Furon", "M.E. Houle", "M. Radovanović", "N.X. Vinh"],
    "year": 2016
  }, {
    "title": "Evasion attacks against machine learning at test time",
    "authors": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. Šrndić", "P. Laskov", "G. Giacinto", "F. Roli"],
    "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 387–402. Springer.",
    "year": 2013
  }, {
    "title": "Rates of convergence for the cluster tree",
    "authors": ["K. Chaudhuri", "S. Dasgupta"],
    "venue": "J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 343–351. Curran Associates, Inc.",
    "year": 2010
  }, {
    "title": "Rates of convergence for nearest neighbor classification",
    "authors": ["K. Chaudhuri", "S. Dasgupta"],
    "venue": "Advances in Neural Information Processing Systems, pages 3437– 3445.",
    "year": 2014
  }, {
    "title": "Nearest neighbor pattern classification",
    "authors": ["T. Cover", "P. Hart"],
    "venue": "IEEE Transactions on Information Theory, 13, 21–27.",
    "year": 1967
  }, {
    "title": "On the strong universal consistency of nearest neighbor regression function estimates",
    "authors": ["L. Devroye", "L. Gyorfi", "A. Krzyzak", "G. Lugosi"],
    "venue": "The Annals of Statistics, pages 1371–1385.",
    "year": 1994
  }, {
    "title": "The strong uniform consistency of nearest neighbor density estimates",
    "authors": ["L.P. Devroye", "T.J. Wagner"],
    "venue": "The Annals of Statistics, pages 536–540.",
    "year": 1977
  }, {
    "title": "Robustness of classifiers: from adversarial to random noise",
    "authors": ["A. Fawzi", "Moosavi-Dezfooli", "S.-M.", "P. Frossard"],
    "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 1632–1640.",
    "year": 2016
  }, {
    "title": "Additive logistic regression: a statistical view of boosting",
    "authors": ["J. Friedman", "T. Hastie", "R Tibshirani"],
    "year": 2000
  }, {
    "title": "Adversarial spheres",
    "authors": ["J. Gilmer", "L. Metz", "F. Faghri", "S.S. Schoenholz", "M. Raghu", "M. Wattenberg", "I. Goodfellow"],
    "venue": "arXiv preprint arXiv:1801.02774.",
    "year": 2018
  }, {
    "title": "Explaining and harnessing adversarial examples",
    "authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"],
    "venue": "arXiv preprint arXiv:1412.6572.",
    "year": 2014
  }, {
    "title": "Formal guarantees on the robustness of a classifier against adversarial manipulation",
    "authors": ["M. Hein", "M. Andriushchenko"],
    "venue": "Advances in Neural Information Processing Systems, pages 2263–2273.",
    "year": 2017
  }, {
    "title": "Towards proving the adversarial robustness of deep neural networks",
    "authors": ["G. Katz", "C. Barrett", "D.L. Dill", "K. Julian", "M.J. Kochenderfer"],
    "venue": "arXiv preprint arXiv:1709.02802.",
    "year": 2017
  }, {
    "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
    "authors": ["J.Z. Kolter", "E. Wong"],
    "venue": "arXiv preprint arXiv:1711.00851.",
    "year": 2017
  }, {
    "title": "A bayes consistent 1-nn classifier",
    "authors": ["A. Kontorovich", "R. Weiss"],
    "venue": "Artificial Intelligence and Statistics Conference.",
    "year": 2015
  }, {
    "title": "Rates of convergence of nearest neighbor estimation under arbitrary sampling",
    "authors": ["S. Kulkarni", "S. Posner"],
    "venue": "IEEE Transactions on Information Theory, 41(4), 1028– 1039.",
    "year": 1995
  }, {
    "title": "Adversarial learning",
    "authors": ["D. Lowd", "C. Meek"],
    "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641–647. ACM.",
    "year": 2005
  }, {
    "title": "Towards deep learning models resistant to adversarial attacks",
    "authors": ["A. Mądry", "A. Makelov", "L. Schmidt", "D. Tsipras", "A. Vladu"],
    "venue": "stat, 1050, 9.",
    "year": 2017
  }, {
    "title": "Probability and computing: Randomized algorithms and probabilistic analysis",
    "authors": ["M. Mitzenmacher", "E. Upfal"],
    "venue": "Cambridge university press.",
    "year": 2005
  }, {
    "title": "Deepfool: a simple and accurate method to fool deep neural networks",
    "authors": ["Moosavi-Dezfooli", "S.-M.", "A. Fawzi", "P. Frossard"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
    "year": 2016
  }, {
    "title": "The limitations of deep learning in adversarial settings",
    "authors": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"],
    "venue": "Proceedings of the 1st IEEE European Symposium on Security and Privacy. arXiv preprint arXiv:1511.07528.",
    "year": 2016
  }, {
    "title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
    "authors": ["N. Papernot", "P. McDaniel", "I. Goodfellow"],
    "venue": "arXiv preprint arXiv:1605.07277.",
    "year": 2016
  }, {
    "title": "2017a). cleverhans v2.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768",
    "authors": ["N. Papernot", "N. Carlini", "I. Goodfellow", "R. Feinman", "F. Faghri", "A. Matyasko", "K. Hambardzumyan", "Juang", "Y.-L", "A. Kurakin", "R. Sheatsley", "A. Garg", "Y.C. Lin"],
    "year": 2017
  }, {
    "title": "Practical black-box attacks against deep learning systems using adversarial examples",
    "authors": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "B. Celik", "A. Swami"],
    "venue": "Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security.",
    "year": 2017
  }, {
    "title": "Consistent nonparametric regression",
    "authors": ["C. Stone"],
    "venue": "Annals of Statistics, 5, 595–645.",
    "year": 1977
  }, {
    "title": "Intriguing properties of neural networks",
    "authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"],
    "venue": "arXiv preprint arXiv:1312.6199.",
    "year": 2013
  }],
  "id": "SP:3afa42542ee1f6f38af25c72f8231b12e7ae130d",
  "authors": [{
    "name": "Yizhen Wang",
    "affiliations": []
  }, {
    "name": "Somesh Jha",
    "affiliations": []
  }, {
    "name": "Kamalika Chaudhuri",
    "affiliations": []
  }],
  "abstractText": "Motivated by safety-critical applications, testtime attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-parametric classifier – the k-nearest neighbors. Our analysis shows that its robustness properties depend critically on the value of k – the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit. Our experiments 1 suggest that this classifier may have good robustness properties even for reasonable data set sizes.",
  "title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples"
}