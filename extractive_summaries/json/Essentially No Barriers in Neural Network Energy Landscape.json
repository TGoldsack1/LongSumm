{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Neural networks have achieved remarkable success in practical applications such as object recognition (He et al., 2016; Huang et al., 2017), machine translation (Bahdanau et al., 2015; Vinyals & Le, 2015), speech recognition (Hinton et al., 2012; Graves et al., 2013; Xiong et al., 2017) etc. Theoretical insights on why neural networks can be trained successfully despite their high-dimensional and non-convex loss functions are few or based on strong assumptions such as the eigenvalues of the Hessian at critical points being random (Dauphin et al., 2014), linear activations (Choromanska et al., 2014; Kawaguchi, 2016) or wide hidden layers (Soudry & Carmon, 2016; Nguyen & Hein, 2017).\nIn the current literature, minima of the loss function are typically depicted as points at the bottom of a strictly convex valley of a certain width that reflects the generalisation of the network, with network parameters given by the location of the minimum (Keskar et al., 2016). This is also the picture obtained when the loss function of neural networks is visualised in low dimension (Li et al., 2017).\nIn this work, we conjecture that neural network loss minima are not isolated points in parameter space, but essentially\n1Heidelberg Collaboratory for Image Processing (HCI), IWR, Heidelberg University, D-69120 Heidelberg, Germany 2Institut für Theoretische Physik, Heidelberg University, D-69120 Heidelberg, Germany. Correspondence to: Fred A. Hamprecht <fred.hamprecht@iwr.uni-heidelberg.de>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nform a connected manifold. More precisely, we argue that the part of the parameter space where the loss remains below a certain low threshold forms one single connected component.\nWe support the above claim by studying the energy landscape of several ResNets and DenseNets on CIFAR10 and CIFAR100: For random pairs of minima, we construct continuous paths through parameter space for which the loss remains very close to the value found directly at the minima. An example for such a path is shown in Figure 1.\nOur main contribution is the finding of paths\n1. that connect minima trained from different initialisations which are not related to each other via known loss-conserving operations like rescaling,\n2. along which the training loss remains essentially at the same value as at the minima,\n3. along which the test loss remains essentially constant while the test error rate slightly increases.\nThe existence of such paths suggests that modern neural networks have enough parameters such that they can achieve\ngood predictions while a big part of the network undergoes structural changes. In closing, we offer qualitative justification of this behaviour that may offer a handle for future theoretical investigation."
  }, {
    "heading": "2. Related Work",
    "text": "In discussions about why neural networks generalise despite the extremely large number of parameters, one often finds the argument that wide minima generalise better (Keskar et al., 2016). This picture is confirmed when visualising the parameter space on a random plane around a minimum (Li et al., 2017). We draw a completely different image of the loss landscape: Minima are not located in finite-width valleys, but there are paths through the parameter space along which the loss remains very close to the value at the minima. A similar view had previously been conjectured by (Sagun et al., 2017). They find flat linear paths between minima that are close in parameter space by construction. We extend their work by constructing flat paths between arbitrary minima.\nIt has previously been shown that minima of networks with ReLU activations are degenerate (Dinh et al., 2017): One can scale all parameters in one layer by a constant α and in following layer by α−1 without changing the output of the network. Here, we provide evidence for a different kind of degeneracy: We construct paths between independent minima that are essentially flat.\n(Freeman & Bruna, 2016) showed that local minima are connected without large barriers for a CNN on MNIST and an RNN on PTB next word prediction. On CIFAR10 however, they found significant barriers between minima for the CNN considered. We extend their work in two ways: First, we consider ResNets and DenseNets that outperform plain CNNs by a large margin. Second, we apply a state of the art method for connecting minima from molecular statistical mechanics: The Automated Nudged Elastic Band (AutoNEB) algorithm (Kolsbjerg et al., 2016) which in turn is based on the Nudged Elastic Band (NEB) algorithm (Jónsson et al., 1998). We additionally systematically replace paths that contain relatively high loss barriers. Combining the above we find paths with essentially no energy barrier.\nNEB has so far been applied to a multi-layer perceptron with a single hidden layer (Ballard et al., 2016). High energy barriers between the minima of network were found when using three hidden neurons, and disappeared upon adding more neurons to the hidden layer. In follow-up work, (Ballard et al., 2017) trained a multi-layer perceptron with a single hidden layer on MNIST. They found that with l2-regularisation, the landscape had no significant energy barriers. However, for their network they report an error rate of 14.8% which is higher than the 12% achieved even by a\nlinear classifier (LeCun et al., 1998) and the 0.35% achieved with a standard CNN (Ciresan et al., 2011).\nIn this work, we apply AutoNEB to a nontrivial network for the first time, and make the surprising observation that different minima of state of the art networks on CIFAR10 and CIFAR100 are connected through essentially flat paths.\nAfter submission of this work to the International Machine Learning Conference (ICML) 2018, (Garipov et al., 2018) independently reported that they also constructed paths between neural network minima. They study the loss landscape of several architectures on CIFAR10 and CIFAR100 and report the same surprising observation: minima are connected by paths with constantly low loss."
  }, {
    "heading": "3. Method",
    "text": "In the following, we use the terms energy and loss interchangeably."
  }, {
    "heading": "3.1. Minimum Energy Path",
    "text": "A neural network loss function depends on the architecture, the training set and the network parameters θ. Keeping the former two fixed, we simply write L(θ) and start with two parameter sets θ1 and θ2. In our case, they are minima of the loss function, i.e. they result from training the networks to convergence. The goal is to find the continuous path p∗ from θ1 to θ2 through parameter space with the lowest maximum loss:\np(θ1, θ2) ∗ = arg min\np from θ1 to θ2\n{ max θ∈p L(θ) } .\nFor this optimisation to be tractable, the loss function must be sufficiently smooth, i.e. contain no jumps along the path. The output and loss of neural networks are continuous functions of the parameters (Montúfar et al., 2014); only the derivative is discontinuous for the case of ReLU activations. However, we cannot give any bounds on how steep the loss function may be. We address this problem by sampling all paths very densely.\nSuch a lowest path p∗ is called the minimum energy path (MEP) (Jónsson et al., 1998). We refer to the parameter set with the maximum loss on a path as the “saddle point” of the path because it is a true saddle point of the loss function.\nIn low-dimensional spaces, it is easy to construct the exact minimum energy path between two minima, for example by using dynamic programming on a densely sampled grid.\nThis is not possible for present day’s neural networks with parameter spaces that have millions of dimensions. We thus must resort to methods that construct an approximation of the MEP between two points using some local heuristics. In particular, we resort to the Automated Nudged Elastic\nBand (AutoNEB) algorithm (Kolsbjerg et al., 2016). This method is based on the Nudged Elastic Band (NEB) algorithm (Jónsson et al., 1998).\nNEB bends a straight line segment by applying gradient forces until there are no more gradients perpendicular to the path. Then, as for the MEP, the highest point of the resulting path is a critical point. While this critical point is not necessarily the saddle point we were looking for, it gives an upper bound for the energy at the saddle point.\nIn the following, we present the mechanical model behind and the details of NEB. We then proceed to AutoNEB.\nMechanical Model A chain of N + 2 pivots (parameter sets) pi for i = 0, . . . , N + 1 is connected via springs of stiffness k. The initial and the final pivots are fixed to the minima to connect, i.e. p0 = θ1 and pN+1 = θ2. Using gradient descent, the path that minimises the following energy function is found:\nE(p) = N∑ i=1 L(pi) + N∑ i=0 1 2 k ‖pi+1 − pi‖2 (1)\nThe problem with this energy formulation lies in the choice of the spring constant: If, on the one hand, k is too small, the distances between the pivots become larger in areas with high energy. However, identifying the highest point on the path and its energy is the very goal of the algorithm, so the sampling rate should be high in the high-energy regions. If, on the other hand, k is chosen too large, it becomes energetically advantageous to shorten and hence straighten the path as the spring energy grows quadratically with the total length of the path. This cuts into corners of the loss surface and the resulting path can miss the saddle point.\nNudged Elastic Band Inspired by the above model, (Jónsson et al., 1998) presented the Nudged Elastic Band (NEB). For brevity, we directly present the improved version by (Henkelman & Jónsson, 2000). The force resulting from Equation (1) consists of a force derived from the loss and a force originating from the springs:\nFi = −∇piE(p) = FLi + FSi For NEB, the physical forces are modified, or nudged, so that the loss force only acts perpendicularly to the path and the spring force only parallelly to the path (see also Figure 2):\nFNEBi = F L i ∣∣ ⊥ + F S i ∣∣ ‖.\nThe direction of the path is defined by the local tangent τ̂i to the path. The two forces now read:\nFLi ∣∣ ⊥ = −(∇L(pi)− (∇L(pi) · τ̂i)τ̂i)\nFSi ∣∣ ‖ = (F S i · τ̂i)τ̂i\n(2)\nwhere the spring force opposes unequal distances along the path:\nFSi = −k(‖pi − pi−1‖ − ‖pi+1 − pi‖) (3)\nIn this formulation, high energy pivots no longer “slide down” from the saddle point. The spring force only redistributes pivots on the path, but does not straighten it. Pivots can be spaced unequally by introducing target distances or unequal spring constants into Equation (3).\nThe local tangent is chosen to point in the direction of one of the adjacent pivots (N normalises to length one):\nτ̂i = N { pi+1 − pi if L(pi+1) > L(pi−1) pi − pi−1 else.\nThis particular choice of τ̂ prevents kinks in the path and ensures a good approximation near the saddle point (Henkelman & Jónsson, 2000).\nThe above procedure requires the following hyperparameters: The spring stiffness k and number of pivots N .\n(Sheppard et al., 2008) claim that a wide range of k leads to the same result on a given loss surface. However, if chosen too large, the optimisation can become unstable. If it is too small, an excessive number of iterations are needed before the pivots become equally distributed. We did not find a value for k that worked well across different loss surfaces\nand number of pivots N . Instead, we re-distribute the pivots in each iteration t and set the actual spring force to zero. The loss force is still restricted to act parallelly to the path. In the literature, this is sometimes referred to as the string method (Sheppard et al., 2008).\nAlgorithm 1 shows how the initial path is iteratively updated using the above forces. As a companion, Figure 2 visualises the forces in one update step for a two dimensional example. In this formulation, we use gradient descent to update the path. Any other gradient based optimiser can be used. It typically introduces additional hyperparameters, for example a learning rate γ. The number of iterations T should be chosen large enough for the optimisation to converge.\nAlgorithm 1 NEB\nInput: initial path p(0) with N + 2 pivots, p (0) 0 = θ1 and p (0) N+1 = θ2. for t = 1, . . . , T do Redistribute pivots on path p(t−1) and store as p. for i = 1, . . . , N do\nCompute projected loss force Fi = FLi ∣∣ ⊥.\nStore pivot p(t)i = pi + γFi. end for\nend for return final path p(T )\nThe evaluation time of Algorithm 1 rises linearly with the number of iterations and the number of pivots on the path. Computing the NEB forces can trivially be parallelised over the pivots.\nThe number of pivots N trades off between computational effort on the one hand and subsampling artefacts on the other hand. In neural networks, it is not known what sampling density is needed for traversing the parameter space. We use an adaptive procedure that inserts more pivots where needed:\nAutoNEB The Automated Nudged Elastic Band (AutoNEB, Algorithm 2) wraps the above NEB algorithm (Kolsbjerg et al., 2016). It runs NEB only for a small number of iterations T at a time, initially with a small number of pivots N . It is then checked if the current pivots accurately sample the path. If sampling is not dense enough, new pivots are added at locations where it is estimated that the path requires more accuracy, see Appendix A. This procedure is repeated several times."
  }, {
    "heading": "3.2. Local minimum energy paths",
    "text": "AutoNEB is not guaranteed to find the true MEP. Instead, it can get stuck in local minimum energy paths (local MEPs) with spuriously high saddle point losses. The good news is\nAlgorithm 2 AutoNEB Input: Minima to connect θ1, θ2. Initialise N pivots equally spaced on line segment (θ1, θ2). for t′ = 1, . . . , T ′ do\nOptimise path using NEB (see Algorithm 1). Evaluate loss along NEB. Insert pivots where residuum is large.\nend for return path after final iteration.\nthat the graph of minima and local MEPs has an ultrametric property: Suppose some local MEPs from a minimum A to B and from B to C are known. We call them pAB and pBC . The respective saddle point energies give an upper bound for the true saddle point energies (marked with an asterisk):\nL∗AB ≤ LAB = max θ∈pAB L(θ) L∗BC ≤ LBC = max θ∈pBC L(θ)\nThe concatenation of the two paths yields an upper bound for the true saddle point energy between A and C (ultrametric triangle inequality):\nL∗AC ≤ max{LAB , LBC}\nProof. Concatenating the paths pAB and pBC gives a new path pAC connecting A to C. The saddle point is located at the maximum loss along a path and hence the saddle point energy of pAC is LAC = max{LAB , LBC}.\nThis has three consequences:\n1. As soon as the minima and computed local MEPs form one connected graph, upper bounds for all saddle energies are available. We can hence very quickly get upper bounds for all pairs of minima by connecting one minimum to all others.\n2. When AutoNEB finds a bad local MEP, this can be addressed by computing paths between other pairs of minima. As soon as a lower path is found by concatenating other paths, the bad local MEP can be removed. This means that the bad local paths can easily be corrected for.\n3. When we evaluate the saddle point energies of a set of computed local MEPs, we can ignore paths with higher energy than the concatenation of paths with a lower maximal energy. These lowest local MEPs form a minimum spanning tree in the available graph (Gower & Ross, 1969). A\nMinimum Spanning Tree (MST) can be found efficiently, e.g. using Kruskal’s algorithm.\nWe resort to a heuristic (Figure 3, Algorithm 3) to systematically sample edge costs from a latent graph to find or approximate its MST. Since running AutoNEB is computationally expensive (comparable to training the corresponding network once), we stop the iteration when the lightened spanning tree found so far contains only similar saddle point energies."
  }, {
    "heading": "4. Experiments",
    "text": "We connect minima of different CNNs, ResNets (He et al., 2016) and DenseNets (Huang et al., 2017) on the image classification tasks CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009) using AutoNEB. Per architecture, we consider ten minima.1\nThe minima are constructed from multiple random initialisations and are truly distinct: On the test data, the set of misclassified images differs between the minima. More precisely, on the ResNet and DenseNet architectures, we\n1Source code is available at https://github.com/ fdraxler/PyTorch-AutoNEB.\nAlgorithm 3 Energy Landscape Exploration Input: set of minima θi. Connect θ1 to all θi, i 6= 1, yielding a spanning tree. repeat\nRemove edge po with highest loss from spanning tree. From each resulting tree, try to select one minimum,\nso that no local MEP is known for the pair. if search failed then\nRe-insert po and ignore it when searching for the highest edge in the future.\nelse Compute new path pn using AutoNEB. if Lpn < Lpo then\nAdd pn to the tree, making tree “lighter”. else\nRe-insert po to the tree (no better path was found). end if\nend if until one local MEP is known for each pair of minima\nor computational budget is exceeded. return saddle points in minimum spanning tree.\nobserve a maximum 70% overlap of the samples that are misclassified at two minima, proving their distinctiveness.\nWe report the average cross-entropy loss and misclassification rates over the full training and test data for the minima found. For the final evaluation, we reduce the saddle points to the minimum spanning tree with the saddle training loss as weight."
  }, {
    "heading": "4.1. AutoNEB schedule",
    "text": "The set-up is identical for all network architectures, except for the batch sizes which we note in each case.\nThe minimum pairs to connect are ordered by Algorithm 3. For each minimum pair, AutoNEB (see Algorithm 2) is run for a total of 14 cycles of NEB. The loss is evaluated for each pivot on a random batch.\nAfter each cycle, new pivots are inserted at positions where the loss exceeds the energy estimated by linear interpolation between pivots by at least 20% compared to the total energy difference along the path. Comparing to the total loss difference prioritises big errors which is beneficial as each additional pivot implies one more loss evaluations per iteration. The energy is evaluated on nine points between each pair of neighbouring pivots.\nAs optimiser, we use SGD with momentum 0.9 and l2regularisation with λ = 0.0001.\nThe NEB cycles are configured with a learning rate decay:\n1. Four cycles of 1000 steps each with learning rate 0.1.\n2. Two cycles with 2000 steps and learning rate 0.1. The number of steps was increased as it did not prove necessary inserting new pivots after 1000 steps.\n3. Four cycles of 1000 steps with learning rate 0.01. The loss drops significantly in this phase.\n4. No big improvement was seen in the last four cycles of 1000 steps each with a learning rate of 0.001.\nFigure 4 shows typical snapshots of the loss-along-path between the above cycles."
  }, {
    "heading": "4.2. Architectures",
    "text": "We consider a wide range of architectures, from shallow CNNs to recent deep networks with skip connections.\nBasic CNN We analyse CNNs without skip connections with a variety of depths and widths on both CIFAR10 and CIFAR100. We name them “CNN-W×D” where W corresponds to the width of each layer (number of channels) and D to the number of convolutional layers. Each convolution is 5× 5, a max pooling layer of 2 is attached to each convolution, and a single hidden fully connected layer of width 256 and batch normalisation (Ioffe & Szegedy, 2015) are used. We consider the one-layer CNN-12×1, CNN-24×1, CNN-36×1, CNN-48×1 and CNN-96×1, and the multi-layer CNN-48×2 and CNN-48×3.\nResNet We train ResNets on both CIFAR10 and CIFAR100 (ResNet-20, -32, -44 and -56) following the training procedure in (He et al., 2016). For ResNet-20 and ResNet-32, the best local MEPs were found using a batch size of 512 training samples. For ResNet-44 and ResNet-56, this number was decreased to 256.\nDenseNet We train a DenseNet-40-12 and a DenseNet100-12-BC on both CIFAR10 and CIFAR100 following the\ntraining procedure in (Huang et al., 2017). The AutoNEB batch size was set to 256."
  }, {
    "heading": "4.3. Saddle point losses",
    "text": "The saddle point losses for both training and test sets found by AutoNEB are shown in Figure 5, and listed in detail in Table B.1 in Appendix B. They are small for the shallow networks and almost negligible for the deep residual networks.\nCompare the saddle point loss to the loss at the minima on the training and on the test set. For the shallow CNNs on the one hand, the saddle loss is found quite close to the test loss. On the other hand, the saddle loss of the ResNets and DenseNets lies very close to the training loss.\nFurther, we measure how late during training the learning curve crosses the saddle loss, as visualised in Figure 6. The learning curve falls below the saddle point energy only after the first learning rate decay and an additional significant drop of the loss for all architectures. For the wider CNNs on CIFAR10 and the majority of ResNets and DenseNets, the losses meet even after the second decay, i.e. in the final phase of learning.\nWe observe the following trend: The deeper and wider an architecture, the lower are the saddles between minima until they essentially vanish for current-day deep architectures. The more complex dataset CIFAR100 raises the barriers.\nAt the same time, the test accuracy is preserved: The classification error only increases slightly by maximally 0.5% (2.2%) for all deep architectures on CIFAR10 (CIFAR100) compared to the minima.\nWe conclude that the saddle points have surprisingly low loss with respect to the metrics above. In other words, there are essentially no loss barriers in current-day deep architectures."
  }, {
    "heading": "4.4. Properties of obtained local MEPs",
    "text": "The local MEPs between the minima not only have very low loss, they also follow simple trajectories. Figure 7 shows some coordinates of two local MEPs in a parallel coordinate plot. We find that each coordinate has a smooth path. The largest deviations occur near the saddle point of the path. The paths are between 50% to 2.5 times longer than the direct connection between the minima."
  }, {
    "heading": "5. Discussion",
    "text": "We have pointed out an intriguing property of the loss surface of current-day deep networks, by upper-bounding the saddle points between the parameter sets that result from stochastic gradient descent, a.k.a. “minima”. These empiri-\ncal upper bounds are astonishingly close to the loss at the minima themselves. The experiments on the CNNs suggest that the disappearance of barriers emerges as the networks get wider and especially deeper. At this point, we cannot give a formal characterization of the regime in which this finding holds. A formal proof is also complicated by the fact that the loss surface is a function not only of the parameters and the architecture, but also of the training set; and the distribution of real-world structured data such as images or sentences does not lend itself to a compact mathematical representation. That said, we want to make two related arguments that may help explain why we observe no substantial barrier between minima."
  }, {
    "heading": "5.1. Resilience",
    "text": "State of the art neural networks have dozens or hundreds of neurons / channels per layer, and skip connections between non-adjacent layers. Assume that by training, a parameter set with low loss has been identified. Now if we perturb a single parameter, say by adding a small constant, but leave the others free to adapt to this change to still minimise the loss, it may be argued that by adjusting somewhat, the myriad other parameters can “make up” for the change imposed on only one of them. After this relaxation, the procedure and argument can be repeated, though possibly with the perturbation of a different parameter.\nThis type of resilience is exploited and encouraged by procedures such as Dropout (Srivastava et al., 2014) or ensembling (Hansen & Salamon, 1990). It is also the reason why neural networks can be greatly condensed before a substantial increase in loss occurs (Liu et al., 2017)."
  }, {
    "heading": "5.2. Redundancy",
    "text": "Ali ce\nBo b\nMinimum A Ali\nce Bob\nCh arl\nie Ali\nce Bob\nMinimum B\nAli ce\nBo b\nLowest saddle has 25% error.\nExtra neuron “unlocks“ flat path.\nFigure 8. Network capacity for XOR dataset: The continuous transition from one minimum (left) to another minimum (right) is not possible without misclassifying at least one instance (upper middle). (Lower middle) Adding one helper neuron makes the transition possible while always predicting the right class for all data points, i.e. by turning off the outgoing weight of Bob.\nConsider the textbook example of a two-layer perceptron that can fit the XOR problem. The two neurons traditionally used in the first hidden layer – let’s call them Alice and Bob – are shown in Figure 8 on the left. We can obtain an equivalent network by exchanging Alice and Bob (and permuting the weights of the neuron in the second hidden layer, not shown). This network, also corresponding to a minimum of the loss surface, is shown in Figure 8 on the right. Now, any path between these two minima will entail parameter sets such as the one in the upper centre of Figure 8 that incur high loss.\nIf, on the other hand, we introduce an auxiliary neuron, Charlie, we can play a small choreography: Enter Charlie. Charlie stands in for Bob. Bob transitions to Alice’s role via Figure 8, lower centre. Alice takes over from Charlie. Exit Charlie. If the neuron in the second hidden layer adjusts its weights so as to disregard the output from the neuron-intransition, the entire network incurs no higher loss than at the two original minima.\nWe have constructed a perfect minimum energy path through increasing the width. Similarly, it is possible to construct a zero-loss path by adding a second two-neuron layer to the network, that is by increasing the depth of the network."
  }, {
    "heading": "6. Conclusion",
    "text": "We find that the loss surface of deep neural networks contains paths with constantly low loss. The paths connect the minima so that they form one single connected component in the loss landscape. The barriers are especially low with increasing depth and width.\nWe put forth two closely related explanations in the above. Both hold only if the network has some extra capacity, or degrees of freedom, to spare. Empirically, this seems to be the case for modern-day architectures applied to standard problems.\nThis has the profound implication that low Hessian eigenvalues exist apart from the eigenvectors with analytically zero eigenvalues due to scaling.\nWe introduce AutoNEB for the characterisation of currentday architectures for the first time. The method opens the door to further empirical research on the energy landscape of neural networks. When the hyperparameters of AutoNEB are further refined, we expect to find even lower paths up to the level where the true saddle points are recovered. It is then interesting to see if certain minima have a higher barrier between them than others. This makes it possible to recursively form clusters of minima, i.e. using single-linkage clustering. In the traditional energy landscape literature, this kind of clustering is summarised in disconnectivity graphs (Wales et al., 1998) which can help visualise very high-dimensional surfaces.\nOn the practical side, we envisage using the resulting paths as a large ensemble of neural networks (Garipov et al., 2018), especially given that we observe marginally lower test loss along the path.\nMore importantly, we hope these observations will stimulate new theoretical work to better understand the nature of the loss surface, and why local optimisation on such surfaces results in networks that generalize so well."
  }, {
    "heading": "Acknowledgements",
    "text": "FAH gratefully acknowledges support by DFG under grant no. HA 4364/9-1."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"],
    "venue": "In ICLR,",
    "year": 2015
  }, {
    "title": "Energy landscapes for a machine learning application to series data",
    "authors": ["A.J. Ballard", "J.D. Stevenson", "R. Das", "D.J. Wales"],
    "venue": "J. Chem. Phys.,",
    "year": 2016
  }, {
    "title": "Energy landscapes for machine learning",
    "authors": ["A.J. Ballard", "R. Das", "S. Martiniani", "D. Mehta", "L. Sagun", "J.D. Stevenson", "D.J. Wales"],
    "venue": "Physical Chemistry Chemical Physics (Incorporating Faraday Transactions),",
    "year": 2017
  }, {
    "title": "The loss surface of multilayer networks",
    "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"],
    "venue": "CoRR, abs/1412.0233,",
    "year": 2014
  }, {
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "authors": ["Y. Dauphin", "R. Pascanu", "Ç. Gülçehre", "K. Cho", "S. Ganguli", "Y. Bengio"],
    "venue": "CoRR, abs/1406.2572,",
    "year": 2014
  }, {
    "title": "Topology and Geometry of Half-Rectified Network Optimization. ArXiv e-prints, November 2016",
    "authors": ["C.D. Freeman", "J. Bruna"],
    "year": 2016
  }, {
    "title": "Minimum spanning trees and single linkage cluster analysis",
    "authors": ["J.C. Gower", "G.J.S. Ross"],
    "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
    "year": 1969
  }, {
    "title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing",
    "authors": ["A. Graves", "Mohamed", "A.-r", "G. Hinton"],
    "venue": "(icassp), 2013 ieee international conference on,",
    "year": 2013
  }, {
    "title": "Neural network ensembles",
    "authors": ["L.K. Hansen", "P. Salamon"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 1990
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2016
  }, {
    "title": "Improved tangent estimate in the nudged elastic band method for finding minimum energy paths and saddle points",
    "authors": ["G. Henkelman", "H. Jónsson"],
    "venue": "The Journal of chemical physics,",
    "year": 2000
  }, {
    "title": "Densely connected convolutional networks",
    "authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2017
  }, {
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "ArXiv e-prints,",
    "year": 2015
  }, {
    "title": "Nudged elastic band method for finding minimum energy paths of transitions. In Classical and quantum dynamics in condensed phase simulations, pp. 385–404",
    "authors": ["H. Jónsson", "G. Mills", "K.W. Jacobsen"],
    "venue": "World Scientific,",
    "year": 1998
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
    "authors": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"],
    "venue": "arXiv preprint arXiv:1609.04836,",
    "year": 2016
  }, {
    "title": "An automated nudged elastic band method",
    "authors": ["E.L. Kolsbjerg", "M.N. Groves", "B. Hammer"],
    "venue": "The Journal of chemical physics,",
    "year": 2016
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "year": 2009
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Visualizing the loss landscape of neural nets",
    "authors": ["H. Li", "Z. Xu", "G. Taylor", "T. Goldstein"],
    "venue": "arXiv preprint arXiv:1712.09913,",
    "year": 2017
  }, {
    "title": "Learning efficient convolutional networks through network slimming",
    "authors": ["Z. Liu", "J. Li", "Z. Shen", "G. Huang", "S. Yan", "C. Zhang"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "On the number of linear regions of deep neural networks. In Advances in neural information processing",
    "authors": ["G. Montúfar", "R. Pascanu", "K. Cho", "Y. Bengio"],
    "year": 2014
  }, {
    "title": "The loss surface of deep and wide neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Optimization methods for finding minimum energy paths",
    "authors": ["D. Sheppard", "R. Terrell", "G. Henkelman"],
    "venue": "The Journal of Chemical Physics,",
    "year": 2008
  }, {
    "title": "No bad local minima: Data independent training error guarantees for multilayer neural networks. ArXiv e-prints, May 2016",
    "authors": ["D. Soudry", "Y. Carmon"],
    "venue": "URL http: //arxiv.org/abs/1605.08361",
    "year": 2016
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }],
  "id": "SP:2c90d366126a3ccd3c43e47891730650003059da",
  "authors": [{
    "name": "Felix Draxler",
    "affiliations": []
  }, {
    "name": "Kambis Veschgini",
    "affiliations": []
  }, {
    "name": "Manfred Salmhofer",
    "affiliations": []
  }, {
    "name": "Fred A. Hamprecht",
    "affiliations": []
  }],
  "abstractText": "Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.",
  "title": "Essentially No Barriers in Neural Network Energy Landscape"
}