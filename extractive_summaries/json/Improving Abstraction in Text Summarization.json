{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1808–1817 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1808"
  }, {
    "heading": "1 Introduction",
    "text": "Text summarization concerns the task of compressing a long sequence of text into a more concise form. The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation. We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document.\n∗ Work performed while at Salesforce Research.\nA high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct. Because it is difficult to gauge the correctness of the summary, evaluation metrics for summarization models use word overlap with the ground-truth summary in the form of ROUGE (Lin, 2004) scores. However, word overlap metrics do not capture the abstractive nature of high quality human-written summaries: the use of paraphrases with words that do not necessarily appear in the source document.\nThe state-of-the-art abstractive text summarization models have high word overlap performance, however they tend to copy long passages of the source document directly into the summary, thereby producing summaries that are not abstractive (See et al., 2017).\nWe propose two general extensions to summarization models that improve the level of abstraction of the summary while preserving word overlap with the ground-truth summary. Our first contribution decouples the extraction and generation responsibilities of the decoder by factoring it into a contextual network and a language model. The contextual network has the sole responsibility of extracting and compacting the source document whereas the language model is responsible for the generation of concise paraphrases. Our second contribution is a mixed objective that jointly optimizes the n-gram overlap with the ground-truth summary while encouraging abstraction. This is done by combining maximum likelihood estimation with policy gradient. We reward the policy with the ROUGE metric, which measures word overlap with the ground-truth summary, as well as a novel abstraction reward that encourages the generation of words not in the source document.\nWe demonstrate the effectiveness of our contributions on a encoder-decoder summarization\nArticle\n(cnn) to allay possible concerns, boston prosecutors released video friday of the shooting of a police officer last month that resulted in the killing of the gunman. the officer wounded, john moynihan, is white. angelo west, the gunman shot to death by officers, was black. after the shooting, community leaders in the predominantly african-american neighborhood of (...)\nmodel. Our model obtains state-of-the-art ROUGE-L scores, and ROUGE-1 and ROUGE-2 performance comparable to state-of-the-art methods on the CNN/DailyMail dataset. Moreover, we significantly outperform all previous abstractive approaches in our abstraction metrics. Table 1 shows a comparison of summaries generated by our model and previous abstractive models, showing less copying and more abstraction in our model."
  }, {
    "heading": "2 Model",
    "text": ""
  }, {
    "heading": "2.1 Base Model and Training Objective",
    "text": "The base model follows the encoder-decoder architecture with temporal attention and intraattention proposed by Paulus et al. (2017). Let E ∈ Rn×demb denote the matrix of demb dimensional word embeddings of the n words in the source document. The encoding of the source document henc is computed via a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) whose output has dimension dhid.\nhenc = BiLSTM (E) ∈ Rn×dhid (1)\nThe decoder uses temporal attention over the encoded sequence that penalizes input tokens that previously had high attention scores. Let hdect denote the decoder state at time t. The temporal at-\ntention context at time t, ctmpt , is computed as\nstmpti = ( hdect )ᵀ W tmphenci ∈ R (2) qtmpti = exp(stmpti )∑t−1 j=1 exp(s tmp ji ) ∈ R (3) αtmpti = qtmpti∑n j=1 q tmp tj ∈ R (4)\nctmpt = n∑ i=1 αtmpti h enc i ∈ Rd hid (5)\nwhere we set qtmpti to exp(s tmp ti ) for t = 1.\nThe decoder also attends to its previous states via intra-attention over the decoded sequence. The intra-attention context at time t, cintt , is computed as\nsintti = ( hdect )ᵀ W inthdeci ∈ R (6) cintt = t−1∑ i=1 ( sintti∑n j=1 s int tj ) hdeci ∈ Rd hid (7)\nThe decoder generates tokens by interpolating between selecting words from the source document via a pointer network as well as selecting words from a fixed output vocabulary. Let zt denote the ground-truth label as to whether the tth\noutput word should be generated by the selecting from the output vocabulary as opposed to from the source document. We compute p(zt), the probability that the decoder generates from the output vocabulary, as\nrt = [h dec t ; c tmp t ; c int t ] ∈ R3d hid (8)\np(zt) = sigmoid(W zrt + b z) ∈ R (9)\nThe probability of selecting the word yt from a fixed vocabulary at time step t is defined as\npgen(yt) = softmax (W genrt + b gen) (10)\nWe set pcp(yt), the probability of copying the word yt from the source document, to the temporal attention distribution αtmpt . The joint probability of using the generator and generating the word yt at time step t, p(zt, yt), is then\np(zt, yt) = p(yt | zt)p(zt) (11)\nthe likelihood of which is\nlog p(zt, yt) = log p(yt | zt) + log p(zt) = zt log p\ngen(yt) + (1− zt) log pcp(yt) + zt log p(zt) + (1− zt) log (1− p (zt))\n= zt (log p gen(yt) + log p(zt))\n+ (1− zt) (log pcp(yt) + log (1− p (zt))) (12)\nThe objective function combines maximum likelihood estimation with policy learning. Let m denote the length of the ground-truth summary, The maximum likelihood loss Lml is computed as\nLml = − m∑ t=1 log p(zt, yt) (13)\nPolicy learning uses ROUGE-L as its reward function and a self-critical baseline using the greedy decoding policy (Rennie et al., 2016). Let ysam denote the summary obtained by sampling from the current policy p, ygre and zgre the summary and generator choice obtained by greedily choosing from p(zt, yt), R(y) the ROUGE-L score of the summary y, and Θ the model parameters. The policy learning loss is\nR̂ = R (ysam)−R (ygre) (14) Lpg = −E zsam ∼p(z),\nysam ∼p(y|z) [R̂] (15)\nwhere we use greedy predictions by the model according to eq. (13) as a baseline for variance reduction. The policy gradient, as per Schulman et al. (2015), is\n∇ΘLpg ≈ −R̂ m∑ t=1 ∇Θ log p (zsamt , ysamt ) (16)\nThe final loss is a mixture between the maximum likelihood loss and the policy learning loss, weighted by a hyperparameter γ.\nL = (1− γ)Lml + γLpg (17)"
  }, {
    "heading": "2.2 Language Model Fusion",
    "text": "The decoder is an essential component of the base model. Given the source document and the previously generated summary tokens, the decoder both extracts relevant parts of the source document through the pointer network as well as composes paraphrases from the fixed vocabulary. We decouple these two responsibilities by augmenting the decoder with an external language model. The language model assumes responsibility of generating from the fixed vocabulary, and allows the decoder to focus on attention and extraction. This decomposition has the added benefit of easily incorporating external knowledge about fluency or domain specific styles via pre-training the language model on a large scale text corpora.\nThe architecture of our language model is based on Merity et al. (2018). We use a 3-layer unidirectional LSTM with weight-dropped LSTM units.\nLet et denote the embedding of the word generated during time step t. The hidden state of the language model at the l-th layer is\nhlml,t = LSTM lm 3 ( et−1, h lm l,t−1 ) (18)\nAt each time step t, we combine the hidden state of the last language model LSTM layer, hlm3,t, with rt defined in eq. (8) in a fashion similar to Sriram et al. (2017). Let denote element-wise multiplication. We use a gating function whose output\ngt filters the content of the language model hidden state.\nft = sigmoid ( W lm[rt;h lm 3,t] + b lm ) (19)\ngt = W fuse([rt; gt hlm3,t]) + bfuse (20) hfuset = ReLU (gt) (21)\nWe then replace the output distribution of the language model pgen (yt) in eq. 10 with\npgen (yt) = softmax ( W genhfuset + b gen ) (22)"
  }, {
    "heading": "2.3 Abstractive Reward",
    "text": "In order to produce an abstractive summary, the model cannot exclusively copy from the source document. In particular, the model needs to parse large chunks of the source document and create concise summaries using phrases not in the source document. To encourage this behavior, we propose a novelty metric that promotes the generation of novel words.\nWe define a novel phrase in the summary as one that is not in the source document. Let ng (x, n) denote the function that computes the set of unique n-grams in a document x, xgen the generated summary, xsrc the source document, and ‖s‖ the number of words in s. The unnormalized novelty metric N is defined as the fraction of unique n-grams in the summary that are novel.\nN (xgen, n) = ‖ng (xgen, n)− ng (xsrc, n)‖\n‖ng (xgen, n)‖ (23)\nTo prevent the model for receiving high novelty rewards by outputting very short summaries, we normalize the metric by the length ratio of the generated and ground-truth summaries. Let xgt denote the ground-truth summary. We define the novelty metric as\nRnov (xgen, n) = N (xgen, n) ‖xgen‖ ‖xgt‖\n(24)\nWe incorporate the novelty metric as a reward into the policy gradient objective in eq. (15), alongside the original ROUGE-L metric. In doing so, we encourage the model to generate summaries that both overlap with human written ground-truth summaries as well as incorporate novel words not in the source document:\nR (y) = λrouRrou (ysam) + λnovRnov (ysam) (25)\nwhere λrou and λnov are hyperparameters that control the weighting of each reward."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Datasets",
    "text": "We train our model on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016). Previous works on abstractive summarization either use an anonymized version of this dataset or the original article and summary texts. Due to these different formats, it is difficult to compare the overall ROUGE scores and performance between each version. In order to compare against previous results, we train and evaluate on both versions of this dataset. For the anonymized version, we follow the pre-processing steps described in Nallapati et al. (2016), and the pre-processing steps of See et al. (2017) for the the full-text version.\nWe use named entities and the source document to supervise the model regarding when to use the pointer and when to use the generator (e.g. zt in eq. (13). Namely, during training, we teach the model to point from the source document if the word in the ground-truth summary is a named entity, an out-of-vocabulary word, or a numerical value that is in the source document. We obtain the list of named entities from Hermann et al. (2015)."
  }, {
    "heading": "3.2 Language Models",
    "text": "For each dataset version, we train a language model consisting of a 400-dimensional word embedding layer and a 3-layer LSTM with each layer having a hidden size of 800 dimensions, except the last input layer which has an output size of 400. The final decoding layer shares weights with the embedding layer (Inan et al., 2017; Press and Wolf, 2016). We also use DropConnect (Wan et al., 2013) in the hidden-to-hidden connections, as well as the non-monotonically triggered asynchronous gradient descent optimizer from Merity et al. (2018).\nWe train this language model on the CNN/Daily Mail ground-truth summaries only, following the same training, validation, and test splits as our main experiments."
  }, {
    "heading": "3.3 Training details",
    "text": "The two LSTMs of our bidirectional encoder are 200-dimensional, and out decoder LSTM is 400- dimensional. We restrict the input vocabulary for the embedding matrix to 150,000 tokens, and the output decoding layer to 50,000 tokens. We limit the size of input articles to the first 400 tokens, and the summaries to 100 tokens. We use scheduled sampling (Bengio et al., 2015) with a probability of 0.25 when calculating the maximum-likelihood training loss. We also set n = 3 when computing our novelty reward Rnov(xgen, n). For our final training loss using reinforcement learning, we set γ = 0.9984, λrou = 0.9, and λnov = 0.1. Finally, we use the trigram repetition avoidance heuristic defined by Paulus et al. (2017) during beam search decoding to ensure that the model does not output twice the same trigram in a given summary, reducing the amount of repetitions."
  }, {
    "heading": "3.4 Novelty baseline",
    "text": "We also create a novelty baseline by taking the outputs of our base model, without RL training and without the language model, and inserting random words not present in the article after each summary token with a probability r = 0.0005. This baseline will intuitively have a higher percentage of novel n-grams than our base model outputs while being very similar to these original outputs, hence keeping the ROUGE score difference relatively small."
  }, {
    "heading": "4 Results",
    "text": ""
  }, {
    "heading": "4.1 Quantitative analysis",
    "text": "We obtain a validation and test perplexity of 65.80 and 66.61 respectively on the anonymized dataset, and 81.13 and 82.98 on the full-text dataset with the language models described in Section 3.2.\nThe ROUGE scores and novelty scores of our final summarization model on both versions of the CNN/Daily Mail dataset are shown in Table 2. We report the ROUGE-1, ROUGE-2, and ROUGEL F-scores as well as the percentage of novel ngrams, marked NN-n, in the generated summaries, with n from 1 to 4. Results are omitted in cases where they have not been made available by previous authors. We also include the novel n-gram scores for the ground-truth summaries as a comparison to indicate the level of abstraction of human written summaries.\nEven though our model outputs significantly fewer novel n-grams than human written summaries, it has a much higher percentage of novel n-grams than all the previous abstractive approaches. It also achieves state-of-the-art ROUGE-L performance on both dataset versions, and obtains ROUGE-1 and ROUGE-2 scores close to state-of-the-art results."
  }, {
    "heading": "4.2 Ablation study",
    "text": "In order to evaluate the relative impact of each of our individual contributions, we run ablation studies comparing our model ablations against each other and against the novelty baseline. The results of these different models on the validation set of the anonymized CNN/Daily Mail dataset are shown in Table 3. Results show that our base model trained with the maximum-likelihood loss only and using the language model in the decoder (ML, with LM) has higher ROUGE scores, novel unigrams, and novel bigrams scores than our base model without the language model (ML). ML with LM also beats the novelty baseline for these metrics. When training these models with reinforcement learning using the ROUGE reward (ML+RL ROUGE and ML+RL ROUGE with LM), the model with language model obtains higher ROUGE-1 and ROUGE-2 scores. However, it also loses its novel unigrams and bigrams advantage. Finally, using the mixed ROUGE and novelty rewards (ML+RL ROUGE+Novel) produces both higher ROUGE scores and more novel unigrams with the language model than without\nit. This indicates that the combination of the language model in the decoder and the novelty reward during training makes our model produce more novel unigrams while maintaining high ROUGE scores."
  }, {
    "heading": "4.3 ROUGE vs novelty trade-off",
    "text": "In order to understand the correlation between ROUGE and novel n-gram scores across different architectures, and to find the model type that gives the best trade-off between each of these metrics, we plot the ROUGE-1 and novel unigram scores for the five best iterations of each model type on the anonymized dataset, as well as the ROUGE-2 and novel bigram scores on a separate plot. We also include the novelty baseline described in Section 4.2 for values of r between 0.005 and 0.035. For each model type, we indicate the Pareto frontier by a line plot (Ben-Tal, 1980), illustrating which models of a given type give the best combination of ROUGE and novelty scores. These plots are shown in Figure 2.\nThese plots show that there exist an inverse correlation between ROUGE and novelty scores in all model types, illustrating the challenge of choosing a model that performs well in both. Given that, our final model (ML+RL ROUGE+Novel, with LM) provides the best trade-off of ROUGE-1 scores compared to novel unigrams, indicated by the higher Pareto frontier in the first plot. Similarly, our final model gives one of the best trade-offs of ROUGE-2 scores to novel bigrams, even though the same model without LM produces more novel\nbigrams with a lower ROUGE-2 score."
  }, {
    "heading": "4.4 Qualitative evaluation",
    "text": "In order to ensure the quality of our model outputs, we ask 5 human evaluators to rate 100 randomly selected full-text test summaries, giving them two scores from 1 to 10 respectively for readability and relevance given the original article. We also include the full-text test outputs from See et al. (2017) and Liu et al. (2018) for comparison. Evaluators are shown different summaries corresponding to the same article side by side without being told which models have generated them. The mean score and confidence interval at 95% for each model and each evaluation criterion are reported in Table 4. These results show that our model matches the relevance score of See et al. (2017) and Liu et al. (2018), but is slightly inferior to them in terms of readability."
  }, {
    "heading": "5 Related work",
    "text": "Text summarization. Existing summarization approaches are usually either extractive or abstrac-\ntive. In extractive summarization, the model selects passages from the input document and combines them to form a shorter summary, sometimes with a post-processing step to ensure final coherence of the output (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015; Nallapati et al., 2017). While extractive models are usually robust and produce coherent summaries, they cannot create concise summaries that paraphrase the source document using new phrases.\nAbstractive summarization allows the model to paraphrase the source document and create concise summaries with phrases not in the source document. The state-of-the-art abstractive summarization models are based on sequence-tosequence models with attention (Bahdanau et al., 2015). Extensions to this model include a selfattention mechanism (Paulus et al., 2017) and an article coverage vector (See et al., 2017) to prevent repeated phrases in the output summary. Different training procedures have also been used improve the ROUGE score (Paulus et al., 2017) or textual\nentailment (Pasunuru and Bansal, 2018) with reinforcement learning; as well as generative adversarial networks to generate more natural summaries (Liu et al., 2018).\nSeveral datasets have been used to train and evaluate summarization models. The Gigaword (Graff and Cieri, 2003) and some DUC datasets (Over et al., 2007) have been used for headline generation models (Rush et al., 2015; Nallapati et al., 2016), where the generated summary is shorter than 75 characters. However, generating longer summaries is a more challenging task, especially for abstractive models. Nallapati et al. (2016) have proposed using the CNN/Daily Mail dataset (Hermann et al., 2015) to train models for generating longer, multi-sentence summaries up to 100 words. The New York Times dataset (Sandhaus, 2008) has also been used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017).\nTraining strategies for sequential models. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, the model is given the previous ground-truth output and predicts the current output. The sequence objective is the accumulation of cross entropy losses from each time step.\nDespite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016). Goyal et al. (2016) propose one way to reduce exposure bias by explicitly forcing the hidden representations of the model to be similar during training and inference. Bengio et al. (2015) and Wiseman and Rush (2016) propose an alternate method that exposes the network to the test dynamics during training. Reinforcement learning methods (Sutton and Barto, 1998), such as policy learning (Sutton\net al., 1999), mitigate the mismatch between the optimization objective and the evaluation metrics by directly optimizing evaluation metrics. This approach has led to consistent improvements in domains such as image captioning (Zhang et al., 2017) and abstractive text summarization (Paulus et al., 2017).\nA recent approach to training sequential models utilizes generative adversarial networks to improving the human perceived quality of generated outputs (Fedus et al., 2018; Guimaraes et al., 2017; Liu et al., 2018). Such models use an additional discriminator network that distinguishes between natural and generated output to guide the generative model towards outputs akin to human-written text."
  }, {
    "heading": "6 Conclusions",
    "text": "We introduced a new abstractive summarization model which uses an external language model in the decoder, as well as a new reinforcement learning reward to encourage summary abstraction. Experiments on the CNN/Daily Mail dataset show that our model generates summaries that are much more abstractive that previous approaches, while maintaining high ROUGE scores close to or above the state of the art. Future work could be done on closing the gap to match human levels of abstraction, which is still very far ahead from our model in terms of novel n-grams. Including mechanisms to promote paraphrase generation in the summary generator could be an interesting direction."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Characterization of pareto and lexicographic optimal solutions",
    "authors": ["Aharon Ben-Tal."],
    "venue": "Multiple Crite-",
    "year": 1980
  }, {
    "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
    "authors": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Heads: Headline generation as sequence prediction using an abstract feature-rich space",
    "authors": ["Carlos A Colmenares", "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri."],
    "venue": "HLT-NAACL, pages 133–142.",
    "year": 2015
  }, {
    "title": "Hedge trimmer: A parse-and-trim approach to headline generation",
    "authors": ["Bonnie Dorr", "David Zajic", "Richard Schwartz."],
    "venue": "HLT-NAACL.",
    "year": 2003
  }, {
    "title": "Learning-based single-document summarization with compression and anaphoricity constraints",
    "authors": ["Greg Durrett", "Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Maskgan: Better text generation via filling in the",
    "authors": ["William Fedus", "Ian J. Goodfellow", "Andrew M. Dai."],
    "venue": "ICLR.",
    "year": 2018
  }, {
    "title": "Overcoming the lack of parallel data in sentence compression",
    "authors": ["Katja Filippova", "Yasemin Altun."],
    "venue": "Proceedings of EMNLP, pages 1481–1491. Citeseer.",
    "year": 2013
  }, {
    "title": "Professor forcing: A new algorithm for training recurrent networks",
    "authors": ["Anirudh Goyal", "Alex Lamb", "Ying Zhang", "Saizheng Zhang", "Aaron C. Courville", "Yoshua Bengio."],
    "venue": "NIPS.",
    "year": 2016
  }, {
    "title": "English gigaword, linguistic data consortium",
    "authors": ["David Graff", "C Cieri"],
    "year": 2003
  }, {
    "title": "Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models",
    "authors": ["Gabriel Lima Guimaraes", "Benjamin SanchezLengeling", "Pedro Luis Cunha Farias", "Alán Aspuru-Guzik."],
    "venue": "CoRR, abs/1705.10843.",
    "year": 2017
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101",
    "authors": ["Ferenc Huszar"],
    "year": 2015
  }, {
    "title": "Tying word vectors and word classifiers: A loss framework for language modeling",
    "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Proc. ACL workshop on Text Summarization Branches Out, page 10.",
    "year": 2004
  }, {
    "title": "Generative adversarial network for abstractive text summarization",
    "authors": ["Linqing Liu", "Yao Lu", "Min Yang", "Qiang Qu", "Jia Zhu", "Hongyan Li."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "Regularizing and optimizing lstm language models",
    "authors": ["Stephen Merity", "Nitish Shirish Keskar", "Richard Socher."],
    "venue": "ICLR.",
    "year": 2018
  }, {
    "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
    "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."],
    "venue": "AAAI.",
    "year": 2017
  }, {
    "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
    "authors": ["Ramesh Nallapati", "Bowen Zhou", "Çağlar Gülçehre", "Bing Xiang"],
    "venue": "Proceedings of SIGNLL Conference on Computational Natural Language Learning",
    "year": 2016
  }, {
    "title": "Automatic text summarization using a machine learning approach",
    "authors": ["Joel Larocca Neto", "Alex A Freitas", "Celso AA Kaestner."],
    "venue": "Brazilian Symposium on Artificial Intelligence, pages 205–215. Springer.",
    "year": 2002
  }, {
    "title": "Duc in context",
    "authors": ["Paul Over", "Hoa Dang", "Donna Harman."],
    "venue": "Inf. Process. Manage., 43(6):1506–1520.",
    "year": 2007
  }, {
    "title": "Multireward reinforced summarization with saliency and entailment",
    "authors": ["Ramakanth Pasunuru", "Mohit Bansal."],
    "venue": "CoRR, abs/1804.06451.",
    "year": 2018
  }, {
    "title": "A deep reinforced model for abstractive summarization",
    "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Using the output embedding to improve language models",
    "authors": ["Ofir Press", "Lior Wolf."],
    "venue": "arXiv preprint arXiv:1608.05859.",
    "year": 2016
  }, {
    "title": "Self-critical sequence training for image captioning",
    "authors": ["Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."],
    "venue": "CoRR, abs/1612.00563.",
    "year": 2016
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of EMNLP.",
    "year": 2015
  }, {
    "title": "The new york times annotated corpus",
    "authors": ["Evan Sandhaus."],
    "venue": "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
    "year": 2008
  }, {
    "title": "Gradient estimation using stochastic computation graphs",
    "authors": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Cold fusion: Training seq2seq models together with language models",
    "authors": ["Anuroop Sriram", "Heewoo Jun", "Sanjeev Satheesh", "Adam Coates."],
    "venue": "CoRR, abs/1708.06426.",
    "year": 2017
  }, {
    "title": "Reinforcement learning - an introduction",
    "authors": ["Richard S. Sutton", "Andrew G. Barto."],
    "venue": "Adaptive computation and machine learning. MIT Press.",
    "year": 1998
  }, {
    "title": "Policy gradient methods for reinforcement learning with function approximation",
    "authors": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour."],
    "venue": "NIPS.",
    "year": 1999
  }, {
    "title": "Regularization of neural networks using dropconnect",
    "authors": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann Le Cun", "Rob Fergus."],
    "venue": "ICML.",
    "year": 2013
  }, {
    "title": "Sequence-to-sequence learning as beam-search optimization",
    "authors": ["Sam Wiseman", "Alexander M. Rush."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Actor-critic sequence training for image captioning",
    "authors": ["Li Zhang", "Flood Sung", "Feng Liu", "Tao Xiang", "Shaogang Gong", "Yongxin Yang", "Timothy M. Hospedales."],
    "venue": "CoRR, abs/1706.09601.",
    "year": 2017
  }],
  "id": "SP:653d2a29f9f7e67327758b249eb95f6cb9d14902",
  "authors": [{
    "name": "Wojciech Kryściński",
    "affiliations": []
  }, {
    "name": "Romain Paulus",
    "affiliations": []
  }, {
    "name": "Caiming Xiong",
    "affiliations": []
  }, {
    "name": "Richard Socher",
    "affiliations": []
  }],
  "abstractText": "Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.ive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.",
  "title": "Improving Abstraction in Text Summarization"
}