{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Developing provably efficient algorithms for learning commonly used neural network architectures continues to be a core challenge in machine learning. The underlying difficulty arises from the highly non-convex nature of the optimization problems posed by neural networks. Obtaining provable guarantees for learning even very basic architectures remains open.\nIn this paper we consider a simple convolutional neural network with a single filter and overlapping patches fol-\n*Equal contribution 1Department of Computer Science, University of Texas at Austin 2Department of Computer Science, UCLA. Correspondence to: Surbhi Goel <surbhi@cs.utexas.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nlowed by average pooling (Figure 1). More formally, for an input image x, we consider k patches of size r indicated by selection matrices P1, . . . , Pk ∈ {0, 1}r×n where each matrix has exactly one 1 in each row and at most one 1 in each column. The neural network is computed as fw(x) = 1k ∑k i=1 σ(w\nTPix) where σ is the activation function and w ∈ Rr is the weight vector corresponding to the convolution filter. We focus on ReLU and leaky ReLU activation functions."
  }, {
    "heading": "1.1. Our Contributions",
    "text": "The main contribution of this paper is a simple, stochastic update algorithm Convotron (Algorithm 1) for provably learning the above convolutional architecture. The algorithm has the following properties:\n• Works for general classes of overlapping patches and requires mild distributional conditions.\n• Proper recovery of the unknown weight vector.\n• Stochastic in nature with a “gradient-like” update step.\n• Requires no special/random initialization scheme or tuning of the learning rate.\n• Tolerates noise and succeeds in the probabilistic concept model of learning.\n• Logarithmic convergence in 1/ , the error parameter, in the realizable setting.\nThis is the first efficient algorithm for learning general classes of overlapping patches (and the first algorithm for any class of patches that succeeds under mild distributional assumptions). Prior work has focused on analyzing SGD in the realizable/noiseless setting with the caveat of requiring either disjoint patches (Brutzkus & Globerson, 2017; Du et al., 2017b) with Gaussian inputs or technical conditions linking the underlying true parameters and the “closeness of patches” (Du et al., 2017a).\nIn contrast, our conditions depend only on the patch structure itself and can be efficiently verified. Commonly used patch structures in computer vision applications such as 1D/2D grids satisfy our conditions. Additionally, we require only that the underlying distribution on samples is symmetric and induces a covariance matrix on the patches with polynomially bounded condition number1. All prior work handles only continuous distributions. Another major difference from prior work is that we give guarantees using purely empirical updates. That is, we do not require an assumption that we have access to exact quantities such as the population gradient of the loss function.\nWe further show that in the commonly studied setting of Gaussian inputs and non-overlapping patches, updating with respect to a single non-overlapping patch is sufficient to guarantee convergence. This indicates that the Gaussian/nooverlap assumption is quite strong."
  }, {
    "heading": "1.2. Our Approach",
    "text": "Our approach is to exploit the monotonicity of the activation function instead of the strong convexity of the loss surface. We use ideas from isotonic regression and extend them in the context of convolutional networks. These ideas have been successful for learning generalized linear models (Kakade et al., 2011), improperly learning fully connected, depththree neural networks (Goel & Klivans, 2017b), and learning graphical models (Klivans & Meka, 2017)."
  }, {
    "heading": "1.3. Related Work",
    "text": "It is known that in the worst case, learning even simple neural networks is computationally intractable. For example, in the non-realizable (agnostic) setting, it is known that learning a single ReLU (even for bounded distributions and unit norm hidden weight vectors) with respect to square-loss is as hard as learning sparse parity with noise (Goel et al., 2016), a notoriously difficult problem from computational learning theory. For learning one hidden layer convolutional networks, Brutzkus and Globerson (Brutzkus & Glober-\n1Brutzkus and Globerson (Brutzkus & Globerson, 2017) proved that the problem, even with disjoint patches, is NP-hard in general, and so some distributional assumption is needed for efficient learning.\nson, 2017) proved that distribution-free recoverability of the unknown weight vector is NP-hard, even if we restrict to disjoint patch structures.\nAs such, a major open question is to discover the mildest assumptions that lead to polynomial-time learnability for simple neural networks. In this paper, we consider the very popular class of convolutional neural networks (for a summary of other recent approaches for learning more general architectures see (Goel & Klivans, 2017a)). For convolutional networks, all prior research has focused on analyzing conditions under which (Stochastic) Gradient Descent converges to the hidden weight vector in polynomial-time.\nAlong these lines, Brutzkus and Globerson (Brutzkus & Globerson, 2017) proved that with respect to the spherical Gaussian distribution and for disjoint (non-overlapping) patch structures, gradient descent recovers the weight vector in polynomial-time. Zhong et al. (Zhong et al., 2017a) showed that gradient descent combined with tensor methods can recover one hidden layer involving multiple weight vectors but still require a Gaussian distribution and nonoverlapping patches. Du et al. (Du et al., 2017b) proved that gradient descent recovers a hidden weight vector involved in a type of two-layer convolutional network under the assumption that the distribution is a spherical Gaussian, the patches are disjoint, and the learner has access to the true population gradient of the loss function.\nWe specifically highlight the work of Du, Lee, and Tian (Du et al., 2017a), who proved that gradient descent recovers a hidden weight vector in a one-layer convolutional network under certain technical conditions that are more general than the Gaussian/no-overlap patch scenario. Their conditions involve a certain “alignment” of the unknown patch structure, the hidden weight vector, and the (continuous) marginal distribution. However, it is unclear which concrete patch-structure/distributional combinations their framework captures. We also note that all of the above results assume there is no noise; i.e.,, they work in the realizable setting.\nOther related works analyzing gradient descent with respect to the Gaussian distribution (but for non-convolutional networks) include (Soltanolkotabi, 2017; Ge et al., 2017; Zhong et al., 2017b; Tian, 2016; Li & Yuan, 2017; Zhang et al., 2017).\nIn contrast, we consider an alternative to gradient descent, namely Convotron, that is based on isotonic regression. The exploration of alternative algorithms to gradient descent is a feature of our work, as it may lead to new algorithms for learning deeper networks."
  }, {
    "heading": "2. Preliminaries",
    "text": "|| · || corresponds to the l2 -norm for vectors and the spectral norm for matrices. The identity matrix is denoted by I . We denote the input-label distribution by D over input drawn from X and label drawn from Y . The marginal distribution on the input is denoted by DX and the corresponding probability density function is denoted by PX .\nIn this paper we consider a simple convolution neural network with one hidden layer and average pooling. Given input x ∈ Rn, the network computes k patches of size r where each patch’s location is indicated by matrices P1, . . . , Pk ∈ {0, 1}r×n. Each matrix Pi has exactly one 1 in each row and at most one 1 in every column. As before, the neural network is computed as follows:\nfw(x) = 1\nk k∑ i=1 σ ( wTPix ) where σ is the activation function and w ∈ Rr is the weight vector corresponding to the convolution filter.\nWe study the problem of learning the teacher network with true weight w∗ under the square loss from noisy labels, that is, we wish to find a w such that\nL(w) := Ex∼DX [ (fw(x)− fw∗(x))2 ] ≤ .\nAssumptions 1. We make the following assumptions:\n(a) Learning Model: Probabilistic Concept Model (Kearns & Schapire, 1990), that is, for all (x, y) ∼ D, y = fw∗(x)+ξ, for some unknownw∗ where ξ is noise with E[ξ|x] = 0 and E[ξ4|x] ≤ ρ for some ρ > 0. Note we do not require that the noise is independent of the instance.2\n(b) Distribution: The marginal distribution on the input space DX is a symmetric distribution about the origin, that is, for all x, PX (x) = PX (−x). (c) Patch Structure: The minimum eigenvalue of PΣ :=∑k i,j=1 PiΣP T j where Σ = Ex∼DX [xxT ] and the max-\nimum eigenvalue of P := ∑k i,j=1 PiP T j are polynomially bounded.\n(d) Activation Function: The activation function has the following form:\nσ(x) = { x if x ≥ 0 αx otherwise\nfor some constant α ∈ [0, 1]. 2In the realizable setting, as in previous works, it is assumed\nthat ξ = 0.\nThe distributional assumption includes common assumptions such as Gaussian inputs, but is far less restrictive. For example, we do not require the distribution to be continuous nor do we require it to have identity covariance. In Section 4, we show that commonly used patch schemes from computer vision satisfy our patch requirements. The assumption on activation functions is satisfied by popular activations such as ReLU (α = 0) and leaky ReLU (α > 0)."
  }, {
    "heading": "2.1. Some Useful Properties",
    "text": "The activations we consider in this paper have the following useful property under the stated distributional assumption:\nLemma 1. For all a, b ∈ R,\nEx∼DX [σ(aTx)(bTx)] = 1 + α\n2 Ex∼DX [(aTx)(bTx)].\nThe loss function can be upper bounded by the l2-norm distance of weight vectors using the following lemma.\nLemma 2. For any w, we have\nL(w) ≤ 1 + α 2 λmax(Σ)||w∗ − w||2.\nLemma 3. For all w and x,\n(fw∗(x)− fw(x))2 ≤ ||w∗ − w||2||x||2\nThe Gershgorin Circle Theorem, stated below, is useful for bounding the eigenvalues of matrices.\nTheorem 1 ((Weisstein, 2003)). For a n × n matrix A, define Ri := ∑n j=1,j 6=i |Ai,j |. Each eigenvalue of A must lie in at least one of the disks {z : |z −Ai,i| ≤ Ri}.\nNote: The proofs of lemmas in this section have been deferred to the Supplemental section."
  }, {
    "heading": "3. The Convotron Algorithm",
    "text": "In this section we describe our main algorithm Convotron and give a proof of its correctness. Convotron is an iterative algorithm similar in flavor to SGD with a modified (aggressive) gradient update. Unlike SGD (Algorithm 3), Convotron comes with provable guarantees and also does not need a good initialization scheme for convergence.\nThe following theorem describes the convergence rate of our algorithm:\nTheorem 2. If Assumptions 1 are satisfied then for\nη = Ω ( λmin(PΣ) kλmax(P ) min ( 1 Ex[||x||4] , δ||w∗||2√ ρEx[||x||4] )) and\nT = O (\nk ηλmin(PΣ)\nlog (\n1 δ\n)) , with probability 1 − δ, the\nweight vector w computed by Convotron satisfies\n||w − w∗||2 ≤ ||w∗||2.\nAlgorithm 1 Convotron Initialize w1 := 0 ∈ Rr. for t = 1 to T do\nDraw (xt, yt) ∼ D Let Gt = (yt − fwt(xt)) (∑k i=1 Pixt ) Set wt+1 = wt + ηGt\nend for Return wT+1\nProof. Define St = {(x1, y1), . . . , (xt, yt)} The dynamics of Convotron can be expressed as follows:\nExt,yt [||wt − w∗||2 − ||wt+1 − w∗||2|St−1] = 2ηExt,yt [(w∗ − wt)TGt|St−1] − η2Ext,yt [||Gt||2|St−1]\nWe need to bound the RHS of the above equation. We have,\nExt,yt [(w∗ − wt)TGt|St−1]\n= Ext,yt [ (w∗ − wt)T (yt − fwt(xt)) ( k∑ i=1 Pixt )∣∣∣∣∣St−1 ]\n= Ext,ξt [ (w∗ − wt)T (fw∗(xt) + ξt\n−fwt(xt)) ( k∑ i=1 Pixt )∣∣∣∣∣St−1 ]\n= Ext [ (w∗ − wt)T (fw∗(xt)− fwt(xt)) ( k∑ i=1 Pixt )∣∣∣∣∣St−1 ]\n(1)\n= 1\nk ∑ 1≤i,j≤k Ext [(σ(wT∗ Pixt)− σ(wTt Pixt)) (2)\n(wT∗ − wTt )Pjxt|St−1]\n= 1 + α\n2k ∑ 1≤i,j≤k Ext [((wT∗ − wTt )Pixt)\n((wT∗ − wTt )Pjxt)|St−1] (3)\n= 1 + α\n2k (wT∗ − wTt )  ∑ 1≤i≤k Pi Ext [xtxTt ] ∑ 1≤j≤k PTj\n (w∗ − wt) = 1 + α\n2k (wT∗ − wTt )  ∑ 1≤i,j≤k PiΣP T j  (w∗ − wt) = 1 + α\n2k (wT∗ − wTt )PΣ(w∗ − wt)\n≥ 1 + α 2k λmin(PΣ)||w∗ − wt||2. (4)\n(1) follows using linearity of expectation and the fact that that E[ξt|xt] = 0 and (3) follows from using Lemma 1. (4) follows from observing that PΣ is symmetric, thus ∀x, xTPΣx ≥ λmin(PΣ)||x||2.\nNow we bound the variance of Gt. Note that E[Gt] = 0. Further,\nExt,yt [||Gt||2|St−1]\n= Ext,yt (yt − fwt(xt))2 ∣∣∣∣∣ ∣∣∣∣∣ k∑ i=1 Pixt ∣∣∣∣∣ ∣∣∣∣∣ 2 ∣∣∣∣∣∣St−1  ≤ λmax(P )Ext,yt [ (yt − fwt(xt))2||xt||2\n∣∣St−1] (5) = λmax(P )Ext,ξt [ (fw∗(xt) + ξt − fwt(xt))2||xt||2\n∣∣St−1] = λmax(P )Ext,ξt [ ((fw∗(xt)− fwt(xt))2 + ξ2t\n+ 2(fw∗(xt)− fwt(xt))ξt||xt||2 ∣∣St−1]\n= λmax(P ) ( Ext [ (fw∗(xt)− fwt(xt))2||xt||2 ∣∣St−1] + Ext,ξt [ξ2t ||xt||2] ) (6)\n≤ λmax(P ) ( Ext [||xt||4]||w∗ − wt||2 + √ ρExt [||xt||4] ) (7)\n(5) follows from observing ∣∣∣∣∣∣∑ki=1 Pix∣∣∣∣∣∣2 ≤ λmax(P )||x||2 for all x, (6) follows from observing Eξ[ξ|x] = 0 and (7) follows from applying Lemma 3 and bounding Ext,ξt [ξ2t ||xt||2] using Cauchy-Schwartz inequality.\nCombining the above equations and taking expectation over St−1, we get\nESt [||wt+1 − w∗||2] ≤ (1− 3ηβ + η2γ)ESt−1 [||wt − w∗||2] + η2B\nfor β = 1+α3k λmin(PΣ), γ = λmax(P )Ex[||x|| 4] and B = λmax(P ) √ ρEx[||x||4].\nWe set η = βmin (\n1 γ , δ||w∗||2 B\n) and break the analysis to\ntwo cases:\nCase 1: ESt−1 [||wt − w∗||2] > ηB β . This implies that ESt [||wt+1 − w∗||2] ≤ (1− ηβ)ESt−1 [||wt − w∗||2].\nCase 2: ESt−1 [||wt − w∗||2] ≤ ηB β ≤ ||w∗|| 2.\nObserve that once Case 2 is satisfied, we have ESt [||wt+1− w∗||2] ≤ (1 − 2ηβ)ηBβ + η\n2B ≤ ηBβ . Hence, for any iteration > t, Case 2 will continue to hold true. This implies that either at each iteration ESt−1 [||wt − w∗||2] decreases by a factor (1−ηβ) or it is less than δ||w∗||2. Thus if Case 1 is not satisfied for any iteration up to T , then we have,\nEST [||wT+1 − w||2] ≤ (1− ηβ) T ||w∗||2 ≤ e−ηβT ||w∗||2\nsince at initialization ||w1 − w∗|| = ||w∗||. Setting T = O (\n1 ηβ log ( 1 δ )) and using Markov’s inequality, with probability 1 − δ, over the choice of ST , ||wT+1 − w∗|| ≤ ||w∗||2.\nBy using Lemma 2, we can get a bound on L(wT ) ≤ ||w∗||2 by appropriately scaling ."
  }, {
    "heading": "3.1. Convotron in the Realizable Case",
    "text": "For the realizable (no noise) setting, that is, for all (x, y) ∼ D, y = fw∗(x), for some unknown w∗, Convotron achieves faster convergence rates.\nCorollary 1. If Assumptions 1 are satisfied with the learning model restricted to the realizable case, then for suitably choosen η, after T = O ( k2λmax(P )Ex[||x||4] λmin(PΣ)2 log ( 1 δ )) iterations, with probability 1−δ, the weight vector w computed by Convotron satisfies\n||w − w∗||2 ≤ ||w∗||2.\nProof. Since the setting has no noise, ρ = 0. Setting that parameter in Theorem 2 gives us η = Ω ( λmin(PΣ)\nkλmax(P )Ex[||x||4] ) as δ||w∗||\n2√ ρEx[||x||4] tends to infinity as ρ tends to 0 and taking the minimum removes this dependence from η. Substituting this η gives us the required result.\nObserve that the dependence of in the convergence rate is log(1/ ) for the realizable setting, compared to the 1/ dependence in the noisy setting."
  }, {
    "heading": "4. Which Patch Structures are Easy to Learn?",
    "text": "In this section, we will show that the commonly used convolutional filters in practice (“patch and stride”) have good eigenvalues giving us fast convergence by Theorem 2. We will start with the 1D case and then subsequently extend the result for the 2D case."
  }, {
    "heading": "4.1. 1D Convolution",
    "text": "Here we formally describe a patch and stride convolution in the one-dimensional setting. Consider a 1D image of dimension n. Let the patch size be r and stride be d. Let the patches be indexed from 1 and let patch i start at position (i−1)d+1 and be contiguous through position (i−1)d+r. The matrix Pi of dimension r × n corresponding to patch i looks as follows,\nPi = ( 0r×((i−1)d+1)Ir0r×(n−r−(i−1)d) ) where 0a×b indicates a matrix of dimension a× b with all zeros and Ia indicates the identity matrix of size a.\nThus, the total number of patches is k = bn−rd c + 1. We will assume that n ≥ 2r− 1 and r ≥ d. The latter condition is to ensure there is some overlap, non-overlapping case, which is easier, is handled in the next section. We will bound the extremal eigenvalues of P =∑k i,j=1 PiP T j . Simple algebra gives us the following structure for P ,\nPi,j = { k − a if |i− j| = ad 0 otherwise\nFor understanding, we show the matrix structure for d = 1 and n ≥ 2r. k k − 1 . . . k − r + 1 k − 1 k . . . k − r + 2 ... ... . . . ...\nk − r + 1 k − r + 2 . . . k\n ."
  }, {
    "heading": "4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P",
    "text": "The following lemmas bound the extremal eigenvalues of P .\nLemma 4. Maximum eigenvalue of P satisfies λmax(P ) ≤ k(p + 1) − (p − p2)(p2 + 1) = O(kp) where p = b r−1d c and p2 = bp2c.\nProof. Using Theorem 1, we have λmax(P ) ≤ maxi ( Pi,i + ∑ j 6=i |Pi,j | ) = maxi ∑k j=1 Pi,j . Observe\nthat P is bisymmetric thus ∑k j=1 Pi,j = ∑k j=1 Pr−i+1,j and we can restrict to the top half of the matrix. The structure of P indicates that in a fixed row, the diagonal entry is maximum and the non-zero entries decrease monotonically by 1 as we move away from the diagonal. Also, there can be at most p+ 1 non-zero entries in any row. Thus the sum is maximized when there are p+ 1 non-zero entries and the diagonal entry is the middle entry, that is at position p2d+1. By simple algebra,\nλmax(P ) ≤ k∑ j=1 Pp2d+1,j\n= k + 2 p2∑ j=1 (k − j) + (p− 2p2)(k − p2 − 1)\n= k(p+ 1)− (p− p2)(p2 + 1).\nLemma 5. Minimum eigenvalue of P satisfies λmin(P ) ≥ 0.5.\nProof. We break the analysis into following two cases:\nd < r/2: We can show that λmax(P−1) ≥ 2 using the structure of P (see Lemma A and B in Supplemental). Since λmin(P ) = 1/λmax(P −1), we have λmin(P ) ≥ 0.5.\nd ≥ r/2: In this case we directly bound the minimum eigenvalue of P . Using Theorem 1, we know that λmin(P ) ≥ mini ( Pi,i − ∑ j 6=i |Pi,j | ) . For Pi,j 6= 0, |i − j| = ad for some a. The maximum value that |i − j| can take is r − 1 and since d ≥ r/2, a must be either 0 or 1. Also, for any i, there exists a unique j such that |i − j| = d since r/2 ≤ d < r, thus there are exactly 2 non-zero entries in each row of P , Pi,i. This gives us, for each i, ∑ j 6=i Pi,j = k − 1. Thus, we get that λmin(P ) ≥\nmini ( Pi,i − ∣∣∣∑j 6=i Pi,j∣∣∣) = k − (k − 1) = 1. Combining both, we get the required result."
  }, {
    "heading": "4.1.2. LEARNING RESULT FOR 1D",
    "text": "Augmenting the above analysis with Theorem 2 gives us learnability of 1D convolution filters.\nCorollary 2. If Assumptions 1(a),(b), and (d) are satisfied and the patches have a patch and stride structure with parameters n, r, d, then for suitably chosen η and T =\nO\n( n3r\nd4λmin(Σ)2 max\n( Ex[||x||4], √ ρEx[||x||4] ||w∗||2 ) log ( 1 δ )) ,\nwith probability 1 − δ, the weight vector w output by Convotron satisfies\n||w − w∗||2 ≤ ||w∗||2.\nProof. Combining the above Lemmas gives us that λmax(P ) = O(pk) = O(nr/d\n2) and λmin(P ) = Ω(1). Observe that λmin(PΣ) ≥ λmin(P )λmin(Σ). Substituting these values in Theorem 2 gives us the desired result.\nComparing with SGD, (Brutzkus & Globerson, 2017) showed that even for r = 2 and d = 1, Gradient descent can get stuck in a local minima with probability ≥ 1/4."
  }, {
    "heading": "4.2. 2D Convolution",
    "text": "Here we formally define stride and patch convolutions in two dimensions. Consider a 2D image of dimension n1×n2. Let the patch size be r1× r2 and stride in both directions be d1, d2 respectively. Enumerate patches such that patch (i, j) starts at position ((i − 1)d1 + 1, (j − 1)d2 + 1) and is a rectangle with diagonally opposite point ((i−1)d2+r1, (j− 1)d2 + r2). Let k1 = bn1−r1d1 c+ 1 and k2 = b n2−r2 d2 c+ 1. Let us vectorize the image row-wise into a n1n2 dimension vector and enumerate each patch row-wise to get a r1r2 dimensional vector. Let Q(i,j) be the indicator matrix of dimension r1r2 × n1n2 with 1 at (a, b) if the ath location of patch (i, j) is b. More formally, (Q(i,j))a,b = 1 for all a = pr2 + q + 1 for 0 ≤ p < r1, 0 ≤ q < r2, and\nb = ((i− 1)d1 + p)n2 + jd2 + q+ 1 else 0. Note that there are k1 · k2 patches in total with the corresponding patch matrices being Q(i,j) for 1 ≤ i ≤ k1, 1 ≤ j ≤ k2."
  }, {
    "heading": "4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q",
    "text": "We will bound the extremal eigenvalues of Q =∑k1 i,p=1 ∑k2 j,q=1Q(i,j)Q T (p,q). Let P (1) i ’s be the patch matrices corresponding to the 1D convolution for parameters n1, r1, d1 defined as in the previous section and let P (1) = ∑k1 i,j=1 P (1) i (P (1) j )\nT . Define P (2)i ’s for 1 ≤ i ≤ k2 and P (2) similarly with parameters n2, r2, d2 instead of n1, r1, d1.\nLemma 6. Q(i,j) = P (1) i ⊗ P (2) j .\nProof. Intuitively P (1)i and P (2) j give the indices corresponding to the row and column of the 2D patch and the Kronecker product vectorizes it to give us the (i, j)th patch. More formally, we will show that (Q(i,j))a,b = 1 iff (P\n(1) i ⊗ P (2) j )a,b = 1.\nLet a = pr2 + q + 1 with 0 ≤ p < r1, 0 ≤ q < r2 and b = rn2 + s + 1 with 0 ≤ r < n1, 0 ≤ s < n2. Then, (P\n(1) i ⊗ P (2) j )a,b = 1 iff (P (1) i )p,r = 1 and (P (2) j )q,s = 1. We know that (P (1)i )p,r = 1 iff r = (i− 1)d1 + p+ 1 and (P\n(2) j )q,s = 1 iff s = (j − 1)d2 + q + 1. This gives us that b = ((i − 1)d1 + p)n1 + (j − 1)d2 + q + 1, which is the same condition for (Q(i,j))a,b = 1. Thus Q(i,j) = P\n(1) i ⊗ P (2) j .\nLemma 7. Q = P (1) ⊗ P (2).\nProof. We have,\nQ = k1∑ i,p=1 k2∑ j,q=1 Q(i,j)Q T (p,q)\n= k1∑ i,p=1 k2∑ j,q=1 (P (1) i ⊗ P (2) j )(P (1) p ⊗ P (2)q )T\n= k1∑ i,p=1 k2∑ j,q=1 (P (1) i ⊗ P (2) j )((P (1) p ) T ⊗ (P (2)q )T )\n= k1∑ i,p=1 k2∑ j,q=1 (P (1) i (P (1) p ) T )⊗ (P (2)j (P (2) q ) T )\n=  k1∑ i,p=1 P (1) i (P (1) p ) T ⊗  k2∑ j,q=1 P (2) j (P (2) q ) T  = P (1) ⊗ P (2).\nLemma 8. We have λmin(Q) ≥ 0.25 and λmax(Q) = O(k1p1k2p2) where p1 = b r1−1d1 c and p2 = b r2−1 d2 c.\nProof. Since Q = P (1) ⊗ P (2) and Q,P (1), P (2) are positive semi-definite, λmin(Q) = λmin(P )λmin(P (2)) and λmax(Q) = λmax(P (1))λmax(P (2)). Using the lemmas from the previous section gives us the required result.\nNote that this technique can be extended to higher dimensional patch structures as well."
  }, {
    "heading": "4.2.2. LEARNING RESULT FOR 2D",
    "text": "Similar to the 1D case, combining the above analysis with Theorem 2 gives us learnability of 2D convolution filters.\nCorollary 3. If Assumptions 1(a),(b), and (d) are satisfied and the patches have a 2D patch and stride structure with parameters n1, n2, r1, r2, d1, d2, then for suitably chosen η and T =\nO\n( n31n 3 2r1r2\nd31d 3 2λmin(Σ)\n2 max ( Ex[||x||4], √ ρEx[||x||4] ||w∗||2 ) log ( 1 δ )) ,\nwith probability 1 − δ, the weight vector w output by Convotron satisfies\n||w − w∗||2 ≤ ||w∗||2.\nProof. Lemma 8 gives us that λmax(Q) = O(n1n2r1r2/(d1d2)\n2) and λmin(P ) = Ω(1). Observe that λmin(PΣ) ≥ λmin(P )λmin(Σ). Substituting these values in Theorem 2 gives us the desired result."
  }, {
    "heading": "5. Non-overlapping Patches are Easy",
    "text": "In this section, we will show that if there is one patch that does not overlap with any patch and the covariance matrix is identity then we can easily learn the filter even if the other patches have arbitrary overlaps. This includes the commonly used Gaussian assumption. WLOG we assume\nAlgorithm 2 Convotron-No-Overlap Initialize w1 := 0 ∈ Rr. for t = 1 to T do\nDraw (xt, yt) ∼ D Let Gt = (yt − fwt(xt))P1xt Set wt+1 = wt + ηGt\nend for Return wT+1\nthat P1 is the patch that does not overlap with any other patch implying P1PTj = P T j P1 = 0 for all j 6= 1.\nObserve that the algorithm ignores the directions of all other patches and yet succeeds. This indicates that with respect to a Gaussian distribution, in order to have an interesting patch structure (for one layer networks), it is necessary to avoid having even a single disjoint patch. The following theorem shows the convergence of Convotron-No-Overlap.\nTheorem 3. If Assumptions 1 are satisfied with Σ = I , then for η = (1+α)3k min ( 1 Ex[||x||4] , δ||w∗||2√ ρEx[||x||4] ) and T ≥\n1 ηδ log ( 1 δ ) , with probability 1 − δ, the weight vector w outputted by Convotron-No-Overlap satisfies\n||w − w∗||2 ≤ ||w∗||2.\nProof. The proof follows the outline of the Convotron proof very closely. We use the same definitions as in the previous proof. We have,\nExt,yt [(w∗ − wt)TGt|St−1]\n= 1\nk ∑ 1≤i≤k Ext [(σ(wT∗ Pixt)− σ(wTt Pixt))(wT∗\n− wTt )P1xt|St−1]\n= 1 + α\n2k ∑ 1≤i≤k Ext [((wT∗ − wTt )Pixt)((wT∗\n− wTt )P1xt)|St−1]\n= 1 + α\n2k (wT∗ − wTt )  ∑ 1≤i≤k Pi Ext [xtxTt ]P1(w∗ − wt) = 1 + α\n2k ||wT∗ − wTt ||2\nThe last equality follows since PTi P1 = 0 for all i 6= 1 and PT1 P1 is a permutation of identity.\nSimilarly,\nExt,yt [||Gt||2|St−1] = Ext,yt [ (yt − fwt(xt))2 ||Pixt|| 2 ∣∣∣St−1]\n≤ Ext,yt [ (yt − fwt(xt))2||xt||2 ∣∣St−1]\nAlgorithm 3 SGD Randomly initialize w1 ∈ Rr. for t = 1 to T do\nDraw (xt, yt) ∼ D Let Gt = (yt − fwt(xt)) (∑k i=1 σ ′(wTt Pixt)Pixt ) Set wt+1 = wt + ηGt\nend for Return wT+1\n≤ Ext [||xt||4]||w∗ − wt||2 + √ ρExt [||xt||4]\nFollowing the rest of the analysis for η and T as in the theorem statement gives us the required result."
  }, {
    "heading": "6. Experiments: SGD vs Convotron",
    "text": "To further support our theoretical findings, we empirically compare the performance of SGD (Algorithm 3) with our algorithm Convotron. We measure performance based on the failure probability, that is, the fraction of runs the algorithm fails to converge on randomly initialized runs (the randomness is over both the choice of initialization for SGD and the draws from the distribution). More formally, we say that the algorithm fails if the closeness in l2-norm of the difference of the final weight vector obtained (wT ) and the true weight parameter (w∗), that is, ||wT − w∗|| is greater than a threshold θ. We choose this measure because in practice, due to the high computation time of training neural networks, random restarts are expensive.\nIn the experiments, given a fixed true weight vector, for varying learning rates (increments of 0.01), we choose 50 random initializations and run the two algorithms with them as starting points. We plot the failure probability (θ = 0.1) with varying learning rate. Note that the lowest learning rate we use is 0.01 as making the learning rate too small requires high number of iterations for convergence for both algorithms.\nWe first test the performance on a simple 1D convolution case with (n, k, d, T ) = (8, 4, 1, 6000) and 2D case with (n1, n2, k1, k2, d1, d2, T ) = (5, 5, 3, 3, 1, 1, 15000) on inputs drawn from a normalized (l2 norm 1) Gaussian distribution with identity covariance matrix. We adversarially choose a fixed weight vector (l2-norm 1). For example, we take the vector to be [1,−1, 1,−1] in the 1D case and normalize. This weight vector can be viewed as an edge detection filter, that is, counting the number of times image goes from black (negative) to white (positive). Figure 3 (Top) shows that SGD has a small data dependent range where it succeeds but may fail with almost 0.5 probability outside this region whereas Convotron always returns a good solution for small enough η chosen according to\nTheorem 2. The failure points observed for SGD show the prevalence of bad local minima where SGD gets stuck.\nFor the second experiment, we choose a fixed weight vector for which SGD performs well with very high probability on a normalized Gaussian input distribution with identity covariance matrix (see Figure 3 (Bottom-left)). However, on choosing a different covariance matrix with higher condition number ∼ 60, the performance of SGD worsens whereas Convotron always succeeds (see Figure 3 (Bottom-Right)). The covariance matrix is generated by choosing random matrices followed by symmetrizing them and adding cI for c > 0 to make the eigenvalues positive.\nThese experiments demonstrate that techniques for finetuning SGD’s learning rate are necessary, even for very simple architectures. In contrast, no fine-tuning is necessary for Convotron: the correct learning rate can be easily computed given the learner’s desired patch structure and estimate of the covariance martix."
  }, {
    "heading": "7. Conclusions and Future Work",
    "text": "We have given the first efficient algorithm with provable guarantees for learning general one layer convolutional networks under symmetric, well-conditioned distributions. The obvious open question is to extend our algorithm to higher depth networks and weaken the distributional assumptions."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Jessica Hoffmann and Philipp Krähenbühl for useful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "Globally optimal gradient descent for a convnet with gaussian inputs",
    "authors": ["A. Brutzkus", "A. Globerson"],
    "venue": "arXiv preprint arXiv:1702.07966,",
    "year": 2017
  }, {
    "title": "When is a convolutional filter easy to learn",
    "authors": ["S.S. Du", "J.D. Lee", "Y. Tian"],
    "venue": "arXiv preprint arXiv:1709.06129,",
    "year": 2017
  }, {
    "title": "Gradient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima",
    "authors": ["S.S. Du", "J.D. Lee", "Y. Tian", "B. Poczos", "A. Singh"],
    "venue": "arXiv preprint arXiv:1712.00779,",
    "year": 2017
  }, {
    "title": "Learning one-hidden-layer neural networks with landscape design",
    "authors": ["R. Ge", "J. Lee", "T. Ma"],
    "venue": "arXiv preprint arXiv:1711.00501,",
    "year": 2017
  }, {
    "title": "Eigenvalue decay implies polynomial-time learnability for neural networks",
    "authors": ["S. Goel", "A. Klivans"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Learning depth-three neural networks in polynomial time",
    "authors": ["S. Goel", "A. Klivans"],
    "venue": "arXiv preprint arXiv:1709.06010,",
    "year": 2017
  }, {
    "title": "Reliably learning the relu in polynomial time",
    "authors": ["S. Goel", "V. Kanade", "A. Klivans", "J. Thaler"],
    "venue": "arXiv preprint arXiv:1611.10258,",
    "year": 2016
  }, {
    "title": "Efficient learning of generalized linear and single index models with isotonic regression",
    "authors": ["S.M. Kakade", "V. Kanade", "O. Shamir", "A. Kalai"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Efficient distributionfree learning of probabilistic concepts",
    "authors": ["M.J. Kearns", "R.E. Schapire"],
    "venue": "In Foundations of Computer Science,",
    "year": 1990
  }, {
    "title": "Learning graphical models using multiplicative weights",
    "authors": ["A. Klivans", "R. Meka"],
    "venue": "arXiv preprint arXiv:1706.06274,",
    "year": 2017
  }, {
    "title": "Convergence analysis of two-layer neural networks with relu activation",
    "authors": ["Y. Li", "Y. Yuan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Learning relus via gradient descent",
    "authors": ["M. Soltanolkotabi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity",
    "authors": ["Y. Tian"],
    "year": 2016
  }, {
    "title": "Electron-proton dynamics in deep learning",
    "authors": ["Q. Zhang", "R. Panigrahy", "S. Sachdeva", "A. Rahimi"],
    "venue": "arXiv preprint arXiv:1702.00458,",
    "year": 2017
  }, {
    "title": "Learning nonoverlapping convolutional neural networks with multiple kernels",
    "authors": ["K. Zhong", "Z. Song", "I.S. Dhillon"],
    "venue": "arXiv preprint arXiv:1711.03440,",
    "year": 2017
  }, {
    "title": "Recovery guarantees for one-hidden-layer neural networks",
    "authors": ["K. Zhong", "Z. Song", "P. Jain", "P.L. Bartlett", "I.S. Dhillon"],
    "venue": "arXiv preprint arXiv:1706.03175,",
    "year": 2017
  }],
  "id": "SP:13972fc5ff5a6d159ded6e53f1b04765b5bd66fa",
  "authors": [{
    "name": "Surbhi Goel",
    "affiliations": []
  }, {
    "name": "Adam Klivans",
    "affiliations": []
  }, {
    "name": "Raghu Meka",
    "affiliations": []
  }],
  "abstractText": "We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches under mild conditions on the underlying distribution. We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and twodimensional “patch and stride” convolutions. Our algorithm– Convotron– is inspired by recent work applying isotonic regression to learning neural networks. Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting). In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum. We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just one disjoint patch P (the other patches may be arbitrary) is easy in the following sense: Convotron can efficiently recover the hidden weight vector by updating only in the direction of P .",
  "title": "Learning One Convolutional Layer with Overlapping Patches"
}