{
  "sections": [{
    "heading": "1. Introduction",
    "text": "One of the key challenges in modern machine learning is to approximate complex distributions. Due to recent advances on learning scalability (Hoffman et al., 2013) and flexibility (Kingma et al., 2016), and the development of automated inference procedures (Ranganath et al., 2014), variational inference (VI) has become a popular approach for general latent variable models (Blei et al., 2017). Variational inference leverages a posterior approximation to derive a lower bound on the log-evidence of the observed samples, which can be efficiently optimized. This bound, commonly known as the evidence lower bound (ELBO), serves as a surrogate objective for maximum likelihood estimation (MLE) of the model parameters. Successful ap-\n*Equal contribution Affiliation: Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA. Correspondence to: Chenyang Tao <chenyang.tao@duke.edu>, Liqun Chen <liqun.chen@duke.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nplications of VI have been reported in document analysis (Blei et al., 2003), neuroscience (Friston et al., 2007), generative modeling (Kingma & Welling, 2014), among many others.\nIt has been widely recognized that tightening the variational bound, in general, significantly improves model performance. Consequently, considerable research has been directed toward this goal. The most direct approach seeks to boost the expressive power of the approximate posterior. Normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016) exploited invertible transformations on latent codes in latent variable models, Ranganath et al. (2016) and Gregor et al. (2015) explored the hierarchical structure of the latent code generation, and Miller et al. (2016) modeled the posterior as a mixture of Gaussians. Adversarial variational Bayes (Mescheder et al., 2017) employed a neural generator to produce posterior samples, and further leveraged a density ratio estimator to compute the ELBO. Notably, the matching between the true and approximate posterior can be made implicit (Pu et al., 2017a) via an application of Stein’s lemma (Liu & Wang, 2016).\nAn alternative direction seeks a modification of the variational objective. The importance-weighted autoencoder (Burda et al., 2016) showed that the bound can be sharpened by leveraging multiply-weighted posterior samples. Further, χ-VI (Dieng et al., 2017), also known as Rényi-VI (Li & Turner, 2016), derived an alternative bound that is sharper than the ELBO. More generally, a sandwich formula holds for the χ bound, thus tightening this gap improves performance. Bamler et al. (2017) developed a more general view on variational bounds and presented a low-variance estimator based on a perturbative argument. It is important to note that sharpening the variational bound may unexpectedly hurt learning of the inference arm of the model (approximate posterior) (Rainforth et al., 2017), thereby compromising performance.\nWhile most studies have focused on the scalability and flexibility of VI, a less-studied issue is that different models can be equally plausible in terms of of the mean evidence lower bound wrt a finite number of samples (Blei et al., 2017). This is a fundamental problem inherited by VI, associated with doing an empirical estimation of the expected log like-\nlihood. To make this more clear, recall that variational inference optimizes a lower bound to the expected log-evidence EX∼pd(x)[log pα(X)], where pd(x) is the unknown data distribution and distribution pα(x) is our model parameterized by α. Consider a set of samples D = {xi}ni=1 drawn from a ground truth model pd(x). A model pα(x) can achieve the same empirical expected log-evidence score as that of pd(x) by yielding lower log-evidence score on an underfitted subset of samples {xi; pα(xi) < pd(xi)}, while compensating its losses on another subset of overfitted samples {xi; pα(xi) > pd(xi)}. Neither underfitting nor overfitting are desirable when learning a probabilistic representation of the data. However, such behavior is not explicitly penalized by the standard variational objective, and each may be a consequence of approximating EX∼pd(x)[log pα(X)] with the finite det of samples in D. Consequently, when doing MLE or VI with a flexible model family on finite samples, the maximizer may not be unique (Mäkeläinen et al., 1981; Dharmadhikari & Joag-Dev, 1985).\nThis paper addresses model selection issues in variational inference. Our key contributions are: (i) An extension of the concept of evidence score and its use as a new modelselection criterion. (ii) Derivation of novel importanceweighted evidence bounds, and proof of their theoretical properties. (iii) Development of a novel variational inference procedure that favors more plausible models, compared to existing VI-based approaches. (iv) A new low bias-variance variational estimator with an improved update rule for optimization."
  }, {
    "heading": "2. ELBO and Variational Inference",
    "text": "Let pd(x) be the true and unknown data-generating distribution, which we seek to model with pα(x) = ∫ pα(x, z)dz where pα(x, z) = pα(x|z)p(z). Here z ∈ Rd represents latent variables responsible for x ∈ Rp, p(z) is a specified prior on z, and pα(x|z) is the conditional distribution of the data with model parameters α. The joint distribution may also be expressed pα(x, z) = pα(x)pα(z|x), where pα(z|x) is the model conditional distribution of latent z given x. The posterior pα(z|x) is typically difficult to compute, and therefore in variational inference it is approximated by qβ(z|x), a distribution with parameters β. The evidence lower bound (ELBO) is defined\nELBO(pα(x, z), qβ(z|x)) = Eqβ(z|x) log [pα(x, z) qβ(z|x) ] . (1)\nIt is well known that ELBO(pα(x, z), qβ(z|x)) = log pα(x) − KL(qβ(z|x)‖pα(z|x)) ≤ log pα(x), where KL(p‖q) is the Kullback-Leibler divergence between distributions p and q. Hence, the ELBO, characterized by cumulative parameters θ = (α, β), serves as a lower bound on evidence log pα(x).\nWhen learning, we seek to maximize the ELBO wrt\nparameters θ. One may also readily show that KL(pd‖pα) = −h(pd)−Ex∼pd [KL(qβ(z|x)‖pα(z|x))]− Ex∼pd [ELBO(pα(x, z), qβ(z|x))], where the differential entropy h(pd) is an unknown constant. Hence, minimization of KL(pα‖pd) corresponds to maximizing Ex∼pd [ELBO(pα(x, z), qβ(z|x))], which has the commensurate goal of pushing Ex∼pd [KL(qβ(z|x)‖pα(z|x))]→ 0. In practice, variational inference (VI) learns the “best” model by optimizing θ wrt the expected ELBO.\nA variational model is defined by pα(x, z) and qβ(z|x), with θ = (α, β). For notational convenience, the corresponding model is denoted Mθ. In the context of variational auto-encoder, α and β are also respectively known as the generator and inference parameters."
  }, {
    "heading": "3. Generalized Evidence Bound",
    "text": "To motivate our formal development below, we first provide some intuitions. Our key observation is that existing bounds are almost exclusively based on the Jensen inequality, which implies the variational gap can be improved with a less convex transform. On the other hand, it is desirable to prioritize underfitted samples when adjusting the model. We now describe a principled framework to address these two points.\nLet φ(u) : R+ → R be a non-decreasing function defined on the non-negative real line, referred to as the evidence function. Further, assume (i) φ(u) is concave, (ii) ψ(u) is a convex and monotonically increasing function, and (iii) h(u) , ψ(φ(u)) is concave. For notational clarity, we omit dependence on φ(u), ψ(u), pα(x, z) and qβ(z|x) when the context is clear. We will refer to φ(pα(x)) as the φ-evidence score of sample x wrt model pα(x).\nDefinition 1. The K-sample Generalized Evidence Lower Bound (GLBO) is defined as\nGLBO(x;K) ,\nψ−1 ( EZ1:K∼qβ [ h ( 1\nK K∑ k=1 pα(x, Zk) qβ(Zk|x)\n)]) , (2)\nwhere Z1:K = {Zk}Kk=1 are K iid samples from qβ(z|x).\nNote that (2) is closely related to importance sampling (Liu, 2008), where the approximate posterior qβ(z|x) is understood as the proposal distribution and the term 1 K ∑K k=1 pα(x,Zk) qβ(Zk|x) is theK-sample importance-weighted estimate of pα(x). When qβ(z|x) equals pα(z|x), the GLBO exactly recovers the φ-evidence score.\nConcerning intuitions for the above assumptions, the concavity of φ(u) in (i) reflects that, in general, we want to use a φ(u) that is monotonically increasing. Importantly, we also want φ(u) to saturate for large values of u, to reduce the influence of the high-evidence region (well-fit samples)\nin the objective, allowing the model to focus on less-well-fit samples (the saturation also minimizes the desire to over-fit when learning). Assumption (ii) from above introduces a convex auxiliary function ψ(u), and (iii) states that the concavity of φ(u) dominates the convexity of ψ(u). As discussed below, the additional convexity from ψ(u) generally improves our variational bound.\nTheorem 2. Under assumptions (i)-(iii),\nGLBO(x; 1) ≤ GLBO(x; 2) ≤ · · · K→∞−→ φ(pα(x)).\nAll proofs are provided in the Supplementary Material (SM). We denote\nNLBO(x;K) , GLBO(x;ψ(u) = u)\n= EZ1:K∼qβ\n[ φ ( 1\nK K∑ k=1 pα(x, Zk) qβ(Zk|x)\n)] ,\nas the naı̈ve bound for the K-sample φ-evidence score. The following theorem shows the concavity introduced via ψ(u) in GLBO improves the NLBO bound.\nTheorem 3. Under assumptions (i)-(iii),\nNLBO(x;K) ≤ GLBO(x;K).\nAs a particular case, recall the K-sample importanceweighted log-evidence lower bound (ELBO) is defined as\nELBO(x;K) , EZ1:K∼qβ [ log ( 1 K ∑ k pα(x,Zk) qβ(Zk|x) )] , (3)\nwhich is the variational objective proposed by Burda et al. (2016). The K-sample GLBO in (2) recovers the importance-weighted ELBO in (3), when (ψ(u), φ(u))→ (u, log(u)). From Theorems 2 and 3, GLBO is generally a tighter bound relative to the importance-weighted ELBO. This is formalized in the following corollary.\nCorollary 4. Under assumptions (i)-(iii),\nELBO(x;K) ≤ GLBO(x;K,φ = log(u)) ≤ log pα(x).\nNote this is for the special case where φ = log(u)."
  }, {
    "heading": "3.1. χ-evidence bounds",
    "text": "We now consider a few concrete examples. Letting T ≥ 1 be a temperature parameter, we define the K-sample χ-Evidence Lower Bound (CLBO) to be\nCLBO(x;K,T ) ,\nGLBO(x;K,φ = log(u), ψ = exp(T−1u)). (4)\nCLBO recovers the χ-evidence bound (Dieng et al., 2017), or Rényi variational bound (RVB) (Li & Turner, 2016) when K = 1. Further, our K-sample CLBO is superior to the K-sample bound used in (Dieng et al., 2017; Li &\nTurner, 2016). Specifically, our lower bound is guaranteed to be sharper than RVB (see Section 5.1, Theorem 8), and our upper bound is guaranteed to be an upper bound (see SM), while for RVB this property only holds in the asymptotic limit. See additional discussion in Section 5.1 and experimental results in Figure 1.\nStronger results can be established for the CLBO in (4). The following theorem proves that CLBO(x;K,T ) is nonincreasing wrt T .\nTheorem 5. Let 1 ≤ T1 ≤ T2, then\nCLBO(x;K,T2) ≤ CLBO(x;K,T1).\nWhile in theory as T → 1 the bound gets sharper, we note that in practice the empirical estimator becomes more unstable as the bound gets sharper. See SM for details on the effects of T related to empirical performance. We further establish asymptotic results for CLBO, as follows.\nTheorem 6. The following asymptotic results hold for CLBO:\nlim T→1\nCLBO(x;K,T )→ log pα(x),\nlim T→∞\nCLBO(x;K,T )→ ELBO(x;K).\nThis implies that asymptotically,\nELBO(x;K) ≤ CLBO(x;K,T ) ≤ log pα(x),\nfor T ∈ (1,∞). Further, for K = 1 it can be shown that Corollary 7. When T is sufficiently large,\nCLBO(x; 1, T ) ≈ ELBO + 1 2T varZ∼qβ(z|x)[f(x, Z)],\nwhere f(x, z) = log pα(x, z)− log qβ(z|x)."
  }, {
    "heading": "4. Model Selection with φ-evidence Score",
    "text": "Conventional VI picks a variational modelMθ that maximizes the expected ELBO wrt data. As discussed in the\nIntroduction, when choosing from a flexible family of variational models, the maximizer may not be unique. Therefore, we need to be more specific about what is a good model to select from these candidate models, which all maximize the variational objective.\nA straightforward strategy is to employ the minimax criteria: select a modelM∗θ that has highest value for its worst evidence bound wrt the data samples, i.e.\nM∗θ = argmax Mθ∈C {min x∈D {ELBO(pα(x, z), qβ(z|x))}}, (5)\nwhere C denotes the collection of all models that maximizes the variational objective. Intuitively, this ensures that the selected model gives a reasonable explanation for the sample least consistent with it.\nUnfortunately (5) does not readily translate into a differentiable objective wrt the variational parameters θ, and therefore cannot be directly optimized within stochastic gradient descent framework. Instead, we can relax (5) by reweighting the data with a weight function w(x), and optimize the ELBO wrt the weighted distribution. Following the spirit of minimax criteria, we want our model to improve its fit on the low evidence (underfitted) samples, and thus we put larger weights on those low evidence samples.\nNow we show optimizing a reweighted ELBO objective is equivalent to optimizing GLBO, with the weighting strategy implicitly implied by φ(u). First consider the gradient of φ-evidence wrt model parameters α\n∇αφ(pα(x)) = φ′(pα(x))pα(x)︸ ︷︷ ︸ (a) ∇α log pα(x)︸ ︷︷ ︸ (b) ,\nwhere φ′(u) denotes the derivative of φ(u). Here term (a) can be identified as the weight wα(x) to term (b), the gradient of the log-evidence. So each gradient update can be considered as an infinitesimal attempt to improve match wrt the reweighted data distribution p̃d(x) ∝ wα(x)pd(x), where the weight is determined by the current model pα(x).\nTo assign larger weights to the low evidence samples, we can specify a φ(u) with faster growth rate in the low evidence region and saturation in the high evidence region, which we call a saturating φ-evidence function. Figure 2 compares the standard log-evidence function with an example saturating φ-evidence function (on the log-scale). With a saturating score function, the model update strives to improve its fit on the samples that are less consistent with the current model. This also helps to prevent the optimization from entering a state of greedy improvement of already well-fitted samples, that may result in overfitting, thus in overoptimistic evidence scores.\nNote that optimizing the GLBO objective with an evidence function other than log(u) is framed as a model regularizer, rather than a primary objective for model fitting. The\nGLBO does not compromise the primary objective, such as maximizing the expected log-evidence bound, in the model selection phase. Second, a saturating φ(u) encourages a low variance evidence distribution. This closely connects to the maximal entropy principle for model selection (Jaynes, 2003), and we provide an informal argument in the SM to support this view."
  }, {
    "heading": "5. Related work",
    "text": "χ2 / Rényi variational inference The Rényi variational bound proposed by Li & Turner (2016) is a special case of the GLBO. The authors also investigated a K-sample importance-weighted variational objective of the form\nRVB(x;K,T ) ,\nEZ1:K∼q\n[ T log ( 1\nK K∑ k=1 ( pα(x, Zk) qβ(Zk|x) )1/T)] . (6)\nHowever, RVB(x;K,T ) is problematic because: (i) it is a loose lower bound when T > 1, (ii) when T < 1 it approaches the upper bound from below as K grows, which means that it may not hold as upper bound until K is sufficiently large (see Figure 2(a) of Li & Turner (2016)). In fact, Li & Turner (2016) maximized a K-sample estimate of an upper bound in their experiments, while with their particular choice of K, the upper bound estimator turns out to be a lower bound. The following theorem states that our CLBO is guaranteed to be sharper than RVB.\nTheorem 8. When T ≥ 1,\nRVB(x;K,T ) ≤ CLBO(x;K,T ) ≤ log pα(x).\nWebb & Teh (2016) hypothesized a better form of importance-weighted estimator for the Rényi variational bound in (6) and validated their hypothesis with some empirical experiments. However, they were unable to provide a theoretical justification for their estimator, thus they left it as future work, which we address here.\nMotivated by the issue of the posterior variance underestimation suffered by ELBO-based VI procedures, Dieng et al. (2017) proposed to minimize the variational upper\nbound rather than the lower bound, which in turn over estimates the posterior variance. The authors focused on the χ2 variational bound, a special case of GLBO’s upper bound (see the SM for more details). They proved the equivalence between the χ2 variational upper bound minimization and the minimization of χ2-divergence between the true and approximate posteriors. However, we note that optimization of variational upper bounds is considerably more numerically unstable relative to its lower bound counterpart. Consequently, χ2-VI is more appropriate for relatively simple problems. The estimator they proposed is essentially the exponential of the estimator used in Rényi variational inference (Li & Turner, 2016). However, this estimator cannot be used to construct a variational auto-encoder. A minimal variance argument was made to establish a connection to importance sampling. However, Dieng et al. (2017) did not consider using importance sampling technique to improve their estimator.\nEfficiency of importance-weighted VI Rainforth et al. (2017) recently analyzed the trade-off of using an importance-weighted estimator in VI. In particular, the authors considered the signal-to-noise ratio (SNR) of a gradient estimator as defined by\nSNR(∇̂`(θ)) , E[∇̂`(θ)]√ var(∇̂`(θ)) , (7)\nwhere ∇̂`(θ) is the gradient of the variational objective `(θ) wrt θ, and E[·] and var(·) are approximated with samples. Rainforth et al. (2017) showed that (7) converges with rates O( √ K) and O(1/ √ K) for the generator parameters α and inference parameters β, respectively. This raises the concern that the gains in the improved bound by increasing K may not be feasible due to unstable updates for β.\nWhile also using an importance-weighted estimator, our GLBO explores some orthogonal directions. We consider the problem of deriving variational bounds for more generalized φ-evidence functions, which can be designed to encourage desirable properties of a solution. Additionally, in our framework the improvement for the bound also comes from the ψ-transformation. We also advocate a new update rule for the parameters to mitigate the SNR issue; see Section 6.2 for details.\nRegularized variational inference While not directly motivated from a model-selection perspective, recent developments in regularized variational inference shares similarities with our approach. In generative modeling, traditional VI has been criticized for producing unrealistic samples. This issue traces back to the fact that the aggregated approximate posterior does not match the prior (Makhzani et al., 2016), as ELBO based inference tend to underestimate posterior variance (Pu et al., 2018). A number of solutions have been proposed to alleviate this\nproblem, most notably, adversarially regularized solutions (Dumoulin et al., 2016; Pu et al., 2017b; Li et al., 2017). These methods introduced an adversarial loss, penalizing the mismatch between the marginal latent distributions p(z) and qβ(z) = ∫ pd(x)qβ(z|x) dx, or the joint distributions pα(x, z) and qβ(x, z) = pd(x)qβ(z|x). These regularization approaches favor models that output more realistic samples.\nRobust variational inference Our work also complements recent developments in robust variational inference (Wang et al., 2017; Figurnov et al., 2016). In Wang et al. (2017), the authors hypothesized that the cause of instability in VI comes from the presence of “bad” observations. To address this, they proposed to dynamically reweight samples, constrained by a prior distribution on the weight vector. This effectively down-weights “bad” observations and relieves the learner from modeling nonconforming examples. However, this method does not follow a standard probabilistic approach, and the results are sensitive to the choice of prior and other hyper parameters. In the work of Figurnov et al. (2016) the authors heuristically applied a soft-thresholded log(u) as the evidence function in ELBO. Similar to the reweighting strategy, this effectively eliminates any signal from low evidence samples during training."
  }, {
    "heading": "6. Optimization of GLBO",
    "text": "In this section we first describe an easy-to-implement lowvariance estimator that improves GLBO training, then discuss a new update rule based on theoretical insights. We also detail the state-of-the-art VI models we tested with, the fact that GLBO improves upon these models demonstrates its wide applicability (see Section 7)."
  }, {
    "heading": "6.1. Improving the bound with moving average",
    "text": "A naı̈ve estimator for the GLBO in the stochastic gradient descent setting is\nVL,K(x, {Zl,k};α, β) =\nψ−1\n( 1\nL L∑ l=1 h\n( 1\nK K∑ k=1 pα(x, Zl,k) qβ(Zl,k|x)\n)) ,\n(8)\nwhere the expectation over Z1:K ∼ qβ(z|x) is replaced with the average of L empirical samples {Zl,1:K}Ll=1 drawn from qβ(z|x). However, this potentially introduces a negative bias, as one can readily derive from the Jensen’s inequality that E{Zl,k}∼qβ(z|x) [VL,K(x, {Zl,k})] ≤ GLBO(x;K). To reduce this bias, we note that our stochastic objective can be rewritten in a more general form as ψ−1(ĥ(x, p, q)), where ĥ(x, p, q) is an estimator for the term E[h] in the definition of the GLBO. Interestingly, this bias can be ameliorated by reducing the variance of estimator ĥ(x, p, q). We provide an asymptotic argument to support this claim in the SM.\nGiven the insights from above, we propose to replace the naı̈ve estimator ĥ with a moving average estimator ĥema, which in principle should reduce the variance and provide tighter estimate for the bound. Specifically, our empirical estimator for GLBO at iteration t is computed as\nV emaK (x, t) = ψ −1(ĥema(x, t)),\nĥema(x, t) = (1− wt)ĥema(x, t− 1)\n+ wth\n( 1\nK K∑ k=1 pα(x, Zt,k) qβ(Zt,k|x)\n) ,\nwhere Zt,1:K ∼ qβ(z|x) are approximate posterior samples and 0 ≤ wt ≤ 1 is the update weight for the averaging estimator. At iteration t, the historical estimate ĥema(x, t− 1) is treated as a constant baseline and the gradients are only propagated through the evaluation on current posterior samples Zt,1:K ."
  }, {
    "heading": "6.2. A high SNR update rule",
    "text": "The GLBO estimator is also vulnerable to the SNR issue associated with importance-weighted estimators analyzed by Rainforth et al. (2017) (see discussion in Section 5 above). Motivated by their theoretical insights, we propose to update the generator parameter α and inference parameter β with objective functions based on estimators of different variational bounds. Consider the naive estimator VL,K(x, {Zl,k}; θ) in (8), and assume we have a budget of S posterior samples for each parameter update. In the SGD setting, we propose to update the parameters with\nαt+1 ← αt + ηt∇αVS,1(xt, {Zl,k};αt, βt), βt+1 ← βt + ηt∇βV1,S(xt, {Zl,k};αt, βt),\n(9)\nwhere xt is the data sampled at iteration t and ηt is the learning rate. This combines the best of two worlds, as α and β are respectively updated by a high SNR gradient estimator. Generalization to the moving average estimator discussed above is straightforward. We note that the extra computational cost for using (9) instead of vanilla update rule θt+1 ← θt + ηt∇θV (xt; θt) is neglectable in the way modern differentiable learning algorithms are implemented."
  }, {
    "heading": "6.3. Local ELBO with flexible posterior approximation",
    "text": "To allow for more flexible posterior representation, we consider nonparametric posterior qβ(z|x) implicitly defined by a latent code generator z = G(x, ξ;β), where ξ ∼ q(ξ) is a simple of randomness for the posterior, e.g., Gaussian.\nTractable posterior approximation If G(x, ξ;β) is invertible wrt ξ, then qβ(z|x) = q(ξ)|det(∇ξG−1(x, ξ;β))|. A well known example of such invertible generators is the normalizing flow (NF) (Tabak et al., 2010; Rezende & Mohamed, 2015), which considers G(x, ξ;β) = zM recursively defined by zm = fm(zm−1, x;β),∀m = 1, · · · ,M .\nHere z0 = ξ and {fm(z, x;β)}Mm=1 is a chain of transformations invertible wrt to z parameterized by β. This allows a flexible posterior approximation log qβ(z|x), with a tractable log density that can be explicitly computed by back tracing the Jacobians of {fm}, e.g. log qβ(z|x) = log q(ξ)− ∑M m=1 log\n∣∣det(∇zm−1fm)∣∣. Intractable posterior approximation Using an unconstrained transformation G(x, ξ;β) allows more expressive posterior approximation at the cost of no explicit expression for qβ(z|x). To overcome this difficulty, we use the adversarial approach proposed by Mescheder et al. (2017). Specifically, we can decompose the local ELBO into the sum of a tractable log-likelihood term and an intractable log-likelihood ratio term (also known as the local KL) log f(x, z) = log pα(x|z) + log p(z)qβ(z|x) . Here we learn rβ(x, z) , log\np(z) qβ(z|x) by training an optimal discriminator\nσ(r(x, z)) between samples drawn from p(z) and qβ(z|x) rβ(x, z) = argmax\nr(x,z;φ)\n{EZ∼p(z)[log σ(r(x, Z;φ))] +\nEZ′∼qβ(z|x)[log(1− σ(r(x, Z ′;φ)))]},\nwhere σ(u) = (1+ e−u)−1 is the sigmoid function. To circumvent the numerical difficulties associated with vanishing likelihood ratios, we further leverage the adaptive contrast (AC) technique proposed in Mescheder et al. (2017), introducing an auxiliary distribution to improve the estimate of rβ(x, z). See the SM for details."
  }, {
    "heading": "7. Experiments",
    "text": "To compare the performance of our new bound and its predecessors, we empirically evaluate the sharpness of these bounds on a toy distribution, and benchmark them on a series of VI tasks. In all K-sample experiments we use the K-sample ELBO estimator to make the comparisons fair wrt computational cost, and report the vanilla ELBO as the log-evidence bound for all models in quantitative evaluations. We use the proposed moving average estimator except for the Bayesian regression experiment. Details of the experimental setup are in the SM, and source code is available (upon publication) from https: //www.github.com/LiqunChen0606/glbo."
  }, {
    "heading": "7.1. Bound sharpness",
    "text": "We consider the following toy distribution to quantitatively evaluate the performance of different bounds X = sin(Z)+ N (0, 0.01), Z ∼ U [0, π], where N (µ, σ2) denotes a Gaussian with mean µ and variance σ2, and U [a, b] to denote a uniform distribution on interval [a, b]. This specifies a simple two-dimensional distribution p(x, z), and we specify a simple unit variance Gaussian q(z|x) = N (π/2, 1) centered at π/2 as our posterior approximation to estimate a bound on marginal p(x) (See Figure SM-1(a-b)). The ground truth p(x) is estimated using a naive Monte Carlo\nestimator p̂(x) = 1S ∑S s=1 p(x, zl), z1:S ∼ U [0, π], where we set S = 10, 000.\nFigure 1(c,d) summarize the estimated GLBO with growing K and decreasing T , respectively. As our theory predicts, GLBO gets sharpened as K increases or as T decreases. Vanilla ELBO does not provide a reasonable bound in this experiment. Figure 1(a-c) compare the K-sample RVB, ELBO and CLBO. Our results verify that RVB does not necessarily improve importance-weighted ELBO, which is consistent with the empirical results reported by the original RVI paper (Li & Turner, 2016). GLBO on the other hand, is guaranteed to improve its ELBO counterpart. Notably, the performance boost is especially significant in the lowsample regime (K < 5) under our experimental conditions."
  }, {
    "heading": "7.2. Variational autoencoder",
    "text": "Our next experiment considers applying GLBO to variational autoencoders for unsupervised learning. To make comparisons fair, we focus on modifying publicly available implementations with our GLBO objective1. All experimental results are produced with the recommended settings from the original implementations.\nWe first compare GLBO with the vanilla variational autoencoder, importance-weighted VAE and RVB under the experimental setups from Li & Turner (2016) 2, on the MNIST dataset. The encoders and decoders are implemented with L ∈ {1, 2} neural network layers and leveragingK ∈ {5, 50} posterior samples. We choose the VR-Max estimator for RVB and set GLBO to CLBO(x;T,K) with T = 200. To evaluate performance, we estimate the true log-likelihood with S = 5, 000 importance-weighted posterior samples, and report the average of test set log-likelihood in Table 1. Our GLBO consistently improves performance, and the gain is more pronounced in the low posterior sample regime. We have also observed that our GLBO leads to\n1 In this work we use the results reported by the original papers, and we are able to reproduce\nthese results with the publicly available code. 2 https://github.com/YingzhenLi/vae_renyi_divergence\nfaster convergence (not shown). We have varied our experimental settings and the results are qualitatively similar.\nWe also examined if GLBO can enhance models with more flexible posterior distribution. We follow the experimental setup used in AVB (Mescheder et al., 2017) 3. In the MNIST experiment, we compare a GLBO version AVB with vanilla VAE, inverse autoregressive flow (IAF) (Kingma et al., 2016), auxiliary VAE (Maaløe et al., 2016) and AVB. No importance sampling is used, as in the original implementation, and we choose the best performing adaptive contrast AVB for comparison. We summarize the ELBO, AIS score (Wu et al., 2017) and reconstruction error in Table 2. Both GLBO and AVB achieved better reconstruction than other competitors, and our GLBO leads the performance on ELBO and AIS by a large margin.\nWe further evaluate GLBO on the more complex CelebA face dataset (Liu et al., 2015). We benchmark K-sample CLBO-VAE against ELBO, IW-ELBO and Rényi VAEs, using a convolutional neural net encoder and deconvolutionlayer based decoder as our architecture (Radford et al., 2016). The training and testing set log-evidence bounds as a function of training epochs are shown in Figure 4. CLBO-VAE showed both better evidence score and more stable training dynamics compared with its counterparts. In Figure 4 we also show the learning curves of each model augmented with NF posterior approximation, trained on the MNIST data. All models except for ELBO performed similarly, possibly because of the highly expressive NF approximation. Additionally, the high SNR update rule failed to improve the vanilla ELBO-VAE on CelebA, but provided slightly better performance compared with all other methods on MNIST+NF.\nFor the last experiment on VAE, we explore the idea of instantiating model-selection with GLBO. We first train the regular log-evidence objective to convergence with the AVB model on MNIST, and then switch to optimize φevidence with GLBO to prioritize more plausible models. Here we use the shifted log-log function φ(u) = log (log(u)− `lower) as our evidence function, so that we can vary `lower to manipulate the shape of φ (see SM for details of our choice). Figure 3 compares the log-evidence dis-\n3 https://github.com/LMescheder/AdversarialVariationalBayes\nTable 3: Test RMSE and log-likelihood results for Bayesian neural net regression.\nTest RMSE (lower is better) Test log-likelihood (higher is better)\nDataset VI PBP Rényi CLBO VI PBP Rényi CLBO\nBoston 4.32 ± .29 3.01 ± .18 2.86 ± .40 2.71± .29 -2.90 ± .07 -2.57 ± .09 -2.46 ± .16 −2.40± .09 Concrete 7.19 ± .12 5.67 ± .09 5.15 ± .25 5.04± .27 -3.39 ± .02 -3.16 ± .02 -3.04 ± .07 −3.02± .05 Energy 2.65 ± .08 1.80 ± .05 1.00 ± .18 0.95± .15 -2.39 ± .03 -2.04 ± .02 -1.67 ± .05 −1.65± .04 Kin8nm 0.10 ± .00 0.10 ± .00 0.08± .00 0.08± .00 0.90 ± .01 0.90 ± .01 1.14± .02 1.14± .02 Naval 0.01 ± .00 0.01 ± .00 0.00± .00 0.00± .00 3.73 ± .12 3.73 ± .01 4.11 ± .11 4.17± .10 CCPP 4.33 ± .04 4.12 ± .03 4.13 ± .04 4.03± .06 -2.89 ± .02 -2.85 ± .05 -2.84 ± .04 −2.81± .02 Winequality 0.65 ± .01 0.64 ± .02 0.62 ± .03 0.61± .03 -0.98 ± .01 -0.97 ± .01 -0.94 ± .04 −0.93± .04 Yacht 6.89 ± .67 1.02 ± .05 0.94 ± .23 0.87± .18 -3.43 ± .16 -1.63 ± .02 -1.61± .00 −1.52± .00\nProtein 4.84 ± .03 4.73 ± .01 4.65 ± .07 4.43± .05 -2.99 ± .01 -2.97 ± .00 -2.93 ± .00 −2.89± .01 Year 9.03 ± NA 8.88 ± NA 8.80 ± NA 8.78±NA -3.62 ± NA -3.60 ± NA -3.60 ± NA −3.57±NA\nFigure 3: Model-selection result on MNIST. log-evidence histogram plot (left), box plot of mean and quantile (right) for a converged and refined AVB model.\ntribution with and without model-selection on the MNIST dataset. The refinement phase improves performance on both the training and testing set, and the boost in generalization is more pronounced (testing +1.47 vs training +0.58 nats). This validates our hypothesis that applying the maximum entropy heuristic favors more plausible models."
  }, {
    "heading": "8. Bayesian Neural Net Regression",
    "text": "Finally we consider the problem of Bayesian regression with neural nets. We use ten datasets from the UCI Machine Learning Repository (Lichman, 2013) and followed the experimental setup from Li & Turner (2016); see SM for details. We use a random 90%/10% split for training and testing, and use test root mean squared error (RMSE) and log-likelihood (LL) for evaluation.\nWe compared CLBO with ELBO, IW-ELBO, Rényi-VI and probabilistic backpropagation (PBP) (Hernández-Lobato & Adams, 2015) in this experiment. For CLBO and Rényi we fixed T = 2. The results are summarized in Table 3.4 The proposed CLBO in general improves over its counterparts. This provides evidence that CLBO learns a better model\n4The results for IW-ELBO is quantitatively similar to those of Rényi-VI, we therefore report it in the SM to save space.\nFigure 4: log-evidence bound evolution wrt training epochs on CelebA (upper panel) and MNIST (lower panel). Low evidence scores rescaled for better visualization. Normalizing flow is used for the MNIST variational models. ALTER denotes CLBO learned with the high SNR update rule proposed in Sec 6.2.\nrather than simply bumping up the evidence bound."
  }, {
    "heading": "9. Conclusion",
    "text": "We have considered generalization of the evidence score, and have proposed a new family of evidence bounds and improved estimators. Our work subsumes many existing bounds as special cases, while also being provably sharper. We carried out experiments to validate our claims, and the results are consistent with our theoretical predictions. We provided empirical evidence that our method improves stateof-the-art approaches. We also investigated the issue of model-selection in variational inference, and proved, empirically, that our theoretically-inspired strategy leads to an improvement in generalization performance.\nIn future work, we intend to build on automated inference procedures with generalized evidence bounds. This involves further understanding of φ-evidence bounds, and designing principled strategies that are guaranteed to achieve desired optimality conditions. Adaptive hyper-parameter tuning is also desirable to simplify φ-evidence based VI."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors would like to thank the anonymous reviewers for their insightful comments. This research was supported in part by DARPA, DOE, NIH, ONR and NSF. The authors would also like to thank S Dai, Dr. Y Li and Dr. Y Pu for fruitful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "Perturbative black box variational inference",
    "authors": ["R. Bamler", "C. Zhang", "M. Opper", "S. Mandt"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Variational inference: A review for statisticians",
    "authors": ["D.M. Blei", "A. Kucukelbir", "J.D. McAuliffe"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2017
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Examples of nonunique maximum likelihood estimators",
    "authors": ["S. Dharmadhikari", "K. Joag-Dev"],
    "venue": "The American Statistician,",
    "year": 1985
  }, {
    "title": "Variational inference via chi upper bound minimization",
    "authors": ["A.B. Dieng", "D. Tran", "R. Ranganath", "J. Paisley", "D.M. Blei"],
    "year": 2017
  }, {
    "title": "Adversarially learned inference",
    "authors": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Robust variational inference",
    "authors": ["M. Figurnov", "K. Struminsky", "D. Vetrov"],
    "venue": "arXiv preprint arXiv:1611.09226,",
    "year": 2016
  }, {
    "title": "Variational free energy and the laplace approximation",
    "authors": ["K. Friston", "J. Mattout", "N. Trujillo-Barreto", "J. Ashburner", "W. Penny"],
    "year": 2007
  }, {
    "title": "Draw: A recurrent neural network for image generation",
    "authors": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Probabilistic backpropagation for scalable learning of bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R. Adams"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Stochastic variational inference",
    "authors": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Probability theory: The logic of science",
    "authors": ["E.T. Jaynes"],
    "venue": "Cambridge university press,",
    "year": 2003
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Improving variational inference with inverse autoregressive flow",
    "authors": ["D.P. Kingma", "T. Salimans", "M. Welling"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Towards understanding adversarial learning for joint distribution matching",
    "authors": ["C. Li", "H. Liu", "C. Chen", "Y. Pu", "L. Chen", "R. Henao", "Carin", "L. Alice"],
    "year": 2017
  }, {
    "title": "Rényi divergence variational inference",
    "authors": ["Y. Li", "R.E. Turner"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Monte Carlo strategies in scientific computing",
    "authors": ["J.S. Liu"],
    "venue": "Springer Science & Business Media,",
    "year": 2008
  }, {
    "title": "Stein variational gradient descent: A general purpose bayesian inference algorithm",
    "authors": ["Q. Liu", "D. Wang"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Deep learning face attributes in the wild",
    "authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "Auxiliary deep generative models",
    "authors": ["L. Maaløe", "C.K. Sønderby", "S.K. Sønderby", "O. Winther"],
    "venue": "arXiv preprint arXiv:1602.05473,",
    "year": 2016
  }, {
    "title": "On the existence and uniqueness of the maximum likelihood estimate of a vector-valued parameter in fixed-size samples",
    "authors": ["T. Mäkeläinen", "K. Schmidt", "G.P. Styan"],
    "venue": "The Annals of Statistics,",
    "year": 1981
  }, {
    "title": "Adversarial variational Bayes: unifying variational autoencoders and generative adversarial networks",
    "authors": ["L. Mescheder", "S. Nowozin", "A. Geiger"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Variational boosting: Iteratively refining posterior approximations",
    "authors": ["A.C. Miller", "N. Foti", "R.P. Adams"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Variational autoencoder for deep learning of images, labels and captions",
    "authors": ["Y. Pu", "Z. Gan", "R. Henao", "X. Yuan", "C. Li", "A. Stevens", "L. Carin"],
    "year": 2016
  }, {
    "title": "VAE learning via Stein variational gradient descent",
    "authors": ["Y. Pu", "Z. Gan", "R. Henao", "C. Li", "S. Han", "L. Carin"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Adversarial symmetric variational autoencoder",
    "authors": ["Y. Pu", "W. Wang", "R. Henao", "L. Chen", "Z. Gan", "C. Li", "L. Carin"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Symmetric variational autoencoder and connections to adversarial learning",
    "authors": ["Y. Pu", "L. Chen", "S. Dai", "W. Wang", "C. Li", "L. Carin"],
    "year": 2018
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Tighter variational bounds are not necessarily better",
    "authors": ["T. Rainforth", "T.A. Le", "Maddison", "M.I.C. J", "Wood", "Y.W.T. F"],
    "venue": "In NIPS workshop",
    "year": 2017
  }, {
    "title": "Black box variational inference",
    "authors": ["R. Ranganath", "S. Gerrish", "D. Blei"],
    "venue": "In AISTATS,",
    "year": 2014
  }, {
    "title": "Hierarchical variational models",
    "authors": ["R. Ranganath", "D. Tran", "D. Blei"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Variational inference with normalizing flows",
    "authors": ["D.J. Rezende", "S. Mohamed"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Density estimation by dual ascent of the log-likelihood",
    "authors": ["E.G. Tabak", "E Vanden-Eijnden"],
    "venue": "Communications in Mathematical Sciences,",
    "year": 2010
  }, {
    "title": "Reweighted data for robust probabilistic models",
    "authors": ["Y. Wang", "A. Kucukelbir", "D.M. Blei"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "A tighter monte carlo objective with rényi α-divergence measures",
    "authors": ["S. Webb", "Y.W. Teh"],
    "venue": "In NIPS Workshop,",
    "year": 2016
  }, {
    "title": "On the quantitative analysis of decoder-based generative models",
    "authors": ["Y. Wu", "Y. Burda", "R. Salakhutdinov", "R. Grosse"],
    "venue": "In ICLR",
    "year": 2017
  }, {
    "title": "Learning structural weight uncertainty for sequential decision-making",
    "authors": ["R. Zhang", "C. Li", "C. Chen", "L. Carin"],
    "venue": "In AISTATS,",
    "year": 2018
  }],
  "id": "SP:fc3e097ea7dd5daa7d314ecebe7faad9af5e62fb",
  "authors": [{
    "name": "Chenyang Tao",
    "affiliations": []
  }, {
    "name": "Liqun Chen",
    "affiliations": []
  }, {
    "name": "Ruiyi Zhang",
    "affiliations": []
  }, {
    "name": "Ricardo Henao",
    "affiliations": []
  }, {
    "name": "Lawrence Carin",
    "affiliations": []
  }],
  "abstractText": "Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and Rényi bounds as special cases, and it is provably sharper than these counterparts. We also present an improved estimator for variational learning, and advocate a novel high signal-to-variance ratio update rule for the variational parameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference procedures, and show how to leverage the flexibility of our new formulation to address them. Empirical evidence is provided to validate our claims.",
  "title": "Variational Inference and Model Selection  with Generalized Evidence Bounds"
}