{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4340–4349 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4340"
  }, {
    "heading": "1 Introduction",
    "text": "Modeling a coherent text is one of the key problems in natural language processing. A wellorganized text with a logical structure is much easier for people to read and understand. Sentence ordering task (Barzilay and Lapata, 2008) has been proposed to cope with this problem. It aims to organize a set of sentences into a coherent text with a logically consistent order and has wide applications in natural language generation such as concept-to-text generation (Konstas and Lapata, 2012a,b, 2013), retrieval-based question answering (Yu et al., 2018; Verberne, 2011), and extractive multi-document summarization (Barzilay and Elhadad, 2002; Galanis et al., 2012; Nallapati et al., 2017), where the improper ordering of sentences would introduce ambiguity and degrade readability. An example of this task is shown in Table 1.\nTraditional methods developed for this task employ handcrafted linguistic features to model the document structure such as Entity Grid (Barzilay and Lapata, 2008), Content Model (Barzilay\n∗Corresponding author\nand Lee, 2004), and Probabilistic Model (Lapata, 2003). However, manual feature engineering heavily relies on linguistic knowledge and also limits these systems to be domain specific. Inspired by the success of deep learning, datadriven approaches based on neural networks have been proposed including Pairwise Ranking Model (Chen et al., 2016) which learns the relative order of sentence pairs to predict the pairwise ordering of sentences, and Window network (Li and Hovy, 2014) sliding a window over the text to evaluate the coherence.\nRecently, hierarchical RNN-based approaches (Gong et al., 2016; Logeswaran et al., 2018) have been proposed to deal with this task. Such methods exploit LSTMs based paragraph encoder to compute a context representation for the whole sequential sentences and then adopt a pointer network (Vinyals et al., 2015) as the decoder to predict their order. However, since LSTM works sequentially, paragraph encoder only based on LSTMs suffers from the incorrect input sentence order and has difficulty in capturing a logically reliable representation through the recurrent connections, which makes trouble for the decoder to find the correct order.\nTo overcome the above limitation, in this work, we develop a novel deep attentive sentence ordering network (referred as ATTOrderNet) by inte-\ngrating self-attention mechanism (Vaswani et al., 2017) with LSTMs to learn a relatively reliable paragraph representation for subsequent sentence ordering. In particular, the bidirectional LSTM is first adopted as a sentence encoder to map the input sentences to the corresponding distributed vectors, and then a self-attention based paragraph encoder is introduced to capture structural relationships across sentences and to obtain a hierarchical context representation of the entire set of sentences. Consequently, based on the learned paragraph vector, a pointer network is applied to perform sentence ordering by decoding an ordered sequence. Figure 1 shows the architecture of ATTOrderNet, where self-attention mechanism is introduced to capture the dependencies among sentences.\nIn contrast to the previous paragraph encoders with LSTMs, self-attention mechanism is less sensitive to the input order of sentence sequence and is effective in modeling the accurate relationships across sentences, which reduces the influence of the original order of the input sentences and perfectly meets the requirement of our task. Further, unlike Transformer (Vaswani et al., 2017), we do not add any positional encodings in our model to minimize the influence of the unclear order information.\nExtensive evaluations are conducted on the sentence ordering task and order discrimination task to investigate the performances of ATTOrderNet. The experimental results on seven public sentence ordering datasets show the superior performances of the framework to the competing models. Meanwhile, the visualization of the attention layer in\nparagraph encoder is provided for better understanding of the effectivity of self-attention mechanism. Besides, in the Order Discrimination task, our model also achieves the state-of-the-art performance with remarkable improvements on two benchmark datasets."
  }, {
    "heading": "2 Deep Attentive Sentence Ordering Network",
    "text": "In this section, we first formulate the sentence ordering problem and then describe the proposed model ATTOrderNet, which is based on the encoder-decoder architecture applying selfattention mechanism as paragraph encoder and a pointer network as the decoder. This combination effectively captures the intrinsic relations across a set of sentences with the desirable property of being invariant to the sentence order, which directly helps address the difficulty of this task."
  }, {
    "heading": "2.1 Problem formulation",
    "text": "The sentence ordering task aims to order a set of sentences as a coherent text. Specifically, a set of n sentences with the order o = [o1, o2, · · · , on] can be described as s = [so1, so2, · · · , son ]. The goal is to find the correct order o∗ for them, o∗ = [o∗1, o∗2, · · · , o∗n], with which the whole sentences have the highest coherence probability:\nP(o∗ |s) > P(o|s), ∀o ∈ ψ (1) where o indicates any order of these sentences and ψ denotes the set of all possible orders. For instance, in Table 1, the current order o is [4, 1, 3, 2] and o∗ = [1, 2, 3, 4] is the correct order for these sentences."
  }, {
    "heading": "2.2 Intuition and Model architecture",
    "text": "Given a set of sentences, the existing hierarchical RNN-based models first transform each sentence into a distributed vector with a sentence encoder and then these sentence embeddings are fed to a LSTMs-based paragraph encoder. Consequently, based on the learned paragraph vector, a pointer network is exploited to decode the order of the input sentences. However, since LSTM works sequentially while the order of these sentences is unknown and quite possibly wrong in this problem, LSTMs-based paragraph encoder has difficulty in capturing a convincing representation through the recurrent connections, which influences the performance of sentence ordering.\nIn this paper, we use self-attention mechanism for paragraph encoder instead. In particular, we employ this mechanism without encoding any positional information of the sentences. Ignoring the current order of the sentences, self-attention based paragraph encoder perfectly meets the requirement of the sentence ordering task and learns a logically reliable representation of the whole paragraph by globally capturing the relationships across sentences. With this learned representation vector, a decoder is then designed to generate a coherent order assignment for the input sentences.\nIn the following, we elaborate on the main building blocks of our ATTOrderNet in details: a sentence encoder, a self-attention mechanism based paragraph encoder, and a decoder."
  }, {
    "heading": "2.3 Sentence Encoder",
    "text": "For a sentence, we first apply word embedding matrix to translate the raw words in the sentence into distributional representations, and then adopt bidirectional LSTMs to learn a sentence-level representation for summarizing its high level semantic concepts.\nSpecifically, assume that a sentence soi containing nw raw words as soi = [w1, · · · ,wnw ], these words are transformed to dense vectors through a word embedding matrix We: xt = Wewt , t ∈ [1, nw]. The sequence of vectors [x1, · · · , xnw ] is then fed into bidirectional LSTMs sequentially to compute a semantic representation of the sentence.\nLong Short-term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) is capable of learning long-term dependencies and alleviating the problems of gradient vanishment and exploding. Here, we adopt bidirectional LSTMs (Bi-LSTMs) to take full advantages of additional backward information and enhance the memory capability. In particular, the Bi-LSTMs contain the forward LSTMs which process the sentence soi from w1 to wnw and backward LSTMs which read soi in the reversed direction:\n−→h t,−→c t = LSTM( −→h t−1,−→c t−1, xt) ←−h t,←−c t = LSTM( ←−h t+1,←−c t+1, xt)\nht = [ −→h t, ←−h t ]\n(2)\nwhere ht denotes the representation of position t by concatenating the forward hidden state\n−→h t and backward hidden state\n←−h t together. The output of the last hidden state of the Bi-LSTMs is\ntaken to be the sentence representation vector as soi = hnw , which incorporates the contextual information from both directions in the sentence.\nSo far, we have obtained a syntactic and semantic representation for a single sentence. In the following, a self-attention based paragraph encoder is proposed to obtain a high level representation for all given sentences by capturing sequential structures and logical relationships among them."
  }, {
    "heading": "2.4 Paragraph Encoder",
    "text": ""
  }, {
    "heading": "2.4.1 Self-attention mechanism",
    "text": "We start by introducing the scaled dot-product attention, which is the foundation of self-attention mechanism used in ATTOrderNet. Given a matrix of n query vectors Q ∈ Rn×d, keys K ∈ Rn×d, and values V ∈ Rn×d, the scaled dot-product attention computes the output matrix as:\nAttention(Q,K,V) = softmax(QK T\n√ d )V (3)\nThe multi-head attention with h parallel heads is employed, where each head is an independent scaled dot-product attention. The mathematical formulation is shown below:\nMi = Attention(QWQi ,KWKi ,VWVi ) (4) MH(Q,K,V) = Concat(M1, · · · ,Mh)W (5)\nwhere WQi ,W K i ,W V i ∈ Rd×da with da = d/h are the projection matrices for the i-th head and W ∈ Rhda×d.\nSelf-attention (Vaswani et al., 2017; Tan et al., 2017; Shen et al., 2017) is a special case of attention mechanism that only requires a single sequence to compute its representation where queries, keys, and values are all from the same place."
  }, {
    "heading": "2.4.2 Self-attention based Paragraph Encoder",
    "text": "The paragraph encoder is composed of multiple self-attention layers followed by an average pooling layer.\nSentence vectors encoded by the sentence encoder are first packed together into a paragraph matrix S = [so1, so2, · · · , son ] as E0. This paragraph matrix S ∈ Rn×d is then fed forward to L self-attention layers, where each layer learns a representation El+1 = U(El) by taking the output\nfrom the previous layer l:\nU(El) = Φ(FN(D(El)),D(El)) (6) D(El) = Φ(MH(El,El,El),El) (7) Φ(v,w) = LayerNorm(v + w) (8)\nFN(x) = ReLU(xWl1 + bl1)Wl2 + bl2 (9)\nwhereΦ(·) performs layer normalization (Ba et al., 2016) on the residual output to preserve the autoregressive property, and FN(·) represents the fully connected feed-forward networks which consists of two linear layers with ReLU nonlinearity in the middle. Wl1 ∈ Rd×d f , bl1 ∈ Rd f ,Wl2 ∈ Rd f ×d, and bl2 ∈ Rd are trainable parameters. We set df = 1024 in all our experiments.\nSelf-attention mechanism adopted in the paragraph encoder directly relate sentences at different positions from the text by computing the attention score (relevance) between each pair of sentences. This allows each sentence to build links with all other sentences in the text, which enables the encoder to exploit latent dependency relationships among sentences without regarding to their input order. Then, attention mechanism uses weighted sum operation to establish a higher level representation for the entire sentence set. As we see, there is no order information used in the encoding process which prevents the model from being affected by the incorrect sentence order. Therefore, self-attention based paragraph encoder is efficient in modeling of dependencies while being invariant to the sentence order.\nThe final paragraph representation v is obtained in the average pooling layer by averaging the output matrix EL ∈ Rn×d from the last self-attention layer: v = 1n ∑n i=1 e\nLi , where n is the number of sentences and eLi denotes the i-th row in EL . This learned representation vector can be viewed as a hierarchical encoding of the entire set of sentences which will then be used as the input of the decoder to perform sentence ordering."
  }, {
    "heading": "2.5 Decoder",
    "text": "The aim of decoder is to predict a consistent order for the input set of sentences.\nFollowing the previous approaches (Gong et al., 2016; Logeswaran et al., 2018), the coherence probability of given sentences s with the order o is formalized as:\nP(o|s) = n∏ i=1 P(oi |oi−1, · · · , o1, s) (10)\nThe higher the probability, the more coherent sentences assignment is.\nTo calculate P(o|s), we employ the pointer network architecture (Vinyals et al., 2015) as our decoder which consists of LSTMs cells (Equation 11-13). The LSTM takes the embedding of the previous sentence as the input to decoder step. During training, the correct order of sentences o∗ is known, so the input sequence [x1, x2, · · · , xn] = [so∗1, so∗2, · · · , so∗n ]. For step i, the input to the decoder is xi−1 = so∗\ni−1 . At test time, the predicted\nsentence assignment sôi−1 is used instead. The initial state of the decoder LSTM is initialized with the final paragraph vector from the encoder: h0 = vT . And the input at the first step in decoder x0 ∈ Rd is a vector of zeros. The mathematical formulation for the i-th step in decoder is as follows:\nhi, ci = LSTM(hi−1, ci−1, xi−1) (11) uij = g\nT tanh(W1so j +W2hi) (12) P(oi |oi−1, · · · , o1, s) = softmax(ui) (13)\nwhere g ∈ Rd, W1 ∈ Rd×d, and W2 ∈ Rd×d are learnable parameters and j ∈ (1, · · · , n). The softmax function normalizes the vector ui ∈ Rn to produce an output distribution over all input sentences. And P(oi |oi−1, · · · , o1, s) can be interpreted as the coherence probability for the current output sequence when soi being the sentence choice at position i conditioned on the previous sentences assignment. Order Prediction: The predicted order ô = [ô1, ô2, · · · , ôn] is the one with the highest coherence probability:\nô = argmax o P(o|s) (14)\nIn this work, we use beam search strategy to find a sub optimal result."
  }, {
    "heading": "2.6 Training",
    "text": "For each ordered document, we use one random permutation of sentences as the input sample at each epoch during the training and testing process. Assume that there are K documents in the training set. We define (qj, yj)Kj=1, where yj is in the correct order o∗ of original document j and qj denotes the set of sentences with a specific permutation of yj . P(yj |qj) = P(o∗ |s = qj) can be interpreted as the probability that sentences are assigned in the correct order when given sentences qj .\nWe aim to train the overall model to maximize this probability by minimizing the loss function:\nL = − 1 K K∑ j=1 log P(yj |qj ; θ) + λ 2 | |θ | |22 (15)\nwhere θ represents all trainable parameters in the networks and λ is a regularization parameter."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Datasets",
    "text": "Accident, Earthquake: two datasets obtained from (Barzilay and Lapata, 2008). The first one is a collection of aviation accident reports from the National Transportation Safety Board and the second one comprises Associated Press articles related to earthquake. Since the original datasets do not provide validation samples, we follow the setup in (Louis and Nenkova, 2012; Li and Hovy, 2014) and use 10-fold cross validation on the training data. NIPS abstract, AAN abstract, NSF abstract: these three datasets are from (Logeswaran et al., 2018) containing abstracts from NIPS papers, ACL papers, and the NSF Research Award Abstracts dataset respectively. arXiv abstract, SIND caption: we further consider two datasets used in (Gong et al., 2016). The former consists of abstracts from papers on arXiv website (Chen et al., 2016) and the other contains captions from SIND dataset (Huang et al., 2016).\nFurther statistics about seven datasets are illustrated in Table 2."
  }, {
    "heading": "3.2 Training setup",
    "text": "We use pre-trained 100 dimensional GloVe word embeddings (Pennington et al., 2014). And all the out-of-vocabulary words are replaced with <UNK>, whose embeddings are updated during training process. The nltk sentence tokenizer is\nused for word tokenization.1 Parameter optimization is performed using stochastic gradient descent. We adopt Adadelta (Zeiler, 2012) as the optimizer with = 106 and ρ = 0.95. The learning rate is initialized to 1.0, the batch size is 16, and the beam size is set to 64. The hidden layer size of LSTMs in sentence encoder is 256, and is 512 in the decoder. The number of attention layers in the paragraph encoder is 6 for AAN abstract, 4 for NSF abstract and arXiv abstract, and 2 for the rest of datasets. We employ 8 parallel heads throughout all self-attention layers and use L2 weight decay on the trainable variables with regularization parameter λ = 10−5. The model is implemented with TensorFlow2. Hyperparameters are chosen using the validation set."
  }, {
    "heading": "3.3 Sentence Ordering",
    "text": "We first evaluate our model on the sentence ordering task, as proposed by Barzilay and Lapata (2008). Given a set of permuted sentences, our goal is to return the original order for them which is considered to be the most coherent."
  }, {
    "heading": "3.3.1 Baselines",
    "text": "We compare ATTOrderNet against a random baseline and all the competing models. These baseline methods can be categorized into three classes and results are reported in (Soricut and Marcu, 2006; Gong et al., 2016; Logeswaran et al., 2018). (1) Traditional approaches: Probabilistic Model (Lapata, 2003); Content Model (Barzilay and Lee, 2004); Utility-Trained model (Soricut and Marcu, 2006); Entity Grid (Barzilay and Lapata, 2008). These four methods employ handcrafted features in modeling the document structure. (2) Data-driven methods: Window network (Li and Hovy, 2014); Seq2seq (Li and Jurafsky,\n1NLTK implementation: http://www.nltk.org/ 2https://www.tensorflow.org/\n2017); Pairwise Ranking Model (Chen et al., 2016). These three approaches capture the local coherence of text based on the neural networks. (3) Hierarchical RNN-based models: VarientLSTM+PtrNet, RNN Decoder (Logeswaran et al., 2018); CNN+PtrNet, LSTM+PtrNet (Gong et al., 2016). These architectures adopt RNN based approaches to obtain the representation for the input set of sentences and employ the pointer network as the decoder to predict order. The main difference between ATTOrderNet and these models lies in the design of paragraph encoder.\nFor thorough comparison, besides the models proposed in the existing literature, we further implement two variants of ATTOrderNet. ATTOrderNet (ATT): The sentence encoder in this model is also entirely based on self-attention mechanism with 4 self-attention layers and 5 heads. Different from the paragraph encoder, the positional encoding method proposed by Vaswani et al. (2017) is applied here to encode temporal information of each input word. ATTOrderNet (CNN): This model employs convolutional neural networks to model sentences. In experiment, the number of feature maps is set to 512 and the width of convolution filter is 4."
  }, {
    "heading": "3.3.2 Evaluation Metrics",
    "text": "To provide assessments on the quality of the orderings we predict in this task, we use the following three metrics: Kendall’s tau (τ): Kendall’s tau is one of the most\nfrequently used metrics for the automatic evaluation of document coherence (Lapata, 2003; Logeswaran et al., 2018; Li and Jurafsky, 2017). It could be formalized as: τ = 1 − 2× (number of inversions) / (n 2 ) , where n is the length of the sequence and the number of inversions denotes the number of pairs in the predicted sequence with incorrect relative order. This metric ranges from -1 (the worst) to 1 (the best). Accuracy (Acc): We follow (Logeswaran et al., 2018) in employing Accuracy to measure how often the absolute position of a sentence was correctly predicted. Compared to τ, it penalizes correctly predicted subsequences that are shifted. Perfect Match Ratio (PMR): Perfect match ratio (Gong et al., 2016) is the most stringent measurement in this task. It calculates the radio of exactly matching orders: PMR= 1K ∑K i=1 1(̂oi = oi∗), where ôi and oi∗ are predicted and correct orders of the i-th text respectively."
  }, {
    "heading": "3.3.3 Results",
    "text": "The experimental results on all datasets are reported in Table 3. Results show that ATTOrderNet gives the best performance across most datasets and under most evaluation measurements.\nThe improvement is regardless of data sizes. In particular, for smaller datasets such as Accident and Earthquake datasets, ATTOrderNet outperforms the previous best baseline methods by 6% and 7% tau score respectively. As for medium size datasets including NIPS abstract and AAN\nabstract, ATTOrderNet shows absolute improvements of 4.54% and 5.03% accuracy score over the previous state-of-the-art. Such finding is consistent across larger datasets. ATTOrderNet outperforms the previous state-of-the-art systems by 4.50% accuracy score with 3% tau score on NSF abstract, 1.75% PMR score with 1% tau score on arXiv abstract, and 1.67% PMR score with 1% tau score on SIND caption. Interestingly, ATTOrderNet reaches 42.19% PMR score on arXiv abstract, which means that more than 2/5 texts in the test set can be ordered exactly right. This performance clearly demonstrates the adaptability and flexibility of the proposed model.\nAs shown in Table 3, ATTOrderNet performs much better than data-driven methods by a significant margin on all corresponding datasets. It proves the importance of exploiting the context by self-attention mechanism as these competing models only consider the local coherence in the text. Among the traditional ordering approaches, Content Model (Barzilay and Lee, 2004) representing topics as states and capturing possible orderings for global coherence performs better than other methods with the tau score of 0.81 on Earthquake dataset, which also demonstrates that global context is important to sentence ordering. However, Content Model requires manual feature engineering that costs great human efforts. In contrast, the self-attention mechanism used in ATTOrderNet directly captures the global dependences for the whole text while requiring no linguistic knowledge anymore and enables ATTOrderNet to further improve tau score to 0.92 on the same dataset.\nIn addition, hierarchical RNN-based models capture the global coherence among sentences with LSTMs and outperform the traditional methods and data-driven approaches in most cases. However, these models still suffer from the permutation of sentences within the document since LSTM works sequentially. ATTOrderNet achieves superior performances to them by adopting the self-attention mechanism to reduce the influence of the permutation of sentences.\nFurther, ATTOrderNet (CNN) has better performances than ATTOrderNet (ATT) on most of the datasets. We conjecture that this is due to the limitation of data size. Since ATTOrderNet (ATT) applies self-attention mechanism in both sentence and paragraph encoders requiring more data to train the model, however the size of the\ndatasets used in this task is smaller than those in other tasks such as document classification (Yang et al., 2016). Given larger datasets in the future, we believe ATTOrderNet (ATT) would perform much better. Among three sentence encoders, ATTOrderNet presents a superior performance across the board. This indicates that LSTM is more efficient in learning semantic representation for sentence level in this task. ATTOrderNet becomes more competitive through combining both advantages of LSTMs and self-attention mechanism.\nSince the first and the last sentences of the text are more special to discern (Chen et al., 2016; Gong et al., 2016), we also evaluate the ratio of correctly predicting the first and the last sentences. Table 4 summarizes our performances on arXiv abstract and SIND caption. As we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one. It is observed that ATTOrderNet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets."
  }, {
    "heading": "3.3.4 Visualization of attention",
    "text": "The aim of this section is to visualize the relationship between sentences captured by self-attention mechanism and understand how it helps perform the sentence ordering task. A technique for visualizing attention mechanism in neural networks is proposed by Vaswani et al. (2017) 3. Inspired by this work, we select a text from AAN abstract dataset to visualize the hierarchical attention layer from the paragraph encoder of ATTOrderNet in Figure 2. Different from visualizing the dependencies of one word with the other words in the sentence (Vaswani et al., 2017), our visualization\n3https://github.com/tensorflow/tensor2tensor\nshows the dependencies between each sentence and all other sentences in the text.\nFor the example in Figure 2, the left text is the input sample to our model which contains a set of permuted sentences with the correct order besides it. The right side is a copy of the input text, which is presented for showing the relevance between each pair of sentences more clearly. The line with grey color on the right text is an example sentence chosen to visualize the attention weights with other sentences. On the top of the text are 8 colored squares representing 8 different attention heads used in the paragraph encoder. Colored columns on the left text show the performance of their corresponding heads. The darkness of the color in column denotes the normalized distribution of the attention weight for the example sentence in the head. Sentences in darker shades show more attention weight which reflects stronger links they have with the example sentence.\nIn particular, Figure 2 shows the attention distribution for the first sentence in the original document. We present the weight distribution in four heads as an instance. It is interesting to see that all of them showing significant higher attention weights on the true second sentence “However, text use changes ...” than other sentences in the text. This indicates that these heads are able to learn the latent dependency relationships from sentences and can successfully distinguish which one is the true next following among all sentence candidates. These heads build much stronger links between this sentence with the chosen one in order to keep structural information for higher level representation, such as paragraph representation."
  }, {
    "heading": "3.4 Order Discrimination",
    "text": "In this section, we assess ATTOrderNet on another common evaluation task which is usually\nadopted in the existing literature: order discrimination task.\nOrder discrimination (Barzilay and Lapata, 2008; Elsner and Charniak, 2011, 2008) aims to compare a document to a randomly permuted version of it. Models are evaluated with Pairwise Accuracy: the ratio of correctly identifying the original document with higher coherence probability (defined in Equation 10) than the probability of its permutation.\nAmong seven datasets mentioned above, we use two of them to assess the performance of ATTOrderNet on the order discrimination task: Accident and Earthquake datasets. These two have been widely used for this task in the previous literature (Li and Hovy, 2014; Logeswaran et al., 2018). This gives us the convenience of directly comparing the result of the proposed model against the reported results. Following the setup in (Barzilay and Lapata, 2008), a maximum of 20 random permutations were generated for each training and testing article to create the pairwise data. There are 1986 and 1956 test pairs in Accident and Earthquake datasets respectively."
  }, {
    "heading": "3.4.1 Baselines",
    "text": "To demonstrate that ATTOrderNet truly improves the order discrimination performance, we compare ATTOrderNet with the following representative models: Graph from (Guinaudeau and Strube, 2013), HMM and HMM+Entity from (Louis and Nenkova, 2012), Entity Grid from (Barzilay and Lapata, 2008), Recurrent and Recursive from (Li and Hovy, 2014), Discriminative model from (Li and Jurafsky, 2017), Varient-LSTM+PtrNet from (Logeswaran et al., 2018), CNN+PtrNet and LSTM+PtrNet from (Gong et al., 2016). The results of the last two methods were obtained by training their models on two datasets."
  }, {
    "heading": "3.4.2 Results",
    "text": "Table 5 reports the results of ATTOrderNet and currently competing architectures in this evaluation task. ATTOrderNet also achieves the stateof-the-art performance, showing a remarkable advancement of about 1.8% gain on Accident dataset and further improving the pairwise accuracy to 99.8 on Earthquake dataset.\nLSTM+PtrNet and CNN+ PtrNet (Gong et al., 2016) fall short of Varient-LSTM+PtrNet (Logeswaran et al., 2018) in performance. This could also be blamed for their paragraph encoder. Documents in both datasets are much longer than those in others, which brings more trouble for LSTMs in paragraph encoder to build logical representations. Compared to the result in the sentence ordering task, Entity Grid (Barzilay and Lapata, 2008) achieves a good performance in this task and even outperforms Recurrent neural networks and Recursive neural networks (Li and Hovy, 2014) on Accident dataset. However, Entity Grid requires hand-engineered features and heavily relies on linguistic knowledge which restrain the model to be adapted to other tasks."
  }, {
    "heading": "4 Conclusion",
    "text": "In this paper, we develop a novel deep attentive sentence ordering model (referred as ATTOrderNet) integrating self-attention mechanism with LSTMs. It enables us to directly capture logical relationships among sentences regardless of their\ninput order and obtain a reliable representation of the sentence set. With this representation, a pointer network is applied to generate an ordered sequence. ATTOrderNet is evaluated on Sentence Ordering and Order Discrimination tasks. The experimental results demonstrate its effectiveness and show promising improvements over existing models across most datasets."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by National Natural Science Foundation of China (No. 61702448, 61672456) and the Fundamental Research Funds for the Central Universities (No. 2017QNA5008, 2017FZA5007). We thank all reviewers for their valuable comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Layer normalization",
    "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton."],
    "venue": "arXiv preprint arXiv:1607.06450.",
    "year": 2016
  }, {
    "title": "Inferring strategies for sentence ordering in multidocument news summarization",
    "authors": ["Regina Barzilay", "Noemie Elhadad."],
    "venue": "Journal of Artificial Intelligence Research, 17:35–55.",
    "year": 2002
  }, {
    "title": "Modeling local coherence: An entity-based approach",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Computational Linguistics, 34(1):1–34.",
    "year": 2008
  }, {
    "title": "Catching the drift: Probabilistic content models, with applications to generation and summarization",
    "authors": ["Regina Barzilay", "Lillian Lee."],
    "venue": "HLT-NAACL, pages 113–120.",
    "year": 2004
  }, {
    "title": "Neural sentence ordering",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "arXiv preprint arXiv:1607.06952.",
    "year": 2016
  }, {
    "title": "Coreference-inspired coherence modeling",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "ACL, pages 41–44.",
    "year": 2008
  }, {
    "title": "Extending the entity grid with entity-specific features",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "ACL, pages 125–129.",
    "year": 2011
  }, {
    "title": "Extractive multi-document summarization with integer linear programming and support vector regression",
    "authors": ["Dimitrios Galanis", "Gerasimos Lampouras", "Ion Androutsopoulos."],
    "venue": "COLING, pages 911– 926.",
    "year": 2012
  }, {
    "title": "End-to-end neural sentence ordering using pointer network",
    "authors": ["Jingjing Gong", "Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "arXiv preprint arXiv:1611.04953.",
    "year": 2016
  }, {
    "title": "Graph-based local coherence modeling",
    "authors": ["Camille Guinaudeau", "Michael Strube."],
    "venue": "ACL, volume 1, pages 93–103.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Conceptto-text generation via discriminative reranking",
    "authors": ["Ioannis Konstas", "Mirella Lapata."],
    "venue": "ACL, pages 369–378. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Unsupervised concept-to-text generation with hypergraphs",
    "authors": ["Ioannis Konstas", "Mirella Lapata."],
    "venue": "NAACL, pages 752–761. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Inducing document plans for concept-to-text generation",
    "authors": ["Ioannis Konstas", "Mirella Lapata."],
    "venue": "EMNLP, pages 1503–1514.",
    "year": 2013
  }, {
    "title": "Probabilistic text structuring: Experiments with sentence ordering",
    "authors": ["Mirella Lapata."],
    "venue": "ACL, pages 545–552. Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "A model of coherence based on distributed sentence representation",
    "authors": ["Jiwei Li", "Eduard Hovy."],
    "venue": "EMNLP, pages 2039–2048.",
    "year": 2014
  }, {
    "title": "Neural net models of open-domain discourse coherence",
    "authors": ["Jiwei Li", "Dan Jurafsky."],
    "venue": "EMNLP, pages 198–209.",
    "year": 2017
  }, {
    "title": "Sentence ordering and coherence modeling using recurrent neural networks",
    "authors": ["Lajanugen Logeswaran", "Honglak Lee", "Dragomir R. Radev."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "A coherence model based on syntactic patterns",
    "authors": ["Annie Louis", "Ani Nenkova."],
    "venue": "EMNLPCONLL, pages 1157–1168. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
    "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."],
    "venue": "AAAI, pages 3075–3081.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Disan: Directional self-attention network for rnn/cnnfree language understanding",
    "authors": ["Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Shirui Pan", "Chengqi Zhang."],
    "venue": "arXiv preprint arXiv:1709.04696.",
    "year": 2017
  }, {
    "title": "Discourse generation using utility-trained coherence models",
    "authors": ["Radu Soricut", "Daniel Marcu."],
    "venue": "COLING/ACL, pages 803–810. Association for Computational Linguistics.",
    "year": 2006
  }, {
    "title": "Deep semantic role labeling with self-attention",
    "authors": ["Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi."],
    "venue": "arXiv preprint arXiv:1712.01586.",
    "year": 2017
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "NIPS, pages 6000–6010.",
    "year": 2017
  }, {
    "title": "Retrieval-based question answering for machine reading evaluation",
    "authors": ["Suzan Verberne."],
    "venue": "CLEF (Notebook Papers/Labs/Workshop).",
    "year": 2011
  }, {
    "title": "Pointer networks",
    "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."],
    "venue": "NIPS, pages 2692–2700.",
    "year": 2015
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "NAACL, pages 1480–1489.",
    "year": 2016
  }, {
    "title": "Qanet: Combining local convolution with global self-attention for reading comprehension",
    "authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V Le."],
    "venue": "arXiv preprint arXiv:1804.09541.",
    "year": 2018
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701.",
    "year": 2012
  }],
  "id": "SP:0c7e41c308d05d6a7d5589e49ea85f4c2c2e0953",
  "authors": [{
    "name": "Baiyun Cui",
    "affiliations": []
  }, {
    "name": "Yingming Li",
    "affiliations": []
  }, {
    "name": "Ming Chen",
    "affiliations": []
  }, {
    "name": "Zhongfei Zhang",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose a novel deep attentive sentence ordering network (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs in the encoding of input sentences. It enables us to capture global dependencies among sentences regardless of their input order and obtains a reliable representation of the sentence set. With this representation, a pointer network is exploited to generate an ordered sequence. The proposed model is evaluated on Sentence Ordering and Order Discrimination tasks. The extensive experimental results demonstrate its effectiveness and superiority to the state-ofthe-art methods.",
  "title": "Deep Attentive Sentence Ordering Network"
}