{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 444–451 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Visual Reasoning (Antol et al., 2015; Andreas et al., 2016; Bisk et al., 2016; Johnson et al., 2017) requires a sophisticated understanding of the compositional language instruction and its relationship with the corresponding image. Suhr et al. (2017) recently proposed a challenging new NLVR task and dataset in this direction with natural and complex language statements that have to be classified as true or false given a multi-image set (shown in Fig. 1). Specifically, each task instance consists of an image with three sub-images and a statement which describes the image. The model is asked to answer the question whether the given statement is consistent with the image or not.\nTo solve the task, the designed model needs to fuse the information from two different domains,\nthe visual objects and the language, and learn accurate relationships between the two. Another difficulty is that the objects in the image do not have a fixed order and the number of objects also varies. Moreover, each statement reasons for truth over three sub-images (instead of the usual single image setup), which also breaks most of the existing models. In our paper, we introduce a novel end-to-end model to address these three problems, leading to strong gains over the previous best model. Our pointer network based LSTM-RNN sorts and learns recurrent representations of the objects in each sub-image, so as to match it better with the order of the phrases in the language statement. For this, it employs an RL-based policy gradient method with a reward extracted from the subsequent comprehension model. With these strong representations of the visual objects and the statement units, a joint-bidirectional attention flow model builds consistent, two-way matchings between the representations in different domains. Finally, since the scores computed by the bidirectional attention are about the three sub-images, a pooling combination layer over the three subimage representations is required to give the final score of the whole image.\nOn the structured-object-representation version of the dataset, our pointer-based, end-to-end bidi-\n444\nrectional attention model achieves an accuracy of 73.9%, outperforming the previous (end-to-end) state-of-the-art method by 6.2% absolute, where both the pointer network and the bidirectional attention modules contribute significantly. We also contribute several other strong baselines for this new NLVR task based on Relation Networks (Santoro et al., 2017) and BiDAF (Seo et al., 2016). Furthermore, we also show the result of our joint bidirectional attention model on the raw-image version (with pixel-level, spatial-filter CNNs) of the NLVR dataset, where our model achieves an accuracy of 69.7% and outperforms the previous best result by 3.6%. On the unreleased leaderboard test set, our model achieves an accuracy of 71.8% and 66.1% on the structured and raw-image versions, respectively, leading to 4% absolute improvements on both tasks."
  }, {
    "heading": "2 Related work",
    "text": "Besides the NLVR corpus with a focus on complex and natural compositional language (Suhr et al., 2017), other useful visual reasoning datasets have been proposed for navigation and assembly tasks (MacMahon et al., 2006; Bisk et al., 2016), as well as for visual Q&A tasks which focus more on complex real-world images (Antol et al., 2015; Johnson et al., 2017). Specifically for the NLVR dataset, previous models have incorporated property- and count-based features of the objects and the language (Suhr et al., 2017), or extra semantic parsing (logical form) annotations (Goldman et al., 2017) – we focus on end-toend models for this visual reasoning task.\nAttention mechanism (Bahdanau et al., 2014; Luong et al., 2015; Xu et al., 2015) has been widely used for conditioned language generation tasks. It is further used to learn alignments between different modalities (Lu et al., 2016; Wang and Jiang, 2016; Seo et al., 2016; Andreas et al., 2016; Chaplot et al., 2017). In our work, a bidirectional attention mechanism is used to learn a joint representation of the visual objects and the words by building matchings between them.\nPointer network (Vinyals et al., 2015) was introduced to learn the conditional probability of an output sequence. Bello et al. (2016) extended this to near-optimal combinatorial optimization via reinforcement learning. In our work, a policy gradient based pointer network is used to “sort” the objects conditioned on the statement, such that the sequence of ordered objects is sent to the subse-\nquent comprehension model for a reward."
  }, {
    "heading": "3 Model",
    "text": "The training datum for this task consists of the statement s, the structured-representation objects o in the image I , and the ground truth label y (which is 1 for true and 0 for false). Our BiATTPointer model (shown in Fig. 2) for the structuredrepresentation task uses the pointer network to sort the object sequence (optimized by policy gradient), and then uses the comprehension model to calculate the probability P (s, o) of the statement s being consistent with the image. Our CNNBiATT model for the raw-image I dataset version is similar but learns the structure directly via pixellevel, spatial-filter CNNs – details in Sec. 5 and the appendix. In the remainder of this section, we first describe our BiATT comprehension model and then the pointer network."
  }, {
    "heading": "3.1 Comprehension Model with Joint Bidirectional Attention",
    "text": "We use one bidirectional LSTM-RNN (Hochreiter and Schmidhuber, 1997) (denoted by LANGLSTM) to read the statement s = w1, w2, . . . , wT, and output the hidden state representations {hi}. A word embedding layer is added before the LSTM to project the words to high-dimension vectors {w̃i}. h1,h2, . . . , hT = LSTM (w̃1, w̃2, . . . , w̃T) (1)\nThe raw features of the objects in the j-th subimage are {ojk} (since the NLVR dataset has 3 subimages per task). A fully-connected (FC) layer without nonlinearity projects the raw features to object embeddings {ejk}. We then go through all the objects in random order (or some learnable order, e.g., via our pointer network, see Sec. 3.2) by another bidirectional LSTM-RNN (denoted by OBJ-LSTM), whose output is a sequence of vectors {gjk}which is used as the (left plus right memory) representation of the objects (the objects in different sub-images are handled separately):\nejk = W o j k + b (2)\ngj1, g j 2, . . . , g j Nj = LSTM (e j 1, e j 2, . . . , e j Nj ) (3)\nwhere Nj is the number of the objects in jth subimage. Now, we have two vector sequences for the representations of the words and the objects, using which the bidirectional attention then calculates the score measuring the correspondence be-\ntween the statement and the image’s object structure. To simplify the notation, we will ignore the sub-image index j. We first merge the LANGLSTM hidden outputs {hi} and the object-aware context vectors {ci} together to get the joint representation {ĥi}. The object-aware context vector ci for a particular word wi is calculated based on the bilinear attention between the word representation hi and the representations of the objects {gk}:\nαi,k = softmaxk (h ᵀ i B1 gk) (4)\nci = ∑\nk\nαi,k · gk (5)\nĥi = relu (WLANG [hi; ci; hi−ci; hi◦ci]) (6) where the symbol ◦ denotes element-wise multiplication.\nImprovement over BiDAF The BiDAF model of Seo et al. (2016) does not use a full objectto-words attention mechanism. The query-todocument attention module in BiDAF added the attended-context vector to the document representation instead of the query representation. However, the inverse attention from the objects to the words is important in our task because the representation of the object depends on its corresponding words. Therefore, different from the BiDAF model, we create an additional ‘symmetric’ attention to merge the OBJ-LSTM hidden outputs {gk} and the statement-aware context vectors {dk} together to get the joint representation {ĝk}. The improvement (6.1%) of our BiATT model over the BiDAF model is shown in Table 1.\nβk,i = softmaxi ( gᵀk B2 hi ) (7)\ndk = ∑\ni\nβk,i · hi (8)\nĝk = relu (WOBJ [gk; dk; gk−dk; gk◦dk]) (9)\nThese above vectors {ĥi} and {ĝk} are the representations of the words and the objects which\nare aware of each other bidirectionally. To make the final decision, two additional bidirectional LSTM-RNNs are used to further process the above attention-based representations via an additional memory-based layer. Lastly, two max pooling layers over the hidden output states create two single-vector outputs for the statement and the sub-image, respectively:\nh̄1, h̄2, . . . , h̄T = LSTM(ĥ1, ĥ2, . . . , ĥT) (10)\nḡ1, ḡ2, . . . , ḡN = LSTM(ĝ1, ĝ2, . . . , ĝN) (11)\nh̄ = ele max i\n{ h̄i }\n(12)\nḡ = ele max k {ḡk} (13)\nwhere the operator ele max denotes the elementwise maximum over the vectors. The final scalar score for the sub-image is given by a 2-layer MLP over the concatenation of h̄ and ḡ as follows:\nscore = W2 tanh ( W1[h̄; ḡ] + b1 ) (14)\nMax-Pooling over Sub-Images In order to address the 3 sub-images present in each NLVR task, a max-pooling layer is used to combine the above-defined scores of the sub-images. Given that the sub-images do not have any specific ordering among them (based on the data collection procedure (Suhr et al., 2017)), a pooling layer is suitable because it is permutation invariant. Moreover, many of the statements are about the existence of a special object or relationship in one sub-image (see Fig. 1) and hence the max-pooling layer effectively captures the meaning of these statements. We also tried other combination methods (meanpooling, concatenation, LSTM, early pooling on the features/vectors, etc.); the max pooling (on scores) approach was the simplest and most effective method among these (based on the dev set).\nThe overall probability that the statement correctly describes the full image (with three subimages) is the sigmoid of the final max-pooled\nscore. The loss of the comprehension model is the negative log probability (i.e., the cross entropy):\nP (s, o) =σ ( max j scorej )\n(15)\nL(s, o, y) =− y logP (s, o) − (1− y) log(1− P (s, o)) (16)\nwhere y is the ground truth label."
  }, {
    "heading": "3.2 Pointer Network",
    "text": "Instead of randomly ordering the objects, humans look at the objects in an appropriate order w.r.t. their reading of the given statement and after the first glance of the image. Following this idea, we use an additional pointer network (Vinyals et al., 2015) to find the best object ordering for the subsequent language comprehension model. The pointer network contains two RNNs, the encoder and the decoder. The encoder reads all the objects in a random order. The decoder then learns a permutation π of the objects’ indices, by recurrently outputting a distribution over the objects based on the attention over the encoder hidden outputs. At each time step, an object is sampled without replacement following this distribution. Thus, the pointer network models a distribution p(π | s, o) over all the permutations:\np(π | s, o) = ∏\ni\np (π(i) | π(< i), s, o) (17)\nFurthermore, the appropriate order of the objects depends on the language statement, and hence the decoder importantly attends to the hidden outputs of the LANG-LSTM (see Eqn. 1).\nThe pointer network is trained via reinforcement learning (RL) based policy gradient optimization. The RL loss LRL(s, o, y) is defined as the expected comprehension loss (expectation over the distribution of permutations):\nLRL(s, o, y) = Eπ∼p(·|s,o)L(s, o[π], y) (18)\nwhere o[π] denotes the permuted input objects for permutation π, and L is the loss function defined in Eqn. 16. Suppose that we sampled a permutation π∗ from the distribution p(π|s, o); then the above RL loss could be optimized via policy gradient methods (Williams, 1992). The reward R is the negative loss of the subsequent comprehension model L(s, o[π∗], y). A baseline b is subtracted from the reward to reduce the variance (we use the\nself-critical baseline of Rennie et al. (2016)). The gradient of the loss LRL could then be approximated as:\nR =− L(s, o[π∗], y) (19) ∇θLRL(s, o, y) ≈ − (R− b)∇θ log p(π∗ | s, o)\n+∇θL(s, o[π∗], y) (20)\nThis overall BiATT-Pointer model (for the structured-representation task) is shown in Fig. 2."
  }, {
    "heading": "4 Experimental Setup",
    "text": "We evaluate our model on the NLVR dataset (Suhr et al., 2017), for both the structured and raw-image versions. All model tuning was performed on the dev set. Given the fact that the dataset is balanced (the number of true labels and false labels are roughly the same), the accuracy of the whole corpus is used as the metric. We only use the raw features of the statement and the objects with minimal standard preprocessing (e.g., tokenization and UNK replacement; see appendix for reproducibility training details)."
  }, {
    "heading": "5 Results and Analysis",
    "text": "Results on Structured Representations Dataset: Table 1 shows our primary model results. In terms of previous work, the state-of-the-art result for end-to-end models is ‘MAXENT’, shown in Suhr et al. (2017).1 Our proposed BiATT-Pointer model (Fig. 2) achieves a 6.2% improvement on the public test set and a 4.0% improvement on the unreleased test set over this SotA model. To show the individual effectiveness of our BiATT and Pointer components, we also provide two ablation results: (1) the bidirectional attention BiATT model without the pointer network; and (2) our BiENC baseline model without any attention or the pointer mechanisms. The BiENC model uses the similarity between the last hidden outputs of the LANGLSTM and the OBJ-LSTM as the score (Eqn. 14).\nFinally, we also reproduce some recent popular frameworks, i.e., Relationship Network (Santoro et al., 2017) and BiDAF model (Seo et al., 2016), which have been proven to be successful in other machine comprehension and visual reasoning tasks. The results of these models are weaker than our proposed model. Reimplementation details are shown in the appendix.\n1There is also recent work by Goldman et al. (2017), who use extra, manually-labeled semantic parsing data to achieve a released/unreleased test accuracy of 80.4%/83.5%, resp."
  }, {
    "heading": "Negative Examples",
    "text": "Results on Raw Images Dataset: To further show the effectiveness of our BiATT model, we apply this model to the raw image version of the NLVR dataset, with minimal modification. We simply replace each object-related LSTM with a visual feature CNN that directly learns the structure via pixel-level, spatial filters (instead of a pointer network which addresses an unordered sequence of structured object representations). As shown in Table 1, this CNN-BiATT model outperforms the neural module networks (NMN) (Andreas et al., 2016) previous-best result by 3.6% on the public test set and 4.1% on the unreleased test set. More details and the model figure are in the appendix. Output Example Analysis: Finally, in Fig. 1, we show some output examples which were successfully solved by our BiATT-Pointer model but failed in our strong baselines. The left two examples in Fig. 1 could not be handled by the BiENC model. The right two examples are incorrect for the BiATT model without the ordering-based pointer network. Our model can quite successfully understand the complex meanings of the attributes and their relationships with the diverse objects, as well as count the occurrence of and reason over objects without any specialized features.\nNext, in Fig. 3, we also show some negative examples on which our model fails to predict the correct answer. The top two examples involve com-\nplex high-level phrases e.g., “touching any edge” or “touching the base”, which are hard for an endto-end model to capture, given that such statements are rare in the training data. Based on the result of the validation set, the max-pooling layer is selected as the combination method in our model. The max-pooling layer will choose the highest score from the sub-images as the final score. Thus, the layer could easily handle statements about single-subimage-existence based reasoning (e.g., the 4 positively-classified examples in Fig. 1). However, the bottom two negatively-classified examples in Fig. 3 could not be resolved because of the limitation of the max-pooling layer on scenarios that consider multiple-subimage-existence. We did try multiple other pooling and combination methods, as mentioned in Sec. 3.1. Among these methods, the concatenation, early pooling and LSTM-fusion approaches might have the ability to solve these particular bottom-two failed statements. In our future work, we are addressing multiple types of pooling methods jointly."
  }, {
    "heading": "6 Conclusion",
    "text": "We presented a novel end-to-end model with joint bidirectional attention and object-ordering pointer networks for visual reasoning. We evaluate our model on both the structured-representation and raw-image versions of the NLVR dataset and achieve substantial improvements over the previous end-to-end state-of-the-art results."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their helpful comments. This work was supported by a Google Faculty Research Award, a Bloomberg Data Science Research Grant, an IBM Faculty Award, and NVidia GPU awards."
  }, {
    "heading": "A Supplementary Material",
    "text": ""
  }, {
    "heading": "A.1 CNN-BiATT Model Details",
    "text": "As shown in Fig. 4, we apply our BiATT model to the raw image dataset with minimal modification. The visual input of the model for this task is changed from the unordered structured representation set of objects o to the raw image pixels I . Hence, we replace all object-related LSTMs (e.g., the OBJ-LSTM and the LSTM-RNN in the bidirectional attention in Fig. 2) with visual feature convolutional neural networks (CNNs) that directly learn the structure via pixel-level, spatial filters (instead of a pointer network which addresses an unordered sequence of structured object representations).\nThe training datum for the NLVR raw-image version consists of the statement s, the image I and the ground truth label y. The image I contains three sub-images x1, x2 and x3. We will use x to indicate any sub-image. The superscript which indicates the index of the sub-image is ignored to simplify the notation. The representation of the statement {hi} is calculated by the LANGLSTM as before. For the image representation, we project the sub-image to a sequence of feature vectors (i.e., the feature map) {al : l = 1, . . . , L} corresponding to the different image locations. L = m × m is the size of the features and m is the width and height of the feature map. The projection consists of ResNet-V2-101 (He et al., 2016) and a following fully-connected (FC) layer. We only use the blocks in the ResNet before the average pooling layer and the output of the ResNet is a feature map of size m×m×2048.\nf1, . . . , fL = ResNet(x) (21)\nal = relu(Wx fl + bx) (22)\nThe joint-representation of the statement {ĥi} is the combination of the LANG-LSTM hidden output states {hi} and the image-aware context vectors {ci}:\nαi,l = softmaxl (h ᵀ i B1 al) (23)\nci = ∑\nl\nαi,l · al (24)\nĥi = relu (WLANG [hi; ci; hi−ci; hi◦ci]) (25)\nThe joint-representation of the image {âl} is cal-\nculated in the same way:\nβl,i = softmaxi ( aᵀl B2 hi ) (26)\ndl = ∑\ni\nβl,i · hi (27)\nâl = relu (WIMG [al; dl; al−dl; al◦dl]) (28)\nThe joint-representation of the statement is further processed by a LSTM-RNN. Different from our BiATT model, a 3-layers CNN is used for modeling the joint-representation of the image {âl}. The output of the CNN layer is another feature map {āl}. Each CNN layer has kernel size 3 × 3 and uses relu as the activation function, and then we finally use element-wise max operator similar to Sec. 3.1:\nh̄1, h̄2, . . . , h̄T = LSTM(ĥ1, ĥ2, . . . , ĥT) (29)\nā1, ā2, . . . , āL’ = CNN(â1, â2, . . . , âL) (30)\nh̄ = ele max i\n{ h̄i }\n(31)\nā = ele max l {āl} (32)\nAt last, we use the same method as our BiATT model to calculate the score and the loss function:\nscore(s, x) =W2 tanh ( W1[h̄; ā] + b1 ) (33)\nP (s, I) =σ ( max j score(s, xj) ) (34)\nL(s, I, y) =− y logP (s, I) − (1− y) log(1− P (s, I)) (35)"
  }, {
    "heading": "A.2 Reimplementation Details for Relationship Network and BiDAF Models",
    "text": "We reimplement a Relationship Network (Santoro et al., 2017), using a three-layer MLP with\n256 units per layer in the G-net and a three-layer MLP consisting of 256, 256 (with 0.3 dropout), and 1 units with ReLU nonlinearities for F-net. We also reimplement a BiDAF model (Seo et al., 2016) using 128-dimensional word embedding, 256-dimensional LSTM-RNN and 0.3 dropout rate. A max pooling layer on top of the modeling layer of BiDAF is used to merge the hidden outputs to a single vector."
  }, {
    "heading": "A.3 Experimental Setup and Training Details for Our BiATT-Pointer, BiENC, and CNN-BiATT Models",
    "text": ""
  }, {
    "heading": "A.3.1 BiATT-Pointer",
    "text": "For preprocessing, we replace the words whose occurrence is less than 3 with the “UNK” token. We create a 9 dimension vector as the feature of each object. This feature contains the location (x, y) in 2D coordinate, the size of the object and two 3-dimensional hot vectors for the shape and the color. The (x, y) coordinates are normalized to the range [−1, 1].\nFor the model hyperparameters (all lightly tuned on dev set), the dimension of the word embedding is 128, and the number of units in an LSTM cell is 256. The word embedding is trained from scratch. The object feature is projected to a 64-dimensional vector. The dimensions of joint representation ĥi and ĝk are both 512. The first fully-connected layer in calculating the subimages score has 512 units. All the trainable variables are initialized with the Xavier initializer. To regularize the training process, we add a dropout rate 0.3 to the hidden output of the LSTM-RNNs and before the last MLP layer which calculates the score for sub-images. We also clip the gradients by their norm to avoid gradient exploding. The losses are optimized by a single Adam optimizer and the learning rate is fixed at 1e-4.\nFor the pointer network, we sample the objects following the distribution of the objects at each decoder step during training. In inference, we select the object with maximum probability. We use the self-critical baseline (Rennie et al., 2016) to stabilize the RL training, where the final score in inference (choosing object with maximum probability) is subtracted from the reward. To reduce the number of parameters, we share the weight of the fully-connected layer which projects the raw object feature to the high dimensional vector in the pointer encoder, the pointer decoder, and the OBJ-\nLSTM. The pointer decoder attends to the hidden outputs of the LANG-LSTM using bilinear attention (Luong et al., 2015)."
  }, {
    "heading": "A.3.2 CNN-BiATT",
    "text": "We initialize our model with weights of the public pretrained ResNet-V2-101 (based on the ImageNet dataset) and freeze it during training. The ResNet projects the sub-image to a feature map of 10× 10 × 2048. The feature map is normalized to a mean of 0 and a standard deviation of 1 before feeding into the FC layer. The fully connected layer after the ResNet has 512 units. Each layer of the 3-layers CNN in the bidirectional attention has kernel size 3× 3 with 512 filters and no padding."
  }, {
    "heading": "A.3.3 BiENC",
    "text": "The BiENC model uses LANG-LSTM and OBJLSTM to read the statement and the objects. A bilinear form calculates the similarity between the last hidden outputs of the two LSTM-RNNs. The similarity is directly used as the score of the subimage. The CNN-BiENC model replaces the OBJLSTM with a CNN."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural module networks",
    "authors": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39–48.",
    "year": 2016
  }, {
    "title": "Vqa: Visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425–2433.",
    "year": 2015
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Neural combinatorial optimization with reinforcement learning",
    "authors": ["Irwan Bello", "Hieu Pham", "Quoc V Le", "Mohammad Norouzi", "Samy Bengio."],
    "venue": "arXiv preprint arXiv:1611.09940.",
    "year": 2016
  }, {
    "title": "Natural language communication with robots",
    "authors": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."],
    "venue": "HLT-NAACL, pages 751–761.",
    "year": 2016
  }, {
    "title": "Gatedattention architectures for task-oriented language grounding",
    "authors": ["Devendra Singh Chaplot", "Kanthashree Mysore Sathyendra", "Rama Kumar Pasumarthi", "Dheeraj Rajagopal", "Ruslan Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1706.07230.",
    "year": 2017
  }, {
    "title": "Weaklysupervised semantic parsing with abstract examples",
    "authors": ["Omer Goldman", "Veronica Latcinnik", "Udi Naveh", "Amir Globerson", "Jonathan Berant."],
    "venue": "arXiv preprint arXiv:1711.05240.",
    "year": 2017
  }, {
    "title": "Identity mappings in deep residual networks",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "European Conference on Computer Vision, pages 630–645. Springer.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
    "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick."],
    "venue": "2017 IEEE Conference on Computer Vi-",
    "year": 2017
  }, {
    "title": "Hierarchical question-image coattention for visual question answering",
    "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."],
    "venue": "Advances In Neural Information Processing Systems, pages 289–297.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1508.04025.",
    "year": 2015
  }, {
    "title": "Walk the talk: Connecting language, knowledge, and action in route instructions",
    "authors": ["Matt MacMahon", "Brian Stankiewicz", "Benjamin Kuipers."],
    "venue": "Def, 2(6):4.",
    "year": 2006
  }, {
    "title": "Self-critical sequence training for image captioning",
    "authors": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."],
    "venue": "arXiv preprint arXiv:1612.00563.",
    "year": 2016
  }, {
    "title": "A simple neural network module for relational reasoning",
    "authors": ["Adam Santoro", "David Raposo", "David GT Barrett", "Mateusz Malinowski", "Razvan Pascanu", "Peter Battaglia", "Timothy Lillicrap."],
    "venue": "arXiv preprint arXiv:1706.01427.",
    "year": 2017
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "arXiv preprint arXiv:1611.01603.",
    "year": 2016
  }, {
    "title": "A corpus of natural language for visual reasoning",
    "authors": ["Alane Suhr", "Mike Lewis", "James Yeh", "Yoav Artzi."],
    "venue": "55th Annual Meeting of the Association for Computational Linguistics, ACL.",
    "year": 2017
  }, {
    "title": "Pointer networks",
    "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."],
    "venue": "Advances in Neural Information Processing Systems, pages 2692–2700.",
    "year": 2015
  }, {
    "title": "Machine comprehension using match-lstm and answer pointer",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "arXiv preprint arXiv:1608.07905.",
    "year": 2016
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Reinforcement Learning, pages 5–32. Springer.",
    "year": 1992
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "International Conference on Machine",
    "year": 2015
  }],
  "id": "SP:241026c7f74d66298a29094854f8f8f47bd34409",
  "authors": [{
    "name": "Hao Tan",
    "affiliations": []
  }, {
    "name": "Mohit Bansal",
    "affiliations": []
  }],
  "abstractText": "Visual reasoning with compositional natural language instructions, e.g., based on the newly-released Cornell Natural Language Visual Reasoning (NLVR) dataset, is a challenging task, where the model needs to have the ability to create an accurate mapping between the diverse phrases and the several objects placed in complex arrangements in the image. Further, this mapping needs to be processed to answer the question in the statement given the ordering and relationship of the objects across three similar images. In this paper, we propose a novel end-to-end neural model for the NLVR task, where we first use joint bidirectional attention to build a two-way conditioning between the visual information and the language phrases. Next, we use an RL-based pointer network to sort and process the varying number of unordered objects (so as to match the order of the statement phrases) in each of the three images and then pool over the three decisions. Our model achieves strong improvements (of 4-6% absolute) over the state-of-theart on both the structured representation and raw image versions of the dataset.",
  "title": "Object Ordering with Bidirectional Matchings for Visual Reasoning"
}