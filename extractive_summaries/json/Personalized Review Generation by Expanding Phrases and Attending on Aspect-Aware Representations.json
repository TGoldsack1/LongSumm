{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 706–711 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n706"
  }, {
    "heading": "1 Introduction",
    "text": "Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017). Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, i.e., to discover their nuanced opinions about each of its individual aspects. A successful model could work (for instance) as (a) a highly-nuanced recommender system that tells users their likely reaction to a product in the form of text fragments; (b) a writing tool that helps users ‘brainstorm’ the review-writing process; or (c) a querying system that facilitates personalized natural lan-\nguage queries (i.e., to find items about which a user would be most likely to write a particular phrase). Some recent works have explored the review generation task and shown success in generating cohesive reviews (Dong et al., 2017; Ni et al., 2017; Zang and Wan, 2017). Most of these works treat the user and item identity as input; we seek a system with more nuance and more precision by allowing users to ‘guide’ the model via short phrases, or auxiliary data such as item specifications. For example, a review writing assistant might allow users to write short phrases and expand these key points into a plausible review.\nReview text has been widely studied in traditional tasks such as aspect extraction (Mukherjee and Liu, 2012; He et al., 2017), extraction of sentiment lexicons (Zhang et al., 2014), and aspectaware sentiment analysis (Wang et al., 2016; McAuley et al., 2012). These works are related to review generation since they can provide prior knowledge to supervise the generative process. We are interested in exploring how such knowledge (e.g. extracted aspects) can be used in the review generation task.\nIn this paper, we focus on designing a review generation model that is able to leverage both user and item information as well as auxiliary, textual input and aspect-aware knowledge. Specifically, we study the task of expanding short phrases into complete, coherent reviews that accurately reflect the opinions and knowledge learned from those phrases.\nThese short phrases could include snippets provided by the user, or manifest aspects about the items themselves (e.g. brand words, technical specifications, etc.). We propose an encoderdecoder framework that takes into consideration three encoders (a sequence encoder, an attribute encoder, and an aspect encoder), and one decoder. The sequence encoder uses a gated recurrent unit\n(GRU) network to encode text information; the attribute encoder learns a latent representation of user and item identity; finally, the aspect encoder finds an aspect-aware representation of users and items, which reflects user-aspect preferences and item-aspect relationships. The aspect-aware representation is helpful to discover what each user is likely to discuss about each item. Finally, the output of these encoders is passed to the sequence decoder with an attention fusion layer. The decoder attends on the encoded information and biases the model to generate words that are consistent with the input phrases and words belonging to the most relevant aspects."
  }, {
    "heading": "2 Related Work",
    "text": "Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016; Ghosh et al., 2017), among others. Among these, review generation is characterized by the need to generate long sequences and estimate high-order interactions between users and items.\nSeveral approaches have been recently proposed to tackle these problems. Dong et al. (2017) proposed an attribute-to-sequence (Attr2Seq) method to encode user and item identities as well as rating information with a multi-layer perceptron and a decoder then generates reviews conditioned on this information. They also used an attention mechanism to strengthen the alignment between\noutput and input attributes. Ni et al. (2017) trained a collaborative-filtering generative concatenative network to jointly learn the tasks of review generation and item recommendation. Zang and Wan (2017) proposed a hierarchical structure to generate long reviews; they assume each sentence is associated with an aspect score, and learn the attention between aspect scores and sentences during training. Our approach differs from these mainly in our goal of incorporating auxiliary textual information (short phrases, product specifications, etc.) into the generative process, which facilitates the generation of higher-fidelity reviews.\nAnother line of work related to review generation is aspect extraction and opinion mining (Park et al., 2015; Qiu et al., 2017; He et al., 2017; Chen et al., 2014). In this paper, we argue that the extra aspect (opinion) information extracted using these previous works can effectively improve the quality of generated reviews. We propose a simple but effective way to combine aspect information into the generative model."
  }, {
    "heading": "3 Approach",
    "text": "We describe the review generation task as follows. Given a user u, item i, several short phrases {d1, d2, ..., dM}, and a group of extracted aspects {A1, A2, ..., Ak}, our goal is to generate a review (w1, w2, ..., wT) that maximizes the probability P (w1:T|u, i, d1:M). To solve this task, we propose a method called ExpansionNet which contains two parts: 1) three encoders to leverage the input phrases and aspect information; and 2) a decoder with an attention fusion layer to generate sequences and align the generation with the input\nsources. The model structure is shown in Figure 1."
  }, {
    "heading": "3.1 Sequence encoder, attribute encoder and aspect encoder",
    "text": "Our sequence encoder is a two-layer bi-directional GRU, as is commonly used in sequence-tosequence (Seq2Seq) models (Cho et al., 2014). Input phrases first pass a word embedding layer, then go through the GRU one-by-one and finally yield a sequence of hidden states {e1, e2..., eL}. In the case of multiple phrases, these share the same sequence encoder and have different lengths L. To simplify notation, we only consider one input phrase in this section.\nThe attribute encoder and aspect encoder both consist of two embedding layers and a projection layer. For the attribute encoder, we define two general embedding layers Eu ∈ R|U|×m and Ei ∈ R|I|×m to obtain the attribute latent factors γu and γi; for the aspect encoder, we use two aspect-aware embedding layers E ′ u ∈ R|U|×k and E ′ i ∈ R|I|×k to obtain aspect-aware latent factors βu and βi. Here |U|, |I|, m and k are the number of users, number of items, the dimension of attributes, and the number of aspects, respectively. After the embedding layers, the attribute and aspect-aware latent factors are concatenated and fed into a projection layer with tanh activation. The outputs are calculated as:\nγu = Eu(u), γi = Ei(i) (1) βu = E ′ u(u), βi = E ′ i(i) (2)\nu = tanh(Wu[γu; γi] + bu) (3)\nv = tanh(Wv[βu;βi] + bv) (4)\nwhere Wu ∈ Rn×2m, bu ∈ Rn, Wv ∈ Rn×2k, bv ∈ Rn are learnable parameters and n is the dimensionality of the hidden units in the decoder."
  }, {
    "heading": "3.2 Decoder with attention fusion layer",
    "text": "The decoder is a two-layer GRU that predicts the target words given the start token. The hidden state of the decoder is initialized using the sum of the three encoders’ outputs. The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word. Specifically:\nh0 = eL + u+ v (5)\nht = GRU(wt,ht−1), (6)\nwhere h0 ∈ Rn is the decoder’s initial hidden state and ht ∈ Rn is the hidden state at time-step t.\nTo fully exploit the encoder-side information, we apply an attention fusion layer to summarize the output of each encoder and jointly determine the final word distribution. For the sequence encoder, the attention vector is defined as in many other applications (Bahdanau et al., 2014; Luong et al., 2015):\na1t = L∑ j=1 α1tjej (7)\nα1tj = exp(tanh(v 1 α > (W 1α[ej ;ht] + b 1 α)))/Z,\n(8)\nwhere a1t ∈ Rn is the attention vector on the sequence encoder at time-step t, α1tj is the attention score over the encoder hidden state ej and decoder hidden state ht, and Z is a normalization term.\nFor the attribute encoder, the attention vector is calculated as:\na2t = ∑ j∈u,i α2tjγj (9)\nα2tj = exp(tanh(v 2 α > (W 2α[γj ;ht] + b 2 α)))/Z,\n(10)\nwhere a2t ∈ Rn is the attention vector on the attribute encoder, and α2tj is the attention score between the attribute latent factor γj and decoder hidden state ht.\nInspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:\nsui =Ws[βu;βi] + bs (11) a3t = tanh(W 3 α[sui; et;ht] + b 3 α), (12)\nwhere sui ∈ Rk is the aspect importance considering the interaction between u and i, et is the decoder input after embedding layer at time-step t, and a3t ∈ Rk is a probability vector to bias each aspect at time-step t. Finally, the first two attention vectors are concatenated with the decoder hidden state at time-step t and projected to obtain the output word distribution Pv. The attention scores from the aspect encoder are then directly added to the aspect words in the final word distribution. The output probability for word w at time-step t is given by:\nPv(wt) = tanh(W [ht;a 1 t ;a 2 t ] + b) (13)\nP (wt) = Pv(wt) + a 3 t [k] · 1wt∈Ak , (14)\nwhere wt is the target word at time-step t, a3t [k] is the probability that aspect k will be discussed at time-step t, Ak represents all words belonging to aspect k and 1wt∈Ak is a binary variable indicating whether wt belongs to aspect k.\nDuring inference, we use greedy decoding by choosing the word with maximum probability, denoted as yt = argmaxwtsoftmax(P (wt)). Decoding finishes when an end token is encountered."
  }, {
    "heading": "4 Experiments",
    "text": "We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model. We convert all text into lowercase, add start and end tokens to each review, and perform tokenization using NLTK.1 We discard reviews with length greater than 100 tokens and consider a vocabulary of 30,000 tokens. After preprocessing, the dataset contains 182,850 users, 59,043 items, and 992,172 reviews (sparsity 99.993%), which is much sparser than the datasets used in previous works (Dong et al., 2017; Ni et al., 2017). On average, each review contains 49.32 tokens as well as a short-text summary of 4.52 tokens. In our experiments, the basic ExpansionNet uses these summaries as input phrases. We split the dataset into training (80%), validation (10%) and test sets (10%). All results are reported on the test set."
  }, {
    "heading": "4.1 Aspect Extraction",
    "text": "We use the method2 in (He et al., 2017) to extract 15 aspects and consider the top 100 words from each aspect. Table 2 shows 10 inferred aspects and representative words (inferred aspects are manually labeled). ExpansionNet calculates an attention score based on the user and item aspect-aware representation, then determines how much these representative words are biased in the output word distribution.\n1 https://www.nltk.org/ 2 https://github.com/ruidan/ Unsupervised-Aspect-Extraction"
  }, {
    "heading": "4.2 Experiment Details",
    "text": "We use PyTorch3 to implement our model.4 Parameter settings are shown in Table 1. For the attribute encoder and aspect encoder, we set the dimensionality to 64 and 15 respectively. For both the sequence encoder and decoder, we use a 2- layer GRU with hidden size 512. We also add dropout layers before and after the GRUs. The dropout rate is set to 0.1. During training, the input sequences of the same source (e.g. review, summary) inside each batch are padded to the same length."
  }, {
    "heading": "4.3 Performance Evaluation",
    "text": "We evaluate the model on six automatic metrics (Table 3): Perplexity, BLEU-1/BLEU-4, ROUGEL and Distinct-1/2 (percentage of distinct unigrams and bi-grams) (Li et al., 2016). We compare\n3 http://pytorch.org/docs/master/index.html 4 https://github.com/nijianmo/textExpansion\nagainst three baselines: Rand (randomly choose a review from the training set), GRU-LM (the GRU decoder works alone as a language model) and a state-of-the-art model Attr2Seq that only considers user and item attribute (Dong et al., 2017). ExpansionNet (with summary, item title, attribute and aspect as input) achieves significant improvements over Attr2Seq on all metrics. As we add more input information, the model continues to obtain better results, except for the ROUGE-L metric. This proves that our model can effectively learn from short input phrases and aspect information and improve the correctness and diversity of generated results.\nFigure 2 presents a sample generation result. ExpansionNet captures fine-grained item information (e.g. that the item is a tablet), which Attr2Seq fails to recognize. Moreover, given a phrase like “easy to use” in the summary, ExpansionNet generates reviews containing the same text. This demonstrates the possibility of using our model in an assistive review generation scenario. Finally, given extra aspect information, the model successfully estimates that the screen would be an important aspect (i.e., for the current user and item); it generates phrases such as “screen is very respon-\nsive” about the aspect “screen” which is also covered in the real (ground-truth) review (“display is beautiful”).\nWe are also interested in seeing how the aspectaware representation can find related aspects and bias the generation to discuss more about those aspects. We analyze the average number of aspects in real and generated reviews and show on average how many aspects in real reviews are covered in generated reviews. We consider a review as covering an aspect if any of the aspect’s representative words exists in the review. As shown in Table 4, Attr2Seq tends to cover more aspects in generation, many of which are not discussed in real reviews. On the other hand, ExpansionNet better captures the distribution of aspects that are discussed in real reviews."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2014
  }, {
    "title": "Aspect extraction with automated prior knowledge learning",
    "authors": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "aglar Gülehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"],
    "year": 2014
  }, {
    "title": "Learning to generate product reviews from attributes",
    "authors": ["Li Dong", "Shaohan Huang", "Furu Wei", "Mirella Lapata", "Ming Zhou", "Ke Xu"],
    "year": 2017
  }, {
    "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
    "authors": ["Albert Gatt", "Emiel Krahmer."],
    "venue": "JAIR.",
    "year": 2017
  }, {
    "title": "Affect-lm: A neural language model for customizable affective text generation",
    "authors": ["Sayan Ghosh", "Mathieu Chollet", "Eugene Laksana", "Louis-Philippe Morency", "Stefan Scherer."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "An unsupervised neural attention model for aspect extraction",
    "authors": ["Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "A persona-based neural conversation model",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P. Spithourakis", "Jianfeng Gao", "William B. Dolan."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "Learning attitudes and attributes from multiaspect reviews",
    "authors": ["Julian J. McAuley", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "ICDM.",
    "year": 2012
  }, {
    "title": "Image-based recommendations on styles and substitutes",
    "authors": ["Julian J. McAuley", "Christopher Targett", "Qinfeng Shi", "Anton van den Hengel."],
    "venue": "SIGIR.",
    "year": 2015
  }, {
    "title": "Aspect extraction through semi-supervised modeling",
    "authors": ["Arjun Mukherjee", "Bing Liu."],
    "venue": "ACL.",
    "year": 2012
  }, {
    "title": "Estimating reactions and recommending products with generative models of reviews",
    "authors": ["Jianmo Ni", "Zachary C. Lipton", "Sharad Vikram", "Julian J. McAuley."],
    "venue": "International Joint Conference on Natural Language Processing.",
    "year": 2017
  }, {
    "title": "Retrieval of relevant opinion sentences for new products",
    "authors": ["Dae Hoon Park", "Hyun Duk Kim", "ChengXiang Zhai", "Lifan Guo."],
    "venue": "SIGIR.",
    "year": 2015
  }, {
    "title": "Aspect extraction from product reviews using category hierarchy information",
    "authors": ["Minghui Qiu", "Yinfei Yang", "Cen Chen", "Forrest Sheng Bao."],
    "venue": "EACL.",
    "year": 2017
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "CVPR.",
    "year": 2015
  }, {
    "title": "Attention-based LSTM for aspectlevel sentiment classification",
    "authors": ["Yequan Wang", "Minlie Huang", "Xiaoyan Zhu", "Li Zhao."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Topic aware neural response generation",
    "authors": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."],
    "venue": "AAAI.",
    "year": 2017
  }, {
    "title": "Towards automatic generation of product reviews from aspectsentiment scores",
    "authors": ["Hongyu Zang", "Xiaojun Wan."],
    "venue": "International Conference on Natural Language Generation.",
    "year": 2017
  }, {
    "title": "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis",
    "authors": ["Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma."],
    "venue": "SIGIR.",
    "year": 2014
  }],
  "id": "SP:50421f070a4c3075f1f28c3a130f1674c06cd966",
  "authors": [{
    "name": "Jianmo Ni",
    "affiliations": []
  }, {
    "name": "Julian McAuley",
    "affiliations": []
  }],
  "abstractText": "In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns ‘aspect-aware’ user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model is capable of generating coherent and diverse reviews that expand the contents of input phrases. In addition, the learned aspectaware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.",
  "title": "Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations"
}