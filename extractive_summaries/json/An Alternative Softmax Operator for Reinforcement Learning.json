{
  "sections": [{
    "heading": "1. Introduction",
    "text": "There is a fundamental tension in decision making between choosing the action that has highest expected utility and avoiding “starving” the other actions. The issue arises in the context of the exploration–exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).\nIn reinforcement learning, an approach to addressing the tension is the use of softmax operators for value-function optimization, and softmax policies for action selection. Examples include value-based methods such as SARSA (Rummery & Niranjan, 1994) or expected SARSA (Sutton & Barto, 1998; Van Seijen et al., 2009), and policy-search methods such as REINFORCE (Williams, 1992).\n1Brown University, USA. Correspondence to: Kavosh Asadi <kavosh@brown.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nAn ideal softmax operator is a parameterized set of operators that:\n1. has parameter settings that allow it to approximate maximization arbitrarily accurately to perform reward-seeking behavior;\n2. is a non-expansion for all parameter settings ensuring convergence to a unique fixed point;\n3. is differentiable to make it possible to improve via gradient-based optimization; and\n4. avoids the starvation of non-maximizing actions.\nLet X = x1, . . . , xn be a vector of values. We define the following operators:\nmax(X) = max i∈{1,...,n} xi ,\nmean(X) = 1\nn\nn∑\ni=1\nxi ,\neps (X) = mean(X) + (1− ) max(X) ,\nboltzβ(X) = ∑n i=1 xi e βxi\n∑n i=1 e βxi .\nThe first operator, max(X), is known to be a non-expansion (Littman & Szepesvári, 1996). However, it is non-differentiable (Property 3), and ignores non-maximizing selections (Property 4).\nThe next operator, mean(X), computes the average of its inputs. It is differentiable and, like any operator that takes a fixed convex combination of its inputs, is a non-expansion. However, it does not allow for maximization (Property 1).\nThe third operator eps (X), commonly referred to as epsilon greedy (Sutton & Barto, 1998), interpolates between max and mean. The operator is a non-expansion, because it is a convex combination of two non-expansion operators. But it is non-differentiable (Property 3).\nThe Boltzmann operator boltzβ(X) is differentiable. It also approximates max as β → ∞, and mean as β → 0. However, it is not a non-expansion (Property 2), and therefore, prone to misbehavior as will be shown in the next section.\nIn the following section, we provide a simple example illustrating why the non-expansion property is important, especially in the context of planning and on-policy learning. We then present a new softmax operator that is similar to the Boltzmann operator yet is a non-expansion. We prove several critical properties of this new operator, introduce a new softmax policy, and present empirical results."
  }, {
    "heading": "2. Boltzmann Misbehaves",
    "text": "We first show that boltzβ can lead to problematic behavior. To this end, we ran SARSA with Boltzmann softmax policy (Algorithm 1) on the MDP shown in Figure 1. The edges are labeled with a transition probability (unsigned) and a reward number (signed). Also, state s2 is a terminal state, so we only consider two action values, namely Q̂(s1, a) and Q̂(s2, b). Recall that the Boltzmann softmax policy assigns the following probability to each action:\nπ(a|s) = e βQ̂(s,a)\n∑ a e βQ̂(s,a) .\nAlgorithm 1 SARSA with Boltzmann softmax policy Input: initial Q̂(s, a) ∀s ∈ S ∀a ∈ A, α, and β for each episode do\nInitialize s a ∼ Boltzmann with parameter β repeat\nTake action a, observe r, s′ a ′ ∼ Boltzmann with parameter β Q̂(s, a)← Q̂(s, a) + α [ r + γQ̂(s′, a′)− Q̂(s, a) ]\ns← s′ , a← a′ until s is terminal\nend for\nIn Figure 2, we plot state–action value estimates at the end of each episode of a single run (smoothed by averaging over ten consecutive points). We set α = .1 and β = 16.55. The value estimates are unstable.\nSARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesvári, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function-approximation setting (Gordon, 2001). There are also variants of the SARSA update rule that converge more generally (Perkins & Precup, 2002; Baird & Moore, 1999; Van Seijen et al., 2009). However, this example is the first, to our knowledge, to show that SARSA fails to converge in the tabular setting with Boltzmann policy. The next section provides background for our analysis of the example."
  }, {
    "heading": "3. Background",
    "text": "A Markov decision process (Puterman, 1994), or MDP, is specified by the tuple 〈S,A,R,P, γ〉, where S is the set of states and A is the set of actions. The functions R : S ×A → R and P : S × A× S → [0, 1] denote the reward and transition dynamics of the MDP. Finally, γ ∈ [0, 1), the discount rate, determines the relative importance of immediate reward as opposed to the rewards received in the future.\nA typical approach to finding a good policy is to estimate how good it is to be in a particular state—the state value function. The value of a particular state s given a policy π and initial action a is written Qπ(s, a). We define the optimal value of a state–action pair Q?(s, a) = maxπ Qπ(s, a). It is possible to defineQ?(s, a) recursively and as a function of the optimal value of the other state–action pairs:\nQ?(s, a) = R(s, a)+ ∑\ns′∈S γ P(s, a, s′) max a′ Q?(s′, a′) .\nBellman equations, such as the above, are at the core of many reinforcement-learning algorithms such as Value Iteration (Bellman, 1957). The algorithm computes the\nvalue of the best policy in an iterative fashion:\nQ̂(s, a)← R(s, a) + γ ∑\ns′∈S P(s, a, s′) max a′ Q̂(s′, a′).\nRegardless of its initial value, Q̂ will converge to Q∗.\nLittman & Szepesvári (1996) generalized this algorithm by replacing the max operator by any arbitrary operator ⊗ , resulting in the generalized value iteration (GVI) algorithm with the following update rule:\nQ̂(s, a)← R(s, a)+γ ∑\ns′∈S γP(s, a, s′)\n⊗\na′\nQ̂(s′, a′). (1)\nAlgorithm 2 GVI algorithm Input: initial Q̂(s, a) ∀s ∈ S ∀a ∈ A and δ ∈ R+ repeat\ndiff← 0 for each s ∈ S do\nfor each a ∈ A do Qcopy ← Q̂(s, a) Q̂(s, a)←∑s′∈S R(s, a, s′)\n+ γP(s, a, s′)⊗ Q̂(s′, .) diff← max { diff, |Qcopy − Q̂(s, a)| }\nend for end for\nuntil diff < δ\nCrucially, convergence of GVI to a unique fixed point follows if operator ⊗ is a non-expansion with respect to the infinity norm: ∣∣∣ ⊗\na\nQ̂(s, a)− ⊗\na\nQ̂′(s, a) ∣∣∣ ≤ max\na\n∣∣∣Q̂(s, a)− Q̂′(s, a) ∣∣∣,\nfor any Q̂, Q̂′ and s. As mentioned earlier, the max operator is known to be a non-expansion, as illustrated in Figure 3. mean and eps operators are also non-expansions. Therefore, each of these operators can play the role of ⊗ in GVI, resulting in convergence to the corresponding unique\nfixed point. However, the Boltzmann softmax operator, boltzβ , is not a non-expansion (Littman, 1996). Note that we can relate GVI to SARSA by observing that SARSA’s update is a stochastic implementation of GVI’s update. Under a Boltzmann softmax policy π, the target of the (expected) SARSA update is the following:\nE π\n[ r + γQ̂(s′, a′) ∣∣s, a ] =\nR(s, a) + γ ∑\ns′∈S P(s, a, s′)\n∑\na′∈A π(a′|s′)Q̂(s′, a′) ︸ ︷︷ ︸ boltzβ ( Q̂(s′,·) ) .\nThis matches the GVI update (1) when ⊗ = boltzβ ."
  }, {
    "heading": "4. Boltzmann Has Multiple Fixed Points",
    "text": "Although it has been known for a long time that the Boltzmann operator is not a non-expansion (Littman, 1996), we are not aware of a published example of an MDP for which two distinct fixed points exist. The MDP presented in Figure 1 is the first example where, as shown in Figure 4, GVI under boltzβ has two distinct fixed points. We also show, in Figure 5, a vector field visualizing GVI updates under boltzβ=16.55. The updates can move the current estimates farther from the fixed points. The behavior of SARSA (Figure 2) results from the algorithm stochastically bouncing back and forth between the two fixed points. When the learning algorithm performs a sequence of noisy updates, it moves from a fixed point to the other. As we will show later, planning will also progress extremely slowly near the fixed points. The lack of the non-expansion property leads to multiple fixed points and ultimately a misbehavior in learning and planning."
  }, {
    "heading": "5. Mellowmax and its Properties",
    "text": "We advocate for an alternative softmax operator defined as follows:\nmmω(X) = log( 1n\n∑n i=1 e ωxi)\nω ,\nwhich can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016). It can also\nbe derived from information theoretical principles as a way of regularizing policies with a cost function defined by KL divergence (Todorov, 2006; Rubin et al., 2012; Fox et al., 2016). Note that the operator has previously been utilized in other areas, such as power engineering (Safak, 1993).\nWe show that mmω , which we refer to as mellowmax, has the desired properties and that it compares quite favorably to boltzβ in practice."
  }, {
    "heading": "5.1. Mellowmax is a Non-Expansion",
    "text": "We prove that mmω is a non-expansion (Property 2), and therefore, GVI and SARSA under mmω are guaranteed to converge to a unique fixed point.\nLet X = x1, . . . , xn and Y = y1, . . . , yn be two vectors of values. Let ∆i = xi − yi for i ∈ {1, . . . , n} be the difference of the ith components of the two vectors. Also, let i∗ be the index with the maximum component-wise difference, i∗ = argmaxi ∆i. For simplicity, we assume that i∗ is unique and ω > 0. Also, without loss of generality, we assume that xi∗ − yi∗ ≥ 0. It follows that:\n∣∣mmω(X)−mmω(Y) ∣∣\n= ∣∣ log( 1\nn\nn∑\ni=1\neωxi)/ω − log( 1 n\nn∑\ni=1\neωyi)/ω ∣∣\n= ∣∣ log 1 n\n∑n i=1 e ωxi\n1 n ∑n i=1 e\nωyi /ω ∣∣\n= ∣∣ log\n∑n i=1 e\nω ( yi+∆i ) ∑n i=1 e ωyi /ω ∣∣\n≤ ∣∣ log\n∑n i=1 e\nω ( yi+∆i∗ ) ∑n i=1 e ωyi /ω ∣∣\n= ∣∣ log e\nω∆i∗ ∑n i=1 e ωyi\n∑n i=1 e\nωyi /ω ∣∣\n= ∣∣ log(eω∆i∗ )/ω ∣∣ = ∣∣∆i∗ ∣∣ = max i ∣∣xi − yi ∣∣ ,\nallowing us to conclude that mellowmax is a non-expansion under the infinity norm."
  }, {
    "heading": "5.2. Maximization",
    "text": "Mellowmax includes parameter settings that allow for maximization (Property 1) as well as for minimization. In particular, as ω goes to infinity, mmω acts like max.\nLet m = max(X) and let W = |{xi = m|i ∈ {1, . . . , n}}|. Note that W ≥ 1 is the number of maximum values (“winners”) in X. Then:\nlim ω→∞ mmω(X) = lim ω→∞\nlog( 1n ∑n i=1 e ωxi)\nω\n= lim ω→∞\nlog( 1ne ωm ∑n i=1 e ω(xi−m))\nω\n= lim ω→∞\nlog( 1ne ωmW )\nω\n= lim ω→∞ log(eωm)− log(n) + log(W ) ω\n= m+ lim ω→∞ − log(n) + log(W ) ω = m = max(X) .\nThat is, the operator acts more and more like pure maximization as the value of ω is increased. Conversely, as ω goes to −∞, the operator approaches the minimum."
  }, {
    "heading": "5.3. Derivatives",
    "text": "We can take the derivative of mellowmax with respect to each one of the arguments xi and for any non-zero ω:\n∂mmω(X) ∂xi = eωxi∑n i=1 e ωxi ≥ 0 .\nNote that the operator is non-decreasing in each component of X.\nMoreover, we can take the derivative of mellowmax with respect to ω. We define nω(X) = log( 1n ∑n i=1 e\nωxi) and dω(X) = ω. Then:\n∂nω(X) ∂ω =\n∑n i=1 xie ωxi\n∑n i=1 e ωxi and ∂dω(X) ∂ω = 1 ,\nand so:\n∂mmω(X) ∂ω\n= ∂nω(X) ∂ω dω(X)− nω(X) ∂dω(X) ∂ω\ndω(X)2 ,\nensuring differentiablity of the operator (Property 3)."
  }, {
    "heading": "5.4. Averaging",
    "text": "Because of the division by ω in the definition of mmω , the parameter ω cannot be set to zero. However, we can examine the behavior of mmω as ω approaches zero and show that the operator computes an average in the limit.\nSince both the numerator and denominator go to zero as ω goes to zero, we will use L’Hôpital’s rule and the derivative given in the previous section to derive the value in the limit:\nlim ω→0 mmω(X) = lim ω→0\nlog( 1n ∑n i=1 e ωxi)\nω\nL’Hôpital = lim\nω→0\n1 n ∑n i=1 xie ωxi\n1 n ∑n i=1 e ωxi\n= 1\nn\nn∑\ni=1\nxi = mean(X) .\nThat is, as ω gets closer to zero, mmω(X) approaches the mean of the values in X."
  }, {
    "heading": "6. Maximum Entropy Mellowmax Policy",
    "text": "As described, mmω computes a value for a list of numbers somewhere between its minimum and maximum. However, it is often useful to actually provide a probability distribution over the actions such that (1) a non-zero probability mass is assigned to each action, and (2) the resulting expected value equals the computed value. Such a probability distribution can then be used for action selection in algorithms such as SARSA.\nIn this section, we address the problem of identifying such a probability distribution as a maximum entropy problem—over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010). We formally define the maximum entropy mellowmax policy of a state s as:\nπmm(s) = argmin π\n∑ a∈A π(a|s) log ( π(a|s) ) (2)\nsubject to { ∑ a∈A π(a|s)Q̂(s, a) = mmω(Q̂(s, .))\nπ(a|s) ≥ 0∑ a∈A π(a|s) = 1 .\nNote that this optimization problem is convex and can be solved reliably using any numerical convex optimization library.\nOne way of finding the solution, which leads to an interesting policy form, is to use the method of Lagrange\nmultipliers. Here, the Lagrangian is:\nL(π, λ1, λ2) = ∑\na∈A π(a|s) log\n( π(a|s) )\n−λ1 (∑\na∈A π(a|s)− 1\n)\n−λ2 (∑\na∈A π(a|s)Q̂(s, a)−mmω\n( Q̂(s, .) )) .\nTaking the partial derivative of the Lagrangian with respect to each π(a|s) and setting them to zero, we obtain:\n∂L ∂π(a|s) = log ( π(a|s) ) +1−λ1−λ2Q̂(s, a) = 0 ∀ a ∈ A .\nThese |A| equations, together with the two linear constraints in (2), form |A| + 2 equations to constrain the |A| + 2 variables π(a|s) ∀a ∈ A and the two Lagrangian multipliers λ1 and λ2.\nSolving this system of equations, the probability of taking an action under the maximum entropy mellowmax policy has the form:\nπmm(a|s) = eβQ̂(s,a)∑ a∈A e βQ̂(s,a) ∀a ∈ A ,\nwhere β is a value for which:\n∑ a∈A eβ ( Q̂(s,a)−mmωQ̂(s,.) )( Q̂(s, a)−mmωQ̂(s, .) ) = 0 .\nThe argument for the existence of a unique root is simple. As β → ∞ the term corresponding to the best action dominates, and so, the function is positive. Conversely, as β → −∞ the term corresponding to the action with lowest utility dominates, and so the function is negative. Finally, by taking the derivative, it is clear that the function is monotonically increasing, allowing us to conclude that there exists only a single root. Therefore, we can find β easily using any root-finding algorithm. In particular, we use Brent’s method (Brent, 2013) available in the Numpy library of Python.\nThis policy has the same form as Boltzmann softmax, but with a parameter β whose value depends indirectly on ω. This mathematical form arose not from the structure of mmω , but from maximizing the entropy. One way to view the use of the mellowmax operator, then, is as a form of Boltzmann policy with a temperature parameter chosen adaptively in each state to ensure that the non-expansion property holds.\nFinally, note that the SARSA update under the maximum entropy mellowmax policy could be thought of as a\nstochastic implementation of the GVI update under the mmω operator:\nE πmm\n[ r + γQ̂(s′, a′) ∣∣s, a ] =\n∑ s′∈S R(s, a, s′) + γP(s, a, s′) ∑ a′∈A πmm(a ′|s′)Q̂(s′, a′) ]\n︸ ︷︷ ︸ mmω ( Q̂(s′,.) )\ndue to the first constraint of the convex optimization problem (2). Because mellowmax is a non-expansion, SARSA with the maximum entropy mellowmax policy is guaranteed to converge to a unique fixed point. Note also that, similar to other variants of SARSA, the algorithm simply bootstraps using the value of the next state while implementing the new policy."
  }, {
    "heading": "7. Experiments on MDPs",
    "text": "We observed that in practice computing mellowmax can yield overflow if the exponentiated values are large. In this case, we can safely shift the values by a constant before exponentiating them due to the following equality:\nlog( 1n ∑n i=1 e ωxi)\nω = c+\nlog( 1n ∑n i=1 e ω(xi−c))\nω .\nA value of c = maxi xi usually avoids overflow.\nWe repeat the experiment from Figure 5 for mellowmax with ω = 16.55 to get a vector field. The result, presented in Figure 6, show a rapid and steady convergence towards the unique fixed point. As a result, GVI under mmω can terminate significantly faster than GVI under boltzβ , as illustrated in Figure 7.\nWe present three additional experiments. The first experiment investigates the behavior of GVI with the softmax operators on randomly generated MDPs. The second experiment evaluates the softmax policies when used in SARSA with a tabular representation. The last\nexperiment is a policy gradient experiment where a deep neural network, with a softmax output layer, is used to directly represent the policy."
  }, {
    "heading": "7.1. Random MDPs",
    "text": "The example in Figure 1 was created carefully by hand. It is interesting to know whether such examples are likely to be encountered naturally. To this end, we constructed 200 MDPs as follows: We sampled |S| from {2, 3, ..., 10} and |A| from {2, 3, 4, 5} uniformly at random. We initialized the transition probabilities by sampling uniformly from [0, .01]. We then added to each entry, with probability 0.5, Gaussian noise with mean 1 and variance 0.1. We next added, with probability 0.1, Gaussian noise with mean 100 and variance 1. Finally, we normalized the raw values to ensure that we get a transition matrix. We did a similar process for rewards, with the difference that we divided each entry by the maximum entry and multiplied by 0.5 to ensure that Rmax = 0.5 .\nWe measured the failure rate of GVI under boltzβ and mmω by stopping GVI when it did not terminate in 1000 iterations. We also computed the average number of iterations needed before termination. A summary of results is presented in the table below. Mellowmax outperforms Boltzmann based on the three measures provided below.\nMDPs, no terminate MDPs, > 1 fixed points average iterations\nboltzβ 8 of 200 3 of 200 231.65 mmω 0 0 201.32"
  }, {
    "heading": "7.2. Multi-passenger Taxi Domain",
    "text": "We evaluated SARSA on the multi-passenger taxi domain introduced by Dearden et al. (1998). (See Figure 8.)\nOne challenging aspect of this domain is that it admits many locally optimal policies. Exploration needs to be set carefully to avoid either over-exploring or under-exploring the state space. Note also that Boltzmann softmax performs remarkably well on this domain, outperforming sophisticated Bayesian\nreinforcement-learning algorithms (Dearden et al., 1998). As shown in Figure 9, SARSA with the epsilon-greedy policy performs poorly. In fact, in our experiment, the algorithm rarely was able to deliver all the passengers. However, SARSA with Boltzmann softmax and SARSA with the maximum entropy mellowmax policy achieved significantly higher average reward. Maximum entropy mellowmax policy is no worse than Boltzmann softmax, here, suggesting that the greater stability does not come at the expense of less effective exploration."
  }, {
    "heading": "7.3. Lunar Lander Domain",
    "text": "In this section, we evaluate the use of the maximum entropy mellowmax policy in the context of a policy-gradient algorithm. Specifically, we represent a policy by a neural network (discussed below) that maps from states to probabilities over actions. A common choice for the activation function of the last layer is the Boltzmann softmax policy. In contrast, we can use maximum entropy mellowmax policy, presented in Section 6, by treating the inputs of the activation function as Q̂ values.\nWe used the lunar lander domain, from OpenAI Gym (Brockman et al., 2016) as our benchmark. A screenshot of the domain is presented in Figure 10. This domain has a continuous state space with 8 dimensions, namely x-y coordinates, x-y velocities, angle and angular velocities, and leg-touchdown sensors. There are 4 discrete actions to control 3 engines. The reward is +100 for a safe landing in the designated area, and −100 for a crash. There is a small shaping reward for approaching the landing area. Using the engines results in a negative reward. An episode finishes when the spacecraft crashes or lands. Solving the domain is defined as maintaining mean episode return higher than 200 in 100 consecutive episodes.\nThe policy in our experiment is represented by a neural network with a hidden layer comprised of 16 units with RELU activation functions, followed by a second layer with 16 units and softmax activation functions. We used REINFORCE to train the network. A batch episode size\nof 10 was used, as we had stability issues with smaller episode batch sizes. We used the Adam algorithm (Kingma & Ba, 2014) with α = 0.005 and the other parameters as suggested by the paper. We used Keras (Chollet, 2015) and Theano (Team et al., 2016) to implement the neural network architecture. For each softmax policy, we present in Figure 11 the learning curves for different values of their free parameter. We further plot average return over all 40000 episodes. Mellowmax outperforms Boltzmann at its peak."
  }, {
    "heading": "8. Related Work",
    "text": "Softmax operators play an important role in sequential decision-making algorithms.\nIn model-free reinforcement learning, they can help strike\na balance between exploration (mean) and exploitation (max). Decision rules based on epsilon-greedy and Boltzmann softmax, while very simple, often perform surprisingly well in practice, even outperforming more advanced exploration techniques (Kuleshov & Precup, 2014) that require significant approximation for complex domains. When learning “on policy”, exploration steps can (Rummery & Niranjan, 1994) and perhaps should (John, 1994) become part of the value-estimation process itself. On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000). Our use of softmax in learning updates reflects this point of view and shows that the value-sensitive behavior of Boltzmann exploration can be maintained even as updates are made stable.\nAnalyses of the behavior of human subjects in choice experiments very frequently use softmax. Sometimes referred to in the literature as logit choice (Stahl & Wilson, 1994), it forms an important part of the most accurate predictor of human decisions in normal-form games (Wright & Leyton-Brown, 2010), quantal level-k reasoning (QLk). Softmax-based fixed points play a crucial role in this work. As such, mellowmax could potentially make a good replacement.\nAlgorithms for inverse reinforcement learning (IRL), the problem of inferring reward functions from observed behavior (Ng & Russell, 2000), frequently use a Boltzmann operator to avoid assigning zero probability to non-optimal actions and hence assessing an observed sequence as impossible. Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesvári, 2007), and maximum likelihood IRL (Babes et al., 2011). Given the recursive nature of value defined in these problems, mellowmax could be a more stable and efficient choice.\nIn linearly solvable MDPs (Todorov, 2006), an operator similar to mellowmax emerges when using an alternative characterization for cost of action selection in MDPs. Inspired by this work Fox et al. (2016) introduced an off-policy G-learning algorithm that uses the operator to perform value-function updates. Instead of performing off-policy updates, we introduced a convergent variant of SARSA with Boltzmann policy and a state-dependent temperature parameter. This is in contrast to Fox et al. (2016) where an epsilon greedy behavior policy is used."
  }, {
    "heading": "9. Conclusion and Future Work",
    "text": "We proposed the mellowmax operator as an alternative to the Boltzmann softmax operator. We showed that mellowmax has several desirable properties and that it works favorably in practice. Arguably, mellowmax could be used in place of Boltzmann throughout reinforcement-learning research.\nA future direction is to analyze the fixed point of planning, reinforcement-learning, and game-playing algorithms when using the mellowmax operators. In particular, an interesting analysis could be one that bounds the sub-optimality of the fixed points found by GVI.\nAn important future work is to expand the scope of our theoretical understanding to the more general function approximation setting, in which the state space or the action space is large and abstraction techniques are used. Note that the importance of non-expansion in the function approximation case is well-established. (Gordon, 1995)\nFinally, due to the convexity of mellowmax (Boyd & Vandenberghe, 2004), it is compelling to use it in a gradient-based algorithm in the context of sequential decision making. IRL is a natural candidate given the popularity of softmax in this setting."
  }, {
    "heading": "10. Acknowledgments",
    "text": "The authors gratefully acknowledge the assistance of George D. Konidaris, as well as anonymous ICML reviewers for their outstanding feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Apprenticeship learning about multiple intentions",
    "authors": ["Babes", "Monica", "Marivate", "Vukosi N", "Littman", "Michael L", "Subramanian", "Kaushik"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Gradient descent for general reinforcement learning",
    "authors": ["Baird", "Leemon", "Moore", "Andrew W"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1999
  }, {
    "title": "Goal inference as inverse planning",
    "authors": ["Baker", "Chris L", "Tenenbaum", "Joshua B", "Saxe", "Rebecca R"],
    "venue": "In Proceedings of the 29th Annual Meeting of the Cognitive Science Society,",
    "year": 2007
  }, {
    "title": "A Practical Guide to Averaging",
    "authors": ["Beliakov", "Gleb", "Sola", "Humberto Bustince", "Sánchez", "Tomasa Calvo"],
    "year": 2016
  }, {
    "title": "A Markovian decision process",
    "authors": ["Bellman", "Richard"],
    "venue": "Journal of Mathematics and Mechanics,",
    "year": 1957
  }, {
    "title": "Algorithms for minimization without derivatives",
    "authors": ["Brent", "Richard P"],
    "venue": "Courier Corporation,",
    "year": 2013
  }, {
    "title": "Elements of Information Theory",
    "authors": ["T.M. Cover", "J.A. Thomas"],
    "year": 2006
  }, {
    "title": "Bayesian Q-learning",
    "authors": ["Dearden", "Richard", "Friedman", "Nir", "Russell", "Stuart"],
    "venue": "In Fifteenth National Conference on Artificial Intelligence (AAAI),",
    "year": 1998
  }, {
    "title": "Taming the noise in reinforcement learning via soft updates",
    "authors": ["Fox", "Roy", "Pakman", "Ari", "Tishby", "Naftali"],
    "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Stable function approximation in dynamic programming",
    "authors": ["Gordon", "Geoffrey J"],
    "venue": "In Proceedings of the twelfth international conference on machine learning,",
    "year": 1995
  }, {
    "title": "Reinforcement learning with function approximation converges to a region, 2001. Unpublished",
    "authors": ["Gordon", "Geoffrey J"],
    "year": 2001
  }, {
    "title": "When the best move isn’t optimal: Q-learning with exploration",
    "authors": ["John", "George H"],
    "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,",
    "year": 1994
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Algorithms for multi-armed bandit problems",
    "authors": ["Kuleshov", "Volodymyr", "Precup", "Doina"],
    "venue": "arXiv preprint arXiv:1402.6028,",
    "year": 2014
  }, {
    "title": "A generalized reinforcement-learning model: Convergence and applications",
    "authors": ["Littman", "Michael L", "Szepesvári", "Csaba"],
    "venue": "Proceedings of the Thirteenth International Conference on Machine Learning,",
    "year": 1996
  }, {
    "title": "Algorithms for Sequential Decision Making",
    "authors": ["Littman", "Michael Lederman"],
    "venue": "PhD thesis,",
    "year": 1996
  }, {
    "title": "Apprenticeship learning using inverse reinforcement learning and gradient methods",
    "authors": ["Neu", "Gergely", "Szepesvári", "Csaba"],
    "venue": "In UAI,",
    "year": 2007
  }, {
    "title": "Algorithms for inverse reinforcement learning",
    "authors": ["Ng", "Andrew Y", "Russell", "Stuart"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2000
  }, {
    "title": "A convergent form of approximate policy iteration",
    "authors": ["Perkins", "Theodore J", "Precup", "Doina"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2002
  }, {
    "title": "Relative entropy policy search",
    "authors": ["Peters", "Jan", "Mülling", "Katharina", "Altun", "Yasemin"],
    "venue": "In AAAI. Atlanta,",
    "year": 2010
  }, {
    "title": "Bayesian inverse reinforcement learning",
    "authors": ["Ramachandran", "Deepak", "Amir", "Eyal"],
    "venue": "In IJCAI,",
    "year": 2007
  }, {
    "title": "Trading value and information in mdps. In Decision Making with Imperfect Decision Makers",
    "authors": ["Rubin", "Jonathan", "Shamir", "Ohad", "Tishby", "Naftali"],
    "year": 2012
  }, {
    "title": "On-line Q-learning using connectionist systems",
    "authors": ["G.A. Rummery", "M. Niranjan"],
    "venue": "Technical Report CUED/F-INFENG/TR 166,",
    "year": 1994
  }, {
    "title": "Statistical analysis of the power sum of multiple correlated log-normal components",
    "authors": ["Safak", "Aysel"],
    "venue": "IEEE Transactions on Vehicular Technology,",
    "year": 1993
  }, {
    "title": "Convergence results for single-step on-policy reinforcement-learning algorithms",
    "authors": ["Singh", "Satinder", "Jaakkola", "Tommi", "Littman", "Michael L", "Szepesvári", "Csaba"],
    "venue": "Machine Learning,",
    "year": 2000
  }, {
    "title": "Experimental evidence on players’ models of other players",
    "authors": ["Stahl", "Dale O", "Wilson", "Paul W"],
    "venue": "Journal of Economic Behavior and Organization,",
    "year": 1994
  }, {
    "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
    "authors": ["Sutton", "Richard S"],
    "venue": "In Proceedings of the Seventh International Conference on Machine Learning,",
    "year": 1990
  }, {
    "title": "Reinforcement Learning: An Introduction",
    "authors": ["Sutton", "Richard S", "Barto", "Andrew G"],
    "year": 1998
  }, {
    "title": "The role of exploration in learning control",
    "authors": ["Thrun", "Sebastian B"],
    "year": 1992
  }, {
    "title": "Linearly-solvable markov decision problems",
    "authors": ["Todorov", "Emanuel"],
    "venue": "In NIPS, pp",
    "year": 2006
  }, {
    "title": "A theoretical and empirical analysis of Expected Sarsa",
    "authors": ["Van Seijen", "Harm", "Van Hasselt", "Hado", "Whiteson", "Shimon", "Wiering", "Marco"],
    "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,",
    "year": 2009
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["Williams", "Ronald J"],
    "venue": "Machine Learning,",
    "year": 1992
  }, {
    "title": "Beyond equilibrium: Predicting human behavior in normal-form games",
    "authors": ["Wright", "James R", "Leyton-Brown", "Kevin"],
    "venue": "In AAAI,",
    "year": 2010
  }],
  "id": "SP:0ca51467d034828689a723b6faeccbbbcada5485",
  "authors": [{
    "name": "Kavosh Asadi",
    "affiliations": []
  }, {
    "name": "Michael L. Littman",
    "affiliations": []
  }],
  "abstractText": "A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.",
  "title": "An Alternative Softmax Operator for Reinforcement Learning"
}