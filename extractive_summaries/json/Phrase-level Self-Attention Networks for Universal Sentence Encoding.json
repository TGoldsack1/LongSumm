{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3729–3738 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3729"
  }, {
    "heading": "1 Introduction",
    "text": "Following the success of word embeddings (Bengio et al., 2003; Mikolov et al., 2013), one of NLP’s next challenges has become the hunt for universal sentence encoders. The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks. The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for\ndownstream NLP tasks, especially for those with relatively small datasets.\nPrevious models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation. RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient. CNNs focus on local or positioninvariant dependencies but do not perform well on many tasks (Shen et al., 2017).\nFully attention-based neural networks have attracted wide interest recently, because they can model both dependencies while being more parallelizable and requiring significantly less time to train. Vaswani et al. (2017) proposed the multihead attention to project a sentence to multiple semantic subspaces, then apply self-attention in each subspace and concatenate the attention results. Shen et al. (2017) proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context. Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length. Furthermore, the syntactic property that is intrinsic to natural language is not considered at all.\nLanguage is inherently tree structured, and the meaning of a sentence comes largely from composing the meanings of subtrees (Chomsky, 1957). Previous syntactic tree-based sentence encoders (Socher et al., 2013; Tai et al., 2015) mainly rely on recursive networks. Although the composition-\nality can be explicitly modeled, their models need expensive recursion computation and are hard to be trained by batched gradient descent methods.\nIn this paper, we propose the Phrase-level SelfAttention Networks (PSAN), for RNN/CNN-free sentence encoding, it inherits all the advantages of fully attention-based models while requires much less memory consumption. In addition, syntactic information can be incorporated into the model more easily. In our model, every sentence is split into multiple phrases based on parse tree, selfattention is performed at the phrase level instead of the sentence level, thus the memory consumption reduces rapidly as the number of phrases increases. Furthermore, a gated memory component is employed to refine word representations hierarchically by incorporating longer-term context dependencies. As a result, syntactic information can be integrated into the model without expensive recursion computation. At last, multi-dimensional attention is applied on the refined word representations to obtain the final sentence representation.\nFollowing Conneau et al. (2017), we trained our sentence encoder on the SNLI (Bowman et al., 2015) dataset, and evaluate the quality of the obtained universal sentence representations on a wide range of transfer tasks. The SNLI dataset is extremely suitable for training sentence encoders because it is the largest high-quality humanannotated dataset that involves reasoning about the semantic relationships within sentences.\nThe main contributions of our work can be summarized as follows:\n• We propose the Phrase-level Self-Attention mechanism (PSA) for contextualization. The memory consumption can be reduced because self-attention is performed at the phrase level instead of the sentence level.\n• A gated memory updating mechanism is proposed to refine each word representation hierarchically by incorporating different levels of contextual information along the parse tree.\n• Our proposed PSAN model outperforms the state-of-the-art supervised sentence encoders on a wide range of transfer tasks with significantly less memory consumption."
  }, {
    "heading": "2 Proposed Model",
    "text": "In this section, we introduce the Phrase-level SelfAttention Networks (PSAN) for sentence encod-\ning. A phrase is a group of words that carry a specific idiomatic meaning and function as a constituent in the syntax of a sentence. Words in a phrase are syntactically and semantically related to each other. Therefore, it can be advantageous to learn a context-aware representation inside a phrase while filtering out information from outside the phrase using self-attention mechanism. In an attempt to better utilize the tree structure which is intrinsic to language, we propose the gated memory updating mechanism to combine different levels of context information. At last, an attention mechanism is utilized to summarize all the token representations into a fixed-length sentence vector."
  }, {
    "heading": "2.1 Phrase Division",
    "text": "The phrase structure organizes words into nested constituents which can be successively divided into their parts as we move down the constituencybased parse trees. One phrase division shows only one aspect of context dependency. In order to capture different levels of context dependencies, we can split a sentence at different granularities. The number of levels T is a hyper-parameter to be tuned.\nWe can break down the nodes at T different layers in the parse tree to capture T levels of context dependencies1, as illustrated in Figure 1."
  }, {
    "heading": "2.2 Phrase-level Self-Attention",
    "text": "This is the core component of our model. It aims to learn a context-aware representation for each token inside a phrase. In order to filter out information that is semantically or syntactically distant, self-attention is performed at the phrase level instead of the sentence level.\nSimilar to directional self-attention network (DiSAN) (Shen et al., 2017), Phrase-level SelfAttention uses multi-dimensional attention to compute the alignment score for each dimension of token embedding. Therefore, it can select the features that can best describe a word’s specific meaning in any given context.\nGiven a phrase P ∈ Rl×d represented as a sequence of word embeddings [p1, . . . ,pl], where l is the length of the phrase and d is the dimension of word embedding representation, we first compute the alignment score for each token pair in the\n1To avoid the situation that the produced phrases are too small, a phrase will not be further divided if its length is smaller than 4.\nphrase: aij = σ ( W a1pi +W a2pj + b a ) +Mij\nMij = { 0, i 6= j −∞, i = j\n(1)\nwhere σ (·) is an activation function, W a1,W a2 ∈ Rd×d and ba ∈ Rd are parameters to be learned, and M is a diagonal-diabled mask (Hu et al., 2017) that aims to prevent a word from being aligned with itself.\nThe output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase:\np̃i = l∑ j=1 [ exp (aij)∑l k=1 exp (aik) pj ] (2)\nwhere means point-wise product. Note that the alignment score for each token pair is a vector rather than a scalar in the multi-dimensional attention.\nThe final output of Phrase-level Self-Attention is obtained by comparing each input representation with its attention-weighted counterpart. We use a comparison function based on absolute difference and element-wise multiplication which was similar to Wang and Jiang (2016). This comparison function has the advantage of measuring the semantic similarity or relatedness of two sequences.\nci = σ (W c [|pi − p̃i| ;pi p̃i] + bc) (3)\nwhere W c ∈ Rd×2d and ba ∈ Rd are parameters to be learned. ci is the representation for the i-th word in the phrase that captures local dependencies within the phrase.\nAt last, we put together the Phrase-level SelfAttention results for non-overlapping phrases from the same phrase division of a sentence. For the t-th phrase division we can get C(t) = [c1, . . . , cls ], the phrase-level self-attention results for the sentence from the t-th layer split, where ls is the sentence length."
  }, {
    "heading": "2.3 Gated Memory Updating",
    "text": "Above describes the Phrase-level Self-Attention (PSA) for one split of the parse tree. The parse tree can be split at different granularities. We propose a novel gated memory updating mechanism to refine each word representation hierarchically with longer-term dependencies captured in a larger granularity. Inspired by the idea of adaptive gate in highway networks (Srivastava et al., 2015), our memory mechanism add a gate to original memory networks (Weston et al., 2014; Sukhbaatar et al., 2015). This gate has the ability to determine the importance of the new input and the original memory in the memory updating.\nC(t) = PSA ( M (t−1) ) G(t) = sigmoid ( W g [ M (t−1);C(t) ] + bg\n) M (t) = G(t) σ ( Wm [ M (t−1);C(t) ] + bm\n) (4)\nwhereW g,Wm ∈ Rd×2d and bg, bm ∈ Rd are parameters to be learned. Note that in order to share representation power and to reduce the number of parameters, the parameters of gated memory updating are shared among different layers."
  }, {
    "heading": "2.4 Sentence Summarization",
    "text": "In this layer, self-attention mechanism is employed to summarize the refined representation of a sentence into a fixed-length vector. The selfattention mechanism can explore the dependencies among tokens within the whole sentence. As a result, global dependencies can also be incorporated in the model.\nei =W e2σ ( W e1m (T ) i + b e1 ) + be2\nv = l∑ i=1 [ exp (ei)∑l j=1 exp (ej) m(T )i ] (5) where W g,Wm ∈ Rd×d and bg, bm ∈ Rd are parameters to be learned. After this step, the refined context-aware sentence representation is compressed into a fixed-length vector."
  }, {
    "heading": "3 Experiments",
    "text": "In this section, we conduct a plethora of experiments to study the effectiveness of the PSAN model. Following Conneau et al. (2017), we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity."
  }, {
    "heading": "3.1 Model Configuration",
    "text": "300-dimensional GloVe (Pennington et al., 2014) word embeddings (Common Crawl, uncased) are used to represent words. Following Parikh et al. (2016), out-of-vocabulary words are hashed to one of 128 random embeddings initialized by uniform distribution between (-0.05, 0.05). All the word embeddings remain fixed during training. Hidden dimension d is set to 300. All other parameters are initialized with Glorot normal initialization (Glorot and Bengio, 2010). Activation function σ (·) is ELU (Clevert et al., 2015) if not specified. Minibatch size is set to 16. The number of levels T is fixed to 3 in all of our experiments. The syntactic parse trees of SNLI are provided within the corpus. parse trees for all test corpus are produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), the same parser that produced parse trees for the SNLI dataset.\nTo train the model, Adadelta optimizer (Zeiler, 2012) with a learning rate of 0.75 is used on the SNLI dataset. The dropout (Srivastava et al., 2014) rate and L2 regularization weight decay factor γ are set to 0.5 and 5e-5. To test the model, the SentEval toolkit (Conneau and Kiela, 2018) is used as the evaluation pipeline for fairer comparison."
  }, {
    "heading": "3.2 Training Setting",
    "text": "Natural language inference (NLI) is a fundamental task in the field of natural language processing that involves reasoning about the semantic relationship between two sentences, which makes it a suitable task to train sentence encoding models.\nWe conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The dataset has 570k human-annotated sentence pairs, each labeled with one of the following pre-defined relationships: Entailment (the premise entails the hypothesis), Contradiction (they contradict each other) and Neutral (they are irrelevant). Following previous work (Bowman et al., 2015; Mou et al., 2016), we remove the instances which annotators can not reach consensus on. In this way we get 549367/9842/9824 sentence pairs for train/validation/test set.\nFollowing the siamese architecture (Bromley et al., 1993), we apply PSAN to both the premise and the hypothesis with their parameters tied. vp and vh are fixed-length vector representations for the premise and the hypothesis respectively. The final sentence-pair representation is formed by concatenating the original vectors with the absolute difference and element-wise multiplication between them:\nvinp = [ vp;vh; ∣∣∣vp − vh∣∣∣ ;vp vh] (6) At last, we feed the sentence-pair representation vinp into a two layer feed-forward network and use a softmax layer to make the classification. This is the de facto scheme for sentence encoders trained on SNLI. (Mou et al., 2016; Liu et al., 2016; Shen et al., 2017)"
  }, {
    "heading": "3.3 Evaluation Setting",
    "text": "To show the modeling capacity and robustness of our proposed model, we evaluate our model across a wide range of tasks that can be solved purely based on the encoded semantics. The set of tasks\nwas selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. To facilitate comparison, we use the same sentence evaluation tool as Conneau et al. (2017) to automate evaluation on all the tasks mentioned in this paper.\nThe transfer tasks used in evaluation can be concluded in the following classes: sentence classification (MR, CR, MPQA, SUBJ, SST2, SST5, TREC), natural language inference (SICKE, SICK-R), semantic relatedness (STS14) and paraphrase detection (MRPC). Table 1 presents some statistics about the datasets 2."
  }, {
    "heading": "3.4 Baselines",
    "text": "We compare our model with the following supervised sentence encoders:\n• BiLSTM-Max (Conneau et al., 2017) is a simple but effective baseline that performs max-pooling over a bi-directional LSTM.\n• AdaSent (Zhao et al., 2015) forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments.\n• TBCNN (Mou et al., 2015) is a tree-based CNN model where convolution is applied over the parse tree.\n2For further information on the datasets, please refer to Conneau et al. (2017).\n• DiSAN (Shen et al., 2017) is composed of a directional self-attention block with temporal order encoded, and a multi-dimensional attention that compresses the sequence into a vector representation."
  }, {
    "heading": "4 Results and Analysis",
    "text": ""
  }, {
    "heading": "4.1 Overall Performance",
    "text": "Experiment results of our model and four baselines are shown in Table 2. Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy. Macro accuracy is the proportion of true results in the population of instances from all tasks. Micro accuracy is the arithmetic mean of dev accuracies for each task.\nPSAN achieves the state-of-the-art performance\nwith considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information. Especially when compared with previous best model BiLSTM-Max, PSAN can outperform their model with only 5% of their parameter numbers, demonstrating the effectiveness of our model at extracting semantically important information from a sentence.\nIn Table 3, we compare our model with baseline sentence encoders in each transfer task. PSAN can consistently outperform the baselines in almost every task considered. On the SICK dataset, which can be seen as an out-domain version of SNLI, our model can outperform the baselines by a large margin, demonstrating the semantic relationship learned on the SNLI can be well transfered to other domains. On the STS14 dataset, where sentence vectors can be more directly measured by the cosine distance, our model can also achieve the stateof-the-art performance, indicating that our learned sentence representations are of high quality."
  }, {
    "heading": "4.2 Ablation Study",
    "text": "For thorough comparison, we implement seven extra baselines to analyze the improvements con-\ntributed by each part of our PSAN model:\n• PSA on the first/second/third layer only only uses the Phrase-level Self-Attention at the first/second/third layer of phrase division.\n• w/o PSA applies self-attention at the sentence level and uses the gated memory updating mechanism to refine each token representation hierarchically.\n• w/o syntactic division divides each sentence equally into small blocks, and applies PSA within each block. The number of blocks equals the number of phrases in that layer.\n• w/o gated memory updating concatenates the outputs of Phrase-level Self-Attention from three layers of phrase division and feeds the result to a feed-forward layer.\n• w/o both applies self-attention at the sentence level, and uses sentence summarization to summarize the attention results into a fixed length vector.\nThe results are listed in Table 4. We can see that (2) performs best among (1), (2) and (3), demonstrating that the second layer split is more expressive, because the number of words per phrase in the second layer is the most suitable. It is neither too small to capture context dependencies, nor too large to filter out irrelevant noise. (8) outperforms (1), (2) and (3), showing that combining phraselevel information from different granularities can further improve performance.\nWe also experiment on models where the alignment matrix is calculated at the sentence level or at the syntactic-irrelevant block level. (5) performs quite well, showing that hierarchical refinement on smaller units can bring about reasonable\nperformance gain. (8) outperforms (4) and (5), demonstrating syntactic information helps in sentence representation.\nWhen comparing (6) with (8), we can tell that gated memory updating is a better method when used to refine token representation along the parse tree. We assume that memory updating resembles the tree structure of language in that larger phrase is composed in the knowledge of how smaller phrases are composed inside it.\nComparing (7) with (1), (2) and (3), we can find that performing self-attention at the phrase level is generally better than at the sentence level, indicating that reducing attention context into phrase level can effectively filter out words that are syntactically and semantically distant, thus focusing on the interaction with important words. Comparing (7) with (4), we can draw the conclusion that memory updating is effective even when the inputs to each layer are the same."
  }, {
    "heading": "4.3 Analysis of Sentence Length",
    "text": "Long-term dependencies are typically hard to capture for sequential models like RNNs (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997). We conduct experiments to see how performance changes as the sentence length increases. In Figure 2, we show the relationship between classification accuracy and the average length of sentence pair on the SNLI dataset. Sentence-level SelfAttention (w/o PSA model described in subsection 4.2) is used as a baseline for our model. PSAN\noutperforms Sentence-level Self-Attention model consistently for longer sentences of length 14 to 20. This demonstrates that incorporating syntactic information by performing self-attention at the phrase level and refining each word’s representation hierarchically can help to capture long-term dependencies across words in a sentence."
  }, {
    "heading": "4.4 Analysis of Memory Consumption",
    "text": "We conduct experiments to analyze the memory consumption reduction resulted from Phrase-level Self-Attention. To this end, we re-implement two fully attention-based models (Vaswani et al., 2017; Shen et al., 2017) on the TREC dataset. To make fair comparison, the dimensions of sentence vectors are set to 300, the same number as our model. Table 5 lists the results. Our PSAN model can outperform the other two fully attention-based models, while being more memory efficient. reducing more than 20% of memory consumption."
  }, {
    "heading": "4.5 Visualization and Case Study",
    "text": "In order to analyze the attention changing process and the importance of each word in the sentence vector, we visualize the attention scores in the alignment matrix of each layer in Phraselevel Self-Attention and sentence summarization layer. To facilitate the visualization of the multidimension attention vector, we use the l2 norm of the attention vector for representation.\nIn Figure 3, we can see that, the difference in attention weights between semantically important and unimportant words gets larger as the context becomes larger. This implies that token representation can be gradually refined by the gated memory updating mechanism. Furthermore, the alignment matrix of a phrase can be refined even if the phrase division does not change between layers. For instance, the word “girl” gets larger attention weight in the second layer division than in the first layer. This demonstrates that the memory\nupdating mechanism can gradually pick out important words for sentence representation. Finally, nouns and verbs dominate the attention weights, while stop words like “a” and “its”, contribute little to the final sentence representation, this indicates that PSAN can effectively pick out semantically important words that are most representative for the meaning of the whole sentence."
  }, {
    "heading": "5 Related Work",
    "text": "Recently, self-attention mechanism has been successfully applied to the field of sentence encoding, it utilizes the attention mechanism to relate elements at different positions from a single sentence. Due to its direct access to each token representation, both long-term and local dependencies can be modeled flexibly. Liu et al. (2016) leveraged the average-pooled word representation to attend words appear in the sentence itself. Cheng et al. (2016) proposed the LSTMN model for machine reading, an attention vector is produced for each of its hidden states during the recurrent iteration, thus empowering the recurrent network with stronger memorization capability and the ability to discover relations among tokens. Lin et al. (2017) obtained a fixed-size sentence embedding matrix by introducing self-attention. Different from the feature-level attention used in our model, their attention mechanism extracted different aspects of the sentence into multiple vector representations, and utilized a penalization term to encourage the diversity of different attention results.\nSyntactic information can be useful for understanding a natural language sentence. Many previous researches utilized syntactic information to build sentence encoder from composing the mean-\nings of subtrees. Tree-LSTM (Tai et al., 2015; Zhu et al., 2015) composed its hidden state from an input vector and the hidden states of arbitrarily many child units. In Tree-based CNN (Mou et al., 2015, 2016), a set of subtree feature detectors slide over the parse tree of a sentence, and a max-pooling layer is utilized to aggregate information along different parts of the tree.\nApart from the models that use parse information, there have been several researches that aimed to learn the hierarchical latent structure of text by recursively composing words into sentence representation. Among them, neural tree indexer (Munkhdalai and Yu, 2017b) utilized LSTM or attentive node composition function to construct full n-ary tree for input text. Gumbel TreeLSTM (Choi et al., 2018) used Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically. A major drawback of these models is that the recursion computation can be expensive and hard to be processed in batches."
  }, {
    "heading": "6 Conclusion",
    "text": "We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding. By applying self-attention at the phrase level, we can filter out distant and unrelated words and focus on modeling interaction between semantically and syntactically important words, a gated memory updating mechanism is utilized to incorporate different levels of contextual information along the parse tree. Empirical results on a wide range of transfer tasks demonstrate the effectiveness of our model."
  }, {
    "heading": "Acknowledgments",
    "text": "Our work is supported by National Natural Science Foundation of China under Grant No.61433015 and the National Key Research and Development Program of China under Grant No.2017YFB1002101. The corresponding authors of this paper are Houfeng Wang."
  }],
  "year": 2018,
  "references": [{
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research, 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."],
    "venue": "IEEE transactions on neural networks, 5(2):157–166.",
    "year": 1994
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Signature verification using a siamese time delay neural network",
    "authors": ["Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard Säckinger", "Roopak Shah."],
    "venue": "Advances in Neural Information Processing Systems 6, [7th NIPS Conference, Denver, Colorado,",
    "year": 1993
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November",
    "year": 2016
  }, {
    "title": "Learning to compose task-specific tree structure",
    "authors": ["Jihun Choi", "Kang Min Yoo", "Sang-goo Lee."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, February 2-7, 2018, New Orleans, Louisiana, USA.",
    "year": 2018
  }, {
    "title": "Syntactic structures",
    "authors": ["Noem Chomsky."],
    "venue": "International Journal of American Linguistics, 149(3):174196.",
    "year": 1957
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "CoRR, abs/1412.3555.",
    "year": 2014
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter."],
    "venue": "CoRR, abs/1511.07289.",
    "year": 2015
  }, {
    "title": "Senteval: An evaluation toolkit for universal sentence representations",
    "authors": ["Alexis Conneau", "Douwe Kiela."],
    "venue": "arXiv preprint arXiv:1803.05449.",
    "year": 2018
  }, {
    "title": "Supervised learning of universal sentence representations from natural language inference data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes"],
    "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Nat-",
    "year": 2017
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sar-",
    "year": 2010
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Reinforced mnemonic reader for machine comprehension",
    "authors": ["Minghao Hu", "Yuxing Peng", "Xipeng Qiu."],
    "venue": "CoRR, abs/1705.02798.",
    "year": 2017
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014,",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural In-",
    "year": 2015
  }, {
    "title": "Accurate unlexicalized parsing",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "A structured self-attentive sentence embedding. CoRR, abs/1703.03130",
    "authors": ["Zhouhan Lin", "Minwei Feng", "Cı́cero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"],
    "year": 2017
  }, {
    "title": "Learning natural language inference using bidirectional LSTM model and inner-attention",
    "authors": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang."],
    "venue": "CoRR, abs/1605.09090.",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Natural language inference by tree-based convolution and heuristic matching",
    "authors": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
    "year": 2016
  }, {
    "title": "Discriminative neural sentence modeling by tree-based convolution",
    "authors": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Neural semantic encoders",
    "authors": ["Tsendsuren Munkhdalai", "Hong Yu."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers.",
    "year": 2017
  }, {
    "title": "Neural tree indexers for text understanding",
    "authors": ["Tsendsuren Munkhdalai", "Hong Yu."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers.",
    "year": 2017
  }, {
    "title": "Shortcutstacked sentence encoders for multi-domain inference",
    "authors": ["Yixin Nie", "Mohit Bansal."],
    "venue": "Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP, RepEval@EMNLP 2017, Copenhagen, Denmark,",
    "year": 2017
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "Deep convolutional neural networks for sentiment analysis of short texts",
    "authors": ["Cicero dos Santos", "Maira Gatti."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers.",
    "year": 2014
  }, {
    "title": "Disan: Directional self-attention network for rnn/cnn-free language understanding",
    "authors": ["Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Shirui Pan", "Chengqi Zhang."],
    "venue": "CoRR, abs/1709.04696.",
    "year": 2017
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 Conference on",
    "year": 2013
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2014
  }, {
    "title": "Highway networks",
    "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber."],
    "venue": "CoRR, abs/1505.00387.",
    "year": 2015
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-",
    "year": 2015
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
    "year": 2017
  }, {
    "title": "A compareaggregate model for matching text sequences",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "CoRR, abs/1611.01747.",
    "year": 2016
  }, {
    "title": "Memory networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "CoRR, abs/1410.3916.",
    "year": 2014
  }, {
    "title": "ADADELTA: an adaptive learning rate method",
    "authors": ["Matthew D. Zeiler."],
    "venue": "CoRR, abs/1212.5701.",
    "year": 2012
  }, {
    "title": "Self-adaptive hierarchical sentence model",
    "authors": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."],
    "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages",
    "year": 2015
  }, {
    "title": "Long short-term memory over recursive structures",
    "authors": ["Xiaodan Zhu", "Parinaz Sobihani", "Hongyu Guo."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 1604– 1612.",
    "year": 2015
  }],
  "id": "SP:cd0bb48f0f26f146759654bdc2dd0aad87e917de",
  "authors": [{
    "name": "Wei Wu",
    "affiliations": []
  }, {
    "name": "Houfeng Wang",
    "affiliations": []
  }, {
    "name": "Tianyu Liu",
    "affiliations": []
  }, {
    "name": "Shuming Ma",
    "affiliations": []
  }],
  "abstractText": "Universal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time. However, the memory consumption of their models grows quadratically with sentence length, and the syntactic information is neglected. To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word’s representation hierarchically with longer-term context dependencies captured in a larger phrase. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. Experiment results show that PSAN can achieve the state-ofthe-art transfer performance across a plethora of NLP tasks including sentence classification, natural language inference and sentence textual similarity.",
  "title": "Phrase-level Self-Attention Networks for Universal Sentence Encoding"
}