{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1812–1822 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics\nIn this paper, we propose to leverage representation learning for conversation disentanglement. A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages. With the estimated similarity scores, our algorithm for conversation identification by similarity ranking (CISIR) then derives conversations based on highconfidence message pairs and pairwise redundancy. Experiments were conducted with four publicly available datasets of conversations from Reddit and IRC channels. The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement."
  }, {
    "heading": "1 Introduction",
    "text": "With the growth of ubiquitous internet and mobile devices, people now commonly communicate in the virtual world. Among the various methods of communication, text-based conversational media, such as internet relay chat (IRC) (Werry, 1996) and Facebook Messenger1, has been and remains one of the most popular choices. In addition, many enterprises have started to use conversational chat platforms such as Slack2 to enhance team collaboration. However, multiple conversations may\n1Facebook Messenger: https://www.messenger. com/\n2Slack: https://slack.com/\noccur simultaneously when conversations involve three or more participants. Aoki et al. (2006) found an average of 1.79 conversations among eight participants at a time. Moreover, some platforms like chatrooms in Twitch may have more concurrent conversations (Hamilton et al., 2014). Interleaved conversations can lead to difficulties in both grasping discussions and identifying messages related to a search result. For example, Figure 1 shows a segment of conversations from the real-world IRC dataset as an example. Five interleaved threads are involved in only ten messages. Messages in the same thread may not have identical keywords. Moreover, a user (i.e., Elma) can participate in multiple threads. Hence, a robust mechanism to disentangle interleaved conversations can improve a user’s satisfaction with a chat system.\nOne solution for conversation disentanglement is to model the task as a topic detection and tracking (TDT) (Allan, 2002) task by deciding whether each incoming message starts a new topic or belongs to an existing conversation. Messages in the same conversation may have higher similarity\n1812\nscores (Shen et al., 2006; Mayfield et al., 2012) or similar context messages (Wang and Oard, 2009). However, similarity thresholds for determining new topics vary depending on context. Embedding of earlier messages, resulting in duplication of parts of messages, can alter the similarity score. More specifically, the similarity scores obtained in previous work cannot well represent conversationlevel relationships between messages.\nSeveral studies have examined the use of statistical (Du et al., 2017) and linguistic features (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012) for predicting user annotations of paired message similarity. These studies employed bag-of-words representations which do not capture term similarity and cannot distinguish word importance and relationships between words in a message. Thus, better representations of messages and their relationships are needed.\nRecent studies have demonstrated the effectiveness of deep learning methods in representation learning (Bengio et al., 2013), aiming to infer low-dimensional distributed representations for sparse data such as text (Hinton and Salakhutdinov, 2006). These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature.\nIn this paper, we aim to leverage deep learning for conversation disentanglement. Our proposed approach consists of two stages: (1) message pair similarity estimation and (2) conversation identification. In the first stage, we propose the Siamese hierarchical convolutional neural network (SHCNN) to estimate conversation-level similarity between pairs of closely posted messages. SHCNN is framed as a Siamese architecture (Mueller and Thyagarajan, 2016) concatenat-\ning the outputs of two hierarchical convolutional neural networks and additional features. Compared to other conventional CNN-based Siamese networks (Severyn and Moschitti, 2015; Yin et al., 2016), SHCNN models not only local information in adjacent words but also more global semantic information in a message. In the second stage, the algorithm of conversation identification by similarity ranking (CISIR) ranks messages within a time window paired with each message and constructs a message graph involving high-rank connections with strong confidence. Although only high-confidence relations are represented in the constructed graph, the redundancy of pairwise relationships can capture the connectivity of messages within a conversation.\nIn summary, the main contributions of this paper are threefold: (1) Deep similarity estimation for conversation disentanglement: To the best of our knowledge, this is the first study applying deep learning to estimate similarities between messages for disentangling conversations. SHCNN simultaneously captures and compares local and global characteristics of two messages to estimate their similarity. Message representations are also optimized towards the task of conversation disentanglement. (2) Efficient and effective method: The selection of message pairs posted closely in time and the proposed CISIR algorithm significantly reduces the computational time from O |M |2 to O (k|M |), where |M | is the number of messages, and k is the maximum number of messages posted within a fixed-length time window. When many messages are posted over a long period, the computational time of our approach could be nearlinear. (3) Empirical improvements over previous work: Extensive experiments have been conducted on four publicly available datasets, including three synthetic conversation datasets and one real conversation dataset from Reddit3 and IRC conversations. Our approach outperforms all comparative baselines for both similarity estimation and conversation disentanglement."
  }, {
    "heading": "2 Related Work",
    "text": "Methods for conversation disentanglement can be simply categorized into unsupervised and supervised approaches. Unsupervised approaches (Wang and Oard, 2009) estimate the relationship between messages through unsupervised similarity functions such cosine similarity, and assign messages to conversations based on a predefined\n3Reddit: https://www.reddit.com/\nthreshold. In contrast, supervised methods exploit a set of user annotations (Elsner and Charniak, 2008; Mayfield et al., 2012; Shen et al., 2006; Du et al., 2017; Mehri and Carenini, 2017) to adapt to different datasets. Our approach can be classified as a supervised approach because a small set of user annotations is used to train the SHCNN.\nIn addition to conversations, some studies predict the partial structure of threaded data, especially for online forums (Aumayr et al., 2011; Wang et al., 2011b,a). These studies merely classify parent-child relationships in disentangled, independent threads. Moreover, they focus only on comments to the same post. Indeed, conversation disentanglement is a more difficult task.\nEstimating the similarity of text pairs is an essential part in our approach. Many studies also focus on similar tasks aside from conversation disentanglement, such as entailment prediction (Mueller and Thyagarajan, 2016; Wang and Jiang, 2017) and question-answering (Severyn and Moschitti, 2015; Amiri et al., 2016; Yin et al., 2016). However, most of their models are complicated and require a larger amount of labeled training data; limited conversational data can lead to unsatisfactory performance as shown in Section 4."
  }, {
    "heading": "3 Conversation Disentanglement",
    "text": "In this section, we formally define the objective of this work and notations used. A two-stage approach is then proposed to address the problem."
  }, {
    "heading": "3.1 Problem Statement",
    "text": "Given a set of speakers S, a message m is defined as a tuple m = (w, s, t), where w = hw1, w2, · · · , wni is a word sequence posted by the speaker s 2 S at time t in seconds. Each message m is associated with a conversation z (m). Messages in different conversations can be posted concurrently, i.e., conversations can be interleaved.\nFollowing the settings of previous work (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012), a set of pairwise annotations A = {(mi, mj , y)}, where y 2 {0, 1}, is given for training the model. More specifically, a Boolean value y indicates whether two messages mi and mj are in the same conversation, i.e., z(mi) and z(mj) are identical.\nGiven a set of messages M and the pairwise annotations A as training data, the goal is to learn a model that can identify whether messages are posted in the same conversation z(m). Note that the number of conversations |Z =\n{z(m) | 8m 2 M} | is always unknown to the system."
  }, {
    "heading": "3.2 Framework Overview",
    "text": "Figure 2 illustrates our two-stage framework. The first stage aims to estimate pairwise similarity among messages. Message pair selection is applied to focus on the similarity between messages that are posted closely in time and thus more likely to be in the same conversation. The Siamese hierarchical CNN (SHCNN) is proposed for learning message representations and estimating pairwise similarity scores. The overlapping hierarchical structure of SHCNN models a message at multiple semantic levels and obtains representations that are more comprehensive.\nIn the second stage, our conversation identification by similarity ranking (CISIR) algorithm exploits the redundancy and connectivity of pairwise relationships to identify conversations as connected components in a message graph."
  }, {
    "heading": "3.3 Message Pair Selection",
    "text": "Most of the previous work on conversation disentanglement focused on pairwise relationships between messages (Mayfield et al., 2012). Especially for single-pass clustering approaches, all pairs of messages need to be enumerated during similarity computation (Wang and Oard, 2009). However, if messages have been collected for a long time, the number of message pairs could be too mammoth to be processed in an acceptable amount of time. More precisely, it leads to at least O(n2) computational time, where n is the number of messages. As shown in Figure 3, the percentage of messages in the same conversation as a given message becomes significantly lower with a longer elapsed time between consecutive messages. In light of this observation, an assumption is made as follows: Assumption 1 The elapsed time between two consecutive messages posted in the same conversation is not greater than T hours, where T is a small number. More specifically, in our dataset every message mi is posted within T hours earlier or later than any other message mj in the same conversation, i.e., |ti tj | 3600 < T for all pairs (mi, mj), where t is in seconds. For example, in the IRC dataset the average elapsed time between consecutive messages in a conversation is only 7 minutes. If a conversation is ongoing, there may not be an extended silence before a new message; conversely, an extended silence could be treated as the start of a new\nconversation. With this assumption, the number of pairs can be reduced to O(kn), where k is the maximum number of messages posted in a T -hour time window. By default T is set to 1 hour in our experiments.\nIn addition, it is worth mentioning that it may be possible to include conversational structure, such as replied-to relations, into the model. For example, after using CISIR to identify conversational threads, structure inference may be performed using methods such as described in (Aumayr et al., 2011) or (Wang et al., 2011b) and the structure used to refine the threads. In this study, we focus on only conversation disentanglement."
  }, {
    "heading": "3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN)",
    "text": "Given a set of message pairs, we propose the Siamese hierarchical CNN (SHCNN) to estimate the similarity between a pair of messages.\n|w| words message input m\nconvolutional message\nmatrix Wc\nd-dimensional word embedding\n... ...\nhigh-level message\nmatrix WH\nlow-level conv. feature\nmap cLi\nhigh-level conv. feature\nmap cHi\n64-dim low-level representation\nm̂L\n64-dim high-level representation\nm̂H\n128-dim message representation m̂\nd ⇥ |w| message matrix W\nd ⇥ |w| message matrix W\nFigure 4: Illustration of hierarchical CNN (HCNN) for message representation. The labels with a larger font size indicate the corresponding tensors, and the labels with a smaller font size explain the operations between tensors."
  }, {
    "heading": "3.4.1 Hierarchical CNN for Message Representation",
    "text": "The effectiveness of CNNs for representing text has already been addressed in previous studies. However, single-layer CNNs (Kim, 2014; Severyn and Moschitti, 2015) may not represent highlevel semantics while low-level information could be diluted with multiple-layer CNNs (Yin et al., 2016). The hierarchical CNN (HCNN) is designed to simultaneously capture low- and highlevel message meanings as shown in Figure 4.\nA message mi is first represented by a d ⇥ |w| message matrix W 2 Rd⇥|w|, where d is the dimension of a word embedding, and |w| is the num-\nber of words in a message. For low-level information, we exploit single-layer CNNs (Kim, 2014; Severyn and Moschitti, 2015) with a set of d⇥ kL kernels, where L denotes “Low”, to extract ngram semantics of kL contiguous words. In this paper, 64 d ⇥ kL kernels, where kL = 5, are applied to obtain 64 low-level features m̂L. Note that the kernel row dimension is identical to the word embedding dimension to jointly consider the full embedding vector. As a consequence, convolution with each kernel produces a vector cLi , which is then aggregated by max-over-time pooling (Collobert et al., 2011; Kim, 2014).\nTo acquire high-level semantics across a message, HCNN uses another multiple-layer CNN for feature extraction. A 1 ⇥ kC kernel is applied to W , thereby generating a convolutional message matrix W C . Features covering broader contents are computed by applying a 1 ⇥ 2 kernel to a max-pooling layer with a stride of 2, producing a high-level message matrix W H . The row sizes of the two kernels are set to 1 to capture relations within each embedding dimension, and convolution is performed on W H with 64 d⇥ kH kernels to capture relations across embedding dimensions. The generated convolutional feature maps cHi are subject to max-over-time pooling, resulting in 64 features m̂H . Finally, a message representation m̂ is constructed by concatenating m̂L and m̂H , i.e., creating a 128-dimensional feature vector, for characterizing both low- and high-level semantics of a message m. In this paper, both kC and kH are set to 5 while computing high-level representations."
  }, {
    "heading": "3.4.2 Siamese Hierarchical CNN (SHCNN)",
    "text": "A Siamese structure with two identical subnetworks is useful to exploit the affinity between representations of two instances in the same hidden space (Severyn and Moschitti, 2015; Yin et al., 2016; Wang and Jiang, 2017). For similarity estimation, we propose the Siamese hierarchical CNN (SHCNN) using a Siamese structure that blends the outputs from two HCNNs as well as some context features.\nFigure 5 shows the structure of the SHCNN for estimating the similarity between two messages mi and mj where the message representations m̂i and m̂j are generated by two sub-networks HCNNs (See Figure 4). There are many ways to deal with two sub-networks, such as using a similarity matrix (Severyn and Moschitti, 2015) or an attention matrix (Yin et al., 2016). However, both methods lead to an enormous number of parame-\nters for long messages. We propose to independently compute the element-wise absolute differences (Mueller and Thyagarajan, 2016) between a pair of message representations m̂i and m̂j , each from a sub-network. More formally, the absolute difference d is a vector where the k-th element is computed as |m̂i(k) m̂j(k)|. This approach provides not only fewer parameters but also the flexibility to observe interactions among different dimensions in representations. Our experiments also show it outperforms the other two approaches in similarity estimation (See Section 4).\nIn addition to message contents, contexts such as temporal and user information were also usually considered in previous studies about conversation disentanglement (Wang and Oard, 2009; Elsner and Charniak, 2010, 2011). In this paper, we focus on the performance of message content representations and only incorporate four context features: speaker identicality, absolute time difference and the number of duplicated words with and without weighting by inverse document frequency (Christopher et al., 2008). SHCNN concatenates the context features x(mi, mj) with the absolute difference d as the input of a fully-connected layer of the same size.\nThe final output of SHCNN ŷ (mi, mj) is normalized by a logistic sigmoid function (Han and Moraga, 1995), representing the probability P (z(mi) = z(mj))."
  }, {
    "heading": "3.4.3 Activation Functions",
    "text": "All convolutional layers and the fully-connected layer require activation functions, and the choice affects the performance (Maas et al., 2013). Popular functions include rectified linear units (ReLUs) (LeCun et al., 2015), hyperbolic tangent\nunits (tanh) and exponential linear units (ELUs) (Clevert et al., 2016). In this study, we conducted informal comparison experiments and ELU was finally chosen for all functions because it performed the best."
  }, {
    "heading": "3.4.4 Optimization and Implementation Details",
    "text": "Given a set of annotated message pairs A = {(mi, mj , y)}, where y is a Boolean value indicating whether two messages are in the same conversation, SHCNN is optimized with binomial cross entropy (Goodfellow et al., 2016). More formally, the objective function is as follows:\nX\n(mi,mj ,y)2A\n[y · log(ŷ + ✏) + (1 y) · log(1 ŷ + ✏)]+ ||✓||2\nwhere ŷ simplifies ŷ(mi, mj), and ✏ is a small number, i.e., 10 9 in our experiments, preventing underflow errors. The term serves as the weight for L2-regularization for the set of parameters ✓.\nIn our experiments, SHCNN is implemented by TensorFlow (Abadi et al., 2016) and trained by the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 10 3. The dropout technique (Srivastava et al., 2014) is utilized in the fully-connected layer with a dropout probability of 0.1. Word embeddings are initialized using the publicly available fastText 300-dimensional pretrained embeddings from Facebook (Bojanowski et al., 2016). The batch size is set to 512, and the maximum number of training epochs is 1,000. The final model is determined by evaluating the mean average precision (MAP) on a validation dataset every 100 iterations."
  }, {
    "heading": "3.5 Conversation Identification by SImilarity Ranking (CISIR)",
    "text": "In the second stage of conversation disentanglement, i.e., part (2) in Figure 2, we aim to separate conversations based on the identified message pairs and their estimated similarity."
  }, {
    "heading": "3.5.1 Graph-based Methods and Conversation Connectivity",
    "text": "It is intuitive to apply graph-based methods if pairwise relationships of messages are exploited (Elsner and Charniak, 2008). Furthermore, methods based on single-pass clustering (Wang and Oard, 2009) can be also be treated as graph-based methods. However, graph-based methods have a risky drawback: A single false positive connection between two messages can be propagated to several messages from different conversations. As shown\nAlgorithm 1: The algorithm of conversation disentanglement by similarity ranking (CISIR).\n1 CISIR (M , D, r, h); Input : Message set M , the set of selected\nmessage pairs D, the threshold of similarity ranks r and the threshold of similarity scores h.\nOutput: A set of conversations C 2 Let G = (M , ;) be an undirected message\ngraph 3 for m 2 M do 4 Dm = {(mi, mj , ŷ) | mi = m _ mj = m} 5 Rank entries in Dm by ŷ in a descending order 6 for k = 1 to min(r, |Dm|) do 7 Let (mi, mj , ŷ) be the k-th entry in ranked Dm 8 if ŷ < h then 9 break\n10 Add an edge (mi, mj) into G\n11 C = ConnectedComponents(G) 12 return C\nin Figure 3, a certain percentage of message pairs are in different conversations, which can lead to numerous false positive connections.\nFalse alarms may be reduced by raising the threshold that determines whether two messages are connected (Wang and Oard, 2009). However, a high threshold can make disentangled conversations fragmented and the best threshold for each pair could vary."
  }, {
    "heading": "3.5.2 The CISIR Algorithm",
    "text": "Instead of setting a high threshold, we propose the algorithm of Conversation Identification by SImilarity Ranking (CISIR). CISIR focuses on the top messages ranked by similarity scores. Based on Assumption 1, for each message, there exists at least one or more other messages in the same conversation posted closely in time. With this redundancy, a few pairs with stronger confidence, i.e., the top-ranked pairs, can be enough to extend a correct connectivity to earlier or later messages, while the low-ranked pairs can be ignored to reduce the risk of error propagation.\nGiven a set of selected message pairs with estimated similarity scores D = {(mi, mj , ŷ)}, Algorithm 1 shows the procedure of CISIR with two parameters r and h, where r is a high threshold\nof similarity ranks and h is a lower threshold of similarity scores. Note that CISIR filters out pairs with low scores because a message can have more than r same-conversation pairs posted in its T - hour time window. For each message, CISIR ranks all of its associated pairs by the estimated similarity and only retrieves the top-r pairs whose similarity scores are greater than h. These retrieved high-confidence pairs are treated as the edges in a message graph G. Finally, CISIR divides G into connected components, and the messages in each connected component are treated as a conversation. In this paper, we use grid search to set r and h as 5 and 0.5, respectively."
  }, {
    "heading": "3.5.3 Improvement of Time Complexity",
    "text": "The efficiency of Algorithm 1 can be further improved. The top-r qualified pairs for each message can be pre-processed by a scan of D with |M | min-heaps which always contain at most r+1 elements. When r is a small constant number, it only takes O(|D|) = O(k · |M |) for pre-processing, where k is the maximum number of messages posted in a T -hour time window. With preprocessed top pairs, CISIR can do graph construction and find connected components in O(k|M |), which compares favorably to conventional methods in O(|M |2)."
  }, {
    "heading": "4 Experiments",
    "text": "In this section, we conduct extensive experiments on four publicly available datasets to evaluate SHCNN and CISIR in two stages."
  }, {
    "heading": "4.1 Datasets and Experimental Settings",
    "text": ""
  }, {
    "heading": "4.1.1 Datasets",
    "text": "Three datasets from Reddit and one dataset of IRC are used as the experimental datasets.\n• Reddit Datasets4 The Reddit dataset is comprised of all posts and corresponding comments in all sub-reddits (i.e., forums in Reddit.com) from June 2016 to May 2017. Comments under a post can be treated as messages in one conversational thread. Here we manually merge all comments in a sub-reddit to construct a synthetic dataset of interleaved conversations. Note that although it is called a “synthetic dataset,” all messages are written by real users. Three subreddits with different popularity levels as shown in Table 1 are selected to build three datasets: gadgets, iPhone and politics. 4The organized Reddit dataset is publicly available in https://files.pushshift.io/reddit/.\n• IRC Dataset. An annotated IRC dataset used in (Elsner and Charniak, 2008) is also included in our experiments. The IRC dataset consists of about 6 hours of messages in interleaved conversations. Even though the IRC dataset is significantly smaller and shorter than the Reddit datasets, it consists of natural, interleaved conversations with ground truth annotations, including thread id."
  }, {
    "heading": "4.1.2 Experimental Settings",
    "text": "Humans may not participate in a large number of simultaneous conversations. e.g., an average of 1.79 for eight people (Aoki et al., 2006), but there could be hundreds of concurrent posts in a subreddit. Hence, we adjusted the datasets to be more similar to real conversations. Specifically we removed some conversations so that every dataset has at most ten conversations at any point in time. Short messages with less than five words are also removed because even for humans they are frequently ambiguous. Too short conversations with less than ten messages are also discarded as outliers (Ren et al., 2011). Training and validation data are randomly chosen from only 10% of the selected message pairs, respectively, because in real situations obtaining labels could be very costly. The remaining 80% of pairs are regarded as testing data. As a result, Table 1 shows the statistics of the four datasets after pre-processing."
  }, {
    "heading": "4.2 Pairwise Similarity Estimation",
    "text": "Message pair similarity estimation is treated as a ranking task and evaluated with three ranking evaluation metrics: precision at 1 (P@1), mean average precision (MAP) and mean reciprocal rank (MRR) (Christopher et al., 2008). We compare the performance with six baseline methods, including the difference of posted time (TimeDiff ), sameness of speakers (Speaker), cosine similarity of text (Text-Sim), the approach proposed by Elsner and Charniak (2008) (Elsner), DeepQA (Severyn and Moschitti, 2015) and ABCNN (Yin et al., 2016). Note that DeepQA and ABCNN are neural network-based models for questionanswering. The approach of Mehri and Carenini\n(2017) was not compared in our experiments because the RNN requires additional message sequences; moreover, its performance was only mildly better than Elsner, which performed poorly on IRC in Table 2.\nTable 2 shows the performance of similarity estimation. Among all methods, neural network approaches (Severyn and Moschitti, 2015; Yin et al., 2016) perform better than other methods in most cases, indicating that message content representation has considerable impact on estimating pairwise similarity. SHCNN outperforms most of the baselines even if only low-level (L) or high-level (H) representations are exploited. When SHCNN captures both low- and high-level semantics, it significantly outperforms all baselines across the four datasets. For example, ABCNN can outperform SHCNN using only either low- or high-level representations in the politics dataset; however, SHCNN turns the tables after using both representations. An interesting observation is that ABCNN is the best baseline in every dataset except for IRC; this may be because the IRC data is too small to train complicated attention structures. On the contrary, our SHCNN can precisely capture semantics even with few parameters and limited data.\nTo shed deeper insights of how SHCNN surpasses other methods, we exhibit the prediction\nresults of the IRC data and demonstrate the capability of SHCNN to simultaneously preserve local and more global information. Figure 6 presents an example to show how SHCNN is better than other methods in capturing more high-level topical information. Even though the main sentences of two messages are clearly on different topics, the baseline method DeepQA (Severyn and Moschitti, 2015) still predicts a high similarity. This could be attributed to the context of author mention (Wang and Oard, 2009) and a bias on the local information, i.e., the exact same term “Arlie”, in the Siamese network used in DeepQA. On the contrary, SHCNN can capture more global information that differentiates the topics and correctly predicts a very low score. Figure 7 illustrates another example of how SHCNN outperforms other methods in preserving the similarity of local information. Both of the messages in the example have some segments related to software engineering. A baseline method ABCNN (Yin et al., 2016) with multiple-layer CNNs, however, still predicts a low score. This might be because both sentences are long so that the local information is diluted after processing by multiple CNN layers. Differently, SHCNN is able to seize local information, correctly predicting a high score."
  }, {
    "heading": "4.3 Conversation Identification",
    "text": "For conversation identification, three clustering metrics are adopted for evaluation: normalized mutual information (NMI), adjusted rand index (ARI) and F1 score (F1). Six methods are implemented as the baselines for conversation disentanglement, including Doc2Vec (Le and Mikolov, 2014), blocks of 10 messages (Block-10), messages of respective speakers (Speaker) (Elsner and Charniak, 2011), context-based message expansion (CBME) (Wang and Oard, 2009) and a graph-theoretical model with chat- and contentspecific features (Elsner and Charniak, 2008) (GTM). The embedding-based clustering method, i.e., Doc2Vec, applies affinity propagation (Frey and Dueck, 2007) to cluster messages embedded using Doc2Vec without being given the number of clusters, with the idea that messages in the same conversation would form a cluster. Note that message pairs in the training and validation data are not utilized in prediction for a fair comparison to all methods.\nTable 3 shows the performance of conversation disentanglement. Note that “Oracle” represents the optimal performance for CISIR when all message pairs in identical conversations in D are correctly retrieved. Because pairs in D may not have enough coverage to connect all messages in a coversation, the optimal performance could be lower than 1.0. CISIR performs better than all baseline methods for all datasets, and achieves excellent performance in IRC, due in part to the high-performing similarity estimates from the first stage. Among the baseline methods, GTM performs relatively well on all datasets except for IRC. This is because messages are more frequently posted in the IRC dataset, thereby increasing the number of incorrect pairs in the constructed graph. Examining the graph constructed by GTM, there are only two connected components, indicating that many conversations were in-\ncorrectly combined; in contrast, CISIR may be exempt from error propagation because it only relies on top-ranked pairs. Doc2Vec is trained to predict words in a document in an unsupervised manner. Its lowest performance in the experiments may point out a need for supervised learning in the specific task of conversation disentanglement to tackle the variation in semantic patterns. Time and author contextual cues do help conversation disentanglement as seen in the results of Block-10 and Speaker. Both of these contexts are integrated into our model."
  }, {
    "heading": "5 Conclusions",
    "text": "In this paper, we propose a novel framework for disentangling conversations, including similarity estimation for message pairs and conversation identification. In contrast to previous work, we assume that we do not need to select all message pairs in the first stage, thereby reducing computational time without sacrificing performance too much. To estimate conversation-level similarity, a Siamese Hierarchical Convolutional Neural Network, SHCNN, is proposed to minimize the estimation error as well as preserve both the lowand high-level semantics of messages. In the second stage, we developed the Conversation Identification by SImilarity Ranking, CISIR, algorithm, which exploits the assumption made in the first stage and identifies individual, entangled conversations with high-ranked message pairs. Extensive experiments conducted on four publicly available datasets show that SHCNN and CISIR outperform several existing approaches in both similarity estimation and conversation identification."
  }, {
    "heading": "Acknowledgement",
    "text": "We would like to thank the anonymous reviewers for their helpful comments. The work was partially supported by NIH U01HG008488, NIH R01GM115833, NIH U54GM114833, and NSF IIS-1313606."
  }],
  "year": 2018,
  "references": [{
    "title": "Tensorflow: A system for large-scale machine learning",
    "authors": ["Martı́n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard"],
    "venue": "In OSDI’16",
    "year": 2016
  }, {
    "title": "Introduction to topic detection and tracking",
    "authors": ["James Allan."],
    "venue": "Topic detection and tracking pages 1–16.",
    "year": 2002
  }, {
    "title": "Learning text pair similarity with context-sensitive autoencoders",
    "authors": ["Hadi Amiri", "Philip Resnik", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "ACL’16. ACL, pages 1882–1892.",
    "year": 2016
  }, {
    "title": "Where’s the party in multiparty?: Analyzing the structure of small-group sociable talk",
    "authors": ["Paul M Aoki", "Margaret H Szymanski", "Luke Plurkowski", "James D Thornton", "Allison Woodruff", "Weilie Yi."],
    "venue": "CSCW’06. ACM, pages 393–402.",
    "year": 2006
  }, {
    "title": "Reconstruction of threaded conversations in online discussion forums",
    "authors": ["Erik Aumayr", "Jeffrey Chan", "Conor Hayes."],
    "venue": "ICWSM’11. pages 26–33.",
    "year": 2011
  }, {
    "title": "Representation learning: A review and new perspectives",
    "authors": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."],
    "venue": "TPAMI 35(8):1798–1828.",
    "year": 2013
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner."],
    "venue": "ACL’14. ACL, pages 655– 665.",
    "year": 2014
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.04606 .",
    "year": 2016
  }, {
    "title": "Introduction to information retrieval",
    "authors": ["D Manning Christopher", "Raghavan Prabhakar", "SCHÜTZE Hinrich."],
    "venue": "An Introduction To Information Retrieval 151:177.",
    "year": 2008
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter."],
    "venue": "ICLR’16.",
    "year": 2016
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "JMLR 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Discovering conversational dependencies between messages in dialogs",
    "authors": ["Wenchao Du", "Pascal Poupart", "Wei Xu."],
    "venue": "AAAI’17. pages 4917–4918.",
    "year": 2017
  }, {
    "title": "You talking to me? a corpus and algorithm for conversation disentanglement",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "ACL’08. ACL, pages 834–842.",
    "year": 2008
  }, {
    "title": "Disentangling chat",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "Computational Linguistics 36(3):389– 409.",
    "year": 2010
  }, {
    "title": "Disentangling chat with local coherence models",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "ACLHLT’11. ACL, pages 1179–1189.",
    "year": 2011
  }, {
    "title": "Clustering by passing messages between data points",
    "authors": ["Brendan J Frey", "Delbert Dueck."],
    "venue": "science 315(5814):972–976.",
    "year": 2007
  }, {
    "title": "Deep learning",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."],
    "venue": "MIT press.",
    "year": 2016
  }, {
    "title": "Streaming on twitch: fostering participatory communities of play within live mixed media",
    "authors": ["William A Hamilton", "Oliver Garretson", "Andruid Kerne."],
    "venue": "CHI’14. ACM, pages 1315–1324.",
    "year": 2014
  }, {
    "title": "The influence of the sigmoid function parameters on the speed of backpropagation learning",
    "authors": ["Jun Han", "Claudio Moraga."],
    "venue": "From Natural to Artificial Neural Computation pages 195–201.",
    "year": 1995
  }, {
    "title": "Reducing the dimensionality of data with neural networks",
    "authors": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."],
    "venue": "Science 313(5786):504–507.",
    "year": 2006
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "EMNLP’14. ACL.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "ICLR’16.",
    "year": 2015
  }, {
    "title": "Recurrent convolutional neural networks for text classification",
    "authors": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."],
    "venue": "AAAI’15. volume 333, pages 2267–2273.",
    "year": 2015
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc Le", "Tomas Mikolov."],
    "venue": "ICML’14. pages 1188–1196.",
    "year": 2014
  }, {
    "title": "Deep learning",
    "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."],
    "venue": "Nature 521(7553):436–444.",
    "year": 2015
  }, {
    "title": "Rectifier nonlinearities improve neural network acoustic models",
    "authors": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng."],
    "venue": "ICML’13. volume 30.",
    "year": 2013
  }, {
    "title": "Hierarchical conversation structure prediction in multi-party chat",
    "authors": ["Elijah Mayfield", "David Adamson", "Carolyn Penstein Rosé."],
    "venue": "SIGDIAL’12. ACL, pages 60–69.",
    "year": 2012
  }, {
    "title": "Chat disentanglement: Identifying semantic reply relationships with random forests and recurrent neural networks",
    "authors": ["Shikib Mehri", "Giuseppe Carenini."],
    "venue": "IJCNLP’17. volume 1, pages 615–623.",
    "year": 2017
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS’13. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Siamese recurrent architectures for learning sentence similarity",
    "authors": ["Jonas Mueller", "Aditya Thyagarajan."],
    "venue": "AAAI’16. pages 2786–2792.",
    "year": 2016
  }, {
    "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis",
    "authors": ["Soujanya Poria", "Erik Cambria", "Alexander Gelbukh."],
    "venue": "EMNLP’15. ACL, pages 2539–2544.",
    "year": 2015
  }, {
    "title": "Summarizing web forum threads based on a latent topic propagation process",
    "authors": ["Zhaochun Ren", "Jun Ma", "Shuaiqiang Wang", "Yang Liu."],
    "venue": "CIKM’11. ACM, pages 879–884.",
    "year": 2011
  }, {
    "title": "Learning to rank short text pairs with convolutional deep neural networks",
    "authors": ["Aliaksei Severyn", "Alessandro Moschitti."],
    "venue": "SIGIR’15. ACM, pages 373–382.",
    "year": 2015
  }, {
    "title": "Thread detection in dynamic text message streams",
    "authors": ["Dou Shen", "Qiang Yang", "Jian-Tao Sun", "Zheng Chen."],
    "venue": "SIGIR’06. ACM, pages 35–42.",
    "year": 2006
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "JMLR 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Learning sentimentspecific word embedding for twitter sentiment classification",
    "authors": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."],
    "venue": "ACL’14. pages 1555–1565.",
    "year": 2014
  }, {
    "title": "Learning online discussion structures by conditional random fields",
    "authors": ["Hongning Wang", "Chi Wang", "ChengXiang Zhai", "Jiawei Han."],
    "venue": "SIGIR’11. ACM, pages 435–444.",
    "year": 2011
  }, {
    "title": "Predicting thread discourse structure over technical web forums",
    "authors": ["Li Wang", "Marco Lui", "Su Nam Kim", "Joakim Nivre", "Timothy Baldwin."],
    "venue": "EMNLP’11. ACL, pages 13–25.",
    "year": 2011
  }, {
    "title": "Contextbased message expansion for disentanglement of interleaved text conversations",
    "authors": ["Lidan Wang", "Douglas W Oard."],
    "venue": "NAACL’09. ACL, pages 200–208.",
    "year": 2009
  }, {
    "title": "A compareaggregate model for matching text sequences",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "ICLR’17.",
    "year": 2017
  }, {
    "title": "Internet relay chat",
    "authors": ["Christopher C Werry."],
    "venue": "Computer-mediated communication: Linguistic, social and cross-cultural perspectives pages 47–63.",
    "year": 1996
  }, {
    "title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs",
    "authors": ["Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou."],
    "venue": "TACL 4:259–272.",
    "year": 2016
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "NIPS’15. pages 649–657.",
    "year": 2015
  }],
  "id": "SP:ce336a1b429f7fc2eccc83f741904155be26ec8c",
  "authors": [{
    "name": "Jyun-Yu Jiang",
    "affiliations": []
  }, {
    "name": "Francine Chen",
    "affiliations": []
  }, {
    "name": "Yan-Ying Chen",
    "affiliations": []
  }, {
    "name": "Wei Wang",
    "affiliations": []
  }],
  "abstractText": "An enormous amount of conversation occurs online every day, such as on chat platforms where multiple conversations may take place concurrently. Interleaved conversations lead to difficulties in not only following discussions but also retrieving relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. In this paper, we propose to leverage representation learning for conversation disentanglement. A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages. With the estimated similarity scores, our algorithm for conversation identification by similarity ranking (CISIR) then derives conversations based on highconfidence message pairs and pairwise redundancy. Experiments were conducted with four publicly available datasets of conversations from Reddit and IRC channels. The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement.",
  "title": "Learning to Disentangle Interleaved Conversational Threads with a Siamese Hierarchical Network and Similarity Ranking"
}