{
  "sections": [{
    "text": "1International Computer Science Institute and Department of Statistics, University of California at Berkeley, USA 2Department of Computer Science, Rensselaer Polytechnic Institute, USA. Correspondence to: Shusen Wang <shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nlem while greatly mitigating the statistical risks incurred by sketching."
  }, {
    "heading": "1. Introduction",
    "text": "Regression is one of the most fundamental problems in machine learning. The simplest and most thoroughly studied regression model is least squares regression (LSR). Given features X = [xT1 ; . . . ,x T n ] ∈ Rn×d and responses y = [y1, . . . , yn] T ∈ Rn, the LSR problem minw ‖Xw − y‖22 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms. Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.\nThis paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011; Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX. Here S ∈ Rn×s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw − STy‖22 instead of the original LSR problem. Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt + Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching. For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform (Drineas et al., 2011), and Ts = O(nd) when S is a CountSketch matrix (Clarkson & Woodruff, 2013).\nThere has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews (Mahoney, 2011;\n1The condition number of XTSSTX is very close to that of XTX, and thus the number of iterations t is almost unchanged.\nWoodruff, 2014) and the references therein.\nThe concept of sketched LSR originated in the theoretical computer science literature, e.g., (Drineas et al., 2006; 2011), where the behavior of sketched LSR was studied from an optimization perspective. Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR. This line of work established that if s = O(d/ + poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most\ntimes worse than ‖Xw?−y ∥∥2 2 . These works also bounded ‖w̃−w?‖22 in terms of the difference in the objective function values and the condition number of XTX.\nA more recent line of work has studied sketched LSR from a statistical perspective: (Ma et al., 2015; Raskutti & Mahoney, 2016; Pilanci & Wainwright, 2015; Wang et al., 2016b) considered statistical properties of sketched LSR like the bias and variance. In particular, Pilanci & Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.\nBoth of these perspectives are important and of practical interest. The optimization perspective is relevant when the data can be taken as deterministic values. The statistical perspective is relevant in machine learning and statistics applications where the data are random, and the regression coefficients are therefore themselves random variables.\nIn practice, regularized regression, e.g., ridge regression and LASSO, exhibit more attractive bias-variance tradeoffs and generalization errors than vanilla LSR. Furthermore, the matrix generalization of LSR, where multiple responses are to be predicted, is often more useful than LSR. However, the properties of sketched regularized matrix regression are largely unknown. Hence, the question: How, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize to sketched regularized regression? We answer this question for sketched matrix ridge regression (MRR).\nRecall that X is n× d. Let Y ∈ Rn×m denote the matrix of corresponding responses. We study the MRR problem\nmin W\n{ f(W) , 1\nn ∥∥XW −Y∥∥2 F + γ‖W‖2F } , (1)\nwhich has optimal solution\nW? = (XTX + nγId) †XTY. (2)\nHere, (·)† denotes the Moore-Penrose inversion operation.\nLSR is a special case of MRR, with m = 1 and γ = 0. The optimal solution W? can be obtained in O(nd2 + nmd) time using a QR decomposition of X. Sketching can be applied to MRR in two ways:\nWc = (XTSSTX + nγId) †(XTSSTY), (3) Wh = (XTSSTX + nγId) †XTY. (4)\nFollowing the convention of Pilanci & Wainwright (2015); Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch, which approximate the optimal solution W?. Table 1 lists the time costs of the three solutions to MRR."
  }, {
    "heading": "1.1. Main Results and Contributions",
    "text": "We first study classical and Hessian sketches from the optimization perspective. Theorems 1 and 2 show that\n• Classical sketch achieves relative error in the objective value. With sketch size s = Õ(d/ ), the objective satisfies f(Wc) ≤ (1 + )f(W?).\n• Hessian sketch does not achieve relative error in the objective value. In particular, if 1n‖Y‖ 2 F is much\nlarger than f(W?), then f(Wh) can be far larger than f(W?).\n• For both classical and Hessian sketch, the relative quality of approximation improves as the regularization parameter γ increases.\nWe then study classical and Hessian sketches from the statistical perspective, by modeling Y = XW0 + Ξ as the sum of a true linear model and random noise, decomposing the risk R(W) = E‖XW − XW0‖2F into bias and variance terms, and bounding these terms. We draw the following conclusions (see Theorems 4, 5, 6):\n• The bias of the classical sketch can be nearly as small as that of the optimal solution. The variance is Θ ( n s ) times that of the optimal solution; this bound\nis optimal. Therefore over-regularization, i.e., large γ, should be used to supress the variance. (As γ increases, the bias increases, and the variance decreases.)\n• Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution. However, Hessian sketch has high bias, especially when nγ is small compared to ‖X‖22. This indicates that over-regularization is necessary for Hessian sketch to have low bias.\nOur empirical evaluations bear out these theoretical results. In particular, in Section 4, we show in Figure 2 that even when the regularization parameter γ is fine-tuned, the risks of classical sketch and Hessian sketch are worse than that\nof the optimal solution by an order of magnitude. This is an empirical demonstration of the fact that the near-optimal properties of sketching from the optimization perspective are much less relevant in a statistical setting than its suboptimal statistical properties.\nWe propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors. Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively. Theorems 7, 8, 10, 11 give the following results:\n• Classical Sketch. Assume the sketch size s = Õ(d ) and ≤ 1g ; then the bound on f(W\nc) − f(W?) is proportional to g . Assume that s = Õ( d 2 ) and\n2 ≤ 1 g ; the bias does not increase; the variance bound is proportional to 1g .\n• Hessian Sketch. Assume that s = Õ(d ) and ≤ 1 g2 ;\nthen the bound on f(Wh) − f(W?) is proportional to g2 . Assume that s = Õ( d 2 ); the variance does not increase; if, additionally, ≤ 1g and nγ is much smaller than the squared spectral norm of X, then the bias bound is proportional to g .\nNote that classical sketch with uniform sampling and model averaging is very well known as bagging (Breiman, 1996) (or pasting (Breiman, 1999) or bootstrap aggregating). Different from bagging, our model averaging approach is not limited to uniform sampling.\nClassical sketch with model averaging has three immediate applications. In the single-machine setting,\n• Classical sketch with model averaging offers a way to improve the statistical performance in the presence of heavy noise. Assume the sketch size is s = Õ( √ nd).\nAs g grows larger than ns , the variance of the averaged solution can be even lower than the optimal solution. See Remark 1 for further discussion. This observation is in accordance with the observation that bagging reduces variance.\nIn the distributed setting, the feature-response pairs (x1,y1), · · · , (xn,yn) ∈ Rd × Rm are divided among g machines. Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement. In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication complexity and latency, and it suggests two further applications of classical sketch with model averaging:\n• Model Averaging for Machine Learning. If a lowprecision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication. If ng is big enough compared to d and the row coherence of X is small, then “one-shot” model averaging has bias and variance comparable to the optimal solution.\n• Model Averaging for Optimization. If a highprecision solution to MRR is required, then an iterative numerical optimization algorithm must be used. The cost of such numerical optimization algorithms heavily depends on the quality of the initialization.2\nA good initialization saves lots of iterations. The averaged model is provably close to the optimal solution, so model averaging provides a high-quality initialization for more expensive algorithms."
  }, {
    "heading": "1.2. Prior Work",
    "text": "The body of work on sketched LSR mentioned earlier (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) shares many similarities with our results. However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered.\nLu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis. Our setting differs in that we consider n d, reduce the number of samples by sketching, and allow for multiple responses.\nThe model averaging analyzed in this paper is similar in spirit to the AVGM algorithm of (Zhang et al., 2013). When classical sketch is used with uniform row sampling without replacement, our model averaging procedure is a special case of AVGM. However, our results do not follow from those of (Zhang et al., 2013): first, we make no assumption on the data, whereas they assumed x1, · · · ,xn are i.i.d. from an unknown distribution; second, our results apply to many other sketching ensembles than uniform sampling without replacement; and third, we provide both optimization and statistical perspectives, whereas they provide only a statistical perspective. Our results clearly indicate that the\n2For example, the conjugate gradient method satisfies ‖W(t)−W?‖2F ‖W(0)−W?‖2\nF\n≤ θt1; the stochastic block coordinate descent (Tu\net al., 2016) satisfies Ef(W (t))−f(W?)\nf(W(0))−f(W?) ≤ θ t 2. Here W(t) is the\noutput of the t-th iteration; θ1, θ2 ∈ (0, 1) depend on the condition number of XTX + nγId and some other factors.\nperformance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013). For similar reasons, our work is different from the divide-andconquer kernel ridge regression algorithm of (Zhang et al., 2015).\nIterative Hessian sketch has been studied by Pilanci & Wainwright (2015); Wang et al. (2016a). By way of comparison, all the algorithms in this paper are “one-shot” rather than iterative. Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016; Thanei et al., 2017). Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging."
  }, {
    "heading": "1.3. Paper Organization",
    "text": "Section 2 defines our notation and introduces the sketching schemes we consider. Section 3 presents our theoretical results. Section 4 conducts experiments to verify our theories and demonstrates the usefulness of model averaging. Proofs of our claims and more empirical evaluations can be found in the technical report version (Wang et al., 2017)."
  }, {
    "heading": "2. Preliminaries",
    "text": "Throughout, we take In to be the n×n identity matrix and 0 to be a vector or matrix of all zeroes of the appropriate size. Given a matrix A = [aij ], the i-th row is denoted by ai:, and a:j denotes the j-th column. The Frobenius and spectral norms of A are written as, respectively, ‖A‖F and ‖A‖2. The set {1, 2, · · · , n} is written [n]. Let O, Ω, and Θ be the standard asymptotic notation. Let Õ conceal logarithm factors.\nThroughout, we fix X ∈ Rn×d as our matrix of features. We set ρ = rank(X) and write the SVD of X as X = UΣVT , where U, Σ, V are respectively n × ρ, ρ × ρ, and d × ρ matrices. We let σ1 ≥ · · · ≥ σρ > 0 be the singular values of X. The Moore-Penrose inverse of X is defined by X† = VΣ−1UT . The row leverage scores of X are li = ‖u:i‖22 for i ∈ [n]. The row coherence of X is µ(X) = nρ maxi ‖u:i‖ 2 2. Throughout, we let µ be shorthand for µ(X).\nMatrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear regression. We denote the process of sketching a matrix X ∈ Rn×d by X′ = STX. Here, S ∈ Rn×s is called a sketching matrix and X′ ∈ Rs×d is called a sketch of X. In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled fromN (0, 1/s)), the sketching matrix S is not formed explicitly. Matrix sketching can be accomplished by random sampling or random projection.\nRandom sampling corresponds to sampling rows of X\ni.i.d. with replacement according to given row sampling probabilities p1, · · · , pm ∈ (0, 1). The corresponding (random) sketching matrix S ∈ Rn×s has exactly one non-zero entry per column, whose position indicates the index of the selected row; in practice, this S is not explicitly formed. Uniform sampling fixes p1 = · · · = pn = 1n . Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X. In practice shrinked leverage score sampling can be a better choice than leverage score sampling (Ma et al., 2015). The sampling probabilities of shrinked leverage score sampling are defined by pi = 12 ( li∑n j=1 lj + 1n ) .3\nGaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson & Lindenstrauss, 1984). Let G ∈ Rm×s be a standard Gaussian matrix, i.e., each entry is sampled independently from N (0, 1). The matrix S = 1√\ns G is a Gaussian projection\nmatrix. It takes O(nds) time to apply S ∈ Rn×s to any n × d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.\nSubsampled randomized Hadamard transform (SRHT) (Drineas et al., 2011; Lu et al., 2013; Tropp, 2011) is a more efficient alternative to Gaussian projection. Let Hn ∈ Rn×n be the Walsh-Hadamard matrix with +1 and −1 entries, D ∈ Rn×n be a diagonal matrix with diagonal entries sampled uniformly from {+1,−1}, and P ∈ Rn×s be the uniform row sampling matrix defined above. The matrix S = 1√\nn DHnP ∈ Rn×s is an SRHT matrix, and\ncan be applied to any n × d matrix in O(nd log s) time. In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n. Their performance and theoretical analyses are very similar.\nCountSketch can be applied to any X ∈ Rn×d in O(nd) time (Charikar et al., 2004; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013; Pham & Pagh, 2013; Weinberger et al., 2009). Though more efficient to apply, CountSketch requires a bigger sketch size than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees. The readers can refer to (Woodruff, 2014) for a detailed description of CountSketch."
  }, {
    "heading": "3. Main Results",
    "text": "Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives. Sec-\n3In fact, pi can be any convex combination of li∑n j=1 lj and 1 n\n(Ma et al., 2015). We use the weight 1 2 for simplicity; our conclusions extend in a straightforward manner to other weightings.\ntions 3.3 and 3.4 capture the impacts of model averaging on, respectively, the optimization and statistical properties of sketched MRR.\nWe described six sketching methods in Section 2. For simplicity, in this section, we refer to leverage score sampling, shrinked leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch. The notation defined in Table 2 are used throughout."
  }, {
    "heading": "3.1. Sketched MRR: Optimization Perspective",
    "text": "Theorem 1 shows that f(Wc), the objective value of classical sketch, is very close to the optimal objective value f(W?). The approximation quality improves as γ increases. Theorem 1 (Classical Sketch). For the four sketching methods with s = Õ ( βd ) , uniform sampling with s =\nO ( µβd log d ) , and CountSketch with s = O ( βd2 ) , the inequality\nf(Wc)− f(W?) ≤ f(W?)\nholds with probability at least 0.9.\nThe corresponding guarantee for the performance of Hessian sketch is given in Theorem 2. It is weaker than the guarantee for classical sketch, especially when 1n‖Y‖ 2 F is far larger than f(W?). If Y is nearly noiseless—Y is wellexplained by a linear combination of the columns of X— and γ is small, then f(W?) is close to zero, and consequently f(W?) can be far smaller than 1n‖Y‖ 2 F . Therefore, in this case which is ideal for MRR, f(Wh) is not close to f(W?) and our theory suggests Hessian sketch does not perform as well as classical sketch. This is verified by our experiments, which show that unless γ is big or a large portion of Y is outside the column space of X, the ratio f(W\nh) f(W?) can be large.\nTheorem 2 (Hessian Sketch). For the four sketching methods with s = Õ\n( β2d ) , uniform sampling with s =\nO ( µβ2d log d ) , and CountSketch with s = O(β 2d2\n), the inequality\nf(Wh)− f(W?) ≤ ( ‖Y‖2F n − f(W?) ) .\nholds with probability at least 0.9.\nThese two results imply that f(Wc) and f(Wh) can be close to f(W?). When this is the case, curvature of the objective function ensures that the sketched solutions Wc and Wh are close to the optimal solution W?. Lemma 3 studies the Mahalanobis distance ‖M(W−W?)‖2F . Here M is any non-singular matrix; in particular, it can be the identity matrix or (XTX)1/2. Lemma 3. Let f be the objective function of MRR defined in (1), W ∈ Rd×m be arbitrary, and W? be the optimal solution defined in (2). For any non-singular matrix M, the Mahalanobis distance satisfies 1\nn\n∥∥M(W −W?)∥∥2 F ≤ f(W)− f(W ?)\nσ2min [ (XTSSTX + nγId)1/2M−1 ] . By choosing M = (XTX)1/2, we can bound 1n‖XW − XW?‖2F in terms of the difference in the objective values:\n1 n ∥∥XW −XW?∥∥2 F ≤ β [ f(W)− f(W?) ] .\nWith Lemma 3, we can directly apply Theorems 1 or 2 to bound 1n‖XW c −XW?‖2F or 1n‖XW h −XW?‖2F ."
  }, {
    "heading": "3.2. Sketched MRR: Statistical Perspective",
    "text": "We consider the following fixed design model. Let X ∈ Rn×d be the observed feature matrix, W0 ∈ Rd×m be the true and unknown model, Ξ ∈ Rn×m contain unknown random noise, and\nY = XW0 + Ξ (5)\nbe the observed responses. We make the following standard weak assumptions on the noise:\nE[Ξ] = 0 and E[ΞΞT ] = ξ2In. We observe X and Y and seek to estimate W0.\nWe can evaluate the quality of the estimate by the risk: R(W) = 1 n E ∥∥XW −XW0∥∥2F , (6)\nwhere the expectation is taken w.r.t. the noise Ξ. We study the risk functions R(W?), R(Wc), and R(Wh) in the following. Theorem 4 (Bias-Variance Decomposition). We consider the data model described in this subsection. Let W be W?, Wc, or Wh, as defined in (2), (3), (4), respectively; then the risk function can be decomposed as\nR(W) = bias2(W) + var(W).\nRecall the SVD of X: X = UΣVT . The bias and variance terms can be written as\nbias ( W? ) = γ √ n ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥\nF ,\nvar ( W? ) = ξ2\nn ∥∥∥(Iρ + nγΣ−2)−1∥∥∥2 F ,\nbias ( Wc ) = γ √ n ∥∥∥(ΣUTSSTUΣ + nγIρ)†ΣVTW0∥∥∥\nF ,\nvar ( Wc ) = ξ2\nn ∥∥∥(UTSSTU + nγΣ−2)†UTSST∥∥∥2 F ,\nbias ( Wh ) = γ √ n ∥∥∥(Σ−2 + UTSSTU−Iρnγ )\n· ( UTSSTU + nγΣ−2 )† ΣVTW0 ∥∥∥ F ,\nvar ( Wh ) = ξ2\nn ∥∥∥(UTSSTU + nγΣ−2)†∥∥∥2 F .\nTheorem 5 provides upper and lower bounds on the bias and variance of the classical sketch. In particular, we see that that bias(Wc) is within a factor of (1± ) of bias(W?). However, var(Wc) is Θ(ns ) times worse than var(W ?). Theorem 5 (Classical Sketch). For Gaussian projection and SRHT sketching with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), or CountSketch with s = O( d2\n2 ), the inequalities\n1− ≤ bias(W c)\nbias(W?) ≤ 1 + ,\n(1− )n s ≤ var(W\nc)\nvar(W?) ≤ (1 + )n s\nhold with probability at least 0.9.\nFor shrinked leverage score sampling with s = O(d log d 2 ), these inequalities, except for the lower bound on the variance,4 hold with probability at least 0.9.\nTheorem 6 establishes similar upper and lower bounds on the bias and variance of Hessian sketch. The situation is the reverse of that with classical sketch: the variance of Wh is close to that of W? if s is large enough, but as the regularization parameter γ goes to zero, bias(Wh) becomes much larger than bias(W?). Theorem 6 (Hessian Sketch). For the four sketching methods with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), and CountSketch with s = O( d2\n2 ), the inequalities\nbias(Wh) bias(W?) ≤ (1 + )\n( 1 +\n‖X‖22 nγ\n) ,\n1− ≤ var(W h)\nvar(W?) ≤ 1 +\nhold with probability at least 0.9. Further assume that σ2ρ ≥ nγ . Then\nbias(Wh)\nbias(W?) ≥ 1 1 + ( σ2ρ nγ − 1 )\nholds with probability at least 0.9.\nThe lower bound on the bias shows that Hessian sketch can suffer from a much higher bias than the optimal solution. The gap between bias(Wh) and bias(W?) can be\n4For shrinked leverage score sampling, ‖S‖22 does not enjoy nontrivial lower bound. This is why we do not have a lower bound on the variance.\nlessened by increasing the regularization parameter γ, but such over-regularization increases the baseline bias(W?) itself. It is also worth mentioning that unlike bias(W?) and bias(Wc), bias(Wh) is not monotonically increasing with γ, as is empirically verified in Figure 2.\nIn sum, our theories show that classical and Hessian sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of regularization. In practice, the regularization parameter γ should be tuned to optimize the prediction accuracy. Our experiments in Figure 2 show that even with fine-tuned γ, the risks of classical and Hessian sketches can be higher than the risk of the optimal solution by an order of magnitude. Formally speaking, minγ R(Wc) minγ R(W?) and minγ R(Wh) minγ R(W?) hold in practice.\nOur empirical study in Figure 2 suggests classical and Hessian sketches both require over-regularization, i.e., setting γ larger than what is best for the optimal solution W?. Formally speaking, argminγ R(W c) > argminγ R(W ?) and argminγ R(W h) > argminγ R(W\n?). Although this is the case for both types of sketches, the underlying explanations are different. Classical sketch has a high variance, so a large γ is required to supress the variance (its variance is non-increasing with γ). Hessian sketch has very high bias when γ is small, so a reasonably large γ is necessary to lower its bias."
  }, {
    "heading": "3.3. Model Averaging: Optimization Perspective",
    "text": "We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions. The model averaging procedure is straightforward: one independently draws g sketching matrices S1, · · · ,Sg ∈ Rn×s, uses these to form g sketched MRR solutions, denoted by {Wci} g i=1 or {Whi} g i=1, and averages these solutions to obtain the fi-\nnal estimate Wc = 1g ∑g i=1 W c i or W h = 1g ∑g i=1 W h i . Practical applications of model averaging are enumerated in Section 1.1.\nTheorems 7 and 8 present guarantees on the optimization accuracy of using model averaging to combine the classical or Hessian sketch solutions. We can contrast these with the guarantees provided for sketched MRR in Theorems 1 and 2. For classical sketch with model averaging, we see that when ≤ 1g , the bound on f(W\nh)−f(W?) is proportional to /g. From Lemma 3 we can see that the distances from Wc to W? also decreases accordingly.\nTheorem 7 (Classical Sketch with Model Averaging). For the four methods, let s = Õ\n( βd ) ; for uniform sampling, let\ns = O ( µβd log d ) . Then the inequality\nf(Wc)− f(W?) ≤ ( g + β2 2 ) f(W?)\nholds with probability at least 0.8.\nFor Hessian sketch with model averaging, if β2 ≤ 1 g2 , then the bound on f(Wh)− f(W?) is proportional to g2 .\nTheorem 8 (Hessian Sketch with Model Averaging). For the four methods let s = Õ\n( β2d ) , and for uniform sam-\npling let s = O ( µβ2d log d ) , then the inequality\nf(Wh)− f(W?) ≤ ( g2 + 2 β2 ) ( ‖Y‖2F n − f(W?) ) .\nholds with probability at least 0.8."
  }, {
    "heading": "3.4. Model Averaging: Statistical Perspective",
    "text": "Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketch solutions. Our first result conducts a bias-variance decomposition for the averaged solution of sketched MRR.\nTheorem 9 (Bias-Variance Decomposition). We consider the fixed design model (5). The risk function defined in (6) can be decomposed as\nR(W) = bias2(W) + var(W).\nThe bias and variance terms are bias ( Wc ) = γ √ n ∥∥∥1 g g∑ i=1 ( ΣUTSiS T i UΣ + nγIρ )† ΣVTW0 ∥∥∥ F , var ( Wc ) = ξ2\nn ∥∥∥1 g g∑ i=1 ( UTSiS T i U + nγΣ −2)†UTSiSTi ∥∥∥2 F ,\nbias ( Wh ) = γ √ n ∥∥∥1 g g∑ i=1 ( Σ−2 + UTSiS T i U− Iρ nγ ) · ( UTSiS T i U + nγΣ −2)†ΣVTW0∥∥∥ F ,\nvar ( Wh ) = ξ2\nn ∥∥∥1 g g∑ i=1 ( UTSiS T i U + nγΣ −2)†∥∥∥2 F .\nTheorems 10 and 11 provide upper bounds on the bias and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch. We can contrast them with Theorems 5 and 6 to see the statistical benefits of model averaging. Theorem 10 (Classical Sketch with Model Averaging). For shrinked leverage score sampling, Gaussian projection, SRHT with s = Õ ( d 2 ) , or uniform sampling with\ns = O ( µd log d 2 ) , the inequalities\nbias(Wc) bias(W?) ≤ 1 + , var(Wc)\nvar(W?) ≤ n s (√ 1+ /g g + )2\nhold with probability at least 0.8. Remark 1. From this result, we see that if ≤ 1√g , then the variance is proportional to 1g . If g and s are at least\ng = O (n s ) and s = Õ (√ nd ) ,\nthen the risk R(Wc) is close to R(W?). If g and s are larger, then the variance var(Wc) can even be even lower than var(W?).\nTheorem 11 shows that model averaging decreases the bias of Hessian sketch without increasing the variance. For Hessian sketch without model averaging, recall that bias(Wh)\nis larger than bias(W?) by a factor of O(‖X‖22/(nγ)). Theorem 11 shows that model averaging reduces this ratio by a factor of g when ≤ 1 g .\nTheorem 11 (Hessian Sketch with Model Averaging). For the four methods with s = Õ ( d 2 ) , or uniform sampling\nwith s = O ( µd log d 2 ) , the inequalities\nbias(Wh) bias(W?) ≤ 1 + + ( g + 2 )‖X‖22 nγ ,\nvar(Wh) var(W?) ≤ 1 +\nhold with probability at least 0.8."
  }, {
    "heading": "4. Sketched Ridge Regression Experiments",
    "text": "Following (Ma et al., 2015; Yang et al., 2016), we constructed X ∈ Rn×d to have condition number κ(XTX) = 1012 and high row coherence, fixed w0 = [10.2d; 0.110.6d; 10.2d], and set y = Xw0 +ε ∈ Rn, where the entries of ε ∈ Rn were i.i.d. sampled from N (0, ξ2). The details of this data model are given in the technical report version (Wang et al., 2017). Let S ∈ Rn×s be any of the six sketching methods considered in this paper. We fix n = 105, d = 500, and s = 5, 000. Because the analytical expressions involve the random sketching matrix S, we randomly generate S, repeat this procedure 10 times, and report the averaged results.\nIn Figure 1, we plot the objective function value f(w) = 1 n‖Xw − y‖ 2 2 + γ‖w‖22 against γ, under different settings of noise intensity ξ. The results verify our theory: classical sketch wc is always close to optimal; Hessian sketch wh is much worse than the optimal when γ is small and y is mostly in the column space of X.\nWe conducted experiments on synthetic data to verify Theorems 5 and 6 and to show the effects of classical and Hessian sketching on the bias and variance. We set the noise intensity to be ξ = 0.1. In Figure 2, we plot the analytical expressions for the squared bias, variance, and risk stated in Theorem 4 against the regularization parameter γ. The results of this experiment match our theory: classical sketch magnified the variance, and Hessian sketch increased the bias. Even if γ is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal solution.5 Our experiments also indicate that classical and Hessian sketches require setting γ larger than the best regularization parameter for the optimal solution W?."
  }, {
    "heading": "5. Conclusions",
    "text": "We studied sketched matrix ridge regression (MRR) from optimization and statistical perspectives. Using classical sketch, by taking a large enough sketch, one can obtain an -accurate approximate solution. Counterintuitively and in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both classical and Hessian sketches can have statistical risks that are worse than the risk of the optimal solution by an order of magnitude. We proposed the use of model averaging to attain better optimization and statistical properties. We have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning.\n5In the experiment yielding Figure 2, Hessian sketch had lower risk than classical sketch. This is not generally true: if we used a smaller ξ, so that the variance is dominated by bias, then classical sketch results in lower risks than Hessian sketch."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the anonymous reviewers for their helpful suggestions. We thank the Army Research Office and the Defense Advanced Research Projects Agency for partial support of this work."
  }],
  "year": 2017,
  "references": [{
    "title": "Sharper bounds for regression and low-rank approximation with regularization",
    "authors": ["Avron", "Haim", "Clarkson", "Kenneth L", "Woodruff", "David P"],
    "venue": "arXiv preprint arXiv:1611.03225,",
    "year": 2016
  }, {
    "title": "Pasting small votes for classification in large databases and on-line",
    "authors": ["Breiman", "Leo"],
    "venue": "Machine Learning,",
    "year": 1999
  }, {
    "title": "Finding frequent items in data streams",
    "authors": ["Charikar", "Moses", "Chen", "Kevin", "Farach-Colton", "Martin"],
    "venue": "Theoretical Computer Science,",
    "year": 2004
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["Clarkson", "Kenneth L", "Woodruff", "David P"],
    "venue": "In Annual ACM Symposium on theory of computing (STOC),",
    "year": 2013
  }, {
    "title": "Sampling algorithms for `2 regression and applications",
    "authors": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan"],
    "venue": "In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),",
    "year": 2006
  }, {
    "title": "Faster least squares approximation",
    "authors": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan", "Sarlós", "Tamás"],
    "venue": "Numerische Mathematik,",
    "year": 2011
  }, {
    "title": "Fast approximation of matrix coherence and statistical leverage",
    "authors": ["Drineas", "Petros", "Magdon-Ismail", "Malik", "Mahoney", "Michael W", "Woodruff", "David P"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Extensions of Lipschitz mappings into a Hilbert space",
    "authors": ["Johnson", "William B", "Lindenstrauss", "Joram"],
    "venue": "Contemporary mathematics,",
    "year": 1984
  }, {
    "title": "Faster ridge regression via the subsampled randomized Hadamard transform",
    "authors": ["Lu", "Yichao", "Dhillon", "Paramveer", "Foster", "Dean P", "Ungar", "Lyle"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "A statistical perspective on algorithmic leveraging",
    "authors": ["Ma", "Ping", "Mahoney", "Michael W", "Yu", "Bin"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "Randomized algorithms for matrices and data",
    "authors": ["Mahoney", "Michael W"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression",
    "authors": ["Meng", "Xiangrui", "Mahoney", "Michael W"],
    "venue": "In Annual ACM Symposium on Theory of Computing (STOC),",
    "year": 2013
  }, {
    "title": "Faster numerical linear algebra algorithms via sparser subspace embeddings",
    "authors": ["Nelson", "John", "Nguyên", "Huy L. Osnap"],
    "venue": "In IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
    "year": 2013
  }, {
    "title": "Fast and scalable polynomial kernels via explicit feature maps",
    "authors": ["Pham", "Ninh", "Pagh", "Rasmus"],
    "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
    "year": 2013
  }, {
    "title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained leastsquares",
    "authors": ["Pilanci", "Mert", "Wainwright", "Martin J"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "A statistical perspective on randomized sketching for ordinary least-squares",
    "authors": ["Raskutti", "Garvesh", "Mahoney", "Michael W"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Random projections for large-scale regression",
    "authors": ["Thanei", "Gian-Andrea", "Heinze", "Christina", "Meinshausen", "Nicolai"],
    "venue": "arXiv preprint arXiv:1701.05325,",
    "year": 2017
  }, {
    "title": "Improved analysis of the subsampled randomized Hadamard transform",
    "authors": ["Tropp", "Joel A"],
    "venue": "Advances in Adaptive Data Analysis,",
    "year": 2011
  }, {
    "title": "Large scale kernel learning using block coordinate descent",
    "authors": ["Tu", "Stephen", "Roelofs", "Rebecca", "Venkataraman", "Shivaram", "Recht", "Benjamin"],
    "venue": "arXiv preprint arXiv:1602.05310,",
    "year": 2016
  }, {
    "title": "Sketching meets random projection in the dual: A provable recovery algorithm for big and highdimensional data",
    "authors": ["Wang", "Jialei", "Lee", "Jason D", "Mahdavi", "Mehrdad", "Kolar", "Mladen", "Srebro", "Nathan"],
    "venue": "arXiv preprint arXiv:1610.03045,",
    "year": 2016
  }, {
    "title": "Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging",
    "authors": ["Wang", "Shusen", "Gittens", "Alex", "Mahoney", "Michael W"],
    "venue": "arXiv preprint arXiv:1702.04837,",
    "year": 2017
  }, {
    "title": "Computationally feasible near-optimal subset selection for linear regression under measurement constraints",
    "authors": ["Wang", "Yining", "Yu", "Adams Wei", "Singh", "Aarti"],
    "venue": "arXiv preprint arXiv:1601.02068,",
    "year": 2016
  }, {
    "title": "Feature hashing for large scale multitask learning",
    "authors": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2009
  }, {
    "title": "A fast randomized algorithm for the approximation of matrices",
    "authors": ["Woolfe", "Franco", "Liberty", "Edo", "Rokhlin", "Vladimir", "Tygert", "Mark"],
    "venue": "Applied and Computational Harmonic Analysis,",
    "year": 2008
  }, {
    "title": "Implementing randomized matrix algorithms in parallel and distributed environments",
    "authors": ["Yang", "Jiyan", "Meng", "Xiangrui", "Mahoney", "Michael W"],
    "venue": "Proceedings of the IEEE,",
    "year": 2016
  }, {
    "title": "Communication-efficient algorithms for statistical optimization",
    "authors": ["Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates",
    "authors": ["Zhang", "Yuchen", "Duchi", "John", "Wainwright", "Martin"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }],
  "id": "SP:892d2ce6d742cd246e747a06c5509a77a8193ef5",
  "authors": [{
    "name": "Shusen Wang",
    "affiliations": []
  }, {
    "name": "Alex Gittens",
    "affiliations": []
  }, {
    "name": "Michael W. Mahoney",
    "affiliations": []
  }],
  "abstractText": "We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the “mass” in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR probInternational Computer Science Institute and Department of Statistics, University of California at Berkeley, USA Department of Computer Science, Rensselaer Polytechnic Institute, USA. Correspondence to: Shusen Wang <shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). lem while greatly mitigating the statistical risks incurred by sketching.",
  "title": "Sketched Ridge Regression: Optimization Perspective,  Statistical Perspective, and Model Averaging"
}