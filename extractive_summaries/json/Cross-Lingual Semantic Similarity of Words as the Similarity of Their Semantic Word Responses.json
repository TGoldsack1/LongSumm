{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2013, pages 106–116, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Cross-lingual semantic word similarity addresses the task of detecting words that refer to similar semantic concepts and convey similar meanings across languages. It ultimately boils down to the automatic identification of translation pairs, that is, bilingual lexicon extraction (BLE). Such lexicons and semantically similar words serve as important resources\nin cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005).\nFrom parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vulić et al., 2011; Prochasson and Fung, 2011).\nApproaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, wS1 given in the source language LS with vocabulary V S and wT2 in the target language LT with vocabulary V T is then:\nSim(wS1 , w T 2 ) = SF (cv(w S 1 ), cv(w T 2 )) (1)\ncv(wS1 ) = [sc S 1 (c1), . . . , sc S 1 (cN )] denotes a context vector for wS1 with N context features ck, where scS1 (ck) denotes the score for w S 1 associated with context feature ck (similar for wT2 ). SF is a similarity function (e.g., cosine, the Kullback-Leibler\n106\ndivergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007).\nIn order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011; Vulić et al., 2011). Context vectors cv(wS1 ) and cv(w T 2 ) for both source and target words are then compared in the semantic space independently of their respective languages.\nIn this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word association. We borrow that concept from the psychology/cognitive science literature. Semantic word responding addresses a task that requires participants to produce first words that come to their mind that are related to a presented cue word (Nelson et al., 2000; Steyvers et al., 2004).\nThe new cross-lingual semantic space is spanned by all vocabulary words in the source and the target language. Each axis in the space denotes a semantic word response. The similarity between two words is then computed as the similarity between the vectors comprising their semantic word responses using any of existing SF -s. Two words are considered semantically similar if they are likely to generate similar semantic word responses and assign similar importance to them.\nWe utilize a shared semantic space of latent crosslingual topics learned by a multilingual probabilistic topic model to obtain semantic word responses and quantify the strength of association between any cue word and its responses monolingually and across languages, and, consequently, to build semantic response vectors. That effectively translates the task of word similarity from the semantic space spanned by latent cross-lingual topics to the semantic space spanned by all vocabulary words in both languages.\nThe main contributions of this article are:\n• We propose a new approach to modeling crosslingual semantic similarity of words based on the similarity of their semantic word responses.\n• We present how to estimate and quantify semantic word responses by means of a multilingual probabilistic topic model.\n• We demonstrate how to employ our novel paradigm that relies on semantic word responding in the task of bilingual lexicon extraction (BLE) from comparable data.\n• We show that the response-based model of similarity is more robust and obtains better results for BLE than the models that operate in the semantic space spanned by latent semantic concepts, i.e., cross-lingual topics directly.\nThe following sections first review relevant prior work and provide a very short introduction to multilingual probabilistic topic modeling, then describe our response-based approach to modeling crosslingual semantic word similarity, and finally present our evaluation and results on the BLE task for a variety of language pairs."
  }, {
    "heading": "2 Related Work",
    "text": "When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (Déjean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods\ndeveloped in such a setting applicable even on distant language pairs with scarce resources.\nRecently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vulić et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daumé III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity."
  }, {
    "heading": "3 Modeling Word Similarity as the Similarity of Semantic Word Responses",
    "text": "This section contains a detailed description of our semantic word similarity method that relies on semantic word responses. Since the method utilizes the concept of multilingual probabilistic topic modeling, we first provide a very short overview of that concept, then present the intuition behind the approach, and finally describe our method in detail."
  }, {
    "heading": "3.1 Multilingual Probabilistic Topic Modeling",
    "text": "Assume that we are given a multilingual corpus C of l languages, and C is a set of text collections {C1, . . . , Cl} in those languages. A multilingual probabilistic topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daumé III, 2010; Zhang et al., 2010) of a multilingual corpus C is defined as a set of semantically coherent multinomial distributions of words with values Pj(w j i |zk), j = 1, . . . , l, for each vocabulary V 1, . . . , V j , . . . , V l associated with text collections C1, . . . , Cj , . . . , Cl ∈ C given in languages L1, . . . , Lj , . . . , Ll. Pj(w j i |zk) is calculated for eachwji ∈ V j . The probability scores Pj(w j i |zk) build per-topic word distributions, and they consti-\ntute a language-specific representation (e.g., a probability value is assigned only for words from V j) of a language-independent cross-lingual latent concept, that is, latent cross-lingual topic zk ∈ Z . Z = {z1, . . . , zK} represents the set of all K latent cross-lingual topics present in the multilingual corpus. Each document in the multilingual corpus is thus considered a mixture of K cross-lingual topics from the set Z . That mixture for some document dji ∈ Cj is modeled by the probability scores Pj(zk|dji ) that altogether build per-document topic distributions.\nEach cross-lingual topic from the set Z can be observed as a latent language-independent concept present in the multilingual corpus, but each language in the corpus uses only words from its own vocabulary to describe the content of that concept. For instance, having a multilingual collection in English, Spanish and Dutch and discovering a topic on Soccer, that cross-lingual topic would be represented by words (actually probabilities over words) {player, goal, coach, . . .} in English, {balón (ball), futbolista (soccer player), goleador (scorer), . . .} in Spanish, and {wedstrijd (match), elftal (soccer team), doelpunt (goal), . . .} in Dutch. We have∑\nwji∈V j Pj(w\nj i |zk) = 1, for each vocabulary V j\nrepresenting language Lj , and for each topic zk ∈ Z . Therefore, the latent cross-lingual topics also span a shared cross-lingual semantic space."
  }, {
    "heading": "3.2 The Intuition Behind the Approach",
    "text": "Imagine the following thought experiment. A group of human subjects who have been raised bilingually and thus are native speakers of two languages LS and LT , is playing a game of word associations. The game consists of possibly an infinite number of iterations, and each iteration consists of 4 rounds. In the first round (the S-S round), given a word in the language LS , the subject has to generate a list of words in the same language LS that first occur to her/him as semantic word responses to the given word. The list is in descending order, with more prominent word responses occurring higher in the list. In the second round (the S-T round), the subject repeats the procedure, and generates the list of word responses to the same word from LS , but now in the other language LT . The third (the T-T round)\nand the fourth round (the T-S round) are similar to the first and the second round, but now a list of word responses in both LS and LT has to be generated for some cue word from LT . The process of generating the lists of semantic responses then continues with other cue words and other human subjects.\nAs the final result, for each word in the source language LS , and each word in the target language LT , we obtain a single list of semantic word responses comprising words in both languages. All lists are sorted in descending order, based on some association score that takes into account both the number of times a word has occurred as an associative response, as well as the position in the list in each round. We can now measure the similarity of any two words, regardless of their corresponding languages, according to the similarity of their corresponding lists that contain their word responses. Words that are equally likely to trigger the same associative responses in the human brain, and moreover assign equal importance to those responses, as provided in the lists of associative responses, are very likely to be closely semantically similar. Additionally, for a given word wS1 in the source language LS , some word wT2 in LT that has the highest similarity score among all words inLT should be a direct word-to-word translation of wS1 ."
  }, {
    "heading": "3.3 Modeling Semantic Word Responses via Cross-Lingual Topics",
    "text": "Cross-lingual topics provide a sound framework to construct a probabilistic model of the aforementioned experiment. To model semantic word responses via the shared space of cross-lingual topics, we have to set a probabilistic mass that quantifies the degree of association. Given two words w1, w2 ∈ V S ∪ V T , a natural way of expressing the asymmetric semantic association is by modeling the probability P (w2|w1) (Griffiths et al., 2007), that is, the probability to generate word w2 as a response given word w1. After the training of a multilingual topic model on a multilingual corpus, we obtain per-topic word distributions with scores PS(wSi |zk) and PT (wTi |zk) (see Sect. 3.1).1 The probability\n1A remark on notation throughout the paper: Since the shared space of cross-lingual topics allows us to construct a uniform representation for all words regardless of a vocabulary they belong to, due to simplicity and to stress the uniformity,\nP (w2|w1) is then decomposed as follows:\nResp(w1, w2) = P (w2|w1) = K∑\nk=1\nP (w2|zk)P (zk|w1) (2)\nThe probability scores P (w2|zk) select words that are highly descriptive for each particular topic. The probability scores P (zk|w1) ensure that topics zk that are semantically relevant to the given word w1 dominate the sum, so the overall high score Resp(w1, w2) of the semantic word response is assigned only to highly descriptive words of the semantically related topics. Using the shared space of cross-lingual topics, semantic response scores can be derived for any two words w1, w2 ∈ V S ∪ V T .1\nThe generative model closely resembles the actual process in the human brain - when we generate semantic word responses, we first tend to associate that word with a related semantic/cognitive concept, in this case a cross-lingual topic (the factor P (zk|w1)), and then, after establishing the concept, we output a list of words that we consider the most prominent/descriptive for that concept (words with high scores in the factor P (w2|zk)) (Nelson et al., 2000; Steyvers et al., 2004). Due to such modeling properties, this model of semantic word responding tends to assign higher association scores for high frequency words. It eventually leads to asymmetric associations/responses. We have detected that phenomenon both monolingually and across languages. For instance, the first response to Spanish word mutación (mutation) is English word gene. Other examples include caldera (boiler)-steam, deportista (sportsman)-sport, horario (schedule)-hour or pescador (fisherman)-fish. In the other association direction, we have detected top responses such as merchant-comercio (trade) or neologism-palabra (word). In the monolingual setting, we acquire English pairs such as songwriter-music, disciplinesport, or Spanish pairs gripe (flu)-enfermedad (disease), cuenca (basin)-rı́o (river), etc."
  }, {
    "heading": "3.4 Response-Based Model of Similarity",
    "text": "Eq. (2) provides a way to measure the strength of semantic word responses. In order to establish the\nwe sometimes use notation P (wi|zk) and P (zk|wi) instead of PS(wi|zk) or PS(zk|wi) (similar for subscript T ). However, the reader must be aware that, for instance, P (wi|zk) actually means PS(wi|zk) if wi ∈ V S , and PT (wi|zk) if wi ∈ V T .\nfinal similarity between two words, we have to compare their semantic response vectors, that is, their semantic response scores over all words in both vocabularies. The final model of word similarity closely mimics our thought experiment. First, for each word wSi ∈ V S , we generate probability scores P (wSj |wSi ) for all words wSj ∈ V S (the S-S rounds). Note that P (wSi |wSi ) is also defined by Eq. (2). Following that, for each word wSi ∈ V S , we generate probability scores P (wTj |wSi ), for all words wTj ∈ V T (the S-T rounds). Similarly, we calculate probability scores P (wTj |wTi ) and P (wSj |wTi ), for each wTi , w T j ∈ V T , and for each wSj ∈ V S (the T-T and T-S rounds). Now, each word wi ∈ V S ∪ V T may be represented by a (|V S |+ |V T |)-dimensional context vector cv(wi) as follows:2 [P (wS1 |wi), . . . , P (wS|V S ||wi), . . . , P (w T |V T ||wi)]. We have created a language-independent cross-\n2We assume that the two sets V S and V T are disjunct. It means that, for instance, Spanish word pie (foot) from V S and English word pie from V T are treated as two different word types. In that case, it holds |V S ∪ V T | = |V S |+ |V T |.\nlingual semantic space spanned by all vocabulary words in both languages. Each feature corresponds to one word from vocabularies V S and V T , while the exact score for each feature in the context vector cv(wi) is precisely the probability that this word/feature will be generated as a word response given word wi. The degree of similarity between two words is then computed on the basis of similarity between their feature vectors using some of the standard similarity functions (Cha, 2007).\nThe novel response-based approach of similarity removes the effect of high-frequency words that tend to appear higher in the lists of semantic word responses. Therefore, the real synonyms and translations should occur as top candidates in the lists of similar words obtained by the response-based method. That property may be exploited to identify one-to-one translations across languages and build a bilingual lexicon (see Table 1)."
  }, {
    "heading": "4 Experimental Setup",
    "text": ""
  }, {
    "heading": "4.1 Data Collections",
    "text": "We work with the following corpora:\n• IT-EN-W: A collection of 18, 898 ItalianEnglish Wikipedia article pairs previously used by Vulić et al. (2011).\n• ES-EN-W: A collection of 13, 696 SpanishEnglish Wikipedia article pairs.\n• NL-EN-W: A collection of 7, 612 DutchEnglish Wikipedia article pairs.\n• NL-EN-W+EP: The NL-EN-W corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments.\nAll corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types. We retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Again following their setup, we use TreeTagger (Schmid, 1994) for POS tagging and lemmatization."
  }, {
    "heading": "4.2 Multilingual Topic Model",
    "text": "The multilingual probabilistic topic model we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vulić et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual\n(Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vulić et al., 2011), the best results are obtained with the number of topics set to a few thousands (≈ 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and β = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper."
  }, {
    "heading": "4.3 Compared Methods",
    "text": "We evaluate and compare the following word similarity approaches in all our experiments: 1) The method that regards the lists of semantic word responses across languages obtained by Eq. (2) directly as the lists of semantically similar words (Direct-SWR). 2) The state-of-the-art method that employs a similarity function (SF) on theK-dimensional word vectors cv(wi) in the semantic space of latent crosslingual topics. The dimensions of the vectors are conditional topic distribution scores P (zk|wi) that are obtained by the multilingual topic model directly (Steyvers and Griffiths, 2007; Vulić et al., 2011). We have tested different SF-s (e.g., the Kullback-Leibler and the Jensen-Shannon divergence, the cosine measure), and have detected that in general the best scores are obtained when using the Bhattacharyya coefficient (BC) (Bhattacharyya, 1943; Kazama et al., 2010) (Topic-BC). 3) The best scoring similarity method from Vulić et al. (2011) named TI+Cue. This state-of-the-art method also operates in the semantic space of latent cross-lingual concepts/topics. 4) The response-based similarity described in Sect. 3. As for Topic-BC, we again use BC as the similarity function, but now on |V S ∪ V T |-dimensional context vectors in the semantic space spanned by all words in both vocabularies that represent semantic word responses (Response-BC). Given two N - dimensional word vectors cv(wS1 ) and cv(w T 2 ), the BC or the fidelity measure (Cha, 2007) is defined as:\nBC(cv(wS1 ), cv(w T 2 )) = N∑ n=1 √ scS1 (cn) · scT2 (cn) (3)\nFor the Topic-BC method N = K, while N = |V S ∪ V T | for Response-BC. Additionally, since P (zk|wi) > 0 and P (wk|wi) > 0 for each zk ∈ Z and each wk ∈ V S ∪ V T , a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3"
  }, {
    "heading": "4.4 Evaluation",
    "text": "Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following that, we used the Google Translate tool plus an additional annotator to translate those words to English. The annotator manually revised the lists and retained only words that have\n3The values are set empirically. Calculating similarity Sim(wS1 , w T 2 ) may be interpreted as: “Given word wS1 detect how similar word wT2 is to the word wS1 .” Therefore, when calculating Sim(wS1 , wT2 ), even when dealing with symmetric similarity functions such as BC, we always consider only the scores P (·|wS1 ) for truncating.\n4Available online: http://people.cs.kuleuven.be /∼ivan.vulic/software/\ntheir corresponding translation in the English vocabulary. Additionally, only one possible translation was annotated as correct. When more than 1 translation is possible, the annotator marked as correct the translation that occurs more frequently in the English Wikipedia data. Finally, we built a set of 1000 one-to-one translation pairs for Dutch-English and Spanish-English. The same procedure was followed for Italian-English, but there we obtained the ground truth one-to-one translation pairs for 1000 most frequent Italian nouns in order to test the effect of word frequency on the quality of semantic word responses and the overall lexicon quality. Evaluation metrics. All the methods under consideration actually retrieve ranked lists of semantically similar words that could be observed as potential translation candidates. We measure the performance on BLE as Top M accuracy (AccM ). It denotes the number of source words from ground truth translation pairs whose top M semantically similar words contain the correct translation according to our ground truth over the total number of ground truth translation pairs (=1000) (Tamura et al., 2012). Additionally, we compute the mean reciprocal rank (MRR) scores (Voorhees, 1999)."
  }, {
    "heading": "5 Results and Discussion",
    "text": "Table 2 displays the performance of each compared method on the BLE task. It shows the difference in results for different language pairs and different corpora used to extract latent cross-lingual topics and estimate the lists of semantic word responses. Example lists of semantically similar words over all 3 language pairs are shown in Table 3. Based on these results, we are able to derive several conclusions: (i) Response-BC performs consistently better than the other 3 methods over all corpora and all language pairs. It is more robust and is able to find some cross-lingual similarities omitted by the other meth-\nods (see Table 4). The overall quality of the crosslingual word similarities and lexicons extracted by the method is dependent on the quality of estimated semantic response vectors. The quality of these vectors is of course further dependent on the quality of multilingual training data. For instance, for Dutch-English, we may observe a rather spectacular increase in overall scores (the tests are performed over the same set of 1000 words) when we augment Wikipedia data with Europarl data (compare the scores for NL-EN-W and NL-EN-W+EP). (ii) A transition from a semantic space spanned by cross-lingual topics (Topic-BC) to a semantic space spanned by vocabulary words (Response-BC) leads to better results over all corpora and language pairs. The difference is less visible when using training data of lesser quality (the scores for NL-EN-W). Moreover, since the shared space of cross-lingual topics is used to obtain and quantify semantic word responses, the quality of learned cross-lingual topics influences the quality of semantic word responses. If the semantic coherence of the cross-lingual topical space is unsatisfying, the method is unable to generate good semantic response vectors, and ul-\ntimately unable to correctly identify semantically similar words across languages. (iii) Due to its modeling properties that assign more importance to high-frequency words, Direct-SWR produces reasonable results in the BLE task only for high-frequency words (see results for IT-EN-W). Although Eq. (2) models the concept of semantic word responding in a sound way (Griffiths et al., 2007), using the semantic word responses directly is not suitable for the actual BLE task. (iv) The effect of word frequency is clearly visible when comparing the results obtained on IT-ENW with the results obtained on the other Wikipedia corpora. High-frequency words produce more redundancies in training data that are captured by statistical models such as latent topic models. Highfrequency words then obtain better estimates of their semantic response vectors which consequently leads to better overall scores. The effect of word frequency on statistical methods in the BLE task was investigated before (Pekar et al., 2006; Prochasson and Fung, 2011; Tamura et al., 2012), and we also confirm their findings. (v) Unlike (Koehn and Knight, 2002; Haghighi et al., 2008), our response-based method does not rely on any orthographic features such as cognates or words shared across languages. It is a pure statistical method that only relies on word distributions over a multilingual corpus. Based on these distributions, it performs the initial shallow semantic analysis of the corpus by means of a multilingual probabilistic model. The method then builds, via the concept of semantic word responding, a language-\nindependent semantic space spanned by all vocabulary words/responses in both languages. That makes the method portable to distant language pairs. However, for similar languages, including more evidence such as orthographic clues might lead to further increase in scores, but we leave that for future work."
  }, {
    "heading": "6 Conclusion",
    "text": "We have proposed a new statistical approach to identifying semantically similar words across languages that relies on the paradigm of semantic word responding previously defined in cognitive science. The proposed approach is robust and does not make any additional language-pair dependent assumptions (e.g., it does not rely on a seed lexicon, orthographic clues or predefined concept categories). That effectively makes it applicable to any language pair. Our experiments on the task of bilingual lexicon extraction for a variety of language pairs have proved that the response-based approach is more robust and outperforms the methods that operate in the semantic space of latent concepts (e.g., cross-lingual topics) directly."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Steven Bethard and the anonymous reviewers for their useful suggestions. This research has been carried out in the framework of the TermWise Knowledge Platform (IOFKP/09/001) funded by the Industrial Research Fund, KU Leuven, Belgium."
  }],
  "year": 2013,
  "references": [{
    "title": "A study on similarity and relatedness using distributional and WordNet-based approaches",
    "authors": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."],
    "venue": "Proceedings of NAACL-HLT, pages 19–27.",
    "year": 2009
  }, {
    "title": "Robust measurement and comparison of context similarity for finding translation pairs",
    "authors": ["Daniel Andrade", "Tetsuya Nasukawa", "Junichi Tsujii."],
    "venue": "Proceedings of COLING, pages 19–27.",
    "year": 2010
  }, {
    "title": "On smoothing and inference for topic models",
    "authors": ["Arthur Asuncion", "Max Welling", "Padhraic Smyth", "Yee Whye Teh."],
    "venue": "Proceedings of UAI, pages 27–34.",
    "year": 2009
  }, {
    "title": "Phrasal translation and query expansion techniques for cross",
    "authors": ["Lisa Ballesteros", "W. Bruce Croft"],
    "year": 1997
  }, {
    "title": "On a measure of divergence between two statistical populations defined by their probability distributions",
    "authors": ["A. Bhattacharyya."],
    "venue": "Bulletin of the Calcutta Mathematical Society, 35:199–209.",
    "year": 1943
  }, {
    "title": "Latent Dirichlet Allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Multilingual topic models for unaligned text",
    "authors": ["Jordan Boyd-Graber", "David M. Blei."],
    "venue": "Proceedings of UAI, pages 75–82.",
    "year": 2009
  }, {
    "title": "The mathematics of statistical machine translation: parameter estimation",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."],
    "venue": "Computational Linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Comprehensive survey on distance/similarity measures between probability density functions",
    "authors": ["Sung-Hyuk Cha."],
    "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, 1(4):300– 307.",
    "year": 2007
  }, {
    "title": "Domain adaptation for machine translation by mining unseen words",
    "authors": ["Hal Daumé III", "Jagadeesh Jagarlamudi."],
    "venue": "Proceedings of ACL, pages 407–412.",
    "year": 2011
  }, {
    "title": "Crosslanguage linking of news stories on the Web using interlingual topic modeling",
    "authors": ["Wim De Smet", "Marie-Francine Moens."],
    "venue": "CIKM Workshop on Social Web Search and Mining (SWSM), pages 57–64.",
    "year": 2009
  }, {
    "title": "An approach based on multilingual thesauri and model combination for bilingual lexicon extraction",
    "authors": ["Hervé Déjean", "Eric Gaussier", "Fatia Sadat."],
    "venue": "Proceedings of COLING, pages 1–7.",
    "year": 2002
  }, {
    "title": "Topic models for meaning similarity in context",
    "authors": ["Georgiana Dinu", "Mirella Lapata."],
    "venue": "Proceedings of COLING, pages 250–258.",
    "year": 2010
  }, {
    "title": "Automatic cross-linguistic information retrieval using Latent Semantic Indexing",
    "authors": ["Susan T. Dumais", "Thomas K. Landauer", "Michael Littman."],
    "venue": "Proceedings of the SIGIR Workshop on Cross-Linguistic Information Retrieval, pages 16–23.",
    "year": 1996
  }, {
    "title": "Mining verynon-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM",
    "authors": ["Pascale Fung", "Percy Cheung."],
    "venue": "Proceedings of EMNLP, pages 57–63.",
    "year": 2004
  }, {
    "title": "An IR approach for translating new words from nonparallel, comparable texts",
    "authors": ["Pascale Fung", "Lo Yuen Yee."],
    "venue": "Proceedings of COLING, pages 414–420.",
    "year": 1998
  }, {
    "title": "A geometric view on bilingual lexicon extraction from comparable corpora",
    "authors": ["Eric Gaussier", "Jean-Michel Renders", "Irina Matveeva", "Cyril Goutte", "Hervé Déjean."],
    "venue": "Proceedings of ACL, pages 526–533.",
    "year": 2004
  }, {
    "title": "Topics in semantic representation",
    "authors": ["Thomas L. Griffiths", "Mark Steyvers", "Joshua B. Tenenbaum."],
    "venue": "Psychological Review, 114(2):211–244. 114",
    "year": 2007
  }, {
    "title": "Learning bilingual lexicons from monolingual corpora",
    "authors": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "Proceedings of ACL, pages 771–779.",
    "year": 2008
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig S. Harris."],
    "venue": "Word, 10(23):146–162.",
    "year": 1954
  }, {
    "title": "Cross-lingual semantic relatedness using encyclopedic knowledge",
    "authors": ["Samer Hassan", "Rada Mihalcea."],
    "venue": "Proceedings of EMNLP, pages 1192–1201.",
    "year": 2009
  }, {
    "title": "Probabilistic Latent Semantic Indexing",
    "authors": ["Thomas Hofmann."],
    "venue": "Proceedings of SIGIR, pages 50–57.",
    "year": 1999
  }, {
    "title": "Extracting multilingual topics from unaligned comparable corpora",
    "authors": ["Jagadeesh Jagarlamudi", "Hal Daumé III."],
    "venue": "Proceedings of ECIR, pages 444–456.",
    "year": 2010
  }, {
    "title": "A Bayesian method for robust estimation of distributional similarities",
    "authors": ["Jun’ichi Kazama", "Stijn De Saeger", "Kow Kuroda", "Masaki Murata", "Kentaro Torisawa"],
    "venue": "In Proceedings of ACL,",
    "year": 2010
  }, {
    "title": "Learning a translation lexicon from monolingual corpora",
    "authors": ["Philipp Koehn", "Kevin Knight."],
    "venue": "ACL Workshop on Unsupervised Lexical Acquisition, pages 9–16.",
    "year": 2002
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Proceedings of MT Summit, pages 79–86.",
    "year": 2005
  }, {
    "title": "Solutions to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge",
    "authors": ["Thomas K. Landauer", "Susan T. Dumais."],
    "venue": "Psychological Review, 104(2):211– 240.",
    "year": 1997
  }, {
    "title": "Revisiting context-based projection methods for term-translation spotting in comparable corpora",
    "authors": ["Audrey Laroche", "Philippe Langlais."],
    "venue": "Proceedings of COLING, pages 617–625.",
    "year": 2010
  }, {
    "title": "Measures of distributional similarity",
    "authors": ["Lillian Lee."],
    "venue": "Proceedings of ACL, pages 25–32.",
    "year": 1999
  }, {
    "title": "Dictionary-based techniques for cross-language information retrieval",
    "authors": ["Gina-Anne Levow", "Douglas W. Oard", "Philip Resnik."],
    "venue": "Information Processing and Management, 41:523–547.",
    "year": 2005
  }, {
    "title": "Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA",
    "authors": ["Yue Lu", "Qiaozhu Mei", "ChengXiang Zhai."],
    "venue": "Information Retrieval, 14(2):178–203.",
    "year": 2011
  }, {
    "title": "Polylingual topic models",
    "authors": ["David Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum."],
    "venue": "Proceedings of EMNLP, pages 880–889.",
    "year": 2009
  }, {
    "title": "Bilingual terminology mining using brain, not brawn comparable corpora",
    "authors": ["Emmanuel Morin", "Béatrice Daille", "Koichi Takeuchi", "Kyo Kageura."],
    "venue": "Proceedings of ACL, pages 664–671.",
    "year": 2007
  }, {
    "title": "What is free association and what does it measure? Memory and Cognition, 28:887–899",
    "authors": ["Douglas L. Nelson", "Cathy L. McEvoy", "Simon Dennis"],
    "year": 2000
  }, {
    "title": "Mining multilingual topics from Wikipedia",
    "authors": ["Xiaochuan Ni", "Jian-Tao Sun", "Jian Hu", "Zheng Chen."],
    "venue": "Proceedings of WWW, pages 1155–1156.",
    "year": 2009
  }, {
    "title": "Cross lingual text classification by mining multilingual topics from Wikipedia",
    "authors": ["Xiaochuan Ni", "Jian-Tao Sun", "Jian Hu", "Zheng Chen."],
    "venue": "Proceedings of WSDM, pages 375–384.",
    "year": 2011
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Computational Linguistics, 29(1):19–51.",
    "year": 2003
  }, {
    "title": "Finding translations for lowfrequency words in comparable corpora",
    "authors": ["Viktor Pekar", "Ruslan Mitkov", "Dimitar Blagoev", "Andrea Mulloni."],
    "venue": "Machine Translation, 20(4):247–266.",
    "year": 2006
  }, {
    "title": "Translingual document representations from discriminative projections",
    "authors": ["John C. Platt", "Kristina Toutanova", "Wen-Tau Yih."],
    "venue": "Proceedings of EMNLP, pages 251–261.",
    "year": 2010
  }, {
    "title": "Rare word translation extraction from aligned comparable documents",
    "authors": ["Emmanuel Prochasson", "Pascale Fung."],
    "venue": "Proceedings of ACL, pages 1327–1335.",
    "year": 2011
  }, {
    "title": "Automatic identification of word translations from unrelated English and German corpora",
    "authors": ["Reinhard Rapp."],
    "venue": "Proceedings of ACL, pages 519–526.",
    "year": 1999
  }, {
    "title": "A mixture model with sharing for lexical semantics",
    "authors": ["Joseph Reisinger", "Raymond J. Mooney."],
    "venue": "Proceedings of EMNLP, pages 1173–1182.",
    "year": 2010
  }, {
    "title": "Probabilistic part-of-speech tagging using decision trees",
    "authors": ["Helmut Schmid."],
    "venue": "International Conference on New Methods in Language Processing.",
    "year": 1994
  }, {
    "title": "Probabilistic topic models",
    "authors": ["Mark Steyvers", "Tom Griffiths."],
    "venue": "Handbook of Latent Semantic Analysis, 427(7):424–440.",
    "year": 2007
  }, {
    "title": "Word association spaces for predicting semantic similarity effects in episodic memory",
    "authors": ["Mark Steyvers", "Richard M. Shiffrin", "Douglas L. Nelson."],
    "venue": "Experimental Cognitive Psychology and Its Applications, pages 237–249.",
    "year": 2004
  }, {
    "title": "Bilingual lexicon extraction from comparable corpora using label propagation",
    "authors": ["Akihiro Tamura", "Taro Watanabe", "Eiichiro Sumita."],
    "venue": "Proceedings of EMNLP, pages 24–36.",
    "year": 2012
  }, {
    "title": "The TREC-8 question answering track report",
    "authors": ["Ellen M. Voorhees."],
    "venue": "Proceedings of TREC, pages 77–",
    "year": 1999
  }, {
    "title": "Identifying word translations from comparable corpora using latent topic models",
    "authors": ["Ivan Vulić", "Wim De Smet", "Marie-Francine Moens."],
    "venue": "Proceedings of ACL, pages 479–484.",
    "year": 2011
  }],
  "id": "SP:84da1fc1d590715d015c83e5f8ca7af0f3023fda",
  "authors": [{
    "name": "Ivan Vulić",
    "affiliations": []
  }],
  "abstractText": "We propose a new approach to identifying semantically similar words across languages. The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses. Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word. The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors. We evaluate our approach in the task of bilingual lexicon extraction (BLE) for a variety of language pairs. We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics.",
  "title": "Cross-Lingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses"
}