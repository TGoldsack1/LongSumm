{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 587–593 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n587"
  }, {
    "heading": "1 Introduction",
    "text": "In the last decade, the distributed word representation (a.k.a word embedding) has attracted tremendous attention in the field of natural language processing (NLP). Instead of large vectors, such as the one-hot representation, the distributed word representation embeds semantic and syntactic characteristics of words into a low-dimensional space, which makes it popular in NLP applications.\nThe main idea of most word embedding models follows the distributional hypothesis (Harris, 1954), i.e., the embedding of each word may be inferred using its context. An important model family for distributional word representation learning is built based on the global matrix factorization approach (Deerwester et al., 1990; Lee and Seung, 2001; Srebro et al., 2005; Mnih and Hinton, 2007; Li et al., 2015; Wang and Cohen, 2016), in which a dimensionality reduction over a sparse matrix is performed to capture the statistical information about a corpus in low-dimensional vectors. Another model family is neural word embeddings (Levy and Goldberg, 2014b), some attempts include the famous Neural Probabilistic Language Model (Bengio et al., 2003), SGNS and\nCBOW (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014) and their variants (Shazeer et al., 2016; Kenter et al., 2016; Ling et al., 2017; Patel et al., 2017).\nMost of these models capture the context information of each word using the co-occurrence matrix. However, the co-occurrence matrix only represents relatively local information, i.e., it describes context associations based on word pairs’ co-occurrence counts without considering global context perspective. Besides, the co-occurrence matrix is only an estimation of a corpus, which is only a sample of a language. A mass of related word pairs may not be observed in the corpus, and the latent relations between unobserved word pairs may not be modeled well due to the missing knowledge.\nFew attempts are carried out to indirectly deal with unobserved co-occurrence for dense neural word embeddings. SGNS (Mikolov et al., 2013a,b) indirectly addresses this problem through negative sampling. Swivel (Shazeer et al., 2016) improves GloVe by using a “soft hinge” loss to prevent from over-estimating zero co-occurrences. However, the latent relations between unobserved word pairs are not explicitly represented. There are also some works around semantic composition and distributional inference (Mitchell and Lapata, 2008; Erk and Padó, 2008, 2010; Reisinger and Mooney, 2010; Thater et al., 2011; Kartsaklis et al., 2013; Kober et al., 2016) that are explored to address the sparseness problem, but they are not designed for training neural word embeddings.\nIn this paper, we explore an approach that utilizes context overlap information to dig up more effective co-occurrence relations and propose extensions for GloVe and Swivel to validate the positive impact of introducing context overlap."
  }, {
    "heading": "2 Quantify Context Overlap",
    "text": "In this work, we explore quantifying context overlap based on the observation that to a certain extent the overlap of Point-wise Mutual Information (PMI) (Church and Hanks, 1990) reflects context overlap.\nAs shown in Figure 1, two separate words may exhibit a particular aspect of interest or be semantically related when the overlap area between their PMI is relatively large.\nThe calculation of complete PMI-weighted context overlap may be time-consuming when the number of words is large. To make the time complexity affordable, only the context words that have strong lexical association with a target word i are considered:\nSi = {k ∈ V |PMI(i, k) > hPMI} (1)\nin which V is the vocabulary, hPMI is a threshold which acts as a magnitude to shift PMI, and Si denotes the set that consists of the context words that have enough large PMI values with the target word i. It is expected that most context information associated with the word i can be captured by its PMI values over Si. Then, we measure the degree of context overlap (CO) between two target words i, j as a function of their PMI values over the intersection of Si and Sj , i.e.,\nCO(i, j) = ∑\nk∈Si∩Sj\nmin(f(PMI(i, k)), f(PMI(j, k)))\n(2) where f is a monotonic mapping function to rectify the data characteristics for certain objective function in word embedding training.\nCompared to identity function f(x) = x, we find exponential function f(x) = exp(x) works much better in our experiments. For the quantized context overlap, the exponential mapping function results in a similar data distribution as the cooccurrence counts, i.e., few word pairs have extremely large values while most word pairs’ values are distributed in a relatively small range."
  }, {
    "heading": "3 Extend to Existing Models",
    "text": "We consider the original co-occurrence matrix as a description of first order co-occurrence relations, while the quantized context overlap as a description of second order co-occurrence relations (Schütze, 1998), i.e., co-co-occurrences, which is represented by “non-logarithmic PMI-weighted\ncontext overlap” in this work. The context overlap between two words can be inferred even when they never co-occur in the corpus. According to our statistics, more than 84% word pairs in the second order co-occurrence matrix are not included in the first order co-occurrence matrix. We expect introducing second order co-occurrence relations may enhance the quality of the word embedding that is originally trained on first order co-occurrence relations. GloVe (Pennington et al., 2014) and Swivel (Shazeer et al., 2016) are extended by joint training with context overlap information in this paper.\nGloVe The logarithmic co-occurrence matrix is factorized in GloVe with bias terms, and a weighted least squares loss function is optimized:\nJGloV e = ∑ i,j λij(w T i w̃j+bi+b̃j−logXij)2 (3)\nwhere Xij denotes the word-context cooccurrence count between a target word i and a context word j. The model parameters to be learned include wi ∈ Rd, w̃j ∈ Rd, bi and b̃j , which correspond to target word vector, context word vector, bias terms associated with the target word and the context word, respectively. λij is a weight whose value equals to (min(Xij , xmax)/xmax)\nα. To extend GloVe, two tasks are trained in parallel during the training process: One is the main task that follows the original GloVe training pro-\ncess as above; Another one is an auxiliary task that tunes word embeddings using context overlap. The parameters of word embeddings are shared in both tasks.\nFollowing GloVe-style loss function, in the auxiliary task, the dot products of word vectors are pushed to estimate logarithmic second order cooccurrence. J (2)GloV e = ∑ i,j λ (2) ij (Aw T i wj+b (2) i +b (2) j −logX (2) ij ) 2 (4) where the superscripts (2) are used to differentiate with the terms in the original GloVe. X(2)ij = CO(i, j) represents context overlap, a word independent learnable scale A is adopted to relieve the potential inconformity between first order and second order co-occurrences. The weight λ(2)ij is similar to the original λij , but using a different hyperparameter x(2)max.\nThe multi-task (Ruder, 2017) loss function is the weighted sum of the two tasks, i.e., J = JGloV e+β ·J (2) GloV e, where the weight β is a hyperparameter.\nSwivel As pointed out by (Levy et al., 2015) , if the bias terms in GloVe are fixed to the logarithmic count of the corresponding word, the dot products of target word vectors and context word vectors are almost equivalent to the approximation of logarithmic PMI matrix with a shift of log ∑ i,j Xij . Submatrix-wise Vector Embedding Learner (Swivel) directly reconstructs the PMI matrix by dot product between target vectors and context vectors and deals with unobserved co-occurrences using a “soft hinge” loss function. (Shazeer et al., 2016) details its loss functions and training process. In our extended version, we add a supplementary loss function to handle second order co-occurrences. When the second order cooccurrence X(2)ij is more than zero, the PMI of context overlap is approximated.\n1 2 λ (2) ij (Aw T i wj +B − PMI(2)(i, j))2 (5)\nin which A, B are word independent learnable scale parameters, and PMI(2)(i, j) is the Pointwise Mutual Information computed on the second order co-occurrence matrix [X(2)ij ]."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Setup",
    "text": "Corpus The training dataset contains 6 billion tokens collected from diversified corpora, including the News Crawl corpus (Chelba et al., 2013), the April 2010 Wikipedia dump (Shaoul, 2010; Lee and Chen, 2017), and a year-2012 subset of the Reddit comment datasets 1.\nPreprocessing Following (Lee and Chen, 2017), the Stanford tokenizer is used to process the training corpus, which are split into sentences with characters converted to lower cases. Punctuations are removed.\nParameter Configuration The vocabularies are limited to the 200K most frequent words. Following (Pennington et al., 2014), a decreasing weighting function is adopted to construct the cooccurrence matrix. We use symmetric context window of five words to the left and five words to the right.\nFor GloVe, recommended parameters in (Pennington et al., 2014) are used. Specifically, we set α = 34 , xmax = 100, initial learning rate as 0.05, 100 iterations. For Swivel, recommended parameters in (Shazeer et al., 2016) are used. The weighting function is 0.1 + 0.25x0.5ij , each shard is sampled about 100 times. But we set the block size as 4000 so that the vocabulary size can be divided exactly.\nFor the auxiliary tasks, we tune the hyperparameters on the small News Crawl corpus. And we find that in an appropriate range, the threshold hPMI is not sensitive to the performance. In this paper, hPMI , x (2) max and β are set to log 100, 10000 and 0.2 respectively. Since there is no difference between target vectors and context vectors (except random initialization), in order to keep symmetry, we not only approximate context overlap between target vectors, but also approximate context overlap between context vectors simultaneously. Final vectors are the sum of w and w̃ in both GloVe and Swivel."
  }, {
    "heading": "4.2 Intrinsic Evaluation",
    "text": "Table 1 shows the evaluation results of word similarity tasks and word analogy tasks. Word similarity is measured as the Spearman’s rank correlation ρ between human-judged similarity and cosine distance of word vectors. In word analogy\n1Available at https://files.pushshift.io/ reddit/comments/\ntask, the questions are answered over the whole vocabulary through 3CosMul (Levy and Goldberg, 2014a). In addition to GloVe and Swivel, the evaluations of SGNS are also reported for reference. We train SGNS with the word2vec tool, using symmetric context window of five words to the left and five words to the right, and 5 negative samples.\nAs can be seen from the table, the context overlap information enhanced word embeddings perform better in most word similarity tasks and get higher analogy accuracy in semantic aspect at the cost of syntactic score. The improved semantics performance, to a certain extent, reflects second order co-occurrence relations are more semantic."
  }, {
    "heading": "4.3 Text Classification",
    "text": "Text classification tasks are conducted on five shared benchmark datasets from (Kim, 2014) including binary classification tasks CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), Subj (Pang and Lee, 2004) and multiple classification tasks TREC (Li and Roth, 2002), SST1 (Socher et al., 2013). Texts are preprocessed following the description of Section 4.1. We train Convolutional Neural Networks (CNN) on top of our static pretrained word vectors following (Kim, 2014). To avoid the high-risk of single-run estimate being false (Melis et al., 2017; Reimers and Gurevych, 2017), average classification accuracies of 20 runs are reported as the final scores. The results are shown in Table 2. As can be seen from the results that the enhanced word embeddings outperform the baselines."
  }, {
    "heading": "5 Model Analysis",
    "text": "As it is known to all, word frequency plays an important role in the computation of word embeddings (Gittens et al., 2017). Inspired from\nthe graph in (Shazeer et al., 2016), relations between word analogy accuracy and the log mean frequency of the words in analogy questions and answers are plotted on Figure 2. The word embeddings trained by GloVe with or without context overlap information are used here.\nAn obvious semantic performance improvement is observed in the range of low frequency. Our observation of second order co-occurrences may explain this fact. We randomly sample 1 million word pairs, and rank these word pairs in descending order by their quantized context overlap. In all the word pairs, average word frequency is 13934.4. However, it is only 1676.1 in the top 0.1% word pairs, it is 3984.8 in the top 1%, and it is 7904.9 in the top 10%. This may be caused by PMI’s bias towards infrequent words, but it illustrates infrequent words carry more information in second order co-occurrence relations."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we propose an empirical metric to enhance the word embeddings through estimating second order co-occurrence relations using con-\ntext overlap. Instead of only local statistical information, context overlap leverages global association distribution to measure word pairs correlation.\nThe proposed method is easy to extend to existing models, such as GloVe and Swivel, by an auxiliary objective function. The improvement in experimental results helps to validate the positive impact of introducing quantized context overlap.\nWe have considered the feasibility of enriching SGNS and CBOW with information from contextoverlap. However, because of their training mode, we can’t remake them in a straightforward way following their “original spirit”. When training SGNS and CBOW, the program scans the training text. The target and context words are chosen using a slide window and negative sampling is used. In this process, no co-occurrence matrix is explicitly computed, and we fail to extend it in a united form as we extend GloVe and Swivel. The extensions for GloVe and Swivel can also be used for reference for extending other word embedding approaches that are trained on co-occurrence matrix. The exploration for second order co-occurrence can be traced back to 1990s. We think it is helpful to revive the classical method in a modern, embedding driven way. How to integrate second order co-occurrence information for approaches like SGNS, CBOW should be an interesting future work.\nAs future works, we suggest further investigating the characteristics of context overlap in diversified ways."
  }],
  "year": 2018,
  "references": [{
    "title": "A neural probabilistic lan",
    "authors": ["Christian Jauvin"],
    "year": 2003
  }, {
    "title": "Multimodal distributional semantics",
    "authors": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."],
    "venue": "J. Artif. Intell. Res.(JAIR), 49(2014):1–47.",
    "year": 2014
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."],
    "venue": "arXiv preprint arXiv:1312.3005.",
    "year": 2013
  }, {
    "title": "Word association norms, mutual information, and lexicography",
    "authors": ["Kenneth Ward Church", "Patrick Hanks."],
    "venue": "Computational linguistics, 16(1):22–29.",
    "year": 1990
  }, {
    "title": "Indexing by latent semantic analysis",
    "authors": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman."],
    "venue": "Journal of the American society for information science, 41(6):391.",
    "year": 1990
  }, {
    "title": "A structured vector space model for word meaning in context",
    "authors": ["Katrin Erk", "Sebastian Padó."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897– 906. Association for Computational Linguistics.",
    "year": 2008
  }, {
    "title": "Exemplar-based models for word meaning in context",
    "authors": ["Katrin Erk", "Sebastian Padó."],
    "venue": "Proceedings of the acl 2010 conference short papers, pages 92–",
    "year": 2010
  }, {
    "title": "Placing search in context: The concept revisited",
    "authors": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."],
    "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406–",
    "year": 2001
  }, {
    "title": "Skip-gram-zipf+ uniform= vector additivity",
    "authors": ["Alex Gittens", "Dimitris Achlioptas", "Michael W Mahoney."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 69–76.",
    "year": 2017
  }, {
    "title": "Large-scale learning of word relatedness with constraints",
    "authors": ["Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren."],
    "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1406–",
    "year": 2012
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig Harris."],
    "venue": "Word, pages 10(23):146–162.",
    "year": 1954
  }, {
    "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Mining and summarizing customer reviews",
    "authors": ["Minqing Hu", "Bing Liu."],
    "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.",
    "year": 2004
  }, {
    "title": "Improving word representations via global context and multiple word prototypes",
    "authors": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-",
    "year": 2012
  }, {
    "title": "Separating disambiguation from composition in distributional semantics",
    "authors": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."],
    "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 114–",
    "year": 2013
  }, {
    "title": "Siamese cbow: Optimizing word embeddings for sentence representations",
    "authors": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."],
    "venue": "arXiv preprint arXiv:1606.04640.",
    "year": 2016
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Improving sparse word representations with distributional inference for semantic composition",
    "authors": ["Thomas Kober", "Julie Weeds", "Jeremy Reffin", "David Weir."],
    "venue": "arXiv preprint arXiv:1608.06794.",
    "year": 2016
  }, {
    "title": "Algorithms for non-negative matrix factorization",
    "authors": ["Daniel D Lee", "H Sebastian Seung."],
    "venue": "Advances in neural information processing systems, pages 556–562.",
    "year": 2001
  }, {
    "title": "Muse: Modularizing unsupervised sense embeddings",
    "authors": ["Guang-He Lee", "Yun-Nung Chen."],
    "venue": "arXiv preprint arXiv:1704.04601.",
    "year": 2017
  }, {
    "title": "Linguistic regularities in sparse and explicit word representations",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "Proceedings of the eighteenth conference on computational natural language learning, pages 171– 180.",
    "year": 2014
  }, {
    "title": "Neural word embedding as implicit matrix factorization",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "Advances in neural information processing systems, pages 2177–2185.",
    "year": 2014
  }, {
    "title": "Improving distributional similarity with lessons learned from word embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:211–225.",
    "year": 2015
  }, {
    "title": "Learning question classifiers",
    "authors": ["Xin Li", "Dan Roth."],
    "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.",
    "year": 2002
  }, {
    "title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective",
    "authors": ["Yitan Li", "Linli Xu", "Fei Tian", "Liang Jiang", "Xiaowei Zhong", "Enhong Chen."],
    "venue": "IJCAI, pages 3650–3656.",
    "year": 2015
  }, {
    "title": "Integrating extra knowledge into word embedding models for biomedical nlp tasks",
    "authors": ["Yuan Ling", "Yuan An", "Mengwen Liu", "Sadid A Hasan", "Yetian Fan", "Xiaohua Hu."],
    "venue": "Neural Networks (IJCNN), 2017 International Joint Conference on, pages 968–",
    "year": 2017
  }, {
    "title": "Better word representations with recursive neural networks for morphology",
    "authors": ["Thang Luong", "Richard Socher", "Christopher D Manning."],
    "venue": "CoNLL, pages 104–113.",
    "year": 2013
  }, {
    "title": "On the state of the art of evaluation in neural language models",
    "authors": ["Gábor Melis", "Chris Dyer", "Phil Blunsom."],
    "venue": "arXiv preprint arXiv:1707.05589.",
    "year": 2017
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Vector-based models of semantic composition",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "proceedings of ACL-08: HLT, pages 236–244.",
    "year": 2008
  }, {
    "title": "Three new graphical models for statistical language modelling",
    "authors": ["Andriy Mnih", "Geoffrey Hinton."],
    "venue": "Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.",
    "year": 2007
  }, {
    "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Com-",
    "year": 2004
  }, {
    "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 115–124. Association for",
    "year": 2005
  }, {
    "title": "Adapting pre-trained word embeddings for use in medical coding",
    "authors": ["Kevin Patel", "Divya Patel", "Mansi Golakiya", "Pushpak Bhattacharyya", "Nilesh Birari."],
    "venue": "BioNLP 2017, pages 302–306.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
    "authors": ["Nils Reimers", "Iryna Gurevych."],
    "venue": "arXiv preprint arXiv:1707.09861.",
    "year": 2017
  }, {
    "title": "Multi-prototype vector-space models of word meaning",
    "authors": ["Joseph Reisinger", "Raymond J Mooney."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
    "year": 2010
  }, {
    "title": "An overview of multi-task learning in deep neural networks",
    "authors": ["Sebastian Ruder."],
    "venue": "arXiv preprint arXiv:1706.05098.",
    "year": 2017
  }, {
    "title": "Automatic word sense discrimination",
    "authors": ["Hinrich Schütze."],
    "venue": "Computational linguistics, 24(1):97–123.",
    "year": 1998
  }, {
    "title": "The westbury lab wikipedia corpus",
    "authors": ["Cyrus Shaoul."],
    "venue": "Edmonton, AB: University of Alberta.",
    "year": 2010
  }, {
    "title": "Swivel: Improving embeddings by noticing what’s missing",
    "authors": ["Noam Shazeer", "Ryan Doherty", "Colin Evans", "Chris Waterson."],
    "venue": "arXiv preprint arXiv:1602.02215.",
    "year": 2016
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 conference on",
    "year": 2013
  }, {
    "title": "Maximum-margin matrix factorization",
    "authors": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola."],
    "venue": "Advances in neural information processing systems, pages 1329–1336.",
    "year": 2005
  }, {
    "title": "Word meaning in context: A simple and effective vector model",
    "authors": ["Stefan Thater", "Hagen Fürstenau", "Manfred Pinkal."],
    "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1134–1143.",
    "year": 2011
  }, {
    "title": "Learning first-order logic embeddings via matrix factorization",
    "authors": ["William Yang Wang", "William W Cohen."],
    "venue": "IJCAI, pages 2132–2138.",
    "year": 2016
  }],
  "id": "SP:f6bef606c7c6eb91b9aecb18f885d26086f03322",
  "authors": [{
    "name": "Yimeng Zhuang",
    "affiliations": []
  }, {
    "name": "Jinghui Xie",
    "affiliations": []
  }, {
    "name": "Yinhe Zheng",
    "affiliations": []
  }, {
    "name": "Xuan Zhu",
    "affiliations": []
  }],
  "abstractText": "Most models for learning word embeddings are trained based on the context information of words, more precisely first order cooccurrence relations. In this paper, a metric is designed to estimate second order cooccurrence relations based on context overlap. The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models. Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.",
  "title": "Quantifying Context Overlap for Training Word Embeddings"
}