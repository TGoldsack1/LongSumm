{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 643–653 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1060"
  }, {
    "heading": "1 Introduction",
    "text": "An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. In spoken language understanding, new domains of interest for categorizing user utterances are added on a regular basis1. For instance, we may\n1A scenario frequently arising in practice is having a request for creating a new virtual domain targeting a specific application. One typical use case is that of building natural language capability through intent and slot modeling (without actually building a domain classifier) targeting a specific application.\nadd ORDERPIZZA domain and desire a domainspecific intent and semantic slot tagger with a limited amount of training data. Training only on the target domain fails to utilize the existing resources in other domains that are relevant (e.g., labeled data for PLACES domain with place name, location as the slot types), but naively training on the union of all domains does not work well since different domains can have widely varying distributions.\nDomain adaptation offers a balance between these extremes by using all data but simultaneously distinguishing domain types. A common approach for adapting to a new domain is to retrain a global model across all K + 1 domains using well-known techniques, for example the feature augmentation method of Daumé III (2009) which trains a single model that has one domaininvariant component along with K + 1 domainspecific components each of which is specialized in a particular domain. While such a global model is effective, it requires re-estimating a model from scratch on all K + 1 domains each time a new domain is added. This is burdensome particularly in our scenario in which new domains can arise frequently.\nIn this paper, we present an alternative solution based on attending an ensemble of domain experts. We assume that we have already trained K domain-specific models on respective domains. Given a new domainK+1 with a small amount of training data, we train a model on that data alone but queries the K experts as part of the training procedure. We compute an attention weight for each of these experts and use their combined feedback along with the model’s own opinion to make predictions. This way, the model is able to selectively capitalize on relevant domains much like in\n643\nstandard domain adaptation but without explicitly re-training on all domains together.\nIn experiments, we show clear gains in a domain adaptation scenario across 7 test domains, yielding average error reductions of 44.97% for intent classification and 32.30% for slot tagging compared to baselines that do not use domain adaptation. Moreover we have higher accuracy than the full re-training approach of Kim et al. (2016c), a neural analog of Daumé III (2009)."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Domain Adaptation",
    "text": "There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daumé III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daumé III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-Tür et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent.\nAll these methods require one to re-train a model from scratch to make it learn the correlation and invariance between domains. This becomes difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method."
  }, {
    "heading": "2.2 Spoken Language Understanding",
    "text": "Recently, there has been much investment on the personal digital assistant (PDA) technology in in-\ndustry (Sarikaya, 2015; Sarikaya et al., 2016). Apples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of personal digital assistants. Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a).\nNaturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-Tür et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015).\nThere are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b)."
  }, {
    "heading": "3 Method",
    "text": "We use an LSTM simply as a mapping φ : Rd × Rd′ → Rd′ that takes an input vector x and a state vector h to output a new state vector h′ = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a character layer, a BiLSTM at word level, and feedfoward network at the output."
  }, {
    "heading": "3.1 Individual Model Architecture",
    "text": "Let C denote the set of character types and W the set of word types. Let ⊕ denote the vector concatenation operation. A wildly successful architecture for encoding a sentence (w1 . . . wn) ∈ Wn is given by bidirectional LSTMs (BiLSTMs) (Schuster and Paliwal, 1997; Graves, 2012). Our model first constructs a network over an utterance closely following Lample et al. (2016). The model parameters Θ associated with this BiLSTM layer are\n• Character embedding ec ∈ R25 for each c ∈ C\n• Character LSTMs φCf , φCb : R25×R25 → R25\n• Word embedding ew ∈ R100 for each w ∈ W\n• Word LSTMs φWf , φWb : R150×R100 → R100\nLetw1 . . . wn ∈ W denote a word sequence where word wi has character wi(j) ∈ C at position j. First, the model computes a character-sensitive word representation vi ∈ R150 as\nfCj = φ C f ( ewi(j), f C j−1 )\n∀j = 1 . . . |wi| bCj = φ C b ( ewi(j), b C j+1 ) ∀j = |wi| . . . 1 vi = f C |wi| ⊕ b C 1 ⊕ ewi\nfor each i = 1 . . . n.2 Next, the model computes\nfWi = φ W f ( vi, f W i−1 )\n∀i = 1 . . . n bWi = φ W b ( vi, b W i+1 ) ∀i = n . . . 1\nand induces a character- and context-sensitive word representation hi ∈ R200 as\nhi = f W i ⊕ bWi (1)\nfor each i = 1 . . . n. These vectors can be used to perform intent classification or slot tagging on the utterance.\nIntent Classification We can predict the intent of the utterance using (h1 . . . hn) ∈ R200 in (1) as follows. Let I denote the set of intent types. We introduce a single-layer feedforward network gi : R200 → R|I| whose parameters are denoted by Θi. We compute a |I|-dimensional vector\nµi = gi\n( n∑\ni=1\nhi\n)\nand define the conditional probability of the correct intent τ as\np(τ |h1 . . . hn) ∝ exp ( µiτ )\n(2)\n2For simplicity, we assume some random initial state vectors such as fC0 and bC|wi|+1 when we describe LSTMs.\nThe intent classification loss is given by the negative log likelihood:\nLi ( Θ,Θi ) = − ∑\nl\nlog p ( τ (l)|h(l) ) (3)\nwhere l iterates over intent-annotated utterances.\nSlot Tagging We predict the semantic slots of the utterance using (h1 . . . hn) ∈ R200 in (1) as follows. Let S denote the set of semantic types and L the set of corresponding BIO label types 3 that is, L = {B-e : e ∈ E}∪{I-e : e ∈ E}∪{O}. We add a transition matrix T ∈ R|L|×|L| and a singlelayer feedforward network gt : R200 → R|L| to the network; denote these additional parameters by Θt. The conditional random field (CRF) tagging layer defines a joint distribution over label sequences of y1 . . . yn ∈ L of w1 . . . wn as\np(y1 . . .yn|h1 . . . hn)\n∝ exp ( n∑\ni=1\nTyi−1,yi × gtyi(hi) ) (4)\nThe tagging loss is given by the negative log likelihood:\nLt ( Θ,Θt ) = − ∑\nl\nlog p ( y(l)|h(l) ) (5)\nwhere l iterates over tagged sentences in the data. Alternatively, we can optimize the local loss:\nLt−loc ( Θ,Θt ) = − ∑\nl\n∑\ni\nlog p ( y (l) i |h (l) i )\n(6)\nwhere p(yi|hi) ∝ exp ( gtyi(hi) ) ."
  }, {
    "heading": "4 Method",
    "text": ""
  }, {
    "heading": "4.1 Domain Attention Architecture",
    "text": "Now we assume that for each of theK domains we have an individual model described in Section 3.1. Denote these domain experts by Θ(1) . . .Θ(K). We now describe our model for a new domain K + 1. Given an utterance w1 . . . wn, it uses a BiLSTM layer to induce a feature representation h1 . . . hn as specified in (1). It further invokes K domain experts Θ(1) . . .Θ(K) on this utterance to obtain the feature representations h(k)1 . . . h (k) n for\n3For example, to/O San/B-Source Francisco/I-Source airport/O.\nk = 1 . . .K. For each word wi, the model computes an attention weight for each domain k = 1 . . .K domains as\nqdoti,k = h > i h (k) (7)\nin the simplest case. We also experiment with the bilinear function\nqbii,k = h > i Bh (k) (8)\nwhere B is an additional model parameter, and also the feedforward function\nqfeedi,k = W tanh ( Uh>i + V h (k) + b1 ) + b2 (9)\nwhere U, V,W, b1, b2 are additional model parameters. The final attention weights a(1)i . . . a (1) i are obtained by using a softmax layer\nai,k = exp(qi,k)∑K k=1 exp(qi,k)\n(10)\nThe weighted combination of the experts’ feedback is given by\nhexpertsi =\nK∑\nk=1\nai,kh (k) i (11)\nand the model makes predictions by using h̄1 . . . h̄n where\nh̄i = hi ⊕ hexpertsi (12)\nThese vectors replace the original feature vectors hi in defining the intent or tagging losses."
  }, {
    "heading": "4.2 Domain Attention Variants",
    "text": "We also consider two variants of the domain attention architecture in Section 4.1.\nLabel Embedding In addition to the state vectors h(1) . . . h(K) produced by K experts, we further incorporate their final (discrete) label predictions using pre-trained label embeddings. We induce embeddings ey for labels y from all domains using the method of Kim et al. (2015d). At the i-th word, we predict the most likely label y(k) under the k-th expert and compute an attention weight as\nq̄doti,k = h > i e y(k) (13)\nThen we compute an expectation over the experts’ predictions\nāi,k = exp(q̄i,k)∑K k=1 exp(q̄i,k)\n(14)\nhlabeli =\nK∑\nk=1\nāi,ke y(k) i (15)\nand use it in conjunction with h̄i. Note that this makes the objective a function of discrete decision and thus non-differentiable, but we can still optimize it in a standard way treating it as learning a stochastic policy.\nSelective Attention Instead of computing attention over all K experts, we only consider the top K ′ ≤ K that predict the highest label scores. We only compute attention over these K ′ vectors. We experiment with various values of K ′"
  }, {
    "heading": "5 Experiments",
    "text": "In this section, we describe the set of experiments conducted to evaluate the performance of our model. In order to fully assess the contribution of our approach, we also consider several baselines and variants besides our primary expert model."
  }, {
    "heading": "5.1 Test domains and Tasks",
    "text": "To test the effectiveness of our proposed approach, we apply it to a suite of 7 Microsoft Cortana domains with 2 separate tasks in spoken language understanding: (1) intent classification and (2) slot (label) tagging. The intent classification task is a multi-class classification problem with the goal of determining to which one of the |I| intents a user utterance belongs within a given domain. The slot tagging task is a sequence labeling problem with the goal of identifying entities and chunking of useful information snippets in a user utterance. For example, a user could say “reserve a table at joeys grill for thursday at seven pm for five people”. Then the goal of the first task would be to classify this utterance as “make reservation” intent given the places domain, and the goal of the second task would be to tag “joeys grill” as restaurant, “thursday” as date, “seven pm” as time, and “five” as number people.\nThe short descriptions on the 7 test domains are shown in Table 1. As the table shows, the test domains have different granularity and diverse semantics. For each personal assistant test domain,\nwe only used 1000 training utterances to simulate scarcity of newly labeled data. The amount of development and test utterance was 100 and 10k respectively.\nThe similarities of test domains, represented by overlapping percentage, with experts or source domains are represented in Table 2. The intent overlapping percentage ranges from 30% on FITNESS domain to 70% on EVENTS, which averages out at 51.49%. And the slots for test domains overlaps more with those of source domains ranging from 60% on TV domain to 100% on both M-TICKET and TAXI domains, which averages out at 81.69%."
  }, {
    "heading": "5.2 Experimental Setup",
    "text": "In testing our approach, we consider a domain adaptation (DA) scenario, where a target domain has a limited training data and the source domain has a sufficient amount of labeled data. We further consider a scenario, creating a new virtual domain targeting a specific scenario given a large inventory of intent and slot types and underlying models build for many different applications and scenarios. One typical use case is that of building natural language capability through intent and slot modeling (without actually building a domain classifier) targeting a specific application. Therefore, our experimental settings are rather different from previ-\nously considered settings for domain adaptation in two aspects:\n• Multiple source domains: In most previous works, only a pair of domains (source vs. target) have been considered, although they can be easily generalized to K > 2. Here, we experiment with K = 25 domains shown in Table 3.\n• Variant output: In a typical setting for domain adaptation, the label space is invariant across all domains. Here, the label space can be different in different domains, which is a more challenging setting. See Kim et al. (2015d) for details of this setting.\nFor this DA scenario, we test whether our approach can effectively make a system to quickly generalize to a new domain with limited supervision given K existing domain experts shown in 3 .\nIn summary, our approach is tested with 7 Microsoft Cortana personal assistant domains across 2 tasks of intent classification and slot tagging. Below shows more detail of our baselines and variants used in our experiments.\nBaselines: All models below use same underlying architecture described in Section 3.1\n• TARGET: a model trained on a targeted domain without DA techniques.\n• UNION: a model trained on the union of a targeted domain and 25 domain experts.\n• DA: a neural domain adaptation method of Kim et al. (2016c) which trains domain specific K LSTMs with a generic LSTM on all domain training data.\nDomain Experts (DE) variants: All models below are based on attending on an ensemble of 25 domain experts (DE) described in Section 4.1, where a specific set of intent and slots models are trained for each domain. We have two feedback from domain experts: (1) feature representation from LSTM, and (2) label embedding from feedfoward described in Section 4.1 and Section 4.2, respectively.\n• DEB: DE without domain attention mechanism. It uses the unweighted combination of first feedback from experts like bag-of-word model.\n• DE1: DE with domain attention with the weighted combination of the first feedbacks from experts.\n• DE2: DE1 with additional weighted combination of second feedbacks.\n• DES2: DE2 with selected attention mechanism, described in Section 4.2.\nIn our experiments, all the models were implemented using Dynet (Neubig et al., 2017) and were trained using Stochastic Gradient Descent (SGD) with Adam (Kingma and Ba, 2015)—an adaptive learning rate algorithm. We used the initial learning rate of 4× 10−4 and left all the other hyper parameters as suggested in Kingma and Ba (2015). Each SGD update was computed without a minibatch with Intel MKL (Math Kernel Library)4. We used the dropout regularization (Srivastava et al., 2014) with the keep probability of 0.4 at each LSTM layer.\nTo encode user utterances, we used bidirectional LSTMs (BiLSTMs) at the character level and the word level, along with 25 dimensional character embedding and 100 dimensional word embedding. The dimension of both the input and output of the character LSTMs were 25, and the dimensions of the input and output of the word LSTMs were 1505 and 100, respectively. The dimension of the input and output of the final feedforward network for intent, and slot were 200 and the number of their corresponding task. Its activation was rectified linear unit (ReLU).\nTo initialize word embedding, we used word embedding trained from (Lample et al., 2016). In the following sections, we report intent classification results in accuracy percentage and slot results in F1-score. To compute slot F1-score, we used the standard CoNLL evaluation script6"
  }, {
    "heading": "5.3 Results",
    "text": "We show our results in the DA setting where we had a sufficient labeled dataset in the 25 source domains shown in Table 3, but only 1000 labeled data in the target domain. The performance of the baselines and our domain experts DE variants are shown in Table 4. The top half of the table shows\n4https://software.intel.com/en-us/articles/intelr-mkl-andc-template-libraries\n5We concatenated last two outputs from the character LSTM and word embedding, resulting in 150 (25+25+100)\n6http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nthe results of intent classification and the results of slot tagging is in the bottom half.\nThe baseline which trained only on the target domain (TARGET) shows a reasonably good performance, yielding on average 87.7% on the intent classification and 83.9% F1-score on the slot tagging. Simply training a single model with aggregated utterance across all domains (UNION) brings the performance down to 77.4% and 75.3%. Using DA approach of Kim et al. (2016c) shows a significant increase in performance in all 7 domains, yielding on average 90.3% intent accuracy and 86.2%.\nThe DE without domain attention (DEB) shows similar performance compared to DA. Using DE model with domain attention (DE1) shows another increase in performance, yielding on average 90.9% intent accuracy and 86.9%. The performance increases again when we use both feature representation and label embedding (DE2), yielding on average 91.4% and 88.2% and observe nearly 93.6% and 89.1% when using selective attention (DES2). Note that DES2 selects the appropriate number of experts per layer by evaluation on a development set.\nThe results show that our expert variant approach (DES2) achieves a significant performance gain in all 7 test domains, yielding average error reductions of 47.97% for intent classification and 32.30% for slot tagging. The results suggest that our expert approach can quickly generalize to a new domain with limited supervision given K existing domains by having only a handful more data of 1k newly labeled data points. The poor performance of using the union of both source and target domain data might be due to the relatively very small size of the target domain data, overwhelmed by the data in the source domain. For example, a word such as “home” can be labeled as place type under the TAXI domain, but in the source domains can be labeled as either home screen under the PHONE domain or contact name under the CALENDAR domain."
  }, {
    "heading": "5.4 Training Time",
    "text": "The Figure 3 shows the time required for training DES2 and DA of Kim et al. (2016c). The training time for DES2 stays almost constant as the number of source domains increases. However, the training time for DA grows exponentially in the number of source domains. Specifically, when trained\nwith 1 source or expert domain, both took around a minute per epoch on average. When training with full 25 source domains, DES2 took 3 minutes per epoch while DA took 30 minutes per epoch. Since we need to iterate over all 25+1 domains to re-train the global model, the net training time ratio could be over 250."
  }, {
    "heading": "5.5 Learning Curve",
    "text": "We also measured the performance of our methods as a function of the number of domain experts. For each test domain, we consider all possible sizes of experts ranging from 1 to 25 and we then take the average of the resulting performances obtained from the expert sets of all different sizes. Figure 4 shows the resulting learning curves for each test domain. The overall trend is clear: as the more expert domains are added, the more the test performance improves. With ten or more expert domains added, our method starts to get saturated achiev-\ning more than 90% in accuracy across all seven domains."
  }, {
    "heading": "5.6 Attention weights",
    "text": "From the heatmap shown in Figure 5, we can see that the attention strength generally agrees with common sense. For example, the M-TICKET and TAXI domain selected MOVIE and PLACES as their top experts, respectively."
  }, {
    "heading": "5.7 Oracle Expert",
    "text": "The results in Table 5 show the intent classification accuracy of DE2 when we already have the same domain expert in the expert pool. To simulate such a situation, we randomly sampled 1,000, 100, and 100 utterances from each domain as training, development and test data, respectively. In both ALARM and HOTEL domains, the trained models only on the 1,000 training utterances (TARGET) achieved only 70.1%and 65.2% in accuracy, respectively. Whereas, with our method (DE2) applied, we reached almost the full training performance by selectively paying attention to the oracle expert, yielding 98.2% and 96.9%, respectively. This result again confirms that the behavior of the trained attention network indeed matches the semantic closeness between different domains."
  }, {
    "heading": "5.8 Selective attention",
    "text": "The results in Table 6 examines how the intent prediction accuracy of DES2 varies with respect to the\nnumber of experts in the pool. The rationale behind DES2 is to alleviate the downside of soft attention, namely distributing probability mass over all items even if some are bad items. To deal with such issues, we apply a hard cut-off at top k domains. From the result, a threshold at top 3 or 5 yielded better results than that of either 1 or 25 experts. This matches our common sense that their are only a few of domains that are close enough to be of help to a test domain. Thus it is advisable to find the optimal k value through several rounds of experiments on a development dataset."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we proposed a solution for scaling domains and experiences potentially to a large number of use cases by reusing existing data labeled for different domains and applications. Our solution is based on attending an ensemble of domain experts. When given a new domain, our model uses a weighted combination of domain experts’ feedback along with its own opinion to make prediction on the new domain. In both intent classification and slot tagging tasks, the model significantly outperformed baselines that do not use domain adaptation and also performed better than the full re-training approach. This approach enables creation of new virtual domains through a weighted combination of domain experts’ feedback reducing the need to collect and annotate the similar intent and slot types multiple times for different domains. Future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well."
  }],
  "year": 2017,
  "references": [{
    "title": "Task specific continuous word representations for mono and multi-lingual spoken language understanding",
    "authors": ["Tasos Anastasakos", "Young-Bum Kim", "Anoop Deoras."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International",
    "year": 2014
  }, {
    "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
    "authors": ["John Blitzer", "Mark Dredze", "Fernando Pereira"],
    "venue": "In ACL",
    "year": 2007
  }, {
    "title": "Domain adaptation with structural correspondence learning",
    "authors": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."],
    "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing. Association for Computational Linguistics,",
    "year": 2006
  }, {
    "title": "An empirical investigation of word class-based features for natural language understanding",
    "authors": ["Asli Celikyilmaz", "Ruhi Sarikaya", "Minwoo Jeong", "Anoop Deoras."],
    "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)",
    "year": 2016
  }, {
    "title": "Convolutional neural network based semantic tagging with entity embeddings",
    "authors": ["Asli Celikyilmaz", "Silicon Valley", "Dilek HakkaniTur."],
    "venue": "genre .",
    "year": 2010
  }, {
    "title": "Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models",
    "authors": ["Yun-Nung Chen", "Dilek Hakkani-Tür", "Xiaodong He."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-",
    "year": 2016
  }, {
    "title": "Frustratingly easy domain adaptation",
    "authors": ["Hal Daumé III."],
    "venue": "arXiv preprint arXiv:0907.1815 .",
    "year": 2009
  }, {
    "title": "Domain adaptation for statistical classifiers",
    "authors": ["Hal Daume III", "Daniel Marcu."],
    "venue": "Journal of Artificial Intelligence Research 26:101–126.",
    "year": 2006
  }, {
    "title": "Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs",
    "authors": ["Ali El-Kahky", "Derek Liu", "Ruhi Sarikaya", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck"],
    "year": 2014
  }, {
    "title": "Zero-shot semantic parser for spoken language understanding",
    "authors": ["Emmanuel Ferreira", "Bassam Jabaian", "Fabrice Lefèvre."],
    "venue": "Sixteenth Annual Conference of the International Speech Communication Association.",
    "year": 2015
  }, {
    "title": "Neural networks",
    "authors": ["Alex Graves."],
    "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, Springer, pages 15–35.",
    "year": 2012
  }, {
    "title": "Joint semantic utterance classification and slot filling with recursive neural networks",
    "authors": ["Daniel Guo", "Gokhan Tur", "Wen-tau Yih", "Geoffrey Zweig"],
    "year": 2014
  }, {
    "title": "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
    "authors": ["Dilek Hakkani-Tür", "Gokhan Tur", "Asli Celikyilmaz", "Yun-Nung Chen", "Jianfeng Gao", "Li Deng", "YeYi Wang."],
    "venue": "Proceedings of The 17th Annual Meeting of the International",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Domain adaptation of recurrent neural networks for natural language understanding",
    "authors": ["Aaron Jaech", "Larry Heck", "Mari Ostendorf."],
    "venue": "arXiv preprint arXiv:1604.00117 .",
    "year": 2016
  }, {
    "title": "Triangular-chain conditional random fields",
    "authors": ["Minwoo Jeong", "Gary Geunbae Lee."],
    "venue": "IEEE Transactions on Audio, Speech, and Language Processing 16(7):1287–1302.",
    "year": 2008
  }, {
    "title": "Weakly supervised slot tagging with partially labeled sequences from web search click logs",
    "authors": ["Young-Bum Kim", "Minwoo Jeong", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of the NAACL. Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Natural language model reusability for scaling to different domains",
    "authors": ["Young-Bum Kim", "Alexandre Rochette", "Ruhi Sarikaya."],
    "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP). Association for Com-",
    "year": 2016
  }, {
    "title": "Adversarial adaptation of synthetic or stale data",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Annual Meeting of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Compact lexicon selection with spectral methods",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Xiaohu Liu", "Ruhi Sarikaya."],
    "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.",
    "year": 2015
  }, {
    "title": "Pre-training of hidden-unit crfs",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. pages 192–198.",
    "year": 2015
  }, {
    "title": "Domainless adaptation by constrained decoding on a schema lattice",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING) .",
    "year": 2016
  }, {
    "title": "Frustratingly easy neural domain adaptation",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING) .",
    "year": 2016
  }, {
    "title": "Scalable semi-supervised query classification using matrix sketching",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 8.",
    "year": 2016
  }, {
    "title": "New transfer learning techniques for disparate label sets",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong."],
    "venue": "ACL. Association for Computational Linguistics .",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "The International Conference on Learning Representations (ICLR). .",
    "year": 2015
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "arXiv preprint arXiv:1603.01360 .",
    "year": 2016
  }, {
    "title": "Extracting structured information from user queries with semi-supervised conditional random fields",
    "authors": ["Xiao Li", "Ye-Yi Wang", "Alex Acero."],
    "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information",
    "year": 2009
  }, {
    "title": "Attention-based recurrent neural network models for joint intent detection and slot filling",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "Interspeech 2016. pages 685–689.",
    "year": 2016
  }, {
    "title": "Joint online spoken language understanding and language modeling with recurrent neural networks",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Com-",
    "year": 2016
  }, {
    "title": "A discriminative model based entity dictionary weighting approach for spoken language understanding",
    "authors": ["Xiaohu Liu", "Ruhi Sarikaya."],
    "venue": "Spoken Language Technology Workshop (SLT). IEEE, pages 195–199.",
    "year": 2014
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"],
    "year": 2017
  }, {
    "title": "Domain adaptation via transfer component analysis",
    "authors": ["Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang."],
    "venue": "IEEE Transactions on Neural Networks 22(2):199–210.",
    "year": 2011
  }, {
    "title": "The technology powering personal digital assistants",
    "authors": ["Ruhi Sarikaya."],
    "venue": "Keynote at Interspeech, Dresden, Germany.",
    "year": 2015
  }, {
    "title": "Shrinkage based features for slot tagging with conditional random fields",
    "authors": ["Ruhi Sarikaya", "Asli Celikyilmaz", "Anoop Deoras", "Minwoo Jeong."],
    "venue": "INTERSPEECH. pages 268–272.",
    "year": 2014
  }, {
    "title": "An overview of endto-end language understanding and dialog",
    "authors": ["Ruhi Sarikaya", "Paul Crook", "Alex Marin", "Minwoo Jeong", "Jean-Philippe Robichaud", "Asli Celikyilmaz", "Young-Bum Kim", "Alexandre Rochette", "Omar Zia Khan", "Xiuahu Liu"],
    "year": 2016
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["Mike Schuster", "Kuldip K Paliwal."],
    "venue": "IEEE Transactions on Signal Processing 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Multitask learning for spoken language understanding",
    "authors": ["Gokhan Tur."],
    "venue": "In Proceedings of the ICASSP. Toulouse, France.",
    "year": 2006
  }, {
    "title": "Convolutional neural network based triangular crf for joint intent detection and slot filling",
    "authors": ["Puyang Xu", "Ruhi Sarikaya."],
    "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, pages 78–83.",
    "year": 2013
  }, {
    "title": "A joint model of intent determination and slot filling for spoken language understanding",
    "authors": ["Xiaodong Zhang", "Houfeng Wang."],
    "venue": "IJCAI.",
    "year": 2016
  }],
  "id": "SP:785bd8ff24188829f8522e2be58574a1df1a7841",
  "authors": [{
    "name": "Young-Bum Kim",
    "affiliations": []
  }, {
    "name": "Karl Stratos",
    "affiliations": []
  }, {
    "name": "Dongchan Kim",
    "affiliations": []
  }],
  "abstractText": "An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision givenK existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daumé III (2009). However, it is desirable to adapt without having to reestimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domainspecific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts’ feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.",
  "title": "Domain Attention with an Ensemble of Experts"
}