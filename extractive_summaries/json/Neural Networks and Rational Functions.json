{
  "sections": [{
    "heading": "1. Overview",
    "text": "Significant effort has been invested in characterizing the functions that can be efficiently approximated by neural networks. The goal of the present work is to characterize neural networks more finely by finding a class of functions which is not only well-approximated by neural networks, but also well-approximates neural networks.\nThe function class investigated here is the class of rational functions: functions represented as the ratio of two polynomials, where the denominator is a strictly positive polynomial. For simplicity, the neural networks are taken to always use ReLU activation σr(x) := max{0, x}; for a review of neural networks and their terminology, the reader is directed to Section 1.4. For the sake of brevity, a network with ReLU activations is simply called a ReLU network."
  }, {
    "heading": "1.1. Main results",
    "text": "The main theorem here states that ReLU networks and rational functions approximate each other well in the sense\n1University of Illinois, Urbana-Champaign; work completed while visiting the Simons Institute. Correspondence to: your friend <mjt@illinois.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthat -approximating one class with the other requires a representation whose size is polynomial in ln(1 / ), rather than being polynomial in 1/ . Theorem 1.1. 1. Let ∈ (0, 1] and nonnegative inte-\nger k be given. Let p : [0, 1]d → [−1,+1] and q : [0, 1]d → [2−k, 1] be polynomials of degree ≤ r, each with≤ smonomials. Then there exists a function f : [0, 1]d → R, representable as a ReLU network of size (number of nodes)\nO ( k7 ln(1 / )3\n+ min { srk ln(sr / ), sdk2 ln(dsr / )2 }) ,\nsuch that\nsup x∈[0,1]d ∣∣∣∣f(x)− p(x)q(x) ∣∣∣∣ ≤ .\n2. Let ∈ (0, 1] be given. Consider a ReLU network f : [−1,+1]d → R with at most m nodes in each of at most k layers, where each node computes z 7→ σr(a\n>z + b) where the pair (a, b) (possibly distinct across nodes) satisfies ‖a‖1 + |b| ≤ 1. Then there exists a rational function g : [−1,+1]d → R with degree (maximum degree of numerator and denominator)\nO ( ln(k/ )kmk )\nsuch that\nsup x∈[−1,+1]d\n∣∣f(x)− g(x)∣∣ ≤ .\nPerhaps the main wrinkle is the appearance of mk when approximating neural networks by rational functions. The following theorem shows that this dependence is tight.\nTheorem 1.2. Let any integer k ≥ 3 be given. There exists a function f : R → R computed by a ReLU network with 2k layers, each with ≤ 2 nodes, such that any rational function g : R → R with ≤ 2k−2 total terms in the numerator and denominator must satisfy∫\n[0,1]\n|f(x)− g(x)|dx ≥ 1 64 .\nNote that this statement implies the desired difficulty of approximation, since a gap in the above integral (L1) distance implies a gap in the earlier uniform distance (L∞), and furthermore an r-degree rational function necessarily has ≤ 2r + 2 total terms in its numerator and denominator.\nAs a final piece of the story, note that the conversion between rational functions and ReLU networks is more seamless if instead one converts to rational networks, meaning neural networks where each activation function is a rational function.\nLemma 1.3. Let a ReLU network f : [−1,+1]d → R be given as in Theorem 1.1, meaning f has at most l layers and each node computes z 7→ σr(a>z+b) where where the pair (a, b) (possibly distinct across nodes) satisfies ‖a‖1 + |b| ≤ 1. Then there exists a rational function R of degree O(ln(l/ )2) so that replacing each σr in f with R yields a function g : [−1,+1]d → R with\nsup x∈[−1,+1]d\n|f(x)− g(x)| ≤ .\nCombining Theorem 1.2 and Lemma 1.3 yields an intriguing corollary.\nCorollary 1.4. For every k ≥ 3, there exists a function f : R → R computed by a rational network with O(k) layers andO(k) total nodes, each node invoking a rational activation of degreeO(k), such that every rational function g : R→ R with less than 2k−2 total terms in the numerator and denominator satisfies∫\n[0,1]\n|f(x)− g(x)|dx ≥ 1 128 .\nThe hard-to-approximate function f is a rational network which has a description of size O(k2). Despite this, attempting to approximate it with a rational function of the usual form requires a description of size Ω(2k). Said another way: even for rational functions, there is a benefit to a neural network representation!"
  }, {
    "heading": "1.2. Auxiliary results",
    "text": "The first thing to stress is that Theorem 1.1 is impossible with polynomials: namely, while it is true that ReLU networks can efficiently approximate polynomials (Yarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017), on the other hand polynomials require degree Ω(poly(1/ )), rather than O(poly(ln(1/ ))), to approximate a single ReLU, or equivalently the absolute value function (Petrushev & Popov, 1987, Chapter 4, Page 73).\nAnother point of interest is the depth needed when converting a rational function to a ReLU network. Theorem 1.1 is impossible if the depth is o(ln(1/ )): specifically, it is impossible to approximate the degree 1 rational function x 7→ 1/x with size O(ln(1/ )) but depth o(ln(1/ )). Proposition 1.5. Set f(x) := 1/x, the reciprocal map. For any > 0 and ReLU network g : R→ R with l layers and m < (27648 )−1/(2l)/2 nodes,∫\n[1/2,3/4]\n|f(x)− g(x)|dx > .\nLastly, the implementation of division in a ReLU network requires a few steps, arguably the most interesting being a “continuous switch statement”, which computes reciprocals differently based on the magnitude of the input. The ability to compute switch statements appears to be a fairly foundational operation available to neural networks and rational functions (Petrushev & Popov, 1987, Theorem 5.2), but is not available to polynomials (since otherwise they could approximate the ReLU)."
  }, {
    "heading": "1.3. Related work",
    "text": "The results of the present work follow a long line of work on the representation power of neural networks and related functions. The ability of ReLU networks to fit continuous functions was no doubt proved many times, but it appears the earliest reference is to Lebesgue (Newman, 1964, Page 1), though of course results of this type are usu-\nally given much more contemporary attribution (Cybenko, 1989). More recently, it has been shown that certain function classes only admit succinct representations with many layers (Telgarsky, 2015). This has been followed by proofs showing the possibility for a depth 3 function to require exponentially many nodes when rewritten with 2 layers (Eldan & Shamir, 2016). There are also a variety of other result giving the ability of ReLU networks to approximate various function classes (Cohen et al., 2016; Poggio et al., 2017).\nMost recently, a variety of works pointed out neural networks can approximate polynomials, and thus smooth functions essentially by Taylor’s theorem (Yarotsky, 2016; Safran & Shamir, 2016; Liang & Srikant, 2017). This somewhat motivates this present work, since polynomials can not in turn approximate neural networks with a dependence O(poly log(1/ )): they require degree Ω(1/ ) even for a single ReLU.\nRational functions are extensively studied in the classical approximation theory literature (Lorentz et al., 1996; Petrushev & Popov, 1987). This literature draws close connections between rational functions and splines (piecewise polynomial functions), a connection which has been used in the machine learning literature to draw further connections to neural networks (Williamson & Bartlett, 1991). It is in this approximation theory literature that one can find the following astonishing fact: not only is it possible to approximate the absolute value function (and thus the ReLU) over [−1,+1] to accuracy > 0 with a rational function of degreeO(ln(1/ )2) (Newman, 1964), but moreover the optimal rate is known (Petrushev & Popov, 1987; Zolotarev, 1877)! These results form the basis of those results here which show that rational functions can approximate ReLU networks. (Approximation theory results also provide other functions (and types of neural networks) which rational functions can approximate well, but the present work will stick to the ReLU for simplicity.)\nAn ICML reviewer revealed prior work which was embarrassingly overlooked by the author: it has been known, since decades ago (Beame et al., 1986), that neural networks using threshold nonlinearities (i.e., the map x 7→ 1[x ≥ 0]) can approximate division, and moreover the proof is similar to the proof of part 1 of Theorem 1.1! Moreover, other work on threshold networks invoked Newman polynomials to prove lower bound about linear threshold networks (Paturi & Saks, 1994). Together this suggests that not only the connections between rational functions and neural networks are tight (and somewhat known/unsurprising), but also that threshold networks and ReLU networks have perhaps more similarities than what is suggested by the differing VC dimension bounds, approximation results, and algorithmic results (Goel et al., 2017)."
  }, {
    "heading": "1.4. Further notation",
    "text": "Here is a brief description of the sorts of neural networks used in this work. Neural networks represent computation as a directed graph, where nodes consume the outputs of their parents, apply a computation to them, and pass the resulting value onward. In the present work, nodes take their parents’ outputs z and compute σr(a>z + b), where a is a vector, b is a scalar, and σr(x) := max{0, x}; another popular choice of nonlineary is the sigmoid x 7→ (1 + exp(−x))−1. The graphs in the present work are acyclic and connected with a single node lacking children designated as the univariate output, but the literature contains many variations on all of these choices.\nAs stated previously, a rational function f : Rd → R is ratio of two polynomials. Following conventions in the approximation theory literature (Lorentz et al., 1996), the denominator polynomial will always be strictly positive. The degree of a rational function is the maximum of the degrees of its numerator and denominator."
  }, {
    "heading": "2. Approximating ReLU networks with rational functions",
    "text": "This section will develop the proofs of part 2 of Theorem 1.1, Theorem 1.2, Lemma 1.3, and Corollary 1.4."
  }, {
    "heading": "2.1. Newman polynomials",
    "text": "The starting point is a seminal result in the theory of rational functions (Zolotarev, 1877; Newman, 1964): there exists a rational function of degree O(ln(1/ )2) which can approximate the absolute value function along [−1,+1] to accuracy > 0. This in turn gives a way to approximate the ReLU, since\nσr(x) = max{0, x} = x+ |x|\n2 . (2.1)\nThe construction here uses the Newman polynomials (New-\nman, 1964): given an integer r, define\nNr(x) := r−1∏ i=1 (x+ exp(−i/ √ r)).\nThe Newman polynomials N5, N9, and N13 are depicted in Figure 3. Typical polynomials in approximation theory, for instance the Chebyshev polynomials, have very active oscillations; in comparison, the Newman polynomials look a little funny, lying close to 0 over [−1, 0], and quickly increasing monotonically over [0, 1]. The seminal result of Newman (1964) is that\nsup |x|≤1 ∣∣∣∣∣|x| − x ( Nr(x)−Nr(−x) Nr(x) +Nr(−x) )∣∣∣∣∣ ≤ 3 exp(−√r)/2. Thanks to this bound and eq. (2.1), it follows that the ReLU can be approximated to accuracy > 0 by rational functions of degree O(ln(1/ )2).\n(Some basics on Newman polynomials, as needed in the present work, can be found in Appendix A.1.)"
  }, {
    "heading": "2.2. Proof of Lemma 1.3",
    "text": "Now that a single ReLU can be easily converted to a rational function, the next task is to replace every ReLU in a ReLU network with a rational function, and compute the approximation error. This is precisely the statement of Lemma 1.3.\nThe proof of Lemma 1.3 is an induction on layers, with full details relegated to the appendix. The key computation, however, is as follows. Let R(x) denote a rational approximation to σr. Fix a layer i + 1, and let H(x) denote the multi-valued mapping computed by layer i, and let HR(x) denote the mapping obtained by replacing each σr in H with R. Fix any node in layer i + 1, and let x 7→ σr(a>H(x) + b) denote its output as a function of the input. Then∣∣∣σr(a>H(x) + b)−R(a>HR(x) + b)∣∣∣\n≤ ∣∣∣σr(a>H(x) + b)− σr(a>HR(x) + b)∣∣∣︸ ︷︷ ︸\n♥ + ∣∣∣σr(a>HR(x) + b)−R(a>HR(x) + b)∣∣∣︸ ︷︷ ︸\n♣\n.\nFor the first term ♥, note since σr is 1-Lipschitz and by Hölder’s inequality that\n♥ ≤ ∣∣∣a>(H(x)−HR(x))∣∣∣ ≤ ‖a‖1‖H(x)−HR(x)‖∞,\nmeaning this term has been reduced to the inductive hypothesis since ‖a‖1 ≤ 1. For the second term ♣, if\na>HR(x) + b can be shown to lie in [−1,+1] (which is another easy induction), then ♣ is just the error between R and σr on the same input."
  }, {
    "heading": "2.3. Proof of part 2 of Theorem 1.1",
    "text": "It is now easy to find a rational function that approximates a neural network, and to then bound its size. The first step, via Lemma 1.3, is to replace each σr with a rational functionR of low degree (this last bit using Newman polynomials). The second step is to inductively collapse the network into a single rational function. The reason for the dependence on the number of nodes m is that, unlike polynomials, summing rational functions involves an increase in degree:\np1(x) q1(x) + p1(x) q2(x) = p1(x)q2(x) + p2(x)q1(x) q1(x)q2(x) ."
  }, {
    "heading": "2.4. Proof of Theorem 1.2",
    "text": "The final interesting bit is to show that the dependence on ml in part 2 of Theorem 1.1 (where m is the number of nodes and l is the number of layers) is tight.\nRecall the “triangle function”\n∆(x) :=  2x x ∈ [0, 1/2], 2(1− x) x ∈ (1/2, 1], 0 otherwise.\nThe k-fold composition ∆k is a piecewise affine function with 2k−1 regularly spaced peaks (Telgarsky, 2015). This function was demonstrated to be inapproximable by shallow networks of subexponential size, and now it can be shown to be a hard case for rational approximation as well.\nConsider the horizontal line through y = 1/2. The function ∆k will cross this line 2k times. Now consider a rational function f(x) = p(x)/q(x). The set of points where f(x) = 1/2 corresponds to points where 2p(x)−q(x) = 0.\nA poor estimate for the number of zeros is simply the degree of 2p−q, however, since f is univariate, a stronger tool becomes available: by Descartes’ rule of signs, the number of zeros in f − 1/2 is upper bounded by the number of terms in 2p− q."
  }, {
    "heading": "3. Approximating rational functions with ReLU networks",
    "text": "This section will develop the proof of part 1 of Theorem 1.1, as well as the tightness result in Proposition 1.5"
  }, {
    "heading": "3.1. Proving part 1 of Theorem 1.1",
    "text": "To establish part 1 of Theorem 1.1, the first step is to approximate polynomials with ReLU networks, and the second is to then approximate the division operation.\nThe representation of polynomials will be based upon constructions due to Yarotsky (2016). The starting point is the following approximation of the squaring function. Lemma 3.1 ((Yarotsky, 2016)). Let any > 0 be given. There exists f : x → [0, 1], represented as a ReLU network with O(ln(1/ )) nodes and layers, such that supx∈[0,1] |f(x)− x2| ≤ and f(0) = 0.\nYarotsky’s proof is beautiful and deserves mention. The approximation of x2 is the function fk, defined as\nfk(x) := x− k∑ i=1 ∆i(x) 4i ,\nwhere ∆ is the triangle map from Section 2. For every k, fk is a convex, piecewise-affine interpolation between points along the graph of x2; going from k to k + 1 does not adjust any of these interpolation points, but adds a new set of O(2k) interpolation points.\nOnce squaring is in place, multiplication comes via the polarization identity xy = ((x+ y)2 − x2 − y2)/2. Lemma 3.2 ((Yarotsky, 2016)). Let any > 0 and B ≥ 1 be given. There exists g(x, y) : [0, B]2 → [0, B2], represented by a ReLU network with O(ln(B/ ) nodes and layers, with\nsup x,y∈[0,1]\n|g(x, y)− xy| ≤\nand g(x, y) = 0 if x = 0 or y = 0.\nNext, it follows that ReLU networks can efficiently approximate exponentiation thanks to repeated squaring. Lemma 3.3. Let ∈ (0, 1] and positive integer y be given. There exists h : [0, 1] → [0, 1], represented by a ReLU network with O(ln(y/ )2) nodes and layers, with\nsup x,y∈[0,1]\n∣∣h(x)− xy∣∣ ≤\nWith multiplication and exponentiation, a representation result for polynomials follows.\nLemma 3.4. Let ∈ (0, 1] be given. Let p : [0, 1]d → [−1,+1] denote a polynomial with ≤ s monomials, each with degree ≤ r and scalar coefficient within [−1,+1]. Then there exists a function q : [0, 1]d → [−1,+1] computed by a network of size O ( min{sr ln(sr/ ), sd ln(dsr/ )2} ) , which satisfies supx∈[0,1]d |p(x)− q(x)| ≤ .\nThe remainder of the proof now focuses on the division operation. Since multiplication has been handled, it suffices to compute a single reciprocal.\nLemma 3.5. Let ∈ (0, 1] and nonnegative integer k be given. There exists a ReLU network q : [2−k, 1] → [1, 2k], of sizeO(k2 ln(1/ )2) and depthO(k4 ln(1/ )3) such that\nsup x∈[2−k,1] ∣∣∣∣q(x)− 1x ∣∣∣∣ ≤ .\nThis proof relies on two tricks. The first is to observe, for x ∈ (0, 1], that\n1 x =\n1 1− (1− x) = ∑ i≥0 (1− x)i.\nThanks to the earlier development of exponentiation, truncating this summation gives an expression easily approximate by a neural network as follows.\nLemma 3.6. Let 0 < a ≤ b and > 0 be given. Then there exists a ReLU network q : R → R with O(ln(1/(a ))2) layers and O((b/a) ln(1/(a ))3) nodes satisfying\nsup x∈[a,b] ∣∣∣∣q(x)− 1x ∣∣∣∣ ≤ 2 .\nUnfortunately, Lemma 3.6 differs from the desired statement Lemma 3.6: inverting inputs lying within [2−k, 1] requires O(2k ln(1/ )2) nodes rather than O(k4 ln(1/ )3)!\nTo obtain a good estimate with only O(ln(1/ )) terms of the summation, it is necessary for the input to be x bounded below by a positive constant (not depending on k). This leads to the second trick (which was also used by Beame et al. (1986)!).\nConsider, for positive constant c > 0, the expression\n1 x =\nc\n1− (1− cx) = c ∑ i≥0 (1− cx)i.\nIf x is small, choosing a larger c will cause this summation to converge more quickly. Thus, to compute 1/x accurately over a wide range of inputs, the solution here is to multiplex approximations of the truncated sum for many choices of c. In order to only rely on the value of one of them, it is possible to encode a large “switch” style statement in a neural network. Notably, rational functions can also representat switch statements (Petrushev & Popov, 1987, Theorem 5.2), however polynomials can not (otherwise they could approximate the ReLU more efficiently, seeing as it is a switch statement of 0 (a degree 0 polynomial) and x (a degree 1 polynomial). Lemma 3.7. Let > 0, B ≥ 1, reals a0 ≤ a1 ≤ · · · ≤ an ≤ an+1 and a function f : [a0, an+1] → R be given. Moreover, suppose for i ∈ {1, . . . , n}, there exists a ReLU network gi : R → R of size ≤ mi and depth ≤ ki with gi ∈ [0, B] along [ai−1, ai+1] and\nsup x∈[ai−1,ai+1]\n|gi(x)− f | ≤ .\nThen there exists a function g : R → R computed by a ReLU network of size O ( n ln(B/ ) + ∑ imi ) and depth\nO ( ln(B/ ) + maxi ki ) satisfying\nsup x∈[a1,an]\n|g(x)− f(x)| ≤ 3 ."
  }, {
    "heading": "3.2. Proof of Proposition 1.5",
    "text": "It remains to show that shallow networks have a hard time approximating the reciprocal map x 7→ 1/x.\nThis proof uses the same scheme as various proofs in (Telgarsky, 2016), which was also followed in more recent works (Yarotsky, 2016; Safran & Shamir, 2016): the idea is to first upper bound the number of affine pieces in ReLU networks of a certain size, and then to point out that each linear segment must make substantial error on a curved function, namely 1/x.\nThe proof is fairly brute force, and thus relegated to the appendices."
  }, {
    "heading": "4. Summary of figures",
    "text": "Throughout this work, a number of figures were presented to show not only the astonishing approximation properties\nof rational functions, but also the higher fidelity approximation achieved by both ReLU networks and rational functions as compared with polynomials. Of course, this is only a qualitative demonstration, but still lends some intuition.\nIn all these demonstrations, rational functions and polynomials have degree 9 unless otherwise marked. ReLU networks have two hidden layers each with 3 nodes. This is not exactly apples to apples (e.g., the rational function has twice as many parameters as the polynomial), but still reasonable as most of the approximation literature fixes polynomial and rational degrees in comparisons.\nFigure 1 shows the ability of all three classes to approximate a truncated reciprocal. Both rational functions and ReLU networks have the ability to form “switch statements” that let them approximate different functions on different intervals with low complexity (Petrushev & Popov, 1987, Theorem 5.2). Polynomials lack this ability; they can not even approximate the ReLU well, despite it being low degree polynomials on two separate intervals.\nFigure 2 shows that rational functions can fit the threshold function errily well; the particular rational function used here is based on using Newman polynomials to approximate (1 + |x|/x)/2 (Newman, 1964).\nFigure 3 shows Newman polynomialsN5,N9,N13. As discussed in the text, they are unlike orthogonal polynomials, and are used in all rational function approximations except Figure 1, which used a least squares fit.\nFigure 4 shows that rational functions (via the Newman polynomials) fit ∆ very well, whereas polynomials have trouble. These errors degrade sharply after recursing, namely when approximating ∆3 as in Figure 6.\nFigure 5 shows how polynomials and rational functions fit the ReLU, where the ReLU representation, based on Newman polynomials, is the one used in the proofs here. Despite the apparent slow convergence of polynomials in this regime, the polynomial fit is still quite respectable."
  }, {
    "heading": "5. Open problems",
    "text": "There are many next steps for this and related results.\n1. Can rational functions, or some other approximating class, be used to more tightly bound the generalization properties of neural networks? Notably, the VC dimension of sigmoid networks uses a conversion to polynomials (Anthony & Bartlett, 1999).\n2. Can rational functions, or some other approximating class, be used to design algorithms for training neural networks? It does not seem easy to design reasonable algorithms for minimization over rational functions; if this is fundamental and moreover in contrast with neural networks, it suggests an algorithmic benefit of neural networks.\n3. Can rational functions, or some other approximating class, give a sufficiently refined complexity estimate of neural networks which can then be turned into a regularization scheme for neural networks?"
  }, {
    "heading": "Acknowledgements",
    "text": "The author thanks Adam Klivans and Suvrit Sra for stimulating conversations. Adam Klivans and the author both thank Almare Gelato Italiano, in downtown Berkeley, for necessitating further stimulating conversations, but now on the topic of health and exercise. Lastly, the author thanks the University of Illinois, Urbana-Champaign, and the Simons Institute in Berkeley, for financial support during this work."
  }],
  "year": 2017,
  "references": [{
    "title": "Neural Network Learning: Theoretical Foundations",
    "authors": ["Anthony", "Martin", "Bartlett", "Peter L"],
    "year": 1999
  }, {
    "title": "Log depth circuits for division and related problems",
    "authors": ["Beame", "Paul", "Cook", "Stephen A", "Hoover", "H. James"],
    "venue": "SIAM Journal on Computing,",
    "year": 1986
  }, {
    "title": "On the expressive power of deep learning: A tensor analysis",
    "authors": ["Cohen", "Nadav", "Sharir", "Or", "Shashua", "Amnon"],
    "venue": "COLT",
    "year": 2016
  }, {
    "title": "Approximation by superpositions of a sigmoidal function",
    "authors": ["Cybenko", "George"],
    "venue": "Mathematics of Control, Signals and Systems,",
    "year": 1989
  }, {
    "title": "The power of depth for feedforward neural networks",
    "authors": ["Eldan", "Ronen", "Shamir", "Ohad"],
    "venue": "In COLT,",
    "year": 2016
  }, {
    "title": "Reliably learning the relu in polynomial time",
    "authors": ["Goel", "Surbhi", "Kanade", "Varun", "Klivans", "Adam", "Thaler", "Justin"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "Why deep neural networks for function approximation",
    "authors": ["Liang", "Shiyu", "R. Srikant"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Constructive approximation : advanced problems",
    "authors": ["G.G. Lorentz", "Golitschek", "Manfred von", "Makovoz", "Yuly"],
    "venue": "Michigan Math. J., 11(1):11–14,",
    "year": 1996
  }, {
    "title": "Approximating threshold circuits by rational functions",
    "authors": ["Paturi", "Ramamohan", "Saks", "Michael E"],
    "venue": "Inf. Comput.,",
    "year": 1994
  }, {
    "title": "Rational approximation of real functions. Encyclopedia of mathematics and its applications",
    "authors": ["Petrushev", "P.P. Penco Petrov", "Popov", "Vasil A"],
    "year": 1987
  }, {
    "title": "Why and when can deep – but not shallow – networks avoid the curse of dimensionality: a review",
    "authors": ["Poggio", "Tomaso", "Mhaskar", "Hrushikesh", "Rosasco", "Lorenzo", "Miranda", "Brando", "Liao", "Qianli"],
    "year": 2017
  }, {
    "title": "Depth separation in relu networks for approximating smooth non-linear functions. 2016",
    "authors": ["Safran", "Itay", "Shamir", "Ohad"],
    "year": 2016
  }, {
    "title": "Representation benefits of deep feedforward networks",
    "authors": ["Telgarsky", "Matus"],
    "year": 2015
  }, {
    "title": "Benefits of depth in neural networks",
    "authors": ["Telgarsky", "Matus"],
    "venue": "In COLT,",
    "year": 2016
  }, {
    "title": "Splines, rational functions and neural networks",
    "authors": ["Williamson", "Robert C", "Bartlett", "Peter L"],
    "venue": "In NIPS,",
    "year": 1991
  }, {
    "title": "Error bounds for approximations with deep relu networks. 2016",
    "authors": ["Yarotsky", "Dmitry"],
    "year": 2016
  }],
  "id": "SP:d75943b9f1be1c97f977ca4df62f8eb514b13373",
  "authors": [{
    "name": "Matus Telgarsky",
    "affiliations": []
  }],
  "abstractText": "Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degreeO(poly log(1/ )) which is -close, and similarly for any rational function there exists a ReLU network of size O(poly log(1/ )) which is -close. By contrast, polynomials need degree Ω(poly(1/ )) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.",
  "title": "Neural Networks and Rational Functions"
}