{
  "sections": [{
    "text": "1 e2 ) approximation ratio for general mono-\ntone submodular functions and general matroid constraints. We demonstrate the effectiveness of our approach on several real-world applications where running the maximization problem on the reduced ground set leads to two orders of magnitude speed-up while incurring almost no loss."
  }, {
    "heading": "1. Introduction",
    "text": "Motivated by applications in data summarization (Lin & Bilmes, 2011; Wei et al., 2013; Mirzasoleiman et al., 2016d) and recommender systems (El-Arini et al., 2009; Yue & Guestrin, 2011; Mirzasoleiman et al., 2016a), we tackle the challenge of efficiently solving many statistically related submodular maximization problems. In these applications, submodularity arises in the form of user-specific\n1Yale University, New Haven, Connecticut, USA 2Google Research, New York, NY 10011, USA 3ETH Zurich, Zurich, Switzerland. Correspondence to: Amin Karbasi <amin.karbasi@yale.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nutility functions for valuating sets of items, and a prototypical problem is to find sets of say k items with nearmaximal value (Krause & Golovin, 2012; Mirzasoleiman et al., 2016b). Even though efficient greedy algorithms exist for submodular maximization, those become infeasible when serving many users and optimizing over large item collections. To this end, given training examples of (users and their utility) functions drawn from some unknown distribution, we seek to reduce the ground set to a small (ideally sublinear) size. The hope is that optimizing new functions drawn from the same distribution will incur little loss when optimized on the reduced set compared to optimizing over the full set.\nOptimizing the empirical objective is an instance of twostage submodular maximization, a problem recently considered by Balkanski et al. (2016) who provide a 0.316 approximation guarantee for the case of coverage functions (more about it in the related work). One of our key technical contributions is a computationally efficient novel local-search based algorithm, called ReplacementGreedy, for two-stage submodular maximization, that provides a constant-factor 0.432 approximation guarantee for the general case of monotone submodular functions. Our approach also generalizes to arbitrary matroid constraints, and empirically compares favorably to prior work. We further analyze conditions under which our approach provably enables approximate submodular optimization based on substantially reduced ground sets, resulting the first viable approach towards sublinear-time submodular maximization.\nLastly, we demonstrate the effectiveness of our approach on recommender systems for which we compare the utility value and running time of maximizing a submodular function over the full data set and its reduced version returned by our algorithm. We consistently observe that the loss we incur is negligible (around 1%) while the speed up is enormous (about two orders of magnitude).\nRelated work Submodular maximization has found many applications in machine learning, ranging from feature/variable selection (Krause & Guestrin, 2005) to dictionary learning (Das & Kempe, 2011) to data summarization (Wei et al., 2014b; Lin & Bilmes, 2011; Tschiatschek et al., 2014) to recommender systems (Yue & Guestrin,\n2011; El-Arini et al., 2009). A seminal result of Nemhauser et al. (1978) proves that a simple greedy algorithm provides an optimal constant factor (1 − 1/e) approximation guarantee. Given the size of modern data sets, much work has focused on solving submodular maximization at scale. This work ranges from accelerating the greedy algorithm itself (Minoux, 1978; Mirzasoleiman et al., 2015; 2016a; Badanidiyuru & Jan, 2014; Buchbinder et al., 2014) to distributed (Kumar et al., 2013; Mirzasoleiman et al., 2013) and streaming (Krause & Gomes, 2010; Badanidiyuru et al., 2014) approaches, as well as algorithms based on filtering the ground set in multiple stages (Wei et al., 2014a; Feldman et al., 2017). All of these approaches aim to solve a single fixed problem instance, and have computational cost at least linear in the ground set size. In contrast, we seek to solve multiple related problems with sublinear effort.\nSolving multiple submodular problems arises in online submodular optimization (Streeter & Golovin, 2008; Hazan & Kale, 2009; Jegelka & Bilmes, 2011). In this setting the goal is to design algorithms that perform well in hindsight (minimize some form of regret). The computational complexity of these algorithms is typically (super)-linear in the ground set size. Studying online variants of our problem is an interesting direction for future work.\nClosest to our work is the approach of Balkanski et al. (2016) that we build on and compare within this paper. For general submodular objectives, they propose two algorithms: one based on continuous optimization (with prohibitive computational complexity in terms of the size of the ground set n), which provides constant factor approximation guarantees only for large values of k (i.e., cardinality constraint), as well as one that has exponential complexity in terms of k. To circumvent the large computational complexity of the above algorithms, they also proposed a heuristic local search method that offers a 12 (1− 1 e ) approximation for the special case of coverage functions. The query complexity of this algorithm is O(km`n2 log n) where ` is the size of the summary and m is the number of considered submodular functions in the two-stage optimization. One of our key contributions is a novel and computationally efficient algorithm for the two-stage problem. More specifically, our method ReplacementGreedy provides a 12 (1 − e\n−2) approximation guarantee for general monotone submodular functions subject to a general matroid constraint with only O(rm`n) query complexity (here r denotes the rank of the matroid). As argued by Balkanski et al. (2016), two-stage submodular maximization can be seen as a discrete analogue of representation learning problems like dictionary learning. It is important to note that the two-stage submodular maximization problem is fractionally subadditive (XOS) (Feige, 2009). Although it is tempting to use the XOS property of our two-\nstage function especially given the positive results for social welfare XOS maximization, there are several obstacles preventing us from doing so. First, evaluating the two stage function is NP-hard, so we cannot have access to oracle value queries. Second, as shown by Singer (2010); Shahar Dobzinski & Schapira (2005); Ashwinkumar Badanidiyuru (2012), any n1/2− approximation of a XOS requires exponentially many oracle value queries. Therefore the XOS property itself is not sufficient to get any positive algorithmic result for our problem. Nevertheless, we can still provide constant factor approximation with computationally efficient algorithms.\nRepeatedly optimizing related classes of submodular functions is a key subroutine in many applications, such as structured prediction (Lin & Bilmes, 2012) or linear submodular bandits (Yue & Guestrin, 2011). In both of these problems, one needs to repeatedly maximize weighted combinations of submodular functions, for changing weights. Our work can be viewed as providing an approach towards accelerating this central subroutine."
  }, {
    "heading": "2. Problem Setup",
    "text": "In this paper, we consider the problem of frequently optimizing monotone submodular functions f : 2Ω → R+ that are drawn from some unknown probability distribution D. Hereby, Ω denotes the ground set of size n over which the submodular functions are defined. W.l.o.g. we assume that the maximum value of any function f drawn from D does not exceed 1. This setting arises in many applications, such as recommender systems, where the random function f = fu refers to the (predicted) valuation over sets of items for a particular user u, which may vary depending on their features (c.f., Yue & Guestrin (2011)). These applications typically dictate some constraints, i.e., one seeks to solve\nT ∗ = arg max f(T ) s.t. T ∈ I,\nwhere I ⊆ 2Ω is a collection of feasible sets. In this paper we primarily consider cardinality constraints, i.e., I = {T ⊆ Ω : |T | ≤ k}. Our results will hold also in the more general setting where I is the collection of independent sets in some matroid (Calinescu et al., 2011). Throughout the paper r denotes the rank of the matroid. In the special case of cardinality constraint, the rank is r = k.\nWhile NP-hard, good approximation algorithms are known for submodular maximization. For example, the classical greedy algorithm of Nemhauser et al. (1978) or its accelerated variants (Minoux, 1978; Mirzasoleiman et al., 2015; 2016a; Badanidiyuru & Jan, 2014; Buchbinder et al., 2014) provide an optimal constant factor (1−1/e) approximation for maximization under cardinality constraints. In modern applications, however, the system may face a large number of users, and large collections of items Ω. Hence the\nnaive strategy of even greedily optimizing fu for each user separately may be too costly.\nTo remedy this situation, in this paper we consider the following approach: Given training data (i.e., a sample collection of functions f1, . . . , fm), we invest computation once to obtain a reduced ground set S of size ` n = |Ω|. The hope is that optimizing new functions arising at test time will provide almost as much value when restricting the choice to items in S, than when considering arbitrary items in Ω, while being substantially more computationally efficient.\nMore formally, the expected performance when using the candidate reduced ground set S is\nG(S) = Ef∼D max T∈I(S) f(T ), (1)\nwhere we use I(S) ≡ {T ∈ I and T ⊆ S} to refer to the collection of feasible sets restricted to those containing only elements from S. The optimum achievable performance would be G(Ω). Our goal will be to pick a set S of small size ` to maximize G(S), or equivalently make |G(Ω)−G(S)| small.\nSpecial cases. Some observations are in order. If D is deterministic, i.e., puts all mass on a single function f , then we simply recover classical constrained submodular maximization, since G(S) = f(S) for sets up to size k. If D is known to be the uniform distribution over m functions, G(S) becomes\nGm(S) = 1\nm m∑ i=1 max T∈I(S) fi(T ). (2)\nGm(S) is not generally submodular, but Balkanski et al. (2016) have developed approximation algorithms for maximizing Gm under the constraint that |S| ≤ `,1 i.e., for the problem:\nSm,` = arg max S⊂Ω,|S|≤`\n1\nm m∑ i=1 max T∈I(S) fi(T ). (3)\nThe general case. In this paper, we consider the problem of maximizing G(S) for general distributions D. I.e., we seek to solve:\nS∗` = arg max S⊂Ω,|S|≤` G(S). (4)"
  }, {
    "heading": "3. Probabilistic Submodular Maximization",
    "text": "Since the distribution D is unknown, we cannot find S∗` or its corresponding optimum value, G(S∗` ). Instead, we\n1Balkanski et al. (2016) only consider cardinality constraints.\ncan sample functions from D and construct the empirical average, similar to Problem (2), and try to optimize it by finding Sm,`. Hence, the total generalization error we incur in this process is bounded by\nerror = |G(Ω)−G(Sm,`)| ≤ |G(Ω)−G(S∗` )|\ncompression error + |G(S∗` )−G(Sm,`)| approximation error .(5)\nNote that once we have error ≤ for some small > 0 then maximizing over Sm,` is almost as good as maximizing over Ω (but possibly much faster). To that end we need to bound both compression error and approximation error. In fact we can prove the following result for the required number of samples to ensure small approximation error.\nTheorem 1. For any , δ > 0, and any set S of size at most ` we can ensure that\nPr max S⊂Ω |S|≤` |G(S)−Gm(S)| >  < δ as long as m = O((` log(n) + log(1/δ))/ 2).\nIn contrast, the compression error cannot be made arbitrarily small in general as the following example shows.\nBad Example. Suppose Ω = [n], m = n and D is the uniform distribution over the functions f1, . . . , fn, where fi(S) is 1 if i ∈ S, 0 otherwise. Each fi is in fact modular. Let k = 1. It is easy to see that G(S) = |S|/n. Hence to achieve compression error less than , |S| must be greater than (1 − )n. In particular for < 1/n, S must be equal to the full set.\nSufficient conditions for sublinear `. The above example shows that one needs additional structural assumptions. One simple special case arises when the union of the optimal sets for all functions in the support of D is of size ` < n, i.e.,∣∣{T ∗ : T ∗ = arg max\nT∈I f(T ) for some f ∈ supp(D)} ∣∣ = `. I.e., if D is any distribution over at most m functions, clearly ` ≥ km suffices.\nThis assumption might be too strong in practice, however. Instead, we will consider another set of assumptions that allow bounding the compression error in the case of cardinality constraints. We assume Ω is endowed with a metric d. This metric is extended to sets of equal size so that for any sets T, T ′ of equal size, d(T, T ′) is the weight of a minimal matching of elements in T to elements in T ′, where the weight of (v, v′) for v ∈ T and v′ ∈ T ′ is d(v, v′). We will assume that any function f ∼ D is L-Lipschitz continuous w.r.t. d, i.e., |f(T )−f(T ′)| ≤ Ld(T, T ′) for some constant\nL for all sets T and T ′ of size k. Many natural submodular functions arising in data summarization tasks satisfy this condition, such as exemplar-based clustering and certain log-determinants, see, e.g., (Mirzasoleiman et al., 2016c).\nTheorem 2. Suppose each function f ∼ D is L-Lipschitz continuous in (Ω, d). Then, for any > 0, the compression error is bounded by as long as ` ≥ kn /2kL, where nδ is the δ-covering number of (Ω, d).\nThe proofs of Theorems 1 and 2 are given in the appendix."
  }, {
    "heading": "4. Algorithm",
    "text": "From the discussions in the previous section, we concluded that under appropriate statistical conditions, we can ensure that the error in Eq. 5 can be made small if we have enough samples dictated by Theorem 1. However, our conclusion heavily relied on the fact that we can find the set Sm,` in Problem 3. As we noted earlier, the objective function in Problem 3 is not submodular in general (Balkanski et al., 2016), thus the classical greedy algorithm may not provide any approximation guarantees.\nOur proposed algorithm ReplacementGreedy works in ` rounds where in each round it tries to augment the solution in a particular greedy fashion. It starts out with an empty set S = ∅, and checks (in each round) whether a new element can be added without violating the matroid constraint (i.e., stay an independent set) or otherwise it can be replaced with an element in the current solution while increasing the value of the objective function. To describe how these decisions are made we need a few definitions. Let\n∆i(x,A) = fi({x} ∪A)− fi(A)\ndenote the marginal gain of adding x to the set A if we consider function fi. Similarly, we can define the gain of removing an element y and replacing it with x as ∇i(x, y,A) = fi({x} ∪ A \\ {y}) − fi(A). Since fi is monotone we know that ∆i(x,A) ≥ 0. However, ∇i(x, y,A) may or may not be positive. Let us consider I(x,A) = {y ∈ A : A ∪ {x} \\ {y} ∈ I}. This is the set of all elements in A such that if we replace them with x we will not violate the matroid constraint. Then, we define the replacement gain of x w.r.t. a set A as follows:\n∇i(x,A) = { ∆i(x,A) if A ∪ {x} ∈ I, max{0,maxy∈I(x,A)∇i(x, y,A)} o.w.\nIn words,∇i(x,A) denotes how much we can increase the value of fi(A) by either inserting x into A or replacing x with one element of A while keeping A an independent set. Finally, let Repi(x,A) be the element that should be replaced by x to maximize the gain and stay independent.\nFormally,\nRepi(x,A) = { ∅ if A ∪ {x} ∈ I, arg maxy∈I(x,A)∇i(x, y,A) o.w.\nWith the above definitions it is easy to explain how ReplacementGreedy works. At all times, it maintains a solution S and a collection of feasible solutions Ti ⊆ S for each function fi (all initialized to the empty set in the beginning). In each iteration, it picks the top element x∗ from the ground set Ω, based on its total contribution to fi’s, i.e, ∑m i=1∇i(x, Ti), and updates S. Then ReplacementGreedy checks whether any of Ti’s can be augmented. This is done either by simply adding x∗ (without violating the matroid constraint) or replacing it with an element from Ti. The condition ∇i(x∗, Ti) > 0 ensures that such replacement is executed only if the gain is positive.\nWhy does ReplacementGreedy work? Note that solving maxT∈I(S) fi(T ) is an NP-hard problem. However, ReplacementGreedy finds and maintains independent sets Ti ⊆ S throughout the course of the algorithm. In fact, the collection {S, T1, . . . , Tm} lower bounds Gm(S) by 1m ∑m i=1 fi(Ti). Moreover, each iteration increases the\naggregate value ∑m i=1 fi(Ti) by ∑m i=1∇i(x∗, Ti). What we show in the following theorem is that after ` iterations, the accumulation of those gains reaches a constant factor approximation to the optimum value.\nTheorem 3. In only O(`mnr) function evaluations ReplacementGreedy returns a set S of size at most ` along with independent sets Ti ∈ I(S) such that\nGm(S) ≥ 1\n2\n( 1− 1\ne2\n) Gm(S m,`).\nA few comments are in order. Balkanski et al. (2016) proposed an algorithm with (1− 1/e)/2-approximation guarantee for the case where fi’s are coverage functions and the constraint is a uniform matroid. This is achieved by solving (potentially large) linear programs while maintaining O(k`mn2 log n) function evaluations. In contrast, our result holds for any collection of monotone submodular functions and any matroid constraint. Our approximation guarantee (1 − e−2)/2 is better than (1 − 1/e)/2. Finally, our algorithm is arguably faster in terms of both running time (as it simply runs a modified greedy method) and query complexity (as it is linear in all the parameters)."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we describe our experimental setting. We offer details on the datasets we used and the baselines we ran ReplacementGreedy against. We first show that ReplacementGreedy is highly efficient (i.e., high utility,\nAlgorithm 1 ReplacementGreedy S ← ∅, Ti ← ∅ (∀1 ≤ i ≤ m) for 1 ≤ j ≤ ` do x∗ ← arg maxx∈Ω ∑m i=1∇i(x, Ti)\nS ← S ∪ {x∗} for all 1 ≤ i ≤ m do\nif ∇i(x∗, Ti) > 0 then Ti ← Ti ∪ {x∗}\\Repi(x∗, Ti)\nend if end for\nend for Return sets S and T1, T2, · · · , Tm\nlow running time) when solving the two-stage submodular maximization problem on two concrete summarization applications: article summarization and image summarization. We then demonstrate sublinear submodular maximization by showing that ReplacementGreedy can efficiently reduce the dataset while incurring minimum loss. We test the performance of ReplacementGreedy on a movie dataset where movies should be recommended to users with user-specific utility functions."
  }, {
    "heading": "5.1. Baselines",
    "text": "LocalSearch. This is the main algorithm described in Balkanski et al. (2016)2. In our experiments, we use = 0.2. We initialize S by incrementally picking ` elements such that at each step we maximize the sum of marginal gains for the m functions fi.\nGreedySum. It greedily selects ` elements while maximizing the submodular function F̂ (S) = ∑m i=1 fi(S). To find k elements for each fi it runs another greedy algorithm.\nGreedyMerge. It ideally serves as an upper bound for the objective value, by greedily selecting k elements for each function fi and returning their union. GreedyMerge can easily violate the cardinality constraint ` as its solution can have as many as mk elements."
  }, {
    "heading": "5.2. Metrics",
    "text": "Objective value. We compare algorithms’ solutions S according to the scores Gm(S) for the two-stage (empirical) problem, and G(S) for the sublinear maximization problem.\nLoss. For the sublinear maximization problem, we measure the relative loss in performance when using the summary, compared to the full ground set, i.e., report 1−G(S)/G(Ω).\n2We thank the authors for providing us with their implementation.\nRunning time. We also compare the algorithms based on their wall-clock running time. The experiments were ran in a Python 2.7 environment on a OSX 10.12.13 machine. The processor was a 2.5 GHz Intel Core i7 with 16 GB 1600 MHz DDR3 memory."
  }, {
    "heading": "5.3. Two-Stage Submodular Maximization",
    "text": "Article summarization on Wikipedia. The aim is to select a small, highly relevant subset of wikipedia articles from a larger corpus. For this task, we reproduce the experiment from Balkanski et al. (2016) on Wikipedia articles for Machine Learning. The dataset contains n = 407 articles divided into m = 22 categories, where each category represents a subtopic from Machine Learning. The relevance of a set S with respect to a category i is measured by the submodular function fi that counts the number of pages that belong to category i with a link to at least one page in S. These submodular functions are L-Lipschitz with L = 1 by considering the distance between two articles as the fraction of all pages that have a link to exactly one of these two articles. Fig. 1a and 1b show the objective values for a fix k = 5 (and varying `) and a fixed ` = 20 (and varying k). We find that ReplacementGreedy and LocalSearch perform the same while GreedySum falls off somewhat for larger values of `. However, if we look at the running times (log scale) in Fig. 1e and 1f we observe that ReplacementGreedy is considerably faster than LocalSearch and close to GreedySum.\nImage summarization on VOC2012. For this application we use a subset of the VOC2012 dataset (Everingham et al., 2014) where we consider n = 150 withm = 20 categories. Each category indicates a certain visual queue appearing in the image such as chair, bird, hand, etc. We wish to obtain a subset of these images that are relevant to all the categories. To that end we use Exemplar Based Clustering (c.f., Mirzasoleiman et al. (2013)). Let Ωi be the portion of the ground set associated to category i. For any set S we also let Si = Ωi ∩ S denote its subset that is part of category i. We define fi(S) = Li({e0}) − Li(S ∪ {e0}) where Li(S) =\n1 |Ωi| ∑ x∈Ωi miny∈Si d(x, y). Here, d measures\nthe distance between images (e.g., `2 norm), and e0 is an auxiliary element. With respect to distance d, our submodular functions are L-Lipschitz with L = 1. Also, images are represented by feature vectors obtained from categories. For example, if there were two categories a and b, and an image had features [a, a, b], its feature vector would be (2, 1). Again if we look at Fig. 1c (for fixed k = 5 and varying `) and Fig. 1d (for ` = 20 and varying k) we see that ReplacementGreedy and LocalSearch achieve the same objective value. However, Fig. 1g and 1h show that LocalSearch is significantly slower than ReplacementGreedy."
  }, {
    "heading": "5.4. Sublinear Summarization",
    "text": "In this part, we experimentally show how ReplacementGreedy can reduce the size of the dataset to ` nwithout incurring too much loss in the process. To do so, we consider a movie recommendation application.\nMovie recommendation with missing ratings. The dataset consists of user ratings on a scale of 1 to 5, along with some movie information. There are 20 genres (Animation, Comedy, Thriller, etc) and each movie can have one or more of these genres assigned to it. The goal is to find a small set S of movies with the property that each user will be able to find k enjoyable movies from S.\nIn our setting we consider the top 2000 highest rated movies (that were rated by at least 20 users) alongside the top 200 users ordered by number of movies they rated from this set. We assign a user-specific utility function fi to each user i as follows. Let Ai be the subset of A which user i rated. Let g be the number of movie genres. Furthermore, we letR(i, A, j) represent the highest rating given by user i to a movie from the set A with genre j. We also define wi,j to be the proportion of movies in genre j that user i rated out of all the movie-genre ratings she provided. Sowi,j will be higher for the genres that the user provided more feed-\nback on, indicating that she might like these genres better. Then, the valuation function by user i is as follows:\nfi(A) = g∑ j=1 wi,jR(i, A, j).\nIn words, the way a user evaluates a set A is by picking the highest rated movie from each genre (contained in A) and then take a weighted average. If we define the distance between two movies to be the maximum difference of ratings they received from the same user, the submodular functions fi will be L-Lipschitz with L = 1.\nFor the experiment, we split the users in half. We use the first 100 of them as a training set for the algorithms to built up their reduced ground set S of size `. We then compare submodular maximization with cardinality constraint k = 3 on the reduced sets S (returned by the baselines) and that of the whole ground set Ω. To this end, we sample 20 users from the test group and compute their average values. Given the size of this experiment, we were unable to run LocalSearch alongside ReplacementGreedy and GreedySum. In its place, we introduce the random baseline that simply returns a set of size ` at random. Fig 3a shows what fraction of the utility is preserved if we reduce the ground set by ReplacementGreedy, GreedySum,\nand RandomSelection. Clearly, RandomSelection performs poorly. ReplacementGreedy has the highest utility, starting from 80% and practically closing the gap by reducing the ground set to only 60 movies. GreedySum also closely follows ReplacementGreedy. Fig. 3b, 3c, 3d show the loss versus the running time for ` = 10, ` = 30, and ` = 60. Of course, if we use use the whole ground set Ω, the loss will be zero. This is shown by GreedyMerge. However, this comes at the cost of maximizing the utility of each user on 2000 movies. Instead, by using ReplacementGreedy, we see that the loss is negligible (even for ` = 30) while the running time suddenly improves by two orders of magnitude.\nComplete matrix movie recommendation. The previous experiment suffers from the potential issue that values are estimated conservatively: i.e., a user derives value only if we happen to select movies that she actually rated in the data. To explore this potential bias, we repeat the same experiment, this time by first completing the matrix of (movie, rating) using standard techniques (Candés & Recht, 2008). We again consider 200 users and divide them into training and test sets. The results are shown in Fig. 3e, 3f, 3g, 3h. We observe exactly the same trends. Basically, a dataset with size 60 is approximately as good as a data set of size 2000 if the reduced ground set is carefully selected by ReplacementGreedy."
  }, {
    "heading": "6. Analysis",
    "text": "In this section, we prove that Algorithm ReplacementGreedy returns a valid solution S of at most ` elements along with independent sets Ti for all different categories\nsuch that the aggregate value 1m ∑m i=1 fi(Ti) is at least 1 2 (1 − 1 e2 ) > 0.43 fraction of the optimum solution’s objective value, namely Gm(Sm,`). We note that the objective value of ReplacementGreedy’s solution, Gm(S), is at least 1m ∑m i=1 fi(Ti), and therefore Algorithm ReplacementGreedy is a 12 (1 − 1 e2 )-approximation algorithm for our two stage submodular maximization problem.\nProof of Theorem 3. Since Algorithm ReplacementGreedy runs in ` iterations, and adds an element to S in each iteration, the final output size, |S|, will not be more than `. Each set Ti is initialized with ∅ at the beginning which is an independent set. Also whenever ReplacementGreedy adds an element x∗ to Ti, it removes Repi(x ∗, Ti). By definition, either Repi(x ∗, Ti) is equal to some element y ∈ Ti such that set {x∗}∪Ti \\ {y} is an independent set or Repi(x\n∗, Ti) is the empty set and {x} ∪ Ti is an independent set. In either case, set Ti after the update will remain an independent set. Therefore the output of ReplacementGreedy consists of independent sets Ti ∈ I(S) for every category i. It remains to lower bound the aggregate values of these m sets.\nWe lower bound the aggregate increments of values incurred by adding each element we add to S in terms of the gap between the current objective value 1m ∑m i=1 fi(Ti) and the optimum objective value Gm(Sm,`). This way, we can show that for each of the ` elements added to S, the current objective value is incremented enough that in total we reach at least 12 (1− 1 e2 ) fraction of the optimum value. By definition of∇ and the update operations of Algorithm ReplacementGreedy, addition of element x to S, increases the summation ∑m i=1 fi(Ti) by ∑m i=1∇i(x, Ti). We also\nnote that the selected element x∗ maximizes this aggregate increment. We prove a lower bound benchmark according to the potential increments of values by elements in Sm,` if we add them instead of x∗. In particular, we know that:\nm∑ i=1 ∇i(x∗, Ti) ≥ 1 |Sm,`| ∑ x∈Sm,` m∑ i=1 ∇i(x, Ti)\nThis equation holds because the rightmost side is the average increments of values of optimum elements if we add them instead of x∗, and we know that x∗ is the maximizer of the total value increments. Let Sm,`i be the independent subset of Sm,` with maximum fi value, i.e. Sm,`i = arg maxA∈I(Sm,`) fi(A). Since the ∇ values are all non-negative, we can narrow down the rightmost side of above equation, and imply that:\nm∑ i=1 ∇i(x∗, Ti) ≥ 1 |Sm,`| m∑ i=1 ∑ x∈Sm,`i ∇i(x, Ti) (6)\nWe can apply exchange properties of matroids to lower bound the total ∇ values (the rightmost side) for each category. By Corollary 39.12a of (Schrijver, 2003), we know that there exists a mapping π that maps every element of Sm,`i \\Ti to either the empty set or an element of Ti \\S m,` i such that:\n• for every x ∈ Sm,`i \\ Ti, the set {x} ∪ Ti \\ {π(x)} is an independent set, and\n• for every x1, x2 ∈ Sm,`i \\ Ti, we either have π(x1) 6= π(x2) or both of π(x1) and π(x2) are equal to the empty set.\nWe note that Corollary 39.12a of (Schrijver, 2003) is stated for equal size sets (e.g. |Ti| = |Sm,`i |) which can be easily adapted to non-equal size sets and achieve the above mapping by applying the exchange property of matroids iteratively, and making these two sets equal size. Given the mapping π, the next step is to lower bound each ∇i(x, Ti) by how much we can increase the value of Ti by replacing π(x) with x in set Ti. Since {x} ∪ Ti \\ {π(x)} is an independent set, we have\n∇i(x, Ti) ≥ fi({x} ∪ Ti \\ {π(x)})− fi(Ti) = ∆i(x, Ti)−∆i(π(x), Ti ∪ {x} \\ {π(x)}) ≥ ∆i(x, Ti)−∆i(π(x), Ti \\ {π(x)})\nwhere the equality holds by definition of ∆i values, and the last inequality holds by submodularity of fi. Combining this lower bound on∇ with Equation 6 implies that in each step, adding element x∗ to set S increases the total value of the current solution by at least:\n1\n|Sm,`| m∑ i=1 ∑ x∈Sm,`i \\Ti ∆i(x, Ti)−∆i(π(x), Ti \\ {π(x)})\nIt is well-known (see Lemma 5 of (Bateni et al., 2010)) that submodularity of fi implies ∑ x∈Sm,`i \\Ti\n∆i(x, Ti) ≥ fi(S m,` i )−fi(Ti). We also have ∑ x∈Sm,`i \\Ti\n∆i(π(x), Ti\\ {π(x)}) ≤ ∑ y∈Ti ∆i(y, Ti \\ {y}) because range of mapping π is a subset of Ti, and no two elements are mapped to the same y ∈ Ti. By submodularity of fi, the latter term∑ y∈Ti ∆i(y, Ti \\ {y}) is upper bounded by fi(Ti). We conclude that in each step the total value is increased by at least 1`m ∑m i=1 fi(S m,` i ) − 2f(Ti) (we assume |Sm,`| = ` since objective function Gm is monotone increasing). In other words, in each iteration 1 ≤ t ≤ `, the increment Xt − Xt−1 is at least 1`Gm(S\nm,`) − 2`Xt−1 where Xt is defined to be the total value 1m ∑m i=1 fi(Ti) at the end of iteration t. Solving this recurrence equation inductively yields Xt ≥ 12 (1 − (1 − 1 ` ) 2t)Gm(S m,`). We note that X0 = 0 denotes the total value before the algorithm starts. The induction step is proved as follows:\nXt+1 −Xt ≥ Gm(S\nm,`)\n` − 2Xt `\n=⇒ Xt+1 ≥ Gm(S\nm,`)\n` + (1− 2 ` )Xt\n≥ Gm(S m,`) ` + (1− 2 ` ) 1 2 (1− (1− 1 ` )2t)Gm(S m,`) ≥ 1 2 (1− (1− 1 ` )2t+2)Gm(S m,`)\nAt the end of Algorithm ReplacementGreedy, the total value X` is at least 12 (1 − (1 − 1 ` ) 2`)Gm(S m,`) ≥ 12 (1 −\n1 e2 )Gm(S m,`) which completes the proof."
  }, {
    "heading": "7. Conclusions",
    "text": "In this paper, we have studied the novel problem of sublinear time probabilistic submodular maximization: By investing computational effort once to reduce the ground set based on training instances, faster optimization is achieved on test instances. Our key technical contribution is ReplacementGreedy, a novel algorithm for two-stage submodular maximization (the empirical variant of our problem). Compared to prior approaches, ReplacementGreedy provides constant factor approximation guarantees while applying to general submodular objectives, handling arbitrary matroid constraints and scaling linearly in all relevant parameters.\nAcknowledgments. This work was supported by DARPA Young Faculty Award (D16AP00046), Simons-Berkeley fellowship, and ERC StG 307036. This work was done in part while Amin Karbasi and Andreas Krause were visiting Simons Institute for the Theory of Computing."
  }],
  "year": 2017,
  "references": [{
    "title": "Optimization with demand",
    "authors": ["Ashwinkumar Badanidiyuru", "Shahar Dobzinski", "Sigal Oren"],
    "venue": "oracles. EC,",
    "year": 2012
  }, {
    "title": "Fast algorithms for maximizing submodular functions",
    "authors": ["Badanidiyuru", "Ashwinkumar", "Jan", "Vondrák"],
    "venue": "In SODA,",
    "year": 2014
  }, {
    "title": "Streaming submodular maximization: Massive data summarization on the fly",
    "authors": ["Badanidiyuru", "Ashwinkumar", "Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Krause", "Andreas"],
    "venue": "In ACM KDD,",
    "year": 2014
  }, {
    "title": "Learning sparse combinatorial representations via two-stage submodular maximization",
    "authors": ["Balkanski", "Erik", "Krause", "Andreas", "Mirzasoleiman", "Baharan", "Singer", "Yaron"],
    "venue": "In Proc. International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Submodular secretary problem and extensions. In Approximation, Randomization, and Combinatorial Optimization",
    "authors": ["Bateni", "MohammadHossein", "Hajiaghayi", "MohammadTaghi", "Zadimoghaddam", "Morteza"],
    "venue": "Algorithms and Techniques,",
    "year": 2010
  }, {
    "title": "Submodular maximization with cardinality constrains",
    "authors": ["Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joesph (Seffi", "Schwartz", "Roy"],
    "venue": "In SODA,",
    "year": 2014
  }, {
    "title": "Maximizing a monotone submodular function subject to a matroid constraint",
    "authors": ["Calinescu", "Gruia", "Chekuri", "Chandra", "Pál", "Martin", "Vondrák", "Jan"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Exact matrix completion via convex optimization",
    "authors": ["E. Candés", "B. Recht"],
    "venue": "In Foundations of Computational Mathematics,",
    "year": 2008
  }, {
    "title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection",
    "authors": ["Das", "Abhimanyu", "Kempe", "David"],
    "year": 2011
  }, {
    "title": "Turning down the noise in the blogosphere",
    "authors": ["El-Arini", "Khalid", "Veda", "Gaurav", "Shahaf", "Dafna", "Guestrin", "Carlos"],
    "venue": "In KDD,",
    "year": 2009
  }, {
    "title": "On maximizing welfare when utility functions are subadditive",
    "authors": ["Feige", "Uriel"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Greed is good: Near-optimal submodular maximization via greedy optimization",
    "authors": ["Feldman", "Moran", "Harshaw", "Christopher", "Karbasi", "Amin"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "Online submodular minimization",
    "authors": ["Hazan", "Elad", "Kale", "Satyen"],
    "venue": "In NIPS,",
    "year": 2009
  }, {
    "title": "Online submodular minimization for combinatorial structures",
    "authors": ["Jegelka", "Stefanie", "Bilmes", "Jeff A"],
    "venue": "In International Conference on Machine Learning (ICML), Bellevue, Washington,",
    "year": 2011
  }, {
    "title": "Near-optimal nonmyopic value of information in graphical models",
    "authors": ["A. Krause", "C. Guestrin"],
    "venue": "In UAI,",
    "year": 2005
  }, {
    "title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems",
    "authors": ["Krause", "Andreas", "Golovin", "Daniel"],
    "year": 2012
  }, {
    "title": "Budgeted nonparametric learning from data streams",
    "authors": ["Krause", "Andreas", "Gomes", "Ryan G"],
    "venue": "In ICML,",
    "year": 2010
  }, {
    "title": "Fast greedy algorithms in mapreduce and streaming",
    "authors": ["Kumar", "Ravi", "Moseley", "Benjamin", "Vassilvitskii", "Sergei", "Vattani", "Andrea"],
    "venue": "In SPAA,",
    "year": 2013
  }, {
    "title": "A class of submodular functions for document summarization",
    "authors": ["Lin", "Hui", "Bilmes", "Jeff"],
    "venue": "In ACL,",
    "year": 2011
  }, {
    "title": "Learning mixtures of submodular shells with application to document summarization",
    "authors": ["Lin", "Hui", "Bilmes", "Jeff"],
    "venue": "In Uncertainty in Artificial Intelligence (UAI), Catalina Island,",
    "year": 2012
  }, {
    "title": "Accelerated greedy algorithms for maximizing submodular set functions",
    "authors": ["Minoux", "Michel"],
    "venue": "In Proc. of the 8th IFIP Conference on Optimization Techniques. Springer,",
    "year": 1978
  }, {
    "title": "Distributed submodular maximization: Identifying representative elements in massive data",
    "authors": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Lazier than lazy greedy",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondrak", "Jan", "Krause", "Andreas"],
    "venue": "In AAAI,",
    "year": 2015
  }, {
    "title": "Fast constrained submodular maximization: Personalized data summarization",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Distributed submodular maximization",
    "authors": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Distributed submodular maximization",
    "authors": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2016
  }, {
    "title": "Fast distributed submodular cover: Public-private data summarization",
    "authors": ["Mirzasoleiman", "Baharan", "Zadimoghaddam", "Morteza", "Karbasi", "Amin"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions - I",
    "authors": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "Combinatorial optimization-polyhedra and efficiency",
    "authors": ["Schrijver", "Lex"],
    "venue": "Algorithms and Combinatorics,",
    "year": 2003
  }, {
    "title": "Approximation algorithms for combinatorial auctions with complement-free bidders",
    "authors": ["Shahar Dobzinski", "Noam Nisan", "Schapira", "Michael"],
    "year": 2005
  }, {
    "title": "Budget feasible mechanisms",
    "authors": ["Singer", "Yaron"],
    "year": 2010
  }, {
    "title": "An online algorithm for maximizing submodular functions",
    "authors": ["Streeter", "Matthew", "Golovin", "Daniel"],
    "venue": "In NIPS,",
    "year": 2008
  }, {
    "title": "Learning Mixtures of Submodular Functions for Image Collection Summarization",
    "authors": ["Tschiatschek", "Sebastian", "Iyer", "Rishabh", "Wei", "Haochen", "Bilmes", "Jeff"],
    "venue": "In Neural Information Processing Systems (NIPS),",
    "year": 2014
  }, {
    "title": "Using document summarization techniques for speech data subset selection",
    "authors": ["Wei", "Kai", "Liu", "Yuzong", "Kirchhoff", "Katrin", "Bilmes", "Jeff"],
    "venue": "In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2013
  }, {
    "title": "Fast multi-stage submodular maximization",
    "authors": ["Wei", "Kai", "Iyer", "Rishabh", "Bilmes", "Jeff"],
    "year": 2014
  }, {
    "title": "Submodular subset selection for largescale speech training data",
    "authors": ["Wei", "Kai", "Liu", "Yuzong", "Kirchhoff", "Katrin", "Bartels", "Chris", "Bilmes", "Jeff"],
    "venue": "In ICASSP,",
    "year": 2014
  }, {
    "title": "Linear submodular bandits and their application to diversified retrieval",
    "authors": ["Yue", "Yisong", "Guestrin", "Carlos"],
    "year": 2011
  }],
  "id": "SP:f4d3fd996c837553361fbb4c8aac23975d9594f9",
  "authors": [{
    "name": "Serban Stan",
    "affiliations": []
  }, {
    "name": "Morteza Zadimoghaddam",
    "affiliations": []
  }, {
    "name": "Andreas Krause",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.g., via user feature vectors), we seek to reduce the ground set so that optimizing new functions drawn from the same distribution will provide almost as much value when restricted to the reduced ground set as when using the full set. We cast this problem as a two-stage submodular maximization and develop a novel efficient algorithm for this problem which offers a 1 2 (1− 1 e2 ) approximation ratio for general monotone submodular functions and general matroid constraints. We demonstrate the effectiveness of our approach on several real-world applications where running the maximization problem on the reduced ground set leads to two orders of magnitude speed-up while incurring almost no loss.",
  "title": "Probabilistic Submodular Maximization in Sub-Linear Time"
}