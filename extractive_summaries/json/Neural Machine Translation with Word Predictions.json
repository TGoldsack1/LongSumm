{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 136–145 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly. Sutskever et al. (2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al., 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014). In this framework, the fixedlength vector plays the crucial role of transition-\ning the information of the sentence from the source side to the target side. Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state. Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector. On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected. We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures. Due to the large number of parameters, it may be difficult for the NMT system to learn the proper sentence representation as the initial state. Thus, themodel is very likely to get stuck in local minimums, making the translation process arbitrary and unstable. In this paper, we propose to augment the current NMT architecture with a word prediction mechanism. More specifically, we require the initial state of the decoder to be able to predict all the words in the target sentence. In this way, there is a specific objective for learning the initial state. Thus the learnt source side representation will be better constrained. We further extend this idea by applying the word predictions mechanism to all the hidden states of the decoder. So the transition between different decoder states could be controlled\n136\nas well. Our mechanism is simple and requires no additional data or annotation. The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding. Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality."
  }, {
    "heading": "2 Related Work",
    "text": "Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source sidemonolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information. However, the purpose and the way of our mechanism are different from them. The word prediction technique has been applied in the research of both statistical machine transla-\ntion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training."
  }, {
    "heading": "3 Notations and Backgrounds",
    "text": "We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism. Denote a source-target sentence pair as {x, y} from the training set, where x is the source word sequence (x1, x2, · · · , x|x|) and y is the target word sequence (y1, y2, · · · , y|y|), |x| and |y| are the length of x and y, respectively. In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1,h2, · · · ,h|x|). For each xi, the representation hi is:\nhi = [ −→hi ;←−hi ] (1)\nwhere [·; ·] denotes the concatenation of column vectors;\n−→hi and ←−hi denote the hidden vectors for the word xi in the forward and backward RNNs, respectively. The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014). Formally, the hidden state hi at time step i of the forward RNN encoder is defined by the GRU function g−→e (·, ·), as follows:\n−→h i = g−→e ( −→h i−1, embxi) (2)\n= (1−−→z i)⊙−→h i−1 +−→z i ⊙ −→ h′ i\n−→z i = σ(−→Wz[embxi ; −→h i−1]) (3) −→ h′ i = tanh( −→W[embxi ; (−→r i ⊙ −→h i−1)]) (4) −→r i = σ(−→Wr[embxi ; −→h i−1]) (5)\nwhere ⊙ denotes element-wise product between vectors and embxi is the word embedding of the xi. tanh(·) and σ(·) are the tanh and sigmoid transformation functions that can be applied elementwise on vectors, respectively. For simplicity, we\nomit the bias term in each network layer. The backward RNN encoder is defined likewise. In the decoding stage, the decoder starts with the initial state s0, which is the average of source representations (Bahdanau et al., 2014).\ns0 = σ(Ws 1 |x| |x|∑ i=1 hi) (6)\nAt each time step j, the decoder maximizes the conditional probability of generating the jth target word, which is defined as:\nP (yj |y<j , x) = fd(td([embyj−1 ; sj ; cj ])) (7) fd(u) = softmax(Wfu) (8) td(v) = tanh(Wtv) (9)\nwhere sj is the decoder’s hidden state, which is computed by another GRU (as in Equation 2):\nsj = gd(sj−1, [embyj−1 ; cj ]) (10)\nand the context vector cj is from the attention mechanism (Luong et al., 2015b):\ncj = |x|∑ i=1 ajihi (11)\naji = exp(eji)∑|x|\nk=1 exp(ejk) (12)\neji = tanh(Wattd [sj−1;hi]). (13)"
  }, {
    "heading": "4 NMT with Word Predictions",
    "text": ""
  }, {
    "heading": "4.1 Word Prediction for the Initial State",
    "text": "The decoder starts the generation of target sentence from the initial state s0 (Equation 6) generated by the encoder. Currently, the update for the encoder\nonly happens when a translation error occurs in the decoder. The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder. As there are hundreds of millions of parameters in the NMT system, it is hard for the model to learn the exact representation of source sentences. As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances. We propose word prediction as a mechanism to control the values of initial state. The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the target sentence. Thus, we optimize the initial state by making prediction for all target words. For simplicity, we assume each target word is independent of each other. Here the word prediction mechanism is a simpler sub-task of translation, where the order of words is not considered. The prediction task could be trained jointly with the translation task in a multi-task learningway (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder. In other words, word prediction for the initial state could be interpreted as an improvement for the encoder. We denote this mechanism as WPE . As shown in Figure 1, a prediction network is added to the initial state. We define the conditional probability of WPE as follows:\nPWPE(y|x) = |y|∏\nj=1\nPWPE(yj |x) (14)\nPWPE(yj |x) = fp(tp([s0; cp])) (15) where fp(·) and tp(·) are the softmax layer and non-linear layer as defined in Equation 8-9, with different parameters; cp is defined similar as the\nattention network, so the source side information could be enhanced.\ncp = |x|∑ i=1 aihi (16) ai = exp(ei)∑|x|\nk=1 exp(ek) (17)\nei = tanh(Wattp [s0,hi]). (18)"
  }, {
    "heading": "4.2 Word Predictions for Decoder’s Hidden States",
    "text": "Similar intuition is also applied for the decoder. Because the hidden states of the decoder are responsible for the translation of target words, they should be able to predict the target words as well. The only difference is that we remove the already generated words from the prediction task. So each hidden state in the decoder is required to predict the target words which remain untranslated. For the first state s1 of the decoder, the prediction task is similar with the task for the initial state. Since then, the prediction is no longer a separate training task, but integrated into each time step of the training process. We denote this mechanism as WPD. As shown in Figure 2, for each time step j in the decoder, the hidden state sj is used for the prediction of (yj , yj+1, · · · , y|y|). The conditional probability of WPD is defined as:\nPWPD(yj , yj+1, · · · , y|y||y<j , x) (19)\n= |y|∏\nk=j\nPWPD(yk|y<j , x)\nPWPD(yk|y<j , x) =fd(p(td([embyj−1 ; sj ; cj ]))) (20)\nwhere fd(·) and td(·) are the softmax layer and non-linear layer as defined in Equation 8-9; p(·)\nis another non-linear transformation layer, which prepares the current state for the prediction:\np(u) = tanh(Wpu). (21)"
  }, {
    "heading": "4.3 Training",
    "text": "NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x, denoted by LT.\nLT = 1 |y| |y|∑ j=1 logP (yj |y<j , x) (22)\nwhere P (yj |y<j , x) is defined in Equation 7. To optimize the word prediction mechanism, we propose to add extra likelihood functionsLWPE and LWPD into the training procedure. For the WPE, we directly optimize the likelihood of translation and word prediction:\nL1 = LT + LWPE (23) LWPE = logPWPE (24)\nwhere PWPE is defined in Equation 14. For the WPD, we optimize the likelihood as:\nL2 = LT + LWPD (25)\nLWPD = |y|∑\nj=1\n1 |y| − j + 1 logPWPD (26)\nwhere PWPD is defined in Equation 19; the coefficient of the logarithm is used to calculate the average probability of each prediction. The two mechanisms could also work together, so that both the encoder and the decoder could be improved:\nL3 = LT + LWPE + LWPD . (27)"
  }, {
    "heading": "4.4 Making Use of the Word Predictor",
    "text": "The previously proposed word prediction mechanism could be used only as a extra training objective, which will not be computed during the translation. Thus the computational complexity of our models for translation stays exactly the same. On the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency. If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016). Our word prediction mechanismWPE provides a natural solution for generating a possible set of target words at sentence level. The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016)."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Data",
    "text": "We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks. For the CH-EN, the training data consists of about 8million sentence pairs 1. We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets. These sets have 878, 919, 1597 and 1082 source sentences, respectively, with 4 references for each sentence. For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs. We use newstest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set. These sets have 3000 and 2737 source sentences, respectively, with 1 reference for each sentence. Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017)."
  }, {
    "heading": "5.2 Systems and Techniques",
    "text": "We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, denoted as baseNMT. Then our proposed word prediction mechanism on initial state and hidden states of decoder are implemented on the baseNMT system, denoted as WPE and WPD, respectively. We denote the system\n1includes LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, LDC2005T10, LDC2006E26 and LDC2007T09\nuse both techniques as WPED. We implement systemswith variable-sized vocabulary following (Mi et al., 2016). For comparison, we also implement systems with dropout (with dropout rate 0.5 on the output layer) and ensemble (ensemble of 4 systems at the output layer) techniques."
  }, {
    "heading": "5.3 Implementation Details",
    "text": "Both our CH-EN and DE-EN experiments are implemented on the open source toolkit dl4mt 2, with most default parameter settings kept the same. We train the NMT systemswith the sentences of length up to 50 words. The source and target vocabularies are limited to the most frequent 30K words for both Chinese and English, respectively, with the out-of-vocabulary words mapped to a special token UNK. The dimension of word embedding is set to 512 and the size of the hidden layer is 1024. The recurrent weight matrices are initialized as random orthogonal matrices, and all the bias vectors as zero. Other parameters are initialized by sampling from the Gaussian distribution N (0, 0.01). We use the mini-batch stochastic gradient descent (SGD) approach to update the parameters, with a batch size of 32. The learning rate is controlled by AdaDelta (Zeiler, 2012). For efficient training of our system, we adopt a simple pre-train strategy. Firstly, the baseNMT system is trained. The training results are used as the initial parameters for pre-training our proposed models with word predictions. For decoding during test time, we simply decode until the end-of-sentence symbol eos occurs, using a beam search with a beam width of 5."
  }, {
    "heading": "5.4 Translation Experiments",
    "text": "To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CH-EN and DE-EN tasks. The detailed results are show in the Table 1 and Table 2. Compared to the baseNMT system, all of our models achieve significant improvements. On the CH-EN experiments, simply adding word predictions to the initial state (WPE) already brings considerable improvements. The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality. Adding word predic-\n2https://github.com/nyu-dl/dl4mt-tutorial\ntions to the hidden states in the decoder (WPD) leads to further improvements against baseNMT (4.15 BLEU), because WPD adds constraints to the state transitions through different time steps in the decoder. Using both techniques improves the baseline by 4.53 BLEU. On the DE-EN experiments, the improvement of WPE model is 0.41 BLEU and WPD model is 0.86 BLEU on test set. When use both techniques, the WPED improves on the test set is 1.3 BLEU. We compare our models with systems using dropout and ensemble techniques. The results show in Table 3 and 4. On the CH-EN experiments, the dropout method successfully improves the baseNMT system by 2.06 BLEU. However, it does not work on our WPED system. The ensemble technique improves the baseNMT system by 2.75 BLEU. It still improves WPED by 1.26\nBLEU, but the improvement is smaller than on the baseNMT. On the DE-EN experiments, the phenomenon of experiments is similar to CH-EN experiments. The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method. The dropout technique also does not work on WPED and the ensemble technique improves 1.79 BLEU. These comparisons suggests that our system already learns better and stable values for the parameters, enjoying some of the benefits of general training techniques like dropout and ensemble. Compared to dropout and ensemble, our method WPED achieves the highest improvement against the baseline system on both CH-EN and DE-EN experiments. Along with ensemble method, the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively."
  }, {
    "heading": "5.5 Word Prediction Experiments",
    "text": "Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved. For each sentence in the test set, we use the initial state of the given model to make prediction about the possible words. We denote the set of top nwords as Tn, the set of words in all the references\nas R. We define the precision, recall of the word prediction as follows:\nprecision = |Tn ∩R| |Tn| ∗ 100% (28)\nrecall = |Tn ∩R| |R| ∗ 100% (29)\nWe compare the prediction performance of baseNMT and WPE. WPED has similar prediction results withWPE, so we omit its results. As shown in Table 5, baseNMT system has a relatively lower prediction precision, for example, 45% in top 10 prediction. With an explicit training, the WPE could achieve a much higher precision in all conditions. Specifically, the precision reaches 73% in top 10. This indicates that the initial state in WPE contains more specific information about the prediction of the target words, which may be a step towards better semantic representation, and leads to better translation quality. Because the total words in the references are limited (around 50), the precision goes down, as expected, when a larger prediction set is considered. On the other hand, the recall of WPE is also much higher than baseNMT. When given 1k predictions, WPE could successfully predict 89% of the words in the reference. The recall goes up to 95% with 5k predictions, which is only 1/6 of the current vocabulary. To analyze the process of word prediction, we draw the attention heatmap (Equation 16) between the initial state s0 and the bi-directional representation of each source side word hi for an example sentence. As shown in Figure 3, both examples show that the initial states have a very strong attention with all the content words in the source sentence. The blank cells are mostly functions words\nor high frequent tokens such as “的 (’s)”, “是 (is)”, “而 (and)”, “它 (it)”, comma and period. This indicates that the initial state successfully encodes information about most of the content words in the source sentence, which contributes for a high prediction performance and leads to better translation."
  }, {
    "heading": "5.6 Improving Decoding Efficiency",
    "text": "To make use of the word prediction, we conduct experiments using the predicted vocabulary, with different vocabulary size (1k to 10k) on the CHEN experiments, denoted as WPE-V and WPED-V. The comparison is made in both translation quality and decoding time. As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WPED for comparison. Figure 4 and 5 show the results. When we start the experiments with top 1k vocabulary (1/30 of the baseline settings), the translation quality of both WPE-V and WPED-V are already higher than the baseNMT; while their decoding time is less than 1/3 of an NMT system with 30k vocabulary. When the size of vocabulary increases, the translation quality improves as well. With a 6k predicted vocabulary (1/5 of the baseline settings), the decoding time is about 60% of a full-\nvocabulary system; the performances of both systems with variable size vocabulary are comparable their corresponding fixed-vocabulary systems, which is higher than the baseNMT by 2.53 and 4.53 BLEU, respectively. Although the comparison may not be fair enough due to the language pair and training conditions, the above relative improvements (e.g. WPED-V v.s. baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016). This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch, it also brings improvement to the overall translation model. Please note that un-\nlike these research, we keep the target vocabulary to be 30k in all our experiments, becausewe are not focusing on increasing the vocabulary size in this paper. It will be interesting to combine our mechanism with larger vocabulary to further enhance the translation performance. Again, our mechanism requires no extra annotation, dictionary, alignment or separate discriminative predictor, etc."
  }, {
    "heading": "5.7 Translation Analysis",
    "text": "We also analyze real-case translations to see the difference between different systems (Table 6). It is easy to see that the baseNMT systemmisses the translations of several important words, such as “advertising”, “1.5”, which are marked with underline in the reference. It also wrongly translates the company name “time warner inc.” as the redundant information “internet company”; “america online” as “us line”. The results of dropout or ensemble show improvement compared to the baseNMT. But they still make mistakes about the translation of “online” and the company name “time warner inc.”. WithWPED, most of these errors no longer exist, because we force the encoder and decoder to carry the exact information during translation."
  }, {
    "heading": "6 Conclusions",
    "text": "The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language. However, due to the large amount of parameters and relatively small training data set, the end-toend learning of an NMT model may not be able to learn the best solution. We argue that at least part of the problem is caused by the long error backpropagation pipeline of the recurrent structures in multiple time steps, which provides no direct control of the information carried by the hidden states in both the encoder and decoder. Instead of looking for other annotated data, we notice that the words in the target language sentence could be viewed as a natural annotation. We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well. Experiments show promising results on the Chinese-English and German-English translation tasks. As a byproduct, the word predictor could be used to improve the efficiency of decoding, which may be\ncrucial for large scale applications. Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough. In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process. It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their insightful comments. Shujian Huang is the corresponding author. This work is supported by the National Science Foundation of China (No. 61672277, 61472183), the Jiangsu Provincial Research Foundation for Basic Research (No. BK20170074)."
  }],
  "year": 2017,
  "references": [{
    "title": "Massive Exploration of Neural Machine Translation Architectures",
    "authors": ["Denny Britz", "Anna Goldie", "Minh-Thang Luong", "Quoc Le."],
    "venue": "ArXiv e-prints .",
    "year": 2017
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "CoRR abs/1412.3555. http://arxiv.org/abs/1412.3555.",
    "year": 2014
  }, {
    "title": "Multi-task learning for multiple language translation",
    "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-",
    "year": 2015
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }, {
    "title": "A discriminative lexicon model for complex morphology",
    "authors": ["Minwoo Jeong", "Kristina Toutanova", "Hisami Suzuki", "Chris Quirk."],
    "venue": "Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010).",
    "year": 2010
  }, {
    "title": "Vocabulary selection strategies for neural machine translation",
    "authors": ["Gurvan L’Hostis", "David Grangier", "Michael Auli"],
    "venue": "CoRR abs/1610.00072",
    "year": 2016
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "CoRR abs/1511.06114. http://arxiv.org/abs/1511.06114.",
    "year": 2015
  }, {
    "title": "Achieving open vocabulary neural machine translation with hybrid wordcharacter models",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "CoRR abs/1604.00788. http://arxiv.org/abs/1604.00788.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association",
    "year": 2015
  }, {
    "title": "Extending statistical machine translation with discriminative and trigger-based lexicon models",
    "authors": ["Arne Mauser", "Saša Hasan", "Hermann Ney."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2009
  }, {
    "title": "Interactive attention for neural machine translation",
    "authors": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu."],
    "venue": "CoRR abs/1610.05011. http://arxiv.org/abs/1610.05011.",
    "year": 2016
  }, {
    "title": "Vocabulary manipulation for neural machine translation",
    "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."],
    "venue": "CoRR abs/1605.03209. http://arxiv.org/abs/1605.03209.",
    "year": 2016
  }, {
    "title": "Bleu: A method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.",
    "year": 2002
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "J. Mach. Learn. Res. 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-",
    "year": 2014
  }, {
    "title": "Rnnbased encoder-decoder approach with word frequency estimation",
    "authors": ["Jun Suzuki", "Masaaki Nagata."],
    "venue": "CoRR abs/1701.00138. http://arxiv.org/abs/1701.00138.",
    "year": 2017
  }, {
    "title": "Word translation prediction for morphologically rich languages with bilingual neural networks",
    "authors": ["Ke Tran", "Arianna Bisazza", "Christof Monz."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "ADADELTA: an adaptive learning rate method",
    "authors": ["Matthew D. Zeiler."],
    "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.",
    "year": 2012
  }, {
    "title": "Exploiting source-side monolingual data in neural machine translation",
    "authors": ["Jiajun Zhang", "Chengqing Zong."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for",
    "year": 2016
  }],
  "id": "SP:b317ce95a7db3b0c7561f29bc3fa45af9ef2f58e",
  "authors": [{
    "name": "Rongxiang Weng",
    "affiliations": []
  }, {
    "name": "Shujian Huang",
    "affiliations": []
  }, {
    "name": "Zaixiang Zheng",
    "affiliations": []
  }, {
    "name": "Xinyu Dai",
    "affiliations": []
  }, {
    "name": "Jiajun Chen",
    "affiliations": []
  }],
  "abstractText": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use word predictions as a mechanism for direct supervision. More specifically, we require these vectors to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.",
  "title": "Neural Machine Translation with Word Predictions"
}