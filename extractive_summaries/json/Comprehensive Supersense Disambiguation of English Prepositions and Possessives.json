{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 185–196 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n185"
  }, {
    "heading": "1 Introduction",
    "text": "Grammar, as per a common metaphor, gives speakers of a language a shared toolbox to construct and deconstruct meaningful and fluent utterances. Being highly analytic, English relies heavily on word order and closed-class function words like prepositions, determiners, and conjunctions. Though function words bear little semantic content, they are nevertheless crucial to the meaning. Consider prepositions: they serve, for example, to convey place and time (We met at/in/outside the restaurant for/after an hour), to express configurational relationships like quantity, possession, part/whole, and membership (the coats of dozens of children in the class), and to indicate semantic roles in argument structure (Grandma cooked dinner for the children\n∗nathan.schneider@georgetown.edu\nvs. Grandma cooked the children for dinner). Frequent prepositions like for are maddeningly polysemous, their interpretation depending especially on the object of the preposition—I rode the bus for 5 dollars/minutes—and the governor of the prepositional phrase (PP): I Ubered/asked for $5. Possessives are similarly ambiguous: Whistler’s mother/painting/hat/death. Semantic interpretation requires some form of sense disambiguation, but arriving at a linguistic representation that is flexible enough to generalize across usages and types, yet simple enough to support reliable annotation, has been a daunting challenge (§2).\nThis work represents a new attempt to strike that balance. Building on prior work, we argue for an approach to describing English preposition and possessive semantics with broad coverage. Given the semantic overlap between prepositions and possessives (the hood of the car vs. the car’s hood or its hood), we analyze them using the same inventory of semantic labels.1 Our contributions include:\n• a new hierarchical inventory (“SNACS”) of 50 supersense classes, extensively documented in guidelines for English (§3); • a gold-standard corpus with comprehensive annotations: all types and tokens of prepositions and possessives are disambiguated (§4; example sentences appear in figure 1); • an interannotator agreement study that\n1Some uses of certain other closed-class markers— intransitive particles, subordinators, infinitive to—are also included (§3.1).\nshows the scheme is reliable and generalizes across genres—and for the first time demonstrating empirically that the lexical semantics of a preposition can sometimes be detached from the PP’s semantic role (§5); • disambiguation experiments with two supervised classification architectures to establish the difficulty of the task (§6)."
  }, {
    "heading": "2 Background: Disambiguation of Prepositions and Possessives",
    "text": "Studies of preposition semantics in linguistics and cognitive science have generally focused on the domains of space and time (e.g., Herskovits, 1986; Bowerman and Choi, 2001; Regier, 1996; Khetarpal et al., 2009; Xu and Kemp, 2010; Zwarts and Winter, 2000) or on motivated polysemy structures that cover additional meanings beyond core spatial senses (Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; Lindstromberg, 2010). Possessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015).\nCorpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general-\n2Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012).\nize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitations of our approach became clear to us over time.\nFirst, as pointed out by Hwang et al. (2017), the one-label-per-token assumption in STREUSLE is flawed because it in some cases puts into conflict the semantic role of the PP with respect to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotation study or release a corpus to establish its feasibility empirically. We address that gap here.\nSecond, 75 categories is an unwieldy number for both annotators and disambiguation systems. Some are quite specialized and extremely rare in STREUSLE 3.0, which causes data sparseness issues for supervised learning. In fact, the only published disambiguation system for preposition supersenses collapsed the distinctions to just 12 labels (Gonen and Goldberg, 2016). Hwang et al. (2017) remarked that solving the aforementioned problem could remove the need for many of the specialized categories and make the taxonomy more tractable for annotators and systems. We substantiate this here, defining a new hierarchy with just 50 categories (SNACS, §3) and providing disambiguation results for the full set of distinctions.\nFinally, given the semantic overlap of possessive case and the preposition of, we saw an opportunity to broaden the application of the scheme to include possessives. Our reannotated corpus, STREUSLE 4.0, thus has supersense annotations for over 1000 possessive tokens that were not semantically annotated in version 3.0. We include these in our annotation and disambiguation experiments alongside reannotated preposition tokens."
  }, {
    "heading": "3 Annotation Scheme",
    "text": ""
  }, {
    "heading": "3.1 Lexical Categories of Interest",
    "text": "Apart from canonical prepositions and possessives, there are many lexically and semantically overlap-\n3https://github.com/nert-gu/streusle/\nping closed-class items which are sometimes classified as other parts of speech, such as adverbs, particles, and subordinating conjunctions. The Cambridge Grammar of the English Language (Huddleston and Pullum, 2002) argues for an expansive definition of ‘preposition’ that would encompass these other categories. As a practical measure, we decided to encourage annotators to focus on the semantics of these functional items rather than their syntax, so we take an inclusive stance.\nAnother consideration is developing annotation guidelines that can be adapted for other languages. This includes languages which have postpositions, circumpositions, or inpositions rather than prepositions; the general term for such items is adpositions.4 English possessive marking (via ’s or possessive pronouns like my) is more generally an example of case marking. Note that prepositions (4a–4c) differ in word order from possessives (4d), though semantically the object of the preposition and the possessive nominal pattern together:\n(4) a. eat in a restaurant b. the man in a blue shirt c. the wife of the ambassador d. the ambassador’s wife\nCross-linguistically, adpositions and case marking are closely related, and in general both grammatical strategies can express similar kinds of semantic relations. This motivates a common semantic inventory for adpositions and case.\nWe also cover multiword prepositions (e.g., out_of, in_front_of), intransitive particles (He flew away), purpose infinitive clauses (Open the door to let in some air5), prepositions with clausal complements (It rained before the party started), and idiomatic prepositional phrases (at_large). Our annotation guidelines give further details."
  }, {
    "heading": "3.2 The SNACS Hierarchy",
    "text": "The hierarchy of preposition and possessive supersenses, which we call Semantic Network of Adposition and Case Supersenses (SNACS), is shown in figure 2. It is simpler than its predecessor— Schneider et al.’s (2016) preposition supersense hierarchy—in both size and structural complexity.\n4In English, ago is arguably a postposition because it follows rather than precedes its complement: five minutes ago, not *ago five minutes.\n5 To can be rephrased as in_order_to and have prepositional\ncounterparts like in Open the door for some air.\nCircumstance 77\nTemporal 0\nTime 371\nStartTime 28 EndTime 31\nFrequency 9\nDuration 91 Interval 35\nLocus 846\nSource 189 Goal 419\nPath 49\nDirection 161 Extent 42\nMeans 17 Manner 140 Explanation 123\nPurpose 401\nParticipant 0\nCauser 15\nAgent 170\nCo-Agent 65\nTheme 238\nCo-Theme 14 Topic 296\nStimulus 123 Experiencer 107\nOriginator 134\nRecipient 122\nCost 48 Beneficiary 110\nInstrument 30\nConfiguration 0\nIdentity 85\nSpecies 39\nGestalt 709\nPossessor 492 Whole 250 Characteristic 140\nPossession 21 PartPortion 57\nStuff 25\nAccompanier 49\nInsteadOf 10 ComparisonRef 215\nRateUnit 5 Quantity 191\nApproximator 76\nSocialRel 240\nOrgRole 103\nFigure 2: SNACS hierarchy of 50 supersenses and their token counts in the annotated corpus described in §4. Counts are of direct uses of labels, excluding uses of subcategories. Role and function positions are not distinguished (so if a token has different role and function labels, it will count toward two supersense frequencies).\nSNACS has 50 supersenses at 4 levels of depth; the previous hierarchy had 75 supersenses at 7 levels. The top-level categories are the same:\n• CIRCUMSTANCE: Circumstantial information, usually non-core properties of events (e.g., location, time, means, purpose) • PARTICIPANT: Entity playing a role in an event • CONFIGURATION: Thing, usually an entity or property, involved in a static relationship to some other entity The 3 subtrees loosely parallel adverbial adjuncts, event arguments, and adnominal complements, respectively. The PARTICIPANT and CIRCUMSTANCE subtrees primarily reflect semantic relationships prototypical to verbal arguments/adjuncts and were inspired by VerbNet’s thematic role hierarchy (Palmer et al., 2017; Bonial et al., 2011). Many CIRCUMSTANCE subtypes, like LOCUS (the concrete or abstract location of something), can be governed by eventive and non-eventive nominals as well as verbs: eat in the restaurant, a party in the restaurant, a table in the restaurant. CONFIGURATION mainly encompasses non-spatiotemporal relations holding between entities, such as quantity, possession, and part/whole. Unlike the previous hierarchy, SNACS does not use multiple inheritance, so there is no overlap between the 3 regions.\nThe supersenses can be understood as roles in fundamental types of scenes (or schemas) such as: LOCATION—THEME is located at LO-\nCUS; MOTION—THEME moves from SOURCE along PATH to GOAL; TRANSITIVE ACTION— AGENT acts on THEME, perhaps using an INSTRUMENT; POSSESSION—POSSESSION belongs to POSSESSOR; TRANSFER—THEME changes possession from ORIGINATOR to RECIPIENT, perhaps with COST; PERCEPTION—EXPERIENCER is mentally affected by STIMULUS; COGNITION— EXPERIENCER contemplates TOPIC; COMMUNICATION—information (TOPIC) flows from ORIGINATOR to RECIPIENT, perhaps via an INSTRUMENT. For AGENT, CO-AGENT, EXPERIENCER, ORIGINATOR, RECIPIENT, BENEFICIARY, POSSESSOR, and SOCIALREL, the object of the preposition is prototypically animate.\nBecause prepositions and possessives cover a vast swath of semantic space, limiting ourselves to 50 categories means we need to address a great many nonprototypical, borderline, and special cases. We have done so in a 75-page annotation manual with over 400 example sentences (Schneider et al., 2018).\nFinally, we note that the Universal Semantic Tagset (Abzianidze and Bos, 2017) defines a crosslinguistic inventory of semantic classes for content and function words. SNACS takes a similar approach to prepositions and possessives, which in Abzianidze and Bos’s (2017) specification are simply tagged REL, which does not disambiguate the nature of the relational meaning. Our categories can thus be understood as refinements to REL."
  }, {
    "heading": "3.3 Adopting the Construal Analysis",
    "text": "Hwang et al. (2017) have pointed out the perils of teasing apart and generalizing preposition semantics so that each use has a clear supersense label. One key challenge they identified is that the preposition itself and the situation as established by the verb may suggest different labels. For instance:\n(5) a. Vernon works at Grunnings. b. Vernon works for Grunnings.\nThe semantics of the scene in (5a, 5b) is the same: it is an employment relationship, and the PP contains the employer. SNACS has the label ORGROLE for this purpose.6 At the same time, at in (5a) strongly suggests a locational relationship, which would correspond to the label LOCUS; consistent with this\n6ORGROLE is defined as “Either a party in a relation between an organization/institution and an individual who has a stable affiliation with that organization, such as membership or a business relationship.”\nhypothesis, Where does Vernon work? is a perfectly good way to ask a question that could be answered by the PP. In this example, then, there is overlap between locational meaning and organizationalbelonging meaning. (5b) is similar except the for suggests a notion of BENEFICIARY: the employee is working on behalf of the employer. Annotators would face a conundrum if forced to pick a single label when multiple ones appear to be relevant. Schneider et al. (2016) handled overlap via multiple inheritance, but entertaining a new label for every possible case of overlap is impractical, as this would result in a proliferation of supersenses.\nInstead, Hwang et al. (2017) suggest a construal analysis in which the lexical semantic contribution, or henceforth the function, of the preposition itself may be distinct from the semantic role or relation mediated by the preposition in a given sentence, called the scene role. The notion of scene role is a widely accepted idea that underpins the use of semantic or thematic roles: semantics licensed by the governor7 of the prepositional phrase dictates its relationship to the prepositional phrase. The innovative claim is that, in addition to a preposition’s relationship with its head, the prepositional choice introduces another layer of meaning or construal that brings additional nuance, creating the difficulty we see in the annotation of (5a, 5b). Construal is notated by ROLE;FUNCTION. Thus, (5a) would be annotated ORGROLE;LOCUS and (5b) as ORGROLE;BENEFICIARY to expose their common truth-semantic meaning but slightly different portrayals owing to the different prepositions.\nAnother useful application of the construal analysis is with the verb put, which can combine with any locative PP to express a destination:\n(6) Put it on/by/behind/on_top_of/. . . the door. GOAL;LOCUS\nI.e., the preposition signals a LOCUS, but the door serves as the GOAL with respect to the scene. This approach also allows for resolution of various se-\n7By “governor” of the preposition or prepositional phrase, we mean the head of the phrase to which the PP attaches in a constituency representation. In a dependency representation, this would be the head of the preposition itself or of the object of the preposition depending on which convention is used for PP headedness: e.g., the preposition heads the PP in CoNLL and Stanford Dependencies whereas the object is the head in Universal Dependencies. The governor is most often a verb or noun. Where the PP is a predicate complement (e.g. Vernon is with Grunnings), there is no governor to specify the nature of the scene, so annotators must rely on world knowledge and context to determine the scene.\nmantic phenomena including perceptual scenes (e.g., I care about education, where about is both the topic of cogitation and perceptual stimulus of caring: STIMULUS;TOPIC), and fictive motion (Talmy, 1996), where static location is described using motion verbiage (as in The road runs through the forest: LOCUS;PATH).\nBoth role and function slots are filled by supersenses from the SNACS hierarchy. Annotators have the option of using distinct supersenses for the role and function; in general it is not a requirement (though we stipulate that certain SNACS supersenses can only be used as the role). When the same label captures both role and function, we do not repeat it: Vernon lives in/LOCUS England. Figure 1 shows some real examples from our corpus.\nWe apply the construal analysis in SNACS annotation of our corpus to test its feasibility. It has proved useful not only for prepositions, but also possessives, where the general sense of possession may overlap with other scene relations, like creator/initial-possessor (ORIGINATOR): Da Vinci’s/ORIGINATOR;POSSESSOR sculptures."
  }, {
    "heading": "4 Annotated Reviews Corpus",
    "text": "We applied the SNACS annotation scheme (§3) to prepositions and possessives in the STREUSLE corpus (§2), a collection of online consumer reviews taken from the English Web Treebank (Bies et al., 2012). The sentences from the English Web Treebank also comprise the primary reference treebank for English Universal Dependencies (UD; Nivre et al., 2016), and we bundle the UD version 2 syntax alongside our annotations. Table 1 shows the total number of tokens present and those that we annotated. Altogether, 5,455 tokens were annotated for scene role and function.\nTable 2 shows the most and least common labels occurring as scene role and function. Three labels never appear in the annotated corpus: TEMPORAL from the CIRCUMSTANCE hierarchy, and PARTICIPANT and CONFIGURATION which are both the highest supersense in their respective hierarchies. While all remaining supersenses are attested as scene roles, there are some that never occur as functions, such as ORIGINATOR, which is most often realized as POSSESSOR or SOURCE, and EXPERIENCER. It is interesting to note that every subtype of CIRCUMSTANCE (except TEMPORAL) appears as both scene role and function, whereas many of the subtypes of the other two hierarchies are lim-\n8Blodgett and Schneider (2018) detail the extension of the scheme to possessives.\n9In the corpus, lexical expression tokens appear alongside a lexical category indicating which inventory of supersenses, if any, applies. SNACS-annotated units are those with ADP (adposition), PP, PRON.POSS (possessive pronoun), etc., whereas DISC (discourse) and CCONJ expressions do not receive any supersense. Refer to the STREUSLE README for details.\nited to either role or function. This reflects our view that prepositions primarily capture circumstantial notions such as space and time, but have been extended to cover other semantic relations.10"
  }, {
    "heading": "5 Interannotator Agreement Study",
    "text": "Because the online reviews corpus was so central to the development of our guidelines, we sought to estimate the reliability of the annotation scheme on a new corpus in a new genre. We chose SaintExupéry’s novella The Little Prince, which is readily available in many languages and has been annotated with semantic representations such as AMR (Banarescu et al., 2013). The genre is markedly different from online reviews—it is quite literary, and employs archaic or poetic figures of speech. It is also a translation from French, contributing to the markedness of the language. This text is therefore a challenge for an annotation scheme based on colloquial contemporary English. We addressed this issue by running 3 practice rounds of annotation on small passages from The Little Prince, both to assess whether the scheme was applicable without major guidelines changes and to prepare the annotators for this genre. For the final annotation study, we chose chapters 4 and 5, in which 242 markables of 52 types were identified heuristically (§6.2). The types of, to, in, as, from, and for, as well as possessives, occurred at least 10 times. Annotators had the option to mark units as false positives using special labels (see §4) in addition to expressing uncertainty about the unit.\nFor the annotation process, we adapted the open source web-based annotation tool UCCAApp (Abend et al., 2017) to our workflow, by extending it with a type-sensitive ranking module for the list of categories presented to the annotators. Annotators. Five annotators (A, B, C, D, E), all authors of this paper, took part in this study. All are computational linguistics researchers with advanced training in linguistics. Their involvement in the development of the scheme falls on a spectrum, with annotator A being the most active figure in guidelines development, and annotator E not being\n10All told, 41 supersenses are attested as both role and function for the same token, and there are 136 unique construal combinations where the role differs from the function. Only four supersenses are never found in such a divergent construal: EXPLANATION, SPECIES, STARTTIME, RATEUNIT. Except for RATEUNIT which occurs only 5 times, their narrow use does not arise because they are rare. EXPLANATION, for example, occurs over 100 times, more than many labels which often appear in construal.\ninvolved in developing the guidelines and learning the scheme solely from reading the manual. Annotators A, B, and C are native speakers of English, while Annotators D and E are nonnative but highly fluent speakers.\nResults. In the Little Prince sample, 40 out of 47 possible supersenses were applied at least once by some annotator; 36 were applied at least once by a majority of annotators; and 33 were applied at least once by all annotators. APPROXIMATOR, COTHEME, COST, INSTEADOF, INTERVAL, RATEUNIT, and SPECIES were not used by any annotator.\nTo evaluate interannotator agreement, we excluded 26 tokens for which at least one annotator has assigned a non-semantic label, considering only the 216 tokens that were identified correctly as SNACS targets and were clear to all annotators. Despite varying exposure to the scheme, there is no obvious relationship between annotators’ backgrounds and their agreement rates.11\nTable 3 shows the interannotator agreement rates, averaged across all pairs of annotators. Average agreement is 74.4% on the scene role and 81.3% on the function (row 1).12 All annotators agree on the role for 119, and on the function for 139 tokens. Agreement is higher on the function slot than on the scene role slot, which implies that the former is an easier task than the latter. This is expected considering the definition of construal: the function of an adposition is more lexical and less contextdependent, whereas the role depends on the context (the scene) and can be highly idiomatic (§3.3).\nThe supersense hierarchy allows us to analyze agreement at different levels of granularity (rows\n11See table 7 in appendix A for a more detailed description of the annotators’ backgrounds and pairwise IAA results.\n12Average of pairwise Cohen’s k is 0.733 and 0.799 on, respectively, role and function, suggesting strong agreement. However, it is worth noting that annotators selected labels from a ranked list, with the ranking determined by preposition type. The model of chance agreement underlying k does not take the identity of the preposition into account, and thus likely underestimates the probability of chance agreement.\n2–4 in table 3; see also confusion matrix in supplement). Coarser-grained analyses naturally give better agreement, with depth-1 coarsening into only 3 categories. Results show that most confusions are local with respect to the hierarchy."
  }, {
    "heading": "6 Disambiguation Systems",
    "text": "We now describe systems that identify and disambiguate SNACS-annotated prepositions and possessives in two steps. Target identification heuristics (§6.2) first determine which tokens (single-word or multiword) should receive a SNACS supersense. A supervised classifier then predicts a supersense analysis for each identified target. The research objectives are (a) to study the ability of statistical models to learn roles and functions of prepositions and possessives, and (b) to compare two different modeling strategies (feature-rich and neural), and the impact of syntactic parsing."
  }, {
    "heading": "6.1 Experimental Setup",
    "text": "Our experiments use the reviews corpus described in §4. We adopt the official training/development/ test splits of the Universal Dependencies (UD) project; their sizes are presented in table 1. All systems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters. Gold tokenization was used throughout. Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation—i.e., tokens with special labels (see §4) were excluded.\nTo test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0 (Manning et al., 2014).13 Named entity tags from the default 12-class CoreNLP model were used in all conditions."
  }, {
    "heading": "6.2 Target Identification",
    "text": "§3.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as\n13The CoreNLP parser was trained on all 5 genres of the English Web Treebank—i.e., a superset of our training set. Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested with UDv1 parses produced by CoreNLP.\nwell as possessive case markers and multiword expressions. 61.2% of the units annotated in our corpus are adpositions according to gold POS annotation, 20.2% are possessives, and 18.6% belong to other POS classes. Furthermore, 14.1% of tokens labeled as adpositions or possessives are not annotated because they are part of a multiword expression (MWE). It is therefore neither obvious nor trivial to decide which tokens and groups of tokens should be selected as targets for SNACS annotation.\nTo facilitate both manual annotation and automatic classification, we developed heuristics for identifying annotation targets. The algorithm first scans the sentence for known multiword expressions, using a blacklist of non-prepositional MWEs that contain preposition tokens (e.g., take_care_of ) and a whitelist of prepositional MWEs (multiword prepositions like out_of and PP idioms like in_town). Both lists were constructed from the training data. From segments unaffected by the MWE heuristics, single-word candidates are identified by matching a high-recall set of parts of speech, then filtered through 5 different heuristics for adpositions, possessives, subordinating conjunctions, adverbs, and infinitivals. Most of these filters are based on lexical lists learned from the training portion of the STREUSLE corpus, but there are some specific rules for infinitivals that handle forsubjects (I opened the door for Steve to take out the trash—to, but not for, should receive a supersense) and comparative constructions with too and enough (too short to ride)."
  }, {
    "heading": "6.3 Classification",
    "text": "The next step of disambiguation is predicting the role and function labels. We explore two different modeling strategies. Feature-rich Model. Our first model is based on the features for preposition relation classification developed by Srikumar and Roth (2013), which were themselves extended from the preposition sense disambiguation features of Hovy et al. (2010). We briefly describe the feature set here, and refer the reader to the original work for further details. At a high level, it consists of features extracted from selected neighboring words in the dependency tree (i.e., heuristically identified governor and object) and in the sentence (previous verb, noun and adjective, and next noun). In addition, all these features are also conjoined with the lemma of the rightmost word in the preposition token to capture\ntarget-specific interactions with the labels. The features extracted from each neighboring word are listed in the supplementary material.\nUsing these features extracted from targets, we trained two multi-class SVM classifiers to predict the role and function labels using the LIBLINEAR library (Fan et al., 2008).\nNeural Model. Our second classifier is a multilayer perceptron (MLP) stacked on top of a BiLSTM. For every sentence, tokens are first embedded using a concatenation of fixed pre-trained word2vec (Mikolov et al., 2013) embeddings of the word and the lemma, and an internal embedding vector, which is updated during training.14 Token embeddings are then fed into a 2-layer BiLSTM encoder, yielding a list of token representations.\nFor each identified target unit u, we extract its first token, and its governor and object headword. For each of these tokens, we construct a feature vector by concatenating its token representation with embeddings of its (1) language-specific POS tag, (2) UD dependency label, and (3) NER label. We additionally concatenate embeddings of u’s lexical category, a syntactic label indicating whether u is predicative/stranded/subordinating/none of these, and an indicator of whether either of the two tokens following the unit is capitalized. All these embeddings, as well as internal token embedding vectors, are considered part of the model parameters and are initialized randomly using the Xavier initialization (Glorot and Bengio, 2010). A NONE label is used when the corresponding feature is not given, both in training and at test time. The concatenated feature vector for u is fed into two separate 2-layered MLPs, followed by a separate softmax layer that yields the predicted probabilities for the role and function labels.\nWe tuned hyperparameters on the development set to maximize F-score (see supplementary material). We used the cross-entropy loss function, optimizing with simple gradient ascent for 80 epochs with minibatches of size 20. Inverted dropout was used during training. The model is implemented with the DyNet library (Neubig et al., 2017).\nThe model architecture is largely comparable to that of Gonen and Goldberg (2016), who experimented with a coarsened version of STREUSLE 3.0. The main difference is their use of unlabeled multilingual datasets to improve pre-\n14Word2vec is pre-trained on the Google News corpus. Zero vectors are used where vectors are not available.\ndiction by exploiting the differences in preposition ambiguities across languages."
  }, {
    "heading": "6.4 Results & Analysis",
    "text": "Following the two-stage disambiguation pipeline (i.e. target identification and classification), we separate the evaluation across the phases. Table 4 reports the precision, recall, and F-score (P/R/F) of the target identification heuristics. Table 5 reports the disambiguation performance of both classifiers with gold (left) and automatic target identification (right). We evaluate each classifier along three dimensions—role and function independently, and full (i.e. both role and function together). When we have the gold targets, we only report accuracy because precision and recall are equal. With automatically identified targets, we report P/R/F for each dimension. Both tables show the impact of syntactic parsing on quality. The rest of this section presents analyses of the results along various axes. Target identification. The identification heuristics described in §6.2 achieve an F1 score of 89.2% on the test set using gold syntax.15 Most false positives (47/54=87%) can be ascribed to tokens that are part of a (non-adpositional or larger adpositional) multiword expression. 9 of the 50 false negatives (18%) are rare multiword expressions not occurring in the training data and there are 7 partially identified ones, which are counted as both false positives and false negatives.\nAutomatically generated parse trees slightly decrease quality (table 4). Target identification, being the first step in the pipeline, imposes an upper bound on disambiguation scores. We observe this degradation when we compare the Gold ID and the Auto ID blocks of table 5, where automatically identified targets decrease F-score by about 10 points in all settings.16\nClassification. Along with the statistical classifier results in table 5, we also report performance\n15Our evaluation script counts tokens that received special labels in the gold standard (see §4) as negative examples of SNACS targets, with the exception of the tokens labeled as unintelligible/nonnative/etc., which are not counted toward or against target ID performance.\n16A variant of the target ID module, optimized for recall, is used as preprocessing for the agreement study discussed in §5. With this setting, the heuristic achieves an F1 score of 90.2% (P=85.3%, R=95.6%) on the test set.\nfor the most frequent baseline, which selects the most frequent role–function label pair given the (gold) lemma according to the training data. Note that all learned classifiers, across all settings, outperform the most frequent baseline for both role and function prediction. The feature-rich and the neural models perform roughly equivalently despite the significantly different modeling strategies.\nFunction and scene role performance. Function prediction is consistently more accurate than role prediction, with roughly a 10-point gap across all systems. This mirrors a similar effect in the interannotator agreement scores (see §5), and may be due to the reduced ambiguity of functions compared to roles (as attested by the baseline’s higher accuracy for functions than roles), and by the more literal nature of function labels, as opposed to role labels that often require more context to determine.\nImpact of automatic syntax. Automatic syntactic analysis decreases scores by 4 to 7 points, most likely due to parsing errors which affect the identification of the preposition’s object and governor. In the auto ID/auto syntax condition, the worse target ID performance with automatic parses (noted above) contributes to lower classification scores."
  }, {
    "heading": "6.5 Errors & Confusions",
    "text": "We can use the structure of the SNACS hierarchy to probe classifier performance. As with the interannotator study, we evaluate the accuracy of predicted labels when they are coarsened post hoc by moving up the hierarchy to a specific depth. Table 6 shows this for the feature-rich classifier for different depths, with depth-1 representing the coarsening of the labels into the 3 root labels. Depth-4 (Exact) represents the full results in table 5. These results show that the classifiers often mistake a label for another that is nearby in the hierarchy. Examining the most frequent confusions of both models, we observe that LOCUS is overpredicted\n(which makes sense as it is most frequent overall), and SOCIALROLE–ORGROLE and GESTALT– POSSESSOR are often confused (they are close in the hierarchy: one inherits from the other)."
  }, {
    "heading": "7 Conclusion",
    "text": "This paper introduced a new approach to comprehensive analysis of the semantics of prepositions and possessives in English, backed by a thoroughly documented hierarchy and annotated corpus. We found good interannotator agreement and provided initial supervised disambiguation results. We expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models. Our guidelines, corpus, and software are available at https://github.com/nert-gu/streusle/ blob/master/ACL2018.md."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Oliver Richardson, whose codebase we adapted for this project; Na-Rae Han, Archna Bhatia, Tim O’Gorman, Ken Litkowski, Bill Croft, and Martha Palmer for helpful discussions and support; and anonymous reviewers for useful feedback. This research was supported in part by DTRA HDTRA116-1-0002/Project #1553695, by DARPA 15-18- CwC-FP-032, and by grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel."
  }],
  "year": 2018,
  "references": [{
    "title": "UCCAApp: Web-application for syntactic and semantic phrase-based annotation",
    "authors": ["Omri Abend", "Shai Yerushalmi", "Ari Rappoport."],
    "venue": "Proc. of ACL 2017, System Demonstrations, pages 109–114, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Towards universal semantic tagging",
    "authors": ["Lasha Abzianidze", "Johan Bos."],
    "venue": "Proc. of IWCS, Montpellier, France.",
    "year": 2017
  }, {
    "title": "A Semantic Scattering model for the automatic interpretation of English genitives",
    "authors": ["Adriana Badulescu", "Dan Moldovan."],
    "venue": "Natural Language Engineering, 15(2):215–239.",
    "year": 2009
  }, {
    "title": "Abstract Meaning Representation for sembanking",
    "authors": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."],
    "venue": "Proc. of the 7th Linguistic An-",
    "year": 2013
  }, {
    "title": "English Web Treebank",
    "authors": ["Ann Bies", "Justin Mott", "Colin Warner", "Seth Kulick."],
    "venue": "Technical Report LDC2012T13, Linguistic Data Consortium, Philadelphia, PA.",
    "year": 2012
  }, {
    "title": "Semantic supersenses for English possessives",
    "authors": ["Austin Blodgett", "Nathan Schneider."],
    "venue": "Proc. of LREC, pages 1529–1534, Miyazaki, Japan.",
    "year": 2018
  }, {
    "title": "A hierarchical unification of LIRICS and VerbNet semantic roles",
    "authors": ["Claire Bonial", "William Corvey", "Martha Palmer", "Volha V. Petukhova", "Harry Bunt."],
    "venue": "Fifth IEEE International Conference on Semantic Computing, pages 483–489, Palo Alto, CA,",
    "year": 2011
  }, {
    "title": "Shaping meanings for language: universal and languagespecific in the acquisition of spatial semantic categories",
    "authors": ["Melissa Bowerman", "Soonja Choi."],
    "venue": "Melissa Bowerman and Stephen Levinson, editors, Language Acquisition and Conceptual De-",
    "year": 2001
  }, {
    "title": "The story of ‘over’: polysemy, semantics and the structure of the lexicon",
    "authors": ["Claudia Brugman."],
    "venue": "MA thesis, University of California, Berkeley, Berkeley, CA. Published New York: Garland, 1981.",
    "year": 1981
  }, {
    "title": "Joint learning of preposition senses and semantic roles of prepositional phrases",
    "authors": ["Daniel Dahlmeier", "Hwee Tou Ng", "Tanja Schultz."],
    "venue": "Proc. of EMNLP, pages 450–458, Suntec, Singapore.",
    "year": 2009
  }, {
    "title": "LIBLINEAR: a library for large linear classification",
    "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."],
    "venue": "Journal of Machine Learning Research, 9(Aug):1871–1874.",
    "year": 2008
  }, {
    "title": "A frames approach to semantic analysis",
    "authors": ["Charles J. Fillmore", "Collin Baker."],
    "venue": "Bernd Heine and Heiko Narrog, editors, The Oxford Handbook of Linguistic Analysis, pages 791–816. Oxford University Press, Oxford, UK.",
    "year": 2009
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proc. of AISTATS, pages 249–256, Chia Laguna, Sardinia, Italy.",
    "year": 2010
  }, {
    "title": "Semi supervised preposition-sense disambiguation using multilingual data",
    "authors": ["Hila Gonen", "Yoav Goldberg."],
    "venue": "Proc. of COLING, pages 2718–2729, Osaka, Japan.",
    "year": 2016
  }, {
    "title": "Possession: Cognitive Sources, Forces, and Grammaticalization",
    "authors": ["Bernd Heine."],
    "venue": "Cambridge University Press, Cambridge, UK.",
    "year": 2006
  }, {
    "title": "Language and spatial cognition: an interdisciplinary study of the prepositions in English",
    "authors": ["Annette Herskovits."],
    "venue": "Cambridge University Press, Cambridge, UK.",
    "year": 1986
  }, {
    "title": "What’s in a preposition? Dimensions of sense disambiguation for an interesting word class",
    "authors": ["Dirk Hovy", "Stephen Tratz", "Eduard Hovy."],
    "venue": "Coling 2010: Posters, pages 454–462, Beijing, China.",
    "year": 2010
  }, {
    "title": "Models and training for unsupervised preposition sense disambiguation",
    "authors": ["Dirk Hovy", "Ashish Vaswani", "Stephen Tratz", "David Chiang", "Eduard Hovy."],
    "venue": "Proc. of ACL-HLT, pages 323–328, Portland, Oregon, USA.",
    "year": 2011
  }, {
    "title": "Double trouble: the problem of construal in semantic annotation of adpositions",
    "authors": ["Jena D. Hwang", "Archna Bhatia", "Na-Rae Han", "Tim O’Gorman", "Vivek Srikumar", "Nathan Schneider"],
    "venue": "In Proc. of *SEM,",
    "year": 2017
  }, {
    "title": "Spatial terms reflect near-optimal spatial categories",
    "authors": ["Naveen Khetarpal", "Asifa Majid", "Terry Regier."],
    "venue": "Proc. of the 31st Annual Conference of the Cognitive Science Society, pages 2396–2401, Amsterdam.",
    "year": 2009
  }, {
    "title": "Women, fire, and dangerous things: what categories reveal about the mind",
    "authors": ["George Lakoff."],
    "venue": "University of Chicago Press, Chicago.",
    "year": 1987
  }, {
    "title": "English Prepositions Explained, revised edition",
    "authors": ["Seth Lindstromberg."],
    "venue": "John Benjamins, Amsterdam.",
    "year": 2010
  }, {
    "title": "Pattern Dictionary of English Prepositions",
    "authors": ["Ken Litkowski."],
    "venue": "Proc. of ACL, pages 1274–1283, Baltimore, Maryland, USA.",
    "year": 2014
  }, {
    "title": "The Preposition Project",
    "authors": ["Ken Litkowski", "Orin Hargraves."],
    "venue": "Proc. of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications, pages 171–179,",
    "year": 2005
  }, {
    "title": "SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions",
    "authors": ["Ken Litkowski", "Orin Hargraves."],
    "venue": "Proc. of SemEval, pages 24–29, Prague, Czech Republic.",
    "year": 2007
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Proc. of ACL: System Demonstrations, pages 55–60, Baltimore, Maryland,",
    "year": 2014
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean."],
    "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-",
    "year": 2013
  }, {
    "title": "Models for the semantic classification of noun phrases",
    "authors": ["Dan Moldovan", "Adriana Badulescu", "Marta Tatu", "Daniel Antohe", "Roxana Girju."],
    "venue": "HLTNAACL 2004: Workshop on Computational Lexical Semantics, pages 60–67, Boston, Massachusetts,",
    "year": 2004
  }, {
    "title": "The annotation of preposition senses in German",
    "authors": ["Antje Müller", "Claudia Roch", "Tobias Stadtfeld", "Tibor Kiss."],
    "venue": "Britta Stolterfoht and Sam Featherston, editors, Empirical Approaches to Linguistic Theory: Studies in Meaning and Structure, pages",
    "year": 2012
  }, {
    "title": "DyNet: The dynamic neural network toolkit",
    "authors": ["Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."],
    "venue": "arXiv:1701.03980.",
    "year": 2017
  }, {
    "title": "The meanings of the genitive: a case study in semantic structure and semantic change",
    "authors": ["Kiki Nikiforidou."],
    "venue": "Cognitive Linguistics, 2(2):149–205.",
    "year": 1991
  }, {
    "title": "Universal Dependencies v1: a multilingual",
    "authors": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajič", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman"],
    "year": 2016
  }, {
    "title": "Towards comparability of linguistic graph banks for semantic parsing",
    "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinkova", "Dan Flickinger", "Jan Hajic", "Angelina Ivanova", "Zdenka Uresova."],
    "venue": "Proc. of LREC, pages 3991–",
    "year": 2016
  }, {
    "title": "Exploiting semantic role resources for preposition disambiguation",
    "authors": ["Tom O’Hara", "Janyce Wiebe"],
    "venue": "Computational Linguistics,",
    "year": 2009
  }, {
    "title": "VerbNet: Capturing English verb behavior, meaning and usage",
    "authors": ["Martha Palmer", "Claire Bonial", "Jena D. Hwang."],
    "venue": "Susan E. F. Chipman, editor, The Oxford Handbook of Cognitive Science, pages 315–336. Oxford University Press.",
    "year": 2017
  }, {
    "title": "The Proposition Bank: an annotated corpus of semantic roles",
    "authors": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."],
    "venue": "Computational Linguistics, 31(1):71–106.",
    "year": 2005
  }, {
    "title": "TimeML: Robust specification of event and temporal expressions in text",
    "authors": ["James Pustejovsky", "José M. Castaño", "Robert Ingria", "Roser Saurí", "Robert J. Gaizauskas", "Andrea Setzer", "Graham Katz", "Dragomir R. Radev."],
    "venue": "IWCS-5, Fifth In-",
    "year": 2003
  }, {
    "title": "A linguistically grounded annotation language for spatial information",
    "authors": ["James Pustejovsky", "Jessica Moszkowicz", "Marc Verhagen."],
    "venue": "TAL, 53(2):87– 113.",
    "year": 2012
  }, {
    "title": "The human semantic potential: spatial language and constrained connectionism",
    "authors": ["Terry Regier."],
    "venue": "MIT Press, Cambridge, MA.",
    "year": 1996
  }, {
    "title": "Genitive variation in English: conceptual factors in synchronic and diachronic studies",
    "authors": ["Anette Rosenbach."],
    "venue": "Mouton de Gruyter, Berlin.",
    "year": 2002
  }, {
    "title": "PrepNet: a multilingual lexical description of prepositions",
    "authors": ["Patrick Saint-Dizier."],
    "venue": "Proc. of LREC, volume 6, pages 1021–1026, Genoa, Italy.",
    "year": 2006
  }, {
    "title": "Adposition and Case Supersenses v2: Guidelines for English. arXiv:1704.02134",
    "authors": ["Nathan Schneider", "Jena D. Hwang", "Archna Bhatia", "NaRae Han", "Vivek Srikumar", "Tim O’Gorman", "Sarah R. Moeller", "Omri Abend", "Austin Blodgett", "Jakob Prange"],
    "year": 2018
  }, {
    "title": "A corpus of preposition supersenses",
    "authors": ["Nathan Schneider", "Jena D. Hwang", "Vivek Srikumar", "Meredith Green", "Abhijit Suresh", "Kathryn Conger", "Tim O’Gorman", "Martha Palmer"],
    "venue": "In Proc. of LAW X – the 10th Linguistic Annotation Workshop,",
    "year": 2016
  }, {
    "title": "A hierarchy with, of, and for preposition supersenses",
    "authors": ["Nathan Schneider", "Vivek Srikumar", "Jena D. Hwang", "Martha Palmer."],
    "venue": "Proc. of The 9th Linguistic Annotation Workshop, pages 112–123, Denver, Colorado, USA.",
    "year": 2015
  }, {
    "title": "Rhythm’s role in genitive construction choice in spoken English",
    "authors": ["Stephanie Shih", "Jason Grafmiller", "Richard Futrell", "Joan Bresnan."],
    "venue": "Ralf Vogel and Ruben van de Vijver, editors, Rhythm in cognition and grammar: a Germanic perspective, pages",
    "year": 2015
  }, {
    "title": "A joint model for extended semantic role labeling",
    "authors": ["Vivek Srikumar", "Dan Roth."],
    "venue": "Proc. of EMNLP, pages 129–139, Edinburgh, Scotland, UK.",
    "year": 2011
  }, {
    "title": "Modeling semantic relations expressed by prepositions",
    "authors": ["Vivek Srikumar", "Dan Roth."],
    "venue": "Transactions of the Association for Computational Linguistics, 1:231–242.",
    "year": 2013
  }, {
    "title": "Fictive motion in language and “ception",
    "authors": ["Leonard Talmy."],
    "venue": "Paul Bloom, Mary A. Peterson, Nadel Lynn, and Merrill F. Garrett, editors, Language and Space, pages 211–276. MIT Press, Cambridge, MA.",
    "year": 1996
  }, {
    "title": "Possessives in English: An Exploration in Cognitive Grammar",
    "authors": ["John R. Taylor."],
    "venue": "Clarendon Press, Oxford, UK.",
    "year": 1996
  }, {
    "title": "Disambiguation of preposition sense using linguistically motivated features",
    "authors": ["Stephen Tratz", "Dirk Hovy."],
    "venue": "Proc. of NAACL-HLT Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado.",
    "year": 2009
  }, {
    "title": "Automatic interpretation of the English possessive",
    "authors": ["Stephen Tratz", "Eduard Hovy."],
    "venue": "Proc. of ACL, pages 372–381, Sofia, Bulgaria.",
    "year": 2013
  }, {
    "title": "The Semantics of English Prepositions: Spatial Scenes, Embodied Meaning and Cognition",
    "authors": ["Andrea Tyler", "Vyvyan Evans."],
    "venue": "Cambridge University Press, Cambridge, UK.",
    "year": 2003
  }, {
    "title": "Dative and genitive variability in Late Modern English: Exploring crossconstructional variation and change",
    "authors": ["Christoph Wolk", "Joan Bresnan", "Anette Rosenbach", "Benedikt Szmrecsanyi."],
    "venue": "Diachronica, 30(3):382–419.",
    "year": 2013
  }, {
    "title": "Constructing spatial concepts from universal primitives",
    "authors": ["Yang Xu", "Charles Kemp."],
    "venue": "Proc. of CogSci, pages 346–351, Portland, Oregon.",
    "year": 2010
  }, {
    "title": "MELB-YB: Preposition sense disambiguation using rich semantic features",
    "authors": ["Patrick Ye", "Timothy Baldwin."],
    "venue": "Proc. of SemEval, pages 241–244, Prague, Czech Republic.",
    "year": 2007
  }, {
    "title": "Vector space semantics: a model-theoretic analysis of locative prepositions",
    "authors": ["Joost Zwarts", "Yoad Winter."],
    "venue": "Journal of Logic, Language and Information, 9:169–211.",
    "year": 2000
  }],
  "id": "SP:c7ce154ca54a83b44827b2d435e423c5f58d3f05",
  "authors": [{
    "name": "Nathan Schneider",
    "affiliations": []
  }, {
    "name": "Jena D. Hwang",
    "affiliations": []
  }, {
    "name": "Vivek Srikumar",
    "affiliations": []
  }, {
    "name": "Jakob Prange",
    "affiliations": []
  }, {
    "name": "Austin Blodgett",
    "affiliations": []
  }, {
    "name": "Sarah R. Moeller",
    "affiliations": []
  }, {
    "name": "Aviram Stern",
    "affiliations": []
  }, {
    "name": "Adi Bitan",
    "affiliations": []
  }, {
    "name": "Omri Abend",
    "affiliations": []
  }],
  "abstractText": "Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.",
  "title": "Comprehensive Supersense Disambiguation of English Prepositions and Possessives"
}