{
  "sections": [{
    "heading": "1 INTRODUCTION",
    "text": "The dynamics of a large number of physical phenomenon are governed by the incompressible Navier-Stokes equations. In this work, we follow the Eulerian viewpoint for simulating these equations, which approximates quantities on a regular grid (Foster & Metaxas, 1996). Euler methods are able to produce precise results simulating fluids like water or smoke, at the cost of a high computational load.\nThe most demanding portion of this method is solving the discrete Poisson equation, which enforces the incompressibility condition. Exact solutions can be found using the Preconditioned Conjugate Gradient (PCG) algorithm or via stationary iterative methods such as the Jacobi or Gauss-Seidel methods. A number of numerical methods have been proposed to mitigate this limitation for offline applications, notably multi-grid approximations (McAdams et al., 2010). However, in real-time Jacobi iterations are truncated before reaching convergence, rendering these methods inexact and the obtained velocity fields divergent. A natural approach is to tackle the problem in a data-driven manner, adapting the solver to the specifics of the data of interest. For instance, by operating on a representation of the simulation space of significantly lower dimensionality (Treuille et al., 2006; De Witt et al., 2012). More recently, approaches have been proposed which train black-box machine learning systems to predict the output produced by an exact solver, e.g. using random regression forests (Ladický et al., 2015) or neural networks (Yang et al., 2016) for Lagrangian and Eulerian methods respectively. A major limitation of these methods is that they require a dataset of linear system solutions provided by an exact solver. Hence, targets cannot be computed during training and models are trained to predict the ground-truth output always starting from an initial frame produced by an exact solver, while at test time this initial frame is actually generated by the model itself. This discrepancy between training and simulation can yield errors that can accumulate quickly along the generated sequence. Additionally, the ConvNet architecture proposed by Yang et al. is not suited to our more general use-case; in particular it cannot accurately simulate long-range phenomena, such as gravity or buoyancy. While providing encouraging results that offer a significant speedup over their PCG baseline, their work is limited to data closely matching the training conditions (as we will discuss in Section 3).\nThe contributions of this work are as follows: (i) the learning task can be phrased as a completely unsupervised learning problem; since obtaining ground-truth data is no longer necessary, we can incorporate loss information from a composition of multiple time-steps and perform various forms of non-trivial data-augmentation. (ii) we propose a collection of domain-specific ConvNet architectural optimizations motivated by the linear system structure itself, which lead to both qualitative and\n0 10 20 30 40 50 60 0\n5\n10\n15\n20\nTimestep\nE (| |∇\n· û || )\nJacobi 34 iterations this work: small model this work: multi-frame this work: single frame\nFigure 3: Test-set E (‖∇ · ûi‖) versus time-step\nquantitative improvements. (iii) the proposed simulator is stable and permits real-time simulation, showing good generalization properties to unseen settings.\nAn alternative to our approach is learning an end-to-end mapping that predicts the velocity field directly at each time-step. We argue that our hybrid approach restricts the learning task to a stable projection step, relieving the need for modeling the well understood advection and external body forces and enabling the use of enhancing tools such as vorticity confinement. In addition to the above the technical contributions, we contribute a dataset that can be of interest for people working on real-time simulations and as a benchmarking framework for end-to-end approaches."
  }, {
    "heading": "2 MODEL",
    "text": "When a fluid has zero viscosity and is incompressible it can be modeled by the Euler equations:\n∂u ∂t = −u · ∇u− 1 ρ ∇p+ f subject to, ∇ · u = 0, (1)\nwhere u is the velocity (a 2D or 3D vector field), t is time, p is the pressure (a scalar field), f is the summation of external forces applied to the fluid body (buoyancy, gravity, etc) and ρ is fluid density. We numerically calculate all partial derivatives using finite difference (FD) methods on a MAC grid Harlow & Welch (1965). Equations 1 can be solved via the standard operator splitting method described in Algorithm 1. At a high level, step 1 ignores the pressure term (−∇p of (1)) to create an advected velocity field, u?t , which includes unwanted divergence (see see (Selle et al., 2008) for details), and then step 7 solves for pressure, p, to satisfy the constraint in (1). This produces a divergence free velocity field, ut. In addition, we use vorticity confinement (Steinhoff & Underhill, 1994) to counteract unwanted numerical dissipation. Step 8 is computationally demanding as it involves solving the Poisson equation: ∇2pt = 1∆t∇ · u?t . Rewriting this equation results in a large sparse linear system Apt = b, where A is referred to in the literature as the 5 or 7 point Laplacian matrix (for 2D and 3D grids respectively). After solving for pressure, the divergence free velocity is calculated by subtracting the FD gradient of pressure, ut = u?t − 1ρ∇p. We propose a learned approximate inference mechanism to find fast and efficient solutions to the linear system Apt = b. The key observation is that, while there is no closed form solution, the function mapping input data to the optimum of an optimization problem is deterministic. Therefore one can attempt to approximate it using a powerful regressor such as a deep neural network. A block diagram of our high-level model architecture is depicted in Figure 1, and shows the computational blocks required to calculate ût for a single time-step. The advect block is a fixed function unit solving step 1 of Algorithm 1. Then we add the body and vorticity confinement forces and obtain the divergence of the velocity field ∇ · u?t which, along with geometry, is fed through a multi-stage ConvNet to produce p̂t. We then calculate the pressure divergence, and subtract it from the divergent velocity to produce ût. Note that the only block with trainable parameters is the ConvNet model.\nWe define an objective function and formulate the inference solution as an unsupervised machine learning task where the loss function is given by,\nfobj = ∑ i wi {∇ · ût}2i = ∑ i wi { ∇ · ( u?t − 1 ρ ∇p̂t )}2 i\n(2)\nWhere ût and p̂t are the predicted divergence free velocity and pressure fields respectively and wi is a per-vertex weighting term which emphasizes the divergence of voxels on geometry boundaries. Note that the bottle-neck architecture in the ConvNet avoids obtaining trivial solutions.\nThe internal structure of the ConvNet architecture is shown in Figure 2. It consists of 5 stages of convolution (spatial or volumetric) and Rectifying Linear layers (ReLU). The convolutional operator itself mimics the local sparsity structure of our linear system. However a single resolution network would have limited context, which limits the network‘s ability to model long-range external forces (such as gravity or buoyancy). As such, we add multi-resolution features to enable modeling long range physical phenomenon, processing each resolution in parallel then upsampling the resultant low resolution features before accumulating them."
  }, {
    "heading": "3 RESULTS AND ANALYSIS",
    "text": "The model of Section 2 was implemented in Torch7 Collobert et al. (2011), with two CUDA baseline methods for comparison; a Jacobi-based iterative solver and a PCG-based solver (with incomplete Cholesky L0 preconditioner). All tests are performed on a tailored dataset, see Appendix A.\nTo implement the model of Yang et al. (2016) for comparison, we rephrase their fully-connected architecture as an equivalent, but significantly faster, sliding window model (on a 96x128x96 grid, Yang et al. report 515ms/frame, while our implementation takes 9.4ms/frame). Unfortunately, their loss function fails to learn an accurate projection on our dataset. This is because our divergent velocity frames include gravity and buoyancy terms, which result in a high amplitude, low frequency gradient in the ground-truth pressure. The small 3x3x3 context of the Yang et al. model cannot infer such low frequency output, which dominates the loss function and results in over-training. By contrast, our unsupervised objective minimizes divergence after the pressure gradient operator, whose FD calculation acts as a high-pass filter. This is a significant advantage; our objective function is “softer” on the divergence contribution for phenomena that the network cannot easily infer. For the remaining experimental results, we will evaluate an improved version of the Yang et al. model as our “small model” (i.e. a single resolution with only 3x3x3 context, trained using the loss function, top level architectural improvements and training procedure of this work).\nFor fair quantitative comparison of output residual, we choose the number of Jacobi iterations (34) to approximately match the FPROP time of our network. PCG is orders of magnitude slower at all resolutions. The “small-model” provides a significant speedup over other methods. The runtime for the PCG, Jacobi, this work, and the “small model” are 2521ms, 47.6ms, 39.9ms and 16.9ms respectively. See Appendix B for details, including timing as a function of resolution in Figure 5.\nWe simulated a 3D smoke plume using our system and baseline methods (visual results are shown in Appendix C, figures 6 and 7) 1. Note that this boundary condition is not present in the training set; it is a difficult test of generalization performance. Qualitatively, the PCG and 100-iteration Jacobi solvers and our network produce visually similar results. The “small model”, cannot accurately simulate the large vortex under the plume, and as a result the plume rises too quickly and exhibits density blurring. Similarly the Jacobi method, when truncated early at 34 iterations, introduces implausible high frequency noise and has an elongated shape due to inaccurate modeling of buoyancy. Both ConvNet based methods lose some smoke density inside the arch model due to residual negative divergence at the fluid-geometry boundary. The maximum residual norm was <1e-3, 1.235, 1.966, 0.872 for the PCG, Jacobi, small model and this work respectively.\nAs a test of long-term stability, we record the mean residual norm (E (‖∇ · ûi‖)) across all samples in our test-set for each frame after the initial condition, shown in Figure 1. Our model outperforms the small model (Yang et al. sizing), and is competitive with Jacobi. We also present the results of our model when a single time-step loss is used; without the multi-frame loss, single time-step accuracy is degraded, and the divergence increases over time as error is accumulated.\n1Video examples of these experiments can be found in at http://cims.nyu.edu/˜schlacht/CNNFluids.htm."
  }, {
    "heading": "A DATASET CREATION AND MODEL TRAINING",
    "text": "While we do not need ground-truth label information to train the ConvNet model of Section 2, we need a collection of ground-truth pressure solutions to evaluate the precision of our model, and additionally our model does benefit from an efficient sampling of “realistic” initial conditions. That is, the space of all divergent velocity fields is unconstrained, and so our network’s generalization performance is improved when using a dataset of natural initial conditions that approximately samples the manifold of real-world fluid simulation states. To this end, we propose a procedural method to generate a corpus of initial frames for use in training.\nWe use synthetic data generated using an offline 3D solver, mantaflow Pfaff & Thuerey - an opensource research library for solving incompressible fluid flow. We then seed this solver with initial condition states generated via a simple procedure using a combination of i. a pseudo-random turbulent field to initialize the velocity ii. a random placement of geometry within this field, and iii. procedurally adding localized input perturbations. We will now describe this procedure in detail.\nFirstly, we use the wavelet turbulent noise of Kim et al. (2008) to initialize a pseudo-random, divergence free velocity field. At the beginning of each simulation we randomly sample a set of noise parameters (uniformly sampling the wavelet spatial scale and amplitude) and we generate a random seed, which we then use to generate the velocity field.\nNext, we generate an occupancy grid by selecting objects from a database of models and randomly scaling, rotating and translating these objects in the simulation domain. We use a subset of 100 objects from the NTU 3D Model Database Pu & Ramani (2006); 50 models are used only when generating training set initial conditions and 50 models are used when generating test samples. Figure 4 shows a selection of these models. Each model is voxelized using the binvox library Min (2016). For generating 2D simulation data, we simply take a 2D slice of the 3D voxel grid.\nFinally, we simulate small divergent input perturbations by modeling inflow moving across the velocity field using a collection of emitter particles. We do this by generating a random set of emitters (with random time duration, position, velocity and size) and adding the output of these emitters to the velocity field throughout the simulation.\nWith the above initial conditions defined, we use manta to calculate u?t by advecting the velocity field and adding forces. We also step the simulator forward 256 frames (using Manta’s PCG-based solver), recording the divergent velocity every 8 frame steps.\nUsing the above procedure, we generate a training set of 320 “scenes” (each with a random initial condition) and a test set of an additional 320 scenes. Each “scene” contains 32 frames, each 0.8 seconds apart. We use a disjoint set of geometry for the test and training sets to test generalization performance. We will make this dataset public (as well as the code for generating it) for future research use. All materials are located at http://cims.nyu.edu/˜schlacht/CNNFluids.htm."
  }, {
    "heading": "B DETAILS OF THE EVALUATION OF THE COMPUTATIONAL COMPLEXITY",
    "text": "Figure 5, shows the computation time of the Jacobi method, the small-model version (with Yang et al. sizing) and this work. This runtime includes the pressure projection steps only: including velocity divergence calculation, the linear system solve, and the velocity update. Note that for\nfair quantitative comparison of output residual (shown in Section 3 of the paper), we choose the number of Jacobi iterations (34) to approximately match the FPROP time of our network. Since the asymptotic complexity as a function of resolution is the same for Jacobi and our ConvNet, the FPROP times are equivalent. We use an NVIDIA Titan X GPU with 12GB of ram and an Intel Xeon E5-2690 CPU. PCG is orders of magnitude slower at all resolutions and has been left off for clarity. The model of Yang et al. provides a significant speedup over other methods. The runtime for the PCG, Jacobi, this work, and Yang et al. at 1283 grid resolution are 2521ms, 47.6ms, 39.9ms and 16.9ms respectively.\nNote that with custom hardware Movidius; Google Inc., separable convolutions and other architectural enhancements, we believe the runtime of our ConvNet could be reduced significantly. However, we leave this to future work."
  }, {
    "heading": "C QUALITATIVE COMPARISON OF SIMULATIONS",
    "text": "This appendix shows rendered frames for the proposed method as well as baseline alternatives.\nFigure 6 shows a rendered frame of our plume simulation (without geometry) for all methods. Note that this boundary condition is not present in the training set and represents an input divergent flow approximately 5 times wider than the largest impulse present during training. It is a difficult test of generalization performance for data-driven methods. Qualitatively, the PCG and Jacobi (with 100 iterations) and our network produce visually similar results. The model of Yang et al., trained using the loss function of this work, cannot accurately simulate the large vortex under the plume, and as a result the plume rises too quickly and exhibits density blurring under the plume itself. Similarly the Jacobi method, when truncated early at 34 iterations, introduces implausible high frequency noise and has an elongated shape due to inaccurate modeling of buoyancy forces.\nWe also repeat the above simulation with solid cells from the “arch” model held out of our training set. Single frame results for this simulation are shown in Figure 7. Since this scene exhibits lots of turbulent flow, qualitative comparison is less useful. However, the network of Yang et al. has difficulty minimizing divergence around large flat boundaries and results in high-frequency density artifacts as shown. Both ConvNet based methods lose some smoke density inside the arch model due to negative divergence at the fluid-geometry boundary (specifically at the large flat ceiling), like a result of this wide plume interaction being outside the scope of the training samples."
  }],
  "year": 2017,
  "references": [{
    "title": "pressible flow of fluid with free surface",
    "authors": ["Theodore Kim", "Nils Thürey", "Doug James", "Markus Gross"],
    "venue": "Physics of Fluids,",
    "year": 1965
  }, {
    "title": "Modification of the euler equations for vorticity confinement",
    "authors": ["John Steinhoff", "David Underhill"],
    "venue": "tionally stable maccormack method. J. Sci. Comput.,",
    "year": 2008
  }, {
    "title": "Data-driven projection method in fluid simulation",
    "authors": ["Xubo Yang", "Xiangyun Xiao"],
    "year": 1962
  }],
  "id": "SP:248fb08767b2ae2f5ade7853af432fd54d615afc",
  "authors": [{
    "name": "Jonathan Tompson",
    "affiliations": []
  }, {
    "name": "Kristofer Schlachter",
    "affiliations": []
  }],
  "abstractText": "Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large linear system with many free-parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.",
  "title": "ACCELERATING EULERIAN FLUID SIMULATION WITH CONVOLUTIONAL NETWORKS"
}