{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 23–33 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n23"
  }, {
    "heading": "1 Introduction",
    "text": "Word embeddings are vector space representations of word meaning (Mikolov et al., 2013b; Pennington et al., 2014). A remarkable property of these models is that they capture various lexical relationships, beyond mere similarity. For example, (Mikolov et al., 2013b) found that analogy questions of the form “a is to b what c is to ?” can often be answered by finding the word d that maximizes cos(wb−wa+wc, wd), where we write wx for the vector representation of a word x.\nIntuitively, the word vector wa represents a in terms of its most salient features. For example, wparis implicitly encodes that Paris is located in France and that it is a capital city, which is intuitively why the ‘capital of’ relation can be modeled in terms of a vector difference. Other relationships, however, such as the fact that Macron succeeded Hollande as president of France, are un-\nlikely to be captured by word embeddings. Relation extraction methods can discover such information by analyzing sentences that contain both of the words or entities involved (Mintz et al., 2009; Riedel et al., 2010; dos Santos et al., 2015), but they typically need a large number of training examples to be effective.\nA third alternative, which we consider in this paper, is to characterize the relatedness between two words s and t by learning a relation vector rst in an unsupervised way from corpus statistics. Among others, such vectors can be used to find word pairs that are similar to a given word pair (i.e. finding analogies), or to find the most prototypical examples among a given set of relation instances. They can also be used as an alternative to the aforementioned relation extraction methods, by subsequently training a classifier that uses the relation vectors as input, which might be particularly effective in cases where only limited amounts of training data are available (with the case of analogy finding from a single instance being an extreme example).\nThe most common unsupervised approach for learning relation vectors consists of averaging the embeddings of the words that occur in between s and t, in sentences that contain both (Weston et al., 2013; Fan et al., 2015; Hashimoto et al., 2015). While this strategy is often surprisingly effective (Hill et al., 2016), it is sub-optimal for two reasons. First, many of the words co-occurring with s and t will be semantically related to s or to t, but will not actually be descriptive for the relationship between s and t; e.g. the vector describing the relation between Paris and France should not be affected by words such as eiffel (which only relates to Paris). Second, it gives too much weight to stopwords, which cannot be addressed in a straightforward way as some stop-words are actually crucial for modeling relationships (e.g. prepositions such\nas ‘in’ or ‘of’ or Hearst patterns (Indurkhya and Damerau, 2010)).\nIn this paper, we propose a method for learning relation vectors directly from co-occurrence statistics. We first introduce a variant of GloVe, in which word vectors can be directly interpreted as smoothed PMI-weighted bag-of-words representations. We then represent relationships between words as weighted bag-of-words representations, using generalizations of PMI to three arguments, and learn vectors that correspond to smoothed versions of these representations.\nAs far as the possible applications of our methodology is concerned, we imagine that relation vectors can be used in various ways to enrich the input to neural network models. As a simple example, in a question answering system, we could “annotate” mentions of entities with relation vectors encoding their relationship to the different words from the question. As another example, we could consider a recommendation system which takes advantage of vectors expressing the relationship between items that have been bought (or viewed) by a customer and other items from the catalogue. Finally, relation vectors should also be useful for knowledge completion, especially in cases where few training examples per relation type are given (meaning that neural network models could not be used) and where relations cannot be predicted from the already available knowledge (meaning that knowledge graph embedding methods could not be used, or are at least not sufficient)."
  }, {
    "heading": "2 Related Work",
    "text": "The problem of characterizing the relationship between two words has been studied in various settings. From a learning point of view, the most straightforward setting is where we are given labeled training sentences, with each label explicitly indicating what relationship is expressed in the sentence. This fully supervised setting has been the focus of several evaluation campaigns, including as part of ACE (Doddington et al., 2004) and at SemEval 2010 (Hendrickx et al., 2010). A key problem with this setting, however, is that labeled training data is hard to obtain. A popular alternative is to use known instances of the relations of interest as a form of distant supervision (Mintz et al., 2009; Riedel et al., 2010). Some authors have also considered unsupervised relation extraction methods (Shinyama and Sekine, 2006;\nBanko et al., 2007), in which case the aim is essentially to find clusters of patterns that express similar relationships, although these relationships may not correspond to the ones that are needed for the considered application. Finally, several systems have also used bootstrapping strategies (Brin, 1998; Agichtein and Gravano, 2000; Carlson et al., 2010), where a small set of instances are used to find extraction patterns, which are used to find more instances, which can in turn be used to find better extraction patterns, etc.\nTraditionally, relation extraction systems have relied on a variety of linguistic features, such as lexical patterns, part-of-speech tags and dependency parsers. More recently, several neural network architectures have been proposed for the relation extraction problem. These architectures rely on word embeddings to represent the words in the input sentence, and manipulate these word vectors to construct a relation vector. Some approaches simply represent the sentence (or the phrase connecting the entities whose relationship we want to determine) as a sequence of words, and use e.g. convolutional networks to aggregate the vectors of the words in this sequence (Zeng et al., 2014; dos Santos et al., 2015). Another possibility, explored in (Socher et al., 2012), is to use parse trees to capture the structure of the sentence, and to use recursive neural networks (RNNs) to aggregate the word vectors in a way which respects this structure. A similar approach is taken in (Xu et al., 2015), where LSTMs are applied to the shortest path between the two target words in a dependency parser. A straightforward baseline method is to simply take the average of the word vectors (Mitchell and Lapata, 2010). While conceptually much simpler, variants of this approach have obtained state-of-the-art performance for relation classification (Hashimoto et al., 2015) and a variety of tasks that require sentences to be represented as a vector (Hill et al., 2016).\nGiven the effectiveness of word vector averaging, in (Kenter et al., 2016) a model was proposed that explicitly tries to learn word vectors that generalize well when being averaged. Similarly, the model proposed in (Hashimoto et al., 2015) aims to produce word vectors that perform well for the specific task of relation classification. The ParagraphVector method from (Le and Mikolov, 2014) is related to the aformentioned approaches, but it explicitly learns a vector representation for each\nparagraph along with the word embeddings. However, this method is computationally expensive, and often fails to outperform simpler approaches (Hill et al., 2016).\nTo the best of our knowledge, existing methods for learning relation vectors are all based on manipulating pre-trained word vectors. In contrast, we will directly learn relation vectors from corpus statistics, which will have the important advantage that we can focus on words that describe the interaction between the two words s and t, i.e. words that commonly occur in sentences that contain both s and t, but are comparatively rare in sentences that only contain s or only contain t.\nFinally, note that our work is fundamentally different from Knowledge Graph Embedding (KGE) (Wang et al., 2014b), (Wang et al., 2014a), (Bordes et al., 2011) in at least two ways: (i) KGE models start from a structured knowledge graph whereas we only take a text corpus as input, and (ii) KGE models represent relations as geometric objects in the “entity embedding” itself (e.g. as translations, linear maps, combinations of projections and translations, etc), whereas we represent words and relations in different vector spaces."
  }, {
    "heading": "3 Word Vectors as PMI Encodings",
    "text": "Our approach to relation embedding is based on a variant of the GloVe word embedding model (Pennington et al., 2014). In this section, we first briefly recall the GloVe model itself, after which we discuss our proposed variant. A key advantage of this variant is that it allows us to directly interpret word vectors in terms of the Pointwise Mutual Information (PMI), which will be central to the way in which we learn relation vectors."
  }, {
    "heading": "3.1 Background",
    "text": "The GloVe model (Pennington et al., 2014) learns a vector wi for each word i in the vocabulary, based on a matrix of co-occurrence counts, encoding how often two words appear within a given window. Let us write xij for the number of times word j appears in the context of word i in some text corpus. More precisely, assume that there are m sentences in the corpus, and letP li ⊆ {1, ..., nl} be the set of positions from the lth sentence where the word i can be found (with nl the length of the\nsentence). Then xij is defined as follows:\nm∑ l=1 ∑ p∈Pli ∑ q∈Plj weight(p, q)\nwhere weight(p, q) = 1|p−q| if 0 < |p − q| ≤ W , and weight(p, q) = 0 otherwise, where the window size W is usually set to 5 or 10.\nThe GloVe model learns for each word i two vectors wi and w̃i by optimizing the following objective:∑\ni ∑ j:xij 6=0 f(xij)(wi·w̃j + bi + b̃j − log xij)2\nwhere f is a weighting function, aimed at reducing the impact of rare terms, and bi and b̃j are bias terms. The GloVe model is closely related to the notion of pointwise mutual information (PMI), which is defined for two words i and j as PMI(i, j) = log ( P (i,j) P (i)P (j) ) , where P (i, j) is the probability of seeing the words i and j if we randomly pick a word position from the corpus and a second word position within distance W from the first position. The PMI between i and j is usually estimated as follows:\nPMIX(i, j) = log ( xijx∗∗ xi∗x∗j ) where xi∗ = ∑ j xij , x∗j = ∑ i xij and x∗∗ =∑\ni ∑ j xij . In particular, it is straightforward to see that after the reparameterization given by bi 7→ bi + log xi∗ − log x∗∗ and bj 7→ bj + log x∗j , the GloVe model is equivalent to∑ i ∑ j\nxij 6=0\nf(xij)(wi·w̃j + bi + b̃j − PMIX(i, j))2\n(1)"
  }, {
    "heading": "3.2 A Variant of GloVe",
    "text": "In this paper, we will use the following variant of the formulation in (1):∑\ni ∑ j∈Ji 1 σ2j (wi·w̃j + b̃j − PMIS(i, j))2 (2)\nDespite its similarity, this formulation differs from the GloVe model in a number of important ways. First, we use smoothed frequency counts instead of the observed frequency counts xij . In particular, the PMI between words i and j is given as:\nPMIS(i, j) = log ( P (i, j)\nP (i)P (j)\n)\nwhere the probabilities are estimated as follows:\nP (i) = xi∗ + α\nx∗∗ + nα P (j) =\nx∗j + α\nx∗∗ + nα\nP (i, j) = xij + α\nx∗∗ + n2α\nwhere α ≥ 0 is a parameter controlling the amount of smoothing and n is the size of the vocabulary. This ensures that the estimation of PMI(i, j) is well-defined even in cases where xij = 0, meaning that we no longer have to restrict the inner summation to those j for which xij > 0. For efficiency reasons, in practice, we only consider a small subset of all context words j for which xij = 0, which is similar in spirit to the use of negative sampling in Skip-gram (Mikolov et al., 2013b). In particular, the set Ji contains each j such that xij > 0 as well as M uniformly1 sampled context words j for which xij = 0, where we choose M = 2 · |{j : xij > 0}|.\nSecond, following (Jameel and Schockaert, 2016), the weighting function f(xij) has been replaced by 1\nσ2j , where σ2j is the residual variance of\nthe regression problem for context word j, estimated follows:\nσ2j = 1\n|J−1j | ∑ i∈J−1j (wi · w̃j + b̃j − PMIS(i, j))2\nwith J−1j = {i : j ∈ Ji}. Since we need the word vectors to estimate this residual variance, we reestimate σ2j after every five iterations of the SGD optimization. For the first 5 iterations, where no estimation for σ2j is available, we use the GloVe weighting function.\nThe use of smoothed frequency counts and residual variance based weighting make the word embedding model more robust for rare words. For instance, if w only co-occurs with a handful of other terms, it is important to prioritize the most informative context words, which is exactly what the use of the residual variance achieves, i.e. σ2j is small for informative terms and large for stop words; see (Jameel and Schockaert, 2016). This will be important for modeling relations, as the relation vectors will often have to be estimated from very sparse co-occurrence counts.\n1While the negative sampling method used in Skip-gram favors more frequent words, initial experiments suggested that deviating from a uniform distribution almost had no impact in our setting.\nFinally, the bias term bi has been omitted from the model in (2). We have empirically found that omitting this bias term does not affect the performance of the model, while it allows us to have a more direct connection between the vector wi and the corresponding PMI scores."
  }, {
    "heading": "3.3 Word Vectors and PMI",
    "text": "Let us define PMIW as follows:\nPMIW (i, j) = wi·w̃j + b̃j\nClearly, when the word vectors are trained according to (2), it holds that PMIW (i, j) ≈ PMIS(i, j). In other words, we can think of the word vector wi as a low-dimensional encoding of the vector (PMIS(i, 1), ...,PMIS(i, n)), with n the number of words in the vocabulary. This view allows us to assign a natural interpretation to some word vector operations. In particular, the vector difference wi−wk is commonly used as a model for the relationship between words i and k. For a given context word j, we have\n(wi − wk) · w̃j = PMIW (i, j)− PMIW (k, j) The latter is an estimation of log (\nP (i,j) P (i)P (j)\n) −\nlog (\nP (k,j) P (k)P (j)\n) = log ( P (j|i) P (j|k) ) . In other words,\nthe vector translation wi − wk encodes for each context word j the (log) ratio of the probability of seeing j in the context of i and in the context of k, which is in line with the original motivation underlying the GloVe model (Pennington et al., 2014). In the following section, we will propose a number of alternative vector representations for the relationship between two words, based on generalizations of PMI to three arguments."
  }, {
    "heading": "4 Learning Global Relation Vectors",
    "text": "We now turn to the problem of learning a vector rik that encodes how the source word i and target word k are related. The main underlying idea is that rik will capture which context words j are most closely associated with the word pair (i, k). Whereas the GloVe model is based on statistics about (main word, context word) pairs, here we will need statistics on (source word, context word, target word) triples. First, we discuss how cooccurrence statistics among three words can be expressed using generalizations of PMI to three arguments. Then we explain how this can be used to learn relation vectors in natural way."
  }, {
    "heading": "4.1 Co-occurrence Statistics for Triples",
    "text": "Let P li ⊆ {1, ..., nl} again be the set of positions from the lth sentence corresponding to word i. We define:\nyijk = m∑ l=1 ∑ p∈Pli ∑ q∈Plj ∑ r∈Plk weight(p, q, r)\nwhere weight(p, q, r) = max( 1q−p , 1 r−q ) if p < q < r and r−p ≤W , and weight(p, q, r) = 0 otherwise. In other words, yijk reflects the (weighted) number of times word j appears between words i and k in a sentence in which i and k occur sufficiently close to each other, in that order. Note that by taking word order into account in this way, we will be able to model asymmetric relationships.\nTo model how strongly a context word j is associated with the word pair (i, k), we will consider the following two well-known generalizations of PMI to three arguments (Van de Cruys, 2011):\nSI1(i, j, k) = log ( P (i, j)P (i, k)P (j, k)\nP (i)P (j)P (k)P (i, j, k) ) SI2(i, j, k) = log ( P (i, j, k)\nP (i)P (j)P (k) ) where P (i, j, k) is the probability of seeing the word triple (i, j, k) when randomly choosing a sentence and three (ordered) word positions in that sentence within a window size of W . In addition we will also consider two ways in which PMI can be used more directly:\nSI3(i, j, k) = log ( P (i, j, k)\nP (i, k)P (j) ) SI4(i, j, k) = log ( P (i, k|j)\nP (i|j)P (k|j) ) Note that SI3(i, j, k) corresponds to the PMI between (i, k) and j, whereas SI4(i, j, k) is the PMI between i and k conditioned on the fact that j occurs. The measures SI3 and SI4 are closely related to SI1 and SI2 respectively2. In particular, the following identities are easy to show:\nPMI(i, j) + PMI(j, k)− SI1(i, j, k) = SI3(i, j, k) SI2(i, j, k)− PMI(i, j)− PMI(j, k) = SI4(i, j, k)\n2Note that probabilities of the form P (i, j) or P (i) here refer to marginal probabilities over ordered triples. In contrast, the PMI scores from the word embedding model are based on probabilities over unordered word pairs, as is common for word embeddings.\nUsing smoothed versions of the counts yijk, we can use the following probability estimates for SI1(i, j, k)–SI4(i, j, k):\nP (i, j, k) = yijk + α\ny∗∗∗ + n3α P (i, j) =\nyij∗ + α\ny∗∗∗ + n2α\nP (i, k) = yi∗k + α\ny∗∗∗ + n2α P (j, k) =\ny∗jk + α\ny∗∗∗ + n2α\nP (i) = yi∗∗ + α\ny∗∗∗ + nα P (j) =\ny∗j∗ + α\ny∗∗∗ + nα\nP (k) = y∗∗k + α\ny∗∗∗ + nα where yij∗ = ∑\nk yijk, and similar for the other counts. For efficiency reasons, the counts of the form yij∗, yi∗k and y∗jk are pre-computed for all word pairs, which can be done efficiently due to the sparsity of co-occurrence counts (i.e. these counts will be 0 for most pairs of words), similarly to how to the counts xij are computed in GloVe. From these counts, we can also efficiently pre-compute the counts yi∗∗, y∗j∗, y∗∗k and y∗∗∗. On the other hand, the counts yijk cannot be precomputed, since the total number of triples for which yijk 6= 0 is prohibitively high in a typical corpus. However, using an inverted index, we can efficiently retrieve the sentences that contain the words i and k, and since this number of sentences is typically small, we can efficiently obtain the counts yijk corresponding to a given pair (i, k) whenever they are needed."
  }, {
    "heading": "4.2 Relation Vectors",
    "text": "Our aim is to learn a vector rik that models the relationship between i and k. Computing such a vector for each pair of words (which co-occur at least once) is not feasible, given the number of triples (i, j, k) that would need to be considered. Instead, we first learn a word embedding, by optimizing (2). Then, fixing the context vectors w̃j and bias terms bj , we learn a vector representation for a given pair (i, k) of interest by solving the following objective:∑\nj∈Ji,k\n(rik·w̃j + b̃j − SI(i, j, k))2 (3)\nwhere SI refers to one of SI1S , SI 2 S , SI 3 S , SI 4 S . Note that (3) is essentially the counterpart of (1), where we have replaced the role of the PMI measure by SI. In this way, we can exploit the representations of the context words from the word embedding model for learning relation vectors. Note that the\nfactor 1 σ2j has been omitted. This is because words j that are normally relatively uninformative (e.g. stop words), for which σ2j would be high, can actually be very important for characterizing the relationship between i and k. For instance, the phrase “X such as Y ” clearly suggests a hyponomy relationship between X and Y , but both ‘such’ and ‘as’ would be associated with a high residual variance σ2j . The set Ji,k contains every j for which yijk > 0 as well as a random sample of m words for which yijk = 0, where m = 2 · |{j : yijk > 0|. Note that because w̃j is now fixed, (3) is a linear least squares regression problem, which can be solved exactly and efficiently.\nThe vector rik is based on words that appear between i and k. In the same way, we can learn a vector sik based on the words that appear before i and a vector tik based on the words that appear after k, in sentences where i occurs before k. Furthermore, we also learn vectors rki, ski and tki from the sentences where k occurs before i. As the final representation Rik of the relationship between i and k, we concatenate the vectors rik, rki, sik, ski, tik, tki as well as the word vectors wi and wk. We write Rlik to denote the vector that results from using measure SIl (l ∈ {1, 2, 3, 4})."
  }, {
    "heading": "5 Experimental Results",
    "text": "In our experiments, we have used the Wikipedia dump from November 2nd, 2015, which consists of 1,335,766,618 tokens. We have removed punctuations and HTML/XML tags, and we have lowercased all tokens. Words with fewer than 10 occurrences have been removed from the corpus. To detect sentence boundaries, we have used the Apache sentence segmentation tool. In all our experiments, we have set the number of dimensions to 300, which was found to be a good choice in previous work, e.g. (Pennington et al., 2014). We use a context window size W of 10 words. The number of iterations for SGD was set to 50. For our model, we have tuned the smoothing parameter α based on held-out tuning data, considering values from {0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001}. We have noticed that in most of the cases the value of α was automatically selected as 0.00001. To efficiently compute the triples, we have used the Zettair3 retrieval engine.\nAs our main baselines, we use three popular unsupervised methods for constructing relation vec-\n3http://www.seg.rmit.edu.au/zettair/\ntors. First, Diff uses the vector difference wk−wi, following the common strategy of modeling relations as vector differences, as e.g. in (Vylomova et al., 2016). Second, Conc uses the concatenation ofwi andwk. This model is more general than Diff but it uses twice as many dimensions, which may make it harder to learn a good classifier from few examples. The use of concatenations is popular e.g. in the context of hypernym detection (Baroni et al., 2012). Finally, Avg averages the vector representations of the words occurring in sentences that Diff, contain i and k. In particular, let ravgik be obtained by averaging the word vectors of the context words appearing between i and k for each sentence containing i and k (in that order), and then averaging the vectors obtained from each of these sentences. Let savgik and t avg ik be similarly obtained from the words occurring before i and the words occurring after k respectively. The considered relation vector is then defined as the concatenation of ravgik , r avg ki , s avg ik , s avg ki , t avg ik , t avg ki , wi and wk. The Avg will allow us to directly compare how much we can improve relation vectors by deviating from the common strategy of averaging word vectors."
  }, {
    "heading": "5.1 Relation Induction",
    "text": "In the relation induction task, we are given word pairs (s1, t1), ..., (sk, tk) that are related in some way, and the task is to decide for a number of test examples (s, t) whether they also have this relationship. Among others, this task was considered in (Vylomova et al., 2016), and a ranking version of this task was studied in (Drozd et al., 2016). As test sets we use the Google Analogy Test Set (Mikolov et al., 2013a), which contains instances of 14 different types of relations, and the DiffVec dataset, which was introduced in (Vylomova et al., 2016). This dataset contains instances of 36 dif-\nferent types of relations4. Note that both datasets contain a mix of semantic and syntactic relations.\nIn our evaluation, we have used 10-fold crossvalidation (or leave-one-out for relations with fewer than 10 instances). In the experiments, we consider for each relation in the test set a separate binary classification task, which was found to be considerably more challenging than a multi-class classification setting in (Vylomova et al., 2016). To generate negative examples in the training data (resp. test data), we have used three strategies, following (Vylomova et al., 2016). First, for a given positive example (s, t) of the considered relation, we add (t, s) as a negative example. Second, for each positive example (s, t), we generate two negative examples (s, t1) and (s, t2) by randomly selecting two tail words t1, t2 from the other training (resp. test) examples of the same relation. Finally, for each positive example, we also generate a negative example by randomly selecting two words from the vocabulary. For each relation, we then train a linear SVM classifier. To set the parameters of the SVM, we initially use 25% of the training data for tuning, and then retrain the SVM with the optimal parameters on the full training data.\nThe results are summarized in Table 1 in terms of accuracy and (macro-averaged) precision, recall and F1 score. As can be observed, our model outperforms the baselines on both datasets, with the R2ik variant outperforming the others.\nTo analyze the benefit of our proposed word embedding variant, Table 2 shows the results that were obtained when we use standard word embedding models. In particular, we show results for the standard GloVe model, SkipGram and the Continuous Bag of Words (CBOW) model. As can be observed, our variant leads to better results than the original GloVe model, even for the baselines.\n4Note that in contrast to (Vylomova et al., 2016) we use all 36 relations from this dataset, including those with very few instances.\nThe difference is particularly noticeable for DiffVec. The difference is also larger for our relation vectors than for the baselines, which is expected as our method is based on the assumption that context word vectors can be interpreted in terms of PMI scores, which is only true for our variant.\nSimilar as in the GloVe model, the context words in our model are weighted based on their distance to the nearest target word. Table 3 shows the results of our model without this weighting, for the relation induction task. Comparing these results with those in Table 1 shows that the weighting scheme indeed leads to a small improvement (except for the accuracy of R1ik for DiffVec). Similarly, in Table 3, we show what happens if the relation vectors sik, ski, tik and tki are omitted. In other words, for the results in Table 3, we only use context words that appear between the two target words. Again, the results are worse than those in Table 1 (with the accuracy ofR1ik for DiffVec again being an exception), although the differences are very small in this case. While including the vectors sik, ski, tik, tki should be helpful, it also significantly increases the dimensionality of the vectors Rlik. Given that the number of instances per relation is typically quite small for this\ntask, this can also make it harder to learn a suitable classifier."
  }, {
    "heading": "5.2 Measuring Degrees of Prototypicality",
    "text": "Instances of relations can often have different degrees of prototypicality. For example, for the relation “X characteristically makes the sound Y ”, the pair (dog,bark) should be considered more prototypical than the pair (floor,squeak), even though both pairs might be considered to be instances of the relation (Jurgens et al., 2012). A suitable relation vector should allow us to rank word pairs according to how prototypical they are as instances of that relation. We evaluate this ability using a dataset that was produced in the aftermath of SemEval 2012 Task 2. In particular, we have used the “Phase2AnswerScaled” data from the platinum rankings dataset, which is available from the SemEval 2012 Task 2 website5. In this dataset, 79 ranked list of word pairs are provided, each of which corresponds to a particular relation. For each relation, we first split the associated ranking into 60% training, 20% tuning, and 20% testing (i.e. we randomly select 60% of the word pairs and use their ranking as training data, and similar for tuning and test data). We then train a linear SVM regression model on the ranked word pairs. Note that this task slightly differs from the task that was considered at SemEval 2012, to allow us to use an SVM based model for consistency with the rest of the paper.\nWe report results using Spearman’s ρ in Table 4. Our model again outperforms the baselines, with R2ik again being the best variant. Interestingly, in this case, the Avg baseline is considerably stronger than Diff and Conc. Intuitively, we might indeed expect that this ranking problem requires a more fine-grained representation than the relation induction setting. Note that the Diff representations were found to achieve near state-of-theart performance on a closely related task in (Zhila et al., 2013). The only model that was found to perform (slightly) better was a hybrid model, combining Diff representations with linguistic patterns\n5https://sites.google.com/site/semeval2012task2/download\n(inspired by (Rink and Harabagiu, 2012)) and lexical databases, among others."
  }, {
    "heading": "5.3 Relation Extraction",
    "text": "Finally, we consider the problem of relation extraction from a text corpus. Specifically, we consider the task proposed in (Riedel et al., 2010), which is to extract (subject,predicate,object) triples from the New York Times (NYT) corpus. Rather than having labelled sentences as training data, the task requires using the existing triples from Freebase as a form of distant supervision, i.e. for some pairs of entities we know some of the relations that hold between them, but not which sentences assert these relationships (if any). To be consistent with published results for this task, we have used a word embedding that was trained from the NYT corpus6, rather than Wikipedia (using the same preprocessing and set-up). We have used the training and test data that was shared publicly for this task7, which consist of sentences from articles published in 2005-2006 and in 2007, respectively. Each of these sentences contains two entities, which are already linked to Freebase. We learn relation vectors from the sentences in the training and test sets, and learn a linear SVM classifier based on the Freebase triples that are available in the training set. Initially, we split the training data into 75% training and 25% tuning to find the optimal parameters of the linear SVM model. We tuned the parameters for each test fold sepa-\n6https://catalog.ldc.upenn.edu/LDC2008T19 7http://iesl.cs.umass.edu/riedel/ecml/\nrately. For each test fold, we used 25% of the 9 training folds as tuning data. After the optimal parameters have been determined, we retrain the model on the full training data, and apply it on the test fold. We used this approach (rather than e.g. fixing a train/tune/test split) because the total number of examples for some of the relations is very small. After tuning, we re-train the SVM models on the full training data. As the number of training examples is larger for this task, we also consider SVMs with a quadratic kernel.\nFollowing earlier work on this task, we report our results on the test set as a precisionrecall graph in Figure 1. This shows that the best performance is again achieved by R2ik, especially for larger recall values. Furthermore, using a quadratic kernel (only shown for R2ik) outperforms the linear SVM models. Note that the differences between the baselines are more pronounced in this task, with Avg being clearly better than Diff, which is in turn better than Conc. For this relation extraction task, a large number of methods have already been proposed in the literature, with variants of convolutional neural network models with attention mechanisms achieving state-of-the-art performance8. A comparison with these models9 is shown in Figure 2. The performance of R2ik is comparable with the state-of-\n8Note that such models would not be suitable for the evaluation tasks in Sections 5.1 and 5.2, due to the very limited number of training examples.\n9Results for the neural network models have been obtained from https://github.com/thunlp/ TensorFlow-NRE/tree/master/data.\nthe-art PCNN+ATT model (Lin et al., 2016), outperforming it for larger recall values. This is remarkable, as our model is conceptually much simpler, and has not been specifically tuned for this task. For instance, it could easily be improved by incorporating the attention mechanism from the PCNN+ATT model to focus the relation vectors on the considered task. Similarly, we could consider a supervised variant of (3), in which a learned relation-specific weight is added to each term."
  }, {
    "heading": "6 Conclusions",
    "text": "We have proposed an unsupervised method which uses co-occurrences statistics to represent the relationship between a given pair of words as a vector. In contrast to neural network models for relation extraction, our model learns relation vectors in an unsupervised way, which means that it can be used for measuring relational similarities and related tasks. Moreover, even in (distantly) supervised tasks (where we need to learn a classifier on top of the unsupervised relation vectors), our model has proven competitive with state-of-the-art neural network models. Compared to approaches that rely on averaging word vectors, our method is able to learn more faithful representations by focusing on the words that are most strongly related to the considered relationship."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by ERC Starting Grant 637277. Experiments in this work were performed using the computational facilities of the Advanced Research Computing at Cardiff (ARCCA) Division, Cardiff University and the ICARUS computational facility from Information Services, at the University of Kent."
  }],
  "year": 2018,
  "references": [{
    "title": "Snowball: Extracting relations from large plain-text collections",
    "authors": ["Eugene Agichtein", "Luis Gravano."],
    "venue": "Proceedings of the Fifth ACM Conference on Digital libraries. pages 85–94.",
    "year": 2000
  }, {
    "title": "Open information extraction from the web",
    "authors": ["Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."],
    "venue": "Proc. IJCAI. pages 2670–2676.",
    "year": 2007
  }, {
    "title": "Entailment above the word level in distributional semantics",
    "authors": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."],
    "venue": "Proc. EACL. pages 23–32.",
    "year": 2012
  }, {
    "title": "Learning structured embeddings of knowledge bases",
    "authors": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio."],
    "venue": "AAAI.",
    "year": 2011
  }, {
    "title": "Extracting patterns and relations from the world wide web",
    "authors": ["Sergey Brin."],
    "venue": "International Workshop on The World Wide Web and Databases. pages 172–183.",
    "year": 1998
  }, {
    "title": "Toward an architecture for neverending language learning",
    "authors": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam .R. Hruschka Jr.", "Tom M. Mitchell."],
    "venue": "Proc. AAAI. pages 1306–1313.",
    "year": 2010
  }, {
    "title": "The automatic content extraction (ACE) program - tasks, data, and evaluation",
    "authors": ["George R. Doddington", "Alexis Mitchell", "Mark A. Przybocki", "Lance A. Ramshaw", "Stephanie Strassel", "Ralph M. Weischedel."],
    "venue": "Proc. LREC.",
    "year": 2004
  }, {
    "title": "Classifying relations by ranking with convolutional neural networks",
    "authors": ["Cı́cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"],
    "venue": "In Proc. ACL",
    "year": 2015
  }, {
    "title": "Word embeddings, analogies, and machine learning: Beyond king - man + woman = queen",
    "authors": ["Aleksandr Drozd", "Anna Gladkova", "Satoshi Matsuoka."],
    "venue": "Proc. COLING. pages 3519–3530.",
    "year": 2016
  }, {
    "title": "Jointly embedding relations and mentions for knowledge population",
    "authors": ["Miao Fan", "Kai Cao", "Yifan He", "Ralph Grishman."],
    "venue": "Proc. RANLP. pages 186– 191.",
    "year": 2015
  }, {
    "title": "Task-oriented learning of word embeddings for semantic relation classification",
    "authors": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."],
    "venue": "Proc. CoNLL. pages 268–278.",
    "year": 2015
  }, {
    "title": "Semeval-2010 task 8: Multi-way classification of semantic relations",
    "authors": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"],
    "year": 2010
  }, {
    "title": "Learning distributed representations of sentences from unlabelled data",
    "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."],
    "venue": "Proc. NAACL-HLT . pages 1367–1377.",
    "year": 2016
  }, {
    "title": "Handbook of natural language processing, volume 2",
    "authors": ["Nitin Indurkhya", "Fred J Damerau."],
    "venue": "CRC Press.",
    "year": 2010
  }, {
    "title": "D-GloVe: A feasible least squares model for estimating word embedding densities",
    "authors": ["Shoaib Jameel", "Steven Schockaert."],
    "venue": "Proc. COLING. pages 1849–1860.",
    "year": 2016
  }, {
    "title": "Semeval-2012 task 2: Measuring degrees of relational similarity",
    "authors": ["David A Jurgens", "Peter D Turney", "Saif M Mohammad", "Keith J Holyoak."],
    "venue": "Proc. *SEM. pages 356–364.",
    "year": 2012
  }, {
    "title": "Siamese CBOW: optimizing word embeddings for sentence representations",
    "authors": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc Le", "Tomas Mikolov."],
    "venue": "Proc. ICML. pages 1188–1196.",
    "year": 2014
  }, {
    "title": "Neural relation extraction with selective attention over instances",
    "authors": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proc. ICLR.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."],
    "venue": "Proc. NIPS. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Distant supervision for relation extraction without labeled data",
    "authors": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."],
    "venue": "Proc. ACL. pages 1003–1011.",
    "year": 2009
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive Science 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proc. EMNLP. pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Modeling relations and their mentions without labeled text",
    "authors": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."],
    "venue": "Proc. ECML/PKDD. pages 148– 163.",
    "year": 2010
  }, {
    "title": "UTD: Determining relational similarity using lexical patterns",
    "authors": ["Bryan Rink", "Sanda Harabagiu."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics. pages 413–418.",
    "year": 2012
  }, {
    "title": "Preemptive information extraction using unrestricted relation discovery",
    "authors": ["Yusuke Shinyama", "Satoshi Sekine."],
    "venue": "Proc. NAACL-HLT . pages 304– 311.",
    "year": 2006
  }, {
    "title": "Semantic compositionality through recursive matrix-vector spaces",
    "authors": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proc. EMNLP. pages 1201–1211.",
    "year": 2012
  }, {
    "title": "Two multivariate generalizations of pointwise mutual information",
    "authors": ["Tim Van de Cruys."],
    "venue": "Proceedings of the Workshop on Distributional Semantics and Compositionality. pages 16–20.",
    "year": 2011
  }, {
    "title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning",
    "authors": ["Ekaterina Vylomova", "Laura Rimell", "Trevor Cohn", "Timothy Baldwin."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Knowledge graph and text jointly embedding",
    "authors": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen."],
    "venue": "EMNLP. pages 1591–1601.",
    "year": 2014
  }, {
    "title": "Knowledge graph embedding by translating on hyperplanes",
    "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."],
    "venue": "AAAI. pages 1112– 1119.",
    "year": 2014
  }, {
    "title": "Connecting language and knowledge bases with embedding models for relation extraction",
    "authors": ["Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier."],
    "venue": "Proc. EMNLP. pages 1366– 1371.",
    "year": 2013
  }, {
    "title": "Classifying relations via long short term memory networks along shortest dependency paths",
    "authors": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."],
    "venue": "Proc. EMNLP. pages 1785–1794.",
    "year": 2015
  }, {
    "title": "Relation classification via convolutional deep neural network",
    "authors": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"],
    "venue": "In Proc. COLING",
    "year": 2014
  }, {
    "title": "Combining heterogeneous models for measuring relational similarity",
    "authors": ["Alisa Zhila", "Wen-tau Yih", "Christopher Meek", "Geoffrey Zweig", "Tomas Mikolov."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for",
    "year": 2013
  }],
  "id": "SP:08946229229440a014f84a9c4a132311cc676df0",
  "authors": [{
    "name": "Shoaib Jameel",
    "affiliations": []
  }, {
    "name": "Zied Bouraoui",
    "affiliations": []
  }, {
    "name": "Steven Schockaert",
    "affiliations": []
  }],
  "abstractText": "Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that cooccurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.",
  "title": "Unsupervised Learning of Distributional Relation Vectors"
}