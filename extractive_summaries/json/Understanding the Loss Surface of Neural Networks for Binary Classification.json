{
  "sections": [{
    "heading": "1 INTRODUCTION",
    "text": "Local search algorithms like stochastic gradient descent (Bottou, 2010) or variants have gained huge success in training deep neural networks (see, (Krizhevsky et al., 2012; Goodfellow et al., 2013; Wan et al., 2013), for example). Despite the spurious saddle points and local minima on the loss surface (Dauphin et al., 2014), it has been widely conjectured that all local minima of the empirical loss lead to similar training performance (LeCun et al., 2015; Choromanska et al., 2015).\nIn the setting of regression problems, theoretical justifications (Andoni et al., 2014; Sedghi & Anandkumar, 2014; Janzamin et al., 2015; Haeffele & Vidal, 2015; Gautier et al., 2016; Brutzkus & Globerson, 2017; Soltanolkotabi, 2017; Soudry & Hoffer, 2017; Goel & Klivans, 2017; Du et al., 2017; Zhong et al., 2017; Li & Yuan, 2017; Baldi & Hornik, 1989; Kawaguchi, 2016; Freeman & Bruna, 2016; Hardt & Ma, 2017; Yun et al., 2017; Nguyen & Hein, 2017a;b) has been established to support the conjecture that all local minima lead to similar training performance. Although the loss surfaces in regression tasks have been well studied, the theoretical understanding of loss surfaces in classification tasks is still limited. Nguyen & Hein (2017b), Boob & Lan (2017) and Soltanolkotabi et al. (2017) treat the classification problem as the regression problem by using quadratic loss, and show that (almost) all local minima are global minima. However, the global minimum of the quadratic loss does not necessarily have zero misclassification error even in the simplest cases (e.g., every global minimum of quadratic loss can have non-zero misclassification error even when the dataset is linearly separable and the network is a linear network). This issue was mentioned in (Nguyen & Hein, 2017a) and a different loss function was used, but their result only studied the linearly separable case and a subset of the critical points.\nIn view of the prior work, the context and contributions of our paper are as follows:\n• Prior work on quadratic and related loss functions suggest that one can achieve zero misclassification error at all local minima by overparameterizing the neural network. The reason for over-parameterization is that the quadratic loss function tries to match the output of the neural network to the label of each training sample.\n• On the other hand, hinge loss-type functions only try to match the sign of the outputs with the labels. So it may be possible to achieve zero misclassification error without over-\nparametrization. We provide conditions under which the misclassification error of single layered neural networks is zero at all local minima for hinge-loss functions.\n• Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function is a smooth version of the hinge loss function.\n• We provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold.\n• We establish our results under the assumption that either the dataset is linearly separable or the positively and negatively labeled samples are located on different subspaces. Whether this assumption is necessary is an open problem.\nThe outline of this paper is as follows. In Section 2, we present the necessary definitions. In Section 3, we present the main results and discuss the impact of loss functions on the main results. Conclusions are presented in Section 4. A longer version of this paper will appear on arXiv."
  }, {
    "heading": "2 PRELIMINARIES",
    "text": "Network models. Given an input x of dimension d, we consider a single layered network with a single output for binary classification, i.e., f(x;θ) = a0 + a⊤σ(W⊤x), where scalar a0 denotes the bias, vector a ∈ RM denotes the weight vector, W ∈ Rd×M denotes the weight matrix, integer M denotes the number of neurons and vector θ denotes the parameterization of the network. We make the following assumption on the neural activation function.\nAssumption 1 Assume that neurons σ in the network f are real analytic and satisfy σ′′(z) > 0 for all z ∈ R.\nHere, we list a few neurons which can be used in the network: softplus neuron, i.e., σ(z) = log2(1+ ez), quadratic neuron, i.e, σ(z) = z2, etc.\nData distribution. In this paper, we consider binary classification tasks where each sample (X, Y ) ∈ Rd × {−1, 1} is drawn from an underlying data distribution PX×Y defined on Rd×{−1, 1}. The sample (X, Y ) is considered positive if Y = 1, and negative otherwise. Let E = {e1, ..., ed} denote a set of orthonormal basis on the space Rd. Let U+ and U− denote two subsets of E such that all positive and negative samples are located on the linear span of the set U+ and U−, respectively, i.e., PX|Y (X ∈ Span(U+)|Y = 1) = 1 and PX|Y (X ∈ Span(U−)|Y = −1) = 1. Let r denote the size of the set U+ ∪ U−, r+ denote the size of the set U+ and r− denote the size of the set U−, respectively. Assumption 2 Assume that for random vectors X1, ...,Xr+ independently drawn from the distribution PX|Y=1 and Z1, ...,Zr− independently drawn from the distribution PX|Y=−1, matrices( X1, ...,Xr+ ) ∈ Rr+×d and ( Z1, ...,Zr− ) ∈ Rr−×d are full rank matrices with probability one.\nAssumption 2 states that support of the conditional distribution PX|Y=1 is sufficiently rich so that r+ samples drawn from it will be linearly independent. In other words, by stating this assumption, we are avoiding trivial cases where all the positively labeled points are located in a very small subset of the linear span of U+. Similarly for the negatively labeled samples. Assumption 3 Assume |U+ ∪ U−| > max{|U+|, |U−|}, i.e., r > max{r+, r−}.\nAssumption 3 assumes that the positive and negative samples are not located on the same linear subspace. Previous works (Belhumeur et al., 1997; Chennubhotla & Jepson, 2001; Cootes et al., 2001; Belhumeur et al., 1997) have observed that some classes of natural images (e.g., images of faces, handwritten digits, etc) can be reconstructed from lower-dimensional representations. For example, using dimensionality reduction methods such as PCA, one can approximately reconstruct the original image from only a small number of principal components (Belhumeur et al., 1997; Chennubhotla & Jepson, 2001). Here, Assumption 3 states that both the positively and negatively labeled samples have lower-dimensional representations, and they do not exist in the same lowerdimensional subspace.\nLoss and error. Let D = {(xi, yi)}ni=1 denote a dataset with n samples, each independently drawn from the distribution PX×Y . In this paper, we consider the following loss function. Assumption 4 Assume that the loss function is ℓp(z) = [max{z + 1, 0}]p+1, where p ∈ N.\nGiven a neural network f(x;θ) parameterized by θ, in binary classification tasks, we define the empirical loss L̂n(θ) as the average loss of the network f on a sample in the dataset D, i.e., L̂n(θ; p) = 1 n ∑n i=1 ℓp(−yif(xi;θ)). We define the training error (also called the misclassification error) R̂n(θ) as the misclassification rate of the neural network f(x;θ) on the dataset D, i.e., R̂n(θ) = 1n ∑n i=1 I{yi ̸= sgn(f(xi;θ))}, where I{·} is the indicator function and sgn is the sign function, i.e., sgn(z) = 1 if z ≥ 0 and sgn(z) = −1, otherwise."
  }, {
    "heading": "3 MAIN RESULTS AND DISCUSSIONS",
    "text": "In this section, we present the following theorem to show that when assumptions 1-4 are satisfied, every local minimum of the empirical loss function has zero training error if the number of neurons in the network fS are chosen appropriately.\nTheorem 1 Suppose that assumptions 1-4 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n ≥ 1 are independently drawn from the distribution PX×Y . Assume that the number of neurons M in the network f satisfies M ≥ 2max{ n∆r , r+, r−}, where ∆r = r −max{r+, r−}. If θ∗ is a local minimum of the loss function L̂n(θ; p) and p ≥ 6, then R̂n(θ∗) = 0 holds with probability one.\nRemark: The positiveness of ∆r is guaranteed by Assumption 3. In the worst case (e.g., ∆r = 1 and ∆r = 2), the number of neurons needs to be at least greater than the number of samples, i.e., M ≥ n. However, when the two orthonormal basis sets U+ and U− differ significantly (i.e., ∆r ≫ 1), the number of neurons required by Theorem 1 can be significantly smaller than the number of samples (i.e., n ≫ 2n/∆r). In fact, we can show that, when the neuron has quadratic activation function σ(z) = z2, the assumption M ≥ 2n/∆r can be further relaxed such that the number of neurons is independent of the number of samples.\nQuadratic loss. The quadratic loss ℓ(z) = (1 − z)2 has been well-studied in prior works. It has been shown that when the loss function is quadratic, under certain assumptions, all local minima of the empirical loss are global minima. However, the global minimum of the quadratic loss does not necessarily have zero misclassification error, even in the realizable case (i.e., the case where there exists a set of parameters such that the network achieves zero misclassification error on the dataset or the data distribution). To illustrate this, we provide an example in Appendix A and show that, when the loss function is replaced with quadratic loss, even if the other conditions in Theorem 1 are satisfied, every global minimum of the empirical loss has a training error larger than 1/8 with a positive probability. In other words, our main results do hold for the quadratic loss.\nLogistic loss. The logistic loss ℓ(z) = log2 (1 + ez) is different from the loss functions conditioned in Assumption 4, since the logistic loss does not have a global minimum on R. Here, for the logistic loss function, we show that even if the remaining assumptions in Theorem 1 hold, every critical point is a saddle point. In other words, Theorem 1 does not hold for logistic loss.\nProposition 1 Assume that the loss function is the logistic loss, i.e., ℓ(z) = log2(1 + ez). Assume that assumptions 2-1 are satisfied. Assume that samples in the dataset D = {(xi, yi)}ni=1, n ≥ 1 are independently drawn from the distribution PX×Y . Assume that the number of neurons M in the network f satisfies M ≥ 2max{ n∆r , r+, r−}, where ∆r = r −max{r+, r−}. If θ\n∗ denotes a critical point of the empirical loss L̂n(θ), then θ∗ is a saddle point. In particular, there are no local minima.\nIn addition, we note here that when the neurons are replaced with rectified linear units (ReLUs), leaky rectified linear units (Leaky-ReLU) and sigmoid neurons, Theorem 1 does not hold."
  }, {
    "heading": "4 CONCLUSIONS",
    "text": "In this paper, we studied the surface of a smooth version of the hinge loss function in binary classification problems. We provided conditions under which the neural network has zero misclassification error at all local minima and also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold. Further work involves exploiting our results to design efficient training algorithms classification tasks using neural networks."
  }, {
    "heading": "A EXAMPLE",
    "text": "Example 1 Let the distribution PX×Y defined on R2 × {−1, 1} satisfy that P(Y = 1) = P(Y = −1) = 0.5, P(X = (α, 0)|Y = 1) = P(X = (1, 0)|Y = 1) = 0.5 and P(X = (0, α)|Y = −1) = P(X = (0, 1)|Y = −1) = 0.5. Assume that samples in the dataset D = {(xi, yi)}4ni=1 are independently drawn from the distribution PX×Y . Assume that the single layered network f has M ≥ 2 neurons and all neurons in the network f are quadratic neurons, i.e., σ(z) = z2. Then there exists an α ∈ [0, 1] such that every global minimum of the empirical loss function L̂4n(θ) = 1 4n ∑4n i=1(1 − yif(xi;θ))2 has a training error greater than 1/8 with probability at least Ω(1/n3).\nRemark: This is a counterexample for Theorem 1. It is easy to check that the distribution satisfies assumption 2 and 3, where r = 2 > max{1, 1} = max{r+, r−}."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning polynomials with neural networks",
    "authors": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Neural networks and principal component analysis: Learning from examples without local minima",
    "authors": ["P. Baldi", "K. Hornik"],
    "venue": "Neural networks,",
    "year": 1989
  }, {
    "title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection",
    "authors": ["P.N. Belhumeur", "J. P Hespanha", "D.J. Kriegman"],
    "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
    "year": 1997
  }, {
    "title": "Theoretical properties of the global optimizer of two layer neural network",
    "authors": ["D. Boob", "G. Lan"],
    "venue": "arXiv preprint arXiv:1710.11241,",
    "year": 2017
  }, {
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["L. Bottou"],
    "venue": "In Proceedings of COMPSTAT’2010,",
    "year": 2010
  }, {
    "title": "Globally optimal gradient descent for a convnet with gaussian inputs",
    "authors": ["A. Brutzkus", "A. Globerson"],
    "venue": "arXiv preprint arXiv:1702.07966,",
    "year": 2017
  }, {
    "title": "Sparse pca. extracting multi-scale structure from data",
    "authors": ["C. Chennubhotla", "A. Jepson"],
    "venue": "In ICCV,",
    "year": 2001
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Arous", "Y. LeCun"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Active appearance models",
    "authors": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"],
    "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
    "year": 2001
  }, {
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "authors": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "When is a convolutional filter easy to learn",
    "authors": ["S.S. Du", "J.D. Lee", "Y. Tian"],
    "venue": "arXiv preprint arXiv:1709.06129,",
    "year": 2017
  }, {
    "title": "Topology and geometry of half-rectified network optimization",
    "authors": ["C D. Freeman", "J. Bruna"],
    "year": 2016
  }, {
    "title": "Globally optimal training of generalized polynomial neural networks with nonlinear spectral methods",
    "authors": ["A. Gautier", "Q.N. Nguyen", "M. Hein"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Learning depth-three neural networks in polynomial time",
    "authors": ["S. Goel", "A. Klivans"],
    "venue": "arXiv preprint arXiv:1709.06010,",
    "year": 2017
  }, {
    "title": "Global optimality in tensor factorization, deep learning, and beyond",
    "authors": ["B. D Haeffele", "R. Vidal"],
    "venue": "arXiv preprint arXiv:1506.07540,",
    "year": 2015
  }, {
    "title": "Identity matters in deep learning",
    "authors": ["M. Hardt", "T. Ma"],
    "year": 2017
  }, {
    "title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
    "authors": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"],
    "venue": "arXiv preprint arXiv:1506.08473,",
    "year": 2015
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Convergence analysis of two-layer neural networks with relu activation",
    "authors": ["Y. Li", "Y. Yuan"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Convergent learning: Do different neural networks learn the same representations",
    "authors": ["Y. Li", "J. Yosinski", "J. Clune", "H. Lipson", "J. Hopcroft"],
    "venue": "arXiv preprint arXiv:1511.07543,",
    "year": 2015
  }, {
    "title": "The loss surface and expressivity of deep convolutional neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "arXiv preprint arXiv:1710.10928,",
    "year": 2017
  }, {
    "title": "The loss surface of deep and wide neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "arXiv preprint arXiv:1704.08045,",
    "year": 2017
  }, {
    "title": "Provable methods for training neural networks with sparse connectivity",
    "authors": ["H. Sedghi", "A. Anandkumar"],
    "venue": "arXiv preprint arXiv:1412.2693,",
    "year": 2014
  }, {
    "title": "Learning relus via gradient descent",
    "authors": ["M. Soltanolkotabi"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks",
    "authors": ["M. Soltanolkotabi", "A. Javanmard", "J.D. Lee"],
    "venue": "arXiv preprint arXiv:1707.04926,",
    "year": 2017
  }, {
    "title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks",
    "authors": ["D. Soudry", "E. Hoffer"],
    "venue": "arXiv preprint arXiv:1702.05777,",
    "year": 2017
  }, {
    "title": "Regularization of neural networks using dropconnect",
    "authors": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. Le Cun", "R. Fergus"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Global optimality conditions for deep neural networks",
    "authors": ["C. Yun", "S. Sra", "A. Jadbabaie"],
    "venue": "arXiv preprint arXiv:1707.02444,",
    "year": 2017
  }, {
    "title": "Recovery guarantees for one-hidden-layer neural networks",
    "authors": ["K. Zhong", "Z. Song", "P. Jain", "P. L Bartlett", "I. S Dhillon"],
    "venue": "arXiv preprint arXiv:1706.03175,",
    "year": 2017
  }],
  "id": "SP:33356e7766a6afe94288c77699b70a9eb18fc9a9",
  "authors": [{
    "name": "BINARY CLASSI",
    "affiliations": []
  }, {
    "name": "Shiyu Liang",
    "affiliations": []
  }, {
    "name": "Ruoyu Sun",
    "affiliations": []
  }, {
    "name": "R. Srikant",
    "affiliations": []
  }, {
    "name": "Yixuan Li",
    "affiliations": []
  }],
  "abstractText": "It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of single-layered neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of a smooth hinge loss function. Our conditions are roughly in the following form: the neurons have to be strictly convex and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that when the loss function is replaced with quadratic loss or logistic loss, the result may not hold."
}