{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.\nRecursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction.\nOur goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models."
  }, {
    "heading": "1 Introduction",
    "text": "Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text.\nFor tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models:\nRecurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order.\nRecursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014).\n2304\nOne possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). However we do not know if this advantage is truly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful.\nOn the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset.\nOur goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties.\n• Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive.\n• Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether pars-\ning is useful for finding similarities between question sentences and target phrases.\n• Semantic Relation Classification on the SemEval-2010 (Hendrickx et al., 2009) data can help understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.\n• Discourse parsing (RST dataset) is useful for measuring the extent to which parsing improves discourse tasks that need to combine meanings of larger text units. Discourse parsing treats elementary discourse units (EDUs) as basic units to operate on, which are usually short clauses. The task also sheds light on the extent to which syntactic structures help acquire shot text representations.\nThe principal motivation for this paper is to understand better when, and why, recursive models are needed to outperform simpler models by enforcing apples-to-apples comparison as much as possible. This paper applies existing models to existing tasks, barely offering novel algorithms or tasks. Our goal is rather an analytic one, to investigate different versions of recursive and recurrent models. This work helps understand the limitations of both classes of models, and suggest directions for improving recurrent models.\nThe rest of this paper organized as follows: We detail versions of recursive/recurrent models in Section 2, present the tasks and results in Section 3, and conclude with discussions in Section 4."
  }, {
    "heading": "2 Recursive and Recurrent Models",
    "text": ""
  }, {
    "heading": "2.1 Notations",
    "text": "We assume that the text unit S, which could be a phrase, a sentence or a document, is comprised of a sequence of tokens/words: S = {w1, w2, ..., wNS}, where Ns denotes the number of tokens in S. Each word w is associated with a K-dimensional vector embedding ew = {e1w, e2w, ..., eKw }. The goal of recursive and recurrent models is to map the sequence to a Kdimensional eS , based on its tokens and their correspondent embeddings.\nStandard Recurrent/Sequence Models successively take word wt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the re-\nsulting current embedding ht, and passes it to the next step. The embedding ht for the current time t is thus:\nht = f(W · ht−1 + V · et) (1)\nwhereW and V denote compositional matrices. If Ns denotes the length of the sequence, hNs represents the whole sequence S.\nStandard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order. It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη as follows:\neη = f(W · eηleft + V · eηright) (2)\nBidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly:\nh→t = f(W → · h→t−1 + V→ · et) h←t = f(W ← · h←t+1 + V← · et)\n(3)\nNormally, final representations for sentences can be achieved either by concatenating vectors calculated from both directions [e←1 , e→NS ] or using further compositional operation to preserve vector dimensionality\nht = f(WL · [h←t , h→t ]) (4)\nwhere WL denotes a K×2K dimensional matrix. Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot. We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. The vector representation ht for each time-step t is given by:\n it ft ot lt  =  σ σ σ tanh W · [ ht−1et ]\n(5)\nct = ft · ct−1 + it · lt (6) hst = ot · ct (7)\nwhere W ∈ R4K×2K . Labels at the phrase/sentence level are predicted representations outputted from the last time step.\nTree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees.\nBi-directional LSTMs These combine bidirectional models and LSTMs."
  }, {
    "heading": "3 Experiments",
    "text": "In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks.\n• Binary sentiment classification on the Pang et al. (2002) dataset. This addresses the issues where supervision only appears globally after a long sequence of operations.\n• Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned.\n• Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models.\n• Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially.\n• Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations.\nIn each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows:\n• Standard tree models vs standard sequence models vs standard bi-directional sequence models\n• LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models.\nWe employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011). Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated.\nFor settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994). Test scores that achieve significance level of 0.05 are marked by an asterisk (*)."
  }, {
    "heading": "3.1 Stanford Sentiment TreeBank",
    "text": "Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013). This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words.\nOf course, any conclusions drawn from implementing sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected. Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations.\nFor recursive models, we followed the protocols in Socher et al. (2013) where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier. We transformed the dataset for recurrent model use as illustrated in Figure 1. Each phrase is reconstructed from parse tree nodes and treated as a separate data point. As the treebank contains 11,855\nsentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances).\nResults are shown in Table 1 and 21. When comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bisequences), but yields no significant improvement at the phrase level.\nLSTM Tai et al. (2015) discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models. We explore this task a bit more by training deeper and more sophisticated models. We examine the following three models:\n1. Tree-structured LSTM models (Tai et al., 2015)2.\n2. Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence.\n3. Deep Bi-LSTM hierarchical sequence models (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of subsentences by using a look-up table of punctuations (i.e., comma, period, question mark\n1The performance of our implementations of recursive models is not exactly identical to that reported in Socher et al. (2013), but the relative difference is around 1% to 2%.\n2Tai et al.. achieved 0.510 accuracy in terms of finegrained evaluation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504).\nand exclamation mark). The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to join the subsentences. Illustrations are shown in Figure2.\nWe consider the third model because the dataset used in Tai et al. (2015) contains long sentences and the evaluation is performed only at the sentence root level. Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advantage could be approximated by using punctuationbased approximations to clause boundaries.\nWe run 15 iterations for each algorithm. Parameters are harvested at the end of each iteration; those performing best on the development set are used on the test set. The whole process takes roughly 15-20 minutes on a single GPU machine3. For a more convincing comparison, we did not use the bootstrap test where parallel examples are generated from one same dataset. Instead, we repeated the aforementioned procedure for each algorithm 20 times and report accuracies\n3Tesla K40m, 2880 Cuda cores.\nwith standard deviation in Table 3.\nTree LSTMs are equivalent or marginally better than standard bi-directional sequence model (two-tailed p-value equals 0.041*, and only at the root level, with p-value for the phrase level at 0.376). The hierarchical sequence model achieves the same performance with a p-value of 0.198.\nDiscussion The results above suggest that clausal segmentation of long sentences offers a slight performance boost, a result also supported by the fact that very little difference exists between the three models for phrase-level sentiment evaluation. Clausal segmentation of long sentences thus provides a simple approximation to parse-tree based models.\nWe suggest a few reasons for this slightly better performances introduced by clausal segmentation:\n1. Treating clauses as basic units (to the extent that punctuation approximates clauses) preserves the semantic structure of text.\n2. Semantic compositions such as negations or conjunctions usually appear at the clause level. Working on clauses individually and then combining them model inter-clause compositions.\n3. Errors are back-propagated to individual tokens using fewer steps in hierarchical models than in standard models. Consider a movie\nreview “simple as the plot was , i still like it a lot”. With standard recurrent models it takes 12 steps before the prediction error gets back to the first token “simple”:\nerror→lot→a→it→like→still→i→,→was →plot→ the→as→simple In a hierarchical model, the second clause is compacted into one component, and the error propagation is thus given by:\nerror→ second-clause → first-clause → was→plot→the→as→simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance."
  }, {
    "heading": "3.2 Binary Sentiment Classification (Pang)",
    "text": "Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4. Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below:\nDiscussion Why don’t parse trees help on this task? One possible explanation is the distance\n4https://code.google.com/p/word2vec/\nof the supervision signal from the local compositional structure. The Pang et al. dataset has an average sentence length of 22.5 words, which means it takes multiple steps before sentiment related evidence comes up to the surface. It is therefore unclear whether local compositional operators (such as negation) can be learned; there is only a small amount of training data (around 8,000 examples) and the sentiment supervision only at the level of the sentence may not be easy to propagate down to deeply buried local phrases."
  }, {
    "heading": "3.3 Question-Answer Matching",
    "text": "Task Description: In the question-answering dataset QANTA5, each answer is a token or short phrase. The task is different from standard generation focused QA task but formalized as a multiclass classification task that matches a source question with a candidates phrase from a predefined pool of candidate phrases We give an illustrative example here:\nQuestion: He left unfinished a novel whose title character forges his father’s signature to get out of school and avoids the draft by feigning desire to join. Name this German author of The Magic Mountain and Death in Venice.\nAnswer: Thomas Mann from the pool of phrases. Other candidates might include George Washington, Charlie Chaplin, etc.\nThe model of Iyyer et al. (2014) minimizes the distances between answer embeddings and node embeddings along the parse tree of the question. Concretely, let c denote the correct answer to question S, with embedding ~c, and z denoting any random wrong answer. The objective function sums over the dot product between representation for every node η along the question parse trees and the answer representations:\nL = ∑\nη∈[parse tree] ∑ z max(0, 1−~c ·eη+~z ·eη) (8)\n5http://cs.umd.edu/˜miyyer/qblearn/. Because the publicly released dataset is smaller than the version used in (Iyyer et al., 2014) due to privacy issues, our numbers are not comparable to those in (Iyyer et al., 2014).\nwhere eη denotes the embedding for parse tree node calculated from the recursive neural model. Here the parse trees are dependency parses following (Iyyer et al., 2014).\nBy adjusting the framework to recurrent models, we minimize the distance between the answer embedding and the embeddings calculated from each timestep t of the sequence:\nL = ∑\nt∈[1,Ns]\n∑ z max(0, 1− ~c · et + ~z · et) (9)\nAt test time, the model chooses the answer (from the set of candidates) that gives the lowest loss score. As can be seen from results presented in Table 5, the difference is only significant for the LSTM setting between the tree model and the sequence model; no significant difference is observed for other settings.\nDiscussion The UMD-QA task represents a group of situations where because we have insufficient supervision about matching (it’s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)).\nThe results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree models and different time steps for sequence models), components from sequence models are able to embed important information, despite the fact that sequence model components are just sentence fragments and hence usually not linguistically meaningful components in the way that parse tree constituents are."
  }, {
    "heading": "3.4 Semantic Relationship Classification",
    "text": "Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2”\nclassifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009; Socher et al. (2012) for details.\nFor the recursive implementations, we follow the neural framework defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier6. Retrieved paths are transformed for the recurrent models as shown in Figure 5.\nDiscussion Unlike for earlier tasks, here recursive models yield much better performance than the corresponding recurrent versions for all versions (e.g., standard tree vs. standard sequence, p = 0.004). These results suggest that it is the need to integrate structures far apart in the sentence that characterizes the tasks where recursive models surpass recurrent models. In parse-based models, the two target words are drawn together much earlier in the decision process than in recurrent models, which must remember one target until the other one appears."
  }, {
    "heading": "3.5 Discourse Parsing",
    "text": "Task Description: Our final task, discourse parsing based on the RST-DT corpus (Carlson et\n6(Socher et al., 2012) achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering. Again, because MV-RNN is difficult to adapt to a recurrent version, we do not employ this state-of-the-art model, adhering only to the general versions of recursive models described in Section 2, since our main goal is to compare equivalent recursive and recurrent models rather than implement the state of the art.\nal., 2003), is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs). Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compositional meaning than sentiment or nominal relations. See Hernault et al. (2010) for more details on discourse parsing and the RST-DT corpus.\nRepresentations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in Li et al. (2014). Related EDUs are then merged into a new EDU, the representation of which is obtained through an operation of neural composition based on the previous two related EDUs. This step is repeated until all units are merged.\nDiscourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sentences, with an average length of 7.2 words. Recursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for discourse parsing. We use this task for two reasons: (1) to illustrate whether syntactic parse trees are useful for acquiring representations for short clauses. (2) to measure the extent to which pars-\ning improves discourse tasks that need to combine the meanings of larger text units.\nModels are traditionally evaluated in terms of three metrics, i.e., spans7, nuclearity8, and identifying the rhetorical relation between two clauses. Due to space limits, we only focus the last one, rhetorical relation identification, because (1) relation labels are treated as correct only if spans and nuclearity are correctly labeled (2) relation identification between clauses offer more insights about model’s abilities to represent sentence semantics. In order to perform a plain comparison, no additional human-developed features are added.\nDiscussion We see no large differences between equivalent recurrent and recursive models. We suggest two possible explanations. (1) EDUs tend to be short; thus for some clauses, parsing might not change the order of operations on words. Even for those whose orders are changed by parse trees, the influence of short phrases on the final representation may not be great enough. (2) Unlike earlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs."
  }, {
    "heading": "4 Discussions and Conclusions",
    "text": "We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014).\nAs with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur-\n7on blank tree structures. 8on tree structures with nuclearity indication.\nsive/recurrent models rather than various sophisticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neural compositions—despite the fact that deep neural models with multiple layers give better results. Our conclusions might thus be limited to the algorithms employed in this paper, and it is unclear whether they can be extended to other variants or to the latest state-of-the-art. Second, in order to compare models “fairly”, we force every model to be trained exactly in the same way: AdaGrad with minibatches, same set of initializations, etc. However, this may not necessarily be the optimal way to train every model; different training strategies tailored for specific models may improve their performances. In that sense, our attempts to be “fair” in this paper may nevertheless be unfair.\nPace these caveats, our conclusions can be summarized as follows:\n• In tasks like semantic relation extraction, in which single headwords need to be associated across a long distance, recursive models shine. This suggests that for the many other kinds of tasks in which long-distance semantic dependencies play a role (e.g., translation between languages with significant reordering like Chinese-English translation), syntactic structures from recursive models may offer useful power.\n• Tree models tend to help more on long sequences than shorter ones with sufficient supervision: tree models slightly help root level identification on the Stanford Sentiment Treebank, but do not help much at the phrase level. Adopting bi-directional versions of recurrent models seem to largely bridge this gap, producing equivalent or sometimes better results.\n• On long sequences where supervision is not sufficient, e.g., in Pang at al.,’s dataset (supervision only exists on top of long sequences), no significant difference is observed between tree based and sequence based models.\n• In cases where tree-based models do well, a simple approximation to tree-based models\nseems to improve recurrent models to equivalent or almost equivalent performance: (1) break long sentences (on punctuation) into a series of clause-like units, (2) work on these clauses separately, and (3) join them together. This model sometimes works as well as tree models for the sentiment task, suggesting that one of the reasons tree models help is by breaking down long sentences into more manageable units.\n• Despite that the fact that components (outputs from different time steps) in recurrent models are not linguistically meaningful, they may do as well as linguistically meaningful phrases (represented by parse tree nodes) in embedding informative evidence, as demonstrated in UMD-QA task. Indeed, recent work in parallel with ours (Bowman et al., 2015) has shown that recurrent models like LSTMs can discover implicit recursive compositional structure."
  }, {
    "heading": "5 Acknowledgments",
    "text": "We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, advice, and suggestions. We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful advice on various aspects of this work. We acknowledge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully acknowledge support from an Enlight Foundation Graduate Fellowship, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040, and the NSF via award IIS-1514268. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Bloomberg L.P., DARPA, AFRL, NSF, or the US government."
  }],
  "year": 2015,
  "references": [{
    "title": "Neural machine translation by jointly 2312",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"],
    "year": 2014
  }, {
    "title": "Recursive neural networks for learning logical semantics",
    "authors": ["Samuel R Bowman", "Christopher Potts", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1406.1827.",
    "year": 2014
  }, {
    "title": "Tree-structured composition in neural networks without tree-structured architectures",
    "authors": ["Samuel R Bowman", "Christopher D Manning", "Christopher Potts."],
    "venue": "arXiv preprint arXiv:1506.04834.",
    "year": 2015
  }, {
    "title": "Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192",
    "authors": ["Samuel R Bowman"],
    "year": 2013
  }, {
    "title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory",
    "authors": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."],
    "venue": "Current and New Directions in Discourse and Dialogue Text, Speech and Language Technology. vol-",
    "year": 2003
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "The Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Better mini-batch algorithms via accelerated gradient methods",
    "authors": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan."],
    "venue": "Advances in Neural Information Processing Systems, pages 1647– 1655.",
    "year": 2011
  }, {
    "title": "Adaptive recursive neural network for target-dependent twitter sentiment classification",
    "authors": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-",
    "year": 2014
  }, {
    "title": "Neural networks for time series processing",
    "authors": ["Georg Dorffner."],
    "venue": "Neural Network World.",
    "year": 1996
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "An introduction to the bootstrap",
    "authors": ["Bradley Efron", "Robert J Tibshirani."],
    "venue": "CRC press.",
    "year": 1994
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L Elman."],
    "venue": "Cognitive science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "Learning task-dependent distributed representations by backpropagation through structure",
    "authors": ["Christoph Goller", "Andreas Kuchler."],
    "venue": "Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE.",
    "year": 1996
  }, {
    "title": "Offline handwriting recognition with multidimensional recurrent neural networks",
    "authors": ["Alex Graves", "Juergen Schmidhuber."],
    "venue": "Advances in Neural Information Processing Systems, pages 545–552.",
    "year": 2009
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645–6649. IEEE.",
    "year": 2013
  }, {
    "title": "Supervised sequence labeling with recurrent neural networks, In Studies in Computational Intelligence",
    "authors": ["Alex Graves."],
    "venue": "volume 385. Springer.",
    "year": 2012
  }, {
    "title": "Simple customization of recursive neural networks for semantic relation classification",
    "authors": ["Kazuma Hashimoto", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama."],
    "venue": "EMNLP, pages 1372– 1376.",
    "year": 2013
  }, {
    "title": "Semeval-2010 task 8: Multi-way classification of semantic relations",
    "authors": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"],
    "year": 2009
  }, {
    "title": "Hilda: a discourse parser using support vector machine classification",
    "authors": ["Hugo Hernault", "Helmut Prendinger", "Mitsuru Ishizuka."],
    "venue": "Dialogue & Discourse, 1(3).",
    "year": 2010
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Bidirectional recursive neural networks for token-level labeling with structure",
    "authors": ["Ozan Irsoy", "Claire Cardie."],
    "venue": "arXiv preprint arXiv:1312.0493.",
    "year": 2013
  }, {
    "title": "Deep recursive neural networks for compositionality in language",
    "authors": ["Ozan Irsoy", "Claire Cardie."],
    "venue": "Advances in Neural Information Processing Systems, pages 2096–2104.",
    "year": 2014
  }, {
    "title": "A neural network for factoid question answering over paragraphs",
    "authors": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daumé III."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    "year": 2014
  }, {
    "title": "Recurrent continuous translation models",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "EMNLP, pages 1700–1709.",
    "year": 2013
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc V Le", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1405.4053.",
    "year": 2014
  }, {
    "title": "A model of coherence based on distributed sentence representation",
    "authors": ["Jiwei Li", "Eduard Hovy."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    "year": 2014
  }, {
    "title": "Recursive deep models for discourse parsing",
    "authors": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods",
    "year": 2014
  }, {
    "title": "Review of neural networks for speech recognition",
    "authors": ["Richard P Lippmann."],
    "venue": "Neural computation, 1(1):1–38.",
    "year": 1989
  }, {
    "title": "Addressing the rare word problem in neural machine translation",
    "authors": ["Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."],
    "venue": "Proceedings of ACL. 2015.",
    "year": 2014
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."],
    "venue": "HLT-NAACL, pages 746– 751.",
    "year": 2013
  }, {
    "title": "Thumbs up?: sentiment classification using machine learning techniques",
    "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."],
    "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. As-",
    "year": 2002
  }, {
    "title": "Global belief recursive neural networks",
    "authors": ["Romain Paulus", "Richard Socher", "Christopher D Manning."],
    "venue": "Advances in Neural Information Processing Systems, pages 2888–2896.",
    "year": 2014
  }, {
    "title": "Learning state space trajectories in recurrent neural networks",
    "authors": ["Barak A Pearlmutter."],
    "venue": "Neural Computation, 1(2):263–269.",
    "year": 1989
  }, {
    "title": "The use of recurrent neural networks in continuous speech recognition",
    "authors": ["Tony Robinson", "Mike Hochberg", "Steve Renals."],
    "venue": "Automatic speech and speaker recognition, pages 233–258. Springer.",
    "year": 1996
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["Mike Schuster", "Kuldip K Paliwal."],
    "venue": "Signal Processing, IEEE Transactions on, 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
    "authors": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Advances in Neural Information Processing Systems, pages 801–809.",
    "year": 2011
  }, {
    "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-",
    "year": 2011
  }, {
    "title": "Semantic compositionality through recursive matrix-vector spaces",
    "authors": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
    "year": 2012
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the Conference on Em-",
    "year": 2013
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."],
    "venue": "Advances in Neural Information Processing Systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"],
    "year": 2015
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "arXiv preprint arXiv:1412.7449.",
    "year": 2014
  }, {
    "title": "Long short-term memory over recursive structures",
    "authors": ["Xiaodan Zhu", "Parinaz Sobihani", "Hongyu Guo."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1604–1612.",
    "year": 2015
  }],
  "id": "SP:c785227de60d4e21600691a642eb07e3f989365a",
  "authors": [{
    "name": "Jiwei Li",
    "affiliations": []
  }, {
    "name": "Minh-Thang Luong",
    "affiliations": []
  }, {
    "name": "Dan Jurafsky",
    "affiliations": []
  }, {
    "name": "Eduard Hovy",
    "affiliations": []
  }],
  "abstractText": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction. Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",
  "title": "When Are Tree Structures Necessary for Deep Learning of Representations?"
}