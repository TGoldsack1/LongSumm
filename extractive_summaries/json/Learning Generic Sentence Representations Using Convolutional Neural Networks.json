{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2390–2400 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Learning sentence representations is central to many natural language modeling applications. The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences. Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013). Most of these models are task-dependent: they are trained specifically for a certain task. However, these methods may be-\ncome inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task. In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains.\nSeveral approaches have been proposed for learning generic sentence embeddings. The paragraphvector model of Le and Mikolov (2014) incorporates a global context vector into the log-linear neural language model (Mikolov et al., 2013) to learn the sentence representation; however, at prediction time, one needs to perform gradient descent to compute a new vector. The sequence autoencoder of Dai and Le (2015) describes an encoder-decoder model to reconstruct the input sentence, while the skip-thought model of Kiros et al. (2015) extends the encoder-decoder model to reconstruct the surrounding sentences of an input sentence. Both the encoder and decoder of the methods above are modeled as RNNs.\nCNNs have recently achieved excellent results in various task-dependent natural language applications as the sentence encoder (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014). This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decoder models proposed by Sutskever et al. (2014); Cho et al. (2014). Specifically, a CNN encoder performs convolution and pooling operations on an input sentence, then uses a fullyconnected layer to produce a fixed-length encoding of the sentence. This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence. Depending on the task, we propose three models: (i) CNNLSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the intra-sentence information; (ii) CNN-LSTM future\n2390\npredictor: this model aims to predict a future sentence, by leveraging inter-sentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence. This composite model aims to learn a sentence encoder that captures both intra- and inter-sentence information.\nThe proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context. In order to capture longerterm dependencies between sentences, we further introduce a hierarchical encoder-decoder model. This model abstracts the RNN language model of Mikolov et al. (2010) to the sentence level. That is, instead of using the current word in a sentence to predict future words (sentence continuation), we encode a sentence to predict multiple future sentences (paragraph continuation). This model is termed hierarchical CNN-LSTM model.\nAs in Kiros et al. (2015), we first train our proposed models on a large collection of novels. We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN. We show that our trained sentence encoder yields generic repre-\nsentations that perform as well as, or better, than those of Kiros et al. (2015); Hill et al. (2016), in all the tasks considered.\nSummarizing, the main contribution of this paper is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for learning generic sentence representations. Inspired by the skip-thought model (Kiros et al., 2015), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context windows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for predicting multiple future sentences."
  }, {
    "heading": "2 Model description",
    "text": ""
  }, {
    "heading": "2.1 CNN-LSTM model",
    "text": "Consider the sentence pair (sx, sy). The encoder, a CNN, encodes the first sentence sx into a feature vector z, which is then fed into an LSTM decoder that predicts the second sentence sy. Let wtx ∈ {1, . . . , V } represent the t-th word in sentences sx, where wtx indexes one element in a V - dimensional set (vocabulary); wty is defined similarly w.r.t. sy. Each word wtx is embedded into a k-dimensional vector xt = We[wtx], where We ∈ Rk×V is a word embedding matrix (learned), and notation We[v] denotes the v-th column of matrix We. Similarly, we let yt = We[wty].\nCNN encoder The CNN architecture in Kim (2014); Collobert et al. (2011) is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map. A sentence of length T (padded where necessary) is represented as a matrix X ∈ Rk×T , by concatenating its word embeddings as columns, i.e., the t-th column of X is xt.\nA convolution operation involves a filter Wc ∈ Rk×h, applied to a window of h words to produce a new feature. According to Collobert et al. (2011), we can induce one feature map c = f(X ∗Wc + b) ∈ RT−h+1, where f(·) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b ∈ RT−h+1 is a bias vector, and ∗ denotes the convolutional operator. Convolving the same filter with the h-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence. We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.e., ĉ = max{c}, as the feature corresponding to this particular filter. This pooling scheme tries to capture the most important feature, i.e., the one with the highest value, for each feature map, effectively filtering out less informative compositions of words. Further, pooling also guarantees that the extracted features are independent of the length of the input sentence.\nThe above process describes how one feature is extracted from one filter. In practice, the model uses multiple filters with varying window sizes (Kim, 2014). Each filter can be considered as a linguistic feature detector that learns to recognize a specific class of n-grams (or h-grams, in the above notation). However, since the h-grams are computed in the embedding space, the model naturally handles similar h-grams composed of synonyms. Assume we have m window sizes, and for each window size, we use d filters; then we obtain a md-dimensional vector to represent a sentence.\nCompared with the LSTM encoders used in Kiros et al. (2015); Dai and Le (2015); Hill et al. (2016), a CNN encoder may have the following advantages. First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements (Goodfellow et al., 2016). For example, excluding the number of parameters used in the word embeddings, our trained CNN sentence encoder has 3 million parameters,\nwhile the skip-thought vector of Kiros et al. (2015) contains 40 million parameters. Second, a CNN is easy to implement in parallel over the whole sentence, while an LSTM needs sequential computation.\nLSTM decoder The CNN encoder maps sentence sx into a vector z. The probability of a length-T sentence sy given the encoded feature vector z is defined as\np(sy|z) = T∏\nt=1\np(wty|w0y, . . . , wt−1y , z) (1)\nwhere w0y is defined as a special start-of-thesentence token. All the words in the sentence are sequentially generated using the RNN, until the end-of-the-sentence symbol is generated. Specifically, each conditional p(wty|w<ty , z), where < t = {0, . . . , t− 1}, is specified as softmax(Vht), where ht, the hidden units, are recursively updated through ht = H(yt−1,ht−1, z), and h0 is defined as a zero vector (h0 is not updated during training). V is a weight matrix used for computing a distribution over words. Bias terms are omitted for simplicity throughout the paper. The transition function H(·) is implemented with an LSTM (Hochreiter and Schmidhuber, 1997).\nGiven the sentence pair (sx, sy), the objective function is the sum of the log-probabilities of the target sentence conditioned on the encoder representation in (1): ∑T t=1 log p(w t y|w<ty , z). The total objective is the above objective summed over all the sentence pairs.\nApplications Inspired by Srivastava et al. (2015), we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model. These models share the same CNN-LSTM model architecture, but are different in terms of the choices of the target sentence. An illustration of the proposed encoder-decoder models is shown in Figure 1(left).\nThe autoencoder (i) aims to reconstruct the same sentence as the input. The intuition behind this is that an autoencoder learns to represent the data using features that explain its own important factors of variation, and hence model the internal structure of sentences, effectively capturing the intrasentence information. Another natural task is encoding an input sentence to predict the subsequent sentence. The future predictor (ii) achieves this, effectively capturing the inter-sentence information,\nwhich has been shown to be useful to learn the semantics of a sentence (Kiros et al., 2015). These two tasks can be combined to create a composite model (iii), where the CNN encoder is asked to learn a feature vector that is useful to simultaneously reconstruct the input sentence and predict a future sentence. This composite model encourages the sentence encoder to incorporate contextual information both within and beyond the sentence."
  }, {
    "heading": "2.2 Hierarchical CNN-LSTM model",
    "text": "The future predictor described in Section 2.1 only considers the immediately subsequent sentence as context. By utilizing a larger surrounding context, it is likely that we can learn even higher-quality sentence representations. Inspired by the standard RNN-based language model (Mikolov et al., 2010) that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences. An illustration of the hierarchical model is shown in Figure 1(right), with details provided in Figure 2.\nOur proposed hierarchical model characterizes the hierarchy word-sentence-paragraph. A paragraph is modeled as a sequence of sentences, and each sentence is modeled as a sequence of words. Specifically, assume we are given a paragraph D = (s1, . . . , sL), that consists of L sentences. The probability for paragraph D is then defined as\np(D) = L∏\n`=1\np(s`|s<`) (2)\nwhere s0 is defined as a special start-of-theparagraph token. As shown in Figure 2(left), each p(s`|s<`) in (2) is calculated as\np(s`|s<`) = p(s`|h(p)` ) (3) h\n(p) ` = LSTMp(h (p) `−1, z`−1) (4)\nz`−1 = CNN(s`−1) (5)\nwhere h(p)` denotes the `-th hidden state of the LSTM paragraph generator, and h(p)0 is fixed as a zero vector. The CNN in (5) is as described in Section 2.1, encoding the sentence s`−1 into a vector representation z`−1.\nEquation (4) serves as the paragraph-level language model (Mikolov et al., 2010), which encodes all the previous sentence representations z<` into a vector representation h(p)` . This hidden state h (p) `\nis used to guide the generation of the `-th sentence through the decoder (3), which is defined as\np(s`|h(p)` ) = T∏̀\nt=1\np(w`,t|w`,<t,h(p)` ) (6)\nwhere w`,0 is defined as a special start-of-thesentence token. T` is the length of sentence `, and w`,t denotes the t-th word in sentence `. As shown in Figure 2(right), each p(w`,t|w`,<t,h(p)` ) in (6) is calculated as\np(w`,t|w`,<t,h(p)` ) = softmax(Vh(s)`,t ) (7) h\n(s) `,t = LSTMs(h (s) `,t−1,x`,t−1,h (p) ` ) (8)\nwhere h(s)`,t denotes the t-th hidden state of the LSTM decoder for sentence `, x`,t−1 denotes the word embedding for w`,t−1, and h (s) `,0 is fixed as a zero vector for all ` = 1, . . . , L. V is a weight matrix used for computing distribution over words."
  }, {
    "heading": "3 Related work",
    "text": "Various methods have been proposed for sentence modeling, which generally fall into two categories. The first consists of models trained specifically for a certain task, typically combined with downstream applications. Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017).\nThe other category consists of methods aiming to learn generic sentence representations that can be used across domains. This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 2016). Hill et al. (2016) also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors. Our model falls into this category, and is most related to Kiros et al. (2015).\nHowever, there are two key aspects that make our model different from Kiros et al. (2015). First, we use CNN as the sentence encoder. The combination of CNN and LSTM has been considered in image captioning (Karpathy and Fei-Fei, 2015), and in some recent work on machine translation (Kalchbrenner and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016). Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different. Our work aims to use a CNN to learn generic sentence embeddings.\nSecond, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015). Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences. Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016). Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences."
  }, {
    "heading": "4 Experiments",
    "text": "We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking. As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor. To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trained sentence encoder on the 5 clas-\nsification benchmarks. All the CNN-LSTM models are trained using the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books.\nWe train four models in total: (i) an autoencoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model. For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector. For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.\nThe CNN-LSTM models are trained with a vocabulary size of 22,154 words. In order to learn a generic sentence encoder that can encode a large number of possible words, we use two methods of considering words not in the training set. Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors (Mikolov et al., 2013), in which all test words are assumed to reside.\nThe first method learns a linear mapping between the word2vec embedding space Vw2v and the learned word embedding space Vcnn by solving a linear regression problem (Kiros et al., 2015). Thus, any word from Vw2v can be mapped into Vcnn for encoding sentences. The second method fixes the word vectors in Vcnn as the corresponding word vectors in Vw2v , and we do not update the word embedding parameters during training. Thus, any word vector from Vw2v can be naturally used to encode sentences. By doing this, our trained sentence encoder can successfully encode 931,331 words.\nFor training, all weights in the CNN and nonrecurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01]. Orthogonal initialization is employed on the recurrent matrices in the LSTM. All bias terms are initialized to zero. The initial forget gate bias for LSTM is set to 3. Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014). The Adam algorithm (Kingma and Ba, 2015) with learning rate 2× 10−4 is utilized for optimization. For all the CNN-LSTM models, we use mini-batches of size 64. For the hierarchical CNN-LSTM model, we use mini-batches of size 8, and each paragraph is composed of 8 sentences. We do not perform any regularization other than dropout (Srivastava et al., 2014). All experiments are implemented\nin Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory."
  }, {
    "heading": "4.1 Qualitative analysis",
    "text": "We first demonstrate that the sentence representation learned by our model exhibits a structure that makes it possible to perform analogical reasoning using simple vector arithmetics, as illustrated in Table 1. It demonstrates that the arithmetic operations on the sentence representations correspond to wordlevel addition and subtractions. For instance, in the 3rd example, our encoder captures that the difference between sentence B and C is “you\" and “him\", so that the former word in sentence A is replaced by the latter (i.e., “you”-“you”+“him”=“him”), resulting in sentence D.\nTable 2 shows nearest neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset. Nearest neighbors are scored by cosine similarity from a random sample of 1 million sentences from the BookCorpus dataset. As can be seen, our encoder learns to accurately\ncapture semantic and syntax of the sentences."
  }, {
    "heading": "4.2 Quantitative evaluations",
    "text": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002). On all the datasets, we separately train a logistic regression model on top of the extracted sentence features. We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison. We also provide the state-of-the-art results using task-dependent learning methods for reference. Results are summarized in Table 3. Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets.\nWe highlight some observations. First, the autoencoder performs better than the future predictor, indicating that the intra-sentence information may be more important for classification than the inter-sentence information. Second, the hierarchi-\ncal model performs better than the future predictor, demonstrating the importance of capturing longterm dependencies across multiple sentences. Our combined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best. This may be due to that: (i) both intra- and long-term inter-sentence information are leveraged; (ii) it is easier to linearly separate the feature vectors in higher dimensional spaces. Further, using (fixed) pre-trained word embeddings consistently provides better performance than using the learned word embeddings. This may be due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset (containing 100 billion words) (Mikolov et al., 2013).\nTo further demonstrate the advantage of the learned generic representations, we train a CNN classifier (i.e., a CNN encoder with a logistic regression model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM composite model. Results are shown in Figure 3(left). The pretraining provides substantial improvements\n(3.52% on average) over random initialization of CNN parameters. Figure 3(right) shows the effect of pretraining as the number of labeled sentences is varied. For the TREC dataset, the performance improves from 79.7% to 84.1% when only 10% sentences are labeled. As the size of the set of labeled sentences grows, the improvement becomes smaller, as expected. For future work, our CNNLSTM model can be also used for semi-supervised\nlearning, with the autoencoder on all the data (labeled and unlabled), and the classifier only on the labeled data.\nParaphrase detection Now we consider paraphrase detection on the MSRP dataset (Dolan et al., 2004). On this task, one needs to predict whether or not two sentences are paraphrases. The training set consists of 4076 sentence pairs, and the test set has 1725 pairs. As in Tai et al. (2015), given two sentence representations zx and zy, we first compute their element-wise product zx zy and their absolute difference |zx − zy|, and then concatenate them together. A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases. We present our results on the last column of Table 3. Our best result is better than the other results that use task-independent methods.\nImage-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other. We use the COCO dataset (Lin et al., 2014), which contains 123,287 images each with 5 captions. For development and testing we use the same splits as Karpathy and Fei-Fei (2015). The development and test sets each contain 5000 images. We further split them into 5 random sets of 1000 images, and report the average performance over the 5 splits. Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results. We also report the median rank of the closest ground truth result\nin the ranked list. We represent images using 4096-dimensional feature vectors from VggNet (Simonyan and Zisserman, 2015). Each caption is encoded using our trained CNN encoder. The training objective is the same pairwise ranking loss as used in Kiros et al. (2015), which takes the form of max(0, α− f(xn, yn) + f(xn, ym)), where f(·, ·) is the image-sentence score. (xn, yn) denotes the related image-sentence pair, and (xn, ym) is the randomly sampled unrelated image-sentence pair with n 6= m. For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa. The objective is to force the matching score of the related pair (xn, yn) to be greater than the unrelated pair (xn, ym) by a margin α, which is set to 0.1 in our experiments.\nTable 4 shows our results. Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported. As can be seen, we obtain the same median rank as in Kiros et al. (2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015). The performance gain between our encoder and the combine-skip model of Kiros et al. (2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re-\ntrieving the most correct item than the skip-thought vector.\nSemantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset (Marelli et al., 2014), consisting of 9927 sentence pairs. Given two sentences, our goal is to produce a real-valued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores. We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset. We follow the method in Tai et al. (2015), and use the crossentropy loss for training. Results are summarized in Table 5. Our result is better than the combineskip model of Kiros et al. (2015). This suggests that CNN also provides competitive performance at matching human relatedness judgements."
  }, {
    "heading": "5 Conclusion",
    "text": "We presented a new class of CNN-LSTM encoderdecoder models to learn sentence representations from unlabeled text. Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of Kiros et al. (2015). Compelling experimental results on several tasks demonstrated the advantages of our approach. In future work, we aim to use more advanced CNN architectures (Conneau et al., 2016) for learning generic sentence embeddings."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF."
  }],
  "year": 2017,
  "references": [{
    "title": "A simple but tough-to-beat baseline for sentence embeddings",
    "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Theano: new features and speed improvements",
    "authors": ["Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."],
    "venue": "arXiv:1211.5590.",
    "year": 2012
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Very deep convolutional networks for natural language processing",
    "authors": ["Alexis Conneau", "Holger Schwenk", "Loïc Barrault", "Yann Lecun."],
    "venue": "arXiv:1606.01781.",
    "year": 2016
  }, {
    "title": "Semi-supervised sequence learning",
    "authors": ["Andrew M Dai", "Quoc V Le."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
    "authors": ["Bill Dolan", "Chris Quirk", "Chris Brockett."],
    "venue": "COLING.",
    "year": 2004
  }, {
    "title": "Character-level deep conflation for business data analytics",
    "authors": ["Zhe Gan", "PD Singh", "Ameet Joshi", "Xiaodong He", "Jianshu Chen", "Jianfeng Gao", "Li Deng."],
    "venue": "arXiv preprint arXiv:1702.02640.",
    "year": 2017
  }, {
    "title": "A convolutional encoder model for neural machine translation",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N Dauphin."],
    "venue": "arXiv:1611.02344.",
    "year": 2016
  }, {
    "title": "Deep Learning",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."],
    "venue": "MIT Press.",
    "year": 2016
  }, {
    "title": "Multiperspective sentence similarity modeling with convolutional neural networks",
    "authors": ["Hua He", "Kevin Gimpel", "Jimmy J Lin."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "Learning distributed representations of sentences from unlabelled data",
    "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation.",
    "year": 1997
  }, {
    "title": "Convolutional neural network architectures for matching natural language sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "NIPS.",
    "year": 2014
  }, {
    "title": "Mining and summarizing customer reviews",
    "authors": ["Minqing Hu", "Bing Liu."],
    "venue": "SIGKDD.",
    "year": 2004
  }, {
    "title": "Learning deep structured semantic models for web search using clickthrough data",
    "authors": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."],
    "venue": "CIKM.",
    "year": 2013
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daumé III."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Effective use of word order for text categorization with convolutional neural networks",
    "authors": ["Rie Johnson", "Tong Zhang."],
    "venue": "NAACL HLT.",
    "year": 2015
  }, {
    "title": "Recurrent continuous translation models",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Li Fei-Fei."],
    "venue": "CVPR.",
    "year": 2015
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc Le", "Tomas Mikolov."],
    "venue": "ICML.",
    "year": 2014
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Learning question classifiers",
    "authors": ["Xin Li", "Dan Roth."],
    "venue": "ACL.",
    "year": 2002
  }, {
    "title": "Hierarchical recurrent neural network for document modeling",
    "authors": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "Microsoft coco: Common objects in context",
    "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick."],
    "venue": "ECCV.",
    "year": 2014
  }, {
    "title": "A structured self-attentive sentence embedding",
    "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
    "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
    "authors": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"],
    "year": 2014
  }, {
    "title": "Encoding source language with convolutional neural network for machine translation",
    "authors": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur."],
    "venue": "INTERSPEECH.",
    "year": 2010
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS.",
    "year": 2013
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive science.",
    "year": 2010
  }, {
    "title": "Unsupervised learning of sentence embeddings using compositional n-gram features",
    "authors": ["Matteo Pagliardini", "Prakhar Gupta", "Martin Jaggi."],
    "venue": "arXiv:1703.02507.",
    "year": 2017
  }, {
    "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "ACL.",
    "year": 2004
  }, {
    "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "ACL.",
    "year": 2005
  }, {
    "title": "A latent semantic model with convolutional-pooling structure for information retrieval",
    "authors": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grégoire Mesnil."],
    "venue": "CIKM.",
    "year": 2014
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["Karen Simonyan", "Andrew Zisserman."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
    "authors": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "NIPS.",
    "year": 2011
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
    "authors": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."],
    "venue": "CIKM.",
    "year": 2015
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "JMLR.",
    "year": 2014
  }, {
    "title": "Unsupervised learning of video representations using lstms",
    "authors": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhudinov."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "NIPS.",
    "year": 2014
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Larger-context language modelling with recurrent neural network",
    "authors": ["Tian Wang", "Kyunghyun Cho."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Annotating expressions of opinions and emotions in language",
    "authors": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."],
    "venue": "Language resources and evaluation.",
    "year": 2005
  }, {
    "title": "Towards universal paraphrastic sentence embeddings",
    "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "ICLR.",
    "year": 2016
  }, {
    "title": "Convolutional neural network for paraphrase identification",
    "authors": ["Wenpeng Yin", "Hinrich Schütze."],
    "venue": "HLT-NAACL.",
    "year": 2015
  }, {
    "title": "Learning composition models for phrase embeddings",
    "authors": ["Mo Yu", "Mark Dredze."],
    "venue": "TACL.",
    "year": 2015
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Self-adaptive hierarchical sentence model",
    "authors": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."],
    "venue": "arXiv:1504.05070.",
    "year": 2015
  }, {
    "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
    "authors": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "ICCV.",
    "year": 2015
  }],
  "id": "SP:26ca85f120521344a4710d1e755d4b698da24303",
  "authors": [{
    "name": "Zhe Gan",
    "affiliations": []
  }, {
    "name": "Yunchen Pu",
    "affiliations": []
  }, {
    "name": "Ricardo Henao",
    "affiliations": []
  }, {
    "name": "Chunyuan Li",
    "affiliations": []
  }, {
    "name": "Xiaodong He",
    "affiliations": []
  }, {
    "name": "Lawrence Carin",
    "affiliations": []
  }],
  "abstractText": "We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoderdecoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.",
  "title": "Learning Generic Sentence Representations Using Convolutional Neural Networks"
}