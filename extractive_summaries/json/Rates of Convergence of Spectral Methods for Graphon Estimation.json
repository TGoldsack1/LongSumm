{
  "sections": [{
    "text": "1The Fuqua School of Business, Duke University, Durham, NC, USA.. Correspondence to: J. Xu <jiaming.xu868@duke.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\napproximations of the graphon function f ."
  }, {
    "heading": "1. Introduction",
    "text": "Many modern systems and datasets can be represented as networks with vertices denoting the objects and edges (possibly weighted or labelled) encoding their interactions. Examples include online social networks such as Facebook friendship network, biological networks such as protein-protein interaction networks, and recommender systems such as movie rating datasets. A key task in network analysis is to estimate the underlying network generating mechanism, i.e., how the edges are formed in a network. It is useful for many important applications such as studying network evolution over time (Pensky, 2016), predicting missing links in networks (Miller et al., 2009; Airoldi et al., 2013; Gao et al., 2016), learning hidden user prefererences in recommender systems (Song et al., 2016), and correcting errors in crowdsourcing systems (Lee & Shah, 2017). However, in practice we usually observe only a very small fraction of edge connections in these networks, which obscures the underlying network generating mechanism. For example, around 80% of the molecular interactions in cells of Yeast (Yu et al., 2008) are unknown. In Netflix movie dataset, about 99% of movie ratings are missing and the observed ratings are noisy.\nIn this paper, we are interested in understanding when and how the underlying network generating mechanism can be efficiently inferred from a partial observation of a network. We assume the network is generated according to the graphon model (Lovász & Szegedy, 2006). Concretely, given n vertices, the edges are generated independently, connecting each pair of two distinct vertices i and j with a probability\nMij = f(xi, xj), (1)\nwhere xi ∈ X is the latent feature vector of vertex i that captures various characteristics of vertex i; f : X × X → [0, 1] is a symmetric function called graphon. We assume no self loop and setMii = 0 for 1 ≤ i ≤ n. We further assume the feature vectors xi’s are drawn i.i.d. from a measurable space X according to a probability distribution µ.\nGraphon model captures a key characteristic of real networks, that is, the edge connections are dependent on latent\nfeatures of vertices rather than specific vertex identities. Graphon model was originally developed as a limit of a sequence of graphs with growing sizes (Lovász, 2012), and has been applied to various network analysis problems ranging from testing graph properties to counting homomorphisms to charactering distances between two graphs (Lovász, 2012; Borgs et al., 2008; 2012) to detecting communities (Bickel & Chen, 2009). Graphon model encompasses many existing network models as special cases. Setting f to be a constant p, it gives rise to Erdős-Rényi random graphs, where each edge is formed independently with probability p. In the case where f is a step function or X is a discrete set, the model specializes to the stochastic block model (Holland et al., 1983), where each vertex belongs to a community, and the edge probability between i and j depends only on which communities they are in. If X is a Euclidean space of dimension d and f(xi, xj) is a function of the Euclidean distance ‖xi−xj‖, then the grahon model reduces to the latent space model (Hoff et al., 2002; Handcock et al., 2007).\nTo capture a partial observation of the network, we assume every edge is observed independently with probability ρ ∈ [0, 1], where ρ = ρn may converge to 0 as n → ∞. Given the resulting observed graph, the problem of interest is to estimate the underlying network generating mechanism – the graphon f . However, without observing xi’s, there is no way to uniquely identify f. To overcome this identifiability issue, we follow the prior work (Gao et al., 2015) and consider estimating f under the expected empirical loss1:\n1\nn2 E  ∑ i,j∈[n] ( f̂(xi, xj)− f(xi, xj) )2 . This is equivalent to estimating the edge probability matrix M under the mean squared error (Gao et al., 2015):\nMSE(M̂) , 1 n2 E [∥∥∥M̂ −M∥∥∥2 F ] , (2)\nwhere M̂ij = f̂(xi, xj). The fundamental estimation limits are phrased in terms of the minimax mean-squared error:\nR∗n , inf M̂ sup f∈F sup µ∈P MSE(M̂),\nwhere F denotes a set of admissible graphon functions f , and P denotes the set of all possible probability measures\n1 By the definition of the expected empirical loss, we only need to estimate the edge probabilities {f(xi, xj)}. An alternative way to overcome the identifiablity issue is to consider estimating f up to weak isometry under the expected integral loss (Wolfe & Olhede, 2013). As shown in (Klopp et al., 2015)[Section 3], the results of estimating f under the expected empirical loss can be readily extended to estimating f up to weak isometry under the expected integral loss by including an extra agnostic error term due to the discretization of X by xi’s.\non X . The minimax estimation error depends on the smoothness of graphon f , the structure of latent space (X , µ), and the observation probability ρ.\nThere is a recent surge of interest in graphon estimation. Various procedures have been proposed and analyzed (Gao et al., 2015; Klopp et al., 2015; Gao et al., 2016; Wolfe & Olhede, 2013; Airoldi et al., 2013; Yang et al., 2014; Chan & Airoldi, 2014; Cai et al., 2014; Zhang et al., 2015; Borgs et al., 2015a; Klopp & Verzelen, 2017; Borgs et al., 2017). A recent line of work (Gao et al., 2015; Klopp et al., 2015; Gao et al., 2016) has characterized the minimax error rate in certain special cases. In particular, for stochastic block model with k blocks, it is shown that the minimax error rate is k 2\nn2ρ + log k nρ . For fully observed graphons with f being Hölder smooth on X = [0, 1] and ρ = 1, the minimax error rate turns out be n−1 log k + n−2α/(α+1), where α is the smoothness index of f . This result was extended by (Klopp et al., 2015; Gao et al., 2016) to sparse regimes where ρ→ 0 as n→∞.\nFrom a computational perspective, the problem appears to be much harder and far less well-understood. In the special case where f is α-Hölder smooth on X = [0, 1], a universal singular value thresholding (USVT) algorithm is shown in (Chatterjee, 2015) to achieve an error rate of n−1/3ρ−1/2. However, this performance guarantee is far from the minimax optimal rate log(nρ)/(nρ). A similar spectral method is shown in (Xu et al., 2014) to achieve a vanishing MSE when nρ log n but without an explicit characterization of the rate of the convergence. The nearestneighbor based approach is analyzed in (Song et al., 2016) under a stringent assumption nρ √ n. A simple degree sorting algorithm (Borgs et al., 2015b) is shown to achieve an error rate of (log(nρ)/(nρ))α/(4α+d) for α ∈ (0, 1] under the restrictive assumption that ∫ 1 0 f(x, y)dy is strictly monotone in x.\nIn summary, despite the recent significant effort devoted to developing fundamental limits and efficient algorithms for graphon estimation, an understanding of the statistical and computational aspects of graphon estimation is still lacking. In particular, there is a wide gap between the known performance bounds of polynomial-time estimator and the minimax optimal estimation rate. This raises a fundamental question:\nIs there a polynomial-time estimator that is guaranteed to achieve the minimax optimal rate?\nIn this paper, we provide a partial answer to this question by analyzing the universal singular value thresholding (USVT) algorithm proposed by Chatterjee (Chatterjee, 2015). The universal singular value thresholding is a simple and versatile method for structured matrix estimation and has been applied to a variety of different problems such as rank-\ning (Shah et al., 2016). It truncates the singular values of A at a threshold slightly above the spectral norm ‖A−E [A] ‖, and estimates M by a properly rescaled A after truncation. It is computationally efficient, however, its performance guarantee established in (Chatterjee, 2015) requires the total number of observed edges to be much larger than n(2d+2)/(d+2) to attain a vanishing MSE. In contrast, our improved performance bound shows that the total number of observed edges only needs to be a constant factor larger than n log n, irrespective of the latent space dimension d.\nMore formally, by assuming the average vertex degree is at least logarithmic in n, i.e., nρ = Ω(log n), and X is a compact subset in Rd, the mean-squared error rate of USVT is shown to be upper bounded by (nρ)−2α/(2α+d), when f belongs to either α-smooth Hölder function class H(α,L) or α-smooth Sobolev space S(α,L). This convergence rate of USVT is approaching the minimax optimal rate log(nρ)/(nρ) provided in (Gao et al., 2015) for d = 1, as f becomes smoother, i.e., α increases. In fact, we show that if f is analytic with infinitely many times differentiability2, then the error rate is upper bounded by logd(nρ)/(nρ).\nIn the special case where f is a step fuction or X is a discrete set, then the graphon model specializes to the stochastic block model with k blocks for some k. In this case, the error rate of USVT is shown to be k/(nρ), which is larger than the optimal minimax rate by at most a multiplicative factor k/ log k. This factor coincides with the ratio of the Kesten-Stigum threshold and information-theoretic threshold in community detection (Banks et al., 2016; Abbe & Sandon, 2015; Banks et al., 2018). Based on compelling but non-rigorous statistical physics arguments, it is believed that no polynomial-time algorithms are able to detect the communities between the KS-threshold and IT-threshold (Moore, 2017). This coincidence indicates that k/(nρ) may be the optimal estimation rate among all polynomial-time algorithms, and the minimax optimal rate may not be attainable in polynomial-time. During the preparation of this manuscript, we became aware of an earlier arXiv preprint (Klopp & Verzelen, 2017)[Proposition 4] which also derives the error rate of k/(nρ).\nOur proof incorporates three interesting ingredients. One is a characterization of the estimation error of USVT in terms of the tail of eigenvalues of M , and the spectral norm of the noise perturbation ‖A − E [A|M ] ‖, see e.g., (Shah et al., 2016)[Lemma 3]. The second one is a high-probability upper bound on ‖A− E [M |A] ‖ obtained from matrix concentration inequalities initially developed by (Feige & Ofek, 2005). The last but most important one is a characterization\n2The minimax lower bound in (Gao et al., 2015)[Appendix A.1] is only established for the α-smooth Hölder function class for any fixed α and d = 1. It is an open question whether the error rate of logd(nρ)/(nρ) is minimax-optimal for analytic graphons.\nAlgorithm 1 Universal Singular Value Thresholding (USVT) (Chatterjee, 2015)\n1: Input: A ∈ Rn×n, ρ ∈ [0, 1] and a threshold τ > 0. 2: Let A = ∑n i=1 siuiv > i be its singular value decompo-\nsition with s1 ≥ s2 ≥ · · · ≥ sn. 3: Let S be the set of “thresholded” singular values: S = {i : si ≥ τ}. 4: Let Â =\n∑ i∈S siuiv > i\nand M̃ = Â/ρ. 5: Output a matrix M̂ ∈ [0, 1]n×n such that M̂ii = 0 for\nall i ∈ [n], and for 1 ≤ i < j ≤ n, M̂ij = M̂ji and\nM̂ij =  M̃ij , if M̃ij ∈ [0, 1] 1, if M̃ij > 1 0, if M̃ij < 0.\nof the tail of eigenvalues of M using piecewise polynomial approximations of f , which were originally used to study the spectrum of integral operators defined by f (Birman & Solomyak, 1967; 1977). The piecewise constant approximations of f have appeared in the previous work on graphon estimation (Chatterjee, 2015; Gao et al., 2015; Klopp et al., 2015), and are sufficient for the purpose of deriving minimax estimation rates because the smoothness of f beyond α = 1 does not improve the rates. However, piecewise degree-bαc polynomial approximations are needed for showing USVT to achieve a faster converging rate as α increases.\nNotation For a vector x ∈ Rd, let ‖x‖2 denote its `2 norm and ‖x‖∞ = max1≤i≤d |xi| denote its `-infinity norm. For any matrix M , let ‖M‖ denote its spectral norm and ‖M‖F denote its Frobenius norm. For any positive integer n, let [n] = {1, . . . , n}. For any positive constant α, let bαc denotes the largest integer strictly smaller than α. For two real numbers α and β, let α ∧ β = min{α, β} and α ∨ β = max{α, β}. If κ = (κ1, . . . , κd) is a multi-index with κi ∈ N, then |κ| = ∑d i=1 κi, κ! = ∏d i=1 κi!, and\nxκ = ∏d i=1 x κi i for a vector x ∈ Rd."
  }, {
    "heading": "2. Main results",
    "text": "Let A denote the adjacency matrix of the observed graph with Aii = 0 by convention. Then conditional on x = (x1, . . . , xn), for 1 ≤ i < j ≤ n, Aij = Aji are independently distributed as Bern (ρMij). In particular, E [A|M ] = ρM .\nTo describe our main results, we first recall the universal singular value thresholding (USVT) algorithm (Chatterjee,\n2015) as stated in Algorithm 1. Note that according to the graphon model (1), the edge probability matrix M may not be of low-rank. Nevertheless, it is possible that the singular values of M , or equivalently magnitudes of eigenvalues, drop off fast enough and as a consequence M is approximately low-rank. If this is indeed the case, then a natural idea to estimate M is via low-rank approximations of A. In particular, USVT truncates the singular values of A at a proper threshold τ , and estimates M by the rescaled A after truncation.\nNote that Algorithm 1 applies hard-thresholding to the singular values of A. Alternatively, we can use softthresholding (Koltchinskii et al., 2011) and let Â =∑ i∈S(si − τ)uiv>i . Our main results with the hardthresholding also apply to the soft-thresholding. As argued in (Chatterjee, 2015), the cut-off threshold τ is chosen to be slightly above ‖A − E [A|M ] ‖, so that noise is suppressed and signals corresponding to large singular values of E [A|M ] are maintained. Since conditional on M , A is a random matrix with independent entries bounded in [0, 1] of variance at most ρ, it is expected that ‖A − E [A|M ] ‖ . √nρ with high probability, in view of standard matrix concentration inequalities. This turns out to be true if the observed graph is not too sparse, i.e., there exists a positive constant C such that\nnρ ≥ C log n. (3)\nHowever, when the observed graph is sparse with nρ = o(log n), due to the existence of high-degree vertices, ‖A− E [A|M ] ‖ √nρ with high probability (Krivelevich & Sudakov, 2003).\nMotivated by the discussion above, we focus on the relatively sparse regime where (3) holds, and set τ = c0 √ nρ for a positive large constant c0, whose value depends on the constant C in (3). It is known that with high probability,\n‖A− E [A|M ] ‖ ≤ κ√nρ,\nwhere\nκ =\n{ 4 + o(1) nρ = ω(log n)\n2 + o(1) nρ = ω(log4 n) , (4)\nsee, e.g., (Hajek et al., 2016)[Lemma 30]. Hence, the constant c0 can be set to be a universal constant strictly larger than 4 in the case of nρ log(n) and 2 in the case of nρ log4(n). Notably, in these cases, the cutoff threshold τ is universal, independent of the underlying graphon f .\nNext, we present our main results without proofs. The excluded details can be found in the full paper (Xu, 2017). Our first result provides an upper bound to the estimation error of USVT. A similar result without explicit constants is proved\nin (Shah et al., 2016)[Lemma 3], which improves on the previous result in (Chatterjee, 2015)[Lemma 3.5]. Another similar result with slightly different constants is proved in (Koltchinskii et al., 2011)[Theorem 1] for soft singular value thresholding and in (Klopp et al., 2011)[Theorem 2] for hard singular value thresholding.\nTheorem 1. Consider the relatively sparse regime where (3) holds. For all c > 0 there exists a positive constant κ such that if τ = (1 + δ)κ √ nρ for a fixed constant δ > 0, then conditional on M , with probability at least 1− n−c,\n1\nn2 ‖M̂ −M‖2F\n≤ 16(1 + δ)2 min 0≤r≤n κ2r nρ + 1 n2δ2 ∑ i≥r+1 λ2i (M)  . It further follows that MSE(M̂) is bounded by the same error term as above plus the failing probability n−c.\nTheorem 1 gives an upper bound to the estimation error of USVT in terms of the tail of eigenvalues of M and the observation probability ρ. The upper bound invovles minimization of a sum of two terms over integers 0 ≤ r ≤ n: the first term r/(nρ) can be viewed as the estimation error for a rank-r matrix; the second term n−2 ∑ i≥r+1 λ 2 i (M) is the tail of eigenvalues of M and charaterizes the approximation error of M by the best rank-r matrix. The optimal r is chosen to achieve the best trade-off between the estimation error and the approximaiton error. Moreover, a lighter tail of eigenvalues of M implies a faster convergence rate of the estimation error. To characterize different tails of eigenvalues of M , we introduce the following definitions of polynomial and super-polynomial decays.\nDefinition 1 (Polynomial decay). We say the eigenvalues of M asymptotically satisfy a polynomial decay with rate β > 0 if for all integers 0 ≤ r ≤ n− 1,\n1\nn2 ∑ i≥r+1 E [ λ2i (M) ] ≤ c0r−β + c1n−1,\nwhere c0 and c1 are two constants independent of n and r.\nDefinition 2 (Super-polynomial decay). We say the eigenvalues of M asymptotically satisfy a super-polynomial decay with rate α > 0 if for all integers 0 ≤ r ≤ n− 1,\n1\nn2 ∑ i≥r+1 E [ λ2i (M) ] ≤ c0e−c2r α + c1n −1,\nwhere c0, c1, c2 are constants independent of n and r.\nWe remark that in the above two definitions, we allow for a residual term c1n−1, which is responsible for the contribution of diagonal entries of M . According to Theorem 1,\nthis residual term only induces an additional n−1 error in the upper bound to MSE and will not affect our main results. The following corollary readily follows from Theorem 1 by choosing the optimal r according to the decay rates of eigenvalues of M .\nCorollary 1. Consider the relatively sparse regime where (3) holds and suppose the eigenvalues of M satisfy a polynomial decay with rate β > 0. Then there exists a positive constant κ > 0 such that if τ = (1 + δ)κ √ nρ for a fixed constant δ > 0,\nMSE(M̂) ≤ c′(nρ)− β β+1 .\nIf instead the eigenvalues of M satisfy a super-polynomial decay with rates α > 0, then\nMSE(M̂) ≤ c′ (log(nρ)) 1/α\nnρ ,\nwhere c′ is a positive constant independent of n.\nProof. The first conclusion follows from Theorem 1 by choosing c = 1 and r = b(nρ)1/(β+1)c and the second one follows by choosing c = 1 and r = b(log(nρ)/c2)1/αc.\nNext we specialize our general results in different settings by deriving the decay rates of eigenvalues of M."
  }, {
    "heading": "2.1. Stochastic block model",
    "text": "We first present results on the rate of convergence in the stochastic block model setting, where xi ∈ {1, 2, . . . , k} indicating which community that vertex i belongs to. In this case, Mij only depends on the communities of vertex i and vertex j, and M has rank at most k.\nTheorem 2. Assume (3) holds under the stochastic block model with k blocks. There exists a positive constant κ > 0 such that if τ = (1+δ)κ √ nρ for some fixed constant δ > 0, then\nMSE(M̂) ≤ c′′ [ k nρ ∧ 1 ] .\nwhere c′′ is a positive constant depending on κ and δ.\nProof. Under the stochastic block model, M is of rank at most k. Thus λi(M) = 0 for all i ≥ k+ 1. Moreover, since Mij ∈ [0, 1], it follows that ∑k i=1 λ 2 i (M) = ‖M‖2F ≤ n2. Applying Theorem 1 with r = 0 and r = k yields the desired result.\nTheorem 2 shows that the convergence rate of MSE of USVT is at most knρ ∧ 1, while the previous result in (Chatterjee, 2015) establishes that the convergence rate is at most\n√ k/n for ρ = 1. During the preparation of this manuscript, we became aware of an earlier arXiv preprint (Klopp & Verzelen, 2017)[Proposition 4] which also proves the error rate of k/(nρ).\nThe minimax optimal rate derived in (Klopp et al., 2015; Gao et al., 2016) is ( k2\nn2ρ + log k nρ\n) ∧ 1. Hence, the error\nrate of USVT is larger than the minimax optimal rate by at most a multiplicative factor of k/ log k, which resembles the computational gap observed in community detection (Banks et al., 2016; Abbe & Sandon, 2015) and the related high-dimensional statistical inference problems discussed in (Banks et al., 2018). In particular, it is shown in (Banks et al., 2016; Abbe & Sandon, 2015) that estimation better than randomly guessing is attainable efficiently by spectral methods when above the Kesten-Stigum threshold, while it is information-theoretically possible even strictly below the KS threshold by a multiplicative factor k/ log k for large k. In between the KS threshold and informationtheoretic threshold, non-trivial estimation is informationtheoretically possible but believed to require exponential time. The same conclusion also holds for exact community recovery as shown in (Chen & Xu, 2014). Due to this coincidence, it is tempting to believe that knρ ∧ 1 might be the optimal estimation rate among all polynomial-time algorithms; however, we do not have a proof."
  }, {
    "heading": "2.2. Smooth graphon",
    "text": "Next we proceed to the smooth graphon setting. We assume X = [0, 1)d for simplicity. 3. There are various notions to characterize the smoothness of graphon. In this paper, we focus on the following two notions, which are widely adopted in the non-parametric regression literature (Tsybakov, 2008). Given a function g : X → R and a multi-index κ, let\n∇κg(x) = ∂|κ|g(x)\n(∂x)κ (5)\ndenote its partial derivative whenever it exists.\nDefinition 3 (Hölder class). Let α and L be two positive numbers. The Hölder class H(α,L) on X is defined as the set of functions g : X → R whose partial derivatives satisfy4∑ κ:|κ|=bαc 1 κ! |∇κg(x)−∇κg(x′)| ≤ L‖x− x′‖α−bαc∞ .\n(6)\n3If X is a compact set in Rd, then there exists a positive constant a such that X ⊂ [−a, a)d. Hence, the general compact set case can be reduced to X = [0, 1)d by a proper scaling.\n4 Changing the infinity-norm to a different norm (e.g. `2 norm) only changes L (by a factor may depending on d and α) and thus will not affect rates of convergence.\nNote that if α ∈ (0, 1], then (6) is equivalent to the Lip-α condition:\n|g(x)− g(x′)| ≤ L‖x− x′‖α∞. (7)\nOne can also measure the smoothness with respect to the underlying measure µ. This leads to the consideration of Sobolev space (Leoni, 2009). For ease of exposition, we assume µ is the Lebesgue measure. The main results can be extended to more general Borel measures.\nDefinition 4 (Sobolev space). Let α and L be two positive numbers. The Sobolev space S(α,L) on (X , µ) is defined as the set of functions g : X → R whose partial derivatives5 satsify∑\nκ:|κ|=α\n∫ X ‖∇κg(x)‖22 dx ≤ L2, for integral α,\nand for non-integral α,\n∑ κ:|κ|=bαc ∫ X×X ‖∇κg(x)−∇κg(y)‖22 ‖x− y‖2(α−bαc)+d2 dxdy ≤ L2.\nNote that the graphon f(x, y) is a bi-variate function. We treat it as a function of x for every fixed y, and introduce the following two conditions on f .\nCondition 1 (Hölder condition on f ). There exist two positive numbers α and L such that f(·, y) ∈ H(α,L) for every y ∈ X . Condition 2 (Sobolev condition on f ). There exist two positive numbers α and L such that f(·, y) ∈ S(α,L(y)) for every y, where L(y) : X → R satisfies that ∫ X L\n2(y)dy ≤ L2.\nThe following key result shows that the eigenvalues of M drop off to zero in a polynomial rate depending on the smoothness index α of f.\nProposition 1. Suppose that f satisfies either Condition 1 or Condition 2. Then there exists a constantC = C(α,L, d) only depending on α, L, and d such that for all integers 0 ≤ r ≤ n− 1,\n1\nn2 ∑ i≥r+1 E [ λ2i (M) ] ≤ C(α,L, d) ( n−1 + r−2α/d ) .\nRemark 1. In the special case where f is Hölder smooth with α = 1, Proposition 1 has been proved in (Chatterjee, 2015). In particular, it is shown in (Chatterjee, 2015) that f can be well-approximated by a piecewise constant function. As a consequence, M can be approximated by a rank-r\n5More generally, the Sobolev space is defined when only weak derivatives exist (Leoni, 2009).\nblock matrix with r2 blocks, and the entry-wise approximation error in the squared Frobenius norm is shown to be approximately r−2α/d. The same idea can be readily extended to the case α ∈ [0, 1]. However, piecewise constant approximations of f no longer suffice for α > 1, because Hölder smoothness condition (6) no longer implies Lip-α condition (7). In fact (7) with α > 1 will imply that f ≡ C for some constant C. Instead, we show that f can be well approximated by piecewise polynomials of degree bαc.\nBy combining Proposition 1 with Corollary 1, we immediately get the following result on the convergence rate of the estimation error of USVT.\nTheorem 3. Under the graphon estimation model, assume (3) holds, and f satisfies either Condition 1 or Condition 2. There exists a positive constant κ such that if τ = (1 + δ)κ √ nρ for some fixed constant δ > 0, then\nMSE(M̂) ≤ c′′(nρ)− 2α 2α+d ,\nwhere c′′ is a positive constant independent of n.\nTheorem 3 implies that if f is infinitely many times differentiable, then the MSE of USVT converges to zero faster than (nρ)−1+ for an arbitrarily small constant > 0. In fact, we can prove a sharper result when f is analytic, i.e., f is infinitely differentiable and its Taylor series expansion around any point in its domain converges to the function in some neighborhood of the point. One concerete example of analytic function which appears in the study of matrix completion is f(x, y) = 1/(1+exp(−〈x, y〉)) (Ganti et al., 2015).\nTheorem 4. Under the graphon estimation model, suppose there there exists positive constants a and b such that for all multi-indices κ and all y ∈ X\nsup x∈X\n∂|κ|f(x, y)\n(∂x)κ ≤ ba|κ|κ!. (8)\nThere exists positive constants c0 and c1 only depending on a, b, d such that for all integers 0 ≤ r ≤ n− 1,\n1\nn2 ∑ i≥r+1 λ2i (M) ≤ c1 ( n−1 + exp ( −c0r1/d )) . (9)\nMoreover, assume (3) holds. Then there exists positive constants c′, c′′ such that if τ = c′′ √ nρ,\nMSE(M̂) ≤ c′ log d (nρ)\nnρ .\nWe remark that for a fixed y ∈ X , (8) is a sufficient and necessary condition for f(·, y) being analytic (Komatsu, 1960). Note that (9) implies the eigenvalues of M has a super-polynomial decay with rate α = 1/d. Its proof is\nbased on approximating f(·, y) using its Taylor series truncated at degree ` r1/d. When d = 1, the eigenvalues of M decays to zero exponentially fast in r; such an exponential decay can be also proved via Chebyshev polynomial approximation of f as shown in (Little & Reade, 1984)."
  }, {
    "heading": "2.2.1. COMPARISON TO MINIMAX OPTIMAL RATES",
    "text": "We compare the rates of convergence of USVT for estimating Hölder smooth graphons to the minimax optimal rates when the dimension of latent feature space d = 1 (Gao et al., 2015; Klopp et al., 2015; Gao et al., 2016):\nR∗n  1, nρ = O(1) log(nρ) nρ , ω(1) ≤ nρ ≤ n α(log n)α+1\n(n2ρ)− α α+1 , nρ ≥ nα(log n)α+1\n.\n(10)\nThus, as graphon gets smoother, i.e., α increases, the upper bound to the rate of convergence of USVT (nρ)−2α/(2α+1) approaches the minimax optimal rate log(nρ)/(nρ)."
  }, {
    "heading": "2.3. Connections to spectrum of integral operators",
    "text": "We state a useful result, connecting the eigenvalues of M to the spectrum of an integral operator defined in terms of f. This allows us to translate existing results on the decay rates of eigenvalues of integral operators to those of M. Define an operator T : L2(X , µ)→ L2(X , µ) as\n(T g) (x) , ∫ X f(x, y)g(y)µ(dy), ∀g ∈ L2(X , µ), (11)\nwhere f acts as a kernal function. Hence, M can be also viewed as a kernal matrix. We assume that the graphon f is square-integrable, i.e., ∫ X×X f\n2(x, y)µ(dx)µ(dy) <∞. In this case, the operator T is known as Hilbert-Schmidt integral operator, which is compact. Therefore it admits a discrete spectrum with finite multiplicity of all of its nonzero eigenvalues (see e.g. (Kato, 1966; Koltchinskii, 1998; von Luxburg et al., 2005)). Moreover, any of its eigenfunctions is continuous onX . Denote the eigenvalues of operator T sorted in decreasing order by |λ1(T )| ≥ |λ2(T )| ≥ · · · and its corresponding eigenfunctions with unit L2(X , µ) norm by φ1, φ2, · · · . By the definition of λk and φk, we have∫ X×X ( f(x, y)− m∑ k=1 λk(T )φk(x)φk(y) )2 µ(dx)µ(dy)\n→ 0, as m→∞ (12)\nsee, e.g., (Kato, 1966)[Chapter Five, Section 2.4].\nThe following theorem upper bounds the tail of eigenvalues of M in expectation using the tail of eigenvalues of T . Previous results in (Koltchinskii & Giné, 2000) provide\nsimilar upper bounds to the `2 distance between the ordered eigenvalues of M and those of T . Theorem 5. For any integer r ≥ 0,\n1\nn2 ∑ k>r E [ λ2k(M) ] ≤ ∞∑ k>r λ2k(T ) + 1 n r∑ k,`=1 λk(T )λ`(T )E [ φ2k(x1)φ 2 `(x1) ] .\n(13)\nThe second term on the right hand side of (13) is responsible for the contribution of the diagonal entries of M . When E [ φ2k(x1)φ 2 `(x1) ] is bounded and ∑∞ k=1 λk(T ) <∞, this second term is on the order of n−1.\nIt is well known that if the kernel function f is smoother, the eigenvalues of T drops to zero faster. There is vast literature on estimating the decay rates of the eigenvalues of T in terms of the smoothness conditions of f , see, e.g., (Krein, 1965; Birman & Solomyak, 1977; König, 2013; Delgado & Ruzhansky, 2014). Theorem 5 allows us to translate those existing results on the decay rates of eigenvalues of T to those of M , as illustrated by examples in Section 3."
  }, {
    "heading": "3. Numerical examples",
    "text": "We provide numerical results on synthetic datasets, which corroborate our theoretical results. Additional numerical results on stochastic block models can be found in the full paper (Xu, 2017). We assume the sparsity level ρ is known and set the threshold τ = 2.01 √ nρ throughout the experiments. In the case where ρ is unknown, one can apply cross-validation procedure to adaptively choose the sparsity level ρ as shown in (Gao et al., 2016). We first apply USVT with input (A, τ, ρ), and then output the estimator M̂ , and finally calculate the MSE error MSE(M̂)."
  }, {
    "heading": "3.1. Translation invariant graphon",
    "text": "For some a > 0, let h : [−a, a] → R denote an even function, i.e., h(x) = h(−x). Let us extends its domain to the real line by the periodic extension such that h(x + 2ka) = h(x) for all x ∈ [−a, a] and integers k ∈ Z. By construction h has a period 2a. Using this function, we can define a translation-invariant graphon on the product space [−a, a]× [−a, a] via f(x, y) = h(x− y). Since h is even, it follows that f is symmetric. Then the integral operator T defined in (11) reduces to: for all x ∈ [−a, a],\n(T g) (x) = 1 2a ∫ a −a h(x− y)g(y)dy = 1 2a (h ∗ g) (x),\nwhere ∗ denotes the convolution. Hence, we can explicitly determine the eigenvalues of T via Fourier analysis. In\nparticular, let ĥ[k] denote the Fourier coefficients:\nĥ[k] = 1\n2a ∫ a −a h(x)e−jπkx/adx,\nwhere throughout this section j denotes the imaginary part such that j2 = −1, Since h is even, it follows that ĥ[k]’s are real and ĥ[k] = ĥ[−k]. Fourier analysis entails a one-toone correspondence between eigenvalues of T and Fourier coefficients of h: λk(T ) = ĥ[k].\nWe specify h : [−1, 1]→ R as h(x) = |x| and simulate the graphon model with f(x, y) = h(x− y) for x, y ∈ [−1, 1] and the underlying measure µ being uniform over [−1, 1]. Since h(x) = |x|, the Fourier coefficients can be explicitly computed as λk(T ) = ĥ[k] = 2 sin2(πk/2)/(π2k2) with eigenfunctions given by {cos(πkx)}∞k=0 and sin(πkx)}∞k=1. It follows from Theorem 5 that the eigenvalues of M satisfy\n1\nn2 ∑ i≥r+1 E [ λ2i (M) ] ≤ O(n−1) +O(r−3) (14)\nuniformly over all integers r ≥ 0. Therefore, our theory predicts that the MSE of USVT converges to zero at least in a rate of (nρ)−3/4. The simulation results for varying observation probabilities are depicted in Fig. 1. Panel (a) shows the MSE converges to 0 as the number of vertices n increases. In Panel (b), we rescale the x-axis to log(nρ) and the y-axis to the log of MSE. The curves for different ρ align well with each other after the rescaling and decrease linearly with a slope of approximately 0.8, which is close to 3/4 as predicted by our theory."
  }, {
    "heading": "3.2. Sobolev graphon",
    "text": "In this section, we simulate the graphon model with X = [0, 1] and µ being the uniform measure and f(x, y) = min{x, y}. Then ∇xf(x, y) = 1{x≤y} and ∇yf(x, y) = 1{y≤x}. Moreover, |f(x, y)−f(x′, y′)| ≤ |x−x′|+|y−y′|.\nHowever, the second-order weak derivatives of f do not exist. Therefore, f is Sobolev smooth with α = 1. In this case, one can get a bound on the eigenvalue decay rate tighter than Proposition 1 by directly computing λk(T ) and invoking Theorem 5. Note that\n(T g) (x) = ∫ x 0 yg(y)dy + x ∫ 1 x g(y)dy.\nSuppose φ is an eigenfunction of T with eigenvalue λ. Then∫ x 0 yφ(y)dy + x ∫ 1 x φ(y)dy = λφ(x).\nIt follows that φ(0) = 1 and λφ′(x) = ∫ 1 x φ(y)dy. It further implies that φ′(1) = 0 and λφ′′ + φ = 0. Therefore, the eigenfunction and eigenvalue pairs are given by\nφk(x) = sin (2k − 1)πx\n2 , and λk(T ) =\n( 2\n(2k − 1)π\n)2 .\nIt follows from Theorem 5 that the eigenvalues of M satisfy (14) uniformly over all integers r ≥ 0. Therefore, our theory predicts that the MSE of USVT converges to zero in a rate of (nρ)−3/4. The simulation results for varying observation probabilities are depicted in Fig. 2. The curves in Panel (b) for different ρ align well with each other after the rescaling and decrease linearly with a slope of approximately 0.7, which is close to 3/4 as predicted by our theory."
  }, {
    "heading": "4. Conclusions and future work",
    "text": "In this paper, we establish upper bounds to the graphon estimation error of USVT when the average vertex degree is at least logarithmic in n. Our results can be extended to the case of bounded average degrees by first trimming the highdegree vertices (Feige & Ofek, 2005) and then applying USVT. We leave this extension as future work. Another fundamental and open question is whether the minimax optimal rate can be achieved in polynomial-time."
  }],
  "year": 2018,
  "references": [{
    "title": "Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic bp, and the information-computation gap",
    "authors": ["E. Abbe", "C. Sandon"],
    "year": 2015
  }, {
    "title": "Stochastic blockmodel approximation of a graphon: Theory and consistent estimation",
    "authors": ["E.M. Airoldi", "T.B. Costa", "S.H. Chan"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Information-theoretic thresholds for community detection in sparse networks",
    "authors": ["J. Banks", "C. Moore", "J. Neeman", "P. Netrapalli"],
    "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,",
    "year": 2016
  }, {
    "title": "Information-theoretic bounds and phase transitions in clustering, sparse pca, and submatrix localization",
    "authors": ["J. Banks", "C. Moore", "R. Vershynin", "N. Verzelen", "J. Xu"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2018
  }, {
    "title": "A nonparametric view of network models and newman–girvan and other modularities",
    "authors": ["P.J. Bickel", "A. Chen"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2009
  }, {
    "title": "Estimates of singular numbers of integral operators",
    "authors": ["M.S. Birman", "M.Z. Solomyak"],
    "venue": "Russian Mathematical Surveys,",
    "year": 1977
  }, {
    "title": "Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing",
    "authors": ["C. Borgs", "J.T. Chayes", "L. Lovász", "V.T. Sós", "K. Vesztergombi"],
    "venue": "Advances in Mathematics,",
    "year": 2008
  }, {
    "title": "Convergent sequences of dense graphs ii. multiway cuts and statistical physics",
    "authors": ["C. Borgs", "J.T. Chayes", "L. Lovász", "V.T. Sós", "K. Vesztergombi"],
    "venue": "Annals of Mathematics,",
    "year": 2012
  }, {
    "title": "Private graphon estimation for sparse graphs",
    "authors": ["C. Borgs", "J. Chayes", "A. Smith"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Consistent nonparametric estimation for heavy-tailed sparse graphs",
    "authors": ["C. Borgs", "J.T. Chayes", "H. Cohn", "S. Ganguly"],
    "venue": "arXiv preprint arXiv:1508.06675,",
    "year": 2015
  }, {
    "title": "Thy friend is my friend: Iterative collaborative filtering for sparse matrix estimation",
    "authors": ["C. Borgs", "J. Chayes", "C.E. Lee", "D. Shah"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "An iterative step-function estimator for graphons",
    "authors": ["D. Cai", "N. Ackerman", "C. Freer"],
    "venue": "arXiv preprint arXiv:1412.2129,",
    "year": 2014
  }, {
    "title": "A consistent histogram estimator for exchangeable graph models",
    "authors": ["S. Chan", "E. Airoldi"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Matrix estimation by universal singular value thresholding",
    "authors": ["S. Chatterjee"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices",
    "authors": ["Y. Chen", "J. Xu"],
    "venue": "In Proceedings of ICML 2014 (Also",
    "year": 2014
  }, {
    "title": "Schatten classes on compact manifolds: kernel conditions",
    "authors": ["J. Delgado", "M. Ruzhansky"],
    "venue": "Journal of Functional Analysis,",
    "year": 2014
  }, {
    "title": "Spectral techniques applied to sparse random graphs",
    "authors": ["U. Feige", "E. Ofek"],
    "venue": "Random Struct. Algorithms,",
    "year": 2005
  }, {
    "title": "Matrix completion under monotonic single index models",
    "authors": ["R. Ganti", "L. Balzano", "R. Willett"],
    "venue": "In Proceedings of Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Rate-optimal graphon estimation",
    "authors": ["C. Gao", "Y. Lu", "Zhou", "H. H"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Optimal estimation and completion of matrices with biclustering structures",
    "authors": ["C. Gao", "Y. Lu", "Z. Ma", "H.H. Zhou"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Semidefinite programs for exact recovery of a hidden community",
    "authors": ["B. Hajek", "Y. Wu", "J. Xu"],
    "venue": "In Proceedings of Conference on Learning Theory (COLT),",
    "year": 2016
  }, {
    "title": "Modelbased clustering for social networks",
    "authors": ["M.S. Handcock", "A.E. Raftery", "J.M. Tantrum"],
    "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),",
    "year": 2007
  }, {
    "title": "Latent space approaches to social network analysis",
    "authors": ["P.D. Hoff", "A.E. Raftery", "M.S. Handcock"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2002
  }, {
    "title": "Perturbation Theory for Linear Operators",
    "authors": ["T. Kato"],
    "year": 1966
  }, {
    "title": "Optimal graphon estimation in cut distance",
    "authors": ["O. Klopp", "N. Verzelen"],
    "venue": "arXiv preprint arXiv:1703.05101,",
    "year": 2017
  }, {
    "title": "Oracle inequalities for network models and sparse graphon estimation",
    "authors": ["O. Klopp", "A.B. Tsybakov", "N. Verzelen"],
    "venue": "arXiv preprint arXiv:1507.04118,",
    "year": 2015
  }, {
    "title": "Rank penalized estimators for highdimensional matrices",
    "authors": ["O Klopp"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2011
  }, {
    "title": "Random matrix approximation of spectra of integral operators",
    "authors": ["V. Koltchinskii", "E. Giné"],
    "year": 2000
  }, {
    "title": "Nuclearnorm penalization and optimal rates for noisy low-rank matrix completion",
    "authors": ["V. Koltchinskii", "K. Lounici", "Tsybakov", "A. B"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "Asymptotics of spectral projections of some random matrices approximating integral operators",
    "authors": ["V.I. Koltchinskii"],
    "venue": "Progress in Probability,",
    "year": 1998
  }, {
    "title": "A characterization of real analytic functions",
    "authors": ["H. Komatsu"],
    "venue": "Proceedings of the Japan Academy,",
    "year": 1960
  }, {
    "title": "Eigenvalue distribution of compact operators, volume",
    "authors": ["H. König"],
    "venue": "Birkhäuser,",
    "year": 2013
  }, {
    "title": "Introduction to the theory of linear nonselfadjoint operators in hilbert space",
    "authors": ["Krein", "I.G.Y. M"],
    "venue": "American Mathematical Society,",
    "year": 1965
  }, {
    "title": "The largest eigenvalue of sparse random graphs",
    "authors": ["M. Krivelevich", "B. Sudakov"],
    "venue": "Combinatorics, Probability and Computing,",
    "year": 2003
  }, {
    "title": "Unifying framework for crowd-sourcing via graphon estimation",
    "authors": ["C.E. Lee", "D. Shah"],
    "venue": "arXiv preprint arXiv:1703.08085,",
    "year": 2017
  }, {
    "title": "A first course in Sobolev spaces, volume 105",
    "authors": ["G. Leoni"],
    "venue": "American Mathematical Society Providence,",
    "year": 2009
  }, {
    "title": "Eigenvalues of analytic kernels",
    "authors": ["G. Little", "J. Reade"],
    "venue": "SIAM journal on mathematical analysis,",
    "year": 1984
  }, {
    "title": "Large networks and graph limits, volume 60",
    "authors": ["L. Lovász"],
    "venue": "American Mathematical Society Providence,",
    "year": 2012
  }, {
    "title": "Limits of dense graph sequences",
    "authors": ["L. Lovász", "B. Szegedy"],
    "venue": "Journal of Combinatorial Theory, Series B,",
    "year": 2006
  }, {
    "title": "Nonparametric latent feature models for link prediction",
    "authors": ["K. Miller", "M.I. Jordan", "T.L. Griffiths"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "The computer science and physics of community detection: landscapes, phase transitions, and hardness",
    "authors": ["C. Moore"],
    "venue": "arXiv preprint arXiv:1702.00467,",
    "year": 2017
  }, {
    "title": "Dynamic network models and graphon estimation",
    "authors": ["M. Pensky"],
    "venue": "arXiv preprint arXiv:1607.00673,",
    "year": 2016
  }, {
    "title": "Stochastically transitive models for pairwise comparisons: Statistical and computational issues",
    "authors": ["N. Shah", "S. Balakrishnan", "A. Guntuboyina", "M. Wainwright"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Blind regression: Nonparametric regression for latent variable models via collaborative filtering",
    "authors": ["D. Song", "C.E. Lee", "Y. Li", "D. Shah"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Introduction to Nonparametric Estimation",
    "authors": ["A.B. Tsybakov"],
    "venue": "1st edition,",
    "year": 2008
  }, {
    "title": "On the convergence of spectral clustering on random samples: the normalized case",
    "authors": ["U. von Luxburg", "O. Bousquet", "M. Belkin"],
    "year": 2005
  }, {
    "title": "Nonparametric graphon estimation",
    "authors": ["P.J. Wolfe", "S.C. Olhede"],
    "venue": "arXiv preprint arXiv:1309.5936,",
    "year": 2013
  }, {
    "title": "Rates of convergence of spectral methods for graphon estimation",
    "authors": ["J. Xu"],
    "venue": "arXiv 1709.03183,",
    "year": 2017
  }, {
    "title": "Edge label inference in generalized stochastic block models: from spectral theory to impossibility results",
    "authors": ["J. Xu", "L. Massoulié", "M. Lelarge"],
    "venue": "In COLT, pp",
    "year": 2014
  }, {
    "title": "Nonparametric estimation and testing of exchangeable graph models",
    "authors": ["J. Yang", "C. Han", "E. Airoldi"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2014
  }, {
    "title": "High-quality binary protein interaction map of the yeast interactome",
    "authors": ["H. Yu", "P. Braun", "M.A. Yıldırım", "I. Lemmens", "K. Venkatesan", "J. Sahalie", "T. Hirozane-Kishikawa", "F. Gebreab", "N. Li", "N Simonis"],
    "year": 2008
  }, {
    "title": "Estimating network edge probabilities by neighborhood smoothing",
    "authors": ["Y. Zhang", "E. Levina", "J. Zhu"],
    "venue": "arXiv preprint arXiv:1509.08588,",
    "year": 2015
  }],
  "id": "SP:590e2404cd62c018fb3aeba36f30b3be5a9171e6",
  "authors": [{
    "name": "Jiaming Xu",
    "affiliations": []
  }],
  "abstractText": "This paper studies the problem of estimating the graphon function – a generative mechanism for a class of random graphs that are useful approximations to real networks. Specifically, a graph of n vertices is generated such that each pair of two vertices i and j are connected independently with probability ρn × f(xi, xj), where xi is the unknown d-dimensional label of vertex i, f is an unknown symmetric function, and ρn, assumed to be Ω(log n/n), is a scaling parameter characterizing the graph sparsity. The task is to estimate graphon f given the graph. Recent studies have identified the minimax optimal estimation error rate for d = 1. However, there exists a wide gap between the known error rates of polynomialtime estimators and the minimax optimal error rate. We improve on the previously known error rates of polynomial-time estimators, by analyzing a spectral method, namely universal singular value thresholding (USVT) algorithm. When f belongs to either Hölder or Sobolev space with smoothness index α, we show the error rates of USVT are at most (nρ)−2α/(2α+d). These error rates approach the minimax optimal error rate log(nρ)/(nρ) proved in prior work for d = 1, as α increases, i.e., f becomes smoother. Furthermore, when f is analytic with infinitely many times differentiability, we show the error rate of USVT is at most log(nρ)/(nρ). When f is a step function which corresponds to the stochastic block model with k blocks for some k, the error rate of USVT is at most k/(nρ), which is larger than the minimax optimal error rate by at most a multiplicative factor k/ log k. This coincides with the computational gap observed in community detection. A key ingredient of our analysis is to derive the eigenvalue decaying rate of the edge probability matrix using piecewise polynomial The Fuqua School of Business, Duke University, Durham, NC, USA.. Correspondence to: J. Xu <jiaming.xu868@duke.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s). approximations of the graphon function f .",
  "title": "Rates of Convergence of Spectral Methods for Graphon Estimation"
}