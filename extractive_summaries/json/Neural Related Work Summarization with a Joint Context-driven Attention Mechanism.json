{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1776–1786 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1776"
  }, {
    "heading": "1 Introduction",
    "text": "In scientific fields, scholars need to contextualize their contribution to help readers acquire an understanding of their research papers. For this purpose, the related work section of an article serves as a pivot to connect prior domain knowledge, in which the innovation and superiority of current work are displayed by a comparison with previous studies. While citation prediction can assist in drafting a reference collection (Nallapati et al., 2008), consuming all these papers is still a laborious job, where authors must read every source document carefully and locate the most relevant content cautiously.\nAs a solution in saving authors’ efforts, automatic related work summarization is essentially a topic-biased multi-document problem (Cong and Kan, 2010), which relies heavily on humanengineered features to retrieve snippets from the references. Most recently, neural networks enable\n∗Corresponding author\na data-driven architecture sequence-to-sequence (seq2seq) for natural language generation (Bahdanau et al., 2014, 2016), where an encoder reads a sequence of words/sentences into a context vector, from which a decoder yields a sequence of specific outputs. Nonetheless, compared to scenarios like machine translation with an end-to-end nature, aligning a related work section to its source documents is far more challenging.\nTo address the summarization alignment, former studies try to apply an attention mechanism to measure the saliency/novelty of each candidate word/sentence (Tan et al., 2017), with the aim of locating the most representative content to retain primary coverage. However, toward summarizing a related work section, authors should be more creative when organizing text streams from the reference collection, where the selected content ought to highlight the topic bias of current work, rather than retell each reference in a compressed but balanced fashion. This motivates us to introduce the contextual relevance and characterize the relationship among scientific publications accurately.\nGenerally speaking, for a pair of documents, a larger lexical overlap often implies a higher similarity in their research backgrounds. Yet such a hypothesis is not always true when sampling content from multiple relevant topics. Take “DSSM”1 as an example, from viewpoint of the abstract similarity, those references investigating “Information Retrieval”, “Latent Semantic Model” or “Clickthrough Data Mining” could be of more importance in correlation and should be greatly sampled for the related work section. But in reality, this article spends a bit larger chunk of texts (about 58%) to elaborate “Deep Learning” during the literature review, which is quite difficult for machines to grasp the contextual relevance therein. In addi-\n1Learning deep structured semantic models for web search using clickthrough data (Huang et al., 2013)\ntion, other situations like emerging new concepts also suffer from the terminology variation or paraphrasing in varying degrees.\nIn this study, we utilize a heterogeneous bibliography graph to embody the relationship within a scalable scholarly database. Over the recent past, there is a surge of interest in exploiting diverse relations to analyze bibliometrics, ranging from literature recommendation (Yu et al., 2015) to topic evolvement (Jensen et al., 2016). In a graphical sense, interconnected papers transfer the credit among each other directly/indirectly through various patterns, such as paper citation, author collaboration, keyword association and releasing on series of venues, which constitutes the graphic context for outlining concerned topics. Unfortunately, a variety of edge types may pollute the information inquiry, where a slice of edges are not so important as the others on sampling content. Meanwhile, most existing solutions in mining heterogeneous graphs depend on the human supervision, e.g., hyperedge (Bu et al., 2010) and metapath (Swami et al., 2017). This is usually not easy to access due to the complexity of graph schemas.\nOur contribution is threefold: First, we explore the edge-type usefulness distribution (EUD) on a heterogeneous bibliography graph, which enables the relationship discovery (between any pair of papers) for sampling the interested information. Second, we develop a novel seq2seq summarizer for the automatic related work summarization, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within both textual and graphic contexts. Third, we conduct experiments on 8,080 papers with native related work sections, and experimental results show that our approach outperforms a typical seq2seq summarizer and five classical summarization baselines significantly."
  }, {
    "heading": "2 Related Work",
    "text": "This study touches on several strands of research within automatic related work summarization and seq2seq summarizer as follows.\nThe idea of creating a related work section automatically is pioneered by Cong and Kan (2010) who design two rule-based strategies to extract sentences for general and detailed topics respectively. Subsequently, Hu and Wan (2014) exploit probabilistic latent semantic indexing to split candidate texts into different topic-biased parts, then\napply several regression models to learn the importance of each sentence. Similarly, Widyantoro and Amin (2014) transform the summarization problem into classifying rhetorical categories of sentences, where each sentence is represented as a feature vector containing word frequency, sentence length and etc. Most recently, Chen and Hai (2016) construct a graph of representative keywords, in which a minimum steiner tree is figured out to guide the summarization as finding the least number of sentences to cover the discriminated nodes. In general, compared to traditional summaries, the automatic related work summarization receives less concerns over the past. However, these existing solutions cannot work without manual intervention, which limits the application scale to an extremely small size (see Table 1).\nThe earliest seq2seq summarizer stems from Rush et al. (2015) which utilizes a feed-forward network for compressing sentences, and later is expanded by Chopra et al. (2016) with a recurrent neural network (RNN). On this basis, Nallapati et al. (2016a,c) and Chen et al. (2016) both present a set of RNN-based models to address various aspects of abstractive summarization. Typically, Cheng and Lapata (2016) propose a general seq2seq summarizer, where an encoder learns the representation of documents while a decoder generates each word/sentence using an attention mechanism. With further research, Nallapati et al. (2016b) extend the sentence compression by trying a hierarchical attention architecture and a limited vocabulary during the decoding phase. Next, Narayan et al. (2017) leverage the side information as an attention cue to locate focus regions for summaries. Recently, inspired by PageRank, Tan et al. (2017) introduce a graph-based attention mechanism to tackle the saliency problem. Nonetheless, these methods all discuss the single-document scenario, which is far from the nature of automatic related work summarization.\nIn this study, derived from the general seq2seq summarizer of Cheng and Lapata (2016), we propose a joint context-driven attention mechanism to\nmeasure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. To our best knowledge, we make the first attempt to develop a neural data-driven solution for the automatic related work summarization, and the practice of using the joint context as an attention cue is also less explored to date. Besides, this study is launched on a dataset with up to 8,080 papers, which is much greater than previous studies and makes our results more convincing.\nSince text summarization via word-by-word generation is not mature at present (Cheng and Lapata, 2016; Nallapati et al., 2016b; Tan et al., 2017), we adopt the extractive sentential fashion for our summarizer, where a related work section is created by extracting and linking sentences from a reference collection. Meanwhile, this study follows the mode of Cong and Kan (2010) who assume that the collection is given as part of the input, and do not consider the citation sentences of each reference."
  }, {
    "heading": "3 Methodology",
    "text": ""
  }, {
    "heading": "3.1 Problem Formulation",
    "text": "To adapt the seq2seq paradigm, we formulate the automatic related work summarization into a sequential text generation problem as follows.\nGiven an unedited paper t (target document) and its n-size reference collection Rt = {rt1:n}, we draw up a related work section for t by selecting sentences from Rt. To be specific, each reference (source document) will be traversed one time sequentially, and without loss of generality, in the descending order of their significance to t. Consequently, all sentences to be selected are concatenated into an m-length sequence St = {st1:m} to feed the summarizer. For each candidate sentence stj , once being visited, a label y t j ∈ {0, 1} will be determined synchronously based on whether or not this sentence should be covered into the output. Our objective is to maximize the loglikelihood probability of observed labels Yt = {yt1:m} under Rt, St and summarizer parameters θ, as shown below.\nmax m∑ j=1 log Pr(ytj | Rt; St; θ) (1)"
  }, {
    "heading": "3.2 Random Walk on Heterogeneous Bibliography Graph",
    "text": "Prior works have illustrated that one of the most promising channels for information recommendation is the community network (Guo and Liu, 2015). In this study, we verify this hypothesis toward the content sampling of scientific summarization, by investigating heterogeneous relations among different kinds of objects such as papers, authors, keywords and venues.\nFor measuring the relationship among scientific publications, we introduce a directed graph G = (V, E) to contain various bibliographical connections, as shown in Figure 1, which involves four objects and ten edge types in total. Each edge ej,i ∈ E is assigned a value π(ej,i)z ∈ [0, 1] to indicate the transition probability between two nodes vj , vi ∈ V, where π(ej,i) ∈ R returns the unknown edge-type usefulness of ej,i, and z ∈ R is a normalizing weight. For most of edge types, we model the weight as one divided by the number of outgoing links of the same kind. But regarding the “contribution” category, the weight modeling is accomplished by PageRank with Priors (White and Smyth, 2003). Note that different edge types usually take very uneven importance in one particular task (Yu et al., 2015), and it is quite difficult to enable the classical heterogeneous graph mining without expert defined paths for random walk (Bu et al., 2010; Swami et al., 2017).\nIn this study, we propose an unsupervised approach to capture the connectivity diversity, by introducing an optimal EUD for navigating random walkers on the heterogeneous bibliography graph. Given a target document t, the optimized useful-\nness assignment can help those walkers lock a topn recommendation R̄t to best match the reference collection Rt, as shown in Eq. 2. On this basis, a well-performing algorithm node2vec (Grover and Leskovec, 2016) is adopted to conduct an unsupervised random walk to vectorize every node ∀v∗ ∈ V into a d-dimensional embedding ϕ(v∗) ∈ Rd so that any edge ∀e∗ ∈ E can be calculated therefrom. Specifically, we employ evolutionary algorithm (EA) to tune the EUD, which enjoys advantages over conventional gradient methods in both convergence speed and accuracy.\narg max ∑ t n∑ j=1 log Pr(rtj ∈ R̄t | EUD) (2)\nEA Setup We use an array of real numbers x1:10 to code an individual in the population, where xj ∈ [0, 1] denotes the usefulness of j-th edge type. Given an EUD, PageRank (Page, 1998) runs on graph to infer the relative importance of each node for each target document, and a fitness function is applied to judge how well this EUD satisfies locating the ground truth references as Eq. 3, in which if rtj belongs to R̄t, then α(r t j , R̄t) ∈ N returns the ranking of rtj within R̄t, and otherwise a big penalty coefficient to prevent irrelevant references to be recommended. Like most other optimizations, this procedure starts with a randomly generated population.\nmax 1∑\nt ∑n j=1 ∣∣∣j − α(rtj , R̄t)∣∣∣ (3) EA Operator We choose the operator from differential evolution (Das and Suganthan, 2011) to generate offsprings for each individual. The basic idea is to utilize the difference between different individuals to disturb each trial object. First, three distinct individuals xr11:10, x r2 1:10, x r3 1:10 are sampled randomly from current population to create a variant xvar1:10, as shown in Eq. 4, where f ∈ R indicates the scaling factor. Next, xvar1:10 is crossed with a trial object xtri1:10 to build a hybrid one x hyb 1:10 as Eq. 5, in which c ∈ [0, 1] denotes the crossover factor and u ∈ [0, 1] represents an uniform random number. At last, the fitnesses of xtri1:10 and x hyb 1:10 are compared, and the better one will be saved as the offspring into a new round of evolution.\nxvarj = x r1 j + f × (x r2 j − x r3 j ) (4)\nx hyb j = x var j , if u ≤ c\nxtrij , otherwise (5)"
  }, {
    "heading": "3.3 Neural Extractive Summarization",
    "text": "As Figure 2 shows, we model our seq2seq summarizer with a hierarchical encoder and an attentionbased decoder, as described below. Hierarchical Encoder Our encoder consists of two major layers, namely a convolutional neural network (CNN) and a long-short-term memory (LSTM)-based RNN. Specifically, the CNN deals with word-level texts to derive sentencelevel meanings, which are then taken as inputs to the RNN for handling longer-range dependency within lager units like a paragraph and even a whole paper. This conforms to the nature of document that is composed from words, sentences and higher levels of abstraction (Narayan et al., 2017).\nConsider a sentence of p words stj = {wtj,1:p}, where each word wtj,i can be represented by a ddimensional embedding φ(wtj,i) ∈ Rd. Previous studies have illustrated the strength of CNN in presenting sentences, because of its capability to learn compressed expressions and address sentences with variable lengths (Kim, 2014). First, a convolution kernel k ∈ Rd×q×d is applied to each possible window of q words to construct a list of feature maps as:\ngtj,i = tanh ( k × φ(wtj,i:i+q−1) + b ) (6)\nwhere b ∈ Rd denotes the bias term. Next, maxover-time pooling (Collobert et al., 2011) is performed on all generated features to obtain the sentence embedding as:\nφ(stj ) = max 1≤i≤d\n( gtj,1:p−q+1[i, :] ) (7)\nwhere [i, :] denotes the i-th row of matrix. Given a sequence of sentences St = {st1:m}, we then take the RNN to yield an equal-length array of hidden states, in which LSTM has proved to alleviate the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Each hidden state can be viewed as a local representation with focusing on current and former sentences together, which is updated as: htj = LSTM ( φ(stj ), h t j−1 ) ∈ Rd.\nIn practice, we use multiple kernels with various widths to produce a group of embeddings for each\nsentence, and average them to capture the information inside different n-grams. As Figure 2 (bottom) shows, the sentence stj involves six words, and two kernels of widths two (orange) and three (green) abstract a set of five and four feature maps respectively. Meanwhile, since rhetorical structure theory (Mann and Thompson, 2009) points out that association must exist in any two parts of coherent texts, RNN is only applicable to manage the sentence relation within a single document, because we cannot expect the dependency between two sections from different references.\nAttention-based Decoder Our decoder labels each sentence stj as 0/1 sequentially, according to whether it is salient or novel enough, plus if relevant to the target document t or not. As shown in Figure 2 (top), the binary decision ytj is made by both the hidden state htj and the context vector h̄ t j from an attention mechanism (grey background). In particular, this attention (red dash line) is acted as an intermediate stage to determine which sentences to highlight so as to provide the contextual information for current decision (Bahdanau et al., 2014). Given Ht = {ht1:m}, this decoder returns\nthe probability of ytj = 1 as below:\nPr(ytj = 1 | Rt; St; θ) = sigmoid ( δ(htj , h̄ t j ) ) (8)\nh̄tj = m∑ i=1 aj,ih t i (9)\nwhere δ(htj , h̄ t j ) ∈ R denotes a fully connected layer with as input the concatenation of htj and h̄ t j , and aj,i ∈ [0, 1] is the attention weight indicating how much the supporting sentence sti contributes to extracting the candidate one stj .\nApart from saliency and novelty two traditional attention factors (Chen et al., 2016; Tan et al., 2017), we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far, as shown in Eq. 10 and Eq. 11. To be specific: 1) htTj Wsh t i represents the saliency of sti to s t j ; 2) −dtTj Wnhti indicates the novelty of sti to the dynamic output d t j ; 3) φ(t)TWthti denotes the relevance of s t i to t from the textual context; 4) ϕ(t)TWgϕ(hti ) refers to the relevance from the graphic context. More\nconcretely, W∗ ∈ Rd characterizes the learnable matrix, φ(t) returns the average of hidden states from t, ϕ(t) and ϕ(hti ) return the node embeddings of both t and the source document that hti belongs to respectively. Note that φ(·) and ϕ(·) represent two distinct embedding spaces, where the former reflects the lexical collocations of corpus, and the latter embodies the connectivity patterns of associated graph.\naj,i = h tT j Wsh t i # saliency\n−dtTj Wnhti # novelty\n+φ(t)TWth t i # relevance1\n+ϕ(t)TWgϕ(h t i ) # relevance2\n(10)\ndtj = j−1∑ i=1 Pr(ytj = 1 | Rt; St; θ)× hti (11)\nThe basic idea behind our attention mechanism is as follows: if a supporting sentence more resembles a candidate one, or overlaps less with the dynamic output, or is more relevant to the target document, then it can provide more contextual information to facilitate current decision on being extracted or not, thereby taking a higher weight in the generated context vector. This innovative attention will guide our goal related work section to maximize the representativeness of selected sentences (saliency & novelty), while minimizing the semantic distance to the target document (relevance). This is consistent with the way that scholars consume a reference collection, with the minmax objective in their minds."
  }, {
    "heading": "4 Experiment",
    "text": ""
  }, {
    "heading": "4.1 Experimental Setup",
    "text": "This section presents the experimental setup for assessing our approach, including 1) dataset used for training and testing, 2) implementation details, 3) contrast methods and evaluation metrics. Dataset We conduct experiments on a dataset2 created from the ACM digital library, where metadata and full texts are derived from PDF files. To be detailed, this dataset includes 371,891 papers,\n2To help readers reproduce the experiment outcome, we share part of the experiment data while the copyrighted information is removed. https://github.com/kuadmu/ 2018EMNLP\n779,810 authors, 9,204 keywords and 807 venues in total. Note that we ignore the keyword with frequency below a certain threshold, and adopt greedy matching of Guo et al. (2013) to generate pseudo keywords for papers lacking topic descriptions. For each target document, the references are traversed by the descending order of the cited number in related work section (primary) and in full paper (secondary) successively. We first apply a series of pre-processings such as lowercasing and stemming to standardize candidate sentences, then remove those which are too short/long (< 7 or > 80 words). On this basis, a total of 8,080 papers are selected to evaluate our approach, each containing more than 15 references found in the dataset and a related work section of at least 500 words. But as for the heterogeneous bibliography graph, all source data have to be imported to ensure the structural integrity of communities. Besides, this graph should be constructed year-byyear to preclude the effect of later publications on earlier ones.\nImplementation We use Tensorflow for implementation, where both the dimensions of embedding and hidden state are equally 128. For the CNN, word2vec (Mikolov et al., 2013) is utilized to initialize the word embeddings, which can be further tuned during the training phase. Meanwhile, we follow the work of Kim (2014) to apply a list of kernels with widths {3, 4, 5}. As for the RNN, each LSTM module is set to one single layer, and all input documents are padded to the same length, along with a mark to indicate the real number of sentences. Based on these settings, we train our summarizer using Adam with the default in Kingma and Ba (2014), and perform mini-batch cross-entropy training with a batch of one target document for 20 epochs.\nTo create training data for our summarizer, each reference needs to be annotated with the ground truth in advance, i.e., candidate sentences are tagged with 0/1 for indicating summary-worthy or not. Specifically, we follow a heuristic practice of Cao et al. (2016) and Nallapati et al. (2016b) to compute ROUGE-2 score (Lin and Hovy, 2003) for each sentence, in terms of the native related work sections (gold standards). Next, those sentences with high scores are chosen as the positive samples, and the rest as the negative ones, such that the total score of selected sentences is maximized with respect to the gold standard. As for\ntesting, we relax the number of sentences to be selected, and focus on the classification probability from Eq. 8. In this study, cross validation is applied to split the dataset into ten parts equally at random, in which nine are used for training and the other one for testing.\nEvaluation We adopt the widely used toolkit ROUGE (Lin and Hovy, 2003) to evaluate the summarization performance automatically. In particular, we report ROUGE-1 and ROUGE-2 (unigram and bigram overlapping) as a way to assess the informativeness, and ROUGE-L (the longest common subsequence) as a means to assess the fluency, in terms of fixed bytes of gold standards.\nTo validate the proposed attention mechanism, we compare our approach (denoted as P.S+N+Rteg+EUD) against six variants, including: 1) P.void: a plain seq2seq summarizer without attentions; 2) P.S: use the saliency as an only attention factor; 3) P.S+N: leverage both the saliency and novelty; 4) P.S+N+Rt: incorporate the relevance from the textual context; 5) P.S+N+Rtog: gain the relevance from the graphic context of a homogeneous citation graph; 6) P.S+N+Rteg: utilize the heterogeneous bibliography graph, but with each edge type the same usefulness.\nIn addition, we also select six representative summarization methods as a benchmark group. The first one is the general seq2seq summarizer by Cheng and Lapata (2016), denoted as PointerNet, which employs an attention mechanism to extract sentences directly after reading them. Following are five classical generic solutions, including: 1) Luhn (Luhn, 1958): a heuristic summarization based on word frequency and distribution; 2) MMR (Carbonell and Goldstein, 1998): a diversity-based re-ranking to produce summaries; 3) LexRank (Erkan et al., 2004): a graph-based summary technique inspired by PageRank and HITS; 4) SumBasic (Nenkova and Vanderwende, 2005): a frequency-based summarizer with duplication removal; 5) NltkSum (Acanfora et al., 2014): a natural language tookit (NLTK)-based implementation for summarization.\nFor clarity, Luhn, LexRank and SumBasic are analogous to the work of Hu and Wan (2014) which extracts sentences scoring the highest in significance, and they are also contrasted in the latest studies on neural summarizers (Chen et al., 2016; Tan et al., 2017). Meanwhile, MMR often serves as a part/post-processing of existing tech-\nniques to avoid the redundancy (Cohan and Goharian, 2017), and we introduce NltkSum to investigate the impact of grammatical/semantic analysis to the automatic related work summarization. Note that former studies specially for this task require extensive human involvements (see Table 1), thus we cannot apply them to such a large dataset of this study."
  }, {
    "heading": "4.2 Results and Discussion",
    "text": "Table 2 reports the evaluation comparison over ROUGE metrics. From the top half, all scores appear a gradual upward trend with incorporation of saliency, novelty, relevance (from both textual and graphic contexts) and EUD into consideration one after another, which demonstrates the validity of our attention mechanism for summarizing related work sections. To be specific, we further reach the following conclusions:\n1) P.void vs. P.S vs. P.S+N: Both saliency and novelty are two effective factors to locate the required content for summaries, which is consistent with prior studies.\n2) P.S+N vs. P.S+N+Rt: Contextual relevance does contribute to address the alignment between a related work section and its source documents.\n3) P.S+N+Rt vs. P.S+N+Rtog: Textual context alone cannot provide entire evidence to characterize the relationship among scientific publications exactly.\n4) P.S+N+Rtog vs. P.S+N+Rteg: Heterogeneous bibliography graph involves richer contextual information than a homogeneous citation graph.\n5) P.S+N+Rteg vs. P.S+N+Rteg+EUD: EUD plays an indispensable role in organizing accurate contextual relevance on a heterogeneous graph.\nContinuing the “DSSM”, Figure 3 visualizes the number of extracted words on each reference\ncluster3 under different attention factors. It can be seen that only after adding the relevance especially that from the graphic context into attentions, our summarizer can correctly sample the content from “Deep Learning” (yellow line), and eliminate that originated from “Other Sources” by a big margin (green line). As this example falls into the methodology transferring, a host of its involved word collocations are not idiomatic combinations yet, such as “Deep Neural Network” cooccurs with “Clickthrough Data” that is more frequently related to “Latent Semantic Analysis” at that time, which results in a somewhat biased textual context. By contrast, the graphic context will suffer less from this bias because it characterizes the connectivity patterns (real-time setup) instead of n-gram statistics, thus offering a more robust measure for the contextual relevance.\nThe bottom half of Table 2 illustrates the superiority of our approach over six representative summarization methods. Above all, Luhn, LexRank and MMR three summarizers that simply exploit shallow text features (word frequency and associated sentence similarity) to measure either significance or redundancy fall far behind the plain variant P.void, which partly reflects the strength of seq2seq paradigm in summarizing a related work section. Second, with combination of significance and redundancy, SumBasic achieves a drastic increase on ROUGE-1 and a mild raise on\n3We pack the references cited in the same subsection of the related work section as one reference cluster.\nROUGE-2 respectively, but it still cannot improve ROUGE-L marginally. This is because simple text statistics cannot present deeper levels of natural language understanding to catch larger-grained units of co-occurrence. Third, NltkSum benefits from a NLTK library so as to access grammatical/semantic supports, thereby having the best informativeness (ROUGE-1 and ROUGE-2) among the five generic baselines, and meanwhile a comparable fluency (ROUGE-L) with our approach. Finally, as a deep learning solution, although PointerNet takes both hidden states and previously labeled sentences into account, at each decoding step it focuses on only current and just one previous sentences, lacking a comprehensive consideration on saliency, novelty and more importantly the contextual relevance (< P.S+N).\nTo better verify the summarization performance, we also conduct a human evaluation on 35 papers containing more than 30 references in the dataset. We assign a number of raters to compare each generated related work section against the gold standard, and judge by three independent aspects as: 1) How compliant is the related work section to the target document? 2) How intuitive is the related work section for readers to grasp the key content? 3) How useful is the related work section for researchers to prepare their final literature reviews? Note that we do not allow any ties during the comparison, and each property is assessed with a 5-point scale of 1 (worst) to 5 (best).\nTable 3 displays how often raters rank each summarizer as the 1st, 2nd and so on, in terms of\nbest-to-worst. Specifically, our approach comes the 1st on 40% of the time, which is followed by NltkSum that is considered the best on 21% of the time (almost half of ours), and PointerNet with quite equal proportions on each ranking. Furthermore, the other four summarizers account for obviously lower ratings in general. To attain the statistical significance, one-way analysis of variance (ANOVA) is performed on the obtained ratings, and the results show that our approach is better than all six contrast methods significantly (p < 0.01), which means that the conclusion drawn by Table 2 is sustained."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we highlight the contextual relevance for the automatic related work summarization, and analyze the graphic context to characterize the relationship among scientific publications accurately. We develop a neural data-driven summarizer by leveraging the seq2seq paradigm, where a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Extensive experiments demonstrate the validity of the proposed attention mechanism, and the superiority of our approach over six representative summarization baselines.\nIn future work, an appealing direction is to organize the selected sentences in a logical fashion, e.g., by leveraging a topic hierarchy tree to determine the arrangement of the related work section (Cong and Kan, 2010). We also would like to take the citation sentences of each reference into consideration, which is another concise and universal data source for scientific summarization (Chen and Hai, 2016; Cohan and Goharian, 2017). At the end of this paper, we believe that extractive methods are by no means the final solutions for literature review generation due to plagiarism concerns,\nand we are going to put forward a fully abstractive version in further studies."
  }, {
    "heading": "Acknowledgement",
    "text": "We would like to thank the anonymous reviewers for their valuable comments. This work is partially supported by the National Science Foundation of China under grant No. 71271034."
  }],
  "year": 2018,
  "references": [{
    "title": "Natural language processing: generating a summary of flood disasters",
    "authors": ["Joseph Acanfora", "Marc Evangelista", "David Keimig", "Myron Su."],
    "venue": "Cell, 41(2):383–94.",
    "year": 2014
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Endto-end attention-based large vocabulary speech recognition",
    "authors": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio."],
    "venue": "Proceedings of the 41st IEEE ICASSP International Conference on Acoustics,",
    "year": 2016
  }, {
    "title": "Music recommendation by unified hypergraph:combining social media information and music content",
    "authors": ["Jiajun Bu", "Shulong Tan", "Chun Chen", "Can Wang", "Hao Wu", "Lijun Zhang", "Xiaofei He."],
    "venue": "Proceedings of the ACM SIGMM International Con-",
    "year": 2010
  }, {
    "title": "Attsum: Joint learning of focusing and summarization with neural attention",
    "authors": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei", "Yanran Li."],
    "venue": "arXiv preprint arXiv:1604.00125.",
    "year": 2016
  }, {
    "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
    "authors": ["Jaime Carbonell", "Jade Goldstein."],
    "venue": "Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Re-",
    "year": 1998
  }, {
    "title": "Summarization of related work through citations",
    "authors": ["Jingqiang Chen", "Zhuge Hai."],
    "venue": "Proceedings of the 12th IEEE SKG International Conference on Semantics, Knowledge and Grids, Beijing, China, pages 54–61.",
    "year": 2016
  }, {
    "title": "Distraction-based neural networks for modeling documents",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Si Wei", "Si Wei", "Hui Jiang."],
    "venue": "Proceedings of the ACM IJCAI International Joint Conference on Artificial Intelligence, New York, USA, pages 2754–2760.",
    "year": 2016
  }, {
    "title": "Neural summarization by extracting sentences and words",
    "authors": ["Jianpeng Cheng", "Mirella Lapata."],
    "venue": "Proceedings of the 54th ACL Annual Meeting of the Association for Computational Linguistics, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Abstractive sentence summarization with attentive recurrent neural networks",
    "authors": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."],
    "venue": "Proceedings of the NAACL Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2016
  }, {
    "title": "Scientific article summarization using citation-context and article’s discourse structure",
    "authors": ["Arman Cohan", "Nazli Goharian."],
    "venue": "arXiv preprint arXiv:1704.06619, pages 390–400.",
    "year": 2017
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(1):2493–2537.",
    "year": 2011
  }, {
    "title": "Towards automated related work summarization",
    "authors": ["Duy Vu Hoang Cong", "Min Yen Kan."],
    "venue": "Proceedings of the 23rd ACM COLING International Conference on Computational Linguistics, Beijing, China, pages 427–435.",
    "year": 2010
  }, {
    "title": "Differential evolution: A survey of the state-of-the-art",
    "authors": ["Swagatam Das", "Ponnuthurai Nagaratnam Suganthan."],
    "venue": "IEEE Transactions on Evolutionary Computation, 15(1):4–31.",
    "year": 2011
  }, {
    "title": "Lexrank: graphbased lexical centrality as salience in text summarization",
    "authors": ["Erkan", "Radev", "R Dragomir."],
    "venue": "Journal of Qiqihar Junior Teachers College, 22:2004.",
    "year": 2004
  }, {
    "title": "node2vec: Scalable feature learning for networks",
    "authors": ["Aditya Grover", "Jure Leskovec."],
    "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, Usa, pages 855–864.",
    "year": 2016
  }, {
    "title": "Automatic feature generation on heterogeneous graph for music recommendation",
    "authors": ["Chun Guo", "Xiaozhong Liu."],
    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santi-",
    "year": 2015
  }, {
    "title": "Scientific metadata quality enhancement for scholarly publications",
    "authors": ["Chun Guo", "Jinsong Zhang", "Xiaozhong Liu."],
    "venue": "Ischools.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jrgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Automatic generation of related work sections in scientific papers: an optimization approach",
    "authors": ["Yue Hu", "Xiaojun Wan."],
    "venue": "Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, pages",
    "year": 2014
  }, {
    "title": "Learning deep structured semantic models for web search using clickthrough data",
    "authors": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."],
    "venue": "Proceedings of the 22nd ACM CIKM international Conference on Informa-",
    "year": 2013
  }, {
    "title": "Generation of topic evolution trees from heterogeneous bibliographic networks",
    "authors": ["Scott Jensen", "Xiaozhong Liu", "Yingying Yu", "Stasa Milojevic."],
    "venue": "Journal of Informetrics, 10(2):606–621.",
    "year": 2016
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Eprint Arxiv.",
    "year": 2014
  }, {
    "title": "Adam: a method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Computer Science.",
    "year": 2014
  }, {
    "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
    "authors": ["Chin Yew Lin", "Eduard Hovy."],
    "venue": "Proceedings of the NAACL The Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2003
  }, {
    "title": "The automatic creation of literature abstracts",
    "authors": ["H.P. Luhn."],
    "venue": "IBM Corp.",
    "year": 1958
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C. Mann", "Sandra A. Thompson."],
    "venue": "Text & Talk, 8(3):243–281.",
    "year": 2009
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Computer Science.",
    "year": 2013
  }, {
    "title": "Sequence-to-sequence rnns for text summarization",
    "authors": ["Ramesh Nallapati", "Bing Xiang", "Bowen Zhou."],
    "venue": "Proceedings of the International Conference on Learning Representations, Workshop track, San Juan, Puerto Rico.",
    "year": 2016
  }, {
    "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
    "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."],
    "venue": "arXiv preprint arXiv:1611.04230v1.",
    "year": 2016
  }, {
    "title": "Abstractive text summarization using sequenceto-sequence rnns and beyond",
    "authors": ["Ramesh Nallapati", "Bowen Zhou", "Cicero Nogueira Dos Santos", "Caglar Gulcehre", "Bing Xiang."],
    "venue": "arXiv preprint arXiv:1602.06023v5.",
    "year": 2016
  }, {
    "title": "Joint latent topic models for text and citations",
    "authors": ["Ramesh M. Nallapati", "Amr Ahmed", "Eric P. Xing", "William W. Cohen."],
    "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Usa,",
    "year": 2008
  }, {
    "title": "Neural extractive summarization with side information",
    "authors": ["Shashi Narayan", "Nikos Papasarantopoulos", "Shay B. Cohen", "Mirella Lapata."],
    "venue": "arXiv preprint arXiv:1704.04530.",
    "year": 2017
  }, {
    "title": "The impact of frequency on summarization",
    "authors": ["Ani Nenkova", "Lucy Vanderwende."],
    "venue": "Microsoft Research.",
    "year": 2005
  }, {
    "title": "The pagerank citation ranking : Bringing order to the web, online manuscript",
    "authors": ["L Page."],
    "venue": "Stanford Digital Libraries Working Paper, 9(1):1–14.",
    "year": 1998
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the ACL EMNLP Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, pages",
    "year": 2015
  }, {
    "title": "metapath2vec: Scalable representation learning for heterogeneous networks",
    "authors": ["Ananthram Swami", "Ananthram Swami", "Ananthram Swami."],
    "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and",
    "year": 2017
  }, {
    "title": "Abstractive document summarization with a graph-based attentional neural model",
    "authors": ["Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao", "Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao."],
    "venue": "Proceedings of the 55th ACL Annual Meeting of the Association for Computa-",
    "year": 2017
  }, {
    "title": "Algorithms for estimating relative importance in networks",
    "authors": ["Scott White", "Padhraic Smyth."],
    "venue": "Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, USA, pages 266–275.",
    "year": 2003
  }, {
    "title": "Citation sentence identification and classification for related work summarization",
    "authors": ["Dwi H Widyantoro", "Imaduddin Amin."],
    "venue": "Proceedings of the ICACSIS International Conference on Advanced Computer Science and Information Systems, pages",
    "year": 2014
  }, {
    "title": "Random walk and feedback on scholarly network",
    "authors": ["Yingying Yu", "Xiaozhong Liu", "Zhuoren Jiang."],
    "venue": "Proceedings of the 1st ACM GSB@SIGIR International Workshop on Graph Search and Beyond, Santiago, Chile, pages 33–37.",
    "year": 2015
  }],
  "id": "SP:664f52347fcdcd24083b6213bf6244ecc86c8a0e",
  "authors": [{
    "name": "Yongzhen Wang",
    "affiliations": []
  }, {
    "name": "Xiaozhong Liu",
    "affiliations": []
  }, {
    "name": "Zheng Gao",
    "affiliations": []
  }],
  "abstractText": "Conventional solutions to automatic related work summarization rely heavily on humanengineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.",
  "title": "Neural Related Work Summarization with a Joint Context-driven Attention Mechanism"
}