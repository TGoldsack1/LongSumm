{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A mystery about deep nets is that they generalize (i.e., predict well on unseen data) despite having far more parameters than the number of training samples. One commonly voiced explanation is that regularization during training –whether implicit via use of SGD (Neyshabur et al., 2015c; Hardt et al., 2016) or explicit via weight decay, dropout (Srivastava et al., 2014), batch normalization (Ioffe and Szegedy, 2015), etc. –reduces the effective capacity of the net. But Zhang et al. (2017) questioned this received wisdom and\nAuthors listed in alphabetical order 1Princeton University, Computer Science Department 2Duke University, Computer Science Department 3Institute for Advanced Study, School of Mathematics. Correspondence to: Rong Ge <rongge@cs.duke.edu>, Behnam Neyshabur <bneyshabur@ias.edu>, Yi Zhang <y.zhang@cs.princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfueled research in this area by showing experimentally that standard architectures using SGD and regularization can still reach low training error on randomly labeled examples (which clearly won’t generalize).\nClearly, deep nets trained on real-life data have some properties that reduce effective capacity, but identifying them has proved difficult —at least in a quantitative way that yields sample size upper bounds similar to classical analyses in simpler models such as SVMs (Bartlett and Mendelson, 2002; Evgeniou et al., 2000; Smola et al., 1998) or matrix factorization (Fazel et al., 2001; Srebro et al., 2005).\nQualitatively (Hochreiter and Schmidhuber, 1997; Hinton and Van Camp, 1993) suggested that nets that generalize well are flat minima in the optimization landscape of the training loss. Recently Keskar et al. (2016) show using experiments with different batch-sizes that sharp minima do correlate with higher generalization error. A quantitative version of “flatness” was suggested in (Langford and Caruana, 2001): the net’s output is stable to noise added to the net’s trainable parameters. Using PAC-Bayes bound (McAllester, 1998; 1999) this noise stability yielded generalization bounds for fully connected nets of depth 2. The theory has been extended to multilayer fully connected nets (Neyshabur et al., 2017b), although thus far yields sample complexity bounds much worse than naive parameter counting. (Same holds for the earlier Bartlett and Mendelson (2002); Neyshabur et al. (2015b); Bartlett et al. (2017); Neyshabur et al. (2017a); Golowich et al. (2017); see Figure 3). Another notion of noise stability —closely related to dropout and batch normalization—is stability of the output with respect to the noise injected at the nodes of the network, which was recently shown experimentally (Morcos et al., 2018) to improve in tandem with generalization ability during training, and to be absent in nets trained on random data. Chaudhari et al. (2016) suggest adding noise to gradient descent to bias it towards finding flat minima.\nWhile study of generalization may appear a bit academic — held-out data easily establishes generalization in practice— the ultimate hope is that it will help identify simple, measurable and intuitive properties of well-trained deep nets, which in turn may fuel superior architectures and faster training. We hope the detailed study —theoretical and empirical—in the current paper advances this goal.\nContributions of this paper. 1. A simple compression framework (Section 2) for prov-\ning generalization bounds, perhaps a more explicit and intuitive form of the PAC-Bayes work. It also yields elementary short proofs of recent generalization results (Section 2.2).\n2. Identifying new form of noise-stability for deep nets: the stability of each layer’s computation to noise injected at lower layers. (Earlier papers worked only with stability of the output layer.) Figure 1 visualizes the stability of network w.r.t. Gaussian injected noise. Formal statements require a string of other properties (Section 3). All are empirically studied, including their correlation with generalization (Section 6).\n3. Using the above properties to derive efficient and provably correct algorithms that reduce the effective number of parameters in the nets, yielding generalization bounds that: (a) are better than naive parameter counting (Section 6) (b) depend on simple, intuitive and measurable properties of the network (Section 4) (c) apply also to convolutional nets (Section 5) (d) empirically correlate with generalization (Section 6).\nThe main idea is to show that noise stability allows individual layers to be compressed via a linear-algebraic procedure Algorithm 1. This results in new error in the output of the layer. This added error is “Gaussian-like” and tends to get attenuated as it propagates to higher layers.\nOther related works. Dziugaite and Roy (2017) use nonconvex optimization to optimize the PAC-Bayes bound and get a non-vacuous sample bound on MNIST. While very creative, this provides little insight into favorable properties of networks. Liang et al. (2017) have suggested FisherRao metric, a regularization based on the Fisher matrix and showed that this metric correlate with generalization. Unfortunately, they could only apply their method to linear\nnetworks. Recently Kawaguchi et al. (2017) connects PathNorm (Neyshabur et al., 2015a) to generalization. However, the proved generalization bound depends on the distribution and measuring it requires vector operations on exponentially high dimensional vectors. Other works have designed experiments to empirically evaluate potential properties of the network that helps generalization(Arpit et al., 2017; Neyshabur et al., 2017b; Dinh et al., 2017). The idea of compressing trained deep nets is very popular for low-power applications; for a survey see Cheng et al. (2018).\nFinally, note that the terms compression and stability are traditionally used in a different sense in generalization theory (Littlestone and Warmuth, 1986; Kearns and Ron, 1999; Shalev-Shwartz et al., 2010). Our framework is compared to other notions in the remarks after Theorem 2.1.\nNotation: We use standard formalization of multiclass classification, where data consists of sample x and its label y (an integer from 1 to k). A multiclass classifier f maps input x to f(x) ∈ Rk and the maximum coordinate of f(x) is the predicted label. The classification loss for any distribution D is defined as P(x,y)∼D [f(x)[y] < maxi6=y f(x)[j]] where f(x)[y] is the y-th coordinate of f(x). If γ > 0 is some desired margin, then the expected margin loss is\nLγ(f) = P(x,y)∼D [ f(x)[y] ≤ γ + max\ni6=y f(x)[j] ] (Notice, the classification loss corresponds to γ = 0.) Let L̂γ denote empirical estimate of the margin loss. Generalization error is the difference between the two.\nFor most of the paper we assume that deep nets have fully connected layers, and use ReLU activations. We treat convolutional nets in Section 5. If the net has d layers, we label the vector before activation at these layers by x0, x1, xd for the d layers where x0 is the input to the net, also denoted simply x. So xi = Aiφ(xi−1) where Ai is the weight matrix of the ith layer. (Here φ(x) if x is a vector applies the ReLU component-wise. The ReLU is allowed a trainable bias parameter, which is omitted from the notation because it has no effect on any calculations below.) We denote the number of hidden units in layer i by hi and set h = maxdi=1 h\ni. Let fA(x) be the function calculated by the above network.\nStable rank of a matrix B is ‖B‖2F /‖B‖22, where ‖ · ‖F denotes Frobenius norm and ‖ · ‖2 denotes spectral norm. Note that stable rank is at most (linear algebraic) rank.\nFor any two layer i ≤ j, denote by M i,j the operator for composition of these layers and J i,jx be the Jacobian of this operator at input x (a matrix whose p, q is the partial derivative of the pth output coordinate with respect to the q’th input input). Therefore, we have xj = M i,j(xi). Furthermore, since the activation functions are ReLU, we have\nM i,j(xi) = J i,jxi x i."
  }, {
    "heading": "2. Compression and Generalization",
    "text": "Our compression framework rests on the following obvious fact. Suppose the training data contains m samples, and f is a classifier from a complicated class (e.g., deep nets with much more than m parameters) that incurs very low empirical loss. We are trying to understand if it will generalize. Now suppose we can compute a classifier g with discrete trainable parameters much less than m and which incurs similar loss on the training data as f . Then g must incur low classification error on the full distribution. This framework has the advantage of staying with intuitive parameter counting and to avoid explicitly dealing with the hypothesis class that includes f (see note after Theorem 2.1). Notice, the mapping from f to g merely needs to exist, not to be efficiently computable. But in all our examples the mapping will be explicit and fairly efficient. Now we formalize the notions. The proofs are elementary via concentration bounds and appear in the appendix.\nDefinition 1 ((γ,S)-compressible). For any set A of parameter values, let f be a classifier and GA = {gA|A ∈ A} be a class of classifiers. We say f is (γ, S)-compressible via GA if there exists A ∈ A such that for any x ∈ S, we have for all y\n|f(x)[y]− gA(x)[y]| ≤ γ.\nWe also consider a different setting where the compression algorithm is allowed a“helper string” s, which is arbitrary but fixed before looking at the training samples. Often s will contain random numbers. A simple example is to let s be the random initialization used for training the deep net and then compress the difference between the final weights and s. This can give better generalization bounds, similar to (Dziugaite and Roy, 2017). Other nontrivial examples appear later.\nDefinition 2 ((γ,S)-compressible using helper string s). Suppose GA,s = {gA,s|A ∈ A} is a class of classifiers indexed by trainable parameters A and fixed strings s. A classifier f is (γ, S)-compressible with respect to GA,s using helper string s if there exists A ∈ A such that for any x ∈ S, we have for all y\n|f(x)[y]− gA,s(x)[y]| ≤ γ.\nTheorem 2.1. Suppose GA,s = {gA,s|A ∈ A} where A is a set of q parameters each of which can have at most r discrete values and s is a helper string. Let S be a training set with m samples. For any margin γ > 0, if the trained classifier f is (γ, S)-compressible via GA,s with helper string s, then there exists A ∈ A such that with high probability\nover the training set,\nL0(gA) ≤ L̂γ(f) +O (√ q log r\nm\n) .\nRemarks: (1) The framework proves the generalization not of f but of its compression gA. (An exception is if the two are shown to have similar loss at every point in the domain, not just the training set. This is the case in Theorem 2.2.) (2) The previous item highlights how our framework steps away from uniform convergence framework, e.g., covering number arguments (Dudley, 2010; Anthony and Bartlett, 2009). There, one needs to fix a hypothesis class independent of the training set. By contrast we have no hypothesis class, only a single neural net that has some specific properties (described in Section 3) on a single finite training set. But if we can compress this specific neural net to a simpler neural nets with fewer parameters then we can use covering number argument on this simpler class to get the generalization of the compressed net. (3) Issue (1) exists also in standard PAC-Bayes framework for deep nets (see tongue-in-cheek title of Langford and Caruana (2001)). They yield generalization bounds not for f but for a noised version of f (i.e., net given by W + η, where W is parameter vector of f and η is a noise vector). (4) As we will see later, our compression which is achieved via a randomized algorithm seems “non-destructive” and should not overfit to the training set more than the original network. Moreover, for us issue (1) could be fixed by showing that if f satisfies the properties of Section 3 on training data then it satisfies them on the entire domain. This is left for future work."
  }, {
    "heading": "2.1. Example 1: Linear classifiers with margin",
    "text": "To illustrate the above compression method and its connection to noise stability, we use linear classifiers with high margins. Let c ∈ Rh(‖c‖ = 1) be a classifier for binary classification whose output on input x is sgn(c · x). Let D be a distribution on inputs (x, y) where ‖x‖ = 1 and y ∈ {±1}. Say c has margin γ if for all (x, y) in the training set we have y(c>x) ≥ γ. If we add Gaussian noise vector η with coordinate-wise variance σ2 to c, then E[x · (c+ η)] is c · x and the variance is σ2. (A similar analysis applies to noising of x instead of c.) Thus the margin is large if and only if the classifier’s output is somewhat noise-stable.\nA classifier with margin γ can be compressed to one that has onlyO(1/γ2) non-zero entries. For each coordinate i, toss a coin with Pr[heads] = 8c2i /γ\n2 and if it comes up heads set the coordinate to equal to γ2/8ci (see Algorithm 2 in supplementary material). This yields a vector ĉ with only O(1/γ2) nonzero entries such that for any vector u, with reasonable\nprobability |ĉ>u− c>u| ≤ γ, so ĉ and c will make the same prediction. We can then apply Theorem 2.1 on a discretized version of ĉ to show that the sparsified classifier has good generalization with O(log d/γ2) samples.\nThis compressed classifier works correctly for a fixed input x with good probability but not high probability. To fix this, one can recourse to the “compression with fixed string” model. The fixed string is a random linear transformation. When applied to unit vector x, it tends to equalize all coordinates and the guarantee |ĉ>u− c>u| ≤ γ can hold with high probability. This random linear transformation can be fixed before seeing the training data. See Section A.2 in supplementary material for details."
  }, {
    "heading": "2.2. Example 2: Existing generalization bounds",
    "text": "Our compression framework gives easy and short proof of the generalization bounds of a recent paper; see appendix for slightly stronger result of Bartlett et al. (2017).\nTheorem 2.2. ((Neyshabur et al., 2017a)) For any deep net with layers A1, A2, . . . Ad and output margin γ on a training set S, the generalization error can be bounded by\nÕ  √√√√hd2 maxx∈S ‖x‖∏di=1 ‖Ai‖22∑di=1 ‖Ai‖2F‖Ai‖22\nγ2m\n .\nThe second part of this expression ( ∑d i=1 ‖Ai‖2F ‖Ai‖22\n) is sum of stable ranks of the layers, a natural measure of their true parameter count. The first part ( ∏d i=1 ‖Ai‖22) is related to the Lipschitz constant of the network, namely, the maximum norm of the vector it can produce if the input is a unit vector. The Lipschitz constant of a matrix operator B is just its spectral norm ‖B‖2. Since the network applies a sequence of matrix operations interspersed with ReLU, and ReLU is 1-Lipschitz we conclude that the Lipschitz constant of the full network is at most ∏d i=1 ‖Ai‖2.\nTo prove Theorem 2.2 we use the following lemma to compress the matrix at each layer to a matrix of smaller rank. Since a matrix of rank r can be expressed as the product of two matrices of inner dimension r, it has 2hr parameters (instead of the trivial h2). (Furthermore, the parameters can be discretized via trivial rounding to get a compression with discrete parameters as needed by Definition 1.)\nLemma 1. For any matrix A ∈ Rm×n, let Â be the truncated version of A where singular values that are smaller than δ‖A‖2 are removed. Then ‖Â−A‖2 ≤ δ‖A‖2 and Â has rank at most ‖A‖2F /(δ2‖A‖22).\nProof. Let r be the rank of Â. By construction, the maximum singular value of Â − A is at most δ‖A‖2. Since\nthe remaining singular values are at least δ‖A‖2, we have ‖A‖F ≥ ‖Â‖F ≥ √ rδ‖A‖2.\nFor each i replace layer i by its compression using the above lemma, with δ = γ(3‖x‖d∏di=1 ‖Ai‖2)−1. How much error does this introduce at each layer and how much does it affect the output after passing through the intermediate layers (and getting magnified by their Lipschitz constants)? Since A − Âi has spectral norm (i.e., Lipschitz constant) at most δ, the error at the output due to changing layer i in isolation is at most δ‖xi‖∏dj=1 ‖Aj‖2 ≤ γ/3d. A simple induction (see (Neyshabur et al., 2017a) if needed) can now show the total error incurred in all layers is bounded by γ. The generalization bound follows immediately from Theorem 2.1."
  }, {
    "heading": "3. Noise Stability Properties of Deep Nets",
    "text": "This section introduces noise stability properties of deep nets that imply better compression (and hence generalization). They help overcome the pessimistic error analysis of our proof of Theorem 2.2: when a layer was compressed, the resulting error was assumed to blow up in a worst-case manner according to the Lipschitz constant (namely, product of spectral norms of layers). This hurt the amount of compression achievable. The new noise stability properties roughly amount to saying that noise injected at a layer has very little effect on the higher layers. Our formalization starts with noise sensitivity, which captures how an operator transmits noise vs signal.\nDefinition 3. IfM is a mapping from real-valued vectors to real-valued vectors, and N is some noise distribution then noise sensitivity of M at x with respect to N , is\nψN (M,x) = Eη∈N [‖M(x+ η‖x‖)−M(x)‖2 ‖M(x)‖2 ] ,\nThe noise sensitivity of M with respect to N on a set of inputs S, denoted ψN ,S(M), is the maximum of ψN (M,x) over all inputs x in S.\nTo illustrate, we examine noise sensitivity of a matrix (i.e., linear mapping) with respect to Gaussian distribution. Low sensitivity turns out to imply that the matrix has some large singular values (i.e., low stable rank), which give directions that can preferentially carry the “signal”x whereas noise η attenuates because it distributes uniformly across directions.\nProposition 3.1. The noise sensitivity of a matrix M at any vector x 6= 0 with respect to Gaussian distribution N (0, I) is exactly ‖M‖2F ‖x‖2/‖Mx‖2, and at least its stable rank.\nProof. Using E[ηη>] = I , we bound the numerator by\nEη[‖M(x+ η‖x‖)−Mx‖2] = Eη[‖x‖2‖Mη‖2]\n= Eη[‖x‖2tr(Mηη>M>)] = ‖x‖2tr(MM>) = ‖M‖2F ‖x‖2.\nThus noise sensitivity ψ at x is ‖M‖2F ‖x‖2/‖Mx‖2, which is at least the stable rank ‖M‖2F /‖M‖22 since ‖Mx‖ ≤ ‖M‖2‖x‖.\nThe above proposition suggests that if a vector x is aligned to a matrix M (i.e. correlated with high singular directions of M ), then matrix M becomes less sensitive to noise at x. This intuition will be helpful in understanding the properties we define later to formalize noise stability.\nThe above discussion motivates the following approach. We compress each layer i by an appropriate randomized compression algorithm, such that the noise/error in its output is “Gaussian-like”. If layers i + 1 and higher have low sensitivity to this new noise, then the compression can be more extreme produce much higher noise. We formalize this idea using Jacobian J i,j , which describes instantaneous change of M i,j(x) under infinitesimal perturbation of x."
  }, {
    "heading": "3.1. Formalizing Error-resilience",
    "text": "Now we formalize the error-resilience properties. Section 6 reports empirical findings about these properties. The first is cushion, to be thought of roughly as reciprocal of noise sensitivity. We first formalize it for single layer. Definition 4 (layer cushion). The layer cushion of layer i is similarly defined to be the largest number µi such that for any x ∈ S, µi‖Ai‖F ‖φ(xi−1)‖ ≤ ‖Aiφ(xi−1)‖.\nIntuitively, cushion considers how much smaller the output Aiφ(xi−1) is compared to the upper bound ‖Ai‖F ‖φ(xi−1)‖. Using argument similar to Proposition 3.1, we can see that 1/µ2i is equal to the noise sensitivity of matrixAi at input φ(xi−1) with respect to Gaussian noise η ∼ N (0, I). Of course, for nonlinear operators the definition of error resilience is less clean. Let’s denote by M i,j : Rhi → Rhj the operator corresponding to the portion of the deep net from layer i to layer j, and by J i,j its Jacobian. If infinitesimal noise is injected before level i then M i,j passes it like J i,j , a linear operator. When the noise is small but not infinitesimal then one hopes that M i,j still behaves roughly linearly (recall that ReLU nets are piecewise linear). To formalize this, we define Interlayer Cushion (Definition 5) that captures the local linear approximation of the operator M . Definition 5 (Interlayer Cushion). For any two layers i ≤ j, we define the interlayer cushion µi,j as the largest number such that for any x ∈ S:\nµi,j‖J i,jxi ‖F ‖xi‖ ≤ ‖J i,j xi x i‖\nFurthermore, for any layer i we define the minimal interlayer cushion as µi→ = mini≤j≤d µi,j = min{1/ √ hi,mini<j≤d µi,j}1.\nSince J i,jx is a linear transformation, a calculation similar to Proposition 3.1 shows that its noise sensitivity at xi with respect to Gaussian distribution N (0, I) is at most 1\nµ2ij .\nThe next property quantifies the intuitive observation on the learned networks that for any training data, almost half of the ReLU activations at each layer are active. If the input to the activations is well-distributed and the activations do not correlate with the magnitude of the input, then one would expect that on average, the effect of applying activations at any layer is to decrease the norm of the pre-activation vector by at most some small constant factor. Definition 6 (Activation Contraction). The activation contraction c is defined as the smallest number such that for any layer i and any x ∈ S,\n‖φ(xi)‖ ≥ ‖xi‖/c.\nWe discussed how the interlayer cushion captures noiseresilience of the network if behaves linearly, namely, when the set of activated ReLU gates does not change upon injecting noise. In general the activations do change, but the deviation from linear behavior is bounded for small noise vectors, as quantified next. Definition 7 (Interlayer Smoothness). Let η be the noise generated as a result of substituting weights in some of the layers before layer i using Algorithm 1. We define interlayer smoothness ρδ to be the largest number such that with probability 1− δ over noise η for any two layers i < j any x ∈ S:\n‖M i,j(xi + η)− J i,jxi (xi + η)‖ ≤ ‖η‖‖xj‖ ρδ‖xi‖ .\nFor a single layer, ρδ captures the ratio of input/weight alignment to noise/weight alignment. Since the noise behaves similar to Gaussian, one expects this number to be greater than one for a single layer. When j > i + 1, the weights and activations create more dependencies. However, since these dependences are applied on both noise and input, we again expect that if the input is more aligned to the weights than noise, this should not change in higher layers. In Section 6, we show that the interlayer smoothness is indeed good: 1/ρδ is a small constant. Please see Appendix A.4 for a more detailed discussion on interlayer smoothness."
  }, {
    "heading": "4. Fully Connected Networks",
    "text": "We prove generalization bounds using for fully connected multilayer nets. Details appear in Appendix Section B.\n1Note that J i,i xi\n= I and µi,i = 1/ √ hi\nTheorem 4.1. For any fully connected network fA with ρδ ≥ 3d, any probability 0 < δ ≤ 1 and any margin γ, Algorithm 1 generates weights Ã for the network fÃ such that with probability 1− δ over the training set and fÃ, the expected error L0(fÃ) is bounded by\nL̂γ(fA) + Õ  √√√√c2d2 maxx∈S ‖fA(x)‖22∑di=1 1µ2iµ2i→\nγ2m  where µi, µi→, c and ρδ are layer cushion, interlayer cushion, activation contraction and interlayer smoothness defined in Definitions 4,5,6 and 7 respectively.\nTo prove this we describe a compression of the net with respect to a fixed (random) string. In contrast to the deterministic compression of Lemma 1, this randomized compression ensures that the resulting error in the output behaves like a Gaussian. The proofs are similar to standard JL dimension reduction.\nAlgorithm 1 Matrix-Project (A, ε, η)\nRequire: Layer matrix A ∈ Rh1×h2 , error parameter ε, η. Ensure: Returns Â s.t. ∀ fixed vectors u, v,\nPr[|u>Âv − u>Av‖ ≥ ε‖A‖F ‖u‖‖v‖] ≤ η.\nSample k = log(1/η)/ε2 random matrices M1, . . . ,Mk with entries i.i.d. ±1 (“helper string”) for k′ = 1 to k do\nLet Zk′ = 〈A,Mk′〉Mk′ . end for Let Â = 1k ∑k k′=1 Zk′\nNote that the helper string of random matrices Mi’s were chosen and fixed before training set S was picked. Each weight matrix is thus represented as only k real numbers 〈A,Mi〉 for i = 1, 2, ..., k. Lemma 2. For any 0 < δ, ε ≤ 1, let G = {(U i, xi)}mi=1 be a set of matrix/vector pairs of sizem where U ∈ Rn×h1 and x ∈ Rh2 , let Â ∈ Rh1×h2 be the output of Algorithm 1 with η = δ/mn and ∆ = Â−A. With probability at least 1− δ we have for any (U, x) ∈ G, ‖U∆x‖ ≤ ε‖A‖F ‖U‖F ‖x‖.\nNext Lemma bounds the number of parameters of the compressed network resulting from applying Algorithm 1 to all the layer matrices of the net. The proof does induction on the layers and bounds the effect of the error on the output of the network using properties defined in Section 3.1.\nLemma 3. For any fully connected network fA with ρδ ≥ 3d, any probability 0 < δ ≤ 1 and any error 0 < ε ≤ 1, Algorithm 1 generates weights Ã for a network with 72c2d2 log(mdh/δ)\nε2 · ∑d i=1 1 µ2iµ 2 i→ total parameters such that\nwith probability 1− δ/2 over the generated weights Ã, for any x ∈ S:\n‖fA(x)− fÃ(x)‖ ≤ ε‖fA(x)‖.\nwhere µi, µi→, c and ρδ are layer cushion, interlayer cushion, activation contraction and interlayer smoothness defined in Definitions 4,5,6 and 7 respectively.\nSome obvious improvements: (i) Empirically it has been observed that deep net training introduces fairly small changes to parameters as compared to the (random) initial weights (Dziugaite and Roy, 2017). We can exploit this by incorporating the random initial weights into the helper string and do the entire proof above not with the layer matrices Ai but only the difference from the initial starting point. Experiments in Section 6 show this improves the bounds. (ii) Cushions and other quantities defined earlier are datadependent, and required to hold for the entire training set. However, the proofs go through if we remove say ζ fraction of outliers that violate the definitions; this allows us to use more favorable values for cushion etc. and lose an additive factor ζ in the generalization error."
  }, {
    "heading": "5. Convolutional Neural Networks",
    "text": "Now we sketch how to provably compress convolutional nets. (Details appear in Section C of supplementary.) Intuitively, this feels harder because the weights are already compressed— they’re shared across patches!\nTheorem 5.1. For any convolutional neural network fA with ρδ ≥ 3d, any probability 0 < δ ≤ 1 and any margin γ, Algorithm 4 generates weights Ã for the network fÃ such that with probability 1− δ over the training set and fÃ:\nL0(fÃ) ≤ L̂γ(fA)\n+ Õ  √√√√c2d2 maxx∈S ‖fA(x)‖22∑di=1 β2(dκi/sie)2µ2iµ2i→\nγ2m  where µi, µi→, c, ρδ and β are layer cushion, interlayer cushion, activation contraction, interlayer smoothness and well-distributed Jacobian defined in Definitions 4,8,6, 7 and 9 respectively. Furthermore, si and κi are stride and filter width in layer i.\nLet’s realize that obvious extensions of earlier sections fail. Suppose layer i of the neural network is an image of dimension ni1 × ni2 and each pixel has hi channels, the size of the filter at layer i is κi × κi with stride si. The convolutional filter has dimension hi−1 × hi × κi × κi. Applying matrix compression (Algorithm 1) independently to each copy of a convolutional filter makes number of new parameters proportional to ni1n i 2, a big blowup.\nCompressing a convolutional filter once and reusing it in all patches doesn’t work because the interlayer analysis implicitly requires the noise generated by the compression behave similar to a spherical Gaussian, but the shared filters introduce correlations. Quantitatively, using the fully connected analysis would require the error to be less than interlayer cushion value µi→ (Definition 5) which is at most 1/ √ hini1n i 2, and this can never be achieved from compressing matrices that are far smaller than ni1 × ni2 to begin with. We end up with a solution in between fully independent and fully dependent: p-wise independence. The algorithm generates p-wise independent compressed filters Â(a,b) for each convolution location (a, b) ∈ [ni1]× [ni2]. It results in p times more parameters than a single compression. If p grows logarithmically with relevant parameters, the filters behave like fully independent filters. Using this idea we can generalize the definition of interlayer margin to the convolution setting: Definition 8 (Interlayer Cushion, Convolution Setting). For any two layers i ≤ j, we define the interlayer cushion µi,j as the largest number such that for any x ∈ S:\nµi,j · 1√ ni1n i 2 ‖J i,jxi ‖F ‖xi‖ ≤ ‖J i,j xi x i‖\nFurthermore, for any layer i we define the minimal interlayer cushion as µi→ = mini≤j≤d µi,j = min{1/ √ hi,mini<j≤d µi,j}2.\nRecall that interlayer cushion is related to the noise sensitivity of J i,jxi at x\ni with respect to Gaussian distribution N (0, I). When we consider J i,jxi applied to a noise η, if different pixels in η are independent Gaussian random variables, then we can indeed expect ‖J i,jxi η‖ ≈ 1√ hini1n i 2 ‖J i,jxi ‖‖η‖, which explains the extra 1√ni1ni2 factor in Definition 8 compared to Definition 5. The proof also needs to assume —in line with intuition behind convolution architecture— that information from the entire image field is incorporated somewhat uniformly across pixels. It is formalized using the Jacobian which gives the partial derivative of the output with respect to pixels at previous layer. Definition 9 (Well-distributed Jacobian). Let J i,jx be the Jacobian of M i,j at x, we know J i,jx ∈ Rhi×ni1×ni2×hj×n j 1×n j 2 . We say the Jacobian is β welldistributed if for any x ∈ S, any i, j, any (a, b) ∈ [ni1×ni2],\n‖[J i,jx ]:,a,b,:,:,:‖F ≤ β√ ni1n i 2 ‖J i,jx ‖F"
  }, {
    "heading": "6. Empirical Evaluation",
    "text": "We study noise stability properties (defined in Section 3) of an actual trained deep net, and compute a generalization\n2Note that J i,i xi\n= I and µi,i = 1/ √ hi\nbound from Theorem 5.1. Experiments were performed by training a VGG-19 architecture (Simonyan and Zisserman, 2014) and a AlexNet (Krizhevsky et al., 2012) for multi-class classification task on CIFAR-10 dataset. Optimization used SGD with mini-batch size 128, weight decay 5e-4, momentum 0.9 and initial learning rate 0.05, but decayed by factor 2 every 30 epochs. Drop-out was used in fully-connected layers. We trained both networks for 299 epochs and the final VGG-19 network achieved 100% training and 92.45% validation accuracy while the AlexNet achieved 100% training and 77.22% validation accuracy. To investigate the effect of corrupted label, we trained another AlexNet, which 100% training and 9.86% validation accuracy, on CIFAR-10 dataset with randomly shuffled labels.\nOur estimate of the sample complexity bound used exact computation of norms of weight matrices (or tensors) in all bounds(||A||1,∞, ||A||1,2, ||A||2, ||A||F ). Like previous bounds in generalization theory, ours also depend upon nuisance parameters like depth d, logarithm of h, etc. which probably are an artifact of the proof. These are ignored in the computation (also in computing earlier bounds) for simplicity. Even the generalization based on parameter counting arguments does have an extra dependence on depth (Bartlett et al., 2017). A recent work, (Golowich et al., 2017) showed that many such depth dependencies can be improved."
  }, {
    "heading": "6.1. Empirical investigation of noise stability properties",
    "text": "Section 3 identifies four properties in the networks that contribute to noise-stability: layer cushion, interlayer cushion, contraction, interlayer smoothness. Figure 2 plots the distribution of over different data points in the training set and compares to a Gaussian random network and then scaled properly. The layer cushion, which quantifies its noise stability, is drastically improved during the training, especially for the higher layers (8 and higher) where most parameters live. Moreover, we observe that interlayer cushion, activation contraction and interlayer smoothness behave nicely even after training. These plots suggest that the driver of the generalization phenomenon is layer cushion. The other properties are being maintained in the network and prevent the network from falling prey to pessimistic assumptions that causes the other older generalization bounds to be very high. The assumptions made in section 3 (also in B.1) are verified on the VGG-19 net in appendix D.1 by histogramming the distribution of layer cushion, interlayer cushion, contraction, interlayer smoothness, and well-distributedness of the Jacobians of each layer of the net on each data point in the training set. Some examples are shown in Figure 2."
  }, {
    "heading": "6.2. Correlation to generalization error",
    "text": "We evaluate our generalization bound during the training, see Figure 3, Right. After 120 epochs, the training error is\nalmost zero but the test error continues to improve in later epochs. Our generalization bound continues to improve, though not to the same level. Thus our generalization bound captures part of generalization phenomenon, not all. Still, this suggests that SGD somehow improves our generalization measure implicitly. Making this rigorous is a good topic for further research.\nFurthermore, we investigate effect of training with normal data and corrupted data by training two AlexNets respectively on original and corrupted CIFAR-10 with randomly shuffled labels. We identify two key properties that differ significantly between the two networks: layer cushion and activation contraction, see D.2. Since our bound predicts larger cushion and lower contraction indicates better generalization, our bound is consistent w with the fact that the net trained on normal data generalizes (77.22% validation accuracy)."
  }, {
    "heading": "6.3. Comparison to other generalization bounds",
    "text": "Figure 3 compares our proposed bound to other neural-net generalization bounds on the VGG-19 net and compares to naive VC dimension bound (which of course is too pessimistic). All previous generalization bounds are orders of magnitude worse than ours; the closest one is spectral norms times average `1,2 of the layers (Bartlett et al., 2017) which is still about 1018, far greater than VC dimension. (As mentioned we’re ignoring nuisance factors like depth\nand log h which make the comparison to VC dimension a bit unfair, but the comparison to previous bounds is fair.) This should not be surprising as all other bounds are based on product of norms is pessimistic (see note at the start of Section 3) which we avoid due to the noise stability analysis resulting in a bound that has more dependence on the data.\nTable 1 shows the compressibility of various layers according to the bounds given by our theorem. Again, this is a qualitative due to ignoring nuisance factors, but it gives an idea of which layers are important in the calculation."
  }, {
    "heading": "7. Conclusions",
    "text": "With a new compression-based approach, the paper has made progress on several open issues regarding generalization properties of deep nets. The approach also adapts specially to convolutional nets. The empirical verification of the theory in Section 6 shows a rich set of new properties satisfied by deep nets trained on realistic data, which we hope will fuel further theory work on deep learning, including how these properties play into optimization and expressivity. Another possibility is a more rigorous understanding of deep net compression, which sees copious empirical work motivated by low-power applications. Perhaps our p-wise independence idea used for compressing convnets (Section 5) has practical implications."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was done with support from NSF, ONR, Darpa, SRC, Simons Foundation, Mozilla Research, and Schmidt Foundation."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural network learning: Theoretical foundations",
    "authors": ["Martin Anthony", "Peter L Bartlett"],
    "venue": "cambridge university press,",
    "year": 2009
  }, {
    "title": "A closer look at memorization in deep networks",
    "authors": ["Devansh Arpit", "Stanislaw Jastrzebski", "Nicolas Ballas", "David Krueger", "Emmanuel Bengio", "Maxinder S Kanwal", "Tegan Maharaj", "Asja Fischer", "Aaron Courville", "Yoshua Bengio"],
    "venue": "arXiv preprint arXiv:1706.05394,",
    "year": 2017
  }, {
    "title": "Spectrally-normalized margin bounds for neural networks",
    "authors": ["Peter Bartlett", "Dylan J Foster", "Matus Telgarsky"],
    "venue": "arXiv preprint arXiv:1706.08498,",
    "year": 2017
  }, {
    "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
    "authors": ["Peter L Bartlett", "Shahar Mendelson"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
    "authors": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun"],
    "venue": "arXiv preprint arXiv:1611.01838,",
    "year": 2016
  }, {
    "title": "Model compression and acceleration for deep neural networks: The principles, progress, and challenges",
    "authors": ["Yu Cheng", "Duo Wang", "Pan Zhou", "Tao Zhang"],
    "venue": "IEEE Signal Proc. Magazine,",
    "year": 2018
  }, {
    "title": "Sharp minima can generalize for deep nets",
    "authors": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"],
    "venue": "arXiv preprint arXiv:1703.04933,",
    "year": 2017
  }, {
    "title": "Universal donsker classes and metric entropy",
    "authors": ["Richard M Dudley"],
    "venue": "In Selected Works of RM Dudley,",
    "year": 2010
  }, {
    "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
    "authors": ["Gintare Karolina Dziugaite", "Daniel M Roy"],
    "venue": "arXiv preprint arXiv:1703.11008,",
    "year": 2017
  }, {
    "title": "Regularization networks and support vector machines",
    "authors": ["Theodoros Evgeniou", "Massimiliano Pontil", "Tomaso Poggio"],
    "venue": "Advances in computational mathematics,",
    "year": 2000
  }, {
    "title": "A rank minimization heuristic with application to minimum order system approximation",
    "authors": ["Maryam Fazel", "Haitham Hindi", "Stephen P Boyd"],
    "venue": "In American Control Conference,",
    "year": 2001
  }, {
    "title": "Sizeindependent sample complexity of neural networks",
    "authors": ["Noah Golowich", "Alexander Rakhlin", "Ohad Shamir"],
    "venue": "arXiv preprint arXiv:1712.06541,",
    "year": 2017
  }, {
    "title": "Train faster, generalize better: Stability of stochastic gradient descent",
    "authors": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Keeping the neural networks simple by minimizing the description length of the weights",
    "authors": ["Geoffrey E Hinton", "Drew Van Camp"],
    "venue": "In Proceedings of the sixth annual conference on Computational learning theory,",
    "year": 1993
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["Sergey Ioffe", "Christian Szegedy"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Generalization in deep learning",
    "authors": ["Kenji Kawaguchi", "Leslie Pack Kaelbling", "Yoshua Bengio"],
    "venue": "arXiv preprint arXiv:1710.05468,",
    "year": 2017
  }, {
    "title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation",
    "authors": ["Michael Kearns", "Dana Ron"],
    "venue": "Neural computation,",
    "year": 1999
  }, {
    "title": "On largebatch training for deep learning: Generalization gap and sharp minima",
    "authors": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"],
    "venue": "arXiv preprint arXiv:1609.04836,",
    "year": 2016
  }, {
    "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
    "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
    "year": 2012
  }, {
    "title": "not) bounding the true error",
    "authors": ["John Langford", "Rich Caruana"],
    "venue": "In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic,",
    "year": 2001
  }, {
    "title": "Fisher-rao metric, geometry, and complexity of neural networks",
    "authors": ["Tengyuan Liang", "Tomaso Poggio", "Alexander Rakhlin", "James Stokes"],
    "venue": "arXiv preprint arXiv:1711.01530,",
    "year": 2017
  }, {
    "title": "Relating data compression and learnability",
    "authors": ["Nick Littlestone", "Manfred Warmuth"],
    "venue": "Technical report,",
    "year": 1986
  }, {
    "title": "Some PAC-Bayesian theorems",
    "authors": ["David A McAllester"],
    "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,",
    "year": 1998
  }, {
    "title": "PAC-Bayesian model averaging",
    "authors": ["David A McAllester"],
    "venue": "In Proceedings of the twelfth annual conference on Computational learning theory,",
    "year": 1999
  }, {
    "title": "On the importance of single directions for generalization",
    "authors": ["Ari Morcos", "David GT Barrett", "Matthew Botvinick", "Neil Rabinowitz"],
    "venue": "In Proceeding of the International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "Path-sgd: Path-normalized optimization in deep neural networks",
    "authors": ["Behnam Neyshabur", "Ruslan R Salakhutdinov", "Nati Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Norm-based capacity control in neural networks",
    "authors": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"],
    "venue": "In Proceeding of the 28th Conference on Learning Theory (COLT),",
    "year": 2015
  }, {
    "title": "In search of the real inductive bias: On the role of implicit regularization in deep learning",
    "authors": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"],
    "venue": "Proceeding of the International Conference on Learning Representations workshop track,",
    "year": 2015
  }, {
    "title": "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks",
    "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nathan Srebro"],
    "venue": "arXiv preprint arXiv:1707.09564,",
    "year": 2017
  }, {
    "title": "Exploring generalization in deep learning",
    "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nati Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Hoeffding’s inequality for sums of weakly dependent random variables",
    "authors": ["Christos Pelekis", "Jan Ramon"],
    "venue": "arXiv preprint arXiv:1507.06871,",
    "year": 2015
  }, {
    "title": "Learnability, stability and uniform convergence",
    "authors": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["Karen Simonyan", "Andrew Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "The connection between regularization operators and support vector kernels",
    "authors": ["Alex J Smola", "Bernhard Schölkopf", "Klaus-Robert Müller"],
    "venue": "Neural networks,",
    "year": 1998
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 1929
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }],
  "id": "SP:91fce4a9f9ff21088ae6fb542dde4fc374737a95",
  "authors": [{
    "name": "Sanjeev Arora",
    "affiliations": []
  }, {
    "name": "Rong Ge",
    "affiliations": []
  }, {
    "name": "Behnam Neyshabur",
    "affiliations": []
  }, {
    "name": "Yi Zhang",
    "affiliations": []
  }],
  "abstractText": "Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net — a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified “noise stability”properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.",
  "title": "Stronger Generalization Bounds for Deep Nets via a Compression Approach"
}