{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Contextual bandits refer to a learning setting where the learner repeatedly observes a context, takes an action and observes a reward for the chosen action in the observed context, but no feedback on any other action. An example is movie recommendation, where the context describes a user, actions are candidate movies and the reward measures if the user enjoys the recommended movie. The learner produces a policy, meaning a mapping from contexts to actions. A common question in such settings is, given a target policy, what is its expected reward? By letting the policy choose actions (e.g., recommend movies to users), we can compute its reward. Such online evaluation is typically costly since it exposes users to an untested experimental policy, and does\n1Carnegie Mellon University, Pittsburgh, PA 2Microsoft Research, New York, NY. Correspondence to: Yu-Xiang Wang <yuxiangw@cs.cmu.edu>, Alekh Agarwal <alekha@microsoft.com>, Miroslav Dudík <mdudik@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nnot scale to evaluating many different target policies.\nOff-policy evaluation is an alternative paradigm for the same question. Given logs from the existing system, which might be choosing actions according to a very different logging policy than the one we seek to evaluate, can we estimate the expected reward of the target policy? There are three classes of approaches to address this question: the direct method (DM), also known as regression adjustment, inverse propensity scoring (IPS) (Horvitz & Thompson, 1952) and doubly robust (DR) estimators (Robins & Rotnitzky, 1995; Bang & Robins, 2005; Dudík et al., 2011; 2014).\nOur first goal in this paper is to study the optimality of these three classes of approaches (or lack thereof), and more fundamentally, to quantify the statistical hardness of offpolicy evaluation. This problem was previously studied for multi-armed bandits (Li et al., 2015) and is related to a large body of work on asymptotically optimal estimators of average treatment effects (ATE) (Hahn, 1998; Hirano et al., 2003; Imbens et al., 2007; Rothe, 2016), which can be viewed as a special case of off-policy evaluation. In both settings, a major underlying assumption is that rewards can be consistently estimated from the features (i.e., covariates) describing contexts and actions, either via a parametric model or non-parametrically. Under such consistency assumptions, it has been shown that DM and/or DR are optimal (Imbens et al., 2007; Li et al., 2015; Rothe, 2016),1 whereas standard IPS is not (Hahn, 1998; Li et al., 2015), but it becomes (asymptotically) optimal when the true propensity scores are replaced by suitable estimates (Hirano et al., 2003).\nUnfortunately, consistency of a reward model can be difficult to achieve in practice. Parametric models tend to suffer from a large bias (see, e.g., the empirical evaluation of Dudík et al., 2011) and non-parametric models are limited to small dimensions, otherwise non-asymptotic terms become too large (see, e.g., the analysis of non-parametric regression by Bertin et al., 2004). Therefore, here we ask: What can be said about hardness of policy evaluation in the absence of reward-model consistency?\nIn this pursuit, we provide the first rate-optimal lower bound on the mean-squared error (MSE) for off-policy evaluation\n1The precise assumptions vary for each estimator, and are somewhat weaker for DR than for DM.\nin contextual bandits without consistency assumptions. Our lower bound matches the upper bounds of IPS and DR up to constants, when given a non-degenerate context distributions. This result is in contrast with the suboptimality of IPS under previously studied consistency assumptions, which implies that the two settings are qualitatively different.\nWhereas IPS and DR are both minimax optimal, our experiments (similar to prior work) show that IPS is readily outperformed by DR, even when using a simple parametric regression model that is not asymptotically consistent. We attribute this to a lower variance of the DR estimator. We also empirically observe that while DR is generally highly competitive, it is sometimes substantially outperformed by DM. We therefore ask whether it is possible to achieve an even better bias-variance tradeoff than DR. We answer affirmatively and propose a new class of estimators, called the SWITCH estimators, that adaptively interpolate between DM and DR (or IPS). We show that SWITCH has MSE no worse than DR (or IPS) in the worst case, but is robust to large importance weights and can achieve a substantially smaller variance than DR or IPS.\nWe empirically evaluate the SWITCH estimators against a number of strong baselines from prior work, using a previously used experimental setup to simulate contextual bandit problems on real-world multiclass classification data. The results affirm the superior bias-variance tradeoff of SWITCH estimators, with substantial improvements across a number of problems.\nIn summary, the first part of our paper initiates the study of optimal estimators in a finite-sample setting and without making strong modeling assumptions, while the second part shows how to practically exploit domain knowledge by building better estimators."
  }, {
    "heading": "2. Setup",
    "text": "In contextual bandit problems, the learning agent observes a context x, takes an action a and observes a scalar reward r for the action chosen in the context. Here the context x is a feature vector from some domain X ⊆ Rd, drawn according to a distribution λ. Actions a are drawn from a finite set A. Rewards r have a distribution conditioned on x and a denoted by D(r | x, a). The decision rule of the agent is called a policy, which maps contexts to distributions over actions, allowing for randomization in the action choice. We write µ(a | x) and π(a | x) to denote the logging and target policies respectively. Given a policy π, we extend it to a joint distribution over (x, a, r), where x ∼ λ, action a ∼ π(a | x), and r ∼ D(r | x, a). With this notation, given n i.i.d. samples (xi, ai, ri) ∼ µ, we wish to compute the value of π:\nvπ = Eπ[r] = Ex∼λEa∼π(·|x)Er∼D(·|a,x)[r]. (1)\nIn order to correct for the mismatch in the action distributions under µ and π, it is typical to use importance weights, defined as ρ(x, a) :=π(a|x)/µ(a|x). For consistent estimation, it is standard to assume that ρ(x, a) 6=∞, corresponding to absolute continuity of π with respect to µ, meaning that whenever π(a|x) > 0, then also µ(a|x) > 0. We make this assumption throughout the paper. In the remainder of the setup we present three common estimators of vπ .\nThe first is the inverse propensity scoring (IPS) estimator (Horvitz & Thompson, 1952), defined as\nv̂πIPS = n∑ i=1 ρ(xi, ai)ri. (2)\nIPS is unbiased and makes no assumptions about how rewards might depend on contexts and actions. When such information is available, it is natural to posit a parametric or non-parametric model of E[r | x, a] and fit it on the logged data to obtain a reward estimator r̂(x, a). Policy evaluation can now simply be performed by scoring π according to r̂ as\nv̂πDM = 1\nn n∑ i=1 ∑ a∈A π(a | xi)r̂(xi, a), (3)\nwhere the DM stands for direct method (Dudík et al., 2011), also known as regression adjustment or imputation (Rothe, 2016). IPS can have a large variance when the target and logging policies differ substantially, and parametric variants of DM can be inconsistent, leading to a large bias. Therefore, both in theory and practice, it is beneficial to combine the approaches into a doubly robust estimator (Cassel et al., 1976; Robins & Rotnitzky, 1995; Dudík et al., 2011), such as the following variant,\nv̂πDR = 1\nn n∑ i=1 [ ρ(xi, ai) ( ri − r̂(xi, ai) ) + ∑ a∈A π(a | xi)r̂(xi, a) ] . (4)\nNote that IPS is a special case of DR with r̂ ≡ 0. In the sequel, we mostly focus on IPS and DR, and then suggest how to improve them by further interpolating with DM."
  }, {
    "heading": "3. Limits of Off-policy Evaluation",
    "text": "In this section, we study the off-policy evaluation problem in a minimax setup. After setting up the framework, we present our lower bound and the matching upper bounds for IPS and DR under appropriate conditions.\nWhile minimax optimality is standard in statistical estimations, it is not the only notion of optimality. An alternative framework is that of asymptotic optimality, which establishes Cramer-Rao style bounds on the asymptotic variance of estimators. We use the minimax framework, because it\nis the most amenable to finite-sample lower bounds, and is complementary to previous asymptotic results, as we discuss after presenting our main results."
  }, {
    "heading": "3.1. Minimax Framework",
    "text": "Off-policy evaluation is a statistical estimation problem, where the goal is to estimate vπ given n i.i.d. samples generated according to a policy µ. We study this problem in a standard minimax framework and seek to answer the following question. What is the smallest MSE that any estimator can achieve in the worst case over a large class of contextual bandit problems? As is usual in the minimax setting, we want the class of problems to be rich enough so that the estimation problem is not trivial, and to be small enough so that the lower bounds are not driven by complete pathologies. In our problem, we fix λ, µ and π, and only take worst case over a class of reward distributions. This allows the upper and lower bounds to depend on λ, µ and π, highlighting how these ground-truth parameters influence the problem difficulty. The family of reward distributionsD(r |x, a) that we study is a natural generalization of the class studied by Li et al. (2015) for multi-armed bandits. We assume we are given maps Rmax : X ×A → R+ and σ : X ×A → R+, and define the class of reward distributionsR(σ,Rmax) as2\nR(σ,Rmax) := { D(r|x, a) : 0≤ED[r|x, a]≤Rmax(x, a)\nand VarD[r|x, a]≤σ2(x, a) for all x, a } .\nNote that σ and Rmax are allowed to change over contexts and actions. Formally, an estimator is any function v̂ : (X × A × R)n → R that takes n data points collected by µ and outputs an estimate of vπ. The minimax risk of offpolicy evaluation over the class R(σ,Rmax), denoted by Rn(π;λ, µ, σ,Rmax), is defined as\ninf v̂ sup D(r|x,a)∈R(σ,Rmax)\nE [ (v̂ − vπ)2 ] . (5)\nRecall that the expectation is taken over the n samples drawn from µ, along with any randomness in the estimator. The main goal of this section is to obtain a lower bound on the minimax risk. To state our bound, recall that ρ(x, a) = π(a | x)/µ(a | x) < ∞ is an importance weight at (x, a). We make the following technical assumption on our problem instances, described by tuples of the form (π, λ, µ, σ,Rmax):\nAssumption 1. There exists > 0 such that Eµ [ (ρσ)2+ ] and Eµ [ (ρRmax) 2+ ] are finite.\nThis assumption is only a slight strengthening of the assumption that Eµ[(ρσ)2] and Eµ[(ρRmax)2] be finite, which is\n2Technically, the inequalities in the definition ofR(σ,Rmax) need to hold almost surely with x ∼ λ and a ∼ µ(· | x).\nrequired for consistency of IPS (see, e.g., Dudík et al., 2014). Our assumption holds for instance when the context space is finite, because then both ρ and Rmax are bounded."
  }, {
    "heading": "3.2. Minimax Lower Bound for Off-policy Evaluation",
    "text": "With the minimax setup in place, we now give our main lower bound on the minimax risk for off-policy evaluation and discuss its consequences. Our bound depends on a parameter γ ∈ [0, 1] and a derived indicator random variable ξγ(x, a) := 1(µ(x, a) ≤ γ), which separates out the pairs (x, a) that appear “frequently” under µ.3 As we will see, the “frequent” pairs (x, a) (where ξγ = 0) correspond to the intrinsically realizable part of the problem, where consistent reward models can be constructed. The “infrequent” pairs (where ξγ = 1) constitute the part that is non-realizable in the worst-case. When X ⊆ Rd and λ is continuous with respect to the Lebesgue measure, then ξγ(x, a) = 1 for all γ ∈ [0, 1], so the problem is non-realizable everywhere in the worst-case. Our result uses the following problemdependent constant (defined with the convention 0/0 = 0):\nCγ := 2 2+ max\n{ Eµ[(ρσ)2+ ]2\nEµ[(ρσ)2]2+ , Eµ[ξγ(ρRmax)2+ ]2 Eµ[ξγ(ρRmax)2]2+\n} .\nTheorem 1. Assume that a problem instance satisfies Assumption 1 with some > 0. Then for any γ ∈ [0, 1] and any n ≥ max { 16C 1/ γ , 2C 2/ γ Eµ[σ2/R2max] } , the minimax risk Rn(π;λ, µ, σ,Rmax) satisfies the lower bound\nEµ [ ρ2σ2 ] + Eµ [ ξγρ 2R2max ]( 1− 350nγ log(5/γ) )\n700n .\nThe bound holds for every γ ∈ [0, 1], and we can take the maximum over γ. In particular, we get the following simple corollary under continuous context distributions.\nCorollary 1. Under conditions of Theorem 1, assume further that λ has a density relative to Lebesgue measure. Then\nRn(π;λ, µ, σ,Rmax) ≥ Eµ [ ρ2σ2 ] + Eµ [ ρ2R2max ] 700n .\nIf λ is a mixture of a density and point masses, then γ = 0 will exclude the point masses from the second term of the lower bound. In general, choosing γ = O ( 1/(n log n) ) excludes the contexts likely to appear multiple times, and ensures that the second term in Theorem 1 remains nontrivial (when µ(x, a) ≤ γ with positive probability).\nBefore sketching the proof of Theorem 1, we discuss its preconditions and implications.\n3Formally, µ(x, a) corresponds to µ( {(x, a)} ), i.e., the measure under µ of the set {(x, a)}. For example, when λ is a continuous distribution then µ(x, a) = 0 everywhere.\nPreconditions of the theorem: The theorem assumes the existence of a (problem-dependent) constant Cγ which depends on the constant γ and various moments of the importance-weighted rewards. When Rmax and σ are bounded (a common situation), Cγ measures how heavytailed the importance weights are. Note that Cγ < ∞ for all γ ∈ [0, 1] whenever Assumption 1 holds, and so the condition on n in Theorem 1 is eventually satisfied as long as the random variable σ/Rmax has a bounded second moment. This is quite reasonable since in typical applications the a priori bound on expected rewards is on the same order or larger than the a priori bound on the reward noise. For the remainder of the discussion, we assume that n is appropriately large so the preconditions of the theorem hold.\nComparison with upper bounds: The setting of Corollary 1 is typical of many contextual bandit applications. In this setting both IPS and DR achieve the minimax risk up to a multiplicative constant. Let r∗(x, a) := E[r | x, a]. Recall that DR is using an estimator r̂(x, a) of r∗(x, a), and IPS can be viewed as a special case of DR with r̂ ≡ 0. By Lemma 3.3(i) of Dudík et al. (2014), the MSE of DR is\nE[(v̂πDR − vπ)2]\n= 1\nn\n( Eµ[ρ2σ2] + Varx∼DEa∼µ(·|x)[ρr∗]\n+ Ex∼DVara∼µ(·|x)[ρ(r̂ − r∗)] ) . (6)\nNote that 0 ≤ r∗ ≤ Rmax, so if the estimator r̂ also satisfies 0 ≤ r̂ ≤ Rmax, we obtain that the risk of DR (with IPS as a special case) is at most O ( 1 n (Eµ[ρ 2σ2] + Eµ[ρ2R2max]) ) . This means that IPS and DR are unimprovable, in the worst case, beyond constant factors. Another implication is that the lower bound of Corollary 1 is sharp, and the minimax risk is precisely Θ ( 1 n (Eµ[ρ 2σ2] + Eµ[ρ2R2max]) ) . While IPS and DR exhibit the same minimax rates, Eq. (6) also immediately shows that DR will be better than IPS whenever r̂ is even moderately good (better than r̂ ≡ 0).\nComparison with asymptotic optimality results: As discussed in Section 1, previous work on optimal off-policy evaluation, specifically the average treatment estimation, assumes that it is possible to consistently estimate r∗(x, a) = E[r | x, a]. Under such an assumption it is possible to (asymptotically) match the risk of DR with the perfect reward estimator r̂ ≡ r?, and this is the best possible asymptotic risk (Hahn, 1998). This optimal risk is 1 n ( Eµ[ρ2σ2] + Varx∼DEπ[r∗ | x] ) , corresponding to the first two terms of Eq. (6), with no dependence on Rmax. Several estimators achieve this risk, including the multiplicative constant, under various consistency assumptions (Hahn, 1998; Hirano et al., 2003; Imbens et al., 2007; Rothe, 2016). Note that this is strictly below our lower bound for continuous λ. That is, consistency assumptions yield a better asymptotic risk than possible in the agnostic setting. The\ngap in constants between our upper and lower bounds is due to the finite-sample setting, where lower-order terms cannot be ignored, but have to be explicitly bounded. Indeed, apart from the result of Li et al. (2015), discussed below, ours is the first finite-sample lower bound for off-policy evaluation.\nComparison with multi-armed bandits: For multi-armed bandits, equivalent to contextual bandits with a single context, Li et al. (2015) show that the minimax risk equals Θ(Eµ[ρ2σ2]/n) and is achieved, e.g., by DM, whereas IPS is suboptimal. They also obtain a similar result for contextual bandits, assuming that each context appears with a large-enough probability to estimate its associated rewards by empirical averages (amounting to realizability). While we obtain a larger lower bound, this is not a contradiction, because we allow arbitrarily small probabilities of individual contexts and even continuous distributions, where the probability of any single context is zero.\nOn a closer inspection, the first term of our bound in Theorem 1 coincides with the lower bound of Li et al. (2015) (up to constants). The second term (optimized over γ) is non-zero only if there are contexts with small probabilities relative to the number of samples. In multi-armed bandits, we recover the bound of Li et al. (2015). When the context distribution is continuous, or the probability of seeing repeated contexts in a data set of size n is small, we get the minimax optimality of IPS.\nOne of our key contributions is to highlight this agnostic contextual regime where IPS is optimal. In the non-contextual regime, where each context appears frequently, the rewards for each context-action pair can be consistently estimated by empirical averages. Similarly, the asymptotic results discussed earlier focus on a setting where rewards can be consistently estimated thanks to parametric assumptions or smoothness (for non-parametric estimation), with the goal of asymptotic efficiency. Our work complements that line of research. In many practical situations, we wish to evaluate policies on high-dimensional context spaces, where the consistent estimation of rewards is not a feasible option. In other words, the agnostic contextual regime dominates.\nThe distinction between the contextual and non-contextual regime is also present in our proof, which combines a noncontextual lower bound due to the reward noise, similar to the analysis of Li et al. (2015), and an additional bound arising for non-degenerate context distributions. This latter result is a key technical novelty of our paper.\nProof sketch: We only sketch some of the main ideas here and defer the full proof to Appendix A. For simplicity, we discuss the case where λ is a continuous distribution. We consider two separate problem instances corresponding to the two terms in Theorem 1. The first part is relatively straightforward and reduces the problem to Gaussian mean\nestimation. We focus on the second part which depends on Rmax. Our construction defines a prior over the reward distributions, D(r | x, a). Given any (x, a), a problem instance is given by\nE[r | x, a] = η(x, a) = { Rmax(x, a) w.p. θ(x, a), 0 w.p. 1− θ(x, a),\nfor θ(x, a) to be appropriately chosen. Once η is drawn, we consider a problem instance defined by η where the rewards are deterministic and the only randomness is in the contexts. In order to lower bound the MSE across all problems, it suffices to lower bound Eθ[MSEη(v̂)]. That is, we can compute the MSE of an estimator for each individual η, and take expectation of the MSEs under the prior prescribed by θ. If the expectation is large, we know that there is a problem instance where the estimator incurs a large MSE.\nA key insight in our proof is that this expectation can be lower bounded by MSEEθ[η(x,a)](v̂), corresponding to the MSE of a single problem instance with the actual rewards, rather than η(x, a), drawn according to θ and with the mean reward function Eθ[η(x, a)]. This is powerful, since this new problem instance has stochastic rewards, just like Gaussian mean estimation, and is amenable to standard techniques. The lower bound by MSEEθ[η(x,a)](v̂) is only valid when the context distribution λ is rich enough (e.g., continuous). In that case, our reasoning shows that with enough randomness in the context distribution, a problem with even a deterministic reward function is extremely challenging."
  }, {
    "heading": "4. Incorporating Reward Models",
    "text": "As discussed in the previous section, it is generally possible to beat our minimax bound when consistent reward models exist. We also argued that even in the absence of a consistent model, when DR and IPS both achieve optimal risk rates, the performance of DR on finite samples will be better than IPS as long as the reward model is even moderately good (see Eq. 6). However, under a large reward noise σ, DR may still suffer from high variance when the importance weights are large, even when given a perfect reward model. In this section, we derive a class of estimators that leverage reward models to directly address this source of high variance, in a manner very different from the standard DR approach."
  }, {
    "heading": "4.1. The SWITCH Estimators",
    "text": "Our starting point is the observation that insistence on maintaining unbiasedness puts the DR estimator at one extreme end of the bias-variance tradeoff. Prior works have considered ideas such as truncating the rewards or importance weights when the importance weights are large (see, e.g., Bottou et al. 2013), which can dramatically reduce the variance at the cost of a little bias. We take the intuition a step\nfurther and propose to estimate the rewards for actions by two distinct strategies, based on whether they have a large or a small importance weight in a given context. When importance weights are small, we continue to use our favorite unbiased estimators, but switch to directly applying the (potentially biased) reward model on actions with large importance weights. Here, “small” and “large” are defined via a threshold parameter τ . Varying this parameter between 0 and∞ leads to a family of estimators which we call the SWITCH estimators as they switch between an agnostic approach (such as DR or IPS) and the direct method.\nWe now formalize this intuition, and begin by decomposing vπ according to importance weights:\nEπ[r] = Eπ[r1(ρ ≤ τ)] + Eπ[r1(ρ > τ)] = Eµ[ρr1(ρ ≤ τ)]\n+ Ex∼λ [∑ a∈A ED[r | x, a]π(a | x)1(ρ(x, a)>τ) ] .\nConceptually, we split our problem into two. The first problem always has small importance weights, so we can use unbiased estimators such as IPS or DR. The second problem, where importance weights are large, is addressed by DM. Writing this out leads to the following estimator:\nv̂SWITCH = 1\nn n∑ i=1 [riρi1(ρi ≤ τ)]\n+ 1\nn n∑ i=1 ∑ a∈A r̂(xi, a)π(a | xi)1(ρ(xi, a) > τ). (7)\nNote that the above estimator specifically uses IPS on the first part of the problem. When DR is used instead of IPS, we refer to the resulting estimator as SWITCH-DR. The reward model used within the DR part of the SWITCH-DR estimator can be the same or different from the reward model used to impute rewards in the second part. We next present a bound on the MSE of the SWITCH estimator using IPS. A similar bound holds for SWITCH-DR. Theorem 2. Let (a, x) := r̂(a, x)− E[r|a, x] be the bias of r̂ and assume r̂(x, a) ∈ [0, Rmax(x, a)] almost surely. Then for v̂SWITCH, with τ > 0, the MSE is at most\n2\nn\n{ Eµ [( σ2+R2max ) ρ21(ρ≤ τ) ] + Eπ [ R2max1(ρ> τ) ] } + Eπ [ 1(ρ> τ) ]2 .\nThe proposed estimator interpolates between DM and IPS. For τ = 0, SWITCH coincides with DM, while τ → ∞ yields IPS. Consequently, SWITCH estimator is minimax optimal when τ is appropriately chosen. However, unlike IPS and DR, the SWITCH and SWITCH-DR estimators are by design more robust to large (or heavy-tailed) importance weights. Several estimators related to SWITCH have been previously studied:\n1. Bottou et al. (2013) consider a special case of SWITCH with r̂ ≡ 0, meaning that all the actions with large importance weights are eliminated from IPS. We refer to this method as Trimmed IPS. 2. Thomas & Brunskill (2016) study an estimator similar to SWITCH in the more general setting of reinforcement learning. Their MAGIC estimator can be seen as using several candidate thresholds τ and then evaluating the policy by a weighted sum of the estimators corresponding to each τ . Similar to our approach of automatically determining τ , they determine the weighting of estimators via optimization (as we discuss below)."
  }, {
    "heading": "4.2. Automatic Parameter Tuning",
    "text": "So far we have discussed the properties of the SWITCH estimators assuming that the parameter τ is chosen well. Our goal is to obtain the best of IPS and DM, but a poor choice of τ might easily give us the worst of the two estimators. Therefore, a method for selecting τ plays an essential role. A natural criterion would be to pick τ that minimizes the MSE of the resulting estimator. Since we do not know the precise MSE (as vπ is unknown), an alternative is to minimize its data-dependent estimate. Recalling that the MSE can be written as the sum of variance and squared bias, we estimate and bound the terms individually.\nRecall that we are working with a data set (xi, ai, ri) and ρi := π(ai | xi)/µ(ai | xi). Using this data, it is straightforward to estimate the variance of the SWITCH estimator. Let Yi(τ) denote the estimated value that π obtains on the data point xi according to the SWITCH estimator with the threshold τ , that is\nYi(τ) := riρi1(ρi≤τ)+ ∑ a∈A r̂(xi, a)π(a|xi)1(ρ(xi, a)>τ),\nand Ȳ (τ) := 1n ∑n i=1 Yi(τ). Since v̂SWITCH = Ȳ (τ) and the xi are i.i.d., the variance can be estimated as\nVar(Ȳ (τ)) ≈ 1 n2 n∑ i=1 (Yi(τ)− Ȳ (τ))2 =: V̂arτ , (8)\nwhere the approximation above is clearly consistent since the random variables Yi are appropriately bounded as long as the rewards are bounded, because the importance weights are capped at the threshold τ .\nNext we turn to the bias term. For understanding bias, we look at the MSE bound in Theorem 2, and observe that the last term in that theorem is precisely the squared bias. Rather than using a direct bias estimate, which would require knowledge of the error in r̂, we will upper bound this term. We assume that the functionRmax(x, a) is known. This is not limiting since in most practical applications an a priori bound on the rewards is known. Then we can upper\nbound the squared bias as\nEπ [ 1(ρ > τ) ]2 ≤ Eπ[Rmax1(ρ > τ)]2. Replacing the expectation with an average, we obtain\nB̂ias 2\nτ :=\n[ 1\nn n∑ i=1 Eπ [ Rmax1(ρ > τ) ∣∣ xi]]2. With these estimates, we pick the threshold τ̂ by optimizing the sum of estimated variance and the upper bound on bias,\nτ̂ := argmin τ\nV̂arτ + B̂ias 2 τ . (9)\nOur upper bound on the bias is rather conservative, as it upper bounds the error of DM at the largest possible value for every data point. This has the effect of favoring the use of the unbiased part in SWITCH whenever possible, unless the variance would overwhelm even an arbitrarily biased DM. This conservative choice, however, immediately implies the minimax optimality of the SWITCH estimator using τ̂ , because the incurred bias is no more than our upper bound, and it is incurred only when the minimax optimal IPS estimator would be suffering an even larger variance.\nOur automatic tuning is related to the MAGIC estimator of Thomas & Brunskill (2016). The key differences are that we pick only one threshold τ , while they combine the estimates with many different τs using a weighting function. They pick this weighting function by optimizing a bias-variance tradeoff, but with significantly different bias and variance estimators. In our experiments, the automatic tuning using Eq. (9) generally works better than MAGIC."
  }, {
    "heading": "5. Experiments",
    "text": "We next empirically evaluate the proposed SWITCH estimators on the 10 UCI data sets previously used for off-policy evaluation (Dudík et al., 2011). We convert the multi-class classification problem to contextual bandits by treating the labels as actions for a policy µ, and recording the reward of 1 if the correct label is chosen, and 0 otherwise.\nIn addition to this deterministic reward model, we also consider a noisy reward model for each data set, which reveals the correct reward with probability 0.5 and outputs a random coin toss otherwise. Theoretically, this should lead to bigger σ2 and larger variance in all estimators. In both reward models, Rmax ≡ 1 is a valid bound.\nThe target policy π is the deterministic decision of a logistic regression classifier learned on the multi-class data, while the logging policy µ samples according to the probability estimates of a logistic model learned on a covariate-shifted version of the data. The covariate shift is obtained as in prior work (Dudík et al., 2011; Gretton et al., 2009).\nIn each data set with n examples, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards. Then, in the simulator, we randomly draw i.i.d. data sets of size 100, 200, 500, 1000, 2000, 5000, 10000, . . . until reaching n, with 500 different repetitions of each size. We estimate MSE of each estimator by taking the empirical average of the squared error over the 500 replicates; note that we can calculate the squared error exactly, because we know vπ. For some of the methods, e.g., IPS and DR, the MSE can have a very large variance due to the potentially large importance weights. This leads to very large error bars if we estimate their MSE even with 500 replicates. To circumvent this issue, we report a clipped version of the MSE that truncates the squared error to 1, namely MSE = E[(v̂ − vπ)2 ∧ 1]. This allows us to get valid confidence intervals for our empirical estimates of this quantity. Note that this does not change the MSE estimate of our approach at all, but is significantly more favorable towards IPS and DR. In this section, whenever we refer to “MSE”, we are referring to this truncated version.\nWe compare SWITCH and SWITCH-DR against the following baselines: 1. IPS; 2. DM trained via logistic regression; 3. DR; 4. Truncated and Reweighted IPS (TrunIPS); and 5. Trimmed IPS (TrimIPS).\nIn DM, we train r̂ and then evaluate the policy on the same contextual bandit data set. Following Dudík et al. (2011), DR is constructed by randomly splitting the contextual bandit data into two folds, estimating r̂ on one fold, and then evaluating π on the other fold and vice versa, obtaining two estimates. The final estimate is the average of the two. TrunIPS is a variant of IPS, where importance weights are capped at a threshold τ and then renormalized to sum to one (see, e.g., Bembom & van der Laan, 2008). TrimIPS is a special case of SWITCH due to Bottou et al. (2013) described earlier, where r̂ ≡ 0.\nFor SWITCH and SWITCH-DR as well as TrunIPS and TrimIPS we select the parameter τ by our automatic tuning from Section 4.2. To evaluate our tuning approach, we also include the results for the τ tuned optimally in hindsight, which we refer to as the oracle setting, and also show results obtained by the multi-threshold MAGIC approach. In all these approaches we optimize among 21 possible thresholds, from an exponential grid between the smallest and the largest importance weight observed in the data, considering all actions in each observed context.\nIn order to stay comparable across data sets and data sizes, our performance measure is the relative MSE with respect to the IPS. Thus, for each estimator v̂, we calculate Rel. MSE(v̂) = MSE(v̂)MSE(v̂IPS) .\nThe results are summarized in Figure 1, plotting the number\nof data sets where each method achieves at least a given relative MSE.4 Thus, methods that achieve smaller MSE across more data sets are towards the top-left corner of the plot, and a larger area under the curve indicates better performance. Some of the differences in MSE are several orders of magnitude large since the relative MSE is shown on the logaritmic scale. As we see, SWITCH-DR dominates all baselines and our empirical tuning of τ is not too far from the best possible. The automatic tuning by MAGIC tends to revert to DM, because its bias estimate is too optimistic and so DM is preferred whenever IPS or DR have some significant variance. The gains of SWITCH-DR are even greater in the noisy-reward setting, where we add label noise to UCI data.\nIn Figure 2, we illustrate the convergence of MSE as n increases. We select two data sets and show how SWITCH-DR performs against baselines in two typical cases: (i) when the direct method works well initially but is outperformed by IPS and DR as n gets large, and (ii) when the direct method works poorly. In the first case, SWITCH-DR outperforms both DM and IPS, while DR improves over IPS only moderately. In the second case, SWITCH-DR performs about as well as IPS and DR despite a poor performance of DM. In all cases, SWITCH-DR is robust to additional noise in the reward, while IPS and DR suffer from higher variance. Results for the remaining data sets are in Appendix D."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper we have carried out minimax analysis of offpolicy evaluation in contextual bandits and showed that IPS and DR are minimax optimal in the worst-case, when no consistent reward model is available. This result complements existing asymptotic theory with assumptions on reward models, and highlights the differences between agnostic and consistent settings. Practically, the result further motivates the importance of using side information, possibly by modeling rewards directly, especially when importance weights are too large. Given this observation, we propose a new class of estimators called SWITCH that can be used to combine any importance weighting estimators, including IPS and DR, with DM. The estimators adaptively switch between DM when the importance weights are large and either IPS or DR when the importance weights are small. We show that the new estimators have favorable theoretical properties and also work well on real-world data. Many interesting directions remain open for future work, including high-probability upper bounds on the finite-sample MSE of SWITCH estimators, as well as sharper finite-sample lower bounds under realistic assumptions on the reward model.\n4For clarity, we have excluded SWITCH, which significantly outperforms IPS, but is dominated by SWITCH-DR. Similarly, we only report the better of oracle-TrimIPS and oracle-TrunIPS."
  }, {
    "heading": "Acknowledgments",
    "text": "The work was partially completed during YW’s internship at Microsoft Research NYC from May 2016 to Aug 2016. The authors would like to thank Lihong Li and John Langford for helpful discussions, Edward Kennedy for bringing our attention to related problems and recent developments in causal inference, and an anonymous reviewer for pointing out relevant econometric references and providing valuable feedback that helped connect our work with research on average treatment effects."
  }],
  "year": 2017,
  "references": [{
    "title": "Doubly robust estimation in missing data and causal inference models",
    "authors": ["Bang", "Heejung", "Robins", "James M"],
    "year": 2005
  }, {
    "title": "Data-adaptive selection of the truncation level for inverse-probabilityof-treatment-weighted estimators",
    "authors": ["Bembom", "Oliver", "van der Laan", "Mark J"],
    "year": 2008
  }, {
    "title": "Asymptotically exact minimax estimation in sup-norm for anisotropic Hölder",
    "authors": ["Bertin", "Karine"],
    "venue": "classes. Bernoulli,",
    "year": 2004
  }, {
    "title": "Some results on generalized difference estimation and generalized regression estimation for finite populations",
    "authors": ["Cassel", "Claes M", "Särndal", "Carl E", "Wretman", "Jan H"],
    "year": 1976
  }, {
    "title": "Doubly robust policy evaluation and learning",
    "authors": ["Dudík", "Miroslav", "Langford", "John", "Li", "Lihong"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Doubly robust policy evaluation and optimization",
    "authors": ["Dudík", "Miroslav", "Erhan", "Dumitru", "Langford", "John", "Li", "Lihong"],
    "venue": "Statistical Science,",
    "year": 2014
  }, {
    "title": "Covariate shift by kernel mean matching",
    "authors": ["Gretton", "Arthur", "Smola", "Alex", "Huang", "Jiayuan", "Schmittfull", "Marcel", "Borgwardt", "Karsten", "Schölkopf", "Bernhard"],
    "venue": "Dataset Shift in Machine Learning,",
    "year": 2009
  }, {
    "title": "On the role of the propensity score in efficient semiparametric estimation of average treatment effects",
    "authors": ["Hahn", "Jinyong"],
    "year": 1998
  }, {
    "title": "Efficient estimation of average treatment effects using the estimated propensity",
    "authors": ["Hirano", "Keisuke", "Imbens", "Guido W", "Ridder", "Geert"],
    "venue": "score. Econometrica,",
    "year": 2003
  }, {
    "title": "Probability inequalities for sums of bounded random variables",
    "authors": ["Hoeffding", "Wassily"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1963
  }, {
    "title": "A generalization of sampling without replacement from a finite universe",
    "authors": ["Horvitz", "Daniel G", "Thompson", "Donovan J"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1952
  }, {
    "title": "Meansquared-error calculations for average treatment effects",
    "authors": ["Imbens", "Guido", "Newey", "Whitney", "Ridder", "Geert"],
    "venue": "Technical report,",
    "year": 2007
  }, {
    "title": "URL http://www.stat.cmu.edu/ ~larry/=sml/Minimax.pdf",
    "authors": ["Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"],
    "venue": "Minimax theory,",
    "year": 2008
  }, {
    "title": "Toward minimax off-policy value estimation",
    "authors": ["Li", "Lihong", "Munos", "Rémi", "Szepesvári", "Csaba"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Semiparametric efficiency in multivariate regression models with missing data",
    "authors": ["Robins", "James M", "Rotnitzky", "Andrea"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1995
  }, {
    "title": "The value of knowing the propensity score for estimating average treatment effects",
    "authors": ["Rothe", "Christoph"],
    "venue": "IZA Discussion Paper Series,",
    "year": 2016
  }, {
    "title": "Sur les fonctions d’ensemble additives et continues",
    "authors": ["Sierpiński", "Wacław"],
    "venue": "Fundamenta Mathematicae,",
    "year": 1922
  }, {
    "title": "Data-efficient offpolicy policy evaluation for reinforcement learning",
    "authors": ["Thomas", "Philip S", "Brunskill", "Emma"],
    "venue": "In ICML,",
    "year": 2016
  }],
  "id": "SP:030015d1786aa4d9d6e8ef681f1ff1c346d7224e",
  "authors": [{
    "name": "Yu-Xiang Wang",
    "affiliations": []
  }, {
    "name": "Alekh Agarwal",
    "affiliations": []
  }, {
    "name": "Miroslav Dudík",
    "affiliations": []
  }],
  "abstractText": "We study the off-policy evaluation problem— estimating the value of a target policy using data collected by another policy—under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of data sets, often outperforming prior work by orders of magnitude.",
  "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits"
}