{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The advent of the Big Data era presents special challenges to practitioners of Bayesian modeling because typical sampling-based inference methods have a computational cost per sample linear in the size of the dataset. This computational burden has been addressed in recent years through two major approaches (see (Bardenet et al., 2015) for a recent overview): (i) split the data into batches and combine posterior samples obtained in parallel from each batch, or (ii) use variants of the Markov Chain Monte Carlo (MCMC) algorithm that only query a subset of the data at every iteration. Our interest in the paper is in the latter approach, where many methods are based on modifying both steps of the Metropolis-Hastings (MH) algorithm: in the proposal step, only a mini-batch of the data is used, and the accept-reject step is either ignored or approximated (Korattikara et al., 2013; Bardenet et al., 2014). This strategy has been explored using proposals from Langevin (Welling & Teh, 2011), Riemannian Langevin (Patterson & Teh, 2013), Hamiltonian (Chen et al., 2014) and Riemannian Hamiltonian (Ma et al., 2015) dynamics. Other relevant works\n*Equal contribution 1Statistics Department and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA 2Duke University, Durham, NC 27708, USA. Correspondence to: Ari Pakman <aripakman@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ninclude (Ahn et al., 2012; Ding et al., 2014).\nDespite the success of the above approach, the partial accept-reject step is a source of bias, the precise size of which is difficult to control, and which tends to be amplified by the noisy evaluation of the gradient. This has motivated the search for unbiased stochastic samplers, such as the Firefly MCMC algorithm (Maclaurin & Adams, 2014), the debiased pseudolikelihood approach of (Quiroz et al., 2016), and the quasi-stationary distribution approach of (Pollock et al., 2016).\nThe present work is motivated by the idea that the bias could be reduced by starting from a rejection-free MCMC algorithm, avoiding thus the Metropolis-Hastings algorithm altogether. Two similar algorithms of this type have been recently proposed: the Bouncy Particle Sampler (BPS) (Peters & de With, 2012; Bouchard-Côté et al., 2015), and Zig-Zag Monte Carlo (Bierkens & Roberts, 2015; Bierkens et al., 2016). These algorithms sample from the target distribution through non-reversible, piecewise linear Markov processes. Non-reversibility (i.e., the failure to satisfy detailed balance) has been shown in many cases to yield faster mixing rates (Neal, 2004; Vucelja, 2014; Bouchard-Côté et al., 2015).\nOur contributions in this paper are twofold. Firstly, we show that the BPS algorithm is particularly well suited to sample from posterior distributions of big datasets, because the target distribution is invariant under zero-mean noisy perturbations of the log-likelihood gradient, such as those introduced by using mini-batches of the full dataset in each iteration.\nStochastic variants of BPS or Zig-Zag that preserve exactly the target distribution have been proposed, such as Local BPS (Bouchard-Côté et al., 2015) or Zig-Zag with subsampling (ZZ-SS) (Bierkens et al., 2016), but they lead to extremely slow mixing because are based on overly conservative bounds (which moreover must be derived on a case-by-case basis, and in many cases may not hold at all). This leads us to our second contribution, the Stochastic Bouncy Particle Sampler (SBPS), a stochastic version of the BPS algorithm which trades a small amount of bias for significantly reduced variance, yielding superior performance (and requiring no parameter tuning or derivation of problem-specific bounds) compared to exist-\nAlgorithm 1 Bouncy Particle Sampler\nInitialize particle position w0 ∈ RD and velocity v ∈ SD−1 while desired do\nSample Poisson process first arrivals tr, tb with rates λr,λ(t) = [v · ∇U(w0 + vt)]+ Let t = min(tb, tr) Move wt = w0 + vt , if tb < tr then\nReflect v← v − 2 (v·∇U(wt))∇U(wt)||∇U(wt)||2 else\nRefresh: sample v ∼ Unif[SD−1] end if Let w0 ← wt\nend while RETURN piecewise linear trajectory of w\ning subsampling-based Monte Carlo methods. SBPS inherits the piecewise linear sample paths of BPS, and therefore enjoys faster convergence of empirical means, particularly of rapidly varying test functions, compared to more standard approaches.\nWe organize this paper as follows. In Section 2 we review the Bouncy Particle Sampler, in Section 3 we study the invariance of the target distribution under noise perturbations to the BPS updates, in Section 4 we introduce SBPS, and in Section 5 a preconditioned variant. In Section 6 we discuss related works and in Section 7 we illustrate the advantages of SBPS in several examples."
  }, {
    "heading": "2. The Bouncy Particle Sampler",
    "text": "Consider a distribution p(w) ∝ e−U(w) ,w ∈ RD, where the normalization factor may be intractable. The Bouncy Particle Sampler (BPS), proposed in (Peters & de With, 2012; Monmarché, 2014) and formalized and developed in (Bouchard-Côté et al., 2015), introduces a random velocity vector v distributed uniformly in the unit sphere SD−1, and defines a continuous Markov process in (w,v). To describe this process we begin in discrete time and then take the continuous-time limit. Denoting time by t, consider a discrete-time Markov process that acts on the variables (w,v) as\n(w,v)t+∆t =\n{ (w+v∆t,v) w/prob. 1−∆t[G]+\n(w+v∆t,vr) w/prob. ∆t[G]+ (1)\nwhere\n[x]+ = max(x, 0) , (2) G = v · ∇U(w) , (3)\nvr = v − 2 (v·∇U(w))∇U(w)||∇U(w)||2 . (4)\nNote that G in (3) is the directional derivative of U(w) in the direction v, and vr is a reflection of v with respect to the plane perpendicular to the gradient ∇U , satisfying vr · ∇U = −v · ∇U and (vr)r = v. In other words, the particle w moves along a straight line in the direction of v and this direction is reflected as (4) with probability ∆t[G]+. This probability is non-zero only if the particle is moving in a direction of lower target probability p(w), or equivalently higher potential U(w).\nApplying the transition (1) repeatedly and taking ∆t → 0, the random reflection point becomes an event in an inhomogeneous Poisson process with intensity [G]+. The resulting sampling procedure generates a piecewise linear Markov process (Davis, 1984; Dufour et al., 2015), and is summarized in Algorithm 1. Note that the algorithm also includes occasional resamplings of v, to ensure ergodicity (Bouchard-Côté et al., 2015). Remarkably, in the limit ∆t → 0, the algorithm leaves the joint factorized distribution p(w)p(v) invariant, as we review in Supp. Material A.1.\nThe Zig-Zag process (Bierkens & Roberts, 2015; Bierkens et al., 2016) is similar to BPS, but velocity components can take only ±1 values, and the piecewise linear trajectories change direction only in a single coordinate at each random breakpoint. For a review of these methods, see (Fearnhead et al., 2016; Bierkens et al., 2017)."
  }, {
    "heading": "3. Noise Resilience and Big Data",
    "text": ""
  }, {
    "heading": "3.1. Noise Resilience",
    "text": "Let us assume that only a noisy version of the gradient is available to compute the probability of bouncing and the reflected velocity in (4). In the Big Data scenario described below, this is the result of using a random subset of the data at each gradient evaluation, and can be represented as\n∇Ũ(w) = ∇U(w) + nw , nw ∼ p(nw|w) , (5)\nwhere nw ∈ RD and p(nw|w) has zero mean.\nTheorem 1: The invariance of p(w,v) under the BPS algorithm is unaffected by the zero-mean noise (5) if nw1 and nw2 are independent for w1 6= w2.\nSee Supp. Material A.2 for a proof sketch. Defining G̃ = v · ∇Ũ(w), the intensity of the inhomogeneous Poisson process [G̃]+, which determines the time of the velocity bounce, now becomes stochastic, and the resulting point process is called a doubly stochastic, or Cox, process (Cox, 1955; Grandell, 1976). The effect of the gradient noise is to increase the average point process intensity, since E [ [G̃]+ ] ≥ [ E[G̃] ] + , from Jensen’s inequality. This\nleads to more frequent bounces and typically a slower mixing of the Markov process, as illustrated in Figure 1.\nMany Cox processes are based on Poisson intensities obeying stochastic differential equations, or assume that the joint distribution at several w’s has a non-trivial wdependent structure. Our case is different because we assume that nw1 and nw2 are independent even when w1 and w2 are infinitesimally close."
  }, {
    "heading": "3.2. Sampling from Big Data posteriors",
    "text": "In a prototypical Bayesian setting, we have a prior f(w), i.i.d. data points xi, and the negative log-posterior gradient is\n∇U(w) = −∇ [ log f(w) +\nN∑ i=1 log p(xi|w) ] . (6)\nWhen N is big we consider replacing the above gradient by the noisy approximation\n∇Ũ(w) = −∇ [ log f(w) + Nn n∑ i=1 log p(xri |w) ] , (7)\nwhere n N and the n indices {ri} are sampled randomly without replacement. To sample from the posterior using the noisy gradient (7), we want to simulate the first arrival time in a doubly stochastic Poisson process with random intensity [G̃(t)]+, where\nG̃(t) = v · ∇Ũ(w + vt) . (8)\nNote that Ũ is a stochastic process, and noise independence for different w’s implies that different t’s require independent mini-batches. Out of several methods to sample from\n(noisy) Poisson processes, the thinning method (Lewis & Shedler, 1979) is compatible with the noise independence assumption. This is a form of rejection sampling which proposes a first arrival time t, sampled from an inhomogeneous Poisson process with intensity λ(t) such that λ(t) ≥ [G̃(t)]+ The particle moves a distance tv, and accepts the proposal to bounce the velocity with probability [G̃(t)]+/λ(t). Note that this accept-reject step is different from the MH algorithm (Robert & Casella, 2013), since the particle always moves the distance tv, and a rejection only affects the velocity bouncing. This can greatly improve the efficiency of the sampler. As in the noiseless case, one should in general also resample v occasionally, to ensure ergodicity (Bouchard-Côté et al., 2015), although in the examples we considered this was not empirically necessary, since the mini-batch noise serves to randomize the velocity sufficiently, preventing “non-ergodic” trajectories that do not explore the full space.\nIn some special cases one can derive a bound λ(t) that always holds (Bouchard-Côté et al., 2015; Bierkens et al., 2017). But this is atypical, due to the dependence of G̃(t) in (8) on the changing velocity v and the mini-batch noise. Even when such bounds do exist, they tend to be conservatively high, leading to an inefficient sampler with many rejected proposals (wasting many mini-batches of data) before accepting.\nInstead, we propose below an adaptive approximate bound which achieves a bias-variance trade-off between the frequency of the bounce proposals and a controllable probability of bound violation."
  }, {
    "heading": "4. Proposal from Local Regression",
    "text": "Our approach to an adaptive and tractable proposal intensity λ(t) relies on a predictive model of G̃ based on previous observations; the key idea is to exploit the correlations between nearby G̃ values. The upper value of the resulting predictive confidence band can then be used as λ(t), and this band is adaptively updated as more proposals are generated.\nWhile there are many possibilities for such a predictive model, we found that a simple local linear model was very effective and computationally trivial. Consider then the linear regression of m observed values G̃i ≡ G̃(ti) since the previous bounce,\nG̃i = β1ti + β0 + εti εti ∼ N(0, c2ti) , (9)\nwhere i = 1, . . . ,m and the noise variance can be estimated from the mini-batch in (7) as\nc2t = N2 n (1− n N )Vari [v · ∇ log p(xri |w)] . (10)\nHere Vari denotes the sample variance of the mini-batch, and we included the finite population correction factor (1 − nN ) because the indices {ri} are sampled without replacement. The Gaussian noise assumption in G̃(t) in (9) is valid when the mini-batch is sufficiently large that we can appeal to a central limit theorem. (For heavy-tailed noise we could consider more robust estimators, but we do not pursue this direction here.)\nAdding a Gaussian priorN(µ, σ2) to β1, and defining xi ≡ (1, ti), the log posterior of β = (β0, β1)T is\n2 log p(β|{ti, G̃i, c2ti}) = − m∑ i=1 (G̃i−xi·β)2 c2ti\n− (β1−µ) 2\nσ2 + const.\nLet β̂ and Σ be the mean and covariance of this distribution. Using these estimates, we obtain the predictive distribution Ĝ(t) for G̃(t) for t > tm,\nĜ(t) = β̂1t+ β̂0 + ηt ηt ∼ N(0, ρ2(t)) (11)\nwhere ρ2(t) = xΣxT + c2tm (12)\nwith x = (1, t). Note that as usual the noise variance is different in (9) and (11), since in (9) we are fitting observed pairs G̃i, ti, while in (11) we are predicting the value of G̃(t) and we include the uncertainty from the β̂ estimates. Also, for simplicity we extrapolate the observation noise to be the same as in the last mini-batch, c2tm .\nWe can now construct a tractable approximate thinning proposal intensity by choosing a confidence band multiple k,\nand defining γ(t) as a linear interpolation between selected points along the non-linear curve\nβ̂1t+ β̂0 + kρ(t) . (13)\nThe proposal intensity is now λ(t) = [γ(t)]+, and sampling from an inhomogeneous Poisson process with piecewise linear rate λ(t) can be done analytically using the inverse CDF method. When a bounce time is proposed at time t, the particle moves a distance tv, a noisy observation G̃(t) is made as in (8) and the bounce time is accepted with probability min(1, [G̃(t)]+/λ(t)). If the bounce is accepted, the velocity is reflected as in (4) (using Ũ instead of U ), and the set of observed values is reinitialized with (−G̃(t), ct), which are the values one would obtain from sampling the same mini-batch after the bounce, since vr · Ũ = −v · Ũ = −G̃(t). On the other hand, if the proposal is rejected, the observed (G̃(t), ct) are added to the set of observed values. The hyperparameters µ, σ2 of the regression model can be learned by performing, after each bounce, a gradient ascent step on the marginal likelihood, p({G̃i}|µ, σ2); this gradient can be computed analytically and does not significantly impact the computational cost.\nThe linear model for G̃ is good when the target distribution can be locally approximated by a Gaussian, since G̃(t) in (8) is a projection of the derivative of the negative log\nposterior. When the posterior is highly non-Gaussian, a decaying weight can be used for more-distant observations, leading to a local regression; the scale of this decay can be fit again via stochastic gradient ascent on the predictive likelihood. We have also explored a Gaussian Process regression model, but it did not improve over the linear model in the cases we considered. In Supp. Material E we discuss a potential problem with our approach in the case of multimodal distributions, and propose a solution for such cases.\nFinally, note that the directional derivative of Ũ(w) needed in (8) can in many cases be computed at a cheaper cost (by a factor of d = dim(w)) than the full gradient. The latter is only needed when a bounce is accepted. This is in contrast to other gradient based samplers which require the full gradient at every step.\nWe dub this approach to BPS with noisy gradients Stochastic BPS (SBPS). See Supp. Material C for pseudocode. Figure 2 illustrates the evolution of these dynamic proposal intensities in a simple example. In Section 5, we consider a variant to SBPS, called pSBPS, that learns a diagonal preconditioning factor for the gradient, and leads to a more efficient exploration of the space when the posterior is highly anisotropic and roughly axis-aligned."
  }, {
    "heading": "4.1. Bias in the Samples",
    "text": "The constant k in (13) controls the tradeoff between bias from possible [G̃(t)]+/λ(t) > 1 cases and lower computational cost: higher k leads to a more conservative (higher) proposal intensity and therefore a less-biased but more data-inefficient sampler. We present a bound on the Wasserstein distance between the exact and bias distributions in Supp. Material B, and explore this bias-variance tradeoff further in Supp. Material F. A quick bias diagnostic is the rate at which the bound is violated, i.e., cases with [G̃(t)]+/λ(t) > 1; if this rate is significantly higher than expected under the local linear regression model, then a different approach should be considered."
  }, {
    "heading": "5. Preconditioned SBPS",
    "text": "Consider now the linear transformation w = Az with an arbitrary square matrix A. A distribution p(w) of interest can be expressed in terms of z as\npz(z)dz = p(w(z))dw = p(Az)|A|dz , (14) = exp(−Uz(z))dz . (15)\nThe SBPS algorithm can be applied to the density pz(z) using the gradients of U(w). For this note that∇zUz(z) = A∇wU(w). The Poisson intensity to compute bounces is [G]+, with G = v ·A∇U(w), and the velocity reflection\nis computed as\nvr = v − 2 (v ·A∇U(w))A∇U(w)\n||A∇U(w)||2 . (16)\nThe piecewise linear trajectory zt = z0+vt becomes wt = w0 + Avt. The matrix A is called a preconditioner in the optimization literature, but can also be used in a sampling context to reduce anisotropy of posterior distributions; it is often the case that a good preconditioner is not known in advance but is instead learned adaptively (Duchi et al., 2011).\nWe use a diagonal preconditioner for simplicity. Denoting the ith component at the jth evaluation of the gradient by gji , we define\naji = β(g j i ) 2 + (1− β)aj−1i , (17) ãj = 1d ∑d i=1\n1√ aji+ , (18)\nfor some 0 ≤ β ≤ 1, 1. The preconditioner at iteration j is defined as Aj = Diag (\nãj√ aji+\n) . This is the same\npreconditioner used in (Li et al., 2016), up to the ãj factor; the latter is needed here in order to prevent scaling of G̃.\nAs noted in (Li et al., 2016), a time dependent preconditioner requires adding a term proportional to ∂A j\n∂w to the gradient, yet this term is negligibly small and can be ignored when β ≈ 1, since in this parameter regime the preconditioner changes slowly as a function of j and thus of w.\nWe call this preconditioned variant pSBPS. It performs favorably compared to SBPS when the posterior is anisotropic and axis-aligned, since we use a diagonal approximation of the Hessian in the preconditioner. See (Bierkens et al., 2017) for a related approach. As Figure 3 shows, pSBPS converges to the posterior mode faster than SBPS, and mixes faster in the direction of greatest covariance.1\n1pSBPS code at https://github.com/dargilboa/SBPS-public."
  }, {
    "heading": "6. Related Works",
    "text": "Biased Samplers: Many stochastic gradient samplers (e.g. (Welling & Teh, 2011)) can be formulated exactly using a Wiener process (Ma et al., 2015), but they are biased because (i) the Gaussian assumption in the noise may not hold for small mini-batches, and (ii) the MH correction to the time discretization is avoided or approximated. Recently, irreversible samplers have been studied in this context (Ma et al., 2016). Choosing the step size in these samplers can be quite challenging, as discussed below: too-large step sizes increase the bias, while too-small step sizes slow the mixing, and in generic high-dimensional examples there is no way to automatically tune the step size (though see (Giles et al., 2016) for recent progress). In contrast, the bias in SBPS, controlled by the constant k, does not come from time discretization, but from easy-to-track violations of the thinning bound when [G̃(t)]+/λ(t) > 1.\nExact non-BPS-like Samplers: Firefly MCMC (Maclaurin & Adams, 2014) augments the target distribution with one binary variable per data point, and yields unbiased samples while only querying a subset of data points at each iteration. But it needs distribution-dependent lower bounds on the likelihood and requires an initial full sweep of the\ndata. Also mixing can be extremely slow (Quiroz et al., 2015; Bardenet et al., 2015), and all the dataset must be available for access all the time.\nTwo recent novel proposals are (Quiroz et al., 2016), based on debiased pseudolikelihood combined with variance reduction techniques, and (Pollock et al., 2016), based on quasi-stationary distributions. These methods are relatively more complex, and we have not yet systematically compared them against SBPS.\nExact BPS-like Samplers: Two subsampling variants of BPS which preserve the exact distribution are Local BPS (Bouchard-Côté et al., 2015), that needs a preprocessing step of computational cost O(N logN), and ZZ-SS (Bierkens et al., 2016). In these approaches, the requirement to preserve the distribution exactly leads to extremely conservative thinning bounds, which in turn yield a very slow exploration of the space, as we will see below. Also, the bounds need to be rederived for each new model (if possible at all), unlike SBPS which can be used for any differentiable posterior distribution."
  }, {
    "heading": "7. Experiments",
    "text": ""
  }, {
    "heading": "7.1. Logistic Regression",
    "text": "Although simpler MCMC methods perform well in Bayesian logistic regression (BLR) models (Chopin & Ridgway, 2015), we begin with this well-understood case for comparing SBPS against a few of the existing stochastic MCMC methods discussed in the previous section. To generate the data, we sampled the components of the true w ∈ Rd from Unif[−5, 5] and N data points {xi} from a d-dimensional zero-mean Gaussian, with one component of the diagonal covariance set to 6 and all the rest to 1. Labels {yi} are drawn from yi ∼ Bern(σ(w · xi)), where σ(x) = 1/(1 + ex). In the regime d N the Laplace approximation holds fairly well, providing another good comparison method. Figure 4 shows results for N = 1000, d = 20, k = 3, n = 100.\nWe run comparisons against the biased stochastic samplers Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011) and multivariate Stochastic Gradient NoseHoover Thermostat (mSGNHT) (Li et al., 2015) with fixed step sizes. As noted above, choosing optimal step sizes for these samplers is challenging. To allow SGLD and mSGNHT to perform best, we performed a scan to find the largest (fastest-mixing) step size that did not lead to overly large bias compared to the Laplace approximation. (Note, importantly, that this scan is expensive and is not possible in high-dimensional examples where the Laplace approximation does not hold - precisely the cases where MCMC methods are most valuable.) See Supp. Material E for details of this scan, which led to an optimal step size of 0.1 for SGLD. Larger step sizes led to visible biases in the samples (not shown); we also show the results with step size 0.01 for comparison to note that the results do depend sensitively on this parameter.\nWe also compare against ZZ-SS. Instead of Local BPS, we ran comparisons against an unbiased method we call lipSBPS (short for Lipshitz BPS), where the velocity bounces occur as first arrival events in a Poisson process with noisy intensity [v · ∇Ũ(w)]+ built from a noisy gradient (7) of minimal size n = 1, and simulated with thinning using an exact upper bound derived in Supp. Material F. One can verify that the resulting stochastic process is identical to that of Local BPS. Our bound is higher than that used in (Bouchard-Côté et al., 2015) by up to a factor of 2, which results in up to twice as many bounce proposals. On the other hand, our bound can be computed in O(N) time, does not require non-negative covariates, and can be used also for n > 1. Again, we note that this lipSBPS method, like Local BPS and ZZ-SS, are not generally applicable because the derived bounds only apply in special cases.\nThe results of Figure 4 show that SBPS outperforms the op-\ntimally tuned SGLD and mSGNHT, and converges orders of magnitude faster than lipSBPS and ZZ-SS. While the latter two methods are unbiased, our results suggest that the small bias introduced by SBPS is worth the massive reduction in variance.\nIn Supp. Material F we explore the effects of the hyperparameters: k, n, and v refresh rate λr. The conclusion is that in this logistic example no manual hyperparameter tuning was required (in stark contrast to the careful step size tuning required for SGLD): the bias-controlling constant k can be set in the range k ∈ [3, 5] (consistent with the tails of the Gaussian in the linear regression model) and the minibatch size n should be small, but large enough for the CLT to justify the noise term in (9); n = 100 worked well, but the results were not sensitively dependent on n. For small values of n the mini-batch variability provided sufficient velocity randomness that no additional velocity refreshes were necessary, so we did not have to tune λr either.\nThe comparison to pSBPS shows an improvement in the rate of convergence to the posterior mode. The MAP estimator ŵ was calculated using SAG (Roux et al., 2012), and the Hessian was computed exactly."
  }, {
    "heading": "7.2. Continuous Trajectory Sampling",
    "text": "A unique feature of BPS-like samplers is that their output is a continuous trajectory. Given w0 and a set of R velocities and bounce times {vi, ti}, the estimated expectation of a test function f(w) is\n〈f(w)〉BPS ≡ 1\nT R−1∑ i=0 ti∫ 0 f(wi + vit)dt (19)\nwhere wi+1 = wi + viti and T is the total particle travel time. For simple test functions this integral is analytic, while more generally it can be computed numerically\nwith standard efficient one-dimensional quadrature methods. When f(w) varies across a characteristic length r shorter than the average trajectory length b of the linear segments, we intuitively expect the error in the estimate (19) to be smaller than in estimators based on discrete samples. Note that this advantage tends to diminish for higher SBPS noise, since the linear segments become shorter.\nFigure 5 explores empirically this idea in a simple setting by comparing the value of the expectation of f(w) = sin((w − ŵ)/r) under the posterior distribution of the logistic example considered above. Here (w, ŵ) are the first coordinates of the vectors (w, ŵ), ŵ is the MAP value, and r the characteristic length of f . As expected, the error in the expectation is lower in the continuous case for r/b < 1."
  }, {
    "heading": "7.3. Neural Network Posterior Sampling",
    "text": "We considered a simple model of one hidden layer followed by a softmax. For Bayesian approaches to neural networks see (Neal, 2012; Gal, 2016). The likelihood was the standard cross entropy with an additional L2 regular-\nization term L = − N∑ i=1 log(pi) + c 2 d∑ j=1 w2j where pi is the probability of classifying the ith example correctly. L was approximated via subsampling, and c = 0.001. This architecture was trained on the MNIST dataset. A subset of the training set was preprocessed by downsampling the images to 7 × 7, removing pixels that are 0 for all training examples and decreasing the number of digits to 4. The resulting training set size was N = 8340. The resulting dimensionality of the posterior was d = 192. Mini-batch size was n = 500 for all methods. All weights were initialized at 0 and all methods were run for 104 epochs. SBPS is compared with SGLD at different step sizes, and performance\nis comparable to SGLD with an appropriate step size without requiring an expensive scan over step sizes. Since the additional regularization term can lead to unbounded gradients of the log posterior ∇U(w) one can no longer use the bounds derived for the Local BPS and ZZ-SS algorithms and thus they cannot be applied to this problem without further work. This is not the case for SBPS. The posterior is not Gaussian due to the likelihood terms and thus the Laplace approximation is not effective unless the posterior is dominated by the prior.\nIn order to assess the quality of the sampling, we compare the trajectories to a standard costly Metropolis-Hastings MCMC using a Gaussian with variance 0.2 as the proposal distribution. This algorithm was run for 4 ∗ 105 epochs and the proposal acceptance rate was 0.43. Figure 6 shows samples in the directions of the largest, median and smallest variance of the empirical covariance matrix of the Metropolis-Hastings samples."
  }, {
    "heading": "8. Conclusions",
    "text": "This paper introduced a non-reversible sampler that can be applied to big datasets by means of subsampling the data in each iteration. At the price of a small, controllable bias, it provides the benefits of (i) high mixing speed associated with non-reversibility, and (ii) continuous sample trajectories, with (iii) minimal hyperparameter tuning required, leading to state of the art performance and making it a convenient alternative to biased, difficult-to-tune MHbased stochastic samplers."
  }],
  "year": 2017,
  "references": [{
    "title": "Bayesian posterior sampling via stochastic gradient fisher scoring",
    "authors": ["Ahn", "Sungjin", "Korattikara", "Anoop", "Welling", "Max"],
    "year": 2012
  }, {
    "title": "Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach",
    "authors": ["Bardenet", "Rémi", "Doucet", "Arnaud", "Holmes", "Chris"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "On Markov chain Monte Carlo methods for tall data",
    "authors": ["Bardenet", "Rémi", "Doucet", "Arnaud", "Holmes", "Chris"],
    "year": 2015
  }, {
    "title": "A piecewise deterministic scaling limit of Lifted Metropolis-Hastings in the Curie-Weiss model",
    "authors": ["Bierkens", "Joris", "Roberts", "Gareth"],
    "year": 2015
  }, {
    "title": "The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data",
    "authors": ["Bierkens", "Joris", "Fearnhead", "Paul", "Roberts", "Gareth"],
    "venue": "arXiv preprint arXiv:1607.03188,",
    "year": 2016
  }, {
    "title": "The Bouncy Particle Sampler: A NonReversible Rejection-Free",
    "authors": ["Bouchard-Côté", "Alexandre", "Vollmer", "Sebastian J", "Doucet", "Arnaud"],
    "venue": "Markov Chain Monte Carlo Method",
    "year": 2015
  }, {
    "title": "Stochastic gradient HMC",
    "authors": ["Chen", "Tianqi", "Fox", "Emily B", "Guestrin", "Carlos"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Leave Pima Indians alone: binary regression as a benchmark for Bayesian computation",
    "authors": ["Chopin", "Nicolas", "Ridgway", "James"],
    "venue": "arXiv preprint arXiv:1506.08640,",
    "year": 2015
  }, {
    "title": "Some statistical methods connected with series of events",
    "authors": ["Cox", "David R"],
    "venue": "J. Royal Stat. Soc., Series B (Methodological), pp",
    "year": 1955
  }, {
    "title": "Piecewise-deterministic Markov processes: A general class of non-diffusion stochastic models",
    "authors": ["Davis", "Mark HA"],
    "venue": "J. Royal Stat. Soc., Series B (Methodological),",
    "year": 1984
  }, {
    "title": "Bayesian sampling using stochastic gradient thermostats",
    "authors": ["Ding", "Nan", "Fang", "Youhan", "Babbush", "Ryan", "Chen", "Changyou", "Skeel", "Robert D", "Neven", "Hartmut"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Numerical Methods for Simulation and Optimization of Piecewise Deterministic Markov Processes",
    "authors": ["Dufour", "François", "Zhang", "Huilong"],
    "year": 2015
  }, {
    "title": "Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo",
    "authors": ["Fearnhead", "Paul", "Bierkens", "Joris", "Pollock", "Murray", "Roberts", "Gareth O"],
    "venue": "arXiv preprint arXiv:1611.07873,",
    "year": 2016
  }, {
    "title": "Uncertainty in Deep Learning (Cambridge PhD Thesis)",
    "authors": ["Gal", "Yarin"],
    "year": 2016
  }, {
    "title": "Multilevel Monte Carlo for Scalable Bayesian Computations",
    "authors": ["Giles", "Mike", "Nagapetyan", "Tigran", "Szpruch", "Lukasz", "Vollmer", "Sebastian", "Zygalakis", "Konstantinos"],
    "venue": "arXiv preprint arXiv:1609.06144,",
    "year": 2016
  }, {
    "title": "Doubly stochastic Poisson processes",
    "authors": ["Grandell", "Jan"],
    "year": 1976
  }, {
    "title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget",
    "authors": ["Korattikara", "Anoop", "Chen", "Yutian", "Welling", "Max"],
    "year": 2013
  }, {
    "title": "Simulation of nonhomogeneous Poisson processes by thinning",
    "authors": ["Lewis", "Peter A", "Shedler", "Gerald S"],
    "venue": "Naval Research Logistics Quarterly,",
    "year": 1979
  }, {
    "title": "High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep Models",
    "authors": ["Li", "Chunyuan", "Chen", "Changyou", "Fan", "Kai", "Carin", "Lawrence"],
    "venue": "arXiv preprint arXiv:1512.07662,",
    "year": 2015
  }, {
    "title": "Preconditioned stochastic gradient langevin dynamics for deep neural networks",
    "authors": ["Li", "Chunyuan", "Chen", "Changyou", "Carlson", "David", "Carin", "Lawrence"],
    "year": 2016
  }, {
    "title": "A complete recipe for stochastic gradient MCMC",
    "authors": ["Ma", "Yi-An", "Chen", "Tianqi", "Fox", "Emily"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "A Unifying Framework for Devising Efficient and Irreversible MCMC Samplers",
    "authors": ["Ma", "Yi-An", "Chen", "Tianqi", "Wu", "Lei", "Fox", "Emily B"],
    "venue": "arXiv preprint arXiv:1608.05973,",
    "year": 2016
  }, {
    "title": "Firefly Monte Carlo: Exact MCMC with subsets of data",
    "authors": ["Maclaurin", "Dougal", "Adams", "Ryan P"],
    "year": 2014
  }, {
    "title": "Piecewise deterministic simulated annealing",
    "authors": ["Monmarché", "Pierre"],
    "venue": "arXiv preprint arXiv:1410.1656,",
    "year": 2014
  }, {
    "title": "Improving asymptotic variance of MCMC estimators: Non-reversible chains are better",
    "authors": ["Neal", "Radford M"],
    "venue": "arXiv preprint math/0407281,",
    "year": 2004
  }, {
    "title": "Bayesian learning for neural networks, volume 118",
    "authors": ["Neal", "Radford M"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Stochastic gradient Riemannian Langevin dynamics on the probability simplex",
    "authors": ["Patterson", "Sam", "Teh", "Yee Whye"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Rejection-free Monte Carlo sampling for general potentials",
    "authors": ["Peters", "EAJF", "G. de With"],
    "venue": "Phys. Rev. E,",
    "year": 2012
  }, {
    "title": "The Scalable Langevin Exact Algorithm: Bayesian Inference for Big Data",
    "authors": ["Pollock", "Murray", "Fearnhead", "Paul", "Johansen", "Adam M", "Roberts", "Gareth O"],
    "venue": "arXiv preprint arXiv:1609.03436,",
    "year": 2016
  }, {
    "title": "Speeding up MCMC by efficient data subsampling",
    "authors": ["Quiroz", "Matias", "Villani", "Mattias", "Kohn", "Robert"],
    "venue": "Riksbank Research Paper Series,",
    "year": 2015
  }, {
    "title": "Monte Carlo statistical methods",
    "authors": ["Robert", "Christian", "Casella", "George"],
    "venue": "Springer Science & Business Media,",
    "year": 2013
  }, {
    "title": "A stochastic gradient method with an exponential convergence rate for finite training sets",
    "authors": ["Roux", "Nicolas L", "Schmidt", "Mark", "Bach", "Francis R"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Lifting–A Nonreversible MCMC Algorithm",
    "authors": ["Vucelja", "Marija"],
    "year": 2014
  }, {
    "title": "Bayesian learning via stochastic gradient Langevin dynamics",
    "authors": ["Welling", "Max", "Teh", "Yee W"],
    "venue": "In ICML, pp",
    "year": 2011
  }],
  "id": "SP:7a5eae558395d9daea6a7d33b1ef20c2fb212070",
  "authors": [{
    "name": "Ari Pakman",
    "affiliations": []
  }, {
    "name": "Dar Gilboa",
    "affiliations": []
  }, {
    "name": "David Carlson",
    "affiliations": []
  }, {
    "name": "Liam Paninski",
    "affiliations": []
  }],
  "abstractText": "We introduce a stochastic version of the nonreversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples which outperform previous approaches.",
  "title": "Stochastic Bouncy Particle Sampler"
}