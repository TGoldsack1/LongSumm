{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Over the past few years, the media have paid considerable attention to machine learning systems and their ability to inadvertently discriminate against minorities, historically disadvantaged populations, and other protected groups when allocating resources (e.g., loans) or opportunities (e.g., jobs). In response to this scrutiny—and driven by ongoing debates and collaborations with lawyers, policy-makers, social scientists, and others (e.g., Barocas & Selbst, 2016)—machine learning researchers have begun to turn their attention to the topic of “fairness in machine learning,” and, in particular, to the design of fair classification and regression algorithms.\nIn this paper we study the task of binary classification subject to fairness constraints with respect to a pre-defined protected attribute, such as race or sex. Previous work in this area can be divided into two broad groups of approaches.\nThe first group of approaches incorporate specific quantitative definitions of fairness into existing machine learning\n1Microsoft Research, New York 2Yahoo! Research, New York. Correspondence to: A. Agarwal <alekha@microsoft.com>, A. Beygelzimer <beygel@gmail.com>, M. Dudı́k <mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>, H. Wallach <wallach@microsoft.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmethods, often by relaxing the desired definitions of fairness, and only enforcing weaker constraints, such as lack of correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017; Johnson et al., 2016; Kamishima et al., 2011; Donini et al., 2018). The resulting fairness guarantees typically only hold under strong distributional assumptions, and the approaches are tied to specific families of classifiers, such as SVMs.\nThe second group of approaches eliminate the restriction to specific classifier families and treat the underlying classification method as a “black box,” while implementing a wrapper that either works by pre-processing the data or post-processing the classifier’s predictions (e.g., Kamiran & Calders, 2012; Feldman et al., 2015; Hardt et al., 2016; Calmon et al., 2017). Existing pre-processing approaches are specific to particular definitions of fairness and typically seek to come up with a single transformed data set that will work across all learning algorithms, which, in practice, leads to classifiers that still exhibit substantial unfairness (see our evaluation in Section 4). In contrast, post-processing allows a wider range of fairness definitions and results in provable fairness guarantees. However, it is not guaranteed to find the most accurate fair classifier, and requires test-time access to the protected attribute, which might not be available.\nWe present a general-purpose approach that has the key advantage of this second group of approaches—i.e., the underlying classification method is treated as a black box—but without the noted disadvantages. Our approach encompasses a wide range of fairness definitions, is guaranteed to yield the most accurate fair classifier, and does not require test-time access to the protected attribute. Specifically, our approach allows any definition of fairness that can be formalized via linear inequalities on conditional moments, such as demographic parity or equalized odds (see Section 2.1). We show how binary classification subject to these constraints can be reduced to a sequence of cost-sensitive classification problems. We require only black-box access to a cost-sensitive classification algorithm, which does not need to have any knowledge of the desired definition of fairness or protected attribute. We show that the solutions to our sequence of cost-sensitive classification problems yield a randomized classifier with the lowest (empirical) error subject to the desired fairness constraints.\nCorbett-Davies et al. (2017) and Menon & Williamson\n(2018) begin with a similar goal to ours, but they analyze the Bayes optimal classifier under fairness constraints in the limit of infinite data. In contrast, our focus is algorithmic, our approach applies to any classifier family, and we obtain finite-sample guarantees. Dwork et al. (2018) also begin with a similar goal to ours. Their approach partitions the training examples into subsets according to protected attribute values and then leverages transfer learning to jointly learn from these separate data sets. Our approach avoids partitioning the data and assumes access only to a classification algorithm rather than a transfer learning algorithm.\nA preliminary version of this paper appeared at the FAT/ML workshop (Agarwal et al., 2017), and led to extensions with more general optimization objectives (Alabi et al., 2018) and combinatorial protected attributes (Kearns et al., 2018).\nIn the next section, we formalize our problem. While we focus on two well-known quantitative definitions of fairness, our approach also encompasses many other previously studied definitions of fairness as special cases. In Section 3, we describe our reductions approach to fair classification and its guarantees in detail. The experimental study in Section 4 shows that our reductions compare favorably to three baselines, while overcoming some of their disadvantages and also offering the flexibility of picking a suitable accuracy– fairness tradeoff. Our results demonstrate the utility of having a general-purpose approach for combining machine learning methods and quantitative fairness definitions."
  }, {
    "heading": "2. Problem Formulation",
    "text": "We consider a binary classification setting where the training examples consist of triples (X,A, Y ), whereX ∈ X is a feature vector, A ∈ A is a protected attribute, and Y ∈ {0, 1} is a label. The feature vector X can either contain the protected attribute A as one of the features or contain other features that are arbitrarily indicative of A. For example, if the classification task is to predict whether or not someone will default on a loan, each training example might correspond to a person, whereX represents their demographics, income level, past payment history, and loan amount; A represents their race; and Y represents whether or not they defaulted on that loan. Note that X might contain their race as one of the features or, for example, contain their zipcode—a feature that is often correlated with race. Our goal is to learn an accurate classifier h : X→ {0, 1} from some set (i.e., family) of classifiers H, such as linear threshold rules, decision trees, or neural nets, while satisfying some definition of fairness. Note that the classifiers in H do not explicitly depend on A."
  }, {
    "heading": "2.1. Fairness Definitions",
    "text": "We focus on two well-known quantitative definitions of fairness that have been considered in previous work on\nfair classification; however, our approach also encompasses many other previously studied definitions of fairness as special cases, as we explain at the end of this section.\nThe first definition—demographic (or statistical) parity— can be thought of as a stronger version of the US Equal Employment Opportunity Commission’s “four-fifths rule,” which requires that the “selection rate for any race, sex, or ethnic group [must be at least] four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate.”1\nDefinition 1 (Demographic parity—DP). A classifier h satisfies demographic parity under a distribution over (X,A, Y ) if its prediction h(X) is statistically independent of the protected attribute A—that is, if P[h(X) = ŷ | A = a] = P[h(X) = ŷ] for all a, ŷ. Because ŷ ∈ {0, 1}, this is equivalent to E[h(X) |A = a] = E[h(X)] for all a.\nThe second definition—equalized odds—was recently proposed by Hardt et al. (2016) to remedy two previously noted flaws with demographic parity (Dwork et al., 2012). First, demographic parity permits a classifier which accurately classifies data points with one value A = a, such as the value a with the most data, but makes random predictions for data points with A 6= a as long as the probabilities of h(X) = 1 match. Second, demographic parity rules out perfect classifiers whenever Y is correlated with A. In contrast, equalized odds suffers from neither of these flaws.\nDefinition 2 (Equalized odds—EO). A classifier h satisfies equalized odds under a distribution over (X,A, Y ) if its prediction h(X) is conditionally independent of the protected attribute A given the label Y—that is, if P[h(X) = ŷ | A = a, Y = y] = P[h(X) = ŷ | Y = y] for all a, y, and ŷ. Because ŷ ∈ {0, 1}, this is equivalent to E[h(X) |A = a, Y = y] = E[h(X) | Y = y] for all a, y.\nWe now show how each definition can be viewed as a special case of a general set of linear constraints of the form\nMµ(h) ≤ c, (1)\nwhere matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe the linear constraints, each indexed by k ∈ K, and µ(h) ∈ R|J| is a vector of conditional moments of the form\nµj(h) = E [ gj(X,A, Y, h(X)) ∣∣ Ej ] for j ∈ J, where gj : X×A× {0, 1} × {0, 1} → [0, 1] and Ej is an event defined with respect to (X,A, Y ). Crucially, gj depends on h, while Ej cannot depend on h in any way.\nExample 1 (DP). In a binary classification setting, demographic parity can be expressed as a set of |A| equality constraints, each of the form E[h(X) |A = a] = E[h(X)]. Letting J = A ∪ {?}, gj(X,A, Y, h(X)) = h(X) for all j,\n1See the Uniform Guidelines on Employment Selection Procedures, 29 C.F.R. §1607.4(D) (2015).\nEa = {A = a}, and E? = {True}, where {True} refers to the event encompassing all points in the sample space, each equality constraint can be expressed as µa(h) = µ?(h).2 Finally, because each such constraint can be equivalently expressed as a pair of inequality constraints of the form\nµa(h)− µ?(h) ≤ 0 −µa(h) + µ?(h) ≤ 0,\ndemographic parity can be expressed as equation (1), where K = A×{+,−},M(a,+),a′ = 1{a′ = a},M(a,+),? = −1, M(a,−),a′ = −1{a′ = a}, M(a,−),? = 1, and c = 0. Expressing each equality constraint as a pair of inequality constraints allows us to control the extent to which each constraint is enforced by positing ck > 0 for some (or all) k.\nExample 2 (EO). In a binary classification setting, equalized odds can be expressed as a set of 2 |A| equality constraints, each of the form E[h(X) | A = a, Y = y] = E[h(X) | Y = y]. Letting J = (A ∪ {?}) × {0, 1}, gj(X,A, Y, h(X)) = h(X) for all j, E(a,y) = {A = a, Y = y}, and E(?,y) = {Y = y}, each equality constraint can be equivalently expressed as\nµ(a,y)(h)− µ(?,y)(h) ≤ 0 −µ(a,y)(h) + µ(?,y)(h) ≤ 0.\nAs a result, equalized odds can be expressed as equation (1), where K = A× Y× {+,−}, M(a,y,+),(a′,y′) = 1{a′= a, y′= y}, M(a,y,+),(?,y′) = −1, M(a,y,−),(a′,y′) = −1{a′= a, y′= y}, M(a,y,−),(?,y′) = 1, and c = 0. Again, we can posit ck > 0 for some (or all) k to allow small violations of some (or all) of the constraints.\nAlthough we omit the details, we note that many other previously studied definitions of fairness can also be expressed as equation (1). For example, equality of opportunity (Hardt et al., 2016) (also known as balance for the positive class; Kleinberg et al., 2017), balance for the negative class (Kleinberg et al., 2017), error-rate balance (Chouldechova, 2017), overall accuracy equality (Berk et al., 2017), and treatment equality (Berk et al., 2017) can all be expressed as equation (1); in contrast, calibration (Kleinberg et al., 2017) and predictive parity (Chouldechova, 2017) cannot because to do so would require the event Ej to depend on h. We note that our approach can also be used to satisfy multiple definitions of fairness, though if these definitions are mutually contradictory, e.g., as described by Kleinberg et al. (2017), then our guarantees become vacuous."
  }, {
    "heading": "2.2. Fair Classification",
    "text": "In a standard (binary) classification setting, the goal is to learn the classifier h ∈ H with the minimum classification\n2Note that µ?(h) = E[h(X) | True] = E[h(X)].\nerror: err(h) := P[h(X) 6= Y ]. However, because our goal is to learn the most accurate classifier while satisfying fairness constraints, as formalized above, we instead seek to find the solution to the constrained optimization problem3\nmin h∈H\nerr(h) subject to Mµ(h) ≤ c. (2)\nFurthermore, rather than just considering classifiers in the set H, we can enlarge the space of possible classifiers by considering randomized classifiers that can be obtained via a distribution over H. By considering randomized classifiers, we can achieve better accuracy–fairness tradeoffs than would otherwise be possible. A randomized classifier Q makes a prediction by first sampling a classifier h ∈ H from Q and then using h to make the prediction. The resulting classification error is err(Q) = ∑ h∈HQ(h) err(h) and\nthe conditional moments are µ(Q) = ∑ h∈HQ(h)µ(h) (see Appendix A for the derivation). Thus we seek to solve\nmin Q∈∆\nerr(Q) subject to Mµ(Q) ≤ c, (3)\nwhere ∆ is the set of all distributions over H.\nIn practice, we do not know the true distribution over (X,A, Y ) and only have access to a data set of training examples {(Xi, Ai, Yi)}ni=1. We therefore replace err(Q) and µ(Q) in equation (3) with their empirical versions êrr(Q) and µ̂(Q). Because of the sampling error in µ̂(Q), we also allow errors in satisfying the constraints by setting ĉk = ck + εk for all k, where εk ≥ 0. After these modifications, we need to solve the empirical version of equation (3):\nmin Q∈∆\nêrr(Q) subject to Mµ̂(Q) ≤ ĉ. (4)"
  }, {
    "heading": "3. Reductions Approach",
    "text": "We now show how the problem (4) can be reduced to a sequence of cost-sensitive classification problems. We further show that the solutions to our sequence of cost-sensitive classification problems yield a randomized classifier with the lowest (empirical) error subject to the desired constraints."
  }, {
    "heading": "3.1. Cost-sensitive Classification",
    "text": "We assume access to a cost-sensitive classification algorithm for the set H. The input to such an algorithm is a data set of training examples {(Xi, C0i , C1i )}ni=1, where C0i and C1i denote the losses—costs in this setting—for predicting the labels 0 or 1, respectively, for Xi. The algorithm outputs\narg min h∈H n∑ i=1 h(Xi)C 1 i + (1− h(Xi))C0i . (5)\n3We consider misclassification error for concreteness, but all the results in this paper apply to any error of the form err(h) = E[gerr(X,A, Y, h(X))], where gerr(·, ·, ·, ·) ∈ [0, 1].\nThis abstraction allows us to specify different costs for different training examples, which is essential for incorporating fairness constraints. Moreover, efficient cost-sensitive classification algorithms are readily available for several common classifier representations (e.g., Beygelzimer et al., 2005; Langford & Beygelzimer, 2005; Fan et al., 1999). In particular, equation (5) is equivalent to a weighted classification problem, where the input consists of labeled examples {(Xi, Yi,Wi)}ni=1 with Yi ∈ {0, 1} and Wi ≥ 0, and the goal is to minimize the weighted classification error ∑n i=1Wi 1{h(Xi) 6= Yi}. This is equivalent to equation (5) if we set Wi = |C0i − C1i | and Yi = 1{C0i ≥ C1i }."
  }, {
    "heading": "3.2. Reduction",
    "text": "To derive our fair classification algorithm, we rewrite equation (4) as a saddle point problem. We begin by introducing a Lagrange multiplier λk ≥ 0 for each of the |K| constraints, summarized as λ ∈ R|K|+ , and form the Lagrangian\nL(Q,λ) = êrr(Q) + λ> ( Mµ̂(Q)− ĉ ) .\nThus, equation (4) is equivalent to\nmin Q∈∆ max λ∈R|K|+ L(Q,λ). (6)\nFor computational and statistical reasons, we impose an additional constraint on the `1 norm of λ and seek to simultaneously find the solution to the constrained version of (6) as well as its dual, obtained by switching min and max:\nmin Q∈∆ max λ∈R|K|+ , ‖λ‖1≤B L(Q,λ), (P)\nmax λ∈R|K|+ , ‖λ‖1≤B min Q∈∆ L(Q,λ). (D)\nBecause L is linear in Q and λ and the domains of Q and λ are convex and compact, both problems have solutions (which we denote by Q† and λ†) and the minimum value of (P) and the maximum value of (D) are equal and coincide with L(Q†,λ†). Thus, (Q†,λ†) is the saddle point of L (Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).\nWe find the saddle point by using the standard scheme of Freund & Schapire (1996), developed for the equivalent problem of solving for an equilibrium in a zero-sum game. From game-theoretic perspective, the saddle point can be viewed as an equilibrium of a game between two players: the Q-player choosing Q and the λ-player choosing λ. The LagrangianL(Q,λ) specifies how much theQ-player has to pay to the λ-player after they make their choices. At the saddle point, neither player wants to deviate from their choice.\nOur algorithm finds an approximate equilibrium in which neither player can gain more than ν by changing their choice\nAlgorithm 1 Exp. gradient reduction for fair classification Input: training examples {(Xi, Yi, Ai)}ni=1\nfairness constraints specified by gj , Ej , M, ĉ bound B, accuracy ν, learning rate η\nSet θ1 = 0 ∈ R|K| for t = 1, 2, . . . do\nSet λt,k = B exp{θk} 1+ ∑ k′∈K exp{θk′} for all k ∈ K ht ← BESTh(λt) Q̂t ← 1t ∑t t′=1 ht′ , L← L ( Q̂t,BESTλ(Q̂t)\n) λ̂t ← 1t ∑t t′=1 λt′ , L← L ( BESTh(λ̂t), λ̂t\n) νt ← max { L(Q̂t, λ̂t)− L, L− L(Q̂t, λ̂t)\n} if νt ≤ ν then\nReturn (Q̂t, λ̂t) end if Set θt+1 = θt + η (Mµ̂(ht)− ĉ)\nend for\n(where ν > 0 is an input to the algorithm). Such an approximate equilibrium corresponds to a ν-approximate saddle point of the Lagrangian, which is a pair (Q̂, λ̂), where\nL(Q̂, λ̂) ≤ L(Q, λ̂) + ν for all Q ∈ ∆,\nL(Q̂, λ̂) ≥ L(Q̂,λ)− ν for all λ ∈ R|K|+ , ‖λ‖1 ≤ B.\nWe proceed iteratively by running a no-regret algorithm for the λ-player, while executing the best response of the Qplayer. Following Freund & Schapire (1996), the average play of both players converges to the saddle point. We run the exponentiated gradient algorithm (Kivinen & Warmuth, 1997) for the λ-player and terminate as soon as the suboptimality of the average play falls below the pre-specified accuracy ν. The best response of the Q-player can always be chosen to put all of the mass on one of the candidate classifiers h ∈ H, and can be implemented by a single call to a cost-sensitive classification algorithm for the set H.\nAlgorithm 1 fully implements this scheme, except for the functions BESTλ and BESTh, which correspond to the bestresponse algorithms of the two players. (We need the best response of the λ-player to evaluate whether the suboptimality of the current average play has fallen below ν.) The two best response functions can be calculated as follows.\nBESTλ(Q): the best response of the λ-player. The best response of the λ-player for a given Q is any maximizer of L(Q,λ) over all valid λs. In our setting, it can always be chosen to be either 0 or put all of the mass on the most violated constraint. Letting γ̂(Q) := Mµ̂(Q) and letting ek denote the kth vector of the standard basis, BESTλ(Q) returns{\n0 if γ̂(Q) ≤ ĉ, Bek∗ otherwise, where k∗ = arg maxk[γ̂k(Q)− ĉk].\nBESTh(λ): the best response of theQ-player. Here, the best response minimizes L(Q,λ) over all Qs in the simplex. Because L is linear in Q, the minimizer can always be chosen to put all of the mass on a single classifier h. We show how to obtain the classifier constituting the best response via a reduction to cost-sensitive classification. Letting pj := P̂[Ej ] be the empirical event probabilities, the Lagrangian for Q which puts all of the mass on a single h is then\nL(h,λ) = êrr(h) + λ> ( Mµ̂(h)− ĉ ) = Ê [ 1{h(X) 6= Y } ] − λ>ĉ +\n∑ k,j Mk,jλkµ̂j(h)\n= −λ>ĉ + Ê [ 1{h(X) 6= Y } ] + ∑ k,j Mk,jλk pj Ê [ gj ( X,A,Y,h(X) ) 1{(X,A,Y ) ∈ Ej} ] .\nAssuming a data set of training examples {(Xi, Ai, Yi)}ni=1, the minimization of L(h,λ) over h then corresponds to costsensitive classification on {(Xi, C0i , C1i )}ni=1 with costs4\nC0i = 1{Yi 6= 0} + ∑ k,j Mk,jλk pj gj(Xi,Ai,Yi, 0)1{(Xi,Ai,Yi) ∈ Ej}\nC1i = 1{Yi 6= 1} + ∑ k,j Mk,jλk pj gj(Xi,Ai,Yi, 1)1{(Xi,Ai,Yi) ∈ Ej}.\nTheorem 1. Letting ρ := maxh‖Mµ̂(h) − ĉ‖∞, Algorithm 1 satisfies the inequality\nνt ≤ B log(|K|+ 1)\nηt + ηρ2B.\nThus, for η = ν2ρ2B , Algorithm 1 will return a ν-approximate saddle point of L in at most 4ρ 2B2 log(|K|+1)\nν2 iterations.\nThis theorem, proved in Appendix B, bounds the suboptimality νt of the average play (Q̂t, λ̂t), which is equal to its suboptimality as a saddle point. The right-hand side of the bound is optimized by η = √ log(|K|+ 1) / (ρ √ t), lead-\ning to the bound νt ≤ 2ρB √\nlog(|K|+ 1) / t. This bound decreases with the number of iterations t and grows very slowly with the number of constraints |K|. The quantity ρ is a problem-specific constant that bounds how much any single classifier h ∈ H can violate the desired set of fairness constraints. Finally, B is the bound on the `1-norm of λ, which we introduced to enable this specific algorithmic scheme. In general, larger values of B will bring the problem (P) closer to (6), and thus also to (4), but at the cost of\n4For general error, err(h) = E[gerr(X,A, Y, h(X))], the costs C0i and C 1 i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and gerr(Xi, Ai, Yi, 1) instead of 1{Yi 6= 0} and 1{Yi 6= 1}.\nneeding more iterations to reach any given suboptimality. In particular, as we derive in the theorem, achieving suboptimality ν may need up to 4ρ2B2 log(|K|+ 1) / ν2 iterations. Example 3 (DP). Using the matrix M for demographic parity as described in Section 2, the cost-sensitive reduction for a vector of Lagrange multipliers λ uses costs\nC0i = 1{Yi 6= 0}, C1i = 1{Yi 6= 1}+ λAi pAi − ∑ a∈A λa,\nwhere pa := P̂[A = a] and λa := λ(a,+) − λ(a,−), effectively replacing two non-negative Lagrange multipliers by a single multiplier, which can be either positive or negative. Because ck = 0 for all k, ĉk = εk. Furthermore, because all empirical moments are bounded in [0, 1], we can assume εk ≤ 1, which yields the bound ρ ≤ 2. Thus, Algorithm 1 terminates in at most 16B2 log(2 |A|+ 1) / ν2 iterations. Example 4 (EO). For equalized odds, the cost-sensitive reduction for a vector of Lagrange multipliers λ uses costs\nC0i = 1{Yi 6= 0},\nC1i = 1{Yi 6= 1}+ λ(Ai,Yi) p(Ai,Yi) − ∑ a∈A λ(a,Yi) p(?,Yi) ,\nwhere p(a,y) := P̂[A = a, Y = y], p(?,y) := P̂[Y = y], and λ(a,y) := λ(a,y,+) − λ(a,y,−). If we again assume εk ≤ 1, then we obtain the bound ρ ≤ 2. Thus, Algorithm 1 terminates in at most 16B2 log(4 |A|+ 1) / ν2 iterations."
  }, {
    "heading": "3.3. Error Analysis",
    "text": "Our ultimate goal, as formalized in equation (3), is to minimize the classification error while satisfying fairness constraints under a true but unknown distribution over (X,A, Y ). In the process of deriving Algorithm 1, we introduced three different sources of error. First, we replaced the true classification error and true moments with their empirical versions. Second, we introduced a bound B on the magnitude of λ. Finally, we only run the optimization algorithm for a fixed number of iterations, until it reaches suboptimality level ν. The first source of error, due to the use of empirical rather than true quantities, is unavoidable and constitutes the underlying statistical error. The other two sources of error, the bound B and the suboptimality level ν, stem from the optimization algorithm and can be driven arbitrarily small at the cost of additional iterations. In this section, we show how the statistical error and the optimization error affect the true accuracy and the fairness of the randomized classifier returned by Algorithm 1—in other words, how well Algorithm 1 solves our original problem (3).\nTo bound the statistical error, we use the Rademacher complexity of the classifier family H, which we denote by Rn(H), where n is the number of training examples. We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and\nα ≤ 1/2. We note that α = 1/2 in the vast majority of classifier families, including norm-bounded linear functions (see Theorem 1 of Kakade et al., 2009), neural networks (see Theorem 18 of Bartlett & Mendelson, 2002), and classifier families with bounded VC dimension (see Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).\nRecall that in our empirical optimization problem we assume that ĉk = ck + εk, where εk ≥ 0 are error bounds that account for the discrepancy between µ(Q) and µ̂(Q). In our analysis, we assume that these error bounds have been set in accordance with the Rademacher complexity of H.\nAssumption 1. There exists C,C ′ ≥ 0 and α ≤ 1/2 such that Rn(H) ≤ Cn−α and εk = C ′ ∑ j∈J|Mk,j |n −α j , where nj is the number of data points that fall in Ej ,\nnj := ∣∣{i : (Xi, Ai, Yi) ∈ Ej}∣∣.\nThe optimization error can be bounded via a careful analysis of the Lagrangian and the optimality conditions of (P) and (D). Combining the three different sources of error yields the following bound, which we prove in Appendix C.\nTheorem 2. Let Assumption 1 hold for C ′ ≥ 2C + 2 + √ ln(4/δ) / 2, where δ > 0. Let (Q̂, λ̂) be any νapproximate saddle point of L, let Q? minimize err(Q) subject to Mµ(Q) ≤ c, and let p?j = P[Ej ]. Then, with probability at least 1− (|J|+ 1)δ, the distribution Q̂ satisfies\nerr(Q̂) ≤ err(Q?) + 2ν + Õ(n−α),\nγk(Q̂) ≤ ck + 1+2ν B + ∑ j∈J |Mk,j | Õ(n−αj ) for all k,\nwhere Õ(·) suppresses polynomial dependence on ln(1/δ). If np?j ≥ 8 log(2/δ) for all j, then, for all k,\nγk(Q̂) ≤ ck + 1+2ν B + ∑ j∈J |Mk,j | Õ ( (np?j ) −α ) .\nIn other words, the solution returned by Algorithm 1 achieves the lowest feasible classification error on the true distribution up to the optimization error, which grows linearly with ν, and the statistical error, which grows as n−α. Therefore, if we want to guarantee that the optimization error does not dominate the statistical error, we should set ν ∝ n−α. The fairness constraints on the true distribution are satisfied up to the optimization error (1 + 2ν) /B and up to the statistical error. Because the statistical error depends on the moments, and the error in estimating the moments grows as n−αj ≥ n−α, we can setB ∝ nα to guarantee that the optimization error does not dominate the statistical error. Combining this reasoning with the learning rate setting of Theorem 1 yields the following theorem (proved in Appendix C).\nTheorem 3. Let ρ := maxh‖Mµ̂(h)− ĉ‖∞. Let Assumption 1 hold for C ′ ≥ 2C + 2 + √ ln(4/δ) / 2, where δ > 0.\nLet Q? minimize err(Q) subject to Mµ(Q) ≤ c. Then Algorithm 1 with ν ∝ n−α, B ∝ nα and η ∝ ρ−2n−2α terminates in O(ρ2n4α ln |K|) iterations and returns Q̂, which with probability at least 1− (|J|+ 1)δ satisfies\nerr(Q̂) ≤ err(Q?) + Õ(n−α), γk(Q̂) ≤ ck + ∑ j∈J |Mk,j | Õ(n−αj ) for all k.\nExample 5 (DP). If na denotes the number of training examples with Ai = a, then Assumption 1 states that we should set ε(a,+) = ε(a,−) = C ′(n−αa + n\n−α) and Theorem 3 then shows that for a suitable setting of C ′, ν, B, and η, Algorithm 1 will return a randomized classifier Q̂ with the lowest feasible classification error up to Õ(n−α) while also approximately satisfying the fairness constraints∣∣∣E[h(X) |A = a]− E[h(X)]∣∣∣ ≤ Õ(n−αa ) for all a, where E is with respect to (X,A, Y ) as well as h ∼ Q̂. Example 6 (EO). Similarly, if n(a,y) denotes the number of examples with Ai = a and Yi = y and n(?,y) denotes the number of examples with Yi = y, then Assumption 1 states that we should set ε(a,y,+) = ε(a,y,−) = C ′(n −α (a,y) +n −α (?,y)) and Theorem 3 then shows that for a suitable setting ofC ′, ν, B, and η, Algorithm 1 will return a randomized classifier Q̂ with the lowest feasible classification error up to Õ(n−α) while also approximately satisfying the fairness constraints∣∣∣E[h(X) |A = a, Y = y]−E[h(X) | Y = y]∣∣∣ ≤ Õ(n−α(a,y)) for all a, y. Again, E includes randomness under the true distribution over (X,A, Y ) as well as h ∼ Q̂."
  }, {
    "heading": "3.4. Grid Search",
    "text": "In some situations, it is preferable to select a deterministic classifier, even if that means a lower accuracy or a modest violation of the fairness constraints. A set of candidate classifiers can be obtained from the saddle point (Q†,λ†). Specifically, because Q† is a minimizer of L(Q,λ†) and L is linear in Q, the distribution Q† puts non-zero mass only on classifiers that are theQ-player’s best responses to λ†. If we knew λ†, we could retrieve one such best response via the reduction to cost-sensitive learning introduced in Section 3.2.\nWe can compute λ† using Algorithm 1, but when the number of constraints is very small, as is the case for demographic parity or equalized odds with a binary protected attribute, it is also reasonable to consider a grid of values λ, calculate the best response for each value, and then select the value with the desired tradeoff between accuracy and fairness.\nExample 7 (DP). When the protected attribute is binary, e.g., A ∈ {a, a′}, then the grid search can in fact be conducted in a single dimension. The reduction formally takes\ntwo real-valued arguments λa and λa′ , and then adjusts the costs for predicting h(Xi) = 1 by the amounts\nδa = λa pa − λa − λa′ and δa′ = λa′ pa′ − λa − λa′ ,\nrespectively, on the training examples with Ai = a and Ai = a\n′. These adjustments satisfy paδa + pa′δa′ = 0, so instead of searching over λa and λa′ , we can carry out the grid search over δa alone and apply the adjustment δa′ = −paδa/pa′ to the protected attribute value a′.\nWith three attribute values, e.g., A ∈ {a, a′, a′′}, we similarly have paδa + pa′δa′ + pa′′δa′′ = 0, so it suffices to conduct grid search in two dimensions rather than three.\nExample 8 (EO). If A ∈ {a, a′}, we obtain the adjustment\nδ(a,y) = λ(a,y) p(a,y) − λ(a,y) + λ(a′,y) p(?,y)\nfor an example with protected attribute value a and label y, and similarly for protected attribute value a′. In this case, separately for each y, the adjustments satisfy\np(a,y)δ(a,y) + p(a′,y)δ(a′,y) = 0,\nso it suffices to do the grid search over δ(a,0) and δ(a,1) and set the parameters for a′ to δ(a′,y) = −p(a,y)δ(a,y)/p(a′,y)."
  }, {
    "heading": "4. Experimental Results",
    "text": "We now examine how our exponentiated-gradient reduction5 performs at the task of binary classification subject to either demographic parity or equalized odds. We provide an evaluation of our grid-search reduction in Appendix D.\nWe compared our reduction with the score-based postprocessing algorithm of Hardt et al. (2016), which takes as its input any classifier, (i.e., a standard classifier without any fairness constraints) and derives a monotone transformation of the classifier’s output to remove any disparity with respect to the training examples. This post-processing algorithm works with both demographic parity and equalized odds, as well as with binary and non-binary protected attributes.\nFor demographic parity, we also compared our reduction with the reweighting and relabeling approaches of Kamiran & Calders (2012). Reweighting can be applied to both binary and non-binary protected attributes and operates by changing importance weights on each example with the goal of removing any statistical dependence between the protected attribute and label.6 Relabeling was developed for\n5https://github.com/Microsoft/fairlearn 6Although reweighting was developed for demographic parity, the weights that it induces are achievable by our grid search, albeit the grid search for equalized odds rather than demographic parity.\nbinary protected attributes. First, a classifier is trained on the original data (without considering fairness). The training examples close to the decision boundary are then relabeled to remove all disparity while minimally affecting accuracy. The final classifier is then trained on the relabeled data.\nAs the base classifiers for our reductions, we used the weighted classification implementations of logistic regression and gradient-boosted decision trees in scikit-learn (Pedregosa et al., 2011). In addition to the three baselines described above, we also compared our reductions to the “unconstrained” classifiers trained to optimize accuracy only.\nWe used four data sets, randomly splitting each one into training examples (75%) and test examples (25%):\n• The adult income data set (Lichman, 2013) (48,842 examples). Here the task is to predict whether someone makes more than $50k per year, with gender as the protected attribute. To examine the performance for non-binary protected attributes, we also conducted another experiment with the same data, using both gender and race (binarized into white and non-white) as the protected attribute. Relabeling, which requires binary protected attributes, was therefore not applicable here. • ProPublica’s COMPAS recidivism data (7,918 examples). The task is to predict recidivism from someone’s criminal history, jail and prison time, demographics, and COMPAS risk scores, with race as the protected attribute (restricted to white and black defendants). • Law School Admissions Council’s National Longitudinal Bar Passage Study (Wightman, 1998) (20,649 examples). Here the task is to predict someone’s eventual passage of the bar exam, with race (restricted to white and black only) as the protected attribute. • The Dutch census data set (Dutch Central Bureau for\nStatistics, 2001) (60,420 examples). Here the task is to predict whether or not someone has a prestigious occupation, with gender as the protected attribute.\nWhile all the evaluated algorithms require access to the protected attribute A at training time, only the post-processing algorithm requires access to A at test time. For a fair comparison, we included A in the feature vector X , so all algorithms had access to it at both the training time and test time.\nWe used the test examples to measure the classification error for each approach, as well as the violation of the desired fairness constraints, i.e., maxa ∣∣E[h(X) |A = a]− E[h(X)]∣∣ and maxa,y\n∣∣E[h(X) | A = a, Y = y]− E[h(X) | Y = y]∣∣ for demographic parity and equalized odds, respectively.\nWe ran our reduction across a wide range of tradeoffs between the classification error and fairness constraints. We considered ε ∈ {0.001, . . . , 0.1} and for each value ran Algorithm 1 with ĉk = ε across all k. As expected, the returned randomized classifiers tracked the training Pareto\nfrontier (see Figure 2 in Appendix D). In Figure 1, we evaluate these classifiers alongside the baselines on the test data.\nFor all the data sets, the range of classification errors is much smaller than the range of constraint violations. Almost all the approaches were able to substantially reduce or remove disparity without much impact on classifier accuracy. One exception was the Dutch census data set, where the classification error increased the most in relative terms.\nOur reduction generally dominated or matched the baselines. The relabeling approach frequently yielded solutions that were not Pareto optimal. Reweighting yielded solutions on the Pareto frontier, but often with substantial disparity. As expected, post-processing yielded disparities that were statistically indistinguishable from zero, but the resulting classification error was sometimes higher than achieved by our reduction under a statistically indistinguishable disparity. In addition, and unlike the post-processing algorithm, our reduction can achieve any desired accuracy–fairness tradeoff, allows a wider range of fairness definitions, and does not require access to the protected attribute at test time.\nOur grid-search reduction, evaluated in Appendix D, sometimes failed to achieve the lowest disparities on\nthe training data, but its performance on the test data very closely matched that of our exponentiated-gradient reduction. However, if the protected attribute is non-binary, then grid search is not feasible. For instance, for the version of the adult income data set where the protected attribute takes on four values, the grid search would need to span three dimensions for demographic parity and six dimensions for equalized odds, both of which are prohibitively costly."
  }, {
    "heading": "5. Conclusion",
    "text": "We presented two reductions for achieving fairness in a binary classification setting. Our reductions work for any classifier representation, encompass many definitions of fairness, satisfy provable guarantees, and work well in practice.\nOur reductions optimize the tradeoff between accuracy and any (single) definition of fairness given training-time access to protected attributes. Achieving fairness when trainingtime access to protected attributes is unavailable remains an open problem for future research, as does the navigation of tradeoffs between accuracy and multiple fairness definitions."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank Aaron Roth, Sam Corbett-Davies, and Emma Pierson for helpful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "A reductions approach to fair classication",
    "authors": ["A. Agarwal", "A. Beygelzimer", "M. Dudı́k", "J. Langford"],
    "year": 2017
  }, {
    "title": "Unleashing linear optimizers for group-fair learning and optimization",
    "authors": ["D. Alabi", "N. Immorlica", "A.T. Kalai"],
    "venue": "In Proceedings of the 31st Annual Conference on Learning Theory (COLT),",
    "year": 2018
  }, {
    "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
    "authors": ["P.L. Bartlett", "S. Mendelson"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "Fairness in criminal justice risk assessments: The state of the art",
    "authors": ["R. Berk", "H. Heidari", "S. Jabbari", "M. Kearns", "A. Roth"],
    "year": 2017
  }, {
    "title": "Error limiting reductions between classification tasks",
    "authors": ["A. Beygelzimer", "V. Dani", "T.P. Hayes", "J. Langford", "B. Zadrozny"],
    "venue": "In Proceedings of the Twenty-Second International Conference on Machine Learning (ICML), pp",
    "year": 2005
  }, {
    "title": "Theory of classification: a survey of some recent advances",
    "authors": ["S. Boucheron", "O. Bousquet", "G. Lugosi"],
    "venue": "ESAIM: Probability and Statistics,",
    "year": 2005
  }, {
    "title": "Optimized pre-processing for discrimination prevention",
    "authors": ["F. Calmon", "D. Wei", "B. Vinzamuri", "K.N. Ramamurthy", "K.R. Varshney"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
    "authors": ["A. Chouldechova"],
    "venue": "Big Data, Special Issue on Social and Technical Trade-Offs,",
    "year": 2017
  }, {
    "title": "Algorithmic decision making and the cost of fairness",
    "authors": ["S. Corbett-Davies", "E. Pierson", "A. Feller", "S. Goel", "A. Huq"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2017
  }, {
    "title": "Empirical risk minimization under fairness constraints. 2018",
    "authors": ["M. Donini", "L. Oneto", "S. Ben-David", "J. Shawe-Taylor", "M. Pontil"],
    "year": 2018
  }, {
    "title": "Fairness through awareness",
    "authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"],
    "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,",
    "year": 2012
  }, {
    "title": "Decoupled classifiers for group-fair and efficient machine learning",
    "authors": ["C. Dwork", "N. Immorlica", "A.T. Kalai", "M. Leiserson"],
    "venue": "In Conference on Fairness, Accountability and Transparency (FAT ),",
    "year": 2018
  }, {
    "title": "Adacost: Misclassification cost-sensitive boosting",
    "authors": ["W. Fan", "S.J. Stolfo", "J. Zhang", "P.K. Chan"],
    "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning (ICML),",
    "year": 1999
  }, {
    "title": "Certifying and removing disparate impact",
    "authors": ["M. Feldman", "S.A. Friedler", "J. Moeller", "C. Scheidegger", "S. Venkatasubramanian"],
    "venue": "In Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2015
  }, {
    "title": "Game theory, on-line prediction and boosting",
    "authors": ["Y. Freund", "R.E. Schapire"],
    "venue": "In Proceedings of the Ninth Annual Conference on Computational Learning Theory (COLT),",
    "year": 1996
  }, {
    "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
    "authors": ["Y. Freund", "R.E. Schapire"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 1997
  }, {
    "title": "Equality of opportunity in supervised learning",
    "authors": ["M. Hardt", "E. Price", "N. Srebro"],
    "venue": "In Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Impartial predictive modeling: Ensuring fairness in arbitrary models",
    "authors": ["K.D. Johnson", "D.P. Foster", "R.A. Stine"],
    "year": 2016
  }, {
    "title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
    "authors": ["S.M. Kakade", "K. Sridharan", "A. Tewari"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Data preprocessing techniques for classification without discrimination",
    "authors": ["F. Kamiran", "T. Calders"],
    "venue": "Knowledge and Information Systems,",
    "year": 2012
  }, {
    "title": "Fairness-aware learning through regularization approach",
    "authors": ["T. Kamishima", "S. Akaho", "J. Sakuma"],
    "venue": "IEEE 11th International Conference on Data Mining Workshops,",
    "year": 2011
  }, {
    "title": "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness",
    "authors": ["M. Kearns", "S. Neel", "A. Roth", "Z.S. Wu"],
    "venue": "In Proceedings of the 35th International Conference on Machine Learning (ICML),",
    "year": 2018
  }, {
    "title": "Exponentiated gradient versus gradient descent for linear predictors",
    "authors": ["J. Kivinen", "M.K. Warmuth"],
    "venue": "Information and Computation,",
    "year": 1997
  }, {
    "title": "Inherent trade-offs in the fair determination of risk scores",
    "authors": ["J. Kleinberg", "S. Mullainathan", "M. Raghavan"],
    "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science Conference,",
    "year": 2017
  }, {
    "title": "Sensitive error correcting output codes",
    "authors": ["J. Langford", "A. Beygelzimer"],
    "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),",
    "year": 2005
  }, {
    "title": "Probability in Banach Spaces: Isoperimetry and Processes",
    "authors": ["M. Ledoux", "M. Talagrand"],
    "year": 1991
  }, {
    "title": "The cost of fairness in binary classification",
    "authors": ["A.K. Menon", "R.C. Williamson"],
    "venue": "In Proceedings of the Conference on Fiarness, Accountability, and Transparency,",
    "year": 2018
  }, {
    "title": "Convex analysis",
    "authors": ["R.T. Rockafellar"],
    "year": 1970
  }, {
    "title": "Online learning and online convex optimization",
    "authors": ["S. Shalev-Shwartz"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Learning non-discriminatory predictors",
    "authors": ["B.E. Woodworth", "S. Gunasekar", "M.I. Ohannessian", "N. Srebro"],
    "venue": "In Proceedings of the 30th Conference on Learning Theory (COLT),",
    "year": 2017
  }, {
    "title": "Fairness constraints: Mechanisms for fair classification",
    "authors": ["M.B. Zafar", "I. Valera", "M.G. Rodriguez", "K.P. Gummadi"],
    "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2017
  }],
  "id": "SP:5a8f99c24bce7241a23c0941ab4be7e7b8faf49d",
  "authors": [{
    "name": "Alekh Agarwal",
    "affiliations": []
  }, {
    "name": "Alina Beygelzimer",
    "affiliations": []
  }, {
    "name": "Miroslav Dudı́k",
    "affiliations": []
  }, {
    "name": "John Langford",
    "affiliations": []
  }, {
    "name": "Hanna Wallach",
    "affiliations": []
  }],
  "abstractText": "We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",
  "title": "A Reductions Approach to Fair Classification"
}