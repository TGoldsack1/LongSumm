{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Networks represent the elements of a system and their interconnectedness as a set of nodes and arcs (connections) between them. Applications of network analysis range from biological systems such as gene regulatory networks and brain connectivity networks, to social networks and interactions between financial indices.\nWhen dealing with continuous observations, a commonly used framework for this purpose is linear causal models (Bollen, 1989; Pearl, 2000; Spirtes et al., 2000), in which the data-generation process is defined such that the observations from each node are a linear combination of the observations from other nodes and additive noise with – typically – a constant mean and variance. Hence, temporal variations in the observations from a node are either associated to the other nodes in the network, or to the changes in latent confounders; i.e., in the absence of any change in these two components, observations from a node are as-\n1UNSW, Sydney. 2Started work at Data61. 3Data61, the Australian National University and the University of Sydney. Correspondence to: Amir Dezfouli <akdezfuli@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nsumed to follow the noise distribution and thus unaffected by the time-varying signals that come from outside the network. This comes as a significant limitation for real-world problems where observations from a node can also follow a network-independent trend. For example, when considering property prices in the suburbs of a city, some of them can follow a decreasing/increasing trend over time, essentially independent of other suburbs.\nOur first contribution overcomes this limitation. We propose a network-structure discovery model that generalizes linear causal models by incorporating a networkindependent component for each node, which is determined by a Gaussian process (GP) prior capturing the inter-dependencies between observations over time. Consequently, the output of a node is now given by a sum of the network-independent component and a (noisy) linear combination of the observations from the other nodes. Our model considers the parameters of this linear combination (which determine the structure of the network) as random variables. This modeling approach provides a more flexible data-generation process due to the non-parametric nature of the GP prior but, of course, raises the question of what algorithms can be used to learn such more general models.\nOur second contribution provides an answer to this question, including an efficient variational inference approach for the posterior over the network-dependent parameters. A key part relies on showing that, by marginalizing the latent functions corresponding to the network-independent components, our approach is closely related to multi-task GP models under a product covariance (Bonilla et al., 2008; Rakitsch et al., 2013). This allows us to exploit properties of Kronecker products in order to compute the marginal likelihood (conditioned on the network parameters) efficiently. We estimate the posterior over the network-dependent parameters building upon recent breakthroughs in variational inference (Rezende et al., 2014; Kingma & Welling, 2014; Maddison et al., 2016).\nComputational efficiency is not the only concern of previous popular approaches: all rely on more or less stringent assumptions to make sure that parameters do not deviate from a prescribed, finite regime. In short, “There is a stability caveat to retrieve the network”. What finiteness is related\nto takes various forms: this can be the number of events for Hawkes process modeling (Linderman & Adams, 2014), the variance of the process (Shimizu et al., 2006), the iterated dynamical system parameters (Hyvärinen & Smith, 2013), the non-singularity of the mixing matrix (Shimizu et al., 2011), etc. It is legitimate to ask where our more general model and algorithm position us with respect to this caveat.\nOur third contribution is a formal proof that stability is not an issue in our case. Concrete distributions (Maddison et al., 2016) happen to be important in our setting not just for their convenience in the reparameterization trick (Kingma & Welling, 2014): they help to get stability for free. Furthermore, we investigate what we get with the assumptions used in previous work to guarantee stability (Linderman & Adams, 2014). What we get, which to our knowledge has never been documented, is that key parameters are not just stable: with high probability, they are easy to bound and meet some form of statistical robustness. The variance of the network signal, for example, is of the same order as that of the network-independent parameters, and therefore robust to changes in the network-dependent distributions. This result is highly relevant, considering the choices we make to get tractable families of distributions over network parameters for variational inference.\nExperiments. We benchmark our approach against the state of the art on three very different and challenging problems: discovering brain functional connectivity, modeling property prices in Sydney, and understanding regulation in the yeast genome. We provide a quantitative evaluation of our approach, showing that it consistently outperforms competitive baselines. Furthermore, when investigating the full yeast genome regulation, our qualitative analyses show that even in a large network (up to 38,000,000+ arcs), our technique is able to recover both high-level and low-level knowledge that is strikingly consistent with the previous literature and hints on original findings."
  }, {
    "heading": "1.1. Related Work",
    "text": "Our approach is different from standard linear causal models with Gaussian noise (e.g. Bollen, 1989; Pearl, 2000) in three key aspects: (i) we do not assume that the underlying network is a directed acyclic graph (e.g. Spirtes et al., 2000); (ii) we represent the connection strengths using random matrices; and (iii) we incorporate the network-independent Gaussian process component. Concerning aspect (i), cyclic models are particularly important for the analysis of biological data in which the underlying networks typically include reciprocal connections (e.g., connections between different brain regions) or cycles (such as gene regulatory networks). Such models have been explored in the previous works, however, they differ from the current model in aspects (ii) and (iii) mentioned above. Examples of these studies in-\nclude early work of Richardson (1996) and more recent works such as Hyttinen et al. (2012); Mooij et al. (2011) and Hyvärinen & Smith (2013). On a different vein and regarding aspect (ii) mentioned above, our use of random matrices representing network structure is similar to the model in Linderman & Adams (2014), but that model is focused on point-process data rather than continuous-valued observations.\nWith regard to aspect (iii), as observations in our model are generated from several latent Gaussian processes, our framework is related to GP latent variable models (Lawrence, 2005). However, our goal is to recover the underlying network structure, instead of carrying out dimensionality reduction or predicting observations for the nodes. Other models in this class can be used for causal inference (Zhang et al., 2010; Huang et al., 2015), which are different from our model in aspects (i) and (ii). With regard to multi-task GP models (Bonilla et al., 2008; Rakitsch et al., 2013) and more general frameworks for modeling vector-valued outputs (Wilson & Ghahramani, 2010), other approaches have considered Bayesian inference in multi-task learning subject to specific constraints, such as rank constrains (Koyejo & Ghosh, 2013). However, their work is mostly focused on dealing with the problem of high-dimensional data instead of network discovery. Finally, unlike our work, other approaches assume a non-Gaussian additive noise (Shimizu et al., 2006) or a nonlinear transformation of the networkdependent component (Hoyer et al., 2009)."
  }, {
    "heading": "2. Model Specification",
    "text": "Given a dataset D of vector-valued observations Y = {yi}Ni=1 and their corresponding times1 {ti}Ni=1 from N nodes in a network, our goal is to infer the existence and strength of the arcs between the nodes. For simplicity in the notation, we assume that each observation yi is T - dimensional and denote n = N × T as the total number of observations. Let yi(t) be the output of node i at time t,\nyi(t) = fi(t) + it, it ∼ N(0, σ2y), (1)\nwhere σ2y is the observation-noise variance. To model latent function fi, we assume that it is generated by two sources: (i) a network-independent component, denoted by zi(t), and (ii) a network-dependent component, i.e., a weighted sum of the inputs received from the rest of the network:\nfi(t) = zi(t) + N∑ j=1 j 6=i AijWij [fj(t) + ξjt] , zi(t) ∼ GP(0, κ(t, t′;θ)), ξjt ∼ N(0, σ2f ),\n(2)\n1Although we refer to time indexes throughout this paper, the applicability of our method is not constrained to time-series data or one-dimensional inputs.\nwhere Aij ∈ {0, 1} represents the existence of an arc from node j to node i and Wij ∈ R determines the weight of the connection from node j to node i (assuming Aii = Wii = 0). These are elements of the adjacency matrix A and weight matrix W, respectively, which we will refer to as network parameters. The network-independent component zi(t) is drawn from a Gaussian process (GP; Rasmussen & Williams, 2006) with covariance function κ(t, t′;θ) and hyperparameters θ."
  }, {
    "heading": "2.1. Prior over Network Parameters",
    "text": "Eq. (1) defines the likelihood of our observations and eq. (2) defines the prior over the latent functions given the network parameters A,W. We assume these parameters are also random variables and their prior is defined as:\np(A,W) = p(A)p(W) = ∏ ij p(Aij)p(Wij),\np(Aij) = Bern(ρ), p(Wij) = N(0, σ2w), (3)\nwhere Bern(ρ) denotes a Bernoulli distribution with parameter ρ. Note that defining separate priors over A and W – as above – allows us to specify separately our beliefs about the sparsity of the network connections and their strengths. Furthermore, these priors can be more effective than weaksparsity inducing priors (Mohamed et al., 2012)."
  }, {
    "heading": "3. Inference",
    "text": "Our main inference task is to estimate the posterior over the network parameters p(A,W|D). To this end, by exploiting the closeness of GPs under linear operators, we will first show in §3.1 the exact expression for the (conditional) marginal likelihood p(Y|A,W) obtained when marginalizing the latent functions. Furthermore, by establishing a relationship of our model to multi-task learning (Rakitsch et al., 2013; Bonilla et al., 2008), we show how to compute this marginal likelihood efficiently. Subsequently, due to the highly nonlinear dependence of p(Y|A,W) on A,W, we will approximate the posterior over these network parameters using variational inference in §3.2."
  }, {
    "heading": "3.1. Marginal Likelihood given Network Parameters",
    "text": "Let us denote the values of all latent functions fi(t) at time t with f(t) = [f1(t), . . . , fN (t)], and similarly z(t) = [z1(t), . . . , zN (t)]. Hence, we can rewrite eq. (2) as:\nf(t) = (I−A W)−1(z(t) + A Wξt), (4)\nwhere is the Hadamard product. We refer to the model in eq. (4) as the inverse model and its the detailed derivation is in the supplement (§II.4). We can see now that, for fixed A,W, the resulting distribution over fi is also a Gaussian\nprocess. Hence, we only need to figure out the mean function and the covariance function of the resulting process. Below we present the main results and leave the details to the supplement (§II.4).\nLet B def= A W and define the following intermediate matrices (which are a function of the network parameters):\nE = (I−B)−1BBT (I−B)−T , Kf = (I−B)−1(I−B)−T .\n(5)\nThen we have that the mean function and covariance function of the latent processes are:\nµi(t) = E [fi(t)] = 0, Cov[fi(t), fj(t′)] = [Kf ]i,jκ(t, t′;θ) + [E]i,jσ2f , (6)\nwhere [M]i,j denotes the i, j entry of matrix M. Consequently, the distribution of the noisy process yi is also a Gaussian process. Let us assume synchronized observations, i.e. that observations for all nodes lie on a grid in time, t = 1, . . . , T . Furthermore, let Y be the N × T matrix of observations and define y = vec(Y), where vec(·) takes the columns of the matrix argument and stacks them into a single vector. Therefore, the log-marginal likelihood conditioned on the network parameters is given by (⊗ is Kronecker product):\nlog p(y|A,W) = −1 2 log |Σy| − 1 2 yTΣ−1y y + C,\nwith Σy = Kf ⊗Kt + (σ2fE + σ2yI)⊗ I, (7)\nwhere C = −n/2 log(2π); Kt is the T × T covariance matrix induced by evaluating the covariance function κ(t, t′;θ) at all observed times; E and Kf are defined as in eq. 5; and, as before, n = N × T is the total number observations."
  }, {
    "heading": "3.1.1. RELATIONSHIP WITH MULTI-TASK LEARNING",
    "text": "Remarkably, the marginal likelihood of the model described in eq. (7) reveals an interesting relationship with multi-task learning when using Gaussian process priors. Indeed, it boils down to the marginal likelihood of multi-task GP models under a product covariance (Bonilla et al., 2008; Rakitsch et al., 2013). In our case, the nodes in the network can be seen as the tasks in a multi-task GP model and are associated with a task-dependent covariance Kf , which is fully determined by the parameters of the network A,W. This contrasts with multi-task models where Kf is, in general, a free parameter (Bonilla et al., 2008). Similarly, the input covariance Kt is the covariance of the observation times.\nFinally, conditioned on A,W, our model’s marginal likelihood exhibits a more complex noise covariance σ2fE+ σ 2 yI, which depends strongly on the network parameters. Such a covariance structure was not studied by Bonilla et al. (2008),\nas they considered only diagonal noise-covariances. However, Rakitsch et al. (2013) did consider the more general case of Gaussian systems with a covariance given by the sum of two Kronecker products. In the following section, we exploit their results in order to compute, for fixed A,W, the marginal likelihood of our model."
  }, {
    "heading": "3.1.2. COMPUTATIONAL EFFICIENCY",
    "text": "In this section we show an efficient expression for the computation of the log-marginal likelihood in eq. (7). For simplicity, we consider the synchronized case where all the N nodes in the network have T observations at the same times and, as before, we denote the total number of observations with n = N × T . The main difficulties of computing the log-marginal likelihood above are the calculation of the logdeterminant of an n dimensional matrix, as well as solving an n-dimensional system of linear equations. Our goal is to show that we never need to solve these operations on an n-dimensional matrix, which are O(n3) but instead use O(N3 + T 3) operations. The results in this section have been previously shown by Rakitsch et al. (2013) for covariances with a sum of two Kronecker products.\nWe show our derivations in the supplement (§II.5) and present the results specific to our model here. To give some intuition behind such derivations, the main idea is to “factorout” the noise matrix σ2fE+σ 2 yI from the covariance matrix Σy and then apply properties of the Kronecker product. Hence, given the following matrix definitions along with their eigen-decompositions:\nΩ def = (σ2fE + σ 2 yI) = QΩΛΩQ T Ω,\nK̃f def = Λ −1/2 Ω Q T ΩKfQΩΛ −1/2 Ω = Q̃f Λ̃fQ̃ T f ;\n(8)\nthe log-determinant and the quadratic term in eq. (7) are\nlog |Σy| = T N∑ i=1 log λ (i) Ω + N∑ i=1 T∑ j=1 log(λ̃ (i) f λ̃ (j) t + 1),\nyTΣ−1y y = tr(Ỹ T Q̃tỸtfQ̃ T f ),\n(9) where: [Ỹtf ]i,j = [Q̃Tt ỸQ̃f )]i,j/[λ̃tλ̃ T f + 1]i,j , Ỹ = YQΩΛ −1/2 Ω , and Q̃tΛ̃tQ̃ T t is the eigen-decomposition of Kt. We see that the above computations only require the eigen-decompositions of the N × N matrix K̃f and the T × T matrix Kt, while avoiding matrix operations on the whole n× n matrix of covariances Σy ."
  }, {
    "heading": "3.2. Variational Inference over Network Parameters",
    "text": "Having marginalized the latent functions f corresponding to the network-independent component, our next step is to use variational inference to approximate the true posterior p(A,W|D) with a tractable family of distributions\nq(A,W) that factorizes as\nq(A,W) =q(A)q(W) = ∏ i,j q(Aij)q(Wij),\ni, j = 1 . . . N , and i 6= j. (10)\nFollowing standard variational-inference arguments, we aim to optimize the variational objective, so-called evidence lower-bound (Lelbo), which is given by:\nLelbo def = Lkl + Lell,\nLkl = −KL(q(A,W)||p(A,W)), Lell = Eq(A,W)[log p(Y|A,W)],\n(11)\nwhere KL(q||p) denotes the Kullback-Leibler divergence between distributions q and p, and p(A,W) is the prior over the network-dependent parameters as defined in eq. (3).\nFor non-trivial models and approximate posteriors, the expectations required in the objective above are analytically intractable. Modern variational inference methods estimate Lelbo and its gradients using Monte Carlo samples and the re-parameterization trick (see e.g. Kingma & Welling, 2014; Rezende et al., 2014). Thus, we set our approximate posterior over Wij as q(Wij) = N(µij , σ2ij), which can be re-parameterized easily using Wij = µij + σijzw, where zw ∼ N(0, 1). Furthermore, since the re-parameterization trick cannot be applied to discrete distributions, we use a continuous relaxation of discrete random variables known as the Concrete distribution (Maddison et al., 2016; Jang et al., 2016). In particular we set q(Aij) = Concrete(αij , λc), and sample from it using its re-parameterization:\nU ∼ Uniform(0, 1), aij = (logαij + logU− log(1− U))/λc, Aij = 1/(1 + exp(−aij)),\n(12)\nwhere αij are variational parameters and λc is a constant. Analogously to Maddison et al. (2016), we also relax our priors and estimate the log-probabilities in Lkl using:\nlog q(Aij) = log λc − λcaij + logαij − 2 log(1 + exp(−λcaij + logαij)), (13)\nand similarly for p(Aij). Having relaxed our discrete variables, we proceed with optimization of the Lelbo in eq. (11) by using Monte Carlo samples from q(W,A) to estimate Lell. For computing KL(q(A)||p(A)) we use samples from q(A), p(A) and their log-probabilities as defined in eq. (13). Finally, for KL(q(W)||p(W)) we use the analytical form for the KL-divergence between two Gaussians.\nAs a consequence of the relaxation of the prior and posterior over A via the Concrete distribution, defining a sound joint (dependent) distribution between A and W is quite\nchallenging, which motivates the independence assumptions made in eq. (10). However, A and W still interact in the likelihood and these interactions are captured during variational learning."
  }, {
    "heading": "4. Stability and Robustness",
    "text": "Lkl is straightforward to compute as explained in section 3.2, so the eventual stability burden relies on calculating log p(y|W,A) in Lell using samples of W and A. At the core of this problem lies the non-singularity of (I − B). This problem appears in the most popular approaches as well, sometimes as is (Shimizu et al., 2011), sometimes coming with stronger constraints on the boundedness of the coordinates of B (Hyvärinen & Smith, 2013) or its eigenvalues (Linderman & Adams, 2014). Such constraints can be related to stability issues because, when they fail to hold, parameters diverge. What we now show is that stability is not an issue for our model: we get it for free.\nTheorem 1 For any value of the parameters of the concrete distributions (λc ≥ 0 and αij ≥ 0 (∀i 6= j)), I −A W is non-singular with probability one.\nProof sketch: The proof (given in extenso in supplement, §II.1) is non-trivial to handle the limit cases of λc = 0 or αij = 0. To understand the importance of concrete distributions, we sketch here the case λc > 0 or αij > 0 (∀i 6= j). For any2 N ≥ 2, denote g1, g2, ..., gN the columns of I − A W. Each can be thought of as a random vector where one coordinate takes value 1 with probability 1, and this coordinate is different for each two vectors. I−A W is non invertible iff g1, g2, ..., gN are linearly dependent. None of the gjs can be the null vector, so if I − A W is not invertible, then ∃j > 1 : gj ∈ span(g1, g2, ..., gj−1). As a consequence,\nPr(det(I−A W) = 0) ≤ ∑ j Pr(gj ∈ span(g1, g2, ..., gj−1)) , (14)\nwhere the distribution is the product distribution over the columns of I−A W. Fix any g1, g2, ..., gj−1 belonging to the respective supports of the columns, and let qj def = Pr(gj ∈ span(g1, g2, ..., gj−1)|g1, g2, ..., gj−1). It is not hard to check that the densities of gj for j ≥ 1 are all absolutely continuous with respect to Lebesgue measure — a key fact, developed in supplement, authorized by the fact that the concrete distribution in eq. (13) “passes through” the absolute continuity of the input distribution (uniform). Along with the fact that span(g1, g2, ..., gj−1) has strictly positive codimension for any j ≤ N , it comes qj = 0,∀j ≥ 2,∀g1, g2, ..., gj−1 fixed. Integrating over the choices of g1, g2, ..., gj−1, we get Pr(gj ∈ span(g1, g2, ..., gj−1)) =\n2Whenever N = 1, I−A W def= [1] is always invertible.\n0,∀j ≤ N and so Pr(det(I − A W) = 0) = 0 from eq. (14). As a consequence, I − A W is non-singular with probability one, as claimed. Now, if say λc = 0 or some αij = 0, some atom events for column sampling appear with non-zero probability, but the associated determinant can be reduced to that of a squared submatrix for which the previous analysis holds, leading to the same result. Given Theorem 1, the following result is not surprising.\nTheorem 2 For any value of the parameters of the concrete distributions (λc ≥ 0, αij ≥ 0 (i 6= j)), any σ2y > 0, |Lell| ∞. The complete proofs of these theorems are given in the supplement, §II.1, §II.2.\nSince we get stability for free, where other popular approaches need to make assumptions to get it, one might ask what more we can get under assumptions that would look alike. Such assumptions constrain the network parameters, typically using the moments or values, eventually including the network size (Hyvärinen & Smith, 2013; Linderman & Adams, 2014). What we now show is that under similar assumptions, we do not just get stability for inference, we make it numerically easy with high probability, and this holds for a sampling model (M) more general than ours, meaning that one could make alternative choices to the concrete distributions we use and yet keep the same property:\n(M ) (∀i, j) (i) weight Wij is picked as N(µij , σ2ij) (µij ∈ R, σij > 0), and (ii) adjacency Aij is picked as Bern(ρij) with ρij ∼ V, where V is any random variable with support in [0, 1] (letting pij def = E[ρij ]).\nTo get our result, we need two functions that aggregate the complete network signal coming from (or to) each node, U, S : {1, 2, ..., 2N} → R+, defined as:\nU(i) def =\n2 N · { ∑ j pij(µ 2 ij + σ 2 ij) if i ≤ N∑\nj pji∗(µ 2 ji∗ + σ 2 ji∗) otherwise\n,\nS(i) def =\n1 N · { ∑ j µ 2 ij + σ 2 ij if i ≤ N∑\nj µ 2 ji∗ + σ 2 ji∗ otherwise\n,\nwith i∗ def= i−N . For any diagonalizable U, λ(U) denotes its eigenspectrum, and λ↑(U) def= max |λ(U)|, λ↓(U) def= min |λ(U)|. Let us now state our main result. We shall comment afterwards the assumptions it makes.\nTheorem 3 Fix any constants c > 0 and 0 < γ < 1 and let λ◦ def = (λ↓(Kt)/2) + σ 2 y , λ• def = 2λ↑(Kt) + σ 2 f + σ 2 y and g(z,y) def = (N/2) log z+ z‖y‖22−C, where C is defined as in (7). Under sampling model M , suppose that\nmax i U(i) ∈\n[ maxi S(i)\nNγ ,\n1\n100N2\n] . (15)"
  }, {
    "heading": "If N is larger than some constant depending on c and γ,",
    "text": "then with probability ≥ 1− 1/N c, we have:\n− log p(y|W,A) ∈ [g(λ◦,y), g(λ•,y)] ,∀y .(16)\nHence, if the non-network dependent “signal” is not flat (say, λ↓(Kt), σ2y are above machine zero), then it is in fact pretty easy to sample Lell over most of its support. As discussed in the supplement, the constraint of eq. (15) can be weakened for specific Vs (e.g. for more “informative” distributions). We also remark that we do not face the sparsity constraints of the model of Linderman & Adams (2014), such as a mandatory sparsity increase with N .\nTheorem 3 is a direct consequence of another Theorem which can be roughly summarized as:\n\"modulo an assumption on the network-dependent parameters, the covariance of observations is of the same\norder as the (co)variance of the network independent component plus that of the noise.\"\nIn short, there is a form of robustness (in the statistical sense) achieved on the output with respect to the networkdependent parameters. This can also be viewed as a balance achieved on the second-order moments, between the network-dependent “signal” versus the one which is not network-dependent. We put signal in quotes since the rest includes the noise parameters. Theorem 4 Under the conditions of Theorem 3, with probability ≥ 1 − (1/N c) over the sampling of W and A, we have that λ(Σy) ⊂ [λ◦, λ•]. (Proof in supplement, §II.3.) It is not hard to check that eq. (16) is a direct consequence of Theorem 4. Let us finally comment those assumptions on the network-dependent parameters made in eq. (15). To be nonempty, the interval puts the implicit constraint that maxi S(i) = O(1/N ζ) for some constant ζ, i.e. roughly, the expected square signal (node-wise) has to be bounded. Such a bound in the signal’s values can be found in Hyvärinen & Smith (2013). Consider now the upperbound in eq. (15). It is quantitatively not so different from Linderman & Adams (2014)’s assumptions. They consider two assumptions, the first of which being3\nσ2 ≤ 1 N , (17)\nand also pick network parameters µ, σ in such a way that large deviations for edge weights are controlled with high probability, with a condition that roughly looks like:\nµ2 + c\nN2 · σ2 = O\n( 1\nN2\n) , (18)\nfor some constant c > 0. The constraint put on the maximum node-wise network signal in eq. (15) is in fact quite similar to the constraints imposed by eqs. (17) and (18).\n3We consider variances for the assumption to rely on the same scales as ours.\nSummary of theoretical results and consequences: “Stability”, as used in various works, takes on two forms, describing either the stability of the model (Linderman & Adams, 2014; Shimizu et al., 2006) or the numerical stability of the learning algorithm (Shimizu et al., 2011). Our results contribute to both: while Theorems 1 and 2 are essentially numerical stability results, the robustness result of Theorem 4 is a model stability result, since it bounds the overall model’s signal as a function of the external signal to the network. Theorem 3 lies in between, but its key purpose may be more practical. It says that one can approximate some key variational inference parameters with high probability, which can therefore save time at the expense of an affordable approximation."
  }, {
    "heading": "5. Experiments",
    "text": "We evaluate our approach on three distinct domains: discovering brain functional connectivity (BRAIN), modeling property prices in Sydney (SYDNEY) and regulation in the yeast genome (YEAST). We considered the methods used recently by Peters et al. (2014) as baselines for comparison. These include: (1) PC algorithm (Spirtes et al., 2000); (2) Conservative PC algorithm (CPC, Ramse et al., 2006); and (3) LiNGAM (Shimizu et al., 2006). In addition to the above, we considered (4) IAMB (Tsamardinos et al., 2003), and (5) Pairwise LiNGAM (PW-LINGAM, Hyvärinen & Smith, 2013), which is a cyclic model and has been developed specifically for discovering connectivity between different brain regions. For the reasons detailed in the supplement (§III), other methods used in Peters et al. (2014) were not applicable to the datasets analyzed here. We used the squared exponential covariance function. For more details of the baseline methods, prior setting and optimization specifics see the supplement, §III. Given the posterior distribution q(Aij), the posterior probability of existence of a connection from node j to i was calculated as αij/(1 + αij)."
  }, {
    "heading": "5.1. BRAIN Domain",
    "text": "Here we want to discover the connectivity between different brain regions, which is a crucial element of neuroscience studies. We analyzed the benchmarks of Smith et al. (2011), in which the activity of different brain regions is simulated for 50 subjects at 200 time points (T = 200) for networks with a different number of nodes (N = 5, 10, 15). The true underlying network connectivities are reported in Smith et al. (2011), which we used to calculate the area under the ROC curve (AUC) for the links predicted for each subject. Ground-truth data includes whether there is a connection (edge) from node i to j (directional; “positive” is when there is a connection), which were used to calculate true/false positive rates by varying the discrimination threshold (see the supplement, §III.2, for details). As it is not possible to de-\nfine comparable discriminative thresholds across different methods, using the AUC avoids the selection of a single threshold altogether. We note that the underlying network is a directed acyclic graph (DAG); however networks discovered by LATNET are not restricted to DAGs, and therefore baseline methods assuming the underlying network is a DAG have a favorable bias.\nFigure 1(a) shows the AUC of each method (for N = 15) using box-plots (top and bottom edges of the box correspond to the first and third quartiles respectively). Results for N = 5, 10 are given in the supplement (§III.2). In the case of LATNET, we used the posterior uncertainties around the predicted connections and their strengths to determine the\ndiscriminative thresholds for the AUC calculations. We see that, although other methods are favorably biased about the underlying structure, LATNET consistently outperforms all the baseline methods."
  }, {
    "heading": "5.2. SYDNEY Domain",
    "text": "Here we aim to discover the relationship between property prices in different suburbs of Sydney. The data include quarterly median sale prices for 51 suburbs in Sydney and surrounding area from 1995 to 2014. Since the underlying network is unknown, we cannot compute the AUC and instead use two other performance measures concerned with spatial coherence and temporal stability. For this purpose, we compute the air distance between the suburbs that are discovered to be connected (the shorter the better) and the proportion (r) of networks in which a connection was present (for each discovered connection) when our method is applied to different time windows (the higher the better). We set the discrimination threshold for each method so that on average each method finds 17-19 edges in the network.\nFigure 1(b) shows the air distance between the connected nodes discovered by each method. The average distance by LATNET is 21km, which is almost half of the average distance by the other methods (CPC:38km, PC:40km, IAMB:43km, PW-LINGAM:60km; p-values < 0.001 for all t-tests between arc distances in LATNET and each of the competitors). Therefore, the networks discovered by LATNET are more spatially coherent than the baselines’. Similarly, Figure 1(c) shows the r values for the different methods. Less than 8%, 5%, 1% of PC, CPC and IAMB arcs were significant (risk α = 0.05), respectively, while more than 29% of LATNET arcs are significant. Interestingly, PW-LINGAM did not find any significant arcs. We then conclude that temporal stability of the connections discovered by LATNET is significantly better than all the baseline methods. Additional details and results can be found in the supplement (§III.5)."
  }, {
    "heading": "5.3. YEAST Domain",
    "text": "We use LATNET to infer local and global genome regulation patterns for one extensively studied species, Saccharomyces cerevisiae (Spellman et al., 1998). This represents 100,000+ data points and a network with up to 38,000,000+ arcs. The true underlying network is unknown but there is extensive literature about its major features. Here we take as references the cell cycle transcriptionally regulated genes (Rowicka et al., 2007) and http://www.yeastgenome.org as a more general resource. For space reasons, we summarize experiments here, and leave to the supplement their exhaustive treatment.\nAnalysis of the sentinels of the yeast cell cycle (YCC), which represents ≈ tenth of the network (Spellman et al., 1998). We extracted key genes and connections discovered by LATNET based on a subset of strong arcs. Figure 2 (left)\nsummarizes the results. We observe that (i) in terms of nodes, the key discovered genes are known to be involved in the cell structure dynamics. Strikingly, although SFG1 is not among the reported key genes, it happened to be neighbors of SPS4 (a key gene) on the same chromosome, and therefore it is likely to be part of the network and can be targeted for future investigations. (ii) In terms of the connections, the topmost strong arcs belong to a small connected component (Gw) that are asymmetrically organized around gene WSC4, which is consistent with the fact that the underlying network should be directed.\nWe then aimed to obtain a broader network map without filtering arcs. Figure 2 (center) shows the manifold coordinates induced by the network’s graph, built upon Meila & Shi (2001). With such a technique, clusters of genes that are “significantly far” from each other should represent different key network structure components. In our case, it is evident that there is a “crowd versus the rest of the crowd” structure, and this rest of the crowd gathers almost only heavily regulated genes, that is, genes that are known to be heavily connected in the true network. It is therefore apparent that LATNET has succeeded in recovering a prominent network structure around such genes. Consistent with the literature, a small number of key genes drive the coordinates. Last, Figure 2 (right) summarizes the broad picture of strong arcs between YCC phases: it should come at no surprise that cell splitting, (M)itosis, has the largest number of these arcs.\nAnalysis of the full genome (results in supplement, III.4). A successful technique should recover three essential features of the complete network, from local to global: (i) the fact that it is locally highly asymmetric by nature, with a small number of direct feedbacks relatively to the genome\nsize; (ii) the fact that key sub-networks like YCC should still be in the top rank of the global network and (iii) known connected sub-networks should still appear with as little noise as possible brought by the overall network. LATNET clearly succeeds at (i), with more than twice strong arcs going outside the YCC compared to arcs coming in the YCC from non-YCC genes. LATNET is also good at (ii), and we see that YCC genes tend to be outnumbered by genes that are perhaps more “all-purpose” but still supposed to be involved in heavy regulation mechanisms, which makes sense. The most prominent result is perhaps on (iii): the predominance of gap phase G1 compared to G2 that we still observe with all genes hints on the yeast species from which our data comes from: this is a indeed a known feature of Saccharomyces cerevisiae, versus other species like S. pombe for example. None of these patterns, in particular (ii) and (iii), were discovered using the baseline methods."
  }, {
    "heading": "6. Conclusion & Discussion",
    "text": "We have proposed a Bayesian framework for network structure discovery for continuous-valued observations; developed an efficient inference algorithm for it; and shown its benefits on real applications. Our theoretical analysis shows that the traditional constraints for stability (Hyvärinen & Smith, 2013; Linderman & Adams, 2014; Shimizu et al., 2006) in networks are alleviated. What we get with such assumptions is a non-negligible uplift in the easiness of the expected log-likelihood part, the bottleneck of the ELBO, and a robustness of the output’s variance with respect to the network parameters. These results also hold for a class of posteriors broader than the ones we use, opening interesting avenues of applications for Concrete distributions."
  }, {
    "heading": "Acknowledgements",
    "text": "AD was supported by a Research Fellowship from UNSW Sydney. We thank Louis Tiao for feedback on the manuscript."
  }],
  "year": 2018,
  "references": [{
    "title": "Structural equations with latent variables",
    "authors": ["Bollen", "K.-A"],
    "year": 1989
  }, {
    "title": "Multitask Gaussian process prediction",
    "authors": ["Bonilla", "E.-V", "Chai", "K.-M. A", "Williams", "C.-K.-I"],
    "venue": "In NIPS,",
    "year": 2008
  }, {
    "title": "Nonlinear causal discovery with additive noise models",
    "authors": ["Hoyer", "P.-O", "D. Janzing", "Mooij", "J.-M", "J. Peters", "B. Schölkopf"],
    "venue": "In NIPS,",
    "year": 2009
  }, {
    "title": "Identification of time-dependent causal model: A Gaussian process treatment",
    "authors": ["B. Huang", "K. Zhang", "B. Schölkopf"],
    "venue": "In IJCAI, pp",
    "year": 2015
  }, {
    "title": "Learning linear cyclic causal models with latent variables",
    "authors": ["A. Hyttinen", "F. Eberhardt", "P.O. Hoyer"],
    "year": 2012
  }, {
    "title": "Pairwise likelihood ratios for estimation of non-Gaussian structural equation models",
    "authors": ["A. Hyvärinen", "Smith", "S.-M"],
    "venue": "JMLR, 14:111–152,",
    "year": 2013
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["E. Jang", "S. Gu", "B. Poole"],
    "year": 2016
  }, {
    "title": "Auto-Encoding Variational Bayes",
    "authors": ["D. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Constrained bayesian inference for low rank multitask learning",
    "authors": ["O. Koyejo", "J. Ghosh"],
    "venue": "In UAI,",
    "year": 2013
  }, {
    "title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models",
    "authors": ["N. Lawrence"],
    "venue": "JMLR, 6:1783–1816,",
    "year": 2005
  }, {
    "title": "Discovering Latent Network Structure in Point Process Data",
    "authors": ["Linderman", "S.-W", "Adams", "R.-P"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
    "authors": ["Maddison", "C.-J", "A. Mnih", "Teh", "Y.-W"],
    "year": 2016
  }, {
    "title": "Learning segmentation by random walks",
    "authors": ["M. Meila", "J. Shi"],
    "venue": "In NIPS,",
    "year": 2001
  }, {
    "title": "Bayesian and l1 approaches for sparse unsupervised learning",
    "authors": ["S. Mohamed", "K.A. Heller", "Z. Ghahramani"],
    "venue": "In ICML,",
    "year": 2012
  }, {
    "title": "On causal discovery with cyclic additive noise models",
    "authors": ["J.M. Mooij", "D. Janzing", "T. Heskes", "B. Schölkopf"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "Causality: Models, Reasoning, and Inference",
    "authors": ["J. Pearl"],
    "year": 2000
  }, {
    "title": "Causal discovery with continuous additive noise models",
    "authors": ["J. Peters", "Mooij", "J.-M", "D. Janzing", "B. Schölkopf"],
    "year": 2009
  }, {
    "title": "It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals",
    "authors": ["B. Rakitsch", "C. Lippert", "K. Borgwardt", "O. Stegle"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Adjacency-faithfulness and conservative causal inference",
    "authors": ["J. Ramse", "J. Zhang", "P. Spirtes"],
    "venue": "In UAI,",
    "year": 2006
  }, {
    "title": "Gaussian processes for machine learning",
    "authors": ["C.E. Rasmussen", "C.K.I. Williams"],
    "year": 2006
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Rezende", "D.-J", "S. Mohamed", "D. Wierstra"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Feedback models: Interpretation and discovery",
    "authors": ["T. Richardson"],
    "venue": "PhD thesis,",
    "year": 1996
  }, {
    "title": "High-resolution timing of cell cycle-regulated gene expression",
    "authors": ["M. Rowicka", "A. Kudlicki", "Tu", "B.-P", "Z. Otwinowski"],
    "venue": "PNAS, 104(43):16892–16897,",
    "year": 2007
  }, {
    "title": "A linear non-Gaussian acyclic model for causal discovery",
    "authors": ["S. Shimizu", "Hoyer", "P.-O", "A. Hyvärinen", "A. Kerminen"],
    "year": 2003
  }, {
    "title": "DirectLiNGAM: A direct method for learning a linear nongaussian structural equation model",
    "authors": ["S. Shimizu", "T. Inazumi", "Y. Sogawa", "A. Hyvärinen", "Y. Kawahara", "T. Washio", "Hoyer", "P.-O", "K. Bollen"],
    "year": 2011
  }, {
    "title": "Network modelling methods for FMRI",
    "authors": ["Smith", "S.-M", "Miller", "K.-L", "G. Salimi-Khorshidi", "M. Webster", "Beckmann", "C.-F", "Nichols", "T.-E", "Ramsey", "J.-D", "Woolrich", "M.-W"],
    "year": 2011
  }, {
    "title": "Algorithms for Large Scale Markov Blanket Discovery",
    "authors": ["I. Tsamardinos", "Aliferis", "C.-F", "A.R. Statnikov", "E. Statnikov"],
    "venue": "In FLAIRS,",
    "year": 2003
  }, {
    "title": "Generalised wishart processes",
    "authors": ["Wilson", "A.-G", "Z. Ghahramani"],
    "venue": "In UAI,",
    "year": 2010
  }, {
    "title": "Invariant Gaussian Process Latent Variable Models and Application in Causal Discovery",
    "authors": ["K. Zhang", "B. Schölkopf", "D. Janzing"],
    "venue": "In UAI,",
    "year": 2010
  }],
  "id": "SP:7a2148d5e1736804fd3ddd88ba4801d90adafa5c",
  "authors": [{
    "name": "Amir Dezfouli",
    "affiliations": []
  }, {
    "name": "Edwin V. Bonilla",
    "affiliations": []
  }, {
    "name": "Richard Nock",
    "affiliations": []
  }],
  "abstractText": "Traditional methods for the discovery of latent network structures are limited in two ways: they either assume that all the signal comes from the network (i.e. there is no source of signal outside the network) or they place constraints on the network parameters to ensure model or algorithmic stability. We address these limitations by proposing a model that incorporates a Gaussian process prior on a network-independent component and formally proving that we get algorithmic stability for free, while providing a novel perspective on model stability as well as robustness results and precise intervals for key inference parameters. We show that, on three applications, our approach outperforms previous methods consistently.",
  "title": "Variational Network Inference: Strong and Stable with Concrete Support"
}