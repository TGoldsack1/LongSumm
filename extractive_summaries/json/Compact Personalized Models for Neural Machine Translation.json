{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 881–886 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n881"
  }, {
    "heading": "1 Introduction",
    "text": "Professional translators typically translate a collection of related documents drawn from a domain for which they have a set of previously translated examples. Domain adaptation is critical to providing high quality suggestions for interactive machine translation and post-editing interfaces. When many translators use the same shared service, the system must train and apply a personalized adapted model for each user. We describe a system architecture and training method that achieve high space efficiency, time efficiency, and translation performance by encouraging structured sparsity in the set of offset tensors stored for each user. Effective model personalization requires both batch adaptation to an in-domain training set, as well as incremental adaptation to the test set. Batch adaptation is applied when a user uploads relevant translated documents before starting to work. Incremental adaptation is applied when a user provides a correct translation of each segment just after receiving machine translation suggestions, and the system is able to train on that correction before generating\nsuggestions for the next segment. This is referred to as a posteriori adaptation by Turchi et al. (2017). Our experiments compare both types of adaptation. There are cases for which incremental adaptation achieves better performance using fewer examples, as examples drawn directly from the test set are often highly relevant to subsequent parts of that test set. There are also cases for which the gains from both types of domain adaptation are additive. The time required to translate and to adapt both must be minimal in a personalized translation service. Interactive translation requires suggestions to be generated at typing speed, and incremental adaptationmust occur within a few hundredmilliseconds to keep up with a translator’s typical workflow. The service can be expected to store models for a large number of users and dynamically load and adapt models for many active users concurrently. Therefore, minimizing the number of parameters stored for each user’s personalizedmodel is important both for reducing storage requirements and latency. We achieve space and time efficiency by representing each user’s model as an offset from the unadapted baseline parameters and encouraging most offset tensors to be zero during adaptation. We show that group lasso regularization can be applied to a self-attentive Transformer model to freeze up to 75% of the parameters with minimal or no loss of adapted translation quality across experiments on four English→German data sets. We confirm these findings for six additional language pairs."
  }, {
    "heading": "2 Related Work",
    "text": "There is extensive work on incremental adaptation from human post edits or simulated post edits, both for statistical machine translation (Green et al., 2013; Denkowski et al., 2014a,b; Wuebker et al., 2015) and neural machine translation (Peris et al.,\n2017; Turchi et al., 2017; Karimova et al., 2017). Both Turchi et al. (2017) and Karimova et al. (2017) apply vanilla fine-tuning algorithms. In addition to fine-tuning towards user corrections, the former applies a priori adaptation to retrieved data that is similar to the incoming source sentences. Peris et al. (2017) propose a variant of fine-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models.\nRegularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1/`2 regularization for feature selection in an N - best reranking task. A similar approach, employing `1/`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization.\nTechniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009).\nAn extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting."
  }, {
    "heading": "3 Self-Attentive Translation Model",
    "text": "The neural machine translation systems used in this work are based on the Transformer model introduced by Vaswani et al. (2017), which uses selfattention rather than recurrent or convolutional layers to aggregate information across words. In addition to its superior performance, its main practical advantage over recurrent models is faster training. The Transformer follows the encoder-decoder paradigm. Source word vectors x1, . . . , xm are chosen from an embedding matrix Xe. A series of stacked encoder layers generate intermediate representations z1, . . . , zm. Each layer of the encoder consists of two sub-layers: a multi-head selfattention layer that uses scaled dot-product atten-\ntion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are applied to each sub-layer. A series of stacked decoder layers produces a sequence of target word vectors y1, . . . , yn. Each decoder layer has three sub-layers: self-attention, encoder-attention, and a filter. For target position j, the self-attention layer can attend to any previous target position j′ ∈ [1, j], with target words offset by one so that representations at j can observe word j−1, but not word j. The encoder-attention layer can attend to the final encoder state zi for any source position i ∈ [1,m]. Observed target word vectors are chosen from an embedding matrix Ye, and target word j is predicted from yj via a soft-max layer parameterized by an output projection matrix Yo. The encoders in this work have six layers that have a self-attention sub-layer size of 256 and a filter sub-layer size of 512. Each filter performs two linear transformations and a ReLU activation:\nf(x) = max(0, xW1 + b1)W2 + b2.\nThe decoders in this work have three layers, and all sub-layer sizes are 256. The decoder sublayers are simplified versions of those described in Vaswani et al. (2017): The filter sub-layers perform only a single linear transformation, and layer normalization is only applied once per decoder layer after the filter sub-layer.\nUnlike in Vaswani et al. (2017), none of Xe, Ye, or Yo share parameters in our TensorFlow1 implementation. Baseline models are optimized with Adam (Kingma and Ba, 2015)."
  }, {
    "heading": "4 Compact Adaptation",
    "text": ""
  }, {
    "heading": "4.1 Fine Tuning",
    "text": "Personalized machine translation requires batch adaptation to a domain-relevant bitext (such as a user provided translation memory) as well as incremental adaptation to the test set. We apply finetuning, which involves continuing to train model parameters with a gradient-based method on domainrelevant data, as a simple and effective method for neural translation domain adaptation (Luong and Manning, 2015). The fine-tuned model without regularization and clipping is denoted as the Full Model. Confirming previous work, we found that\n1https://www.tensorflow.org/\nstochastic gradient descent (SGD) is the most effective optimizer for fine tuning (Turchi et al., 2017). In our experiments, batch adaptation uses a batch size of 7000 words for 10 Epochs and a fixed learning rate of 0.1, dropout of 0.1, and label smoothing with ls = 0.1 (Szegedy et al., 2016).\nIncremental adaptation uses a batch size of one and a learning rate of 0.01. To ensure a strong adaptation effect within a single document, we set dropout and label smoothing to zero and perform up to three SGD updates on each segment. After each update, we measure the model perplexity on the current training example and continue with another update if the perplexity is still above 1.5."
  }, {
    "heading": "4.2 Offset Tensors",
    "text": "In a personalized translation service, adapted models need to be loaded quickly, so a space-efficient representation is critical for time efficiency as well. Production speed requirements using contemporary cloud hardware limit model sizes to roughly 10M parameters per user, while a high-quality baseline Transformer model typically requires 35M parameters or more. We propose to store the parameters of an adapted model as an offset from the baseline model. Each tensor is a sumW = Wb+Wu, where Wb is from the baseline model and is shared across all adapted models, while the offsetWu is specific to an individual user domain. Space efficiency is achieved by only storingWu for a subset of tensors and setting the rest of the offset tensors to zero. One approach to achieving model sparsity is to manually partition the network into a small number of regions and systematically evaluate translation performance when storing offsets for only one region. We define five distinct regions, which are evaluated in isolation: Outer layers (the first and last layers of both encoder and decoder), inner layers (all the remaining layers), the two embedding matrices Xe and Ye, and the output projection matrix Yo. The latter three are each stored as a single matrix and each contributes 10.3M parameters to the full model size in English→German. During adaptation, the embedding matrices are only updated for vocabulary present in the training examples, and so the offsets can be stored efficiently as a sparse collection of columns. The same principle can be applied to the output projection matrix by only updating parameters corresponding to vocabulary items that appears in the adaptation examples (denoted Sparse Output Proj. in Table 1).\nA second approach to achieving model sparsity is to use a procedure to select the subset of offset tensors that are stored. For example, we evaluate a simple policy that stores an offset for all tensors whose average change in parameter values is higher than a threshold. This set is selected on a development domain and held fixed for all other domains. We refer to this method as fixed adaptation."
  }, {
    "heading": "4.3 Tensor Selection via Group Lasso",
    "text": "A group sparse regularization penalty such as group lasso can be applied to the offset tensors for simultaneous regularization and tensor selection. This penalty drives entire offset tensors to zero, so that they do not need to be stored or loaded. We add the following regularization term to the loss function (Scardapane et al., 2017):\nR`1,2(T ) = ∑ T∈T √ |T |‖∆T‖2 (1)\n‖∆T‖2 = ∑ τ∈T ∆τ2 (2)\nHere, each tensor corresponds to one group. T denotes the set of all tensors in the model, τ ∈ T the set of all weights within a single tensor and ∆τ the size of the offset for τ . Note that we are regularizing the difference between the parameters of the adapted model and the baseline model, rather than regularizing the full network parameters directly. In this way, we maintain the expressive power of the full network while minimizing the size of the adapted models. Group lasso regularization is equivalent to `1 regularization when the group size is 1. Sparsity among groups is encouraged because the `1 norm serves as a convex proxy for the `0 norm, which would explicitly penalize the number of non-zero elements (Yuan and Lin, 2006). To facilitate tensor selection, we define a threshold ϑ to clip offset tensors ∆T with average weight 1|T | ∑ τ∈T ∆τ < ϑ to zero. Both the threshold ϑ and the regularization weight λ were manually tuned on a development domain and set to ϑ = 10−4 and λ = 10−6. We apply clipping to all tensors except the embedding and output projection matrices Xe, Ye and Yo. As our production constraints allow us to retain only one of the three, we pre-select the sparse output projection as part of the model and exclude the embedding matrices from adaptation. This method will be denoted as Lasso."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Data",
    "text": "We first evaluate all techniques on an English→German Transformer network trained on 98M parallel sentence pairs. We apply byte pair encoding (Sennrich et al., 2016) separately to each language and obtain vocabularies with 40K unique tokens each. We refer to the unadapted model as Baseline. We evaluate on four domains. For development, we use a data set labeled User1 that was gathered from a user of the browser-based CAT (computer-aided translation) tool Lilt2 and contains documents from the financial domain with 48K segments for batch adaptation and 1790 segments for testing and incremental adaptation. We further evaluate on a second user test set User2 (technical support, 31k batch adaptation, 1000 test segments); the public Autodesk corpus3, where we select the first 20k segments for batch adaptation and the next 1000 segments for testing; and the IWSLT corpus4 (semi-technical talks), where we use all provided 206K sentences for batch adaptation\n2https://lilt.com 3https://autodesk.app.box.com/\nAutodesk-PostEditing 4http://workshop2017.iwslt.org/\nand the dev2010 set (888 sentences) for testing. The overall best performing compact adaptation technique, group lasso regularization, is further evaluated on six other language pairs trained using production data sets collected from Lilt’s user base: English↔French, English↔Russian and English↔Chinese. Adaptation is performed on user data from various domains (technical manuals, finance, legal), each with 8k-10k segments for batch adaptation and 2000 segments for testing and incremental adaptation. Translation quality is evaluated using the cased Bleu (Papineni et al., 2002) measure."
  }, {
    "heading": "5.2 Results",
    "text": "Table 1 shows English→German results. Full model adaptation, where all offsets are stored, improves over the baseline in all cases to various degrees. This full model contains only 25.8M parameters, as offsets for both embedding matrices are stored as sparse collections of columns for the vocabulary present in the adaptation data. Next, we evaluate the impact of storing offsets only for one region at a time. We observe that among the three vocabulary matrices, the output projection Yo has the strongest impact on quality, which is not dimin-\nished by storing a sparse variant that is restricted only to observed vocabulary.\nIn addition, we evaluate twomethods of choosing a subset of tensors procedurally. We first experiment with a fixed subset of tensor offsets that was chosen by selecting all tensors for which parameters were offset by more than 0.002 on average after batch adaptation on the User1 data set. This simple procedure approaches the performance of full model adaptation, but stores only 27% of its parameters. Dynamically selecting tensor offsets for each data set using group lasso regularization improves performance on 6 out of 8 data conditions.\nThe combination of batch and incremental adaptation yields further improvements, with the exception of the User2 and IWSLT tasks, where incremental adaptation overall performs not as well as batch adaptation. For these tasks, both tests sets exhibit lower repetition rates5 (Cettolo et al., 2014) than the test sets for the two other tasks (see the two bottom lines in Table 1). The User2 test set is furthermore a random sample of non-consecutive text from a translation memory, which is suboptimal for incremental learning. Altogether, we are able to achieve translation performance similar to full model adaptation with 25% of the total network parameters. Note that due to the selection of entire tensors with groupwise regularization, there is nearly zero space overhead incurred by storing a sparse set of offset tensors. Table 2 confirms our main findings on six other language pairs. We observe average improvements of 14.3 Bleu with our final compact model, which compares to 15.5 Bleu for full model adaptation."
  }, {
    "heading": "6 Conclusion",
    "text": "We describe an efficient approach to personalized machine translation that stores a sparse set of ten-\n5Repetition rates have been confirmed to be a suitable indicator for gains through incremental adaptation in numerous works (Wuebker et al., 2015; Bertoldi et al., 2014).\nsor offsets for each user domain. Group lasso regularization applied to the offsets during adaptation achieves high space and time efficiency while yielding translation performance close to a full adapted model, for both batch and incremental adaptation and their combination."
  }],
  "year": 2018,
  "references": [{
    "title": "Layer normalization",
    "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton."],
    "venue": "arXiv preprint arXiv:1607.06450.",
    "year": 2016
  }, {
    "title": "Online adaptation to post-edits for phrase-based statistical machine translation",
    "authors": ["Nicola Bertoldi", "Patrick Simianer", "Mauro Cettolo", "Katharina Wäschle", "Marcello Federico", "Stefan Riezler."],
    "venue": "Machine Translation, 28(3-4):309–339.",
    "year": 2014
  }, {
    "title": "The repetition rate of text as a predictor of the effectiveness of machine translation adaptation",
    "authors": ["Mauro Cettolo", "Nicola Bertoldi", "andMarcello Federico"],
    "venue": "In Conference of the Association for Machine Translation in the Americas (AMTA),",
    "year": 2014
  }, {
    "title": "Learning from post-editing: Online model adaptation for statistical machine translation",
    "authors": ["Michael Denkowski", "Chris Dyer", "Alon Lavie."],
    "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Lin-",
    "year": 2014
  }, {
    "title": "Real time adaptive machine translation for post-editing with cdec and transcenter",
    "authors": ["Michael Denkowski", "Alon Lavie", "Isabel Lacruz", "Chris Dyer."],
    "venue": "Proceedings of the EACL 2014 Workshop on Humans and Computer-assisted Translation, pages",
    "year": 2014
  }, {
    "title": "N-best reranking by multitask learning",
    "authors": ["Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada", "Hideki Isozaki", "Masaaki Nagata."],
    "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 375–383, Uppsala,",
    "year": 2010
  }, {
    "title": "The efficacy of human postediting for language translation",
    "authors": ["Spence Green", "Sida Wang", "Daniel Cer", "Christopher D. Manning."],
    "venue": "InACMCHIConference on Human Factors in Computing Systems, Paris, France.",
    "year": 2013
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
    "year": 2016
  }, {
    "title": "A user-study on online adaptation of neural machine translation to human post-edits",
    "authors": ["Sariya Karimova", "Patrick Simianer", "Stefan Riezler."],
    "venue": "CoRR, abs/1712.04853.",
    "year": 2017
  }, {
    "title": "Sequencelevel knowledge distillation",
    "authors": ["Yoon Kim", "Alexander M. Rush."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Lei Ba."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Stanford neural machine translation systems for spoken language domain",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "International Workshop on Spoken Language Translation, Da Nang, Vietnam.",
    "year": 2015
  }, {
    "title": "Extreme adaptation for personalized neural machine translation",
    "authors": ["Paul Michel", "Graham Neubig."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 312–318. Association for Com-",
    "year": 2018
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "ACL.",
    "year": 2002
  }, {
    "title": "Online learning for neural machine translation post-editing",
    "authors": ["Álvaro Peris", "Luis Cebrián", "Francisco Casacuberta."],
    "venue": "CoRR, abs/1706.03196.",
    "year": 2017
  }, {
    "title": "Group sparse regularization for deep neural networks",
    "authors": ["Simone Scardapane", "Danilo Comminiello", "Amir Hussain", "Aurelio Uncini."],
    "venue": "Neurocomput., 241(C):81–89.",
    "year": 2017
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
    "year": 2016
  }, {
    "title": "Multitask learning for improved discriminative training",
    "authors": ["Patrick Simianer", "Stefan Riezler"],
    "year": 2013
  }, {
    "title": "Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT",
    "authors": ["Patrick Simianer", "Stefan Riezler", "Chris Dyer."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Volume 1:",
    "year": 2012
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Rethinking the inception architecture for computer vision",
    "authors": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826.",
    "year": 2016
  }, {
    "title": "Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty",
    "authors": ["Yoshimasa Tsuruoka", "Jun’ichi Tsujii", "Sophia Ananiadou"],
    "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the",
    "year": 2009
  }, {
    "title": "Continuous learning from human post-edits for neural machine translation",
    "authors": ["Marco Turchi", "Matteo Negri", "M Amin Farajian", "Marcello Federico."],
    "venue": "The Prague Bulletin of Mathematical Linguistics, 108(1):233–244.",
    "year": 2017
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
    "year": 2017
  }, {
    "title": "Hierarchical incremental adaptation for statistical machine translation",
    "authors": ["Joern Wuebker", "Spence Green", "John DeNero."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1059–1065.",
    "year": 2015
  }, {
    "title": "Model selection and estimation in regression with grouped variables",
    "authors": ["Ming Yuan", "Yi Lin."],
    "venue": "Journal of the Royal Statistical Society: Series B (Statis-",
    "year": 2006
  }],
  "id": "SP:973b8b178cdc1c32e04064570e24aed64d017df8",
  "authors": [{
    "name": "Joern Wuebker",
    "affiliations": []
  }, {
    "name": "Patrick Simianer",
    "affiliations": []
  }, {
    "name": "John DeNero",
    "affiliations": []
  }],
  "abstractText": "We propose and compare methods for gradientbased domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture—combining a state-of-the-art self-attentive model with compact domain adaptation—provides high quality personalized machine translation that is both space and time efficient.",
  "title": "Compact Personalized Models for Neural Machine Translation"
}