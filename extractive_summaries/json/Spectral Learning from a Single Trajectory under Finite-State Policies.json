{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Spectral methods of moments are a powerful tool for designing provably correct learning algorithms for latent variable models. Successful applications of this approach include polynomial-time algorithms for learning topic models (Anandkumar et al., 2012; 2014), hidden Markov models (Hsu et al., 2012; Siddiqi et al., 2010; Anandkumar et al., 2014), mixtures of Gaussians (Anandkumar et al., 2014; Hsu & Kakade, 2013), predictive state representations (Boots et al., 2011; Hamilton et al., 2014; Bacon et al., 2015; Langer et al., 2016), weighted automata (Bailly, 2011; Balle et al., 2011; Balle & Mohri, 2012; Balle et al., 2014a;b; Glaude & Pietquin, 2016), and weighted contextfree grammars (Bailly et al., 2010; Cohen et al., 2013; 2014). All these methods can be split into two classes depending on which spectral decomposition they rely on. The first class includes algorithms based on an Singular Value Decomposition (SVD) decomposition of a matrix contain-\n1Amazon Research, Cambridge, UK (work done at Lancaster University) 2Inria Lille - Nord Europe, Villeneuve d’Ascq, France. Correspondence to: Borja Balle <pigem@amazon.co.uk>, Odalric-Ambrym Maillard <odalric.maillard@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ning (estimated) moments of the target distribution (e.g. Hsu et al. (2012); Boots et al. (2011); Balle et al. (2014a)). The other class includes algorithms relying on symmetric tensor decomposition methods (e.g. Anandkumar et al. (2014); Hsu & Kakade (2013)). The advantage of tensor methods is that their output is always a proper probabilistic model. On the other hand, SVD methods, which do not always output a probabilistic model, provide learning algorithms for models which are provably non-learnable using tensor methods. A notable example is the class of stochastic weighted automata that do not admit a probabilistic parametrization (Jaeger, 2000; Denis & Esposito, 2008).\nNatural language processing (NLP) and reinforcement learning (RL) are the two main application domains of spectral learning. For example, SVD methods for learning weighted context-free grammars have proved very successful in language-related problems (Cohen et al., 2013; Luque et al., 2012). In the context of RL, efficient SVD methods for learning predictive state representations were proposed in (Boots et al., 2011; Hamilton et al., 2014). A recent application of tensor methods to RL is given in (Azizzadenesheli et al., 2016), where the authors use a spectral algorithm to obtain a PAC-RL learning result for POMDP under memory-less policies. All these results have in common that they provide learning algorithms for models over sequences. However, there is a fundamental difference between the nature of data in NLP and RL. With the exception of a few problems, most of NLP “safely” relies on the assumption that i.i.d. data from the target distribution is available. In RL, however, the most general scenario assumes that the learner can only collect a single continuous trajectory of data while all existing analysis of the SVD method for sequential models1 rely on the i.i.d. assumption (Hsu et al., 2012; Balle & Mohri, 2015; Glaude & Pietquin, 2016). Regarding tensor methods, (Azizzadenesheli et al., 2016) gave the first analysis under dependent data satisfying certain mixing conditions.\nThe purpose of this paper is to provide the first rigorous analyses of spectral SVD methods for learning sequential models from non-i.i.d. data. We provide efficient algorithms with provable guarantees for learning several sequential models from a single trajectory. Specifically, we\n1See (Thon & Jaeger, 2015) for a survey of sequential models learnable with SVD methods.\nconsider three models: probabilistic automata, stochastic weighted automata, and PSRs under control from a finitestate policy. The first two results extend existing results in the literature for i.i.d. data (Hsu et al., 2012; Balle et al., 2014a). The last result is an analog of the environment learning result for POMDP – not the whole RL result – of (Azizzadenesheli et al., 2016), with the difference that our analysis provides guarantees under a much larger set of policies (finite-state, as opposed to memory-less). This result can also be interpreted as an extension of the batchbased PSR learning algorithm from (Boots et al., 2011) to the non-i.i.d. case, although they do not provide finitesample guarantees. Our analysis is especially relevant since the single trajectory spectral algorithm we analyze has been used previously without an explicit instantiation or analysis. For example, (Kulesza et al., 2015; Shaban et al., 2015) present experiments with datasets containing single or few long trajectories which are broken into short subsequences and given as input to an spectral algorithm designed for i.i.d. data. A more detailed list of our contributions is as follows:\n(1) A single-trajectory spectral-learning algorithm for probabilistic automata whose sample complexity depends on the mixing properties of the target automaton (Section 3).\n(2) An extension of this result showing that the same algorithm also learns stochastic weighted automata (Section 4). In this case the analysis is more involved, and requires a novel notion of mixing for stochastic weighted automata and tools from the theory of linearinvariant cones (Berman & Plemmons, 1994).\n(3) A generalization of the algorithm that learns reactive PSR controlled by a finite-state stochastic policy (Section 5). We provide for this algorithm finite-sample bounds under a simple exploration assumption.\nThe most important tool in our analysis is a concentration inequality for functions of dependent random variables. These inequalities depend on the mixing coefficients of the underlying process. We provide technical estimates for the relevant mixing coefficients in each of the three cases listed above. Our goal for future work is to extend (3) to prove a PAC-RL for PSR under finite-state policies. We also think that the tools we develop to prove (2) can be used to improve the sample complexity of algorithms for learning stochastic weighted automata in the i.i.d. case.\nIn Section 2, we start by recalling several facts about weighted automata, spectral learning, and mixing that will play a role in the sequel. For space reasons, most of our proofs are deferred to the Supplementary Material."
  }, {
    "heading": "2. Background",
    "text": "Let Σ be a finite alphabet, Σ? denote the set of words of finite length on Σ, Σω the set of all infinite words on Σ, and be the empty word. Given two sets of words U ,V ⊂ Σ? we write U · V to denote the set of words {uv|u∈U , v∈V} obtained by concatenating all words in U with all words in V . Let P(Σω) be the set of probability distributions over Σω . A member ρ ∈ P(Σω) is called a stochastic process and a random infinite word ξ∼ρ is called a trajectory.\nWeighted and probabilistic automata A weighted finite automaton (WFA) with n states is a tuple A = 〈α, β, {Aσ}σ∈Σ〉 where α, β∈Rn are vectors of initial and final weights respectively, and Aσ ∈Rn×n are matrices of transition weights. A weighted automaton A computes a function fA : Σ? → R given by fA(w) = α>Awβ where Aw = Aw1 · · ·Awt for w = w1 · · ·wt. A WFA is minimal if there does not exist another WFA with less states computing the same function. A WFA A is stochastic if there exists a stochastic process ρ such that for every w ∈ Σ?, fA(w) = P[ξ ∈ wΣω]; that is, A provides a representation for the probabilities of prefixes under the distribution of ρ. A weighted automaton is irreducible if the labelled directed graph with n vertices obtained by adding a transition from i to j with label σ whenever Aσ(i, j) 6= 0 is strongly connected. It can be shown that irreducibility implies minimality, and that almost all WFA are irreducible in the sense that the set of irreducible WFA are dense on the set of all WFA (Balle et al., 2017).\nA probabilistic finite automaton (PFA) is a stochastic WFA A = 〈α, β, {Aσ}〉 where the weights have a probabilistic interpretation. Namely, α is a probability distribution over [n], Aσ(i, j) is the probability of emitting symbol σ and transitioning to state j starting from state i, and β(i) = 1 for all i ∈ [n]. It is immediate to check that a PFA satisfying these conditions induces a stochastic process. However not all stochastic WFA admit an equivalent PFA (Jaeger, 2000; Denis & Esposito, 2008).\nIf A is a PFA, then the matrix A = ∑ σ∈ΣAσ yields the Markov kernel A(i, j) = P[j | i] on the state space [n] after marginalizing over the observations. It is easily checked that A is row-stochastic, and thus Aβ = β. Furthermore, for every distribution α0 ∈ Rn over [n] we have α>0 A = α1 for some other probability distribution α1 over [n]. In the case of PFA irreducibility coincides with the usual concept of irreducibility of the Markov chain induced by A.\nHankel matrices and spectral learning The Hankel matrix of a function f : Σ? → R is the infinite matrix Hf ∈ RΣ\n?×Σ? with entries Hf (u, v) = f(uv). Given finite sets U ,V ⊂ Σ?, HU,Vf ∈RU×V denotes the restriction of matrix Hf to prefixes in U and suffixes in V . Fliess’ theorem (Fliess, 1974) states that a Hankel matrix\nHf has finite rank n if and only if there exists a WFA A with n states such that f =fA. This implies that a WFA A with n states is minimal if and only if n = rank(HfA). The spectral learning algorithm for WFA (Balle et al., 2014a) provides a mechanism for recovering such a WFA from a finite sub-block HU,Vf of Hf such that: 1) ∈ U ∩ V , 2) there exists a set U ′ such that U = U ′ ∪ (∪σ∈ΣU ′σ), 3) rank(Hf ) = rank(H U ′,V f ). A pair (U ,V) that satisfies these conditions is called a complete basis for f . The pseudo-code of this algorithm is given below:\nAlgorithm 1: Spectral Learning for WFA Input: number of states n, Hankel matrix HU,V\nFind U ′ such that U = U ′ ∪ (∪σ∈ΣU ′σ) Let H = HU ′,V Compute the rank n SVD H ≈ UDV > Let hV = H{ },V and take α = V >hV Let hU ′ = HU\n′,{ } and take β = D−1U>hU ′ foreach σ ∈ Σ do\nLet Hσ = HU ′σ,V and take Aσ = D−1U>HσV\nreturn A = 〈α, β, {Aσ}〉\nThe main strength of Algorithm 1 is its robustness to noise. Specifically, if only an approximation ĤU,V of the Hankel matrix is known, then the error between the target automaton A and the automaton Â learned from ĤU,V can be controlled in terms of the error ‖HU,V − ĤU,V‖2; see (Hsu et al., 2012) for a proof in the HMM case and (Balle, 2013) for a proof in the general WFA case. These tedious but now standard arguments readily reduce the problem of learning WFA via spectral learning to that of estimating the corresponding Hankel matrix.\nClassical applications of spectral learning assume one has access to i.i.d. samples from a stochastic process ρ. In this setting one can obtain a sample S = (ξ(1), . . . , ξ(N)) containing N finite-length trajectories from ρ, and use them to estimate a Hankel matrix ĤU,VS as follows:\nĤU,VS (u, v) = 1\nN N∑ i=1 I{ξ(i) ∈ uvΣω} .\nIf ρ = ρA for some stochastic WFA, then obviously ES [ĤU,VS ] = H U,V fA and a large sample size N will provide a good approximation ĤU,VS of H U,V fA\n. Explicit concentration bounds for Hankel matrices bounding the error ‖HU,VfA − Ĥ U,V S ‖2 can be found in (Denis et al., 2016).\nIn this paper we consider the more challenging setup where we only have access to a sample S = {ξ} of size N = 1 from ρ. In particular, we show it is possible to replace the empirical average above by a Césaro average and still use the spectral learning algorithm to recover the transition ma-\ntrices of a stochastic WFA. To obtain a finite-sample analysis of this single-trajectory learning algorithm we prove concentration results for Césaro averages of Hankel matrices. Our analysis relies on concentration inequalities for functions of dependent random variables which depend on mixing properties of the underlying process.\nMixing and concentration Let ρ ∈ P(Σω) be a stochastic process and ξ = x1x2 · · · a random word drawn from ρ. For 1 6 s < t 6 T and u ∈ Σs we let ρt:T (·|u) denote the distribution of xt · · ·xT conditioned on x1 · · ·xs = u. With this notation we define the quantity\nηt(u, σ, σ ′) = ‖ρt:T (·|uσ)− ρt:T (·|uσ′)‖TV\nfor any u ∈ Σs−1, and σ, σ′ ∈ Σ. Then the η-mixing coefficients of ρ at horizon T are given by\nηs,t = sup u∈Σs−1,σ,σ′∈Σ\nηt(u, σ, σ ′) .\nMixing coefficients are useful in establishing concentration properties of functions of dependent random variables. The Lipschitz constant of a function g : ΣT → R with respect to the Hamming distance is defined as\n‖g‖Lip = sup |g(w)− g(w′)| ,\nwhere the supremum is taken over all pairs of words w,w′ ∈ ΣT differing in exactly one symbol. The following theorem proved in (Chazottes et al., 2007; Kontorovich et al., 2008) provides a concentration inequality for Lipschitz functions of weakly dependent random variables.\nTheorem 1 Let ρ∈P(Σω) and ξ=x1x2 · · ·∼ρ. Suppose g : ΣT→R satisfies ‖g‖Lip61 and let Z=g(x1, . . . , xT ). Let ηρ = 1+max1<s<T ∑T t=s+1 ηs,t, where ηs,t are the η-mixing coefficients of ρ at horizon T . Then the following holds for any ε > 0:\nPξ [Z − EZ > εT ] 6 exp ( −2ε2T η2ρ ) ,\nwith an identical bound for the other tail.\nTheorem 1 shows that the mixing coefficient ηρ is a key quantity in order to control the concentration of a function of dependent variables. In fact, upper-bounding ηρ in terms of geometric ergodicity coefficients of a latent variable stochastic process enables (Kontorovich & Weiss, 2014) to analyze the concentration of functions of HMMs and (Azizzadenesheli et al., 2016) to provide PAC guarantees for an RL algorithm for POMDP based on spectral tensor decompositions. Our Lemma 2 uses a similar but more refined bounding strategy that directly applies when the transition and observation processes are not conditionally independent. Lemma 4 refines this strategy further to control ηρ for stochastic WFA (for which there may be no underlying Markov stochastic process in general). To the best of our knowledge this yields the first concentration results for the challenging setting of stochastic WFA."
  }, {
    "heading": "3. Single-Trajectory Spectral Learning of PFA",
    "text": "In this section we focus on the problem of learning the transition structure of a PFA A using single trajectory is generated by A. We provide a spectral learning algorithm for this problem and a finite-sample analysis consisting of a concentration bound for the error on the Hankel matrix estimated by the algorithm. We assume the learner has access to a single infinite-length trajectory ξ ∼ ρA that is progressively uncovered. The algorithm uses a length t prefix from ξ to estimate a Hankel matrix whose entries are Césaro averages. This Hankel matrix is then processed by the usual spectral learning algorithm to recover an approximation to an automaton with transition weights equivalent to those of A. We want to analyze the quality of the model learned by the algorithm after observing the first t symbols from ξ.\nWe start by showing that Césaro averages provide a consistent mechanism for learning the transition structure of A. Then we proceed to analyze the accuracy of the Hankel estimation step. As discussed in Section 2, this is enough to obtain finite-sample bounds for learning PFA. The general case of stochastic WFA is considered in Section 4."
  }, {
    "heading": "3.1. Learning with Césaro Averages is Consistent",
    "text": "Let A = 〈α, β, {Aσ}〉 be a PFA computing a function fA : Σ?→R and defining a stochastic process ρA∈P(Σω). For convenience we drop the subscript and just write f and ρ. Since we only have access to a single trajectory ξ from ρwe cannot obtain an approximation of the Hankel matrix for f by averaging over multiple i.i.d. trajectories. Instead, we compute Césaro averages over the trajectory ξ to obtain a Hankel matrix whose expectation is related to A as follows. For any t ∈ N let f̄t : Σ? → R be the function given by f̄t(w) = (1/t) ∑t−1 s=0 f(Σ\nsw), where f(Σsw) =∑ u∈Σs f(uw). We shall sometimes write fs(w) = f(Σsw). Using the definition of the function computed by a WFA it is easy to see that∑\nu∈Σs f(uw) = ∑ u∈Σs α>AuAwβ = α >AsAwβ ,\nwhere A= ∑ σ Aσ is the Markov kernel on the state space\nof A. Thus, introducing ᾱ>t = (1/t) ∑t−1 s=0 α\n>As we get f̄t(w) = ᾱ > t Awβ. Since α is a probability distribution, A is a Markov kernel, and probability distributions are closed by convex combinations, then ᾱt is also a probability distribution over [n]. Thus, we have just proved the following:\nLemma 1 (Consistency) The Césaro average of f over t steps, f̄t, is computed by the probabilistic automaton Āt = 〈ᾱt, β, {Aσ}〉. In particular, A and Āt have the same number of states and the same transition probability matrices. Furthermore, if A is irreducible then Āt is minimal.\nThe irreducibility claim follows from (Balle et al., 2017).\nFor convenience, in the sequel we write H̄U,Vt for the (U ,V)-block of the Hankel matrix HfĀt .\nRemark 1 The irreducible condition simply ensures there is a unique stationary distribution, and that the Hankel matrix of Āt has the same rank as the Hankel matrix of A (otherwise it could be smaller)."
  }, {
    "heading": "3.2. Spectral Learning Algorithm",
    "text": "Algorithm 2 describes the estimation of the empirical Hankel matrix ĤU,Vt,ξ from the first t+L symbols of a single trajectory using the corresponding Césaro averages. To avoid cumbersome notations, in the sequel we may drop super and subscripts when not needed and write Ĥt or Ĥ when U , V , and ξ are clear from the context. Note that by Lemma 1 the expectation E[Ĥ] over ξ∼ρ is equal to the Hankel matrix H̄t of the function f̄t computed by the PFA Āt.\nAlgorithm 2: Single Trajectory Spectral Learning (Generative Case)\nInput: number of states n, length t, prefixes U ⊂ Σ?, suffixes V ⊂ Σ?\nLet L = maxw∈U·V |w| Sample trajectory ξ = x1x2 · · ·xt+L · · · ∼ ρ foreach u ∈ U and v ∈ V do\nLet Ĥ(u, v) = 1t ∑t−1 s=0 I{xs+1:s+|uv| = uv}\nApply the spectral algorithm to Ĥ with rank n"
  }, {
    "heading": "3.3. Concentration Results",
    "text": "Now we proceed to analyze the error Ĥt − H̄t in the Hankel matrix estimation inside Algorithm 2. In particular, we provide concentration bounds that depend on the length t, the mixing coefficient ηρ of the process ρ, and the structure of the basis (U ,V). The main result of this section is the matrix-wise concentration bound Theorem 3 where we control the spectral norm of the error matrix. For comparison we also provide a simpler entry-wise bound and recall the equivalent matrix-wise bound in the i.i.d. setting.\nBefore trying to bound the concentration of the errors using Theorem 1 we need to analyze the mixing coefficient of the process generated by a PFA. This is the goal of the following result, whose proof is provided in Appendix A.\nLemma 2 (η-mixing for PFA) Let A be PFA and assume that it is (C, θ)-geometrically mixing in the sense that for some constants C > 0, θ ∈ (0, 1) we have\n∀t ∈ N, µAt = sup α,α′ ‖αAt − α′At‖1 ‖α− α′‖1 6 Cθt ,\nwhere the supremum is over all probability vectors. Then we have ηρA 6 C/(θ(1− θ)).\nRemark 2 A sufficient condition for the geometric control of µAt is that A admits a spectral gap. In this case θ can be chosen to be the modulus of the second eigenvalue |λ2(A)| < 1 of the transition kernel A.\nBefore the main result of this section we provide a concentration result for each individual entry of the estimated Hankel matrix as a warmup (see Appendix D).\nTheorem 2 (Single-trajectory, entry-wise) Let A be a (C, θ)-geometrically mixing PFA and ξ ∼ ρA a trajectory of observations. Then for any u∈U , v∈V and δ∈(0, 1),\nP [ ĤU,Vt,ξ (u, v)−H̄ U,V t (u, v) >\n|uv|C θ(1− θ)\n√( 1 + |uv| − 1\nt\n) log(1/δ) 2t ] 6 δ ,\nwith an identical bound for the other tail.\nA naive way to handle the concentration of the whole Hankel matrix is to control the Frobenius norm ‖Ĥt− H̄t‖F by taking a union bound over all entries using Theorem 2. However, the resulting concentration bound would scale as√ |U||V|. To have better dependency with the dimension (the matrix has dimension |U| × |V|) can split the empirical Hankel matrix Ĥ into blocks containing strings of the same length (as suggested by the dependence of the bound above on |uv|). We thus introduce the maximal length L = maxw∈U·V |w|, and the set U` = {u ∈ U : |u| = `} for any ` ∈ N. We use these to define the quantity nU = |{` ∈ [0, L] : |U`| > 0}|, and introduce likewise V`, nV with obvious definitions. With this notation we can now state the main result of this section.\nTheorem 3 (Single-trajectory, matrix-wise) Let A be as in Theorem 2. Let m = ∑ u∈U,v∈V f̄t(uv) be the probability mass and d = min{|U||V|, 2nUnV} be the effective dimension. Then, for all δ ∈ (0, 1) we have\nP [ ‖ĤU,Vt,ξ −H̄ U,V t ‖2 > ( √ L+ √ 2C\n1− θ\n)√ 2m\nt\n+ 2LC\nθ(1−θ)\n√( 1+\nL−1 t )d ln(1/δ) 2t ] 6 δ .\nRemark 3 Note that quantity nUnV in d can be exponentially smaller than |U||V|. Indeed, for U = V = Σ6L/2 we have |U||V| = Θ(|Σ|L) while nUnV = Θ(L2).\nFor comparison, we recall a state-of-the-art concentration bound for estimating the Hankel matrix of a stochastic language2 from N i.i.d. trajectories.\n2A stochastic language is a probability distribution over Σ?.\nTheorem 4 (Theorem 7 in (Denis et al., 2014)) Let A be a stochastic WFA with stopping probabilities and S = (ξ(1), . . . , ξ(N)) be an i.i.d. sample of size N from the distribution ρA ∈ P(Σ?). Let m = ∑ u∈U,v∈V fA(uv). Then, for all c > 0 we have\nP [ ‖ĤU,VS −H U,V fA ‖2 > √ 2cm\nN +\n2c\n3N\n] 6\n2c\nec − c− 1 ."
  }, {
    "heading": "3.4. Sketch of the Proof of Theorem 3",
    "text": "In this section we sketch the main steps of the proof of Theorem 3 (the full proof is given in Appendices A and D). We focus on highlighting the main difficulties and paving the path for the extension of Theorem 3 to stochastic WFA given in Section 4.\nThe key of the proof is to study the function g(ξ) = ‖ĤU,Vt,ξ − H̄ U,V t ‖2, in view of applying Theorem 1. To this end, we first control the η-mixing coefficients using Lemma 2. The next step is to control the Lipschitz constant ‖g‖Lip. This part is not very difficult and we derive after a few careful steps the bound ‖g‖Lip 6 L √ d/t.\nThe second and most interesting part of the proof is about the control of E[g(ξ)]. Let us give some more details.\nDecomposition step We control ‖ĤU,Vt − H̄ U,V t ‖2 by its Frobenius norm and get\nE[‖ĤU,Vt −H̄ U,V t ‖2]2 6 ∑ w∈U·V |w|U,VE[(f̂t(w)−f̄t(w))2] ,\nwhere we introduced |w|U,V = |{(u, v) ∈ U × V : uv = w}|, and f̂t(w) = 1t ∑t s=1bs(w) using the shorthand notation bs(w) = I{xs . . . xs+|w|−1 = w}. Also, f̄t(w) = E[f̂t(w)] = 1t ∑t s=1 fs(w), where fs(w) = ρA(Σ s−1wΣω). This implies that we have a sum of variances, where each of the terms can be written as\nE[(f̂t(w)− f̄t(w))2] =\n1 t2 E ( t∑ s=1 bs(w) )2− 1 t2 ( t∑ s=1 fs(w) )2 .\nSlicing step An important observation is that each probability term satisfies fs(w) = α>As−1Awβ because of the PFA assumption on ρA. Furthermore, it follows from A being a PFA that ∑ |w|=l fs(w) = 1 for all s and l. This suggests that we group the terms in the sum over W = U ·V by length, so we writeWl =W∩Σl and define Ll = maxw∈Wl |w|U,V the maximum number of ways to write a string of length l inW as a concatenation of a prefix in U and a suffix in V . Note that we always have Ll 6 l+1.\nA few more steps lead to the following bound\nE[‖ĤU,Vt − H̄ U,V t ‖2]2 6 (1)\n1\nt2 ∞∑ l=0 ∑ w∈Wl |w|U,V [ t∑ s=1 (1−fs(w))fs(w)\n+2 ∑\n16s<s′6t\n( E[bs(w)bs′(w)]−fs(w)fs′(w) )] .\nWe control the first term in (1) using ∞∑ l=0 ∑ w∈Wl |w|U,V t∑ s=1 (1−fs(w))fs(w)6 t ∑ u∈U,v∈V f̄t(uv) .\nCross terms Regarding the remaining “cross”-term in (1) we fix w ∈ Wl and obtain the equation\nE[bs(w)bs′(w)]− fs(w)fs′(w)\n= α>s−1 ( As ′−s w −Awβα>s′−1Aw ) β , (2)\nwhere we introduced the vectors α>s =α >As and transition matrixAs ′−s w = ∑ x∈Σs′−sw Ax corresponding to the “event” Σs ′−s w =wΣ s′−s ∩ Σs′−sw. We now discuss two cases.\nFirst control If s′− s < l, we use the simplifying fact that Σs ′−s w ⊂ wΣs ′−s to upper bound (2) by\nα>s−1Aw ( As ′−s − βα>s′−1Aw ) β\n= fs(w)(1− fs′(w)) 6 fs(w) .\nSecond control When s′−s > |w| = l we have Σs′−sw = wΣs ′−s−lw and As ′−s w =AwA\ns′−s−lAw. Thus, we rewrite (2) and bound it using Hölder’s inequality as follows:\nα>s−1Aw ( As ′−s−l−βα>s′−1 ) Awβ 6 (3)\n‖α>s−1Aw‖1‖As ′−s−l − βα>s′−1‖∞‖Awβ‖∞ .\nUsing Lemma 6 in Appendix A we bound the induced norm as ‖As′−s−l − βα>s′−1‖∞ 6 2µAs′−s−l, where µAt is the mixing coefficient defined in Lemma 2. Also, it holds that ‖Awβ‖∞ 6 1. Finally, since α>s−1Aw is a sub-distribution over states, we have the key equalities∑ w∈Wl |w|U,V‖α>s−1Aw‖1 = ∑ w∈Wl |w|U,Vα>s−1Awβ (4)\n= ∑ w∈Wl |w|U,Vfs(w) = ∑ u∈U,v∈V:uv∈Wl fs(uv) .\nThe proof is concluded by collecting the previous bounds, plugging them into (1), and using Lemma 2 to get\nE[g(ξ)]2 6 (\n2L− 1 + 4C 1− θ\n) m\nt . (5)"
  }, {
    "heading": "4. Extension to Stochastic WFA",
    "text": "We now generalize the results in previous section to the case where the distribution over ξ is generated by a stochastic weighted automaton that might not have a probabilistic representation. The key observation is that Algorithm 2 can learn stochastic WFA without any change, and the consistency result in Lemma 1 extends verbatim to stochastic WFA. However, the proof of the concentration bound in Theorem 3 requires further insights into the mixing properties of stochastic WFA. Before describing the changes required in the proof, we discuss some important geometric properties of stochastic WFA."
  }, {
    "heading": "4.1. The State-Space Geometry of SWFA",
    "text": "Recall that a stochastic WFA (SWFA) A = 〈α, β, {Aσ}〉 defines a stochastic process ρA and computes a function fA such that fA(w) = P[ξ ∈ wΣω], where ξ ∼ ρA. It is immediate to check that this implies that the weights of A satisfy the properties: (i) α>Axβ > 0 for all x ∈ Σ?, and (ii) α>Atβ = ∑ |w|=t α\n>Awβ = 1 for all t > 0, where A = ∑ σ∈ΣAσ . Without loss of generality we assume throughout this section that A is a minimal SWFA of dimension n, meaning that any SWFA computing the same probability distribution than A must have dimension at least n. Importantly, the weights in α, β, and Aσ are not required to be non-negative in this definition. Nonetheless, it follows from these properties that β is an eigenvector of A of eigenvalue 1 exactly like in the case of PFA. We now introduce further facts about the geometry of SWFA.\nA minimal SWFA A is naturally associated with a proper (i.e. pointed, closed, and solid) cone in K⊂Rn called the backward cone (Jaeger, 2000), and characterized by the following properties: 1) β∈K, 2) AσK⊆K for all σ∈Σ, and 3) α>v>0 for all v∈K. Condition 2) says that every transition matrix Aσ leaves K invariant, and in particular the backward vector Awβ belongs to K for all w ∈ Σ?. The vector of final weights β plays a singular role in the geometry of the state space of a SWFA. This follows from facts about the theory of invariant cones (Berman & Plemmons, 1994) which provides a generalization of the classical Perron–Frobenius theory of non-negative matrices to arbitrary matrices. We recall from (Berman & Plemmons, 1994) that a norm on Rn can be associated with every vector in the interior of K. In particular, we will take the norm associated with the final weights β ∈ K. This norm, denoted by ‖ · ‖β , is completely determined by its unit ball Bβ = {v ∈ Rn : −β 6K v 6K β}, where u 6K v means v − u∈K. In particular, ‖v‖β = inf{r > 0 : v ∈ rBβ}. Induced and dual norms are derived from ‖ · ‖β as usual. When A is a PFA one can takeK to be the cone of vectors in Rn with non-negative entries, in which case β = (1, . . . , 1) and ‖ · ‖β reduces to ‖ · ‖∞ (Berman & Plemmons, 1994). The following result shows that ‖ · ‖β provides the right\ngeneralization to SWFA of the norm ‖·‖∞ used in the Second control step of the proof for PFA (see Appendix B).\nLemma 3 For any w ∈ Σ?: (i) ‖Awβ‖β 6 1, and (ii) ‖α>Aw‖β,∗ = α>Awβ.\nIt is also natural to consider mixing coefficients for stochastic processes generated by SWFA in terms of the dual βnorm. This provides a direct analog to Lemma 2 for PFA:\nLemma 4 (η-mixing for SWFA) Let A be SWFA and assume that it is (C, θ)-geometrically mixing in the sense that for some C > 0, θ ∈ (0, 1),\nµAt = sup α0,α1:α>0 β=α > 1 β=1 ‖α>0 At − α>1 At‖β,? ‖α0 − α1‖β,? 6 Cθt .\nThen the η-mixing coefficient satisfies ηρA 6 C/θ(1− θ).\nRemark 4 A sufficient condition for the geometric control of µAt is that A admits a spectral gap. In this case θ can be chosen to be the modulus of the second eigenvalue |λ2(A)|<1 of A. Another sufficient condition is that θ=γβ(A)<1, where\nγβ(A) = sup { ||Aν||β,? ||ν||β,? : ν s.t. ||ν||β,? 6= 0, ν>β = 0 } ."
  }, {
    "heading": "4.2. Concentration of Hankel Matrices for SWFA",
    "text": "We are now ready to extend the proof of Theorem 3 to SWFA. Using that both PFA and SWFA define probability distributions over prefixes it follows that any argument in Section 3.4 that only appeals to the function computed by the automaton can remain unchanged. Therefore, the only arguments that need to be revisited are described in the Second control step. In particular, we must provide versions of (3) and (4) for SWFA.\nRecalling that Hölder’s inequality can be applied with any pair of dual norms, we start by replacing the norms ‖ · ‖∞ and ‖ · ‖1 in (3) with the cone-norms ‖ · ‖β and ‖ · ‖β,? respectively. Next we use Lemma 3 to obtain, for any w ∈ Σ?, the bound ‖Awβ‖β 6 1 and the equation ‖α>Aw‖β,∗ = α>Awβ which are direct analogs of the results used for PFA. Then it only remains to relate the β-norm of As\n′−s−l − βα>s′−1 to the mixing coefficients µAt . Applying Lemma 8 in Appendix A yields ‖As′−s−l − βα>s′−1‖β 6 2µAs′−s−l. Thus we obtain for SWFA exactly the same concentration result that we obtained for empirical Hankel matrices estimated from a single trajectory of observations generated by a PFA.\nTheorem 5 (Single-trajectory, SWFA) Let A be a SWFA that is (C, θ)-geometrically mixing with the definition in Lemma 4. Then the concentration bound in Theorem 3 also holds for trajectories ξ ∼ ρA."
  }, {
    "heading": "5. The Controlled Case",
    "text": "This section describes the final contribution of the paper: a generalization of our analysis of spectral learning from a single trajectory the case of dynamical systems under finite-state control. We consider discrete-time dynamical systems with finite set of observations O and finite set of actions A, and let Σ =O×A. We assume the learner has access to a single trajectory ξ=(ot, at)t>1 in Σω . The trajectory is generated by coupling an environment defining a distribution over observations conditioned on actions and a policy defining a distribution over actions conditioned on observations. Assuming the joint action-observation distribution can be represented by a stochastic WFA is equivalent to saying that the environment corresponds to a POMDP or PSR, and the policy has finite memory. To fix some notation we assume the environment is represented by a conditional3 stochastic WFA A = 〈α, β, {Aσ}〉 with n states. This implies the semantics fA(w) = P[o1 · · · ot|a1 · · · at] for the function computed by A, where w = w1 · · ·wt with wi = (oi, au). For any w ∈ Σ? we shall write wA = a1 · · · at andwO = o1 · · · ot. We also assume there is a stochastic policy π represented by a conditional PFA Aπ = 〈απ, βπ, {Πσ}〉 with k states; that is, fAπ (w) = π(w\nA|wO) = P[a1 · · · at|o1 · · · ot]. In particular, Aπ represents a stochastic policy that starts in a state s1 ∈ [k] sampled according to απ(i) = P[s1 = i], and at each time step samples an action and changes state according to Πo,a(i, j) = P[st+1 = j, at = a|ot = o, st = i]. The trajectory ξ observed by the learner is generated by the stochastic process ρ ∈ P(Σω) obtained by coupling A and Aπ . A standard construction in the theory of weighted automata (Berstel & Reutenauer, 1988) shows that this process can be computed by the product automaton B = A⊗Aπ = 〈α⊗, β⊗, {Bσ}〉, where α⊗ = α⊗απ , β⊗ = β ⊗ βπ , and Bo,a = Ao,a ⊗Πo,a. It is easy to verify that B is a stochastic WFA with nk states computing the function fB(w) = fA(w)fAπ (w) = P[ξ ∈ wΣω]. At this point, the spectral algorithm from Section 4 could be used to learn B directly from a trajectory ξ∼ ρB. However, since the agent interacting with environment A knows the policy π, we would like to leverage this information to learn directly a model of the environment. This approach is formalized in Algorithm 3, which provides a singletrajectory version of the algorithm in (Bowling et al., 2006) for learning PSR from non-blind policies with i.i.d. data. The main difference with Algorithm 2 is that in the reactive case we need a smoothing parameter κ that will prevent the entries in the empirical Hankel matrix Ĥ to grow unboundedly plus that the policy π satisfies an exploration assumption. κ plays a similar role in our analysis as the smoothing parameter introduced in (Denis et al., 2014) for learning\n3Such WFA are also called reactive predictive state representations in the RL literature.\nAlgorithm 3: Single Trajectory Spectral Learning (Reactive Case)\nInput: number of states n, length t, U ,V⊂(A×O)?, policy π, smoothing coefficient κ\nLet L = maxw∈U·V |w| Sample trajectory o1a1o2a2 · · · ot+Lat+L using π foreach u ∈ U and v ∈ V do\nLet Ĥ(u, v) = 1 t ∑t−1 s=0 I{os+1as+1···os+|uv|as+|uv|=uv} κsπ(a1···as+|uv||o1···os+|uv|)\nApply the spectral algorithm to Ĥ with rank n\nstochastic languages from factor estimates in the i.i.d. case. The difference is that in our case the smoothing parameter must satisfy κε > 1, where ε is the exploration probability of the policy π provided by the following assumption.\nAssumption 1 (Exploration) There exists some ε > 0 such that for each w ∈ Σ? the policy π satisfies π(wA|wO) > ε|w|. In particular, at every time step each action a ∈ A is picked with probability at least ε.\nBefore moving to the next section, where we provide finitesample concentration results for the Hankel matrix estimated by Algorithm 3, we show that Algorithm 3 is consistent, that is it learns in expectation a WFA whose transition matrices are equivalent to those of the environment A. The proof of the following lemma is provided in Appendix E.\nLemma 5 The Hankel matrix Ĥ = ĤU,Vt,ξ computed in Algorithm 3 satisfies E[ĤU,Vt,ξ ] = H̃ U,V t , where H̃ U,V t is a block of the Hankel matrix corresponding to the stochastic WFA Ãt=〈α̃t, β, {Aσ}〉 where we introduced the modified vector α̃t=(1/t) ∑t−1 s=0 α\n>(A/κ)s. We denote by f̃t the function computed by Ãt."
  }, {
    "heading": "5.1. Concentration Results",
    "text": "Broadly speaking, a concentration bound for the estimation error ‖ĤU,Vt,ξ −H̃ U,V t ‖2 can be obtained by following a proof strategy similar to the ones used in Theorems 3 and 5. However, almost all the bounds used in the previous proofs need to be reworked to account for (i) the effect of the extra dependencies introduced by the policy π, and (ii) the fact that the target automaton A to be learned is not a stochastic WFA in the sense of Section 4 but rather a conditional stochastic WFA.\nPoint (i) is addressed in our proof by introducing a “normalized” reference process ρĀ corresponding to the coupling Ā = A ⊗ Aunif between the environment A and the uniform random policy that at each step takes each action independently with probability 1/|A|. Assuming the smoothing parameter satisfies κε> 1 for some exploration\nparameter ε (cf. Assumption 1), then 1/κ 6 1/|A|. This observation is used, for example, to bound some variance terms in E[g(ξ)] by replace occurrences of f̃t with f̃unift , the function computed by taking the Césaro average of the first t steps of Ā. Ultimately, this makes our bound depend not only on the mixing properties of ρB, but also on those of the normalized process ρĀ induced by the SWFA Ā. Incidentally, this argument is also used to address point (ii): by bounding quantities involving κ by quantities computed by a SWFA we can use again the arguments sketched in Section 4.1.\nPursuing the ideas above, and assuming that Ā is (C̄, θ̄)geometrically mixing, we obtain the following bound which can be compared to the one in (5):\nE[g(ξ)]2 6 m̃\ntεL(1− 1/(κε)2) +\n2m̄\ntε2L\n( L+ C̄\n1− θ̄\n) ,\nwhere L = maxw∈U·V |w|, m̃ = ∑ u∈U,v∈V f̃t(uv), and\nm̄ = ∑ u∈U,v∈V f̃ unif t (uv).\nTheorem 6 Suppose that B is (C, θ)-geometrically mixing and Ā is (C̄, θ̄)-geometrically mixing. Suppose π satisfies Assumption 1 and the smoothing coefficient κ satisfies κε > 1. Let d = ∑ w∈U·V |w|U,V , and define L, m̃, m̄ as above. Then for any δ ∈ (0, 1) we have\nP [ ‖ĤU,Vt,ξ − H̃ U,V t ‖2 > √ m̃\ntεL(1− κ−2ε−2) +√\n2m̄\ntε2L\n( L+ C̄\n1−θ̄\n) +\nC\nθ(1−θ)εL\n√ 2d ln(1/δ)\nt\n] 6 δ .\nOn a final note we remark that the dependence on εL might be unavoidable due to inherent increase in variance produced by importance sampling estimators."
  }, {
    "heading": "6. Conclusion",
    "text": "We present the first rigorous analysis of single-trajectory SVD-based spectral learning algorithms for sequential models with latent variables. Our analysis highlights the role of mixing properties of WFA and their relation with the geometry of the underlying state space. In the controlled case we obtain a result for control with finite-state policies, a much more general class than previously considered memoryless policies. In future work we will use our results to get upper confidence bounds on the predictions made by the learned environment with the goal of solving the full RL problem for PSR with complex control policies."
  }, {
    "heading": "Acknowledgements",
    "text": "O.-A. M. acknowledges the support of the French Agence Nationale de la Recherche (ANR), under grant ANR-16CE40-0002 (project BADASS)."
  }],
  "year": 2017,
  "references": [{
    "title": "A spectral algorithm for latent dirichlet allocation",
    "authors": ["Anandkumar", "Anima", "Foster", "Dean P", "Hsu", "Daniel J", "Kakade", "Sham M", "Liu", "Yi-Kai"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Tensor decompositions for learning latent variable models",
    "authors": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel J", "Kakade", "Sham M", "Telgarsky", "Matus"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Reinforcement learning of pomdps using spectral methods",
    "authors": ["Azizzadenesheli", "Kamyar", "Lazaric", "Alessandro", "Anandkumar", "Animashree"],
    "venue": "In 29th Annual Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Learning and planning with timing information in markov decision processes",
    "authors": ["Bacon", "Pierre-Luc", "Balle", "Borja", "Precup", "Doina"],
    "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "A spectral approach for probabilistic grammatical inference on trees",
    "authors": ["R. Bailly", "A. Habrard", "F. Denis"],
    "venue": "In Algorithmic Learning Theory, pp",
    "year": 2010
  }, {
    "title": "Quadratic weighted automata: Spectral algorithm and likelihood maximization",
    "authors": ["Bailly", "Raphaël"],
    "venue": "In Proceedings of the 3rd Asian Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Learning Finite-State Machines: Algorithmic and Statistical Aspects",
    "authors": ["B. Balle"],
    "venue": "PhD thesis, Universitat Politècnica de Catalunya,",
    "year": 2013
  }, {
    "title": "Spectral learning of general weighted automata via constrained matrix completion",
    "authors": ["Balle", "Borja", "Mohri", "Mehryar"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2012
  }, {
    "title": "Learning weighted automata",
    "authors": ["Balle", "Borja", "Mohri", "Mehryar"],
    "venue": "In International Conference on Algebraic Informatics,",
    "year": 2015
  }, {
    "title": "A spectral learning algorithm for finite state transducers",
    "authors": ["Balle", "Borja", "Quattoni", "Ariadna", "Carreras", "Xavier"],
    "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
    "year": 2011
  }, {
    "title": "Spectral learning of weighted automata",
    "authors": ["Balle", "Borja", "Carreras", "Xavier", "Luque", "Franco M", "Quattoni", "Ariadna"],
    "venue": "Machine learning,",
    "year": 2014
  }, {
    "title": "Methods of moments for learning stochastic languages: Unified presentation and empirical comparison",
    "authors": ["Balle", "Borja", "Hamilton", "William L", "Pineau", "Joelle"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Bisimulation metrics for weighted automata",
    "authors": ["Balle", "Borja", "Gourdeau", "Pascale", "Panangaden", "Prakash"],
    "venue": "In 44rd International Colloquium on Automata, Languages, and Programming,",
    "year": 2017
  }, {
    "title": "Nonnegative matrices in the mathematical sciences",
    "authors": ["Berman", "Abraham", "Plemmons", "Robert J"],
    "year": 1994
  }, {
    "title": "Rational Series and Their Languages",
    "authors": ["Berstel", "Jean", "Reutenauer", "Christophe"],
    "year": 1988
  }, {
    "title": "Closing the learning-planning loop with predictive state representations",
    "authors": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"],
    "venue": "The International Journal of Robotics Research,",
    "year": 2011
  }, {
    "title": "Learning predictive state representations using non-blind policies",
    "authors": ["Bowling", "Michael", "McCracken", "Peter", "James", "Neufeld", "Wilkinson", "Dana"],
    "venue": "In Proceedings of the 23rd international conference on Machine learning,",
    "year": 2006
  }, {
    "title": "Concentration inequalities for random fields via coupling",
    "authors": ["Chazottes", "J-R", "Collet", "Pierre", "Külske", "Christof", "Redig", "Frank"],
    "venue": "Probability Theory and Related Fields,",
    "year": 2007
  }, {
    "title": "Experiments with spectral learning of latentvariable PCFGs",
    "authors": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"],
    "venue": "In Proceedings of NAACL,",
    "year": 2013
  }, {
    "title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity",
    "authors": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Dimension-free concentration bounds on hankel matrices for spectral learning",
    "authors": ["Denis", "François", "Gybels", "Mattias", "Habrard", "Amaury"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "On rational stochastic languages",
    "authors": ["Denis", "François", "Esposito", "Yann"],
    "venue": "Fundamenta Informaticae,",
    "year": 2008
  }, {
    "title": "Dimension-free concentration bounds on hankel matrices for spectral learning",
    "authors": ["Denis", "François", "Gybels", "Mattias", "Habrard", "Amaury"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Matrices de Hankel",
    "authors": ["M. Fliess"],
    "venue": "Journal de Mathématiques Pures et Appliquées,",
    "year": 1974
  }, {
    "title": "Pac learning of probabilistic automaton based on the method of moments",
    "authors": ["Glaude", "Hadrien", "Pietquin", "Olivier"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Efficient learning and planning with compressed predictive states",
    "authors": ["Hamilton", "William L", "Fard", "Mahdi Milani", "Pineau", "Joelle"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions",
    "authors": ["Hsu", "Daniel", "Kakade", "Sham M"],
    "venue": "In Innovations in Theoretical Computer Science,",
    "year": 2013
  }, {
    "title": "A spectral algorithm for learning hidden markov models",
    "authors": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 2012
  }, {
    "title": "Observable operator models for discrete stochastic time series",
    "authors": ["Jaeger", "Herbert"],
    "venue": "Neural Computation,",
    "year": 2000
  }, {
    "title": "Uniform chernoff and dvoretzky-kiefer-wolfowitz-type inequalities for markov chains and related processes",
    "authors": ["Kontorovich", "Aryeh", "Weiss", "Roi"],
    "venue": "Journal of Applied Probability,",
    "year": 2014
  }, {
    "title": "Concentration inequalities for dependent random variables via the martingale method",
    "authors": ["Kontorovich", "Leonid Aryeh", "Ramanan", "Kavita"],
    "venue": "The Annals of Probability,",
    "year": 2008
  }, {
    "title": "Geometric ergodicity and the spectral gap of non-reversible markov chains",
    "authors": ["Kontoyiannis", "Ioannis", "Meyn", "Sean P"],
    "venue": "Probability Theory and Related Fields,",
    "year": 2012
  }, {
    "title": "Low-rank spectral learning with weighted loss functions",
    "authors": ["Kulesza", "Alex", "Jiang", "Nan", "Singh", "Satinder"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2015
  }, {
    "title": "Learning multi-step predictive state representations",
    "authors": ["Langer", "Lucas", "Balle", "Borja", "Precup", "Doina"],
    "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Convergence rates for markov chains",
    "authors": ["Rosenthal", "Jeffrey S"],
    "venue": "Siam Review,",
    "year": 1995
  }, {
    "title": "Learning latent variable models by improving spectral solutions with exterior point method",
    "authors": ["Shaban", "Amirreza", "Farajtabar", "Mehrdad", "Xie", "Bo", "Song", "Le", "Boots", "Byron"],
    "venue": "In UAI, pp",
    "year": 2015
  }, {
    "title": "Links between multiplicity automata, observable operator models and predictive state representations–a unified learning framework",
    "authors": ["Thon", "Michael", "Jaeger", "Herbert"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }],
  "id": "SP:7e2991cf378fae265b9a0719d154d18354df70ce",
  "authors": [{
    "name": "Borja Balle",
    "affiliations": []
  }, {
    "name": "Odalric-Ambrym Maillard",
    "affiliations": []
  }],
  "abstractText": "We present spectral methods of moments for learning sequential models from a single trajectory, in stark contrast with the classical literature that assumes the availability of multiple i.i.d. trajectories. Our approach leverages an efficient SVD-based learning algorithm for weighted automata and provides the first rigorous analysis for learning many important models using dependent data. We state and analyze the algorithm under three increasingly difficult scenarios: probabilistic automata, stochastic weighted automata, and reactive predictive state representations controlled by a finite-state policy. Our proofs include novel tools for studying mixing properties of stochastic weighted automata.",
  "title": "Spectral Learning from a Single Trajectory under Finite-State Policies"
}