{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 219–230 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n219"
  }, {
    "heading": "1 Introduction",
    "text": "Generating high quality arguments plays a crucial role in decision-making and reasoning processes (Bonet and Geffner, 1996; Byrnes, 2013). A multitude of arguments and counter-arguments are constructed on a daily basis, both online and offline, to persuade and inform us on a wide range of issues. For instance, debates are often conducted in legislative bodies to secure enough votes for bills to pass. In another example, online deliberation has become a popular way of soliciting public opinions on new policies’ pros and cons (Albrecht, 2006; Park et al., 2012). Nonetheless, constructing persuasive arguments is a daunting task, for both human and computers. We believe that developing effective argument generation models will enable a broad range of compelling applications, including debate coaching, improving students’ essay writing skills, and pro-\nviding context of controversial issues from different perspectives. As a consequence, there exists a pressing need for automating the argument construction process.\nTo date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017). Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output.\nIn general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of domains. Especially, most previous NLG systems rely on tem-\nplates that are either constructed by rules (Hovy, 1993; Belz, 2008; Bouayad-Agha et al., 2011), or acquired from a domain-specific corpus (Angeli et al., 2010) to enhance grammaticality and coherence. This makes them unwieldy to be adapted for new domains.\nIn this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance. To address the above challenges, we present a neural network-based argument generation framework augmented with externally retrieved evidence. Our model is inspired by the observation that when humans construct arguments, they often collect references from external sources, e.g., Wikipedia or research papers, and then write their own arguments by synthesizing talking points from the references. Figure 1 displays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who argue against the motion that “government should be allowed to view private emails”. Both replies leverage information drawn from Wikipedia, such as “political corruption” and “Fourth Amendment on protections of personal privacy”.\nConcretely, our neural argument generation model adopts the popular encoder-decoderbased sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014), which has achieved significant success in various text generation tasks (Bahdanau et al., 2015; Wen et al., 2015; Wang and Ling, 2016; Mei et al., 2016; Wiseman et al., 2017). Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia2. Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of “talking points”, and the other then generates an argument based on both input and keyphrases.\nAutomatic evaluation based on BLEU (Papineni et al., 2002) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models (Bahdanau et al., 2015) that are also trained with retrieved evidence. We further design a novel evaluation procedure to measure whether the arguments are on-topic by predicting their relevance to the given statement based on a separately trained\n1 https://www.reddit.com/r/changemyview 2 https://en.wikipedia.org/\nrelevance estimation model. Results suggest that our model generated arguments are more likely to be predicted as on-topic, compared to other seq2seq-based generations models.\nThe rest of this paper is organized as follows. Section 2 highlights the roadmap of our system. The dataset used for our study is introduced in Section 3. The model formulation and retrieval methods are detailed in Sections 4 and 5. We then describe the experimental setup and results in Sections 6 and 7, followed by further analysis and future directions in Section 8. Related work is discussed in Section 9. Finally, we conclude in Section 10."
  }, {
    "heading": "2 Framework",
    "text": "Our argument generation pipeline, consisting of evidence retrieval and argument construction, is depicted in Figure 2. Given a statement, a set of queries are constructed based on its topic signature words (e.g., “government” and “national security”) to retrieve a list of relevant articles from Wikipedia. A reranking component further extracts sentences that may contain supporting evidence, which are used as additional input information for the neural argument generation model.\nThe generation model then encodes the statement and the evidence with a shared encoder in sequence. Two decoders are designed: the keyphrase decoder first generates an intermediate representation of talking points in the form of keyphrases (e.g., “right to privacy”, “political corruption”), followed by a separate argument decoder which produces the final argument."
  }, {
    "heading": "3 Data Collection and Processing",
    "text": "We draw data from Reddit subcommunity /r/ChangeMyView (henceforth CMV), which focuses on facilitating open discussions on a wide range of disputed issues. Specifically, CMV is structured as discussion threads, where the original post (OP) starts with a viewpoint on a controversial topic, followed with detailed reasons, then other users reply with counter-arguments. Importantly, when a user believes his view has been changed by an argument, a delta is often awarded to the reply.\nIn total, 26,761 threads from CMV are downloaded, dating from January 2013 to June 20173.\n3Dataset used in this paper is available at http:// xinyuhua.github.io/Resources/.\nOnly root replies (i.e., replies directly addressing OP) that meet all of the following requirements are included: (1) longer than 5 words, (2) without offensive language4, (3) awarded with delta or with more upvotes than downvotes, and (4) not generated by system moderators.\nAfter filtering, the resultant dataset contains 26,525 OPs along with 305,475 relatively high quality root replies. We treat each OP as the input statement, and the corresponding root replies as target arguments, on which our model is trained and evaluated. A Focused Domain Dataset. The current dataset contains diverse domains with unbalanced numbers of arguments. We therefore choose samples from the politics domain due to its large volume of discussions and good coverage of popular arguments in the domain.\nHowever, topic labels are not available for the discussions. We thus construct a domain classifier for politics vs. non-politics posts based on a logistic regression model with unigram features, trained from our heuristically labeled Wikipedia abstracts5. Concretely, we manually collect two lists of keywords that are indicative of politics and non-politics. Each abstract is labeled as politics\n4 We use offensive words collected by Google’s What Do You Love project: https://gist.github.com/ jamiew/1112488, last accessed on February 22nd, 2018.\n5About 1.3 million English Wikipedia abstracts are downloaded from http://dbpedia.org/page/.\nor non-politics if its title only matches keywords from one category.6 In total, 264,670 politics abstracts and 827,437 of non-politics are labeled. Starting from this dataset, our domain classifier is trained in a bootstrapping manner by gradually adding OPs predicted as politics or non-politics.7 Finally, 12,549 OPs are labeled as politics, each of which is paired with 9.4 high-quality target arguments on average. The average length for OPs is 16.1 sentences of 356.4 words, and 7.7 sentences of 161.1 words for arguments."
  }, {
    "heading": "4 Model",
    "text": "In this section, we present our argument generation model, which jointly learns to generate talking points in the form of keyphrases and produce arguments based on the input and keyphrases. Extended from the successful seq2seq attentional model (Bahdanau et al., 2015), our proposed model is novel in the following ways. First, two separate decoders are designed, one for generating keyphrases, the other for argument construction. By sharing the encoder with keyphrase generation, our argument decoder is better aware of salient talking points in the input. Second, a novel\n6Sample keywords for politics: “congress”, “election”, “constitution”; for non-politics: “art”, “fashion”,“music”. Full lists are provided in the supplementary material.\n7More details about our domain classifier are provided in the supplementary material.\nattention mechanism is designed for argument decoding by attending both input and the previously generated keyphrases. Finally, a reranking-based beam search decoder is introduced to promote topic-relevant generations."
  }, {
    "heading": "4.1 Model Formulation",
    "text": "Our model takes as input a sequence of tokens x = {xO;xE}, where xO is the statement sequence and xE contains relevant evidence that is extracted from Wikipedia based on a separate retrieval module. A special token <evd> is inserted between xO and xE . Our model then first generates a set of keyphrases as a sequence yp = {ypl }, followed by an argument ya = {yat }, by maximizing logP (y|x), where y = {yp;ya}. The objective is further decomposed into∑ t logP (yt|y1:t−1,x), with each term estimated by a softmax function over a non-linear transformation of decoder hidden states sat and s p t , for argument decoder and keyphrase decoder, respectively. The hidden states are computed as done in Bahdanau et al. (2015) with attention:\nst = g(st−1, ct, yt) (1)\nct = T∑ j=1 αtjhj (2) αtj = exp(etj)∑T k=1 exp(etk) (3) etj = v T tanh(Whhj +Wsst + battn) (4)\nNotice that two sets of parameters and different state update functions g(·) are learned for separate decoders: {W ah , W as , baattn, ga(·)} for the argument decoder; {W ph , W p s , b p attn, g\np(·)} for the keyphrase decoder. Encoder. A two-layer bidirectional LSTM (biLSTM) is used to obtain the encoder hidden states hi for each time step i. For biLSTM, the hidden state is the concatenation of forward and backward hidden states: hi = [ −→ hi; ←− hi]. Word representations are initialized with 200-dimensional pre-trained GloVe embeddings (Pennington et al., 2014), and updated during training. The last hidden state of encoder is used to initialize both decoders. In our model the encoder is shared by argument and keyphrase decoders. Decoders. Our model is equipped with two decoders: keyphrase decoder and argument decoder, each is implemented with a separate two-layer unidirectional LSTM, in a similar spirit with one-\nto-many multi-task sequence-to-sequence learning (Luong et al., 2015). The distinction is that our training objective is the sum of two loss functions:\nL(θ) =− α Tp ∑ (x,yp)∈D logP (yp|x; θ)\n− (1− α) Ta ∑ (x,ya)∈D logP (ya|x; θ) (5)\nwhere Tp and Ta denote the lengths of reference keyphrase sequence and argument sequence. α is a weighting parameter, and it is set as 0.5 in our experiments.\nAttention over Both Input and Keyphrases. Intuitively, the argument decoder should consider the generated keyphrases as talking points during the generation process. We therefore propose an attention mechanism that can attend both encoder hidden states and the keyphrase decoder hidden states. Additional context vector c′t is then computed over keyphrase decoder hidden states spj , which is used for computing the new argument decoder state:\nsat = g ′(sat−1, [ct; c ′ t], y a t ) (6)\nc′t = Tp∑ j=1 α′tjs p j (7) α′tj = exp(e′tj)∑Tp k=1 exp(e ′ tk) (8) e′tj = v ′T tanh(W ′ps p j +W ′ as a t + b ′ attn) (9)\nwhere spj is the hidden state of keyphrase decoder at position j, sat is the hidden state of argument decoder at timestep t, and ct is computed in Eq. 2.\nDecoder Sharing. We also experiment with a shared decoder between keyphrase generation and argument generation: the last hidden state of the keyphrase decoder is used as the initial hidden state for the argument decoder. A special token <arg> is inserted between the two sequences, indicating the start of argument generation."
  }, {
    "heading": "4.2 Hybrid Beam Search Decoding",
    "text": "Here we describe our decoding strategy on the argument decoder. We design a hybrid beam expansion method combined with segment-based reranking to promote diversity of beams and informativeness of the generated arguments.\nHybrid Beam Expansion. In the standard beam search, the top k words of highest probability are\nselected deterministically based on the softmax output to expand each hypothesis. However, this may lead to suboptimal output for text generation (Wiseman and Rush, 2016), e.g., one beam often dominates and thus inhibits hypothesis diversity. Here we only pick the top n words (n < k), and randomly draw another k− n words based on the multinomial distribution after removing the n expanded words from the candidates. This leads to a more diverse set of hypotheses. Segment-based Reranking. We also propose to rerank the beams every p steps based on beam’s coverage of content words from input. Based on our observation that likelihood-based reranking often leads to overly generic arguments (e.g., “I don’t agree with you”), this operation has the potential of encouraging more informative generation. k = 10, n = 3, and p = 10 are used for experiments. The effect of parameter selection is studied in Section 7."
  }, {
    "heading": "5 Relevant Evidence Retrieval",
    "text": ""
  }, {
    "heading": "5.1 Retrieval Methodology",
    "text": "We take a two-step approach for retrieving evidence sentences: given a statement, (1) constructing one query per sentence and retrieving relevant articles from Wikipedia, and (2) reranking paragraphs and then sentences to create the final set of evidence sentences. Wikipedia is used as our evidence source mainly due to its objective perspective and broad coverage of topics. A dump of December 21, 2016 was downloaded. For training, evidence sentences are retrieved with queries constructed from target user arguments. For test, queries are constructed from OP. Article Retrieval. We first create an inverted index lookup table for Wikipedia as done in Chen et al. (2017). For a given statement, we construct one query per sentence to broaden the diversity of retrieved articles. Therefore, multiple passes of retrieval will be conducted if more than one query is created. Specifically, we first collect topic signature words of the post. Topic signatures (Lin and Hovy, 2000) are terms strongly correlated with a given post, measured by log-likelihood ratio against a background corpus. We treat posts from other discussions in our dataset as background. For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signature word. For instance, a query “the government, my e-mails,\nnational security” is constructed for the first sentence of OP in the motivating example (Figure 2). Top five retrieved articles with highest TF-IDF similarity scores are kept per query. Sentence Reranking. The retrieved articles are first segmented into paragraphs, which are reranked by TF-IDF similarity to the given statement. Up to 100 top ranked paragraphs with positive scores are retained. These paragraphs are further segmented into sentences, and reranked according to TF-IDF similarity again. We only keep up to 10 top sentences with positive scores for inclusion in the evidence set."
  }, {
    "heading": "5.2 Gold-Standard Keyphrase Construction",
    "text": "To create training data for the keyphrase decoder, we use the following rules to identify keyphrases from evidence sentences that are reused by human writers for argument construction: • Extract noun phrases and verb phrases\nfrom evidence sentences using Stanford CoreNLP (Manning et al., 2014). • Keep phrases of length between 2 and 10 that\noverlap with content words in the argument. • If there is span overlap between phrases, the\nlonger one is kept if it has more content word coverage of the argument; otherwise the shorter one is retained.\nThe resultant phrases are then concatenated with a special delimiter <phrase> and used as gold-standard generation for training."
  }, {
    "heading": "6 Experimental Setup",
    "text": ""
  }, {
    "heading": "6.1 Final Dataset Statistics",
    "text": "Encoding the full set of evidence by our current decoder takes a huge amount of time. We there propose a sampling strategy to allow the encoder to finish encoding within reasonable time\nby considering only a subset of the evidence: For each sentence in the statement, up to three evidence sentences are randomly sampled from the retrieved set; then the sampled sentences are concatenated. This procedure is repeated three times per statement, where a statement is an user argument for training data and an OP for test set. In our experiments, we remove duplicates samples and the ones without any retrieved evidence sentence. Finally, we break down the augmented data into a training set of 224,553 examples (9,737 unique OPs), 13,911 for validation (640 OPs), and 30,417 retained for test (1,892 OPs)."
  }, {
    "heading": "6.2 Training Setup",
    "text": "For all models, we use a two-layer biLSTM as encoder and a two-layer unidirectional LSTM as decoder, with 200-dimensional hidden states in each layer. We apply dropout (Gal and Ghahramani, 2016) on RNN cells with a keep probability of 0.8. We use Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001 to optimize the cross-entropy loss. Gradient clipping is also applied with the maximum norm of 2. The input and output vocabulary sizes are both 50k. Curriculum Training. We train the models in three stages where the truncated input and output lengths are gradually increased. Details are listed in Table 2. Importantly, this strategy allows model training to make rapid progress during early stages. Training each of our full models takes about 4 days on a Quadro P5000 GPU card with a batch size of 32. The model converges after about 10 epochs in total with pre-training initialization, which is described below.\nAdding Pre-training. We pre-train a two-layer seq2seq model with OP as input and target argument as output from our training set. After 20 epochs (before converging), parameters for the\nfirst layer are used to initialize the first layer of all comparison models and our models (except for the keyphrase decoder). Experimental results show that pre-training boosts all methods by roughly 2 METEOR (Denkowski and Lavie, 2014) points. We describe more detailed results in the supplementary material."
  }, {
    "heading": "6.3 Baseline and Comparisons",
    "text": "We first consider a RETRIEVAL-based baseline, which concatenates retrieved evidence sentences to form the argument. We further compare with three seq2seq-based generation models with different training data: (1) SEQ2SEQ: training with OP as input and the argument as output; (2) SEQ2SEQ + encode evd: augmenting input with evidence sentences as in our model; (3) SEQ2SEQ + encode KP: augmenting input with gold-standard keyphrases, which assumes some of the talking points are known. All seq2seq models use a regular beam search decoder with the same beam size as ours.\nVariants of Our Models. We experiment with variants of our models based on the proposed separate decoder model (DEC-SEPARATE) or using a shared decoder (DEC-SHARED). For each, we further test whether adding keyphrase attention for argument decoding is helpful (+ attend KP).\nSystem vs. Oracle Retrieval. For test time, evidence sentences are retrieved with queries constructed from OP (System Retrieval). We also experiment with an Oracle Retrieval setup, where the evidence is retrieved based on user arguments, to indicate how much gain can be expected with better retrieval results."
  }, {
    "heading": "7 Results",
    "text": ""
  }, {
    "heading": "7.1 Automatic Evaluation",
    "text": "For automatic evaluation, we use BLEU (Papineni et al., 2002), an n-gram precision-based metric (up to bigrams are considered), and METEOR (Denkowski and Lavie, 2014), measuring unigram recall and precision by considering paraphrases, synonyms, and stemming. Human arguments are used as the gold-standard. Because each OP may be paired with more than one highquality arguments, we compute BLEU and METEOR scores for the system argument compared against all arguments, and report the best. We do not use multiple reference evaluation because\nthe arguments are often constructed from different angles and cover distinct aspects of the issue. For models that generate more than one arguments based on different sets of sampled evidence, the one with the highest score is considered.\nAs can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.\nMoreover, utilizing attention over both input and the generated keyphrases further boosts our models’ performance. Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing. The reason could be that arguments generated based on system retrieval contain less topic-specific words and more generic argumentative phrases. Since the later is often observed in human written arguments, it may lead to higher precision and thus better BLEU scores.\nDecoder Strategy Comparison. We also study the effect of our reranking-based decoder by varying the reranking step size (p) and the number of top words expanded to beam hypotheses deterministically (k). From the results in Figure 3, we find that reranking with a smaller step size, e.g.,\np = 5, can generally lead to better METEOR scores. Although varying the number of top words for beam expansion does not yield significant difference, we do observe more diverse beams from the system output if more candidate words are selected stochastically (i.e. with a smaller k)."
  }, {
    "heading": "7.2 Topic-Relevance Evaluation",
    "text": "During our pilot study, we observe that generic arguments, such as “I don’t agree with you” or “this is not true”, are prevalent among generations by seq2seq models. We believe that good arguments should include content that addresses the given topic. Therefore, we design a novel evaluation method to measure whether the generated arguments contain topic-relevant information.\nTo achieve the goal, we first train a topicrelevance estimation model inspired by the latent semantic model in Huang et al. (2013). A pair of OP and argument, each represented as the average of word embeddings, are separately fed into a twolayer transformation model. A dot-product is computed over the two projected low-dimensional vectors, and then a sigmoid function outputs the relevance score. For model learning, we further divide our current training data into training, developing, and test sets. For each OP and argument pair, we first randomly sample 100 arguments from other threads, and then pick the top 5 dissimilar ones, measured by Jaccard distance, as negative training samples. This model achieves a Mean Reciprocal Rank (MRR) score of 0.95 on the test set. Descriptions about model formulation and related training\ndetails are included in the supplementary material. We then take this trained model to evaluate the relevance between OP and the corresponding system arguments. Each system argument is treated as positive sample; we then select five negative samples from arguments generated for other OPs whose evidence sentences most similar to that of the positive sample. Intuitively, if an argument contains more topic relevant information, then the relevance estimation model will output a higher score for it; otherwise, the argument will receive a lower similarity score, and thus cannot be easily distinguished from negative samples. Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in Table 4. The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model.\nMoreover, we manually pick 29 commonly used generic responses (e.g., “I don’t think so”) and count their frequency in system outputs. For the seq2seq model, more than 75% of its outputs contain at least one generic argument, compared to 16.2% by our separate decoder model with attention over keyphrases. This further implies that our model generates more topic-relevant content."
  }, {
    "heading": "7.3 Human Evaluation",
    "text": "We also hire three trained human judges who are fluent English speakers to rate system arguments for the following three aspects on a scale of 1\nto 5 (with 5 as best): Grammaticality—whether an argument is fluent, informativeness—whether the argument contains useful information and is not generic, and relevance—whether the argument contains information of a different stance or offtopic. 30 CMV threads are randomly selected, each of which is presented with randomly-shuffled OP statement and four system arguments.\nTable 5 shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant arguments than seq2seq trained without evidence.8 However, we also observe that human judges prefer the retrieved arguments over generation-based models, illustrating the gap between system arguments and human edited text. Sample arguments are displayed in Figure 4."
  }, {
    "heading": "8 Further Discussion",
    "text": "Keyphrase Generation Analysis. Here we provide further analysis over the generated keyphrases by our separate decoder model. First, about 10% of the keyphrases output by our model also appear in the gold-standard (i.e., used by human arguments). Furthermore, 36% of generated keyphrases are reused by our system arguments. With human inspection, we find that although some keyphrases are not directly reused by the argument decoder, they represent high level talking points in the argument. For instance, in the first sample argument by our model in Figure 4, keyphrases “the motive” and “russian” are generated. Although not used, they suggest the topics that the argument should stay on.\nSample Arguments and Future Directions. As can be seen from the sample outputs in Figure 4, our model generally captures more relevant concepts, e.g., “military army” and “wars\n8Inter-rater agreement scores for these three aspects are 0.50, 0.60, and 0.48 by Krippendorff’s α.\nof the world”, as discussed in the first example. Meanwhile, our model also acquires argumentative style language, though there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning."
  }, {
    "heading": "9 Related Work",
    "text": "There is a growing interest in argumentation mining from the natural language processing research\ncommunity (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the previous work represents a fully automated argument generation system. This work aims to close the gap by proposing an end-to-end trained argument construction framework.\nAdditionally, argument retrieval and extraction are investigated (Rinott et al., 2015; Hua and Wang, 2017) to deliver relevant arguments for user-specified queries. Wachsmuth et al. (2017) build a search engine from arguments collected from various online debate portals. After the retrieval step, sentence ordering algorithms are often applied to improve coherence (Sato et al., 2015; Reisert et al., 2015). Nevertheless, simply merging arguments from different resources inevitably introduces redundancy. To the best of our knowledge, this is the first automatic argument generation system that can synthesize retrieved content from different articles into fluent arguments."
  }, {
    "heading": "10 Conclusion",
    "text": "We studied the novel problem of generating arguments of a different stance for a given statement. We presented a neural argument generation framework enhanced with evidence retrieved from Wikipedia. Separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument. Both automatic evaluation against human arguments and human assessment showed that our model produced more informative arguments than popular sequence-to-sequence-based generation models."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was partly supported by National Science Foundation Grant IIS-1566382, and a GPU gift from Nvidia. We thank three anonymous reviewers for their insightful suggestions on various aspects of this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Whose voice is heard in online deliberation?: A study of participation and representation in political debates on the internet",
    "authors": ["Steffen Albrecht."],
    "venue": "Information, Community and Society 9(1):62–82.",
    "year": 2006
  }, {
    "title": "A simple domain-independent probabilistic approach to generation",
    "authors": ["Gabor Angeli", "Percy Liang", "Dan Klein."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
    "year": 2010
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models",
    "authors": ["Anja Belz."],
    "venue": "Natural Language Engineering 14(4):431–455.",
    "year": 2008
  }, {
    "title": "Arguing for decisions: A qualitative model of decision making",
    "authors": ["Blai Bonet", "Hector Geffner."],
    "venue": "Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., pages 98–105.",
    "year": 1996
  }, {
    "title": "Content selection from an ontology-based knowledge base for the generation of football summaries",
    "authors": ["Nadjet Bouayad-Agha", "Gerard Casamayor", "Leo Wanner."],
    "venue": "Proceedings of the 13th European Workshop on Natural",
    "year": 2011
  }, {
    "title": "The nature and development of decision-making: A self-regulation model",
    "authors": ["James P Byrnes."],
    "venue": "Psychology Press.",
    "year": 2013
  }, {
    "title": "A strategy for generating evaluative arguments",
    "authors": ["Giuseppe Carenini", "Johanna Moore."],
    "venue": "INLG’2000 Proceedings of the First International Conference on Natural Language Generation. Association for Computational Lin-",
    "year": 2000
  }, {
    "title": "Reading wikipedia to answer opendomain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association",
    "year": 2017
  }, {
    "title": "Meteor universal: Language specific translation evaluation for any target language",
    "authors": ["Michael Denkowski", "Alon Lavie."],
    "venue": "Proceedings of the Ninth Workshop on Statistical Machine",
    "year": 2014
  }, {
    "title": "Computer-intensive methods for",
    "authors": [],
    "year": 1989
  }, {
    "title": "Bleu: a method",
    "authors": ["Wei-Jing Zhu"],
    "year": 2002
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
    "year": 2014
  }, {
    "title": "The role of saliency in generating natural language arguments",
    "authors": ["Chris Reed."],
    "venue": "IJCAI. pages 876– 883.",
    "year": 1999
  }, {
    "title": "An architecture for argumentative dialogue planning",
    "authors": ["Chris Reed", "Derek Long", "Maria Fox."],
    "venue": "International Conference on Formal and Applied Practical Reasoning. Springer, pages 555–566.",
    "year": 1996
  }, {
    "title": "A computational approach for generating toulmin model argumentation",
    "authors": ["Paul Reisert", "Naoya Inoue", "Naoaki Okazaki", "Kentaro Inui."],
    "venue": "Proceedings of the 2nd Workshop on Argumentation Mining. Association for Compu-",
    "year": 2015
  }, {
    "title": "Show me your evidence - an automatic method for context dependent evidence detection",
    "authors": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M. Khapra", "Ehud Aharoni", "Noam Slonim."],
    "venue": "Proceedings of the 2015 Con-",
    "year": 2015
  }, {
    "title": "End-to-end argument generation system in debating",
    "authors": ["Misa Sato", "Kohsuke Yanai", "Toshinori Miyoshi", "Toshihiko Yanase", "Makoto Iwayama", "Qinghua Sun", "Yoshiki Niwa."],
    "venue": "Proceedings of ACL-IJCNLP 2015 System Demon-",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems. pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Building an argument search engine for the web",
    "authors": ["Henning Wachsmuth", "Martin Potthast", "Khalid Al Khatib", "Yamen Ajjour", "Jana Puschmann", "Jiani Qu", "Jonas Dorsch", "Viorel Morari", "Janek Bevendorff", "Benno Stein."],
    "venue": "In",
    "year": 2017
  }, {
    "title": "Winning on the merits: The joint effects of content and style on debate outcomes",
    "authors": ["Lu Wang", "Nick Beauchamp", "Sarah Shugars", "Kechen Qin."],
    "venue": "Transactions of the Association for Computational Linguistics 5:219–232.",
    "year": 2017
  }, {
    "title": "Neural networkbased abstract generation for opinions and arguments",
    "authors": ["Lu Wang", "Wang Ling."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2016
  }, {
    "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrkšić", "Pei-Hao Su", "David Vandyke", "Steve Young."],
    "venue": "Proceedings of the 2015 Con-",
    "year": 2015
  }, {
    "title": "Sequence-to-sequence learning as beam-search optimization",
    "authors": ["Sam Wiseman", "Alexander M. Rush."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
    "year": 2016
  }, {
    "title": "Challenges in data-to-document generation",
    "authors": ["Sam Wiseman", "Stuart Shieber", "Alexander Rush."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "Using argumentation strategies in automated argument generation",
    "authors": ["Ingrid Zukerman", "Richard McConachy", "Sarah George."],
    "venue": "INLG’2000 Proceedings of the First International Conference on Natural Language Gen-",
    "year": 2000
  }],
  "id": "SP:d5b31e0d6de7fc016b47fee9661279f069eda760",
  "authors": [{
    "name": "Xinyu Hua",
    "affiliations": []
  }, {
    "name": "Lu Wang",
    "affiliations": []
  }],
  "abstractText": "High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topicrelevant content than a popular sequence-tosequence generation model according to both automatic evaluation and human assessments.",
  "title": "Neural Argument Generation Augmented with Externally Retrieved Evidence"
}