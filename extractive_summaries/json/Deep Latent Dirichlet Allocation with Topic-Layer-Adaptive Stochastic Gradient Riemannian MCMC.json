{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The increasing amount and complexity of data call for largecapacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al., 2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo (SG-MCMC) that provides posterior samples in a non-batch learning setting (Welling & Teh, 2011; Patterson & Teh, 2013; Ma et al., 2015). Unfortunately, most deep LVMs,\n1National Laboratory of Radar Signal Processing, Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China. 2McCombs School of Business, The University of Texas at Austin, Austin, TX 78712, USA. Correspondence to: Bo Chen <bchen@mail.xidian.edu.cn>, Mingyuan Zhou <mingyuan.zhou@mccombs.utexas.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nsuch as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), use greedy layerwise training, without a principled way to jointly learn multilayers in an unsupervised manner (Bengio et al., 2007). While SG-MCMC has recently been successfully applied to several “shallow” LVMs, such as mixture models (Welling & Teh, 2011) and mixedmembership models (Patterson & Teh, 2013), it has been rarely applied to “deep” ones, probably due to the lack of understanding on how to jointly learn the latent variables of different layers and adjust the layer and topic specific learning rates in a non-batch learning setting.\nTo investigate scalable SG-MCMC inference for deep LVMs, we focus our study on the recently proposed Poisson gamma belief network (PGBN), whose hidden layers are parameterized with gamma distributed hidden units and connected with Dirichlet distributed basis vectors (Zhou et al., 2016a). The PGBN is capable of extracting topics from a text corpus at multiple layers and outperforms a large number of topic modeling algorithms. However, the PGBN is currently trained with a batch Gibbs sampler that is not scalable to big data. In this paper, we focus on developing scalable multilayer joint inference for the PGBN.\nWe will show that scalable multilayer joint inference of the PGBN could be facilitated by its Fisher information matrix (FIM) (Amari, 1998; Girolami & Calderhead, 2011; Pascanu & Bengio, 2013), which, although seemingly impossible to derive and challenging to work with due to the need to compute the expectations over trigamma functions, is readily available under an alternative representation of the PGBN, referred to as deep latent Dirichlet allocation (DLDA). DLDA, derived by exploiting data augmentation and marginalization techniques on the PGBN, can be considered as a multilayer generalization of latent Dirichlet allocation (LDA) (Blei et al., 2003). Following a general framework for SG-MCMC (Ma et al., 2015), the block diagonal structure of the FIM of DLDA makes it be easily inverted to precondition the mini-batch based noisy gradients to exploit the second-order local curvature information, leading to topic-layer-adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as a natural gradient based batch-learning algorithm (Amari, 1998; Pascanu & Bengio, 2013). To the best of our knowl-\nedge, this is the first time that the FIM of a deep LVM is shown to have an analytical and practical form. How we derive the FIM for the PGBN using data augmentation and marginalization techniques in this paper may serve as an example to help derive the FIMs for other deep LVMs.\nBesides presenting the analytical FIM of the PGBN, important for the marriage of a deep LVM and SG-MCMC, we make another contribution in showing how to facilitate SG-MCMC for an LVM equipped with simplex-constrained model parameters φk = (φ1k, . . . , φV k)\nT , which means∑V v=1 φvk = 1 and φvk ∈ R+, where R+ := {x, x ≥ 0}, by using a reduced-mean simplex parameterization together with a fast sampling procedure recently introduced in Cong et al. (2017). Unlike other simplex parameterizations, the reduced-mean one does not make heuristic pseudolikelihood assumptions. Though it has previously been deemed unsound, it is successfully integrated into our SG-MCMC framework to deliver state-of-the-art results. Exploiting the analytical FIM of DLDA and novel inference for simplexconstrained parameters under a general SG-MCMC framework (Ma et al., 2015), we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC for DLDA, which automatically adjusts the learning rates of global model parameters across all layers and topics, without the need to set the same learning rate for all that is commonly used in practice due to the difficulty in identifying an appropriate combination of the learning rates for different layers and topics."
  }, {
    "heading": "2. PGBN and SG-MCMC",
    "text": "The generative model of the Poisson gamma belief network (PGBN) (Zhou et al., 2016a) with L hidden layers, from top to bottom, is expressed as\nθ (L) j ∼ Gam\n( r, 1/c\n(L+1) j\n) ,\n· · · θ (l) j ∼ Gam ( Φ(l+1)θ (l+1) j , 1/c (l+1) j ) ,\n· · ·\nx (1) j ∼ Pois\n( Φ(1)θ\n(1) j\n) , θ\n(1) j ∼ Gam\n( Φ(2)θ\n(2) j ,\np (2) j\n1−p(2)j\n) ,\n(1)\nwhere the jth observed or latent V -dimensional count vectorsx(1)j ∈ ZV , where Z := {0, 1, . . .}, are factorized under the Poisson (Pois) likelihood; the hidden units θ(l)j ∈ R Kl + of layer l are factorized under the gamma (Gam) likelihood into the product of the basis vector matrix Φ(l) =( φ\n(l) 1 , . . . ,φ (l) Kl ) ∈ RKl−1×Kl+ and the hidden units of the\nnext layer, whereφ(l)k ∼ Dir ( η(l)1Kl−1 ) are Dirichlet (Dir) distributed and 1Kl−1 is a Kl−1-dimensional vector of all ones; the gamma shape parameters r = (r1, · · · , rKL) T at the top layer are shared across all j; {1/c(l)j }3,L+1 are gamma scale parameters, where c(l)j ∼ Gam(e0, 1/f0), and c (2) j := ( 1 − p(2)j ) /p (2) j , where p (2) j ∼ Beta(a0, b0) are\nintroduced to help reduce the dependencies between θ(1)jk and c(2)j . The PGBN in (1) can be further extended un-\nder the Bernoulli-Poisson link as b(1)j = 1 ( x (1) j > 0 ) , and under the Poisson randomized gamma link as y(1)j ∼ Gam ( x (1) j , 1/aj ) , where aj ∼ Gam(e0, 1/f0).\nThe PGBN infers a multilayer deep representation of the data, whose inferred basis vectors φ(l)k at hidden layer l can be directly visualized as [∏l−1 t=1 Φ (t) ] φ (l) k , which are their projections into the V -dimensional probability simplex. The information of the whole data set is compressed by the PGBN into the inferred sparse network {Φ(1), . . . ,Φ(L)}, where φ(l)k1k2 indicates the connection strength between node (basis vector) k1 of layer l − 1 and node k2 of layer l. Moreover, the network structure can be inferred from the data by combining the gamma-negative binomial process of Zhou & Carin (2015) with a greedy layer-wise training strategy. Extensive experiments in Zhou et al. (2016a) show that the PGBN can extract basis vectors that are very specific/abstract in the bottom layer and become increasingly more general when moving upwards from the bottom to top hidden layers, and the K1 hidden units θ (1) j in the first hidden layer, which are unsupervisedly extracted and regularized with the deep network, are well suited for out-of-sample prediction and being used as features for classification.\nDespite all these attractive model properties, the current inference of the PGBN relies on an upward-downward Gibbs sampler that requires processing all data in each iteration and hence often does not scale well to big data unless with parallel computing. To make its inference scalable to allow processing a large amount of data sufficiently fast on a regular personal computer, we resort to SG-MCMC that subsamples the data and utilizes stochastic gradients in each MCMC iteration to generate posterior samples for globally shared model parameters. Let us denote the posterior of model parameters z given the data X = {xj}1,J as p (z |X ) ∝ e−H(z), with potential function H (z) = − ln p (z) − ∑ j ln p (xj |z ). As in Theorem 1 of Ma et al. (2015), p (z |X ) is the stationary distribution of the dynamics defined by the stochastic differential equation (SDE) dz = f (z) dt + √ 2D (z)dW (t), if the deterministic drift f (z) is restricted to the form\nf (z) = − [D (z) + Q (z)]∇H (z) + Γ (z) , (2) Γi (z) = ∑ j ∂ ∂zj [Dij (z) + Qij (z)] , (3)\nwhere D (z) is a positive semidefinite diffusion matrix, W(t) is a Wiener process, Q (z) is a skew-symmetric curl matrix, and Γi(z) is the ith element of the compensation vector Γ(z). Thus one has a mini-batch update rule as\nzt+1 =zt + εt { − [ D(zt)+Q(zt) ] ∇H̃(zt)+Γ(zt) } +N ( 0, εt [ 2D (zt)− εtB̂t ]) , (4)\nwhere εt denotes step sizes, H̃ (z) = − ln p (z) − ρ ∑ x∈X̃ ln p (x |z ), X̃ the mini-batch, ρ the ratio of the dataset size |X| to the mini-batch size |X̃|, and B̂t an estimate of the stochastic gradient noise variance satisfying a positive definite constraint as 2D (zt)− εtB̂t 0.\nAs shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh (2013) is a special case with D (z) = G(z)−1,Q (z) = 0, B̂t = 0, where G (z) denotes the Fisher information matrix (FIM). SGRLD is designed to solve the inference on the probability simplex, where four different parameterizations of the simplex-constrained basis vectors are discussed, including reduced-mean, expanded-mean, reduced-natural, and expanded-natural. Here, we consider both expandedmean, previously shown to provide the best overall results, and reduced-mean, which, although discarded in Patterson & Teh (2013) due to its unstable gradients, is used in this paper to produce state-of-the-art results.\nLet us denote φk ∈ RV+ as a vector on the probability simplex, φ̂k ∈ RV+ as a nonnegative vector, and ϕk ∈ RV−1+ as a nonnegative vector constrained with ϕ·k := ∑V−1 v=1 ϕvk ≤ 1. For convenience, the symbol “·” will denote the operation of summing over the corresponding index. We use ( φ̂1k, · · · , φ̂V k )T/ ∑ v φ̂vk as an expanded-mean parameterization of φk and( ϕ1k, · · · , ϕ(V−1)k, 1− ∑ v<V ϕvk )T as a reduced-mean parametrization of φk. SGRLD focuses on a single-layer model with a multinomial likelihood nk ∼ Mult (n·k,φk) and a Dirichlet distributed prior φk ∼ Dir (η1V ). For inference, it adopts the expanded-mean parameterization of φk and makes a heuristic assumption that n·k ∼ Pois ( φ̂·k ) . While that heuristic pseudolikelihood assumption of SGRLD is neither supported by the original generative model nor rigorously justified in theory, it converts a Dirichlet-multinomial model into a gamma-Poisson one, allowing a simple sampling equation for φ̂k as(\nφ̂k ) t+1 = ∣∣∣(φ̂k)t+εt[(nk+η)−(n·k+φ̂·k)(φk)t] +N ( 0, 2εtdiag [( φ̂k ) t\n])∣∣∣ , (5) where the absolute operation |·| is used to ensure positivevalued φ̂k. Below we show how to eliminate that heuristic assumption by parameterizing φk with reduced-mean, and develop efficient SG-MCMC for the PGBN, which reduces to LDA when the number of hidden layers is one."
  }, {
    "heading": "3. Deep Latent Dirichlet Allocation",
    "text": "While the original construction of PGBN in (1) makes it seemingly impossible to compute the FIM, as shown in Appendix A, we find that, by exploiting data augmentation and marginalization techniques, the PGBN generative model\ncan be rewritten under an alternative representation that marginalizes out all the gamma distributed hidden units, as shown in the following Lemma, where Log(·) denotes the logarithmic distribution (Johnson et al., 1997), m ∼ SumLog(x, p) represents the sum-logarithmic distribution generated with m = ∑x i=1 ui, ui ∼ Log(p) (Zhou et al., 2016b). The proof is deferred to the Appendix.\nLemma 3.1. Denote q(l+1)j = ln ( 1 + q (l) j /c (l+1) j ) for l = 1, . . . , L, where q(1)j := 1, which means q (l+1) j =\nln ( 1 + 1\nc (l+1) j\nln { 1 + 1\nc (l) j\nln [ 1 + · · · ln ( 1 + 1\nc (2) j\n)]}) .\nWith p(l)j := 1 − e −q(l)j and p̃ := q(L+1)· /(c0 + q (L+1) · ), one may re-express the hierarchical model of the PGBN as deep latent Dirichlet allocation (DLDA) as\nx (L+1) k· ∼ Log(p̃),KL ∼ Pois[−γ0 ln(1− p̃)], X(L+1) = ∑KL\nk=1 x (L+1) k· δφ(L)\nk ,( x (L+1) vj ) j ∼ Mult [ x (L+1) v· , ( q (L+1) j ) j / q (L+1) · ] ,\nm (L)(L+1) vj ∼ SumLog(x (L+1) vj , p (L+1) j ),\n· · · x (l) vj = ∑Kl k=1 x (l) vkj , ( x (l) vkj ) v ∼ Mult ( m (l)(l+1) kj ,φ (l) k ) ,\nm (l−1)(l) vj ∼ SumLog(x (l) vj , p (l) j ),\n· · · x (1) vj = ∑K1 k=1 x (1) vkj , ( x (1) vkj ) v ∼Mult ( m (1)(2) kj ,φ (1) k ) . (6)\nNote that the equations in the first four lines of (6) precisely represent a random count matrix generated from a gammanegative binomial process that can also be generated from\nx (L+1) kj ∼ Pois ( rkq (L+1) j ) , rk ∼ Gam (γ0/K, 1/c0) ,\nm (L)(L+1) kj ∼ SumLog ( x (L+1) kj , p (L+1) j ) (7)\nby letting K →∞ (Zhou et al., 2016b). When L = 1, the PGBN whose (rk,φk) are the points of a gamma process reduces to the gamma-negative binomial process PFA of Zhou & Carin (2015), whose alternative representation is provided in Corollary D.1 in the Appendix. Note that how we re-express the PGBN as DLDA is related to how Schein et al. (2016) re-express their Poisson–gamma dynamic systems into an alternative representation that facilitates inference.\nDLDA, designed to infer a multilayer representation of observed or latent high-dimensional sparse count vectors, constrains all the basis vectors of different layers to probability simplices. It is clear from (6) that a data point backpropagates its counts through the network one layer at a time via a sum-logarithmic distribution to enlarge each element of a Kl-dimensional count vector, a multinomial distribution to partition that enlarged count vector into a Kl−1 ×Kl count matrix, and then a row-sum operation to aggregate\nthat latent count matrix into a Kl−1-dimensional count vector, where K0 := V is the feature dimension. Below we show that such an alternative representation that repeats the enlarge-partition-augment operation brings significant benefits when it comes to deriving SG-MCMC inference with preconditioned gradients."
  }, {
    "heading": "3.1. Fisher Information Matrix of DLDA",
    "text": "In deep LVMs, whose parameters of different layers are often highly correlated to each other, it is often difficult to tune the step sizes of different layers together and hence one often chooses to train an unsupervised deep model in a greedy layer-wise manner (Bengio et al., 2007), which is a sensible but not optimal training strategy. To address that issue, we resort to the inverse of the FIM that is widely used to precondition the gradients to adjust the learning rates of different model parameters (Amari, 1998; Pascanu & Bengio, 2013; Ma et al., 2015; Li et al., 2016). However, it is often difficult to compute the FIMs of deep LVMs as\nG (z) = EΩ|z [ − ∂ 2\n∂z2 ln p (Ω |z )\n] , (8)\nwhere z denotes the set of all global variables and Ω is the set of all observed and local variables.\nAlthough deriving the FIM for the PGBN generative model shown in (1) seems impossible, we find it to be straightforward under the alternative DLDA representation in (6). Since the likelihood of (6) is fully factorized with respect to the global parameters z, i.e., φ(l)k and r, one may readily show the FIM G (z) of (6) has a block diagonal form as\ndiag [ I ( ϕ\n(1) 1\n) , · · · , I ( ϕ\n(L) KL\n) , I (r) ] ; (9)\nwith the likelihood ( x (l) vkj ) v ∼ Mult ( m (l)(l+1) kj ,φ (l) k ) and the reduced-mean parameterization, we have\nI ( ϕ\n(l) k\n) =−E [ ∂2\n∂ϕ (l)2 k\nln (∏ jMult [ (x (l) vkj)v;m (l)(l+1) kj ,φ (l) k ])] = M\n(l) k\n[ diag ( 1/ϕ\n(l) k ) +11T /(1−ϕ(l)·k ) ] , (10)\nwhere M (l)k := E [ m (l)(l+1) k· ] =E [ x (l) ·k· ] . Similarly, with the likelihood x(L+1)kj ∼ Pois(rkq (L+1) j ), we have\nI (r) = M (L+1)diag (1/r) , (11) where M (L+1) := E [ q (L+1) · ] .\nThe block diagonal structure of the FIM of DLDA makes it computationally appealing to apply its inverse for preconditioning. Under the framework suggested by (4), we adopt the similar settings used in SGRLD (Patterson & Teh, 2013) that lets D (z) = G(z)−1,Q (z) = 0, and B̂t = 0. While other more sophisticated settings described in Ma et al. (2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and\nstochastic gradient thermostats in Ding et al. (2014), may be used to further improve the performance, we choose this specific one to make a direct comparison with SGRLD.\nBy substituting the FIM G (z) and the adopted settings into (4), it is apparent that we only need to choose a single step size εt, relying on the FIM to automatically adjust the relatively learning rates for different parameters across all layers and topics. Moreover, the block-diagonal structure of G (z) will be carried over to its inverse D (z), making it simple to perform updating using (4), as described below."
  }, {
    "heading": "3.2. Inference on the Probability Simplex",
    "text": "As discussed in Section 2, to sample simplex-constrained model parameters for a Dirichlet-multinomial model, the SGRLD of Patterson & Teh (2013) adopts the expandedmean parameterization of simplex-constrained vectors and makes a pseudolikelihood assumption to simplify the derivation of update equations. In this paper, without replying on that pseudolikelihood assumption, we choose to use the reduced-mean parameterization of simplex-constrained vectors, despite being considered as an unsound choice in Patterson & Teh (2013). In the following discussion, we omit the layer-index superscript (l) for simplicity.\nWith the multinomial likelihood in (6) and the Dirichletmultinomial conjugacy, the conditional posterior of φk can be expressed as (φk | −) ∼ Dir(x1k· + η, . . . , xV k· + η). Taking the gradient with respect to ϕk ∈ RV−1+ on the summation of the negative log-likelihood of a mini-batch X̃ scaled by ρ = |X|/|X̃| and the negative log-likelihood of the Dirichlet prior, we have\n∇ϕk [ −H̃(ϕk) ] = ρx̄:k·+η−1\nϕk − ρx̃V k·+η−1 1− ϕ·k , (12)\nwhere x̃vk· = ∑\nj:xj∈X̃ xvkj and x̄:k· := (x̃1k·, . . . , x̃(V−1)k·)\nT . Note the gradient in (12) becomes unstable when some components of ϕk approach zeros, a key reason that this approach is mentioned but not further pursued in Patterson & Teh (2013).\nHowever, after preconditioning the noisy gradient with the inverse of the FIM, it is intriguing to find out that the stability issue completely disappears. More specifically, by plugging both the FIM in (10) and noisy gradient in (12) into the SGMCMC update in (4), a noisy estimate of the deterministic drift defined in (2) obtained using the current mini-batch can be expressed as\nI (ϕk) −1∇ϕk [ −H̃ (ϕk) ] + Γ (ϕk)\n= M−1k [(ρx̄:k·+η)−(ρx̃·k·+ηV )ϕk] , (13)\nwhere Γ (ϕk) = M −1 k [1− Vϕk] according to (3), as derived in detail in Appendix B. Consequently, with [·]4 de-\nnoting the constraint that ϕvk ≥ 0 and ∑V−1\nv=1 ϕvk ≤ 1, using (4), the sampling of ϕk becomes\n(ϕk)t+1 = [ (ϕk)t+ εt Mk [ (ρx̄:k·+η)−(ρx̃·k·+ηV )(ϕk)t ] +N ( 0, 2εt\nMk\n[ diag (ϕk)t−(ϕk)t (ϕk) T t ])] 4 . (14)\nEven without the [·]4 constraint, the multivariate normal (MVN) simulation in (14), although easy to interpret and numerically stable, is computationally expensive if the Cholesky decomposition, with O((V − 1)3) complexity (Golub & Van Loan, 2012), is adopted directly. Fortunately, using Theorem 2 of Cong et al. (2017), the special structure of its covariance matrix allows an equivalent but substantially more efficient simulation of O(V ) complexity by transforming a random variable drawn from a related MVN that has a diagonal covariance matrix. More specifically, the sampling of (14) can be efficiently realized in a V -dimensional space as\n(φk)t+1 = [ (φk)t+\nεt Mk\n[ (ρx̃:k·+η)−(ρx̃·k·+ηV )(φk)t ] +N ( 0,\n2εt Mk diag (φk)t )] ∠ , (15)\nwhere [·]∠ denotes the simplex constraint that φvk ≥ 0 and∑V v=1 φvk = 1. More details on simulating (14) and (15) can be found in Examples 1-3 of Cong et al. (2017).\nSimilarly, with the gamma-Poisson construction in (7), we have Γk (r) = 1/M (L+1), as in Appendix B, and\n∇rk [ −H̃(r) ] =r−1k ( ρx̃ (L+1) k· + γ0 KL −1 ) − ( c0 + ρq̃ (L+1) · ) ,\n(16) which also becomes unstable if rk approaches zero. Substituting (16) and (11) into (4) leads to\nrt+1 = ∣∣∣∣rt+ εtM (L+1) [( ρx̃(L+1):· + γ0 KL ) −rt ( c0+ρq̃ (L+1) · )] +N ( 0,\n2εt M (L+1) diag (rt) )∣∣∣∣, (17) for which there is no stability issue."
  }, {
    "heading": "3.3. Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC",
    "text": "Note that M (L+1) and M (l)k for l ∈ {1, . . . , L}, appearing as denominates in (17) and (15), respectively, are expectations that need to be approximately calculated. We update them using annealed weighting (Polatkan et al., 2015) as\nM (l) k =\n( 1− ε ′ t ) M (l) k + ε ′ tρE [ x̃ (l) ·k· ] , (18)\nM (L+1) = ( 1− ε ′ t ) M (L+1) + ε ′ tρE [ q̃ (L+1) · ] , (19)\nwhere E[·] denotes averaging over the collected MCMC samples. For simplicity, we set ε ′\nt = εt in this paper, which is found to work well in practice.\nAlgorithm 1 TLASGR MCMC for DLDA (PGBN). Input: Data mini-batches; Output: Global parameters of DLDA (PGBN). 1: for t = 1, 2, · · · do 2: /* Collect local information 3: Upward-downward Gibbs sampling (Zhou et al., 2016a) on\nthe tth mini-batch for x̃:k·, x̃·k·, x̃ (L+1) :· , and q̃ (L+1) · ;\n4: /* Update global parameters 5: for l = 1, · · · , L and k = 1, · · · ,Kl do 6: Update M (l)k with (18); then φ (l) k with (15); 7: end for 8: Update M (L+1) with (19) and then r with (17). 9: end for\nNote that as in (15) and (17), instead of having a single learning rate for all layers and topics, a common practice due to the difficulty to adapt the step sizes to different layers and/or topics, the proposed inference employs topic-layer-adaptive learning rates as εt/M (l) k , where M (L+1) k := M\n(L+1), adapting a single step size εt to different topics and layers by multiplying it with the weights 1/M (l)k for l ∈ {1, . . . , L} and k ∈ {1, . . . ,Kl}. We refer to the proposed inference algorithm with adaptive learning rates as topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC, as summarized in Algorithm 1 that is simple to implement."
  }, {
    "heading": "4. Related Work",
    "text": "Both LDA (Blei et al., 2003) and the related Poisson factor analysis (PFA) (Zhou et al., 2012) are equipped with scalable inference, such as stochastic variational inference (SVI) (Hoffman et al., 2010; Mimno et al., 2012) and SGRLD (Patterson & Teh, 2013). However, both are shallow LVMs whose modeling capacities are often insufficient for big and complex data. The deep Poisson factor models of Gan et al. (2015) and Henao et al. (2015) are proposed to generalize PFA with deep structures, but both of them only explore the deep information in binary topic usage patterns instead of the full connection weights that are used in the PGBN. The proposed DLDA shares some similarities with the pachinko allocation model of Li & McCallum (2006) in that they both adopt layered construction and use Dirichlet distributed topics. Ranganath et al. (2015) propose deep exponential family (DEF), which differs from the PGBN in connecting adjacent layers via the gamma rate parameters and using black-box variational inference (BBVI) (Ranganath et al., 2014).\nSome commonly used neural networks, such as deep belief network (DBN) (Hinton et al., 2006) and deep Boltzmann machines (DBM) (Salakhutdinov & Hinton, 2009), have also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al., 2013). Although they may work well for certain text analysis tasks, they are not naturally designed for count data and often yield latent structures that are not readily interpretable.\nThe neural variational document model (NVDM) of Miao et al. (2016), even though using deep neural networks in its variational auto-encoder (VAE) (Kingma & Welling, 2013), still relies on a single-layer model for data generalization.\nGenerally speaking, it is challenging to develop an efficient and principled multilayer joint learning algorithm for deep LVMs. Scalable variational inference, such as BBVI, often makes the restrictive mean-field assumption. Neural variational inference and learning (NVIL) relies on variance reduction techniques that are often difficult to be generalized for discrete LVMs (Mnih & Gregor, 2014; Rezende et al., 2014). When a SG-MCMC algorithm is used, a single learning rate is often applied for different variables across all layers (Welling & Teh, 2011; Neal et al., 2011; Chen et al., 2014; Ding et al., 2014). It is possible to improve SG-MCMC by adjusting its noisy gradients with some stochastic optimization technique, such as Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014), and RMSprop (Tieleman & Hinton, 2012). For example, Li et al. (2016) show that preconditioning the gradients with diagonal approximated FIM improves SG-MCMC in both training speed and predictive accuracy for supervised learning where gradients are easy to calculate. Other efforts exploiting similar preconditioning idea focus on shallow and/or binary models (Mimno et al., 2012; Patterson & Teh, 2013; Grosse & Salakhutdinov, 2015; Song et al., 2016; Simsekli et al., 2016), and it is unclear how that idea can be extended to deep LVMs whose gradients and FIM maybe difficult to approximate."
  }, {
    "heading": "5. Experiment results",
    "text": "We present experimental results on three benchmark corpora: 20Newsgroups (20News), Reuters Corpus Volume I (RCV1) that is moderately large, and Wikipedia (Wiki) that is huge. 20News consists of 18,845 documents with a vocabulary size of 2,000, partitioned into 11,315 training documents and 7,531 test ones. RCV1 consists of 804,414 documents with a vocabulary size of 10,000, where 10,000 documents are randomly selected for testing. Wiki consists of 10 million documents randomly downloaded from Wikipedia using the scripts provided in Hoffman et al. (2010); as in Hoffman et al. (2010), Gan et al. (2015), and Henao et al. (2015), we use a vocabulary with 7,702 words and randomly select 1,000 documents for testing. To make a fair comparison, these corpora, including the training/testing partitions, are set to be the same as those in Gan et al. (2015) and Henao et al. (2015). To be consistent with the settings of Gan et al. (2015) and Henao et al. (2015), no precautions are taken in the scripts for Wikipedia to prevent a testing document from being downloaded into a mini-batch for training.\nWe consider two related performance measures. The first one is the commonly-used per-heldout-word perplexity cal-\nculated as follows: for each test document, we randomly select 80% of the word tokens to sample the local variables specific to the document, under the global model parameters of each MCMC iteration; after the burn-in period, we accumulate the first layer’s Poisson rates in each collected MCMC sample; in the end, we normalize these accumulated Poisson rates to calculate the perplexity using the remaining 20% word tokens. Similar evaluation methods have been widely used, e.g., in Wallach et al. (2009), Paisley et al. (2011), and Zhou & Carin (2015). Although a good measure for overall performance, the per-heldout-word perplexity, calculated based on multiple collected MCMC samples of global parameters, may not be ideal to check the performance in real time to assess how efficient an iterative algorithm improves its performance as time increases. Therefore, we slightly modify it to provide a point per-heldout-word perplexity calculated based on only the global parameters of the most recent MCMC sample. For simplicity, we refer to (point) per-heldout-word perplexity as (point) perplexity.\nFor comparison, we consider LDA of Blei et al. (2003), focused topic model (FTM) of Williamson et al. (2010), replicated softmax (RSM) of Hinton & Salakhutdinov (2009), nested Hierarchical Dirichlet process (nHDP) of Paisley et al. (2015), DPFA of Gan et al. (2015), and DPFM of Henao et al. (2015). For these methods, the perplexity results are taken from Gan et al. (2015) and Henao et al. (2015). For the proposed algorithms, we set the mini-batch size as 200, and use as burn-in 2000 mini-batches for both 20News and RCV1 and 3500 mini-batches for Wiki. We collect 1500 samples to calculate perplexity. For point perplexity, given the global parameters of an MCMC sample, we sample the local variables with 600 iterations and collect one sample every two iterations during the last 400 iterations. The hyperparameters of DLDA are set as: η(l) = 1/Kl, a0 = b0 = 0.01, and γ0 = c0 = e0 = f0 = 1. Note η(l) and Kl are set similar to that of DPFM for fair comparisons, while other hyperparameters follow Zhou et al. (2016a).\nTo demonstrate the advantages of using the reduced-mean simplex parameterization and inverting the FIM for preconditioning to obtain topic-layer-adaptive learning rates, we consider four different inference methods:\n1) TLASGR: topic-layer-adaptive stochastic gradient Riemannian MCMC for DLDA, as described in Algorithm 1.\n2) TLFSGR: topic-layer-fixed stochastic gradient Riemannian MCMC for DLDA that replaces the adaptive learning rates εt/M (l) k of TLASGR with εt/( ∑K1 k=1M (1) k /K1).\n3) SGRLD: updating each φ(l)k under the expanded-mean parameterization as in (5), served as a good scalable baseline for comparison since it was shown in Patterson & Teh (2013) to perform significantly better than SVI. It differs from TLFSGR mainly in using a different parameterization for\nφ (l) k and adding a pseudolikelihood assumption.\n4) Gibbs: the upward-downward Gibbs sampler in Zhou et al. (2016a).\nBoth TLASGR and TLFSGR differ from SGRLD mainly in how the global parameters φ(l)k are updated. While TLASGR uses topic-layer-adaptive learning rates, both TLFSGR and SGRLD use a single learning rate, a common practice due to the difficulty of tuning the step sizes across layers and topics. We keep the same stepsize schedule of εt = a(1 + t/b)\n−c as in Patterson & Teh (2013) and Ma et al. (2015).\nLet us first examine how various inference algorithms perform on 20News with a single-layer DLDA of size 128, which can be considered as a topic model that imposes an\nasymmetric prior on a document’s proportion over these 128 topics. Under this setting, as shown in Fig. 1(a), TLFSGR clearly outperforms SGRLD in providing lower point perplexities as time progresses, which is not surprising as under the reduced-mean simplex parameterization, to derive its sampling equations, TLFSGR does not rely on a pseudolikelihood assumption that is adopted by SGRLD in its expanded-mean simplex parameterization. Moreover, TLASGR is found to further improve TLFSGR, suggesting that even for a single-layer model, replacing a fixed learning rate as εt/( ∑K1 k=1M (1) k /K1) with topic-adaptive learning rates as εt/M (1) k could further improve the performance.\nLet us then examine how these algorithms perform on two larger corpora—RCV1 and Wiki—with a two-layer DLDA of size 128-64, which improves the single-layer one by capturing the co-occurrence patterns between the topics of the first layer with those of the second layer (Zhou et al., 2016a). As show in Figs. 1(b) and 1(c), it is clear that the proposed TLASGR performs the best for the two-layer DLDA and consistently outperforms TLFSGR as time progresses. In comparison, SGRLD quickly improves its performance as a function of time in the beginning but its point perplexity remains higher even after 10,000 seconds, whereas Gibbs sampling slowly improves its performance as a function of time in the beginning but moves its point perplexity closer and closer to that of TLASGR as time progresses.\nNote that for 20News, the point perplexity of the mini-batch based TLASGR quickly decreases as time increases, while that of Gibbs sampling decreases relatively slowly. That discrepancy of convergence rate as a function of time becomes much more evident for both RCV1 and Wiki, as shown in Figs. 1(b) and 1(c). This is expected as both RCV1 and Wiki are much larger corpora, for which a mini-batch based inference algorithm can already make significant progress in learning the global model parameters, before a batchlearning Gibbs sampler finishes a single iteration that needs to go through all documents.\nTo illustrate the working mechanism of TLASGR, we show how its inferred learning rates are adapted to different layers in Fig. 2. By contrast, TLFSGR admits a fixed learning rate that leads to worse performance. Several interesting observations can be made for TLASGR from Figs. 2(a)-2(c): 1) for Φ(l), higher layers prefer larger step sizes, which may be explained by the enlarge-partition-augment data generating mechanism of DLDA; 2) larger datasets prefer slower learning rates, reflected by the scales of the vertical axes; 3) and the relative learning rates between different layers vary across different datasets.\nTo further verify the excellent performance of DLDA inferred with TLASGR, we compare a wide variety of models and inference algorithms in Table 1. For 20News and RCV1, DLDA with Gibbs sampling performs the best in terms of perplexity and exhibits a clear trend of improvement as the number of hidden layers increases. For Wiki, a single iteration of the DLDA Gibbs sampler on the full corpus is so expensive in both time and memory that its performance is not reported. For DLDA on 20News and RCV1, TLASGR only slightly underperforms Gibbs sampling, and the performance degradation from Gibbs sampling to TLASGR is significantly smaller than that from MCMC to SVI for DPFM. That relative small degradation caused by changing from Gibbs sampling to the mini-batch based TLASGR could be attributed to the Fisher efficiency brought by the FIM. Generally speaking, in comparison to SGRLD, TLASGR brings a clear boost in performance, which is particularly evident for a deeper DLDA, and TLASGR consistently outperforms TLFSGR that does not adapt its learning rates to different topics and layers.\nMNIST. To further illustrate the advantages of using the inverse of the FIM for preconditioning in a deep generative model, and to visualize the benefits of automatically adjusting the relative learning rates of different hidden layers, we apply a three-layer Poisson randomized gamma gamma belief network (PRG-GBN) (Zhou et al., 2016a) to 60,000 MNIST digits and present the learned dictionary atoms after one full epoch, as shown in Fig. 3. It is clear that, with topic-layer-adaptive learning rates, which are made possible by utilizing the FIM, TLASGR provides more effective mini-batch based stochastic updates to allow better information propagation between different hidden layers, extracting more interpretable features at multiple layers."
  }, {
    "heading": "6. Conclusions",
    "text": "For scalable multilayer joint inference of the Poisson gamma belief network (PGBN), we introduce an alternative representation of the PGBN, which is referred to as deep latent Dirichlet allocation (DLDA) that can be considered as a multilayer generalization of latent Dirichlet allocation. We show how to reparameterize the simplex constrained basis vectors, derive a block-diagonal Fisher information matrix (FIM), and efficiently compute the inverse of the FIM, leading to a stochastic gradient MCMC algorithm referred to as topiclayer-adaptive stochastic gradient Riemannian (TLASGR) MCMC. The proposed TLASGR-MCMC is able to jointly learn the parameters of different layers with topic-layeradaptive step sizes, which makes DLDA (PGBN) much more practical in a big data setting. Compelling experimental results on large text corpora and the MNIST dataset demonstrated the advantages of TLASGR-MCMC."
  }, {
    "heading": "Acknowledgements",
    "text": "Bo Chen thanks the support of the Thousand Young Talent Program of China, NSFC (61372132), and NDPR9140A07010115DZ01019. Hongwei Liu thanks the support of NSFC for Distinguished Young Scholars (61525105)."
  }],
  "year": 2017,
  "references": [{
    "title": "Natural gradient works efficiently in learning",
    "authors": ["S. Amari"],
    "venue": "Neural Computation,",
    "year": 1998
  }, {
    "title": "Greedy layer-wise training of deep networks",
    "authors": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"],
    "venue": "In NIPS, pp",
    "year": 2007
  }, {
    "title": "Stochastic gradient Hamiltonian Monte Carlo",
    "authors": ["T. Chen", "E.B. Fox", "C. Guestrin"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Fast simulation of hyperplane-truncated multivariate normal distributions",
    "authors": ["Y. Cong", "B. Chen", "M. Zhou"],
    "venue": "Bayesian Analysis Advance Publication,",
    "year": 2017
  }, {
    "title": "Bayesian sampling using stochastic gradient thermostats",
    "authors": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer"],
    "venue": "JMLR, 12(Jul):2121–2159,",
    "year": 2011
  }, {
    "title": "Scalable deep Poisson factor analysis for topic modeling",
    "authors": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix",
    "authors": ["R.B. Grosse", "R. Salakhutdinov"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Deep Poisson factor modeling",
    "authors": ["R. Henao", "Z. Gan", "J. Lu", "L. Carin"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Replicated softmax: an undirected topic model",
    "authors": ["G.E. Hinton", "R.R. Salakhutdinov"],
    "venue": "In NIPS, pp",
    "year": 2009
  }, {
    "title": "A fast learning algorithm for deep belief nets",
    "authors": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"],
    "venue": "Neural Computation,",
    "year": 2006
  }, {
    "title": "Online learning for latent Dirichlet allocation",
    "authors": ["M.D. Hoffman", "F.R. Bach", "D.M. Blei"],
    "venue": "In NIPS,",
    "year": 2010
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "year": 2014
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["Kingma", "Diederik P", "Welling", "Max"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "A neural autoregressive topic model",
    "authors": ["H. Larochelle", "S. Lauly"],
    "venue": "In NIPS,",
    "year": 2012
  }, {
    "title": "Preconditioned stochastic gradient Langevin dynamics for deep neural networks",
    "authors": ["C. Li", "C. Chen", "D. Carlson", "L. Carin"],
    "year": 2016
  }, {
    "title": "Pachinko allocation: DAGstructured mixture models of topic correlations",
    "authors": ["W. Li", "A. McCallum"],
    "venue": "In ICML, pp",
    "year": 2006
  }, {
    "title": "A complete recipe for stochastic gradient MCMC",
    "authors": ["Y. Ma", "T. Chen", "E. Fox"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Neural variational inference for text processing",
    "authors": ["Y. Miao", "L. Yu", "P. Blunsom"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Sparse stochastic inference for latent Dirichlet allocation",
    "authors": ["D. Mimno", "M.D. Hoffman", "D.M. Blei"],
    "venue": "In ICML, pp",
    "year": 2012
  }, {
    "title": "Neural variational inference and learning in belief networks",
    "authors": ["A. Mnih", "K. Gregor"],
    "year": 2014
  }, {
    "title": "MCMC using Hamiltonian dynamics",
    "authors": ["Neal", "R. M"],
    "venue": "Handbook of Markov Chain Monte Carlo,",
    "year": 2011
  }, {
    "title": "The discrete infinite logistic normal distribution for mixed-membership modeling",
    "authors": ["J. Paisley", "C. Wang", "D. Blei"],
    "venue": "In AISTATS,",
    "year": 2011
  }, {
    "title": "Revisiting natural gradient for deep networks",
    "authors": ["R. Pascanu", "Y. Bengio"],
    "year": 2013
  }, {
    "title": "Stochastic gradient Riemannian Langevin dynamics on the probability simplex",
    "authors": ["S. Patterson", "Y.W. Teh"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "A Bayesian nonparametric approach to image superresolution",
    "authors": ["G. Polatkan", "M. Zhou", "L. Carin", "D. Blei", "I. Daubechies"],
    "year": 2015
  }, {
    "title": "Black box variational inference",
    "authors": ["R. Ranganath", "S. Gerrish", "D.M. Blei"],
    "venue": "In AISTATS,",
    "year": 2014
  }, {
    "title": "Deep exponential families",
    "authors": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Deep Boltzmann machines",
    "authors": ["R. Salakhutdinov", "G.E. Hinton"],
    "venue": "In AISTATS,",
    "year": 2009
  }, {
    "title": "Poisson–gamma dynamical systems",
    "authors": ["A. Schein", "M. Zhou", "H. Wallach"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Stochastic quasi-Newton Langevin Monte Carlo",
    "authors": ["U. Simsekli", "R. Badeau", "A.T. Cemgil", "Richard"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Learning sigmoid belief networks via Monte Carlo expectation maximization",
    "authors": ["Z. Song", "R. Henao", "D. Carlson", "L. Carin"],
    "venue": "In AISTATS,",
    "year": 2016
  }, {
    "title": "Modeling documents with deep Boltzmann machines",
    "authors": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"],
    "venue": "In UAI,",
    "year": 2013
  }, {
    "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
    "authors": ["T. Tieleman", "G. Hinton"],
    "venue": "COURSERA: Neural networks for machine learning,",
    "year": 2012
  }, {
    "title": "Evaluation methods for topic models",
    "authors": ["H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"],
    "venue": "In ICML,",
    "year": 2009
  }, {
    "title": "Bayesian learning via stochastic gradient Langevin dynamics",
    "authors": ["M. Welling", "Teh", "Y.-W"],
    "venue": "In ICML, pp",
    "year": 2011
  }, {
    "title": "The IBP compound Dirichlet process and its application to focused topic modeling",
    "authors": ["S. Williamson", "C. Wang", "K.A. Heller", "D.M. Blei"],
    "venue": "In ICML,",
    "year": 2010
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["M.D. Zeiler"],
    "year": 2012
  }, {
    "title": "Negative binomial process count and mixture modeling",
    "authors": ["M. Zhou", "L. Carin"],
    "venue": "PAMI, 37(2):307–320,",
    "year": 2015
  }, {
    "title": "Betanegative binomial process and Poisson factor analysis",
    "authors": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"],
    "venue": "In AISTATS,",
    "year": 2012
  }, {
    "title": "Augmentable gamma belief networks",
    "authors": ["M. Zhou", "Y. Cong", "B. Chen"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Priors for random count matrices derived from a family of negative binomial processes",
    "authors": ["M. Zhou", "O. Padilla", "J.G. Scott"],
    "venue": "J. Amer. Statist. Assoc.,",
    "year": 2016
  }],
  "id": "SP:760bd3d31fa623b37dd5c68fdad432ed1075a0d0",
  "authors": [{
    "name": "Yulai Cong",
    "affiliations": []
  }, {
    "name": "Bo Chen",
    "affiliations": []
  }, {
    "name": "Hongwei Liu",
    "affiliations": []
  }, {
    "name": "Mingyuan Zhou",
    "affiliations": []
  }],
  "abstractText": "It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.",
  "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC"
}