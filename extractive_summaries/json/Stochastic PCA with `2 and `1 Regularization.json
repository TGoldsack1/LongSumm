{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Principal component analysis (PCA), a ubiquitous procedure in scientific analysis, can be posed as the following learning problem: given a zero-mean random vector x ∈ Rd with some (unknown) distribution D, find the k-dimensional subspace that captures the maximal mass of the distribution. If we represent a subspace by an orthogonal basis matrix U ∈ Rd×k that spans the subspace, then PCA returns the\n1Department of Computer Science, Johns Hopkins University, Baltimore, USA. Correspondence to: Raman Arora <arora@cs.jhu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nsubspace that maximizes the variance in data projected onto the subspace, i.e. E[‖U>x‖2], among all k-dimensional subspaces of Rd. Formally, we can write PCA as the following stochastic optimization problem:\nmin U∈Rd×k\n− E[‖U>x‖2]\nsubject to U>U = Ik . (1)\nEquivalently, we can represent a subspace by an orthogonal rank-k projection matrix M = UU>, where U is any orthogonal basis matrix for the subspace. This gives the following equivalent formulation for the PCA problem:\nmin M∈Rd×d\n− E[x>Mx]\nsubject to rank(M) = k, λi(M) ∈ {0, 1},∀i ∈ [d] (2)\nIt is easy to check that this maximal variance subspace is given by the span of top-k eigenvectors of the covariance matrix C := E[xx>]. In other words given eigendecomposition of C = ∑d i=1 λiuiu > i , for λ1 ≥ λ2 ≥ · · · ≥ λd, the optimal solution to Problem 1 is given by the basis matrix U∗ = [u1, . . . , uk], and the optimal solution to Problem 2 is given as M∗ = U∗U>∗ . Furthermore, given access to the distribution D only through a sample {xi}ni=1 ∼ Dn drawn independently from D, the sample average approximation (SAA, or equivalently empirical risk minimization) approach to learning the maximal variance subspace amounts to finding the top-k eigenvectors of the empirical covariance matrix, Ĉ := 1n ∑n i=1 xix > i .\nAn alternative computationally attractive approach to solving Problem 1 is based on stochastic approximation (SA) algorithms that attempt to directly minimize the objective given access to a first order oracle. For instance, stochastic gradient descent (SGD), a staple SA algorithm, on Problem 1 yields the following updates:\nUt+1 = Gram-Schmidt(Ut + ηtxtx>t Ut), (3)\nwhere Gram-Schmidt orthogonalization gives an orthogonal basis matrix for the column span of Ut + ηtxtx>t Ut; this is also known as Oja’s algorithm (Oja, 1982). Such first order methods for solving Problem 1 have received a lot of attention in recent years (Arora et al., 2012; 2013; Balsubramani et al., 2013; Mitliagkas et al., 2013; Shamir,\n2016; Allen-Zhu & Li, 2017; Jain et al., 2016; Balcan et al., 2016). It is remarkable that even though Problem 1 is nonconvex1, Oja’s algorithm works reasonably well in practice and has been shown to enjoy strong theoretical guarantees (Allen-Zhu & Li, 2017; Jain et al., 2016; Balcan et al., 2016; Shamir, 2016).\nRather than directly solve the nonconvex problem, one can consider a convex relaxation of the equivalent formulation in Problem 2. Following Arora et al. (2013), we take the convex hull of the constraint set in Problem 2 to obtain the following convex program:\nmin M∈Rd×d\n− E[x>Mx]\nsubject to Tr (M) = k, 0 M I . (4)\nStochastic gradient descent on Problem 4 yields the following update, also referred to as matrix stochastic gradient (MSG) in (Arora et al., 2013):\nMt+1 ←P(Mt + ηtxtx>t ), (5)\nwhere P(·) is the projection operator onto the feasible set of Problem 4 with respect to the Frobenius norm. Assuming the fourth moment of the distribution is bounded by a constant, the standard analysis of (Shamir & Zhang, 2013) yields the following guarantee for MSG:\nE[x>M∗x]− E[x>M̄x] = O (\nlog T√ T\n) ,\nwhere M∗ is the optimal solution to the PCA Problem 2 and M̄ = rounding(MT+1) is a rank-k projection matrix obtained using randomized rounding (Warmuth & Kuzmin, 2008) of the final iterate of MSG.\nCompared to Oja’s algorithm, MSG has two major drawbacks: first, Oja’s algorithm achieves a faster Õ( 1T ) rate of convergence, and second, Oja’s algorithm is computationally efficient since at each iteration it only keeps a d × k orthogonal matrix, while rank of the projection matrix Mt in MSG can possibly grow at each iteration.\nMore generally, methods based on convex relaxation are usually hard to scale to large problems due to both statistical inefficiency and higher computational cost. On the other hand, methods that directly solve the non-convex problems are usually preferable by practitioners, and have been shown recently to achieve optimal convergence rates in many problems. In particular, the classical Oja’s algorithm which is simply SGD on the original non-convex PCA problem is provably optimal both statistically and computationally. However, empirically, it has been shown that convex relaxation based methods either match or outperform Oja’s\n1since the objective is concave and the set of orthogonal matrices is non-convex\nperformance (see for example (Arora et al., 2012; 2013)). Thus, a natural question to ask is if the suboptimal guarantees on statistical and computational efficiency of methods based on convex relaxation are artifacts of analysis.\nTo understand these issues better, in this paper we study various modifications to the MSG update in equation (4) that impart the resulting algorithm with desirable computational properties including faster convergence and better overall runtime. These modifications are based on principled design techniques – each of the proposed variants is given as stochastic gradient descent on the the MSG objective in Problem 4 with an additional regularization term. In particular, we consider (a) an `2 regularization which yields a faster convergence rate, (b) an `1 regularization term which prevents the rank of the intermediate iterates of MSG from growing unbounded, and (c) an elastic-net (i.e. joint `1 + `2) regularization that is empirically shown to achieve a faster convergence rate with small computational cost per iteration. Our experimental results show that the proposed (`1 + `2)-regularized MSG achieves state-ofthe-art results outperforming MSG and Oja’s algorithm for various parameter settings and choice of datasets.\nIt is important to note that we are interested in principled design and analysis of stochastic approximation algorithms for principal component analysis (i.e. Problems 1 and 2). The proposed formulations of the regularized PCA objective are purely for computational reasons unlike usual regularized learning problems where the goal is to avoid overfitting (equivalently, injecting an inductive bias)."
  }, {
    "heading": "1.1. Notation",
    "text": "We denote matrices with capital Roman letters, e.g. U, and vectors with small Roman letters, e.g. u. For any integer k, we denote the set {1, . . . , k} by [k]. Furthermore, Ik denotes the identity matrix of size k × k. We drop the subscript k whenever the size is clear from the context. Frobenius norm and spectral norm of matrices are denoted by ‖·‖F and ‖·‖2 resepctively, and for vectors, ‖·‖ denotes the `2 norm. For any two matrices M1,M2 ∈ Rd×d, the standard inner-product is written as 〈M1,M2〉 = Tr ( M>1 M2 ) . For a real symmetric matrix C, λk(C) represents the kth largest eigenvalue of C. For notational convenience, when clear from the context we will denote λk(C) by λk. We denote the eigen-decomposition of the covariance matrix by C = ∑d i=1 λiuiu > i , where λ1 ≥ · · · ≥ λd. For any i ∈ [d−1], we denote the eigengap at i as gi := λi − λi+1. The projection matrix onto the subspace spanned by the top-k eigenvectors of C will be represented as Πk(C) = ∑k i=1 uiu > i . The convex hull of the set of rank-k orthogonal projection matrices is denoted by M := {M ∈ Rd×d : Tr (M) ≤ k, 0 M I}. Finally, our analysis for regularized variants leverages the strong\nAlgorithm 1 `2-Regularized MSG (`2-RMSG)\nRequire: Input data {xt}Tt=1, output dimension k, regularization parameter λ Ensure: M̃ 1: M1 ← 0 2: for t = 1, · · · , T do 3: ηt ← 1λt 4: Mt+ 12 ← (1− ληt)Mt + ηtxtx > t\n5: Mt+1 ←PM (\nMt+ 12 ) 6: end for 7: M̃←Prank−k(MT+1) {Return top-k subspace of MT+1}\nconvexity / smoothness of the corresponding objectives.\nDefinition 1.1 (Strongly convex / smooth). A function f : M → R is λ-strongly convex and µ-smooth for some λ, µ > 0 if for all M,M′ ∈M and gM := ∇f(M) , we have\nλ 2 ‖M′−M‖2F ≤f(M′)−f(M)−〈gM,M′−M〉≤ µ 2 ‖M′−M‖2F .\n2. PCA with `2-regularization In this section, we study how adding a strongly convex regularizer to the linear PCA objective in Problem 4 changes the optimization problem and if we can leverage strong convexity of the resulting objective to guarantee a faster convergence rate without changing the optimum. We consider the following `2-regularized PCA optimization problem:\nmin M∈Rd×d\n− E[x>Mx] + λ 2 ‖M‖2F\nsubject to Tr (M) ≤ k, 0 M I . (6)\nSGD on Problem 6 yields the following `2-RMSG updates, Mt+1 ←P ( (1− ληt)Mt + ηtxtx>t ) . (7)\nNote that as is the case with MSG, the final iterate, MT+1, of `2-RMSG is not guaranteed to be a rank-k projection matrix. As we show below, we can guarantee convergence in terms of the distance between the subspaces, which allows us to perform a simple deterministic rounding by simply returning the top-k eigenspace of MT+1 (see proof of Theorem 2.4). This yields the procedure detailed in Algorithm 1.\nNext, we study the iteration complexity of `2-RMSG and conditions on the size of the regularization constant that keep the optimum unchanged. Before giving formal results, we make a few remarks summarizing the key observations.\nFast rate: The objective in Problem 6 is λ-strongly convex. We leverage this to show that `2-RMSG enjoys a fast rate of O ( 1 λ2T ) ; see Theorem 2.4 for a formal statement.\nAdmissible λ: While the bound above suggests that larger values of λ are preferred, it is important to note that for large λ the optima of the Problems 2 and 6 may fail to coincide. Our analysis shows that if there exists an eigengap at k in the spectrum of the covariance matrix, i.e. if gk = λk(C)− λk+1(C) > 0, then for λ < gk, the global optimum of the regularized PCA problem is a global optimum for the original problem, even when there is no eigengap at k – we refer to such values of the regularization parameter as admissible; see Section 2.1 for more details. For illustration, we plot the landscape of the `2-regularized objective for different values of λ in Figure 1(a) and (b) for covariance matrices with and without an eigengap at k, respectively.\nOverall runtime: The `2-RMSG algorithm has the same computational cost per iterate as MSG. To see this, note that the Step 4 of Algorithm 1 can be written equivalently in terms of eigen-decomposition of Mt = UtΛtU>t as follows,\n(1− ληt)UtΛtU>t + ηtxtx>t\n= [ Ut rt‖rt‖ ] [(1− ληt)Λt + x̂tx̂>t ‖rt‖ x̂t ‖rt‖ x̂t> ‖rt‖2 ] ︸ ︷︷ ︸\nQt∈R(l+1)×(l+1)\n[ U>t r>t ‖rt‖ ]\nwhere x̂t = √ η t U>t xt and rt = √ η t xt − Utx̂t. Therefore, the update in Step 4 amounts to computing the eigendecompostion of matrix Qt = ŨΛ̃Ũ and matrix multiplication, Ut+1 = [Ut rt‖rt‖ ]Ũ. This rank-one eigenupdate requires O(k3t + dk 2 t ) time and O(dkt) space where kt = rank(Mt); this computational trick is well-known in literature (Arora et al., 2013). The projection in Step 5 of Algorithm 1 can be implemented efficiently as well via a simple shift-and-cap procedure, Algorithm 2 of (Arora et al., 2013), on the vector consisting of diagonal entries of Λ̃; this requires O(kt log kt) time."
  }, {
    "heading": "2.1. Admissible values of regularization parameter",
    "text": "We begin with a formal definition of admissibility. Definition 2.1. We say that the regularization parameter λ takes an admissible value, if any optimum of the regularized PCA Problem 6 is also an optimum of the original Problem 4.\nIn what follows, we give sufficient conditions on admissible values of λ under two distinct settings – with and without an eigengap at k. We define the eigengap at k as gk := λk(C)−λk+1(C). We say that there exists an eigengap at k, if gk > 0, otherwise (i.e. if gk = 0), we say that there is no eigengap at k. When we have an eigengap at k, we can give the following simple characterization of admissible values for the regularization parameter λ. All proofs are deferred to the Appendix in the supplementary file. Lemma 2.2 (Admissible λ with an eigengap at k). Let gk > 0. Then, for any regularization parameter 0 ≤ λ < gk,\nthe optimum of the original PCA Problem (6) and the `2- regularized PCA Problem (4) are unique and identical.\nNext, we consider the case when there is no eigengap at k, i.e. gk = 0. We do assume that an eigengap exists somewhere in the spectrum, since otherwise the covariance matrix is simply a multiple of the identity matrix which is not an interesting case – any k-dimensional subspace is an optimum of the corresponding PCA problem. Without loss of generality, assume that rank(C) > k. Moreover, lets denote λ0(C) := +∞ and λd+1(C) := 0 for notational convenience. Then, we have the following result. Lemma 2.3 (Admissible λ without an eigengap at k). Let p and q be respectively the largest index smaller than k and the smallest index larger than k, at which C has an eigengap:\np := max {i : i ∈ {0, . . . , k − 1}, λi > λi+1} q := min {i : i ∈ {k + 1, . . . , d}, λi > λi+1},\nThen, for 0 < λ < min {gp, gq}, the optimum of Problem 6 is uniquely given by\nM∗ = p∑ i=1 uiu>i + k − p q − p q∑ j=p+1 uju>j .\nFurthermore, M∗ is an optimum solution to Problem 4.\nIn order to better understand the expression for the global optimum, M∗, in Lemma 2.3, consider the following. If\nthere is no eigengap at k, then there is a maximal subset of indices S := {p + 1, . . . , q} ⊆ [d] such that k ∈ S and λp+1 = · · · = λq. In terms of the variance captured, there is no advantage in choosing any particular (convex combination) of the rank-1 subspaces associated with the eigenvectors indexed by S. However, to minimize the Frobenius norm penalty, `2-RMSG picks the average of these subspaces. This is vivid in the expression for M∗ – the top-p rank-1 subspaces are included in M∗, and the remaining k − p mass is distributed equally among the q − p rank-1 subspaces indexed by S. Figure 1 provides a geometric illustration. As can be seen in the bottom row, all points on the left edge are equally good in terms of the objective. But, the `2-regularizer chooses the average of the subspaces with equal eigenvalues, which minimizes the `2-penalty.\n2.2. Convergence Analysis of `2-RMSG\nOur main result of this section bounds the sub-optimality of the output of Algorithm 1, for admissible values of the regularization parameter λ, in terms of the distance from the optimal subspace, M∗. We show the convergence in terms of the parameter as well as a faster rate as compared with MSG (Arora et al., 2013).\nTheorem 2.4. Assume E[‖x‖2] ≤ 1. Then, for any admissible value of λ, after T iterations of Algorithm 1 starting at\nM1 = 0, with step-size sequence ηt = 1λt , we have that\nE[ ∥∥M̃−M∗∥∥2F ] ≤ 16(1 + λ √ k)2\nλ2T , (8)\nwhere M∗ is an optimum of Problem 4, M̃ is the output after rounding the final iterate to a rank-k matrix, and the expectation is with respect to the distribution D.\nMinimax Optimality: Theorem 2.4 guarantees that the iterates of Algorithm 1 converge to the optimal projection matrix in Frobenius norm. It is easy to see that this metric is directly related to the angle between subspaces, i.e. to ‖Ũ>U∗‖2F ,\n‖M∗ − M̃‖2F = 2 ( k − ‖Ũ>U∗‖2F ) ,\nwhere M̃ = ŨŨ >\nand M∗ = U∗U>∗ . This provides a basis for comparison against previous works of Shamir (2016) and Allen-Zhu & Li (2017) which measure convergence in terms of the angle between the subspaces. Furthermore, as a corollary of Theorem 2.4, we have the following bound in terms of the angle between subspaces for `2-RMSG,\nE[‖Ũ>U∗‖2F ] ≤ k − 8(1 + λ\n√ k)2\nλ2T ,\nwhich is minimax optimal in an information-theoretic sense (see Theorem 6 of Allen-Zhu & Li (2017)).\nFinally, convergence in parameter implies the following guarantee in terms of PCA objective for `2-RMSG.\nTheorem 2.5. Under same assumptions as Theorem 2.4, we have that after T iterations of Algorithm 1,\nE[x>M∗x− x>M̃x] ≤ 8λ1(1 + λ\n√ k)2\nλ2T . (9)\nA proof of Theorem 2.5 is provided in the supplementary.\n3. PCA with `1-Regularization As discussed above, the overall runtime needed for MSG to find an -suboptimal solution depends critically on the rank, kt, of the intermediate iterates, Mt. If kt is as large as d, then MSG achieves a runtime that is cubic in the dimensionality. In order to overcome this computational barrier, which appears to be a natural artifact of convex relaxations, we consider regularizing the PCA objective with an `1 penalty. In particular, we consider the following problem in this section:\nmin M∈Rd×d\n− Ex[x>Mx] + µTr (M)\nsubject to Tr (M) ≤ k, 0 M I . (10)\nAlgorithm 2 `1-Regularized MSG (`1-RMSG)\nRequire: Input data {xt}Tt=1, output dimension k, regularization parameter µ Ensure: M̃ 1: M1 ← 0 2: for t = 1, . . . , T do 3: ηt ← 21+µ√d √ k t\n4: Mt+ 12 ← Mt + ηtxtx > t − µηtI 5: Mt+1 ←PM (\nMt+ 12 ) 6: end for 7: M̃← rounding(MT+1) {Algorithm 2 of (Warmuth\n& Kuzmin, 2008)}\nThe objective in Problem 10 is a linear function in M. Projected gradient descent on this problem yields the following updates:\nMt+1 = PM(Mt + ηtxtx>t − ηtµI), (11)\nwhere PM(·) projects onto the feasible set M with respect to the Frobenius norm. This gives the `1-RMSG procedure described in Algorithm 2. The design rationale motivating the `1 penalty is that it promotes low-rank iterates, thereby controlling computational cost per iteration. To see this, note that each update in equation (11) involves a shift by −µηtI, which shrinks the spectrum of Mt + ηtxtx>t by µηt. In other words, the value µηt will serves as a cut-off parameter that will zero out any eigenvalue smaller than µηt.\nAdmissible µ. As in the previous section, we are interested in µ such that the regularized problem has the same optimum as the original problem. Formally, we say that the regularization parameter µ takes an admissible value, if any solution to the regularized PCA Problem 10 is also a solution to the original Problem 4. The following lemma gives a sufficient condition on the admissibility of µ. Lemma 3.1. Let gk > 0. Then, for any regularization parameter 0 ≤ µ ≤ λk(C), the optimum of Problem 10 is unique, and is an optimum for Problem 4.\nKey insights. KKT first-order optimality condition on Problem 10 gives λk(C) = µ − γk + ωk + β, where γk, ωk, β ≥ 0 are Lagrange multipliers associated with the constraints λk(M) ≥ 0, λk(M) ≤ 1 and Tr M ≤ k. If µ > λk(C), and since β, ωk ≥ 0, it should hold that γk > 0. By complementary slackness (i.e. γkλk(M∗) = 0), we conclude that λk(M∗) = 0. In this case, M∗ cannot be a solution to Problem 4. This further implies that the condition in Lemma 3.1 is also necessary.\n3.1. Convergence Analysis of `1-RMSG Our first main result of this section gives a bound on the suboptimality of Algorithm 2 in terms of the PCA objective.\nTheorem 3.2. Assume E[‖x‖2] ≤ 1. Then, for any admissible regularization parameter µ ≤ min{λk2 , 1√ d }, after T\niterations of Algorithm 2 with step size ηt = 21+µ√d √ k t , and starting at M1 = 0, we have that:\nE[x>M∗x]− E[x>M̃x] ≤ 64 √ k log T√ T ,\nwhere M∗ is an optimum of Problem 4, M̃ is the output after rounding, and the expectation is with respect to the distribution D and the randomization in the algorithm.\nThe result above shows that for sufficiently small admissible µ, `1-RMSG converges at the same rate as MSG, i.e. O( log T√\nT ). However, we argue in the next section that in\nterms of the overall runtime, `1-RMSG has an advantage. We show formally that the `1 regularization prevents the rank of `1-RMSG iterates from growing too large.\n3.2. Rank Control for `1-RMSG\nTo get a handle on the rank of the iterates, we first need to show that iterates have a small tail, i.e. the bottom eigenvalues of Mt are small enough to get eliminated by the shrinkage step. The following lemma formalizes this intuition. Lemma 3.3. For all t = 1, . . . , T , it holds that\nd∑ i=k+1 λi(Mt) ≤ 1 gk 〈C− µI,M∗ −Mt〉,\nwhere gk = λk − λk+1 is the eigengap at k.\nNext, we need to show that if the tail of Mt is small, then the update Mt + ηtxtx>t will also have a small tail. Lemma 3.4. For any iterate t ∈ {1, . . . , T}, it holds that\nE[ d∑\ni=k+1\nλi(Mt + ηtxtx>t )] ≤ d∑\ni=k+1\nλi(Mt) + ηt.\nFinally, we argue that shrinking the spectrum by −µηtI will likely eliminate bottom eigenvalues of the update Mt + ηtxtx>t , resulting in a low rank iterate. Formally, our second main result of this section states the following. Theorem 3.5. Under the same assumptions as Theorem 3.2, for any admissible µ, we have that for all iterates\nE[rank(Mt+1)] ≤ k + 33 log t\nµgk\nNote that since µ ≤ 1/ √ d, it is easy to check that E[rank(Mt+1)] ≤ k + Õ( √ d/gk). Theorem 3.5 together with Theorem 3.2 demonstrates an interesting regime, where MSG and `1-RMSG show exact same statistical convergence behavior, but `1-RMSG iterates are guaranteed to have a significantly smaller rank than the input dimension.\nAlgorithm 3 `2 + `1-Regularized MSG (`2,1-RMSG)\nRequire: {xt}Tt=1, k, λ, µ Ensure: M̃\n1: M1 ← 0 2: for t = 1, . . . , T do 3: ηt ← 1λt 4: Mt+ 12 ← (1− ληt)Mt + ηtxtx > t − µηtI\n5: Mt+1 ←PM (\nMt+ 12 ) 6: end for 7: M̃←Prank−k(MT+1) {Return top-k subspace of MT+1}"
  }, {
    "heading": "4. PCA with Elastic-net Regularization",
    "text": "We saw in the previous sections that `2-RMSG for solving PCA with Frobenius norm regularization enjoys a faster convergence rate whereas `1-RMSG for solving PCA with trace norm regularization is better in terms of computational cost per iteration. In this section, we propose a variant that combines both the `1 and `2 regularization in the hope that it yields the best of both worlds, i.e. simultaneously provide fast rate with rank-control. We consider PCA with the following elastic-net regularization:\nmin M∈Rd×d\n− Ex[x>Mx] + µTr (M) + λ\n2 ‖M‖2F\nsubject to Tr (M) ≤ k, 0 M I . (12)\nSGD on Problem 12 yields the following updates:\nMt+1 = PM((1− ληt)Mt + ηtxtx>t − µηtI), (13)\nwhere PM(·) projects onto the feasible set M w.r.t the Frobenius norm; detailed procedure is given in Algorithm 3.\nAgain, we should be judicious in our choice of the regularization parameters µ and λ so as to ensure that the regularized problem has the same optimum as the original problem; the following result provides a sufficient condition.\nLemma 4.1 (Admissibility of (λ, µ)). Assume that gk > 0. Then, for any pair of regularization parameters λ and µ such that 0 < λ < gk and 0 ≤ λ + µ ≤ λk, the optimum of Problem 12 is unique, and is an optimum for Problem 4. We call any such (λ, µ)-pair an admissible regularization pair.\nWe conclude this section by noting that it is straightforward to adapt Theorem 2.4 to get a faster O( 1 ) iteration complexity for `2,1-RMSG, as long as we choose learning rate ηt = O( 1 t ). Also, one can show rank-control as in Theorem 3.5, under the learning rate ηt = O( 1√t ). It would be desirable to guarantee both faster statistical rates and computational cost per iteration simultaneously. Our current analysis falls short in establishing such a result. However, in Section 5, we provide empirical evidence to support statistical and computational benefits of `2,1-RMSG."
  }, {
    "heading": "5. Experimental Results",
    "text": "We provide empirical results for our proposed algorithms `2-RMSG, `1-RMSG, and `2,1-RMSG, compared to vanilla MSG, Oja’s algorithm, and Follow The Leader (FTL) algorithm, on both synthetic and real datasets. The synthetic data is drawn from a d = 100 dimensional zero-mean multivariate Gaussian distribution with an exponential decay in the spectrum of the covariance matrix. The synthetic consists of n = 30K samples, out of which 20K samples are used for training and 5K each for tuning and testing. For comparisons on a real dataset, we choose MNIST which consists of n = 60K samples each of size d = 784.\nThe plots in Figures 2 and 3 correspond to the progress\nin terms of suboptimality in objective as a function of number of samples (top) and the CPU clock time (bottom) for the synthetic dataset and MNIST, respectively. Figure 4 tracks the rank of the iterates for various algorithms. All plots are averaged over 100 runs of the algorithms. The runtime is captured in a controlled setting – each run for every algorithm was on a dedicated identical compute node. The target dimensionality in our experiments is k ∈ {20, 25, 30, 35, 40}, however we observed similar behavior for other values of k as well.\nFor MSG and `1-RMSG, the learning rate is set to η0√t , and for `2-RMSG, `2,1-RMSG and Oja the learning rate was set to η0t as suggested by theory. We choose η0 (ini-\ntial learning rate), λ and µ by tuning2 each over the set {10−3, 10−2, 10−1, 1, 10, 102, 103} on held-out data, for k = 40. These parameters are then used for all experiments (different values of k). Few remarks are in order.\nIteration complexity. On both the synthetic and the MNIST datasets, we observe that `2-RMSG and `2,1-RMSG enjoy faster convergence (better iteration complexity) compared to other MSG variants, as suggested by theory. Surprisingly, Oja’s algorithm, despite having a fast O( 1t ) iteration complexity, is always among the slowest to converge; we remark that similar behavior was noted in previous studies as well (Arora et al., 2012).\nRank of iterates. As can be seen in Figure 4, the rank of the iterates of `2-RMSG and vanilla MSG quickly increases and hits the maximum while `1-RMSG and `2,1-RMSG exhibit good control on the rank of the intermediate iterates. For Oja’s algorithm and FTL, by construction, the rank of the iterates is always equal to the desired rank k.\nOverall runtime. It can be seen that `2,1-RMSG and `1- RMSG consistently outperform other stochastic algorithms. This is interesting because `2-RMSG has better iteration complexity than `1-RMSG, but since `1-RMSG controls the rank of the intermediate iterates, it enjoys a better overall runtime. Recall that each iteration of MSG variants requires O(dk′2) runtime, where k′ is the rank of the current iterate.\nOverall analysis: In our experiments, `1-RMSG and `2,1- RMSG consistently outperform other stochastic algorithms including vanilla MSG, `2-RMSG and Oja’s algorithm. While `2-RMSG, `2,1-RMSG and Oja’s algorithm enjoy optimal O( 1t ) iteration complexity, in our experiments, Oja’s algorithm is always dominated by `2-RMSG and `2,1- RMSG both in iteration complexity as well as the overall runtime. This makes a strong case for MSG variants since Oja’s algorithm has optimal computational cost per iteration (linear in input dimension)."
  }, {
    "heading": "6. Discussion",
    "text": "In this paper, we study variants of stochastic gradient descent for a convex relaxation of principal component analysis (PCA), with `2, `1, and `2,1 regularization. We characterize sufficient conditions on the regularization parameters under which an optimum of the regularized problem is also an optimum of the original problem. We show that SGD on the `2-regularized problem, which we term `2-RMSG, achieves optimal O( 1 ) iteration complexity, whereas SGD on the `1-regularized problem, which we term `1-RMSG, provides better control on the rank of the intermediate iter-\n2The performance measure is the PCA objective on the heldout set, after consecutive iterates of the algorithm doesn’t improve the objective by a certain amount.\nates, which results in smaller computation cost per iteration. We also study `2,1-RMSG, which leverages both `1- and `2- regularization, with the goal of simultaneously improving iteration complexity and computational cost per iteration.\nOur analysis shows that if the learning rate and regularization parameters are chosen appropriately, the expected rank of the iterates of `1-RMSG is upper bounded by Õ( √ d/gk). While this results in significant improvement over the per iteration computational cost of MSG (from worst case bound ofO(d3) toO(d2)), it is not yet optimal. In particular, Oja’s algorithm enjoys per iteration cost of O(dk2). Bridging this gap will be the subject for future work.\nWe provide empirical evidence of our theoretical findings, by comparing several stochastic algorithms on both synthetic and real datasets. Our experiments suggest that `2- and `2,1-RMSG are fastest in terms of iteration complexity, while `1- and `2,1-RMSG usually enjoy the fastest overall runtime. While our iteration complexity results in Section 2 provide minimax optimal rates for `2-RMSG and close the gap between iteration complexity of SGD on convex and non-convex programs in Problems 1 and 4, respectively, our analysis fails to provide a certificate for best overall runtime. However, our experiments show that `2,1-RMSG does benefit from the `2- and `1-regularizations, both in the iteration complexity as well as the computational cost per iterate. Providing theoretical guarantees for this empirical finding is another open question we leave to future work.\nAnother interesting research direction is to revisit issues of statistical and computational efficiency for convex relaxations for related component analysis techniques such as partial least squares (Arora et al., 2016) and canonical correlation analysis (Arora et al., 2017) compared with counterparts based on Oja’s algorithm (Ge et al., 2016). Finally, it is natural to consider extensions of the methods proposed here to noisy streaming settings of Marinov et al. (2018)."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by NSF BIGDATA grant IIS-1546482."
  }],
  "year": 2018,
  "references": [{
    "title": "First efficient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate",
    "authors": ["Allen-Zhu", "Zeyuan", "Li", "Yuanzhi"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2017
  }, {
    "title": "Stochastic optimization for PCA and PLS",
    "authors": ["Arora", "Raman", "Cotter", "Andrew", "Livescu", "Karen", "Srebro", "Nathan"],
    "venue": "In Communication, Control, and Computing (Allerton),",
    "year": 2012
  }, {
    "title": "Stochastic optimization of PCA with capped msg",
    "authors": ["Arora", "Raman", "Cotter", "Andy", "Srebro", "Nati"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Stochastic optimization for multiview representation learning using partial least squares",
    "authors": ["Arora", "Raman", "Mianjy", "Poorya", "Marinov", "Teodor"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Stochastic approximation for canonical correlation analysis",
    "authors": ["Arora", "Raman", "Marinov", "Teodor Vanislavov", "Mianjy", "Poorya", "Srebro", "Nati"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "An improved gap-dependency analysis of the noisy power method",
    "authors": ["Balcan", "Maria-Florina", "Du", "Simon S", "Wang", "Yining", "Yu", "Adams Wei"],
    "venue": "In 29th Annual Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "The fast convergence of incremental PCA",
    "authors": ["Balsubramani", "Akshay", "Dasgupta", "Sanjoy", "Freund", "Yoav"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Streaming PCA: Matching matrix bernstein and near-optimal finite sample guarantees for ojas algorithm",
    "authors": ["Jain", "Prateek", "Jin", "Chi", "Kakade", "Sham M", "Netrapalli", "Praneeth", "Sidford", "Aaron"],
    "venue": "In 29th Annual Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Streaming principal component analysis in noisy settings",
    "authors": ["Marinov", "Teodor Vanislavov", "Mianjy", "Poorya", "Arora", "Raman"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2018
  }, {
    "title": "Memory limited, streaming PCA",
    "authors": ["Mitliagkas", "Ioannis", "Caramanis", "Constantine", "Jain", "Prateek"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Simplified neuron model as a principal component analyzer",
    "authors": ["Oja", "Erkki"],
    "venue": "Journal of mathematical biology,",
    "year": 1982
  }, {
    "title": "Making gradient descent optimal for strongly convex stochastic optimization",
    "authors": ["Rakhlin", "Alexander", "Shamir", "Ohad", "Sridharan", "Karthik"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2012
  }, {
    "title": "Fast stochastic algorithms for SVD and PCA: Convergence properties and convexity",
    "authors": ["Shamir", "Ohad"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
    "authors": ["Shamir", "Ohad", "Zhang", "Tong"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Topics in random matrix theory, volume 132",
    "authors": ["Tao", "Terence"],
    "venue": "American Mathematical Society Providence,",
    "year": 2012
  }, {
    "title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension",
    "authors": ["Warmuth", "Manfred K", "Kuzmin", "Dima"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }],
  "id": "SP:23d95f2f7bdb16ceeb35b45348a2ca89ea8330fd",
  "authors": [{
    "name": "Poorya Mianjy",
    "affiliations": []
  }, {
    "name": "Raman Arora",
    "affiliations": []
  }],
  "abstractText": "We revisit convex relaxation based methods for stochastic optimization of principal component analysis (PCA). While methods that directly solve the nonconvex problem have been shown to be optimal in terms of statistical and computational efficiency, the methods based on convex relaxation have been shown to enjoy comparable, or even superior, empirical performance – this motivates the need for a deeper formal understanding of the latter. Therefore, in this paper, we study variants of stochastic gradient descent for a convex relaxation of PCA with (a) `2, (b) `1, and (c) elastic net (`1 +`2) regularization in the hope that these variants yield (a) better iteration complexity, (b) better control on the rank of the intermediate iterates, and (c) both, respectively. We show, theoretically and empirically, that compared to previous work on convex relaxation based methods, the proposed variants yield faster convergence and improve overall runtime to achieve a certain userspecified -suboptimality on the PCA objective. Furthermore, the proposed methods are shown to converge both in terms of the PCA objective as well as the distance between subspaces. However, there still remains a gap in computational requirements for the proposed methods when compared with existing nonconvex approaches.",
  "title": "Stochastic PCA with `2 and `1 Regularization"
}