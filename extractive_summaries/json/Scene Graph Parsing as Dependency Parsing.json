{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 397–407 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Recent years have witnessed the rise of interest in many tasks at the intersection of computer vision and natural language processing, including semantic image retrieval (Johnson et al., 2015; Vendrov et al., 2015), image captioning (Mao et al., 2014; Karpathy and Li, 2015; Donahue et al., 2015; Liu et al., 2017b), visual question answering (Antol et al., 2015; Zhu et al., 2016; Andreas et al., 2016), and referring expressions (Hu et al., 2016; Mao et al., 2016; Liu et al., 2017a). The pursuit for these tasks is in line with people’s desire for high level understanding of visual content, in particular, using textual descriptions or questions to help understand or express images and scenes.\n1Code is available at https://github.com/ Yusics/bist-parser/tree/sgparser\nWhat is shared among all these tasks is the need for a common representation to establish connection between the two different modalities. The majority of recent works handle the vision side with convolutional neural networks, and the language side with recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) or word embeddings (Mikolov et al., 2013; Pennington et al., 2014). In either case, neural networks map original sources into a semantically meaningful (Donahue et al., 2014; Mikolov et al., 2013) vector representation that can be aligned through end-toend training (Frome et al., 2013). This suggests that the vector embedding space is an appropriate choice as the common representation connecting different modalities (see e.g. Kaiser et al. (2017)).\nWhile the dense vector representation yields impressive performance, it has an unfortunate limitation of being less intuitive and hard to interpret. Scene graphs (Johnson et al., 2015), on the other hand, proposed a type of directed graph to encode information in terms of objects, attributes of objects, and relationships between objects (see Figure 1 for visualization). This is a more structured and explainable way of expressing the knowledge from either modality, and is able to serve as an alternative form of common representation. In fact, the value of scene graph representation has already been proven in a wide range of visual tasks, including semantic image retrieval (Johnson et al., 2015), caption quality evaluation (Anderson et al., 2016), etc. In this paper, we focus on scene graph generation from textual descriptions.\nPrevious attempts at this problem (Schuster et al., 2015; Anderson et al., 2016) follow the same spirit. They first use a dependency parser to obtain the dependency relationship for all words in a sentence, and then use either a rule-based or a learned classifier as post-processing to generate the scene graph. However, the rule-based classifier cannot\n397\nlearn from data, and the learned classifier is rather simple with hand-engineered features. In addition, the dependency parser was trained on linguistics data to produce complete dependency trees, some parts of which may be redundant and hence confuse the scene graph generation process.\nTherefore, our model abandons the two-stage pipeline, and uses a single, customized dependency parser instead. The customization is necessary for two reasons. First is the difference in label space. Standard dependency parsing has tens of edge labels to represent rich relationships between words in a sentence, but in scene graphs we are only interested in three types, namely objects, attributes, and relations. Second is whether every word needs a head. In some sense, the scene graph represents the “skeleton” of the sentence, which suggests that empty words are unlikely to be included in the scene graph. We argue that in scene graph generation, it is unnecessary to require a parent word for every single word.\nWe build our model on top of a neural depen-\ndency parser implementation (Kiperwasser and Goldberg, 2016) that is among the state-of-theart. We show that our carefully customized dependency parser is able to generate high quality scene graphs by learning from data. Specifically, we use the Visual Genome dataset (Krishna et al., 2017), which provides rich amounts of region description - region graph pairs. We first align nodes in region graphs with words in the region descriptions using simple rules, and then use this alignment to train our customized dependency parser. We evaluate our parser by computing the F-score between the parsed scene graphs and ground truth scene graphs. We also apply our approach to image retrieval to show its effectiveness."
  }, {
    "heading": "2 Related Works",
    "text": ""
  }, {
    "heading": "2.1 Scene Graphs",
    "text": "The scene graph representation was proposed in Johnson et al. (2015) as a way to represent the rich, structured knowledge within an image. The nodes in a scene graph represent either an object, an attribute for an object, or a relationship between two objects. The edges depict the connection and association between two nodes. This representation is later adopted in the Visual Genome dataset (Krishna et al., 2017), where a large number of scene graphs are annotated through crowd-sourcing.\nThe scene graph representation has been proved useful in various problems including semantic image retrieval (Johnson et al., 2015), visual question answering (Teney et al., 2016), 3D scene synthesis (Chang et al., 2014), and visual relationship detection (Lu et al., 2016). Excluding Johnson et al. (2015) which used ground truth, scene graphs are obtained either from images (Dai et al., 2017; Xu et al., 2017; Li et al., 2017) or from textual descriptions (Schuster et al., 2015; Anderson et al., 2016). In this paper we focus on the latter.\nIn particular, parsed scene graphs are used in Schuster et al. (2015) for image retrieval. We show that with our more accurate scene graph parser, performance on this task can be further improved."
  }, {
    "heading": "2.2 Parsing to Graph Representations",
    "text": "The goal of dependency parsing (Kübler et al., 2009) is to assign a parent word to every word in a sentence, and every such connection is associated with a label. Dependency parsing is particularly suitable for scene graph generation because it directly models the relationship between individual\nwords without introducing extra nonterminals. In fact, all previous work (Schuster et al., 2015; Anderson et al., 2016) on scene graph generation run dependency parsing on the textual description as a first step, followed by either heuristic rules or simple classifiers. Instead of running two separate stages, our work proposed to use a single dependency parser that is end-to-end. In other words, our customized dependency parser generates the scene graph in an online fashion as it reads the textual description once from left to right.\nIn recent years, dependency parsing with neural network features (Chen and Manning, 2014; Dyer et al., 2015; Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Shi et al., 2017) has shown impressive performance. In particular, Kiperwasser and Goldberg (2016) used bidirectional LSTMs to generate features for individual words, which are then used to predict parsing actions. We base our model on Kiperwasser and Goldberg (2016) for both its simplicity and good performance.\nApart from dependency parsing, Abstract Meaning Representation (AMR) parsing (Flanigan et al., 2014; Werling et al., 2015; Wang et al., 2015; Konstas et al., 2017) may also benefit scene graph generation. However, as first pointed out in Anderson et al. (2016), the use of dependency trees still appears to be a common theme in the literature, and we leave the exploration of AMR parsing for scene graph generation as future work.\nMore broadly, our task also relates to entity and relation extraction, e.g. Katiyar and Cardie (2017), but there object attributes are not handled. Neural module networks (Andreas et al., 2016) also use dependency parses, but they translate questions into a series of actions, whereas we parse descriptions into their graph form. Finally, Krishnamurthy and Kollar (2013) connected parsing and grounding by training the parser in a weakly supervised fashion."
  }, {
    "heading": "3 Task Description",
    "text": "In this section, we begin by reviewing the scene graph representation, and show how its nodes and edges relate to the words and arcs in dependency parsing. We then describe simple yet reliable rules to align nodes in scene graphs with words in textual descriptions, such that customized dependency parsing, described in the next section, may be trained and applied."
  }, {
    "heading": "3.1 Scene Graph Definition",
    "text": "There are three types of nodes in a scene graph: object, attribute, and relation. Let O be the set of object classes, A be the set of attribute types, and R be the set of relation types. Given a sentence s, our goal in this paper is to parse s into a scene graph:\nG(s) = 〈O(s), A(s), R(s)〉 (1)\nwhere O(s) = {o1(s), . . . , om(s)}, oi(s) ∈ O is the set of object instances mentioned in s, A(s) ⊆ O(s) × A is the set of attributes associated with object instances, and R(s) ⊆ O(s)×R×O(s) is the set of relations between object instances. G(s) is a graph because we can first create an object node for every element inO(s); then for every (o, a) pair in A(s), we create an attribute node and add an unlabeled edge o→ a; finally for every (o1, r, o2) triplet inR(s), we create a relation node and add two unlabeled edges o1 → r and r → o2. The resulting directed graph exactly encodes information in G(s). We call this the node-centric graph representation of a scene graph.\nWe realize that a scene graph can be equivalently represented by no longer distinguishing between the three types of nodes, yet assigning labels to the edges instead. Concretely, this means there is now only one type of node, but we assign a ATTR label for every o→ a edge, a SUBJ label for every o1 → r edge, and a OBJT label for every r → o2 edge. We call this the edge-centric graph representation of a scene graph.\nWe can now establish a connection between scene graphs and dependency trees. Here we only consider scene graphs that are acyclic2. The edgecentric view of a scene graph is very similar to a dependency tree: they are both directed acyclic graphs where the edges/arcs have labels. The difference is that in a scene graph, the nodes are the objects/attributes/relations and the edges have label space {ATTR, SUBJ, OBJT}, whereas in a dependency tree, the nodes are individual words in a sentence and the edges have a much larger label space."
  }, {
    "heading": "3.2 Sentence-Graph Alignment",
    "text": "We have shown the connection between nodes in scene graphs and words in dependency parsing. With alignment between nodes in scene\n2In Visual Genome, only 4.8% region graphs have cyclic structures.\ngraphs and words in the textual description, scene graph generation and dependency parsing becomes equivalent: we can construct the generated scene graph from the set of labeled edges returned by the dependency parser. Unfortunately, such alignment is not provided between the region graphs and region descriptions in the Visual Genome (Krishna et al., 2017) dataset. Here we describe how we use simple yet reliable rules to do sentence-graph (word-node) alignment.\nThere are two strategies that we could use in deciding whether to align a scene graph node d (whose label space is O ∪ A ∪ R) with a word/phrase w in the sentence:\n• Word-by-word match (WBW): d ↔ w only when d’s label and w match word-for-word.\n• Synonym match (SYN)3: d ↔ w when the wordnet synonyms of d’s label contain w.\nObviously WBW is a more conservative strategy than SYN.\nWe propose to use two cycles and each cycle further consists of three steps, where we try to align objects, attributes, relations in that order. The pseudocode for the first cycle is in Algorithm 1. The second cycle repeats line 4-15 immediately afterwards, except that in line 6 we also allow SYN. Intuitively, in the first cycle we use a conservative strategy to find “safe” objects, and then scan for their attributes and relations. In the second cycle we relax and allow synonyms in aligning object nodes, also followed by the alignment of attribute and relation nodes.\nThe ablation study of the alignment procedure is reported in the experimental section."
  }, {
    "heading": "4 Customized Dependency Parsing",
    "text": "In the previous section, we have established the connection between scene graph generation and dependency parsing, which assigns a parent word for every word in a sentence, as well as a label for this directed arc. We start by describing our base dependency parsing model, which is neural network based and performs among the state-of-theart. We then show why and how we do customization, such that scene graph generation is achieved with a single, end-to-end model.\n3This strategy is also used in (Denkowski and Lavie, 2014) and (Anderson et al., 2016).\nAlgorithm 1: First cycle of the alignment procedure.\n1 Input: Sentence s; Scene graph G(s) 2 Initialize aligned nodes N as empty set 3 Initialize aligned words W as empty set 4 for o in object nodes of G(s) \\N do 5 for w in s \\W do 6 if o↔ w according to WBW then 7 Add (o, w); N = N ∪ {o};\nW =W ∪ {w}\n8 for a in attribute nodes of G(s) \\N do 9 for w in s \\W do\n10 if a↔ w according to WBW or SYN and a’s object node is in N then 11 Add (a,w); N = N ∪ {a}; W =W ∪ {w}\n12 for r in relation nodes of G(s) \\N do 13 for w in s \\W do 14 if r ↔ w according to WBW or SYN\nand r’s subject and object nodes are both in N then\n15 Add (r, w); N = N ∪ {r}; W =W ∪ {w}"
  }, {
    "heading": "4.1 Neural Dependency Parsing Base Model",
    "text": "We base our model on the transition-based parser of Kiperwasser and Goldberg (2016). Here we describe its key components: the arc-hybrid system that defines the transition actions, the neural architecture for feature extractor and scoring function, and the loss function.\nThe Arc-Hybrid System In the arc-hybrid system, a configuration consists of a stack σ, a buffer β, and a set T of dependency arcs. Given a sentence s = w1, . . . , wn, the system is initialized with an empty stack σ, an empty arc set T , and β = 1, . . . , n,ROOT, where ROOT is a special index. The system terminates when σ is empty and β contains only ROOT. The dependency tree is given by the arc set T upon termination.\nThe arc-hybrid system allows three transition actions, SHIFT, LEFTl, RIGHTl, described in Table 1. The SHIFT transition moves the first element of the buffer to the stack. The LEFT(l) transition yields an arc from the first element of the buffer to the top element of the stack, and then removes the top element from the stack. The\nRIGHT(l) transition yields an arc from the second top element of the stack to the top element of the stack, and then also removes the top element from the stack.\nThe following paragraphs describe how to select the correct transition action (and label l) in each step in order to generate a correct dependency tree.\nBiLSTM Feature Extractor Let the word embeddings of a sentence s be w1, . . . ,wn. An LSTM cell is a parameterized function that takes as input wt, and updates its hidden states:\nLSTM cell : (wt,ht−1)→ ht (2)\nAs a result, an LSTM network, which simply applies the LSTM cell t times, is a parameterized function mapping a sequence of input vectors w1:t to a sequence of output vectors h1:t. In our notation, we drop the intermediate vectors h1:t−1 and let LSTM(w1:t) represent ht.\nA bidirectional LSTM, or BiLSTM for short, consists of two LSTMs: LSTMF which reads the input sequence in the original order, and LSTMB which reads it in reverse. Then\nBILSTM(w1:n, i) =\nLSTMF (w1:i) ◦ LSTMB(wn:i) (3)\nwhere ◦ denotes concatenation. Intuitively, the forward LSTM encodes information from the left side of the i-th word and the backward LSTM encodes information to its right, such that the vector vi = BILSTM(w1:n, i) has the full sentence as context.\nWhen predicting the transition action, the feature function φ(c) that summarizes the current configuration c = (σ, β, T ) is simply the concatenated BiLSTM vectors of the top three elements in the stack and the first element in the buffer:\nφ(c) = vs2 ◦ vs1 ◦ vs0 ◦ vb0 (4)\nMLP Scoring Function The score of transition action y under the current configuration c is determined by a multi-layer perceptron with one hidden layer:\nf(c, y) =MLP (φ(c))[y] (5)\nwhere\nMLP (x) =W2 · tanh(W1 · x+ b1) + b2 (6)\nHinge Loss Function The training objective is to raise the scores of correct transitions above scores of incorrect ones. Therefore, at each step, we use a hinge loss defined as:\nL = max(0, 1− max y+∈Y + f(c, y+)\n+ max y−∈Y \\Y +\nf(c, y−)) (7)\nwhere Y is the set of possible transitions and Y + is the set of correct transitions at the current step. In each training step, the parser scores all possible transitions using Eqn. 5, incurs a loss using Eqn. 7, selects a following transition, and updates the configuration. Losses at individual steps are summed throughout the parsing of a sentence, and then parameters are updated using backpropagation.\nIn test time, we simply choose the transition action that yields the highest score at each step."
  }, {
    "heading": "4.2 Customization",
    "text": "In order to generate scene graphs with dependency parsing, modification is necessary for at least two reasons. First, we need to redefine the label space of arcs so as to reflect the edge-centric representation of a scene graph. Second, not every word in the sentence will be (part of) a node in the scene graph (see Figure 2 for an example). In other words, some words in the sentence may not have a parent word, which violates the dependency parsing setting. We tackle these two challenges by redesigning the edge labels and expanding the set of transition actions.\nRedesigning Edge Labels We define a total of five edge labels, so as to faithfully bridge the edgecentric view of scene graphs with dependency parsing models:\n• CONT: This label is created for nodes whose label is a phrase. For example, the phrase “in front of” is a single relation node in the scene graph. By introducing the CONT label, we expect the parsing result to be either\nin CONT−−−→ front CONT−−−→ of (8)\nor in CONT←−−− front CONT←−−− of (9)\nwhere the direction of the arcs (left or right) is predefined by hand.\nThe leftmost word under the right arc rule or the rightmost word under the left arc rule is called the head of the phrase. A single-word node does not need this CONT label, and the head is itself.\n• ATTR: The arc label from the head of an object node to the head of an attribute node.\n• SUBJ: The arc label from the head of an object node (subject) to the head of a relation node.\n• OBJT: The arc label from the head of a relation node to the head of an object node (object).\n• BEGN: The arc label from the ROOT index to all heads of object nodes without a parent.\nExpanding Transition Actions With the three transition actions SHIFT, LEFT(l), RIGHT(l), we only drop an element (from the top of the stack) after it has already been associated with an arc. This design ensures that an arc is associated with every word. However, in our setting for scene graph generation, there may be no arc for some of the words, especially empty words.\nOur solution is to augment the action set with a REDUCE action, that pops the stack without adding to the arc set (see Table 1). This action is often used in other transition-based dependency parsing systems (e.g. arc-eager (Nivre, 2004)). More recently, Hershcovich et al. (2017) and Buys and Blunsom (2017) also included this action when parsing sentences to graph structures.\nWe still minimize the loss function defined in Eqn. 7, except that now |Y | increases from 3 to 4. During training, we impose the oracle to select the REDUCE action when it is in Y +. In terms of loss function, we increment by 1 the loss incurred by the other 3 transition actions if REDUCE incurs zero loss."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Implementation Details",
    "text": "We train and evaluate our scene graph parsing model on (a subset of) the Visual Genome (Krishna et al., 2017) dataset. Each image in Visual Genome contains a number of regions, and each region is annotated with both a region description and a region scene graph. Our training set is the intersection of Visual Genome and MS COCO (Lin et al., 2014) train2014 set, which contains a total of 34027 images/ 1070145 regions. We evaluate on the intersection of Visual Genome and MS COCO val2014 set, which contains a total of 17471 images/ 547795 regions.\nIn our experiments, the number of hidden units in BiLSTM is 256; the number of layers in BiLSTM is 2; the word embedding dimension is 200; the number of hidden units in MLP is 100. We use fixed learning rate 0.001 and Adam optimizer (Kingma and Ba, 2014) with epsilon 0.01. Training usually converges within 4 epochs.\nWe will release our code and trained model upon acceptance."
  }, {
    "heading": "5.2 Quality of Parsed Scene Graphs",
    "text": "We use a slightly modified version of SPICE score (Anderson et al., 2016) to evaluate the quality of\nscene graph parsing. Specifically, for every region, we parse its description using a parser (e.g. the one used in SPICE or our customized dependency parser), and then calculate the F-score between the parsed graph and the ground truth region graph (see Section 3.2 of Anderson et al. (2016) for more details). Note that when SPICE calculates the Fscore, a node in one graph could be matched to several nodes in the other, which is problematic. We fix this and enforce one-to-one matching when calculating the F-score. Finally, we report the average F-score across all regions.\nTable 2 summarizes our results. We see that our customized dependency parsing model achieves\nan average F-score of 49.67%, which significantly outperforms the parser used in SPICE by 5 percent. This result shows that our customized dependency parser is very effective at learning from data, and generates more accurate scene graphs than the best previous approach.\nAblation Studies First, we study how the sentence-graph alignment procedure affects the final performance. Recall that our procedure involves two cycles, each with three steps. Of the six steps, synonym match (SYN) is only not used in the first step. We tried two more settings, where SYN is either used in all six steps or none of the six steps. We can see from Table 2 that the final\nF-score drops in both cases, hence supporting the procedure that we chose.\nSecond, we study whether changing the direction of CONT arcs from pointing left to pointing right will make much difference. Table 2 shows that the two choices give very similar performance, suggesting that our dependency parser is robust to this design choice.\nFinally, we report the oracle score, which is the similarity between the aligned graphs that we use during training and the ground truth graphs. The F-score is relatively high at 69.85%. This shows that improving the parser (about 20% margin) and improving the sentence-graph alignment (about 30% margin) are both promising directions for future research.\nQualitative Examples We provide one parsing example in Figure 2 and Figure 3. This is a sentence that is relatively simple, and the underlying scene graph includes two object nodes, one attribute node, and one compound word relation node. In parsing this sentence, all four actions listed in Table 1 are used (see Figure 3) to produce the edge-centric scene graph (bottom left of Figure 2), which is then trivially converted to the node-centric scene graph (bottom right of Figure 2)."
  }, {
    "heading": "5.3 Application in Image Retrieval",
    "text": "We test if the advantage of our parser can be propagated to computer vision tasks, such as image retrieval. We directly compare our parser with the Stanford Scene Graph Parser (Schuster et al., 2015) on the development set and test set of the image retrieval dataset used in Schuster et al. (2015) (not Visual Genome).\nFor every region in an image, there is a humanannotated region description and region scene graph. The queries are the region descriptions. If the region graph corresponding to the query is a subgraph of the complete graph of another image,\nthen that image is added to the ground truth set for this query. All these are strictly following Schuster et al. (2015). However, since we did not obtain nor reproduce the CRF model used in Johnson et al. (2015) and Schuster et al. (2015), we used F-score similarity instead of the likelihood of the maximum a posteriori CRF solution when ranking the images based on the region descriptions. Therefore the numbers we report in Table 3 are not directly comparable with those reported in Schuster et al. (2015).\nOur parser delivers better retrieval performance across all three evaluation metrics: recall@5, recall@10, and median rank. We also notice that the numbers in our retrieval setting are higher than those (even with oracle) in Schuster et al. (2015)’s retrieval setting. This strongly suggests that generating accurate scene graphs from images is a very promising research direction in image retrieval, and grounding parsed scene graphs to bounding box proposals without considering visual attributes/relationships (Johnson et al., 2015) is suboptimal."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we offer a new perspective and solution to the task of parsing scene graphs from textual descriptions. We begin by moving the labels/types from the nodes to the edges and introducing the edge-centric view of scene graphs. We further show that the gap between edge-centric scene graphs and dependency parses can be filled with a careful redesign of label and action space. This motivates us to train a single, customized, end-to-end neural dependency parser for this task, as opposed to prior approaches that used generic dependency parsing followed by heuristics or simple classifier. We directly train our parser on a subset of Visual Genome (Krishna et al., 2017), without transferring any knowledge from Penn Treebank (Marcus et al., 1993) as previous works did.\nThe quality of our trained parser is validated in terms of both SPICE similarity to the ground truth graphs and recall rate/median rank when performing image retrieval.\nWe hope our paper can lead to more thoughts on the creative uses and extensions of existing NLP tools to tasks and datasets in other domains. In the future, we plan to tackle more computer vision tasks with this improved scene graph parsing technique in hand, such as image region grounding. We also plan to investigate parsing scene graph with cyclic structures, as well as whether/how the image information can help boost parsing quality."
  }, {
    "heading": "Acknowledgments",
    "text": "The majority of this work was done when YSW and XZ were visiting Johns Hopkins University. We thank Peter Anderson, Sebastian Schuster, Ranjay Krishna, Tsung-Yi Lin for comments and help regarding the experiments. We also thank Tianze Shi, Dingquan Wang, Chu-Cheng Lin for discussion and feedback on the draft. This work was sponsored by the National Science Foundation Center for Brains, Minds, and Machines NSF CCF-1231216. CL also acknowledges an award from Snap Inc."
  }],
  "year": 2018,
  "references": [{
    "title": "SPICE: semantic propositional image caption evaluation",
    "authors": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."],
    "venue": "ECCV . Springer, volume 9909 of Lecture Notes in Computer Science, pages 382–398.",
    "year": 2016
  }, {
    "title": "Neural module networks",
    "authors": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."],
    "venue": "CVPR. IEEE Computer Society, pages 39–48.",
    "year": 2016
  }, {
    "title": "VQA: visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."],
    "venue": "ICCV . IEEE Computer Society, pages 2425–2433.",
    "year": 2015
  }, {
    "title": "Robust incremental neural semantic graph parsing",
    "authors": ["Jan Buys", "Phil Blunsom."],
    "venue": "ACL. Association for Computational Linguistics, pages 1215–1226.",
    "year": 2017
  }, {
    "title": "Learning spatial knowledge for text to 3d scene generation",
    "authors": ["Angel X. Chang", "Manolis Savva", "Christopher D. Manning."],
    "venue": "EMNLP. ACL, pages 2028–2038.",
    "year": 2014
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher D. Manning."],
    "venue": "EMNLP. ACL, pages 740–750.",
    "year": 2014
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "EMNLP. ACL,",
    "year": 2014
  }, {
    "title": "Incremental parsing with minimal features using bi-directional LSTM",
    "authors": ["James Cross", "Liang Huang."],
    "venue": "ACL (2). The Association for Computer Linguistics.",
    "year": 2016
  }, {
    "title": "Detecting visual relationships with deep relational networks",
    "authors": ["Bo Dai", "Yuqi Zhang", "Dahua Lin."],
    "venue": "CVPR. IEEE Computer Society, pages 3298– 3308.",
    "year": 2017
  }, {
    "title": "Meteor universal: Language specific translation evaluation for any target language",
    "authors": ["Michael J. Denkowski", "Alon Lavie."],
    "venue": "WMT@ACL. The Association for Computer Linguistics, pages 376–380.",
    "year": 2014
  }, {
    "title": "Long-term recurrent convolutional networks for visual recognition and description",
    "authors": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko."],
    "venue": "CVPR. IEEE Computer",
    "year": 2015
  }, {
    "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
    "authors": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell."],
    "venue": "ICML. JMLR.org, volume 32 of JMLR Workshop and Con-",
    "year": 2014
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "CoRR abs/1611.01734.",
    "year": 2016
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."],
    "venue": "ACL (1). The Association for Computer Linguistics, pages 334–343.",
    "year": 2015
  }, {
    "title": "A discriminative graph-based parser for the abstract meaning representation",
    "authors": ["Jeffrey Flanigan", "Sam Thomson", "Jaime G. Carbonell", "Chris Dyer", "Noah A. Smith."],
    "venue": "ACL (1). The Association for Computer Linguistics, pages 1426–1436.",
    "year": 2014
  }, {
    "title": "Devise: A deep visualsemantic embedding model",
    "authors": ["Andrea Frome", "Gregory S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc’Aurelio Ranzato", "Tomas Mikolov"],
    "venue": "In NIPS",
    "year": 2013
  }, {
    "title": "A transition-based directed acyclic graph parser for UCCA",
    "authors": ["Daniel Hershcovich", "Omri Abend", "Ari Rappoport."],
    "venue": "ACL. Association for Computational Linguistics, pages 1127–1138.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Natural language object retrieval",
    "authors": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell."],
    "venue": "CVPR. IEEE Computer Society, pages 4555–4564.",
    "year": 2016
  }, {
    "title": "Image retrieval using scene graphs",
    "authors": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "LiJia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li."],
    "venue": "CVPR. IEEE Computer Society, pages 3668–3678.",
    "year": 2015
  }, {
    "title": "One model to learn them all",
    "authors": ["Lukasz Kaiser", "Aidan N. Gomez", "Noam Shazeer", "Ashish Vaswani", "Niki Parmar", "Llion Jones", "Jakob Uszkoreit."],
    "venue": "CoRR abs/1706.05137.",
    "year": 2017
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Fei-Fei Li."],
    "venue": "CVPR. IEEE Computer Society, pages 3128–3137.",
    "year": 2015
  }, {
    "title": "Going out on a limb: Joint extraction of entity mentions and relations without dependency trees",
    "authors": ["Arzoo Katiyar", "Claire Cardie."],
    "venue": "ACL. Association for Computational Linguistics, pages 917–928.",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "TACL 4:313– 327.",
    "year": 2016
  }, {
    "title": "Neural AMR: sequence-to-sequence models for parsing and generation",
    "authors": ["Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer."],
    "venue": "ACL (1). Association for Computational Linguistics, pages 146–157.",
    "year": 2017
  }, {
    "title": "Visual genome: Connecting language and vision",
    "authors": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Li Fei-Fei"],
    "year": 2017
  }, {
    "title": "Jointly learning to parse and perceive: Connecting natural language to the physical world",
    "authors": ["Jayant Krishnamurthy", "Thomas Kollar."],
    "venue": "TACL 1:193–206.",
    "year": 2013
  }, {
    "title": "Dependency Parsing",
    "authors": ["Sandra Kübler", "Ryan T. McDonald", "Joakim Nivre."],
    "venue": "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers.",
    "year": 2009
  }, {
    "title": "Scene graph generation from objects, phrases and caption regions",
    "authors": ["Yikang Li", "Wanli Ouyang", "Bolei Zhou", "Kun Wang", "Xiaogang Wang."],
    "venue": "CoRR abs/1707.09700.",
    "year": 2017
  }, {
    "title": "Microsoft COCO: common objects in context",
    "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick."],
    "venue": "ECCV . Springer, volume 8693 of Lecture Notes in Computer Science,",
    "year": 2014
  }, {
    "title": "Recurrent multimodal interaction for referring image segmentation",
    "authors": ["Chenxi Liu", "Zhe Lin", "Xiaohui Shen", "Jimei Yang", "Xin Lu", "Alan L. Yuille."],
    "venue": "ICCV . IEEE Computer Society, pages 1280– 1289.",
    "year": 2017
  }, {
    "title": "Attention correctness in neural image captioning",
    "authors": ["Chenxi Liu", "Junhua Mao", "Fei Sha", "Alan L. Yuille."],
    "venue": "AAAI. AAAI Press, pages 4176–4182.",
    "year": 2017
  }, {
    "title": "Visual relationship detection with language priors",
    "authors": ["Cewu Lu", "Ranjay Krishna", "Michael S. Bernstein", "Fei-Fei Li."],
    "venue": "ECCV . Springer, volume 9905 of Lecture Notes in Computer Science, pages 852– 869.",
    "year": 2016
  }, {
    "title": "Generation and comprehension of unambiguous object descriptions",
    "authors": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan L. Yuille", "Kevin Murphy."],
    "venue": "CVPR. IEEE Computer Society, pages 11–20.",
    "year": 2016
  }, {
    "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
    "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille."],
    "venue": "CoRR abs/1412.6632.",
    "year": 2014
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."],
    "venue": "Computational Linguistics 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "CoRR abs/1301.3781.",
    "year": 2013
  }, {
    "title": "Incrementality in deterministic dependency parsing",
    "authors": ["Joakim Nivre."],
    "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. Association for Computational Linguistics, pages 50–57.",
    "year": 2004
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "EMNLP. ACL, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval",
    "authors": ["Sebastian Schuster", "Ranjay Krishna", "Angel Chang", "Li Fei-Fei", "Christopher D Manning."],
    "venue": "Proceedings of the fourth workshop on vision and",
    "year": 2015
  }, {
    "title": "Fast(er) exact decoding and global training for transition-based dependency parsing via a minimal feature set",
    "authors": ["Tianze Shi", "Liang Huang", "Lillian Lee."],
    "venue": "EMNLP. Association for Computational Linguistics, pages 12–23.",
    "year": 2017
  }, {
    "title": "Graph-structured representations for visual question answering",
    "authors": ["Damien Teney", "Lingqiao Liu", "Anton van den Hengel."],
    "venue": "CoRR abs/1609.05600.",
    "year": 2016
  }, {
    "title": "Order-embeddings of images and language",
    "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."],
    "venue": "CoRR abs/1511.06361.",
    "year": 2015
  }, {
    "title": "A transition-based algorithm for AMR parsing",
    "authors": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."],
    "venue": "HLT-NAACL. The Association for Computational Linguistics, pages 366–375.",
    "year": 2015
  }, {
    "title": "Robust subgraph generation improves abstract meaning representation parsing",
    "authors": ["Keenon Werling", "Gabor Angeli", "Christopher D. Manning."],
    "venue": "ACL (1). The Association for Computer Linguistics, pages 982–991.",
    "year": 2015
  }, {
    "title": "Scene graph generation by iterative message passing",
    "authors": ["Danfei Xu", "Yuke Zhu", "Christopher B. Choy", "Li FeiFei."],
    "venue": "CVPR. IEEE Computer Society, pages 3097–3106.",
    "year": 2017
  }, {
    "title": "Visual7w: Grounded question answering in images",
    "authors": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei."],
    "venue": "CVPR. IEEE Computer Society, pages 4995–5004.",
    "year": 2016
  }],
  "id": "SP:fbedfe317e60e5ec83c8fd0554bc345404ca90f5",
  "authors": [{
    "name": "Yu-Siang Wang",
    "affiliations": []
  }, {
    "name": "Chenxi Liu",
    "affiliations": []
  }, {
    "name": "Xiaohui Zeng",
    "affiliations": []
  }, {
    "name": "Alan Yuille",
    "affiliations": []
  }],
  "abstractText": "In this paper, we study the problem of parsing structured knowledge graphs from textual descriptions. In particular, we consider the scene graph representation (Johnson et al., 2015) that considers objects together with their attributes and relations: this representation has been proved useful across a variety of vision and language applications. We begin by introducing an alternative but equivalent edgecentric view of scene graphs that connect to dependency parses. Together with a careful redesign of label and action space, we combine the two-stage pipeline used in prior work (generic dependency parsing followed by simple post-processing) into one, enabling end-toend training. The scene graphs generated by our learned neural dependency parser achieve an F-score similarity of 49.67% to ground truth graphs on our evaluation set, surpassing best previous approaches by 5%. We further demonstrate the effectiveness of our learned parser on image retrieval applications.1",
  "title": "Scene Graph Parsing as Dependency Parsing"
}