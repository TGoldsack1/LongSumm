{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Modern machine learning methods have demonstrated stateof-art performance in representing complex functions in a variety of applications. Nevertheless, the translation of complex learning methods in natural sciences and in the clinical domain is still challenged by the need of interpretable solutions. To this end, several approaches have been proposed in order to constrain the solution dynamics to plausible forms such as boundedness (Da Veiga & Marrel, 2012), monotonicity (Riihimäki & Vehtari, 2010), or mechanistic behaviors (Alvarez et al., 2013). This is a crucial require-\n*Equal contribution 1University of Cote d’Azur, INRIA Sophia Antipolis, EPIONE research group, France 2EURECOM, Department of Data Science, Sophia Antipolis, France. Correspondence to: Marco Lorenzi <marco.lorenzi@inria.fr>, Maurizio Filippone <maurizio.filippone@eurecom.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nment to provide a more precise and realistic description of natural phenomena. For example, monotonicity of the interpolating function is a common assumption when modeling disease progression in neurodegenerative diseases (Lorenzi et al., 2017; Donohue et al., 2014), while bio-physical or mechanistic models are necessary when analyzing and simulating experimental data in bio-engineering (Vyshemirsky & Girolami, 2007; Konukoglu et al., 2011).\nHowever, accounting for the complex properties of biological systems in data-driven modeling approaches poses important challenges. For example, functions are often non-smooth and characterized by nonstationaries which are difficult to encode in “shallow” models. Complex cases can arise already in classical ODE systems for certain configurations of the parameters, where functions can exhibit sudden temporal changes (Goel et al., 1971; FitzHugh, 1955). Within this context, approaches based on stationary models, even when relaxing the smoothness assumptions, may lead to suboptimal results for both data modeling (interpolation), and estimation of dynamics parameters. To provide insightful illustrations of this problem we anticipate the results of Section 4.4.1 and Figure 5. Moreover, the application to real data requires to account for the uncertainty of measurements and underlying model parameters, as well as for the – often large – dimensionality characterizing the experimental data. Within this context, deep probabilistic approaches may represent a promising modeling tool, as they combine the flexibility of deep models with a systematic way to reason about uncertainty in model parameters and predictions. The flexibility of these approaches stems from the fact that deep models implement compositions of functions, which considerably extend the complexity of signals that can be represented with “shallow” models (LeCun et al., 2015). Meanwhile, their probabilistic formulation introduces a principled approach to quantify uncertainty in parameters estimation and predictions, as well as to model selection problems (Neal, 1996; Ghahramani, 2015).\nIn this work, we aim at extending deep probabilistic models to account for constraints on their dynamics. In particular, we focus on a general and flexible formulation capable of imposing a rich set of constraints on functions and derivatives of any order. We focus on: i) equality constraints on the function and its derivatives, required when the model should satisfy given physical laws implemented through\na mechanistic description of a system of interest; and ii) inequality constraints, arising in problems where the class of suitable functions is characterized by specific properties, such as monotonicity or convexity/concavity (Riihimäki & Vehtari, 2010).\nIn case of equality constraints, we tackle the challenge of parameters inference in Ordinary Differential Equations (ODE). Exact parameter inference of ODE models is computationally expensive due to the need for repeatedly solving ODEs within the Bayesian setting. To this end, previous works attempted to recover tractability by introducing approximate solutions of ODEs (see, e.g., Macdonald & Husmeier (2015) for a review). Following these ideas, we introduce “soft” constraints through a probabilistic formulation that penalizes functions violating the ODE on a set of virtual inputs. Note that this is in contrast to previous approaches, such as the ones proposed with probabilistic ODE solvers (Wheeler et al., 2014; Schober et al., 2014), where a given dynamics is strictly enforced to the model posterior. By deriving a lower bound on the model evidence, we enable the use of stochastic variational inference to achieve end-to-end posterior inference over model and constraint parameters.\nIn what follows we shall focus on a class of deep probabilistic models implementing a composition of Gaussian processes (GPs) (Rasmussen & Williams, 2006) into Deep Gaussian Processes (DGPs) (Damianou & Lawrence, 2013). More generally, our formulation can be straightforwardly extended to probabilistic Deep Neural Networks (DNNs) (Neal, 1996). On the practical side, our formulation allows us to take advantage of automatic differentiation tools, leading to flexible and easy-to-implement methods for inference in constrained deep probabilistic models. As a result, our method scales linearly with the number of observations and constraints. Furthermore, in the case of mean-field variational inference, it also scales linearly with the number of parameters in the constraints. Finally, it can easily be parallelized/distributed and exploit GPU computing.\nThrough an in-depth series of experiments, we demonstrate that our proposal achieves state-of-the-art performance in a number of constrained modeling problems while being characterized by attractive scalability properties. The paper is organized as follows: Section 2 reports on related work, whereas the core of the methodology is presented in Section 3. Section 4 contains an in-depth validation of the proposed model against the state-of-the-art. We demonstrate the application of equality constraints in the challenging problem of parameter inference in ODE, while we showcase the application of inequality constraints in the monotonic regression of count data. Additional insights and conclusions are given in Section 5. Results that we could not fit in the manuscript are deferred to the supplementary material."
  }, {
    "heading": "2. Related Work",
    "text": "Equality constraints where functions are enforced to model the solution of ODE systems have been considered in a variety of problems, particularly in the challenging task of accelerated inference of ODE parameters. Previous approaches to accelerate ODE parameter optimization involving interpolation date back to Varah (1982). This idea has been developed in several ways, including splines, GPs, and Reproducing Kernel Hilbert spaces. Works that employ GPs as interpolants have been proposed in Ramsay et al. (2007), Liang & Wu (2008), Calderhead et al. (2009), and Campbell & Steele (2012). Such approaches have been extended to introduce a novel formulation to regularize the interpolant based on the ODE system, notably Dondelinger et al. (2013); Barber & Wang (2014). An in-depth analysis of the model in Barber & Wang (2014) is provided by Macdonald et al. (2015). Recently, Gorbach et al. (2017) extended previous works by proposing mean-field variational inference to obtain an approximate posterior over ODE parameters. Our work improves previous approaches by considering a more general class of interpolants than “shallow” GPs, and proposes a scalable framework for inferring the family of interpolating functions jointly with the parameters of the constraint, namely ODE parameters.\nAnother line of research that builds on gradient matching approaches uses a Reproducing Kernel Hilbert space formulation. For example, González et al. (2014) proposes to exploit the linear part of ODEs to accelerate the interpolation, while Niu et al. (2016) exploits the quadratic dependency of the objective with respect to the parameters of the interpolant to improve the computational efficiency of the ODE regularization. Interestingly, inspired by Calandra et al. (2016), the latter approach was extended to handle nonstationarity in the interpolation through warping (Niu et al., 2018). The underlying idea is to estimate a transformation of the input domain to account for nonstationarity of the signal, in order to improve the fitting of stationary GP interpolants. A key limitation of this approach is the lack of a probabilistic formulation, which prevents one from approximating the posterior over ODE parameters. Moreover, the warping approach is tailored to periodic functions, thus limiting the generalization to more complex signals. In our work, we considerably improve on these aspects by effectively modeling the warping through GPs/DGPs that we infer jointly with ODE parameters.\nInequality constraints on the function derivatives have been considered in several works such as in Meyer (2008); Groeneboom & Jongbloed (2014); Mašić et al. (2017); Riihimäki & Vehtari (2010); Da Veiga & Marrel (2012); Salzmann & Urtasun (2010). In particular, the GP setting provides a solid and elegant theoretical background for tackling this problem; thanks to the linearity of differentiation,\nboth mean and covariance functions of high-order derivatives of GPs can be expressed in closed form, leading to exact formulations for linearly-constrained GPs (Da Veiga & Marrel, 2012). In case of inequality constraints on the derivatives, instead, this introduces non-conjugacy between the likelihood imposing the derivative constraint and the GP prior, thus requiring approximations (Riihimäki & Vehtari, 2010). Although this problem can be tackled through sampling schemes or variational inference methods, such as Expectation Propagation (Minka, 2001), scalability to large dimensions and sample size represents a critical limitation. In this work, we extend these methods by considering a more general class of functions based on DGPs, and develop scalable inference that makes our method applicable to large data and dimensions."
  }, {
    "heading": "3. Methods",
    "text": ""
  }, {
    "heading": "3.1. Equality constraints in probabilistic modeling",
    "text": "In this section we provide a derivation of the posterior distribution of our model when we introduce equality constraints in the dynamics. Let Y be a set of n observed multivariate variables yi ∈ Rs associated with measuring times t collected into t; the extension where the n variables are measured at different times is notationally heavier but straightforward. Let f(t) be a multivariate interpolating function with associated noise parameters ψ, and define F similarly to Y to be the realization of f at t. In this work, f(t) will be either modeled using a GP, or deep probabilistic models based on DGPs. We introduce functional constraints on the dynamics of the components of f(t) by specifying a family of admissible functions whose derivatives of order h evaluated at the inputs t satisfy some given constraint\nChi = { f(t) ∣∣∣∣∣dhfi(t)dth = Hhi ( t, f , df dt , . . . , dqf dtq ,θ )∣∣∣∣∣ t } .\nHere the constraint is expressed as a function of the input, the function itself, and high-order derivatives up to order q. The constraint also includes θ as dynamics parameters that should be inferred. We are going to consider the intersection of all the constraints for a set of indices I comprising pairs (h, i) of interest\nC = ⋂\n(h,i)∈I\nChi\nTo keep the notation uncluttered, and without loss of generality, in the following we will assume that all the terms are evaluated at t; we can easily relax this by allowing for the constraints to be evaluated at different sampling points than t. As a concrete example, consider the constraints induced by the Lotka-Volterra ODE system (more details in the experiments section); for this system, θ = {α, β, γ, δ},\nand the family of functions is identified by the conditions\ndg1(t)\ndt ∣∣∣∣∣ t = H11 (f(t)) ∣∣∣∣∣ t = αf1(t)− βf1(t)f2(t),\ndg2(t)\ndt ∣∣∣∣∣ t = H12 (f(t)) ∣∣∣∣∣ t = −γf2(t) + δf1(t)f2(t),\nwhere the products f1(t)f2(t) are element-wise.\nDenote by F̃ = {fhi} the set of realizations of f and of its derivatives at any required order h evaluated at timed t. We define the constrained regression problem through two complementary likelihood-based elements: a data attachment term p(Y |F,ψ), and a term quantifying the constraint on the dynamics, p(C|F̃ ,θ,ψD), where ψD is the associated noise parameter. To solve the inference problem, we shall determine a lower bound for the marginal\np(Y, C|t,ψ,ψD) = (1)∫ p(Y |F,ψ)p(C|F̃ ,θ,ψD)p(F, F̃ |t,ψ)p(θ)dFdF̃dθ,\nwhere p(F, F̃ |t,ψ) = p(F̃ |F )p(F |t,ψ).\nNote that F̃ is in fact completely identified by F .\nEquation (1) requires specifying suitable models for both likelihood and functional constraints. This problem thus implies the definition of noise models for both observations and model dynamics. In the case of continuous observations, the likelihood can be assumed to be Gaussian:\np(Y |F,ψ) = N (Y |F,Σ(ψ)), (2)\nwhere Σ(ψ) is a suitable multivariate covariance. Extensions to other likelihood functions are possible, and in the experiments we show an application to regression on counts where the likelihood is Poisson with rates equal to the exponential of the elements of F .\nConcerning the noise model for the derivative observations, we assume independence across the constraints Chi so that\np(C|F̃ , θ,ψD) = ∏\n(h,i)∈I\np(Chi|F̃ , θ,ψD). (3)\nWe can again assume a Gaussian likelihood: p(Chi|F̃ , θ,ψD) = ∏ t N (fhi(t)|Hhi(t, F̃ , θ),ψD), (4)\nor, in order to account for potentially heavy-tailed error terms on the derivative constraints, we can assume a Studentt distribution:\np(Chi|F̃ , θ,ψD) = ∏ t T (fhi(t)|Hhi(t, F̃ , θ),ψD, ν),\n(5) where T (z|µ, λ, ν) ∝ 1λ [1+ (z−µ)2 νλ2 ]\n−(ν+1)/2. We test these two noise models for F̃ in the experiments."
  }, {
    "heading": "3.2. Inequality constraints in probabilistic modeling",
    "text": "In the case of inequality constraints we can proceed analogously as in the previous section. In particular, we are interested in the class of functions satisfying the following conditions:\nChi = { f(t) ∣∣∣∣∣dhfi(t)dth > Hhi ( t, f , df dt , . . . , dqf dtq ,θ )∣∣∣∣∣ t } .\nFor example, a monotonic univariate regression problem can be obtained with a constraint of the form dfdt > 0.\nIn this case, the model dynamics can be enforced by a logistic function:\np(Chi|F̃ , ψD) = n∏ j=1\n1\n1 + exp(−ψD dfdt (tj)) , (6)\nwhere the parameter ψD controls the strength of the monotonicity constraint."
  }, {
    "heading": "3.3. Optimization and inference in constrained regression with DGPs",
    "text": "After recalling the necessary methodological background, in this section we derive an efficient inference scheme for the model posterior introduced in Section 3.1.\nTo recover tractability, our scheme leverages on recent advances in modeling and inference in DGPs through approximation via random feature expansions (Rahimi & Recht, 2008; Cutajar et al., 2017). Denoting with F (l) the GP random variables at layer l, an (approximate) DGP is obtained by composing GPs approximated by Bayesian linear models, F (l) ≈ Φ(l)W (l). The so-called random features Φ(l) are obtained by multiplying the layer input by a random matrix Ω(l) and by applying a nonlinear transformation h(·). For example, in case of the standard RBF covariance, the elements in Ω(l) are Gaussian distributed with covariance function parameterized through the length-scale of the RBF covariance. The nonlinearity is obtained through trigonometric functions, h(·) = (cos(·), sin(·)), while the prior over the elements of W (l) is standard normal. As a result, the interpolant becomes a Bayesian Deep Neural Network (DNN), where for each layer we have weights Ω(l) and W (l), and activation functions h(·) applied to the input of each layer multiplied by the weights Ω(l)."
  }, {
    "heading": "3.3.1. DERIVATIVES IN DGPS WITH RANDOM FEATURE EXPANSIONS",
    "text": "To account for function derivatives consistently with the theory developed in Cutajar et al. (2017), we need to extend the random feature expansion formulation of DGPs to highorder derivatives. Fortunately, this is possible thanks to the chain rule and to the closure under linear operations of\nthe approximated GPs. More precisely, the derivatives of a “shallow” GP model with form F = h(tΩ)W can still be expressed through linear composition of matrix-valued operators depending on W and Ω only: dFdt = dh(tΩ) dt W . The computational tractability is thus preserved and the GP function and derivatives are identified by the same sets of weights Ω and W . The same principle clearly extends to DGP architectures where the derivatives at each layer can be combined following the chain rule to obtain the derivatives of the output function with respect to the input."
  }, {
    "heading": "3.3.2. VARIATIONAL LOWER BOUND",
    "text": "In the constrained DGP setting, we are interested in carrying out inference of the functions F (l) and of the associated covariance parameters at all layers. Moreover, we may want to infer any dynamics parameters θ that parameterize the constraint on the derivatives. Within this setting, the inference of the latent variables F (l) in the marginal (1) is generally not tractable. Nevertheless, the Bayesian DNN structure provided by the random feature approximation allows the efficient estimation of its parameters, and the tractability of the inference is thus recovered.\nIn particular, let Ω, W, and ψ be the collections of all Ω(l), W (l), and covariance and likelihood parameters, respectively. Recalling that we can obtain random features at each layer by sampling the elements in Ω from a given prior distribution, we propose to tackle the inference problem through variational inference of the parameters W and θ. We could also attempt to infer Ω, although in this work we are going to assume them sampled from the prior with fixed randomness, which allows us to optimize covariance parameters using the reparameterization trick (option PRIORFIXED in Cutajar et al. (2017)). We also note that we could infer, rather than optimize, ψ; we leave this for future work.\nUsing Jensen’s inequality, the variational approach allows us to obtain a lower bound on the log-marginal likelihood L := log [p(Y, C|t,Ω,ψ,ψD)] of equation (1), as follows:\nL ≥ Eq(W) (log[p(Y |Ω,W,ψ)]) + Eq(W)q(θ) (log[p(C|Ω,W,ψD,θ)]) − DKL(q(W)‖p(W))−DKL(q(θ)‖p(θ)). (7)\nThe distribution q(W) acts as a variational approximation and is assumed to be Gaussian, factorizing completely across weights and layers (l):\nq(W) = ∏ j,k,l p(W (l) jk ) = ∏ j,k,l N ( m (l) jk , (s 2) (l) jk ) . (8)\nExtensions to approximations where we relax the factorization assumption are possible. Similarly, we are going to assume q(θ) to be Gaussian, and will assume no factorization, so that q(θ) = N (µθ,Σθ)."
  }, {
    "heading": "4. Experiments",
    "text": "This section reports an in-depth validation of the proposed method on a variety of benchmarks. We are going to study the proposed variational framework for constrained dynamics in DGP models for ODE parameter estimates using equality constraints, and compare it against state-of-the-art methods. We will then consider the application of inequality constraints for a regression problem on counts, which was previously considered in the literature of monotonic GPs."
  }, {
    "heading": "4.1. Settings for the proposed constrained DGP",
    "text": "We report here the configuration that we used across all benchmarks for the proposed method. Due to the generally low sample size n used across experiments (in most cases n < 50), unless specified otherwise the tests were performed with a two-layer DGP f(t) = f (2) ◦ f (1)(t), with dimension of the “hidden” GP layer f (1)(t) equal to 2, and RBF kernels. The length-scale of the RBF covariances was initialized to λ0 = log(tmax − tmin), while the marginal standard deviation to α0 = log(ymax − ymin); the initial likelihood noise was set to σ20 = α0/10\n5. Finally, the initial ODE parameters were set to the value of 0.1. The optimization was carried out through stochastic gradient descent with Adaptive moment Estimation (Adam) (Kingma & Ba, 2017), through the alternate optimization of i) the approximated posterior over W and likelihood/covariance parameters (q(W) and ψ), and ii) likelihood parameters of ODE constraints and the approximate posterior over ODE parameters (ψD and q(θ)). We note that the optimization of the ODE constraints parameters (the noise and scale parameters for Gaussian and Student-t likelihoods, respectively) is aimed at identifying in a fully data-driven manner the optimal trade-off between data attachment (likelihood term) and regularity (constraints on the dynamics). In what follows, DGP-t and DGP-G respectively denote the model tested with Student-t and Gaussian noise models on the ODE constraints."
  }, {
    "heading": "4.2. Equality constraints from ODE systems",
    "text": "The proposed framework was tested on a set of ODE systems extensively studied in previous works: Lotka-Volterra (Goel et al., 1971), FitzHugh-Nagumo (FitzHugh, 1955), and protein biopathways from Vyshemirsky & Girolami (2007). For each experiment, we used the experimental setting proposed in previous studies (Niu et al., 2016; Macdonald & Husmeier, 2015). In particular, for each test, we identified two experimental configurations with increasing modeling difficulty (e.g. less samples, lower signal-to-noise ratio, . . .). A detailed description of the models and testing parameters is provided in the supplementary material. The experimental results are reported for parameter inference and model estimation performed on 5 different realizations of the noise."
  }, {
    "heading": "4.2.1. BENCHMARK",
    "text": "We tested the proposed method against several reference approaches from the state-of-art to infer parameters of ODE systems.\nRKG3: We tested the method presented in Niu et al. (2016) using the implementation in the R package KGode. This method implements gradient matching, where the interpolant is modeled using functions in Reproducing Kernel Hilbert spaces. This approach, for which ODE parameters are estimated and not inferred, was shown to achieve state-of-the-art performance on a variety of ODE estimation problems. We used values ranging from 10−4 to 1 for the parameter λ that the method optimizes using cross-validation.\nWarp: In the R package KGode there is also an implementation of the warping approach presented in Niu et al. (2018). This method extends gradient matching techniques by attempting to construct a warping of the input where smooth Reproducing Kernel Hilbert spaces-based interpolants can effectively model nonstationary observations. The warping attempts to transform the original signal via assumptions on periodicity and regularity conditions. We used the default parameters and initialized the optimization of the warping function from a period equal to the interval where observations are available. Similarly to RKG3, ODE parameters are estimated and not inferred.\nAGM: We report results on the Approximate Gradient Matching (AGM) approach in Dondelinger et al. (2013), implemented in the recently released R package deGradInfer. AGM implements a population Markov chain Monte Carlo approach tempering from the prior to the approximate posterior of ODE parameters based on an interpolation with GPs. In the experiments we use 10 parallel chains and we run them for 104 iterations. In the implementation of AGM, the variance of the noise on the observations is assumed known and it is fixed; we expect this to give a slight advantage to this method.\nMCMC: In the R package deGradInfer there is also an implementation of a population Markov chain Monte Carlo sampler where the ODE is solved explicitly. In this case too we use 10 parallel chains that we run for 104 iterations. In contrast to AGM, in this implementation, the variance of the noise on the observations is learned together with ODE parameters."
  }, {
    "heading": "4.2.2. RESULTS",
    "text": "Figure 4.2.2 shows the distribution of the root mean squared error (RMSE) across folds for each experimental setting (see supplement for details). We note that the proposed method consistently leads to better RMSE values compared to the reference approaches (except some folds in one of the Fitz-Hugh-Nagumo experiments, according to a Mann-\nWhitney nonparametric test), and that DGP-t provides more consistent parameter estimates than DGP-G. This latter result may indicate a lower sensitivity to outliers derivatives involved in the functional constraint term. This is a crucial aspect due to the generally noisy derivative terms of nonparametric regression models. The distribution of the parameters for all the datasets tested in this study, which we report in the supplementary material, reveals that, unlike the nonprobabilistic methods RKG3 and WARP, our approach is capable of inferring ODE parameters yielding meaningful uncertainty estimation.\nLotka-Volterra"
  }, {
    "heading": "4.3. Scalability test - large n",
    "text": "We tested the scalability of the proposed method with respect to sample size. To this end, we repeated the test on the Lotka-Volterra system with n = 20, 40, 80, 150, 500, 103, and 104 observations. For each instance of the model, the execution time was recorded and compared with the competing methods. All the experiments were performed on a 1.3GHz Intel Core i5 MacBook. The proposed method scales linearly with n (Figure 2), while it has an almost constant execution when n < 500; we attribute this effect to overheads in the framework we used to code our method. For small n, the running time of our method is comparable with competing methods, and it is considerably faster in case of large n."
  }, {
    "heading": "4.4. Scalability test - large s",
    "text": "In order to assess the ability of the framework to scale to a large number of ODEs, we tested our method on the Lorenz96 system with increasing number of equations, s = 125 to s = 1000 (Lorenz & Emanuel, 1998). To the best of our knowledge, the solution of this challenging problem via gradient matching approaches has only been previously attempted in Gorbach et al. (2017). We could not find an implementation of their method to carry out a direct comparison, so we are going to refer to the results reported in their paper. The system consists of a set of drift states functions (f1(x(t), θ), f2(x(t), θ), . . . , fs(x(t), θ)) recursively linked by the relationship:\nfi(x(t), θ) = (xi+1(t)− xi−2(t))xi−1(t)− xi(t) + θ,\nwhere θ ∈ R is the drift parameter. Consistently with the setting proposed in Gorbach et al. (2017); Vrettas et al. (2015), we set θ = 8 and generated 32 equally spaced observations over the interval [0, 4] seconds, with additive Gaussian noise σ2 = 1. We performed two tests by training (i) on all the states, and (ii) by keeping one third of the states as unobserved, and by applying our method to identify model dynamics on both observed and unobserved states.\nFigure 3 shows the average RMSE in the different experimental settings. As expected, the modeling accuracy is\nsensibly higher when trained on the full set of equations. Moreover, the RMSE is lower on observed states compared to unobserved ones. This is confirmed by visual inspection of the modeling results for sample training and testing states (Figure 4). The observed states are generally associated with lower uncertainty in the predictions and by an accurate fitting of the solutions (Figure 4, top). The model still provides remarkable modeling results on unobserved states (Figure 4, bottom), although with decreased accuracy and higher uncertainty. We are investigating the reasons for the posterior distribution over θ not covering the true value of the parameter across different experimental conditions."
  }, {
    "heading": "4.4.1. DEEP VS SHALLOW",
    "text": "We explore here the capability of a DGP to accommodate for the data nonstationarity typical of ODE systems. In particular, the tests are performed in two different settings with large and small sample size n. By using the same experimental setting of Section 4.1, we sampled 80 and 1000 points, respectively, from the FitzHugh-Nagumo equations. The data is modeled with DGPs composed by one (“shallow” GP), two and three layers, all with RBF covariances.\nFigure 5 shows the modeling results obtained on the two configurations. We note that the shallow GP consistently underfits the complex dynamics producing smooth interpolants. On the contrary, DGPs provide a better representation of the nonstationarity. As expected, the three-layer DGP leads to sub-optimal results in the low-sample size\nLorenz96 - Observed\nLorenz 96 - Unobserved\nsetting. Furthermore, in order to motivate the importance of nonstationarity, which we implement through DGPs, we further compared against shallow GPs with lower degrees of smoothness through the use of Matérn covariances with degrees ν = 1/2, 1, 3/2, 5/2.\nThe overall performance in parameter estimation and data fit is reported in Table 1. According to the results, a two-layer DGP provides the best solution overall in terms of modeling accuracy and complexity. Interestingly, the Matérn covariance, with an appropriate degree of smoothness, achieves superior performance in parameter estimation in case of low sample size. However, the nonstationarity implemented by the DGP outperforms the stationary Matérn in the data fit, as well as in the parameter estimation when the sample size is large. For an illustration of the data fit of the Matérn GP we refer the reader to the supplementary material. Crucially, these results indicate that our approach provides a practical and scalable way to learn nonstationarity within the framework of variational inference for deep probabilistic models."
  }, {
    "heading": "4.5. Inequality constraints",
    "text": "We conclude our experimental validation by applying monotonic regression on counts as an illustration of the proposed framework for inequality constrains in DGP models dynamics. We applied our approach to the mortality dataset from (Broffitt, 1988), with a two-layer DGP initialized with an analogous setting to the one proposed in Section 4.1. In\nparticular, the sample rates were modeled with with a Poisson likelihood of the form p(yi|µi) = exp(−µi)µ yi i\nyi! , and\nlink function µi = exp(f(ti)). Monotonicity on the solution was strictly enforced by setting ψD = 5. Figure 6 shows the regression results without (top) and with (bottom) monotonicity constraint. The effect of the constraint on the dynamics can be appreciated by looking at the distribution of the derivatives (right panel). In the monotonic case the GP derivatives lie on the positive part of the plane. This experiment leads to results compatible with those obtained with the monotonic GP proposed in (Riihimäki & Vehtari, 2010), and implemented in the GPstuff toolbox (Vanhatalo et al., 2013). However, our approach is characterized by appealing scalability properties and can implement monotonic constraints on DGPs, which offer a more general class of functions than GPs."
  }, {
    "heading": "5. Conclusions",
    "text": "We introduced a novel generative formulation of deep probabilistic models implementing “soft” constraints on functions dynamics. The proposed approach was extensively tested in several experimental settings, leading to highly competitive results in challenging modeling applications, and favorably comparing with the state-of-the-art in terms of modeling accuracy and scalability. Furthermore, the proposed variational formulation allows for a meaningful uncertainty quantification of both model parameters and predictions. This is an important aspect intimately related to the application of our proposal in real scenarios, such as in biology and epidemiology, where data is often noisy and scarce.\nAlthough in this study we essentially focused on the problem of ODE parameters inference and monotonic regression, the generality of our approach enables several other applications that will be subject of future investigations. We will focus on the extension to manifold valued data, such as spatiotemporal observations represented by graphs, meshes, and 3D volumes, occurring for example in medical imaging and system biology."
  }, {
    "heading": "Acknowledgements",
    "text": "This work has been supported by the French government, through the UCAJEDI Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-15-IDEX-01 (project MetaImaGen). MF gratefully acknowledges support from the AXA Research Fund.\nThis work is dedicated to Mattia Filippone."
  }],
  "year": 2018,
  "references": [{
    "title": "Linear latent force models using Gaussian processes",
    "authors": ["M.A. Alvarez", "D. Luengo", "N.D. Lawrence"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2013
  }, {
    "title": "Increasing and increasing convex Bayesian graduation",
    "authors": ["J.D. Broffitt"],
    "venue": "Transactions of the Society of Actuaries,",
    "year": 1988
  }, {
    "title": "Manifold Gaussian Processes for regression",
    "authors": ["R. Calandra", "J. Peters", "C.E. Rasmussen", "M.P. Deisenroth"],
    "venue": "In 2016 International Joint Conference on Neural Networks,",
    "year": 2016
  }, {
    "title": "Smooth functional tempering for nonlinear differential equation models",
    "authors": ["D. Campbell", "R.J. Steele"],
    "venue": "Statistics and Computing,",
    "year": 2012
  }, {
    "title": "Gaussian process modeling with inequality constraints",
    "authors": ["S. Da Veiga", "A. Marrel"],
    "venue": "Annales de la faculté des sciences de Toulouse Mathématiques,",
    "year": 2012
  }, {
    "title": "Deep Gaussian Processes",
    "authors": ["A.C. Damianou", "N.D. Lawrence"],
    "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2013
  }, {
    "title": "Mathematical models of threshold phenomena in the nerve membrane",
    "authors": ["R. FitzHugh"],
    "venue": "The bulletin of mathematical biophysics,",
    "year": 1955
  }, {
    "title": "Probabilistic machine learning and artificial intelligence",
    "authors": ["Z. Ghahramani"],
    "venue": "Nature,",
    "year": 2015
  }, {
    "title": "On the volterra and other nonlinear models of interacting populations",
    "authors": ["N.S. Goel", "S.C. Maitra", "E.W. Montroll"],
    "venue": "Reviews of modern physics,",
    "year": 1971
  }, {
    "title": "Reproducing kernel Hilbert space based estimation of systems of ordinary differential equations",
    "authors": ["J. González", "I. Vujačić", "E. Wit"],
    "venue": "Pattern Recognition Letters,",
    "year": 2014
  }, {
    "title": "Nonparametric Estimation under Shape Constraints: Estimators, Algorithms and Asymptotics. Cambridge Series in Statistical and Probabilistic Mathematics",
    "authors": ["P. Groeneboom", "G. Jongbloed"],
    "year": 2014
  }, {
    "title": "Adam: A Method for Stochastic Optimization, January 2017",
    "authors": ["D.P. Kingma", "J. Ba"],
    "year": 2017
  }, {
    "title": "Optimal sites for supplementary weather observations: Simulation with a small model",
    "authors": ["E.N. Lorenz", "K.A. Emanuel"],
    "venue": "Journal of the Atmospheric Sciences,",
    "year": 1998
  }, {
    "title": "Probabilistic disease progression modeling to characterize diagnostic uncertainty: Application to staging and prediction in Alzheimer’s disease",
    "authors": ["M. Lorenzi", "M. Filippone", "G.B. Frisoni", "D.C. Alexander", "S. Ourselin"],
    "year": 2017
  }, {
    "title": "Gradient Matching Methods for Computational Inference in Mechanistic Models for Systems Biology: A Review and Comparative Analysis",
    "authors": ["B. Macdonald", "D. Husmeier"],
    "venue": "Frontiers in Bioengineering and Biotechnology,",
    "year": 2015
  }, {
    "title": "Shape constrained splines as transparent black-box models for bioprocess modeling",
    "authors": ["A. Mašić", "S. Srinivasan", "J. Billeter", "D. Bonvin", "K. Villez"],
    "venue": "Computers & Chemical Engineering,",
    "year": 2017
  }, {
    "title": "Inference using shape-restricted regression splines",
    "authors": ["M.C. Meyer"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2008
  }, {
    "title": "Expectation Propagation for approximate Bayesian inference",
    "authors": ["T.P. Minka"],
    "venue": "In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence,",
    "year": 2001
  }, {
    "title": "Bayesian Learning for Neural Networks (Lecture Notes in Statistics). Springer, 1 edition",
    "authors": ["R.M. Neal"],
    "year": 1996
  }, {
    "title": "Statistical inference in mechanistic models: time warping for improved gradient matching",
    "authors": ["M. Niu", "B. Macdonald", "S. Rogers", "M. Filippone", "D. Husmeier"],
    "venue": "Computational Statistics,",
    "year": 2018
  }, {
    "title": "Random Features for Large-Scale Kernel Machines",
    "authors": ["A. Rahimi", "B. Recht"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2008
  }, {
    "title": "Parameter estimation for differential equations: a generalized smoothing approach",
    "authors": ["J.O. Ramsay", "G. Hooker", "D. Campbell", "J. Cao"],
    "venue": "Journal of the Royal Statistical Society Series B,",
    "year": 2007
  }, {
    "title": "Gaussian Processes for Machine Learning",
    "authors": ["C.E. Rasmussen", "C. Williams"],
    "year": 2006
  }, {
    "title": "Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation",
    "authors": ["M. Salzmann", "R. Urtasun"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2065
  }, {
    "title": "GPstuff: Bayesian modeling with Gaussian processes",
    "authors": ["J. Vanhatalo", "J. Riihimäki", "J. Hartikainen", "P. Jylänki", "V. Tolvanen", "A. Vehtari"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "A Spline Least Squares Method for Numerical Parameter Estimation in Differential Equations",
    "authors": ["J.M. Varah"],
    "venue": "SIAM Journal on Scientific and Statistical Computing,",
    "year": 1982
  }, {
    "title": "Variational mean-field algorithm for efficient inference in large systems of stochastic differential equations",
    "authors": ["M.D. Vrettas", "M. Opper", "D. Cornford"],
    "venue": "Physical Review E,",
    "year": 2015
  }, {
    "title": "Bayesian ranking of biochemical system",
    "authors": ["V. Vyshemirsky", "M.A. Girolami"],
    "venue": "models. Bioinformatics,",
    "year": 2007
  }, {
    "title": "Mechanistic hierarchical Gaussian processes",
    "authors": ["M.W. Wheeler", "D.B. Dunson", "S.P. Pandalai", "B.A. Baker", "A.H. Herring"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2014
  }],
  "id": "SP:8a10b6daee5db20b0e975f0f9887f2582820f6eb",
  "authors": [{
    "name": "Marco Lorenzi",
    "affiliations": []
  }, {
    "name": "Maurizio Filippone",
    "affiliations": []
  }],
  "abstractText": "We introduce a novel generative formulation of deep probabilistic models implementing “soft” constraints on their function dynamics. In particular, we develop a flexible methodological framework where the modeled functions and derivatives of a given order are subject to inequality or equality constraints. We then characterize the posterior distribution over model and constraint parameters through stochastic variational inference. As a result, the proposed approach allows for accurate and scalable uncertainty quantification on the predictions and on all parameters. We demonstrate the application of equality constraints in the challenging problem of parameter inference in ordinary differential equation models, while we showcase the application of inequality constraints on the problem of monotonic regression of count data. The proposed approach is extensively tested in several experimental settings, leading to highly competitive results in challenging modeling applications, while offering high expressiveness, flexibility and scalability.",
  "title": "Constraining the Dynamics of Deep Probabilistic Models"
}