{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1044–1054 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1044"
  }, {
    "heading": "1 Introduction",
    "text": "Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017).\nIn contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schn-\nabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006).\nIn this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily—with a few additions—with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored.\nIn particular, we re-evaluate three traditional bootstrapping methods, self-training (Yarowsky, 1995), tri-training (Zhou and Li, 2005), and tritraining with disagreement (Søgaard, 2010) for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature.\nWe make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning (Laine and Aila, 2017) and recent neural adaptation approaches (Ganin et al., 2016; Saito et al., 2017).\nIn addition, we propose multi-task tri-training, which reduces the main deficiency of tri-training, namely its time and space complexity. It establishes a new state of the art on unsupervised domain adaptation for sentiment analysis but it is outperformed by classic tri-training for POS tagging.\nContributions Our contributions are: a) We propose a novel multi-task tri-training method. b) We show that tri-training can serve as a strong and robust semi-supervised learning baseline for the current generation of NLP models. c) We perform an extensive evaluation of bootstrapping1 algorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model."
  }, {
    "heading": "2 Neural bootstrapping methods",
    "text": "We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to (Abney, 2007; Chapelle et al., 2006; Zhu and Goldberg, 2009). We introduce our novel multitask tri-training method in §2.3."
  }, {
    "heading": "2.1 Self-training",
    "text": "Self-training (Yarowsky, 1995; McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model’s own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next.\nSelf-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predetermined threshold τ , x is added to the labeled examples with p(x) = argmaxm(x) as pseudo-label. This instantiation is the most widely used and shown in Algorithm 1.\nCalibration It is well-known that output probabilities in neural networks are poorly calibrated (Guo et al., 2017). Using a fixed threshold τ is thus\n1We use the term bootstrapping as used in the semisupervised learning literature (Zhu, 2005), which should not be confused with the statistical procedure of the same name (Efron and Tibshirani, 1994).\nAlgorithm 1 Self-training (Abney, 2007) 1: repeat 2: m← train_model(L) 3: for x ∈ U do 4: if maxm(x) > τ then 5: L← L ∪ {(x, p(x))} 6: until no more predictions are confident\nnot the best choice. While the absolute confidence value is inaccurate, we can expect that the relative order of confidences is more robust.\nFor this reason, we select the top n unlabeled examples that have been predicted with the highest confidence after every epoch and add them to the labeled data. This is one of the many variants for self-training, called throttling (Abney, 2007). We empirically confirm that this outperforms the classic selection in our experiments.\nOnline learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best.\nClassic self-training has shown mixed success. In parsing it proved successful only with small datasets (Reichart and Rappoport, 2007) or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; Suzuki and Isozaki, 2008). Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012), while others report limited success on a variety of NLP tasks (Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017). Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift."
  }, {
    "heading": "2.2 Tri-training",
    "text": "Tri-training (Zhou and Li, 2005) is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training (cf. Algorithm 2) first trains three models m1, m2, and m3 on bootstrap samples of the labeled data L. An unlabeled data point is added to the training set of a modelmi if the other two modelsmj andmk agree on its label. Training stops when the classifiers do not change anymore.\nTri-training with disagreement (Søgaard, 2010)\nAlgorithm 2 Tri-training (Zhou and Li, 2005) 1: for i ∈ {1..3} do 2: Si ← bootstrap_sample(L) 3: mi ← train_model(Si) 4: repeat 5: for i ∈ {1..3} do 6: Li ← ∅ 7: for x ∈ U do 8: if pj(x) = pk(x)(j, k 6= i) then 9: Li ← Li ∪ {(x, pj(x))}\nmi ← train_model(L ∪ Li) 10: until none of mi changes 11: apply majority vote over mi\nis based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. In order to achieve this, it adds a simple modification to the original algorithm (altering line 8 in Algorithm 2), requiring that for an unlabeled data point on which mj and mk agree, the other model mi disagrees on the prediction. Tri-training with disagreement is more data-efficient than tritraining and has achieved competitive results on part-of-speech tagging (Søgaard, 2010).\nSampling unlabeled data Both tri-training and tri-training with disagreement can be very expensive in their original formulation as they require to produce predictions for each of the three models on all unlabeled data samples, which can be in the millions in realistic applications. We thus propose to sample a number of unlabeled examples at every epoch. For all traditional bootstrapping approaches we sample 10k candidate instances in each epoch. For the neural approaches we use a linearly growing candidate sampling scheme proposed by (Saito et al., 2017), increasing the candidate pool size as the models become more accurate.\nConfidence thresholding Similar to selftraining, we can introduce an additional requirement that pseudo-labeled examples are only added if the probability of the prediction of at least one model is higher than some threshold τ . We did not find this to outperform prediction without threshold for traditional tri-training, but thresholding proved essential for our method (§2.3).\nThe most important condition for tri-training and tri-training with disagreement is that the models are diverse. Typically, bootstrap samples are used\nto create this diversity (Zhou and Li, 2005; Søgaard, 2010). However, training separate models on bootstrap samples of a potentially large amount of training data is expensive and takes a lot of time. This drawback motivates our approach."
  }, {
    "heading": "2.3 Multi-task tri-training",
    "text": "In order to reduce both the time and space complexity of tri-training, we propose Multi-task Tritraining (MT-Tri). MT-Tri leverages insights from multi-task learning (MTL) (Caruana, 1993) to share knowledge across models and accelerate training. Rather than storing and training each model separately, we propose to share the parameters of the models and train them jointly using MTL.2 All models thus collaborate on learning a joint representation, which improves convergence.\nThe output softmax layers are model-specific and are only updated for the input of the respective model. We show the model in Figure 1 (as instantiated for POS tagging). As the models leverage a joint representation, we need to ensure that the features used for prediction in the softmax layers of the different models are as diverse as possible, so that the models can still learn from each other’s predictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training.\nTo guarantee diversity, we introduce an orthogonality constraint (Bousmalis et al., 2016) as an additional loss term, which we define as follows:\nLorth = ‖W>m1Wm2‖ 2 F (1)\nwhere | · ‖2F is the squared Frobenius norm and Wm1 and Wm2 are the softmax output parameters\n2Note: we use the term multi-task learning here albeit all tasks are of the same kind, similar to work on multi-lingual modeling treating each language (but same label space) as separate task e.g., (Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint.\nof the two source and pseudo-labeled output layers m1 and m2, respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m1 and m2,3 while m3 is gradually trained to be more target-specific. We parameterize Lorth by γ=0.01 following (Liu et al., 2017). We do not further tune γ.\nMore formally, let us illustrate the model by taking the sequence prediction task (Figure 1) as illustration. Given an utterance with labels y1, .., yn, our Multi-task Tri-training loss consists of three task-specific (m1,m2,m3) tagging loss functions (where ~h is the uppermost Bi-LSTM encoding):\nL(θ) = − ∑ i ∑ 1,..,n logPmi(y|~h) + γLorth (2)\nIn contrast to classic tri-training, we can train the multi-task model with its three model-specific outputs jointly and without bootstrap sampling on the labeled source domain data until convergence, as the orthogonality constraint enforces different representations between models m1 and m2. From this point, we can leverage the pair-wise agreement of two output layers to add pseudo-labeled examples as training data to the third model. We train the third output layerm3 only on pseudo-labeled target instances in order to make tri-training more robust to a domain shift. For the final prediction, majority voting of all three output layers is used, which resulted in the best instantiation, together with confidence thresholding (τ = 0.9, except for highresource POS where τ = 0.8 performed slightly better). We also experimented with using a domainadversarial loss (Ganin et al., 2016) on the jointly learned representation, but found this not to help. The full pseudo-code is given in Algorithm 3.\nComputational complexity The motivation for MT-Tri was to reduce the space and time complexity of tri-training. We thus give an estimate of its efficiency gains. MT-Tri is ~3× more spaceefficient than regular tri-training; tri-training stores one set of parameters for each of the three models, while MT-Tri only stores one set of parameters (we use three output layers, but these make up a comparatively small part of the total parameter budget). In terms of time efficiency, tri-training first\n3We also tried enforcing orthogonality on a hidden layer rather than the output layer, but this did not help.\nAlgorithm 3 Multi-task Tri-training 1: m← train_model(L) 2: repeat 3: for i ∈ {1..3} do 4: Li ← ∅ 5: for x ∈ U do 6: if pj(x) = pk(x)(j, k 6= i) then 7: Li ← Li ∪ {(x, pj(x))} 8: if i = 3 then mi = train_model(Li) 9: elsemi ← train_model(L ∪ Li)\n10: until end condition is met 11: apply majority vote over mi\nrequires to train each of the models from scratch. The actual tri-training takes about the same time as training from scratch and requires a separate forward pass for each model, effectively training three independent models simultaneously. In contrast, MT-Tri only necessitates one forward pass as well as the evaluation of the two additional output layers (which takes a negligible amount of time) and requires about as many epochs as tri-training until convergence (see Table 3, second column) while adding fewer unlabeled examples per epoch (see Section 3.4). In our experiments, MT-Tri trained about 5-6× faster than traditional tri-training.\nMT-Tri can be seen as a self-ensembling technique, where different variations of a model are used to create a stronger ensemble prediction. Recent approaches in this line are snapshot ensembling (Huang et al., 2017) that ensembles models converged to different minima during a training run, asymmetric tri-training (Saito et al., 2017) (ASYM) that leverages agreement on two models as information for the third, and temporal ensembling (Laine and Aila, 2017), which ensembles predictions of a model at different epochs. We tried to compare to temporal ensembling in our experiments, but were not able to obtain consistent results.4 We compare to the closest most recent method, asymmetric tritraining (Saito et al., 2017). It differs from ours in two aspects: a) ASYM leverages only pseudolabels from data points on which m1 and m2 agree, and b) it uses only one task (m3) as final predictor. In essence, our formulation of MT-Tri is closer to the original tri-training formulation (agreements on two provide pseudo-labels to the third) thereby incorporating more diversity.\n4We suspect that the sparse features in NLP and the domain shift might be detrimental to its unsupervised consistency loss."
  }, {
    "heading": "3 Experiments",
    "text": "In order to ascertain which methods are robust across different domains, we evaluate on two widely used unsupervised domain adaptation datasets for two tasks, a sequence labeling and a classification task, cf. Table 1 for data statistics."
  }, {
    "heading": "3.1 POS tagging",
    "text": "For POS tagging we use the SANCL 2012 shared task dataset (Petrov and McDonald, 2012) and compare to the top results in both low and high-data conditions (Schnabel and Schütze, 2014; Yin et al., 2015). Both are strong baselines, as the FLORS tagger has been developed for this challenging dataset and it is based on contextual distributional features (excluding the word’s identity), and hand-crafted suffix and shape features (including some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger.\nOur POS tagging model is a state-of-the-art Bi-LSTM tagger (Plank et al., 2016) with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings (Pennington et al., 2014). The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with σ = 0.2 and word dropout with p = 0.25 (Kiperwasser and Goldberg, 2016).\nRegarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sen-\ntences. We use 100,000 WSJ sentences from 1988 as unlabeled data, following Schnabel and Schütze (2014).5 As target data, we use the five SANCL domains (answers, emails, newsgroups, reviews, weblogs). We restrict the amount of unlabeled data for each SANCL domain to the first 100k sentences, and do not do any pre-processing. We consider the development set of ANSWERS as our only target dev set to set hyperparameters. This may result in suboptimal per-domain settings but better resembles an unsupervised adaptation scenario."
  }, {
    "heading": "3.2 Sentiment analysis",
    "text": "For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). Reviews with 1 to 3 stars are ranked as negative, while reviews with 4 or 5 stars are ranked as positive. The dataset consists of four domains, yielding 12 adaptation scenarios. We use the same pre-processing and architecture as used in (Ganin et al., 2016; Saito et al., 2017): 5,000-dimensional tf-idf weighted unigram and bigram features as input; 2k labeled source samples and 2k unlabeled target samples for training, 200 labeled target samples for validation, and between 3k-6k samples for testing. The model is an MLP with one hidden layer with 50 dimensions, sigmoid activations, and a softmax output. We compare against the Variational Fair Autoencoder (VFAE) (Louizos et al., 2015) model and domain-adversarial neural networks (DANN) (Ganin et al., 2016)."
  }, {
    "heading": "3.3 Baselines",
    "text": "Besides comparing to the top results published on both datasets, we include the following baselines:\na) the task model trained on the source domain; b) self-training (Self); c) tri-training (Tri); d) tri-training with disagreement (Tri-D); and e) asymmetric tri-training (Saito et al., 2017).\nOur proposed model is multi-task tri-training (MTTri). We implement our models in DyNet (Neubig et al., 2017). Reporting single evaluation scores might result in biased results (Reimers and Gurevych, 2017). Throughout the paper, we report mean accuracy and standard deviation over five runs for POS tagging and over ten runs for\n5Note that our unlabeled data might slightly differ from theirs. We took the first 100k sentences from the 1988 WSJ dataset from the BLLIP 1987-89 WSJ Corpus Release 1.\nsentiment analysis. Significance is computed using bootstrap test. The code for all experiments is released at: https://github.com/bplank/ semi-supervised-baselines."
  }, {
    "heading": "3.4 Results",
    "text": "Sentiment analysis We show results for sentiment analysis for all 12 domain adaptation scenarios in Figure 2. For clarity, we also show the accuracy scores averaged across each target domain as well as a global macro average in Table 2.\nSelf-training achieves surprisingly good results but is not able to compete with tri-training. Tritraining with disagreement is only slightly better than self-training, showing that the disagreement component might not be useful when there is a strong domain shift. Tri-training achieves the best\naverage results on two target domains and clearly outperforms the state of the art on average.\nMT-Tri finally outperforms the state of the art on 3/4 domains, and even slightly traditional tritraining, resulting in the overall best method. This improvement is mainly due to the B->E and D->E scenarios, on which tri-training struggles. These domain pairs are among those with the highest Adistance (Blitzer et al., 2007), which highlights that tri-training has difficulty dealing with a strong shift in domain. Our method is able to mitigate this deficiency by training one of the three output layers only on pseudo-labeled target domain examples.\nIn addition, MT-Tri is more efficient as it adds a smaller number of pseudo-labeled examples than tri-training at every epoch. For sentiment analysis, tri-training adds around 1800-1950/2000 unlabeled examples at every epoch, while MT-Tri only adds around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition, adding fewer examples poses a smaller risk of swamping the learned representations with useless signals and is more akin to fine-tuning, the standard method for supervised domain adaptation (Howard and Ruder, 2018).\nWe observe an asymmetry in the results between some of the domain pairs, e.g. B->D and D->B. We hypothesize that the asymmetry may be due to properties of the data and that the domains are relatively far apart e.g., in terms of A-distance. In fact, asymmetry in these domains is already reflected\nin the results of Blitzer et al. (2007) and is corroborated in the results for asymmetric tri-training (Saito et al., 2017) and our method.\nWe note a weakness of this dataset is high variance. Existing approaches only report the mean, which makes an objective comparison difficult. For this reason, we believe it is essential to evaluate proposed approaches also on other tasks.\nPOS tagging Results for tagging in the low-data regime (10% of WSJ) are given in Table 3.\nSelf-training does not work for the sequence prediction task. We report only the best instantia-\ntion (throttling with n=800). Our results contribute to negative findings regarding self-training (Plank, 2011; Van Asch and Daelemans, 2016).\nIn the low-data setup, tri-training with disagreement works best, reaching an overall average accuracy of 89.70, closely followed by classic tritraining, and significantly outperforming the baseline on 4/5 domains. The exception is newsgroups, a difficult domain with high OOV rate where none of the approches beats the baseline (see §3.4). Our proposed MT-Tri is better than asymmetric tritraining, but falls below classic tri-training. It beats\nthe baseline significantly on only 2/5 domains (answers and emails). The FLORS tagger (Yin et al., 2015) fares better. Its contextual distributional features are particularly helpful on unknown word-tag combinations (see § 3.4), which is a limitation of the lexicalized generic bi-LSTM tagger.\nFor the high-data setup (Table 4) results are similar. Disagreement, however, is only favorable in the low-data setups; the effect of avoiding easy points no longer holds in the full data setup. Classic tritraining is the best method. In particular, traditional tri-training is complementary to word embedding initialization, pushing the non-pre-trained baseline to the level of SRC with Glove initalization. Tritraining pushes performance even further and results in the best model, significantly outperforming the baseline again in 4/5 cases, and reaching FLORS performance on weblogs. Multi-task tritraining is often slightly more effective than asymmetric tri-training (Saito et al., 2017); however, improvements for both are not robust across domains, sometimes performance even drops. The model likely is too simplistic for such a high-data POS setup, and exploring shared-private models might prove more fruitful (Liu et al., 2017). On the test sets, tri-training performs consistently the best.\nPOS analysis We analyze POS tagging accuracy with respect to word frequency6 and unseen word-tag combinations (UWT) on the dev sets. Table 5 (top rows) provides percentage of un-\n6The binned log frequency was calculated with base 2 (bin 0 are OOVs, bin 1 are singletons and rare words etc).\nknown tags, OOVs and unknown word-tag (UWT) rate. The SANCL dataset is overall very challenging: OOV rates are high (6.8-11% compared to 2.3% in WSJ), so is the unknown word-tag (UWT) rate (answers and emails contain 2.91% and 3.47% UWT compared to 0.61% on WSJ) and almost all target domains even contain unknown tags (Schnabel and Schütze, 2014) (unknown tags: ADD,GW,NFP,XX), except for weblogs. Email is the domain with the highest OOV rate and highest unknown-tag-for-known-words rate. We plot accuracy with respect to word frequency on email in Figure 3, analyzing how the three methods fare in comparison to the baseline on this difficult domain.\nRegarding OOVs, the results in Table 5 (second part) show that classic tri-training outperforms the source model (trained on only source data) on 3/5 domains in terms of OOV accuracy, except on two domains with high OOV rate (newsgroups and weblogs). In general, we note that tri-training works best on OOVs and on low-frequency tokens, which is also shown in Figure 3 (leftmost bins). Both other methods fall typically below the baseline in terms of OOV accuracy, but MT-Tri still outperforms Asym in 4/5 cases. Table 5 (last part) also shows that no bootstrapping method works well on unknown word-tag combinations. UWT tokens are very difficult to predict correctly using an unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains."
  }, {
    "heading": "4 Related work",
    "text": "Learning under Domain Shift There is a large body of work on domain adaptation. Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a), shared feature representations (Blitzer et al., 2006, 2007) and instance weighting (Jiang and Zhai, 2007). Recent ap-\nproaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016). There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017), albeit is not compared to classic tri-training.\nNeural network ensembling Related work on self-ensembling approaches includes snapshot ensembling (Huang et al., 2017) or temporal ensembling (Laine and Aila, 2017). In general, the line between “explicit” and “implicit” ensembling (Huang et al., 2017), like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling.\nMulti-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders."
  }, {
    "heading": "5 Conclusions",
    "text": "We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift. For the two examined NLP tasks classic tri-training works the best and even outperforms a recent state-of-the-art method. The drawback of tri-training it its time and space complexity. We therefore propose a more efficient multi-task tri-training model, which outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis. For POS tagging, classic tri-training is superior, performing especially well on OOVs and low frequency to-\nkens, which suggests it is less affected by error propagation. Overall we emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their valuable feedback. Sebastian is supported by Irish Research Council Grant Number EBPPG/2014/30 and Science Foundation Ireland Grant Number SFI/12/RC/2289. Barbara is supported by NVIDIA corporation and thanks the Computing Center of the University of Groningen for HPC support."
  }],
  "year": 2018,
  "references": [{
    "title": "Semisupervised learning for computational linguistics",
    "authors": ["Steven Abney."],
    "venue": "CRC Press.",
    "year": 2007
  }, {
    "title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces",
    "authors": ["Isabelle Augenstein", "Sebastian Ruder", "Anders Søgaard."],
    "venue": "Proceedings of NAACL-HLT 2018.",
    "year": 2018
  }, {
    "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
    "authors": ["John Blitzer", "Mark Dredze", "Fernando Pereira."],
    "venue": "Annual Meeting-Association for Computational Linguistics, 45(1):440.",
    "year": 2007
  }, {
    "title": "Domain Adaptation with Structural Correspondence Learning",
    "authors": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."],
    "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP ’06), pages 120–128.",
    "year": 2006
  }, {
    "title": "Domain Separation Networks",
    "authors": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."],
    "venue": "NIPS.",
    "year": 2016
  }, {
    "title": "Multitask learning: A knowledgebased source of inductive bias",
    "authors": ["Rich Caruana."],
    "venue": "Proceedings of the Tenth International Conference on Machine Learning.",
    "year": 1993
  }, {
    "title": "Semi-Supervised Learning, volume 1",
    "authors": ["Olivier Chapelle", "Bernhard Schölkopf", "Alexander Zien."],
    "venue": "MIT press.",
    "year": 2006
  }, {
    "title": "Open-domain name error detection using a multitask rnn",
    "authors": ["Hao Cheng", "Hao Fang", "Mari Ostendorf."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 737–746. Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation",
    "authors": ["Trevor Cohn", "Lucia Specia."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2013
  }, {
    "title": "Stronger baselines for trustable results in neural machine translation",
    "authors": ["Michael Denkowski", "Graham Neubig."],
    "venue": "arXiv preprint arXiv:1706.09733.",
    "year": 2017
  }, {
    "title": "Deep Biaffine Attention for Neural Dependency Parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "Proceedings of ICLR 2017.",
    "year": 2017
  }, {
    "title": "An introduction to the bootstrap",
    "authors": ["Bradley Efron", "Robert J Tibshirani."],
    "venue": "CRC press.",
    "year": 1994
  }, {
    "title": "Learning when to trust distant supervision: An application to lowresource pos tagging using cross-lingual projection",
    "authors": ["Meng Fang", "Trevor Cohn."],
    "venue": "Proceedings of CoNLL-16.",
    "year": 2016
  }, {
    "title": "Model transfer for tagging low-resource languages using a bilingual dictionary",
    "authors": ["Meng Fang", "Trevor Cohn."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 587–593. Associa-",
    "year": 2017
  }, {
    "title": "Domain-Adversarial Training of Neural Networks",
    "authors": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Francois Laviolette", "Mario Marchand", "Victor Lempitsky."],
    "venue": "Journal of Machine Learning Research, 17:1–35.",
    "year": 2016
  }, {
    "title": "To normalize, or not to normalize: The impact of normalization on part-of-speech tagging",
    "authors": ["Rob van der Goot", "Barbara Plank", "Malvina Nissim."],
    "venue": "Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 31–39, Copenhagen, Den-",
    "year": 2017
  }, {
    "title": "On Calibration of Modern Neural Networks",
    "authors": ["Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger."],
    "venue": "Proceedings of ICML 2017.",
    "year": 2017
  }, {
    "title": "Deep semantic role labeling: What works and what’s next",
    "authors": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2017
  }, {
    "title": "Universal Language Model Fine-tuning for Text Classification",
    "authors": ["Jeremy Howard", "Sebastian Ruder."],
    "venue": "Proceedings of ACL 2018.",
    "year": 2018
  }, {
    "title": "Snapshot Ensembles: Train 1, get M for free",
    "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger."],
    "venue": "Proceedings of ICLR 2017.",
    "year": 2017
  }, {
    "title": "Instance weighting for domain adaptation in nlp",
    "authors": ["Jing Jiang", "ChengXiang Zhai."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271. Association for Computational Linguistics.",
    "year": 2007
  }, {
    "title": "Adversarial adaptation of synthetic or stale data",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1297–1307, Vancouver,",
    "year": 2017
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional lstm feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:313–327.",
    "year": 2016
  }, {
    "title": "Temporal Ensembling for Semi-Supervised Learning",
    "authors": ["Samuli Laine", "Timo Aila."],
    "venue": "Proceedings of ICLR 2017.",
    "year": 2017
  }, {
    "title": "Neural Architectures for Named Entity Recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "NAACL-HLT 2016.",
    "year": 2016
  }, {
    "title": "Adversarial multi-task learning for text classification",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1–10, Vancouver,",
    "year": 2017
  }, {
    "title": "The variational fair autoencoder",
    "authors": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel."],
    "venue": "arXiv preprint arXiv:1511.00830.",
    "year": 2015
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "arXiv preprint arXiv:1511.06114.",
    "year": 2015
  }, {
    "title": "Computational linguistics and deep learning",
    "authors": ["Christopher D Manning."],
    "venue": "Computational Linguistics, 41(4):701–707.",
    "year": 2015
  }, {
    "title": "Effective self-training for parsing",
    "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, New York City, USA. Association for Computational Linguis-",
    "year": 2006
  }, {
    "title": "Reranking and Self-Training for Parser Adaptation",
    "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson."],
    "venue": "International Conference on Computational Linguistics (COLING) and Annual Meeting of the Association for Computational Linguistics",
    "year": 2006
  }, {
    "title": "On the State of the Art of Evaluation in Neural Language Models",
    "authors": ["Gábor Melis", "Chris Dyer", "Phil Blunsom."],
    "venue": "arXiv preprint arXiv:1707.05589.",
    "year": 2017
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"],
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Overview of the 2012 shared task on parsing the web",
    "authors": ["Slav Petrov", "Ryan McDonald."],
    "venue": "Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL), 59.",
    "year": 2012
  }, {
    "title": "Domain adaptation for parsing",
    "authors": ["Barbara Plank."],
    "venue": "University Library Groningen.",
    "year": 2011
  }, {
    "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
    "authors": ["Barbara Plank", "Anders Søgaard", "Yoav Goldberg."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-",
    "year": 2016
  }, {
    "title": "Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets",
    "authors": ["Roi Reichart", "Ari Rappoport."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616–623.",
    "year": 2007
  }, {
    "title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
    "authors": ["Nils Reimers", "Iryna Gurevych."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 338–",
    "year": 2017
  }, {
    "title": "Learning what to share between loosely related tasks",
    "authors": ["Sebastian Ruder", "Joachim Bingel", "Isabelle Augenstein", "Anders Søgaard."],
    "venue": "arXiv preprint arXiv:1705.08142.",
    "year": 2017
  }, {
    "title": "Asymmetric Tri-training for Unsupervised Domain Adaptation",
    "authors": ["Kuniaki Saito", "Yoshitaka Ushiku", "Tatsuya Harada."],
    "venue": "ICML 2017.",
    "year": 2017
  }, {
    "title": "FLORS: Fast and Simple Domain Adaptation for Part-ofSpeech Tagging",
    "authors": ["Tobias Schnabel", "Hinrich Schütze."],
    "venue": "TACL, 2:15–26.",
    "year": 2014
  }, {
    "title": "Improving neural machine translation models with monolingual data",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2016
  }, {
    "title": "Simple semi-supervised training of part-of-speech taggers",
    "authors": ["Anders Søgaard."],
    "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 205–208.",
    "year": 2010
  }, {
    "title": "Deep multitask learning with low level tasks supervised at lower layers",
    "authors": ["Anders Søgaard", "Yoav Goldberg."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 231–235, Berlin, Germany. Association",
    "year": 2016
  }, {
    "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15:1929–1958.",
    "year": 2014
  }, {
    "title": "Example selection for bootstrapping statistical parsers",
    "authors": ["Mark Steedman", "Rebecca Hwa", "Stephen Clark", "Miles Osborne", "Anoop Sarkar", "Julia Hockenmaier", "Paul Ruhlen", "Steven Baker", "Jeremiah Crim."],
    "venue": "Proceedings of the 2003 Human Lan-",
    "year": 2003
  }, {
    "title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data",
    "authors": ["Jun Suzuki", "Hideki Isozaki."],
    "venue": "pages 665–673.",
    "year": 2008
  }, {
    "title": "Predicting the effectiveness of self-training: Application to sentiment classification",
    "authors": ["Vincent Van Asch", "Walter Daelemans."],
    "venue": "arXiv preprint arXiv:1601.03288.",
    "year": 2016
  }, {
    "title": "Sentiment Domain Adaptation with Multiple Sources",
    "authors": ["Fangzhao Wu", "Yongfeng Huang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 301–310.",
    "year": 2016
  }, {
    "title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
    "authors": ["David Yarowsky."],
    "venue": "Proceedings of the 33rd annual meeting on Association for Computational Linguistics.",
    "year": 1995
  }, {
    "title": "Online Updating of Word Representations for Part-of-Speech Tagging",
    "authors": ["Wenpeng Yin", "Tobias Schnabel", "Hinrich Schütze."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, September, pages 1329–1334.",
    "year": 2015
  }, {
    "title": "Bi-transferring deep neural networks for domain adaptation",
    "authors": ["Guangyou Zhou", "Zhiwen Xie", "Jimmy Xiangji Huang", "Tingting He."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
    "year": 2016
  }, {
    "title": "Tri-Training: Exploiting Unlabeled Data Using Three Classifiers",
    "authors": ["Zhi-Hua Zhou", "Ming Li."],
    "venue": "IEEE Trans.Data Eng., 17(11):1529–1541.",
    "year": 2005
  }, {
    "title": "Semi-Supervised Learning Literature Survey",
    "authors": ["Xiaojin Zhu."],
    "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.",
    "year": 2005
  }, {
    "title": "Introduction to semi-supervised learning",
    "authors": ["Xiaojin Zhu", "Andrew B Goldberg."],
    "venue": "Synthesis lectures on artificial intelligence and machine learning, 3(1):1–130.",
    "year": 2009
  }],
  "id": "SP:8a08f75b6ebf606a427f6fb18d72f142854d877c",
  "authors": [{
    "name": "Sebastian Ruder",
    "affiliations": []
  }, {
    "name": "Barbara Plank",
    "affiliations": []
  }],
  "abstractText": "Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.",
  "title": "Strong Baselines for Neural Semi-Supervised Learning under Domain Shift"
}