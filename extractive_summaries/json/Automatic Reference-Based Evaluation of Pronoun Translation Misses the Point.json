{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4797–4802 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4797"
  }, {
    "heading": "1 Introduction",
    "text": "As the general quality of machine translation (MT) increases, there is a growing interest in improving the translation of specific linguistic phenomena. A case in point that has been studied in the context of both statistical (Hardmeier, 2014; Guillou, 2016; Loáiciga, 2017) and neural MT (Bawden et al., 2017; Voita et al., 2018) is that of pronominal anaphora. In the simplest case, translating anaphoric pronouns requires the generation of corresponding word forms respecting the grammatical constraints on agreement in the target language, as in the following English-French example, where the correct form of the pronoun in the second sentence varies depending on which of the (equally correct) translations of the word bicycle was used in the first:\n(1) a. I have a bicycle. It is red. b. J’ai un vélo. Il est rouge. [ref] c. J’ai une bicyclette. Elle est rouge. [MT]\nHowever, the problem is more complex in practice because there is often no 1 : 1 correspondence between pronouns in two languages. This is easily demonstrated at the corpus level by observing that the number of pronouns varies significantly across languages in parallel texts (Mitkov\n*Both authors contributed equally.\nand Barbu, 2003), but it tends to be difficult to predict in individual cases.\nIn general MT research, significant progress was enabled by the invention of automatic evaluation metrics based on reference translations, such as BLEU (Papineni et al., 2002). Attempting to create a similar framework for efficient research, researchers have proposed automatic reference-based evaluation metrics specifically targeting pronoun translation: AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and PopescuBelis, 2017). We study the performance of these metrics on a dataset of English-French translations and investigate to what extent automatic evaluation based on reference translations provides insights into how well an MT system handles pronouns. Our analysis clarifies the conceptual differences between AutoPRF and APT, uncovering weaknesses in both metrics, and investigates the effects of the alignment correction heuristics used in APT. By using the fine-grained PROTEST categories of pronoun function, we find that the accuracy of the automatic metrics varies across pronouns of different functions, suggesting that certain linguistic patterns are captured better in the automatic evaluation than others. We argue that fully automatic wide-coverage evaluation of this phenomenon is unlikely to drive research forward, as it misses essential parts of the problem despite achieving some correlation with human judgements. Instead, semiautomatic evaluation involving automatic identification of correct translations with high precision and low recall appears to be a more achievable goal. Another more realistic option is a test suite evaluation with a very limited scope."
  }, {
    "heading": "2 Pronoun Evaluation Metrics for MT",
    "text": "Two reference-based automatic metrics of pronoun translation have been proposed in the literature.\nThe first (Hardmeier and Federico, 2010) is a variant of precision, recall and F-score that measures the overlap of pronouns in the MT output with a reference translation. It lacks an official name, so we refer to it as AutoPRF following the terminology of the DiscoMT 2015 shared task (Hardmeier et al., 2015). The scoring process relies on a word alignment between the source and the MT output, and between the source and the reference translation. For each input pronoun, it computes a clipped count (Papineni et al., 2002) of the overlap between the aligned tokens in the reference and the MT output. The clipped count of a given word is defined as the number of times it occurs in the MT output, limited by the number of times it occurs in the reference translation. The final metric is then calculated as the precision, recall and F-score based on these clipped counts.\nMiculicich Werlen and Popescu-Belis (2017) propose a metric called Accuracy of Pronoun Translation (APT) that introduces several innovations over the previous work. It is a variant of accuracy, so it counts, for each source pronoun, whether its translation can be considered correct, without considering multiple alignments. Since word alignment is problematic for pronouns, the authors propose an heuristic procedure to improve alignment quality. Finally, it introduces the notion of pronoun equivalence, assigning partial credit to pronoun translations that differ from the reference translation in specific ways deemed to be acceptable. In particular, it considers six possible cases when comparing the translation of a pronoun in MT output and the reference. The pronouns may be: (1) identical, (2) equivalent, (3) different/incompatible, or there may be no translation in: (4) the MT output, (5) the reference, (6) either the MT output or the reference. Each of these cases may be assigned a weight between 0 and 1 to determine the level of correctness."
  }, {
    "heading": "3 The PROTEST Dataset",
    "text": "We study the behaviour of the two automatic metrics using the PROTEST test suite (Guillou and Hardmeier, 2016). The test suite comprises 250 hand-selected personal pronoun tokens taken from the DiscoMT2015.test dataset of TED talk transcriptions and translations (Hardmeier et al., 2016) and annotated according to the ParCor guidelines (Guillou et al., 2014). It is structured according to a linguistic typology motivated by work on func-\ntional grammar by Dik (1978) and Halliday (2004). Pronouns are first categorised according to their function:\nanaphoric: I have a bicycle. It is red. event: He lost his job. It was a shock. pleonastic: It is raining. addressee reference: You’re welcome. They are then subcategorised according to morphosyntactic criteria, whether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic).\nOur dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite. The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO-POSTEDIT (Guillou, 2015), UU-HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU-TIEDEMANN (Tiedemann, 2015), a rule-based system ITS2 (Loáiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018).\nManual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016). The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases where it impacted the annotator’s ability to make a judgement). The annotations were carried out by two bilingual English-French speakers, both of whom are native speakers of French. Our human judgements differ in important ways from the human evaluation conducted for the same set of systems at DiscoMT 2015 (Hardmeier et al., 2015), which was carried out by non-native speakers over an unbalanced data sample using a gap-filling methodology. In the gap-filling task annotators are asked to select, from a predefined list (including an uninformative catch-all group “other”), those pronouns that could fill the pronoun translation slot. Unlike in the PROTEST evaluation, the pronoun translations were obscured in the MT output. This avoided priming the annotators with the output of\nthe candidate translation, but it occasionally caused valid translations to be rejected because they were missed by the annotator."
  }, {
    "heading": "4 Accuracy versus Precision/Recall",
    "text": "There are three ways in which APT differs from AutoPRF: the scoring statistic, the alignment heuristic in APT, and the definition of pronoun equivalence.\nAPT is a measure of accuracy: It reflects the proportion of source pronouns for which an acceptable translation was produced in the target. AutoPRF, by contrast, is a precision/recall metric on the basis of clipped counts. Hardmeier and Federico (2010) motivate the use of precision and recall by pointing out that word alignments are not 1 : 1, so each pronoun can be linked to multiple elements in the target language, both in the reference translation and in the MT output. Their metric is designed to account for all linked words in such cases.\nTo test the validity of this argument, we examined the subset of examples of 8 systems in our English-French dataset1 giving rise to a clipped count greater than one2 and found that these examples follow very specific patterns. All 143 cases included exactly one personal pronoun. In 99 cases, the additional matched word was the complementiser que ‘that’. In 31 and 4 cases, respectively, it was a form of the auxiliary verbs avoir ‘to have’ and être ‘to be’. One example matched both que and a form of être. Two had reflexive pronouns, and one an imperative verb form. With the possible exception of the two reflexive pronouns, none of this seems to be relevant to pronoun correctness. We conclude that it is more reasonable to restrict the counts to a single pronominal item per example. With this additional restriction, however, the recall score of AutoPRF becomes equivalent to a version of APT without equivalent pronouns and alignment correction. We therefore limit the remainder of our study to APT."
  }, {
    "heading": "5 Effects of Word Alignment",
    "text": "APT includes an heuristic alignment correction procedure to mitigate errors in the word alignment between a source-language text and its translation (reference or MT output). We ran experiments to\n1Excluding the YANDEX system, which was added later. 2A clipped count greater than one for a given pronoun translation indicates that the MT output and the reference translation aligned to this pronoun overlap in more than one token.\nassess the correlation of APT with human judgements, with and without the alignment correction heuristics.\nTable 1 displays the APT results in both conditions and the proportion of pronouns in the PROTEST test suite marked as correctly translated. For better comparison with the PROTEST test suite results, we restricted APT to the pronouns in the test suite. We used two different weight settings:3 APT-A uses weight 1 for identical matches and 0 for all other cases. APT-B uses weight 1 for identical matches, 0.5 for equivalent matches and 0 otherwise.\nThere is little difference in the APT scores when we consider the use of alignment heuristics. This is due to the small number of pronouns for which alignment improvements are applied for most systems (typically 0–12 per system). The exception is the ITS2 system output for which 18 alignment improvements are made. For the following systems we observe a very small increase in APT score for each of the two weight settings we consider, when alignment heuristics are applied: UU-HARDMEIER (+0.8), ITS2 (+0.8), BASELINE (+0.8), YANDEX (+0.8), and NYU (+0.4). However, these small improvements are not sufficient to affect the system rankings. It seems, therefore, that the alignment heuristic has only a small impact on the validity of the score.\nTo assess differences in correlation with human judgment for pairs of APT settings, we run Williams’s significance test (Williams, 1959; Graham and Baldwin, 2014). The test reveals that differences in correlation between the various configurations of APT and human judgements are not statistically significant (p > 0.2 in all cases).\n3Personal recommendation by Lesly Miculicich Werlen."
  }, {
    "heading": "6 Metric Accuracy per Category",
    "text": "Like Miculicich Werlen and Popescu-Belis (2017), we use Pearson’s and Spearman’s correlation coefficients to assess the correlation between APT and our human judgements (Table 2). Although APT does correlate with the human judgements over the PROTEST test suite, the correlation is weaker than that with the DiscoMT gap-filling evaluations reported in Miculicich Werlen and Popescu-Belis (2017). A Williams significance test reveals that the difference in correlation (for those systems common to both studies) is not statistically significant (p > 0.3). Table 1 also shows that the rankings induced from the PROTEST and APT scores are rather different. The differences are due to the different ways in which the two metrics define pronoun correctness, and the different sources against which correctness is measured (reference translation vs. human judgement).\nWe also study how the results of APT (with alignment correction) interact with the categories in PROTEST. We consider a pronoun to be measured as correct by APT if it is assigned case 1\n(identical) or 2 (equivalent). Likewise, a pronoun is considered incorrect if it is assigned case 3 (incompatible). We compare the number of pronouns marked as correct/incorrect by APT and by the human judges, ignoring APT cases in which no judgement can be made: no translation of the pronoun in the MT output, reference or both, and pronouns for which the human judges were unable to make a judgement due to factors such as poor overall MT quality, incorrect word alignments, etc. The results of this comparison are displayed in Table 3.\nAt first glance, we can see that APT disagrees with the human judgements for almost a quarter (24.3%) of the assessed translations. The distribution of the disagreements over APT cases is very skewed and ranges from 8% for case 1 to 32% for case 2 and 49% for case 3. In other words, APT identifies correct pronoun translations with good precision, but relatively low recall. We can also see that APT rarely marks pronouns as equivalent (case 2).\nPerformance for anaphoric pronouns is mixed. In general, there are three main problems affecting anaphoric pronouns (Table 4). 1) APT, which does not incorporate knowledge of anaphoric pronoun antecedents, does not consider pronoun-antecedent head agreement so many valid alternative translations involving personal pronouns are marked as incompatible (i.e. incorrect, case 3), but as correct by the human judges. Consider the following example, in which the pronoun they is deemed correctly translated by the YANDEX system (according to the human judges) as it agrees in number and grammatical gender with the translation of the antecedent extraits (clips). However, the pronoun translation ils is marked as incorrect by APT as it does not match the translation in the reference (elles).\nSOURCE: so what these two clips show is not just the devastating consequence of the disease, but they also tell us something about the shocking pace of the disease. . .\nYANDEX: donc ce que ces deux extraits[masc.,pl.] montrent n’est pas seulement la conséquence dévastatrice de la maladie, mais ils[masc. pl.] nous disent aussi quelque chose sur le rythme choquant de la maladie. . .\nREFERENCE: ce que ces deux vidéos[fem.,pl.] montrent, ce ne sont pas seulement les conséquences dramatiques de cette maladie, elles[fem. pl.] nous montrent aussi la vitesse fulgurante de cette maladie. . .\n2) Substitutions between pronouns are governed by much more complex rules than the simple pronoun equivalence mechanism in APT. For example, the dictionary of pronouns used in APT lists il and ce as equivalent. However, while il can often replace ce as a pleonastic pronoun in French, it has a much stronger tendency to be interpreted as anaphoric, rendering pleonastic use unacceptable if there is a salient masculine antecedent in the context. 3) APT does not consider the use of impersonal pronouns such as c’ in place of the feminine personal pronoun elle or the plural forms ils and elles.\nCategory V E I O\nAnaphoric intra-sent. subj. it 22 9 8 8 intra-sent. non-subj. it 16 – 1 2 inter-sent. subj. it 35 6 22 – inter-sent. non-subj. it – – – 13 intra-sent. they 25 – 3 9 inter-sent. they 22 – 3 22 singular they 40 – – 18 group it/they 21 – – 10\nEvent it – 16 – 44\nPleonastic it – 11 – 35\nV: Valid alternative translation I: Impersonal translation E: Incorrect equivalence O: Other\nTable 4: Common cases of disagreement for anaphoric, pleonastic, and event reference pronouns\nAs with anaphoric pronouns, APT incorrectly marks some pleonastic and event translations as equivalent, in disagreement with the human judges. Other common errors arise from 1) the use of alternative translations marked as incompatible (i.e. incorrect) by APT but correct by the human judges, for example il (personal) in the MT output when the reference contained the impersonal pronoun cela or ça (30 cases for pleonastic, 7 for event), or 2) the presence of il in both the MT output and reference marked by APT as identical but by the human judges as incorrect (3 cases for pleonastic, 15 event).\nSome of these issues could be addressed by incorporating knowledge of pronoun function in the source language, of pronoun antecedents, and of the wider context of the translation surrounding the pronoun. However, whilst we might be able to derive language-specific rules for some scenarios, it would be difficult to come up with more general or language-independent rules. For example, il and ce can be anaphoric or pleonastic pronouns, but\nil has a more referential character. Therefore in certain constructions that are strongly pleonastic (e.g. clefts) only ce is acceptable. This rule would be specific to French, and would not cover other scenarios for the translation of pleonastic it. Other issues include the use of pronouns in impersonal constructions such as il faut [one must/it takes] in which evaluation of the pronoun requires consideration of the whole expression, or transformations between active and passive voice, where the perspective of the pronouns changes."
  }, {
    "heading": "7 Conclusions",
    "text": "Our analyses reveal that despite some correlation between APT and the human judgements, fully automatic wide-coverage evaluation of pronoun translation misses essential parts of the problem. Comparison with human judgements shows that APT identifies good translations with relatively high precision, but fails to reward important patterns that pronoun-specific systems must strive to generate. Instead of relying on fully automatic evaluation, our recommendation is to emphasise high precision in the automatic metrics and implement semiautomatic evaluation procedures that refer negative cases to a human evaluator, using available tools and methods (Hardmeier and Guillou, 2016). Fully automatic evaluation of a very restricted scope may still be feasible using test suites designed for specific problems (Bawden et al., 2017)."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank our annotators, Marie Dubremetz and Miryam de Lhoneux, for their many hours of painstaking work, Lesly Miculicich Werlen for providing APT results for the DiscoMT 2015 systems, Elena Voita, Sébastien Jean, Stanislas Lauly and Rachel Bawden for providing the NMT system outputs, and the three anonymous reviewers. The annotation work was funded by the European Association for Machine Translation. The work carried out at The University of Edinburgh was funded by the ERC H2020 Advanced Fellowship GA 742137 SEMANTAX and a grant from The University of Edinburgh and Huawei Technologies. The work carried out at Uppsala University was funded by the Swedish Research Council under grant 2017-930."
  }],
  "year": 2018,
  "references": [{
    "title": "Evaluating discourse phenomena in neural machine translation",
    "authors": ["Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow."],
    "venue": "CoRR, abs/1711.00513.",
    "year": 2017
  }, {
    "title": "Functional Grammar",
    "authors": ["Simon C. Dik."],
    "venue": "Amsterdam, North-Holland.",
    "year": 1978
  }, {
    "title": "Testing for significance of increased correlation with human judgment",
    "authors": ["Yvette Graham", "Timothy Baldwin."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, Doha, Qatar. Associ-",
    "year": 2014
  }, {
    "title": "Automatic post-editing for the DiscoMT pronoun translation task",
    "authors": ["Liane Guillou."],
    "venue": "Proceedings of the Second Workshop on Discourse in Machine Translation, pages 65–71, Lisbon, Portugal. Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Incorporating Pronoun Function into Statistical Machine Translation",
    "authors": ["Liane Guillou."],
    "venue": "Ph.D. thesis, Edinburgh University, Department of Informatics.",
    "year": 2016
  }, {
    "title": "PROTEST: A test suite for evaluating pronouns in machine translation",
    "authors": ["Liane Guillou", "Christian Hardmeier."],
    "venue": "Proceedings of the Eleventh Language Resources and Evaluation Conference, LREC 2016, pages 636–643, Portorož, Slovenia.",
    "year": 2016
  }, {
    "title": "ParCor 1.0: A parallel pronoun-coreference corpus to support statistical MT",
    "authors": ["Liane Guillou", "Christian Hardmeier", "Aaron Smith", "Jörg Tiedemann", "Bonnie Webber"],
    "venue": "In Proceedings of the 9th International Conference on Language Resources and Eval-",
    "year": 2014
  }, {
    "title": "An introduction to functional grammar, 3rd edition",
    "authors": ["Michael A.K. Halliday."],
    "venue": "Hodder Arnold London.",
    "year": 2004
  }, {
    "title": "Discourse in Statistical Machine Translation",
    "authors": ["Christian Hardmeier."],
    "venue": "Ph.D. thesis, Uppsala University, Department of Linguistics and Philology.",
    "year": 2014
  }, {
    "title": "Modelling pronominal anaphora in statistical machine translation",
    "authors": ["Christian Hardmeier", "Marcello Federico."],
    "venue": "Proceedings of the 7th International Workshop on Spoken Language Translation, IWSLT 2010, pages 283–289, Paris, France.",
    "year": 2010
  }, {
    "title": "A graphical pronoun analysis tool for the protest pronoun evaluation test suite",
    "authors": ["Christian Hardmeier", "Liane Guillou."],
    "venue": "Baltic Journal of Modern Computing, (2):318–330.",
    "year": 2016
  }, {
    "title": "Pronoun-focused MT and cross-lingual pronoun prediction: Findings of the 2015 DiscoMT shared task on pronoun translation",
    "authors": ["Christian Hardmeier", "Preslav Nakov", "Sara Stymne", "Jörg Tiedemann", "Yannick Versley", "Mauro Cettolo."],
    "venue": "Proceedings",
    "year": 2015
  }, {
    "title": "DiscoMT 2015 Shared Task on Pronoun Translation",
    "authors": ["Christian Hardmeier", "Jörg Tiedemann", "Preslav Nakov", "Sara Stymne", "Yannick Versely."],
    "venue": "LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in",
    "year": 2016
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "ArXiv e-prints, 1412.2007.",
    "year": 2014
  }, {
    "title": "Rule-based pronominal anaphora treatment for machine translation",
    "authors": ["Sharid Loáiciga", "Eric Wehrli."],
    "venue": "Proceedings of the Second Workshop on Discourse in Machine Translation, pages 86–93, Lisbon, Portugal. Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Pronominal anaphora and verbal tenses in machine translation",
    "authors": ["Sharid Loáiciga."],
    "venue": "Ph.D. thesis, Université de Genève.",
    "year": 2017
  }, {
    "title": "Pronoun translation and prediction with or without coreference links",
    "authors": ["Ngoc Quang Luong", "Lesly Miculicich Werlen", "Andrei Popescu-Belis."],
    "venue": "Proceedings of the Second Workshop on Discourse in Machine Translation, pages 94–100, Lisbon, Portu-",
    "year": 2015
  }, {
    "title": "Validation of an automatic metric for the accuracy of pronoun translation (APT)",
    "authors": ["Lesly Miculicich Werlen", "Andrei Popescu-Belis."],
    "venue": "Proceedings of the Third Workshop on Discourse in Machine Translation (DiscoMT). Association for Com-",
    "year": 2017
  }, {
    "title": "Using bilingual corpora to improve pronoun resolution",
    "authors": ["Ruslan Mitkov", "Catalina Barbu."],
    "venue": "Languages in Contrast, 4(2):201–211.",
    "year": 2003
  }, {
    "title": "BLEU: A method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia",
    "year": 2002
  }, {
    "title": "Baseline models for pronoun prediction and pronoun-aware translation",
    "authors": ["Jörg Tiedemann."],
    "venue": "Proceedings of the Second Workshop on Discourse in Machine Translation, pages 108–114, Lisbon, Portugal. Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Context-aware neural machine translation learns anaphora resolution",
    "authors": ["Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Association for Compu-",
    "year": 2018
  }, {
    "title": "Regression Analysis, volume 14",
    "authors": ["Evan J. Williams."],
    "venue": "Wiley, New York.",
    "year": 1959
  }],
  "id": "SP:0cadbb4732abb5559b4b117da101d083378190ab",
  "authors": [{
    "name": "Liane Guillou",
    "affiliations": []
  }, {
    "name": "Christian Hardmeier",
    "affiliations": []
  }],
  "abstractText": "We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semiautomatic metrics and test suites in place of fully automatic metrics.",
  "title": "Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point"
}