{
  "sections": [{
    "heading": "1. Introduction",
    "text": ""
  }, {
    "heading": "1.1. Symmetric distribution properties",
    "text": "Let def= {(p 1 , . . . ,p k ) : p i 0, Pk i=1 p i =1, 1  k  1} denote the collection of all discrete distributions over finite or infinite support. A distribution property is a mapping f : ! R. It is symmetric if it remains unchanged under relabeling of domain symbols, namely if it is determined by just the probability multiset {p\n1 , p 2 , . . . ,p k }. Many important properties are symmetric. For example:\nSupport size S(p) = |{x : p(x) > 0}|, plays an important role in population and vocabulary estimation.\nSupport coverage S m (p) = P\nx (1 (1 p(x))m), the expected number of elements observed in m samples, arises in ecological and biological studies, e.g., (Colwell et al., 2012).\nShannon entropy H(p) = P\nx p(x) log 1 p(x) , central to information theory (Cover & Thomas, 2006), has numerous\n*Equal contribution 1Cornell University, Ithaca, NY 2Yahoo Inc!, Sunnyvale, CA 3University of California, San Diego 4Google Research. Correspondence to: Jayadev Acharya <acharya@cornell.edu>, Hirakendu Das <hdas@yahooinc.com>, Alon Orlitsky <alon@ucsd.edu>, Ananda Theertha Suresh <theertha@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\napplications.\nDistance to uniform kp uk 1 = P\nx |p(x) 1/|X ||, where u is the uniform distribution over the domain X of p. This distance measure appears in the error of hypothesis testing, and the uniform distribution is arguably one of the commonest discrete distributions."
  }, {
    "heading": "1.2. Distribution estimation",
    "text": "Considerable research, over many years, has focused on estimating distribution properties. In the common setting, an unknown underlying distribution p 2 generates n independent samples Xn def= X\n1 , , . . . ,X n , and the objective is to estimate a given property f(p) as accurately as possible.\nSpecifically, an estimator for a distribution p over X is a function ˆf : Xn ! R mapping observed samples to a property estimate. The sample complexity of ˆf is the smallest number of samples it requires to estimate a property f with accuracy \" and confidence probability , for all distributions in a collection P ✓ ,\nC ˆ f (f,P, , \") def= min n n : p(|f(p) ˆf(Xn)| \")  8p 2 P o .\nThe sample complexity of estimating f is the lowest sample complexity of any estimator,\nC⇤(f,P, , \") = min ˆ\nf\nC ˆ f (f,P, , \").\nBy taking the median of about log 1 independent estimators, the error rate can be driven down from a constant to . Therefore, the sample complexity depends on only through a factor of at most log 1\n. For simplicity, we therefore abbreviate C ˆf (f,P, 1/3, \") by C ˆf (f,P, \")."
  }, {
    "heading": "1.3. Result summary",
    "text": "Recent research has shown that while simple estimators for the aforementioned properties require sample size n proportional to the support size k, more sophisticated techniques need only a sub-linear sample size n = ⇥(k/ log k).\nHowever, each of the problems was approximated via different estimators and analysis techniques, that for some properties were rather complex.\nMotivated by the principle of maximum likelihood, we show that a single, simple, plug-in estimator—profile maximum likelihood (PML) (Orlitsky et al., 2004b)— is competitive for estimating any symmetric property. Its sample complexity is at most quadratically worse than that of any estimator.\nSpecifically, we show that if a symmetric property can be estimated using n samples with confidence , then the PML plug-in estimator can estimate it using as many samples with confidence ·epn. While this increase may seem high, note that it is sub-exponential. We show that if a property has an estimator that has a small bounded difference constant (how much the estimator changes when we change one sample), then the error probability reduces exponentially with n (Please see Section 7.1). Combined, these two facts imply that for properties with locally-smooth estimators, the PML plug-in estimator is optimal up to a constant: CPML = ⇥(C⇤). We then show that all the above properties have locally-smooth estimators, hence they can be estimated by the PML plug-in estimator with up to a constant factor more than the optimal number of samples."
  }, {
    "heading": "1.4. Outline",
    "text": "The rest of the paper is organized as follows. In Section 2 we describe existing results and those shown in this paper. In Section 3 we formally define the quantities involved and state the results. In Section 4 we define profiles and PML. In Section 5, we outline the new approach. In Section 6, we demonstrate auxiliary results for maximum likelihood estimators. In Section 7, we outline how we apply maximum likelihood to support, support coverage, entropy, and uniformity. In Section 8, we provide the details for support, and support coverage and in the appendix we outline results for distance to uniformity and entropy."
  }, {
    "heading": "2. Previous and New Results",
    "text": ""
  }, {
    "heading": "2.1. Previous Results",
    "text": "Plug-in estimation is a general approach for estimating distribution properties. It uses the samples Xn to find an approximation p̂ of p, and lets f(p̂) estimate f(p).\nOne of the most common distribution estimators, dating back to Fisher is maximum likelihood, that for clarity we call sequence maximum likelihood (SML) (Aldrich, 1997). To any sample xn it assigns the distribution p that maximizes p(xn). The SML estimate is exceedingly simple to derive. The multiplicity N\nx\ndef = N\nx (xn) of symbol x is the number of times it appears in the sequence xn. The empiri-\ncal frequency estimator assigns to each symbol x, the fraction p̂(x) def= N\nx /n of times it appears in the sample xn. For example, if x7 =bananas, empirical frequency would assign p̂(a) = 3/7, p̂(n) = 2/7, and p̂(b) = p̂(s) = 1/7. It can be readily shown that SML is exactly the empirical frequency estimator.\nWhile the SML plug-in estimator performs well in the limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.\nSupport size. With finitely many samples, S(p) cannot be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed. Motivated by databases, where each entry appears at least once, (Raskhodnikova et al., 2009) considered distributions whose non-zero probabilities are at least 1\nk\n,\n1 k\ndef = {p 2 : p(x) 2 {0} [ [1/k, 1]} ,\nand estimated the normalized support ˜S(p) def= S(p)/k. It can be shown that CSML( ˜S(p), 1\nk\n, \") = ⇥(k log 1 \" ). Yet (Valiant & Valiant, 2011a; Wu & Yang, 2015) showed that C⇤( ˜S(p), 1\nk\n, \") = ⇥ ⇣ k\nlog k · log2 1 \"\n⌘\n.\nSupport coverage. Here too we consider the normalized coverage ˜S\nm\n(p) def = S\nm (p)/m. (Good & Toulmin, 1956) proposed the Good Toulmin (GT) estimator that achieves CGT( ˜S\nm (p), , \") = m/2. Recently, (Orlitsky et al., 2016) derived a simple estimator showing that C⇤( ˜S\nm (p), , \") = ⇥( m logm · log 1 \"\n). (Zou et al., 2016) derived a more complex estimator with similar dependence on m but worse dependence on \".\nShannon entropy. Since elements with arbitrarily small probability can contribute to an arbitrarily high entropy, H(p) cannot be estimated over aribtrary support with finitely many samples. Therefore researchers are mostly interested in estimating entropy of distributions with support size at most k.\nk\ndef = {p 2 : S(p)  k}.\nIt can be shown that CSML(H(p), k\n, \") = ⇥(k \" ) (Paninski, 2003). Moreover, (Paninski, 2003) showed that C⇤(H(p),\nk , \") is sublinear in k, (Valiant & Valiant, 2011a) showed that the optimal dependence on k is k/ log k and (Wu & Yang, 2016; Jiao et al., 2015) obtained the optimal dependence on both k, and \", and showed that C⇤(H(p),\nk\n, \") = ⇥ ⇣ k\nlog k · 1 \"\n⌘\n.\nDistance to uniform. (Valiant & Valiant, 2011b) showed that C⇤(kp uk\n1 , k\n, \") = O ⇣ k\nlog k · 1 \" 2\n⌘\n, and (Jiao et al., 2016) showed that this bound is tight.\nThese results are summarized in Table 1.\nOther properties were considered as well. (Bar-Yossef et al., 2001; Acharya et al., 2015; Caferov et al., 2015; Obremski & Skorski, 2017) estimated Rényi entropy and (Bu et al., 2016) estimated KL divergence. (Canonne, 2015) surveyed testing whether distributions have certain properties, and (Jiao et al., 2014) studied the performance of SML estimators for several properties. Closest to this work in terms of approach and techniques are (Acharya et al., 2011; 2012; 2013a;b; Valiant & Valiant, 2013; Orlitsky & Suresh, 2015) that design algorithms whose sample complexity is provably close to the best possible regardless of the domain size."
  }, {
    "heading": "2.2. Profile Maximum Likelihood",
    "text": "Symmetric distribution properties do not depend on the symbol labels. They are determined by a simple sufficient statistic: the number of elements appearing any given number of times. The profile of a sequence Xn, denoted '(Xn) is the multiset of the multiplicities of all the symbols appearing in Xn. For example, '(a b r a c a d a b r a) = {1, 1, 2, 2, 5}, as two symbols appearing once, two appearing twice, and one symbol appearing five times, removing the association of the individual symbols with the multiplicities. Profiles are also referred to as histograms of histograms (Batu et al., 2000), histogram order statistics (Paninski, 2003), and fingerprints (Valiant & Valiant, 2011a).\nMotivated by the principle of maximum likelihood, (Orlitsky et al., 2004b; 2017b) discarded the symbol labels, and considered the profile maximum likelihood (PML) distribution that maximizes the probability of the observed profile.\nA number of PML properties were established. (Orlitsky et al., 2004b; 2005) proved PML’s existence, consistency, and some of its properties. (Orlitsky et al., 2004d; 2005; Orlitsky & Pan, 2009; Pan et al., 2009) described additional properties and derived the PML distributions of several short and simple profiles. (Orlitsky et al., 2017b;c) provide a unified review of several of these results. (Anevski et al., 2013) contains a combination of previously-known and new results. A related distribution-estimation approach is described in (Orlitsky et al., 2004c; 2003).\nSeveral approaches were taken to computing the PML distribution. Algebraic computation was considered in (Acharya et al., 2010). A combination of the EM and MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to support-size estimation (Orlitsky et al., 2004a; 2006; Pan, 2012) and a summary of some of the results appears in (Orlitsky et al., 2017a). (Vontobel, 2012; 2014) derived the Bethe approximation of these algorithms.\nFollowing the first draft of this work, (Vatedka & Vontobel, 2016) showed that both theoretically and empirically plug-in estimators obtained from the PML estimate yield good estimates for symmetric functionals of Markov distributions."
  }, {
    "heading": "2.3. New Results",
    "text": "We show that replacing the SML plug-in estimator by PML yields a unified estimator that, like the best results shown via specialized techniques developed, is optimal.\nTheorem 1. There is a unified approach based on PML distribution that achieves the optimal sample complexity for the problems of estimating the entropy, support, support coverage, and distance to uniformity.\nWe prove in Corollary 1 that the PML approach is competitive with respect to any symmetric property. For symmetric properties, these results are perhaps a justification of Fisher’s thoughts on Maximum Likelihood:\n“Of course nobody has been able to prove that maximum likelihood estimates are best under all circumstances. Maximum likelihood estimates computed with all the information available may turn out to be inconsistent. Throwing away a substantial part of the information may render them consistent.”\nR. A. Fisher’s thoughts on Maximum Likelihood (Le Cam, 1979).\nTo prove these PML guarantees, we establish two results that are of interest on their own right.\n• With n samples, PML estimates any symmetric property of p with essentially the same accuracy, and at most e3 p n times the error, of any other estimator. This\nfollows by combining Theorem 3 with Lemma 1.\n• For a large class of symmetric properties, including all those mentioned above, if there is an estimator that uses n samples, and has an error probability 1/3, we design an estimator using O(n) samples, whose error probability is nearly exponential in n. We remark that this decay is much faster than applying the median trick. This result follows by combining McDiarmid’s inequality with Lemma 2.\nCombined, these results prove that PML plug-in estimators are sample-optimal.\nWe also introduce the notion of -approximate ML distributions, described in Definition 1. These distributions are more relaxed version of PML, hence may be more easily computed, yet they provide essentially the same performance guarantees."
  }, {
    "heading": "3. Formal Definitions and Results",
    "text": "In the past, different sophisticated estimators were used for every property in Table 1. We show that the simple plug-in estimator that uses any PML approximation p̃, has optimal performance guarantees for all these properties.\nIn the next theorem, assume n is at least the optimal sample complexity of estimating entropy, support, support coverage, and distance to uniformity (given in Table 1) respectively. Theorem 2. For all \" > c/n0.2, any plug-in exp ( pn)approximate PML p̃ satisfies,\nEntropy\nC p̃(H(p), k , \") ⇣ C⇤(H(p), k , \"),\nSupport size\nC p̃(S(p)/k, 1 k , \") ⇣ C⇤(S(p)/k, 1 k , \"),\nSupport coverage\nC p̃(S m (p)/m, , \") ⇣ C⇤(S m (p)/m, , \"),\nDistance to uniformity\nC p̃(kp uk 1 , X , \") ⇣ C⇤(kp uk1, k, \")."
  }, {
    "heading": "4. PML: Profile Maximum Likelihood",
    "text": ""
  }, {
    "heading": "4.1. Preliminaries",
    "text": "For a sequence Xn, recall that the multilplicity N x is the number of times x appears in Xn. Discarding, the labels, profile of a sequence (Orlitsky et al., 2004b) is defined below. Let n be all profiles of length-n sequences. Then,\n4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}. In particular, a profile of a length-n sequence is an unordered partition of n. Therefore, | n|, the number of profiles of length-n sequences is equal to the partition number of n. Then, by the Hardy-Ramanujan bounds on the partition number,\nFor a, b > 0, denote a . b or b & a if for some universal constant c, a/b  c. Denote a ⇣ b if both a . b and a & b.\nLemma 1 ((Hardy & Ramanujan, 1918)). | n|  exp(3 p n).\nFor a distribution p, the probability of a profile ' is defined as\np(') def =\nX\nX\nn\n:'(X\nn\n)='\np(Xn),\nthe probability of observing a sequence with profile '. Under i.i.d. sampling, p(') = P\nX\nn\n:'(X\nn\n)='\nQ\nn\ni=1\np(X i ). For example, the probability of observing a sequence with profile ' = {1, 2} is the probability of observing a sequence with one symbol appearing once, and one symbol appearing twice. A sequence with a symbol x appearing twice and y appearing once (e.g., x y x) has probability p(x)2p(y). Appropriately normalized, for any p, the probability of the profile {1, 2} is\np({1, 2})= X\n'(X\nn )={1,2}\nn\nY\ni=1\np(X i ) =\n✓\n3\n1\n◆\nX\na 6=b2X p(a)2p(b),\n(1)\nwhere the normalization factor is independent of p. The summation is a monomial symmetric polynomial in the probability values. See (Pan, 2012) for more examples."
  }, {
    "heading": "4.2. PML Estimation Scheme",
    "text": "Recall that p X n is the distribution maximizing the probability of Xn. Similarly, define (Orlitsky et al., 2004b):\np '\ndef = max\np2P p(')\nas the distribution in P that maximizes the probability of observing a sequence with profile '.\nFor example, for ' = {1, 2}. For P = k , from (1),\np ' = arg max\np2 k\nX a 6=b p(a)2p(b).\nNote that in contrast, SML only maximizes one term of this expression.\nWe give two examples from the table in (Orlitsky et al., 2004b) to distinguish between SML and PML distributions,\nand also show an instance where PML outputs distributions with a larger support than those appearing in the sample. Example 1. Let X = {a, b, . . . , z}. Suppose Xn = x y x, then the SML distribution is (2/3, 1/3). However, the distribution in that maximizes the probability of the profile '(x y x) = {1, 2} is (1/2, 1/2). Another example, illustrating the power of PML to predict new symbols is Xn = a b a c, with profile '(a b a c) = {1, 1, 2}. The SML distribution is (1/2, 1/4, 1/4), but the PML is a uniform distribution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).\nSuppose we want to estimate a symmetric property f(p) of an unknown distribution p 2 P given n independent samples. Our high level approach using PML is described below.\nInput: Class of distributions P , symmetric function f(·), sample Xn\n1. Compute p ' : argmax p2P p('(Xn)).\n2. Output f(p ' ).\nThere are a few advantages of this approach (as is true with any plug-in approach): (i) the computation of PML is agnostic to the function f at hand, (ii) there are no parameters to be tuned, (iii) techniques such as Poisson sampling or median tricks are not necessary, (iv) well motivated by the maximum-likelihood principle.\nComparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a). Our approach is perhaps closest in flavor to the plug-in estimator of (Valiant & Valiant, 2011a). Indeed, as mentioned in (Valiant, 2012), their linear-programming estimator is motivated by the question of estimating the PML. Their result was the first estimator to provide sample complexity bounds in terms of the alphabet size, and accuracy the problems of entropy and support estimation. Before we explain the differences of the two approaches, we briefly explain their approach.\nDefine, ' µ (Xn) to be the number of elements that appear µ times. For example, when Xn = a b r a c a d a b r a, ' 1 = 2,' 2 = 2, and ' 5\n= 1. (Valiant & Valiant, 2011a) design a linear program that uses SML for high values of µ, and formulate a linear program to find a distribution for which E['\nµ ]’s are close to the observed ' µ ’s. They then plug-in this estimate to estimate the property. On the other hand, our approach, by the nature of ML principle, tries to find the distribution that best explains the entire profile of the observed data, not just some partial characteristics. It therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance measures, competitive with the best possible. For example, the guarantees of the linear program approach are suboptimal in terms of the desired accuracy \". For entropy\nestimation the optimal dependence is 1 \" , whereas (Valiant & Valiant, 2011a) yields 1\n\" 2 . This is more prominent for support size and support coverage, which have optimal dependence of polylog( 1\n\" ), whereas (Valiant & Valiant, 2011a) gives a 1\n\" 2 dependence. Besides, we analyze the first method proposed for estimating symmetric properties, designed from the first principles, and show that in fact it is competitive with the optimal estimators for various problems."
  }, {
    "heading": "5. Proof Outline",
    "text": "Our arguments have two components. In Section 6 we prove a general result for the performance of plug-in estimation via maximum likelihood approaches.\nLet P be a class of distributions over Z , and f : P ! R be a function. For z 2 Z , let\np z\ndef = argmax\np2P p(z)\nbe the maximum-likelihood estimator of z in P . Upon observing z, f(p\nz ) is the ML estimator of f . In Theorem 4, we show that if there is an estimator that achieves error probability , then the ML estimator has an error probability at most |Z|. We note that variations of this result in the asymptotic statistics were studied before (see (Lehmann & Casella, 1998)). Our contribution is to use these results in the context of symmetric properties and show sample complexity bounds in the non-asymptotic regime.\nWe emphasize that, throughout this paper Z will be the set of profiles of length n, and P will be distributions induced over profiles by length-n i.i.d. samples. Therefore, we have |Z| = | n|. By Lemma 1, if there is a profile based estimator with error probability , then the PML approach will have error probability at most exp(3 p n). Such arguments were used in hypothesis testing to show the existence of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011; 2012).\nAt its face value this seems like a weak result. Our second key step is to prove that for the properties we are interested, it is possible to obtain very sharp guarantees. For example, we show that if we can estimate the entropy to an accuracy ±\" with error probability 1/3 using n samples, then we can estimate the entropy to accuracy ±2\" with error probability exp( n0.9) using only 2n samples. Using this sharp concentration, the new error probability term dominates | n|, and we obtain our results. The arguments for sharp concentration are based on modifications to existing estimators and a new analysis. Most of these results are technical and are in the appendix."
  }, {
    "heading": "6. Maximum Likelihood Property Estimation",
    "text": "We establish performance guarantees of ML property estimation in a general set-up. Recall that P is a collection of distributions over Z , and f : P ! R. Given a sample Z from an unknown p 2 P , we want to estimate f(p). The maximum likelihood approach is the following twostep procedure.\n1. Find p Z = argmax p2P p(Z).\n2. Output f(p Z ).\nWe bound the performance of this approach in the following theorem. Theorem 3. Suppose there is an estimator ˆf : Z ! R, such that for any p, and Z ⇠ p,\nPr\n⇣\nf(p) ˆf(Z) > \" ⌘ < , (2)\nthen\nPr (|f(p) f(p Z )| > 2\")  · |Z| . (3)\nProof. Consider symbols with p(z) and p(z) < separately. A distribution p with p(z) outputs z with probability at least . For (2) to hold, we must have,\nf(p) ˆf(z) < \". By the definition of ML, p z (z) p(z) , and for (2) to hold for p\nz\n, f(p z ) ˆf(z) < \". By the triangle inequality, for all such z,\n|f(p) f(p z )|  f(p) ˆf(z) + f(p z ) ˆf(z)  2\". Thus if p(z) , then PML satisfies the required guarantee with zero probability of error, and any error occurs only when p(z) < . We bound this probability as follows. When Z ⇠ p,\nPr (p(Z) < )  X\nz2Z:p(z)<\np(z) < · |Z| .\nFor some problems, it might be easier to just approximate the ML, instead of finding it exactly. We define an approximation ML as follows: Definition 1 ( -approximate ML). Let  1. For Z 2 Z , p̃ Z 2 P is a -approximate ML distribution if p̃ z\n(z) · p\nz (z). When Z is profiles of length-n, a -approximate PML is a distribution p̃\n' such that p̃ ' (') · p ' (').\nThe next result proves guarantees for any -approximate ML estimator. Theorem 4. Suppose there exists an estimator satisfying (2). For any p 2 P and Z ⇠ p, any -approximate ML p̃\nZ\nsatisfies:\nPr (|f(p) f(p̃ Z )| > 2\")  · |Z|/ .\nThe proof is very similar to the previous theorem and is presented in the Appendix B."
  }, {
    "heading": "6.1. Competitiveness of ML via Median Trick",
    "text": "Suppose for a property f(p), there is an estimator with sample complexity n that achieves an accuracy ±\" with probability of error at most 1/3. The standard method to boost the error probability is the median trick: (i) Obtain O(log(1/ )) independent estimates using O(n log(1/ )) independent samples. (ii) Output the median of these estimates. This is an \"-accurate estimator of f(p) with error probability at most . By definition, estimators are a mapping from the samples to R. However, in many applications the estimators map from a much smaller (some sufficient statistic) of the samples. Denote by Z\nn the space consisting of all sufficient statistics that the estimator uses. For example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence Z n =\nn. Using the median-trick, we get the following result.\nCorollary 1. Let ˆf : Z n ! R be an estimator of f(p) with accuracy \" and error-probability 1/3. The ML estimator achieves accuracy 2\" with probability at least 2/3 using\nmin\n⇢ n0 : n0\n20 log(3Z n 0 )\n> n samples.\nProof. Since n is the number of samples to get error probability 1/3, by the Chernoff bound, the error after n0 samples is at most exp( (n0/(20n))). Therefore, the error probability of the ML estimator for accuracy 2\" is at most exp( (n0/(20n)))Z\nn 0 , which we desire to be at most 1/3.\nFor estimators that use the profile of sequences, | n| < exp(3 p n). Plugging this in the previous result shows that the PML based approach has a sample complexity of at most O(n2). This result holds for all symmetric properties, independent of \", and the alphabet size k. For the problems mentioned earlier, something much better in possible, namely the PML approach is optimal up to constant factors."
  }, {
    "heading": "7. Sample optimality of PML",
    "text": ""
  }, {
    "heading": "7.1. Sharp Concentration for Some Properties",
    "text": "To obtain sample-optimality guarantees for PML, we need to drive the error probability down much faster than the median trick. We achieve this by using McDiarmid’s inequality stated below. Let ˆf : X ⇤ ! R. Suppose ˆf gets n independent samples Xn from an unknown distribution. Moreover, changing one of the X\nj to any X 0 j changed ˆf by\nat most c⇤. Then McDiarmid’s inequality (bounded difference inequality, (Boucheron et al., 2013)) states that,\nPr\n⇣\nˆf(Xn) E[ ˆf(Xn)] > t ⌘  2 exp ✓ 2t 2\nnc2⇤\n◆\n. (4)\nThis inequality can be used to show strong error probability bounds for many problems. We mention a simple application for estimating discrete distributions.\nExample 2. It is well known (Devroye & Lugosi, 2001) that SML requires ⇥(k/\"2) samples to estimate p in `\n1 distance with probability at least 2/3. In this case, ˆf(Xn) = P\nx\nN\nx\nn p(x) , and therefore c⇤ is at most 2/n. Using McDiarmid’s inequality, it follows that SML has an error probability of = 2 exp( k/2), while still using ⇥(k/\"2) samples.\nLet B n be the bias of an estimator ˆf(Xn) of f(p), namely B\nn\ndef = f(p) E[ ˆf(Xn)] . By the triangle inequality,\nf(p) ˆf(Xn)\n f(p) E[ ˆf(Xn)] + ˆf(Xn) E[ ˆf(Xn)]\n= B n +\nˆf(Xn) E[ ˆf(Xn)] .\nPlugging this in (4),\nPr\n⇣\nf(p) ˆf(Xn)] > t+B n\n⌘ 2 exp ✓ 2t 2\nnc2⇤\n◆\n. (5)\nWith this in hand, we need to show that c⇤ can be bounded for estimators for the properties we consider. In particular, we will show that\nLemma 2. Let ↵ > 0 be a fixed constant. For entropy, support, support coverage, and distance to uniformity there exist profile based estimators that use the optimal number of samples (given in Table 1), have bias \" and if we change any of the samples, changes by at most c · n↵\nn , where c is a positive constant.\nWe prove this lemma by proposing several modifications to the existing sample-optimal estimators. The modified estimators will preserve the sample complexity up to constant factors and also have a small c⇤. The proof details are given in the appendix.\nUsing (5) with Lemma 2,\nTheorem 5. Let n be the optimal sample complexity of estimating entropy, support, support coverage and distance to uniformity (given in table 1) and c be a large positive constant. Let \" c/n0.2, then any for any > exp ( pn), the -PML estimator estimates entropy, support, support\ncoverage, and distance to uniformity to an accuracy of 4\" with probability at least 1 exp( pn).\nProof. Let ↵ = 0.05. By Lemma 2, for each property of interest, there are estimators based on the profiles of the samples such that using near-optimal number of samples, they have bias \" and maximum change if we change any of the samples is at most c0n↵/n for some constant c0. Hence, by McDiarmid’s inequality, an accuracy of 2\" is achieved with probability at least 1 2 exp ⇣ 2\"2n1 2↵/c02 ⌘\n. Now suppose p̃ is any -approximate PML distribution. Then by Theorem 4\nPr (|f(p) f(p̃)| > 4\") < · | n|\n2 exp( 2\" 2n1 2↵/c02 + 3\np n)\n exp( pn), where in the last step we used \"2n1 2↵ & c0pn, and > exp( pn)."
  }, {
    "heading": "8. Support and Support Coverage",
    "text": "We analyze both support coverage and the support estimation via a single approach. We first start with support coverage. Recall that the goal is to estimate S\nm (p), the expected number of distinct symbols that we see after observing m samples from p. By the linearity of expectation,\nS m\n(p) = X\nx2X E[I N x (X m )>0 ] =\nX x2X (1 (1 p(x))m) .\nThe problem is closely related to the support coverage problem (Orlitsky et al., 2016), where the goal is to estimate U\nt (Xn), the number of new distinct symbols that we observe in n · t additional samples. Hence\nS m\n(p) = E \" n X\ni=1\n' i\n#\n+ E[U t ],\nwhere t = (m n)/n. We show that the modification of an estimator in (Orlitsky et al., 2016) is also near-optimal and satisfies conditions in Lemma 2. We propose to use the following estimator\nˆS m\n(p) = n X\ni=1\n' i + USGT t (Xn),\nwhere USGT t (Xn) = P n\ni=1\n' i ( t)i Pr(Z i) and Z is a Poisson random variable with mean r and t = (m n)/n.\nThe above theorem also works for any \" & 1/n0.25 ⌘ for any ⌘ > 0\nWe remark that the proof also holds for Binomial smoothed random variables as discussed in (Orlitsky et al., 2016).\nWe need to bound the maximum coefficient and the bias to apply Lemma 2. We first bound the maximum coefficient of this estimator.\nLemma 3. For all n  m/2, the maximum coefficient of ˆS m (p) is at most 1 + er(t 1).\nProof. For any i, the coefficient of ' i is 1+ ( t)i Pr(Z i). It can be upper bounded as 1 + P t\ni=0\ne r (rt) i\ni!\n= 1 +\ner(t 1).\nThe next lemma bounds the bias of the estimator.\nLemma 4. For all n  m/2, the bias of the estimator is bounded by\n|E[ ˆS m (p)] S m (p)|  2 + 2er(t 1) +min(m,S(p))e r.\nProof. As before let t = (m n)/n.\nE[ ˆS m (p)] S m (p)\n=\nn\nX\ni=1\nE[' i ] + E[USGT t (Xn)] X\nx2X (1 (1 p(x))m)\n= E[USGT t (Xn)] X\nx2X ((1 p(x))n (1 p(x))m) .\nHence by Lemma 8 and Corollary 2, in (Orlitsky et al., 2016), we get\n|E[ ˆS m (p)] S m (p)|2+2er(t 1) +min(m,S(p))e r.\nUsing the above two lemmas we prove results for both the observed support coverage and support estimator."
  }, {
    "heading": "8.1. Support Coverage Estimator",
    "text": "Recall that the quantity of interest in support coverage estimation is S\nm (p)/m, which we wish to estimate to an accuracy of \".\nProof of Lemma 2 for observed. If we choose r = log 3 \" , then by Lemma 3, the maximum coefficient of ˆS\nm\n(p)/m\nis at most 2 m e m n log\n3 \" , which for m  ↵n log(n/21/↵)\nlog(3/\") is at most n↵/m < n↵/n. Similarly, by Lemma 4,\n1 m |E[ ˆS m (p)] S m (p)|  1 m (2 + 2er(t 1) +me r)  \",\nfor all \" > 6n↵/n."
  }, {
    "heading": "8.2. Support Estimator",
    "text": "Recall that the quantity of interest in support estimation is ˜S(p), which we wish to estimate to an accuracy of \".\nProof of Lemma 2 for support. Note that we are interested in distributions with all the non zero probabilities are at least 1/k. We propose to estimate ˜S(p) using ˆS\nm (p)/k, for m = k log 3\n\" . If we choose r = log 3 \" , then by Lemma 3, the maximum coefficient of ˆS\nm (p)/k is at most 2\nk\ne m n log 3 \" , which for n k\n↵ log(k/2 1/↵ ) log\n2\n3\n\" is at most k↵/k < n↵/n.\nTo bound the bias, note that for this choice of m\n0  S(p) S m (p) = X\nx\n(1 p(x))m\n X\nx\ne mp(x)  ke log 3\" = k\" 3 .\nSimilarly, by Lemma 4,\n1 k |E[ ˆS m (p)] S(p)|\n 1 k |E[ ˆS m (p)] S m (p)|+ 1 k |S(p) S m (p)|\n 1 k (2 + 2er(t 1) + ke r) +\n\"\n3\n \",\nfor all \" > 12n↵/n."
  }, {
    "heading": "9. Discussion and Future Directions",
    "text": "We studied estimation of symmetric properties of discrete distributions using the principle of maximum likelihood, and proved optimality of this approach for a number of problems. A number of directions are of interest. We believe that the lower bound requirement on \" is perhaps an artifact of our proof technique, and that the PML based approach is indeed optimal for all ranges of \". Approximation algorithms for estimating the PML distributions would be a fruitful direction to pursue. Given our results, approximations stronger than exp( \"2n) would be very interesting. In the particular case when the desired accuracy is a constant, even an exponential approximation would be sufficient for many properties. We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we consider, and compare with the state of the art provable methods."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors thank the reviewers for valuable feedback and the NSF for support through grants CIF-1564355, CIF1619448, CRII-CIF-1657471, and a Cornell University start-up grant. Jayadev Acharya thanks Clement Canonne, Jiantao Jiao, and Pascal Vontobel for interesting discussions."
  }],
  "year": 2017,
  "references": [{
    "title": "Exact calculation of pattern probabilities",
    "authors": ["Acharya", "Jayadev", "Das", "Hirakendu", "Mohimani", "Hosein", "Orlitsky", "Alon", "Pan", "Shengjun"],
    "venue": "In ISIT,",
    "year": 2010
  }, {
    "title": "Competitive closeness testing",
    "authors": ["Acharya", "Jayadev", "Das", "Hirakendu", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Pan", "Shengjun"],
    "venue": "COLT, 19:47–68,",
    "year": 2011
  }, {
    "title": "Competitive classification and closeness testing",
    "authors": ["Acharya", "Jayadev", "Das", "Hirakendu", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Pan", "Shengjun", "Suresh", "Ananda Theertha"],
    "venue": "In COLT,",
    "year": 2012
  }, {
    "title": "Optimal probability estimation with applications to prediction and classification",
    "authors": ["Acharya", "Jayadev", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"],
    "venue": "In COLT,",
    "year": 2013
  }, {
    "title": "A competitive test for uniformity of monotone distributions",
    "authors": ["Acharya", "Jayadev", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"],
    "venue": "In AISTATS,",
    "year": 2013
  }, {
    "title": "The complexity of estimating Rényi entropy",
    "authors": ["Acharya", "Jayadev", "Orlitsky", "Alon", "Suresh", "Ananda Theertha", "Tyagi", "Himanshu"],
    "venue": "In SODA,",
    "year": 2015
  }, {
    "title": "R.a. fisher and the making of maximum likelihood 1912-1922",
    "authors": ["Aldrich", "John"],
    "venue": "Statistical Science, 12(3):162–176,",
    "year": 1997
  }, {
    "title": "Estimating a probability mass function with unknown labels",
    "authors": ["Anevski", "Dragi", "Gill", "Richard D", "Zohren", "Stefan"],
    "venue": "arXiv preprint arXiv:1312.1200,",
    "year": 2013
  }, {
    "title": "Sampling algorithms: lower bounds and applications",
    "authors": ["Bar-Yossef", "Ziv", "Kumar", "Ravi", "D. Sivakumar"],
    "venue": "In Symposium on Theory of computing,",
    "year": 2001
  }, {
    "title": "Testing that distributions are close",
    "authors": ["Batu", "Tugkan", "Fortnow", "Lance", "Rubinfeld", "Ronitt", "Smith", "Warren D", "White", "Patrick"],
    "venue": "In FOCS, pp",
    "year": 2000
  }, {
    "title": "Concentration Inequalities: A Nonasymptotic Theory of Independence",
    "authors": ["S. Boucheron", "G. Lugosi", "P. Massart"],
    "venue": "OUP Oxford,",
    "year": 2013
  }, {
    "title": "Estimation of KL divergence between large-alphabet distributions",
    "authors": ["Bu", "Yuheng", "Zou", "Shaofeng", "Liang", "Yingbin", "Veeravalli", "Venugopal V"],
    "venue": "In ISIT,",
    "year": 2016
  }, {
    "title": "Optimal bounds for estimating entropy with pmf queries",
    "authors": ["Caferov", "Cafer", "Kaya", "Barış", "ODonnell", "Ryan", "Say", "AC Cem"],
    "venue": "In International Symposium on Mathematical Foundations of Computer Science,",
    "year": 2015
  }, {
    "title": "Testing composite hypotheses, hermite polynomials and optimal estimation of a nonsmooth functional",
    "authors": ["Cai", "T Tony", "Low", "Mark G"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "A survey on distribution testing: Your data is big. but is it blue",
    "authors": ["Canonne", "Clément L"],
    "venue": "Electronic Colloquium on Computational Complexity (ECCC),",
    "year": 2015
  }, {
    "title": "Elements of information theory (2",
    "authors": ["Cover", "Thomas M", "Thomas", "Joy A"],
    "year": 2006
  }, {
    "title": "Combinatorial methods in density estimation",
    "authors": ["Devroye", "Luc", "Lugosi", "Gábor"],
    "year": 2001
  }, {
    "title": "The number of new species, and the increase in population coverage, when a sample is increased",
    "authors": ["IJ Good", "Toulmin", "GH"],
    "year": 1956
  }, {
    "title": "Asymptotic formulaæ in combinatory analysis",
    "authors": ["Hardy", "Godfrey H", "Ramanujan", "Srinivasa"],
    "venue": "Proceedings of the London Mathematical Society,",
    "year": 1918
  }, {
    "title": "Maximum likelihood estimation of functionals of discrete distributions",
    "authors": ["Jiao", "Jiantao", "Venkat", "Kartik", "Han", "Yanjun", "Weissman", "Tsachy"],
    "venue": "arXiv preprint arXiv:1406.6959,",
    "year": 2014
  }, {
    "title": "Minimax estimation of functionals of discrete distributions",
    "authors": ["Jiao", "Jiantao", "Venkat", "Kartik", "Han", "Yanjun", "Weissman", "Tsachy"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Minimax estimation of the L1 distance",
    "authors": ["Jiao", "Jiantao", "Han", "Yanjun", "Weissman", "Tsachy"],
    "venue": "In ISIT,",
    "year": 2016
  }, {
    "title": "Maximum likelihood: an introduction",
    "authors": ["Le Cam", "Lucien Marie"],
    "venue": "JSTOR,",
    "year": 1979
  }, {
    "title": "Theory of point estimation, volume 31",
    "authors": ["Lehmann", "Erich Leo", "Casella", "George"],
    "venue": "Springer Science & Business Media,",
    "year": 1998
  }, {
    "title": "Renyi entropy estimation revisited",
    "authors": ["Obremski", "Maciej", "Skorski"],
    "venue": "In APPROX,",
    "year": 2017
  }, {
    "title": "The maximum likelihood probability of skewed patterns",
    "authors": ["Orlitsky", "Alon", "Pan", "Shengjun"],
    "venue": "In ISIT,",
    "year": 2009
  }, {
    "title": "Competitive distribution estimation: Why is good-turing good",
    "authors": ["Orlitsky", "Alon", "Suresh", "Ananda Theertha"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Always good turing: Asymptotically optimal probability estimation",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana P", "Zhang", "Junan"],
    "venue": "In FOCS,",
    "year": 2003
  }, {
    "title": "Algorithms for modeling distributions over large alphabets",
    "authors": ["Orlitsky", "Alon", "S Sajama", "NP Santhanam", "K Viswanathan", "Zhang", "Junan"],
    "venue": "In ISIT,",
    "year": 2004
  }, {
    "title": "On modeling profiles instead of values",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana P", "Viswanathan", "Krishnamurthy", "Zhang", "Junan"],
    "venue": "In UAI,",
    "year": 2004
  }, {
    "title": "Universal compression of memoryless sources over unknown alphabets",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana P", "Zhang", "Junan"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2004
  }, {
    "title": "Low (size) and order in distribution modeling",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana Prasad", "Viswanathan", "Krishna", "Zhang", "Junan"],
    "year": 2004
  }, {
    "title": "Convergence of profile based estimators",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana", "Viswanathan", "Krishnamurthy", "Zhang", "Junan"],
    "venue": "In Proceedings of the 2005 IEEE International Symposium on Information Theory (ISIT),",
    "year": 2005
  }, {
    "title": "Theoretical and experimental results on modeling low probabilities",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana Prasad", "Viswanathan", "Krishna", "Zhang", "Junan"],
    "venue": "In Information Theory Workshop,",
    "year": 2006
  }, {
    "title": "Optimal prediction of the number of unseen species",
    "authors": ["Orlitsky", "Alon", "Suresh", "Ananda Theertha", "Wu", "Yihong"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "On estimating the probability multiset part i: The pattern maximum likelihood approach. Arxiv, 2017b",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana", "Viswanathan", "Krishnamurthy", "Zhang", "Junan"],
    "year": 2017
  }, {
    "title": "On estimating the probability multiset part ii: Properties of the pattern maximum likelihood estimator. Arxiv, 2017c",
    "authors": ["Orlitsky", "Alon", "Santhanam", "Narayana", "Viswanathan", "Krishnamurthy", "Zhang", "Junan"],
    "year": 2017
  }, {
    "title": "On the theory and application of pattern maximum likelihood",
    "authors": ["Pan", "Shengjun"],
    "venue": "PhD thesis, UC San Diego,",
    "year": 2012
  }, {
    "title": "The maximum likelihood probability of unique-singleton, ternary, and length-7 patterns",
    "authors": ["Pan", "Shengjun", "Acyarya", "Jayadev", "Orlitsky", "Alon"],
    "year": 2009
  }, {
    "title": "Estimation of entropy and mutual information",
    "authors": ["Paninski", "Liam"],
    "venue": "Neural computation,",
    "year": 2003
  }, {
    "title": "Strong lower bounds for approximating distribution support size and the distinct elements problem",
    "authors": ["Raskhodnikova", "Sofya", "Ron", "Dana", "Shpilka", "Amir", "Smith", "Adam"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Theory of Approximation of Functions of a Real Variable",
    "authors": ["A.F. Timan"],
    "year": 1963
  }, {
    "title": "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts",
    "authors": ["Valiant", "Gregory", "Paul"],
    "venue": "In STOC,",
    "year": 2011
  }, {
    "title": "The power of linear estimators",
    "authors": ["Valiant", "Gregory", "Paul"],
    "venue": "In FOCS, pp. 403–412",
    "year": 2011
  }, {
    "title": "Instance-by-instance optimal identity testing",
    "authors": ["Valiant", "Gregory", "Paul"],
    "venue": "Electronic Colloquium on Computational Complexity (ECCC),",
    "year": 2013
  }, {
    "title": "Algorithmic approaches to statistical questions",
    "authors": ["Valiant", "Gregory John"],
    "venue": "PhD thesis, University of California, Berkeley,",
    "year": 2012
  }, {
    "title": "Pattern maximum likelihood estimation of finite-state discrete-time markov chains",
    "authors": ["Vatedka", "Shashank", "Vontobel", "Pascal O"],
    "venue": "In ISIT,",
    "year": 2016
  }, {
    "title": "The bethe approximation of the pattern maximum likelihood distribution",
    "authors": ["Vontobel", "Pascal O"],
    "venue": "In IEEE ISIT, pp. 2012–2016,",
    "year": 2012
  }, {
    "title": "The bethe and sinkhorn approximations of the pattern maximum likelihood estimate and their connections to the valiant-valiant estimate",
    "authors": ["Vontobel", "Pascal O"],
    "venue": "In Information Theory and Applications Workshop,",
    "year": 2014
  }, {
    "title": "Chebyshev polynomials, moment matching, and optimal estimation of the unseen",
    "authors": ["Wu", "Yihong", "Yang", "Pengkun"],
    "venue": "CoRR, abs/1504.01227,",
    "year": 2015
  }, {
    "title": "Minimax rates of entropy estimation on large alphabets via best polynomial approximation",
    "authors": ["Wu", "Yihong", "Yang", "Pengkun"],
    "venue": "IEEE Trans. Information Theory,",
    "year": 2016
  }],
  "id": "SP:a515acf113ce84d87860703ae3606cb9e9d4f5e3",
  "authors": [{
    "name": "Jayadev Acharya",
    "affiliations": []
  }, {
    "name": "Hirakendu Das",
    "affiliations": []
  }, {
    "name": "Alon Orlitsky",
    "affiliations": []
  }, {
    "name": "Ananda Theertha Suresh",
    "affiliations": []
  }],
  "abstractText": "Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that a single, simple, plug-in estimator—profile maximum likelihood (PML)– is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.",
  "title": "A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions"
}