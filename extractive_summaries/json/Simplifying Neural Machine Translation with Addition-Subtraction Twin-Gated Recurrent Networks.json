{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4273–4283 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4273"
  }, {
    "heading": "1 Introduction",
    "text": "Neural machine translation (NMT), typically with an attention-based encoder-decoder framework (Bahdanau et al., 2015), has recently become the dominant approach to machine translation and already been deployed for online translation services (Wu et al., 2016). Recurrent neural networks (RNN), e.g., LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014), are widely used as the encoder and decoder for NMT. In order to alleviate the gradient\n∗Corresponding author.\nvanishing issue found in simple recurrent neural networks (SRNN) (Elman, 1990), recurrent units in LSTMs or GRUs normally introduce different gates to create shotcuts for gradient information to pass through.\nNotwithstanding the capability of these gated recurrent networks in learning long-distance dependencies, they use remarkably more matrix transformations (i.e., more parameters) than SRNN. And with many non-linear functions modeling inputs, hidden states and outputs, they are also less transparent than SRNN. These make NMT which is based on these gated RNNs suffer from not only inefficiency in training and inference due to recurrency and heavy computation in recurrent units (Vaswani et al., 2017) but also difficulty in producing interpretable models (Lee et al., 2017). These also hinder the deployment of NMT models particularly on memory- and computationlimited devices.\nIn this paper, our key interest is to simplify recurrent units in RNN-based NMT. In doing so, we want to investigate how further we can advance RNN-based NMT in terms of the number of parameters (i.e., memory consumption), running speed and interpretability. This simplification shall preserve the capability of modeling longdistance dependencies in LSTMs/GRUs and the expressive power of recurrent non-linearities in SRNN. The simplification shall also reduce computation load and physical memory consumption in recurrent units on the one hand and allow us to take a good look into the inner workings of RNNs on the other hand.\nIn order to achieve this goal, we propose an addition-subtraction twin-gated recurrent network (ATR) for NMT. In the recurrent units of ATR, we only keep the very essential weight matrices: one over the input and the other over the history (similar to SRNN). Comparing with previous\nRNN variants (e.g., LSTM or GRU), we have the smallest number of weight matrices. This will reduce the computation load of matrix multiplication. ATR also uses gates to bypass the vanishing gradient problem so as to capture long-range dependencies. Specifically, we use the addition and subtraction operations between the weighted history and input to estimate an input and forget gate respectively. These add-sub operations not only distinguish the two gates so that we do not need to have different weight matrices for them, but also make the two gates dynamically correlate to each other. Finally, we remove some non-linearities in recurrent units.\nDue to these simplifications, we can easily show that each new state in ATR is an unnormalized weighted sum of previous inputs, similar to recurrent additive networks (Lee et al., 2017). This property not only allows us to trace each state back to those inputs which contribute more but also establishes unnormalized forward self-attention between the current state and all its previous inputs. The self-attention mechanism has already proved very useful in non-recurrent NMT (Vaswani et al., 2017).\nWe build our NMT systems on the proposed ATR with a single-layer encoder and decoder. Experiments on WMT14 English-German and English-French translation tasks show that our model yields competitive results compared with GRU/LSTM-based NMT. When we integrate an orthogonal context-aware encoder (still single layer) into ATR-based NMT, our model (yielding 24.97 and 39.06 BLEU on English-German and English-French translation respectively) is even comparable to deep RNN and non-RNN NMT models which are all with multiple encoder/decoder layers. In-depth analyses demonstrate that ATR is more efficient than LSTM/GRU in terms of NMT training and decoding speed.\nWe adapt our model to other language translation and natural language processing tasks, including NIST Chinese-English translation, natural language inference and Chinese word segmentation. Our conclusions still hold on all these tasks."
  }, {
    "heading": "2 Related Work",
    "text": "The most widely used RNN models are LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014), both of which are good at handling gradient vanishing problem, a\nnotorious bottleneck of the simple RNN (Elman, 1990). The design of gates in our model follows the gate philosophy in LSTM/GRU.\nOur work is closely related to the recurrent additive network (RAN) proposed by Lee et al. (2017). They empirically demonstrate that many non-linearities commonly used in RNN transition dynamics can be removed, and that recurrent hidden states computed as purely the weighted sum of input vectors can be quite efficient in language modeling. Our work follows the same spirit of simplifying recurrent units as they do. But our proposed ATR is significantly different from RAN in three aspects. First, ATR is simpler than RAN with even fewer parameters. There are only two weight matrices in ATR while four different weight matrices in the simplest version of RAN (two for each gate in RAN). Second, since the only difference between the input and forget gate in ATR is the addition/subtraction operation between the history and input, the two gates can be learned to be highly correlated as shown in our analysis. Finally, although RAN is verified effective in language modeling, our experiments show that ATR is better than RAN in machine translation in terms of both speed and translation quality.\nTo speed up RNN models, a line of work has attempted to remove recurrent connections. For example, Bradbury et al. (2016) propose the quasirecurrent neural network (QRNN) which uses convolutional layers and a minimalist recurrent pooling function to improve parallelism. Very recently, Lei and Zhang (2017) propose a simple recurrent unit (SRU). With the cuDNN optimization, their RNN model can be trained as fast as CNNs. However, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system.\nFinally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi-\n× +\n× × tanh\nσ tanhσ σ\nct\nhtht−1\nct−1\nxt\n×\ntanhσ\nht\nxt\nht−1\nσ\n× 1-\n× +\nht\nxt\nht−1 × +\nσ×σ+\nConcatenate CopyNeural Network Layer Pointwise Operation Vector Transfer\n(a) LSTM\n× + × × tanh σ tanhσ σ\nct\nhtht−1\nct−1\nxt\n× tanhσ\nht\nxt\nht−1\nσ\n× 1-\n× +\nht\nxt\nht−1 × +\nσ-\n×σ+\nConcatenate CopyNeural Network Layer Pointwise Operation Vector Transfer\n(b) GRU\n× + × × tanh σ tanhσ σ\nct htht−1 ct−1 xt\n× tanhσ\nht\nxt\nht−1\nσ\n× 1- × +\nht\nxt\nht−1 × +\nσ×σ+\nConcatenate CopyNeural Network Layer Pointwise Operation Vector Transfer\n(c) ATR\nFigure 1: Architecture for LSTM, GRU and ATR. c∗ indicates the memory cell specific to the LSTM network. x∗ and h∗ denote the input and output hidden states respectively.\ntectures as alternatives to RNNs for neural translation. With careful configurations, their deep models achieve state-of-the-art performance on various datasets."
  }, {
    "heading": "3 Addition-Subtraction Twin-Gated Recurrent Network",
    "text": "Given a sequence x = {x1,x2,. . . ,xT }, RNN updates the hidden state ht recurrently as follows:\nht = φ(ht−1,xt) (1)\nwhere ht−1 is the previous hidden state, which is considered to store information from all previous inputs, and xt is the current input. The function φ(·) is a non-linear recurrent function, abstracting away from details in recurrent units.\nGRU can be considered as a simplified version of LSTM. In this paper, theoretically, we use GRU as our benchmark and propose a new recurrent unit to further simplify it. The GRU function is defined as follows (see Figure 1b):\nzt = σ(Wzxt +Uzht−1) (2)\nrt = σ(Wrxt +Urht−1) (3)\nh̃t = tanh(Whxt +Uh(rt ht−1)) (4) ht = zt ht−1 + (1− zt) h̃t (5)\nwhere denotes an element-wise multiplication. The reset gate rt and update gate zt enable manageable information flow from the history and the current input to the new state respectively. Despite the success of these two gates in handling gradient flow, they consume extensive matrix transformations and weight parameters.\nWe argue that many of these matrix transformations are not essential. We therefore propose an addition-subtraction twin-gated recurrent unit\n(ATR), formulated as follows (see Figure 1c):\npt = Whht−1, qt = Wxxt (6)\nit = σ(pt + qt) (7)\nft = σ(pt − qt) (8) ht = it qt + ft ht−1 (9)\nThe hidden state ht in ATR is a weighted mixture of both the current input qt and the history ht−1 controlled by an input gate it and a forget gate ft respectively. Notice that we use the transformed representation qt for the current input rather than the raw vector xt due to the potential mismatch in dimensions between ht and xt.\nSimilar to GRU, we use gates, especially the forget gate, to control the back-propagated gradient flow to make sure gradients will neither vanish nor explode. We also preserve the non-linearities of SRNN in ATR but only in the two gates.\nThere are three significant differences of ATR from GRU. Some of these differences are due to the simplifications introduced in ATR. First, we squeeze the number of weight matrices in gate calculation from four to two (see Equation (2&3) and (7&8)). In all existing gated RNNs, the inputs to gates are weighted sum of the previous hidden state and input. In order to distinguish these gates, the weight matrices over the previous hidden state and the current input should be different for different gates. The number of different weight matrices in gates is therefore 2|#gates| in previous gated RNNs. Different from them, ATR introduces different operations (i.e., addition and subtraction) between the weighted history and input to distinguish the input and forget gate. Therefore, the weight matrices over the previous state/input in the two gates can be the same in ATR. Second, we keep the very essential non-linearities, only in the two gates. In ATR, the role of qt is similar to that of h̃t in GRU (see Equation (4)). However, we completely wipe out the recurrent non-linearity\n1\nof h̃t in qt (i.e., qt = Wxxt). Lee et al. (2017) show that this non-linearity is not necessary in language modeling. We further empirically demonstrate that it is neither essential in machine translation. Third, in GRU the gates for h̃t and ht−1 are coupled and normalized to 1 while we do not explicitly associate the two gates in ATR. Instead, they can be learned to be correlated in an implicit way, as shown in the next subsection and our empirical analyis in Section 5.1."
  }, {
    "heading": "3.1 Twin-Gated Mechanism",
    "text": "Unlike GRU, we use an addition and subtraction operation over the transformed current input qt and history pt to differentiate the two gates in ATR. As the two gates have the same weights for their input components with only a single difference in the operation between the input components, they act like twins. We term the two gates in ATR as twin gates and the procedure, shown in Equation (7&8), as the twin-gated mechanism. This mechanism endows our model with the following two advantages: 1) Both addition and subtraction operations are completely linear so that fast computation can be expected; and 2) No other weight parameters are introduced for gates so that our model is more memory-compact.\nA practical question for the twin-gated mechanism is whether twin gates are really capable of dynamically weighting the input and history information. To this end, we plot the surface of onedimensional σ(x + y) − σ(x − y) in Figure 2. It is clear that both gates are highly non-linearly correlated, and that there are regions where σ(x+ y) is equal to, greater or smaller than σ(x − y). In other words, by adapting the distribution of input\nand forget gates, the twin-gated mechanism has the potential to automatically seek suitable regions in Figure 2 to control its preference between the new and past information. We argue that the input and forget gates are negatively correlated after training, and empirically show their actual correlation in Section 5.1."
  }, {
    "heading": "3.2 Computation Analysis",
    "text": "Here we provide a systematical comparison of computations in LSTM, GRU, RAN and our ATR with respect to the number of weight matrices and matrix transformations. Notice that all these units are building blocks of RNNs so that the total computational complexity and the minimum number of sequential operations required are unchanged, i.e. O(n · d2) and O(n) respectively where n is the sequence length and d is the dimensionality of hidden states. However, the actual number of matrix transformations in the unit indeed significantly affects the running speed of RNN in practice.\nWe summarize the results in Table 1. LSTM contains three different gates and a cell state, including 4 different neural layers with 8 weight matrices and transformations. GRU simplifies LSTM by removing a gate, but still involves two gates and a candidate hidden state. It includes 3 different neural layers with 6 weight matrices and transformations. RAN further simplifies GRU by removing the non-linearity in the state transition and therefore contains 4 weight matrices in its simplest version. Although our ATR also has two gates, however, there are only 2 weight matrices and transformations, accounting for only a third and a quarter of those in GRU and LSTM respectively. To the best of our knowledge, ATR has the smallest number of weight transformations in existing gated RNN units. We provide a detailed and empirical analysis on the speed in Section 5.2."
  }, {
    "heading": "3.3 Interpretability Analysis of Hidden States",
    "text": "An appealing property of the proposed ATR is its interpretability. This can be demonstrated by rolling out Equation (9) as follows:\nht = it qt + ft ht−1\n= it Wtxt + t−1∑ k=1\nik (\nt−k∏ l=1 fk+l\n) Wxxk\n≈ t∑\nk=1\ngk Wxxk\n(10)\nwhere gk can be considered as an approximate weight assigned to the k-th input. Similar to the RAN model (Lee et al., 2017), the hidden state in ATR is a component-wise weighted sum of the inputs. This not only enables ATR to build up essential dependencies between preceding inputs and the current hidden state, but also allows us to easily detect which previous words have the promising impacts on the current state. This desirable property obviously makes ATR highly interpretable.\nAdditionally, this form of weighted sum is also related to self-attention (Vaswani et al., 2017). It can be considered as a forward unnormalized selfattention where each hidden state attends to all its previous positions. As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Setup",
    "text": "We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows:\nEnglish-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set.\nEnglish-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set.\nThe used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent network as its decoder, companied with an additive attention mechanism (Bahdanau et al., 2015). We replaced the recurrent unit with our proposed ATR model. More details are given in Appendix A.1.\nWe also conducted experiments on ChineseEnglish translation, natural language inference and Chinese word segmentation. Details and experiment results are provided in Appendix A.2."
  }, {
    "heading": "4.2 Training",
    "text": "We set the maximum length of training instances to 80 words for both English-German and EnglishFrench task. We used the byte pair encoding compression algorithm (Sennrich et al., 2016) to reduce the vocabulary size as well as to deal with the issue of rich morphology. We set the vocabulary size of both source and target languages to 40K for all translation tasks. All out-of-vocabulary words were replaced with a token “unk”.\nWe used 1000 hidden units for both encoder and decoder. All word embeddings had dimensionality 620. We initialized all model parameters randomly according to a uniform distribution ranging from -0.08 to 0.08. These tunable parameters were then optimized using Adam algorithm (Kingma and Ba, 2015) with the two momentum parameters set to 0.9 and 0.999 respectively. Gradient clipping 5.0 was applied to avoid the gradient explosion problem. We trained all models with a learning rate 5e−4 and batch size 80. We decayed the learning rate with a factor of 0.5 between each training epoch. Translations were generated by a beam search algorithm that was based on loglikelihood scores normalized by sentence length. We used a beam size of 10 in all the experiments. We also applied dropout for English-German and English-French tasks on the output layer to avoid over-fitting, and the dropout rate was set to 0.2.\nTo train deep NMT models, we adopted the GNMT architecture (Wu et al., 2016). We kept all the above settings, except the dimensionality\nof word embedding and hidden state which we set to be 512."
  }, {
    "heading": "4.3 Results on English-German Translation",
    "text": "The translation results are shown in Table 2. We also provide results of several existing systems that are trained with comparable experimental settings to ours. In particular, our single model yields a detokenized BLEU score of 21.99. In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a singlelayer context-aware (CA) encoder (Zhang et al., 2017b) into our system. The ATR+CA system further reaches 22.7 BLEU, outperforming the winner system (Buck et al., 2014) by a substantial improvement of 2 BLEU points. Enhanced with the deep GNMT architecture, the GNMT+ATR system yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR. Notice that different from our system which was trained on the parallel corpus alone, the winner system used a huge mono-\nlingual text to enhance its language model.\nCompared with the existing LSTM-based (Luong et al., 2015a) deep NMT system, our shallow/deep model achieves a gain of 2.41/3.26 tokenized BLEU points respectively. Under the same training condition, our ATR outperforms RAN by a margin of 0.34 tokenized BLEU points, and achieves competitive results against its GRU/LSTM counterpart. This suggests that although our ATR is much simpler than GRU, LSTM and RAN, it still possesses strong modeling capacity.\nIn comparison to several advanced deep NMT models, such as the Google NMT (8 layers, 24.61 tokenized BLEU) (Wu et al., 2016) and the LAU-connected NMT (4 layers, 23.80 tokenized BLEU) (Wang et al., 2017a), the performance of our shallow model (23.31) is competitive. Particularly, when replacing LSTM in the Google NMT with our ATR model, the GNMT+ATR system achieves a BLEU score of 24.16, merely 0.45 BLEU points lower. Notice that although all systems use the same training data of WMT14, the\ntokenization of these work might be different from ours. However, the overall results can indicate the competitive strength of our model. In addition, SRU (Lei and Zhang, 2017), a recent proposed efficient recurrent unit, obtains a BLEU score of 20.70 with 10 layers, far more behind ATR’s.\nWe further ensemble eight likelihood-trained models with different random initializations for the ATR+CA system. The variance in the tokenized BLEU scores of these models is 0.07. As can be seen from Table 2, the ensemble system achieves a tokenized and detokenized BLEU score of 24.97 and 24.33 respectively, obtaining a gain of 1.66 and 1.63 BLEU points over the single model. The final result of the ensemble system, to the best of our knowledge, is a very promising result that can be reached by single-layer NMT systems on WMT14 English-German translation."
  }, {
    "heading": "4.4 Results on English-French Translation",
    "text": "Unlike the above translation task, the WMT14 English-French translation task provides a significant larger dataset. The full training data have approximately 36M sentence pairs, from which we only used 12M instances for experiments following previous work (Jean et al., 2015; Gehring et al., 2017a; Luong et al., 2015b; Wang et al., 2017a). We show the results in Table 3.\nOur shallow model achieves a tokenized BLEU score of 36.89 and 37.88 when it is equipped\nwith the CA encoder, outperforming almost all the listed systems, except the Google NMT (Wu et al., 2016), the ConvS2S (Gehring et al., 2017b) and the Transformer (Vaswani et al., 2017). Enhanced with the deep GNMT architecture, the GNMT+ATR system reaches a BLEU score of 38.59, which beats the base model version of the Transformer by a margin of 0.49 BLEU points. When we use four ensemble models (the variance in the tokenized BLEU scores of these ensemble models is 0.16), the ATR+CA system obtains another gain of 0.47 BLEU points, reaching a tokenized BLEU score of 39.06, which is comparable with several state-of-the-art systems."
  }, {
    "heading": "5 Analysis",
    "text": ""
  }, {
    "heading": "5.1 Analysis on Twin-Gated Mechanism",
    "text": "We provide an illustration of the actual relation between the learned input and forget gate in Figure 3. Clearly, these two gates show strong negative correlation. When the input gate opens with high values, the forget gate prefer to be close. Quantitatively, on the whole test set, the Pearson’s r of the input and forget gate is -0.9819, indicating a high correlation."
  }, {
    "heading": "5.2 Analysis on Speed and Model Parameters",
    "text": "As mentioned in Section 3.2, ATR has much fewer model parameters and matrix transformations. We\nprovide more details in this section by comparing against the following two NMT systems:\n• DeepRNNSearch (GRU): a deep GRUequipped RNNSearch model (Wu et al., 2016) with 5 layers. We set the dimension of word embedding and hidden state to 620 and 1000 respectively.\n• Transformer: a purely attentional translator (Vaswani et al., 2017). We set the dimension of word embedding and filter size to 512 and 2048 respectively. The model was trained with a minibatch size of 256.\nWe also compare with the GRU and LSTM-based RNNSearch. Without specific mention, all other experimental settings for all these models are the same as for our model. We implement all these models using the Theano library, and test the speed on one GeForce GTX TITAN X GPU card. We show the results on Table 4.\nWe observe that the Transformer achieves the best training speed, processing 4961 words per second. This is reasonable since the Transformer can be trained in full parallelization. On the contrary, DeepRNNSearch is the slowest system. As RNN performs sequentially, stacking more layers of RNNs inevitably reduces the training efficiency. However, this situation becomes the reverse when it comes to the decoding procedure. The Transformer merely generates 44 words per second while DeepRNNSearch reaches 70. This is because during decoding, all these beam search-\nbased systems must generate translation one word after another. Therefore the parallelization advantage of the Transformer disappears. In comparison to DeepRNNSearch, the Transformer spends extra time on performing self-attention over all previous hidden states.\nOur model with the CA structure, using only 63.1M parameters, processes 3993 words per second during training and generates 186 words per second during decoding, which yields substantial speed improvements over the GRU- and LSTMequipped RNNSearch. This is due to the light matrix computation in recurrent units of ATR. Notice that the speed increase of ATR over GRU and LSTM does not reach 3x. This is because at each decoding step, there are mainly two types of computation: recurrent unit and softmax layer. The latter consumes the most calculation, which, however, is the same for different models (LSTM/GRU/ATR)."
  }, {
    "heading": "5.3 Analysis on Dependency Modeling",
    "text": "As shown in Section 3.3, a hidden state in our ATR can be formulated as a weighted sum of the previous inputs. In this section, we quantitatively analyze the weights gk in Equation (10) induced from Equation (13). Inspired by Lee et al. (2017), we visualize the captured dependencies of an example in Figure 4 where we connect each word to the corresponding previous word with the highest weight gk.\nObviously, our model can discover strong local dependencies. For example, the token “unglück@@” and “lichen” should be a\nsingle word. Our model successfully associates “unglück@@” closely to the generation of “lichen” during decoding. In addition, our model can also detect non-consecutive longdistance dependencies. Particularly, the prediction of “Parteien” relies heavily on the token “unglücklichen”, which actually entails an amod linguistic dependency relationship. These captured dependencies make our model more interpretable than LSTM/GRU."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "This paper has presented a twin-gated recurrent network (ATR) to simplify neural machine translation. There are only two weight matrices and matrix transformations in recurrent units of ATR, making it efficient in physical memory usage and running speed. To avoid the gradient vanishing problem, ATR introduces a twin-gated mechanism to generate an input gate and forget gate through linear addition and subtraction operation respectively, without introducing any additional parameters. The simplifications allow ATR to produce interpretable results.\nExperiments on English-German and EnglishFrench translation tasks demonstrate the effectiveness of our model. They also show that ATR can be orthogonal to and applied with methods that improve LSTM/GRU-based NMT, indicated by the promising performance of the ATR+CA system. Further analyses reveal that ATR can be trained more efficiently than GRU. It is also able to transparently model long-distance dependencies.\nWe also adapt our ATR to other natural language processing tasks. Experiments show encouraging performance of our model on ChineseEnglish translation, natural language inference and Chinese word segmentation, demonstrating its generality and applicability on various NLP tasks.\nIn the future, we will continue to examine the effectiveness of ATR on different neural models for NMT, such as the hierarchical NMT model (Su et al., 2018b) as well as the generative NMT\nmodel (Su et al., 2018a). We are also interested in adapting our ATR to summarization, semantic parsing etc."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors were supported by National Natural Science Foundation of China (Grants No. 61672440, 61622209 and 61861130364), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49). Biao Zhang greatly acknowledges the support of the Baidu Scholarship. We also thank the reviewers for their insightful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep Neural Machine Translation with Weakly-Recurrent Units",
    "authors": ["M. Antonino", "M. Federico."],
    "venue": "ArXiv e-prints.",
    "year": 2018
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proc. of ICLR.",
    "year": 2015
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."],
    "venue": "Proc. of EMNLP. Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "A fast unified model for parsing and sentence understanding",
    "authors": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."],
    "venue": "Proc. of ACL, pages 1466–1477.",
    "year": 2016
  }, {
    "title": "Quasi-recurrent neural networks",
    "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher."],
    "venue": "CoRR, abs/1611.01576.",
    "year": 2016
  }, {
    "title": "N-gram counts and language models from the common crawl",
    "authors": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."],
    "venue": "Proc. of LREC, pages 3579– 3584, Reykjavik, Iceland.",
    "year": 2014
  }, {
    "title": "Long short-term memory neural networks for chinese word segmentation",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."],
    "venue": "Proc. of EMNLP, pages 1197–1206.",
    "year": 2015
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proc. of EMNLP, pages 551–561.",
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "CoRR.",
    "year": 2014
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L Elman."],
    "venue": "Cognitive science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "A convolutional encoder model for neural machine translation",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin."],
    "venue": "Proc. of ACL, pages 123–135.",
    "year": 2017
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"],
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Comput., 9:1735–1780.",
    "year": 1997
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proc. of ACL-IJCNLP, pages 1–10.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "Proc. of ICLR.",
    "year": 2015
  }, {
    "title": "Recurrent additive networks",
    "authors": ["Kenton Lee", "Omer Levy", "Luke Zettlemoyer."],
    "venue": "CoRR, abs/1705.07393.",
    "year": 2017
  }, {
    "title": "Training RNNs as Fast as CNNs",
    "authors": ["T. Lei", "Y. Zhang."],
    "venue": "ArXiv e-prints.",
    "year": 2017
  }, {
    "title": "Learning natural language inference using bidirectional LSTM model and inner-attention",
    "authors": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang."],
    "venue": "CoRR, abs/1605.09090.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proc. of EMNLP, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "Addressing the rare word problem in neural machine translation",
    "authors": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."],
    "venue": "Proc. of ACL-IJCNLP, pages 11–19.",
    "year": 2015
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proc. of ACL, pages 311–318.",
    "year": 2002
  }, {
    "title": "Maxmargin tensor neural network for chinese word segmentation",
    "authors": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."],
    "venue": "Proc. of ACL, pages 293–303, Baltimore, Maryland. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proc. of EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Reasoning about entailment with neural attention",
    "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Phil Blunsom."],
    "venue": "Proc. of ICLR.",
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proc. of ACL, pages 1715–1725.",
    "year": 2016
  }, {
    "title": "Minimum risk training for neural machine translation",
    "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."],
    "venue": "Proc. of ACL, pages 1683–1692, Berlin, Germany. Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "The first international chinese word segmentation bakeoff",
    "authors": ["Richard Sproat", "Thomas Emerson."],
    "venue": "Proceedings of the Second SIGHAN Workshop on Chinese Language Processing - Volume 17, SIGHAN ’03, pages 133–143.",
    "year": 2003
  }, {
    "title": "Variational recurrent neural machine translation",
    "authors": ["Jinsong Su", "Shan Wu", "Deyi Xiong", "Yaojie Lu", "Xianpei Han", "Biao Zhang."],
    "venue": "arXiv preprint arXiv:1801.05119.",
    "year": 2018
  }, {
    "title": "A hierarchyto-sequence attentional neural machine translation model",
    "authors": ["Jinsong Su", "Jiali Zeng", "Deyi Xiong", "Yang Liu", "Mingxuan Wang", "Jun Xie."],
    "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 26(3):623–632.",
    "year": 2018
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin."],
    "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
    "year": 2017
  }, {
    "title": "Order-embeddings of images and language",
    "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."],
    "venue": "CoRR, abs/1511.06361.",
    "year": 2015
  }, {
    "title": "Deep Neural Machine Translation with Linear Associative Unit",
    "authors": ["M. Wang", "Z. Lu", "J. Zhou", "Q. Liu."],
    "venue": "ArXiv e-prints.",
    "year": 2017
  }, {
    "title": "Deep neural machine translation with linear associative unit",
    "authors": ["Mingxuan Wang", "Zhengdong Lu", "Jie Zhou", "Qun Liu."],
    "venue": "Proc. of ACL, pages 136– 145, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Learning natural language inference with lstm",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "Proc. of NAACL, pages 1442–1451.",
    "year": 2016
  }, {
    "title": "Bilateral multi-perspective matching for natural language sentences",
    "authors": ["Zhiguo Wang", "Wael Hamza", "Radu Florian."],
    "venue": "CoRR, abs/1702.03814.",
    "year": 2017
  }, {
    "title": "The penn chinese treebank: Phrase structure annotation of a large corpus",
    "authors": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer."],
    "venue": "Nat. Lang. Eng., 11(2):207–238.",
    "year": 2005
  }, {
    "title": "A gru-gated attention model for neural machine translation",
    "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su."],
    "venue": "CoRR, abs/1704.08430.",
    "year": 2017
  }, {
    "title": "Accelerating neural transformer via an average attention network",
    "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su."],
    "venue": "Proc of ACL, pages 1789–1798. Association for Computational Linguistics.",
    "year": 2018
  }, {
    "title": "A context-aware recurrent encoder for neural machine translation",
    "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan."],
    "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, PP(99):1–1.",
    "year": 2017
  }, {
    "title": "Incorporating word reordering knowledge into attention-based neural machine translation",
    "authors": ["Jinchao Zhang", "Mingxuan Wang", "Qun Liu", "Jie Zhou."],
    "venue": "Proc. of ACL, pages 1524–1534, Vancouver, Canada. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Asynchronous bidirectional decoding for neural machine translation",
    "authors": ["Xiangwen Zhang", "Jinsong Su", "Yue Qin", "Yang Liu", "Rongrong Ji", "Hongji Wang."],
    "venue": "CoRR, abs/1801.05122.",
    "year": 2018
  }, {
    "title": "Deep learning for chinese word segmentation and pos tagging",
    "authors": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."],
    "venue": "Pro. of EMNLP, pages 647–657.",
    "year": 2013
  }, {
    "title": "Modeling past and future for neural machine translation",
    "authors": ["Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lili Mou", "Xinyu Dai", "Jiajun Chen", "Zhaopeng Tu."],
    "venue": "CoRR, abs/1711.09502.",
    "year": 2017
  }, {
    "title": "Deep recurrent models with fast-forward connections for neural machine translation",
    "authors": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:371–383.",
    "year": 2016
  }],
  "id": "SP:e7fa0af1fef0b219e122bbf66d792b131d0da42b",
  "authors": [{
    "name": "Biao Zhang",
    "affiliations": []
  }, {
    "name": "Deyi Xiong",
    "affiliations": []
  }, {
    "name": "Jinsong Su",
    "affiliations": []
  }, {
    "name": "Qian Lin",
    "affiliations": []
  }, {
    "name": "Huiji Zhang",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",
  "title": "Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks"
}