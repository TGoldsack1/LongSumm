{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1990–1999 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1990"
  }, {
    "heading": "1 Introduction",
    "text": "Social platforms, like Snapchat, Twitter, Instagram and Pinterest, have become part of our lives and play an important role in making communication easier and accessible. Once textcentric, social media platforms are becoming in-\n∗This work was mostly done during the first author’s internship at Snap Research.\n1The Twitter data and associated images presented in this paper were downloaded from https://archive.org/ details/twitterstream\n2We will make the annotations on Twitter data available for research purpose upon request.\ncreasingly multimodal, with users combining images, videos, audios, and texts for better expressiveness. As social media posts become more multimodal, the natural language understanding of the textual components of these messages becomes increasingly challenging. In fact, it is often the case that the textual component can only be understood in combination with the visual context of the message.\nIn this context, here we study the task of Name Tagging for social media containing both image and textual contents. Name tagging is a key task for language understanding, and provides input to several other tasks such as Question Answering, Summarization, Searching and Recommendation. Despite its importance, most of the research in name tagging has focused on news articles and longer text documents, and not as much in multimodal social media data (Baldwin et al., 2015).\nHowever, multimodality is not the only challenge to perform name tagging on such data. The textual components of these messages are often very short, which limits context around names. Moreover, there linguistic variations, slangs, typos and colloquial language are extremely common, such as using ‘looooove’ for ‘love’, ‘LosAngeles’ for ‘Los Angeles’, and ‘#Chicago #Bull’ for ‘Chicago Bulls’. These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging.\nIn this work, we modify and extend the current state-of-the-art model (Lample et al., 2016; Ma and Hovy, 2016) in name tagging to incorporate the visual information of social media posts using an Attention mechanism. Although the usually short textual components of social media posts provide limited contextual information, the accompanying images often provide rich information that can be useful for name tagging. For ex-\nample, as shown in Figure 1, both captions include the phrase ‘Modern Baseball’. It is not easy to tell if each Modern Baseball refers to a name or not from the textual evidence only. However using the associated images as reference, we can easily infer that Modern Baseball in the first sentence should be the name of a band because of the implicit features from the objects like instruments and stage, and the Modern Baseball in the second sentence refers to the sport of baseball because of the pitcher in the image.\nIn this paper, given an image-sentence pair as input, we explore a new approach to leverage visual context for name tagging in text. First, we propose an attention-based model to extract visual features from the regions in the image that are most related to the text. It can ignore irrelevant visual information. Secondly, we propose to use a gate to combine textual features extracted by a Bidirectional Long Short Term Memory (BLSTM) and extracted visual features, before feed them into a Conditional Random Fields(CRF) layer for tag predication. The proposed gate architecture plays the role to modulate word-level multimodal features.\nWe evaluate our model on two labeled datasets collected from Snapchat and Twitter respectively. Our experimental results show that the proposed model outperforms state-for-the-art name tagger in multimodal social media.\nThe main contributions of this work are as follows:\n• We create two new datasets for name tagging in multimedia data, one using Twitter and the other using crowd-sourced Snapchat posts. These new datasets effectively constitute new benchmarks for the task.\n• We propose a visual attention model specifically for name tagging in multimodal social media data. The proposed end-to-end model\nonly uses image-sentence pairs as input without any human designed features, and a Visual Attention component that helps understand the decision making of the model."
  }, {
    "heading": "2 Model",
    "text": "Figure 2 shows the overall architecture of our model. We describe three main components of our model in this section: BLSTM-CRF sequence labeling model (Section 2.1), Visual Attention Model (Section 2.3) and Modulation Gate (Section 2.4).\nGiven a pair of sentence and image as input, the Visual Attention Model extracts regional visual features from the image and computes the weighted sum of the regional visual features as the visual context vector, based on their relatedness with the sentence. The BLSTM-CRF sequence labeling model predicts the label for each word in the sentence based on both the visual context vector and the textual information of the words. The modulation gate controls the combination of the visual context vector and the word representations for each word before the CRF layer."
  }, {
    "heading": "2.1 BLSTM-CRF Sequence Labeling",
    "text": "We model name tagging as a sequence labeling problem. Given a sequence of words: S = {s1, s2, ..., sn}, we aim to predict a sequence of labels: L = {l1, l2, ..., ln}, where li ∈ L and L is a pre-defined label set. Bidirectional LSTM. Long Short-term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) are variants of Recurrent Neural Networks (RNNs) designed to capture long-range dependencies of input. The equations of a LSTM cell are as follows:\nit = σ(Wxixt +Whiht−1 + bi)\nft = σ(Wxfxt +Whfht−1 + bf )\nc̃t = tanh(Wxcxt +Whcht−1 + bc)\nct = ft ct−1 + it c̃t ot = σ(Wxoxt +Whoht−1 + bo)\nht = ot tanh(ct)\nwhere xt, ct and ht are the input, memory and hidden state at time t respectively. Wxi, Whi, Wxf , Whf , Wxc, Whc, Wxo, and Who are weight matrices. is the element-wise product function and σ is the element-wise sigmoid function.\nName Tagging benefits from both of the past (left) and the future (right) contexts, thus we implement the Bidirectional LSTM (Graves et al., 2013; Dyer et al., 2015) by concatenating the left and right context representations, ht = [ −→ ht , ←− ht ], for each word. Character-level Representation. Following (Lample et al., 2016), we generate the character-level representation for each word using another BLSTM. It receives character embeddings as input and generates representations combining implicit prefix, suffix and spelling information. The final word representation xi is the concatenation of word embedding ei and character-level representation ci.\nci = BLSTMchar(si) si ∈ S xi = [ei, ci]\nConditional random fields (CRFs). For name tagging, it is important to consider the constraints of the labels in neighborhood (e.g., I-LOC must follow B-LOC). CRFs (Lafferty et al., 2001) are effective to learn those constraints and jointly predict the best chain of labels. We follow the implementation of CRFs in (Ma and Hovy, 2016)."
  }, {
    "heading": "2.2 Visual Feature Representation",
    "text": "We use Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to obtain the representations of images. Particularly, we use Residual Net (ResNet) (He et al., 2016), which\nachieves state-of-the-art on ImageNet (Russakovsky et al., 2015) detection, ImageNet localization, COCO (Lin et al., 2014) detection, and COCO segmentation tasks. Given an input pair (S, I), where S represents the word sequence and I represents the image rescaled to 224x224 pixels, we use ResNet to extract visual features for regional areas as well as for the whole image (Fig 3):\nVg = ResNetg(I)\nVr = ResNetr(I)\nwhere the global visual vector Vg, which represents the whole image, is the output before the last fully connected layer3. The dimension of Vg is 1,024. Vr are the visual representations for regional areas and they are extracted from the last convolutional layer of ResNet, and the dimension is 1,024x7x7 as shown in Figure 3. 7x7 is the number of regions in the image and 1,024 is the\n3the last fully connect layer outputs the probabilities over 1,000 classes of objects.\ndimension of the feature vector. Thus each feature vector of Vr corresponds to a 32x32 pixel region of the rescaled input image."
  }, {
    "heading": "2.3 Visual Attention Model",
    "text": "The global visual representation is a reasonable representation of the whole input image, but not the best. Sometimes only parts of the image are related to the associated sentence. For example, the visual features from the right part of the image in Figure 4 cannot contribute to inferring the information in the associated sentence ‘I have just bought Jeremy Pied.’ In this work we utilize visual attention mechanism to combat the problem, which has been proven effective for vision-language related tasks such as Image Captioning (Xu et al., 2015) and Visual Question Answering (Yang et al., 2016b; Lu et al., 2016), by enforcing the model to focus on the regions in images that are mostly related to context textual information while ignoring irrelevant regions. Also the visualization of attention can also help us to understand the decision making of the model. Attention mechanism is mapping a query and a set of key-value pairs to an output. The output is a weighted sum of the values and the assigned weight for each value is computed by a function of the query and corresponding key. We encode the sentence into a query vector using an LSTM, and use regional visual representations Vr as both keys and values. Text Query Vector. We use an LSTM to encode the sentence into a query vector, in which the inputs of the LSTM are the concatenations of word embeddings and character-level word representations. Different from the LSTM model used for sequence labeling in Section 2.1, the LSTM here aims to get the semantic information of the sen-\ntence and it is unidirectional:\nQ = LSTMquery(S) (1)\nAttention Implementation. There are many implementations of visual attention mechanism such as Multi-layer Perceptron (Bahdanau et al., 2014), Bilinear (Luong et al., 2015), dot product (Luong et al., 2015), Scaled Dot Product (Vaswani et al., 2017), and linear projection after summation (Yang et al., 2016b). Based on our experimental results, dot product implementations usually result in more concentrated attentions and linear projection after summation results in more dispersed attentions. In the context of name tagging, we choose the implementation of linear projection after summation because it is beneficial for the model to utilize as many related visual features as possible, and concentrated attentions may make the model bias. For implementation, we first project the text query vector Q and regional visual features Vr into the same dimensions:\nPt = tanh(WtQ)\nPv = tanh(WvVr)\nthen we sum up the projected query vector with each projected regional visual vector respectively:\nA = Pt ⊕ Pv\nthe weights of the regional visual vectors:\nE = softmax(WaA+ ba)\nwhereWa is weights matrix. The weighted sum of the regional visual features is:\nvc = ∑ αivi αi ∈ E, vi ∈ Vr\nWe use vc as the visual context vector to initialize the BLSTM sequence labeling model in Section 2.1. We compare the performances of the models using global visual vector Vg and attention based visual context vector Vc for initialization in Section 4."
  }, {
    "heading": "2.4 Visual Modulation Gate",
    "text": "The BLSTM-CRF sequence labeling model benefits from using the visual context vector to initialize the LSTM cell. However, the better way to utilize visual features for sequence labeling is to incorporate the features at word level individually. However visual features contribute quite\ndifferently when they are used to infer the tags of different words. For example, we can easily find matched visual patterns from associated images for verbs such as ‘sing’, ‘run’, and ‘play’. Words/Phrases such as names of basketball players, artists, and buildings are often well-aligned with objects in images. However it is difficult to align function words such as ‘the’, ‘of ’ and ‘well’ with visual features. Fortunately, most of the challenging cases in name tagging involve nouns and verbs, the disambiguation of which can benefit more from visual features.\nWe propose to use a visual modulation gate, similar to (Miyamoto and Cho, 2016; Yang et al., 2016a), to dynamically control the combination of visual features and word representation generated by BLSTM at word-level, before feed them into the CRF layer for tag prediction. The equations for the implementation of modulation gate are as follows:\nβv = σ(Wvhi + Uvvc + bv)\nβw = σ(Wwhi + Uwvc + bw)\nm = tanh(Wmhi + Umvc + bm)\nwm = βw · hi + βv ·m\nwhere hi is the word representation generated by BLSTM, vc is the computed visual context vector, Wv, Ww, Wm, Uv, Uw and Um are weight matrices, σ is the element-wise sigmoid function, and wm is the modulated word representations fed into the CRF layer in Section 2.1. We conduct experiments to evaluate the impact of modulation gate in Section 4."
  }, {
    "heading": "3 Datasets",
    "text": "We evaluate our model on two multimodal datasets, which are collected from Twitter and Snapchat respectively. Table 1 summarizes the data statistics. Both datasets contain four types of named entities: Location, Person, Organization and Miscellaneous. Each data instance contains a pair of sentence and image, and the names in sentences are manually tagged by three expert labelers. Twitter name tagging. The Twitter name tagging dataset contains pairs of tweets and their associated images extracted from May 2016, January 2017 and June 2017. We use sports and social event related key words, such as concert, festival, soccer, basketball, as queries. We don’t take\ninto consideration messages without images for this experiment. If a tweet has more than one image associated to it, we randomly select one of the images. Snap name tagging. The Snap name tagging dataset consists of caption and image pairs exclusively extracted from snaps submitted to public and live stories. They were collected between May and July of 2017. The data contains captions submitted to multiple community curated stories like the Electric Daisy Carnival (EDC) music festival and the Golden State Warrior’s NBA parade.\nBoth Twitter and Snapchat are social media with plenty of multimodal posts, but they have obvious differences with sentence length and image styles. In Twitter, text plays a more important role, and the sentences in the Twitter dataset are much longer than those in the Snap dataset (16.0 tokens vs 8.1 tokens). The image is often more related to the content of the text and added with the purpose of illustrating or giving more context. On the other hand, as users of Snapchat use cameras to communicate, the roles of text and image are switched. Captions are often added to complement what is being portrayed by the snap. On our experiment section we will show that our proposed model outperforms baseline on both datasets.\nWe believe the Twitter dataset can be an important step towards more research in multimodal name tagging and we plan to provide it as a benchmark upon request."
  }, {
    "heading": "4 Experiment",
    "text": ""
  }, {
    "heading": "4.1 Training",
    "text": "Tokenization. To tokenize the sentences, we use the same rules as (Owoputi et al., 2013), except we separate the hashtag ‘#’ with the words after. Labeling Schema. We use the standard BIO schema (Sang and Veenstra, 1999), because we see little difference when we switch to BIOES schema (Ratinov and Roth, 2009). Word embeddings. We use the 100-dimensional GloVe4 (Pennington et al., 2014) embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. Character embeddings. As in (Lample et al., 2016), we randomly initialize the character embeddings with uniform samples. Based on experimental results, the size of the character embeddings affects little, and we set it as 50.\n4https://nlp.stanford.edu/projects/glove/\nPretrained CNNs. We use the pretrained ResNet152 (He et al., 2016) from Pytorch. Early Stopping. We use early stopping (Caruana et al., 2001; Graves et al., 2013) with a patience of 15 to prevent the model from over-fitting. Fine Tuning. The models are optimized with finetuning on both the word-embeddings and the pretrained ResNet. Optimization. The models achieve the best performance by using mini-batch stochastic gradient descent (SGD) with batch size 20 and momentum 0.9 on both datasets. We set an initial learning rate of η0 = 0.03 with decay rate of ρ = 0.01. We use a gradient clipping of 5.0 to reduce the effects of gradient exploding. Hyper-parameters. We summarize the hyperparameters in Table 2."
  }, {
    "heading": "4.2 Results",
    "text": "Table 3 shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets. BLSTM-CRF + Global Image Vector: use global image vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention: use attention based visual context vector to initialize the BLSTM-CRF. BLSTM-CRF + Visual attention + Gate: modulate word representations with visual vector.\nOur final model BLSTM-CRF + VISUAL ATTENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets. Visual features successfully play a role of validating entity types. For example, when there is a person in the image, it\nis more likely to include a person name in the associated sentence, but when there is a soccer field in the image, it is more likely to include a sports team name.\nAll the models get better scores on Twitter dataset than on Snap dataset, because the average length of the sentences in Snap dataset (8.1 tokens) is much smaller than that of Twitter dataset (16.0 tokens), which means there is much less contextual information in Snap dataset.\nAlso comparing the gains from visual features on different datasets, we find that the model benefits more from visual features on Twitter dataset, considering the much higher baseline scores on Twitter dataset. Based on our observation, users of Snapchat often post selfies with captions, which means some of the images are not strongly related to their associated captions. In contrast, users of Twitter prefer to post images to illustrate texts"
  }, {
    "heading": "4.3 Attention Visualization",
    "text": "Figure 5 shows some good examples of the attention visualization and their corresponding name tagging results. The model can successfully focus on appropriate regions when the images are well aligned with the associated sentences. Based on our observation, the multimodal contexts in posts related to sports, concerts or festival are usually better aligned with each other, therefore the visual features easily contribute to these cases. For example, the ball and shoot action in example (a) in Figure 5 indicates that the context should be related to basketball, thus the ‘Warriors’ should be the name of a sports team. A singing person with a microphone in example (b) indicates that the name of an artist or a band (‘Radiohead’) may appear in the sentence.\nThe second and the third rows in Figure 5 show some more challenging cases whose tagging results benefit from visual features. In example (d), the model pays attention to the big Apple logo, thus tags the ‘Apple’ in the sentence as an Organization name. In example (e) and (i), a small\ngroup of people indicates that it is likely to include names of bands (‘Florence and the Machine’ and ‘BTS’). And a crowd can indicate an organization (‘Warriorette’ in example (i)). A jersey shirt on the table indicates a sports team. (‘Leicester’ in example (h) can refer to both a city and a soccer club based in it.)"
  }, {
    "heading": "4.4 Error Analysis",
    "text": "Figure 6 shows some failed examples that are categorized into three types: (1) bad alignments between visual and textual information; (2) blur images; (3) wrong attention made by the model.\nName tagging greatly benefits from visual fea-\ntures when the sentences are well aligned with the associated image as we show in Section 4.3. But it is not always the case in social media. The example (a) in Figure 6 shows a failed example resulted from poor alignment between sentences and images. In this image, there are two bins standing in front of a wall, but the sentence talks about basketball players. The unrelated visual information makes the model tag ‘Cleveland’ as a Location, however it refers to the basketball team ‘Cleveland Cavaliers’.\nThe image in example (b) is blur, so the extracted visual information extracted actually introduces noise instead of additional information. The\nimage in example (c) is about a baseball pitcher, but our model pays attention to the top right corner of the image. The visual context feature computed by our model is not related to the sentence, and results in missed tagging of ‘SBU’, which is an organization name."
  }, {
    "heading": "5 Related Work",
    "text": "In this section, we summarize relevant background on previous work on name tagging and visual attention. Name Tagging. In recent years, (Chiu and Nichols, 2015; Lample et al., 2016; Ma and Hovy, 2016) proposed several neural network architectures for named tagging that outperform traditional explicit features based methods (Chieu and Ng, 2002; Florian et al., 2003; Ando and Zhang, 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). They all use Bidirectional LSTM (BLSTM) to extract features from a sequence of words. For characterlevel representations, (Lample et al., 2016) proposed to use another BLSTM to capture prefix and suffix information of words, and (Chiu and Nichols, 2015; Ma and Hovy, 2016) used CNN to extract position-independent character features. On top of BLSTM, (Chiu and Nichols, 2015) used a softmax layer to predict the label for each word, and (Lample et al., 2016; Ma and Hovy, 2016) used a CRF layer for joint prediction. Compared with traditional approaches, neural networks based approaches do not require hand-crafted features and achieved state-of-the-art performance on name tagging (Ma and Hovy, 2016). However, these methods were mainly developed for newswire and paid little attention to social media. For name tagging in social media, (Ritter et al., 2011) leveraged a large amount of unlabeled data and many dictionaries into a pipeline model. (Limsopatham and Collier, 2016) adapted the BLSTM-CRF model with additional word\nshape information, and (Aguilar et al., 2017) utilized an effective multi-task approach. Among these methods, our model is most similar to (Lample et al., 2016), but we designed a new visual attention component and a modulation control gate. Visual Attention. Since the attention mechanism was proposed by (Bahdanau et al., 2014), it has been widely adopted to language and vision related tasks, such as Image Captioning and Visual Question Answering (VQA), by retrieving the visual features most related to text context (Zhu et al., 2016; Anderson et al., 2017; Xu and Saenko, 2016; Chen et al., 2015). (Xu et al., 2015) proposed to predict a word based on the visual patch that is most related to the last predicted word for image captioning. (Yang et al., 2016b; Lu et al., 2016) applied attention mechanism for VQA, to find the regions in images that are most related to the questions. (Yu et al., 2016) applied the visual attention mechanism on video captioning. Our attention implementation approach in this work is similar to those used for VQA. The model finds the regions in images that are most related to the accompanying sentences, and then feed the visual features into an BLSTM-CRF sequence labeling model. The differences are: (1) we add visual context feature at each step of sequence labeling; and (2) we propose to use a gate to control the combination of the visual information and textual information based on their relatedness. 2"
  }, {
    "heading": "6 Conclusions and Future Work",
    "text": "We propose a gated Visual Attention for name tagging in multimodal social media. We construct two multimodal datasets from Twitter and Snapchat. Experiments show an absolute 3%-4% F-score gain. We hope this work will encourage more research on multimodal social media in the future and we plan on making our benchmark available upon request.\nName Tagging for more fine-grained types (e.g.\nsoccer team, basketball team, politician, artist) can benefit more from visual features. For example, an image including a pitcher indicates that the ‘Giants’ in context should refer to the baseball team ‘San Francisco Giants’. We plan to expand our model to tasks such as fine-grained Name Tagging or Entity Liking in the future."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was partially supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on."
  }],
  "year": 2018,
  "references": [{
    "title": "A multi-task approach for named entity recognition in social media data",
    "authors": ["Gustavo Aguilar", "Suraj Maharjan", "Adrian Pastor López Monroy", "Thamar Solorio."],
    "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text.",
    "year": 2017
  }, {
    "title": "Bottom-up and top-down attention for image captioning and vqa",
    "authors": ["Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang."],
    "venue": "arXiv preprint arXiv:1707.07998.",
    "year": 2017
  }, {
    "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
    "authors": ["Rie Kubota Ando", "Tong Zhang."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2005
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 2015 International Conference on Learning Representations.",
    "year": 2014
  }, {
    "title": "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition",
    "authors": ["Timothy Baldwin", "Marie-Catherine de Marneffe", "Bo Han", "Young-Bum Kim", "Alan Ritter", "Wei Xu."],
    "venue": "Proceedings",
    "year": 2015
  }, {
    "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
    "authors": ["Rich Caruana", "Steve Lawrence", "C Lee Giles."],
    "venue": "Proceedings of the 2001 Advances in Neural Information Processing Systems.",
    "year": 2001
  }, {
    "title": "Abccnn: An attention based convolutional neural network for visual question answering",
    "authors": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia."],
    "venue": "arXiv preprint arXiv:1511.05960.",
    "year": 2015
  }, {
    "title": "Named entity recognition: a maximum entropy approach using global information",
    "authors": ["Hai Leong Chieu", "Hwee Tou Ng."],
    "venue": "Proceedings of the 19th international conference on Computational Linguistics.",
    "year": 2002
  }, {
    "title": "Named entity recognition with bidirectional lstm-cnns",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "Transactions of the Association of Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Named entity recognition through classifier combination",
    "authors": ["Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang."],
    "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL.",
    "year": 2003
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."],
    "venue": "Proceedings of the 2013 IEEE international conference on acoustics, speech and signal processing.",
    "year": 2013
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation.",
    "year": 1997
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning.",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Backpropagation applied to handwritten zip code recognition",
    "authors": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel."],
    "venue": "Neural computation.",
    "year": 1989
  }, {
    "title": "Bidirectional lstm for named entity recognition in twitter messages",
    "authors": ["Nut Limsopatham", "Nigel Henry Collier."],
    "venue": "Proceedings of the 2nd Workshop on Noisy User-generated Text.",
    "year": 2016
  }, {
    "title": "Phrase clustering for discriminative learning",
    "authors": ["Dekang Lin", "Xiaoyun Wu."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.",
    "year": 2009
  }, {
    "title": "Microsoft coco: Common objects in context",
    "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick."],
    "venue": "Proceedings of the 2014 European Conference on Computer Vision.",
    "year": 2014
  }, {
    "title": "Hierarchical question-image coattention for visual question answering",
    "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."],
    "venue": "Proceedings of the 2016 Advances In Neural Information Processing Systems.",
    "year": 2016
  }, {
    "title": "Joint entity recognition and disambiguation",
    "authors": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Gated word-character recurrent language model",
    "authors": ["Yasumasa Miyamoto", "Kyunghyun Cho."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Improved part-of-speech tagging for online conversational text with word clusters",
    "authors": ["Olutobi Owoputi", "Brendan O’Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A Smith"],
    "venue": "Proceedings of the 2013 Conference of the North",
    "year": 2013
  }, {
    "title": "Lexicon infused phrase embeddings for named entity resolution",
    "authors": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."],
    "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning.",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing.",
    "year": 2014
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["Lev Ratinov", "Dan Roth."],
    "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning.",
    "year": 2009
  }, {
    "title": "Named entity recognition in tweets: an experimental study",
    "authors": ["Alan Ritter", "Sam Clark", "Oren Etzioni"],
    "venue": "In Proceedings of the conference on Empirical Methods in Natural Language Processing",
    "year": 2011
  }, {
    "title": "Imagenet large scale visual recognition challenge",
    "authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"],
    "year": 2015
  }, {
    "title": "Representing text chunks",
    "authors": ["Erik F Sang", "Jorn Veenstra."],
    "venue": "Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics.",
    "year": 1999
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "Proceedings of the 2017 Advances in Neural Information Processing Systems.",
    "year": 2017
  }, {
    "title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
    "authors": ["Huijuan Xu", "Kate Saenko."],
    "venue": "Proceedings of the 2016 European Conference on Computer Vision.",
    "year": 2016
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "Proceedings of the 2015 International Con-",
    "year": 2015
  }, {
    "title": "Words or characters? fine-grained gating for reading comprehension",
    "authors": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W Cohen", "Ruslan Salakhutdinov."],
    "venue": "Proceedings of the 2016 International Conference on Learning Representa-",
    "year": 2016
  }, {
    "title": "Stacked attention networks for image question answering",
    "authors": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
    "year": 2016
  }, {
    "title": "Video paragraph captioning using hierarchical recurrent neural networks",
    "authors": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition.",
    "year": 2016
  }, {
    "title": "Visual7w: Grounded question answering in images",
    "authors": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li FeiFei."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
    "year": 2016
  }],
  "id": "SP:df7f6ec612722399458203ed27e6ca57ee31297e",
  "authors": [{
    "name": "Di Lu",
    "affiliations": []
  }, {
    "name": "Leonardo Neves",
    "affiliations": []
  }, {
    "name": "Vitor Carvalho",
    "affiliations": []
  }, {
    "name": "Ning Zhang",
    "affiliations": []
  }, {
    "name": "Heng Ji",
    "affiliations": []
  }],
  "abstractText": "Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: one based on Twitter posts1 and the other based on Snapchat captions (exclusively submitted to public and crowdsourced stories). We then propose a novel model based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-theart baseline methods for this task. 2",
  "title": "Visual Attention Model for Name Tagging in Multimodal Social Media"
}