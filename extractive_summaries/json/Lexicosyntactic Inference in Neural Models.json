{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4717–4724 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4717"
  }, {
    "heading": "1 Introduction",
    "text": "The formal semantics literature has long been concerned with the complex array of inferences that different open class lexical items trigger (Kiparsky and Kiparsky, 1970; Karttunen, 1971a,b; Horn, 1972; Karttunen and Peters, 1979; Heim, 1992; Simons, 2001, 2007; Simons et al., 2010; Abusch, 2002, 2010; Gajewski, 2007; Anand and Hacquard, 2013, 2014). For example, why does (1a) give rise to the inference (2a), while the structurally identical (1b) triggers the inference (2b)? (1) a. Jo doesn’t believe that Bo left.\nb. Jo doesn’t know that Bo left. (2) a. Jo believes that Bo didn’t leave.\nb. Bo left. c. Bo didn’t leave.\nA major finding of this literature is that lexically triggered inferences are conditioned by surprising aspects of the syntactic context that a word occurs in. For example, while (3a), (3b), and (4a) trigger the inference (2b), (4b) triggers the inference (2c). (3) a. Jo remembered that Bo left.\nb. Jo didn’t remember that Bo left. (4) a. Bo remembered to leave.\nb. Bo didn’t remember to leave.\nAccurately capturing such interactions – e.g. between clause-embedding verbs, negation, and embedded clause type – is important for any system that aims to do general natural language inference (MacCartney et al. 2008 et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives).\nIn this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saurı́ and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets. Probing these particular systems is useful for understanding neural systems’ behavior more generally because (i) the best performing neural models for EFP (Rudinger et al., 2018) are simple instances of common baseline models; and (ii) the task itself is relatively constrained.\nTo do this, we substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018) to cover all English clause-embedding verbs in a variety of the syntactic contexts covered by recent psycholinguistic work (White and Rawlins, 2016), and we use the resulting dataset – MegaVeridicality2 – to probe these models’ behavior. We focus on clause-embedding verbs because they show effectively every possible patterning of lexicosyntactic inference (Karttunen, 2012).\nWe discuss three findings: (i) Tree biLSTMs (TbiLSTMs) are better able to correctly predict lexicosyntactic inferences than linear-chain biLSTMs\n(L-biLSTMs); (ii) L-biLSTMs and T-biLSTMs capture different lexicosyntactic inferences, and thus ensembling their predictions can reliably improve performance; and (iii) even when ensembled, these models show systematic errors – e.g. performing well when the polarity of the matrix clause matches the polarity of the true inference, but poorly when these polarities mismatch.\nWe furthermore release MegaVeridicality2 at MegaAttitude.io as a benchmark for probing the ability of neural systems – whether for factuality prediction or for general natural language inference – to capture lexicosyntactic inference."
  }, {
    "heading": "2 Data collection",
    "text": "We substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018), which contains factuality judgments for all English clauseembedding verbs that take tensed subordinate clauses. In White and Rawlins’s annotation protocol, all verbs that are grammatical with such subordinate clauses – based on the MegaAttitude dataset (White and Rawlins, 2016) – are slotted into contexts either like (5a) or (5b), depending on whether they take a direct object or not. (5) a. Someone {knew, didn’t know} that a par-\nticular thing happened. b. Someone {was, wasn’t} told that a particu-\nlar thing happened. For each sentence generated in this way, 10 different annotators are asked to answer the question did that thing happen?: yes, maybe or maybe not, no.\nThere are two important aspects of these contexts to note. First, all lexical items besides the embedding verbs are semantically bleached to ensure that the measured lexicosyntactic inferences are only due to interactions between the embedding predicate – e.g. know or tell – and the syntactic context. Second, the matrix polarity – i.e. the presence or absence of not as a direct dependent of the embedding verb – is manipulated to create two sentences for each verb-context pair.\nOur extension, MegaVeridicality2, includes judgments for a variety of infinitival subordinate clause types, exemplified in (6).1 We investigate infinitival clauses because they can give rise to dif-\n1We also explicitly manipulate two aspects of the subordinate clause in our extension of the MegaVeridicality dataset: (i) how NP embedded subjects are introduced; and (ii) whether the embedded clause contains an eventive predicate (do, happen) or a stative predicate (have). See Appendix A for details on the reasoning behind these manipulations.\nferent lexicosyntactic inferences than finite subordinate clauses – e.g. compare (3) and (4). (6) a. Someone {needed, didn’t need} for a par-\nticular thing to happen. b. Someone {wanted, didn’t want} a particu-\nlar person to do, have a particular thing. c. Someone {wanted, didn’t want} a particu-\nlar person to have a particular thing. d. A particular person {was, wasn’t} over-\njoyed to do a particular thing. e. A particular person {was, wasn’t} over-\njoyed to have a particular thing. f. A particular person {managed, didn’t man-\nage} to do a particular thing. g. A particular person {managed, didn’t man-\nage} to have a particular thing. For each sentence, we also collect judgments from 10 different annotators, using the same question as White and Rawlins for context (6a) and modified questions for contexts (6b)-(6g): did that person do that thing? for (6b), (6d), and (6f); and did that person have that thing? for for (6c), (6e), and (6g). Table 1 shows the number of verb types for each syntactic context. With the polarity manipulation, this yields a total of 3,938 sentences.\nTo build a factuality prediction test set from these sentences, we combine MegaVeridicality1 with our dataset and replace each instance of a particular person or a particular thing with someone or something (respectively). Then, following White and Rawlins, we normalize the 10 responses for each sentence to a single real value using an ordinal mixed model-based procedure. We refer to the resulting dataset as MegaVeridicality2."
  }, {
    "heading": "3 Model and evaluation",
    "text": "We use MegaVeridicality2 to evaluate the performance of three state-of-the-art neural models of\nevent factuality (Rudinger et al., 2018): a linearchain biLSTM (L-biLSTM), a dependency tree biLSTM (T-biLSTM), and a hybrid biLSTM (HbiLSTM) that ensembles the two. To predict the factuality of the event referred to by a particular predicate, these models pass the output state of the biLSTM at that predicate through a two-layer regression. In the case of the H-biLSTM, the output state of both the L- and T-biLSTMs are simply concatenated and passed through the regression.2\nFollowing the multi-task training regime described by Rudinger et al. (2018), we train these models on four standard factuality datasets – FactBank (Saurı́ and Pustejovsky, 2009, 2012), UW (Lee et al., 2015), MEANTIME (Minard et al., 2016), and UDS (White et al., 2016; Rudinger et al., 2018) – with tied biLSTM weights but regression parameters specific to each dataset. We then use these trained models to predict the factuality of the embedded predicate in our dataset.\nTo understand how much of these models’ performance on our dataset is really due to a correct computation of lexicosyntactic inferences, we also generate predictions for the sentences in our dataset with the embedding verbs UNKed. In this case, the model can rely only on the syntactic context surrounding the predicate to make its inferences. We refer to the models with lexical information as the LEX models and the ones without lexical information as the UNK models.\nEach model produces four predictions, corresponding to the four different datasets it was trained on. We consider three different ways of ensembling these predictions using a cross-validated ridge regression: (i) ensembling the four predictions for each specific model (LEX or UNK); (ii) ensembling the predictions for the LEX version of a particular model with the UNK version of that same model (LEX+UNK); and (iii) ensembling the predictions across all models (LEX, UNK, or LEX+UNK). Each ensemble is evaluated in a 10- fold/10-fold nested cross-validation (see Cawley and Talbot, 2010). In each iteration of the outer cross-validation, a 10% test set is split off, and a 10-fold cross-validation to tune the regularization is conducted on the remaining 90%."
  }, {
    "heading": "4 Results",
    "text": "Figure 1 shows the mean correlation between model predictions and true factuality on the outer\n2See Appendix B for further details.\nfold test sets of the nested cross-validation described in §3. We note three aspects of this plot.\nFirst, among the LEX models, the T-biLSTM performs best, followed by the L-biLSTM, then the H-biLSTM. This is somewhat surprising, since Rudinger et al. find the opposite pattern of performance: the L- and H-biLSTMs vie for dominance, both outperforming the T-biLSTM. This indicates that T-biLSTMs are better able to represent the lexicosyntactic inferences relevant to this dataset, even though they underperform on more general datasets. This possibility is bolstered by the fact that, in contrast to the L- and H-biLSTMs, the LEX version of the T-biLSTMs performs significantly better than the UNK version, suggesting that the T-biLSTM is potentially more reliant on the lexical information than the other two.\nSecond, when the LEX and UNK version of each model is ensembled (LEX+UNK), we find comparable performance for all three biLSTMs – each outperforming the LEX version of the TbiLSTM. This indicates that each model captures similar amounts of information about lexicosyntactic inference, but this information is captured in the models’ parameterizations in different ways.\nFinally, when all three models are ensembled, we find that both the LEX and UNK version perform significantly better than any specific LEX+UNK model. This may indicate two things: (i) the models that only have access to syntax can perform just as well as ones that have access to both lexical information and syntax; but (ii) these models appear to capture different aspects of inference, since an ensemble of all models (AllLEX+UNK) performs significantly better than ei-\nther the All-LEX or All-UNK ensembles alone. Interestingly, however, even this ensemble performs more than 10 points worse than each model alone on FactBank, UW, and UDS. This raises the question of which lexicosyntactic inferences these models are missing – investigated below."
  }, {
    "heading": "5 Analysis",
    "text": "We investigate two questions: (i) which inferences do all models do poorly on?; and (ii) what drives the differing strengths of each model? Where do all models fail? Table 2 shows the 20 sentences with the highest prediction errors under the All-LEX+UNK ensemble. There are two interesting things to note about these sentences. First, most of them involve negative lexicosyntactic inferences that the model predicts to be either positive or near zero. Second, when the true inference is not positive, the matrix polarity of the original sentence is negative. This suggests that the models are not able to capture inferences whose polarity mismatches the matrix clause polarity.\nOne question that arises here is whether this inability affects all contexts equally. To answer this, we regress the absolute error of the predictions from this same ensemble (logged and standardized) against true factuality, matrix polarity, and context (as well as all of their two- and three-way interactions).3 We find that the three-way interactions in this regression are reliable ( 2(8)=27.97, p < 0.001) – suggesting that there are nontrivial differences in these state-of-the-art factuality systems’ ability to capture inferential interactions across verbs and syntactic contexts. The differences can be verified visually in Figure 2, which\n3See Appendix C for further details, including a summary of the regression on which the above discussion is based."
  }, {
    "heading": "NP _ed that S NP was _ed that S NP _ed for NP to VP",
    "text": "plots the factuality predicted by this ensemble against the true factuality from MegaVeridicality2.\nTo elaborate, the ensemble does best overall on contexts like (7a) and (7b), and worst overall on contexts like (7c). The contrast between (7b) and (7c) is particularly interesting because (i) (7c) is just the passivized form of (7b); and (ii) we do not observe similar behavior for contexts (7d) and (7e), which are analogous to (7b) and (7c), but replace the stative have with the eventive do. (7) Someone...\na. { ed, didn’t } for something to happen. b. { ed, didn’t } someone to have something. c. {was ed, wasn’t ed} to have something. d. { ed, didn’t } someone to do something. e. {was ed, wasn’t ed} to do something. f. { ed, didn’t } that something happened.\nAn additional nuance is that the ensemble does reliably better on the negative matrix polarity version of (7b) than on the positive, with the opposite true for (7e). This suggests these models do not capture an important inferential interaction between passivization and eventivity.\nThis suggestion is further bolstered by the fact that the ensemble’s ability to predict cases where the matrix polarity mismatches the true factuality are reliably poorer in context (7c) but not in its minimal pairs (7e) and (7b), where the ensemble performs reliably poorer when the two match. Indeed, it is contexts (7c) and (7f) that drive the polarity mismatch effect evident in Table 2. What drives differences between models? In §4, we noted two ways that the biLSTMs we in-\nvestigate differ: (i) the T-biLSTM appears to be more reliant on lexical information than L- and HbiLSTMs; and (ii) each model appears to encode information about lexicosyntactic inference in its parameterizations in different ways. We hypothesize that these two differences are related – specifically, that the T-biLSTM’s heavier reliance on lexical information comes about as a consequence of stronger entanglement between lexical and syntactic information in its hidden states.\nTo probe this, we ask to what extent the embedding verb’s embedding can be recovered from the embedded verb’s hidden state using linear functions. If the lexical information is more strongly entangled with the syntactic information, it should be more difficult to construct a homomorphic (linear) function to decode the embedding verb’s embedding from the embedded verb’s hidden state. To measure this, we conduct a Canonical Correlation Analysis (CCA; Hotelling, 1936) between these two vector space representations for every sentence in our dataset. Given two matrices X (the embedding verb embeddings column stacked) and Y (the embedded verb hidden states column stacked), CCA constructs matrices A and B, such that ai,bi = arga0,b0max corr(a0X,b0Y) and corr(aiX,ajX) = corr(biY,bjY) = 0, 8i < j. This guarantees that the canonical correlation at component i, corr(aiX,biY), is nonincreasing in i, and thus the linearly decodable information about Y in X can be assessed using this function.\nFigure 3 plots the canonical correlations for the first 50 components for each of the biLSTMs we investigated. We find that the canonical correlations associated with the T-biLSTM are substantially lower than those associated with the Land H-biLSTMs across these first 50 components. This suggests that the T-biLSTM more strongly entangles lexical and syntactic information, per-\nhaps explaining its apparently heavier reliance on lexical information, observed in §4.\nOf note here is that the pattern seen in Figure 3 is probably at least partly a consequence of the different nonlinearities used for the L-biLSTM (tanh) and T-biLSTM (ReLU), and not the architectures themselves. But whether or not this pattern is due to the architectures, nonlinearities, or both, the entanglement hypothesis may still help explain the pattern of results discussed in §4."
  }, {
    "heading": "6 Related work",
    "text": "This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996). The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018). This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018)."
  }, {
    "heading": "7 Conclusion",
    "text": "We investigated neural models’ ability to capture lexicosyntactic inference, taking the task of event factuality prediction (EFP) as a case study. We built a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts and used this dataset to probe current stateof-the-art EFP systems. We showed that these systems make certain systematic errors that are clearly visible through the lens of factuality."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was supported by the JHU HLTCOE, DARPA LORELEI and AIDA, NSF-BCS (1748969/1749025), and NSF-GRFP (1232825). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government."
  }],
  "year": 2018,
  "references": [{
    "title": "Lexical alternatives as a source of pragmatic presuppositions",
    "authors": ["Dorit Abusch."],
    "venue": "Semantics and Linguistic Theory, 12:1–19.",
    "year": 2002
  }, {
    "title": "Presupposition triggering from alternatives",
    "authors": ["Dorit Abusch."],
    "venue": "Journal of Semantics, 27(1):37–80.",
    "year": 2010
  }, {
    "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks",
    "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "Epistemics and attitudes",
    "authors": ["Pranav Anand", "Valentine Hacquard."],
    "venue": "Semantics and Pragmatics, 6(8):1–59.",
    "year": 2013
  }, {
    "title": "Factivity, belief and discourse",
    "authors": ["Pranav Anand", "Valentine Hacquard."],
    "venue": "Luka Crnič and Uli Sauerland, editors, The Art and Craft of Semantics: A Festschrift for Irene Heim, volume 1, pages 69–",
    "year": 2014
  }, {
    "title": "The implications of managing",
    "authors": ["Rebekah Baglini", "Itamar Francez."],
    "venue": "Journal of Semantics, 33(3):541–560.",
    "year": 2016
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2015
  }, {
    "title": "A fast unified model for parsing and sentence understanding",
    "authors": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts."],
    "venue": "Proceedings of the 54th Annual Meeting of the Associa-",
    "year": 2016
  }, {
    "title": "Selection and the Categorial Status of Infinitival Complements",
    "authors": ["Željko Bošković."],
    "venue": "Natural Language & Linguistic Theory, 14(2):269–304.",
    "year": 1996
  }, {
    "title": "The syntax of nonfinite complementation: An economy approach",
    "authors": ["Željko Bošković."],
    "venue": "32. MIT Press, Cambridge, MA.",
    "year": 1997
  }, {
    "title": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation",
    "authors": ["Gavin C. Cawley", "Nicola L.C. Talbot."],
    "venue": "J. Mach. Learn. Res., 11:2079–2107.",
    "year": 2010
  }, {
    "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations",
    "authors": ["Alexis Conneau", "Douwe Kiela."],
    "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
    "year": 2018
  }, {
    "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes"],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods",
    "year": 2017
  }, {
    "title": "What you can cram into a single \\$&!#* vector: Probing sentence embeddings for linguistic properties",
    "authors": ["Alexis Conneau", "Germán Kruszewski", "Guillaume Lample", "Loı̈c Barrault", "Marco Baroni"],
    "venue": "Proceedings of the 56th Annual Meeting",
    "year": 2018
  }, {
    "title": "Using the Framework",
    "authors": ["Robin Cooper", "Dick Crouch", "Jan Van Eijck", "Chris Fox", "Josef Van Genabith", "Jan Jaspars", "Hans Kamp", "David Milward", "Manfred Pinkal", "Massimo Poesio", "Steve Pulman", "Ted Briscoe", "Holger Maier", "Karsten Konrad."],
    "venue": "Technical Re-",
    "year": 1996
  }, {
    "title": "The Pascal Recognising Textual Entailment Challenge",
    "authors": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."],
    "venue": "Machine learning challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising textual Entailment, pages",
    "year": 2006
  }, {
    "title": "Evaluating Compositionality in Sentence Embeddings",
    "authors": ["Ishita Dasgupta", "Demi Guo", "Andreas Stuhlmüller", "Samuel J. Gershman", "Noah D. Goodman."],
    "venue": "arXiv:1802.04302.",
    "year": 2018
  }, {
    "title": "Neg-raising and polarity",
    "authors": ["Jon Robert Gajewski."],
    "venue": "Linguistics and Philosophy, 30(3):289–328.",
    "year": 2007
  }, {
    "title": "Control and Restructuring at the Syntax-Semantics Interface",
    "authors": ["Thomas Angelo Grano."],
    "venue": "Ph.D. thesis, University of Chicago.",
    "year": 2012
  }, {
    "title": "Hybrid speech recognition with deep bidirectional LSTM",
    "authors": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed."],
    "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273–278. IEEE.",
    "year": 2013
  }, {
    "title": "Message Understanding Conference-6: A Brief History",
    "authors": ["Ralph Grishman", "Beth Sundheim."],
    "venue": "Proceedings of the 16th Conference on Computational Linguistics - Volume 1, COLING ’96, pages 466–471, Stroudsburg, PA, USA. Association for",
    "year": 1996
  }, {
    "title": "Annotation Artifacts in Natural Language Inference Data",
    "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R. Bowman", "Noah A. Smith."],
    "venue": "arXiv:1803.02324.",
    "year": 2018
  }, {
    "title": "Presupposition projection and the semantics of attitude verbs",
    "authors": ["Irene Heim."],
    "venue": "Journal of Semantics, 9(3):183–221.",
    "year": 1992
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "On the Semantic Properties of Logical Operators in English",
    "authors": ["Laurence Robert Horn."],
    "venue": "Ph.D. thesis, UCLA.",
    "year": 1972
  }, {
    "title": "Relations between two sets of variates",
    "authors": ["Harold Hotelling."],
    "venue": "Biometrika, 28(3/4):321–377.",
    "year": 1936
  }, {
    "title": "A neural network for factoid question answering over paragraphs",
    "authors": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daumé III."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    "year": 2014
  }, {
    "title": "Implicative verbs",
    "authors": ["Lauri Karttunen."],
    "venue": "Language, pages 340–358.",
    "year": 1971
  }, {
    "title": "Some observations on factivity",
    "authors": ["Lauri Karttunen."],
    "venue": "Papers in Linguistics, 4(1):55–69.",
    "year": 1971
  }, {
    "title": "Simple and phrasal implicatives",
    "authors": ["Lauri Karttunen."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 124–131. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "You will be lucky to break even",
    "authors": ["Lauri Karttunen."],
    "venue": "Tracy Holloway King and Valeria dePaiva, editors, From Quirky Case to Representing Space: Papers in Honor of Annie Zaenen, pages 167–180.",
    "year": 2013
  }, {
    "title": "Conventional implicature",
    "authors": ["Lauri Karttunen", "Stanley Peters."],
    "venue": "Syntax and Semantics, 11:1–56.",
    "year": 1979
  }, {
    "title": "The Chameleon-like Nature of Evaluative Adjectives",
    "authors": ["Lauri Karttunen", "Stanley Peters", "Annie Zaenen", "Cleo Condoravdi."],
    "venue": "Empirical Issues in Syntax and Semantics 10, pages 233–250. CSSPCNRS.",
    "year": 2014
  }, {
    "title": "Fact",
    "authors": ["Paul Kiparsky", "Carol Kiparsky."],
    "venue": "Manfred Bierwisch and Karl Erich Heidolph, editors, Progress in Linguistics: A collection of papers, pages 143–173. Mouton, The Hague.",
    "year": 1970
  }, {
    "title": "Event Detection and Factuality Assessment with Non-Expert Supervision",
    "authors": ["Kenton Lee", "Yoav Artzi", "Yejin Choi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–",
    "year": 2015
  }, {
    "title": "The Accommodation Potential of Implicative Verbs",
    "authors": ["Noor van Leusen."],
    "venue": "Logic, Language and Meaning, volume 7218 of Lecture Notes in Computer Science, pages 421–430. Springer.",
    "year": 2012
  }, {
    "title": "A phrase-based alignment model for natural language inference",
    "authors": ["Bill MacCartney", "Michel Galley", "Christopher D Manning."],
    "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 802–811. Association for",
    "year": 2008
  }, {
    "title": "Did it happen? The pragmatic complexity of veridicality assessment",
    "authors": ["Marie-Catherine de Marneffe", "Christopher D. Manning", "Christopher Potts."],
    "venue": "Computational Linguistics, 38(2):301– 333.",
    "year": 2012
  }, {
    "title": "Null case and the distribution of PRO",
    "authors": ["Roger Martin."],
    "venue": "Linguistic Inquiry, 32(1):141–166.",
    "year": 2001
  }, {
    "title": "A minimalist theory of PRO and control",
    "authors": ["Roger Andrew Martin."],
    "venue": "Ph.D. thesis, University of Connecticut, Storrs.",
    "year": 1996
  }, {
    "title": "MEANTIME, the NewsReader Multilingual Event and Time Corpus",
    "authors": ["Anne-Lyse Minard", "Manuela Speranza", "Ruben Urizar", "Begoña Altuna", "Marieke van Erp", "Anneleen Schoen", "Chantal van Son."],
    "venue": "Proceedings of the Tenth International Confer-",
    "year": 2016
  }, {
    "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
    "authors": ["Makoto Miwa", "Mohit Bansal."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1105–1116, Berlin, Germany. Asso-",
    "year": 2016
  }, {
    "title": "Causal necessity and sufficiency in implicativity",
    "authors": ["Prerna Nadathur."],
    "venue": "Semantics and Linguistic Theory, 26:1002–1021.",
    "year": 2016
  }, {
    "title": "Computing relative polarity for textual inference",
    "authors": ["Rowan Nairn", "Cleo Condoravdi", "Lauri Karttunen."],
    "venue": "Proceedings of the Fifth International Workshop on Inference in Computational Semantics (ICoS-5), pages 20–21, Buxton, England. Associa-",
    "year": 2006
  }, {
    "title": "A general and simple method for obtaining R2 from generalized linear mixed-effects models",
    "authors": ["Shinichi Nakagawa", "Holger Schielzeth."],
    "venue": "Methods in Ecology and Evolution, 4(2):133–142.",
    "year": 2013
  }, {
    "title": "Most ”babies” are ”little” and most ”problems” are ”huge”: Compositional Entailment in AdjectiveNouns",
    "authors": ["Ellie Pavlick", "Chris Callison-Burch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2016
  }, {
    "title": "Zero syntax: vol",
    "authors": ["David Pesetsky."],
    "venue": "2: Infinitives.",
    "year": 1991
  }, {
    "title": "On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference",
    "authors": ["Adam Poliak", "Yonatan Belinkov", "James Glass", "Benjamin Van Durme."],
    "venue": "Proceedings of the 2018 Conference of the North American",
    "year": 2018
  }, {
    "title": "Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation",
    "authors": ["Benjamin Van Durme"],
    "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2018
  }, {
    "title": "Hypothesis Only Baselines in Natural Language Inference",
    "authors": ["Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme."],
    "venue": "Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM),",
    "year": 2018
  }, {
    "title": "Neural Models of Factuality",
    "authors": ["Rachel Rudinger", "Aaron Steven White", "Benjamin Van Durme."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, pages 731–744, New Orleans.",
    "year": 2018
  }, {
    "title": "FactBank: a corpus annotated with event factuality",
    "authors": ["Roser Saurı", "James Pustejovsky"],
    "venue": "Language Resources and Evaluation,",
    "year": 2009
  }, {
    "title": "Are you sure that this happened? assessing the factuality degree of events in text",
    "authors": ["Roser Saurı", "James Pustejovsky"],
    "year": 2012
  }, {
    "title": "On the conversational basis of some presuppositions",
    "authors": ["Mandy Simons."],
    "venue": "Semantics and Linguistic Theory, 11:431–448.",
    "year": 2001
  }, {
    "title": "Observations on embedding verbs, evidentiality, and presupposition",
    "authors": ["Mandy Simons."],
    "venue": "Lingua, 117(6):1034–1056.",
    "year": 2007
  }, {
    "title": "What projects and why",
    "authors": ["Mandy Simons", "Judith Tonhauser", "David Beaver", "Craige Roberts."],
    "venue": "Semantics and linguistic theory, 20:309–327.",
    "year": 2010
  }, {
    "title": "Grounded compositional semantics for finding and describing images with sentences",
    "authors": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Transactions of the Association of Computational Linguistics,",
    "year": 2014
  }, {
    "title": "Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets",
    "authors": ["Gabriel Stanovsky", "Judith Eckle-Kohler", "Yevgeniy Puzikov", "Ido Dagan", "Iryna Gurevych."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Compu-",
    "year": 2017
  }, {
    "title": "The tense of infinitives",
    "authors": ["Tim Stowell."],
    "venue": "Linguistic Inquiry, 13(3):561–570.",
    "year": 1982
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in Neural Information Processing Systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment",
    "authors": ["Masatoshi Tsuchiya."],
    "venue": "Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC2018), Miyazaki,",
    "year": 2018
  }, {
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "authors": ["Alex Wang", "Amapreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman."],
    "venue": "arXiv:1804.07461.",
    "year": 2018
  }, {
    "title": "Factive-implicatives and modalized complements",
    "authors": ["Aaron Steven White."],
    "venue": "Proceedings of the 44th annual meeting of the North East Linguistic Society, pages 267–278, University of Connecticut.",
    "year": 2014
  }, {
    "title": "Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework",
    "authors": ["Aaron Steven White", "Pushpendre Rastogi", "Kevin Duh", "Benjamin Van Durme."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural",
    "year": 2017
  }, {
    "title": "A computational model of S-selection",
    "authors": ["Aaron Steven White", "Kyle Rawlins."],
    "venue": "Semantics and Linguistic Theory, 26:641–663.",
    "year": 2016
  }, {
    "title": "The role of veridicality and factivity in clause selection",
    "authors": ["Aaron Steven White", "Kyle Rawlins."],
    "venue": "Proceedings of the 48th Annual Meeting of the North East Linguistic Society, page to appear, Amherst, MA. GLSA Publications.",
    "year": 2018
  }, {
    "title": "Universal decompositional semantics on universal dependencies",
    "authors": ["Aaron Steven White", "Drew Reisinger", "Keisuke Sakaguchi", "Tim Vieira", "Sheng Zhang", "Rachel Rudinger", "Kyle Rawlins", "Benjamin Van Durme."],
    "venue": "Proceedings of the 2016 Confer-",
    "year": 2016
  }, {
    "title": "Tense and aspect in English infinitives",
    "authors": ["Susi Wurmbrand."],
    "venue": "Linguistic Inquiry, 45(3):403–447.",
    "year": 2014
  }, {
    "title": "Learning to execute",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1410.4615.",
    "year": 2014
  }],
  "id": "SP:f81ca3dd8b1198a7468bb5eb9c6e54e8009127a4",
  "authors": [{
    "name": "Aaron Steven White",
    "affiliations": []
  }, {
    "name": "Rachel Rudinger",
    "affiliations": []
  }, {
    "name": "Kyle Rawlins",
    "affiliations": []
  }, {
    "name": "Benjamin Van Durme",
    "affiliations": []
  }],
  "abstractText": "We investigate neural models’ ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.",
  "title": "Lexicosyntactic Inference in Neural Models"
}