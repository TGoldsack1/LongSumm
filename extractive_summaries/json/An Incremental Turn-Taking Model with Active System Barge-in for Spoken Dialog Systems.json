{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2015 Conference, pages 42–50, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Human-human conversation has flexible turntaking behavior: back channeling, overlapping speech and smooth turn transitions. Imitating human-like turn-taking in a spoken dialog system (SDS) is challenging due to the degradation in quality of the dialog when overlapping speech is produced in the wrong place. For this, a traditional SDS often uses a simplified turn-taking model with rigid turn taking. They only respond when users have finished speaking. Thus past research has mostly focused on end-of-turn detection, finding the end of the user utterance as quickly as possible while minimizing the chance of wrongly interrupting the users. We refer here to the interruption issue as false cut-ins (FCs).\nRecent research in incremental dialog processing promises more flexible turn-taking behavior (Atterer et al., 2008; Breslin et al., 2013). Here, the automatic speech recognizer (ASR) and natural language understanding (NLU) incrementally\nproduce partial decoding/understating messages for decision-making. This allows for system barge-in (SB), starting to respond before end-of-utterance. Although this framework has shown promising results in creating flexible SDSs, the following two fundamental issues remain:\n1. We need a model that unifies incremental processing and traditional turn-taking behavior. 2. We also need a systematic procedure that trains a system to produce meaningful SBs.\nThis paper first proposes a finite state machine (FSM) that both shows superior performance in end-of-turn detection compared to previous methods and is compatible with incremental processing. Then we propose a systematic procedure to endow a system with meaningful SB by combining the theory of optimal stopping with reinforcement learning.\nSection 2 of the paper discusses related work; Section 3 describes the finite state machine; Sections 4, 5, and 6 describe how to produce meaningful SB; Section 7 gives experimental results of an evaluation using the CMU Let’s Go Live system and simulation results on the Dialog State Tracking Challenging (DTSC) Corpus and Section 8 concludes."
  }, {
    "heading": "2 Related Work and Limitations",
    "text": "This work is closely related to end-of-turn detection and incremental processing (IP) dialog systems.\nThere are several methods for detecting the endof-turn. Raux (2008) built a decision tree for final pause duration using ASR and NLU features. At runtime, the system first dynamically chooses the final pause duration threshold based on the dialog state and then predicts end-of-turn if final pause duration is longer than that threshold. Other work explored predicting end-of-turn within a user’s speech. This showed substantial improvement in speed of response (Raux and Eske-\n42\nnazi, 2009). Another approach examined prosodic and semantic features such as pitch and speaking rate in human-human conversation for turn-yielding cues (Gravano, 2009).\nThe key limitation of those methods is that the decision made by the end-of-turn detector is treated as a “hard” decision, obliging developers to compromise in a tradeoff between response latency and FC rate (Raux and Eskenazi, 2008). Although adding more complex prosodic and semantic features can improve the performance of the detector, it also increases computation cost and requires significant knowledge of the SDS, which can limit the accessibility for non-expert developers.\nFor IP, Kim (2014) has demonstrated the possibility of learning turn-taking from human dialogs using inverse reinforcement learning. Other work has focused on incremental NLU (DeVault et al., 2009), showing that the correct interpretation of users’ meaning can be predicted before end-of-turn. Another topic is modeling user and system barge-in. Selfridge (2013) has presented a FSM that predicts users’ barge-ins. Also, Ghigi (2014) has shown that allowing SB when users produce lengthy speech increases robustness and task success.\nDifferent from Kim’s work that learns humanlike turn-taking, our approach is more related to Ghigi’s method, which tries to improve dialog efficiency from a system-centric perspective. We take one step further by optimizing the turn-taking using all available features based on a global objective function with machine learning methods."
  }, {
    "heading": "3 A Finite State Turn-Taking Model",
    "text": ""
  }, {
    "heading": "3.1 Model Description",
    "text": "Our model has two distinct modes: passive and active. The passive mode exhibits traditional rigid turn-taking behavior while the active mode has the system respond in the middle of a user turn. We first describe how these two modes operate, and then show how they are compatible with existing incremental dialog approaches.\nThe idea is to combine an aggressive speaker with a patient listener. The speaker consists of the Text-to-Speech (TTS) and Natural Language Generation (NLG) modules. The listener is composed of the ASR and Voice Activity Detection (VAD) modules. The system attempts to respond to a user every time it detects a short pause (e.g. 100ms). But before a long pause (e.g. 1000ms) is detected, the user’s continued speech will stop the system from\nresponding, as shown on Figure 1:\nMost of the system’s attempts to respond will thus be FCs. However, since the listener can stop the system from speaking, the FCs have no effect on the conversation (users may hear the false start of the system’s prompt, but often the respond state is cancelled before the synthesized speech begins). If the attempt is correct, however, the system responds with almost 0-latency, as shown in Figure 2. Furthermore, because the dialog manager (DM) can receive partial ASR output whenever there is a short pause, this model produces relatively stable partial ASR output and supports incremental dialog processing.\nWe then define the short pause as the action threshold (AT) and the long pause as the listening threshold (LT), where 0 < AT ≤ LT, which can be interpreted respectively as the “aggression” and “patience” of the system. By changing the value of each of these thresholds we can modify the system’s behavior from rigid turn taking to active SB.\n1. Passive Agent: act fast and listen patiently (AT = small value, LT = large value)\n2. Active Agent: act and listen impatiently. (AT = LT = small value)\nThis abstraction simplifies the challenge: “when the system should barge in” as the following transition: PassiveAgent Φ(dialog state)−−−−−−−−−→ ActiveAgent where Φ(·) : dialog State → {true, false} is a function that outputs true whenever the agent should take the floor, regardless of the current state of the floor. For example, this function could output true when the current dialog states fulfill certain rules in a hand-crafted system, or could output true when the system has reached its maximal understanding of the user’s intention (DeVault et al., 2009). A natural next step is to use statistical techniques to learn an optimized Φ(·) based on all features related to the dialog states, in order to support more complex SB behavior."
  }, {
    "heading": "3.2 Advantages over Past Methods",
    "text": "First our model solves end-of-turn detection by using a combination of VAD and TTS control, instead of trying to build a perfect classifier. This avoids the tradeoff between response latency and FC. Under the assumption that the TTS can operate at high speed, the proposed system can achieve almost 0-lag and 0-FC by setting AT to be small (e.g. 100ms). Second, the model does not require expensive prosodic and semantic turn-yielding cue detectors, thus simplifying the implementation."
  }, {
    "heading": "4 Toward Active System Barge-in",
    "text": "In state-of-the-art SDS, the DM uses explicit/implicit confirmation to fill each slot and carries out an error recovery strategy for incorrectly recognized slots (Bohus and Rudnicky, 2009). The system should receive many correctly-recognized slots, thus avoiding lengthy error recovery. While a better ASR and NLU could help, Ghigh (2014) has shown that allowing the system to actively respond to users also leads to more correct slots.\nTable 1 demonstrates three cases where active SB can help. The first two rows show the first half of the user’s speech being correctly recognized while the second half is not. In this scenario, if, in the middle of the utterance, the system can tell that the existing ASR hypothesis is sufficient and actively barges on the user, it can potentially avoid the poorly-recognized speech that follows. The third example has noise at the beginning of the user turn. The system could back channel in the middle of the utterance to ask the user to go to a quieter place or to repeat an answer. In these examples active SB can help improve robustness:\n1. Barge in when the current hypothesis has high confidence and contains sufficient information to move the dialog along. 2. Barge in when the hypothesis confidence is low and the predicted future hypothesis will not get better. This can avoid recovering from a large number of incorrect slots.\nA natural choice of objective function to train such a system is to maximize the expected quality of information in the users’ utterances. The quality of the recognized information is positively correlated to number of correctly recognized slots (CS) and inversely correlated to the number of incorrectly recognized slots (ICS). In the next section, we describe how we transform CS and ICS into a real-value reward."
  }, {
    "heading": "5 A Cost Model for System Barge-in",
    "text": "We first design a cost model that defines a reward function. This model is based on the assumption that the system will use explicit confirmation for every slot. We choose this because it is the most basic dialog strategy. A sample dialog for this strategy is as follows:\nSys: Where do you want to leave from? User: Leaving from X. Sys: Do you mean leaving from Y? User: No. Sys: Where do you want to leave from? User: <No Parse> Sys: Where do you want to leave from? User: I am leaving from X. Sys: Do you mean X? User: Yes.\nGiven this dialog strategy the system spends one turn asking the question, and k turns confirming k slots in the user response. Also, for no-parse (0 slot) input, the system asks the same question again. Therefore, the minimum number of turns required\nto acquire n slots is 2n. However, because user responses contain ICS and no-parses, the system takes more than 2n turns to obtain all the slot information (assume confirmation are never misrecognized).\nWe denote csi and icsi as the number of correctly/incorrectly recognized slots in the user response. So the quality of the user response is captured by a tuple, (csi, icsi). The goal is to obtain a reward function that maps from a given user response (csi, icsi) to a reward value ri ∈ <. This reward value should correlate with the overall efficiency of a dialog, which is inversely correlated with the number of turns needed for task completion.\nThen for a dialog task that has n slots to fill, we can denote hi as the number of turns already spent, fi as the estimated number of future turns needed for task completion and E[S] as the expected number of turns needed to fill 1 slot. Then for each new user response (csi, icsi), we update the following recursive formulas:\nInitialization: h0 = 0, f0 = nE[s] Update Rules:\nhi = hi−1 + 1︸︷︷︸ question + csi + icsi︸ ︷︷ ︸ confirm\n(1)\nfi = fi−1 − csiE[S]︸ ︷︷ ︸ acquired slots\n(2)\nBased on the above setup, it is clear that hi + fi equals the estimated total number of turns needed to fill n slots. Then the reward, ri, associated with each user response can be expressed as the difference between the previous and current estimates:\nri = (hi−1 + fi−1)− (hi + fi) (3) = −1 + (E[S]− 1)︸ ︷︷ ︸\nweight to CS\ncsi − icsi (4)\nTherefore, a positive reward means the new user response reduces the estimated number of turns for task completion while a negative reward means the opposite. Another interpretation of this reward function is that for no-parse user response (csi = 0, icsi = 0), the cost is to waste 1 turn asking the same question again. When there is a parse, each correct slot can save E[S] turns in the future, while each slot, regardless of its correctness, needs a 1- turn confirmation. As a result, this rewards function is correlated with the global efficiency of a dialog because it assigns a corpus-dependent weight to csi, based on E[S] estimated from historical dialogs."
  }, {
    "heading": "6 Learning Active Turn-taking Policy",
    "text": "After modeling the cost of a user turn, we learn a turn-taking policy that can maximize the expected reward in user turns, namely the Φ(dialog state) that controls the switching between passive and active agent of our FSM in Section 3.1. Before going into detail, we first introduce the optimal stopping problem and reinforcement learning."
  }, {
    "heading": "6.1 Optimal Stopping Problem and Reinforcement Learning",
    "text": "The theory of optimal stopping is an area of mathematics that addresses the decision of when to take a given action based on a set of sequentially observed random variables, in order to maximize an expected payoff (Ferguson, 2012). A formal description is as follows:\n1. A sequence of random variables X1, X2... 2. A sequence of real-valued reward functions, y0, y1(x1), y2(x1, x2)...\nThe decider may observe the sequence x1, x2... and after observing X1 = x1, ...Xn = xn, the decider may stop and receive the reward yn(x1, ...xn), or continue and observe Xn+1. The optimal stopping problem searches for an optimal stopping rule that maximizes the expected reward.\nReinforcement learning models are based on the Markov decision process (MDP). A (finite) MDP is a tuple (S,A, {Psa}, γ, R), where: • S is a finite set of N states • A = a1, ...ak is a set of k actions • Psa(·) are the state transition probabilities on\ntaking action a in state s. • γ ∈ [0, 1) is the discount factor • R : S → < is the rewards function. Then a policy, π , is a mapping from each state, s ∈ S and action a ∈ A, to the probability π(s, a) of taking action awhen in state s (Sutton and Barto, 1998). Then, for MDPs, the Q-function, is the expected return starting from s taking action a and thereafter following policy π and has the Bellman equation: Qπ(s, a) = R(s) + γ ∑ s′ P (s′|s, a)V π(s′). (5)\nThe goal of reinforcement learning is to find the optimal policy π∗, such that Qπ(s, a) can be maximized. Thus the optimal stopping problem can be formulated as an MDP, where the action space contains two actions {wait, stop}. Also, solving the optimal stopping rule is equivalent to finding the optimal policy, π∗."
  }, {
    "heading": "6.2 Solving Active Turn-taking",
    "text": "Equipped with the above two frameworks, we first show that SB can be formulated as an optimal stopping problem. Then we propose a novel, noniterative, model-free method for solving for the optimal policy.\nAn SDS dialog contains N user utterances. Each user utterance contains K partial hypotheses and each partial hypothesis, pi, is associated with a tuple (csi, icsi) and a feature vector, xi ∈ <f×1, where f is the dimension of the feature vector. We also assume that every user utterance is independent of every other utterance. We will call one user utterance an episode.\nIn an episode, the turn-taking decider will see each partial hypothesis sequentially over time, At each hypothesis it takes an action from {wait, stop}. Wait means it continues to listen. Stop means it takes the floor. The turn-taking decider receives 0 reward for taking the action wait and receives the reward ri from (csi, icsi) according to our cost model for taking the action stop. This is an optimal stopping problem that can be formulated as an MDP:\n• S = {x1, ...{x1...xK}} • A = {wait, stop} • R = −1 + (E[S]− 1)csi − icsi Then the Bellman equations are: Qπ(s, stop) = R(s) = r(s) (6)\nQπ(s, wait) = γ ∑\ns′ P (s′|s, a)V π(s′) (7)\nThe first equation shows that the Q-value for any state, s, with action, stop, is simply the immediate reward for s. The second equation shows that the Q-value for any state s, with action, wait, only depends on the future return by following policy π. This result is crucial because it means that Qπ(s, stop) for any state, s, can be directly calculated based on the cost model, independent of the policy π. Also, given a policy π, Qπ(s, wait)can also be directly calculated as the discounted reward the first time that the policy chooses to stop.\nMeanwhile, for a given episode with known reward ri for each partial hypothesis pi, optimal stopping means always to stop at the largest reward, meaning that we can obtain the oracle action for the training corpus. Given a sequence of reward (ri, ...rK) , the optimal policy, π, chooses to stop at partial pm if m = arg maxj∈(i,K] rj .\nThe Bellman equations become: Qπ(si, stop) = ri (8)\nQπ(si, wait) = γm−irm (9) and the oracle action at any s can be obtained by : a∗i = wait if Q\n∗(si, stop) < Q∗(si, wait) a∗i = stop if Q\n∗(si, stop) ≥ Q∗(si, wait) This special property of optimal stopping problem allows us to use supervised learning methods directly modeling the optimal Q function, by finding a mapping from the input state space, si, into the Q-value for both actions: Q(si, stop)∗ and Q(si, wait)∗. Further, inspired by the work of reinforcement learning as classification (Lagoudakis and Parr, 2003), we decide to map directly from the input state space into the action space: S → A∗, using a Support Vector Machine (SVM).\nAdvantages of solving this problem as a classification rather than a regression include: 1) it explicitly models sign(Q(si, stop)∗−Q(si, wait)∗), which sufficiently determines the behavior of the agent. 2) SVM is known as a state-of-the-art modeler for the binary classification task, due to its ability to find the separating hyperplane in nonlinear space."
  }, {
    "heading": "6.3 Feature Construction",
    "text": "Since SVM requires a fixed input dimension size, while the available features will continue to increase as the turn-taking decider observes more partial hypotheses, we adopt the functional idea used by the openSMILE toolkit (Eyben et al., 2010). There are three categories of features: immediate feature, delta feature and long-term feature. Immediate features come from the ASR and the NLU in the latest partial hypothesis. Delta features are the first-order derivate of immediate features with respect to the previous observed feature. Long-term features are global statistics associated with all the observed features.\nTable 2 shows that we have 18 immediate features, 18 delta features and 18× 7 = 126 long-term features. Then we apply F-score feature selection as described in (Chen and Lin, 2006). The final feature set contains 138 features."
  }, {
    "heading": "7 Experiments and Results",
    "text": "We conducted a live study and a simulation study. The live study evaluates the model’s end-of-turn detection. The simulated study evaluates the active SB behavior."
  }, {
    "heading": "7.1 Live Study",
    "text": "The finite state machine was implemented in the Interaction Manager of the CMU Lets Go system that provides bus information in Pittsburgh (Raux et al., 2005). We compared base system data from November 1-30, 2014 (773 dialogs), to data from our system from December 1-31, 2014 (565 dialogs).\nThe base system used the decision tree endof-turn detector described in (Raux and Eskenazi, 2008) and the active SB algorithm described in (Ghigi et al., 2014). The action threshold (AT) in the new system was set at 60% of the decision tree output in the former system and the listening threshold (LT) was empirically set at 1200ms."
  }, {
    "heading": "7.2 Live Study Metrics",
    "text": "We observed that FCs result in several users’ utterances having overlapping timestamps due to a builtin 500ms padding before an utterances in PocketSphinx. This means that we consider two consecutive utterances with a pause less than 500ms as one utterance. Figure 4 shows that when the end-of-turn detector produces an FC, the continued flow of user\nspeech instantiates a new user utterance which overlaps with the previous one. In this example, utterances 0 and 1 have overlaps while utterance 2 does not. So users actually produce two utterances, while the system thinks there are three due to FC.\nThus, we can automatically calculate the FC rate of every dialog, by counting the number of user utterances with overlaps. We define an utterance fragment ratio (UFR) that measures the FC rate in a dialog.\nUFR = Number of user utterances with overlapsTotal number of user utterances\nWe also manually label task success (TS) of all the dialogs. We define TS as: a dialog is successful if and only if the system conducted a back-end search for bus information with all required slots correctly recognized. In summary, we use the following metrics to evaluate the new system:\n1. Task success rate 2. Utterance fragment ratio (UFR) 3. Average number of system barge-in (ANSB) 4. Proportion of long user utterances interrupted\nby system barge-in (PLUISB) 5. Average response delay (ARD) 6. Average user utterance duration over time"
  }, {
    "heading": "7.3 Live Study Results",
    "text": "Table 3 shows that the TS rate of the new system is 7.5% higher than the previous system (p-value < 0.01). Table 4 shows that overall UFR decreased by 37.1%. UFR for successful and for failed dialogs indicates that the UFR decreases more in failed dialogs than in successful ones. One explanation is that failed dialogs usually have a noisier environment. The UFR reduction explains the increase in success rate since UFRs are positively correlated with TS rate, as reported in (Zhao and Eskenazi, 2015)\nTable 5 shows that the SB algorithm was activated more often in the new system. This is because the SB algorithm described in (Ghigi et al., 2014) only activates for user utterances longer than 3 seconds. FCs will therefore hinder the ability of this algorithm to reliably measure user utterance dura-\ntion. This is an example of how reliable end-of-turn detection can benefit other SDS modules. Table 5 also shows that the new system is 32.5% more responsive than the old system. We purposely set the action threshold to 60% of the threshold in the old system, which demonstrates that the new model can have an response speed equals to action threshold that is independent of the FC rate.\nFigure 5 shows how average user utterance duration evolves in a dialog. Utterance duration is more stable in the new system than in the old one. Two possible explanations are: 1) since UFR is much higher in the old system, the system is more likely to cut in at the wrong time, possibly making users abandon their normal turn-taking behavior and talk over the system. 2) more frequent activation of the SB algorithm entrains the users to produce more concise utterances."
  }, {
    "heading": "7.4 Simulation Study",
    "text": "This part of the experiment uses the DSTC corpus training2 (643 dialogs) (Black et al., 2013). The data was manually transcribed. The reported 1-best word error rate (WER) is 58.2% (Williams et al., 2013). This study focuses on all user responses to:“Where are you leaving from?” and “Where are you going?” which have 688 and 773 utterances respectively.\nAn automatic script, based on the manual transcription, labels the number of correct and incorrect\nslots (csi, icsi) for each partial hypothesis, pi. Also from the training data, the expected number of turns needed to obtain 1 slot, E[S], is 3.82. For simplicity, E[S] is set to be 4. So the reward function discussed in Section 5 is: ri = −1 + 3csi − icsi.\nAfter obtaining the reward value for each hypothesis, the oracle action at each partial hypothesis is calculated based on the procedure discussed in Section 6.3 with γ = 1.\nWe set the SVM kernel as RBF kernel and use a grid search to choose the best parameters for cost and kernel width using 5-fold cross validation on the training data (Hsu et al., 2003). The optimization criterion is the F-measure."
  }, {
    "heading": "7.5 Simulation Study Metrics",
    "text": "The evaluation metrics have two parts: classification-related (precision and recall) and dialog-related. Dialog related metrics are:\n1. Accuracy of system barge-in 2. Average decrease in utterance duration com-\npared to no system barge-in 3. Percentage of no-parse utterance 4. Average CS per utterance 5. Average ICS per utterance 6. Average reward = 1/T ∑ i ri , where T is the\nnumber of utterances in the test set. The learned policy is compared to two reference systems: the oracle and the baseline system. The oracle directly follows optimal policy obtained from the ground-truth label. The baseline system always waits for the last partial (no SB).\nFurthermore, a simple smoothing algorithm is applied to the SVM output for comparison. This algorithm confirms the stop action after two consecutive stop outputs from the classifier. This increases the classifier’s precision."
  }, {
    "heading": "7.6 Simulation Study Results",
    "text": "10-fold cross validation was conducted on the two datasets. Instead of using the SVM binary output, we apply a global threshold of 0.4 on the SVM decision function for output to achieve the best average reward. The threshold is determined based on cross-validation on training data.\nTable 6 shows that the SVM classifier can achieve very high precision and high recall in predicting the correct action. The F-measure (after smoothing) is 84.46% for departure question responses and 85.99% for arrival questions.\nTable 7 shows that learned policy increases the average reward by 27.7% and 14.9% compared to the baseline system for the departure and arrival responses respectively. We notice that the average reward of the baseline arrival responses is significantly higher. A possible reason is that by this second question the users are adapting to the system.\nThe decrease in average utterance duration shows some interesting results. For responses to both questions, the oracle system utterance duration is about 55% shorter than the baseline one. The learned policy is also 45% shorter, which means that at about the middle of a user utterance, the system can already predict that the user either has expressed enough information or that the ASR is so wrong that there is no point of continuing to listen.\nTable 8 expands our understanding of the oracle\nand learned policy behaviors. We see that the oracle produces a much higher percentage of no-parse utterances in order to maximize the average reward, which, at first, seems counter-intuitive. The reason is that some utterances contain a large number of incorrect slots at the end and the oracle chooses to barge in at the beginning of the utterance to avoid the large negative reward for waiting until the end. This is the expected behavior discussed in Section 4. The learned policy is more conservative in producing no-parse utterances because it cannot cheat like the oracle to access future information and know that all future hypotheses will contain only incorrect information. However, although the learned policy only has access to historical information, it manages to predict future return by increasing CS and reducing ICS compared to the baseline."
  }, {
    "heading": "8 Conclusions and Future Directions",
    "text": "This paper describes a novel turn-taking model that unifies the traditional rigid turn-taking model with incremental dialog processing. It also illustrates a systematic procedure of constructing a cost model and teaching a dialog system to actively grab the conversation floor in order to improve system robustness. The turn-taking model was tested for end-of-turn detection and active SB. The proposed model has shown superior performance in reducing FC rate and response delay. Also, the proposed SB algorithm has shown promise in increasing the average reward in user responses.\nFuture studies will include constructing a more comprehensive cost model that not only takes into account of CS/ICS, but also includes other factors such as conversational behavior. Further, since E[S] will decrease after applying the learned policy, it invalidates the previous reward function. Future work should investigate how the change inE[S] impacts the optimality of the policy. Also, we will add more complex actions to the system such as back channeling, clarifications etc."
  }],
  "year": 2015,
  "references": [{
    "title": "Towards incremental end-of-utterance detection in dialogue systems",
    "authors": ["Michaela Atterer", "Timo Baumann", "David Schlangen."],
    "venue": "Proceedings of the 22nd International Conference on Computational Linguistics.",
    "year": 2008
  }, {
    "title": "Dialog state tracking challenge",
    "authors": ["Alan Black", "Maxine Eskenazi", "Milica Gasic", "Helen Hastie", "KAIST Kee-Eung Kim", "Korea Ian Lane", "Sungjin Lee", "NICT Teruhisa Misu", "Japan Olivier Pietquin", "France SUPELEC"],
    "year": 2013
  }, {
    "title": "The ravenclaw dialog management framework: Architecture and systems",
    "authors": ["Dan Bohus", "Alexander I Rudnicky."],
    "venue": "Computer Speech & Language, 23(3):332–361.",
    "year": 2009
  }, {
    "title": "Continuous asr for flexible incremental dialogue",
    "authors": ["Catherine Breslin", "Milica Gasic", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013",
    "year": 2013
  }, {
    "title": "Combining svms with various feature selection strategies",
    "authors": ["Yi-Wei Chen", "Chih-Jen Lin."],
    "venue": "Feature extraction, pages 315–324. Springer, Berlin Heidelberg.",
    "year": 2006
  }, {
    "title": "Can i finish?: learning when to respond to incremental interpretation results in interactive dialogue",
    "authors": ["David DeVault", "Kenji Sagae", "David Traum."],
    "venue": "Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Dis-",
    "year": 2009
  }, {
    "title": "Opensmile: the munich versatile and fast opensource audio feature extractor",
    "authors": ["Florian Eyben", "Martin Wöllmer", "Björn Schuller."],
    "venue": "Proceedings of the international conference on Multimedia, pages 1459– 1462. ACM.",
    "year": 2010
  }, {
    "title": "Optimal stopping and applications",
    "authors": ["Thomas S Ferguson."],
    "venue": "University of California, Los Angeles.",
    "year": 2012
  }, {
    "title": "Incremental dialog processing in a task-oriented dialog",
    "authors": ["Fabrizio Ghigi", "Maxine Eskenazi", "M Ines Torres", "Sungjin Lee."],
    "venue": "Fifteenth Annual Conference of the International Speech Communication Association.",
    "year": 2014
  }, {
    "title": "Turn-taking and affirmative cue words in task-oriented dialogue",
    "authors": ["Agustin Gravano."],
    "venue": "Ph.D. thesis.",
    "year": 2009
  }, {
    "title": "A practical guide to support vector classification",
    "authors": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"],
    "venue": "Technical report,",
    "year": 2003
  }, {
    "title": "Inverse reinforcement learning for micro-turn management",
    "authors": ["Dongho Kim", "Catherine Breslin", "Pirros Tsiakoulis", "Milica Gašic", "Matthew Henderson", "Steve Young."],
    "venue": "Proceedings of the Annual Conference of the International Speech Communication As-",
    "year": 2014
  }, {
    "title": "Reinforcement learning as classification: Leveraging modern classifiers",
    "authors": ["Michail Lagoudakis", "Ronald Parr."],
    "venue": "ICML, volume 3, pages 424–431.",
    "year": 2003
  }, {
    "title": "Optimizing endpointing thresholds using dialogue features in a spoken dialogue system",
    "authors": ["Antoine Raux", "Maxine Eskenazi."],
    "venue": "Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1–10. Association for Computational Linguistics.",
    "year": 2008
  }, {
    "title": "A finitestate turn-taking model for spoken dialog systems",
    "authors": ["Antoine Raux", "Maxine Eskenazi."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2009
  }, {
    "title": "Lets go public! taking a spoken dialog system to the real world",
    "authors": ["Antoine Raux", "Brian Langner", "Dan Bohus", "Alan W Black", "Maxine Eskenazi."],
    "venue": "in Proc. of Interspeech 2005.",
    "year": 2005
  }, {
    "title": "Continuously predicting and processing barge-in during a live spoken dialogue task",
    "authors": ["Ethan Selfridge", "Iker Arizmendi", "Peter Heeman", "Jason Williams."],
    "venue": "Proceedings of the SIGDIAL 2013 Conference.",
    "year": 2013
  }, {
    "title": "Introduction to reinforcement learning",
    "authors": ["Richard S Sutton", "Andrew G Barto."],
    "venue": "MIT Press.",
    "year": 1998
  }, {
    "title": "The dialog state tracking challenge",
    "authors": ["Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black."],
    "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 404–413.",
    "year": 2013
  }, {
    "title": "Humansystem turn taking analysis for the let’s go bus information system",
    "authors": ["Tiancheng Zhao", "Maxine Eskenazi."],
    "venue": "Pittsburgh, May. The Meeting of the Acoustical Society of America.",
    "year": 2015
  }],
  "id": "SP:777d399aa2753464ca1cfbac273ee6e6780f2837",
  "authors": [{
    "name": "Tiancheng Zhao",
    "affiliations": []
  }, {
    "name": "Alan W Black",
    "affiliations": []
  }, {
    "name": "Maxine Eskenazi",
    "affiliations": []
  }],
  "abstractText": "This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection. It includes a flexible framework that enables active system barge-in. In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented. This procedure improves system robustness and success rate. It includes constructing cost models and learning optimal policy using reinforcement learning. Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system. Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.",
  "title": "An Incremental Turn-Taking Model with Active System Barge-in for Spoken Dialog Systems"
}