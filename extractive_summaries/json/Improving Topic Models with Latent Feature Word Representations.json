{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Topic modeling algorithms, such as Latent Dirichlet Allocation (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus, and predict the probabilities of each word in each document belonging to each topic (Teh et al., 2006; Newman et al., 2006; Toutanova and Johnson, 2008; Porteous et al., 2008; Johnson, 2010; Xie and Xing, 2013; Hingmire et al., 2013).\nConventional topic modeling algorithms such as these infer document-to-topic and topic-to-word distributions from the co-occurrence of words within documents. But when the training corpus of documents is small or when the documents are short, the resulting distributions might be based on little evidence. Sahami and Heilman (2006) and Phan et al.\n(2011) show that it helps to exploit external knowledge to improve the topic representations. Sahami and Heilman (2006) employed web search results to improve the information in short texts. Phan et al. (2011) assumed that the small corpus is a sample of topics from a larger corpus like Wikipedia, and then use the topics discovered in the larger corpus to help shape the topic representations in the small corpus. However, if the larger corpus has many irrelevant topics, this will “use up” the topic space of the model. In addition, Petterson et al. (2010) proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution.\nTopic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature (LF) vectors have been used for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora.\nRather than relying solely on a multinomial or latent feature model, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus.\nOur main contribution is that we propose two new latent feature topic models which integrate latent feature word representations into two Dirichlet\nar X\niv :1\n81 0.\n06 30\n6v 1\n[ cs\n.C L\n] 1\n5 O\nct 2\nmultinomial topic models: a Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) and a onetopic-per-document Dirichlet Multinomial Mixture (DMM) model (Nigam et al., 2000). Specifically, we replace the topic-to-word Dirichlet multinomial component which generates the words from topics in each Dirichlet multinomial topic model by a twocomponent mixture of a Dirichlet multinomial component and a latent feature component.\nIn addition to presenting a sampling procedure for the new models, we also compare using two different sets of pre-trained latent feature word vectors with our models. We achieve significant improvements on topic coherence evaluation, document clustering and document classification tasks, especially on corpora of short documents and corpora with few documents."
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 LDA model",
    "text": "The Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003) represents each document d as a probability distribution θd over topics, where each topic z is modeled by a probability distribution φz over words in a fixed vocabulary W .\nAs presented in Figure 1, where α and β are hyper-parameters and T is number of topics, the generative process for LDA is described as follows:\nθd ∼ Dir(α) zdi ∼ Cat(θd) φz ∼ Dir(β) wdi ∼ Cat(φzdi )\nwhere Dir and Cat stand for a Dirichlet distribution and a categorical distribution, and zdi is the topic indicator for the ith word wdi in document d. Here, the topic-to-word Dirichlet multinomial component generates the word wdi by drawing it from the categorical distribution Cat(φzdi ) for topic zdi .\nWe follow the Gibbs sampling algorithm for estimating LDA topic models as described by Griffiths and Steyvers (2004). By integrating out θ and φ, the algorithm samples the topic zdi for the current i th\nword wdi in document d using the conditional distribution P(zdi | Z¬di), where Z¬di denotes the topic assignments of all the other words in the document collection D, so:\nP(zdi = t | Z¬di) ∝ (N td¬i + α) N t,wdi ¬di + β\nN t¬di + V β (1)\nNotation: N t,wd is the rank-3 tensor that counts the number of times that word w is generated from topic t in document d by the Dirichlet multinomial component, which in section 2.1 belongs to the LDA model, while in section 2.2 belongs to the DMM model. When an index is omitted, it indicates summation over that index (so Nd is the number of words in document d).\nWe write the subscript ¬d for the document collection D with document d removed, and the subscript ¬di for D with just the ith word in document d removed, while the subscript d¬i represents document d without its ith word. For example, N t¬di is the number of words labelled a topic t, ignoring the ith word of document d. V is the size of the vocabulary, V = |W |."
  }, {
    "heading": "2.2 DMM model for short texts",
    "text": "Applying topic models for short or few documents for text clustering is more challenging because of data sparsity and the limited contexts in such texts. One approach is to combine short texts into long pseudo-documents before training LDA (Hong and Davison, 2010; Weng et al., 2010; Mehrotra et al., 2013). Another approach is to assume that there is only one topic per document (Nigam et al., 2000; Zhao et al., 2011; Yin and Wang, 2014).\nIn the Dirichlet Multinomial Mixture (DMM) model (Nigam et al., 2000), each document is assumed to only have one topic. The process of generating a document d in the collection D, as shown in Figure 1, is to first select a topic assignment for the document, and then the topic-to-word Dirichlet multinomial component generates all the words in the document from the same selected topic:\nθ ∼ Dir(α) zd ∼ Cat(θ) φz ∼ Dir(β) wdi ∼ Cat(φzd)\nYin and Wang (2014) introduced a collapsed Gibbs sampling algorithm for the DMM model in\nwhich a topic zd is sampled for the document d using the conditional probability P(zd | Z¬d), where Z¬d denotes the topic assignments of all the other documents, so:\nP(zd = t | Z¬d) ∝ (Mt¬d + α) Γ(Nt¬d + V β)\nΓ(Nt¬d +Nd + V β) ∏ w∈W Γ(Nt,w¬d +N w d + β) Γ(Nt,w¬d + β) (2)\nNotation: M t¬d is the number of documents assigned to topic t excluding the current document d; Γ is the Gamma function."
  }, {
    "heading": "2.3 Latent feature vector models",
    "text": "Traditional count-based methods (Deerwester et al., 1990; Lund and Burgess, 1996; Bullinaria and Levy, 2007) for learning real-valued latent feature (LF) vectors rely on co-occurrence counts. Recent approaches based on deep neural networks learn vectors by predicting words given their window-based context (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Liu et al., 2015).\nMikolov et al. (2013)’s method maximizes the log likelihood of each word given its context. Pennington et al. (2014) used back-propagation to minimize the squared error of a prediction of the logfrequency of context words within a fixed window of each word. Word vectors can be trained directly on a new corpus. In our new models, however, in order to incorporate the rich information from very large datasets, we utilize pre-trained word vectors that were trained on external billion-word corpora."
  }, {
    "heading": "3 New latent feature topic models",
    "text": "In this section, we propose two novel probabilistic topic models, which we call the LF-LDA and the LFDMM, that combine a latent feature model with either an LDA or DMM model. We also present Gibbs sampling procedures for our new models.\nIn general, LF-LDA and LF-DMM are formed by taking the original Dirichlet multinomial topic models LDA and DMM, and replacing their topic-to-\nword Dirichlet multinomial component that generates words from topics with a two-component mixture of a topic-to-word Dirichlet multinomial component and a latent feature component.\nInformally, the new models have the structure of the original Dirichlet multinomial topic models, as shown in Figure 2, with the addition of two matrices τ and ω of latent feature weights, where τ t and ωw are the latent-feature vectors associated with topic t and word w respectively.\nOur latent feature model defines the probability that it generates a word given the topic as the categorical distribution CatE with:\nCatE(w | τ tω>) = exp(τ t · ωw)∑\nw′∈W exp(τ t · ωw′) (3)\nCatE is a categorical distribution with log-space parameters, i.e. CatE(w | u) ∝ exp(uw). As τ t and ωw are (row) vectors of latent feature weights, so τ tω> is a vector of “scores” indexed by words. ω is fixed because we use pre-trained word vectors.\nIn the next two sections 3.1 and 3.2, we explain the generative processes of our new models LF-LDA and LF-DMM. We then present our Gibbs sampling procedures for the models LF-LDA and LF-DMM in the sections 3.3 and 3.4, respectively, and explain how we estimate τ in section 3.5."
  }, {
    "heading": "3.1 Generative process for the LF-LDA model",
    "text": "The LF-LDA model generates a document as follows: a distribution over topics θd is drawn for document d; then for each ith word wdi (in sequential order that words appear in the document), the model chooses a topic indicator zdi , a binary indicator variable sdi is sampled from a Bernoulli distribution to determine whether the word wdi is to be generated by the Dirichlet multinomial or latent feature component, and finally the word is generated from the chosen topic by the determined topic-toword model. The generative process is:\nθd ∼ Dir(α) zdi ∼ Cat(θd) φz ∼ Dir(β) sdi ∼ Ber(λ) wdi ∼ (1− sdi)Cat(φzdi ) + sdiCatE(τ zdi ω >)\nwhere the hyper-parameter λ is the probability of a word being generated by the latent feature topic-toword model and Ber(λ) is a Bernoulli distribution with success probability λ."
  }, {
    "heading": "3.2 Generative process for the LF-DMM model",
    "text": "Our LF-DMM model uses the DMM model assumption that all the words in a document share the same topic. Thus, the process of generating a document in a document collection with our LF-DMM is as follows: a distribution over topics θ is drawn for the document collection; then the model draws a topic indicator zd for the entire document d; for every ith word wdi in the document d, a binary indicator variable sdi is sampled from a Bernoulli distribution to determine whether the Dirichlet multinomial or latent feature component will be used to generate the word wdi , and finally the word is generated from the same topic zd by the determined component. The generative process is summarized as:\nθ ∼ Dir(α) zd ∼ Cat(θ) φz ∼ Dir(β) sdi ∼ Ber(λ) wdi ∼ (1− sdi)Cat(φzd) + sdiCatE(τ zd ω >)"
  }, {
    "heading": "3.3 Inference in LF-LDA model",
    "text": "From the generative model of LF-LDA in Figure 2, by integrating out θ and φ, we use the Gibbs sampling algorithm (Robert and Casella, 2004) to perform inference to calculate the conditional topic assignment probabilities for each word. The outline of the Gibbs sampling algorithm for the LF-LDA model is detailed in Algorithm 1.\nAlgorithm 1: An approximate Gibbs sampling algorithm for the LF-LDA model\nInitialize the word-topic variables zdi using the LDA sampling algorithm for iteration iter = 1, 2, ... do for topic t = 1, 2, ..., T do\nτ t = arg maxτ t P(τ t | Z,S) for document d = 1, 2, ..., |D| do\nfor word index i = 1, 2, ..., Nd do sample zdi and sdi from P(zdi = t, sdi | Z¬di ,S¬di , τ ,ω)\nHere, S denotes the distribution indicator variables for the whole document collection D. Instead of sampling τ t from the posterior, we perform MAP estimation as described in the section 3.5.\nFor sampling the topic zdi and the binary indicator variable sdi of the i\nth word wdi in the document d, we integrate out sdi in order to sample zdi and then\nsample sdi given zdi . We sample the topic zdi using the conditional distribution as follows:\nP(zdi = t | Z¬di , τ ,ω) ∝ (N td¬i +K\nt d¬i + α)(\n(1− λ) N t,wdi ¬di + β\nN t¬di + V β + λCatE(wdi | τ t ω>) ) (4) Then we sample sdi conditional on zdi = t with:\nP(sdi=s | zdi=t) ∝  (1− λ)N t,wdi ¬di +β Nt¬di +V β for s = 0\nλ CatE(wdi |τ t ω>) for s = 1 (5)\nNotation: Due to the new models’ mixture architecture, we separate out the counts for each of the two components of each model. We define the rank3 tensor Kt,wd as the number of times a word w in document d is generated from topic t by the latent feature component of the generative LF-LDA or LFDMM model.\nWe also extend the earlier definition of the tensor N t,wd as the number of times a word w in document d is generated from topic t by the Dirichlet multinomial component of our combined models, which in section 3.3 refers to the LF-LDA model, while in section 3.4 refers to the LF-DMM model. For both tensors K and N , omitting an index refers to summation over that index and negation ¬ indicates exclusion as before. So Nwd +K w d is the total number of times the word type w appears in the document d."
  }, {
    "heading": "3.4 Inference in LF-DMM model",
    "text": "For the LF-DMM model, we integrate out θ and φ, and then sample the topic zd and the distribution selection variables sd for document d using Gibbs sampling as outlined in Algorithm 2.\nAlgorithm 2: An approximate Gibbs sampling algorithm for the LF-DMM model\nInitialize the word-topic variables zdi using the DMM sampling algorithm for iteration iter = 1, 2, ... do for topic t = 1, 2, ..., T do\nτ t = arg maxτ t P(τ t | Z,S) for document d = 1, 2, ..., |D| do\nsample zd and sd from P(zd = t, sd | Z¬d,S¬d, τ ,ω)\nAs before in Algorithm 1, we also use MAP estimation of τ as detailed in section 3.5 rather than\nsampling from the posterior. The conditional distribution of topic variable and selection variables for document d is:\nP(zd = t, sd | Z¬d,S¬d, τ ,ω)\n∝ λKd (1− λ)Nd (M t¬d + α) Γ(N t¬d + V β)\nΓ(N t¬d +Nd + V β)∏ w∈W Γ(N t,w¬d +N w d + β) Γ(N t,w¬d + β) ∏ w∈W CatE(w | τ t ω>)K w d (6)\nUnfortunately the ratios of Gamma functions makes it difficult to integrate out sd in this distribution P. As zd and sd are not independent, it is computationally expensive to directly sample from this distribution, as there are 2(N w d +K w d ) different values of sd. So we approximate P with a distribution Q that factorizes across words as follows:\nQ(zd = t, sd | Z¬d,S¬d, τ ,ω) ∝ λKd (1− λ)Nd (M t¬d + α) (7)∏\nw∈W\n( N t,w¬d + β\nN t¬d + V β )Nwd ∏ w∈W CatE(w | τ t ω>)K w d\nThis simpler distribution Q can be viewed as an approximation to P in which the topic-word “counts” are “frozen” within a document. This approximation is reasonably accurate for short documents. This distribution Q simplifies the coupling between zd and sd. This enables us to integrate out sd in Q. We first sample the document topic zd for document d using Q(zd), marginalizing over sd:\nQ(zd = t | Z¬d, τ ,ω)\n∝ (M t¬d + α) ∏ w∈W\n( (1− λ) N t,w ¬d +β\nNt¬d+V β\n+ λ CatE(w | τ t ω>)\n)(Nwd +Kwd ) (8)\nThen we sample the binary indicator variable sdi for each ith word wdi in document d conditional on zd = t from the following distribution:\nQ(sdi=s | zd = t) ∝\n{ (1− λ)N t,wdi ¬d +β\nNt¬d+V β for s = 0\nλ CatE(wdi | τ t ω>) for s = 1 (9)"
  }, {
    "heading": "3.5 Learning latent feature vectors for topics",
    "text": "To estimate the topic vectors after each Gibbs sampling iteration through the data, we apply regularized maximum likelihood estimation. Applying MAP estimation to learn log-linear models for topic models is also used in SAGE (Eisenstein et al., 2011) and SPRITE (Paul and Dredze, 2015). How-\never, unlike our models, those models do not use latent feature word vectors to characterize topic-word distributions. The negative log likelihood of the corpus L under our model factorizes topic-wise into factors Lt for each topic. With L2 regularization1 for topic t, these are:\nLt = − ∑ w∈W Kt,w ( τ t · ωw − log ( ∑ w′∈W exp(τ t · ωw′) ))\n+ µ ‖ τ t ‖22 (10)\nThe MAP estimate of topic vectors τ t is obtained by minimizing the regularized negative log likelihood. The derivative with respect to the jth element of the vector for topic t is: ∂Lt ∂τ t,j = − ∑ w∈W Kt,w ( ωw,j − ∑ w′∈W ωw′,jCatE(w ′ | τ tω>)\n) + 2µτ t,j (11)\nWe used L-BFGS2(Liu and Nocedal, 1989) to find the topic vector τ t that minimizes Lt."
  }, {
    "heading": "4 Experiments",
    "text": "To investigate the performance of our new LF-LDA and LF-DMM models, we compared their performance against baseline LDA and DMM models on topic coherence, document clustering and document classification evaluations. The topic coherence evaluation measures the coherence of topic-word associations, i.e. it directly evaluates how coherent the assignment of words to topics is. The document clustering and document classification tasks evaluate how useful the topics assigned to documents are in clustering and classification tasks.\nBecause we expect our new models to perform comparatively well in situations where there is little data about topic-to-word distributions, our experiments focus on corpora with few or short documents. We also investigated which values of λ perform well, and compared the performance when using two different sets of pre-trained word vectors in these new models."
  }, {
    "heading": "4.1 Experimental setup",
    "text": ""
  }, {
    "heading": "4.1.1 Distributed word representations",
    "text": "We experimented with two state-of-the-art sets of pre-trained word vectors here.\n1The L2 regularizer constant was set to µ = 0.01. 2We used the L-BFGS implementation from the Mallet\ntoolkit (McCallum, 2002).\nGoogle word vectors3 are pre-trained 300- dimensional vectors for 3 million words and phrases. These vectors were trained on a 100 billion word subset of the Google News corpus by using the Google Word2Vec toolkit (Mikolov et al., 2013). Stanford vectors4 are pre-trained 300-dimensional vectors for 2 million words. These vectors were learned from 42-billion tokens of Common Crawl web data using the Stanford GloVe toolkit (Pennington et al., 2014).\nWe refer to our LF-LDA and LF-DMM models using Google and Stanford word vectors as w2v-LDA, glove-LDA, w2v-DMM and glove-DMM."
  }, {
    "heading": "4.1.2 Experimental datasets",
    "text": "We conducted experiments on the 20-Newsgroups dataset, the TagMyNews news dataset and the Sanders Twitter corpus.\nThe 20-Newsgroups dataset5 contains about 19,000 newsgroup documents evenly grouped into 20 different categories. The TagMyNews news dataset6 (Vitale et al., 2012) consists of about 32,600 English RSS news items grouped into 7 categories, where each news document has a news title and a short description. In our experiments, we also used a news title dataset which consists of just the news titles from the TagMyNews news dataset.\nEach dataset was down-cased, and we removed non-alphabetic characters and stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We also removed words shorter than 3 characters and words appearing less than 10 times in the 20-Newsgroups corpus, and under 5 times in the TagMyNews news and news titles datasets. In addition, words not found in both Google and Stanford vector representations were also removed.7 We refer to the cleaned 20-Newsgroups, TagMyNews news\n3 Download at: https://code.google.com/p/word2vec/ 4 Download at: http://www-nlp.stanford.edu/projects/glove/ 5We used the “all-terms” version of the 20-Newsgroups dataset available at http://web.ist.utl.pt/acardoso/datasets/ (Cardoso-Cachopo, 2007).\n6The TagMyNews news dataset is unbalanced, where the largest category contains 8,200 news items while the smallest category contains about 1,800 items. Download at: http: //acube.di.unipi.it/tmn-dataset/\n71366, 27 and 12 words were correspondingly removed out of the 20-Newsgroups, TagMyNews news and news title datasets.\nand news title datasets as N20, TMN and TMNtitle, respectively.\nWe also performed experiments on two subsets of the N20 dataset. The N20short dataset consists of all documents from the N20 dataset with less than 21 words. The N20small dataset contains 400 documents consisting of 20 randomly selected documents from each group of the N20 dataset.\nFinally, we also experimented on the publicly available Sanders Twitter corpus.8 This corpus consists of 5,512 Tweets grouped into four different topics (Apple, Google, Microsoft, and Twitter). Due to restrictions in Twitter’s Terms of Service, the actual Tweets need to be downloaded using 5,512 Tweet IDs. There are 850 Tweets not available to download. After removing the non-English Tweets, 3,115 Tweets remain. In addition to converting into lowercase and removing non-alphabetic characters, words were normalized by using a lexical normalization dictionary for microblogs (Han et al., 2012). We then removed stop-words, words shorter than 3 characters or appearing less than 3 times in the corpus. The four words apple, google, microsoft and twitter were removed as these four words occur in every Tweet in the corresponding topic. Moreover, words not found in both Google and Stanford vector lists were also removed.9 In all our experiments, after removing words from documents, any document with a zero word count was also removed from the corpus. For the Twitter corpus, this resulted in just 2,520 remaining Tweets."
  }, {
    "heading": "4.1.3 General settings",
    "text": "The hyper-parameter β used in baseline LDA and DMM models was set to 0.01, as this is a common setting in the literature (Griffiths and Steyvers,\n8Download at: http://www.sananalytics.com/lab/index.php 9There are 91 removed words.\n2004). We set the hyper-parameter α = 0.1, as this can improve performance relative to the standard setting α = 50T , as noted by Lu et al. (2011) and Yin and Wang (2014).\nWe ran each baseline model for 2000 iterations and evaluated the topics assigned to words in the last sample. For our models, we ran the baseline models for 1500 iterations, then used the outputs from the last sample to initialize our models, which we ran for 500 further iterations.\nWe report the mean and standard deviation of the results of ten repetitions of each experiment (so the standard deviation is approximately 3 standard errors, or a 99% confidence interval)."
  }, {
    "heading": "4.2 Topic coherence evaluation",
    "text": "This section examines the quality of the topic-word mappings induced by our models. In our models, topics are distributions over words. The topic coherence evaluation measures to what extent the highprobability words in each topic are semantically coherent (Chang et al., 2009; Stevens et al., 2012)."
  }, {
    "heading": "4.2.1 Quantitative analysis",
    "text": "Newman et al. (2010), Mimno et al. (2011) and Lau et al. (2014) describe methods for automatically evaluating the semantic coherence of sets of words. The method presented in Lau et al. (2014) uses the normalized pointwise mutual information (NPMI) score and has a strong correlation with humanjudged coherence. A higher NPMI score indicates that the topic distributions are semantically more coherent. Given a topic t represented by its top-N topic words w1, w2, ..., wN , the NPMI score for t is:\nNPMI-Score(t) = ∑\n16i<j6N\nlog P(wi,wj)\nP(wi)P(wj)\n− log P(wi, wj) (12)\nwhere the probabilities in equation (12) are derived from a 10-word sliding window over an external corpus.\nThe NPMI score for a topic model is the average score for all topics. We compute the NPMI score based on top-15 most probable words of each topic and use the English Wikipedia10 of 4.6 million articles as our external corpus.\nFigures 3 and 4 show NPMI scores computed for the LDA, w2v-LDA and glove-LDA models on the\n10We used the Wikipedia-articles dump of July 8, 2014.\nN20short dataset for 20 and 40 topics. We see that λ = 1.0 gives the highest NPMI score. In other words, using only the latent feature model produces the most coherent topic distributions.\nTables 2, 3 and 4 present the NPMI scores produced by the models on the other experimental datasets, where we vary11 the number of topics in steps from 4 to 80. Tables 3 and 4 show that the DMM model performs better than the LDA model on\n11 We perform with T = 6 on the N20 and N20small datasets as the 20-Newsgroups dataset could be also grouped into 6 larger topics instead of 20 fine-grained categories.\nthe TMN, TMNtitle and Twitter datasets. These results show that our latent feature models produce significantly higher scores than the baseline models on all the experimental datasets.\nGoogle word2vec vs. Stanford glove word vectors: In general, our latent feature models obtain competitive NPMI results in using pre-trained Google word2vec and Stanford glove word vectors for a large value of T , for example T = 80. With small values of T , for example T ≤ 7 , using Google word vectors produces better scores than using Stanford word vectors on the small N20small dataset of normal texts and on the short text TMN and TMNtitle datasets. However, the opposite pattern holds on the full N20 dataset. Both sets of the pre-trained word vectors produce similar scores on the small and short Twitter dataset."
  }, {
    "heading": "4.2.2 Qualitative analysis",
    "text": "This section provides an example of how our models improve topic coherence. Table 5 compares the top15 words12 produced by the baseline DMM model\n12In the baseline model, the top-15 topical words output from the 1500th sample are similar to top-15 words from the 2000th\nand our w2v-DMM model with λ = 1.0 on the TMNtitle dataset with T = 20 topics.\nIn table 5, topic 1 of the DMM model consists of words related to “nuclear crisis in Japan” together with other unrelated words. The w2v-DMM model produced a purer topic 1 focused on “Japan earthquake and nuclear crisis,” presumably related to the “Fukushima Daiichi nuclear disaster.” Topic 3 is about “oil prices” in both models. However, all top15 words are qualitatively more coherent in the w2vDMM model. While topic 4 of the DMM model is difficult to manually label, topic 4 of the w2v-DMM model is about the “Arab Spring” event.\nTopics 5, 19 and 14 of the DMM model are not easy to label. Topic 5 relates to “entertainment”, topic 19 is generally a mixture of “entertainment” and “sport”, and topic 14 is about “sport” and “politics.” However, the w2v-DMM model more clearly distinguishes these topics: topic 5 is about “entertainment”, topic 19 is only about “sport” and topic 14 is only about “politics.”"
  }, {
    "heading": "4.3 Document clustering evaluation",
    "text": "We compared our models to the baseline models in a document clustering task. After using a topic model to calculate the topic probabilities of a document, we assign every document the topic with the highest probability given the document (Cai et al., 2008; Lu et al., 2011; Xie and Xing, 2013; Yan et al., 2013). We use two common metrics to evaluate clustering performance: Purity and normalized mutual information (NMI): see (Manning et al., 2008, Section 16.3) for details of these evaluations. Purity and NMI scores always range from 0.0 to 1.0, and higher scores reflect better clustering performance.\nFigures 5 and 6 present Purity and NMI results obtained by the LDA, w2v-LDA and glove-LDA models on the N20short dataset with the numbers of topics T set to either 20 or 40, and the value of the mixture weight λ varied from 0.0 to 1.0.\nWe found that setting λ to 1.0 (i.e. using only the latent features to model words), the glove-LDA produced 1%+ higher scores on both Purity and NMI results than the w2v-LDA when using 20 topics. However, the two models glove-LDA and w2v-LDA returned equivalent results with 40 topics where they\nsample if we do not take the order of the most probable words into account.\ngain 2%+ absolute improvement13 on the two Purity and NMI against the baseline LDA model.\nBy varying λ, as shown in Figures 5 and 6, the w2v-LDA and glove-LDA models obtain their best results at λ = 0.6 where the w2v-LDA model does slightly better than the glove-LDA. Both models sig-\n13Using the Student’s t-Test, the improvement is significant (p < 0.01).\nnificantly outperform their baseline LDA models; for example with 40 topics, the w2v-LDA model attains 4.4% and 4.3% over the LDA model on Purity and NMI metrics, respectively.\nWe fix the mixture weight λ at 0.6, and report experimental results based on this value for the rest of this section. Tables 6, 7 and 8 show clustering results produced by our models and the baseline models on the remaining datasets with different numbers\nof topics. As expected, the DMM model is better than the LDA model on the short datasets of TMN, TMNtitle and Twitter. For example with 80 topics on the TMNtitle dataset, the DMM achieves about 7+% higher Purity and NMI scores than LDA.\nNew models vs. baseline models: On most tests, our models score higher than the baseline models, particularly on the small N20small dataset where we get 6.0% improvement on NMI at T = 6, and on the short text TMN and TMNtitle datasets we obtain 6.1% and 2.5% higher Purity at T = 80. In addition, on the short and small Twitter dataset with T = 4, we achieve 3.9% and 5.3% improvements in Purity and NMI scores, respectively. Those results show that an improved model of topic-word mappings also\nimproves the document-topic assignments. For the small value of T ≤ 7, on the large datasets of N20, TMN and TMNtitle, our models and baseline models obtain similar clustering results. However, with higher values of T , our models perform better than the baselines on the short TMN and TMNtitle datasets, while on the N20 dataset, the baseline LDA model attains a slightly higher clustering results than ours. In contrast, on the short and small Twitter dataset, our models obtain considerably better clustering results than the baseline models with a small value of T .\nGoogle word2vec vs. Stanford glove word vectors: On the small N20short and N20small datasets, using the Google pre-trained word vectors produces\nhigher clustering scores than using Stanford pretrained word vectors. However, on the large datasets N20, TMN and TMNtitle, using Stanford word vectors produces higher scores than using Google word vectors when using a smaller number of topics, for example T ≤ 20. With more topics, for instance T = 80, the pre-trained Google and Stanford word vectors produce similar clustering results. In addition, on the Twitter dataset, both sets of pre-trained word vectors produce similar results."
  }, {
    "heading": "4.4 Document classification evaluation",
    "text": "Unlike the document clustering task, the document classification task evaluates the distribution over topics for each document. Following Lacoste-Julien et al. (2009), Lu et al. (2011), Huh and Fienberg (2012) and Zhai and Boyd-graber (2013), we used Support Vector Machines (SVM) to predict the ground truth labels from the topic-proportion vector of each document. We used the WEKA’s implementation (Hall et al., 2009) of the fast Sequential Minimal Optimization algorithm (Platt, 1999) for learning a classifier with ten-fold cross-validation and WEKA’s default parameters. We present the macroaveraged F1 score (Manning et al., 2008, Section 13.6) as the evaluation metric for this task.\nJust as in the document clustering task, the mixture weight λ = 0.6 obtains the highest classification performances on the N20short dataset. For example with T = 40, our w2v-LDA and gloveLDA obtain F1 scores at 40.0% and 38.9% which are 4.5% and 3.4% higher than F1 score at 35.5% obtained by the LDA model, respectively.\nWe report classification results on the remaining experimental datasets with mixture weight λ = 0.6 in tables 9, 10 and 11. Unlike the clustering results, the LDA model does better than the DMM model for classification on the TMN dataset.\nNew models vs. baseline models: On most eval-\nuations, our models perform better than the baseline models. In particular, on the small N20small and Twitter datasets, when the number of topics T is equal to number of ground truth labels (i.e. 20 and 4 correspondingly), our w2v-LDA obtains 5+% higher F1 score than the LDA model. In addition, our w2v-DMM model achieves 5.4% and 2.9% higher F1 score than the DMM model on short TMN and TMNtitle datasets with T = 80, respectively.\nGoogle word2vec vs. Stanford glove word vectors: The comparison of the Google and Stanford pre-trained word vectors for classification is similar to the one for clustering."
  }, {
    "heading": "4.5 Discussion",
    "text": "We found that the topic coherence evaluation produced the best results with a mixture weight λ = 1, which corresponds to using topic-word distributions defined in terms of the latent-feature word vectors. This is not surprising, since the topic coherence evaluation we used (Lau et al., 2014) is based on word co-occurrences in an external corpus (here, Wikipedia), and it is reasonable that the billion-word corpora used to train the latent feature word vectors are more useful for this task than the much smaller topic-modeling corpora, from which the topic-word multinomial distributions are trained.\nOn the other hand, the document clustering and document classification tasks depend more strongly on possibly idiosyncratic properties of the smaller topic-modeling corpora, since these evaluations reflect how well the document-topic assignments can group or distinguish documents within the topicmodeling corpus. Smaller values of λ enable the models to learn topic-word distributions that include an arbitrary multinomial topic-word distribution, enabling the models to capture idiosyncratic properties of the topic-modeling corpus. Even in these evaluations we found that an intermediate value of λ = 0.6 produced the best results, indicating that better word-topic distributions were produced when information from the large external corpus is combined with corpus-specific topic-word multinomials. We found that using the latent feature word vectors produced significant performance improvements even when the domain of the topic-modeling corpus was quite different to that of the external corpus from which the word vectors were derived, as was the case in our experiments on Twitter data.\nWe found that using either the Google or the Stanford latent feature word vectors produced very similar results. As far as we could tell, there is no reason to prefer either one of these in our topic modeling applications."
  }, {
    "heading": "5 Conclusion and future work",
    "text": "In this paper, we have shown that latent feature representations can be used to improve topic models. We proposed two novel latent feature topic models, namely LF-LDA and LF-DMM, that integrate a latent feature model within two topic models LDA and DMM. We compared the performance of our models LF-LDA and LF-DMM to the baseline LDA and DMM models on topic coherence, document clustering and document classification evaluations. In the topic coherence evaluation, our model outperformed the baseline models on all 6 experimental datasets, showing that our method for exploiting external information from very large corpora helps improve the topic-to-word mapping. Meanwhile, document clustering and document classification results show that our models improve the document-topic assignments compared to the baseline models, especially on datasets with few or short documents.\nAs an anonymous reviewer suggested, it would be interesting to identify exactly how the latent feature word vectors improve topic modeling performance. We believe that they provide useful information about word meaning extracted from the large corpora that they are trained on, but as the reviewer suggested, it is possible that the performance improvements arise because the word vectors are trained on context windows of size 5 or 10, while the LDA and DMM models view documents as bags of words, and effectively use a context window that encompasses the entire document. In preliminary experiments where we train latent feature word vectors from the topic-modeling corpus alone using context windows of size 10 we found that performance was degraded relative to the results presented here, suggesting that the use of a context window alone is not responsible for the performance improvements we reported here. Clearly it would be valuable to investigate this further.\nIn order to use a Gibbs sampler in section 3.4, the conditional distributions needed to be distributions we can sample from cheaply, which is not the case for the ratios of Gamma functions. While we used a simple approximation, it is worth exploring other sampling techniques that can avoid approximations, such as Metropolis-Hastings sampling (Bishop, 2006, Section 11.2.2).\nIn order to compare the pre-trained Google and Stanford word vectors, we excluded words that did not appear in both sets of vectors. As suggested by anonymous reviewers, it would be interesting to learn vectors for these unseen words. In addition, it is worth fine-tuning the seen-word vectors on the dataset of interest.\nAlthough we have not evaluated our approach on very large corpora, the corpora we have evaluated on do vary in size, and we showed that the gains from our approach are greatest when the corpora are small. A drawback of our approach is that it is slow on very large corpora. Variational Bayesian inference may provide an efficient solution to this problem (Jordan et al., 1999; Blei et al., 2003)."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was supported by a Google award through the Natural Language Understanding\nFocused Program, and under the Australian Research Council’s Discovery Projects funding scheme (project numbers DP110102506 and DP110102593). The authors would like to thank the three anonymous reviewers, the action editor and Dr. John Pate at the Macquarie University, Australia for helpful comments and suggestions."
  }],
  "year": 2018,
  "references": [{
    "title": "Pattern Recognition and Machine Learning (Information Science and Statistics)",
    "authors": ["Christopher M. Bishop."],
    "venue": "Springer-Verlag New York, Inc.",
    "year": 2006
  }, {
    "title": "Latent Dirichlet Allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Probabilistic Topic Models",
    "authors": ["David M. Blei."],
    "venue": "Communications of the ACM, 55(4):77–84.",
    "year": 2012
  }, {
    "title": "Extracting semantic representations from word co-occurrence statistics: A computational study",
    "authors": ["John A. Bullinaria", "Joseph P. Levy."],
    "venue": "Behavior Research Methods, 39(3):510–526.",
    "year": 2007
  }, {
    "title": "Modeling Hidden Topics on Document Manifold",
    "authors": ["Deng Cai", "Qiaozhu Mei", "Jiawei Han", "Chengxiang Zhai."],
    "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, pages 911–920.",
    "year": 2008
  }, {
    "title": "A Novel Neural Topic Model and Its Supervised Extension",
    "authors": ["Ziqiang Cao", "Sujian Li", "Yang Liu", "Wenjie Li", "Heng Ji."],
    "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 2210–2216.",
    "year": 2015
  }, {
    "title": "Improving Methods for Single-label Text Categorization",
    "authors": ["Ana Cardoso-Cachopo."],
    "venue": "PhD Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.",
    "year": 2007
  }, {
    "title": "Reading Tea Leaves: How Humans Interpret Topic Models",
    "authors": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-graber", "David M. Blei."],
    "venue": "Advances in Neural Information Processing Systems 22, pages 288–296.",
    "year": 2009
  }, {
    "title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 25th International Conference on Machine Learning, pages 160–167.",
    "year": 2008
  }, {
    "title": "Indexing by Latent Semantic Analysis",
    "authors": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."],
    "venue": "Journal of the American Society for Information Science, 41(6):391– 407.",
    "year": 1990
  }, {
    "title": "Sparse Additive Generative Models of Text",
    "authors": ["Jacob Eisenstein", "Amr Ahmed", "Eric Xing."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 1041–1048.",
    "year": 2011
  }, {
    "title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 513–520.",
    "year": 2011
  }, {
    "title": "Finding scientific topics",
    "authors": ["Thomas L. Griffiths", "Mark Steyvers."],
    "venue": "Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.",
    "year": 2004
  }, {
    "title": "The WEKA Data Mining Software: An Update",
    "authors": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten."],
    "venue": "ACM SIGKDD Explorations Newsletter, 11(1):10–18.",
    "year": 2009
  }, {
    "title": "Automatically Constructing a Normalisation Dictionary for Microblogs",
    "authors": ["Bo Han", "Paul Cook", "Timothy Baldwin."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-",
    "year": 2012
  }, {
    "title": "Document Classification by Topic Labeling",
    "authors": ["Swapnil Hingmire", "Sandeep Chougule", "Girish K. Palshikar", "Sutanu Chakraborti."],
    "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,",
    "year": 2013
  }, {
    "title": "Empirical Study of Topic Modeling in Twitter",
    "authors": ["Liangjie Hong", "Brian D. Davison."],
    "venue": "Proceedings of the First Workshop on Social Media Analytics, pages 80–88.",
    "year": 2010
  }, {
    "title": "Discriminative Topic Modeling Based on Manifold Learning",
    "authors": ["Seungil Huh", "Stephen E. Fienberg."],
    "venue": "ACM Transactions on Knowledge Discovery from Data, 5(4):20:1–20:25.",
    "year": 2012
  }, {
    "title": "PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names",
    "authors": ["Mark Johnson."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157.",
    "year": 2010
  }, {
    "title": "An Introduction to Variational Methods for Graphical Models",
    "authors": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul."],
    "venue": "Machine Learning, 37(2):183–233.",
    "year": 1999
  }, {
    "title": "DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification",
    "authors": ["Simon Lacoste-Julien", "Fei Sha", "Michael I. Jordan."],
    "venue": "Advances in Neural Information Processing Systems 21, pages 897–904.",
    "year": 2009
  }, {
    "title": "Machine Reading Tea Leaves: Automatically",
    "authors": ["Han Jey Lau", "David Newman", "Timothy Baldwin"],
    "year": 2014
  }, {
    "title": "On the Limited Memory BFGS Method for Large Scale Optimization",
    "authors": ["D.C. Liu", "J. Nocedal."],
    "venue": "Mathematical Programming, 45(3):503–528.",
    "year": 1989
  }, {
    "title": "Topical Word Embeddings",
    "authors": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun."],
    "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 2418–2424.",
    "year": 2015
  }, {
    "title": "Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA",
    "authors": ["Yue Lu", "Qiaozhu Mei", "ChengXiang Zhai."],
    "venue": "Information Retrieval, 14:178–203.",
    "year": 2011
  }, {
    "title": "Producing high-dimensional semantic spaces from lexical cooccurrence",
    "authors": ["Kevin Lund", "Curt Burgess."],
    "venue": "Behavior Research Methods, Instruments, & Computers, 28(2):203–208.",
    "year": 1996
  }, {
    "title": "Introduction to Information Retrieval",
    "authors": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze."],
    "venue": "Cambridge University Press.",
    "year": 2008
  }, {
    "title": "MALLET: A Machine Learning for Language Toolkit",
    "authors": ["Andrew McCallum"],
    "year": 2002
  }, {
    "title": "Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling",
    "authors": ["Rishabh Mehrotra", "Scott Sanner", "Wray Buntine", "Lexing Xie."],
    "venue": "Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Informa-",
    "year": 2013
  }, {
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems 26, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Optimizing Semantic Coherence in Topic Models",
    "authors": ["David Mimno", "Hanna M. Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262–",
    "year": 2011
  }, {
    "title": "Statistical Entity-Topic Models",
    "authors": ["David Newman", "Chaitanya Chemudugunta", "Padhraic Smyth."],
    "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 680–686.",
    "year": 2006
  }, {
    "title": "Automatic Evaluation of Topic Coherence",
    "authors": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages",
    "year": 2010
  }, {
    "title": "Text Classification from Labeled and Unlabeled",
    "authors": ["Kamal Nigam", "AK McCallum", "S Thrun", "T Mitchell"],
    "year": 2000
  }, {
    "title": "SPRITE: Generalizing Topic Models with Structured Priors",
    "authors": ["Michael Paul", "Mark Dredze."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:43–57.",
    "year": 2015
  }, {
    "title": "Glove: Global Vectors for Word Representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Word Features for Latent Dirichlet Allocation",
    "authors": ["James Petterson", "Wray Buntine", "Shravan M. Narayanamurthy", "Tibério S. Caetano", "Alex J. Smola."],
    "venue": "Advances in Neural Information Processing Systems 23, pages 1921–1929.",
    "year": 2010
  }, {
    "title": "A Hidden Topic-Based Framework Toward Building Applications with Short Web Documents",
    "authors": ["Xuan-Hieu Phan", "Cam-Tu Nguyen", "Dieu-Thu Le", "LeMinh Nguyen", "Susumu Horiguchi", "Quang-Thuy Ha."],
    "venue": "IEEE Transactions on Knowledge and Data",
    "year": 2011
  }, {
    "title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization",
    "authors": ["John C. Platt."],
    "venue": "Bernhard Schölkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in kernel methods, pages 185–208.",
    "year": 1999
  }, {
    "title": "Fast Collapsed Gibbs Sampling for Latent Dirichlet Allocation",
    "authors": ["Ian Porteous", "David Newman", "Alexander Ihler", "Arthur Asuncion", "Padhraic Smyth", "Max Welling."],
    "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and",
    "year": 2008
  }, {
    "title": "Monte Carlo Statistical Methods (Springer Texts in Statistics)",
    "authors": ["Christian P. Robert", "George Casella."],
    "venue": "Springer-Verlag New York, Inc.",
    "year": 2004
  }, {
    "title": "A Webbased Kernel Function for Measuring the Similarity of Short Text Snippets",
    "authors": ["Mehran Sahami", "Timothy D. Heilman."],
    "venue": "Proceedings of the 15th International Conference on World Wide Web, pages 377– 386.",
    "year": 2006
  }, {
    "title": "Replicated Softmax: an Undirected Topic Model",
    "authors": ["Ruslan Salakhutdinov", "Geoffrey Hinton."],
    "venue": "Advances in Neural Information Processing Systems 22, pages 1607–1614.",
    "year": 2009
  }, {
    "title": "Parsing with Compositional Vector Grammars",
    "authors": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465.",
    "year": 2013
  }, {
    "title": "Modeling Documents with a Deep",
    "authors": ["Nitish Srivastava", "Ruslan Salakhutdinov", "Geoffrey Hinton"],
    "year": 2013
  }, {
    "title": "Exploring Topic Coherence over Many Models and Many Topics",
    "authors": ["Keith Stevens", "Philip Kegelmeyer", "David Andrzejewski", "David Buttler."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
    "year": 2012
  }, {
    "title": "A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation",
    "authors": ["Yee W Teh", "David Newman", "Max Welling."],
    "venue": "Advances in Neural Information Processing Systems 19, pages 1353–1360.",
    "year": 2006
  }, {
    "title": "A Bayesian LDA-based Model for Semi-Supervised Part-of-speech Tagging",
    "authors": ["Kristina Toutanova", "Mark Johnson."],
    "venue": "Advances in Neural Information Processing Systems 20, pages 1521–1528.",
    "year": 2008
  }, {
    "title": "Classification of Short Texts by Deploying Topical Annotations",
    "authors": ["Daniele Vitale", "Paolo Ferragina", "Ugo Scaiella."],
    "venue": "Proceedings of the 34th European Conference on Advances in Information Retrieval, pages 376–387.",
    "year": 2012
  }, {
    "title": "TwitterRank: Finding Topic-sensitive Influential Twitterers",
    "authors": ["Jianshu Weng", "Ee-Peng Lim", "Jing Jiang", "Qi He."],
    "venue": "Proceedings of the Third ACM International Conference on Web Search and Data Mining, pages 261–270.",
    "year": 2010
  }, {
    "title": "Integrating Document Clustering and Topic Modeling",
    "authors": ["Pengtao Xie", "Eric P. Xing."],
    "venue": "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pages 694–703.",
    "year": 2013
  }, {
    "title": "A Biterm Topic Model for Short Texts",
    "authors": ["Xiaohui Yan", "Jiafeng Guo", "Yanyan Lan", "Xueqi Cheng."],
    "venue": "Proceedings of the 22Nd International Conference on World Wide Web, pages 1445–1456.",
    "year": 2013
  }, {
    "title": "A Dirichlet Multinomial Mixture Model-based Approach for Short Text Clustering",
    "authors": ["Jianhua Yin", "Jianyong Wang."],
    "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 233–242.",
    "year": 2014
  }, {
    "title": "Online Latent Dirichlet Allocation with Infinite Vocabulary",
    "authors": ["Ke Zhai", "Jordan L. Boyd-graber."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning, pages 561–569.",
    "year": 2013
  }, {
    "title": "Comparing Twitter and Traditional Media Using Topic Models",
    "authors": ["Wayne Xin Zhao", "Jing Jiang", "Jianshu Weng", "Jing He", "Ee-Peng Lim", "Hongfei Yan", "Xiaoming Li."],
    "venue": "Proceedings of the 33rd European Conference on Advances in Information Retrieval, pages",
    "year": 2011
  }],
  "id": "SP:17c6752767e3e0f438e0fa5f4114a716c8b05132",
  "authors": [{
    "name": "Dat Quoc Nguyen",
    "affiliations": []
  }, {
    "name": "Richard Billingsley",
    "affiliations": []
  }, {
    "name": "Lan Du",
    "affiliations": []
  }, {
    "name": "Mark Johnson",
    "affiliations": []
  }],
  "abstractText": "Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.",
  "title": "Improving Topic Models with Latent Feature Word Representations"
}