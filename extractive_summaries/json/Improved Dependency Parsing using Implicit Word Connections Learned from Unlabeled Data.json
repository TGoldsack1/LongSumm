{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2857–2863 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2857\nPre-trained word embeddings and language model have been shown useful in a lot of tasks. However, both of them cannot directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unlabeled data by a word ordering model with selfattention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our model gets state-of-the-art performance on the English PTB dataset, achieving 96.35% UAS and 95.25% LAS."
  }, {
    "heading": "1 Introduction",
    "text": "Dependency parsing is a fundamental task for language processing which aims to establish syntactic relations between words in a sentence. Graphbased models (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) and transition-based models (Nivre, 2008; Zhang and Nivre, 2011) are the most successful solutions to the challenge.\nRecently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and\nCharniak, 2016) also show promising parsing performance.\nDifferent from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way have noise1 and tend to be short sentences, since it is easier for different parsers to get consistent results.\nPre-trained neural networks are another methods to take advantage of unlabeled data. Pretrained word embeddings (Mikolov et al., 2013) and language model (Józefowicz et al., 2016; Peters et al., 2017, 2018) have been shown useful in modelling NLP tasks since word embeddings could capture word semantic information and language model could capture contextual information at the sentence level. However, connections between words in the sentence cannot be directly captured by word embeddings or language model, which are crucial for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly model word connections by a word ordering model. The purpose of word ordering model is to generate a well-formed sentence given a bag of words. We human could make sentences easily from unordered words since we have syntactic knowledge, thus a model generating wellformed sentences from the bag of words encodes syntactic information. In addition, word ordering task allows us to use self-attention mechanism\n1The tri-training approach accuracy on the tune set is 97.26% UAS (Weiss et al., 2015).\nto model connections between words in the sentence. Different from the tri-training approach, our approach takes advantage of implicit word connections learned by self-attended word ordering model in an unsupervised way.\nExperiments show that pre-trained word ordering model significantly improves our dependency parsing model. Ablation tests also show self-attention mechanism is critical. Moreover, by combining word ordering model and language model, our graph-based dependency parsing model achieves SOTA performance on the English Penn Treebank (Marcus et al., 1993) with 96.35% UAS and 95.25% LAS."
  }, {
    "heading": "2 Neural Word Ordering Model",
    "text": "The target of word ordering is to generate a wellformed sentence given a bag of words. To capture word connections implicated in the sentence, an LSTM-based word ordering model with selfattention is proposed. Self-attention mechanism effectively decides which words in the word bag are more important in generating the next word. It improves the ability of our model to capture word connections. As illustrated in Figure 1, the proposed word ordering model consists of two layers:\nEncoder Layer Given a bag of words w1, w2, ..., wn, we encode each word by a character-level BiLSTM (cwow1:n), which could reduce the parameters used in our model compared with word embeddings. For the input word of current time-step (wi), a selfattention layer is utilized to align the word with its\nrelated words, producing its self-attended vector (sawowi ) as following:\nsij = v TReLU(W sa[cwowi ; c wo wj ]) (1)\nait = exp(s i t)/Σ n j=1exp(s i j) (2)\nsawowi = Σ n j=1a i jc wo wj (3)\nThe scores si1:n in self-attention explicitly represent the connections between words.\nWe then concatenate the character-level word embedding (cwowi ) and its self-attended vector (sawowi ):\nxwoi = [c wo wi ; sa wo wi ] (4)\nxwoi is fed into the decoder layer to generate the next word.\nDecoder Layer Given the current input vector (xwoi ), which contains current word information and weighted information of related words, a forward LSTM is used to generate the next word. We initialize the forward LSTM with an average of the input word embeddings (cwowi:n). A virtual token 〈BOS〉 is added as the input of the first LSTM time-step:\n−→ h woi = LSTM(x wo i , −→ h woi−1) (5)\nAt each time-step, the hidden state −→ h woi is utilized to predict the next word. Due to the output vocabulary is limited in the bag of words, we just compute scores for the given words (w1:n):\nsoij = v TReLU(W o[ −→ h woi ; cd wo wj ])+ −→ h woi T Mwdwowj\n(6)\nTo reduce the parameters, each output word is represented by a character-level BiLSTM embedding (cdwowj ) and a low-dimensional word embedding2 (wdwowj ). M is a matrix projecting a lowdimensional embedding back up to the dimensionality of LSTM hidden states. The scores soi1:n are then normalized with Softmax, and the word with max probability is chosen as the next token.\nThe word ordering model could be trained easily in an unsupervised manner. Given a large set of unlabeled sentences, we can just ignore the word\n2Character-level representations lack the capacity to differentiate between words that have very different meanings but that are spelled similarly. Low-dimensional word embeddings are added to improve the ability.\norder of sentence and train the model to generate the corresponding well-formed sentence in the training set. To be specific, we minimize the sum of negative log probabilities of the ground truth words on the unlabeled data set. Different from language model, the choice for each decoder step is limited in the bag of words. Moreover, selfattention can be introduced into the word ordering model since we have known the bag of words, which could capture the dependency connections between words. We also pre-train a backward word ordering model to generate sentences in reverse order. The forward and backward models share character-level BiLSTM embeddings, selfattention layer, and Softmax layer.\nDifferent from previous word ordering models (Liu et al., 2015; Schmaltz et al., 2016), selfattention mechanism is introduced into our model to capture word connections. Moreover, our more important goal is to implicitly utilize large-scale unlabeled data to help dependency parsing."
  }, {
    "heading": "3 Neural Graph-based Parsing Model",
    "text": "We implement an LSTM-based neural network model as our graph-based dependency parsing baseline, which is similar to (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016). As shown in the Figure 2, it consists of three layers:\nInput Layer Given a n-words input sentence s with words w1, w2, ..., wn and its POS tags p1, p2, ..., pn. The input layer creates a sequence of input vectors\nx1:n in which each xi is a concatenation of its word embedding (ewi), POS tag embedding (epi), character-level BiLSTM embedding (cwi), and word ordering model pre-trained vector (wowi):\nxi = [ewi ; epi ; cwi ;wowi ] (7)\nTo get the word ordering model pre-trained vector (wowi), the sentence s is fed into the pretrained word ordering model. Following Peters et al. (2018), we then combine the input vector (xwoi =[c wo wi ; sa wo wi ]) and L-layer BiLSTM vectors (hwoi,j =[ −→ h woi,j ; ←− h woi,j ] | j=1, 2, ..., L) by a Softmaxnormalized weight (Wwoc) and a scalar parameter (γ):\nwowi = γ(W woc 0 x wo i +Σ L j=1W woc j h wo i,j ) (8)\nThe parameters of word ordering model are fixed during the training of parsing model. However, the weight and scalar parameters are tuned to better adapt to it. The combined outputwowi contains word connections from self-attention, word information from character-level embedding and contextual information from LSTM.\nEncoder & Output Layer To introduce more contextual information, we encode each input element by deep BiLSTMs:\nvi = BiLSTM(x1:n, i) (9)\nTwo different highway networks (Srivastava et al., 2015) are then used to encode head word representations (vhead1:n ) and dependent word representations (vdep1:n ). For a head-dependent dependency pair (wh, wd), the dependency arc and label score are computed by two MLP networks:\nih,d = [v head h ; v dep d ; v head h v dep d ] (10)\nsarch,d = W arc 1 ReLU(W arc 2 ih,d) (11)\nslabelh,d = W label 1 ReLU(W label 2 ih,d) (12)\nWe use the Max-Margin criterion to train our parsing model, which is the same as (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016)."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Datasets",
    "text": "We conduct experiments on the English Penn Treebank and the CoNLL 09 English dataset. For\nPTB dataset, we follow the standard splits. Using section 2-21 for training, section 22 as development set and 23 as test set. The treebank is converted to Stanford Basic Dependencies (Marneffe et al., 2006) by version 3.3.03 of the Stanford parser. The Stanford POS Tagger (Toutanova et al., 2003) is used for assigning POS tags. Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation. For the CoNLL 09 English dataset, we follow the standard practice and include all punctuation in the evaluation. We pre-train our word ordering model on the 1 billion word benchmark (Chelba et al., 2014)."
  }, {
    "heading": "4.2 Implementation Details",
    "text": "The graph-based dependency parsing model and word ordering model are optimized with Adam with an initial learning rate of 2e−3. The β1 and β2 used in Adam are 0.9 and 0.999 respectively.\nThe following hyper-parameters are used in all graph-based dependency parsing models: word embedding size = 300, POS tag embedding size = 32, character embedding size = 50, word-level LSTM hidden vector size = 200, word-level BiLSTM layer number = 3, character-level LSTM hidden vector size = 50, character-level BiLSTM layer number = 2, batch size = 32. We also apply dropout for the input and each layer with dropout rate of 0.3. We use pre-trained casesensitive GloVe embeddings4 to initialize word embeddings. These word embeddings are fine\n3http://nlp.stanford.edu/software/ lex-parser.shtml\n4Downloaded from http://nlp.stanford.edu/\ntuned with the graph-based dependency parsing model. The parameters of pre-trained word ordering model are fixed during the training of dependency parsing model. For deep BiLSTM, we concatenate the outputs of each layer as its final outputs.\nFor our word ordering model: input characterlevel LSTM hidden vector size = 512, input character-level BiLSTM layer number = 1, wordlevel LSTM hidden vector size = 1024, word-level LSTM layer number = 2, output character-level LSTM hidden vector size = 512, output characterlevel BiLSTM layer number = 1, output lowdimensional word embedding size = 64, batch size = 32, dropout for the input and each layer = 0.5."
  }, {
    "heading": "4.3 Main Results & Ablation Study",
    "text": "Table 1 shows the performance of our model and previous work on two English benchmarks. Our model achieves promising results on both datasets. Two sets of experiments are provided to show the effectiveness of pre-trained word ordering model. Although our baseline system is similar to (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) but with subtle differences in architecture, the baseline could perform much better to our surprise and thus constitutes a very strong baseline. Compared with this baseline, introducing the pre-trained word ordering model achieves a significant improvement (almost 0.6% UAS gains for both datasets, p < 0.001). To further show the effectiveness of word ordering model, we also implement an even stronger baseline with pretrained language model5. Compared with this much stronger baseline, incorporating pre-trained word ordering model still achieves a significant improvement (0.3% UAS gains for both datasets, p < 0.01). We attribute the improvement to the ability of word ordering model to capture word connections, which cannot be directly captured by language model. Moreover, by combining with a pre-trained language model, our model outperforms current SOTA model from 95.9% UAS to 96.35% UAS on the PTB dataset. The introduction\ndata/glove.840B.300d.zip. 5We use the pre-trained language model provided by Peters et al. (2018), which can be downloaded at http:// allennlp.org/elmo. The pre-trained language model vectors are added in the input layer, which are included in the same way as word ordering model. Peters et al. (2018) found the pre-trained language model works extremely well in six NLP tasks including QA, SRL, and others, we confirm its effectiveness in parsing task.\nof POS tag features could contribute about 0.2% improvement in our experiments. The word ordering model could be more helpful without POS tag features and seem to compensate for the lack of POS tag features.\nTo show the importance of self-attention mechanism, we do ablation tests on the models with pre-trained word ordering model vectors. We remove self-attention vectors by replacing it with the character-level representations. As shown in table 2, self-attention further improves dependency parsing. Word connections modeled by selfattention are important for dependency parsing.\nFigure 3 shows an example of word connections learned by the model, where we use the solid line to indicate the word connections learned by the word-ordering model and dashed line to the expected dependencies. We can see meaningful overlap could be observed in the example. The percentage of overlap between connections and dependency arcs is over 40% for the sentences less than 10 words. The differences between connections and dependency arcs are because that our word ordering model trained without any supervised dependency information. The connections are actually built to increase the likelihood.\nThis may sound strangely optimistic ."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we propose to implicitly capture word connections from large-scale unlabeled data by a word ordering model with self-attention. Experiments show these features are helpful for dependency parsing. Moreover, with the help of\nword ordering model and language model, our model achieves SOTA results on the PTB dataset. As for future work, we are testing on languages other than English."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank all the anonymous reviewers for their helpful comments. This work is supported by National Natural Science Foundation of China under Grant No.61876004 and No.61751201. The corresponding author of this paper is Baobao Chang."
  }],
  "year": 2018,
  "references": [{
    "title": "Improved transition-based parsing and tagging with neural networks",
    "authors": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portu-",
    "year": 2015
  }, {
    "title": "Globally normalized transition-based neural networks",
    "authors": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."],
    "venue": "Proceedings of the 54th Annual Meeting of the Associ-",
    "year": 2016
  }, {
    "title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing",
    "authors": ["Bernd Bohnet", "Joakim Nivre."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
    "year": 2012
  }, {
    "title": "Experiments with a higherorder projective dependency parser",
    "authors": ["Xavier Carreras."],
    "venue": "EMNLPCoNLL, pages 957–961.",
    "year": 2007
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson."],
    "venue": "INTERSPEECH 2014, 15th Annual Conference of the",
    "year": 2014
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher D. Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740–750.",
    "year": 2014
  }, {
    "title": "Parsing as language modeling",
    "authors": ["Do Kook Choe", "Eugene Charniak."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural",
    "year": 2016
  }, {
    "title": "Incremental parsing with minimal features using bi-directional LSTM",
    "authors": ["James Cross", "Liang Huang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
    "year": 2016
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "CoRR, abs/1611.01734.",
    "year": 2016
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Recurrent neural network grammars",
    "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."],
    "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation.",
    "year": 1997
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Rafal Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."],
    "venue": "CoRR, abs/1602.02410.",
    "year": 2016
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "TACL, 4:313– 327.",
    "year": 2016
  }, {
    "title": "Efficient thirdorder dependency parsers",
    "authors": ["Terry Koo", "Michael Collins."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Transition-based syntactic linearization",
    "authors": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Bing Qin."],
    "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2015
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."],
    "venue": "Computational Linguistics, 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Generating typed dependency parses from phrase structure parses",
    "authors": ["Marie Catherine De Marneffe", "Bill Maccartney", "Christopher D. Manning."],
    "venue": "Lrec, pages 449–454.",
    "year": 2006
  }, {
    "title": "Online large-margin training of dependency parsers",
    "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."],
    "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91–98. Association for Computa-",
    "year": 2005
  }, {
    "title": "Online learning of approximate dependency parsing algorithms",
    "authors": ["Ryan T McDonald", "Fernando CN Pereira."],
    "venue": "EACL. Citeseer.",
    "year": 2006
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."],
    "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
    "year": 2013
  }, {
    "title": "Algorithms for deterministic incremental dependency parsing",
    "authors": ["Joakim Nivre."],
    "venue": "Computational Linguistics.",
    "year": 2008
  }, {
    "title": "An effective neural network model for graph-based dependency parsing",
    "authors": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 313–322.",
    "year": 2015
  }, {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL",
    "year": 2017
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "CoRR, abs/1802.05365.",
    "year": 2018
  }, {
    "title": "Word ordering without syntax",
    "authors": ["Allen Schmaltz", "Alexander M. Rush", "Stuart M. Shieber."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016,",
    "year": 2016
  }, {
    "title": "What do recurrent neural network grammars learn about syntax",
    "authors": ["Noah A. Smith", "Chris Dyer", "Miguel Ballesteros", "Graham Neubig", "Lingpeng Kong", "Adhiguna Kuncoro"],
    "venue": "In Proceedings of the 15th Conference of the European Chapter of the Associa-",
    "year": 2017
  }, {
    "title": "Highway networks",
    "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber."],
    "venue": "CoRR, abs/1505.00387.",
    "year": 2015
  }, {
    "title": "Feature-rich part-ofspeech tagging with a cyclic dependency network",
    "authors": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-",
    "year": 2003
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."],
    "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-",
    "year": 2015
  }, {
    "title": "Graph-based dependency parsing with bidirectional LSTM",
    "authors": ["Wenhui Wang", "Baobao Chang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1:",
    "year": 2016
  }, {
    "title": "Structured training for neural network transition-based parsing",
    "authors": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Transition-based dependency parsing with rich non-local features",
    "authors": ["Yue Zhang", "Joakim Nivre."],
    "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193.",
    "year": 2011
  }],
  "id": "SP:2581d1235ac72494e2855d584c037d014acfd3fa",
  "authors": [{
    "name": "Wenhui Wang",
    "affiliations": []
  }, {
    "name": "Baobao Chang",
    "affiliations": []
  }, {
    "name": "Mairgup Mansur",
    "affiliations": []
  }],
  "abstractText": "Pre-trained word embeddings and language model have been shown useful in a lot of tasks. However, both of them cannot directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unlabeled data by a word ordering model with selfattention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our model gets state-of-the-art performance on the English PTB dataset, achieving 96.35% UAS and 95.25% LAS.",
  "title": "Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data"
}