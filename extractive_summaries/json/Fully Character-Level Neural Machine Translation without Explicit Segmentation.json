{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Nearly all previous work in machine translation has been at the level of words. Aside from our intu-\n∗The majority of this work was completed while the author was visiting New York University.\nitive understanding of word as a basic unit of meaning (Jackendoff, 1992), one reason behind this is that sequences are significantly longer when represented in characters, compounding the problem of data sparsity and modeling long-range dependencies. This has driven NMT research to be almost exclusively word-level (Bahdanau et al., 2015; Sutskever et al., 2014).\nDespite their remarkable success, word-level NMT models suffer from several major weaknesses. For one, they are unable to model rare, out-ofvocabulary words, making them limited in translating languages with rich morphology such as Czech, Finnish and Turkish. If one uses a large vocabulary to combat this (Jean et al., 2015), the complexity of training and decoding grows linearly with respect to the target vocabulary size, leading to a vicious cycle.\nTo address this, we present a fully character-level NMT model that maps a character sequence in a source language to a character sequence in a target language. We show that our model outperforms a baseline with a subword-level encoder on DE-EN and CS-EN, and achieves a comparable result on FI-EN and RU-EN. A purely character-level NMT model with a basic encoder was proposed as a baseline by Luong and Manning (2016), but training it was prohibitively slow. We were able to train our model at a reasonable speed by drastically reducing the length of source sentence representation using a stack of convolutional, pooling and highway layers.\nOne advantage of character-level models is that they are better suited for multilingual translation than their word-level counterparts which require a separate word vocabulary for each language. We\n365\nTransactions of the Association for Computational Linguistics, vol. 5, pp. 365–378, 2017. Action Editor: Adam Lopez. Submission batch: 11/2016; Revision batch: 2/2017; Published 10/2017.\nc©2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nverify this by training a single model to translate four languages (German, Czech, Finnish and Russian) to English. Our multilingual character-level model outperforms the subword-level baseline by a considerable margin in all four language pairs, strongly indicating that a character-level model is more flexible in assigning its capacity to different language pairs. Furthermore, we observe that our multilingual character-level translation even exceeds the quality of bilingual translation in three out of four language pairs, both in BLEU score metric and human evaluation. This demonstrates excellent parameter efficiency of character-level translation in a multilingual setting. We also showcase our model’s ability to handle intra-sentence codeswitching while performing language identification on the fly.\nThe contributions of this work are twofold: we empirically show that (1) we can train character-tocharacter NMT model without any explicit segmentation; and (2) we can share a single character-level encoder across multiple languages to build a multilingual translation system without increasing the model size."
  }, {
    "heading": "2 Background: Attentional Neural Machine Translation",
    "text": "Neural machine translation (NMT) is a recently proposed approach to machine translation that builds a single neural network which takes as an input, a source sentence X = (x1, . . . , xTX ) and generates its translation Y = (y1, . . . , yTY ), where xt and yt′ are source and target symbols (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Cho et al., 2014a). Attentional NMT models have three components: an encoder, a decoder and an attention mechanism.\nEncoder Given a source sentence X , the encoder constructs a continuous representation that summarizes its meaning with a recurrent neural network (RNN). A bidirectional RNN is often implemented as proposed in (Bahdanau et al., 2015). A forward encoder reads the input sentence from left to right: −→ h t = −→ fenc ( Ex(xt), −→ h t−1 ) . Similarly, a backward encoder reads it from right to left: ←− h t = ←− fenc ( Ex(xt), ←− h t+1 ) , where Ex is\nthe source embedding lookup table, and −→ fenc and←− fenc are recurrent activation functions such as long short-term memory units (LSTMs) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRUs) (Cho et al., 2014b). The encoder constructs a set of continuous source sentence representations C by concatenating the forward and backward hidden states at each timestep: C = { h1, . . . ,hTX } , where ht = [−→ h t; ←− h t ] .\nAttention First introduced in Bahdanau et al. (2015), the attention mechanism lets the decoder attend more to different source symbols for each target symbol. More concretely, it computes the context vector ct′ at each decoding time step t′ as a weighted sum of the source hidden states: ct′ = ∑TX t=1 αt′tht. Similarly to Chung et al. (2016) and Firat et al. (2016a), each attentional weight αt′t represents how relevant the t-th source token xt is to the t′-th target token yt′ , and is computed as:\nαt′t = 1 Z exp ( score ( Ey(yt′−1), st′−1,ht )) , (1)\nwhere Z = ∑TX k=1 exp ( score(Ey(yt′−1), st′−1,hk) ) is the normalization constant. score() is a feedforward neural network with a single hidden layer that scores how well the source symbol xt and the target symbol yt′ match. Ey is the target embedding lookup table and st′ is the target hidden state at time t′.\nDecoder Given a source context vector ct′ , the decoder computes its hidden state at time t′ as: st′ = fdec ( Ey(yt′−1), st′−1, ct′ ) . Then, a parametric function outk() returns the conditional probability of the next target symbol being k:\np(yt′ =k|y<t′ , X) = 1\nZ exp ( outk ( Ey(yt′−1), st′ , ct′ )) (2)\nwhere Z is again the normalization constant: Z = ∑ j exp ( outj(Ey(yt′−1), st′ , ct′) ) .\nTraining The entire model can be trained end-toend by minimizing the negative conditional log-\nlikelihood, which is defined as:\nL = − 1 N\nN∑\nn=1\nT (n) Y∑\nt=1\nlog p(yt = y (n) t |y (n) <t , X (n)),\nwhere N is the number of sentence pairs, and X(n) and y(n)t are the source sentence and the t-th target symbol in the n-th pair, respectively."
  }, {
    "heading": "3 Fully Character-Level Translation",
    "text": ""
  }, {
    "heading": "3.1 Why Character-Level?",
    "text": "The benefits of character-level translation over word-level translation are well known. Chung et al. (2016) present three main arguments: character level models (1) do not suffer from out-of-vocabulary issues, (2) are able to model different, rare morphological variants of a word, and (3) do not require segmentation. Particularly, text segmentation is highly non-trivial for many languages and problematic even for English as word tokenizers are either manually designed or trained on a corpus using an objective function that is unrelated to the translation task at hand, which makes the overall system sub-optimal.\nHere we present two additional arguments for character-level translation. First, a character-level translation system can easily be applied to a multilingual translation setting. Between European languages where the majority of alphabets overlaps, for instance, a character-level model may easily identify morphemes that are shared across different languages. A word-level model, however, will need a separate word vocabulary for each language, allowing no cross-lingual parameter sharing.\nAlso, by not segmenting source sentences into words, we no longer inject our knowledge of words and word boundaries into the system; instead, we encourage the model to discover an internal structure of a sentence by itself and learn how a sequence of symbols can be mapped to a continuous meaning representation."
  }, {
    "heading": "3.2 Related Work",
    "text": "To address these limitations associated with wordlevel translation, a recent line of research has investigated using sub-word information.\nCosta-Jussá and Fonollosa (2016) replaced the word-lookup table with convolutional and highway\nlayers on top of character embeddings, while still segmenting source sentences into words. Target sentences were also segmented into words, and predictions were made at word-level.\nSimilarly, Ling et al. (2015) employed a bidirectional LSTM to compose character embeddings into word embeddings. At the target side, another LSTM takes the hidden state of the decoder and generates the target word, character by character. While this system is completely open-vocabulary, it also requires offline segmentation. Character-to-word and word-to-character LSTMs significantly slow down training, as well.\nMost recently, Luong and Manning (2016) proposed a hybrid scheme that consults character-level information whenever the model encounters an outof-vocabulary word. As a baseline, they also implemented a purely character-level NMT model with 4 layers of unidirectional LSTMs with 512 cells, with attention over each character. Despite being extremely slow (approximately 3 months to train), the character-level model gave a comparable performance to the word-level baseline. This shows the possibility of fully character-level translation.\nHaving a word-level decoder restricts the model to only being able to generate previously seen words. Sennrich et al. (2015) introduced a subword-level NMT model that is capable of open-vocabulary translation using subword-level segmentation based on the byte pair encoding (BPE) algorithm. Starting from a character vocabulary, the algorithm identifies frequent character n-grams in the training data and iteratively adds them to the vocabulary, ultimately giving a subword vocabulary which consists of words, subwords and characters. Once the segmentation rules have been learned, their model performs subword-to-subword translation (bpe2bpe) in the same way as word-to-word translation.\nPerhaps the work that is closest to our end goal is (Chung et al., 2016), which used a subword-level encoder from (Sennrich et al., 2015) and a fully character-level decoder (bpe2char). Their results show that character-level decoding performs better than subword-level decoding. Motivated by this work, we aim for fully character-level translation at both sides (char2char).\nOutside NMT, our work is based on a few existing approaches that applied convolutional networks\nto text, most notably in text classification (Zhang et al., 2015; Xiao and Cho, 2016). We also drew inspiration for our multilingual models from previous work that showed the possibility of training a single recurrent model for multiple languages in domains other than translation (Tsvetkov et al., 2016; Gillick et al., 2015)."
  }, {
    "heading": "3.3 Challenges",
    "text": "Sentences are on average 6 (DE, CS and RU) to 8 (FI) times longer when represented in characters. This poses three major challenges to achieving fully character-level translation.\n(1) Training/decoding latency For the decoder, although the sequence to be generated is much longer, each character-level softmax operation costs considerably less compared to a word- or subword-level softmax. Chung et al. (2016) report that characterlevel decoding is only 14% slower than subwordlevel decoding.\nOn the other hand, computational complexity of the attention mechanism grows quadratically with respect to the sentence length, as it needs to attend to every source token for every target token. This makes a naive character-level approach, such as in Luong and Manning (2016), computationally prohibitive. Consequently, reducing the length of the source sequence is key to ensuring reasonable speed in both training and decoding.\n(2) Mapping character sequence to continuous representation The arbitrary relationship between the orthography of a word and its meaning is a wellknown problem in linguistics (de Saussure, 1916). Building a character-level encoder is arguably a more difficult problem, as the encoder needs to learn a highly non-linear function from a long sequence of character symbols to a meaning representation.\n(3) Long range dependencies in characters A character-level encoder needs to model dependencies over longer timespans than a word-level encoder does."
  }, {
    "heading": "4 Fully Character-Level NMT",
    "text": ""
  }, {
    "heading": "4.1 Encoder",
    "text": "We design an encoder that addresses all the challenges discussed above by using convolutional and pooling layers aggressively to both (1) drastically shorten the input sentence; and (2) efficiently capture local regularities. Inspired by the characterlevel language model from Kim et al. (2015), our encoder first reduces the source sentence length with a series of convolutional, pooling and highway layers. The shorter representation, instead of the full character sequence, is passed through a bidirectional GRU to (3) help it resolve long term dependencies. We illustrate the proposed encoder in Figure 1 and discuss each layer in detail below.\nEmbedding We map the sequence of source characters (x1, . . . , xTx) to a sequence of character embeddings of dimensionality dc: X = (C(x1), . . . ,C(xTx)) ∈ Rdc×Tx where Tx is the number of source characters and C is the character embedding lookup table: C ∈ Rdc×|C|.\nConvolution One-dimensional convolution operation is then used along consecutive character embeddings. Assuming we have a single filter f ∈ Rdc×w of width w, we first apply padding to the beginning and the end of X , such that the padded sentence X ′ ∈ Rdc×(Tx+w−1) is w − 1 symbols longer. We then apply a narrow convolution between X ′ and f such that the k-th element of the output Yk is given as:\nYk = (X ′ ∗ f)k =\n∑\ni,j\n(X ′[:,k−w+1:k] ⊗ f)ij , (3)\nwhere ⊗ denotes elementwise matrix multiplication and ∗ is the convolution operation. X ′[:,k−w+1:k] is the sliced subset of X ′ that contains all the rows but only w adjacent columns. The padding scheme employed above, commonly known as half convolution, ensures that the length of the output is identical to the length of the input, (i.e., Y ∈ R1×Tx).\nWe just illustrated how a single convolutional filter of fixed width might be applied to a sentence. In order to extract informative character patterns of different lengths, we employ a set of filters of varying widths. More concretely, we use a filter\nbank F = {f1, . . . , fm} where fi = Rdc×i×ni is a collection of ni filters of width i. Our model uses m = 8, hence extracting character n-grams up to 8 characters long. Outputs from all the filters are stacked upon each other, giving a single representation Y ∈ RN×Tx , where the dimensionality of each column is given by the total number of filters N = ∑m i=1 ni. Finally, rectified linear activation (ReLU) is applied elementwise to this representation.\nMax pooling with stride The output from the convolutional layer is first split into segments of width s, and max-pooling over time is applied to each segment with no overlap. This procedure selects the most salient features to give a segment embedding. Each segment embedding is a summary of meaningful character n-grams occurring in a particular (overlapping) subsequence in the source sentence. Note that the rightmost segment (above ‘on’) in Figure 1 may capture ‘son’ (the filter in green) although ‘s’ occurs in the previous segment. In other words, our segments are overlapping as opposed to in word- or subword-level models with hard segmentation.\nSegments act as our internal linguistic unit from this layer and above: the attention mechanism, for instance, attends to each source segment instead of source character. This shortens the source representation s-fold: Y ′ ∈ RN×(Tx/s). Empirically, we found using a smaller s leads to better performance\nat increased training time. We chose s = 5 in our experiments as it gives a reasonable balance between the two.\nHighway network A sequence of segment embeddings from the max pooling layer is fed into a highway network (Srivastava et al., 2015). Highway networks are shown to significantly improve the quality of a character-level language model when used with convolutional layers (Kim et al., 2015). A highway network transforms input x with a gating mechanism that adaptively regulates information flow:\ny = g ReLU(W1x+ b1) + (1− g) x,\nwhere g = σ((W2x + b2)). We apply this to each segment embedding individually.\nRecurrent layer Finally, the output from the highway layer is given to a bidirectional GRU from §2, using each segment embedding as input.\nSubword-level encoder Unlike a subword-level encoder, our model does not commit to a specific choice of segmentation; instead it is trained to consider every possible character pattern and extract only the most meaningful ones. Therefore, the definition of segmentation in our model is dynamic unlike subword-level encoders. During training, the model finds the most salient character patterns in a sentence via max-pooling, and the character\nsequences extracted by the model change over the course of training. This is in contrast to how BPE segmentation rules are learned: the segmentation is learned and fixed before training begins."
  }, {
    "heading": "4.2 Attention and Decoder",
    "text": "Similarly to the attention model in Chung et al. (2016) and Firat et al. (2016a), a single-layer feedforward network computes the attention score of next target character to be generated with every source segment representation. A standard twolayer character-level decoder then takes the source context vector from the attention mechanism and predicts each target character. This decoder was described as base decoder by Chung et al. (2016)."
  }, {
    "heading": "5 Experiment Settings",
    "text": ""
  }, {
    "heading": "5.1 Task and Models",
    "text": "We evaluate the proposed character-to-character (char2char) translation model against subwordlevel baselines (bpe2bpe and bpe2char) on the WMT’15 DE→EN, CS→EN, FI→EN and RU→EN translation tasks.1 We do not consider word-level models, as it has already been shown that subword-level models outperform them by mitigating issues inherent to closed-vocabulary translation (Sennrich et al., 2015; Sennrich et al., 2016). Indeed, subword-level NMT models have been the de-facto state-of-the-art and are now used in a very large-scale industry NMT system to serve millions of users per day (Wu et al., 2016).\n1http://www.statmt.org/wmt15/translation -task.html\nWe experiment in two different scenarios: 1) a bilingual setting where we train a model on data from a single language pair; and 2) a multilingual setting where the task is many-to-one translation. We train a single model on data from all four language pairs. Hence, our baselines and models are:\n(a) bilingual bpe2bpe: from (Firat et al., 2016a) (b) bilingual bpe2char: from (Chung et al., 2016) (c) bilingual char2char (d) multilingual bpe2char (e) multilingual char2char\nWe train all the models ourselves other than (a), for which we report the results from Firat et al. (2016a). We detail the configuration of our models in Table 1 and Table 2."
  }, {
    "heading": "5.2 Datasets and Preprocessing",
    "text": "We use all available parallel data on the four language pairs from WMT’15: DE-EN, CS-EN, FI-EN and RU-EN.\nFor the bpe2char baselines, we only use sentence pairs where the source is no longer than 50 subword symbols. For our char2char models, we only use pairs where the source sentence is no longer than 450 characters. For all the language pairs apart from FI-EN, we use newstest-2013 as a development set and newstest-2014 and newstest-2015 as test sets. For FI-EN, we use newsdev-2015 and newstest-2015 as development and test sets, respectively. We tokenize2 each corpus using the script from Moses.3\nWhen training bilingual bpe2char models, we extract 20,000 BPE operations from each of the source and target corpus using a script from Sennrich et al. (2015). This gives a source BPE vocabulary of size 20k−24k for each language."
  }, {
    "heading": "5.3 Training Details",
    "text": "Each model is trained using stochastic gradient descent and Adam (Kingma and Ba, 2014) with a learning rate of 0.0001 and minibatch size 64. Training continues until the BLEU score on the validation\n2This is unnecessary for char2char models, yet was carried out for comparison.\n3https://github.com/moses-smt/mosesdecod er\nset stops improving. The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2013). All weights are initialized from a uniform distribution [−0.01, 0.01].\nEach model is trained on a single pre-2016 GTX Titan X GPU with 12GB RAM."
  }, {
    "heading": "5.4 Decoding Details",
    "text": "As done by Chung et al. (2016), a two-layer unidirectional character-level decoder with 1024 GRU units is used for all our experiments. For decoding, we use a beam search algorithm with lengthnormalization to penalize shorter hypotheses. The beam width is 20 for all models."
  }, {
    "heading": "5.5 Training Multilingual Models",
    "text": "Task description We train a model on a many-toone translation task to translate a sentence in any of the four languages (German, Czech, Finnish and Russian) to English. We do not provide a language identifier to the encoder, but merely the sentence itself, encouraging the model to perform language identification on the fly. In addition, by not providing the language identifier, we expect the model to handle intra-sentence code-switching seamlessly.\nModel architecture The multilingual char2char model uses slightly more convolutional filters than the bilingual char2char model, namely (200-250- 300-300-400-400-400-400). Otherwise, the architecture remains the same as shown in Table 1. By not changing the size of the encoder and the decoder, we fix the capacity of the core translation module, and only allow the multilingual model to detect more character patterns.\nSimilarly, the multilingual bpe2char model has the same encoder and decoder as the bilingual bpe2char model, but a larger vocabulary. We learn 50,000 multilingual BPE operations on the multilingual corpus, resulting in 54,544 subwords. See Table 2 for the exact configuration of our multilingual models.\nData scheduling For the multilingual models, an appropriate scheduling of data from different languages is crucial to avoid overfitting to one language too soon. Following Firat et al. (2016a) and Firat et al. (2016b), each minibatch is balanced, in that the proportion of each language pair in a single minibatch corresponds to that of the full corpus. With this minibatch scheme, roughly the same number of updates is required to make one full pass over the entire training corpus of each language pair. Minibatches from all language pairs are combined and presented to the model as a single minibatch. See Table 3 for the minibatch size for each language pair.\nTreatment of Cyrillic To facilitate cross-lingual parameter sharing, we convert every Cyrillic character in the Russian source corpus to Latin alphabet according to ISO-9. Table 4 shows an example of how this conversion may help the multilingual models identify lexemes that are shared across multiple languages.\nMultilingual BPE For the multilingual bpe2char model, multilingual BPE segmentation rules are extracted from a large dataset containing training source corpora of all the language pairs. To ensure the BPE rules are not biased towards one language,\nlarger datasets such as Czech and German corpora are trimmed such that every corpus contains, approximately, an equal number of characters."
  }, {
    "heading": "6 Quantitative Analysis",
    "text": ""
  }, {
    "heading": "6.1 Evaluation with BLEU Score",
    "text": "In this section, we first establish our main hypotheses for introducing character-level and multilingual models, and investigate whether our observations support or disagree with our hypotheses. From our empirical results, we want to verify: (1) if fully character-level translation outperforms subwordlevel translation, (2) in which setting and to what extent is multilingual translation beneficial and (3) if multilingual, character-level translation achieves superior performance to other models. We outline our results with respect to each hypothesis below.\n(1) Character-level vs. subword-level In a bilingual setting, the char2char model outperforms both subword-level baselines on DE-EN (Table 5 (a-c)) and CS-EN (Table 5 (f-h)). On the other two language pairs, it exceeds the bpe2bpe model and achieves a similar performance with the bpe2char baseline (Table 5 (k-m) and (p-r)). We conclude that\nthe proposed character-level model is comparable to or better than both subword-level baselines.\nMeanwhile, in a multilingual setting, the character-level encoder significantly surpasses the subword-level encoder consistently in all the language pairs (Table 5 (d-e), (i-j), (n-o) and (s-t)). From this, we conclude that translating at the level of characters allows the model to discover shared constructs between languages more effectively. This also demonstrates that the character-level model is more flexible in assigning model capacity to different language pairs.\n(2) Multilingual vs. bilingual At the level of characters, we note that multilingual translation is indeed strongly beneficial. On the test sets, the multilingual character-level model outperforms the singlepair character-level model by 2.64 BLEU in FI-EN (Table 5 (m, o)) and 0.78 BLEU in CS-EN (Table 5 (h, j)), while achieving comparable results on DE-EN and RU-EN.\nAt the level of subwords, on the other hand, we do not observe the same degree of performance benefit. The multilingual bpe2char model requires much more updates to reach the performance of the bilingual bpe2char model (see Figure 2). This\nsuggests that learning useful subword segmentation across languages is difficult.\n(3) Multilingual char2char vs. others The multilingual char2char model is the best performer in CS-EN, FI-EN and RU-EN (Table 5 (j, o, t)), and is the runner-up in DE-EN (Table 5 (e)). The fact that the multilingual char2char model outperforms the single-pair models goes to show the parameter efficiency of character-level translation: instead of training N separate models for N language pairs, it is possible to get a better performance with a single multilingual character-level model."
  }, {
    "heading": "6.2 Human Evaluation",
    "text": "It is well known that automatic evaluation metrics such as BLEU encourage reference-like translations and do not fully capture true translation quality (Callison-Burch, 2009; Graham et al., 2015). Therefore, we also carry out a recently proposed evaluation from Graham et al. (2017) where we have human assessors rate both (1) adequacy; and (2) fluency of each system translation on a scale from 0 to 100 via Amazon Mechanical Turk. Adequacy is the degree to which assessors agree that the system translation expresses the meaning of the reference translation. Fluency is evaluated using system translation alone without any reference translation.\nApproximately 1K Turkers assessed a single test set (3K sentences in newstest-2014) for each system and language pair. Each Turker conducted a minimum of 100 assessments for quality control, and the set of scores generated by each Turker was standardized to remove any bias in the individual’s scoring strategy.\nWe consider three models (bilingual bpe2char, bilingual char2char and multilingual char2char) for the human evaluation. We leave out the multilingual bpe2char model to minimize the number of similar systems to improve the interpretability of the evaluation overall.\nFor DE-EN, we observe that the multilingual char2char and bilingual char2char models are tied with respect to both adequacy and fluency (Table 6 (b-c)). For CS-EN, the multilingual char2char and bilingual bpe2char models are tied for adequacy. However, the multilingual char2char model yields significantly better fluency (Table 6 (d, f)). For FIEN and RU-EN, the multilingual char2char model is tied with the bilingual char2char model with respect to adequacy, but significantly outperforms all other models in fluency (Table 6 (g-i, j-l)).\nOverall, the improvement in translation quality yielded by the multilingual character-level model mainly comes from fluency. We conjecture that because the English decoder of the multilingual model is tuned in on all the training sentence pairs, it\nbecomes a better language model than a bilingual model’s decoder. We leave it for future work to confirm if this is indeed the case."
  }, {
    "heading": "7 Qualitative Analysis",
    "text": "In Table 7, we demonstrate our character-level model’s robustness in four translation scenarios from which conventional NMT systems are known to suffer. We also showcase our model’s ability to seamlessly handle intra-sentence code-switching, or mixed utterances from two or more languages.\nWe compare sample translations from the characterlevel model with those from the subword-level model, which already sidesteps some of the issues associated with word-level translation.\nWith real-world text containing typos and spelling mistakes, the quality of word-based translation would severely drop, as every non-canonical form of a word cannot be represented. On the other hand, a character-level model has a much better chance recovering the original word or sentence. Indeed, our char2char model is robust against a few spelling\nmistakes (Table 7 (a)). Given a long, rare word such as “Siebentausendzweihundertvierundfünfzig” (seven thousand two hundred fifty four) in Table 7 (b), the subword-level model segments “Siebentausend” as (Sieb, ent, aus, end), which results in an inaccurate translation. The character-level model performs better on these long, concatenative words with ambiguous segmentation.\nWe expect a character-level model to handle novel and unseen morphological inflections well. We observe that this is indeed the case, as our char2char model correctly understands “gesperrt”, a past participle form of “sperren” (to block) (Table 7 (c)).\nNonce words are terms coined for a single use. They are not actual words but are constructed in a way that humans can intuitively guess what they mean, such as workoliday and friyay. We construct a few DE-EN sentence pairs that contain German nonce words (one example shown in Table 7 (d)), and observe that the character-level model can indeed detect salient character patterns and arrive at a correct translation.\nFinally, we evaluate our multilingual models’ capacity to perform intra-sentence code-switching, by giving them as input mixed sentences from multiple languages. The newstest-2013 development datasets for DE-EN, CS-EN and FI-EN contain intersecting examples with the same English sentences. We compile a list of these sentences in DE/CS/FI and their translation in EN, and uniformly choose a few samples at random from the English side. Words or clauses from different languages are manually inter-\nmixed to create multilingual sentences. We discover that when given sentences with a high degree of language intermixing, as in Table 7 (e), the multilingual bpe2char model fails to seamlessly handle alternation of languages. Overall, however, both multilingual models generate reasonable translations. This is possible because we did not provide a language identifier when training our multilingual models. As a result, they learned to understand a multilingual sentence and translate it into a coherent English sentence.\nThere are indeed cases where the proposed character-level model fails, and we notice that those are often sentences with long-distance dependencies (see Table 8).\nWe show supplementary, sample translations in each scenario on a webpage.4\nTraining and decoding speed On a single Titan X GPU, we observe that our char2char models are approximately 35% slower to train than our bpe2char baselines when the same batch size was used. Our bilingual character-level models can be trained in roughly two weeks.\nWe further note that the bilingual bpe2char model can translate 3,000 sentences in 66.63 minutes while the bilingual char2char model requires 71.71 minutes (online, not in batch). See Table 9 for the exact details.\nFurther observations We also note that the mul4https://sites.google.com/site/dl4mtc2c\ntilingual models are less prone to overfitting than the bilingual models. This is particularly visible for low-resource language pairs such as FI-EN. Figure 2 shows the evolution of the FI-EN validation BLEU scores where the bilingual models overfit rapidly but the multilingual models seem to regularize learning by training simultaneously on other language pairs."
  }, {
    "heading": "8 Conclusion",
    "text": "We propose a fully character-level NMT model that accepts a sequence of characters in the source language and outputs a sequence of characters in the target language. What is remarkable about this model is the absence of explicitly hard-coded knowledge of words and their boundaries, and that the model learns these concepts from a translation task alone.\nOur empirical results show that the fully character-level model performs as well as, or better than, subword-level translation models. The performance gain is distinctly pronounced in the multilingual many-to-one translation task, where results show that the character-level model can assign\nmodel capacities to different languages more efficiently than the subword-level models. We observe a particularly large improvement in FI-EN translation when the model is trained to translate multiple languages, indicating a positive cross-lingual transfer to a low-resource language pair.\nWe discover two main benefits of the multilingual character-level model: (1) it is much more parameter-efficient than the bilingual models; and (2) it can naturally handle intra-sentence codeswitching as a result of the many-to-one translation task. Ultimately, we present a case for fully character-level translation: that translation at the level of character is strongly beneficial and should be encouraged more.\nThe repository https://github.com/nyu-dl /dl4mt-c2c contains the source code and pretrained models for reproducing the experimental results.\nIn the next stage of this research, we will investigate extending our multilingual many-to-one translation models to perform many-to-many translations, which will allow the decoder, similarly with the encoder, to learn from multiple target languages. Furthermore, a more thorough investigation into model architectures and hyperparameters is needed."
  }, {
    "heading": "Acknowledgements",
    "text": "Kyunghyun Cho thanks the support of eBay, Facebook, Google (Google Faculty Award, 2016) and NVidia (NVIDIA AI Lab, 2016-2019). This work was partly supported by the Samsung Advanced Institute of Technology (Deep Learning). Jason Lee was supported by the Qualcomm Innovation Fellowship, and thanks David Yenicelik and Kevin Wallimann for their contribution in designing the qualitative analysis. The authors would like to also thank Prof. Zheng Zhang (NYU, Shanghai) for fruitful discussions and comments, as well as Yvette Graham for her help with the human evaluation. Finally, the authors thank the Action Editor and anonymous reviewers for their constructive feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of 376",
    "year": 2015
  }, {
    "title": "Fast, cheap, and creative: Evaluating translation quality using amazon’s mechanical turk",
    "authors": ["Chris Callison-Burch."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295.",
    "year": 2009
  }, {
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of the 8th Workshop on Syntax, Semantics, and Structure in Statistical Trans-",
    "year": 2014
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of the Empiricial Methods in Nat-",
    "year": 2014
  }, {
    "title": "A character-level decoder without explicit segmentation for neural machine translation",
    "authors": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1693–1703.",
    "year": 2016
  }, {
    "title": "Character-based neural machine translation",
    "authors": ["Marta R. Costa-Jussá", "Josè A.R. Fonollosa."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361.",
    "year": 2016
  }, {
    "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism",
    "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-",
    "year": 2016
  }, {
    "title": "Zero-resource translation with multi-lingual neural machine translation",
    "authors": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman Vural", "Kyunghyun Cho."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
    "year": 2016
  }, {
    "title": "Multilingual language processing from bytes",
    "authors": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics, pages 1296–1306.",
    "year": 2015
  }, {
    "title": "Accurate evaluation of segment-level machine translation metrics",
    "authors": ["Yvette Graham", "Nitika Mathur", "Timothy Baldwin."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
    "year": 2015
  }, {
    "title": "Can machine translation systems be evaluated by the crowd alone",
    "authors": ["Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel."],
    "venue": "Natural Language Engineering, 23(1):3–30.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Semantic Structures, Volume 18",
    "authors": ["Ray S. Jackendoff."],
    "venue": "MIT press.",
    "year": 1992
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 1–5.",
    "year": 2015
  }, {
    "title": "Character-aware neural language models",
    "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence, pages 2741–2749.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of the 3rd International Conference for Learning Representations.",
    "year": 2014
  }, {
    "title": "Character-based neural machine translation",
    "authors": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."],
    "venue": "arXiv preprint arXiv:1511.04586.",
    "year": 2015
  }, {
    "title": "Achieving open vocabulary neural machine translation with hybrid word-character models",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1054–1063.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning, pages 1310–1318.",
    "year": 2013
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725.",
    "year": 2015
  }, {
    "title": "Edinburgh neural machine translation systems for WMT 16",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 1st Conference on Machine Translation.",
    "year": 2016
  }, {
    "title": "Training very deep networks",
    "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber."],
    "venue": "Advances in Neural Information Processing Systems, Volume 28, pages 2377–2385.",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Advances in Neural Information Processing Systems, Volume 27, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Polyglot neural language models: A case study in cross-lingual phonetic representation learning",
    "authors": ["Yulia Tsvetkov", "Sunayana Sitaram", "Manaal Faruqui", "Guillaume Lample", "Patrick Littell", "David Mortensen", "Alan W. Black", "Lori Levin", "Chris Dyer."],
    "venue": "Pro-",
    "year": 2016
  }, {
    "title": "Efficient character-level document classification by combining convolution and recurrent layers",
    "authors": ["Yijun Xiao", "Kyunghyun Cho."],
    "venue": "arXiv preprint arXiv:1602.00367.",
    "year": 2016
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "Advances in Neural Information Processing Systems, Volume 28, pages 649–657. 378",
    "year": 2015
  }],
  "id": "SP:b8bc86a1bc281b15ce45e967cbdd045bcf23a952",
  "authors": [{
    "name": "Jason Lee",
    "affiliations": []
  }, {
    "name": "Kyunghyun Cho",
    "affiliations": []
  }, {
    "name": "Thomas Hofmann",
    "affiliations": []
  }],
  "abstractText": "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subwordlevel encoder on WMT’15 DE-EN and CSEN, and gives comparable performance on FIEN and RU-EN. We then demonstrate that it is possible to share a single characterlevel encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
  "title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation"
}