{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Supervised learning has been successful in many application fields. The vast majority of supervised learning research falls into the Empirical Risk Minimization (ERM) framework (Vapnik, 1998) that assumes a test distribution to be the same as a training distribution. However, such an assumption can be easily contradicted in real-world applications due to sample selection bias or non-stationarity of the environment (Quionero-Candela et al., 2009). Once the distribution shift occurs, the performance of the traditional machine learning techniques can be significantly degraded. This makes the traditional techniques unreliable for practitioners to use in the real world.\n1University of Tokyo, Japan 2RIKEN, Tokyo, Japan. Correspondence to: Weihua Hu <weihua916@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nDistributionally Robust Supervised Learning (DRSL) is a promising paradigm to tackle this problem by obtaining prediction functions explicitly robust to distribution shift. More specifically, DRSL considers a minimax game between a learner and an adversary: the adversary first shifts the test distribution from the training distribution within a pre-specified uncertainty set so as to maximize the expected loss on the test distribution. The learner then minimizes the adversarial expected loss.\nDRSL with f -divergences (Bagnell, 2005; Ben-Tal et al., 2013; Duchi et al., 2016; Namkoong & Duchi, 2016; 2017) is particularly well-studied and lets the uncertainty set for test distributions be an f -divergence ball from a training distribution (see Section 2 for the detail). This DRSL has been mainly studied under the assumption that the same continuous loss is used for training and testing. This is not the case in the classification scenario, in which we care about the 0-1 loss (i.e., the mis-classification rate) at test time, while at training time, we use a surrogate loss for optimization tractability.\nIn this paper, we revisit DRSL with f -divergences, providing novel insight for the classification scenario. In particular, we prove rather surprising results (Theorems 1–3), showing that when the DRSL is applied to classification, the obtained classifier ends up being optimal for the training distribution. This is too pessimistic for DRSL given that DRSL is explicitly formulated for a distribution shift scenario and is naturally expected to give a classifier different from the one that exactly fits the given training distribution. Such pessimism comes from two sources: the particular losses used in classification and the over-flexibility of the uncertainty set used by DRSL with f -divergences.\nMotivated by our analysis, we propose simple DRSL that overcomes the pessimism of the previous DRSL by incorporating structural assumptions on distribution shift (Section 4). We establish convergence properties of our proposed DRSL (Theorem 4) and derive efficient optimization algorithms (Section 5). Finally, we demonstrate the effectiveness of our DRSL through experiments (Section 6). All the appendices of this paper are provided in the supplementary material.\nRelated work: Besides DRSL with f -divergences, different DRSL considers different classes of uncertainty sets for test distributions. DRSL by Globerson & Roweis\n(2006) considered the uncertainty of features deletion, while DRSL by Liu & Ziebart (2014) considered the uncertainty of unknown properties of the conditional label distribution. DRSL by Esfahani & Kuhn (2015), Blanchet et al. (2016) and Sinha et al. (2017) lets the uncertainty set of test distributions be a Wasserstein ball from the training distribution. DRSL with the Wasserstein distance can make classifiers robust to adversarial examples (Sinha et al., 2017), while DRSL with f -divergences can make classifiers robust against adversarial reweighting of data points as shown in Section 2. Recently, in the context of fair machine learning, Hashimoto et al. (2018) applied DRSL with f - divergences in an attempt to achieve fairness without demographic information."
  }, {
    "heading": "2. Review of ERM and DRSL",
    "text": "In this section, we first review the ordinary ERM framework. Then, we explain a general formulation of DRSL and review DRSL with f -divergences.\nSuppose training samples, {(x1, y1), . . . , (xN , yN )} ≡ D, are drawn i.i.d. from an unknown training distribution over X × Y with density p(x, y), where X ⊂ Rd and Y is an output domain. Let gθ be a prediction function with parameter θ, mapping x ∈ X into a real scaler or vector, and let ℓ(ŷ, y) be a loss between y and real-valued prediction ŷ.\nERM: The objective of the risk minimization (RM) is\nmin θ Ep(x,y)[ℓ(gθ(x), y)]︸ ︷︷ ︸ ≡ R(θ) , (1)\nwhere R(θ) is called the risk. In ERM, we approximate the expectation in Eq. (1) by training data D:\nmin θ\n1\nN\nN∑\ni=1\nℓ(gθ(xi), yi)\n︸ ︷︷ ︸ ≡ R̂(θ)\n, (2)\nwhere R̂(θ) is called the empirical risk. To prevent overfitting, we can add regularization term Ω(θ) to Eq. (2) and minimize R̂(θ)+λΩ(θ), where λ ≥ 0 is a trade-off hyperparameter.\nGeneral formulation of DRSL: ERM implicitly assumes the test distribution to be the same as the training distribution, which does not hold in most real-world applications. DRSL is explicitly formulated for a distribution shift scenario, where test density q(x, y) is different from training density p(x, y). Let Qp be an uncertainty set for test distributions. In DRSL, the learning objective is\nmin θ sup q∈Qp Eq(x,y)[ℓ(gθ(x), y)]. (3)\nWe see that Eq. (3) minimizes the risk w.r.t. the worst-case test distribution within the uncertainty set Qp.\nDRSL with f -divergences: Let q ≪ p denote that q is absolutely continuous w.r.t. p, i.e., p(x, y) = 0 implies q(x, y) = 0. Bagnell (2005) and Ben-Tal et al. (2013) considered the particular uncertainty set\nQp = {q ≪ p | Df [q∥p] ≤ δ}, (4)\nwhere Df [·∥·] is an f -divergence defined as Df [q∥p] ≡ Ep [f (q/p)], and f(·) is convex with f(1) = 0. The f - divergence (Ciszar, 1967) measures a discrepancy between probability distributions. When f(x) = x log x, we have the well-known Kullback-Leibler divergence as an instance of it. Hyper-parameter δ > 0 in Eq. (4) controls the degree of the distribution shift. Define r(x, y) ≡ q(x, y)/p(x, y). Through some calculations, the objective of DRSL with f - divergences can be rewritten as\nmin θ sup r∈Uf Ep(x,y)[r(x, y)ℓ(gθ(x), y)] ︸ ︷︷ ︸\n≡ Radv(θ)\n, (5)\nUf ≡ {r(x, y) | Ep(x,y) [f (r(x, y))] ≤ δ,\nEp(x,y)[r(x, y)] = 1,\nr(x, y) ≥ 0, ∀(x, y) ∈ X × Y}. (6)\nWe call Radv(θ) the adversarial risk and call the minimization problem of Eq. (5) the adversarial risk minimization (ARM). In ARM, the density ratio, r(x, y), can be considered as the weight put by the adversary on the loss of data (x, y). Then, Eq. (5) can be regarded as a minimax game between the learner (corresponding to minθ) and the adversary (corresponding to supr∈Uf ): the adversary first reweights the losses using r(·, ·) so as to maximize the expected loss; the learner then minimizes the reweighted expected loss, i.e., adversarial risk Radv(θ). For notational convenience, we denote ℓ(gθ(xi), yi) by ℓi(θ). Also, let r ≡ (r1, . . . , rN ) be a vector of density ratios evaluated at training data points, i.e., ri ≡ r(xi, yi) for 1 ≤ i ≤ N . Equations (5) and (6) can be empirically approximated as1\nmin θ sup r∈Ûf\n1 N\nN∑\ni=1\nriℓi(θ)\n︸ ︷︷ ︸ ≡ R̂adv(θ)\n, (7)\nÛf = { r ∣∣∣∣∣ 1 N N∑\ni=1\nf (ri) ≤ δ, 1 N\nN∑\ni=1\nri = 1, r ≥ 0 } , (8)\n1The formulation in Eqs. (7) and (8) is similar to Duchi et al. (2016), Namkoong & Duchi (2016) and Namkoong & Duchi (2017) except that they decay δ linearly w.r.t. the number of training data N . Different from us, they assume δ = 0 in Eq. (4) (thus, their objective is the ordinary risk) and try to be robust to apparent distribution fluctuations due to the finiteness of training samples. On the other hand, we consider using the same δ > 0 for both Eqs. (4) and (8) and try to be robust to the actual distribution change between training and test stages.\nwhere the inequality constraint for a vector is applied in an element-wise fashion. We call R̂adv(θ) the adversarial empirical risk and call the minimization problem of Eq. (7) the adversarial empirical risk minimization (AERM). In AERM, the adversary (corresponding to supr∈Ûf ) reweights data losses through r to maximize the empirical loss in Eq. (7). To prevent overfitting, we can add regularization term Ω(θ) to Eq. (7)."
  }, {
    "heading": "3. Analysis of DRSL with f -divergences in classification",
    "text": "At first glance, DRSL with f -divergences (which we call ARM and AERM in this paper) is reasonable to give a distributionally robust classifier in the sense that it explicitly minimizes the loss for the shifted worst-case test distribution. However, we show rather surprising results, suggesting that the DRSL, when applied to classification, still ends up giving a classifier optimal for a training distribution. This is too pessimistic for DRSL because it ends up behaving similarly to ordinary ERM-based supervised classification that does not explicitly consider distribution shift. To make a long story short, our results hold because of the particular losses used in classification (especially, the 0-1 loss at test time) and the overly flexible uncertainty sets used by ARM and AERM. We will detail these points after we state our main results.\nClassification setting: Let us first briefly review classification settings to set up notations. In binary classification, we have gθ(·) : x )→ ŷ ∈ R, Y = {+1,−1} and ℓ(·, ·) : R × Y → R≥0. In K-class classification for K ≥ 2, we have gθ(·) : x )→ ŷ ∈ RK , Y = {1, 2, . . . ,K} and ℓ(·, ·) : RK × Y → R≥0. The goal of classification is to learn the prediction function that minimizes the mis-classification rate on the test distribution. The misclassification rate corresponds to the use of the 0-1 loss, i.e., ℓ(ŷ, y) ≡ 1{sign(ŷ) ̸= y} for binary classification, and ℓ(ŷ, y) ≡ 1{argmaxkŷk ̸= y} for multi-class classification, where 1{·} is the indicator function and ŷk is the k-th element of ŷ ∈ RK . However, since the 0-1 loss is non-convex and non-continuous, learning with it is difficult in practice. Therefore, at training time, we instead use surrogate losses that are easy to optimize, such as the logistic loss and the cross-entropy loss.\nIn the following, we state our main results, analyzing ARM and AERM in the classification scenario by considering the use of the 0-1 loss and a surrogate loss.\nThe 0-1 loss case: Theorem 1 establishes the non-trivial relationship between the adversarial risk and the ordinary risk when the 0-1 loss is used.\nTheorem 1. Let ℓ(ŷ, y) be the 0-1 loss. Then, there is a monotonic relationship between Radv(θ) and R(θ) in the sense that for any pair of parameters θ1 and θ2, the followings hold."
  }, {
    "heading": "If Radv(θ1) < 1, then",
    "text": "Radv(θ1) < Radv(θ2)⇐⇒ R(θ1) < R(θ2). (9)"
  }, {
    "heading": "If Radv(θ1) = 1, then",
    "text": "R(θ1) ≤ R(θ2) =⇒ Radv(θ2) = 1. (10)\nThe same monotonic relationship also holds between their empirical approximations: R̂adv(θ) and R̂(θ). See Appendix A for the proof. Theorem 1 shows a surprising result that when the 0-1 loss is used, R(θ) and Radv(θ) are essentially equivalent objective functions in the sense that the minimization of one objective function results in the minimization of another objective function. This readily implies that R(θ) and Radv(θ) have exactly the same set of global minima in the regime of Radv(θ) < 1. An immediate practical implication is that if we select hyper-parameters such as λ for regularization according to the adversarial risk with the 0-1 loss, we will end up choosing hyper-parameters that attain the minimum misclassification rate on the training distribution. The surrogate loss case: We now turn our focus on the training stage of classification, where we use a surrogate loss instead of the 0-1 loss. In particular, for binary classification, we consider a class of classification calibrated losses (Bartlett et al., 2006) that are margin-based, i.e., ℓ(ŷ, y) is a function of product yŷ. For multi-class classification, we consider a class of classification calibrated losses (Tewari & Bartlett, 2007) that are invariant to class permutation, i.e., for any class permutation π : Y → Y , ℓ(ŷπ,π(y)) = ℓ(ŷ, y) holds, where ŷπk = ŷπ(k) for 1 ≤ k ≤ K. Although we only consider the sub-class of general classification-calibrated losses (Bartlett et al., 2006; Tewari & Bartlett, 2007), we note that ours still includes some of the most widely used losses: the logistic, hinge, and exponential losses for binary classification and the softmax cross entropy loss for multi-class classification.\nWe first review Proposition 1 by Bartlett et al. (2006) and Tewari & Bartlett (2007) that justifies the use of classification-calibrated losses in ERM for classification. We then show a surprising fact in Theorem 2 that the similar property also holds for ARM using the sub-class of classification-calibrated losses. Proposition 1 (Bartlett et al. (2006); Tewari & Bartlett (2007)). Let ℓ(ŷ, y) be a classification calibrated loss, and assume that the hypothesis class is equal to all measurable functions. Then, the risk minimization (RM) gives the Bayes optimal classifier2. Theorem 2. Let f(·) be differentiable, the hypothesis class be all measurable functions, and ℓ(ŷ, y) be a classificationcalibrated loss that is margin-based or invariant to class\n2The classifier that minimizes the mis-classification rate for the training density p(x, y) (the 0-1 loss is considered), i.e., the classifier whose prediction on x is equal to argmaxy∈Y p(y|x).\npermutation. Let g(adv) be any solution of ARM3 under the above setting, and define\nr∗ ≡ argmax r∈Uf Ep(x,y)[r(x, y)ℓ(g(adv)(x), y)]. (11)\nThen, the prediction of g(adv) coincides with that of the Bayes optimal classifier almost surely over q∗(x) ≡∑\ny∈Y r ∗(x, y)p(x, y). Furthermore, among the solutions of ARM, there exists g(adv) whose prediction coincides with that of the Bayes optimal classifier almost surely over p(x).\nSee Appendix B for the proof. Theorem 2 indicates that ARM, similarly to RM, ends up giving the optimal decision boundary for the training distribution, if the hypothesis class is all measurable functions and we have access to true density p(x, y). Even though the assumptions made are strong, Theorem 2 together with Proposition 1 highlight the non-trivial fact that when a certain surrogate loss is used, AERM and ERM demonstrate the similar asymptotic behavior in classification.\nWe proceed to consider a more practical scenario, where we only have a finite amount of training data and the hypothesis class is limited. In the rest of the section, we focus on a differentiable loss and a real-valued scalar output, i.e., ŷ ∈ R, which includes the scenario of binary classification.\nWe first define the notion of a steeper loss, which will play a central role in our result.\nDefinition 1 (Steeper loss). Loss function ℓsteep(ŷ, y) is said to be steeper than loss function ℓ(ŷ, y), if there exists a non-constant, non-decreasing and non-negative function h : R≥0 → R≥0 such that\n∂ℓsteep(ŷ, y)\n∂ŷ = h(ℓ(ŷ, y))\n∂ℓ(ŷ, y)\n∂ŷ . (12)\nFor example, following Definition 1, we can show that the exponential loss is steeper than the logistic loss. Intuitively, outlier-sensitive losses are steeper than more outlier-robust losses. Lemma 1 shows an important property of a steeper loss in a classification scenario.\nLemma 1. Let ℓ(ŷ, y) be a margin-based convex classification-calibrated loss. Then, its steeper loss defined in Eq. (12) is also convex classification-calibrated if h(ℓ(0, y)) > 0.\nSee Appendix C for the proof.\nNow we are ready to state our result in Theorem 3 that considers ŷ ∈ R. Theorem 3 holds for any hypothesis class that is parametrized by θ and sub-differentiable w.r.t. θ, e.g., linear-in-parameter models and deep neural networks.\n3There can be multiple solutions that achieve the same minimum adversarial risk.\nTheorem 3. Let θ∗ be a stationary point of AERM in Eq. (7) using ℓ(ŷ, y). Then, there exists a steeper loss function, ℓDRSL(ŷ, y), such that θ∗ is also a stationary point of the following ERM.\nmin θ\n1\nN\nN∑\ni=1\nℓDRSL(gθ(xi), yi). (13)\nSee Appendix D for the proof. Remark 1 (Conditions for convexity). Let ℓ(ŷ, y) be convex in ŷ, gθ(x) be a linear-in-parameter model. Then, both AERM in Eq. (7) and ERM in Eq. (13) become convex in θ. This implies that the stationary point θ∗ in Theorem 3 turns out to be the global optimum for both Eqs. (7) and (13) in this usual setting. Note that Theorem 3 holds for general real-valued scalar prediction, i.e., ŷ ∈ R; thus, the result holds for ordinary regression (using the same loss for training and testing) as well as for binary classification. However, as we discuss in the following, the implication of Theorem 3 is drastically different for the two scenarios.\nImplication for classification: Theorem 3 together with Lemma 1 indicate that under a mild condition,4 AERM using a convex classification-calibrated margin-based loss reduces to Eq. (13), which is ERM using a convex classification-calibrated loss. This implies that AERM, similarly to ordinary ERM using a classification-calibrated loss, will try to give a classifier optimal for the training distribution.\nWhy does the use of the steeper surrogate loss fail to give meaningful robust classifiers? This is because we are dealing with classification tasks, where we care about the performance in terms of the 0-1 loss at test time. The use of the steeper surrogate loss may make a classifier distributionally robust in terms of the surrogate loss,5 but not necessarily so in terms of the 0-1 loss. Moreover, even if we obtain a classifier that minimizes the adversarial risk in terms of the 0-1 loss, the obtained classifier ends up being optimal for the training distribution (see Theorem 1). In any case, the use of the steeper loss does not in general give classifiers that are robust to change from a training distribution.\nIn summary, in the classification scenario, the use of the steeper loss does more harm (making a classifier sensitive\n4The condition that h(ℓ(0, y)) > 0 in Lemma 1. Whether the condition holds or not generally depends on the uncertainty set, the model, the loss function, and training data. Nonetheless, the condition is mild in practice; especially, the condition always holds when the Kullback-Leibler divergence is used. See Appendix C for detailed discussion.\n5For fixed δ (non-decaying w.r.t. N ), whether AERM is consistent with ARM or not is an open problem. Nevertheless, we empirically confirm in Section 6 that AERM achieves lower adversarial risk than other baselines in terms of the surrogate loss.\nto outliers due to the use of the steeper surrogate loss) than good (making a classifier robust to change from a training distribution).\nImplication for ordinary regression: For comparison, let us rethink about the classical regression scenario, in which we use the same loss, e.g., the squared loss, during training and testing. In such a case, the use of the steeper loss may indeed make regressors distributionally robust in terms of the same loss. Nonetheless, learning can be extremely sensitive to outliers due to the use of the steeper loss. Hence, when applying DRSL with f -divergences to real-world regression tasks, we need to pay extra attention to ensure that there are no outliers in datasets."
  }, {
    "heading": "4. DRSL with Latent Prior Probability Change",
    "text": "In this section, motivated by our theoretical analysis in Section 3, we propose simple yet practical DRSL that overcomes the over pessimism of ARM and AERM in the classification scenario. We then analyze its convergence property and discuss the practical use of our DRSL.\nTheoretical motivation: What insight can we get from our theoretical analyses in Section 3? Our key insight from proving the theorems is that the adversary of ARM has too much (non-parametric) freedom to shift the test distribution, and as a result, the learner becomes overly pessimistic. In fact, the proofs of all the theorems rely on the overflexibility of the uncertainty set Uf in Eq. (6), i.e., the values of r(·, ·) are not tied together for different (x, y) within Uf (see Eqs. (5) and (6)). Consequently, the adversary of ARM simply assigns larger weight r(x, y) to data (x, y) with a larger loss. This fact, combined with the fact that we use the different losses during training and testing in classification (see discussion at the end of Section 3), led to the pessimistic results of Theorems 1–3.\nOur theoretical insight suggests that in order to overcome the pessimism of ARM applied to classification, it is crucial to structurally constrain r(·, ·) in Uf , or equivalently, to impose structural assumptions on the distribution shift. To this end, in this section, we propose DRSL that overcomes the limitation of the DRSL by incorporating structural assumptions on distribution shift.\nPractical structural assumptions: In practice, there can be a variety of ways to impose structural assumptions on distribution shift. Here, as one possible way, we adopt the latent prior probability change assumption (Storkey & Sugiyama, 2007) because this particular class of assumptions enjoys the following two practical advantages.\n1. Within the class, users of our DRSL can easily and intuitively model their assumptions on distribution shift (see the discussion at the end of this section).\n2. Efficient learning algorithms can be derived (see Section 5).\nLet us introduce a latent variable z ∈ Z ≡ {1, . . . , S}, which we call a latent category, where S is a constant. The latent prior probability change assumes\np(x, y|z) = q(x, y|z), q(z) ̸= p(z), (14)\nwhere p and q are the training and test densities, respectively. The intuition is that we assume a two-level hierarchical data-generation process: we first sample latent category z from the prior and then sample actual data (x, y) conditioned on z. We then assume that only the prior distribution over the latent categories changes, leaving the conditional distribution intact.\nWe assume the structural assumption in Eq. (14) to be provided externally by users of our DRSL based on their knowledge of potential distribution shift, rather than something to be inferred from data. As we will see at the end of this section, specifying Eq. (14) amounts to grouping training data points according to their latent categories, which is quite intuitive to do in practice.\nObjective function of our DRSL: With the latent prior probability change of Eq. (14), the uncertainty set for test distributions in our DRSL becomes\nQp = {q ≪ p | Df [q(x, y, z)||p(x, y, z)] ≤ δ, q(x, y|z) = p(x, y|z)}. (15)\nThen, corresponding to Eq. (3), the objective of our DRSL can be written as\nmin θ sup w∈Wf Ep(x,y,z) [w(z)ℓ(gθ(x), y)] ︸ ︷︷ ︸\n≡ Rs-adv(θ)\n, (16)\nWf ≡ { w(z) ∣∣∣∣∣ ∑\nz∈Z\np(z)f (w(z)) ≤ δ,\n∑\nz∈Z\np(z)w(z) = 1, w(z) ≥ 0, ∀z ∈ Z } , (17)\nwhere w(z) ≡ q(z)/p(z) = q(x, y, z)/p(x, y, z) because of q(x, y|z) = p(x, y|z). We call Rs-adv(θ) the structural adversarial risk and call the minimization problem of Eq. (16) the structural adversarial risk minimization (structural ARM). Similarly to ARM, structural ARM is a minimax game between the learner and the adversary. Differently from ARM, the adversary of structural ARM (corresponding to supw∈Wf ) uses w(·) to reweight data; hence, it has much less (only parametric) freedom to shift the test distribution compared to the adversary of ARM that uses non-parametric weight r(·, ·) (see Eq. (5)). Because of this limited freedom for the adversary, we can show that Theorems 1–3 do not hold for structural ARM, and we can expect to learn meaningful classifiers that are robust to structurally constrained distribution shift.\nDiscussion and proposal of evaluation metric for distributional robustness: Recall from Theorem 1 that when the 0-1 loss is used, the adversarial risk ends up being equivalent to the ordinary risk as an evaluation metric, which is too pessimistic as a metric for the distributional robustness of a classifier. In contrast, we can easily verify that our structural adversarial risk using the 0-1 loss does not suffer from the pessimism. We argue that our structural adversarial risk can be an alternative metric in distributionally robust classification. To better understand its property, inspired by Namkoong & Duchi (2017), we decompose it as6\nRs-adv(θ) = R(θ)︸ ︷︷ ︸ (a) ordinary risk\n+ √ δ ·\n√∑\nz∈Z p(z)(Rz(θ) − R(θ))2 ︸ ︷︷ ︸ (b) sensitivity , (18)\nwhere Rz(θ)(≡ Ep(x,y|z)[ℓ(gθ(x), y)]) is the risk of the classifier on latent category z ∈ Z . We see that Rs-adv(θ) in Eq. (18) contains the risk variance term (b). This variance term (b) can be large when the obtained classifier performs extremely poorly on a small number of latent categories. Once a test density concentrates on those poorlyperformed latent categories, the test accuracy of the classifier can extremely deteriorate. In this sense, the classifier with large (b) is sensitive to distribution shift. In contrast, the small risk variance (b) indicates that the obtained classifier attains similar accuracy on all the latent categories. In such a case, the test accuracy of the classifier is insensitive to latent category prior change. In this sense, the classifier with small (b) is robust to distribution shift. To sum up, the additional term (b) measures the sensitivity of the classifier to the specified structural distribution shift.\nOn the basis of the above discussion, we see that Rs-adv(θ) in Eq. (18) simultaneously captures (a) the ordinary risk, i.e., the mis-classification rate when no distribution shift occurs, and (b) the sensitivity to distribution shift. In this sense, our structural adversarial risk is an intuitive and reasonable metric for distributional robustness of a classifier, and we will employ this metric in our experiments in Section 6.\nEmpirical approximation: We explain how to empirically approximate the objective functions in Eqs. (16) and (17) using training data D′ ≡ {(x1, y1, z1), . . . , (xN , yN , zN )} drawn independently from p(x, y, z).\nDefine Gs ≡ {i | zi = s, 1 ≤ i ≤ N} for 1 ≤ s ≤ S, which is a set of data indices belonging to latent category s. In our DRSL, users are responsible for specifying the groupings of training data points, i.e., {Gs}Ss=1. By specifying these groupings, the users incorporate their structural\n6This particular decomposition holds when the Pearson (PE) divergence is used and δ is not so large. Refer to Appendix E for the derivation. Analogous decomposition can be also derived when other f -divergences are used.\nassumptions on distribution shift into our DRSL. We will discuss how this can be done in practice at the end of this section. For notational convenience, let ws ≡ w(s), 1 ≤ s ≤ S, and define w ≡ (w1, . . . , wS). Equations (16) and (17) can be empirically approximated as follows using D′:\nmin θ sup w∈Ŵf\n1 N\nS∑\ns=1\nnswsR̂s(θ)\n︸ ︷︷ ︸ ≡ R̂s-adv(θ)\n(19)\nŴf = { w ∈ RS ∣∣∣∣∣ 1 N S∑\ns=1\nnsf (ws) ≤ δ,\n1 N\nS∑\ns=1\nnsws = 1, w ≥ 0 } , (20)\nwhere ns is the cardinality of Gs and R̂s(θ)(≡ 1 ns ∑ i∈Gs ℓi(θ)) is the average loss of all data points in Gs. We call R̂s-adv(θ) the structural adversarial empirical risk and call the minimization problem of Eq. (19) the structural adversarial empirical risk minimization (structural AERM). We can add regularization term Ω(θ) to Eq. (19) to prevent overfitting.\nConvergence rate and estimation error: We establish the convergence rate of the model parameter and the order of the estimation error for structural AERM in terms of the number of training data points N . Due to the limited space, we only present an informal statement here. The formal statement can be found in Appendix G and its proof can be found in Appendix H.\nTheorem 4 (Convergence rate and estimation error, informal statement). Let θ∗ be the solution of structural ARM, and θ̂N be the solution of regularized structural AERM given training data of size N . Assume gθ(x) is linear in θ, and regularization hyper-parameter λ decreases at a rate of O(N−1/2). Under mild conditions, as N → ∞, we have ∥θ̂N − θ∗∥2 = O(N−1/4) and consequently, |Rs-adv(θ̂N )−Rs-adv(θ∗)| = O(N−1/4).\nNotice that the convergence rate of θ̂N to θ∗ is not the optimal parametric rate O(N−1/2). This is because the inner maximization of Eq. (19) converges in O(N−1/4) that slows down the entire convergence rate. Theorem 4 applies to any f -divergence where f(t) is nonlinear in t, while knowing which f -divergence is used may improve the result to the optimal parametric rate.\nDiscussion on groupings: In our structural ARM and AERM, users need to incorporate their structural assumptions by grouping training data points. Here, we discuss how this can be done in practice.\nMost straightforwardly, a user of our DRSL may assume\nclass prior change (Saerens et al., 2002) or sub-category7 prior change. To incorporate such assumptions into our DRSL, the user can simply group training data by class labels or a sub-categories, respectively.\nAlternatively, a user of our DRSL can group data by available meta-information of data such as time and places in which data are collected. The intuition is that data collected in the same situations (e.g., time and places) are likely to “share the same destiny” in the future distribution shift; hence, it is natural to assume that only the prior over the situations changes at test time while the conditionals remain the same.\nIn any case, it is crucial that the users provide structural assumptions on distribution shift so that we can overcome the pessimism of ARM and AERM for classification raised in Section 3."
  }, {
    "heading": "5. Efficient Learning Algorithms",
    "text": "In this section, we derive efficient gradient-based learning algorithms for our structural AERM in Eq. (19). Thanks to Danskin’s theorem (Danskin, 1966), we can obtain the gradient ∇θR̂s-adv(θ) as\n∇θR̂s-adv(θ) = 1\nN\nS∑\ns=1\nnsw ∗ s∇θR̂s(θ), (21)\nwhere w∗ = (w∗1 , . . . , w∗S) is the solution of inner maximization of AERM in Eq. (19).\nIn the following, we show that w∗ can be obtained very efficiently for two well-known instances of f -divergences.\nKullback-Leibler (KL) divergence: For the KL divergence, f(x) = x log x, we have\nw∗s = N\nZ(γ) · exp ( R̂s(θ) γ ) , 1 ≤ s ≤ S, (22)\nwhere γ is a scalar such that the first constraint of Ŵf in Eq. (20) holds with equality, and Z(γ) ≡∑S\ns=1 nsexp ( R̂s(θ)/γ ) is a normalizing constant in or-\nder to satisfy the second constraint of Ŵf . To compute γ, we can simply perform a binary search.\nPearson (PE) divergence: For the PE divergence, f(x) = (x − 1)2. For small δ, w ≥ 0 of Ŵf is often satisfied in practice. We drop the inequality for simplicity; then, the solution of the inner maximization of Eq. (19) becomes analytic and efficient to obtain:\nw∗ =\n√ Nδ\n∑S s=1 nsv 2 s v + 1S , (23)\n7A sub-category (Ristin et al., 2015) is a refined category of a class label, e.g., a “flu” label contains three sub-categories: types A, B, and C flu.\nwhere 1S is the S-dimensional vector with all the elements equal to 1. v is the S-dimensional vector such that vs = R̂s(θ)− R̂(θ), 1 ≤ s ≤ S.\nComputational complexity: The time complexity for obtaining w∗ is: O(mS) for the KL divergence and O(S) for the PE divergence, where m is the number of the binary search iterations to compute γ in Eq. (22). Calculating the adversarial weights therefore adds negligible computational overheads to computing ∇ℓi(θ) and ℓi(θ) for 1 ≤ i ≤ N , which for example requires O(Nb)-time for a b-dimensional linear-in-parameter model."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we experimentally analyze our DRSL (structural AERM) in classification by comparing it with ordinary ERM and DRSL with f -divergences (AERM). We empirically demonstrate (i) the undesirability of AERM in classification and (ii) the robustness of structural AERM against specified distribution shift.\nDatasets: We obtained six classification datasets from the UCI repository (Blake & Merz, 1998), two of which are for multi-class classification. We also obtained MNIST (LeCun et al., 1998) and 20newsgroups (Lang, 1995). Refer to Appendix I for the details of the datasets.\nEvaluation metrics: We evaluated the three methods (ordinary ERM, AERM and structural AERM) with three kinds of metrics: the ordinary risk, the adversarial risk, and the structural adversarial risk, where the 0-1 loss is used for all the metrics.8 We did not explicitly report the adversarial risk in our experiments because of Theorem 1.\nBoth the risk and structural adversarial risk are estimated using held-out test data. In particular, the structural adversarial risk can be estimated similarly to Eqs. (19) and (20), i.e., calculating the mis-classification rate on the held-out test data and structurally and adversarially reweight them. See discussion of Eq. (18) for why the structural adversarial risk is a meaningful evaluation metric to measure distributional robustness of classifiers.\nExperimental protocols: For our DRSL, we consider learning classifiers robust against (a) the class prior change and (b) the sub-category prior change. This corresponds to grouping training data by (a) class labels and (b) subcategory labels, respectively. In the benchmark datasets, the sub-category labels are not available. Hence, we manually created such labels as follows. First, we converted the original multi-class classification problems into classification problems with fewer classes by integrating some classes together. Then, the original class labels are regarded as the subcategories. In this way, we converted the satimage, letter and MNIST datasets into binary classification problems, and 20newsgroups into a 7-class classifica-\n8To gain more insight on the methods, we also reported all the metrics in terms of the surrogate loss in Appendix K.\ntion. Appendix J details how we grouped the class labels.\nFor all the methods, we used linear models with softmax output for the prediction function gθ(x). The cross-entropy loss with ℓ2 regularization was adopted. The regularization hyper-parameter λ was selected from {1.0, 0.1, 0.01, 0.001, 0.0001} via 5-fold cross validation.\nWe used the two f -divergences (the KL and PE divergences) and set δ = 0.5 for AERM and structural AERM. The same δ and f -divergence were used for estimating the structural adversarial risk. At the end of this section, we discuss how we can choose δ in practice. Results: In Table 1, we report experimental results on the classification tasks when the KL divergence is used. Refer to Appendix L for the results when the PE divergence is used, which showed similar tendency.\nWe see from the left half of Table 1 that ordinary ERM achieved lower estimated risks as expected. On the other hand, we see from the entire Table 1 that AERM, which does not incorporate any structural assumptions on distribution shift, performed poorly in terms of both of two evaluation metrics; hence, it also performed poorly in terms of the adversarial risk (see Theorem 1). This may be because AERM was excessively sensitive to outliers as implied by Theorem 3. We see from the right half of Table 1 that structural AERM achieved significantly lower estimated structural adversarial risks. Although this was expected, our experiments confirmed that structural AERM indeed obtained classifiers robust against the structural distribution shift.9\n9When we used the surrogate loss to evaluate the methods\nDiscussion: Here we provide an insight for users to determine δ in our DRSL (structural ARM and AERM). We see from Eq. (18) that the structural adversarial risk can be decomposed into the sum of the ordinary risk and the sensitivity term, where δ acts as a trade-off hyper-parameter between the two terms. In practice, users of our DRSL may want to have good balance between the two terms, i.e., the learned classifier should achieve high accuracy on the training distribution while being robust to specified distribution shift. Since both terms in Eq. (18) can be estimated by cross validation, the users can adjust δ of AERM at training time to best trade-off the two terms for their purposes, e.g., increasing δ during training to decrease the sensitivity term at the expense of a slight increase of the risk term."
  }, {
    "heading": "7. Conclusion",
    "text": "In this paper, we theoretically analyzed DRSL with f - divergences applied to classification. We showed that the DRSL ends up giving a classifier optimal for the training distribution, which is too pessimistic in terms of the original motivation of distributionally robust classification. To rectify this, we presented simple DRSL that gives a robust classifier based on structural assumptions on distribution shift. We derived efficient optimization algorithms for our DRSL and empirically demonstrated its effectiveness. (which is not the case in ordinary classification), we confirmed that the methods indeed achieved the best performance in terms of the metrics they optimized for, i.e., ERM, AERM, and structural AERM performed the best in terms of the ordinary risk, adversarial risk and structural adversarial risk, respectively. See Appendix K for the actual experimental results."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank anonymous reviewers for their constructive feedback. WH was supported by JSPS KAKENHI 18J22289. MS was supported by CREST JPMJCR1403."
  }],
  "year": 2018,
  "references": [{
    "title": "Robust supervised learning",
    "authors": ["J.A. Bagnell"],
    "venue": "In Proceedings of Association for the Advancement of Artificial Intelligence,",
    "year": 2005
  }, {
    "title": "Robust solutions of optimization problems affected by uncertain probabilities",
    "authors": ["A. Ben-Tal", "D. Den Hertog", "A. De Waegenaere", "B. Melenberg", "G. Rennen"],
    "venue": "Management Science,",
    "year": 2013
  }, {
    "title": "Robust wasserstein profile inference and applications to machine learning",
    "authors": ["J. Blanchet", "Y. Kang", "K. Murthy"],
    "venue": "arXiv preprint arXiv:1610.05627,",
    "year": 2016
  }, {
    "title": "Optimization problems with perturbations, a guided tour",
    "authors": ["J.F. Bonnans", "A. Shapiro"],
    "venue": "SIAM Review,",
    "year": 1998
  }, {
    "title": "A Course in Probability Theory",
    "authors": ["K.L. Chung"],
    "year": 1968
  }, {
    "title": "Information-type measures of difference of probability distributions and indirect observations",
    "authors": ["I. Ciszar"],
    "venue": "Studia Sci. Math. Hungar.,",
    "year": 1967
  }, {
    "title": "The theory of max-min, with applications",
    "authors": ["J.M. Danskin"],
    "venue": "SIAM Journal on Applied Mathematics,",
    "year": 1966
  }, {
    "title": "Statistics of robust optimization: A generalized empirical likelihood approach",
    "authors": ["J. Duchi", "P. Glynn", "H. Namkoong"],
    "venue": "arXiv preprint arXiv:1610.03425,",
    "year": 2016
  }, {
    "title": "Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations",
    "authors": ["P.M. Esfahani", "D. Kuhn"],
    "venue": "arXiv preprint arXiv:1505.05116,",
    "year": 2015
  }, {
    "title": "Nightmare at test time: robust learning by feature deletion",
    "authors": ["A. Globerson", "S. Roweis"],
    "venue": "In Proceedings of International Conference on Machine learning,",
    "year": 2006
  }, {
    "title": "Fairness without demographics in repeated loss minimization",
    "authors": ["T.B. Hashimoto", "M. Srivastava", "H. Namkoong", "P. Liang"],
    "venue": "In Proceedings of International Conference on Machine learning,",
    "year": 2018
  }, {
    "title": "Newsweeder: Learning to filter netnews",
    "authors": ["K. Lang"],
    "venue": "In Proceedings of International Conference on Machine Learning,",
    "year": 1995
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "In Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Robust classification under sample selection bias",
    "authors": ["A. Liu", "B. Ziebart"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2014
  }, {
    "title": "Stochastic gradient methods for distributionally robust optimization with fdivergences",
    "authors": ["H. Namkoong", "J.C. Duchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Variance-based regularization with convex objectives",
    "authors": ["H. Namkoong", "J.C. Duchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Dataset shift in machine learning",
    "authors": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"],
    "year": 2009
  }, {
    "title": "From categories to subcategories: large-scale image classification with partial class label refinement",
    "authors": ["M. Ristin", "J. Gall", "M. Guillaumin", "L. Van Gool"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "A characterization of stability in linear programming",
    "authors": ["S.M. Robinson"],
    "venue": "Operations Research,",
    "year": 1977
  }, {
    "title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure",
    "authors": ["M. Saerens", "P. Latinne", "C. Decaestecker"],
    "venue": "Neural Computation,",
    "year": 2002
  }, {
    "title": "Certifiable distributional robustness with principled adversarial training",
    "authors": ["A. Sinha", "H. Namkoong", "J. Duchi"],
    "venue": "arXiv preprint arXiv:1710.10571,",
    "year": 2017
  }, {
    "title": "Mixture regression for covariate shift",
    "authors": ["A.J. Storkey", "M. Sugiyama"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2007
  }, {
    "title": "On the consistency of multiclass classification methods",
    "authors": ["A. Tewari", "P.L. Bartlett"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2007
  }],
  "id": "SP:07b5093aace8e485e7d23b83edb6351618138127",
  "authors": [{
    "name": "Weihua Hu",
    "affiliations": []
  }, {
    "name": "Gang Niu",
    "affiliations": []
  }, {
    "name": "Issei Sato",
    "affiliations": []
  }, {
    "name": "Masashi Sugiyama",
    "affiliations": []
  }],
  "abstractText": "Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f -divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.",
  "title": "Does Distributionally Robust Supervised Learning Give Robust Classifiers?"
}