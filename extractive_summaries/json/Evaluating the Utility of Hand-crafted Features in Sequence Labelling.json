{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Deep neural networks have been proven to be a powerful framework for natural language processing, and have demonstrated strong performance on a number of challenging tasks, ranging from machine translation (Cho et al., 2014b,a), to text categorisation (Zhang et al., 2015; Joulin et al., 2017; Liu et al., 2018b). Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering. For instance, both Lample et al. (2016) and Ma and Hovy (2016) propose end-to-end models for sequence labelling task and achieve state-of-the-art results.\n∗https://github.com/minghao-wu/CRF-AE †Work carried out at The University of Melbourne\nOrthogonal to the advances in deep learning is the effort spent on feature engineering. A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005). Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics.\nThe work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings). Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering.\nMore recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they\nar X\niv :1\n80 8.\n09 07\n5v 1\n[ cs\n.C L\n] 2\n8 A\nug 2\n01 8\nintroduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (BiLSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF). Their model is highly capable of capturing not only word- but also characterlevel features. We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial. Perhaps the closest to this study is the works by Ammar et al. (2014) and Zhang et al. (2017), who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings.\nWith our proposed model, we achieve strong performance on the CoNLL 2003 English NER shared task with an F1 of 91.89, significantly outperforming an array of competitive baselines. We conduct an ablation study to better understand the impacts of each manually-crafted feature. Finally, we further provide an in-depth analysis of model performance when trained with varying amount of data and show that the proposed model is highly competent with only 60% of the training set."
  }, {
    "heading": "2 Methodology",
    "text": "In this section, we first outline the model architecture, then the manually crafted features, and finally how they are incorporated into the model."
  }, {
    "heading": "2.1 Model Architecture",
    "text": "We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first\nintroduced by Ma and Hovy (2016). Given an input sequence of x = {x1, x2, . . . , xT } of length T , the model is capable of tagging each input with a predicted label ŷ, resulting in a sequence of ŷ = {ŷ1, ŷ2, . . . , ŷT } closely matching the gold label sequence y = {y1, y2, . . . , yT }. Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance. Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1.\nChar-CNN. Previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016) have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in Ma and Hovy (2016) but differs in that we use a ReLU activation (Nair and Hinton, 2010).1\nBi-LSTM. We use a Bi-LSTM to learn contextual information of a sequence of words. As inputs to the Bi-LSTM, we first concatenate the pre-trained embedding of each word wi with its character-level representation cwi (the output of the char-CNN) and a vector of manually crafted features fi (described in Section 2.2):\n−→ h i = −−−−→ LSTM( −→ h i−1, [wi; cwi ;fi]) (1) ←− h i = ←−−−− LSTM( ←− h i+1, [wi; cwi ;fi]) , (2)\nwhere [; ] denotes concatenation. The outputs of the forward and backward pass of the Bi-LSTM is then concatenated hi = [ −→ h i; ←− h i] to form the output of the Bi-LSTM, where dropout is also applied.\nCRF. For sequence labelling tasks, it is intuitive and beneficial to utilise information carried between neighbouring labels to predict the best sequence of labels for a given sentence. Therefore,\n1While the hyperbolic tangent activation function results in comparable performance, the choice of ReLU is mainly due to faster convergence.\nwe employ a conditional random field layer (Lafferty et al., 2001) taking as input the output of the Bi-LSTM hi. Training is carried out by maximising the log probability of the gold sequence: LCRF = log p(y|x) while decoding can be efficiently performed with the Viterbi algorithm.\nAuto-encoder loss. Alongside sequence labelling as the primary task, we also deploy, as auxiliary tasks, three auto-encoders for reconstructing the hand-engineered feature vectors. To this end, we add multiple independent fully-connected dense layers, all taking as input the Bi-LSTM output hi with each responsible for reconstructing a particular type of feature: f̂ ti = σ(W\nthi) where σ is the sigmoid activation function, t denotes the type of feature, and W t is a trainable parameter matrix. More formally, we define the auto-encoder loss as:\nLtAE = T∑ i=0 XEntropy(f ti , f̂ t i ) . (3)\nModel training. Training is carried out by optimising the joint loss:\nL = LCRF + ∑ t λtLtAE , (4)\nwhere, in addition to LCRF , we also add the autoencoder loss, weighted by λt. In all our experiments, we set λt to 1 for all ts."
  }, {
    "heading": "2.2 Hand-crafted Features",
    "text": "We consider three categories of widely used features: (1) POS tags; (2) word shape; and (3) gazetteers and present an example in Table 1. While POS tags carry syntactic information regarding sentence structure, the word shape feature focuses on a more fine-grained level, encoding character-level knowledge to complement the loss of information caused by embedding lookup, such as capitalisation. Both features are based on the implementation of spaCy.2 For the gazetteer fea-\n2https://spacy.io/\nture, we focus on PERSON and LOCATION and compile a list for each. The PERSON gazetteer is collected from U.S. census 2000, U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION, taking in both official and alternative names. All the tokens on both lists are then filtered to exclude frequently occurring common words.3 Each category is converted into a one-hot sparse feature vector f ti and then concatenated to form a multi-hot vector fi = [fPOSi ;f shape i ;f gazetteer i ] for the i-th word. In addition, we also experimented with including the label of the incoming dependency edge to each word as a feature, but observed performance deterioration on the development set. While we still study and analyse the impacts of this feature in Table 3 and Section 3.2, it is excluded from our model configuration (not considered as part of fi unless indicated otherwise)."
  }, {
    "heading": "3 Experiments",
    "text": "In this section, we present our experimental setup and results for name entity recognition over the CoNLL 2003 English NER shared task dataset (Tjong Kim Sang and De Meulder, 2003)."
  }, {
    "heading": "3.1 Experimental Setup",
    "text": "Dataset. We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016).\n3Gazetteer data is included in the code release.\nModel configuration. Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300- dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in\nthe range [− √\n3 dim ,+\n√ 3\ndim ]. Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9. Exponential learning rate decay is applied every 5 epochs with a factor of 0.8. To reduce the impact of exploding gradients, we employ gradient clipping at 5.0 (Pascanu et al., 2013).\nWe train our models on a single GeForce GTX TITAN X GPU. With the above hyper-parameter setting, training takes approximately 8 hours for a full run of 40 epochs.\nEvaluation. We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set. We report average F-scores and standard deviation over 5 runs for our model.\nBaseline. In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2, we also re-implement the BiLSTM-CNN-CRF model by Ma and Hovy (2016) (referred to as Neural-CRF in Table 2) and report its average performance."
  }, {
    "heading": "3.2 Results",
    "text": "The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model.\nAblation Study To gain a better understanding of the impacts of each feature, we perform an ab-\nlation study and present the results in Table 3. We observe performance degradation when eliminating POS, word shape and gazetteer features, showing that each feature contributes to NER performance beyond what is learned through deep learning alone. Interestingly, the contribution of gazetteers is much less than that of the other features, which is likely due to the noise introduced in the matching process, with many incorrectly identified false positives.\nIncluding features based on dependency tags into our model decreases the performance slightly. This might be a result of our simple implementation (as illustrated in Table 1), which does not include dependency direction, nor parent-child relationships.\nNext, we investigate the impact of different means of incorporating manually-engineered features into the model. To this end, we experiment with three configurations with features as: (1) input only; (2) output only (equivalent to multi-task learning); and (3) both input and output (Neural-CRF+AE) and present the results in Table 4. Simply using features as either input or output only improves model performance slightly, but insignificantly so. It is only when features are incorporated with the proposed auto-encoder loss do we observe a significant performance boost.\nTraining Requirements Neural systems typically require a large amount of annotated data. Here we measure the impact of training with varying amount of annotated data, as shown in Figure 2. Wtih the proposed model architecture, the amount of labelled training data can be drastically reduced: our model, achieves comparable performance against the baseline Neural-CRF, with as little as 60% of the training data. Moreover, as we increase the amount of training text, the performance of Neural-CRF+AE continues to improve.\nHyperparameters Three extra hyperparameters are introduced into our model, controlling the weight of the autoencoder loss relative to the CRF loss, for each feature type. Figure 3 shows the effect of each hyperparameter on test performance. Observe that setting λi = 1 gives strong performance, and that the impact of the gazetteer is less marked than the other two feature types. While increasing λ is mostly beneficial, performance drops if the λs are overly large, that is, the auto-encoder loss overwhelms the main prediction task."
  }, {
    "heading": "4 Conclusion",
    "text": "In this paper, we set out to investigate the utility of hand-crafted features. To this end, we have presented a hybrid neural architecture to validate this hypothesis extending a Bi-LSTM-CNN-CRF by incorporating an auto-encoder loss to take manual features as input and then reconstruct them. On the task of named entity recognition, we show significant improvements over a collection of competitive baselines, verifying the value of such features. Lastly, the method presented in this work can also be easily applied to other tasks and models, where hand-engineered features provide key insights about the data."
  }],
  "year": 2018,
  "references": [{
    "title": "Conditional random field autoencoders for unsupervised structured prediction",
    "authors": ["Waleed Ammar", "Chris Dyer", "Noah A Smith."],
    "venue": "Proceedings of the 27th International Conference on Neural Informa-",
    "year": 2014
  }, {
    "title": "A framework for learning predictive structures from multiple tasks and unlabeled data",
    "authors": ["Rie Kubota Ando", "Tong Zhang."],
    "venue": "Journal of Machine Learning Research, 6(Nov):1817–1853.",
    "year": 2005
  }, {
    "title": "Improved neural machine translation with a syntax-aware encoder and decoder",
    "authors": ["Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017),",
    "year": 2017
  }, {
    "title": "Named entity recognition: A maximum entropy approach using global information",
    "authors": ["Hai Leong Chieu", "Hwee Tou Ng."],
    "venue": "Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), pages 1–7.",
    "year": 2002
  }, {
    "title": "Named entity recognition with bidirectional lstm-cnns",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:357–370.",
    "year": 2016
  }, {
    "title": "On the properties of neural machine translation: Encoder–decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
    "year": 2014
  }, {
    "title": "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Named entity recognition through classifier combination",
    "authors": ["Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang."],
    "venue": "Proceedings of the Seventh Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2003
  }, {
    "title": "Bidirectional lstm-crf models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "arXiv preprint arXiv:1508.01991.",
    "year": 2015
  }, {
    "title": "Bag of tricks for efficient text classification",
    "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017), pages",
    "year": 2017
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira."],
    "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Modeling source syntax for neural machine translation",
    "authors": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), pages",
    "year": 2017
  }, {
    "title": "Narrative modeling with memory chains and semantic supervision",
    "authors": ["Fei Liu", "Trevor Cohn", "Timothy Baldwin."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 278–284.",
    "year": 2018
  }, {
    "title": "Recurrent entity networks with delayed memory update for targeted aspect-based sentiment analysis",
    "authors": ["Fei Liu", "Trevor Cohn", "Timothy Baldwin."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
    "year": 2018
  }, {
    "title": "Joint entity recognition and disambiguation",
    "authors": ["Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), pages 879–888.",
    "year": 2015
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 1064–1074.",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the 1st International Conference on Learning Representations (ICLR 2013).",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the 26th International Conference on Neural Information Processing Sys-",
    "year": 2013
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["Vinod Nair", "Geoffrey E Hinton."],
    "venue": "Proceedings of the 27th international conference on machine learning (ICML 2010), pages 807–814.",
    "year": 2010
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML 2013), pages 1310–1318.",
    "year": 2013
  }, {
    "title": "Lexicon infused phrase embeddings for named entity resolution",
    "authors": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."],
    "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning (CoNLL 2014), pages 78–86.",
    "year": 2014
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the As-",
    "year": 2018
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["Lev Ratinov", "Dan Roth."],
    "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009), pages 147–155.",
    "year": 2009
  }, {
    "title": "Learning character-level representations for part-of-speech tagging",
    "authors": ["Cicero D Santos", "Bianca Zadrozny."],
    "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML 2014), pages 1818–1826.",
    "year": 2014
  }, {
    "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder."],
    "venue": "Proceedings of the Seventh Conference of the North American Chapter of the Association for Computa-",
    "year": 2003
  }, {
    "title": "Sequence-to-dependency neural machine translation",
    "authors": ["Shuangzhi Wu", "Dongdong Zhang", "Nan Yang", "Mu Li", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), pages 698–707.",
    "year": 2017
  }, {
    "title": "Neural reranking for named entity recognition",
    "authors": ["Jie Yang", "Yue Zhang", "Fei Dong."],
    "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLP 2017), pages 784–792.",
    "year": 2017
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS 2015), pages 649–657.",
    "year": 2015
  }, {
    "title": "Semi-supervised structured prediction with neural crf autoencoder",
    "authors": ["Xiao Zhang", "Yong Jiang", "Hao Peng", "Kewei Tu", "Dan Goldwasser."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017),",
    "year": 2017
  }],
  "id": "SP:f1b9b6d24e0bc0bcaedfbcf4d71e18edbeed91fe",
  "authors": [{
    "name": "Minghao Wu",
    "affiliations": []
  }, {
    "name": "Fei Liu",
    "affiliations": []
  }, {
    "name": "Trevor Cohn",
    "affiliations": []
  }],
  "abstractText": "Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for partof-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of autoencoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.",
  "title": "Evaluating the Utility of Hand-crafted Features in Sequence Labelling∗"
}