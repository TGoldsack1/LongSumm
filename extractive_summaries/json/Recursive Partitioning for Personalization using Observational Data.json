{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Personalization is the problem of determining the best treatment option for a given instance. A treatment can, for example, be a movie recommendation (Zhou et al., 2008), a display ad (Goldfarb & Tucker, 2011), or a pharmacological therapy (Lesko, 2007), and an instance is usually an individual person. In this paper, we study the problem of learning how to personalize from observational data, which is an important problem in emergent contexts such as personalized medicine. In this and related contexts, experimentation can be prohibitively small-scale, costly, dangerous, and unethical in comparison to passive data collection,\n1School of Operations Research and Information Engineering and Cornell Tech, Cornell University. Correspondence to: Nathan Kallus <kallus@cornell.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhich can be potentially massive as in electronic medical records (EMRs) but, at the same time, lack experimental manipulation so that isolated causal effects of specific treatments are hidden by confounding factors. We show that standard approaches that pose the problem as multiple supervised learning tasks fall short in this setting and propose new learning algorithms as well as evaluation methods used for validation, selection, and tuning.\nSpecifically, we consider the problem of learning how to assign the best of m treatments to an instance, given an observation of associated baseline covariates x ∈ Rd. An instance is characterized by the random variables X ∈ Rd and Y (1), . . . , Y (m) ∈ R, which denote the covariates and the m potential outcomes of applying each of the treatments (Imbens & Rubin, 2015, Chs. 1-2). We use the convention that smaller outcome is better. A personalization model is a map τ : Rd → [m] = {1, . . . ,m} that, given an observation of covariates x, prescribes a treatment τ(x). Its (out-of-sample) personalization risk is its average causal effect in the population R(τ) = E [Y (τ(X))] (the expectation is taken with respect to the joint distribution of X,Y (1), . . . , Y (m)). The Bayes optimal risk is R∗ = R(τ∗), where τ∗(x) ∈ T ∗(x) = arg mint∈[m] E [Y (t) | X = x] is the Bayes optimal personalization model.\nThe learning task is to train a personalization model τ̂n(·) on n data points: Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)} , where the observed outcome Yi = Yi(Ti) corresponds only to the treatment Ti administered. This data is observational: we may not control the historic administration of treatment (as we would in a controlled experiment) and the values Yi(t) for t 6= Ti are missing data. We assume the data is independent and identically distributed (iid) and let X,T, Y, Y (1), . . . , Y (m) represent a generic draw. Although the data is iid, the t-treated sample {i : Ti = t} differs systematically from the sample t′-treated {i : Ti = t′} for t 6= t′, i.e., not just by chance as in a randomized controlled trial (RCT). Our second assumption about the data is unconfoundedness:\nAssumption 1. For each t ∈ [m]: Y (t) is independent of T given X and T = t is possible for almost every X , i.e., Y (t) ⊥ T | X and P (P (T = t | X) > 0) = 1.\nThe assumption is standard in causal effect estimation\n(see e.g. Kallus, 2016; 2017b) for ensuring identifiability (Rosenbaum & Rubin, 1983). It says that we measured the right covariates to separate the effect of the treatment itself from the effect of assignment. In the context of personalized medicine, this assumption would be justified if the EMR contained all the patient information used by a doctor to prescribe treatment up to the vagaries and idiosyncrasies of individual doctors or hospitals. Under Asn. 1, the conditional causal effect is equal to regressing Y on X,T :\nE [Y (t) | X = x] = E [Y (t) | X = x, T = t] = E [Y | X = x, T = t] ."
  }, {
    "heading": "1.1. Standard Approach: Regress and Compare",
    "text": "Since under Asn. 1 the optimal model τ∗ chooses a treatment by minimizing among m regression functions, one obvious approach to personalization is to estimate these regression functions, fitting each to the subset of the data that received each treatment, and then use these to predict outcomes and pick the smallest prediction. For example, in medicine, there is a vast literature on predicting patient-specific responses to treatment (Feldstein et al., 1978; Stoehlmacher et al., 2004) and picking the best by comparing (Qian & Murphy, 2011; Bertsimas et al., 2017).\nThe same approach is also generally taken in the contextual multi-arm bandit problem (Li et al., 2010), which is similar to our problem with the differences that we consider an offline learning problem and that bandit arm pulls are controlled interventions (a bandit problem is essentially a dynamic RCT) so the treated subpopulations are always statistically equivalent. The standard solution is to fit m regression functions, and, for a new instance, predict m outcomes and pick the smallest prediction subject to cleverly ensuring sufficient exploration by, e.g., adding confidence bounds that vanish with n. The regression, assumed linear, is done using ridge regression as in Li et al. (2010) (LinUCB), ordinary least squares (OLS) as in Goldenshluger & Zeevi (2013), or LASSO as in Bastani & Bayati (2016).\nThe regress and compare (R&C) approach to personalization from observational data can be summarized as:\n1. For each t ∈ [m]: (a) Consider the t-treated subsample St,nt = {(Xi, Yi) :\ni ∈ [n], Ti = t} of size nt = ∑n i=1 I [Ti = t].\n(b) Fit a regression model µ̂t,nt(x) of the response Y to regressors X using training data St,nt , e.g., by OLS. 1\n1Note that separate OLS on each subsample St,nt is equivalent to OLS on the whole sample if we include interaction terms with dummy variables I [Ti = t]. At the same time, OLS on the whole sample without interaction terms provides no personalization, i.e., τ̂R&Cn (x) is constant. Similarly, separate nonparametric regressions on each subsample is equivalent to using the whole sample and endowing the T variable with a discrete topology.\n2. Personalize by choosing the best predicted outcome: τ̂R&Cn (x) = arg mint∈[m] µ̂t,nt(x).\nUnder Asn. 1, if our regression estimator is consistent then so is R&C personalization consistent as shown below. All proofs are given in the supplementary materials.\nTheorem 1. If Asn. 1 holds and µ̂t,nt(x) are pointwise consistent regressions, i.e., µ̂t,nt(X) → E [Y | X,T = t] almost surely (a.s.) ∀t ∈ [m], then τ̂R&Cn (X) ∈ T ∗(X) eventually a.s.\nExamples of pointwise consistent regression estimators are k-nearest neighbors (kNN) and kernel regression (Walk, 2010). If a linear model is well-specified, then OLS is also pointwise consistent. In practice, however, R&C is not effective for personalization because it attempts to learn much more than it needs to, it splits the training data into m, and in training it addresses estimation or prediction risk rather than personalization risk."
  }, {
    "heading": "1.2. Other Related Problems and Approaches",
    "text": "In learning heterogeneous causal effects, one is concerned with the case of observational data with two treatments, t = 1 (“Control”) and t = 2 (“Treatment”), and the estimation of the relative conditional average treatment effect (CATE), δ(x) = E [Y (2)− Y (1) | X = x]. Under Asn. 1, CATE is the difference between two regression functions: δ(x) = E [Y | X = x, T = 2]−E [Y | X = x, T = 1]. And so one way to estimate it is by regressing outcome in each treatment population and taking differences. When the conditioning variables in CATE are a proper subset of the covariates needed to satisfy Asn. 1, Abrevaya et al. (2015) propose estimates based on propensity-score weighting and kernel regression. Athey & Imbens (2016) develop the Causal Tree (CT), which uses recursive partitioning, as an alternative to differencing two CART regressions.\nFor personalization, learning heterogeneous effects can be used to choose between two treatments by comparing an estimate of their relative CATE to zero. As a learning problem, however, this addresses estimation risk rather than personalization risk and deals only with two treatments. In Sec. 2.6 we propose one-vs-one and one-vs-all strategies for personalization using CATE estimates and show it is consistent. We compare to this strategy using CT in our empirical investigation.\nIn learning from logged bandit feedback (Beygelzimer & Langford, 2009; Swaminathan & Joachims, 2015a;b; Kallus, 2017a), one is concerned with learning a good policy for a contextual multi-arm bandit problem based on logged data from another, known policy, rather than online interactions. This problem differs in that it assumes the data is experimental and the policy that generated the data is known and available. In Sec. 2.6, we discuss adapt-\ning these methods using imputed estimated propensities, to which we compare in our empirical investigation."
  }, {
    "heading": "2. Recursive Partitioning for Personalization",
    "text": "In this section we present three new algorithms that tackle personalization directly as a single learning task."
  }, {
    "heading": "2.1. Recasting the Problem",
    "text": "We begin by reformulating personalization risk. Following Hirano & Imbens (2004, Def. 2.1), we define the generalized propensity score (GPS) as Q = φ(T,X), where φ(t, x) = P (T = t | X = x). The GPS of subject i, Qi, is an unknown quantity given by taking the unknown φ(t, x) and plugging in the known variables Ti, Xi. Using the GPS we can relate the personalization risk of a personalization model τ to its accuracy as a classification model for labels T , weighted by outcome and GPS.\nTheorem 2. Under Asn. 1,\nR(τ) = E [I [T = τ(X)]Y/Q] . (1)\nFor m = 2 and randomized data (φ(1, x) = π constant), Zhao et al. (2012) is a special case of Thm. 2. Thm. 2 suggests using a weighted form of empirical classification risk minimization. When Q is fully known as in the logged bandit setting, this approach is closely related to the approach taken by Beygelzimer & Langford (2009); Swaminathan & Joachims (2015a;b). In the observational setting, we explore estimating and imputing Q to use this approach in Sec. 2.6. However, because estimating the GPS generally either relies heavily on model specification or, in nonparametric settings, can be biased and variable, this will lead to severe instability and limited practical use. Moreover, it does not address the personalization problem as a single learning task, rather as two: learning a propensity model task and then a weighted classification task. Instead, in the following sections we present a single-task approach that does not rely on estimating propensities separately."
  }, {
    "heading": "2.2. An Impurity Measure for Personalization",
    "text": "Classification and regression trees (CART) are predictive models based on recursive partitioning: the covariate space is recursively partitioned by axis-aligned hyperplanes (x` ≤ θ for ` ∈ [d] and θ ∈ R) in order to minimize a within-partition impurity measure (Breiman et al., 1984). Impurities for classification include entropy and Gini and for regression include sum of squared errors. Athey & Imbens (2016) develop impurities for estimating heterogeneous effects. Motivated by Thm. 2, we develop an impurity for personalization leading to a recursive partitioning algorithm called personalization tree (PT).\nNote that for a subset X ⊆ Rd such that X ⊥ T | X ∈ X , we have that φ(t, x) = P (T = t | X ∈ X ) whenever x ∈ X . To develop the personalization impurity, we use this to establish the following as a corollary of Thm. 2:\nCorollary 3. Consider a fixed partition of the covariate space: Rd = X1 ∪ · · · ∪XL where X` ∩X`′ = ∅ whenever ` 6= `′. Suppose that the partition is sufficiently fine so that\nX ⊥ T | X ∈ X` ∀` = 1, . . . , L. (2)\nThen, R̂n(τ) = ∑L `=1 R̂n,X`(τ) is an unbiased and consistent estimator for R(τ), where\nR̂n,X (τ) = ∑n i=1 I[Xi∈X ]\nn\n∑n i=1 I[Xi∈X ,Ti=τ(Xi)]Y∑n i′=1 I[Xi′∈X ,Ti′=τ(Xi)] .\nNote that the conditional independence in the condition (2), which requires that leaf membership be a balancing score as defined by Rosenbaum & Rubin (1983), holds trivially when we condition on X itself. Therefore, the finer the partition X1, . . . ,XL, the more accurate this assumption. (while possibly not truly satisfiable by any finite partition). Given a partition, we can use this risk estimate to optimize τ . Letting S̃` = {(Xi, Ti, Yi) : Xi ∈ X`}, we can rewrite\nminτ :Rd→[m] n ∑L `=1 R̂n,X`(τ) = ∑L `=1 Ipers(S̃`),\nwhere Ipers is the personalization impurity given by\nIpers({(Xi1 , Ti1 , Yi1), . . . , (Xik , Tik , Yik)})\n= mint∈[m] k( ∑k j=1 I[Tij=t]Yij )∑k j=1 I[Tij=t] ,\nTherefore, to achieve good personalization risk we may wish to seek partitions that have minimal sum of withinpartition personalization impurities as defined by Ipers."
  }, {
    "heading": "2.3. Personalization Tree",
    "text": "The PT algorithm attempts to find a fine partition of the data to minimize the sum of within-partition personalization impurities. It does so by partitioning the dataset along axis-aligned cuts, at each stage choosing to cut a partition into the two partitions with least sum of impurities and proceeding recursively. In an attempt to find a fine partition\nthat satisfies condition (2), PT continues to recurse until a specified stopping criterion. One criterion is that there be at least nmin-leaf ≥ 1 subjects of each treatment in the partition.2 Another criterion may be a maximum recursion depth ∆max, but this criterion is not necessary. We also allow for a limited number #features of features to be sampled as candidate cut dimensions. We summarize the PT recursive subroutine as Alg. 1. The PT algorithm is given by passing the whole dataset Sn and initial depth ∆ = 0 to Alg. 1. The PT algorithm is notable for producing an interpretable decision tree for the personalization rule (Fig. 1)."
  }, {
    "heading": "2.4. Personalization Forest",
    "text": "The finer the partition produced by PT, the closer we are to condition (2) and the less bias the estimate of risk has. The coarser the partition, the less variance the estimate has. Therefore, there is an inherent trade-off to the fineness parameters in PT. To address this we can bag (bootstrap aggregate) many very fine PTs, which will have the effect of reducing variance without incurring too much bias as in the case of random forests (Breiman, 2001). The corresponding personalization forest (PF) algorithm is summarized in Alg. 2. Generally, we set #features to √ d to achieve sufficient independence between trees for variance reduction."
  }, {
    "heading": "2.5. Optimal Personalization Tree",
    "text": "PT is a greedy algorithm for minimizing personalization impurity. In this section we propose the optimal personalization tree (OPT) algorithm, which solves the global problem of finding partitions that minimize the sum of withinpartition personalization impurities:\nmin X1∪···∪XL=Rd:(∗) L∑̀ =1 Ipers({(Xi, Ti, Yi) : Xi ∈ X`}), (3)\nwhere (∗) is the restriction that X1, . . . ,XL be disjoint regions defined by the leaves of a binary decision tree. In the case of classification and regression, Bennett & Blue (1996); Bertsimas & Dunn (2016) attempt to find globally optimal prediction trees (while the problem is NP-hard; see Hyafil & Rivest, 1976). For our personalization problem, motivated by Bertsimas & Dunn (2016), we propose a mixed-integer programming (MIP) approach to the optimal personalization tree problem (3).\nWe consider a fixed binary tree structure on nodes 1, . . . , P . Let Ap ⊂ [P ] be the unique path from the root to node p, i.e., its ancestors. For q ∈ Ap, let Rpq = 1 if the right branch is taken to reach p from q, otherwise −1. Let L = {p ∈ [P ] : p /∈ A(q) ∀q ∈ [P ]} be the set of\n2Note that Ipers(X ) is only defined when there is at least 1 subject for each treatment in the partition X . An alternative appropriate for scarce data and large m allows for any number of subjects but chooses the best treatment only from among those with at least nmin-leaf subjects in the partition.\nAlgorithm 1 PT subroutine input: Data part S̃ = { (Xi1 , Ti1 , Yi1 ), . . . , (Xik , Tik , Yik ) } , cur-\nrent depth ∆, tuning parameters nmin-leaf,∆max,#features. 1: for ` ∈ [d] do sort the data along x`: Xiπ(`,1),` ≤ · · · ≤ Xiπ(`,k),`. 2: Set τ̂S̃(x) = arg mint∈[m] ∑k j=1 I[Tij = t]Yij / ∑k j=1 I[Tij = t].\n3: if ∆ < ∆max and mint∈[m] ∑k j=1 I[Tij = t] > nmin-leaf then 4: Set I? =∞, `? = 0, j? = 0. 5: Draw `1, . . . , `#features at random from [d] without replacement. 6: for ` = `1, . . . , `#features do 7: Set kL1 = · · · = k L m = 0, S L 1 = · · · = S L m = 0, k\nL = 0. 8: Set kRt = ∑k j=1 I[Tij = t], S R t = ∑k j=1 I[Tij = t]Yij , k\nR = k. 9: for j ∈ [k − 1] do\n10: Update kL+=1, kR−=1, t = Tiπ(`,j) , k L t+=1, k R t−=1, 11: SLt+=Yiπ(`,j) , S R t−=Yiπ(`,j) . 12: Set I = kL mint∈[m] S L t/k L t + k R mint∈[m] S R t /k R t , 13: kmin = mint∈[m] min(k L t , k R t ). 14: if I < I? and kmin ≥ nmin-leaf then set I? = I , `? = `, j? = j. 15: end for 16: end for 17: if I? <∞ then 18: Set S̃L = {(Xiπ(`?,j) , Tiπ(`?,j) , Yiπ(`?,j) ) : 1 ≤ j ≤ j\n?}, 19: S̃R = {(Xiπ(`?,j) , Tiπ(`?,j) , Yiπ(`?,j) ) : j\n?+1 ≤ j ≤ k}, 20: τ̂S̃L = Alg. 1(S̃ L, ∆ + 1), τ̂S̃R = Alg. 1(S̃ R, ∆ + 1), 21: θ? = 12 (Xiπ(`?,j),` +Xiπ(`?,j+1),`), 22: τ̂S̃(x) = (if x`? ≤ θ ? then τ̂S̃L (x) else τ̂S̃R (x)). 23: end if 24: end if\noutput: τ̂S̃(x).\nleaf nodes and let LC = [P ]\\L be the non-leaf nodes. Let Cp ⊂ [d] × R be the finite set of potential cuts at each non-leaf node p ∈ LC , where (`, θ) ∈ Cp denotes that the cut x` ≤ θ is to be considered at node p. (Usually we take θ to be the data midpoints along dimension `.) Let Y i = Yi − minj∈[n] Yj , Y max = maxi Y i, and M = Y max(maxt∈[m] ∑n i=1 I [Ti = t] − |L|nmin-leaf). For a vector γ with index set C ⊂ [d] × R, let χi(γ, C) = ∑ (`,θ)∈C I [Xi,` ≤ θ] γ`,θ. For p ∈ LC , let kp = dlog2 |Cp|e and Zp ∈ {0, 1}kp×|Cp| be such that (Zp)ij = 1 if bj/2ic is odd and otherwise 0. Our MIP formulation of the OPT problem (3) follows:\nminimize ∑n\ni=1 ∑ p∈L νip subject to (4a)\nw ∈ [0, 1][n]×L, λ ∈ {0, 1}L×m, µ ∈ RL+, ν ∈ R [n]×L + (4b) γp ∈ [0, 1]Cp , δp ∈ {0, 1}kp , Zpγp = δp ∀p ∈ LC (4c) wip ≤ 1+Rpq2 −Rpqχi(γq, Cq) ∀i ∈ [n], p ∈ L, q ∈ Ap (4d) wip ≥ 1 + ∑ q∈Ap( 1−Rpq 2 −Rpqχi(γq, Cq)) (4e) ∀i ∈ [n], p ∈ L∑ i:Ti=t\nwip ≥ nmin-leaf ∀t ∈ [m] (4f) νip ≤ Y maxwip, νip ≤ µp ∀i ∈ [n], p ∈ L (4g) νip ≥ µp − Y max(1− wip) ∀i ∈ [n], p ∈ L (4h)∑\nt∈[m] λpt = 1 ∀p ∈ L (4i)∑ i:Ti=t\n(νip − wipY i) ≤M(1− λpt) ∀p ∈ L, t ∈ [m] (4j)∑ i:Ti=t (νip − wipY i) ≥M(λpt − 1) ∀p ∈ L, t ∈ [m] (4k)\nProblem (4) is a MIP with |L|m + ∑ p∈LC log2 |Cp| binary variables. The variables γp encode choice of cut at each node p and constraint (4c) ensures only one is cho-\nAlgorithm 2 PF input: Data Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)}, tuning parame-\nters T, nmin-leaf, ∆max, #features. 1: for j ∈ [T ] do 2: Draw S(j)n = { (Xi1 , Ti1 , Yi1 ), . . . , (Xin , Tin , Yin ) } at random\nfrom Sn with replacement. 3: Set τ̂(j)n = Alg. 1(S (j) n , 0, nmin-leaf, ∆max, #features). 4: end for output: τ̂n(x) = mode{τ̂(1)n (x), . . . , τ̂ (T ) n (x)}.\nsen (see Yıldız & Vielma, 2013). The variable wip encodes membership of datapoint i to leaf p and constraints (4d4e) enforce that wip is the product of indicators of whether Xi goes in the left or right branch of the ancestor nodes. Since these constraints are integral (Ahuja et al., 1993) we need not enfoce wip be binary. Constraint (4f) ensures at least nmin-leaf samples per leaf. The variable µp encodes the mean outcome of the prescribed treatment in leaf p and the variable νip encodes its product with wip, as ensured by constraints (4g-4h). The variable λpt encodes the choice of treatment t in leaf p and constraint (4i) ensures only one is chosen. Constraints (4j-4k) ensure the consistency between the choice of treatment λpt and the mean outcome µp. We summarize the OPT algorithm for a complete binary tree in Alg. 3. We use Gurobi to solve MIP (4) and use PT as a heuristic warm start, randomly splitting leaf nodes at depth less than ∆."
  }, {
    "heading": "2.6. Adapting Existing Methods to Personalization Using Observational Data",
    "text": "As discussed earlier, methods that estimate CATE, notably CT (Athey & Imbens, 2016), can be used to choose between two treatments by comparing δ(x) = E [Y | X = x, T = 2] − E [Y | X = x, T = 1] to zero. However, such methods are directed at estimation rather than personalization and only address two treatments. To address the latter, we propose one-vs-all (1vA) and one-vsone (1v1) strategies for personalization.\nFor 1vA, for each t ∈ [m] we learn an estimate δ̂tvAn (x) of δtvA(x) = E [Y | X = x, T = t] − E [Y | X = x, T 6= t] by applying a base algorithm (e.g., CT) to the modified dataset StvAn = {(Xi, 1 + I [Ti = t] , Yi) : i ∈ [n]}; then we assign the treatment that does the best compared to the rest: τ̂ 1vAn (x) ∈ arg mint∈[m] δ̂tvA(x).\nFor 1v1, for each t 6= s we learn an estimate δ̂tvsnt+ns(x) of δ\ntvs(x) = E [Y | X = x, T = t] − E [Y | X = x, T = s] on the modified data subset Stvsnt+ns = {(Xi, 1 + I [Ti = t] , Yi) : Ti ∈ {t, s}}; then we either assign the treatment that does the best compared to the worst, τ̂ 1v1-An (x) ∈ arg mint∈[m] mins∈[m] δ̂tvsnt+ns(x), or the one that gets the most votes in one-to-one comparisons, τ̂ 1v1-Bn (x) ∈ arg maxt∈[m] ∑ s 6=t I [ δ̂tvsnt+ns(x) < 0 ] .\nAlgorithm 3 OPT (complete binary tree) input: Data Sn = {(X1, T1, Y1), . . . , (Xn, Tn, Yn)}, tuning parame-\nters nmin-leaf, ∆, #features, #cuts. 1: Set P = 2∆, LC = [2∆−1],Ap = { bp/2jc : j ∈ [∆] } , 2: Rpq = (−1)1+bp/2 ∆−blog2(q)cc. 3: for ` ∈ [d] do sort the data along x`: Xiπ(`,1),` ≤ · · · ≤ Xiπ(`,k),`. 4: for p = 1, . . . , 2∆−1 do 5: DrawFp⊂ [d] with |Fp|= #features. Set J ={1, d n−1#cuts e, . . . , n−1}. 6: Set Cp = {(`, Xiπ(`,j),` +Xiπ(`,j+1,`) 2 ) : ` ∈ Fp, j ∈ J}. 7: end for 8: Find γ, λ that solve problem (4).\noutput: Personalization model τ̂n(x) that proceeds as follows: Set p = 1. for j ∈ [∆] do set (`, θ) = inf{c ∈ Cp : γp,c = 1}, p = 2p+ I [x` > θ]. return inf{t ∈ [m] : λpt = 1}.\nWe can prove that each of our 1vA and 1v1 proposals are consistent given pointwise consistent estimates of CATE:\nTheorem 4. Let Asn. 1 hold. Then:\n1. If δ̂tvAn (X) → δtvA(X) a.s. ∀t ∈ [m], then τ̂ 1vAn (X) ∈ T ∗(X) eventually a.s. 2. If δ̂tvsnt+ns(X) → δ tvs(X) a.s. ∀t 6= s, then\nτ̂ 1v1-An (X), τ̂ 1v1-B n (X) ∈ T ∗(X) eventually a.s.\nNote that 1vA and 1v1 with CT do not inherit trees’ interpretability because the partitions of the 1vX models may not overlap.\nPOEM and NPOEM (Swaminathan & Joachims, 2015a;b) solve the problem of learning from logged bandit feedback, assuming access to the logging policy that generated the data. To adapt these to personalizing from observational data, we propose to impute the logging policy using estimated GPS, i.e., pretend the data were generated by the policy that assigns t when context is x with probability φ̂n(t, x) where φ̂n is a probabilistic classification model fitted to the data {(Xi, Ti) : i ∈ [n]}. We call these IPOEM and INPOEM."
  }, {
    "heading": "3. Submatching for Validating Personalization using Observational Data",
    "text": "In this section we discuss how one can evaluate and validate personalization policies, such as the ones from the last section. Usually, a new policy would be evaluated using a randomized controlled trial, but these can be infeasibly costly. We consider how to evaluate a personalization policy using observational data. Such a dataset can be a subset removed from the training data either for the purpose of testing or for tuning and selection by (cross-)validation. The difficulty in using observational data is that if a policy prescribes any treatment τ(Xi) 6= Ti, then it is not immediately clear how to score this.\nFor offline evaluation of contextual bandits with experimental data, Li et al. (2011) show that rejection sampling is sufficient. A similar solution to evaluation with obser-\nvational data is a combined rejection and importance sampling approach suggested by Thm. 2. If we had the GPS Qi, we could omit any datapoint where τ(Xi) 6= Ti while giving score Yi/Qi to each datapoint where the prescription matched the data, τ(Xi) = Ti. Per Thm. 2 and the law of large numbers, this will provide a consistent estimate for out-of-sample personalization risk. However, not only does this throw away many datapoints, but to implement this in practice we would have to estimate the GPS from data. Estimating the GPS generally either relies heavily on model specification or, in non-parametric settings, can be biased and variable. This may be acceptable for training purposes, as in imputing GPS in IPOEM and INPOEM, as it is already a black box. However, for evaluation, a more reliable estimate of risk is desirable for evidence of success.\nWe propose the use of submatching for evaluation. Matching is a common tool for causal inference (Rosenbaum, 1989; Abadie & Imbens, 2006) where every subject is matched to a subject that received the opposite treatment, creating a complete matched dataset. In submatching, we instead seek only a subset of the data that is well-matched. In this subset, each subject is matched based on a metric ‖x− x′‖ to m− 1 subjects that received each of the treatments the subject did not. Their outcome is imputed as the counterfactual outcome of those treatments for the subject. All matched subjects are not used for training in order to avoid in-sample bias. Usually, Mahalanobis distance is used: ((x− x′)Σ̂−1(x− x′))1/2 where Σ̂ is the sample covariance. (Note that due to personalization on X , matching on propensity scores alone would be insufficient.)"
  }, {
    "heading": "3.1. Greedy Submatching",
    "text": "The simplest way to extract a matched subset of size ntest from Sn is to do so greedily: draw random i1, . . . , intest from [n] without replacement, for each j ∈ [ntest] and t ∈ [m], if t = Tij then set Ŷijt = Yij and if t 6= Tij then find i ∈ arg mini∈[n]:Ti=t ‖Xi −Xij‖ (with replacement), let Ŷijt = Yi and flag subject i, and finally remove all drawn and flagged subjects from training data. The imputed value for the unknown Yij (t) is Ŷijt and our estimate for personalization risk of τ(x) is R̂(τ) = 1ntest ∑ntest j=1 Ŷijτ(Xij ). When matching is exact, i.e. Xi = Xij for all matches, this estimate is unbiased.\nTheorem 5. Under Asn. 1 and exact matching, E[R̂(τ)]=R(τ)."
  }, {
    "heading": "3.2. Optimal Submatching",
    "text": "The greedy method for constructing a matched dataset is simple but it can be wasteful, limiting the amount of the data available for training. We may be able to do better for testing and evaluation when m = 2, when the problem\nreduces to average treatment effect estimation. Consider the problem of finding the subset of the data with the closest matches. That is, finding i11, i12, . . . , inpair1, inpair2 with Tijt = t and minimal ∑npair j=1 ‖Xij1 −Xij2‖, and using the pairs for imputations.3 This problem can be reduced to bipartite matching, which can be solved efficiently (Hopcroft & Karp, 1973). Consider the complete bipartite graph with left nodes being subjects with Ti = 1 and right nodes being subjects with Ti = 2 along with n−npair dummy nodes. Put weight ‖Xi−Xj‖ on edges between datapoints and weight 0 on edges to dummy nodes. The subset of the data with the closest matches is given by the nodes incident to edges not incident to dummy nodes in the least-weight bipartite match. We extract these to construct a well-matched, economical test set with ntest = 2npair. Although this test set may be biased relative to the whole population (e.g., it may emphasize areas of treatment overlap), the corresponding risk estimate is unbiased conditioned on the test set, i.e., it corresponds to risk on an alternative population, which is often sufficient for comparison and selection."
  }, {
    "heading": "3.3. Coefficient of Personalization",
    "text": "In prediction, the coefficient of determination R2 is a unitless quantity bounded by 1 that measures both how well data X predict outcomes Y and how well a predictive model leverages X . One way to interpret out-of-sample R2 is as the percent of the way that X and the model go from a no-X-data solution (Y ’s sample average) to perfect foresight (Y ’s realized value). Using this interpretation, we construct two analogous quantities for personalization, the 1st and 2nd coefficients of personalization:\nP1(τ) = 1− E[Y (τ(X))]−E[mint∈[m] Y (t)]\nmint∈[m] E[Y (t)]−E[mint∈[m] Y (t)] ,\nP2(τ) = 1− E[Y (τ(X))]−E[mint∈[m] Y (t)] E[Y (T )]−E[mint∈[m] Y (t)] .\nThese are also analogous to the coefficient of prescription for conditional stochastic optimization (Bertsimas & Kallus, 2015). The first measures the improvement toward perfect (prescient) personalization relative to no personalization at all and the second does relative to current practice or standard of care (whatever determined T in the data). They are unitless, bounded by 1. Using a matched dataset, we can estimate these as:4\nP̂1(τ) = 1− Σ ntest j=1Ŷijτ(Xij ) −Σntestj=1 mint∈[m] Ŷijt\nmint∈[m] Σ ntest j=1Ŷijt−Σ ntest j=1 mint∈[m] Ŷijt\n,\nP̂2(τ) = 1− Σ ntest j=1Ŷijτ(Xij ) −Σntestj=1 mint∈[m] Ŷijt\nΣ ntest j=1Yij−Σ ntest j=1 mint∈[m] Ŷijt\n.\n3 More efficient estimates may be possible using analogues of Robins & Rotnitzky (1995); Hahn (1998) on the submatched data.\n4 This assumes that potential outcomes are conditionally independent given X . Indeed, the conditional copula of potential outcomes has no physical meaning and is unidentifiable."
  }, {
    "heading": "4. Empirical Investigation",
    "text": "We conclude with an empirical investigation of personalization using observational data and our new algorithms."
  }, {
    "heading": "4.1. Personalized Warfarin Dosing",
    "text": "According to the International Warfarin Pharmacogenetics Consortium, “warfarin is the most widely used oral anticoagulant agent worldwide” and finding the appropriate dose is both difficult and important “because it can vary by a factor of 10 among patients” and “incorrect doses contribute to a high rate of adverse effects” (Consortium, 2009). Currently, the common practice is to start a new patient at 35 mg/week and slowly adjust the dose (Jaffer & Bragg, 2003). We present an application of our methods to personalizing dosage based on data on 5410 warfarin patients collected by Consortium (2009).\nThe baseline data collected on each patient include demographic characteristics (sex, ethnicity, age, weight, height, and smoker), reason for treatment (e.g., atrial fibrillation), current medications, co-morbidities (e.g., diabetes), genotype of two polymorphisms in CYP2C9, and genotype of seven single nucleotide polymorphisms (SNPs) in VKORC1. The correct stable therapeutic dose of warfarin, determined by adjustment over a few weeks, is recorded for each patient and segmented into three dose groups: low (≤ 21 mg/week, t = 1), medium (> 21, < 49 mg/week, t = 2), and high (≥ 49 mg/week, t = 3). The dataset was also studied in an online (bandit) setting in (Bastani & Bayati, 2016) where an R&C approach is analyzed.\nIn our experiment, we let Y (t) be 1 if the dose t is incorrect and otherwise 0. To generate observational data (where dosage is not revealed by experimentation), we consider T chosen based on body mass index (BMI):\nP(T = t | X = x) = e (t−2)(xBMI−µBMI)/σBMI\ne−(xBMI−µBMI)/σBMI +1+e(xBMI−µBMI)/σBMI ,\nwhere µBMI and σBMI are the sample mean and standard deviation of BMI. As an example, we run the PT algorithm with ∆max = 5 on the whole data, generating the tree shown in Fig. 1. (We use ∆max = 5 due to length constraints.) It is known that VKORC1 and CYP2C9 genotypes are strongly associated to warfarin dosage requirements (Li et al., 2006). PT is able to learn this relationship and it provides an efficient and interpretable dosing guideline where the effect of these genotypes is clear.\nTo assess the efficiency of various personalization algorithms, for each n = 100, 200, . . . , 2500, we consider 100 replications in which we randomly select n training subjects and ntest = 2500 test subjects (disjoint, without replacement). In each replication, we run 12 personalization algorithms and evaluate their risk on the test set (where full counterfactuals are available). We test standard R&C using four predictive models: OLS, logistic regression, CART (scikit-learn defaults), and kNN (k = b √ nc). We compare these to our three direct personalization methods: PT (nmin-leaf = 20,∆max = ∞,#features = d), PF (T = 500, nmin-leaf = 10,∆max = ∞,#features = √ d), and OPT (nmin-leaf = 20,#features = d,#cuts = 10,∆ = 2 + I [n ≥ 300], MIP solve time limited to 1 hour). We also compare to our 1vA strategy using Athey & Imbens (2016)’s CT-A (adaptive) and CT-H (honest with 50-50 split) and to IPOEM and INPOEM (parameters tuned on 25% holdout validation as in Swaminathan & Joachims, 2015a;b) with GPS imputed by cross-validated `1-regularized multinomial regression using R package glmnet. (Due to limited space, we focus on 1vA, which outperformed 1v1.)\nWe plot the average risk over replicates in Fig. 2 (note log scale). It is evident that R&C approaches make inefficient use of the available data by splitting it and learning more than is necessary. While eventually reaching low risk (< 0.4), R&C using OLS and logistic regression take\nmuch longer (n = 1300) to get there than our direct methods, which achieve low risk very quickly (n = 200) and near-optimal risk (≤ 0.36) soon after (n = 700). Nonparametric R&C (CART, kNN), IPOEM, and INPOEM converge slowly. 1vA with CT-A and CT-H offers competitive performance for moderate n, but fails to achieve near-optimal risk even at n = 2500. CT-A offers a small edge over CT-H, which can be attributed to CT-H’s splitting of the training data – indeed, CT-H’s primary advantage are correctly sized confidence intervals, which are not used.\nAmong our direct methods, PF appears to work the best overall, for both small and large n, while PT achieve similar performance for n ≥ 2000. For smaller n, OPT outperforms PT (and PF for n = 100) attributed to OPT’s ability to find the best simple tree to fit the scarce data. For larger n, the MIP becomes so large that Gurobi is hardly able to improve the PT warm start, which has very limited depth. Therefore, we see performance deteriorate. We conclude OPT is best either for small datasets or for finding models that are reasonably efficient while being exceedingly simple and interpretable (depth 2–3 compared to depth 9–13 for PT at n = 2500), albeit at computational cost. Our best out-of-sample risk is 0.356, which translates to P̂1 = 0.22, P̂2 = 0.47: i.e., 22% (or, 47%) of the way from no personalization (or, standard of care) to perfect personalization."
  }, {
    "heading": "4.2. Personalized Job Training",
    "text": "We consider an application to personalized recommendations for a job training program. We use data from the National Supported Work Demonstration (LaLonde, 1986) (combining the experimental sample of 465 subjects with the 2490 PSID controls to create an observational dataset). The data includes 2935 individuals, 185 of which received a job training program in 1976-77 (Ti = 1). The data includes information about age, education level, ethnicity, marital status, earnings in years 1974-75, and earnings in 1978. This data is the standard benchmark in evaluation of causal methodologies for estimating an average treatment effect (Dehejia & Wahba, 2002). We consider an alternative setting where we give a personalized recommendation as to whether to enroll in the job training program, assuming enrollment costs $2,000. Therefore, we let Yi equal 1978 earnings less $2,000 if Ti = 1.\nFrom the 2935, we extract an optimal matched test set of 55 pairs (ntest = 110) perfectly matched in all covariates except for a mean absolute deviation of $12 and $17 in 1974 and 1975 earnings, respectively, within pairs. On the remaining n = 2825 subjects, we train the same personalization models as above with the following changes: we omit logistic regression (outcomes not binary), use nmin-leaf = 10 for PT and OPT and = 1 for PF, use ∆ = 4 for OPT and let the MIP solve for 24 hours, use logistic regressions to im-\npute GPS for IPOEM and INPOEM, and include the causal forest (CF) extension (Wager & Athey, 2017) of CT as implemented by the R package gradient.forest.\nWe plot the estimated average personalized net income (after enrollment costs) in Fig. 3. We see a clear benefit to our methods’ direct targeting of personalization and that, with only two treatments, CF and CT-A provide highly competitive performance. Average net income of 4200.8 due to PF translates to P̂1 = 0.22, P̂2 = 0.28: i.e., 22% (or, 28%) of the way from no personalization (or, standard of care) to perfect personalization."
  }, {
    "heading": "5. Conclusions",
    "text": "We developed a new approach to the unique problem of personalization from observational data. The approach was based on a new formulation of the problem and a new impurity measure for personalization. This lead to three recursive-partitioning-based algorithms: the personalization tree that greedily partitions the data to minimize the sum of within-partition personalization impurities, the personalization forest that bagged many personalization trees, and the optimal personalization tree that used a MIP to globally optimize the partitioning problem. We developed new submatching techniques to evaluate and validate these algorithms as well as ones adapted from existing methods. And we used these techniques to evaluate all algorithms in two example applications: personalized warfarin dosing and personalized job training. In both examples, we demonstrated the benefits of our algorithms in terms of efficacy and interpretability. We phrased the success of our personalization techniques in terms of the new coefficients of personalization, which quantify the benefit we achieve from personalization as a percentage of the benefit that impossibly perfect personalization can achieve relative to either no personalization or the standard of care."
  }],
  "year": 2017,
  "references": [{
    "title": "Large sample properties of matching estimators for average treatment",
    "authors": ["Abadie", "Alberto", "Imbens", "Guido W"],
    "venue": "effects. Econometrica,",
    "year": 2006
  }, {
    "title": "Estimating conditional average treatment effects",
    "authors": ["Abrevaya", "Jason", "Hsu", "Yu-Chin", "Lieli", "Robert P"],
    "venue": "J Bus Econ Stat,",
    "year": 2015
  }, {
    "title": "Network flows: theory, algorithms, and applications",
    "authors": ["RK Ahuja", "TL Magnanti", "Orlin", "JB"],
    "year": 1993
  }, {
    "title": "Recursive partitioning for heterogeneous causal effects",
    "authors": ["Athey", "Susan", "Imbens", "Guido"],
    "venue": "PNAS, 113(27):7353–",
    "year": 2016
  }, {
    "title": "Online decisionmaking with high-dimensional covariates",
    "authors": ["Bastani", "Hamsa", "Bayati", "Mohsen"],
    "year": 2016
  }, {
    "title": "From predictive to prescriptive analytics",
    "authors": ["Bertsimas", "Dimitris", "Kallus", "Nathan"],
    "year": 2015
  }, {
    "title": "Personalized diabetes management using electronic medical records",
    "authors": ["Bertsimas", "Dimitris", "Kallus", "Nathan", "Weinstein", "Alexander M", "Zhuo", "Ying Daisy"],
    "venue": "Diabetes Care,",
    "year": 2017
  }, {
    "title": "The offset tree for learning with partial labels",
    "authors": ["Beygelzimer", "Alina", "Langford", "John"],
    "venue": "In SIGKDD,",
    "year": 2009
  }, {
    "title": "Classification and Regression Trees",
    "authors": ["Breiman", "Leo", "Friedman", "Jerome", "Stone", "Charles", "Olshen", "Richard"],
    "year": 1984
  }, {
    "title": "Propensity scorematching methods for nonexperimental causal studies",
    "authors": ["Dehejia", "Rajeev H", "Wahba", "Sadek"],
    "venue": "Rev Econ Stat,",
    "year": 2002
  }, {
    "title": "A statistical model for predicting response of breast cancer patients to cytotoxic chemotherapy",
    "authors": ["Feldstein", "Michael L", "Savlov", "Edwin D", "Hilf", "Russell"],
    "venue": "Cancer Res,",
    "year": 1978
  }, {
    "title": "A linear response bandit problem",
    "authors": ["Goldenshluger", "Alexander", "Zeevi", "Assaf"],
    "venue": "Stoch Syst,",
    "year": 2013
  }, {
    "title": "Online display advertising: Targeting and obtrusiveness",
    "authors": ["Goldfarb", "Avi", "Tucker", "Catherine"],
    "venue": "Market Sci,",
    "year": 2011
  }, {
    "title": "On the role of the propensity score in efficient semiparametric estimation of average treatment effects",
    "authors": ["Hahn", "Jinyong"],
    "year": 1998
  }, {
    "title": "The propensity score with continuous treatments",
    "authors": ["Hirano", "Keisuke", "Imbens", "Guido W"],
    "year": 2004
  }, {
    "title": "An nˆ5/2 algorithm for maximum matchings in bipartite graphs",
    "authors": ["Hopcroft", "John E", "Karp", "Richard M"],
    "venue": "SIAM J Comput,",
    "year": 1973
  }, {
    "title": "Constructing optimal binary decision trees is np-complete",
    "authors": ["Hyafil", "Laurent", "Rivest", "Ronald L"],
    "venue": "Inform Process Lett,",
    "year": 1976
  }, {
    "title": "Causal inference in statistics, social, and biomedical sciences",
    "authors": ["Imbens", "Guido W", "Rubin", "Donald B"],
    "year": 2015
  }, {
    "title": "Practical tips for warfarin dosing and monitoring",
    "authors": ["Jaffer", "Amir", "Bragg", "Lee"],
    "venue": "Clev Clin J Med,",
    "year": 2003
  }, {
    "title": "Generalized optimal matching methods for causal inference",
    "authors": ["Kallus", "Nathan"],
    "year": 2016
  }, {
    "title": "Balanced policy evaluation and learning. 2017a",
    "authors": ["Kallus", "Nathan"],
    "year": 2017
  }, {
    "title": "A framework for optimal matching for causal inference",
    "authors": ["Kallus", "Nathan"],
    "venue": "In AISTATS, pp",
    "year": 2017
  }, {
    "title": "Evaluating the econometric evaluations of training programs with experimental data",
    "authors": ["LaLonde", "Robert J"],
    "venue": "Am Econ Rev,",
    "year": 1986
  }, {
    "title": "Personalized medicine: elusive dream or imminent reality",
    "authors": ["Lesko", "LJ"],
    "venue": "Clin Pharmacol Ther,",
    "year": 2007
  }, {
    "title": "A contextual-bandit approach to personalized news article recommendation",
    "authors": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert E"],
    "venue": "In WWW,",
    "year": 2010
  }, {
    "title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms",
    "authors": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Wang", "Xuanhui"],
    "venue": "In WSDM,",
    "year": 2011
  }, {
    "title": "Polymorphisms in the vkorc1 gene are strongly associated with warfarin dosage requirements in patients receiving anticoagulation",
    "authors": ["Teng-Yi", "Stafford", "Darrel W", "Evans", "James P"],
    "venue": "J Med Genet,",
    "year": 2006
  }, {
    "title": "Performance guarantees for individualized treatment rules",
    "authors": ["Qian", "Min", "Murphy", "Susan A"],
    "venue": "Ann Stat,",
    "year": 2011
  }, {
    "title": "Semiparametric efficiency in multivariate regression models with missing data",
    "authors": ["Robins", "James M", "Rotnitzky", "Andrea"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1995
  }, {
    "title": "Optimal matching for observational studies",
    "authors": ["Rosenbaum", "Paul R"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1989
  }, {
    "title": "The central role of the propensity score in observational studies for causal effects",
    "authors": ["Rosenbaum", "Paul R", "Rubin", "Donald B"],
    "year": 1983
  }, {
    "title": "A multivariate analysis of genomic polymorphisms: prediction of clinical outcome to 5-fu/oxaliplatin combination chemotherapy in refractory colorectal cancer",
    "authors": ["J Stoehlmacher", "DJ Park", "W Zhang", "D Yang", "S Groshen", "S Zahedy", "Lenz", "HJ"],
    "venue": "Brit J Cancer,",
    "year": 2004
  }, {
    "title": "Counterfactual risk minimization: Learning from logged bandit feedback",
    "authors": ["Swaminathan", "Adith", "Joachims", "Thorsten"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "The selfnormalized estimator for counterfactual learning",
    "authors": ["Swaminathan", "Adith", "Joachims", "Thorsten"],
    "venue": "In NIPS, pp",
    "year": 2015
  }, {
    "title": "Estimation and inference of heterogeneous treatment effects using random forests",
    "authors": ["Wager", "Stefan", "Athey", "Susan"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2017
  }, {
    "title": "Strong laws of large numbers and nonparametric estimation",
    "authors": ["Walk", "Harro"],
    "venue": "Recent Developments in Applied Probability and Statistics,",
    "year": 2010
  }, {
    "title": "Incremental and encoding formulations for mixed integer programming",
    "authors": ["Yıldız", "Sercan", "Vielma", "Juan Pablo"],
    "venue": "Oper Res Lett,",
    "year": 2013
  }, {
    "title": "Estimating individualized treatment rules using outcome weighted learning",
    "authors": ["Zhao", "Yingqi", "Zeng", "Donglin", "Rush", "A John", "Kosorok", "Michael R"],
    "venue": "J Am Stat Assoc,",
    "year": 2012
  }, {
    "title": "Large-scale parallel collaborative filtering for the netflix prize",
    "authors": ["Zhou", "Yunhong", "Wilkinson", "Dennis", "Schreiber", "Robert", "Pan", "Rong"],
    "venue": "In AAIM,",
    "year": 2008
  }],
  "id": "SP:1d6e1429feac367a836256f0d726a41218f0cc57",
  "authors": [{
    "name": "Nathan Kallus",
    "affiliations": []
  }],
  "abstractText": "We study the problem of learning to choose from m discrete treatment options (e.g., news item or medical drug) the one with best causal effect for a particular instance (e.g., user or patient) where the training data consists of passive observations of covariates, treatment, and the outcome of the treatment. The standard approach to this problem is regress and compare: split the training data by treatment, fit a regression model in each split, and, for a new instance, predict all m outcomes and pick the best. By reformulating the problem as a single learning task rather than m separate ones, we propose a new approach based on recursively partitioning the data into regimes where different treatments are optimal. We extend this approach to an optimal partitioning approach that finds a globally optimal partition, achieving a compact, interpretable, and impactful personalization model. We develop new tools for validating and evaluating personalization models on observational data and use these to demonstrate the power of our novel approaches in a personalized medicine and a job training application.",
  "title": "Recursive Partitioning for Personalization using Observational Data"
}