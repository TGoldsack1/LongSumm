{
  "sections": [{
    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1009–1019, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Truth-finding algorithms aim to separate true statements (facts) from false information. More specifically, given a set of statements whose truthfulness is unknown (fact candidates), the key goal of truth-finding algorithms is to generate a ranking such that true statements are ranked ahead of false ones. Truth-finders have the potential to address a major obstacle on the Web: the problem of sources spreading inaccurate and conflicting information. This problem continues to grow with the development of tools for easy Web authorship. Blogs, forums and social networking websites are not subject to traditional journalistic standards. Consequently, the accuracy of information reported by these sources is often unclear. Even more established newspapers and websites may sometimes report false information as they race to break stories. Therefore, truth-finding is becoming an in-\ncreasingly important problem. Information extraction projects aim to distill relational facts from natural language text (Auer et al., 2007; Bollacker et al., 2008; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013). These projects have produced knowledge bases containing many millions of relational facts between entities. However, despite these impressive advances, there are still major limitations regarding precision. Within the context of information extraction, fact extractors assign confidence scores to extracted facts. However, such scores are often tied to the extractor’s ability to read and understand natural language text. This is different from a score that indicates the degree to which a given fact candidate is believable. Such a believability score is sometimes also referred to as a credibility score or truthfulness score. The believability score reflects the likelihood that a given statement is true. Truth-finding algorithms aim to compute this score for each fact candidate.\nPrior truth-finding methods are mostly based on iterative voting, where votes are propagated from sources to fact candidates and then back to sources (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011). At the core of iterative voting is the assumption that candidates mentioned by many sources are more likely to be true. However, additional aspects of a source influence its trustworthiness, besides external votes.\nOur goal is to accurately assess truthfulness of fact candidates by taking into account the language of sources that mention them. A Mechanical Turk study we carried out revealed that there is a significant correlation between objectivity of language and trustworthiness of sources. Objectivity of language refers to the use of neutral, impartial language, which is not personal, judgmental, or emotional. Trustworthiness refers to\n1009\na source of information being reliable and truthful. We use linguistics features to detect if a given source objectively states facts or is speculative and opinionated. Additionally, in order to ensure that fact candidates mentioned in similar sources have similar believability scores, our believability computation model incorporates influence of comentions. However, we must avoid falsely boosting co-mentioned fact candidates. Our model addresses potential false boosts in two ways: first, by ensuring that co-mention influence is only propagated to related fact candidates; second, by ensuring that the degree of co-mention influence is determined by the trustworthiness of the sources in which co-mentions occur.\nThe contribution of this paper is a languageaware truth-finding approach. More precisely, we make the following contributions: (1) Alternative Fact Candidates: Truth-finders rank a given fact candidate with respect to its alternatives. For example, alternative places where Barack Obama could have been born. Virtually all existing truth-finders assume that the alternatives are provided. In contrast, we developed a method for generating alternative fact candidates. (2) Objectivity-Trustworthiness Correlation: We hypothesize that objectivity of language and trustworthiness of sources are positively correlated. To test this hypothesis, we designed a Mechanical Turk study. The study showed that this correlation does in fact hold. (3) Objectivity Classifier: Using labeled data from the Mechanical Turk study, we developed and trained an objectivity classifier which performed better than prior proposed lexicons from literature. (4) Believability Computation: We developed FactChecker, a truth-finding method that linearly combines objectivity and comention influence. Our experiments showed that FactChecker outperforms prior methods."
  }, {
    "heading": "2 Fact Candidates",
    "text": "In this section, we formally define what constitutes a fact candidate and describe how we go about understanding semantics of fact candidates. We then present our approach for generating alternative fact candidates."
  }, {
    "heading": "2.1 Representation",
    "text": "The triple format is the most common representation of facts in knowledge bases. A formal specifi-\ncation of the triple format is presented in the RDF primer1. In RDF, data is represented as subjectpredicate-object (SPO) triples. In this work, we restrict predicates to verbs (or verbal phrases such as “plays for”, “graduated from”, etc.). Literature on automatic relation discovery (Fader et al., 2011) has shown that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. Therefore, we define a fact candidate as follows:\nDefinition 1 (Fact Candidate) A fact candidate fi is an 〈S〉 V 〈O〉 triple; where S is the subject, V is a verbal phrase, and O is the object. We aim to compute the truthfulness of fi, τ(fi) ∈ {T, F}, where T and F stand for true and false, respectively.\nNote that in this paper we are interested in cases where τ(fi) is either T or F . That is, we assess truthfulness of factual statements and not opinions whose truthfulness is often both T and F to some degree. For example, the triples: 〈Obama〉 born in 〈Kenya〉 and 〈Obama〉 graduated from 〈Harvard〉 are valid fact candidates. However, the triple: 〈Obama〉 deserves 〈Nobel Peace Prize〉 is not."
  }, {
    "heading": "2.2 Semantics",
    "text": "Based on the SVO triple, the meaning of a fact candidate can be unclear and ambiguous. Therefore, we first determine the semantics of a fact candidate before computing its truthfulness. Entity Types. We first determine the expected types of the subject and object in the SVO. For example, for the SVO 〈Einstein〉 died in 〈Princeton〉, the expected types are person × location. We determine this by first computing the types of entities that are valid for each verb (verbal phrase) in a large SVO collection of 114m SVO triples (Talukdar et al., 2012). Typing verbal phrases is a once-off computation. Our phrase typing method is similar to prior work on typing relational phrases (Nakashole et al., 2012). Examples of typed phrases are: 〈person〉 died in 〈year〉, 〈person〉 died in 〈location〉, and 〈athlete〉 plays for 〈team〉. Given a triple, we look up the types for the subject and the object and then determine which of the typed phrases are compatible with the current triple. We look up entity types in a knowledge\n1http://www.w3.org/TR/rdf-primer/\nbase containing entities and their types. In particular, we use the NELL entity typing API (Carlson et al., 2010). NELL’s entity typing method has high recall because when entities are not in the knowledge base, it performs on-the-fly type inference using the Web. This is not the case for other options such as (Auer et al., 2007; Bollacker et al., 2008; Hoffart et al., 2011). Relation Cardinality. Next, we learn cardinalities of verbal phrases. Cardinality refers to how arguments of a given relation relate to one another numerically. We define the relation cardinality of a verb Card(V ), as the average number of expected arguments per given subject. For example, for the relation “died in”, 1 location is expected for each subject. For other relations, the expected number of arguments can be greater than 1 but less than n : n ∈ R, n > 1. We approximate n using statistics from the 114m SVO corpus based on the average number of arguments per given first argument. In a once-off computation, we generate cardinality approximations per typed verbal phrase V and its inverse V −1. For example, we generate the cardinality estimates for both: 〈person〉 died in 〈location〉 and for 〈location〉 INVERSE-OF(died in) 〈person〉. Synonymous Relations. Natural language is diverse. Semantically similar phrases can be syntactically different. Therefore, we learn other verbs that can be used to substitute V in SVO. We pre-compute synonymous phrases from the 114m SVO corpus using distributional semantics in the same spirit as (Lin and Pantel, 2001; Nakashole et al., 2012).\nSynonymous verbs, relation cardinalities, and entity types enable us to generate alternative fact candidates."
  }, {
    "heading": "2.3 Alternative Fact Candidates",
    "text": "Truth-finding methods rank fi relative to alternative candidates. While prior methods assume the alternatives are known apriori, we developed a method for generating alternative fact candidates. For a given fi, we first identify the fixed argument. The fixed argument is the argument of the SVO which when fixed, requires finding the fewest number of alternative candidates. For example, for 〈Einstein〉 died in 〈Princeton〉, the solution is to fix the subject. This is because the cardinality of 〈person〉 died in 〈location〉 is one (1).\nOn the other hand, the cardinality of “INVERSEOF(died in)” is many(n). In other words, the number of places where a person can be born (one) is much fewer than the number of people that can die in a place (many). In our example, alternatives are possible places, other than Princeton, where Einstein could have died. For example: 〈Einstein〉 died in 〈Germany〉 or 〈Einstein〉 died in 〈Switzerland〉. More generally, the fixed argument of fact candidate fi, is defined as follows:\nDefinition 2 (Fixed Argument) Let Card(V) be the cardinality of V and Card(V −1) be the cardinality of the inverse of V , if Card(V ) < Card(V −1), then the fixed argument is the subject, Argfixed(fi) = S, else it is the object, O. If Card(V ) == Card(V −1), then both arguments are fixed, one at a time.\nWe use the fixed argument to define a topic as the fixed argument plus the verb. Therefore, for the SVO 〈X〉 died in 〈Y〉, the topic “places where X died”, (Argfixed = S), is not the same as the topic “people who died in Y” (Argfixed = O).\nTo locate alternatives, we use the topic (Argfixed + V ) as a query. We search three sources to either locate relevant documents or relevant triples: the Google Web search API, the 114m SVO collection, and the NELL KB. The SVO collection and the KB return triples, however, the Web search API returns documents. Therefore, we apply a triple extractor to the retrieved documents. For all potential alternative triples, we perform type checking to ensure that the arguments of the triples are type-compatible with fi. Furthermore, we generate an additional query for every synonymous verb sVi, replacing V with sVi. Example queries are: “Einstein died in”, “Einstein passed in”, etc."
  }, {
    "heading": "3 Objectivity and Trustworthiness",
    "text": "The principle of objective journalism, which is a significant part of journalistic ethics, aims to promote factual and fair reporting, undistorted by emotion or personal bias (Schudson, 1978; Kaplan, 2002). Objectivity is also required in reference sources such as encyclopedias, scientific publications, and textbooks. For example, Wikipedia enforces a neutral point-of-view policy (NPOV)2. Articles violating the NPOV policy are marked\n2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view\nto indicate potential bias. While opinions, emotions, and speculations can also be expressed using objective language, they are often stated using subjective language (Turney et al., 2002; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Liu et al., 2005; Recasens et al., 2013). For example, consider the following pieces of text:\n(S) Well, I think Obama was born in Kenya because his grandma who lives in Kenya said he was born there. (O) Theories allege that Obama’s published birth certificate is a forgery, that his actual birthplace is not Hawaii but Kenya.\nText S is a snippet from Yahoo Answers and text O is a snippet from the Wikipedia page titled: “Barack Obama Citizenship Conspiracy Theories”. S is subjective, expressing the opinion of the author. On the other hand, O is objective, stating only what has been alleged. Literature on sentiment analysis (Turney et al., 2002; Liu et al., 2005), subjectivity detection (Riloff and Wiebe, 2003; Wiebe et al., 2004), and bias detection (Yu and Hatzivassiloglou, 2003; Recasens et al., 2013) has developed lexicons for identifying subjective language. Due to the principle of objective journalism and the requirement of objectivity placed on reference sources, we hypothesize a link between objectivity and trustworthiness as follows.\nHypothesis 1 Objective sources are more trustworthy than subjective sources. Therefore, we can assume that fact candidates stated in objective sources are more likely to be true than those stated in subjective sources.\nTo test the validity of the hypothesis, we carried out a study where we solicited human input."
  }, {
    "heading": "3.1 Mechanical Turk Study",
    "text": "We deployed an annotation study on Amazon Mechanical Turk (MTurk)3, a crowd-sourcing platform for tasks requiring human input. Tasks on MTurk are small questionnaires consisting of a description and a set of questions. Our study consisted of two independent tasks. The first task was titled “Trustworthiness of News Articles”, where annotators were given a link to a news article and\n3http://www.mturk.com\nFigure 1: Summary of the results of the annotation study on objectivity and trustworthiness.\nasked to judge if they thought it was trustworthy or not. The second task was titled “Objectivity of News Articles”. For this task, annotators were asked to judge if a given article is objective or subjective. For both tasks a third option of “not sure” was provided. We randomly selected 500 news articles from a corpus of about 300,000 news articles obtained from Google News from the topics of Top News, Business, Entertainment, and SciTech. For each task, every article was judged by three annotators. This produced a total of 3000 annotations. When we analyzed the output, we accepted a label as valid for a given article if the label was selected by the majority of the judges. Based on this criteria, we obtained a set of 420 articles that were both labeled for trustworthiness and objectivity.\nA summary of the outcome of the study is shown in Figure 1; 74% of the untrustworthy articles were independently labeled as subjective. On the other hand, 64% of trustworthy articles were independently labeled as objective. These results indicate a non-trivial positive correlation between objectivity and trustworthiness. We leverage this correlation in our believability computation model. To incorporate objectivity in FactChecker, we require for a given source document, an objectivity score ∈ [0, 1], where 0 means the source is subjective and 1 means it is objective. Next, describe our method for automatically determining objectivity of sources."
  }, {
    "heading": "3.2 Automatic Objectivity Detection",
    "text": "We trained a logistic regression classifier to predict the objectivity of a document. For training and testing data, we used the labeled data from the Mechanical Turk study. We additionally used labeled text from prior work on subjectivity detection (Pang and Lee, 2004). This resulted in a total of 4, 600 documents, half subjective and the other half objective. We used 4000 documents for\ntraining, 2000 per label. The rest of the documents were split into a development set (380) and a test set (220).\nA summary of the features we used is shown in Table 1. Features 1-3 refer to lexicons developed by prior methods on subjectivity (Wiebe et al., 2004), sentiment analysis (Liu et al., 2005) and bias detection (Recasens et al., 2013). Feature 4 refers to part-of-speech tags of the terms found in the document that are also in the lexicons. Feature 5 refers to bi-grams that frequently occur (mention frequency of > 10) in the 4, 600 documents. The most contributing features were the lexicons, features (1-3) and the frequent bi-grams, feature 5. We discovered that using frequent bi-gram features instead of uni-grams or bi-grams resulted in higher precision. The classifier was able to determine that for example bi-grams such as “think that”, “so funny” and “you thought” are negative features for objectivity. Evaluation results of our objectivity detector vs. baselines are shown in Table 2. FactChecker’s objectivity detector has precision of 0.7814 ± 0.0539, with a 0.9-confidence Wilson score interval (Brown et al., 2001) and this outperforms the baselines. Next, we describe how we leverage objectivity into FactChecker’s truthfulness model."
  }, {
    "heading": "4 Believability Computation Model",
    "text": "FactChecker computes the believability score of a fact candidate from its: i) objectivity score and (ii) co-mention score. In this section we define each of these scores.\nThe objectivity score reflects the trustworthiness of sources where a fact candidate is mentioned. Given a fact candidate fi, mentioned in a set of documents Di, where each document d ∈\nDi has objectivity O(d), fi’s objectivity score is defined as follows:\nDefinition 3 (Objectivity Score)\nO(fi) = log|Di|.\n∑ dk∈Di O(dk)\n|Di| (1)\nWe do not use the sum of objectivity of sources as the objectivity score because this enables fact candidates mentioned in many low objectivity sources to have high aggregate objectivity. Similarly, we avoid using average objectivity of the sources as it overestimates objectivity of candidates stated in few sources. A candidate mentioned in 10 sources with 0.9 objectivity should have higher objectivity than a candidate stated in 1 source of 0.9 objectivity. In Equation 1, log|Di| addresses this issue.\nThe co-mention score aims to ensure that fact candidates mentioned in similar sources have similar believability scores. Suppose candidate fi is mentioned in many highly objective sources, another candidate fj is stated in only one highly objective source dk where fi is also mentioned. Then the believability of fj should be boosted by it being co-mentioned with fi. If on the other hand fi and fj were co-mentioned in a subjective source, fj should receive less boost from fi. This leads us to the co-mention score µ(fi) of a candidate.\nDefinition 4 (Co-Mention Score)\nµ(fi) = ρ(fi) + ∑ fj∈F wijµ(fj) (2)\nWhere ρ(fi) is the normalized mention frequency of fi. The propagation weight wij controls how much boost is obtained from a co-mentioned candidate. We define propagation weight, wij , as the average of the objectivity of the sources that mention both candidates.\nwij = average O(dk) : dk ∈ (Di ∩Dj) (3)\nwhere O(dk) is the objectivity of document dk, Di and Dj are the sets of documents that mention fi and fj , respectively. Notice that we could boost co-mentioned but not related candidates, thereby causing false boosts. To remedy this, we only allow wij to be greater than zero if the fact candidates fi and fj are on the same topic. Recall that the topic is determined by the fixed argument (Definition 2) and the verb. Allowing only fact candidates on the same topic to influence each other is important considering that many trivial facts are often repeated in sources of diverse quality.\nTo leverage the inter-dependencies among related co-mentioned fact candidates, we model the solution with a graph ranking method. Each fact candidate is a node and there is an edge between each pair of related fact candidate nodes fi and fj , with wij as the edge weight. Thus, equation 2 can be reformulated as µ = Mµ, where µ is the co-mention score vector and M is a Markov matrix which is stochastic, irreducible and aperiodic. Thus, a power method will converge to a solution in a similar manner to PageRank. Implementation consists of iteratively applying Equation 2 until the change in the score is less than a threshold . The solution is the final co-mention scores of fact candidates.\nFinally, to compute the believability score of a fact candidate, we linearly combine its objectivity score with its co-mention as follows:\nDefinition 5 (Believability Score)\nβ(fi) = λO(fi) + (1− λ)µ(fi) (4)\nWhere λ is a weighting parameter ∈ [0, 1] which controls the relative importance of the two aspects of FactChecker. As we show in our experiments, λ can be robustly chosen within the range of 0.2 to 0.6. In our experiments we used λ = 0.6.\nThe entire procedure of FactChecker is summarized in Algorithm 1."
  }, {
    "heading": "5 Evaluation",
    "text": "We evaluated FactChecker for accuracy. We define accuracy as the probability of a true fact candidate having a higher believability score than a false candidate. Let τ(fi) ∈ {T, F} be the truthfulness of a fact candidate fi, accuracy is defined as:\nAlgorithm 1 FactChecker Input: A set F of fact candidates Input: KB K, SVO corpus C, WebW Output: A set L of rankings ∀fi ∈ F L = ∅ while F 6= ∅ do\npick fi from F A= getAlternatives(fi,K,C,W) PriorityQueue Li = ∅ for all alternative fact candidates f ′j ∈ A do β(f ′j) = getBelievabilityScore(f ′j) Li.insert(f ′j , β(f ′ j)) end for β(f i) = getBelievabilityScore(fi) Li.insert(fi, β(fi)) L ∪ Li Remove fi from F\nend while return L\nAcc =\n∑ (τ(fi)=T :τ(fj)=F ) (β(fi) > β(fj))\n|{∀(fi, fj) : τ(fi) = T ∧ τ(fj) = F}|\nDatasets. We evaluated FactChecker on three datasets: i) KB Fact Candidates: The first dataset consists of fact candidates taken from the fact extraction pipeline of a state-of-the-art knowledge base, NELL (Carlson et al., 2010). The fact candidates span four different relation types: company acquisitions, book authors, movie directors and athlete teams. For each fact candidate, we applied our alternative candidate generation method. We only considered fact candidates with non-trivial alternative candidate sets; where the alternative candidate set is greater than zero. Since all of the baselines we compared against assume alternatives are provided, we apply all methods to the same set of alternative fact candidates discovered by our method. Details of this dataset are shown as rows starting with “KB-” in Table 3.\nii) Wikipedia Fact Candidates: For the second dataset, we did not restrict the fact candidates to specific topics from a knowledge base, instead we aimed to evaluate all fact candidates about a given entity. We selected entities from Wikipedia. For this, we chose US politicians: all current state senators, all current state governors, and all 44 presidents. First, we extracted fact candidates\nfrom the infoboxes of the Wikipedia pages of the entities. Second, we applied our alternative candidate generation method to discover alternatives from the Web, SVO corpus, and NELL. Details of the resulting dataset are shown in the row “WKP Politicians” in Table 3.\niii) General Knowledge Quiz: The third dataset consists of questions from a general knowledge quiz 4. We selected questions from the inventions category. Questions are multiple choice, with 4 options per question. Thus, from each question, we created one fact candidate and 3 alternative candidates. Details of the resulting dataset are shown in the row “KWP Quiz” in Table 3. Baselines. We compared FactChecker against five baselines: i) Vote counts the number of sources that mention the fact candidate. ii) TruthFinder is an iterative voting approach where votes are propagated from sources to fact candidates and then back to sources. Implemented as described in (Yin et al., 2007). iii) Investment is also based on transitive voting, however scores are updated differently. A source gets a vote of trust from each candidate it “invests” in, but the vote is weighted by the proportion of trust the source previously “invested” in the candidate relative to other investors. Implemented as described in (Pasternack and Roth, 2010). iv) PooledInvest is a variation of investment, we report both because in their paper, there was no clear winner among the two variations. v) 2-Estimates is a probabilistic model which approximates error rates of sources and fact candidates (Galland et al., 2010)."
  }, {
    "heading": "5.1 Accuracy on KB Fact Candidates",
    "text": "Figure 2 shows accuracy on KB fact candidates. FactChecker achieves accuracy between 70% and 88% and is significantly more accurate than the\n4http://www.indiabix.com/general-knowledge/\nother approaches on all relations except company acquisitions. On book authors, movie directors, and athlete teams, FactChecker outperforms all other approaches by at least 10%, 9%, and 8% respectively. On company acquisitions, the different methods achieve similar accuracy, with TruthFinder being the most accurate and FactChecker is 4% behind. Company acquisitions also yield the lowest difference between Vote and the highest performing method, of 6%. For book authors, movie directors, and athlete teams, the difference between majority Vote and the highest performing method (FactChecker in this case) is 13%, 12%, and 13% respectively."
  }, {
    "heading": "5.2 Accuracy of FactChecker Variations",
    "text": "To quantify how various aspects of our approach affect overall performance, we studied two variations. The first variation is FC-Objectivity which only uses objectivity to compute believability. Thus, λ = 1 in Definition 5. The second variation is FC-CoMention which only uses co-mention scores to compute believability, λ = 0. The\nlast variation is the full FactChecker method using both objectivity and co-mentions with λ = 0.6 From Figure 3, it is clear that both the objectivity of sources and the influence of co-mentions contribute to the overall accuracy of FactChecker. Full-fledged FactChecker performs better than both variations. In most cases, FC-Objectivity performs better than FC-CoMention."
  }, {
    "heading": "5.3 Accuracy on Wikipedia Fact Candidates",
    "text": "Table 4, column “WKP Politicians”, shows accuracy on Wikipedia fact candidates, with a 0.9- confidence Wilson score interval (Brown et al., 2001). For this dataset we again see FactChecker outperforming the other methods under comparison. On this dataset, FactChecker has a accuracy of 0.9 ± 0.07 and a 5% accuracy advantage over the other methods. The second best performance comes from the FC-Objectivity variation, with accuracy of 0.88± 0.08."
  }, {
    "heading": "5.4 Accuracy on General Knowledge Quiz",
    "text": "Table 4, column “GK Quiz ”, shows accuracy on the general knowledge quiz fact candidates. On this dataset, FactChecker and its objectivity-only variation (FC-objectivity) have the highest accuracy of 87%. Notice that this dataset was the only one where we did not generate the alternative fact candidates. Instead, we took the options of the multiple choice questions as alternatives. Since the quiz is meant to be taken by humans, the alternatives are often very close, plausible answers. Yet even in this difficult setting, we see FactChecker outperforming the baselines.\nSample fact candidates, with ranked alternatives from all three datasets are shown in Table 5.\nFigure 4: Effect of λ of FactChecker."
  }, {
    "heading": "5.5 Parameter Sensitivity",
    "text": "We analyzed the effect of the selection of lambda λ (see Definition 5) on FactChecker’s performance. The result of this analysis is shown in Figure 4. FactChecker is insensitive to this parameter when λ is varied from 0.2 to 0.6. Therefore, lambda can be robustly chosen within this range."
  }, {
    "heading": "5.6 Discussion",
    "text": "Overall, from these results we make the following observations: i) Majority vote is a competitive baseline; ii) Iterative voting-based methods provide slight improvements on majority vote. This is due to the fact that at the core of iterative voting is still the assumption that fact candidates mentioned in many sources are more likely to be true. Therefore, for both majority vote and iterative voting, when mention frequencies of various alternatives are the same, accuracy suffers. Based on these observations, it is clear that truthfinding solutions need to incorporate fine-grained content-aware features outside of external votes. FactChecker takes a step in this direction by incorporating the document-level feature of objectivity."
  }, {
    "heading": "6 Related Work",
    "text": "There is a fairly small body of work on truthfinding (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011; Yin and Tan, 2011; Zhao et al., 2012; Pasternack and Roth, 2013). The method underlying most truth-finding algorithms is iterative transitive voting (Yin et al., 2007; Galland et al., 2010; Pasternack and Roth, 2010; Li et al., 2011). Fact candidates are initialized with a score. Trustworthiness of sources is then computed from the believability of the fact candidates they mention. In return, believability of candidates is recomputed based on the trustworthi-\nness of their sources. This process is repeated over several iterations until convergence. (Yin et al., 2007) was the first to implement this idea, subsequent work improved upon iterative voting in several directions. (Dong et al., 2009) incorporates copying-detection; giving high trust to sources that are independently authored. (Galland et al., 2010) approximates error rates of sources and fact candidates. (Pasternack and Roth, 2010) introduces prior knowledge in the form of linear programming constraints in order to ensure that the truth discovered is consistent with what is already known. (Yin and Tan, 2011) introduces supervision by using ground truth facts so that sources that disagree with the ground truth are penalized. (Li et al., 2011) uses search engine APIs to gather additional evidence for believability of fact candidates. WikiTrust (Adler and Alfaro, 2007) is a content-aware but domain-specific method. It computes trustworthiness of wiki authors based on the revision history of the articles they have authored. Motivated by interpretability of probabilistic scores, two recent papers addressed the truth-finding problem as a probabilistic inference problem over the sources and the fact candidates (Zhao et al., 2012; Pasternack and Roth, 2013). Truth-finders based on textual entailment such as TruthTeller (Lotan et al., 2013) determine if a sentence states something or not. The focus is on understanding natural language, including the use of negation. This is similar to the goal of fact extraction (Banko et al., 2007; Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2011; Del Corro and Gemulla, 2013).\nIn a departure from prior work, our method leverages language of sources in its believability\ncomputation model. Furthermore, we introduced a co-mention score which is designed to avoid potential false boots among fact candidates. Additionally, we developed a method for generating alternative fact candidates. Prior methods assume these are readily available. Only (Li et al., 2011) uses the Web to identify alternatives, however, this is only done after manually specifying the fixed argument. In contrast, we introduced a method for identifying the fixed argument based on relation cardinalities learned from SVO statistics."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we presented FactChecker, a language-aware approach to truth-finding. In contrast to prior approaches, which rely on external votes, FactChecker includes objectivity of sources in its believability computation model.\nFactChecker can be seen as a first step towards language-aware truth-finding. Future directions include using more sentence-level features such the use of hedges, assertive verbs, and factive verbs. These types of words fall into a class of words used to express certainties, speculations or doubts — these are important cues that FactChecker can leverage."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank members of the NELL team at CMU for their helpful comments. This research was supported by DARPA under contract number FA8750-13-2-0005."
  }],
  "year": 2014,
  "references": [{
    "title": "Ives: DBpedia: A Nucleus for a Web of Open Data",
    "authors": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "Z.G.R. Cyganiak"],
    "venue": "In Proceedings of the 6th International Semantic Web Conference (ISWC),",
    "year": 2007
  }, {
    "title": "A content-driven reputation system for the wikipedia",
    "authors": ["B.T. Adler", "L. de Alfaro"],
    "venue": "In Proceedings of the 16th International Conference on World Wide Web (WWW),",
    "year": 2007
  }, {
    "title": "Open Information Extraction from the Web",
    "authors": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"],
    "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),",
    "year": 2007
  }, {
    "title": "Interval Estimation for a Binomial Proportion",
    "authors": ["L.D. Brown", "T.T. Cai", "A. Dasgupta"],
    "venue": "Statistical Science",
    "year": 2001
  }, {
    "title": "Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interaction",
    "authors": ["E. Cabrio", "S. Villata"],
    "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2012
  }, {
    "title": "ClausIE: clause-based open information extraction",
    "authors": ["L. Del Corro", "R. Gemulla"],
    "venue": "In Proceedings of the 22nd International Conference on World Wide Web (WWW),",
    "year": 2013
  }, {
    "title": "Truth discovery and copying detection in a dynamic world",
    "authors": ["X. Dong", "L. Berti-Equille", "D. Srivastava"],
    "venue": "In Proceedings of the VLDB Endowment PVLDB,",
    "year": 2009
  }, {
    "title": "Identifying Relations for Open Information Extraction",
    "authors": ["A. Fader", "S. Soderland", "O. Etzioni"],
    "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2011
  }, {
    "title": "Corroborating information from disagreeing views",
    "authors": ["A. Galland", "S. Abiteboul", "A. Marian", "P. Senellart"],
    "venue": "In Proceedings of the 3rd International Conference on Web Search and Web Data Mining (WSDM),",
    "year": 2010
  }, {
    "title": "Alonso: ConceptNet 3: a Flexible, Multilingual Semantic Network for Common Sense Knowledge",
    "authors": ["C. Havasi", "J.R. Speer"],
    "venue": "In Proceedings of the Recent Advances in Natural Language Processing (RANLP), Borovets, Bulgaria,",
    "year": 2007
  }, {
    "title": "Politics and the American Press: The Rise of Objectivity, pages 1865-1920",
    "authors": ["R. Kaplan"],
    "year": 2002
  }, {
    "title": "T-verifier: Verifying truthfulness of fact statements",
    "authors": ["X. Li", "W. Meng", "C.T. Yu"],
    "venue": "In Proceedings of the International Conference on Data Engineering (ICDE),",
    "year": 2011
  }, {
    "title": "DIRT: discovery of inference rules from text",
    "authors": ["D. Lin", "P. Pantel"],
    "venue": "KDD",
    "year": 2001
  }, {
    "title": "Opinion Observer: analyzing and comparing opinions on the Web",
    "authors": ["B. Liu", "M. Hu", "J. Cheng"],
    "venue": "InProceedings of the 14th International Conference on World Wide Web (WWW),",
    "year": 2005
  }, {
    "title": "TruthTeller: Annotating Predicate Truth",
    "authors": ["A. Lotan", "A. Stern", "I. Dagan"],
    "venue": "In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),",
    "year": 2013
  }, {
    "title": "Scalable Knowledge Harvesting with High Precision and High Recall",
    "authors": ["N. Nakashole", "M. Theobald", "G. Weikum"],
    "venue": "In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM),",
    "year": 2011
  }, {
    "title": "Fine-grained Semantic Typing of Emerging Entities",
    "authors": ["N. Nakashole", "T. Tylenda", "G. Weikum"],
    "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2013
  }, {
    "title": "PATTY: A Taxonomy of Relational Patterns with Semantic Types",
    "authors": ["N. Nakashole", "G. Weikum", "F. Suchanek"],
    "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
    "year": 2012
  }, {
    "title": "Elghafari: WikiNet: A Very Large Scale MultiLingual Concept Network",
    "authors": ["V. Nastase", "M. Strube", "B. Boerschinger", "A.C. Zirn"],
    "venue": "In Proceedings of the 7th International Conference on Language Resources and Evaluation(LREC),",
    "year": 2010
  }, {
    "title": "Knowing What to Believe",
    "authors": ["J. Pasternack", "D. Roth"],
    "venue": "In Proceedings the International Conference on Computational Linguistics (COLING),",
    "year": 2010
  }, {
    "title": "Latent credibility analysis",
    "authors": ["J. Pasternack", "D. Roth"],
    "venue": "In Proceedings of the 22nd International Conference on World Wide Web (WWW),",
    "year": 2013
  }, {
    "title": "Learning Learning extraction patterns for subjective expressions",
    "authors": ["E. Riloff", "J. Wiebe"],
    "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2013
  }, {
    "title": "Linguistic Models for Analyzing and Detecting Biased Language",
    "authors": ["M. Recasens", "C. Danescu-Niculescu-Mizil", "D. Jurafsky"],
    "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2013
  }, {
    "title": "DeepDive: Web-scale Knowledge-base Construction using Statistical Learning and Inference",
    "authors": ["F. Niu", "C. Zhang", "C. Re", "J.W. Shavlik"],
    "venue": "In the VLDS Workshop,",
    "year": 2012
  }, {
    "title": "Discovering the News: A Social History of American Newspapers",
    "authors": ["M. Schudson"],
    "venue": "New York: Basic Books",
    "year": 1978
  }, {
    "title": "SOFIE: A Self-organizing Framework for Information Extraction",
    "authors": ["F.M. Suchanek", "M. Sozio", "G. Weikum"],
    "venue": "InProceedings of the 18th International Conference on World Wide Web (WWW),",
    "year": 2009
  }, {
    "title": "Mitchell: Acquiring temporal constraints between relations",
    "authors": ["P.P. Talukdar", "T.M.D.T. Wijaya"],
    "venue": "In Proceeding of the 21st ACM International Conference on Information and Knowledge Management,",
    "year": 2012
  }, {
    "title": "Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews",
    "authors": ["P.D. Turney"],
    "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2002
  }, {
    "title": "Truth Discovery with Multiple Conflicting Information Providers on the Web",
    "authors": ["X. Yin", "J. Han", "P.S. Yu"],
    "venue": "In Proceedings of the International Conference on Knowledge Discovery in Databases (KDD) ,",
    "year": 2007
  }, {
    "title": "Semi-supervised truth discover",
    "authors": ["X. Yin", "W. Tan"],
    "venue": "In Proceedings of the 19th International Conference on World Wide Web (WWW),",
    "year": 2011
  }, {
    "title": "A Bayesian approach to discovering truth from conflicting sources for data integration",
    "authors": ["B. Zhao", "B.I.P. Rubinstein", "J. Gemmell", "J. Han"],
    "venue": "In Proceedings of the VLDB Endowment (PVLDB),",
    "year": 2012
  }],
  "id": "SP:768b5e0b5be77397e7c7308bdd8f2cb05c6bf019",
  "authors": [{
    "name": "Ndapandula Nakashole",
    "affiliations": []
  }, {
    "name": "Tom M. Mitchell",
    "affiliations": []
  }],
  "abstractText": "This paper introduces FactChecker, language-aware approach to truth-finding. FactChecker differs from prior approaches in that it does not rely on iterative peer voting, instead it leverages language to infer believability of fact candidates. In particular, FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated. To ensure that fact candidates mentioned in similar sources have similar believability, FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate. Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches.",
  "title": "Language-Aware Truth Assessment of Fact Candidates"
}