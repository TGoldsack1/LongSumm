{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Principal component analysis (PCA) is a ubiquitous technique in statistics, machine learning and data science. Given a dataset {xi}ni=1 ⊂ Rd, PCA finds a low dimensional subspace of Rd which captures maximum variance in the dataset. PCA is often performed as a pre-processing step for dimensionality reduction to help reduce computational and statistical burden for a downstream learning task, especially in the context of big data.\nPCA can be posed as a stochastic optimization problem, where the samples are assumed to be drawn i.i.d. from an unknown distribution D and the goal is to find a subspace that is almost as good as the optimal subspace in terms of capturing the variance in the distribution; such a view motivates design of stochastic approximation (SA) algorithms that process one sample at a time with a computationally cheap update and can scale to large datasets (Arora et al., 2012; Balsubramani et al., 2013; Jain et al., 2016; Shamir, 2016; Allen-Zhu & Li, 2017a; Mianjy & Arora, 2018).\nIn this paper, we study PCA in a streaming setting. While our algorithms are motivated by previous work on stochastic approximation algorithms for PCA, in our analysis, we also consider a non-stochastic setting where we make no distributional assumptions on data. In particular, the data may have been generated deterministically or even adversarially. Such a setting makes sense for big data applications where the data needs to be processed as it streams in, and it is unreasonable to assume that successive samples are independent or even identically distributed.\n*Equal contribution 1Department of Computer Science, Johns Hopkins University, Baltimore, USA. Correspondence to: Teodor V. Marinov <tmarino2@cs.jhu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nBig data is characterized not only by its sheer “volume” but also by its “veracity”, or the lack thereof. Most big data analysts do not trust the raw data due to large corruptions and deletions. It is therefore crucial to design algorithms for PCA that can handle extremely noisy data as well as missing data and also scale to very large datasets.\nInstead of studying PCA as a stochastic optimization problem, one can consider online PCA. However, the focus of online analysis, and in particular of past work on online PCA (Tsuda et al., 2005; Warmuth & Kuzmin, 2006; 2008; Nie et al., 2013), is on bounding the adversarial regret, rather than on the runtime. A “good” online method might therefore have low online regret, and thus a low iteration complexity as a stochastic procedure, but its expensive runtimeper-iteration might make it unappealing for stochastic approximation. Some recent work by Garber et al. (2015); Allen-Zhu & Li (2017b) on online PCA has focussed on computationally efficient updates. Here, we are concerned both with designing robust streaming algorithms for PCA with arbitrary corruptions void of any distributional assumptions as well as obtaining methods with low overall runtime.\nTherefore, in this paper, we consider both the nonstochastic and stochastic settings – for the former we give variants of online mirror descent with sublinear regret guarantees on the PCA objective, for the latter we give extensions of the computationally efficient Oja’s algorithm. We study PCA in a streaming setting with noisy gradients, missing data, partial observations, and gross outliers.\nTo the best of our knowledge, PCA with corrupted gradients, where corruption can be in the form of noise, missing entries or outliers has not been studied in the online setting. Our first contribution is to provide variants of online mirror descent which obtain optimal regret guarantees in terms of time horizon T and scale gracefully with the amount of corruption to the data.\nIn the stochastic setting, there have been multiple works dealing with subspace recovery when data is missing under some model (Balzano et al., 2010; Lounici et al., 2014; Mitliagkas et al., 2014). Most of these works, either have very stringent requirements on the distribution of the data or prove only local convergence results. Our second main contribution is in extending Oja’s algorithm, when data is assumed to be stochastic, to the setting of corrupted gra-\ndients by nonstochastic noise, missing entries and partial observations.\nThe rest of the paper is organized as follows. In Section 3 we consider streaming PCA in presence of bounded noise, i.e. when the observations are corrupted. In Section 4 we give an algorithm for streaming PCA which is robust to missing data, when the entries are missing at random from a Bernoulli model. We then change the focus in Section 5 to the problem of streaming PCA with partial observations, where only a few entries per sample are observed due to the cost of obtaining measurements. We propose a robust streaming PCA algorithm that can handle outliers in Section 6. Finally, we give experimental evidence for proposed methods in Section 7 and conclude with a discussion in Section 8."
  }, {
    "heading": "2. Notation and Preliminaries",
    "text": "We denote vectors and matrices with small and capital Roman letters, e.g. u and U. The identity matrix of size k is represented by Ik, where the subscript k is dropped whenever the size is clear from the context. The `2 norm of a vector is denoted by ‖·‖. Frobenius norm and spectral norm of matrices are denoted by ‖·‖F and ‖·‖2 respepctively. For any two matrices M1,M2 ∈ Rd×d, the standard inner-product is given as 〈M1,M2〉 = Tr ( M>1 M2 ) . Furthermore, Pk = {P : P2 = P,P = P>, rank(P) = k} denotes the set of rank-k orthogonal projection matrices.\nOnline Setting. The k-dimensional PCA problem in the streaming setting can be formulated as follows. We are presented with a sequence of vectors (xn)∞n=1 in Rd. After observing x1, . . . , xt−1, we need to predict a k-dimensional subspace, represented by a rank-k orthogonal projection matrix P(t), so as to minimize the residual ‖xt − P(t)xt‖2 of the next vector in the stream.\nWe are interested in bounding the adversarial regret, i.e. obtaining an upper bound for ∑T t=1 ‖xt − P\n(t)xt‖2 −∑T t=1 ‖xt−Pxt‖2, that holds for any sequence x1, . . . , xT ∈ Rd, and any competitor P ∈ Pk, where P(t) is the sequence of projection matrices generated by an online algorithm. Minimizing the regret above defined in terms of the residual errors is equivalent to minimizing the regret defined in terms of the variance captured. Therefore, the k-dimensional PCA problem in the streaming setting can be formulated as finding a sequence of subspaces, represented by orthogonal projection matrices, P(t), that minimizes\nR(T, P) = T∑ t=1 x>t Pxt − T∑ t=1 x>t P (t)xt, (1)\nwhere P ∈ Pk is an arbitrary rank-k orthogonal projection matrix. A sublinear regret bound implies that we can drive the average regret, i.e. 1TR(T, P), below any user-specified\n> 0. This allows us to measure the performance of an online algorithm in terms of overall runtime required to achieve -average regret.\nIn the online setting, we consider algorithms that are variants of online mirror descent, a standard algorithm in Online Convex Optimization literature (Beck & Teboulle, 2003). However, since the feasible set Pk is not convex, we relax the feasible set by taking its convex hull,\nC = {P : Tr (P) := k, 0 P I, P = P>}. (2)\nTherefore, our updates are of the following form:\nP(t+1) = ΠF (P(t) + ηg>t ), (3)\nwhere η is the learning rate, gt is the gradient estimate at time t, and ΠF is the projection operator onto the set C with respect to Frobenius norm. The projection step is a simple shift-and-cap procedure described in (Arora et al., 2013).\nSince each iterate P(t) ∈ C can have rank larger than k, we sample a rank-k projection matrix using the rounding procedure described in Algorithm 2 of (Warmuth & Kuzmin, 2008); we denote P̂ (t)\n= rounding(P(t)). Since, the loss function in (1) is linear, is easy to check that the sequence P̂ (t)\nhas the same adversarial regret in expectation w.r.t. the rounding. We refer to these updates as matrix gradient descent (MGD). The following regret bound holds for MGD. Theorem 2.1 (MGD regret). Assume ‖xt‖ ≤ 1 for all t in 1, . . . , T . Then, after T iterations of MGD with step size\nη = √\nk T , and starting at P (1) = 0, we have that\nE[R(T, P)] ≤ √ kT . (4)\nwhere expectation is w.r.t. randomization in the algorithm, and P ∈ Pk is any arbitrary rank-k projection matrix.\nStochastic Setting. The regret bound in Theorem 2.1 is minimax optimal with respect to the time horizon T and dimension d (Nie et al., 2013). The question of computational efficiency, however, still remains. In particular, the per iteration complexity of MGD can grow as large as Ω(d3). This is not desirable or even computationally tractable in a big data setting, where the dimensionality of the data can be very large. To the best of our knowledge, there are no known algorithms in the regret minimization setting which can be computationally better in the worst case and still achieve optimal regret. This gives little hope that we can come up with computationally attractive algorithms for the online problem. Under the additional assumption that xt are sampled i.i.d. from some unknown distribution D and that ‖xt‖ ≤ 1 almost surely, recent work (Allen-Zhu & Li, 2017b) has shown that Oja’s algorithm can obtain sub-linear\nregret for the online PCA problem in the special case of k = 1. At each iteration, Oja’s algorithm, for general k, performs the following updates:\nŨt+1 = P (( I + ηxtx>t ) · · · ( I + ηx1x>1 ) U ) P(t+1) = Ũt+1Ũ > t+1,\nwhere entries of U ∈ Rd×k are sampled from a standard Gaussian distribution and P (A) orthonormalizes the columns of A. Note that computing Ũt+1 takes O(dk2) time, and we never need to form P(t+1) explicitly, so the periteration computational cost of Oja’s algorithm is O(dk2)."
  }, {
    "heading": "3. Streaming PCA with corrupted gradients",
    "text": "Online Setting. We consider a setting where the streaming algorithm receives noisy gradients, i.e. instead of instantaneous gradients xtx>t , it receives the sequence ĝt = xtx>t + Et. The noise could be a result of an inaccurate computation in a big data setting, or a consequence of asynchronous and noisy communication in a distributed or parallel computing scenario. If the noise in the gradient is unbounded, then no learning is possible. Our first main result is in the case of bounded noise and shows that the regret bound for MGD degrades gracefully with the noise level. Furthermore, MGD can easily tolerate noise with overall budget that scales as o(T ) if we desire sublinear regret guarantees.\nTheorem 3.1 (MGD noisy gradient). Assume ‖xt‖ ≤ 1 for all t in 1, . . . , T . Let E1, . . . ,ET be an arbitrary sequence of error in gradients such that ∑T t=1 ‖Et‖2 ≤ E and ‖Et‖F ≤ 1 for all t = 1, · · · , T . Then, after T iterations of MGD with step size η = √ k/T , and starting at P(1) = 0, we have that E[R(T, P)] ≤ 4 √ kT + 2kE, (5)\nwhere expectation is w.r.t. randomization in the algorithm and P ∈ Pk is any arbitrary rank-k projection matrix.\nStochastic Setting. We consider the same stochastic setting as Allen-Zhu & Li (2017b) and further assume that ED[xt] = 0. As before, we consider corrupted gradients, however, we assume that they arise due to additive corruption of data, i.e. each of the points xt are perturbed by some noise vector yt. The noisy gradients we observe are ĝt = (xt + yt)(xt + yt)\n>. We assume yt is independent of xt but make no other stochastic assumption on yt. We do require that the total noise is bounded. We make this explicit in the following theorem.\nTheorem 3.2 (Oja noisy gradient). Assume that xt ∼ D, ‖xt‖ ≤ 1 almost surely for all t = 1, . . . , T and ED[xt] = 0. Assume that ∑T t=1 ‖yt‖+ ‖yt‖ 2 ≤ √ T . After T iterations of Oja’s algorithm starting with u ∼ N (0, I) and using step\nsize η = β√ T for some small enough β, with probability 1− δ it holds that:\nR(T, P∗) ≤ c √ T log(d+ log(2/δ)) β + √ T\nlog (\n8 3δ2 ) β ,\nwhere c is some universal constant not depending on δ or d and P∗ = arg maxP∈P1 ∑T t=1〈gt,P〉.\nWe remark that β has no dependence on d or T , however, it is at mostO(log(1+δ)). Since the gradients we are working with, ĝt, are not unbiased and not bounded by a constant at each iteration necessarily, the result provided in (AllenZhu & Li, 2017b) does not apply directly. We adapt their analysis, by decomposing the instantaneous gradient ĝt = (xt + yt)(xt + yt) > into an unbiased term gt = xtx > t and an error term ĝt − gt. The assumption that the noise is sublinear allows us to control this error term and still achieve a sublinear regret bound."
  }, {
    "heading": "4. Learning with Missing Entries",
    "text": "Often in applications with large volumes of data streaming in, malfunction of the measurement or data acquisition systems results in data corruption or large gaps in the collection. Hence, it is crucial to design algorithms that can reliably handle missing data. In this section, we introduce a simple randomized scheme to extend the streaming MGD algorithm to handle missing data. The key insight here is that with the proposed randomized scheme we can still obtain unbiased estimates of the instantaneous gradient based on missing data.\nThe problem of PCA with missing data has been studied before in the stochastic setting (Balzano et al., 2010; Mitliagkas et al., 2014). The setting we consider here is closely related to that of Mitliagkas et al. (2014). In particular, both the missing-ness model as well as Oja’s updates that we consider here are as in (Mitliagkas et al., 2014). However, there are two key differences. First, we give sublinear regret bounds in a non-stochastic setting. Second, in the stochastic setting, we make less stringent assumptions; while Mitliagkas et al. (2014) assume that the data is generated using a spiked-covariance model, we only need to assume that the distribution of data has bounded support.\nIn the Bernoulli model of missing-ness (Candès et al., 2011) that we consider here, for each data point xt, we sample d Bernoulli random variables Zi ∼ Bernoulli(q), i = 1, . . . , d, to get the index set of observed entries Ω := {i ∈ [d] : Zi = 1}. The observed vector x̃t is then given by\nx̃i = { xi, i ∈ Ω 0, otherwise .\nLets denote the distribution of the observed vector conditioned on x by R. Then, it is easy to check that ER[‖x̃‖0|x] = qd, i.e, on average qd elements of each vector are observed under the model R. It is known that x̃x̃> is not an unbiased estimator of xx> with respect toR (see (Lounici et al., 2014) or Lemma ?? in the Appendix). To address this issue, we propose the following random model for constructing an unbiased estimator of xx>. Assume r entries of x are observed, and let Ω = {i1, · · · , ir} be the indices of observed elements. We construct ĝ := x̂x̂> − zz> where x̂ = 1q x̃ and z = √ r−rq q xiseis and is is sampled uniformly at random from Ω. Let S denote the conditional distribution of z given x. The following holds. Lemma 4.1. ĝ is an unbiased estimator of xx>, that is:\nES,R[x̂x̂> − zz>|x] = xx>, (6)\nwhere the expectation is with respect to both S andR.\nOnline Setting. Lemma 4.1 motivates the following rMGD updates with missing entries based on ĝt := −x̂tx̂ > t + ztz>t : P(t+1) = ΠF ( P(t) − ηtĝt ) . (7)\nMGD enjoys the following regret bound. Theorem 4.2 (MGD regret with missing data). Assume ‖xt‖ ≤ 1 for all t in 1, . . . , T . Then, after T iterations of the MGD update in (7), with step size η = q2√ q2+dq(1−q)3+d2q2(1−q)2 √ k T , and starting at P (1) = 0, we have that: E [R(T, P)] ≤ Cr,d √ kT , (8)\nfor any rank-k projection matrix P. Here, Cr,d =√ q2+dq(1−q)3+d2q2(1−q)2 q2 , and the expectation is w.r.t. randomization in the algorithm.\nNote that the regret bound in Theorem 4.2 degrades gracefully with the parameter q. As q → 1, Cr,d → 1 and we recover the bound in Theorem 2.1. On the other hand, as q becomes smaller, the tradeoff parameter Cr,d grows and for q = O(1/d), we get Cr,d = O(d2).\nStochastic Setting. As discussed in Section 2, MGD can be inefficient. Again, we consider the stochastic setting where xt are sampled i.i.d. from some distribution D and ‖xt‖ ≤ 1 almost surely. We consider Oja’s algorithm for k = 1 with gradients ĝt. However, as the gradients are not guaranteed to be positive-semidefinite, the results in (AllenZhu & Li, 2017b) do not apply directly. We are able to adapt the proof techniques by decomposing the gradient into a positive semidefinite part and a negative semidefinite part. It turns out that the negative semidefinite part can only hurt us in terms of increasing the norm of the gradient, however, this only leads to a constant factor in the regret bound.\nTheorem 4.3 (Oja regret from missing data). Assume that xt ∼ D for all t in 1, . . . , T and that ‖xt‖ ≤ 1 almost surely. Let C = Ex∼D[xx>] with top eigenvalue λ, let αn = 2n−1 qn + 2n−1µn(1−q)n\nq2n , where µn is the n-th moment of the Binomial distribution, and let α = α4 + 4α3 + 6α2. Then, after T iterations of Oja’s algorithm with gradients ĝt, initialization P\n(1) = uu>, where u ∼ N (0, I) and step size η = log(1+δ/9)\n(α+4λ2) √ T , with probability 1− δ it holds that:\nES,R[R(T, P)] ≤ √ T\nlog(1 + δ/18) +\n√ T (α+ 4λ2)O ( log(d+ log ( 1 δ ) )− log ( 1 δ )) log(1 + δ) ,\nfor any rank-1 projection matrix P.\nAs q → 1 we see that the regret tends to O (√\nT log(d+log(1/δ)) log(1+δ)\n) . For any fixed δ, the regret above\nequals O( √ T log(d)), which has an additional multiplicative factor of log(d) compared to the bound in Theorem 4.2. However, this does not take the per-iteration computational cost of both algorithms into account. To better understand the trade-off between Oja’s algorithm with missing entries and MGD with missing entries we look at the overall runtime needed to achieve -average regret. The total runtime of MGD for achieving -average regret is O(d 3\n2 ). On the other hand the per-iteration complexity of Oja’s algorithm is O(d) and thus the total run-time for achieving -average regret is O(d log 2(d) 2 ). Therefore, we see that Oja’s algorithm has much better overall performance in terms of runtime when considering average regret as q → 1. The case is a bit different as we let q → 1d . This suggests that α = O(d\n8) and the regret bound for Oja’s algorithm is O(d8 √ T log(d)), while the regret bound for MGD is O(d2 √ T ). Thus, Oja’s algorithm with missing entries becomes intractable in such a setting. However, we note that the regret bound for Oja’s algorithm with missing entries might not be minimax optimal in terms of the dependence on dimensionality d and in practice we have not observed such discrepancy in our experiments."
  }, {
    "heading": "5. Learning from Partial Observations",
    "text": "In many real world applications, acquiring a full feature vector may be challenging or there may be cost associated with fetching each entry of the vector. Consequently, in such settings, it is essential that data analysis solutions are designed to work reliably and robustly with partially observed data. In this section, we introduce a randomized scheme that ensures obtaining unbiased estimates of the gradient based on partial observations. This allows a simple extension of the streaming MGD algorithm to handle partial observations.\nWe consider the uniform sampling model that has been used extensively in the literature (see, e.g. (Recht, 2011)). In particular, we observe r entries uniformly at random from all subsets of cardinality r. At each iterate, we sample an indexing subset Ω := {i1, .., ir} ⊆ [1 · · · d] with cardinality 0 < r ≤ d uniformly at random from all subsets of size r. The observed vector x̃ is now constructed as\nx̃i = { xi, i ∈ Ω 0, otherwise.\nAs in the previous section, let R denote the conditional distribution of the observed vector. Again, we observe that x̃x̃> is not an unbiased estimator of xx> with respect to R (see Lemma ?? in the Appendix). To construct an unbiased estimator of xx>, we propose the following stochastic\nmodel. We define ĝ := x̂x̂> − zz> where x̂ = √\nd(d−1) r(r−1) x̃, and z = √\ndr−r2 r−1 x̃iseis and ei is the i-th standard basis vec-\ntor and is is sampled uniformly at random from the set of observed elements Ω. Let S be the conditional distribution induced by this model. Then, the following holds. Lemma 5.1. ĝ is an unbiased estimator of xx>, i.e.\nES,R[x̂x̂> − zz>|x] = xx>, (9) where the expectation is with respect to both S andR.\nOnline Setting. Lemma 5.1 motivates the following MGD updates with partial observations based on ĝt := −x̂tx̂>t + ztz>t :\nP(t+1) = ΠF ( P(t) − ηĝt ) . (10)\nWe show that the following regret bound holds for MGD with partial observations. Theorem 5.2 (MGD regret from partial observations). Assume ‖x‖t ≤ 1 for all t in 1, . . . , T . Then, after T iterations of the MGD update in (10), with step size η = r(r−1)√ d2(d−1)2+r4(d−r)2 √ k T , and starting at P (1) = 0, we have that: E [R(T, P)] ≤ Cr,d √ kT , (11)\nfor any rank-k projection matrix P. Here, the expectation is w.r.t. randomization in the algorithm, and the multiplicative factor Cr,d = √ d2(d−1)2+r4(d−r)2\nr(r−1) .\nNote that the regret bound degrades gracefully with the fraction of observed entries. The parameter Cr,d determines the tradeoff between iteration complexity and the cost of data access. As r → d, one can see that Cr,d → 1, which recovers the bound in Theorem 2.1. On the other hand, as r becomes smaller, Cr,d grows and for r = 2, one can see that Cr,d = O(d2). This is especially interesting for applications where abundant number of samples are provided, but obtaining measurements per sample is highly costly.\nStochastic Setting As in Section 4, MGD with partial observations can have worst case per-iteration complexity O(d3). We make the same stochastic assumptions as in the previous section, extend Oja’s algorithm to work with the gradients ĝt and adapt the regret bound from (Allen-Zhu & Li, 2017b). All of our remarks about the per-iteration computational cost of Oja from section 4 still hold.\nTheorem 5.3 (Oja regret from partial observations). Assume that xt ∼ D for all t in 1, . . . , T and that ‖xt‖ ≤ 1 almost surely. Let C = Ex∼D[xx>] with top eigenvalue λ. Then, after T iterations of Oja’s algorithm with gradients ĝt, initialization P(1) = uu>, where u ∼ N (0, I) and step size η = log(1+δ/9)\n(11α2+4λ2) √ T , where α = d(d−1)r(r−1) , with probability 1− δ it holds that:\nES,R[R(T, P)] ≤ 2 √ T\nlog(1 + δ/18) +\n√ T (11α2 + 4λ2)\nlog(1 + δ) O\n( log ( d+ log ( 1\nδ\n)) − log ( 1\nδ\n)) ,\nfor any rank-1 projection matrix P.\nWhen r → d, we essentially recover the regret bound in Theorem 4.3 and the same comparison done in section 4 holds when discussing total computational time for MGD versus Oja to achieve -average regret. When r → 2, we see that α→ d2 and the regret for Oja’s algorithm becomes O ( d4 log(d) √ T )\n. Again we see that MGD with partial observations has a better worst-case regret bound in terms of d and that both algorithms become intractable for large d."
  }, {
    "heading": "6. Robust streaming PCA",
    "text": "Despite its ubiquitous nature, PCA as well as other subspace learning methods have a major weakness – they are extremely sensitive to outliers. Corrupted data points, which we refer to as outliers, can completely throw off the estimate of the principal subspace even with a single outlier (Huber & Ronchetti, 2009). In practice, we may encounter a high percentage of corruption (Zhang & Lerman, 2014) and in theory (under some assumptions) the percentage of outliers tolerated by robust PCA algorithms can be significantly higher than the common 50% breakdown point of point estimators (Zhang & Lerman, 2014; Lerman et al., 2012; Hardt & Moitra, 2013). In such cases, the inliers may still be viewed as arising from D, but the outliers are likely to be generated by a different distribution or may be even hard to model. The presence of these outliers, whose proportion may be significant, can completely distort the estimate of the expected variance and therefore the PCA subspace.\nThere have been several attempts to endow PCA with resilience against outliers or other forms of gross corruptions (see e.g., (De La Torre & Black, 2003; Fischler &\nBolles, 1981; Gnanadesikan & Kettenring, 1972; Hampel et al., 2005; Huber & Ronchetti, 2009; Hubert et al., 2005; Ke & Kanade, 2005; Maronna et al., 2006; Recht et al., 2010; Xu et al., 2010)). Following (Chandrasekaran et al., 2011), Candès et al. (2011) established a convex de-convolution method for extracting low-dimensional subspace structure in the presence of gross but sparse uniformly distributed element-wise corruptions. Given a dataset X ∈ Rd×n, the robust PCA formulation considered by (Chandrasekaran et al., 2011) and (Candès et al., 2011), seeks a rank-k representation of X, denoted by L ∈ Rd×n, that minimizes the `1-norm of the residuals, ‖S‖1, where S := X−L.\nThe seminal work of (Chandrasekaran et al., 2011) and (Candès et al., 2011) inspired the development of many other convex methods for robust PCA, that are robust in the presence of outliers (instead of element-wise corruptions) (Xu et al., 2012; McCoy & Tropp, 2011; Zhang & Lerman, 2014; Lerman et al., 2012). These works consider absolute subspace deviations, i.e. they seek a rank-k subspace that minimizes ∑n i=1 ‖xi−Pxi‖2, where ‖ ·‖ denotes the `2-norm. They involve various convex relaxations of this minimizer. Of particular interest to us are the Geometric Median Subspace (GMS) algorithm (Zhang & Lerman, 2014) and the REAPER algorithm (Lerman et al., 2012). We prefer them since they do not require an arbitrary unknown regularization parameter, they can deal with significantly high percentage of outliers (both in theory and in practice) and their batch formulations are faster.\nHowever, both GMS and REAPER are batch algorithms and therefore do not scale to big data. In this section, we study robust PCA in a streaming setting. We build on absolute subspace deviations model of (Zhang & Lerman, 2014) and (Lerman et al., 2015) and propose a robust analogue of streaming PCA that imparts it robustness in face of outliers. Unlike Goes et al. (2014) who consider robust PCA in a stochastic setting and focus on the -suboptimality, our goal is to bound the following regret,\nRabs(T ) = T∑ t=1 ‖xt − P(t)xt‖2 − inf P∈Pk T∑ t=1 ‖xt − Pxt‖2,\nfor any sequence x1, . . . , xT ∈ Rd.\nThe gradient of the loss function `(xt) = ‖xt − P(t)xt‖2 in the formulation above is given as (I−P\n(t))xtx>t ‖xt−P(t)xt‖\n. This is a rank-one update that is not guaranteed to be symmetric. In order for our analysis to go through we consider the following symmetrized loss: 12Ex[‖x−Px‖2 +‖x−P\n>x‖2]. The instantaneous gradient at the t-th iteration is then given by\ngt = − xtx>t (I−P(t))+(I−P(t))xtx>t\n2‖(I− P(t))xt‖2 .\nWe denote yt = (I − P (t))xt, and ct = η2‖yt‖2 , then the proposed abs-MGD update can be written as:\nP(t+1) = ΠF ( P(t) + ct(xty>t + ytx > t ) ) . (12)\nWe bound the regret of abs-MGD updates in (12) as follows.\nTheorem 6.1. Assume ‖xt‖ ≤ 1 for all t in 1, · · · , T . Then, after T iterations of MGD with step size η = √ k T , and starting at P(1) = 0, we have that:\nRabs(T ) ≤ √ kT ."
  }, {
    "heading": "7. Experimental Results",
    "text": "Per iteration complexity. Before presenting an empirical evaluation of our algorithms we would like to discuss their computational efficiency in theory. Note that each variant of the MGD algorithm involves updating the current iterate P(t) ∈ Rd×d with a rank-1 or a rank-2 matrix and then projecting onto a convex set of constraints. Since the projection step operates on the eigenvalues of the current iterate (Arora et al., 2013), a naive implementation would require O(d3) time per iteration. To avoid recomputing eigenvalues, we can keep an up-to-date eigendecomposition of each iterate and perform an efficient rank-1 (or rank-2) update which takes O(dk̃2) time where k̃ = rank(P(t)). Of course, k̃ may grow as large as d. In contrast, Oja’s algorithm and its variants take only O(dk2) time per iteration.\nDatasets and step-size tuning. We evaluate empirical performance of our algorithms with missing data (MGDMD, Oja-MD) and partial observations (MGD-PO, OjaPO) on two real datasets, MNIST (LeCun et al., 1998) and XRMB (Westbury, 1994) against vanilla MGD and classic Oja’s algorithm (Oja, 1982) as well as with the state-ofthe-art algorithm (GROUSE) of Balzano et al. (2010). The learning rate for variants of MGD and Oja’s algorithm is set to ηt = η0√t , for MGD-PO to ηt = r2η0 d2 √ t , and for MGDMD to ηt = q2 η0√t . The initial learning rate η0 is chosen using cross validation on a held-out set. The learning rate for GROUSE is set to ηt = η0√t , even though the theory suggests a step size proportional to 1t ; this choice was made since GROUSE did not converge in our experiments with a step size of Θ(1/t).\nEmpirical results. Figures 1 and 2 show the objective as a function of the number of observations as well as a function of elapsed time, for different values of rank (k), on XRMB and MNIST datasets, respectively. We see that both MGD-MD and MGD-PO recover the subspace even when nearly 92% of the observations are missing. We see consistently across experiments that (a) MGD outperforms\n88% missing 90% missing 92% missing\nall other algorithms in terms of progress per number of observations, and (b) Oja’s algorithm always performs better than Oja-MD and Oja-PO both of which are nearly as bad as GROUSE. For Oja’s algorithm, we note that even though the theoretical guarantees in Theorems 4.3, and 5.3 only hold for k = 1, our experiments suggest that the sub-linear regret guarantees perhaps still hold for larger k. Furthermore, the experimental results seem to be consistent with theoretical guarantees in terms of per-iteration progress and total runtime.\nComparison with theory. Our theoretical bounds for k = 1 suggest that MGD is always better than MGD-PO and MGD-MD when the observation ratio is small. Again, when the observation ratio is small, Oja-MD and Oja-PO have worse upper bounds on the average regret, compared to MGD-PO and MGD-MD, by at least a factor of dimensionality and that both should perform worse than Oja with full observations. Our experiments confirm these observations. Note that even though both in Figures 1 and 2, MGD-PO and MGD-MD seem to perform as well as MGD, this is in term of observations and not in terms of number of iterations, which are far fewer for MGD. Our experiments suggest extensions of our theory in at least two directions. First, for Oja’s algorithm and its variants, similar theoretical upper bounds on the regret should hold for general k. Second, it is possible that there are matching lower bounds for the algorithms dealing with partially observed and missing data.\nRuntime. As expected, Oja’s algorithm and its variants perform much better in terms of progress per runtime as their per-iteration complexity is only O(dk2). MGD performs as well as GROUSE, Oja-PO, and Oja-MD in terms of runtime. This is because the rank of the iterates P(t) remains in O(k). This is not the case, however, for MGD-PO and MGD-MD and their progress per runtime is significantly slower.\nBecause of space constraints we deferred some experiments, including plots for per-iteration progress of the algorithms and plots for Robust-MGD, to the supplementary; the above observations still hold for the deferred plots."
  }, {
    "heading": "8. Discussion",
    "text": "In this paper, we study PCA in a streaming setting with no distributional assumptions on data. In the big data setting we consider, data is often contaminated with noise, outliers, or observed partially. We propose several efficient algorithms to solve the above problems and prove sublinear regret bounds. As we already discuss in the paper, the data which our algorithms process can be generated by an adversary and thus we quantify the loss of our algorithms in terms of regret. Theorem 2.1 gives a bound on the regret of MGD with respect to any fixed subspace P chosen in hindsight. One might argue that this is not a real-world setting and the subspaces we are comparing against should be allowed to vary as incoming data is observed. We can strengthen our results by comparing against sequences of subspaces, with a bounded total shift, all chosen in hindsight.\nTheorem 8.1 (MGD switch regret). Assume ‖xt‖ ≤ 1 for all t in 1, · · · , T . Then, after T iterations of MGD with step size η = √ k T , and starting at P (1) = 0, we have that T∑ t=1 x>t P (t) ∗ xt− T∑ t=1 Eround [ x>t P (t)xt ] ≤ √ (6 √ kS + k)T\nwhere (P(t)∗ ) T t=1 is any competing sequence of subspaces in C with total shift ∑T t=1 ‖P (t) ∗ − P(t−1)∗ ‖F ≤ S.\nOur experiments suggest the following directions for future work: (a) extend the analysis of the Oja’s algorithm (i.e. results in Theorems 3.2, 4.3 and 5.3) to general k > 1), and (b) show lower bounds for regret guarantees in Section 4 and Section 5 which depend on the number of missing entries.\nWe would also like to investigate Oja-like updates for the `2 Robust PCA formulation in Section 6, which preserve the low-rank structure of the iterates P(t). Analyzing such an algorithm, even in the special cases when data is stochastic and k = 1, seems like a daunting task, because unlike the standard PCA formulation, we do not have a closed form solution for the optimization problem. This in turn is an obstacle when trying to come up with potential functions tracking the progress of the proposed algorithms.\nWe also remark that for robust streaming PCA in Section 6, the iterates P(t) ∈ Rd×d are in the convex set C defined in equation (2), however, they need not necessarily be projection matrices. Furthermore, due to the non-linear nature of the objective we can not simply use the rounding procedure\nas in (Warmuth & Kuzmin, 2008). In practice, we observe that one can use the rank-k projection retrieved from the top k eigen-vectors of P(t). This can be partially justified by the results in (Lerman et al., 2015) which state that under certain mild assumptions on the outliers, the solution to the optimization problem minP∈P ∑T t=1 ‖xt−Pxt‖2 is close to the rank-k projection matrix retrieved from the solution of the convex relaxation minP∈C ∑T t=1 ‖xt−Pxt‖2. The result in Theorem 6.1 can be extended to show that the sequence (P(t))Tt=1 does not suffer large regret against the optimal P∗ ∈ C. In future work, we hope to show that this implies that the average of the iterates (P(t))Tt=1, or the final iterate P(T ), is close in norm to P∗. This together with results in (Lerman et al., 2015) would explain why in practice using the rank-k projection closest to P(t) works well.\nAnother possible direction for future work is to design and analyze streaming algorithms for related component analysis techniques in noisy settings. In particular, algorithms based on online mirror descent have been used in the context of partial least squares (Arora et al., 2016) and canonical correlation analysis (Arora et al., 2017; Ge et al., 2016). It is natural to consider extensions of these algorithms to noisy settings with missing data and outliers."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by NSF BIGDATA grant IIS-1546482."
  }],
  "year": 2018,
  "references": [{
    "title": "First efficient convergence for streaming k-PCA: a global, gap-free, and near-optimal rate",
    "authors": ["Z. Allen-Zhu", "Y. Li"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2017
  }, {
    "title": "Follow the compressed leader: Faster online learning of eigenvectors and faster MMWU",
    "authors": ["Z. Allen-Zhu", "Y. Li"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Stochastic optimization for PCA and PLS",
    "authors": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"],
    "venue": "In Communication, Control, and Computing (Allerton),",
    "year": 2012
  }, {
    "title": "Stochastic optimization of PCA with capped MSG",
    "authors": ["R. Arora", "A. Cotter", "N. Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Stochastic optimization for multiview representation learning using partial least squares",
    "authors": ["R. Arora", "P. Mianjy", "T. Marinov"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Stochastic approximation for canonical correlation analysis",
    "authors": ["R. Arora", "T.V. Marinov", "P. Mianjy", "N. Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "The fast convergence of incremental PCA",
    "authors": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Online identification and tracking of subspaces from highly incomplete information",
    "authors": ["L. Balzano", "R. Nowak", "B. Recht"],
    "venue": "In Communication, Control, and Computing (Allerton),",
    "year": 2010
  }, {
    "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
    "authors": ["A. Beck", "M. Teboulle"],
    "venue": "Operations Research Letters,",
    "year": 2003
  }, {
    "title": "Robust principal component analysis",
    "authors": ["E.J. Candès", "X. Li", "Y. Ma", "J. Wright"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2011
  }, {
    "title": "Rank-sparsity incoherence for matrix decomposition",
    "authors": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"],
    "venue": "SIAM J. Optim.,",
    "year": 2011
  }, {
    "title": "A framework for robust subspace learning",
    "authors": ["F. De La Torre", "M.J. Black"],
    "venue": "International Journal of Computer Vision,",
    "year": 2003
  }, {
    "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography",
    "authors": ["M.A. Fischler", "R.C. Bolles"],
    "venue": "Communications of the ACM,",
    "year": 1981
  }, {
    "title": "Online learning of eigenvectors",
    "authors": ["D. Garber", "E. Hazan", "T. Ma"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Efficient algorithms for large-scale generalized eigenvector computation and canonical correlation analysis",
    "authors": ["R. Ge", "C. Jin", "P. Netrapalli", "A Sidford"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Robust estimates, residuals, and outlier detection with multiresponse data",
    "authors": ["R. Gnanadesikan", "J.R. Kettenring"],
    "year": 1972
  }, {
    "title": "Robust stochastic principal component analysis",
    "authors": ["J. Goes", "T. Zhang", "R. Arora", "G. Lerman"],
    "venue": "In Artificial Intelligence and Statistics (AISTATS),",
    "year": 2014
  }, {
    "title": "Robust Statistics: The Approach Based on Influence Functions (Wiley Series in Probability and Statistics)",
    "authors": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"],
    "venue": "URL http://www",
    "year": 2005
  }, {
    "title": "Can we reconcile robustness and efficiency in unsupervised learning",
    "authors": ["M. Hardt", "A. Moitra"],
    "venue": "In Proceedings of the Twenty-sixth Annual Conference on Learning Theory (COLT",
    "year": 2013
  }, {
    "title": "Robust Statistics. Wiley Series in Probability and Statistics",
    "authors": ["P.J. Huber", "E.M. Ronchetti"],
    "venue": "ISBN 978-0-470-12990-6",
    "year": 2009
  }, {
    "title": "ROBPCA: a new approach to robust principal component analysis",
    "authors": ["M. Hubert", "P.J. Rousseeuw", "K.V. Branden"],
    "year": 2005
  }, {
    "title": "Streaming PCA: Matching matrix bernstein and nearoptimal finite sample guarantees for oja’s algorithm",
    "authors": ["P. Jain", "C. Jin", "S.M. Kakade", "P. Netrapalli", "A. Sidford"],
    "venue": "In Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming",
    "authors": ["Q. Ke", "T. Kanade"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR",
    "year": 2005
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Robust computation of linear models, or how to find a needle in a haystack",
    "authors": ["G. Lerman", "M. McCoy", "J.A. Tropp", "T. Zhang"],
    "year": 2012
  }, {
    "title": "Robust computation of linear models by convex relaxation",
    "authors": ["G. Lerman", "M.B. McCoy", "J.A. Tropp", "T. Zhang"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2015
  }, {
    "title": "High-dimensional covariance matrix estimation with missing observations",
    "authors": ["K Lounici"],
    "year": 2014
  }, {
    "title": "Robust statistics: Theory and methods. Wiley Series in Probability and Statistics",
    "authors": ["R.A. Maronna", "R.D. Martin", "V.J. Yohai"],
    "year": 2006
  }, {
    "title": "Two proposals for robust PCA using semidefinite programming",
    "authors": ["M. McCoy", "J. Tropp"],
    "venue": "Elec. J. Stat.,",
    "year": 2011
  }, {
    "title": "Stochastic PCA with l 2 and l 1 regularization",
    "authors": ["P. Mianjy", "R. Arora"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2018
  }, {
    "title": "Streaming PCA with many missing entries",
    "authors": ["I. Mitliagkas", "C. Caramanis", "P. Jain"],
    "year": 2014
  }, {
    "title": "Online PCA with optimal regrets",
    "authors": ["J. Nie", "W. Kotłowski", "M.K. Warmuth"],
    "venue": "In International Conference on Algorithmic Learning Theory,",
    "year": 2013
  }, {
    "title": "Simplified neuron model as a principal component analyzer",
    "authors": ["E. Oja"],
    "venue": "Journal of mathematical biology,",
    "year": 1982
  }, {
    "title": "A simpler approach to matrix completion",
    "authors": ["B. Recht"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
    "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"],
    "venue": "SIAM review,",
    "year": 2010
  }, {
    "title": "Convergence of stochastic gradient descent for PCA",
    "authors": ["O. Shamir"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Matrix exponentiated gradient updates for on-line learning and bregman projection",
    "authors": ["K. Tsuda", "G. Rätsch", "M.K. Warmuth"],
    "venue": "In Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Online variance minimization",
    "authors": ["M.K. Warmuth", "D. Kuzmin"],
    "venue": "In Learning theory,",
    "year": 2006
  }, {
    "title": "X-ray microbeam speech production database users handbook: Madison",
    "authors": ["J. Westbury"],
    "venue": "WI: Waisman Center, University of Wisconsin,",
    "year": 1994
  }, {
    "title": "Principal component analysis with contaminated data: The high dimensional case",
    "authors": ["H. Xu", "C. Caramanis", "S. Mannor"],
    "venue": "In COLT, pp",
    "year": 2010
  }, {
    "title": "Robust PCA via outlier pursuit",
    "authors": ["H. Xu", "C. Caramanis", "S. Sanghavi"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2012
  }, {
    "title": "A novel M-estimator for robust PCA",
    "authors": ["T. Zhang", "G. Lerman"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }],
  "id": "SP:c3eef6960c6f5132a55140ed166d2c937b6d5fb5",
  "authors": [{
    "name": "Teodor V. Marinov",
    "affiliations": []
  }, {
    "name": "Poorya Mianjy",
    "affiliations": []
  }, {
    "name": "Raman Arora",
    "affiliations": []
  }],
  "abstractText": "We study streaming algorithms for principal component analysis (PCA) in noisy settings. We present computationally efficient algorithms with sub-linear regret bounds for PCA in the presence of noise, missing data, and gross outliers.",
  "title": "Streaming Principal Component Analysis in Noisy Settings"
}