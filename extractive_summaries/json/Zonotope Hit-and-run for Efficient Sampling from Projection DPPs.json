{
  "sections": [{
    "text": "butions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costlyto-evaluate functions, such as summary extraction in our experiments."
  }, {
    "heading": "1. Introduction",
    "text": "Determinantal point processes (DPPs) are distributions over configurations of points that encode diversity through a kernel function. DPPs were introduced by Macchi (1975) and have then found applications in fields as diverse as probability (Hough et al., 2006), number theory (Rudnick & Sarnak, 1996), statistical physics (Pathria & Beale, 2011), Monte Carlo methods (Bardenet & Hardy, 2016), and spatial statistics (Lavancier et al., 2015). In machine learning, DPPs over finite sets have been used as a model of diverse sets of items, where the kernel function takes the\n1\nUniv. Lille, CNRS, Centrale Lille, UMR 9189 — CRIStAL\n2\nINRIA Lille — Nord Europe, SequeL team. Correspondence to:\nGuillaume Gautier <g.gautier@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nform of a finite matrix, see Kulesza & Taskar (2012) for a comprehensive survey. Applications of DPPs in machine learning (ML) since this survey also include recommendation tasks (Kathuria et al., 2016; Gartrell et al., 2017), text summarization (Dupuy & Bach, 2016), or models for neural signals (Snoek et al., 2013).\nSampling generic DPPs over finite sets is expensive. Roughly speaking, it is cubic in the number r of items in a DPP sample. Moreover, generic DPPs are sometimes specified through an n ⇥ n kernel matrix that needs diagonalizing before sampling, where n is the number of items to pick from. In text summarization, r would be the desired number of sentences for a summary, and n the number of sentences of the corpus to summarize. Thus, sampling quickly becomes intractable for large-scale applications (Kulesza & Taskar, 2012). This has motivated research on fast sampling algorithms. While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al., 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b). In particular, there are polynomial bounds on the mixing rates of natural MCMC chains with arbitrary DPPs as their limiting measure; see Anari et al. (2016) for cardinality-constrained DPPs, and Li et al. (2016b) for the general case.\nIn this paper, we contribute a non-obvious MCMC chain to approximately sample from projection DPPs, which are DPPs with a fixed sample cardinality. Leveraging a combinatorial geometry result by Dyer & Frieze (1994), we show that sampling from a projection DPP over a finite set can be relaxed into an easier continuous sampling problem with a lot of structure. In particular, the target of this continuous sampling problem is supported on the volume spanned by the columns of the feature matrix associated to the projection DPP, a convex body also called a zonotope. This zonotope can be partitioned into tiles that uniquely correspond to DPP realizations, and the relaxed target distribution is flat on each tile. Previous MCMC approaches to sampling projections DPPs can be viewed as attempting\nmoves between neighboring tiles. Using linear programming, we propose an MCMC chain that moves more freely across this tiling. Our chain is a natural transformation of a fast mixing hit-and-run Markov chain (Lov´asz & Vempala, 2003) on the underlying zonotope; this empirically results in more uncorrelated MCMC samples than previous work. While the results of Anari et al. (2016) and their generalization by Li et al. (2016b) apply to projection DPPs, our experiments support the fact that our chain mixes faster.\nThe rest of the paper is organized as follows. In Section 2, we introduce projection DPPs and review existing approaches to sampling. In Section 3, we introduce zonotopes and we tailor the hit-and-run algorithm to our needs. In Section 4, we empirically investigate the performance of our MCMC kernel on synthetic graphs and on a summary extraction task, before concluding in Section 5."
  }, {
    "heading": "2. Sampling Projections DPPs",
    "text": "In this section, we introduce projection DPPs in two equivalent ways, respectively following Hough et al. (2006), Kulesza & Taskar (2012), and Lyons (2003). Both definitions shed a different light on the algorithms in Section 3."
  }, {
    "heading": "2.1. Projection DPPs as Particular DPPs",
    "text": "Let E = [n] , {1, . . . n}. Let also K be a real symmetric positive semidefinite n ⇥ n matrix, and for I ⇢ E, write K\nI for the square submatrix of K obtained by keeping only rows and columns indexed by I ⇢ E. The random subset X ⇢ E is said to follow a DPP on E = {1, . . . , n} with kernel K if\nP [I ⇢ X] = detK I , 8I ⇢ E. (1)\nExistence of the DPP described by (1) is guaranteed provided K has all its eigenvalues in [0, 1], see e.g., Kulesza & Taskar (2012, Theorem 2.3). Note that (1) encodes the repulsiveness of DPPs. In particular, for any distinct i, j 2 [n],\nP [{i, j} ⇢ X] =\nK ii K ij K ji K jj\n= P [{i} 2 X]P [{j} 2 X] K2 ij  P [{i} 2 X]P [{j} 2 X] .\nIn other words, K ij encodes departure from independence. Similarly, for constant K ii ,K jj , the larger K2 ij , the less likely it is to have items i and j co-occur in a sample. Projection DPPs are the DPPs such that the eigenvalues of K are either 0 or 1, that is, K is the matrix of an orthogonal projection. Projection DPPs are also sometimes called elementary DPPs (Kulesza & Taskar, 2012). One can show\nthat samples from a projection DPP with kernel matrix K almost surely contain r = Tr(K) points and that general DPPs are mixtures of projection DPPs, see e.g., Kulesza & Taskar (2012, Theorem 2.3)."
  }, {
    "heading": "2.2. Building Projection DPPs from Linear Matroids",
    "text": "Let r < n, and let A be a full-rank r ⇥ n real matrix with columns (a\nj\n) j2[n]. The linear matroid M [A] is defined as the pair (E,B), with E = [n] and\nB = n B⇢ [n] : |B| = r, {a j } j2B are independent o . (2)\nA set of indices B ⇢ [n] is in B if and only if it indexes a basis of the columnspace of A. Because of this analogy, elements of B are called bases of the matroid M [A]. Note that elementary algebra yields that for all B\n1\n, B\n2 2 B and x 2 B\n1 \\B 2 , there exists an element y 2 B 2 \\B 1 such that\n(B\n1 \\ {x}) [ {y} 2 B. (3) Property (3) is known as the basis-exchange property. It is used in the definition of general matroids (Oxley, 2003).\nLyons (2003) defines a projection DPP as the probability measure on B that assigns to B 2 B a mass proportional to | detB|2, where B , A\n:B\nis the square matrix\nformed by the r columns of A indexed by B. Note that this squared determinant is also the squared volume of the parallelotope spanned by the columns indexed by B. In this light, sampling a projection DPP is akin to volume sampling (Deshpande & Rademacher, 2010). Finally, observe that the Cauchy-Binet formula gives the normalization\nX B2B |detA :B |2 = detAAT,\nso that the probability mass assigned to B is\ndetAT B: detA :B\ndetAAT = det\nh\nAT [AAT] 1 A i\nB\n.\nLetting\nK = AT [AAT] 1 A, (4)\ngives the equivalence between Sections 2.1 and 2.2.\nA fundamental example of DPP defined by a matroid is the random set of edges obtained from a uniform spanning tree (Lyons, 2003). Let G be a connected graph with r + 1 vertices and n edges {e i } i2[n]. Let now A be the first r rows of the vertex-edge incidence matrix of G. Then B ⇢ [n] is a basis of M [A] if and only if {e\ni } i2B form a spanning\ntree of G (Oxley, 2003). The transfer current theorem of Burton & Pemantle (1993) implies that the uniform distribution on B is a projection DPP, with kernel matrix (4).\n2.3. On Projection DPPs and k-DPPs in ML\nProjection DPPs are DPPs with realizations of constant cardinality k = r, where r is the rank of K. This constant cardinality is desirable when DPPs are used in summary extraction (Kulesza & Taskar, 2012; Dupuy & Bach, 2016) and the size of the required output is predefined. Another way of constraining the cardinality of a DPP is to condition on the event |X| = k, which leads to the so-called k-DPPs (Kulesza & Taskar, 2012). Projection DPPs and k-DPPs are in general different objects. In particular, a k-DPP is not a DPP in the sense of (1) unless its kernel matrix K is a projection. In that sense, k-DPPs are non-DPP objects that generalize projection DPPs. In this paper, we show that projection DPPs can benefit from fast sampling methods. It is not obvious how to generalize our algorithm to k-DPPs.\nIn ML practice, using projection DPPs is slightly different from using a k-DPP. In some applications, typically with graphs, the DPP is naturally a projection, such as uniform spanning trees described in Section 2.2. But quite often, kernels are built feature-by-feature. That is, for each data item i 2 [n], a normalized vector of features i\n2 Rr is chosen, a marginal relevance q\ni\nis assigned to item i, and a\nmatrix L is defined as\nL ij =\np q\ni i j\np q\nj\n. (5)\nIn text summarization, for instance, items i, j could be sentences, q\ni\nthe marginal relevance of sentence i to the user’s\nquery, and\ni\nfeatures such as tf-idf frequencies of a choice\nof words, and one could draw from a k-DPP associated to L through P [X = I] / detL I , see e.g., Kulesza & Taskar (2012, Section 4.2.1). Alternately, let A be the matrix with columns ( p q\ni i\n) i2[r], and assume r < n and A is full-rank. The latter can be ensured in practice by adding a small i.i.d. Gaussian noise to each entry of A. The projection DPP with kernel K in (4) will yield samples of cardinality r, almost surely, and such that the corresponding columns of A span a large volume, hence feature-based diversity. Thus, if the application requires an output of length p, one can pick r = p, as we do in Section A. Alternatively, if we want an output of size approximately p, we can pick r p and independently thin the resulting sample, which preserves the DPP structure (Lavancier et al., 2015)."
  }, {
    "heading": "2.4. Exact Sampling of Projection DPPs",
    "text": "Hough et al. (2006) give an algorithm to sample general DPPs, which is based on a subroutine to sample projection DPPs. Consider a projection DPP with kernel K such that Tr(K) = r, Hough et al.’s (2006) algorithm follows the chain rule to sample a vector (x\n1\n, . . . , x\nr ) 2 [n]r with successive conditional densities\np (x\n`+1 = i|x 1 = i 1 , . . . , x ` = i ` ) / K ii K i,I\n` K 1 I\n`\nK I\n`\n,i\n,\nwhere I\n` = {i 1 , . . . , i ` }. Forgetting order, {x 1 , . . . , x r } are a draw from the DPP (Hough et al., 2006, Proposition 19), see also Kulesza & Taskar (2012, Theorem 2.3) for a detailed treatment of DPPs on [n]. While exact, this algorithm runs in O(nr3) operations and requires computing and storing the n⇥n matrix K. Storage can be diminished if one has access to A in (4), through QR decomposition of AT. Still, depending on n and r, sampling can become intractable. This has sparked interest in fast approximate sampling methods for DPPs, which we survey in Section 2.5. Interestingly, there exist fast and exact methods for sampling some specific DPPs, which are not based on the approach of Hough et al. (2006). We introduced the DPP behind uniform spanning trees on a connected graph G in Section 2.2. Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r3) and can be o(r 3 ) (Levin et al., 2009), where G has r + 1 vertices. This compares favorably with the algorithm of Hough et al. (2006) above, since each sample contains r edges. The Aldous-Broder algorithm, for instance, starts from an empty set T = ; and an arbitrary node x 0 , and samples a simple random walk (X\nt\n) t2N on the edges of G, starting from X\n0\n= x\n0\n, and adding edge [X\nt\n, X\nt+1\n] to\nT the first time it visits vertex X t+1 . The algorithm stops when each vertex has been seen at least once, that is, at the cover time of the graph."
  }, {
    "heading": "2.5. Approximate Sampling of Projection DPPs",
    "text": "There are two main sets of methods for approximate sampling from general DPPs. The first set uses the generalpurpose tools from numerical algebra and the other is based on MCMC sampling. Consider K = CTC with C of size d ⇥ n, for some d ⌧ n (Kulesza & Taskar, 2011), but still too large for exact sampling using the method of Hough et al. (2006), then Gillenwater et al. (2012) show how projecting C can give an approximation with bounded error. When this decomposition of the kernel is not possible, Affandi et al. (2013) adapt Nystr¨om sampling (Williams & Seeger, 2001) to DPPs and bound the approximation error for DPPs and k-DPPs, which thus applies to projection DPPs.\nApart from general purpose approximate solvers, there exist MCMC-based methods for approximate sampling from projection DPPs. In Section 2.2, we introduced the basisexchange property, which implies that once we remove an element from a basis B\n1\nof a linear matroid, any other ba-\nsis B\n2\nhas an element we can take and add to B\n1\nto make\nit a basis again. This means we can construct a connected\nAlgorithm 1 basisExchangeSampler Input: Either A or K Initialize i 0 and pick B\n0 2 B as defined in (2) while Not converged do\nDraw u ⇠ U [0,1] if u < 1\n2\nthen\nDraw s ⇠ U B\ni\nand t ⇠ U [n]\\B\ni\nP (B i \\ {s}) [ {t} Draw u 0 ⇠ U [0,1]\nif u 0 <\nVol 2 (A:P )\nVol 2 (B\ni\n)+Vol 2 (A:P ) =\ndetK P\ndetK B\ni\n+detK P\nthen\nB\ni+1 P else\nB\ni+1\nB i\nend if\nelse\nB\ni+1\nB i\nend if\ni i+ 1 end while\ngraph G\nbe with B as vertex set, and we add an edge between two bases if their symmetric difference has cardinality 2. G\nbe is called the basis-exchange graph. Feder & Mihail (1992) show that the simple random walk on G\nbe\nhas lim-\niting distribution the uniform distribution on B and mixes fast, under conditions that are satisfied by the matroids involved by DPPs. If the uniform distribution on B is not the DPP we want to sample from, 1 we can add an accept-reject step after each move to make the desired DPP the limiting distribution of the walk. Adding such an acceptance step and a probability to stay at the current basis, Anari et al. (2016); Li et al. (2016b) give precise polynomial bounds on the mixing time of the resulting Markov chains. This Markov kernel on B is given in Algorithm 1. Note that we use the acceptance ratio of Li et al. (2016b). In the following, we make use of the notation Vol defined as follows. For any P ⇢ [n],\nVol\n2 (A :P ) , detAT P : A :P / detK P , (6)\nwhich corresponds to the squared volume of the parallelotope spanned by the columns of A indexed by P . In particular, for subsets P such that |P | > r or such that |P | = r, P /2 B we have Vol2(A\n:P ) = 0. However, for B 2 B, Vol 2 (B) = | detA :B\n|2 > 0. We now turn to our contribution, which finds its place in this category of MCMC-based approximate DPP samplers.\n1 It may not even be a DPP (Lyons, 2003, Corollary 5.5)."
  }, {
    "heading": "3. Hit-and-run on Zonotopes",
    "text": "Our main contribution is the construction of a fast-mixing Markov chain with limiting distribution a given projection DPP. Importantly, we assume to know A in (4). Assumption 1. We know a full-rank r ⇥ n matrix A such that K = AT(AAT) 1A.\nAs discussed in Section 2.3, this is not an overly restrictive assumption, as many ML applications start with building the feature matrix A rather than the similarity matrix K."
  }, {
    "heading": "3.1. Zonotopes",
    "text": "We define the zonotope Z(A) of A as the r-dimensional volume spanned by the column vectors of A,\nZ(A) = A[0, 1]n. (7) As an affine transformation of the unit hypercube, Z(A) is a r-dimensional polytope. In particular, for a basis B 2 B of the matroid M [A], the corresponding Z(B) is a rdimensional parallelotope with volume Vol(B) = |detB|, see Figure 1(a). On the contrary, any P ⇢ [n], such that |P | = r, P /2 B also yields a parallelotope Z(A\n:P\n), but\nits volume is null. In the latter case, the exchange move in Algorithm 1 will never be accepted and the state space of the corresponding Markov chain is indeed B. Our algorithm relies on the proof of the following.\nProposition 1 (see Dyer & Frieze, 1994 for details).\nVol(Z(A)) = X\nB2B Vol(B) =\nX B2B |detB| (8)\nProof. In short, for a good choice of c 2 Rn, Dyer & Frieze (1994) consider for any x 2 Z(A), the following linear program (LP) noted P\nx\n(A, c),\nmin y2Rn c\nT\ny\ns.t. Ay = x 0  y  1.\n(9)\nStandard LP results (Luenberger & Ye, 2008) yield that the unique optimal solution y ⇤ of P\nx\n(A, c) takes the form\ny ⇤ = A⇠(x) +B\nx\nu, (10)\nwith u 2 [0, 1]r and ⇠(x) 2 {0, 1}n such that ⇠(x) i = 0 for i 2 B x . In case the choice of B x is ambiguous, Dyer & Frieze (1994) take the smallest in the lexicographic order. Decomposition (10) allows locating any point x 2 Z(A) as falling inside a uniquely defined parallelotope Z(B\nx\n)\nshifted by ⇠(x). Manipulating the optimality conditions of (9), Dyer & Frieze (1994) prove that each basis B can be realized as a B\nx\nfor some x, and that x 0 2 Z(B x )) B x =\n(a) (b) (c)\nB\nx\n0 . This allows to write Z(A) as the tiling of all Z(B), B 2 B, with disjoint interiors. This leads to Proposition 1.\nNote that c is used to fix the tiling of the zonotope, but the map x 7! B x depends on this linear objective. Therefore, the tiling of Z(A) is may not be unique. An arbitrary c gives a valid tiling, as long as there are no ties when solving (9). Dyer & Frieze (1994) use a nonlinear mathematical trick to fix c. In practice (Section 4.1), we generate a random Gaussian c once and for all, which makes sure no ties appear during the execution, with probability 1. Remark 1. We propose to interpret the proof of Proposition 1 as a volume sampling algorithm: if one manages to sample an x uniformly on Z(A), and then extracts the corresponding basis B = B\nx by solving (9), then B is drawn with probability proportional to Vol(B) = | detB|. Remark 1 is close to what we want, as sampling from a projection DPP under Assumption 1 boils down to sampling a basis B of M [A] proportionally to the squared volume | detB|2 (Section 2.2). In the rest of this section, we explain how to efficiently sample x uniformly on Z(A), and how to change the volume into its square."
  }, {
    "heading": "3.2. Hit-and-run and the Simplex Algorithm",
    "text": "Z(A) is a convex set. Approximate uniform sampling on large-dimensional convex bodies is one of the core questions in MCMC, see e.g., Cousins & Vempala (2016) and references therein. The hit-and-run Markov chain (Turˇcin, 1971; Smith, 1984) is one of the preferred practical and theoretical solutions (Cousins & Vempala, 2016).\nWe describe the Markov kernel P (x, z) of the hit-and-run Markov chain for a generic target distribution ⇡ supported\non a convex set C. Sample a point y uniformly on the unit sphere centered at x. Letting d = y x, this defines the line D\nx , {x+ ↵d ; ↵ 2 R}. Then, sample z from any Markov kernel Q(x, ·) supported on D\nx\nthat leaves the re-\nstriction of ⇡ to D x invariant. In particular, MetropolisHastings kernel (MH, Robert & Casella 2004) is often used with uniform proposal on D x , which favors large moves across the support C of the target, see Figure 1(b). The resulting Markov kernel leaves ⇡ invariant, see e.g., Andersen & Diaconis (2007) for a general proof. Furthermore, the hit-and-run Markov chain has polynomial mixing time for log concave ⇡ (Lov´asz & Vempala, 2003, Theorem 2.1).\nTo implement Remark 1, we need to sample from ⇡\nu / 1Z(A). In practice, we can choose the secondary Markov kernel Q(x, ·) to be MH with uniform proposal on D\nx\n, as\nlong as we can determine the endpoints x+↵\nm (y x) and x+↵\nM (y x) of D x \\Z(A). In fact, zonotopes are tricky convex sets, as even an oracle saying whether a point belongs to the zonotope requires solving LPs (basically, it is Phase I of the simplex algorithm). As noted by Lov´asz & Vempala (2003, Section 4.4), hit-and-run with LP is the state-of-the-art for computing the volume of large-scale zonotopes. Thus, by definition of Z(A), this amounts to solving two more LPs: ↵\nm\nis the optimal solution to the\nlinear program\nmin 2Rn,↵2R ↵\ns.t. x+ ↵d = A 0   1,\n(11)\nwhile ↵\nM\nis the optimal solution of the same linear pro-\ngram with objective ↵. Thus, a combination of hit-andrun and LP solvers such as Dantzig’s simplex algorithm (Luenberger & Ye, 2008) yields a Markov kernel with invariant distribution 1Z(A), summarized in Algorithm 2.\nAlgorithm 2 unifZonoHitAndRun Input: A Initialization:\ni 0 x\n0\nAu with u ⇠ U [0,1] n\nwhile Not converged do\nDraw d ⇠ USr 1 and let Dx i , x i + Rd Draw ex ⇠ UD\nx\ni \\Z(A) #Solve 2 LPs, see (11) x\ni+1 ex i i+ 1\nend while\nAlgorithm 3 extractBasis Input: A, c, x 2 Z(A) Compute y ⇤ the opt. solution of P\nx (A, c) #1 LP, see (9) B {i ; y⇤\ni 2]0, 1[} return B\nThe acceptance in MH is 1 due to our choice of the proposal and the target. By the proof of Proposition 1, running Algorithm 2, taking the output chain (x\ni\n) and extracting\nthe bases (B\nx\ni ) with Algorithm 3, we obtain a chain on B with invariant distribution proportional to the volume of B.\nIn terms of theoretical performance, this Markov chain inherits Lov´asz & Vempala’s (2003) mixing time as it is a simple transformation of hit-and-run targeting the uniform distribution on a convex set. We underline that this is not a pathological case and it already covers a range of applications, as changing the feature matrix A yields another zonotope, but the target distribution on the zonotope stays uniform. Machine learning practitioners do not use volume sampling for diversity sampling yet, but nothing prevents it, as it already encodes the same feature-based diversity as squared volume sampling (i.e., DPPs). Nevertheless, our initial goal was to sample from a projection DPP with kernel K under Assumption 1. We now modify the Markov chain just constructed to achieve that."
  }, {
    "heading": "3.3. From Volume to Squared Volume",
    "text": "Consider the probability density function on Z(A)\n⇡\nv\n(x) =\n|detB x | detAAT 1Z(A)(x),\nrepresented on our example in Figure 1(c). Observe, in particular, that ⇡\nv is constant on each Z(B). Running the hit-and-run algorithm with this target instead of ⇡\nu\nin\nSection 3.2, and extracting bases using Algorithm 3 again, we obtain a Markov chain on B with limiting distribution ⌫(B) proportional to the squared volume spanned by column vectors of B, as required. To see this, note that ⌫(B) is the volume of the “skyscraper” built on top of Z(B) in Figure 1(c), that is Vol(B)⇥Vol(B).\nAlgorithm 4 volZonoHitAndRun Input: A, c, x, B Draw d ⇠ USr 1 and let Dx , x+ Rd Draw ex ⇠ UD\nx \\Z(A) #Solve 2 LPs, see (11) e\nB extractBasis(A, c, ex) #Solve 1 LP, see (9) Draw u ⇠ U\n[0,1]\nif u <\nVol( eB) Vol(B) = detA: eB detA:B then\nreturn ex,\ne\nB\nelse\nreturn x,B\nend if\nAlgorithm 5 zonotopeSampler Input: A, c Initialization:\ni 0 x\ni\nAu, with u ⇠ U [0,1] n\nB\ni\nextractBasis(A, c, x i ) while Not converged do\nx\ni+1\n, B\ni+1\nvolZonoHitAndRun(A, c, x i , B i ) i i+ 1\nend while\nThe resulting algorithm is shown in Algorithm 5. Note the acceptance ratio in the subroutine Algorithm 4 compared to Algorithm 2, since the target of the hit-and-run algorithm is not uniform anymore."
  }, {
    "heading": "3.4. On Base Measures",
    "text": "As described in Section 2.3, it is common in ML to specify a marginal relevance q\ni of each item i 2 [n], i.e., the base measure of the DPP. Compared to a uniform base measure, this means replacing A by eA with columns ea\ni\n= p q\ni\na\ni\n.\nContrary to A, in Algorithm 4, both the zonotope and the acceptance ratio are scaled by the corresponding products of p q\ni\ns. We could equally well define eA by multiplying each column of A by q\ni\ninstead of its square root, and\nleave the acceptance ratio in Algorithm 4 use columns of the original A. By the arguments in Section 3.3, the chain (B\ni\n) would leave the same projection DPP invariant. In particular, we have some freedom in how to introduce the marginal relevance q\ni\n, so we can choose the latter solution\nthat simply scales the zonotope and its tiles to preserve outer angles, while using unscaled volumes to decide acceptance. This way, we do not create harder-to-escape or sharper corners for hit-and-run, which could lead the algorithm to be stuck for a while (Cousins & Vempala, 2016, Section 4.2.1). Finally, since hit-and-run is efficient at moving across convex bodies (Lov´asz & Vempala, 2003), the rationale is that if hit-and-run was empirically mixing fast before scaling, its performance should not decrease."
  }, {
    "heading": "4. Experiments",
    "text": "We investigate the behavior of our Algorithm 5 on synthetic graphs in Section 4.1, in summary extraction in Section 4.2, and on MNIST in Appendix A."
  }, {
    "heading": "4.1. Non-uniform Spanning Trees",
    "text": "We compare Algorithm 1 studied by Anari et al. (2016); Li et al. (2016b) and our Algorithm 5 on two types of graphs, in two different settings. The graphs we consider are the complete graph K\n10\nwith 10 vertices (and 45 edges) and\na realization BA(20, 2) of a Barab´asi-Albert graph with 20 vertices and parameter 2. We chose BA as an example of structured graph, as it has the preferential attachment property present in social networks (Barab´asi & Albert, 1999). The input matrix A is a weighted version of the vertex-edge incidence matrix of each graph for which we keep only the 9 (resp. 19) first rows, so that it satisfies Assumption 1. For more generality, we introduce a base measure, as described in Section 2.3 and 3.4, by reweighting the columns of A with i.i.d. uniform variables in [0, 1]. Samples from the corresponding projection DPP are thus spanning trees drawn proportionally to the products of their edge weights.\nFor Algorithm 5, a value of the linear objective c is drawn once and for all, for each graph, from a standard Gaussian distribution. This is enough to make sure no ties appear during the execution, as mentioned in Section 3.1. This linear objective is kept fixed throughout the experiments so that the tiling of the zonotope remains the same. We run both algorithms for 70 seconds, which corresponds to roughly 50 000 iterations of Algorithm 5. Moreover, we run 100 chains in parallel for each of the two algorithms. For each of the 100 repetitions, we initialize the two algorithms with the same random initial basis, obtained by solving (9) once, with x = Au and u ⇠ U [0,1] n . For both graphs, the total number |B| of bases is of order 108, so computing total variation distances is impractical. We instead compare Algorithms 1 and 5 based on the estimation of inclusion probabilities P [S ⇢ B] for various subsets S ⇢ [n] of size 3. We observed similar behaviors across 3-subsets, so we display here the typical behavior on a 3-subset.\nThe inclusion probabilities are estimated via a running average of the number of bases containing the subsets S. Figures 2(a) and 3(a) show the behavior of both algorithms vs. MCMC iterations for the complete graph K\n10\nand a re-\nalization of BA(20, 2), respectively. Figures 2(b) and 3(b) show the behavior of both algorithms vs. wall-clock time for the complete graph K\n10\nand a realization of BA(20, 2),\nrespectively. In these four figures, bold curves correspond to the median of the relative errors, whereas the frontiers of colored regions indicate the first and last deciles of the relative errors.\nIn Figures 2(c) and 3(c) we compute the Gelman-Rubin statistic (Gelman & Rubin, 1992), also called the potential scale reduction factor (PSRF). We use the PSRF implementation of CODA (Plummer et al., 2006) in R, on the 100 binary chains indicating the presence of the typical 3-subset in the current basis.\nIn terms of number of iterations, our Algorithm 5 clearly mixes faster. Relatedly, we observed typical acceptance rates for our algorithm an order of magnitude larger than Algorithm 1, while simultaneously attempting more global moves than the local basis-exchange moves of Algorithm 1. The high acceptance is partly due to the structure of the zonotope: the uniform proposal in the hit-and-run algorithm already favors bases with large determinants, as the length of the intersection of D\nx\nin Algorithm 4 with any\nZ(B) is an indicator of its volume, see also Figure 1(b). Under the time-horizon constraint, see Figures 2(b) and 3(b), Algorithm 1 has time to perform more than 10 6 iterations compared to roughly 50 000 steps for our chain. The acceptance rate of Algorithm 5 is still 10 times larger, but the time required to solve the linear programs at each MCMC iteration clearly hinders our algorithm in terms of CPU time. Both algorithms are comparable in performance, but given its large acceptance, we would expect our algorithm to perform better if it was allowed to do even only 10 times more iterations. Now this is implementationdependent, and our current implementation of Algorithm 5 is relatively naive, calling the simplex algorithm in the GLPK (Oki, 2012) solver with CVXOPT (Andersen et al., 2008) from Python. We think there are big potential speedups to realize in the integration of linear programming solvers in our code. Moreover, we initialize our simplex algorithms randomly, while the different LPs we solve are related, so there may be additional smart mathematical speed-ups in using the path followed by one simplex instance to initialize the next.\nFinally, we note that the performance of our Algorithm 5 seems stable and independent of the structure of the graph, while the performance of the basis-exchange Algorithm 1 seems more graph-dependent. Further investigation is needed to make stronger statements."
  }, {
    "heading": "4.2. Text Summarization",
    "text": "Looking at Figures 2 and 3, our algorithm will be most useful when the bottleneck is mixing vs. number of iterations rather than CPU time. For instance, when integrating a costly-to-evaluate function against a projection DPP, the evaluation of the integrand may outweigh the cost of one iteration. To illustrate this, we adapt an experiment of Kulesza & Taskar (2012, Section 4.2.1) on minimum Bayes risk decoding for summary extraction. The idea is to find a\n(a) Relative error vs. MCMC iterations. (b) Relative error vs. wall-clock time. (c) PSRF vs. MCMC iterations.\n(a) Relative error vs. MCMC iterations. (b) Relative error vs. wall-clock time. (c) PSRF vs. MCMC iterations.\nsubset Y of sentences of a text that maximizes\n1\nR\nR\nX\nr=1\nROUGE-1F (Y, Y\nr\n) , (12)\nwhere (Y\nr\n)\nr\nare sampled from a projection DPP. ROUGE-\n1F is a measure of similarity of two sets of sentences. We summarize this 64-sentence article as a subset of 11 sentences. In this setting, evaluating once ROUGE-1F in the sum (12) takes 0.1s on a modern laptop, while one iteration of our algorithm is 10 3 s. Our Algorithm 5 can thus compute (12) for R = 10 000 in about the same CPU time as Algorithm 1, an iteration of which costs 10 5 s. We show in Figure 4 the value of (12) for 3 possible summaries\nY\n(i)\n3 i=1 chosen uniformly at random in B, over 50 independent runs. The variance of our estimates is smaller, and the number of different summaries explored is about 50%, against 10% for Algorithm 1. Evaluating (12) using our algorithm is thus expected to be closer to the maximum of the underlying integral. Details are given in Appendix B."
  }, {
    "heading": "5. Discussion",
    "text": "We proposed a new MCMC kernel with limiting distribution being an arbitrary projection DPP. This MCMC kernel leverages optimization algorithms to help making global moves on a convex body that represents the DPP. We provided empirical results supporting its fast mixing when compared to the state-of-the-art basis-exchange chain of Anari et al. (2016); Li et al. (2016b). Future work will focus on an implementation: while our MCMC chain mixes faster, when compared based on CPU time our algorithm suffers from having to solve linear programs at each iter-\nation. We note that even answering the question whether a given point belongs to a zonotope involves linear programming, so that chord-finding procedures used in slice sampling (Neal, 2003, Sections 4 and 5) would not provide significant computational savings.\nWe also plan to investigate theoretical bounds on the mixing time of our Algorithm 4. We can build upon the work of Anari et al. (2016), as our Algorithm 4 is also a weighted extension of our Algorithm 2, and the polynomial bounds for the vanilla hit-and-run algorithm (Lov´asz & Vempala, 2003) already apply to the latter. Note that while not targeting a DPP, our Algorithm 2 already samples items with feature-based repulsion, and could be used independently if the determinantal aspect is not crucial to the application.\nAcknowledgments The research presented was supported by French Ministry of Higher Education and Research, CPER NordPas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, and French National Research Agency projects EXTRA-LEARN (n.ANR-14-CE24-0010-01) and BOB (n.ANR-16-CE23-0003)."
  }],
  "year": 2017,
  "references": [{
    "title": "The random walk construction of uniform spanning trees and uniform labelled trees",
    "authors": ["D.J. Aldous"],
    "venue": "SIAM Journal on Discrete Mathematics,",
    "year": 1990
  }, {
    "title": "Monte-Carlo Markov chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes",
    "authors": ["N. Anari", "S.O. Gharan", "A. Rezaei"],
    "venue": "In Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Hit and run as a unifying device",
    "authors": ["H.C. Andersen", "P.W. Diaconis"],
    "venue": "Journal de la Société Française de Statistique,",
    "year": 2007
  }, {
    "title": "CVXOPT: A python package for convex optimization",
    "authors": ["M. Andersen", "J. Dahl", "L. Vandenberghe"],
    "year": 2008
  }, {
    "title": "Emergence of scaling in random networks",
    "authors": ["asi", "A.-L", "R. Albert"],
    "venue": "Science, 286:11,",
    "year": 1999
  }, {
    "title": "Monte-Carlo with determinantal point processes",
    "authors": ["R. Bardenet", "A. Hardy"],
    "venue": "arXiv preprint arXiv:1605.00361,",
    "year": 2016
  }, {
    "title": "Generating random spanning trees",
    "authors": ["A. Broder"],
    "venue": "In Foundations of Computer Science,",
    "year": 1989
  }, {
    "title": "Local characteristics, entropy and limit theorems for spanning trees and domino tilings via transfer impedances",
    "authors": ["R. Burton", "R. Pemantle"],
    "venue": "The Annals of Probability,",
    "year": 1993
  }, {
    "title": "A practical volume algorithm",
    "authors": ["B. Cousins", "S. Vempala"],
    "venue": "Mathematical Programming Computation,",
    "year": 2016
  }, {
    "title": "Efficient volume sampling for row/column subset selection",
    "authors": ["A. Deshpande", "L. Rademacher"],
    "venue": "In Foundations of Computer Science,",
    "year": 2010
  }, {
    "title": "Learning determinantal point processes in sublinear time",
    "authors": ["C. Dupuy", "F. Bach"],
    "venue": "arXiv preprint arXiv:1610.05925,",
    "year": 2016
  }, {
    "title": "Random walks, totally unimodular matrices, and a randomised dual simplex algorithm",
    "authors": ["M. Dyer", "A. Frieze"],
    "venue": "Mathematical Programming,",
    "year": 1994
  }, {
    "title": "Balanced matroids",
    "authors": ["T. Feder", "M. Mihail"],
    "venue": "Proceedings of the twenty-fourth annual ACM,",
    "year": 1992
  }, {
    "title": "Low-rank factorization of determinantal point processes for recommendation",
    "authors": ["M. Gartrell", "U. Paquet", "N. Koenigstein"],
    "venue": "In AAAI Conference on Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "Inference from iterative simulation using multiple sequences",
    "authors": ["A. Gelman", "D.B. Rubin"],
    "venue": "Statist. Sci.,",
    "year": 1992
  }, {
    "title": "Discovering diverse and salient threads in document collections",
    "authors": ["J. Gillenwater", "A. Kulesza", "B. Taskar"],
    "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
    "year": 2012
  }, {
    "title": "Determinantal processes and independence",
    "authors": ["B. ag"],
    "venue": "Probability surveys,",
    "year": 2006
  }, {
    "title": "Fast determinantal point process sampling with application to clustering",
    "authors": ["B. Kang"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Batched gaussian process bandit optimization via determinantal point processes",
    "authors": ["T. Kathuria", "A. Deshpande", "P. Kohli"],
    "venue": "Neural Information Processing Systems, pp. pp",
    "year": 2016
  }, {
    "title": "Determinantal point processes for machine learning",
    "authors": ["A. Kulesza", "B. Taskar"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "k-dpps: Fixed-size determinantal point processes",
    "authors": ["A. Kulesza", "B. Taskar"],
    "venue": "International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Determinantal point process models and statistical inference",
    "authors": ["F. Lavancier", "J. Møller", "E. Rubak"],
    "venue": "Journal of the Royal Statistical Society. Series B: Statistical Methodology,",
    "year": 2015
  }, {
    "title": "Efficient sampling for kdeterminantal point processes",
    "authors": ["C. Li", "S. Jegelka", "S. Sra"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Fast mixing markov chains for strongly rayleigh measures, dpps, and constrained sampling",
    "authors": ["C. Li", "S. Jegelka", "S. Sra"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Landmarking manifolds with gaussian processes",
    "authors": ["D. Liang", "J. Paisley"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "NLTK: The natural language toolkit",
    "authors": ["E. Loper", "S. Bird"],
    "venue": "In Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,",
    "year": 2002
  }, {
    "title": "Hit and run is fast and fun",
    "authors": ["L. asz", "S. Vempala"],
    "venue": "Technical Report MSR-TR-2003-05,",
    "year": 2003
  }, {
    "title": "Linear and nonlinear programming",
    "authors": ["D.G. Luenberger", "Y. Ye"],
    "year": 2008
  }, {
    "title": "Determinantal probability measures",
    "authors": ["R. Lyons"],
    "venue": "Publications Mathématiques de l’Institut des Hautes Études Scientifiques,",
    "year": 2003
  }, {
    "title": "The coincidence approach to stochastic point processes",
    "authors": ["O. Macchi"],
    "venue": "Advances in Applied Probability,",
    "year": 1975
  }, {
    "title": "Gnu linear programming kit, version 4.61",
    "authors": ["E. Oki"],
    "year": 2012
  }, {
    "title": "What is a matroid",
    "authors": ["J. Oxley"],
    "venue": "Cubo Matemática Educacional,",
    "year": 2003
  }, {
    "title": "Coda: Convergence diagnosis and output analysis for MCMC",
    "authors": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"],
    "venue": "R News,",
    "year": 2006
  }, {
    "title": "Fast mixing for discrete point processes",
    "authors": ["P. Rebeschini", "A. Karbasi"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2015
  }, {
    "title": "Monte-Carlo Statistical Methods",
    "authors": ["C.P. Robert", "G. Casella"],
    "year": 2004
  }, {
    "title": "Zeros of principal L-functions and random matrix theory",
    "authors": ["Z. Rudnick", "P. Sarnak"],
    "venue": "Duke Mathematical Journal,",
    "year": 1996
  }, {
    "title": "Efficient Monte-Carlo procedures for generating points uniformly distributed over bounded regions",
    "authors": ["R.L. Smith"],
    "venue": "Operations Research,",
    "year": 1984
  }, {
    "title": "A determinantal point process latent variable model for inhibition in neural spiking data",
    "authors": ["J. Snoek", "R. Zemel", "R.P. Adams"],
    "venue": "In Neural Information Processing Systems,",
    "year": 1932
  }, {
    "title": "On the computation of multidimensional integrals by the monte-carlo method",
    "authors": ["V.F. cin"],
    "venue": "Theory of Probability & Its Applications,",
    "year": 1971
  }],
  "id": "SP:b2a118d3d800e472d328b9caddb58ee45d3442c3",
  "authors": [{
    "name": "Guillaume Gautier",
    "affiliations": []
  }, {
    "name": "Michal Valko",
    "affiliations": []
  }],
  "abstractText": "Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costlyto-evaluate functions, such as summary extraction in our experiments.",
  "title": "Zonotope Hit-and-run for Efficient Sampling from Projection DPPs"
}