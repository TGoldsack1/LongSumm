{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1131–1141, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The pattern of language use in a multilingual society is a complex interplay of socio-linguistic, discursive and pragmatic factors. Sometimes speakers have a preference for a particular language for certain conversational and discourse settings; on other occasions, there is fluid alteration between two or more languages in a single conversation, also known as Code-switching (CS) or Code-mixing1. Under-\n∗* This work was done when the author was a Research Fellow at Microsoft Research Lab India.\n1Although some linguists differentiate between Codeswitching and Code-mixing, this paper will use the two terms interchangeably.\nstanding and characterizing language preference in multilingual societies has been the subject matter of linguistic inquiry for over half a century (see Milroy and Muysken (1995) for an overview).\nConversational phenomena such as CS were observed only in speech and therefore, all previous studies are based on data collected from a small set of speakers or from interviews. With the growing popularity of social media, we now have an abundance of conversation-like data that exhibit CS and other speech phenomena, hitherto unseen in text (Bali et al., 2014). Leveraging such data from Twitter, we conduct a large-scale study on language preference, if any, for the expression of opinion and sentiment by Hindi-English (Hi-En) bilinguals.\nWe first build a corpus of 430,000 unique Indiaspecific tweets across four domains (sports, entertainment, politics and current events) and automatically classify the tweets by their language: English, Hindi and Hi-En CS. We then develop an opinion detector for each language class to further categorize them into opinionated and non-opinionated tweets. Sentiment detectors further classify the opinionated tweets as positive, negative or neutral. Our study shows that there is a strong preference towards Hindi (i.e. the native language or L1) over English (L2) for expression of negative opinion. The effect is clearly visible in CS tweets, where a switch from English to Hindi is often correlated with a switch from a positive to negative sentiment. This is referred to as the polarity–switch function of CS (Sanchez, 1983). Using the same experimental technique, we also explore other pragmatic functions of CS, such as reinforcement and narrative–evaluative.\n1131\nApart from being the first large-scale quantitative study of language preference in multilingual societies, this work also has several other contributions: (a) We develop one of the first opinion and sentiment classifiers for Romanized Hindi and CS Hi-En tweets with higher accuracy than the only known previous attempt (Sharma et al., 2015b). (b) We present a novel methodology for automatically detecting pragmatic functions of codeswitching through opinion and sentiment detection.\nThe rest of the paper is organized as follows: Sec. 2 introduces language preference, functions of CS and Hindi-English bilingualism on the web. Sec. 3 formulates the problem and presents the fundamental questions that this paper seeks to answer. Sec. 4 and 5 discuss dataset creation and opinion and sentiment detection techniques respectively. Sec. 6 evaluates the hypotheses in light of the observations on the tweet corpus. We conclude in Sec. 7, and raise some interesting sociolinguistic questions for future studies."
  }, {
    "heading": "2 Background and Related Work",
    "text": "In order to situate the questions addressed in our work in existing literature, we present a brief overview of the past research in pragmatic and discursive analysis of code-switching, and specifically, on language preference for emotional expression. A primer to Hi-En bilingualism and its presence in social media shall follow."
  }, {
    "heading": "2.1 CS Functions and Language Preference",
    "text": "In multilingual communities, where there are more than one linguistic channels for information exchange, the choice of the channel depends on a variety of factors, and is usually unpredictable (Auer, 1995). Nevertheless, linguistic studies point out certain frequently-observed patterns. For instance, certain speech activities might be exclusively or more commonly related to a certain language choice (e.g. Fishman (1971) reports use of English for professional purposes and Spanish for informal chat for English-Spanish bilinguals from Puerto Rico). Apart from association between such conversational contexts and language preference, language alteration is often found to be used as a signaling device to imply certain pragmatic functions (Barredo, 1997; Sanchez, 1983; Nishimura, 1995; Maschler,\n1991; Maschler, 1994) such as: (a) reported speech (b) narrative to evaluative switch (c) reiterations or emphasis (d) topic shift (e) puns and language play (f) topic/comment structuring etc. Attempts of predicting the preferred language, or even exhaustively listing such functions, have failed. However, linguists agree that language alteration in multilingual communities is not a random process.\nOf specific interest to us are the studies on language preference for expression of emotions. Through large-scale interviews and two decades of research, Dewaele (2004; 2010) argued that for most multilinguals, L1 (the dominant language, which is often, but not always, the native or mother tongue) is the language preference for emotions, which include emotional inner speech, swearing and even emotional conversations. Dewaele argues that emotionally charged words in L1 elicit stronger emotions than those in other languages, and hence L1 is preferred for emotion expression."
  }, {
    "heading": "2.2 Hindi-English Bilingualism",
    "text": "Around 125 million people in India speak English, half of whom have Hindi as their mother-tongue. The large proportion of the remaining half, especially those residing in the metropolitan cities, also know at least some Hindi. This makes Hi-En codeswitching, commonly called Hinglish, extremely widespread in India. There is historical attestation, as well as recent studies on the growing use of Hinglish in general conversation, and in entertainment and media (see Parshad et al. (2016) and references therein). Several recent studies (Bali et al., 2014; Barman et al., 2014; Solorio et al., 2014; Sequiera et al., 2015) also provide evidence of Hinglish and other instances of CS on online social media such as Twitter and Facebook. In a Facebook dataset analyzed by Bali et al. (2014), almost all sufficiently long conversation threads were found to be multilingual, and as much as 17% of the comments had CS. This study also indicates that on online social media, Hindi is seldom written in the Devanagari script. Instead, loose Roman transliteration, or Romanized Hindi, is common, especially when users code-switch between Hindi and English.\nWhile there has been some effort towards computational processing of CS text (Solorio and Liu, 2008; Solorio and Liu, 2010; Vyas et al., 2014; Peng\net al., 2014), to the best of our knowledge, there has been no study on automatic identification of functional aspects of CS or any large-scale, data-driven study of language preference. The current study adds to the growing repertoire of work on quantitative analysis of social media data for understanding socio-linguistic and pragmatic issues, such as detection of depression (De Choudhury et al., 2013), politeness (Danescu-Niculescu-Mizil et al., 2013), speech acts (Vosoughi and Roy, 2016), and social status (Tchokni et al., 2014)."
  }, {
    "heading": "3 Problem Formulation",
    "text": "Along the lines of (Dewaele, 2010), we ask the following question: Is there a preferred language for expression of opinion and sentiment by the Hi-En bilinguals on Twitter?"
  }, {
    "heading": "3.1 Definitions",
    "text": "More formally, let Λ = {h, e,m} be the set of languages: Hindi (h), English (e) and Mixed (m), i.e., code-switched. Let Σ = {d, r}, be the set of scripts:2 Devanagari (d) and Roman (r). Let us further introduce a set of sentiments, 3 = {+,−, 0,⊗}, where +, − and 0 respectively denote utterances with positive, negative and neutral opinions. ⊗ denote non-opinionated (like factual) texts.\nLet T = {t1, t2, . . . t|T |} be a set of tweets (or any text) generated by Hi-En bilinguals. We define:\n• λ(T ), σ(T ) and (T ) as the subsets of T that respectively contain all tweets in language λ, script σ and sentiment .\n• λσ (T ) = λ(T )∩ σ(T )∩ (T ). Likewise, we also define λ (T ) = λ(T ) ∩ (T ), λσ(T ) = λ(T ) ∩ σ(T ) and σ (T ) = σ(T ) ∩ (T ).\nThe preference towards a language-script pair λσ for expressing a type of sentiment is given by the probability\npr(λσ| ;T ) = pr( |λσ;T )pr(λσ|T ) pr( |T ) (1)\nHowever, pr(λσ), which defines the prior probability of choosing λσ for a tweet is dependent on a large\n2Tweets in mixed script are rare and hence we do not include a symbol for it, though the framework does not preclude such possibilities.\nnumber of socio-linguistic parameters beyond sentiment. For instance, on social media, English is overwhelmingly more common than any Indic language (Bali et al., 2014). This is because (a) English tweets come from a large number of users apart from Hi-En bilinguals and (b) English is the preferred language for tweeting even for Hi-En bilinguals because it expands the target audience of the tweet by manifolds. The preference of λσ for expressing , therefore, can be quantified as:\npr( |λσ;T ) = |λσ (T )||λσ(T )| (2)\nWe say λσ is the preferred language-script choice over λ′σ′ for expressing sentiment if and only if\npr( |λσ;T ) > pr( |λ′σ′;T ) (3)\nThe strength of the preference is directly proportionate the ratio of the probabilities: pr( |λσ;T )/pr( |λ′σ′;T ). An alternative but related way of characterizing the preference is through comparing the odds of choosing a sentiment type to its polar opposite - ′. We say, λσ is the preferred language-script pair for expressing , if\npr( |λσ;T ) pr( ′|λσ;T ) > pr( |λ′σ′;T ) pr( ′|λ′σ′;T ) (4)"
  }, {
    "heading": "3.2 Hypotheses",
    "text": "Now we can formally define the two hypotheses, we intend to test here. Hypothesis I: For Hi-En bilinguals, Hindi is the preferred language for expression of opinion on Twitter. Therefore, we expect\npr({+,−, 0}|hd;T ) > pr({+,−, 0}|er;T ) (5)\ni.e., pr(⊗|hd;T ) < pr(⊗|er;T ) (6) And similarly,\npr(⊗|hr;T ) < pr(⊗|er;T ) (7)\nHypothesis II: For Hi-En bilinguals, Hindi is the preferred language for expression of negative sentiment. Therefore,\npr(−|hd;T ) ≈ pr(−|hr;T ) > pr(−|er;T ) (8)\nIn particular, we would like to hypothesize that the odds of choosing Hindi for negative over positive is really high compared to the odds for English. I.e.,\npr(−|hd;T ) pr(+|hd;T ) ≈ pr(−|hr;T ) pr(+|hr;T ) > pr(−|er;T ) pr(+|er;T ) (9)\nA special case of the above hypotheses arise in the context of code-mixing, i.e., for the set mr(T ). Since the mixed tweets certainly come from proficient bilinguals and have both Hi and En fragments, we can reformulate our hypotheses at a tweet level. Let mhr(T ) and mer(T ) respectively denote the set of Hi and En fragments in mr(T ). Hypothesis Ia: Hindi is the preferred language for expression of opinion in Hi-En code-mixed tweets. Therefore, we expect\ni.e., pr(⊗|mhr;T ) < pr(⊗|mer;T ) (10)\nHypothesis IIa: Hindi is the preferred language for expression of negative sentiment in Hi-En codeswitched tweets. Therefore,\npr(−|mhr;T ) > pr(−|mer;T ) (11)\npr(−|mhr;T ) pr(+|mhr;T ) > pr(−|mer;T ) pr(+|mer;T ) (12)\nLikewise, the above hypotheses also apply for the Devanagari script, though for technical reasons, we do not test them here.\nBesides comparing aggregate statistics onmr(T ), it is also interesting to look at the sentiment of mhr(ti) andmer(ti) for each tweet ti. In particular, for every pair of 6= ′, we want to study the fraction of tweets in mr(T ) where mhr(ti) has sentiment and mer(ti) has ′. Let this fraction be pr(h ↔ e ′;mr(T )). Under “no-preference for language” (i.e., the null) hypothesis, we would expect pr(h ↔ e ′;mr(T )) ≈ pr(h ′ ↔ e ;mr(T )). However, if pr(h ↔ ′;mr(T )) is significantly higher than pr(h ′ ↔ e ;mr(T )), it means that speakers prefer to switch from English to Hindi when they want to express a sentiment and vice versa. Pragmatic Functions of Code-Switching: When native speakers tend to switch from Hindi to English when they switch from an expression with sentiment to one with ′, or in other words ↔ ′, we\nsay this is an observed pragmatic function of codeswitching between Hindi and English (note that the order of the languages is important), if and only if\npr(h ↔ e ′;mr(T )) pr(h ′ ↔ e ;mr(T )) > 1 (13)"
  }, {
    "heading": "3.3 A Note on Statistical Significance",
    "text": "All the statistics defined here are likelihoods; Equations 9, 12 and 13, in particular, state our hypothesis in the form of the Likelihood Ratio Test. However, the true classes λ and are unknown; we predict the class labels using automatic language and sentiment detection techniques that have non-negligible errors. Under such a situation, the likelihoods cannot be considered as true test statistics, and consequently, hypothesis testing cannot be done per se. Nevertheless, we can use these as descriptive statistics and investigate the status of the aforementioned hypotheses."
  }, {
    "heading": "4 Datasets",
    "text": "We collected tweets with certain India-specific hashtags (Table 1) using the Twitter Search API (Twi, 2015b) over three months (December 2014 – February 2015). In this paper, we use tweets in Devanagari script Hindi (hd), and Roman script English (er), Hindi (hr) and Hi-En Mixed (mr). English and mixed tweets written in Devanagari are extremely rare (Bali et al., 2014) and we do not study them here. We filter out tweets labeled by the Twitter API (Twi, 2015a) as German, Spanish, French, Portuguese, Turkish, and all non-Roman script languages (except Hindi).\nWe experiment on the following different corpora: TAll: All tweets after filtering. This corpus contains 430,000 unique tweets posted by 1,25,396 unique users.\nTBL: Tweets from users who are certainly Hi-En bilinguals, which are approximately 55% (240,000) of the tweets in TAll. We define a user to be a Hi-En bilingual if there is at least one mr tweet from the user, or if the user has tweeted at least once in Hindi (hd or hr) and once in English (er). Tspo,Tmov,Tpol,Teve: Topic-wise corpora for sports, movies, politics and events (Table 1). TCS: Tweets with inter-sentential CS. We define these as tweets containing at least one sequence of 5 contiguous Hindi words and one sequence of 5 contiguous English words. The corpus has 3,357 tweets.\nSAC: 1000 monolingual tweets (er, hr, hd) and 260 mixed (mr) tweets manually annotated with sentiment and opinion labels. These were annotated by two linguists, both fluent Hi-En speakers. The annotators first checked whether the tweet is opinionated or⊗ and then identified polarity of the opinionated tweets (+, − or 0). Thus, the tweets are classified into the four classes in the set 3. If a tweet contains both opinion and ⊗, each fragment was individually annotated. The inter-annotator agreement is 77.5% (κ = 0.59) for opinion annotation and 68.4% (κ = 0.64) over all four classes. A third linguist independently corrected the disagreements. LLCTest: 141 er, 137 hr, and 241 mr tweets annotated by a Hi-En bilingual form the test set for the Language Labeling system (Sec. 5.1). SAC and LLCTest can be downloaded and used for research purposes3. Note that apart from SAC and LLCTest, all corpora are subsets of TAll. For generalizability of our observations, it is important to ensure that the tweets in TAll come from a large number of users and the datasets do not over-represent a small set of users. In Figure 1, we plot the minimum fraction of users required (x-axis) to cover a certain percentage of the tweets in TAll (y-axis). Tweets from at least 10%, i.e., 12.5K users are needed to cover 50% of the corpus. As expected, we do observe a powerlaw-like distribution, where a few users contribute a large number of tweets, and a large number of users contribute a few tweets each. We believe that 12.5K users is sufficient to ensure an unbiased study.\nFurther, we classify the users into three specific groups (i) news channels, (ii) general users (having\n3http://www.cnergres.iitkgp.ac.in/codemixing\n≤ 10,000 followers), (iii) popular users or celebrities (having > 10,000 followers). Interestingly, for both TAll, and TBL corpora, we observe that around 98% of all users are general, and 96% of all tweets come from such users. Hence, most observations from these corpora are expected to be representative of the average online linguistic behavior of a Hi-En bilingual."
  }, {
    "heading": "5 Method",
    "text": "Fig. 2 diagrammatically summarizes our experimental method. We identify the language used in each tweet before detecting opinion and sentiment."
  }, {
    "heading": "5.1 Language Labeling",
    "text": "Tweets in Devanagari script are accurately detected by the Twitter API as Hindi tweets – we label these as hd, though a small fraction of them could also be md. To classify Roman script tweets as er, hr or mr, we use the system that performed best in the FIRE 2013 shared task for word-level language detection of Hi-En text (Gella et al., 2013). This system uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi). We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space."
  }, {
    "heading": "5.2 Opinion and Sentiment Detection",
    "text": "Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al.,\n2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences. Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b). Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets.\nWe propose a two-step classification model. We first identify whether a tweet is opinionated or nonopinionated (⊗). If the tweet is opinionated, we further classify it according to its sentiment (+,− or 0). Fig. 2 shows the architecture of the proposed model. Two-step classification was empirically found to be better than a single four-class classifier.\nWe develop individual classifiers for each language class (er, hr, hd, mr) using an SVM with RBF kernel from Scikit-learn (Pedregosa et al.,\n2011). We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3."
  }, {
    "heading": "5.3 Classifier Features",
    "text": "For opinion classification (opinion or ⊗), we propose a set of event-independent lexical features and Twitter-specific features. (i) Subjective words: Expected to be present in opinion tweets. We use lexicons from Volkova et al. (2013) for er and Bakliwal et al. (2012) for hd. We Romanize the hd lexicon for the hr classifiers (ii) Elongated words: Words with one character repeated more than two times, e.g. sooo, naaahhhhi (iii) Exclamations: Presence of contiguous exclamation marks (iv) Emoticons4 (v) Question marks: Queries are generally nonopinionated. (vi) Wh-words: These are used to form questions (vii) Modal verbs: e.g. should, could, would, cud, shud (viii) Excess hashtags: Presence of more than two hashtags (ix) Intensifiers: Generally used to emphasize sentiment, e.g., we shouldn’t get too comfortable (x) Swear words5: Prevalent in opinionated tweets, e.g. that was a f ing no ball!!!! #indvssa (xi) Hashtags: Hashtags might convey user sentiment (Barbosa et al., 2012). We manually identify hashtags in our corpus that represent explicit opinion. (xii) Domain lexicon: For hr, & hd category tweets, we construct sentiment lexicons from 1000 manually annotated tweets. Each word or phrase in this lexicon represents +, or −, or 0 sentiment. (xiii) Twitter user mentions (xiv) Pronouns: Opinion is often in first person using pronouns like I and we.\nFor sentiment classification, we use emoticons, swear words, exclamation marks and elongated words as described above. We also use subjective words from various lexicons (Mohammad and Turney, 2013; Volkova et al., 2013; Bakliwal et al., 2012; Sharma et al., 2015a). Additionally, we use – (i) Sentiment words: From Hashtag Sentiment and Sentiment140 lexicons (Mohammad et al., 2013). We also manually annotate hashtags from our dataset that represent sentiment. (ii) Negation: A negated context is tweet segment that begins with a negation word and ends with a punctuation mark (Pang et al., 2002). The list of negation words are\n4The list of emoticons was extracted from Wikipedia 5Swear word lexicons from noswearing.com, youswear.com\ntaken from Christopher Potts’ sentiment tutorial6. Themr opinion classifier uses the output from the er and hr classifiers as features (Fig. 2), along with an additional feature that represents whether the majority of the words in the tweet are Hindi or not. A similar strategy is used for mr sentiment detection."
  }, {
    "heading": "5.4 Evaluation",
    "text": "We evaluated the language labeling system on the LLCTest corpus, on which the precision (recall) values were 0.93(0.91), 0.90(0.85) and 0.88(0.92) for er, hr and mr classes respectively. The tweetlevel classification accuracy was 89.8%.\nThe opinion and sentiment classifiers were evaluated using 10-fold cross validation on the SAC dataset. Table 2 details the class-wise accuracy. For comparison, we also reimplemented the dictionary and dependency-based method by Qadir (2009). The accuracy of the opinion classifier on the er tweets was found to be 65.7%, 7% lower than our system. We also compared our mr sentiment classifier with that of Sharma et al. (2015b). As their method performs two class sentiment detection (+ and −), we select such tweets from SAC. Their system achieves an accuracy of 68.2%, which is 4% lower than the accuracy of our system.\nAn analysis of the errors showed more false negatives (i.e., opinions labeled⊗) than false positives in opinion classification. Sentiment misclassification is uniformly distributed.\nTable 3 reports the accuracy of the opinion classifier for feature ablation experiments. For all three language-script pairs, lexicon and non-word (emoticons, elongated words, hashtags, exclamation) features are the most effective, though all features have some positive contribution towards the final accuracy of opinion detection. For hr and hd tweets, domain knowledge is significant, as shown by the 4% accuracy drop with removing the domain lexicon.\n6http://sentiment.christopherpotts.net/lingstruc.html"
  }, {
    "heading": "6 Experiments and Observations",
    "text": "In this section, we report our experiments on 430,000 unique tweets (TAll), and its various subsets as defined in Sec 4. First, we run the language detection system on the corpora. Table 4 shows the language-wise distribution. We see that language preference varies by topic, which is not surprising. Due to paucity of space, the correlation between language usage and topic will not be discussed at length here, but we will highlight cases where the differences are striking.\nWe apply the language-specific opinion and sentiment classifiers to tweets detected as the corresponding language class. In the following subsections, we empirically investigate the hypotheses."
  }, {
    "heading": "6.1 Status of Hypotheses I and II",
    "text": "Table 5 shows pr(⊗|λσ;T ), pr(−|λσ;T ) and pr(−|λσ;T )/pr(+|λσ;T ) for TAll, TBL and two randomly selected topics – Movie and Politics. The statistics are fairly consistent over the corpora, with slight differences but similar trends in Tmov.\nWe need the first statistic in order to investigate Hypothesis I (Eqs. 6 and 7), and the two latter ones for verifying Hypothesis II (Eqs. 8 and 9).\nContrary to Eqs. 6 and 7, for all corpora except Tmov, we observe the following trend:\npr(⊗|hd;T ) > pr(⊗|hr;T ) ≥ pr(⊗|er;T )\nIn other words, hd is more commonly preferred for expressing non-opinions than hr and er. Hypothesis I is clearly untrue for these corpora, though due to the small differences between hr and er, we cannot claim that English is the preferred language for expressing opinions. A closer scrutiny of the corpora revealed that hd tweets mostly come from official sources (news channels, political parties, production houses) and celebrities, which are mostly factual. hr tweets are from general users and show similar trends as English. Thus, in general, there seems to be no preferred language for expressing opinion by the Hi-En bilinguals on Twitter.\nIn the context of Hypothesis II, we see the general pattern (with some topic specific variations):\npr(−|hr;T ) > pr(−|hd;T ) ≥ pr(−|er;T )\nThe pattern emerges even more strongly, when we look at pr(−|λσ;T )/pr(+|λσ;T ). The odds of expressing a negative opinion over positive opinion in Hindi is between 1.5 and 6 (Tmov exhibits a slightly different pattern but similar preference, Tpol shows a stronger preference towards Hindi for negative sentiment), whereas the same for English is between 0.1 and 0.6. In other words, English is more preferred\nfor expressing positive opinion, and Hindi for negative opinion. These observations provide very strong evidence in favor of Hypothesis II."
  }, {
    "heading": "6.2 Status of Hypotheses Ia and IIa",
    "text": "Recall that Hypothesis Ia and Hypothesis IIa are essentially same as Hypotheses I and II, but applied on mhr and mer fragments from the TCS corpus.\nTable 6 reports the three statistics necessary for testing these hypotheses. pr(⊗|mer;TCS) is slightly greater than pr(⊗|mhr;TCS), which is what we would expect if Hypothesis Ia was true. However, since the difference is small, we view it as a trend rather than a proof of Hypothesis Ia.\nThe statistics clearly show that Hypothesis IIa holds true for TCS . The fraction of negative sentiment in mhr is over 1.5 times higher than that of mer. Further, the odds of expressing a negative sentiment in Hindi over positive sentiment in Hindi in a code-switched tweet is 6.5 times higher than the same odds for English."
  }, {
    "heading": "6.3 Switching Functions",
    "text": "Recall that using Eq. 13 (Sec. 3), we can estimate the preference, if any, for switching to a particular language while changing the sentiment. In particular, research in socio-linguistics has shown that users often switch between languages when they switch from non-opinion (⊗) to opinion ({+,−, 0}). This is called the Narrative-Evaluative function of CS (Sanchez, 1983). This function appears in 46.1% of the tweets in TCS . We find that\npr(h{+,−, 0} ↔ e⊗;TCS) pr(h⊗ ↔ e{+,−, 0};TCS) = 0.86\nwhich indicates that there is no preference for switching to Hindi (or English) while switching between opinion and non-opinion. This is also confirmed above in the context of hypotheses I and Ia. While switching between opinion and non-opinion in a tweet, users do switch language. However, we\nobserve no particular preference for the languages chosen for each part.\nWe also report two other pragmatic functions:\npr(h− ↔ e{+, 0,⊗};TCS) pr(h{+, 0,⊗} ↔ e−;TCS) = 1.98\npr(h− ↔ e+;TCS) pr(h+↔ e−;TCS) = 10.27\nThe latter function is called polarity switch. The extremely high value for these ratios is an evidence for a strong preference towards switching language from English to Hindi while switching to negative sentiment (and switching to English when sentiment changes from negative to positive).\nWe also observe cases where there is a language switch, but no sentiment switch and hence, we cannot evaluate language preference using Eq. 13 (because = ′). In TCS , 15.3% of the tweets show Positive Reinforcement, where both fragments are of positive sentiment. Negative Reinforcement is defined similarly and is seen in 8.7% of the tweets. Other tweets in TCS likely have pragmatic functions that cannot be identified based on sentiment."
  }, {
    "heading": "6.4 Language Preference for Swearing",
    "text": "Since there is evidence that the native language (Hindi, in this case) is preferred for swearing (De-\nwaele, 2004), we computed the fraction of tweets that contain swear words in each language class. Fig. 3a shows the distribution across topics. The languages hr and mr have a much higher fraction of abusive tweets than er and hd. Fig. 3b shows the distribution of abusive mhr and mer fragments for tweets in TCS . Interestingly, over 90% of the swear words occur in mhr. Both distributions strongly suggest a preference for swearing in Hindi."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, through a large scale empirical study of nearly half a million tweets, we tried to answer a fundamental question regarding multilingualism, namely, is there a preferred language for expression of sentiment. We also looked at some of the pragmatic functions of code-switching. Our results indicate a strong preference for using Hindi, L1 for the users from whom these tweets come, for expressing negative sentiment, including swearing. However, we do not observe any particular preference towards Hindi for expressing opinions.\nPrevious linguistic studies (Dewaele, 2004; Dewaele, 2010) have already shown a preference for L1 for expressing emotion and swearing. However, we observe that for expressing positive emotion, English (which would be L2) is the language of preference. This raises some intriguing socio-linguistic questions. Is it the case that English being the language of aspiration in India, it is preferred for positive expression? Or is it because Hindi is specifically preferred for swearing and therefore, is the language of preference for negative emotion? How do such preferences vary across topics, users and other multilingual communities? How representative of the society is this kind of social media study? We plan to explore some of these questions in the future.\nOur study also indicates that inferences drawn on multilingual societies by analyzing data in just one language (usually English), which has been the norm so far, are likely to be incorrect."
  }, {
    "heading": "Acknowledgement",
    "text": "Koustav Rudra was supported by a fellowship from Tata Consultancy Services."
  }],
  "year": 2016,
  "references": [{
    "title": "The pragmatics of code-switching: a sequential approach",
    "authors": ["Peter Auer."],
    "venue": "Lesley Milroy and Pieter Muysken, editors, One speaker, two languages, pages 115–135. Cambridge University Press.",
    "year": 1995
  }, {
    "title": "Hindi subjective lexicon : A lexical resource for hindi polarity classification",
    "authors": ["Akshat Bakliwal", "Piyush Arora", "Vasudeva Varma."],
    "venue": "Proc. LREC, Austin, Texas, USA, May.",
    "year": 2012
  }, {
    "title": "i am borrowing ya mixing?” an analysis of English-Hindi code mixing in Facebook",
    "authors": ["Kalika Bali", "Yogarshi Vyas", "Jatin Sharma", "Monojit Choudhury."],
    "venue": "Proc. First Workshop on Computational Approaches to Code Switching, EMNLP.",
    "year": 2014
  }, {
    "title": "Characterizing the effectiveness of twitter hashtags to detect and track online population sentiment",
    "authors": ["Glivia A.R. Barbosa", "Wagner Meira Jr", "Ismael S. Silva", "Raquel O. Prates", "Mohammed J. Zaki", "Adriano Veloso."],
    "venue": "Proc. ACM CHI, Austin, Texas, USA,",
    "year": 2012
  }, {
    "title": "Code mixing: A challenge for language identification in the language of social media",
    "authors": ["Utsab Barman", "Amitava Das", "Joachim Wagner", "Jennifer Foster."],
    "venue": "The 1st Workshop on Computational Approaches to Code Switching, EMNLP 2014.",
    "year": 2014
  }, {
    "title": "Pragmatic functions of code-switching among Basque-Spanish bilinguals",
    "authors": ["Inma Muñoa Barredo."],
    "venue": "Retrieved on October, 26:528–541.",
    "year": 1997
  }, {
    "title": "Learning opinionated patterns for contextual opinion detection",
    "authors": ["Caroline Brun."],
    "venue": "COLING (Posters), pages 165–174. Citeseer.",
    "year": 2012
  }, {
    "title": "A computational approach to politeness with application to social factors",
    "authors": ["Cristian Danescu-Niculescu-Mizil", "Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts."],
    "venue": "Proceedings of ACL.",
    "year": 2013
  }, {
    "title": "Predicting depression via social media",
    "authors": ["Munmun De Choudhury", "Michael Gamon", "Scott Counts", "Eric Horvitz."],
    "venue": "ICWSM.",
    "year": 2013
  }, {
    "title": "Blistering barnacles! What language do multilinguals swear in?",
    "authors": ["Jean-Marc Dewaele"],
    "venue": "Estudios de Sociolinguistica,",
    "year": 2004
  }, {
    "title": "Emotions in multiple languages",
    "authors": ["Jean-Marc Dewaele."],
    "venue": "Palgrave Macmillan, Basingstoke, UK.",
    "year": 2010
  }, {
    "title": "Sociolinguistics",
    "authors": ["J.A. Fishman."],
    "venue": "Rowley, Newbury, MA.",
    "year": 1971
  }, {
    "title": "Query word labeling and back transliteration for indian languages: Shared task system description",
    "authors": ["Spandana Gella", "Jatin Sharma", "Kalika Bali"],
    "year": 2013
  }, {
    "title": "The language games bilinguals play: language alternation at language boundaries",
    "authors": ["Yael Maschler."],
    "venue": "Language and communication, 11(2):263–289.",
    "year": 1991
  }, {
    "title": "Appreciation ha’araxa ’o ha’arasta? [valuing or admiration",
    "authors": ["Yael Maschler."],
    "venue": "Negotiating contrast in bilingual disagreement talk, 14(2):207–238.",
    "year": 1994
  }, {
    "title": "Sentiment analysis of hindi review based on negation and discourse relation",
    "authors": ["Namita Mittal", "Basant Agarwal", "Garvit Chouhan", "Nitin Bania", "Prateek Pareek."],
    "venue": "proceedings of International Joint Conference on Natural Language Processing, pages 45–50.",
    "year": 2013
  }, {
    "title": "Crowdsourcing a Word-Emotion Association Lexicon",
    "authors": ["Saif M. Mohammad", "Peter D. Turney."],
    "venue": "29(3):436–465.",
    "year": 2013
  }, {
    "title": "Nrc-canada: Building the state-of-theart in sentiment analysis of tweets",
    "authors": ["Saif Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."],
    "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia,",
    "year": 2013
  }, {
    "title": " emotional tweets",
    "authors": ["Saif M Mohammad."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on",
    "year": 2012
  }, {
    "title": "A functional analysis of Japanese/English code-switching",
    "authors": ["Miwa Nishimura."],
    "venue": "Journal of Pragmatics, 23(2):157–181.",
    "year": 1995
  }, {
    "title": "Thumbs up?: Sentiment classification using machine learning techniques",
    "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."],
    "venue": "Proc. EMNLP, pages 79–86.",
    "year": 2002
  }, {
    "title": "What is India speaking? Exploring the “Hinglish” invasion",
    "authors": ["Rana D. Parshad", "Suman Bhowmick", "Vineeta Chand", "Nitu Kumari", "Neha Sinha."],
    "venue": "Physica A, 449:375–389.",
    "year": 2016
  }, {
    "title": "Scikit-learn: Machine learning in Python",
    "authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"],
    "year": 2011
  }, {
    "title": "Learning polylingual topic models from codeswitched social media documents",
    "authors": ["Nanyun Peng", "Yiming Wang", "Mark Dredze."],
    "venue": "ACL (2), pages 674–679.",
    "year": 2014
  }, {
    "title": "Detecting opinion sentences specific to product features in customer reviews using typed dependency relations",
    "authors": ["Ashequl Qadir."],
    "venue": "Proceedings of the Workshop on Events in Emerging Text Types, pages 38–43. Association for Computational Linguistics.",
    "year": 2009
  }, {
    "title": "A novel two-stage framework for extracting opinionated sentences from news articles",
    "authors": ["Pujari Rajkumar", "Swara Desai", "Niloy Ganguly", "Pawan Goyal."],
    "venue": "TextGraphs-9, page 25.",
    "year": 2014
  }, {
    "title": "Semeval-2015 task 10: Sentiment analysis in twitter",
    "authors": ["Sara Rosenthal", "Preslav Nakov", "Svetlana Kiritchenko", "Saif M Mohammad", "Alan Ritter", "Veselin Stoyanov."],
    "venue": "Proceedings of SemEval-2015.",
    "year": 2015
  }, {
    "title": "Chicano discourse",
    "authors": ["Rosaura Sanchez."],
    "venue": "Rowley, Newbury House.",
    "year": 1983
  }, {
    "title": "Ultimate Goal, and Hindi Senti Lexicon Statistics. 2015a. A sentiment analyzer for hindi using hindi senti lexicon",
    "authors": ["Raksha Sharma", "Pushpak Bhattacharyya"],
    "year": 2015
  }, {
    "title": "Text normalization of code mix and sentiment analysis",
    "authors": ["Shashank Sharma", "Pykl Srinivas", "Rakesh Chandra Balabantaray."],
    "venue": "Advances in Computing, Communications and Informatics (ICACCI), 2015 International Conference on, pages 1468–1473. IEEE.",
    "year": 2015
  }, {
    "title": "Part-of-speech tagging for english-spanish code-switched text",
    "authors": ["Thamar Solorio", "Yang Liu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051–1060. Association for Computational Linguistics.",
    "year": 2008
  }, {
    "title": "Learning to Predict Code-Switching Points",
    "authors": ["Thamar Solorio", "Yang Liu."],
    "venue": "Proc. EMNLP.",
    "year": 2010
  }, {
    "title": "Overview for the first shared task on language identification in code-switched data",
    "authors": ["Thamar Solorio", "Elizabeth Blair", "Suraj Maharjan", "Steven Bethard", "Mona Diab", "Mahmoud Gohneim", "Abdelati Hawwari", "Fahad AlGhamdi", "Julia Hirschberg", "Alison Chang"],
    "year": 2014
  }, {
    "title": "Emoticons and phrases: Status symbols in social media",
    "authors": ["Simo Tchokni", "D.O. Séaghdha", "Daniele Quercia."],
    "venue": "Eighth International AAAI Conference on Weblogs and Social Media.",
    "year": 2014
  }, {
    "title": "Sentiment analysis on monolingual, multilingual and code-switching twitter corpora",
    "authors": ["David Vilares", "Miguel A Alonso", "Carlos GómezRodrıguez."],
    "venue": "6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis.",
    "year": 2015
  }, {
    "title": "Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams",
    "authors": ["Svitlana Volkova", "Theresa Wilson", "David Yarowsky."],
    "venue": "Proc. ACL (Vol2: Short Papers).",
    "year": 2013
  }, {
    "title": "Tweet acts: A speech act classifier for twitter",
    "authors": ["Soroush Vosoughi", "Deb Roy."],
    "venue": "Tenth International AAAI Conference on Web and Social Media.",
    "year": 2016
  }, {
    "title": "POS Tagging of English-Hindi Code-Mixed Social Media Content",
    "authors": ["Yogarshi Vyas", "Spandana Gella", "Jatin Sharma", "Kalika Bali", "Monojit Choudhury."],
    "venue": "Proc. EMNLP, pages 974–979. 1141",
    "year": 2014
  }],
  "id": "SP:442e718f907037f935da4b0cd125c6c63e403019",
  "authors": [{
    "name": "Koustav Rudra",
    "affiliations": []
  }, {
    "name": "Shruti Rijhwani",
    "affiliations": []
  }, {
    "name": "Rafiya Begum",
    "affiliations": []
  }, {
    "name": "Niloy Ganguly",
    "affiliations": []
  }],
  "abstractText": "Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010). Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers. In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment. To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments. Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing. As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection.",
  "title": "Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?"
}