{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn-\n∗Corresponding author. 1 Codes and datasets are publicly available at\nhttp://ir.hit.edu.cn/˜dytang.\ning algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015).\nDocument level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings of sentences before considering semantic relations between them. This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation.\nIn this paper, we introduce a neural network approach to learn continuous document representation for sentiment classification. The method is on the basis of the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a docu-\n1422\nment) depends on the meanings of its constituents. Specifically, the approach models document representation in two steps. In the first step, it uses convolutional neural network (CNN) or long short-term memory (LSTM) to produce sentence representations from word representations. Afterwards, gated recurrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations. These representations are naturally used as features to classify the sentiment label of each document. The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2.\nWe conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4. We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural\n2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html\n3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge\nnetwork in document modeling. The main contributions of this work are as follows: • We present a neural network approach to encode relations between sentences in document representation for sentiment classification. •We report empirical results on four large-scale datasets, and show that the approach outperforms state-of-the-art methods for document level sentiment classification. •We report empirical results that traditional recurrent neural network is weak in modeling document composition, while adding neural gates dramatically improves the classification performance."
  }, {
    "heading": "2 The Approach",
    "text": "We introduce the proposed neural model in this section, which computes continuous vector representations for documents of variable length. These representations are further used as features to classify the sentiment label of each document. An overview of the approach is displayed in Figure 1.\nOur approach models document semantics based on the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a document) comes from the meanings of its constituents and the rules used to combine them. Since a document consists of a list of sentences and each sentence is made up of a list of words, the approach models document representation in two stages. It first produces continuous sentence vectors from word represen-\ntations with sentence composition (Section 2.1). Afterwards, sentence vectors are treated as inputs of document composition to get document representation (Section 2.2). Document representations are then used as features for document level sentiment classification (Section 2.3)."
  }, {
    "heading": "2.1 Sentence Composition",
    "text": "We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition.\nEach word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V | is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words.\nWe use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives.\nSpecifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigram-\ns and trigrams in a sentence. Each filter consists of a list of linear layers with shared parameters. Formally, let us denote a sentence consisting of n words as {w1, w2, ...wi, ...wn}, let lc be the width of a convolutional filter, and let Wc, bc be the shared parameters of linear layers in the filter. Each word wi is mapped to its embedding representation ei ∈ Rd. The input of a linear layer is the concatenation of word embeddings in a fixed-length window size lc, which is denoted as Ic = [ei; ei+1; ...; ei+lc−1] ∈ Rd·lc . The output of a linear layer is calculated as\nOc = Wc · Ic + bc (1)\nwhere Wc ∈ Rloc×d·lc , bc ∈ Rloc , loc is the output length of linear layer. To capture global semantics of a sentence, we feed the outputs of linear layers to an average pooling layer, resulting in an output vector with fixed-length. We further add hyperbolic tangent (tanh) to incorporate pointwise nonlinearity, and average the outputs of multiple filters to get sentence representation.\nWe also try lstm as the sentence level semantic calculator, the performance comparison between these two variations is given in Section 3."
  }, {
    "heading": "2.2 Document Composition with Gated Recurrent Neural Network",
    "text": "The obtained sentence vectors are fed to a document composition component to calculate the document representation. We present a gated recurrent neural network approach for document composition in this part.\nGiven the vectors of sentences of variable length as input, document composition produces a fixed-length document vector as output. To this end, a simple strategy is ignoring the order of sen-\ntences and averaging sentence vectors as document vector. Despite its computational efficiency, it fails to capture complex linguistic relations (e.g. “cause” and “contrast”) between sentences. Convolutional neural network (Denil et al., 2014) is an alternative for document composition, which models local sentence relations with shared parameters of linear layers.\nStandard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht−1. The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh.\nht = tanh(Wr · [ht−1; st] + br) (2) where Wr ∈ Rlh×(lh+loc), br ∈ Rlh , lh and loc are dimensions of hidden vector and sentence vector, respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences. This makes it difficult to model long-distance correlations in a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations. The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015). Specifically, the transition function of the gated RNN used in this work is calculated as follows.\nit = sigmoid(Wi · [ht−1; st] + bi) (3) ft = sigmoid(Wf · [ht−1; st] + bf ) (4)\ngt = tanh(Wr · [ht−1; st] + br) (5) ht = tanh(it gt + ft ht−1) (6)\nwhere stands for element-wise multiplication, Wi, Wf , bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation. Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification. We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities. The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015). We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator. The model is embedded in Figure 1."
  }, {
    "heading": "2.3 Sentiment Classification",
    "text": "The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering. Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, which is calculated as follows.\nPi = exp(xi)∑C\ni′=1 exp(xi′) (7)\nWe conduct experiments in a supervised learning setting, where each document in the training data is accompanied with its gold sentiment label.\nCorpus #docs #s/d #w/d |V | #class Class Distribution\nFor model training, we use the cross-entropy error between gold sentiment distribution P g(d) and predicted sentiment distribution P (d) as the loss function.\nloss = − ∑ d∈T C∑ i=1 P gi (d) · log(Pi(d)) (8)\nwhere T is the training data, C is the number of classes, d represents a document. P g(d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0. We take the derivative of loss function through back-propagation with respect to the whole set of parameters θ = [Wc; bc;Wi; bi;Wf ; bf ;Wr; br;Wsoftmax, bsoftmax], and update parameters with stochastic gradient descent. We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50. We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(−0.01, 0.01), and set learning rate as 0.03."
  }, {
    "heading": "3 Experiment",
    "text": "We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification. We describe experimental settings and report empirical results in this section."
  }, {
    "heading": "3.1 Experimental Setting",
    "text": "We conduct experiments on large-scale datasets consisting of document reviews. Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015. Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of\ndocuments. We do not consider the cases that rating does not match with review texts (Zhang et al., 2014).\nStatistical information of these datasets are given in Table 1. We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014) on all these datasets. We use accuracy (Manning and Schütze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance. We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive).\nMSE = ∑N\ni (goldi − predictedi)2 N\n(9)"
  }, {
    "heading": "3.2 Baseline Methods",
    "text": "We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification.\n(1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set.\n(2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5.\n(3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al.\n5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier.\n(4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier.\n(5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation.\n(6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling.\n(7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014).\n(8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided. Window size is tuned on the development set."
  }, {
    "heading": "3.3 Comparison to Other Methods",
    "text": "Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better). The best method in each dataset and each evaluation metric is in bold.\nFrom Table 2, we can see that majority is the worst method because it does not capture any textual semantics. SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers\n6We use Skipgram as it performs slightly better than CBOW in the experiment. We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus.\namong all baseline methods. Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014). Furthermore, the aforementioned bag-of-features are discrete and sparse. For example, the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features. Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged.\nAverageSG is a straight forward way to compose document representation without feature engineering. Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classification. We try to make better use of the sentiment information to learn a better SSWE (Tang et al., 2014), e.g. setting a large window size. However, its performance is still worse than context-based word embedding. This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in document level reviews, while Tang et al. (2014) learn SSWE by assigning sentiment label of a text to each phrase it contains. How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work.\nSince JMARS outputs real-valued outputs, we only evaluate it in terms ofMSE. We can see that sophisticated baseline methods such as JMARS, paragraph vector and convolutional NN obtain significant performance boosts over AverageSG by\ncapturing deeper semantics of texts. Comparing between CNN and AverageSG, we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents. However, it is somewhat disappointing that these models do not significantly outperform discrete bag-of-ngrams and bag-of-features. The reason might lie in that semantic meanings of documents, e.g. relations between sentences, are not well captured. We can see that the proposed method Conv-GRNN and LSTM-GRNN yield the best performance on all four datasets in two evaluation metrics. Compared with CNN, Conv-GRNN shows its superior power in document composition component, which encodes semantics of sentences and their relations in document representation with gated recurrent neural network. We also find that LSTM (almost) consistently performs better than CNN in modeling the sentence representation."
  }, {
    "heading": "3.4 Model Analysis",
    "text": "As discussed before, document composition contributes a lot to the superior performance of ConvGRNN and LSTM-GRNN. Therefore, we take Conv-GRNN as an example and compare different neural models for document composition in this part. Specifically, after obtaining sentence vectors with convolutional neural network as described in Section 2.1, we carry out experiments in following settings.\n(1) Average. Sentence vectors are averaged to get the document vector.\n(2) Recurrent / GatedNN. Sentence vectors are fed to standard (or gated) recurrent neural network in a sequential way from the beginning of the input document. The last hidden vector is regarded as document representation.\n(3) Recurrent Avg / GatedNN Avg. We extend setting (2) by averaging hidden vectors of recurrent neural network as document vector.\n(4) Bi Recurrent Avg / Bi GatedNN Avg. We extend setting (3) by calculating hidden vectors from both preceding histories and following contexts. Bi-directional hidden vectors are averaged as document representation.\nTable 3 shows the experimental results. We can see that standard recurrent neural network (RNN) is the worst method, even worse than the simple vector average. This is because RNN suffers from the vanishing gradient problem, stating that the influence of a given input on the hidden layer decays exponentially over time on the network output. In this paper, it means that document representation encodes rare semantics of the beginning sentences. This is further justified by the great improvement of Recurrent Avg over Recurrent. Bi Recurrent Avg and Recurrent Avg perform comparably, but disappointingly both of them fail to transcend Average. After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN."
  }, {
    "heading": "4 Related Work",
    "text": "Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005)\ncast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014).\nDespite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo-\nsition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a).\nIn this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning. They verify the effectiveness of LSTM in text generation task."
  }, {
    "heading": "5 Conclusion",
    "text": "We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our approaches achieve state-of-the-art performances on all these datasets. We also find that (1) traditional recurrent neural network is extremely weak in modeling document composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation.\nWe briefly discuss some future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document\nrepresentation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms. However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function is an interesting future work."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions. We also would like to thank three anonymous reviewers for their valuable comments and suggestions. This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61133012 and No. 61273321). Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship."
  }],
  "year": 2015,
  "references": [{
    "title": "Don’t count, predict! a systematic comparison of context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "context-predicting semantic vectors. In ACL, pages 238–247.",
    "year": 2014
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."],
    "venue": "Neural Networks, IEEE Transactions on, 5(2):157–166.",
    "year": 1994
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin."],
    "venue": "Journal of Machine Learning Research, 3:1137–1155.",
    "year": 2003
  }, {
    "title": "Deep learning",
    "authors": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville."],
    "venue": "Book in preparation for MIT Press.",
    "year": 2015
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "EMNLP, pages",
    "year": 2014
  }, {
    "title": "Gated feedback recurrent neural networks",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Modelling, visualising and summarising documents",
    "authors": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"],
    "year": 2014
  }, {
    "title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)",
    "authors": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."],
    "venue": "SIGKDD, pages 193–202. ACM.",
    "year": 2014
  }, {
    "title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis",
    "authors": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu."],
    "venue": "AAAI, pages 1537–1543.",
    "year": 2014
  }, {
    "title": "Liblinear: A library for large linear classification",
    "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."],
    "venue": "JMLR.",
    "year": 2008
  }, {
    "title": "On sense and reference",
    "authors": ["Gottlob Frege."],
    "venue": "Ludlow (1997), pages 563–584.",
    "year": 1892
  }, {
    "title": "Beyond the stars: Improving rating predictions using review text content",
    "authors": ["Gayatree Ganu", "Noemie Elhadad", "Amélie Marian."],
    "venue": "WebDB.",
    "year": 2009
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "ICML, pages 513–520.",
    "year": 2011
  }, {
    "title": "Seeing stars when there aren’t many stars: graph-based semisupervised learning for sentiment categorization",
    "authors": ["Andrew B Goldberg", "Xiaojin Zhu."],
    "venue": "GraphBased Method for NLP, pages 45–52.",
    "year": 2006
  }, {
    "title": "Hybrid speech recognition with deep bidirectional lstm",
    "authors": ["Alex Graves", "Navdeep Jaitly", "A-R Mohamed."],
    "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273–278. IEEE.",
    "year": 2013
  }, {
    "title": "The role of syntax in vector space models of compositional semantics",
    "authors": ["Karl Moritz Hermann", "Phil Blunsom."],
    "venue": "ACL, pages 894–904.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Deep recursive neural networks for compositionality in language",
    "authors": ["Ozan Irsoy", "Claire Cardie."],
    "venue": "NIPS, pages 2096–2104.",
    "year": 2014
  }, {
    "title": "Effective use of word order for text categorization with convolutional neural networks",
    "authors": ["Rie Johnson", "Tong Zhang."],
    "venue": "NAACL.",
    "year": 2015
  }, {
    "title": "Speech & language processing",
    "authors": ["Dan Jurafsky", "James H Martin."],
    "venue": "Pearson Education India.",
    "year": 2000
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "ACL, pages 655–665.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "EMNLP, pages 1746– 1751.",
    "year": 2014
  }, {
    "title": "Sentiment analysis of short informal texts",
    "authors": ["Svetlana Kiritchenko", "Xiaodan Zhu", "Saif M Mohammad."],
    "venue": "Journal of Artificial Intelligence Research, pages 723–762.",
    "year": 2014
  }, {
    "title": "Re-embedding words",
    "authors": ["Igor Labutov", "Hod Lipson."],
    "venue": "Annual Meeting of the Association for Computational Linguistics.",
    "year": 2013
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc V. Le", "Tomas Mikolov."],
    "venue": "ICML, pages 1188–1196.",
    "year": 2014
  }, {
    "title": "2015a. When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185",
    "authors": ["Jiwei Li", "Dan Jurafsky", "Eudard Hovy"],
    "year": 2015
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1506.01057.",
    "year": 2015
  }, {
    "title": "Feature weight tuning for recursive neural networks",
    "authors": ["Jiwei Li."],
    "venue": "Arxiv preprint, 1412.3714.",
    "year": 2014
  }, {
    "title": "Sentiment analysis and opinion mining",
    "authors": ["Bing Liu."],
    "venue": "Synthesis Lectures on Human Language Technologies, 5(1):1–167.",
    "year": 2012
  }, {
    "title": "Learning word vectors for sentiment analysis",
    "authors": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."],
    "venue": "ACL, pages 142–150.",
    "year": 2011
  }, {
    "title": "Foundations of statistical natural language processing",
    "authors": ["Christopher D Manning", "Hinrich Schütze."],
    "venue": "MIT press.",
    "year": 1999
  }, {
    "title": "The stanford corenlp natural language processing toolkit",
    "authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."],
    "venue": "ACL, pages 55–60.",
    "year": 2014
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive Science, 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "A study of information retrieval weighting schemes for sentiment analysis",
    "authors": ["Georgios Paltoglou", "Mike Thelwall."],
    "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 1386–1395.",
    "year": 2010
  }, {
    "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "ACL, pages 115– 124.",
    "year": 2005
  }, {
    "title": "Opinion mining and sentiment analysis",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Foundations and trends in information retrieval, 2(1-2):1–135.",
    "year": 2008
  }, {
    "title": "Thumbs up?: sentiment classification using machine learning techniques",
    "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."],
    "venue": "EMNLP, pages 79–",
    "year": 2002
  }, {
    "title": "Global belief recursive neural networks",
    "authors": ["Romain Paulus", "Richard Socher", "Christopher D Manning."],
    "venue": "NIPS, pages 2888–2896.",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "The bag-of-opinions method for review rating prediction from sparse text patterns",
    "authors": ["Lizhen Qu", "Georgiana Ifrim", "Gerhard Weikum."],
    "venue": "COLING, pages 913–921.",
    "year": 2010
  }, {
    "title": "Parsing with compositional vector grammars",
    "authors": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."],
    "venue": "ACL.",
    "year": 2013
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "EMNLP, pages 1631–1642.",
    "year": 2013
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "NIPS, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Learning sentiment-specific word embedding for twitter sentiment classification",
    "authors": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."],
    "venue": "ACL, pages 1555–1565.",
    "year": 2014
  }, {
    "title": "Learning semantic representations of users and products for document level sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "ACL, pages 1014–1023.",
    "year": 2015
  }, {
    "title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews",
    "authors": ["Peter D Turney."],
    "venue": "ACL, pages 417–424.",
    "year": 2002
  }, {
    "title": "Baselines and bigrams: Simple, good sentiment and topic classification",
    "authors": ["Sida Wang", "Christopher D Manning."],
    "venue": "ACL, pages 90–94.",
    "year": 2012
  }, {
    "title": "Exploring the use of word relation features for sentiment classification",
    "authors": ["Rui Xia", "Chengqing Zong."],
    "venue": "COLING, pages 1336–1344.",
    "year": 2010
  }, {
    "title": "Joint opinion relation detection using one-class deep neural network",
    "authors": ["Liheng Xu", "Kang Liu", "Jun Zhao."],
    "venue": "COLING, pages 677–687.",
    "year": 2014
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Compositional matrix-space models for sentiment analysis",
    "authors": ["Ainur Yessenalina", "Claire Cardie."],
    "venue": "EMNLP, pages 172–182.",
    "year": 2011
  }, {
    "title": "Learning to execute",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1410.4615.",
    "year": 2014
  }, {
    "title": "Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification",
    "authors": ["Yongfeng Zhang", "Haochen Zhang", "Min Zhang", "Yiqun Liu", "Shaoping Ma."],
    "venue": "SIGIR, pages 1027–1030. ACM.",
    "year": 2014
  }, {
    "title": "Self-adaptive hierarchical sentence model",
    "authors": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."],
    "venue": "IJCAI.",
    "year": 2015
  }, {
    "title": "Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities",
    "authors": ["Lanjun Zhou", "Binyang Li", "Wei Gao", "Zhongyu Wei", "Kam-Fai Wong."],
    "venue": "EMNLP, pages 162–171, .",
    "year": 2011
  }, {
    "title": "Long short-term memory over tree structures",
    "authors": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."],
    "venue": "ICML.",
    "year": 2015
  }],
  "id": "SP:82ad2ca5fdca7e6721c0fc1d97d029b25a11328b",
  "authors": [{
    "name": "Duyu Tang",
    "affiliations": []
  }, {
    "name": "Bing Qin",
    "affiliations": []
  }, {
    "name": "Ting Liu",
    "affiliations": []
  }],
  "abstractText": "Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1",
  "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification"
}