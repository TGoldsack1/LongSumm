{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2013, pages 1206–1215, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Despite morphological phenomena’s salience in most human languages, many NLP systems treat fully inflected forms as the atomic units of language. By assuming independence of lexical stems’ various surface forms, this avoidance approach exacerbates the problem of data sparseness. If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules. While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive (Habash and Sadat, 2006).\nNeither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface\nforms and their morphological analyses (Karttunen and Beesley, 2005), and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic). Much linguistic knowledge is encoded in such FSTs.\nIn this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation. The sparsity issue discussed above is addressed by hierarchical priors that share statistical strength across different inflections of the same stem by backing off to word formation models that piece together morphemes using FSTs. Furthermore, because of the nonparametric formulation of our models, the regular morphological patterns found in the long tail of word types will rely more heavily on deeper analysis, while frequent and idiosyncratically behaved forms are modeled opaquely.\nOur prior can be used in virtually any generative model of language as a replacement for multinomial distributions over words, bringing morphological awareness to numerous applications. For various morphologically rich languages, we show that:\n• our model can provide rudimentary unsupervised disambiguation for a highly ambiguous analyzer;\n• integrating morphology into n-gram language models allows better generalization to unseen words and can improve the performance of applications that are truly open vocabulary; and\n• bilingual word alignment models also benefit greatly from sharing translation information\n1206\nacross stems.\nWe are particularly interested in low-resource scenarios, where one has to make the most of the small quantity of available data, and overcoming data sparseness is crucial. If analyzers exist in such settings, they tend to be highly ambiguous, and annotated data for learning to disambiguate are also likely to be scarce or non-existent. Therefore, in our experiments with Russian, we compare two analyzers: a rapidly-developed guesser, which models regular inflectional paradigms but contains no lexicon or irregular forms, and a high-quality analyzer."
  }, {
    "heading": "2 Word Models with Morphology",
    "text": "In this section, we describe a generative model of word formation based on Pitman-Yor processes that generates word types using a finite-state morphological generator. At a high level, the process first produces lexicons of stems and inflectional patterns; then it generates a lexicon of inflected forms using the finite-state generator. Finally, the inflected forms are used to generate observed data. Different independence assumptions can be made at each of these levels to encode beliefs about where stems, inflections, and surface forms should share statistical strength."
  }, {
    "heading": "2.1 Pitman-Yor Processes",
    "text": "Our work relies extensively on Pitman-Yor processes, which provide a flexible framework for expressing backoff and interpolation relationships and extending standard models with richer word distributions (Pitman and Yor, 1997). They have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate power laws (Teh, 2006).\nA draw from a Pitman-Yor process (PYP), denoted G ∼ PY(d, θ,G0), is a discrete distribution over a (possibly infinite) set of events, which we denote abstractly E . The process is parameterized by a discount parameter 0 ≤ d < 1, a strength parameter θ > −d, and a base distribution G0 over the event space E .\nIn this work, our focus is on the base distribution G0. We place vague priors on the hyperparameters d ∼ U([0, 1]) and (θ + d) ∼ Gamma(1, 1). Inference in PYPs is discussed below."
  }, {
    "heading": "2.2 Unigram Morphology Model",
    "text": "The most basic expression of our model is a unigram model of text. So far, we only assume that each word can be analyzed into a stem and a sequence of morphemes forming an inflection pattern. LetGs be a distribution over stems,Gp be a distribution over inflectional patterns, and let GENERATE be a deterministic mapping from 〈stem, pattern〉 pairs to inflected word forms.1 An inflected word type is generated with the following process, which we designate MP(Gs, Gd,GENERATE):\nstem ∼ Gs pattern ∼ Gp\nword = GENERATE(stem, pattern)\nFor example, in Russian, we might sample stem = прочий,2 pattern = STEM+Adj+Pl+Dat, and obtain word = прочим.\nThis model could be used directly to generate observed tokens. However, we have said nothing about Gs and Gp, and the assumption that stems and patterns are independent is clearly unsatisfying. We therefore assume that both the stem and the pattern distributions are generated from PY processes, and that MP(Gs, Gp,GENERATE) is itself the base distribution of a PYP.\nGs ∼ PY(ds, θs, G0s) Gp ∼ PY(dp, θp, G0p) Gw ∼ PY(d, θ,MP(Gs, Gp,GENERATE))\nA draw Gw from this PYP is a unigram distribution over tokens.\n2.3 Base Stem Model G0s In general there are an unbounded number of stems possible in any language, so we set G0s to be character trigram model, which we statically estimate, with Kneser-Ney smoothing, from a large corpus of word types in the language being modeled. While using fixed parameters estimated to maximize likelihood is\n1The assumption of determinism is only inappropriate in cases of inflectional spelling variants (e.g., modeled vs. modelled) or pronunciation variants (e.g., reduced forms in certain environments).\n2прочий (pronounced [pr5tCij]) = other\nquestionable from the perspective of Bayesian learning, it is tremendously beneficial for computational reasons. For some applications (e.g., word alignment), the set of possible stems for a corpus S can be precomputed, so we will also experiment with using a uniform stem distribution based on this set.\n2.4 Base Pattern Model G0p Several choices are possible for the base pattern distribution:\nMP0 We can assume a uniformG0p when the number of patterns is small.\nMP1 To be able to generalize to new patterns, we can draw the length of the pattern from a Poisson distribution and generate morphemes one by one from a uniform distribution.\nMP2 A more informative prior is a Markov chain of morphemes, where each morpheme is generated conditional on the preceding morpheme.\nThe choice of the base pattern distribution could depend on the complexity of the inflectional patterns produced by the morphological analyzer, reflecting the type of morphological phenomena present in a given language. For example, the number of possible patterns can practically be considered finite in Russian, but this assumption is not valid for languages with more extensive derivational morphology like Turkish."
  }, {
    "heading": "2.5 Posterior Inference",
    "text": "For most applications, rather than directly generating from a model using the processes outlined above, we seek to infer posterior distributions over latent parameters and structures, given a sample of data.\nAlthough there is no known analytic form of the PYP density, it is possible to marginalize the draws from it and to work directly with observations. This marginalization produces the classical Chinese restaurant process representation (Teh, 2006). When working with the morphology models we are proposing, we also need to marginalize the different latent forms (stems s and patterns p) that may have given rise to a given word w. Thus, we require that the inverse relation of GENERATE is\navailable to compute the marginal base word distribution:\np(w | G0w) = ∑\nGENERATE(s,p)=w\np(s | Gs) p(p | Gp)\nSince our approach encodes morphology using FSTs, which are invertible, this poses no problem.\nTo illustrate, consider the Russian word прочим, which may be analyzed in several ways:\nпрочий +Adj +Sg +Neut +Instr прочий +Adj +Sg +Masc +Instr прочий +Adj +Pl +Dat\nпрочить +Verb +Pl +1P прочее +Pro +Sg +Ins\nBecause the set of possible analyses is in general small, marginalization is fast and complex blocked sampling is not necessary.\nFinally, to infer hyperparameter values (d, θ, . . .), a Metropolis-Hastings update is interleaved with Gibbs sampling steps for the rest of the hidden variables.3\nHaving described a model for generating words, we now show its usage in several contexts."
  }, {
    "heading": "3 Unsupervised Morphological Disambiguation",
    "text": "Given a rule-based morphological analyzer encoded as an unweighted FST and a corpus on which the analyzer has been run – possibly generating multiple analyses for each token – we can use our unigram model to learn a probabilistic model of disambiguation in an unsupervised setting (i.e., without annotated examples). The corpus is assumed to be generated from the unigram distribution Gw, and the base stem model is set to a fixed character trigram model.4 After learning the parameters of the model, we can find for each word in the vocabulary its most likely analysis and use this as a crude disambiguation step.\n3The proposal distribution for Metropolis-Hastings is a Beta distribution (d) or a Gamma distribution (θ+d) centered on the previous parameter values.\n4Experiments suggest that this is important to constrain the model to realistic stems."
  }, {
    "heading": "3.1 Morphological Guessers",
    "text": "Finite-state morphological analyzers are usually specified in three parts: a stem lexicon, which defines the words in the language and classifies them into several categories according to their grammatical function and their morphological properties; a set of prefixes and suffixes that can be applied to each category to form surface words; and possibly alternation rules that can encode exceptions and spelling variations. The combination of these parts provides a powerful framework for defining a generative model of words. Such models can be reversed to obtain an analyzer. However, while the two latter parts can be relatively easy to specify, enumerating a comprehensive stem lexicon is a time consuming and necessarily incomplete process, as some categories are truly open-class.\nTo allow unknown words to be analyzed, one can use a guesser that attempts to analyze words missing in the lexicon. Can we eliminate the stem lexicon completely and use only the guesser? This is what we try to do by designing a lexicon-free analyzer for Russian. A guesser was developed in three hours; it is prone to over-generation and produces ambiguous analyses for most words but covers a large number of morphological phenomena (gender, case, tense, etc.). For example, the word иврите5 can be correctly analyzed as иврит+Noun+Masc+Prep+Sg but also as the incorrect forms: иврить+Verb+Pres+2P+Pl, иврита+Noun+Fem+Dat+Sg, ивритя+Noun+Fem+Prep+Sg, and more."
  }, {
    "heading": "3.2 Disambiguation Experiments",
    "text": "We train the unigram model on a 1.7M-word corpus of TED talks transcriptions translated into Russian (Cettolo et al., 2012) and evaluate our analyzer against a test set consisting of 1,500 goldstandard analyses obtained from the morphology disambiguation task of the DIALOG 2010 conference (Lyaševskaya et al., 2010).6\nEach analysis is composed of a lemma (иврит), a part of speech (Noun), and a sequence of additional functional morphemes (Masc,Prep,Sg). We consider only open-class categories: nouns, ad-\n5иврите = Hebrew (masculine noun, prepositional case) 6http://ru-eval.ru\njectives, adverbs and verbs, and evaluate the output of our model with three metrics: the lemma accuracy, the part-of-speech accuracy, and the morphology F -measure.7\nAs a baseline, we consider picking a random analysis from output of the analyzer or choosing the most frequent lemma and the most frequent morphological pattern.8 Then, we use our model with each of the three versions of the pattern model described in §2.2. Finally, as an upper bound, we use the gold standard to select one of the analyses produced by the guesser.\nSince our evaluation is not directly comparable to the standard for this task, we use for reference a high-quality analyzer from Xerox9 disambiguated with the MP0 model (all of the models have very close accuracy in this case).\nConsidering the amount of effort put in developing the guesser, the baseline POS tagging accuracy is relatively good. However, the disambiguation is largely improved by using our unigram model with respect to all the evaluation categories. We are still far from the performance of a high-quality analyzer but, in absence of such a resource, our technique might be a sensible option. We also note that there is no clear winner in terms of pattern model, and conclude that this choice is task-specific.\n7F -measure computed for the set of additional morphemes and averaged over the words in the corpus.\n8We estimate these frequencies by assuming each analysis of each token is uniformly likely, then summing fractional counts.\n9http://open.xerox.com/Services/ fst-nlp-tools/Pages/morphology"
  }, {
    "heading": "4 Open Vocabulary Language Models",
    "text": "We now integrate our unigram model in a hierarchical Pitman-Yor n-gram language model (Fig. 1). The training corpus words are assumed to be generated from a distribution Gnw drawn from PY(dn, θn, G n−1 w ), where G n−1 w is defined recursively down to the base model G0w. Previous work Teh (2006) simply used G0w = U(V ) where V is the word vocabulary, but in our case G0w is the MP defined in §2.2.\nWe are interested in evaluating our model in an open vocabulary scenario where the ability to explain new unseen words matters. We expect our model to be able to generalize better thanks to the combination of a morphological analyzer and a stem distribution which is less sparse than the word distribution (for example, for the 1.6M word Turkish corpus, |V | ≈ 3.5|S| ≈ 140k).\nTo integrate out-of-vocabulary words in our evaluation, we use infinite base distributions: G0w (in the baseline model) or G0s (in the MP) are character trigram models. We define perplexity of a held-out test corpus in the standard way:\nppl = exp ( − 1 N N∑ i=1 log p (wi | wi−n+1 · · ·wi−1) )\nbut compared to the common practice, we do not need to discount OOVs from this sum since the model vocabulary is infinite. Note that we also marginalize by summing over all the possible analyses for a given word when computing its base probability according to the MP."
  }, {
    "heading": "4.1 Language Modeling Experiments",
    "text": "We train several trigram models on the Russian TED talks corpus used in the previous section. Our baseline is a hierarchical PY trigram model with a trigram character model as the base word distribution. We compare it with our model using the same character model for the base stem distribution. Both of the morphological analyzers described in the previous section help obtaining perplexity reductions (Table 2). We ran a similar experiment on the Turkish version of this corpus (1.6M words) with a highquality analyzer (Oflazer, 1994) and obtain even larger gains (Table 3).\nThese results can partly be attributed to the high OOV rate in these conditions: 4% for the Russian corpus and 6% for the Turkish corpus."
  }, {
    "heading": "4.2 Predictive Text Input",
    "text": "It is difficult to know whether a decrease in perplexity, as measured in the previous section, will result in a performance improvement in downstream applications. As a confirmation that correctly modeling new words matters, we consider a predictive task with a truly open vocabulary and that requires only a language model: predictive text input.\nGiven some text, we encode it using a lossy deterministic character mapping, and try to recover the original content by computing the most likely word sequence. This task is inspired by predictive text input systems available on cellphones with a 9-key keypad. For example, the string gave me a cup is encoded as 4283 63 2 287, which could also be decoded as: hate of a bus.\nSilfverberg et al. (2012) describe a system designed for this task in Finnish, which is composed of a weighted finite-state morphological analyzer trained on IRC logs. However, their system is restricted to words that are encoded in the analyzer’s lexicon and does not use context for disambiguation.\nIn our experiments, we use the same Turkish TED talks corpus as the previous section. As a baseline, we use a trigram character language model. We produce a character lattice which encodes all the possible interpretations for a word and compose it with a finite-state representation of the character LM using OpenFST (Allauzen et al., 2007). Alternatively, we can use a unigram word model to decode this lattice, backing off to the character language model if no solution is found. Finally, to be able to make use of word context, we can extract the k most likely paths according to the character LM and produce a word lattice, which is in turn decoded with a language model defined over the extracted vocabulary.\nWe measure word and character error rate (WER, CER) on the predicted word sequence and observe large improvements in both of these metrics by modeling morphology, both at the unigram level and when context is used (Table 4).\nPreliminary experiments with a corpus of 1.6M Turkish tweets, an arguably more appropriate domain this task, show smaller but consistent improving: the trigram word error rate is reduced from 26% to 24% when our model is used."
  }, {
    "heading": "4.3 Limitations",
    "text": "While our model is an important step forward in practical modeling of OOVs using morphological processes, we have made the linguistically naive assumption that morphology applies inside the language’s lexicon but has no effect on the process that put inflected lexemes together into sentences. In this\nregard, our model is a minor variant on traditional ngram models that work with “opaque” word forms. How to best relax this assumption in a computationally tractable way is an important open question left for future work."
  }, {
    "heading": "5 Word Alignment Model",
    "text": "Monolingual models of language are not the only models that can benefit from taking into account morphology. In fact, alignment models are a good candidate for using richer word distributions: they assume a target word distribution conditioned on each source word. When the target language is morphologically rich, classic independence assumptions produce very weak models unless some kind of preprocessing is applied to one side of the corpus. An alternative is to use our unigram model as a word translation distribution for each source word in the corpus.\nOur alignment model is based on a simple variant of IBM Model 2 where the alignment distribution is only controlled by two parameters, λ and p0 (Dyer et al., 2013). p0 is the probability of the null alignment. For a source sentence f of length n, a target sentence e of lengthm and a latent alignment a, we define the following alignment link probabilities (j 6= 0):\np(ai = j | n,m) ∝ (1− p0) exp ( −λ ∣∣∣∣ im − jn ∣∣∣∣) λ controls the flatness of this distribution: larger values make the probabilities more peaked around the diagonal of the alignment matrix.\nEach target word is then generated given a source word and a latent alignment link from the word translation distribution p(ei | fai , Gw). Note that this is effectively a unigram distribution over target words, albeit conditioned on the source word fj . Here is where our model differs from classic alignment models: the unigram distribution Gw is assumed be generated from a PY process. There are two choices for the base word distribution:\n• As a baseline, we use a uniform base distribution over the target vocabulary: G0w = U(V ).\n• We define a stem distribution Gs[f ] for each source word f , a shared pattern distributionGp, and set G0w[f ] = MP(Gs[f ], Gp). In this case,\nwe obtain the model depicted in Fig. 2. The stem and the pattern models are also given PY priors with uniform base distribution (G0s = U(S)).\nFinally, we put uninformative priors on the alignment distribution parameters: p0 ∼ Beta(α, β) is collapsed and λ ∼ Gamma(k, θ) is inferred using Metropolis-Hastings.\nExperiments We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side. We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs.\nEnglish-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Çakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections.\nEnglish-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments\n10We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4.\nfrom Bojar and Prokopová (2006). The morphological analyzer is provided by Xerox.\nResults Results are shown in Table 5. Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions. With an identical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages.\nAs an example of how our model generalizes better, consider the sentence pair in Fig. 3, taken from the evaluation data. The two words composing the Turkish sentence are not found elsewhere in the corpus, but several related inflections occur.11 It is therefore trivial for the stem-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points)."
  }, {
    "heading": "6 Related Work",
    "text": "Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and\n11ödevinin, ödevini, ödevleri; bitmez, bitirileceğinden, bitmesiyle, ...\nKay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.\nSince some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajič et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000).\nRule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds.\nMorphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011).\nDespite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit-\n12Developing a high-coverage analyzer can be a timeconsuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science.\ntle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009).\nFinally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006)."
  }, {
    "heading": "7 Conclusion",
    "text": "We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems. We have shown how it can be integrated into several models central to NLP applications and have empirically validated the effectiveness of these changes. Although this paper mostly focused on languages that are well studied and for which high-quality analyzers are available, our models are especially relevant in low-resource scenarios because they do not require disambiguated analyses. In future work, we plan to apply these techniques to languages such as Kinyarwanda, a resource-poor but morphologically rich language spoken in Rwanda. It is our belief that knowledge-rich models can help bridge the gap between low- and high-resource languages."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Kemal Oflazer for making his Turkish language morphological analyzer available to us and Brendan O’Connor for gathering the Turkish tweets used in\nthe predictive text experiments. This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533."
  }],
  "year": 2013,
  "references": [{
    "title": "The impact of Arabic morphological segmentation on broadcoverage English-to-Arabic statistical machine translation",
    "authors": ["H. Al-Haj", "A. Lavie."],
    "venue": "Proc. of AMTA.",
    "year": 2010
  }, {
    "title": "OpenFst: A general and efficient weighted finite-state transducer library",
    "authors": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."],
    "venue": "Implementation and Application of Automata, pages 11–23.",
    "year": 2007
  }, {
    "title": "Discriminative n-gram language modeling for Turkish",
    "authors": ["Ebru Arısoy", "Brian Roark", "Izhak Shafran", "Murat Saraçlar."],
    "venue": "Proc. of Interspeech.",
    "year": 2008
  }, {
    "title": "Modelling out-of-vocabulary words for robust speech recognition",
    "authors": ["Issam Bazzi."],
    "venue": "Ph.D. thesis, MIT.",
    "year": 2002
  }, {
    "title": "Finite-state morphology: Xerox tools and techniques",
    "authors": ["K.R. Beesley", "L. Karttunen."],
    "venue": "CSLI, Stanford.",
    "year": 2003
  }, {
    "title": "Factored language models and generalized parallel backoff",
    "authors": ["Jeff A. Bilmes", "Katrin Kirchhoff."],
    "venue": "Proc. of NAACL.",
    "year": 2003
  }, {
    "title": "CzechEnglish word alignment",
    "authors": ["Ondřej Bojar", "Magdalena Prokopová."],
    "venue": "Proc. of LREC.",
    "year": 2006
  }, {
    "title": "Discovering morphemic suffixes: A case study in MDL induction",
    "authors": ["Michael R. Brent", "Sreerama K. Murthy", "Andrew Lundberg."],
    "venue": "Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics.",
    "year": 1995
  }, {
    "title": "An estimate of an upper bound for the entropy of English",
    "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer", "Jennifer C. Lai."],
    "venue": "Computational Linguistics, 18(1):31–40.",
    "year": 1992
  }, {
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer."],
    "venue": "Computational Linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Scaling high-order character language models to gigabytes",
    "authors": ["Bob Carpenter."],
    "venue": "Proceedings of the ACL Workshop on Software.",
    "year": 2005
  }, {
    "title": "WIT3: Web inventory of transcribed and translated talks",
    "authors": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."],
    "venue": "Proc. of EAMT.",
    "year": 2012
  }, {
    "title": "An empirical study of smoothing techniques for language modeling",
    "authors": ["Stanley F. Chen", "Joshua Goodman."],
    "venue": "Technical Report TR-10-98, Harvard University.",
    "year": 1998
  }, {
    "title": "Unsupervised models for morpheme segmentation and morphology learning",
    "authors": ["Mathias Creutz", "Krista Lagus."],
    "venue": "ACM Transactions on Speech and Language Processing, 4(1).",
    "year": 2007
  }, {
    "title": "Morph-based speech recognition and modeling of out-of-vocabulary words across languages",
    "authors": ["M. Creutz", "T. Hirsimäki", "M. Kurimo", "A. Puurula", "J. Pylkkönen", "V. Siivola", "M. Varjokallio", "E. Arisoy", "M. Saraçlar", "A. Stolcke."],
    "venue": "ACM Transactions on Speech",
    "year": 2007
  }, {
    "title": "Unsupervised Language Acquisition",
    "authors": ["Carl G. de Marcken."],
    "venue": "Ph.D. thesis, MIT.",
    "year": 1996
  }, {
    "title": "A simple, fast, and effective reparameterization of IBM Model 2",
    "authors": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."],
    "venue": "Proc. of NAACL.",
    "year": 2013
  }, {
    "title": "Unsupervised learning of the morphology of a natural language",
    "authors": ["J. Goldsmith."],
    "venue": "Computational Linguistics, 27(2):153–198.",
    "year": 2001
  }, {
    "title": "Improving statistical MT through morphological analysis",
    "authors": ["S. Goldwater", "D. McClosky."],
    "venue": "Proc. of EMNLP.",
    "year": 2005
  }, {
    "title": "Producing power-law distributions and damping word frequencies with two-stage language models",
    "authors": ["Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson."],
    "venue": "Journal of Machine Learning Research, 12:2335–2382.",
    "year": 2011
  }, {
    "title": "Arabic tokenization, part-of-speech tagging, and morphological disambiguation in one fell swoop",
    "authors": ["Nizar Habash", "Owen Rambow."],
    "venue": "Proc. of ACL.",
    "year": 2005
  }, {
    "title": "Arabic preprocessing schemes for statistical machine translation",
    "authors": ["Nizar Habash", "Fatiha Sadat."],
    "venue": "Proc. of NAACL.",
    "year": 2006
  }, {
    "title": "MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization",
    "authors": ["Nizar Habash", "Owen Rambow", "Ryan Roth."],
    "venue": "Proceedings of the Second International Conference on Arabic Lan-",
    "year": 2009
  }, {
    "title": "Serial combination of rules and statistics",
    "authors": ["Jan Hajič", "P. Krbec", "P. Květoň", "K. Oliva", "V. Petrovič."],
    "venue": "Proc. of ACL.",
    "year": 2001
  }, {
    "title": "Statistical morphological disambiguation for agglutinative languages",
    "authors": ["D.Z. Hakkani-Tür", "Kemal Oflazer", "G. Tür."],
    "venue": "Proc. of COLING.",
    "year": 2000
  }, {
    "title": "Information Retrieval: Computational and Theoretical Aspects",
    "authors": ["Harold Stanley Heaps."],
    "venue": "Academic Press.",
    "year": 1978
  }, {
    "title": "Foma: a finite-state compiler and library",
    "authors": ["M. Hulden."],
    "venue": "Proc. of EACL.",
    "year": 2009
  }, {
    "title": "Regular models of phonological rule systems",
    "authors": ["Ronald M. Kaplan", "Martin Kay."],
    "venue": "Computational Linguistics, 20(3):331–378.",
    "year": 1994
  }, {
    "title": "Twentyfive years of finite-state morphology",
    "authors": ["Lauri Karttunen", "Kenneth R. Beesley."],
    "venue": "Inquiries into Words, Constraints and Contexts, pages 71–83. CSLI.",
    "year": 2005
  }, {
    "title": "A general computational model for word-form recognition and production",
    "authors": ["Kimmo Koskenniemi."],
    "venue": "Proc. of ACL-COLING. 1214",
    "year": 1984
  }, {
    "title": "Ocenka metodov avtomatičeskogo",
    "authors": ["O. Lyaševskaya", "I. Astaf’yeva", "A. Bonch-Osmolovskaya", "A. Garejšina", "Y. Grišina", "V. D’yačkov", "M. Ionov", "A. Koroleva", "M. Kudrinskij", "A. Lityagina", "Y. Lučina", "Y. Sidorova", "S. Toldova", "S. Savčuk", "S. Koval"],
    "year": 2010
  }, {
    "title": "Generating complex morphology for machine translation",
    "authors": ["Einat Minkov", "Kristina Toutanova", "Hisami Suzuki."],
    "venue": "Proc. of ACL.",
    "year": 2007
  }, {
    "title": "Statistical machine translation with local language models",
    "authors": ["Christof Monz."],
    "venue": "Proc. of EMNLP.",
    "year": 2011
  }, {
    "title": "Exploring different representational units in English-toTurkish statistical machine translation",
    "authors": ["Kemal Oflazer", "İlknur Durgar El-Kahlout."],
    "venue": "Proc. of StatMT.",
    "year": 2007
  }, {
    "title": "Two-level description of Turkish morphology",
    "authors": ["K. Oflazer."],
    "venue": "Literary and Linguistic Computing, 9(2):137–148.",
    "year": 1994
  }, {
    "title": "Learning sub-word units for open vocabulary speech recognition",
    "authors": ["Carolina Parada", "Mark Dredze", "Abhinav Sethy", "Ariya Rastrow."],
    "venue": "Proc. of ACL.",
    "year": 2011
  }, {
    "title": "The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator",
    "authors": ["Jim Pitman", "Marc Yor."],
    "venue": "Annals of Probability, 25(2):855–90.",
    "year": 1997
  }, {
    "title": "Open vocabulary language modeling for binary response typing interfaces",
    "authors": ["Brian Roark."],
    "venue": "Technical Report CSLU-09-001, Oregon Health & Science University.",
    "year": 2009
  }, {
    "title": "Hierarchical hybrid language models for open vocabulary continuous speech recognition using wfst",
    "authors": ["M. Ali Basha Shaik", "David Rybach", "Stefan Hahn", "Ralf Schluüter", "Hermann Ney."],
    "venue": "Proc. of SAPA.",
    "year": 2012
  }, {
    "title": "Predictive text entry for agglutinative languages using unsupervised morphological segmentation",
    "authors": ["M. Silfverberg", "K. Lindén", "M. Hyvärinen."],
    "venue": "Proc. of Computational Linguistics and Intelligent Text Processing.",
    "year": 2012
  }, {
    "title": "Context-based morphological disambiguation with random fields",
    "authors": ["Noah A. Smith", "David A. Smith", "Roy W. Tromble."],
    "venue": "Proc. of EMNLP.",
    "year": 2005
  }, {
    "title": "A Bayesian model for morpheme and paradigm identification",
    "authors": ["Matt G. Snover", "Michael R. Brent."],
    "venue": "Proc. of ACL.",
    "year": 2001
  }, {
    "title": "Unsupervised multilingual learning for morphological segmentation",
    "authors": ["Benjamin Snyder", "Regina Barzilay."],
    "venue": "Proc. of ACL.",
    "year": 2008
  }, {
    "title": "A hierarchical Bayesian language model based on Pitman-Yor processes",
    "authors": ["Yee Whye Teh."],
    "venue": "Proc. of ACL.",
    "year": 2006
  }, {
    "title": "Statistical parsing of morphologically rich languages: What, how and whither",
    "authors": ["Reut Tsarfaty", "Djamé Seddah", "Yoav Goldberg", "Sandra Kübler", "Marie Candito", "Jennifer Foster", "Yannick Versley", "Ines Rehbein", "Lamia Tounsi."],
    "venue": "Proc. of Workshop on Statistical Pars-",
    "year": 2010
  }, {
    "title": "South-east european times: A parallel corpus of Balkan languages",
    "authors": ["F. Tyers", "M.S. Alperen."],
    "venue": "Proceedings of the LREC workshop on Exploitation of multilingual resources and tools for Central and (South) Eastern European Languages.",
    "year": 2010
  }, {
    "title": "Word alignment for English-Turkish language pair",
    "authors": ["M. Talha Çakmak", "Süleyman Acar", "Gülşen Eryiğit."],
    "venue": "Proc. of LREC.",
    "year": 2012
  }],
  "id": "SP:69d5f6d00b8f76e51c12c13edee143bb1dd2d069",
  "authors": [{
    "name": "Victor Chahuneau",
    "affiliations": []
  }, {
    "name": "Noah A. Smith",
    "affiliations": []
  }, {
    "name": "Chris Dyer",
    "affiliations": []
  }],
  "abstractText": "We present a morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-the-art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages.",
  "title": "Knowledge-Rich Morphological Priors for Bayesian Language Models"
}