{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 150–161 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n150"
  }, {
    "heading": "1 Introduction",
    "text": "Computational modeling of human multimodal language is an upcoming research area in natural language processing. This research area focuses on modeling tasks such as multimodal sentiment analysis (Morency et al., 2011), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014). The multimodal temporal signals include the language (spoken words), visual (facial expressions, gestures) and acoustic modalities (prosody, vocal expressions). At its core, these multimodal signals are\nRecurrent Multistage Fusion\nhighly structured with two prime forms of interactions: intra-modal and cross-modal interactions (Rajagopalan et al., 2016). Intra-modal interactions refer to information within a specific modality, independent of other modalities. For example, the arrangement of words in a sentence according\nto the generative grammar of a language (Chomsky, 1957) or the sequence of facial muscle activations for the presentation of a frown. Cross-modal interactions refer to interactions between modalities. For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed occurrence of a laughter after the end of a sentence. Modeling these interactions lie at the heart of human multimodal language analysis and has recently become a centric research direction in multimodal natural language processing (Liu et al., 2018; Pham et al., 2018; Chen et al., 2017), multimodal speech recognition (Sun et al., 2016; Gupta et al., 2017; Harwath and Glass, 2017; Kamper et al., 2017), as well as multimodal machine learning (Tsai et al., 2018; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).\nRecent advances in cognitive neuroscience have demonstrated the existence of multistage aggregation across human cortical networks and functions (Taylor et al., 2015), particularly during the integration of multisensory information (Parisi et al., 2017). At later stages of cognitive processing, higher level semantic meaning is extracted from phrases, facial expressions, and tone of voice, eventually leading to the formation of higher level crossmodal concepts (Parisi et al., 2017; Taylor et al., 2015). Inspired by these discoveries, we hypothesize that the computational modeling of crossmodal interactions also requires a multistage fusion process. In this process, cross-modal representations can build upon the representations learned during earlier stages. This decreases the burden on each stage of multimodal fusion and allows each stage of fusion to be performed in a more specialized and effective manner.\nIn this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which automatically decomposes the multimodal fusion problem into multiple recursive stages. At each stage, a subset of multimodal signals is highlighted and fused with previous fusion representations (see Figure 1). This divide-and-conquer approach decreases the burden on each fusion stage, allowing each stage to be performed in a more specialized and effective way. This is in contrast with conventional fusion approaches which usually model interactions over multimodal signals altogether in one iteration (e.g., early fusion (Baltrušaitis et al., 2017)). In RMFN, temporal and intra-modal interactions are modeled by integrating our new multistage fusion process\nwith a system of recurrent neural networks. Overall, RMFN jointly models intra-modal and cross-modal interactions for multimodal language analysis and is differentiable end-to-end.\nWe evaluate RMFN on three different tasks related to human multimodal language: sentiment analysis, emotion recognition, and speaker traits recognition across three public multimodal datasets. RMFN achieves state-of-the-art performance in all three tasks. Through a comprehensive set of ablation experiments and visualizations, we demonstrate the advantages of explicitly defining multiple recursive stages for multimodal fusion."
  }, {
    "heading": "2 Related Work",
    "text": "Previous approaches in human multimodal language modeling can be categorized as follows: Non-temporal Models: These models simplify the problem by using feature-summarizing temporal observations (Poria et al., 2017). Each modality is represented by averaging temporal information through time, as shown for language-based sentiment analysis (Iyyer et al., 2015; Chen et al., 2016) and multimodal sentiment analysis (Abburi et al., 2016; Nojavanasghari et al., 2016; Zadeh et al., 2016; Morency et al., 2011). Conventional supervised learning methods are utilized to discover intra-modal and cross-modal interactions without specific model design (Wang et al., 2016; Poria et al., 2016). These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics (Xu et al., 2013). Multimodal Temporal Graphical Models: The application of graphical models in sequence modeling has been an important research problem. Hidden Markov Models (HMMs) (Baum and Petrie, 1966), Conditional Random Fields (CRFs) (Lafferty et al., 2001), and Hidden Conditional Random Fields (HCRFs) (Quattoni et al., 2007) were shown to work well on modeling sequential data from the language (Misawa et al., 2017; Ma and Hovy, 2016; Huang et al., 2015) and acoustic (Yuan and Liberman, 2008) modalities. These temporal graphical models have also been extended for modeling multimodal data. Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song\net al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013). Multimodal Temporal Neural Networks: More recently, with the advent of deep learning, Recurrent Neural Networks (Elman, 1990; Jain and Medsker, 1999) have been used extensively for language and speech based sequence modeling (Zilly et al., 2016; Soltau et al., 2016), sentiment analysis (Socher et al., 2013; dos Santos and Gatti, 2014; Glorot et al., 2011; Cambria, 2016), and emotion recognition (Han et al., 2014; Bertero et al., 2016; Lakomkin et al., 2018). Long-short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997a) have also been extended for multimodal settings (Rajagopalan et al., 2016) and by learning binary gating mechanisms to remove noisy modalities (Chen et al., 2017). Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions. These use Bayesian ranking algorithms (Herbrich et al., 2007) to model both person-independent and person-dependent features (Liang et al., 2018), generative-discriminative objectives to learn either joint (Pham et al., 2018) or factorized multimodal representations (Tsai et al., 2018), external memory mechanisms to synchronize multimodal data (Zadeh et al., 2018a), or lowrank tensors to approximate expensive tensor products (Liu et al., 2018). All these methods assume that cross-modal interactions should be discovered all at once rather than across multiple stages, where each stage solves a simpler fusion problem. Our empirical evaluations show the advantages of the multistage fusion approach."
  }, {
    "heading": "3 Recurrent Multistage Fusion Network",
    "text": "In this section we describe the Recurrent Multistage Fusion Network (RMFN) for multimodal language analysis (Figure 2). Given a set of modalities{l(anguage), v(isual), a(coustic)}, the signal from each modality m ∈ {l, v, a} is represented as a temporal sequence Xm = {xm1 ,xm2 ,xm3 ,,xmT }, where xmt is the input at time t. Each sequence Xm is modeled with an intra-modal recurrent neural network (see subsection 3.3 for details). At time t, each intra-modal recurrent network will output a unimodal representation hmt . The Multistage Fusion Process uses a recursive approach to fuse all unimodal representations hmt into a cross-modal representation zt which is then fed back into each intra-modal recurrent network."
  }, {
    "heading": "3.1 Multistage Fusion Process",
    "text": "The Multistage Fusion Process (MFP) is a modular neural approach that performs multistage fusion to model cross-modal interactions. Multistage fusion is a divide-and-conquer approach which decreases the burden on each stage of multimodal fusion, allowing each stage to be performed in a more specialized and effective way. The MFP has three main modules: HIGHLIGHT, FUSE and SUMMARIZE.\nTwo modules are repeated at each stage: HIGHLIGHT and FUSE. The HIGHLIGHT module identifies a subset of multimodal signals from[hlt,hvt ,hat ] that will be used for that stage of fusion. The FUSE module then performs two subtasks simultaneously: a local fusion of the highlighted features and integration with representations from previous stages. Both HIGHLIGHT and FUSE modules are realized using memorybased neural networks which enable coherence between stages and storage of previously modeled cross-modal interactions. As a final step, the SUMMARIZE module takes the multimodal representation of the final stage and translates it into a cross-modal representation zt.\nFigure 1 shows an illustrative example for multistage fusion. The HIGHLIGHT module selects “neutral words” and “frowning” expression for the first stage. The local and integrated fusion at this stage creates a representation reflecting negative emotion. For stage 2, the HIGHLIGHT module identifies the acoustic feature “loud voice”. The local fusion at this stage interprets it as an expression of emphasis and is fused with the previous fusion results to represent a strong negative emotion. Finally, the highlighted features of “shrug” and “speech elongation” are selected and are locally interpreted as “ambivalence”. The integration with previous stages then gives a representation closer to “disappointed”."
  }, {
    "heading": "3.2 Module Descriptions",
    "text": "In this section, we present the details of the three multistage fusion modules: HIGHLIGHT, FUSE and SUMMARIZE. Multistage fusion begins with the concatenation of intra-modal network outputs ht =m∈M hmt . We use superscript [k] to denote the indices of each stage k = 1,,K during K total stages of multistage fusion. Let ⇥ denote the neural network parameters across all modules. HIGHLIGHT: At each stage k, a subset of the multimodal signals represented in ht will be au-\ntomatically highlighted for fusion. Formally, this module is defined by the process function fH : a[k]t = fH(ht ; a[1∶k−1]t ,⇥) (1) where at stage k, a[k]t is a set of attention weights which are inferred based on the previously assigned attention weights a[1∶k−1]t . As a result, the highlights at a specific stage k will be dependent on previous highlights. To fully encapsulate these dependencies, the attention assignment process is performed in a recurrent manner using a LSTM which we call the HIGHLIGHT LSTM. The initial HIGHLIGHT LSTM memory at stage 0, cHIGHLIGHT[0]t , is initialized using a networkM that maps ht into LSTM memory space: cHIGHLIGHT[0]t =M(ht ; ⇥) (2) This allows the memory mechanism of the HIGHLIGHT LSTM to dynamically adjust to the intra-modal representations ht. The output of the HIGHLIGHT LSTM hHIGHLIGHT[k]t is softmax activated to produce attention weights a[k]t at every stage k of the multistage fusion process:\na[k]t j = exp (h HIGHLIGHT[k] t j)\n∑hHIGHLIGHT[k]t d=1 exp (hHIGHLIGHT[k]t d) (3)\nand a[k]t is fed as input into the HIGHLIGHT LSTM at stage k + 1. Therefore, the HIGHLIGHT LSTM functions as a decoder LSTM (Sutskever\net al., 2014; Cho et al., 2014) in order to capture the dependencies on previous attention assignments. Highlighting is performed by element-wise multiplying the attention weights a[k]t with the concatenated intra-modal representations ht:\nh̃[k]t = ht ⊙ a[k]t (4) where ⊙ denotes the Hadamard product and h̃[k]t are the attended multimodal signals that will be used for the fusion at stage k. FUSE: The highlighted multimodal signals are simultaneously fused in a local fusion and then integrated with fusion representations from previous stages. Formally, this module is defined by the process function fF :\ns[k]t = fF (h̃[k]t ; s[1∶k−1]t ,⇥) (5) where s[k]t denotes the integrated fusion representations at stage k. We employ a FUSE LSTM to simultaneously perform the local fusion and the integration with previous fusion representations. The FUSE LSTM input gate enables a local fusion while the FUSE LSTM forget and output gates enable integration with previous fusion results. The initial FUSE LSTM memory at stage 0, cFUSE[0]t , is initialized using random orthogonal matrices (Arjovsky et al., 2015; Le et al., 2015). SUMMARIZE: After completing K recursive stages of HIGHLIGHT and FUSE, the SUMMARIZE operation generates a cross-modal\nrepresentation using all final fusion representations s[1∶K]t . Formally, this operation is defined as: zt = S(s[1∶K]t ; ⇥) (6) where zt is the final output of the multistage fusion process and represents all cross-modal interactions discovered at time t. The summarized cross-modal representation is then fed into the intra-modal recurrent networks as described in the subsection 3.3."
  }, {
    "heading": "3.3 System of Long Short-term Hybrid Memories",
    "text": "To integrate the cross-modal representations zt with the temporal intra-modal representations, we employ a system of Long Short-term Hybrid Memories (LSTHMs) (Zadeh et al., 2018b). The LSTHM extends the LSTM formulation to include the cross-modal representation zt in a hybrid memory component:\nimt+1 = (Wmi xmt+1 +Umi hmt +Vmi zt + bmi ) (7) fmt+1 = (Wmf xmt+1 +Umf hmt +Vmf zt + bmf ) (8) omt+1 = (Wmo xmt+1 +Umo hmt +Vmo zt + bmo ) (9) c̄mt+1 =Wmc̄ xmt+1 +Umc̄ hmt +Vmc̄ zt + bmc̄ (10) cmt+1 = fmt ⊙ cmt + imt ⊙ tanh(c̄mt+1) (11) hmt+1 = omt+1 ⊙ tanh(cmt+1) (12)\nwhere is the (hard-)sigmoid activation function, tanh is the tangent hyperbolic activation function,⊙ denotes the Hadamard product. i, f and o are the input, forget and output gates respectively. c̄mt+1 is the proposed update to the hybrid memory cmt at time t + 1 and hmt is the time distributed output of each modality. The cross-modal representation zt is modeled by the Multistage Fusion Process as discussed in subsection 3.2. The hybrid memory cmt contains both intra-modal interactions from individual modalities xmt as well as the cross-modal interactions captured in zt."
  }, {
    "heading": "3.4 Optimization",
    "text": "The multimodal prediction task is performed using a final representation E which integrate (1) the last outputs from the LSTHMs and (2) the last crossmodal representation zT . Formally, E is defined as:\nE = ( m∈M h m T )zT (13)\nwhere denotes vector concatenation. E can then be used as a multimodal representation for supervised or unsupervised analysis of multimodal language. It summarizes all modeled intra-modal\nand cross-modal representations from the multimodal sequences. RMFN is differentiable end-toend which allows the network parameters ⇥ to be learned using gradient descent approaches."
  }, {
    "heading": "4 Experimental Setup",
    "text": "To evaluate the performance and generalization of RMFN, three domains of human multimodal language were selected: multimodal sentiment analysis, emotion recognition, and speaker traits recognition."
  }, {
    "heading": "4.1 Datasets",
    "text": "All datasets consist of monologue videos. The speaker’s intentions are conveyed through three modalities: language, visual and acoustic. Multimodal Sentiment Analysis involves analyzing speaker sentiment based on video content. Multimodal sentiment analysis extends conventional language-based sentiment analysis to a multimodal setup where both verbal and non-verbal signals contribute to the expression of sentiment. We use CMU-MOSI (Zadeh et al., 2016) which consists of 2199 opinion segments from online videos each annotated with sentiment in the range [-3,3]. Multimodal Emotion Recognition involves identifying speaker emotions based on both verbal and nonverbal behaviors. We perform experiments on the IEMOCAP dataset (Busso et al., 2008) which consists of 7318 segments of recorded dyadic dialogues annotated for the presence of human emotions happiness, sadness, anger and neutral. Multimodal Speaker Traits Recognition involves recognizing speaker traits based on multimodal communicative behaviors. POM (Park et al., 2014) contains 903 movie review videos each annotated for 12 speaker traits: confident (con), passionate (pas), voice pleasant (voi), credible (cre), vivid (viv), expertise (exp), reserved (res), trusting (tru), relaxed (rel), thorough (tho), nervous (ner), persuasive (per) and humorous (hum)."
  }, {
    "heading": "4.2 Multimodal Features and Alignment",
    "text": "GloVe word embeddings (Pennington et al., 2014), Facet (iMotions, 2017) and COVAREP (Degottex et al., 2014) are extracted for the language, visual and acoustic modalities respectively 1. Forced alignment is performed using P2FA (Yuan and Liberman, 2008) to obtain the exact utterance times\n1Details on feature extraction are in supplementary.\nof each word. We obtain the aligned video and audio features by computing the expectation of their modality feature values over each word utterance time interval (Tsai et al., 2018)."
  }, {
    "heading": "4.3 Baseline Models",
    "text": "We compare to the following models for multimodal machine learning: MFN (Zadeh et al., 2018a) synchronizes multimodal sequences using a multi-view gated memory. It is the current state of the art on CMU-MOSI and POM. MARN (Zadeh et al., 2018b) models intra-modal and cross-modal interactions using multiple attention coefficients and hybrid LSTM memory components. GMELSTM(A) (Chen et al., 2017) learns binary gating mechanisms to remove noisy modalities that are contradictory or redundant for prediction. TFN (Zadeh et al., 2017) models unimodal, bimodal and trimodal interactions using tensor products.\nBC-LSTM (Poria et al., 2017) performs contextdependent sentiment analysis and emotion recognition, currently state of the art on IEMOCAP. EFLSTM concatenates the multimodal inputs and uses that as input to a single LSTM (Hochreiter and Schmidhuber, 1997b). We also implement the Stacked, (EF-SLSTM) (Graves et al., 2013) Bidirectional (EF-BLSTM) (Schuster and Paliwal, 1997) and Stacked Bidirectional (EF-SBLSTM) LSTMs. For descriptions of the remaining baselines, we refer the reader to EF-HCRF (Quattoni et al., 2007), EF/MV-LDHCRF (Morency et al., 2007), MV-HCRF (Song et al., 2012), EF/MVHSSHCRF (Song et al., 2013), MV-LSTM (Rajagopalan et al., 2016), DF (Nojavanasghari et al., 2016), SAL-CNN (Wang et al., 2016), C-MKL (Poria et al., 2015), THMM (Morency et al., 2011), SVM (Cortes and Vapnik, 1995; Park et al., 2014) and RF (Breiman, 2001)."
  }, {
    "heading": "4.4 Evaluation Metrics",
    "text": "For classification, we report accuracy Ac where c denotes the number of classes and F1 score. For regression, we report Mean Absolute Error MAE and Pearson’s correlation r. For MAE lower values indicate stronger performance. For all remaining metrics, higher values indicate stronger performance."
  }, {
    "heading": "5 Results and Discussion",
    "text": ""
  }, {
    "heading": "5.1 Performance on Multimodal Language",
    "text": "Results on CMU-MOSI, IEMOCAP and POM are presented in Tables 1, 2 and 3 respectively2. We achieve state-of-the-art or competitive results for all domains, highlighting RMFN’s capability in human multimodal language analysis. We observe that RMFN does not improve results on IEMOCAP neutral emotion and the model outperforming RMFN is a memory-based fusion baseline (Zadeh et al., 2018a). We believe that this is because neutral expressions are quite idiosyncratic. Some people may always look angry given their facial configuration (e.g., natural eyebrow raises of actor Jack Nicholson). In these situations, it becomes useful to compare the current image with a memorized or aggregated representation of the speaker’s face. Our proposed multistage fusion approach can easily be extended to memory-based fusion methods.\n2Results for all individual baseline models are in supplementary. State-of-the-art (SOTA)1/2/3 represent the three best performing baseline models on each dataset."
  }, {
    "heading": "5.2 Analysis of Multistage Fusion",
    "text": "To achieve a deeper understanding of the multistage fusion process, we study five research questions. (Q1): whether modeling cross-modal interactions across multiple stages is beneficial. (Q2): the effect of the number of stages K during multistage fusion on performance. (Q3): the comparison between multistage and independent modeling of cross-modal interactions. (Q4): whether modeling cross-modal interactions are helpful. (Q5): whether attention weights from the HIGHLIGHT module are required for modeling cross-modal interactions. Q1: To study the effectiveness of the multistage fusion process, we test the baseline RMFN-R1 which performs fusion in only one stage instead of across\nmultiple stages. This model makes the strong assumption that all cross-modal interactions can be modeled during only one stage. From Table 4, RMFN-R1 underperforms as compared to RMFN which performs multistage fusion. Q2: We test baselines RMFN-RK which perform K stages of fusion. From Table 4, we observe that increasing the number of stages K increases the model’s capability to model cross-modal interactions up to a certain point (K = 3) in our experiments. Further increases led to decreases in performance and we hypothesize this is due to overfitting on the dataset. Q3: To compare multistage against independent modeling of cross-modal interactions, we pay close attention to the performance comparison with respect to MARN which models multiple crossmodal interactions all at once (see Table 5). RMFN shows improved performance, indicating that multistage fusion is both effective and efficient for human multimodal language modeling. Q4: RMFN (no MFP) represents a system of LSTHMs without the integration of zt from the MFP to model cross-modal interactions. From Table 5, RMFN (no MFP) is outperformed by RMFN, confirming that modeling cross-modal interactions is crucial in analyzing human multimodal language. Q5: RMFN (no HIGHLIGHT) removes the HIGHLIGHT module from MFP during multistage fusion. From Table 5, RMFN (no HIGHLIGHT) underperforms, indicating that highlighting multimodal representations using attention weights are important for modeling cross-modal interactions."
  }, {
    "heading": "5.3 Visualizations",
    "text": "Using an attention assignment mechanism during the HIGHLIGHT process gives more interpretability to the model since it allows us to visualize the attended multimodal signals at each stage and\ntime step (see Figure 3). Using RMFN trained on the CMU-MOSI dataset, we plot the attention weights across the multistage fusion process for three videos in CMU-MOSI. Based on these visualizations we first draw the following general observations on multistage fusion: Across stages: Attention weights change their behaviors across the multiple stages of fusion. Some features are highlighted by earlier stages while other features are used in later stages. This supports our hypothesis that RMFN learns to specialize in different stages of the fusion process. Across time: Attention weights vary over time and adapt to the multimodal inputs. We observe that the attention weights are similar if the input contains no new information. As soon as new multimodal information comes in, the highlighting mechanism in RMFN adapts to these new inputs. Priors: Based on the distribution of attention weights, we observe that the language and acoustic modalities seem the most commonly highlighted. This represents a prior over the expression of sentiment in human multimodal language and is closely related to the strong connections between language and speech in human communication (Kuhl, 2000). Inactivity: Some attention coefficients are not active (always orange) throughout time. We hypothesize that these corresponding dimensions carry only intra-modal dynamics and are not involved in the formation of cross-modal interactions."
  }, {
    "heading": "5.4 Qualitative Analysis",
    "text": "In addition to the general observations above, Figure 3 shows three examples where multistage fusion learns cross-modal representations across three different scenarios. Synchronized Interactions: In Figure 3(a), the language features are highlighted corresponding to the utterance of the word “fun” that is highly indicative of sentiment (t = 5). This sudden change is also accompanied by a synchronized highlighting of the acoustic features. We also notice that the highlighting of the acoustic features lasts longer across the 3 stages since it may take multiple stages to interpret all the new acoustic behaviors (elongated tone of voice and phonological emphasis). Asynchronous Trimodal Interactions: In Figure 3(b), the language modality displays ambiguous sentiment: “delivers a lot of intensity” can be inferred as both positive or negative. We observe that the circled attention units in the visual and acoustic features correspond to the asynchronous presence of a smile (t = 2 ∶ 5) and phonological emphasis (t = 3) respectively. These nonverbal behaviors resolve ambiguity in language and result in an overall display of positive sentiment. We further\nnote the coupling of attention weights that highlight the language, visual and acoustic features across stages (t = 3 ∶ 5), further emphasizing the coordination of all three modalities during multistage fusion despite their asynchronous occurrences. Bimodal Interactions: In Figure 3(c), the language modality is better interpreted in the context of acoustic behaviors. The disappointed tone and soft voice provide the nonverbal information useful for sentiment inference. This example highlights the bimodal interactions (t = 4 ∶ 7) in alternating stages: the acoustic features are highlighted more in earlier stages while the language features are highlighted increasingly in later stages."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper proposed the Recurrent Multistage Fusion Network (RMFN) which decomposes the multimodal fusion problem into multiple stages, each focused on a subset of multimodal signals. Extensive experiments across three publicly-available datasets reveal that RMFN is highly effective in modeling human multimodal language. In addition to achieving state-of-the-art performance on all datasets, our comparisons and visualizations reveal that the multiple stages coordinate to capture both synchronous and asynchronous multimodal interactions. In future work, we are interested in merging our model with memory-based fusion methods since they have complementary strengths as discussed in subsection 5.1."
  }, {
    "heading": "Acknowledgements",
    "text": "This material is based upon work partially supported by the National Science Foundation (Award #1833355) and Samsung. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Samsung, and no official endorsement should be inferred. The authors thank Yao Chong Lim, Venkata Ramana Murthy Oruganti, Zhun Liu, Ying Shen, Volkan Cirik, and the anonymous reviewers for their constructive comments on this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Multimodal sentiment analysis using deep neural networks",
    "authors": ["Harika Abburi", "Rajendra Prasath", "Manish Shrivastava", "Suryakanth V Gangashetty."],
    "venue": "In",
    "year": 2016
  }, {
    "title": "Unitary evolution recurrent neural networks. CoRR, abs/1511.06464",
    "authors": ["Martı́n Arjovsky", "Amar Shah", "Yoshua Bengio"],
    "year": 2015
  }, {
    "title": "Multimodal machine learning: A survey and taxonomy",
    "authors": ["Tadas Baltrušaitis", "Chaitanya Ahuja", "LouisPhilippe Morency."],
    "venue": "arXiv preprint arXiv:1705.09406.",
    "year": 2017
  }, {
    "title": "Statistical inference for probabilistic functions of finite state markov chains",
    "authors": ["Leonard E Baum", "Ted Petrie."],
    "venue": "The annals of mathematical statistics, 37(6):1554–1563.",
    "year": 1966
  }, {
    "title": "Real-time speech emotion and sentiment recognition for interactive dialogue systems",
    "authors": ["Dario Bertero", "Farhad Bin Siddique", "Chien-Sheng Wu", "Yan Wan", "Ricky Chan", "Pascale Fung"],
    "year": 2016
  }, {
    "title": "Random forests",
    "authors": ["Leo Breiman."],
    "venue": "Mach. Learn., 45(1):5–32.",
    "year": 2001
  }, {
    "title": "Iemocap: Interactive emotional dyadic motion capture database",
    "authors": ["Carlos Busso", "Murtaza Bulut", "Chi-Chun Lee", "Abe Kazemzadeh", "Emily Mower", "Samuel Kim", "Jeannette Chang", "Sungbok Lee", "Shrikanth S. Narayanan."],
    "venue": "Journal of Language Re-",
    "year": 2008
  }, {
    "title": "Affective computing and sentiment analysis",
    "authors": ["E. Cambria."],
    "venue": "IEEE Intelligent Systems, 31(2):102–107.",
    "year": 2016
  }, {
    "title": "Multimodal sentiment analysis with wordlevel fusion and reinforcement learning",
    "authors": ["Minghai Chen", "Sen Wang", "Paul Pu Liang", "Tadas Baltrušaitis", "Amir Zadeh", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 19th ACM International Conference on",
    "year": 2017
  }, {
    "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
    "authors": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Q. Weinberger", "Claire Cardie."],
    "venue": "CoRR, abs/1606.01614.",
    "year": 2016
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "CoRR, abs/1406.1078.",
    "year": 2014
  }, {
    "title": "Syntactic Structures",
    "authors": ["Noam Chomsky."],
    "venue": "Mouton and Co., The Hague.",
    "year": 1957
  }, {
    "title": "Supportvector networks",
    "authors": ["Corinna Cortes", "Vladimir Vapnik."],
    "venue": "Machine learning, 20(3):273–297.",
    "year": 1995
  }, {
    "title": "Covarepa collaborative voice analysis repository for speech technologies",
    "authors": ["Gilles Degottex", "John Kane", "Thomas Drugman", "Tuomo Raitio", "Stefan Scherer."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,",
    "year": 2014
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L Elman."],
    "venue": "Cognitive science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["A. Graves", "A. r. Mohamed", "G. Hinton."],
    "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645–6649.",
    "year": 2013
  }, {
    "title": "Visual features for context-aware speech recognition",
    "authors": ["Abhinav Gupta", "Yajie Miao", "Leonardo Neves", "Florian Metze."],
    "venue": "CoRR, abs/1712.00489.",
    "year": 2017
  }, {
    "title": "Speech emotion recognition using deep neural network and extreme learning machine",
    "authors": ["Kun Han", "Dong Yu", "Ivan Tashev"],
    "year": 2014
  }, {
    "title": "Learning word-like units from joint audio-visual analysis",
    "authors": ["David F. Harwath", "James R. Glass."],
    "venue": "CoRR, abs/1701.07481.",
    "year": 2017
  }, {
    "title": "TrueskillTM: A bayesian skill rating system",
    "authors": ["Ralf Herbrich", "Tom Minka", "Thore Graepel."],
    "venue": "B. Schölkopf, J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 569–576. MIT Press.",
    "year": 2007
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Comput., 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Bidirectional lstm-crf models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "CoRR, abs/1508.01991.",
    "year": 2015
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Recurrent Neural Networks: Design and Applications, 1st edition",
    "authors": ["L.C. Jain", "L.R. Medsker."],
    "venue": "CRC Press, Inc., Boca Raton, FL, USA.",
    "year": 1999
  }, {
    "title": "Visually grounded learning of keyword prediction from untranscribed speech",
    "authors": ["Herman Kamper", "Shane Settle", "Gregory Shakhnarovich", "Karen Livescu."],
    "venue": "CoRR, abs/1703.08136.",
    "year": 2017
  }, {
    "title": "A new view of language acquisition",
    "authors": ["Patricia K. Kuhl."],
    "venue": "Proceedings of the National Academy of Sciences, 97(22):11850–11857.",
    "year": 2000
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
    "year": 2001
  }, {
    "title": "Reusing neural speech representations for auditory emotion recognition",
    "authors": ["Egor Lakomkin", "Cornelius Weber", "Sven Magg", "Stefan Wermter."],
    "venue": "CoRR, abs/1803.11508.",
    "year": 2018
  }, {
    "title": "A simple way to initialize recurrent networks of rectified linear units",
    "authors": ["Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton."],
    "venue": "CoRR, abs/1504.00941.",
    "year": 2015
  }, {
    "title": "Multimodal local-global ranking fusion for emotion recognition",
    "authors": ["Paul Pu Liang", "Amir Zadeh", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI 2018.",
    "year": 2018
  }, {
    "title": "Efficient lowrank multimodal fusion with modality-specific factors",
    "authors": ["Zhun Liu", "Ying Shen", "Varun Bharadhwaj Lakshminarasimhan", "Paul Pu Liang", "AmirAli Bagher Zadeh", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 56th Annual Meeting of",
    "year": 2018
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "ACL 2016.",
    "year": 2016
  }, {
    "title": "Character-based bidirectional lstm-crf with words and characters for japanese named entity recognition",
    "authors": ["Shotaro Misawa", "Motoki Taniguchi", "Yasuhide Miura", "Tomoko Ohkuma."],
    "venue": "Proceedings of the First Workshop on Subword and Character",
    "year": 2017
  }, {
    "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
    "authors": ["Louis-Philippe Morency", "Rada Mihalcea", "Payal Doshi."],
    "venue": "Proceedings of the 13th international conference on multimodal interfaces, pages 169–176. ACM.",
    "year": 2011
  }, {
    "title": "Latent-dynamic discriminative models for continuous gesture recognition",
    "authors": ["Louis-Philippe Morency", "Ariadna Quattoni", "Trevor Darrell."],
    "venue": "Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE.",
    "year": 2007
  }, {
    "title": "Multimodal deep learning",
    "authors": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng."],
    "venue": "ICML, pages 689–696. Omnipress.",
    "year": 2011
  }, {
    "title": "Deep multimodal fusion for persuasiveness prediction",
    "authors": ["Behnaz Nojavanasghari", "Deepak Gopinath", "Jayanth Koushik", "Tadas Baltrušaitis", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 18th",
    "year": 2016
  }, {
    "title": "Emergence of multimodal action representations from neural network selforganization",
    "authors": ["German I. Parisi", "Jun Tani", "Cornelius Weber", "Stefan Wermter."],
    "venue": "Cognitive Systems Research, 43:208 – 221.",
    "year": 2017
  }, {
    "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
    "authors": ["Sunghyun Park", "Han Suk Shim", "Moitreya Chatterjee", "Kenji Sagae", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 16th Interna-",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP, volume 14, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis",
    "authors": ["Hai Pham", "Thomas Manzini", "Paul Pu Liang", "Barnabas Poczos."],
    "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language",
    "year": 2018
  }, {
    "title": "Deep convolutional neural network textual features and multiple kernel learning for utterancelevel multimodal sentiment analysis",
    "authors": ["Soujanya Poria", "Erik Cambria", "Alexander Gelbukh."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods",
    "year": 2015
  }, {
    "title": "Context-dependent sentiment analysis in user-generated videos",
    "authors": ["Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Navonil Majumder", "Amir Zadeh", "Louis-Philippe Morency."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Compu-",
    "year": 2017
  }, {
    "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
    "authors": ["Soujanya Poria", "Iti Chaturvedi", "Erik Cambria", "Amir Hussain."],
    "venue": "Data Mining (ICDM), 2016 IEEE 16th International Conference on, pages 439–448. IEEE.",
    "year": 2016
  }, {
    "title": "Hidden conditional random fields",
    "authors": ["Ariadna Quattoni", "Sybor Wang", "Louis-Philippe Morency", "Michael Collins", "Trevor Darrell."],
    "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 29(10):1848–1852.",
    "year": 2007
  }, {
    "title": "Extending long short-term memory for multi-view structured learning",
    "authors": ["Shyam Sundar Rajagopalan", "Louis-Philippe Morency", "Tadas Baltrušaitis", "Goecke Roland."],
    "venue": "European Conference on Computer Vision.",
    "year": 2016
  }, {
    "title": "Deep convolutional neural networks for sentiment analysis of short texts",
    "authors": ["Cı́cero Nogueira dos Santos", "Maira Gatti"],
    "venue": "In COLING,",
    "year": 2014
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["M. Schuster", "K.K. Paliwal."],
    "venue": "Trans. Sig. Proc., 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 Conference on",
    "year": 2013
  }, {
    "title": "Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition",
    "authors": ["Hagen Soltau", "Hank Liao", "Hasim Sak."],
    "venue": "CoRR, abs/1610.09975.",
    "year": 2016
  }, {
    "title": "Multi-view latent variable discriminative models for action recognition",
    "authors": ["Yale Song", "Louis-Philippe Morency", "Randall Davis."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2120–2127. IEEE.",
    "year": 2012
  }, {
    "title": "Action recognition by hierarchical sequence summarization",
    "authors": ["Yale Song", "Louis-Philippe Morency", "Randall Davis."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3562–3569.",
    "year": 2013
  }, {
    "title": "Multimodal learning with deep boltzmann machines",
    "authors": ["Nitish Srivastava", "Ruslan R Salakhutdinov."],
    "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2222–2230. Cur-",
    "year": 2012
  }, {
    "title": "Look, listen, and decode: Multimodal speech recognition with images",
    "authors": ["F. Sun", "D. Harwath", "J. Glass."],
    "venue": "2016 IEEE Spoken Language Technology Workshop (SLT), pages 573–578.",
    "year": 2016
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "CoRR, abs/1409.3215.",
    "year": 2014
  }, {
    "title": "The global landscape of cognition: hierarchical aggregation as an organizational principle of human cortical networks and functions",
    "authors": ["P. Taylor", "J.N. Hobbs", "J. Burroni", "H.T. Siegelmann."],
    "venue": "Scientific Reports, 5:18112 EP –.",
    "year": 2015
  }, {
    "title": "Learning factorized multimodal representations",
    "authors": ["Yao-Hung Hubert Tsai", "Paul Pu Liang", "Amir Zadeh", "Louis-Philippe Morency", "Ruslan Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1806.06176.",
    "year": 2018
  }, {
    "title": "Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis",
    "authors": ["Haohan Wang", "Aaksha Meghawat", "Louis-Philippe Morency", "Eric P Xing."],
    "venue": "arXiv preprint arXiv:1609.05244.",
    "year": 2016
  }, {
    "title": "A survey on multi-view learning",
    "authors": ["Chang Xu", "Dacheng Tao", "Chao Xu."],
    "venue": "arXiv preprint arXiv:1304.5634.",
    "year": 2013
  }, {
    "title": "Speaker identification on the scotus corpus",
    "authors": ["Jiahong Yuan", "Mark Liberman."],
    "venue": "Journal of the Acoustical Society of America, 123(5):3878.",
    "year": 2008
  }, {
    "title": "Tensor fusion network for multimodal sentiment analysis",
    "authors": ["Amir Zadeh", "Minghai Chen", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency."],
    "venue": "Empirical Methods in Natural Language Processing, EMNLP.",
    "year": 2017
  }, {
    "title": "Memory fusion network for multiview sequential learning",
    "authors": ["Amir Zadeh", "Paul Pu Liang", "Navonil Mazumder", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency."],
    "venue": "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence.",
    "year": 2018
  }, {
    "title": "Multi-attention recurrent network for human communication comprehension",
    "authors": ["Amir Zadeh", "Paul Pu Liang", "Soujanya Poria", "Prateek Vij", "Erik Cambria", "Louis-Philippe Morency."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelli-",
    "year": 2018
  }, {
    "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
    "authors": ["Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency."],
    "venue": "IEEE Intelligent Systems, 31(6):82–88.",
    "year": 2016
  }],
  "id": "SP:a81a8cf811540be14f7840ef6939a3d7b901a8e3",
  "authors": [{
    "name": "Paul Pu Liang",
    "affiliations": []
  }, {
    "name": "Ziyin Liu",
    "affiliations": []
  }, {
    "name": "Amir Zadeh",
    "affiliations": []
  }, {
    "name": "Louis-Philippe Morency",
    "affiliations": []
  }],
  "abstractText": "Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Crossmodal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",
  "title": "Multimodal Language Analysis with Recurrent Multistage Fusion"
}