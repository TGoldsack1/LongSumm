{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2486–2495 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2486"
  }, {
    "heading": "1 Introduction",
    "text": "Entity Linking (EL) systems ground entity mentions in text to entries in Knowledge Bases (KB), such as Wikipedia (Mihalcea and Csomai, 2007). Recently, the task of Cross-lingual Entity Linking (XEL) has gained attention (McNamee et al., 2011; Ji et al., 2015; Tsai and Roth, 2016) with the goal of grounding entity mentions written in any language to the English Wikipedia. For instance, Figure 1 shows a Tamil (a language with >70 million speakers) and an English mention (shown [enclosed])\n1Code at www.github.com/shyamupa/xelms\nand their mention contexts. XEL involves grounding the Tamil mention (which translates to ‘Liverpool’) to the football club Liverpool_F.C., and not the city or the university. XEL enables knowledge acquisition directly from documents in any language, without resorting to machine translation.\nTraining an EL model requires grounded mentions, i.e. mentions of entities that are grounded to a Knowledge Base (KB), as supervision (Figure 1). While millions of such mentions are available in English, by virtue of hyperlinks in the English Wikipedia, this is not the case for most languages. This makes learning XEL models challenging, especially for languages with limited resources (e.g., the Tamil Wikipedia is only 1% of the English Wikipedia in size). To overcome this challenge, it is desirable to augment the limited contextual evidence available in the target language with evidence from high-resource languages like English.\nWe propose XELMS (XEL with Multilingual Supervision) (§2), the first approach that fulfills the above desiderata by using multilingual supervision to train an XEL model. XELMS represents the mention contexts of the same entity from different languages in the same semantic space using a single context encoder (§2.1). Language-agnostic entity representations are jointly learned with the relevant mention context representations, so that an entity and its context share similar representations.\nAdditionally, by encoding freely available structured knowledge, like fine-grained entity types, the entity and context representations can be further improved (§2.2).\nThe ability to use multilingual supervision enables XELMS to learn XEL models for target languages with limited resources by exploiting freely available supervision from high resource languages (like English). We show that XELMS outperforms existing state-of-the-art approaches that only use target language supervision, across 3 benchmark datasets in 8 languages (§5.1). Moreover, while previous XEL models (McNamee et al., 2011; Tsai and Roth, 2016) train separate models for different languages, XELMS can train a single model for performing XEL in multiple languages (§5.2).\nOne of the goals of XEL is to enable understanding of languages with limited resources. We provide experimental analyses in two such settings. In the zero-shot setting (§6.1), where no supervision is available in the target language, we show that the good performance of zero-shot XEL approaches (Sil et al., 2018) can be attributed to the use of prior probabilities. These probabilities are computed from large amount of grounded mentions, which are not available in realistic zero-shot settings. In the low-resource setting (§6.2), where some supervision is available in the target language,\nwe show that even when only a fraction of the available supervision in the target language is provided, XELMS can achieve competitive performance by exploiting supervision from English.\nThe contributions of our work are, • A new XEL approach, XELMS, that learns a\nXEL model for a language with limited resources by exploiting additional supervision from a high-resource language like English. • XELMS can also train a single XEL model\nfor multiple languages jointly, which we show improves on separately trained models. • Analysis of XEL approaches in the zero-shot\nand low-resource settings. Our analysis reveals that in realistic scenarios, zero-shot XEL is not as effective as previously shown. We also show that in low-resource settings jointly training with English leads to better utilization of target language supervision.\n2 Cross-lingual EL with XELMS\nGiven a mention m in a document D written in any language, XEL involves linkingm to its gold entity e∗ in a KB, K = {e1, · · · , en}.\nAn overview of XELMS is shown in Figure 2a. XELMS computes the probability, Pcontext(e | m), of a mention m referring to entity e ∈ K using a mention context vector g ∈ Rh representing\nm’s context, and an entity vector e ∈ Rh, representing the entity e ∈ K (one vector per entity). XELMS can also incorporate structured knowledge like fine-grained entity types (§2.2) using a multitask learning approach (Caruana, 1998), by learning a type vector t ∈ Rh for each possible type t (e.g., sports_team) associated with the entity e. The entity vector e, context vector g and the type vector t are jointly trained, and interact through appropriately defined pairwise loss terms – an EntityContext loss (EC-LOSS), Type-Entity loss (TELOSS) and a Type-Context loss (TC-LOSS).\nThe mention context vector g is generated by a mention context encoder (§2.1), shown in Figure 2b. The mention context of m in a document D consists of: (a) neighboring words around the mention, which we refer to as its local context and, (b) surfaces of other mentions appearing in D, which we refer as its document context. XELMS is trained using grounded mentions in multiple languages (English and Tamil in Figure 2a), which can be derived from Wikipedia (§4.1)."
  }, {
    "heading": "2.1 Mention Context Representation",
    "text": "To learn from mention contexts in multiple languages, we generate mention context representations using a language-agnostic mention context encoder. An overview of the mention context encoder is shown in Figure 2b. Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders.\nMultilingual Word Embeddings (Ammar et al., 2016b; Smith et al., 2017; Duong et al., 2017) jointly encode words in multiple (≥2) languages in the same vector space such that semantically similar words in the same language, and translationally equivalent words in different languages are close (per cosine similarity). Multilingual embeddings generalize bilingual embeddings, which do the same for two languages only.\nWe use FASTTEXT (Bojanowski et al., 2017; Smith et al., 2017), which aligns monolingual embeddings of multiple languages in the same space using a small dictionary (∼2500 pairs) from each language to English. Both monolingual embeddings and the dictionary can be easily obtained for languages with limited resources. We denote the multilingual word embeddings for a set of tokens {w1, w2, · · · , wn} by w1:n = {w1,w2, · · · ,wn}, where each wi ∈ Rd.\nReLU\nOi <latexit sha1_base64=\"8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64=\"8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64=\"8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64=\"8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit>\nr <latexit sha1_base64=\"juw6eQUUFM3i37Bw8Ysq5tbV1Ao=\">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64=\"juw6eQUUFM3i37Bw8Ysq5tbV1Ao=\">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64=\"juw6eQUUFM3i37Bw8Ysq5tbV1Ao=\">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64=\"juw6eQUUFM3i37Bw8Ysq5tbV1Ao=\">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit>\nAverage Pooling\n… …wi <latexit sha1_base64=\"UBwr2R+8MpY5RFOsjT7bXwdRlsw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64=\"UBwr2R+8MpY5RFOsjT7bXwdRlsw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64=\"UBwr2R+8MpY5RFOsjT7bXwdRlsw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64=\"UBwr2R+8MpY5RFOsjT7bXwdRlsw=\">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit>\nMultilingual Token Embeddings of the Context on the Right\n…\nk <latexit sha1_base64=\"FN4g9GFe+3ziaTgM0DMwdUeUKp0=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64=\"FN4g9GFe+3ziaTgM0DMwdUeUKp0=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64=\"FN4g9GFe+3ziaTgM0DMwdUeUKp0=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64=\"FN4g9GFe+3ziaTgM0DMwdUeUKp0=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit>\nCNN CNN ReLU\nFigure 3: Local Context Encoder, for the right context. Figure 2b shows how it fits inside Mention Context Encoder.\nLocal Context Representation The local context of a mention m, spanning tokens i to j, consists of left context (tokens i −W to j) and right context (tokens i to j +W ). For example, for the mention [Liverpool] in Figure 2b, the left and right contexts are “Everton won against Liverpool” and “Liverpool in a FA Cup match” respectively. The local context encoder (Figure 3) encodes the left and the right contexts into vectors l ∈ Rh and r ∈ Rh using a convolutional neural network (CNN). These two vectors are then combined to generate the local context vector c ∈ Rh (Figure 2b).\nThe CNN convolves continuous spans of k tokens using a filter matrix F ∈ Rkd×h to project the concatenation (⊕ operator) of the token embeddings in the span. The resulting vector is passed through a ReLU unit to generate convolutional output Oi. The outputs {Oi} are pooled by averaging,\nOi = RELU(F T (wi ⊕ · · · ⊕wi+k−1)) (1) ENC(w1:n) = AVG(O1, · · · ,On−k+1) (2)\nLeft and right context vectors l and r are computed using respective ENC(.) layers,\nl = ENCleft(wi−W · · ·wj) (3) r = ENCright(wi · · ·wj+W ) (4)\nThese vectors together generate the local context vector c = F2h,h(l ⊕ r). Here Fdi,do : vi → vo denotes a feed-forward layer that takes vi ∈ Rdi as input, and outputs vo ∈ Rdo .\nDocument Context Representation Presence of certain mentions in a document can help disambiguate other mentions. For example, “Suarez”, “Everton” in a document can help disambiguate “Liverpool”. To incorporate this, we define the\ndocument context dm of a mention m appearing in document D to be the bag of all other mentions in D. We encode dm into a dense document context vector d ∈ Rh by a feed-forward layer d = F|V |,h(dm). Here V is the set containing all mention surfaces seen during training. When training jointly over multiple languages, V consists of mention surfaces seen in all languages (e.g. all English and Tamil mention surfaces) during training. This enables parameter sharing by embedding mention surfaces in different languages in the same low-dimensional space. The local and document context vectors c and d are combined to get the mention context vector g = F2h,h(c⊕ d).\nContext Conditional Probability We compute the probability of a mention m linking to entity e using its context vector g and the entity vector e,\nPcontext(e | m) = exp(gTe)∑\ne′∈C(m) exp(gTe′)\n(5)\nwhere C(m) denotes all candidate entities of the mention m (§3.1 explains how C(m) is generated). We minimize the negative log-likelihood of Pcontext(e | m) with respect to the gold entity e∗ against the candidate entities C(m), and call it the Entity-Context loss (EC-LOSS),\nEC-LOSS =− log Pcontext(e ∗ | m)∑\ne′∈C(m) Pcontext(e′ | m)\n(6)"
  }, {
    "heading": "2.2 Including Type Information",
    "text": "Incorporating the fine-grained types of a mention m can help rank entities of the appropriate type higher than others (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018). For instance, knowing the correct type of mention [Liverpool] as sports_team and constraining linking to entities with the relevant type, encourages disambiguation to the correct entity.\nTo make the mention context representation g type-aware, we predict the set of fine-grained types of m, T(m) = {t1, ..., t|T(m)|} using g. Each ti belongs to a pre-defined type vocabulary Γ.2 The probability of a type t belonging to T(m) given the mention context is defined as P(t | m) =σ(tTg), where σ is the sigmoid function and t is the learnable embedding for type t.\n2We use the type vocabulary Γ from Ling and Weld (2012), which contains 112 fine-grained types (|Γ| = 112)\nWe define a Type-Context loss (TC-LOSS) as,\nTC-LOSS = BCE(T(m),P(t | m)) (7)\nwhere BCE is the Binary Cross-Entropy Loss, − ∑\nt∈T(m)\nlog P(t | m)− ∑\nt6∈T(m)\nlog(1− P(t | m))\nWe also incorporate the entity-type information in the entity representations, and define a similar Type-Entity loss (TE-LOSS).\nTo identify the gold types T(m) of a mention m, we make the distant supervision assumption (same as Ling et al. (2015)) and assign the types of the gold entity e∗ to be the types of the mention. Gold fine-grained types of the entities can be acquired from resources like Freebase (Bollacker et al., 2008) or YAGO (Hoffart et al., 2013)."
  }, {
    "heading": "3 Training and Inference",
    "text": "We explain how XELMS generates candidate entities, performs inference, and combines the different training losses."
  }, {
    "heading": "3.1 Candidate Generation",
    "text": "Candidate generation identifies a small number of plausible entities for a mention m to avoid brute force comparison with all KB entities. Given m, candidate generation outputs a list of candidate entities C(m) = {e1, e2, · · · , eK} of size at most K (we use K=20), each associated with a prior probability Pprior(ei | m) indicating the probability of m referring to ei, given only m’s surface. Pprior is estimated from counts over the training mentions.\nWe adopt Tsai and Roth (2016)’s candidate generation strategy with some minor modifications (Appendix A). Using other approaches like CrossWikis (Spitkovsky and Chang, 2012), lead to consistently worse recall. We note that transliteration based candidate generation (McNamee et al., 2011; Pan et al., 2017; Tsai and Roth, 2018; Upadhyay et al., 2018) can further improve recall."
  }, {
    "heading": "3.2 Inference",
    "text": "We combine the context conditional entity probability Pcontext(e | m) (eq. 5) and prior probability Pprior(e | m) by taking their union:\nPmodel(e | m) = Pprior(e | m) + Pcontext(e | m) − Pprior(e | m)× Pcontext(e | m)\nInference for the mention m picks the entity,\nê = arg max e∈C(m)\nPmodel(e | m) (8)"
  }, {
    "heading": "3.3 Training Objective",
    "text": "When only training the mention context encoder and entity vectors, we minimize the EC-LOSS averaged over all training mentions. When using the two type-aware losses, we minimize a weighted sum of EC-LOSS, TE-LOSS, and TC-LOSS, using the weighing scheme of Kendall et al. (2018),\nEC-LOSS 2λ2EC + TE-LOSS 2λ2TE + TC-LOSS 2λ2TC + log λ2EC + log λ 2 TE + log λ 2 TC\n(9)\nHere λi are learnable scalar weighing parameters, and the respective 1\n2λ2i and log λ2i term ensure that\nλ2i does not grow unboundedly. This way, the model learns the relative weight for each loss term.\nDuring training, mentions from different languages are mixed using inverse-ratio mini-batch mixing strategy. That is, if two languages have training data sizes proportional to α : β, at any time during training, mini-batches seen from them are in the ratio 1α : 1 β . This strategy prevents languages with more training data from overwhelming languages with less training data. Though simple, we found this strategy yielded good results."
  }, {
    "heading": "4 Experimental Setup",
    "text": "We briefly describe the training and evaluation datasets, and the previous XEL approaches from the literature used in our comparison."
  }, {
    "heading": "4.1 Training Mentions",
    "text": "Following previous work, we use hyperlinks from Wikipedia (dumps dated 05/20/2017) as our source of grounded mentions for supervision. Wikipedias in different languages have different pages for the same entity, which are resolved by using interlanguage links (e.g., page 利物浦 in Chinese Wikipedia resolves to Liverpool in English). Training mentions statistics are shown in Table 1.\nWe evaluate on 8 languages – German (de), Spanish (es), Italian (it), French (fr), Chinese (zh), Arabic (ar), Turkish (tr) and Tamil (ta), each of which has varying amount of grounded mentions from the respective Wikipedia (Table 1). We note that our method is applicable to any of the 293 Wikipedia languages as a target language."
  }, {
    "heading": "4.2 Evaluation Datasets",
    "text": "We evaluate XELMS on the following benchmark datasets, spanning 8 different languages, thus providing an extensive evaluation.\nMcN-Test dataset from (McNamee et al., 2011). The test set was collected by using parallel document collections, and then crowd-sourcing the ground truths. All the test mentions in this dataset consists of person-names only.\nTH-Test A subset of the dataset used in (Tsai and Roth, 2016), derived from Wikipedia.3 The mentions in the dataset fall in two categories – easy and hard, where hard mentions are those for which the most likely candidate according to the prior probability (i.e., arg max Pprior(e | m)) is not the correct title. Indeed, most Wikipedia mentions can be correctly linked by selecting the most likely candidate (Ratinov et al., 2011). We use all the hard mentions from Tsai and Roth (2016)’s test splits for each language, and collectively call this subset TH-TEST.\nTAC15-Test TAC 2015 (Ji et al., 2015) dataset for Chinese and Spanish. It contains documents from discussion forum articles and news.\nWe evaluate all models using linking accuracy on gold mentions, and assume gold mentions are provided at test time. Table 2 summarizes the different domains of the evaluation datasets.\nTuning We avoid any dataset-specific tuning, instead tuning on a development set and applying the same parameters across all datasets. All tunable parameters were tuned on a development set containing the hard mentions from the train split released by Tsai and Roth (2016). We refer the reader to Appendix B for details on tuning.\n3Pan et al. (2017) also created a dataset using Wikipedia, but did not categorize mentions like Tsai and Roth (2016). Preliminary experiments on their dataset showed XELMS consistently beat Pan et al. (2017)’s model. We chose TH-TEST for more controlled experiments."
  }, {
    "heading": "4.3 Comparative Approaches",
    "text": "We compare against the following state-of-the-art (SoTA) approaches, described with the language from which they use mention contexts in (.),\nTsai and Roth (2016) (Target Only) trains a separate XEL model for each language using mention contexts from the target language Wikipedia only. Current SoTA on TH-TEST.\nPan et al. (2017) (English Only) uses entity coherence statistics from English Wikipedia and the document context of a mention for XEL. Current SoTA on MCN-TEST, except for Italian and Turkish, for which it’s McNamee et al. (2011).\nSil et al. (2018) (English Only) uses multilingual embeddings to transfer a pre-trained English entity linking model to perform XEL for Spanish and Chinese. Prior probabilities Pprior are used as a feature. Current SoTA on TAC15-TEST."
  }, {
    "heading": "5 Experiments",
    "text": "We show that: (a) XELMS can train a better entity linking model for a target language on various benchmark datasets by exploiting additional data from a high resource language like English (§5.1). (b) XELMS can train a single XEL model for multiple related languages and improve upon separately trained models (§5.2). (c) Adding additional type information as multi-task loss to XELMS further improves performance (§5.3).\nIn all tables, we report the linking accuracy of XELMS, averaged over 5 different runs, and mark with ∗ the statistical significance (p < 0.01) of the best result (shown bold) against the state-of-the-art (SoTA) using Student’s one-sample t-test."
  }, {
    "heading": "5.1 Monolingual and Joint Models",
    "text": "In Table 3 and 4 we compare XELMS(mono), which uses monolingual supervision in the target language only, and XELMS(joint), which uses supervi-\nsion from English in addition to the monolingual supervision, with the state-of-the-art approaches.\nWe see that XELMS(mono) achieves similar or slightly better scores than respective SoTA on all datasets. The SoTA for MCN-TEST in Turkish and Chinese enhances the model by using transliteration for candidate generation, explaining their superior performance. XELMS(joint) performs substantially better than XELMS(mono) on all datasets (Table 3 and 4), proving that using additional supervision from a high resource language like English leads to better linking performance. In particular, XELMS(joint) outperforms the SoTA on all languages in TH-TEST, on Spanish in TAC15-Test, and on 4 of the 7 languages in MCN-TEST."
  }, {
    "heading": "5.2 Multilingual Training",
    "text": "XELMS is the first approach that can train a single XEL model for multiple languages. To demonstrate this capability, we train a model, henceforth referred as XELMS(multi), jointly on 5 related languages – Spanish, German, French, Italian and En-\nglish. We compare XELMS(multi) to the respective XELMS(joint) model for each language.\nTable 4 and 5, show that XELMS(multi) is better (or at par) than XELMS(joint) on all datasets. This shows that XELMS(multi) can making more efficient use of available supervision in related languages than previous approaches which trained separate models per language."
  }, {
    "heading": "5.3 Adding Fine-grained Type Information",
    "text": "To study the effect of adding fine-grained type information, in Table 4 we compare XELMS(mono) and XELMS(joint) to XELMS(mono+type) and XELMS(joint+type) respectively, which are versions of XELMS(mono) and XELMS(joint) trained using the two type-aware losses.\nXELMS(mono+type) and XELMS(joint+type) both improve compared to XELMS(mono) and XELMS(joint) on MCN-TEST and TH-TEST (Table 6 vs Table 3), showing the benefit of using structured knowledge in the form of fine-grained types. Similar trends are also seen on TAC15TEST (Table 4), where XELMS(joint+type) improves on the SoTA for Spanish and Chinese."
  }, {
    "heading": "6 Experiments with Limited Resources",
    "text": "The key motivation of XELMS is to exploit supervision from high-resource languages like English to aid XEL for languages with limited resources. In this section, we examine two such scenarios, (a) Zero-shot setting i.e., no supervision available in the target language. Our analysis reveals the limitations of zero-shot XEL approaches and finds that the prior probabilities play an important role in achieving good performance (§6.1), which are unavailable in realistic zero-shot scenarios. (b) Low-resource setting i.e., some supervision available in the target language. We show that\nby combining supervision from a high-resource language, like English, XELMS can achieve competitive performance with a fraction of available supervision in the target language (§6.2)."
  }, {
    "heading": "6.1 Zero-shot Setting",
    "text": "We first explain how XELMS can perform zero-shot XEL, the implications of our zero-shot setting, and how it is more realistic than previous work.\nZero-shot XEL with XELMS XELMS performs zero-shot XEL by training a model using English supervision and multilingual embeddings for English, and directly applying it to the test data in another language using the respective multilingual word embedding instead of English embeddings.\nNo Prior Probabilities Prior probabilities (or prior), i.e., Pprior have been shown to be a reliable indicator of the correct disambiguation in entity linking (Ratinov et al., 2011; Tsai and Roth, 2016). These probabilities are estimated from counts over the training mentions in the target language. In the absence of training data for the target language, as in the zero-shot setting, these prior probabilities are not available to an XEL model.\nComparison to Previous Work The only other model capable of zero-shot XEL is that of Sil et al. (2018). However, Sil et al. (2018) use prior probabilities and coreference chains for the target language in their zero-shot experiments, both of which will not be available in a realistic zero-shot scenario. Compared to Sil et al. (2018), we evaluate the performance of zero-shot XEL in more realistic setting, and show it is adversely affected by absence of prior probabilities.\nIs zero-shot XEL really effective? To evaluate the effectiveness of the zero-shot XEL approach, we perform zero-shot XEL using XELMS on all datasets. Table 7 shows zero-shot XEL results on all datasets, both with and without using the prior during inference. Note that zero-shot XEL (with prior) is close to SoTA (Sil et al. (2018)) on TAC15-TEST, which also uses the prior for zeroshot XEL. However, for zero-shot XEL (without prior) performance drops by more than 20% for TAC15-Test, 2.4% for TH-Test and by 2.1% for McN-Test. This indicates that zero-shot XEL is not effective in a realistic zero-shot setting (i.e., when the prior is unavailable for inference).\nWe found that the prior is indeed a strong indicator of the correct disambiguation. For instance, simply selecting the the most likely candidate using the prior for TAC15-TEST achieved 77.2% and 78.8% for Spanish and Chinese respectively. It is interesting to note that both zero-shot XEL (with or without prior) perform worse than the best possible model on TH-TEST, because TH-TEST was constructed to ensure prior probabilities are not strong indicators (Tsai and Roth, 2016). On MCNTEST, we found that an average of 75.9% mentions have only one (the correct) candidate, making them trivial to link, regardless of the absence of priors.\nThe results show that most of the XEL performance in zero-shot settings can be attributed to availability of prior probabilities for the candidates. It is evident that zero-shot XEL in a realistic setting (i.e., when prior probabilities are not available) is still a challenging problem."
  }, {
    "heading": "6.2 Low-resource Setting",
    "text": "We analyze the behavior of XELMS in a lowresource setting, i.e. when some supervision is available in the target language. The aim of this setting is to estimate how much supervision from\nthe target language is needed to get reasonable performance when using it jointly with supervision from English. To discount the effect of prior probabilities, we report all results without the prior.\nFigure 4 plots results on the TH-Test dataset when training a XELMS(joint) model by gradually increasing the number of mention contexts for target language L (= Spanish, Chinese and Turkish) that are available for supervision. Figure 4 also shows the best results achieved using all available target language supervision (denoted by Lbest). For comparison with the mono-lingually supervised model, we also plot the performance of XELMS(mono), which only uses the target language supervision.\nFigure 4 shows that after training on 0.75M mentions from Turkish and Chinese (and 1.0M mentions from Spanish), the XELMS(joint) model is within 2-3% of the respective L-best model which uses all training mentions in the target language, indicating that XELMS(joint) can reach competitive performance even with a fraction of the full target language supervision. For comparison, a XELMS(mono) model trained on the same number of training mentions is 5-10% behind the respective XELMS(joint) model, showing better utilization of target language supervision by XELMS(joint)."
  }, {
    "heading": "7 Related Work",
    "text": "Existing approaches have taken two main directions to obtain supervision for learning XEL models — (a) using mention contexts appearing in the target language (McNamee et al., 2011; Tsai and Roth, 2016), or (b) using mention contexts appearing only in English (Pan et al., 2017; Sil et al., 2018). We describe these directions and their limitations below, and explain how XELMS overcomes these limitations.\nMcNamee et al. (2011) use annotation projection via parallel corpora to generate mention contexts in the target language, while Tsai and Roth (2016) learns separate XEL models for each language and only use mention contexts in the target language. Both these approach have scalability issues for languages with limited resources. Another limitation of these approaches is that they train separate models for each language, which is inefficient when working with multiple languages. XELMS overcomes these limitations as it can use mention context from multiple languages simultaneously, and train a single model.\nOther approaches only use mention contexts from English. While Pan et al. (2017) compute entity coherence statistics from English Wikipedia, Sil et al. (2018) perform zero-shot XEL for Chinese and Spanish by using multilingual embeddings to transfer a pre-trained English EL model. However, our work suggests that mention contexts in the target language should also be used, if available. Indeed, a recent study (Lewoniewski et al., 2017) found that for language sensitive topics, the quality of information can be better in the relevant language version of Wikipedia than the English version. Our analysis also shows that zero-shot XEL approaches like that of Sil et al. (2018) are not effective in realistic zero-shot scenarios where good prior probabilities are unlikely to be available. In such cases, we showed that combining supervision available in the target language with supervision from a high-resource language like English can yield significant performance improvements.\nThe architecture of XELMS is inspired by several monolingual entity linking systems (FrancisLandau et al., 2016; Nguyen et al., 2016; Gupta et al., 2017), approaches that use type information to aid entity linking (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018), and the recent success of multilingual embeddings for several tasks (Ammar et al., 2016a; Duong et al., 2017)."
  }, {
    "heading": "8 Conclusion",
    "text": "We introduced XELMS, an approach that can combine supervision from multiple languages to train an XEL model. We illustrate its benefits through extensive evaluation on different benchmarks. XELMS is also the first approach that can train a single model for multiple languages, making more efficient use of available supervision than previous approaches which trained separate models.\nOur analysis sheds light on the poor performance of zero-shot XEL in realistic scenarios where the prior probabilities for candidates are unlikely to exist, in contrast to findings in previous work that focused on high-resource languages. We also show how in low-resource settings, XELMS makes it possible to achieve competitive performance even when only a fraction of the available supervision in the target language is provided.\nSeveral future research directions remain open. For all XEL approaches, the task of candidate generation is currently limited by existence of a target language Wikipedia and remains a key challenge. A joint inference framework which enforces coherent predictions (Cheng and Roth, 2013; Globerson et al., 2016; Ganea and Hofmann, 2017) could also lead to further improvements for XEL. Similar techniques can be applied to other information extraction tasks like relation extraction to extend them to multilingual settings."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank Snigdha Chaturvedi, Anne Cocos, Stephen Mayhew, ChenTse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch, John Hewitt and the anonymous EMNLP reviewers for their useful comments and suggestions.\nThis work was supported by Contract HR001115-2-0025 and Agreement HR0011-15-2-0023 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government."
  }],
  "year": 2018,
  "references": [{
    "title": "Many Languages, One Parser",
    "authors": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."],
    "venue": "Transactions of the Association for Computational Linguistics, volume 4.",
    "year": 2016
  }, {
    "title": "Massively Multilingual Word Embeddings",
    "authors": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1602.01925.",
    "year": 2016
  }, {
    "title": "Enriching Word Vectors with Subword Information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics, volume 5.",
    "year": 2017
  }, {
    "title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "Proc. of ACM SIGMOD.",
    "year": 2008
  }, {
    "title": "Multitask Learning",
    "authors": ["Rich Caruana."],
    "venue": "Learning to Learn, pages 95–133. Springer.",
    "year": 1998
  }, {
    "title": "Relational Inference for Wikification",
    "authors": ["Xiao Cheng", "Dan Roth."],
    "venue": "Proc. of EMNLP.",
    "year": 2013
  }, {
    "title": "Multilingual Training of Crosslingual Word Embeddings",
    "authors": ["Long Duong", "Hiroshi Kanayama", "Tengfei Ma", "Steven Bird", "Trevor Cohn."],
    "venue": "Proc. of EACL.",
    "year": 2017
  }, {
    "title": "Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks",
    "authors": ["Matthew Francis-Landau", "Greg Durrett", "Dan Klein."],
    "venue": "Proc. of NAACL-HLT.",
    "year": 2016
  }, {
    "title": "Deep Joint Entity Disambiguation with Local Neural Attention",
    "authors": ["Octavian-Eugen Ganea", "Thomas Hofmann."],
    "venue": "Proc. of EMNLP.",
    "year": 2017
  }, {
    "title": "Collective Entity Resolution with Multi-Focal Attention",
    "authors": ["Amir Globerson", "Nevena Lazic", "Soumen Chakrabarti", "Amarnag Subramanya", "Michael Ringaard", "Fernando Pereira."],
    "venue": "Proc. of ACL.",
    "year": 2016
  }, {
    "title": "Entity Linking via Joint Encoding of Types, Descriptions, and Context",
    "authors": ["Nitish Gupta", "Sameer Singh", "Dan Roth."],
    "venue": "Proc. of EMNLP.",
    "year": 2017
  }, {
    "title": "YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia",
    "authors": ["Johannes Hoffart", "Fabian M. Suchanek", "Klaus Berberich", "Gerhard Weikum."],
    "venue": "Proc. of IJCAI.",
    "year": 2013
  }, {
    "title": "Overview of TAC-KBP2015 Tri-lingual Entity Discovery and Linking",
    "authors": ["Heng Ji", "Joel Nothman", "Ben Hachey", "Radu Florian."],
    "venue": "Text Analysis Conference.",
    "year": 2015
  }, {
    "title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
    "authors": ["Alex Kendall", "Yarin Gal", "Roberto Cipolla."],
    "venue": "Proc. of CVPR.",
    "year": 2018
  }, {
    "title": "Relative Quality and Popularity Evaluation of Multilingual Wikipedia Articles",
    "authors": ["Włodzimierz Lewoniewski", "Krzysztof Wecel", "Witold Abramowicz."],
    "venue": "Journal of Informatics.",
    "year": 2017
  }, {
    "title": "Design Challenges for Entity Linking",
    "authors": ["Xiao Ling", "Sameer Singh", "Daniel S Weld."],
    "venue": "Transactions of the Association for Computational Linguistics, volume 3.",
    "year": 2015
  }, {
    "title": "Fine-grained Entity Recognition",
    "authors": ["Xiao Ling", "Daniel S Weld."],
    "venue": "Proc. of AAAI.",
    "year": 2012
  }, {
    "title": "CrossLanguage Entity Linking",
    "authors": ["Paul McNamee", "James Mayfield", "Dawn Lawrie", "Douglas W Oard", "David S Doermann."],
    "venue": "Proc. of IJCNLP.",
    "year": 2011
  }, {
    "title": "Wikify!: Linking Documents to Encyclopedic Knowledge",
    "authors": ["Rada Mihalcea", "Andras Csomai."],
    "venue": "Proc. of CIKM.",
    "year": 2007
  }, {
    "title": "Joint Learning of Local and Global Features for Entity Linking via Neural Networks",
    "authors": ["Thien Huu Nguyen", "Nicolas Fauceglia", "Mariano Rodriguez Muro", "Oktie Hassanzadeh", "Alfio Massimiliano Gliozzo", "Mohammad Sadoghi."],
    "venue": "Proc. of COLING.",
    "year": 2016
  }, {
    "title": "Crosslingual Name Tagging and Linking for 282 Languages",
    "authors": ["Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji."],
    "venue": "Proc. of ACL.",
    "year": 2017
  }, {
    "title": "DeepType: Multilingual Entity Linking by Neural Type System Evolution",
    "authors": ["Jonathan Raiman", "Olivier Raiman."],
    "venue": "Proc. of AAAI.",
    "year": 2018
  }, {
    "title": "Local and Global Algorithms for Disambiguation to Wikipedia",
    "authors": ["Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson."],
    "venue": "Proc. of ACL-HLT.",
    "year": 2011
  }, {
    "title": "Neural Cross-lingual Entity Linking",
    "authors": ["Avirup Sil", "Gourab Kundu", "Radu Florian", "Wael Hamza."],
    "venue": "Proc. of AAAI.",
    "year": 2018
  }, {
    "title": "Offline Bilingual Word Vectors, Orthogonal Transformations, and the Inverted Softmax",
    "authors": ["Samuel L Smith", "David HP Turban", "Steven Hamblin", "Nils Y Hammerla."],
    "venue": "Proc. of ICLR.",
    "year": 2017
  }, {
    "title": "A Cross-Lingual Dictionary for English Wikipedia Concepts",
    "authors": ["Valentin I. Spitkovsky", "Angel X. Chang."],
    "venue": "Proc. of LREC.",
    "year": 2012
  }, {
    "title": "Cross-lingual Wikification Using Multilingual Embeddings",
    "authors": ["Chen-Tse Tsai", "Dan Roth."],
    "venue": "Proc. of NAACL.",
    "year": 2016
  }, {
    "title": "Learning Better Name Translation for Cross-Lingual Wikification",
    "authors": ["Chen-Tse Tsai", "Dan Roth."],
    "venue": "Proc. of AAAI.",
    "year": 2018
  }, {
    "title": "Bootstrapping Transliteration with Constrained Discovery for Low-Resource Languages",
    "authors": ["Shyam Upadhyay", "Jordan Kodner", "Dan Roth."],
    "venue": "Proc. of EMNLP.",
    "year": 2018
  }],
  "id": "SP:41abb43d9d809e37946b91b8111c825778dc6e09",
  "authors": [{
    "name": "Shyam Upadhyay",
    "affiliations": []
  }, {
    "name": "Nitish Gupta",
    "affiliations": []
  }],
  "abstractText": "Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-theart. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings.1",
  "title": "Joint Multilingual Supervision for Cross-lingual Entity Linking"
}