{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2015 Conference, pages 275–284, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Conventional spoken dialogue systems (SDS) are expensive to build because many of the processing components require a substantial amount of handcrafting (Ward and Issar, 1994; Bohus and Rudnicky, 2009). In the past decade, significant progress has been made in applying statistical methods to automate the speech understanding and dialogue management components of an SDS, including making them more easily extensible to other application domains (Young et al., 2013; Gašić et al., 2014; Henderson et al.,\n2014). However, due to the difficulty of collecting semantically-annotated corpora, the use of data-driven NLG for SDS remains relatively unexplored and rule-based generation remains the norm for most systems (Cheyer and Guzzoni, 2007; Mirkovic and Cavedon, 2011).\nThe goal of the NLG component of an SDS is to map an abstract dialogue act consisting of an act type and a set of attribute-value pairs1 into an appropriate surface text (see Table 1 below for some examples). An early example of a statistical NLG system is HALOGEN by Langkilde and Knight (1998) which uses an n-gram language model (LM) to rerank a set of candidates generated by a handcrafted generator. In order to reduce the amount of handcrafting and make the approach more useful in SDS, Oh and Rudnicky (2000) replaced the handcrafted generator with a set of word-based n-gram LM-based generators, one for each dialogue type and then reranked the generator outputs using a set of rules to produce the final response. Although Oh and Rudnicky (2000)’s approach limits the amount of handcrafting to a small set of post-processing rules, their system incurs a large computational cost in the over-generation phase and it is difficult to ensure that all of the required semantics are covered by the selected output. More recently, a phrase-based NLG system called BAGEL trained from utterances aligned with coarse-grained semantic concepts has been described (Mairesse et al., 2010; Mairesse and Young, 2014). By implicitly modelling paraphrases, Bagel can generate linguistically varied utterances. However, collecting semantically-aligned corpora is expensive and time consuming, which limits Bagel’s scalability to new domains.\nThis paper presents a neural network based NLG system that can be fully trained from dia-\n1Here and elsewhere, attributes are frequently referred to as slots.\n275\nlog act-utterance pairs without any semantic alignments between the two. We start in Section 3 by presenting a generator based on a recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011a) which is trained on a delexicalised corpus (Henderson et al., 2014) whereby each value has been replaced by a symbol representing its corresponding slot. In a final postprocessing phase, these slot symbols are converted back to the corresponding slot values.\nWhile generating, the RNN generator is conditioned on an auxiliary dialogue act feature and a controlling gate to over-generate candidate utterances for subsequent reranking. In order to account for arbitrary slot-value pairs that cannot be routinely delexicalized in our corpus, Section 3.1 describes a convolutional neural network (CNN) (Collobert and Weston, 2008; Kalchbrenner et al., 2014) sentence model which is used to validate the semantic consistency of candidate utterances during reranking. Finally, by adding a backward RNNLM reranker into the model in Section 3.2, output fluency is further improved. Training and decoding details of the proposed system are described in Section 3.3 and 3.4.\nSection 4 presents an evaluation of the proposed system in the context of an application providing information about restaurants in the San Francisco area. In Section 4.2, we first show that new generator outperforms Oh and Rudnicky (2000)’s utterance class LM approach using objective metrics, whilst at the same time being more computationally efficient. In order to assess the subjective performance of our system, pairwise preference tests are presented in Section 4.3. The results show that our approach can produce high quality utterances that are considered to be more natural than a rule-based generator. Moreover, by sampling utterances from the top reranked output, our system can also generate linguistically varied utterances. Section 4.4 provides a more detailed analysis of the contribution of each component of the system to the final performance. We conclude with a brief summary and future work in Section 5."
  }, {
    "heading": "2 Related Work",
    "text": "Conventional approaches to NLG typically divide the task into sentence planning, and surface realisation. Sentence planning maps input semantic symbols into an intermediary tree-like or template structure representing the utterance, then sur-\nface realisation converts the intermediate structure into the final text (Walker et al., 2002; Stent et al., 2004; Dethlefs et al., 2013). As noted above, one of the first statistical NLG methods that requires almost no handcrafting or semantic alignments was an n-gram based approach by Oh and Rudnicky (2000). Ratnaparkhi (2002) later addressed the limitations of n-gram LMs in the overgeneration phase by using a more sophisticated generator based on a syntactic dependency tree.\nStatistical approaches have also been studied for sentence planning, for example, generating the most likely context-free derivations given a corpus (Belz, 2008) or maximising the expected reward using reinforcement learning (Rieser and Lemon, 2010). Angeli et al. (2010) train a set of log-linear models to predict individual generation decisions given the previous ones, using only domain-independent features. Along similar lines, by casting NLG as a template extraction and reranking problem, Kondadadi et al. (2013) show that outputs produced by an SVM reranker are comparable to human-authored texts.\nThe use of neural network-based approaches to NLG is relatively unexplored. The stock reporter system ANA by Kukich (1987) is a network based NLG system, in which the generation task is divided into a sememe-to-morpheme network followed by a morpheme-to-phrase network. Recent advances in recurrent neural network-based language models (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011a) have demonstrated the value of distributed representations and the ability to model arbitrarily long dependencies for both speech recognition and machine translation tasks. Sutskever et al. (2011) describes a simple variant of the RNN that can generate meaningful sentences by learning from a character-level corpus. More recently, Karpathy and Fei-Fei (2014) have demonstrated that an RNNLM is capable of generating image descriptions by conditioning the network model on a pre-trained convolutional image feature representation. This work provides a key inspiration for the system described here. Zhang and Lapata (2014) describes interesting work using RNNs to generate Chinese poetry.\nA specific requirement of NLG for dialogue systems is that the concepts encoded in the abstract system dialogue act must be conveyed accurately by the generated surface utterance, and simple unconstrained RNNLMs which rely on em-\nbedding at the word level (Mikolov et al., 2013; Pennington et al., 2014) are rather poor at this. As a consequence, new methods have been investigated to learn distributed representations for phrases and even sentences by training models using different structures (Collobert and Weston, 2008; Socher et al., 2013). Convolutional Neural Networks (CNNs) were first studied in computer vision for object recognition (Lecun et al., 1998). By stacking several convolutional-pooling layers followed by a fully connected feed-forward network, CNNs are claimed to be able to extract several levels of translational-invariant features that are useful in classification tasks. The convolutional sentence model (Kalchbrenner et al., 2014; Kim, 2014) adopts the same methodology but collapses the two dimensional convolution and pooling process into a single dimension. The resulting model is claimed to represent the state-of-the-art for many speech and NLP related tasks (Kalchbrenner et al., 2014; Sainath et al., 2013)."
  }, {
    "heading": "3 Recurrent Generation Model",
    "text": "The generation model proposed in this paper is based on an RNNLM architecture (Mikolov et al., 2010) in which a 1-hot encoding wt of a token2 wt is input at each time step t conditioned on a recurrent hidden layer ht and outputs the probability distribution of the next token wt+1. Therefore, by sampling input tokens one by one from the output distribution of the RNN until a stop sign is gen-\n2We use token instead of word because our model operates on text for which slot names and values have been delexicalised.\nerated (Karpathy and Fei-Fei, 2014) or some required constraint is satisfied (Zhang and Lapata, 2014), the network can produce a sequence of tokens which can be lexicalised to form the required utterance.\nIn order to ensure that the generated utterance represents the intended meaning, the input vectors wt are augmented by a control vector f constructed from the concatenation of 1-hot encodings of the required dialogue act and its associated slot-value pairs. The auxiliary information provided by this control vector tends to decay over time because of the vanishing gradient problem (Mikolov and Zweig, 2012; Bengio et al., 1994). Hence, f is reapplied to the RNN at every time step as in Karpathy and Fei-Fei (2014).\nIn detail, the recurrent generator shown in Figure 1 is defined as follows:\nht = sigmoid(Whhht−1 + Wwhwt + Wfhft) (1)\nP (wt+1|wt, wt−1, ...w0, ft) = softmax(Whoht) (2)\nwt+1 ∼ P (wt+1|wt, wt−1, ...w0, ft) (3)\nwhere Whh, Wwh, Wfh, and Who are the learned network weight matrices. ft is a gated version of f designed to discourage duplication of information in the generated output in which each segment fs of the control vector f corresponding to slot s is replaced by\nfs,t = fs δt−ts (4)\nwhere ts is the time at which slot s first appears in the output, δ ≤ 1 is a decay factor, and denotes element-wise multiplication. The effect of this gating is to decrease the probability of regenerating slot symbols that have already been generated, and to increase the probability of rendering all of the information encoded in f .\nThe tokenisation resulting from delexicalising slots and values does not work for all cases. For example, some slot-value pairs such as food=dont care or kids allowed=false cannot be directly modelled using this technique because there is no explicit value to delexicalise in the training corpus. As a consequence, the model is prone to errors when these slot-value pairs are required. A further problem is that the RNNLM generator selects words based only on the preceding history, whereas some sentence forms depend on the backward context.\nTo deal with these issues, candidates generated by the RNNLM are reranked using two models. Firstly, a convolutional neural network (CNN) sentence model (Kalchbrenner et al., 2014; Kim, 2014) is used to ensure that the required dialogue act and slot-value pairs are represented in the generated utterance, including the non-standard cases. Secondly, a backward RNNLM is used to rerank utterances presented in reverse order."
  }, {
    "heading": "3.1 Convolutional Sentence Model",
    "text": "The CNN sentence model is shown in Figure 2. Given a candidate utterance of length n, an utterance matrix U is constructed by stacking embeddings wt of each token in the utterance:\nU =  w0 w1 ...\nwn−1  . (5) A set of K convolutional mappings are then applied to the utterance to form a set of feature detectors. The outputs of these detectors are combined and fed into a fully-connected feed-forward network to classify the action type and whether each required slot is mentioned or not.\nEach mapping k consists of a one-dimensional convolution between a filter mk ∈ Rm and the utterance matrix U to produce another matrix Ck:\nCki,j = mkᵀUi−m+1:i,j (6)\nwhere m is the filter size, and i,j is the row and column index respectively. The outputs of each\ncolumn of Ck are then pooled by averaging3 over time:\nhk = [ C̄k:,0, C̄ k :,1, ..., C̄ k :,h−1 ] (7)\nwhere h is the size of embedding and k = 1 . . .K. Last, the K pooled feature vectors hk are passed through a nonlinearity function to obtain the final feature map."
  }, {
    "heading": "3.2 Backward RNN reranking",
    "text": "As noted earlier, the quality of an RNN language model may be improved if both forward and backward contexts are considered. Previously, bidirectional RNNs (Schuster and Paliwal, 1997) have been shown to be effective for handwriting recognition (Graves et al., 2008), speech recognition (Graves et al., 2013), and machine translation (Sundermeyer et al., 2014). However, applying a bidirectional RNN directly in our generator is not straightforward since the generation process is sequential in time. Hence instead of integrating the bidirectional information into a single unified network, the forward and backward contexts are utilised separately by firstly generating candidates using the forward RNN generator, then using the log-likelihood computed by a backward RNNLM to rerank the candidates."
  }, {
    "heading": "3.3 Training",
    "text": "Overall the proposed generation architecture requires three models to be trained: a forward RNN generator, a CNN reranker, and a backward RNN reranker. The objective functions for training the\n3Max pooling was also tested but was found to be inferior to average pooling\ntwo RNN models are the cross entropy errors between the predicted word distribution and the actual word distribution in the training corpus, whilst the objective for the CNN model is the cross entropy error between the predicted dialogue act and the actual dialogue act, summed over the act type and each slot. An l2 regularisation term is added to the objective function for every 10 training examples as suggested in Mikolov et al. (2011b). The three networks share the same set of word embeddings, initialised with pre-trained word vectors provided by Pennington et al. (2014). All costs and gradients are computed and stochastic gradient descent is used to optimise the parameters. Both RNNs were trained with back propagation through time (Werbos, 1990). In order to prevent overfitting, early stopping was implemented using a held-out validation set."
  }, {
    "heading": "3.4 Decoding",
    "text": "The decoding procedure is split into two phases: (a) over-generation, and (b) reranking. In the overgeneration phase, the forward RNN generator conditioned on the given dialogue act, is used to sequentially generate utterances by random sampling of the predicted next word distributions. In the reranking phase, the hamming loss costCNN of each candidate is computed using the CNN sentence model and the log-likelihood costbRNN is computed using the backward RNN. Together with the log-likelihood costfRNN from the forward RNN, the reranking score R is computed as:\nR = −(costfRNN + costbRNN + costCNN ). (8) This is the reranking criterion used to analyse each individual model in Section 4.4.\nGeneration quality can be further improved by introducing a slot error criterion ERR, which is the number of slots generated that is either redundant or missing. This is also used in Oh and Rudnicky (2000). Adding this to equation (8) yields the final reranking score R∗:\nR∗ = −(costfRNN + costbRNN+ costCNN + λERR)\n(9)\nIn order to severely penalise nonsensical utterances, λ is set to 100 for both the proposed RNN system and our implementation of Oh and Rudnicky (2000)’s n-gram based system. This reranking criterion is used for both the automatic evaluation in Section 4.2 and the human evaluation in Section 4.3."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Experimental Setup",
    "text": "The target application area for our generation system is a spoken dialogue system providing information about restaurants in San Francisco. There are 8 system dialogue act types such as inform to present information about restaurants, confirm to check that a slot value has been recognised correctly, and reject to advise that the user’s constraints cannot be met (Table 1 gives the full list with examples); and there are 12 attributes (slots): name, count, food, near, price, pricerange, postcode, phone, address, area, goodformeal, and kidsallowed, in which all slots are categorical except kidsallowed which is binary.\nTo form a training corpus, dialogues from a set of 3577 dialogues collected in a user trial of a statistical dialogue manager proposed by Young et al. (2013) were randomly sampled and shown to workers recruited via the Amazon Mechanical Turk service. Workers were shown each dialogue turn by turn and asked to enter an appropriate system response in natural English corresponding to each system dialogue act. The resulting corpus contains 5193 hand-crafted system utterances from 1006 randomly sampled dialogues. Each categorical value was replaced by a token representing its slot, and slots that appeared multiple times in a dialogue act were merged into one. This resulted in 228 distinct dialogue acts.\nThe system was implemented using the Theano library (Bergstra et al., 2010; Bastien et al., 2012). The system was trained by partitioning the 5193 utterances into a training set, validation set, and testing set in the ratio 3:1:1, respectively. The frequency of each action type and slot-value pair differs quite markedly across the corpus, hence up-sampling was used to make the corpus more uniform. Since our generator works stochastically and the trained networks can differ depending on the initialisation, all the results shown below4 were averaged over 10 randomly initialised networks. The BLEU-4 metric was used for the objective evaluation (Papineni et al., 2002). Multiple references for each test dialogue act were obtained by mapping them back to the 228 distinct dialogue acts, merging those delexicalised templates that have the same dialogue act specification, and then lexicalising those templates back to\n4Except human evaluation, in which only one set of network was used.\nTable 1: The 8 system dialogue acts with example realisations\n# Dialogue act and example realisations of our system, by sampling from top-5 candidates 1 inform(name=”stroganoff restaurant”,pricerange=cheap,near=”fishermans wharf”)\nstroganoff restaurant is a cheap restaurant near fishermans wharf . stroganoff restaurant is in the cheap price range near fishermans wharf .\n2 reject(kidsallowed=yes,food=”basque”) unfortunately there are 0 restaurants that allow kids and serve basque . 3 informonly(name=”bund shanghai restaurant”, food=”shanghainese”) i apologize , no other restaurant except bund shanghai restaurant that serves shanghainese . sorry but there is no place other than the restaurant bund shanghai restaurant for shanghainese . 4 confirm(goodformeal=dontcare) i am sorry . just to confirm . you are looking for a restaurant good for any meal ? can i confirm that you do not care about what meal they offer ? 5 request(near) would you like to dine near a particular location ? 6 reqmore() is there anything else i can do for you ? 7 select(kidsallowed=yes, kidsallowed=no) are you looking for a restaurant that allows kids , or does not allow kids ? 8 goodbye() thank you for calling . good bye .\nform utterances. In addition, the slot error (ERR) as described in Section 3.4, out of 1848 slots in 1039 testing examples, was computed alongside the BLEU score."
  }, {
    "heading": "4.2 Empirical Comparison",
    "text": "As can be seen in Table 2, we compare our proposed RNN-based method with three baselines: a handcrafted generator, a k-nearest neighbour method (kNN), and Oh and Rudnicky (2000)’s n-gram based approach (O&R). The handcrafted generator was tuned over a long period of time and has been used frequently to interact with real users. We found its performance is reliable and robust. The kNN was performed by computing\nFigure 3: Comparison of our method (rnn) with O&R’s approach (5g) in terms of optimising top-5 results over different selection beams.\nthe similarity of the testing dialogue act 1-hot vector against all training examples. The most similar template in the training set was then selected and lexicalised as the testing realisation. We found our RNN generator significantly outperforms these two approaches. While comparing with the O&R system, we found that by partitioning the corpus into more and more utterance classes, the O&R system can also reach a BLEU score of 0.76. However, the slot error cannot be efficiently reduced to zero even when using the error itself as a reranking criterion. This problem is also noted in Mairesse and Young (2014). In contrast, the RNN system produces utterances without slot errors when reranking using the same number of candidates, and it achieves the highest BLEU score. Figure 3 compares the RNN system with O&R’s system when randomly select-\ning from the top-5 ranked results in order to introduce linguistic diversity. Results suggest that although O&R’s approach improves as the selection beam increases, the RNN-based system is still better in both metrics. Furthermore, the slot error of the RNN system drops to zero when the selection beam is around 50. This indicates that the RNN system is capable of generating paraphrases by simply increasing the number of candidates during the over-generation phase."
  }, {
    "heading": "4.3 Human Evaluation",
    "text": "Whilst automated metrics provide useful information for comparing different systems, human testing is needed to assess subjective quality. To do this, about 60 judges were recruited using Amazon Mechanical Turk and system responses were generated for the remaining 2571 unseen dialogues mentioned in Section 4.1. Each judge was then shown a randomly selected dialogue, turn by turn. At each turn, two utterances were generated from two different systems and presented to the judge who was asked to score each utterance in terms of informativeness and naturalness (rating out of 5), and also asked to state a preference between the two taking account of the given dialogue act and the dialogue context. Here informativeness is defined as whether the utterance contains all the information specified in the dialogue act, and naturalness is defined as whether the utterance could have been produced by a human. The trial was run pairwise across four systems: the RNN system using 1-best utterance RNN1, the RNN system sampling from the top 5 utterances RNN5, the O&R approach sampling from top 5 utterances O&R5, and a handcrafted baseline.\nThe result is shown in Table 3. As can be seen, the human judges preferred both RNN1 and RNN5 compared to the rule-based generator and the preference is statistically significant. Furthermore, the RNN systems scored higher in both informativeness and naturalness metrics, though the difference for informativeness is not statistically\nsignificant. When comparing RNN1 with RNN5, RNN1 was judged to produce higher quality utterances but overall the diversity of output offered by RNN5 made it the preferred system. Even though the preference is not statistically significant, it echoes previous findings (Pon-Barry et al., 2006; Mairesse and Young, 2014) that showed that language variability by paraphrasing in dialogue systems is generally beneficial. Lastly, RNN5 was thought to be significantly better than O&R in terms of informativeness. This result verified our findings in Section 4.2 that O&R suffers from high slot error rates compared to the RNN system."
  }, {
    "heading": "4.4 Analysis",
    "text": "In order to better understand the relative contribution of each component in the RNN-based generation process, a system was built in stages training first only the forward RNN generator, then adding the CNN reranker, and finally the whole model including the backward RNN reranker. Utterance candidates were reranked using Equation (8) rather than (9) to minimise manual intervention. As previously, the BLEU score and slot error (ERR) were measured. Gate The forward RNN generator was trained first with different feature gating factors δ. Using a selection beam of 20 and selecting the top 5 utterances, the result is shown in Figure 4 for δ=1 is (equivalent to not using the gate), δ=0.7, and δ=0 (equivalent to turning off the feature immediately its corresponding slot has been generated). As can be seen, use of the feature gating substantially improves both BLEU score and slot error, and the best performance is achieved by setting δ=0. CNN The feature-gated forward RNN generator was then extended by adding a single convolutional-pooling layer CNN reranker. As shown in Figure 5, evaluation was performed on both the original dataset (all) and the dataset containing only binary slots and don’t care values (hard). We found that the CNN reranker can better handle slots and values that cannot be explicitly\ndelexicalised (1.5% improvement on hard comparing to 1% less on all). Backward RNN Lastly, the backward RNN reranker was added and trained to give the full generation model. The selection beam was fixed at 100 and the n-best top results from which to select the output utterance was varied as n = 1, 5 and 10, trading accuracy for linguistic diversity. In each case, the BLEU score was computed with and without the backward RNN reranker. The results shown in Figure 6 are consistent with Section 4.2, in which BLEU score degraded as more n-best utterances were chosen. As can be seen, the backward RNN reranker provides a stable improvement no matter which value n is. Training corpus size Finally, Figure 7 shows the effect of varying the size of the training corpus. As can be seen, if only the 1-best utterance is offered to the user, then around 50% of the data (2000 utterances) is sufficient. However, if the linguistic variability provided by sampling from the top-5 utterances is required, then the figure suggest that more than 4156 utterances in the current training set are required."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "In this paper a neural network-based natural language generator has been presented in which a forward RNN generator, a CNN reranker, and backward RNN reranker are jointly optimised to generate utterances conditioned by the required dialogue act. The model can be trained on any corpus of dialogue act-utterance pairs without any semantic alignment and heavy feature engineering or handcrafting. The RNN-based generator is compared with an n-gram based generator which uses similar information. The n-gram generator can achieve similar BLEU scores but it is less efficient and prone to making errors in rendering all of the information contained in the input dialogue act.\nAn evaluation by human judges indicated that our system can produce not only high quality but linguistically varied utterances. The latter is particularly important in spoken dialogue systems where frequent repetition of identical output forms can rapidly become tedious.\nThe work reported in this paper is part of a larger programme to develop techniques for implementing open domain spoken dialogue. A key potential advantage of neural network based language processing is the implicit use of distributed representations for words and a single compact parameter encoding of a wide range of syntactic/semantic forms. This suggests that it should be possible to transfer a well-trained generator of the form proposed here to a new domain using a much smaller set of adaptation data. This will be the focus of our future work in this area."
  }, {
    "heading": "6 Acknowledgements",
    "text": "Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd, Cambridge Research Laboratory."
  }],
  "year": 2015,
  "references": [{
    "title": "A simple domain-independent probabilistic approach to generation",
    "authors": ["Gabor Angeli", "Percy Liang", "Dan Klein."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 502–512. Associa-",
    "year": 2010
  }, {
    "title": "Theano: new features and speed improvements",
    "authors": ["Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."],
    "venue": "Deep Learning and Unsupervised Feature Learning",
    "year": 2012
  }, {
    "title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models",
    "authors": ["Anja Belz."],
    "venue": "Natural Language Engineering, 14(4):431–455, October.",
    "year": 2008
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."],
    "venue": "Neural Networks, IEEE Transactions on, 5(2):157–166.",
    "year": 1994
  }, {
    "title": "Theano: a CPU and GPU math expression compiler",
    "authors": ["James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."],
    "venue": "Proceedings of the",
    "year": 2010
  }, {
    "title": "The ravenclaw dialog management framework: Architecture and systems",
    "authors": ["Dan Bohus", "Alexander I. Rudnicky."],
    "venue": "Computer Speech and Language, 23(3):332–361, July.",
    "year": 2009
  }, {
    "title": "Method and apparatus for building an intelligent automated assistant",
    "authors": ["Adam Cheyer", "Didier Guzzoni."],
    "venue": "US Patent App. 11/518,292.",
    "year": 2007
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167.",
    "year": 2008
  }, {
    "title": "Conditional random fields for responsive surface realisation using global features",
    "authors": ["Nina Dethlefs", "Helen Hastie", "Heriberto Cuayhuitl", "Oliver Lemon."],
    "venue": "In Proceedings of ACL.",
    "year": 2013
  }, {
    "title": "Incremental on-line adaptation of pomdp-based dialogue managers to extended domains",
    "authors": ["Milica Gašić", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young."],
    "venue": "In Proceedings",
    "year": 2014
  }, {
    "title": "Unconstrained on-line handwriting recognition with recurrent neural networks",
    "authors": ["Alex Graves", "Marcus Liwicki", "Horst Bunke", "Jürgen Schmidhuber", "Santiago Fernández."],
    "venue": "Advances in Neural Information Processing Systems, pages 577–584.",
    "year": 2008
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Alex Graves", "A-R Mohamed", "Geoffrey Hinton."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645–6649. IEEE.",
    "year": 2013
  }, {
    "title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Steve Young."],
    "venue": "Proceedings of IEEE Spoken Language Technology.",
    "year": 2014
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "CoRR, abs/1404.2188.",
    "year": 2014
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Li Fei-Fei."],
    "venue": "CoRR, abs/1412.2306.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746– 1751, Doha, Qatar, October. Association for Com-",
    "year": 2014
  }, {
    "title": "A statistical nlg framework for aggregated planning and realization",
    "authors": ["Ravi Kondadadi", "Blake Howald", "Frank Schilder."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2013
  }, {
    "title": "Where do phrases come from: Some preliminary experiments in connectionist phrase generation",
    "authors": ["Karen Kukich."],
    "venue": "Natural Language Generation, volume 135 of NATO ASI Series, pages 405– 421. Springer Netherlands.",
    "year": 1987
  }, {
    "title": "Generation that exploits corpus-based statistical knowledge",
    "authors": ["Irene Langkilde", "Kevin Knight."],
    "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, ACL ’98, pages 704–710.",
    "year": 1998
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["Yann Lecun", "León Bottou", "Yoshua Bengio", "Patrick Haffner."],
    "venue": "Proceedings of the IEEE, 86(11):2278–2324, Nov.",
    "year": 1998
  }, {
    "title": "Stochastic language generation in dialogue using factored language models",
    "authors": ["François Mairesse", "Steve Young."],
    "venue": "Computer Linguistics, 40(4):763– 799.",
    "year": 2014
  }, {
    "title": "Phrase-based statistical language generation using graphical models and active learning",
    "authors": ["François Mairesse", "Milica Gašić", "Filip Jurčı́ček", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young"],
    "venue": "In Proceedings of the 48th Annual Meeting of the Associa-",
    "year": 2010
  }, {
    "title": "Context dependent recurrent neural network language model",
    "authors": ["Tomáš Mikolov", "Geoffrey Zweig."],
    "venue": "In Proceedings on IEEE SLT workshop.",
    "year": 2012
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomáš Mikolov", "Martin Karafit", "Lukáš Burget", "Jan Černocký", "Sanjeev Khudanpur."],
    "venue": "In Proceedings on InterSpeech.",
    "year": 2010
  }, {
    "title": "Extensions of recurrent neural network language model",
    "authors": ["Tomáš Mikolov", "Stefan Kombrink", "Lukáš Burget", "Jan H. Černocký", "Sanjeev Khudanpur."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.",
    "year": 2011
  }, {
    "title": "Rnnlm recurrent neural network language modeling toolkit",
    "authors": ["Tomáš Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukáš Burget", "Jan Černocký."],
    "venue": "In Proceedings on ASRU.",
    "year": 2011
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomáš Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems 26, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Dialogue management using scripts, February 16",
    "authors": ["Danilo Mirkovic", "Lawrence Cavedon."],
    "venue": "EP Patent 1,891,625.",
    "year": 2011
  }, {
    "title": "Stochastic language generation for spoken dialogue systems",
    "authors": ["Alice H. Oh", "Alexander I. Rudnicky."],
    "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational Systems - Volume 3, ANLP/NAACL-ConvSyst ’00, pages 27–32.",
    "year": 2000
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. Associa-",
    "year": 2014
  }, {
    "title": "Responding to student uncertainty in spoken tutorial dialogue systems",
    "authors": ["Heather Pon-Barry", "Karl Schultz", "Elizabeth Owen Bratt", "Brady Clark", "Stanley Peters."],
    "venue": "International Journal of Artificial Intelligence in Education.",
    "year": 2006
  }, {
    "title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems",
    "authors": ["Adwait Ratnaparkhi."],
    "venue": "Computer Speech and Language. Spoken Language Generation.",
    "year": 2002
  }, {
    "title": "Natural language generation as planning under uncertainty for spoken dialogue systems",
    "authors": ["Verena Rieser", "Oliver Lemon."],
    "venue": "Empirical Methods in Natural Language Generation, pages 105–120.",
    "year": 2010
  }, {
    "title": "Deep convolutional neural networks for lvcsr",
    "authors": ["Tara N Sainath", "A-r Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8614–8618. IEEE.",
    "year": 2013
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["Mike Schuster", "Kuldip K Paliwal."],
    "venue": "Signal Processing, IEEE Transactions on, 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2014 Conference on",
    "year": 2013
  }, {
    "title": "Trainable sentence planning for complex information presentation in spoken dialog systems",
    "authors": ["Amanda Stent", "Rashmi Prasad", "Marilyn Walker."],
    "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 79–86.",
    "year": 2004
  }, {
    "title": "Translation modeling with bidirectional recurrent neural networks",
    "authors": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    "year": 2014
  }, {
    "title": "Generating text with recurrent neural networks",
    "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024, New York, NY, USA. ACM.",
    "year": 2011
  }, {
    "title": "Training a sentence planner for spoken dialogue using boosting",
    "authors": ["Marilyn A Walker", "Owen C Rambow", "Monica Rogati."],
    "venue": "Computer Speech and Language, 16(3):409–433.",
    "year": 2002
  }, {
    "title": "Recent improvements in the cmu spoken language understanding system",
    "authors": ["Wayne Ward", "Sunil Issar."],
    "venue": "Proceedings of the Workshop on Human Language Technology, HLT ’94, pages 213– 216. Association for Computational Linguistics.",
    "year": 1994
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["Paul J Werbos."],
    "venue": "Proceedings of the IEEE, 78(10):1550–1560.",
    "year": 1990
  }, {
    "title": "Pomdp-based statistical spoken dialog systems: A review",
    "authors": ["Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D. Williams."],
    "venue": "Proceedings of the IEEE, 101(5):1160–1179, May.",
    "year": 2013
  }, {
    "title": "Chinese poetry generation with recurrent neural networks",
    "authors": ["Xingxing Zhang", "Mirella Lapata."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680. Association for Compu-",
    "year": 2014
  }],
  "id": "SP:6931942e89ec5e381811f66ead3f268dc9473551",
  "authors": [{
    "name": "Tsung-Hsien Wen",
    "affiliations": []
  }, {
    "name": "Milica Gašić",
    "affiliations": []
  }, {
    "name": "Dongho Kim",
    "affiliations": []
  }, {
    "name": "Nikola Mrkšić",
    "affiliations": []
  }, {
    "name": "Pei-Hao Su",
    "affiliations": []
  }, {
    "name": "David Vandyke",
    "affiliations": []
  }],
  "abstractText": "The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems.",
  "title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking"
}