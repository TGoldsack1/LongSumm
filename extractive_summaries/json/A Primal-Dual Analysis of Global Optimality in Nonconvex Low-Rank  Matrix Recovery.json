{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Low-rank matrix recovery has received increasing attention in recent years, due to its wide range of applications including signal processing, computer vision and collaborative filtering (Rennie & Srebro, 2005; Ahmed & Romberg, 2015). The objective is to estimate an unknown rank-r matrix X∗ ∈ Rd1×d2 based on partially observed measurements. More formally, low-rank matrix recovery can be formulated as the following optimization problem\nmin X∈C\nFn(X) subject to rank(X) ≤ r, (1.1)\n*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, VA 22904, USA. 2Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095, USA. Correspondence to: Quanquan Gu <qgu@cs.ucla.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhere Fn : Rd1×d2 → R denotes a general sample loss function with respect to n measurements, and C denotes a constraint set such that X∗ ∈ C. For example, C is set to be Rd1×d2 in matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), and is chosen to be the set of incoherent matrices in matrix completion (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012) and one-bit matrix completion (Cai & Zhou, 2013; Davenport et al., 2014).\nTremendous efforts have been made to efficiently solve (1.1), among which the most popular ones are nuclear norm relaxation based methods (Srebro et al., 2004; Candès & Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Recht, 2011; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015). These methods can achieve near optimal sample complexity for recovery (Balcan et al., 2017), but a singular value decomposition (SVD) step, whose time complexity is O(d3)1, is required at each iteration, which is computationally expensive for large-scale datasets. To avoid using SVD, the most commonly-used technique is based on BurerMonteiro factorization (Burer & Monteiro, 2003), which reparameterizes the low-rank matrix X as the product of two smaller matrices U ∈ Rd1×r and V ∈ Rd2×r such that X = UV>. Instead of optimizing (1.1) directly, we turn to solve the following nonconvex optimization problem\nmin U∈C1,V∈C2\nFn(UV>), (1.2)\nwhere C1 ⊆ Rd1×r, C2 ⊆ Rd2×r are some constraint sets induced by C (c.f. Section 2.1). Note that (1.2) automatically ensures the low-rankness of the estimated matrix.\nA line of research (Bach et al., 2008; Keshavan et al., 2009; Lee et al., 2013; Jain et al., 2013; Bach, 2013; Hardt, 2014; Hardt & Wootters, 2014; Netrapalli et al., 2014; Jain & Netrapalli, 2014; Haeffele et al., 2014; Sun & Luo, 2015; Bhojanapalli et al., 2015; Chen & Wainwright, 2015; Zhao et al., 2015; Tu et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Park et al., 2016b; Jin et al., 2016; Gu et al., 2016; Wang et al., 2017a;b; Xu et al., 2017; Zhang et al., 2018) proposed to solve (1.2) based on gradient\n1We assume d1 = d2 = d when discussing the sample complexity for simplicity.\ndescent and/or alternating minimization, and established a locally linear convergence property provided that the initial solution falls into a basin of attraction, i.e., a small neighbourhood around the optimum. Recently, another line of research (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) directly characterized the optimization landscape of (1.2) and proved that various low-rank matrix recovery problems, including matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016c; Zhu et al., 2017a), matrix completion (Ge et al., 2016), and robust PCA (Ge et al., 2017), have no spurious local minima, i.e., all local minima are global ones. Based on existing results on finding local minimum for certain nonconvex problems (Ge et al., 2015; Carmon et al., 2016; Agarwal et al., 2016; Jin et al., 2017), they further showed that (1.2) can be successfully solved by saddle-avoiding algorithms, such as perturbed gradient descent. However, none of the aforementioned work is generic enough to cover objective functions beyond square loss, e.g., the sample loss function for one-bit matrix completion (Davenport et al., 2014).\nFollowing the second line of research, we propose a primaldual analysis to characterize the landscape of general objective functions in nonconvex low-rank matrix recovery including both square loss and beyond. By using restricted strongly convex and smooth conditions (Negahban et al., 2009; Negahban & Wainwright, 2011), we are able to characterize a large family of low-rank problems. To incorporate the widely-used incoherence constraints for low-rank matrix estimation, we propose to analyze the corresponding Lagrangian function rather than the primal objective function and use the Karush-Kuhn-Tucker (KKT) condition (Nocedal & Wright, 2006) to characterize the local minima of (1.2). Our analysis shows that the optimization landscape of (1.2) is well-behaved, i.e., there are no spurious local minima. In addition, we demonstrate empirically that the primal-dual based algorithm proposed in (Nocedal & Wright, 2006) can recover the ground truth matrix successfully. Our major contributions are further highlighted as follows."
  }, {
    "heading": "1.1. Contributions",
    "text": "• Our general framework can be applied to any loss function that satisfies the restricted strongly convex and smooth conditions (c.f. Section 3), which covers a broad family of loss functions for low-rank problems. All the existing theoretical analyses (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) are limited to square loss, thus we resolve an open problem raised in Ge et al. (2017) regarding the characterization of global geometry for one-bit matrix completion.\n• Our primal-dual analysis is applicable to various noisy\nlow-rank problems. In particular, our analysis suggests there are no spurious local minima in noisy matrix completion, provided that the number of observations is O(r2d log d). Compared with existing studies (Ge et al., 2016; 2017) whose sample complexity scales to the fourth power with the rank, the sample requirement of our method matches the best-known sample complexity of matrix completion using nonconvex optimization algorithm (Zheng & Lafferty, 2016) under the incoherence condition.\n• Compared with the seminal work (Ge et al., 2016; 2017) along this line that makes use of ad hoc regularizer to deal with incoherence constraints, our primaldual analytic framework directly characterizes the global geometry of constrained nonconvex optimization problem for low-rank matrix recovery using duality theory. We believe the Lagrangian based proof technique is of independent interest, which can be extended to handle more general inequality constraints in other nonconvex problems."
  }, {
    "heading": "1.2. Related Work",
    "text": "Characterizing the landscape of various objective functions has attracted more and more attention in recent years. For instance, Sun et al. (2015) studied the nonconvex geometry of complete dictionary recovery problem, and proved that all local minima are global ones. Sun et al. (2016) showed that a nonconvex fourth-order polynomial objective for phase retrieval has no spurious local minimum and all global minima are equivalent. Lee et al. (2016) showed that gradient descent converges to local minimum almost surely, using the stable manifold theorem from dynamical system. Ge et al. (2016) proved that the commonly used nonconvex objective function for positive semidefinite matrix completion has no spurious local minimum. In an independent work, Bhojanapalli et al. (2016) proved that positive semidefinite (PSD) matrix sensing, a very related problem to matrix completion, has no spurious local minima under the restricted isometry property (RIP). Later on, Park et al. (2016c) extended the geometric analysis of matrix sensing from PSD matrices to rectangular matrices. Zhu et al. (2017a) provided a unified geometric analysis for objective functions satisfying the restricted strong convexity/smoothness property, but their work cannot deal with the constrained optimization, e.g., matrix completion and one-bit matrix completion.\nMost recently, several studies attempted to unify the global geometry analyses for nonconvex low-rank matrix recovery problems. For instance, Li et al. (2016) proposed a general theory to characterize the global geometry of positive semidefinite low-rank matrix factorization problem. Zhu et al. (2017b) further extended the geometric analysis in Li et al. (2016) to rectangular matrix factorization problem.\nNevertheless, both of their analyses require the objective function to be quadratic (i.e., square loss function), which is not applicable to constrained low-rank matrix recovery problems. The most relevant work to ours is Ge et al. (2017), which proposed a general framework to characterize the landscape of nonconvex low-rank matrix recovery problem. More specifically, they incorporated the constraints for matrix completion and robust PCA by a specifically designed regularizer, to make the solution lie in a desired region (e.g., the set of incoherent matrices). However, their framework still requires the loss function to be quadratic, thus unable to analyze low-rank problems with general objective function, such as one-bit matrix completion."
  }, {
    "heading": "1.3. Organization and Notation",
    "text": "The remainder of this paper is organized as follows. We formally state the general low-rank matrix recovery problem and introduce two specific applications in Section 2. In Section 3, we lay out conditions for the proposed primal-dual based framework and present our main theoretical results. In Section 4, we apply the general results to two specific low-rank problems. The primal-dual based method and the numerical experiments are illustrated in Sections 5 and 6, respectively. We conclude in Section 7 and defer the detailed proofs to the supplementary materials.\nWe use capital symbols such as A to denote matrices and [d] to denote index set {1, 2, . . . , d}. Let the i-th row, jth column and (i, j)-th entry of A be Ai,∗, A∗,j and Aij respectively. Denote the i-th standard basis by ei and the `-th largest singular value of A by σ`(A). We use vec(A) to denote the vectorization of matrix A. For any vector x, we use ‖x‖2 to denote its `2 norm. Let ‖A‖F , ‖A‖2 be the Frobenius norm and the spectral norm of matrix A, respectively. We define the largest `2 norm of its rows as ‖A‖2,∞ = maxi ‖Ai,∗‖2. For any two sequences {an} and {bn}, if there exists a constant 0 < C < ∞ such that an ≤ Cbn, then we denote an = O(bn)."
  }, {
    "heading": "2. Constrained Nonconvex Optimization for Low-Rank Matrix Recovery",
    "text": "In this section, we introduce our general framework for low-rank matrix recovery, along with several applications."
  }, {
    "heading": "2.1. Generic Framework",
    "text": "Let the singular value decomposition of the unknown lowrank matrix be X∗ = U ∗ ΣV ∗> and U∗, V∗ be the underlying factorized matrices such that U∗ = U ∗ Σ1/2, V∗ = V ∗ Σ1/2. Denote the sorted singular values of X∗ by σ1 ≥ σ2 ≥ . . . ≥ σr > 0. Recall that we aim to characterize the global optimality of the general nonconvex optimization problem (1.2). Formally, we define the general\nconstraint sets C1, C2 as follows\nC1 = { U ∈ Rd1×r ∣∣ ‖U‖2,∞ ≤ α1}, C2 = { V ∈ Rd1×r\n∣∣ ‖V‖2,∞ ≤ α2}, (2.1) where α1, α2 will be specified for different examples. To guarantee optimization problem (1.2) is well-defined, we assume U∗ ∈ C1 and V∗ ∈ C2. It is worth noting that (2.1) can cover a wide range of low-rank matrix recovery problems. For instance, in matrix completion, constraint sets in the form of (2.1) are proposed to ensure the estimator X = UV> is incoherent.\nIn addition, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016a); Wang et al. (2017a), we add an additional regularization term ‖U>U −V>V‖2F to (1.2) such that the solutions U, V are in similar scale. Specifically, we propose to analyze the following constrained optimization problem with respect to the stacked matrix Z = [U; V] in the lifted space R(d1+d2)×r\nmin Z∈D\nG(Z) = Fn(UV>) + γ\n4 ‖U>U−V>V‖2F , (2.2)\nwhere the constraint set D is defined as D = {Z ∈ R(d1+d2)×r | ‖Z‖2,∞ ≤ α}, where α = max{α1, α2}, and γ denotes the regularization parameter."
  }, {
    "heading": "2.2. Specific Examples",
    "text": "We briefly introduce two specific examples: noisy matrix completion and one-bit matrix completion.\nNoisy matrix completion. The goal of matrix completion (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012) is to estimate the unknown low-rank matrix X∗ from its partially observed (noisy) entries. More specifically, we consider the uniform observation model\nYjk = { X∗jk + Ejk, for any (j, k) ∈ Ω; ∗, otherwise. (2.3)\nwhere Ω ⊆ [d1] × [d2] denotes the observed index set such that for any (j, k) ∈ Ω, j ∼ uniform([d1]) and k ∼ uniform([d2]). Here, Y ∈ Rd1×d2 denotes the observed data matrix, E ∈ Rd1×d2 denotes a random noise matrix such that each entry of E follows i.i.d. Gaussian distribution with variance ν2/(d1d2).\nAs discussed in previous work (Gross, 2011; Negahban & Wainwright, 2012), it is impossible to recover the unknown low-rank matrix X∗ if it is too sparse. To avoid such issue, we impose the following incoherence condition (Candès & Recht, 2009) on the singular spaces of X∗\n‖U∗‖2,∞ ≤ √ βr\nd1 and ‖V∗‖2,∞ ≤\n√ βr\nd2 , (2.4)\nwhere r denotes the rank of X∗, and β denotes the incoherence parameter. Note that based on the incoherence condition (2.4), we can derive ‖X∗‖∞,∞ ≤ ‖U\n∗‖2,∞ · ‖Σ‖2 · ‖V∗‖2,∞ ≤ βrσ1/ √ d1d2, which leads to a dimension-free signal-to-noise ratio in observation model (2.3).\nMore specifically, we consider the following constrained optimization problem for matrix completion\nmin Z∈D\n1\n2p ∑ (j,k)∈Ω (Uj,∗V > k,∗ − Yjk)2 + γ 4 ‖U>U−V>V‖2F ,\n(2.5)\nwhere D is defined in Section 2.1 and p = |Ω|/(d1d2) denotes the sampling rate. In order to guarantee Z∗ = [U∗; V∗] ∈ D, we set α = √ βrσ∗1/d, where d1 and d2 are in the same order O(d) for simplicity.\nOne-bit matrix completion. The objective of one-bit matrix completion (Davenport et al., 2014; Cai & Zhou, 2013; Ni & Gu, 2016) is to recover the unknown low-rank matrix X∗ from a set of binary observations. In detail, the dependence of the observed binary matrix Y ∈ Rd1×d2 on X∗ is presented as follows\nYjk = { +1, if X∗jk + Ejk > 0, −1, if X∗jk + Ejk < 0,\n(2.6)\nwhere E ∈ Rd1×d2 denotes a random noise matrix. Let f be the cumulative distribution function of −Ejk, then the observation model (2.6) can be recast as the following probabilistic model\nYjk = { +1, with probability f(X∗jk), −1, with probability 1− f(X∗jk).\n(2.7)\nIn addition, we consider the same uniform sampling model for the observed index set Ω as in matrix completion, and impose the incoherence condition (2.4) on X∗ to avoid the overly sparse matrices. Specifically, we aim to solve the following optimization problem for one-bit matrix completion\nmin Z∈D − 1 |Ω| ∑ (j,k)∈Ω { 1 (Yjk=1) log ( f(Uj∗V > k∗) )\n+ 1 (Yjk=−1)\nlog ( 1− f(Uj,∗V>k,∗) )} + γ\n4 ‖U>U−V>V‖2F ,\n(2.8)\nwhere Ω ⊆ [d1] × [d2] denotes the observed index set with cardinality |Ω|, and we also set the parameter α =√ βrσ∗1/d in the constraint set D to ensure optimization probelm (2.8) is well-defined."
  }, {
    "heading": "3. Results for Generic Framework",
    "text": "Before presenting our main theoretical results, we first lay out the formal definition of local minimizer and the basic\nnecessary optimality conditions with respect to constrained optimization problem (2.2) .\nDefinition 3.1. Z is a local minimizer of constrained optimization (2.2), if Z satisfies the following conditions: (i) Z ∈ D. (ii) There exists a neighbourhood N of Z such that for any Z̃ ∈ N ∩ D, G(Z̃) ≥ G(Z) holds.\nFor general constrained optimization (2.2), we provide the fundamental first-order necessary condition as follows.\nLemma 3.2. Suppose Z is a local minimizer of constrained optimization problem (3.1). Then for all feasible directions2 ∆ ∈ R(d1+d2)×r, the following inequality holds:\n〈∇G(Z),∆〉 ≥ 0.\nRecall that the constraint set D is defined as D = {Z ∈ R(d1+d2)×r | ‖Z‖2,∞ ≤ α}. Thus, according to the definition of ‖ · ‖2,∞, we can reformulate (2.2) as the following equivalent standard form\nmin Z∈R(d1+d2)×r\nG(Z) = Fn(UV>) + γ\n4 ‖U>U−V>V‖2F\nsubject to hi(Z) ≤ 0, for all i ∈ [d1 + d2], (3.1)\nwhere hi(Z) = ‖e>i Z‖22 − α2, and ei represents the i-th natural basis. Accordingly, the Lagrangian with respect to (3.1) is given as follows\nL(Z,λ) = G(Z) + d1+d2∑ i=1 λihi(Z),\nwhere λ = [λ1, λ2, . . . λ(d1+d2)] > denotes the Lagrange multiplier vector. Based on the Lagrangian, we give the following necessary optimality conditions (Nocedal & Wright, 2006) for constrained optimization problem (3.1).\nLemma 3.3. Let Z be a local minimizer of constrained optimization problem (3.1). Define the set of active constraint gradients at Z as A(Z) = {i ∈ [d1 + d2] | hi(Z) = 0}. Suppose the active set {∇hi(Z) | i ∈ A(Z)} is linearly independent. Then there exists a Lagrange multiplier vector λ such that (Z,λ) satisfies the following Karush-Kuhn-Tucker (KKT) conditions\n1. hi(Z) ≤ 0, for all i ∈ [d1 + d2],\n2. λ ≥ 0,\n3. λihi(Z) = 0, for all i ∈ [d1 + d2],\n4. ∇ZL(Z,λ) = 0. 2We say ∆ is a feasible direction for constraint set D at Z, if\nthere exists t > 0 such that Z + s∆ ∈ D for all s ∈ [0, t].\nLemma 3.4. Under the same conditions as in Lemma 3.3, let λ be a Lagrange multiplier vector such that (Z,λ) satisfies the KKT condition. For any ∆ ∈ R(d1+d2)×r satisfying the condition that 〈∇hi(Z),∆〉 ≤ 0 holds for all i ∈ A(Z), (Z,λ) also satisfies the following inequality\nvec(∆)>∇2ZL(Z,λ)vec(∆) ≥ 0.\nThe aforementioned necessary optimality conditions characterize the properties of local minima with respect to constrained optimization problem. As will be seen in later analysis, these optimality conditions are essential to show that there are no spurious local minima in (3.1).\nNext, we lay out several conditions for the general sample loss function Fn. To begin with, we present the restricted strong convexity and smoothness conditions (Negahban et al., 2009; Loh & Wainwright, 2013).\nCondition 3.5 (Restricted Strong Convexity). The sample loss function Fn is µ-restricted strongly convex, i.e., for all matrices Y,W ∈ Rd1×d2 with rank at most 6r\nFn(Y) ≥ Fn(W) + 〈∇Fn(W),Y −W〉+ µ\n2 ‖Y −W‖2F .\nCondition 3.6 (Restricted Strong Smoothness). The sample loss function Fn is L-restricted strongly smooth, i.e., for all matrices Y,W ∈ Rd1×d2 with rank at most 6r\nFn(Y) ≤ Fn(W) + 〈∇Fn(W),Y −W〉+ L\n2 ‖Y −W‖2F .\nIn addition, we assume there is an upper bound for the gradient of the sample loss function ∇Fn with respect to the unknown low-rank matrix X∗.\nCondition 3.7. Given a fixed sample size n and tolerance parameter δ ∈ (0, 1). Let (n, δ) be the smallest scalar such that with probability at least 1− δ, we have\n‖∇Fn(X∗)‖2 ≤ (n, δ),\nwhere (n, δ) depends on sample size n and δ, and (n, δ) deceases as n increases.\nAs will be clear in the next section and in the proofs, Conditions 3.5, 3.6 and 3.7 can be verified for a wide range of sample loss functions, such as the objective functions in matrix completion and one-bit matrix completion.\nFinally, we present the main results regarding the global optimality of optimization problem (3.1). In particular, we are going to show that under proper conditions, (3.1) has no spurious local minima.\nTheorem 3.8. Assume the sample loss functionFn satisfies Conditions 3.5, 3.6 and 3.7. Under condition that L/µ ∈\n(1, 18/17), for all local minima Z = [U; V] of optimization problem (3.1) with regularization parameter γ satisfying µ − L/2 ≤ γ < min{(22µ − 19L)/4, (3L − 2µ)/2}, the reconstruction error satisfies\n‖UV> −X∗‖2F ≤ Γr 2(n, δ), (3.2)\nwith probability at least 1− δ, where Γ is a constant depending on µ,L and γ.\nRemark 3.9. Theorem 3.8 suggests that for all local minima Z = [U; V] of (3.1), the reconstructed matrix UV> lies in a small neighbourhood of X∗, and the radius of such neighbourhood decreases as the sample size n increases. While in the noiseless case ( (n, δ) = 0), the right hand side of (3.2) is 0, which suggests that all local minima are global ones, i.e., UV> = X∗. Note that we require the condition number L/µ to be close to 1 in Theorem 3.8. As will be clear in the next section and in the proofs, this assumption can be verified for the specific examples discussed in Section 2.1. Similar assumption has been imposed in existing work on matrix sensing (Bhojanapalli et al., 2016; Ge et al., 2017), in that the restricted isometry property parameter is required to be in a small range around 0."
  }, {
    "heading": "4. Implications for Specific Examples",
    "text": "In this section, we illustrate how to apply our general framework to two specific low-rank problems: noisy matrix completion and one-bit matrix completion. Note that given the general results in Section 3, we only need to verify Conditions 3.5, 3.6, 3.7 and the assumption regarding the restricted condition number L/µ for each specific example."
  }, {
    "heading": "4.1. Results for Noisy Matrix Completion",
    "text": "Recall that for noisy matrix completion, we aim to optimize (2.5) under the uniform sampling model (2.3) and incoherence condition (2.4). Specifically, we verify Conditions 3.5, 3.6 and 3.7 for Fn in the following corollary to characterize the global optimality of noisy matrix completion.\nCorollary 4.1. Consider noisy matrix completion problem (2.5) under the uniform sampling model. Suppose the unknown rank-r matrix X∗ satisfies incoherence condition (2.4) and each entry of the noise matrix E follows i.i.d. Gaussian distribution with variance ν2/(d1d2). Provided the number of observed samples |Ω| ≥ c1r2d log d, with regularization parameter γ = 1/2, all local minima Z = [U; V] of optimization problem (2.5) satisfy\n‖UV> −X∗‖F ≤ c2 max { ν, √ rβσ1 }√rd log d n ,\nwith probability at least 1 − c2/d , where c1, c2 are both universal constants.\nRemark 4.2. Due to the existence of noise, it is impossible to exactly recover the unknown low-rank matrix X∗ for noisy matrix completion. However, we show that the reconstruction UV> from any local minimizer of (2.5) is actually a good estimator of X∗. The estimation error is in the order of O( √ r2d log d/n), which suggests that the more observations we have, the smaller estimation error we can achieve. It is worth noting that we only need O(r2d log d) observed entries of X∗ to ensure (3.1) has no spurious local minima, in sharp contrast to existing work (Ge et al., 2016; 2017) whose sample complexity is at least O(r4d log d)."
  }, {
    "heading": "4.2. Results for One-Bit Matrix Completion",
    "text": "Recall that the objective of one-bit matrix completion is to solve optimization problem (2.8). We assume the standard regularity condition (Cai & Zhou, 2013) on the cumulative distribution function f in (2.7) as follows\nsup |x|≤β′\n{ |f ′(x)|/ ( f(x)(1− f(x)) )} ≤ γβ′ , (4.1)\nwhere γβ′ reflects the steepness of the sample loss function, and when f and β′ are given, γβ′ is a fixed constant. We note that this condition holds for a large family of distributions, such as Logistic distribution, Gaussian distribution, and Laplacian distribution. To apply the results in the unified framework, it suffices to prove Conditions 3.5, 3.6 and 3.7 for one-bit matrix completion, respectively.\nCorollary 4.3. Assume that the observed matrix Y follows the binary observation model (2.7) generated based on a cumulative distribution function f satisfying (4.1). Suppose the unknown low-rank matrix X∗ satisfies incoherence condition (2.4) and the observed index set Ω follows the uniform sampling model. If the sample complexity |Ω| exceeds c1r2d log d, and the regularization parameter is set as γ = 1/2, then with probability at least 1 − c2/d, all local minima Z = [U; V] of optimization problem (2.8) satisfy\n‖UV> −X∗‖F ≤ c3 max{γβ′ , √ rβσ1}\n√ rd log d\nn ,\nwhere c1, c2, c3 are all universal constants.\nRemark 4.4. Corollary 4.3 shows all local minima Z = [U; V] of one-bit matrix completion satisfy the condition UV> lies in a close neighborhood around X∗ with radius O( √ r2d log d/n). This suggests that as long as the number of observations is sufficient, we can obtain a good estimator for X∗ by solving (2.8). To the best of our knowledge, all the existing studies on the characterization of global geometry for low-rank problems require the objective function to be square loss, thus our work is the first that can characterize the global optimality for one-bit matrix completion, which resolves an open problem in Ge et al. (2017)."
  }, {
    "heading": "5. The Primal-Dual Algorithm",
    "text": "So far, we have shown that all local minima of inequality constrained optimization problem (3.1) belong to a close neighbourhood of the ground truth matrix, with applications to two specific low-rank problems. It remains to find an efficient and effective algorithm that can find a local minimizer of (3.1) successfully.\nRecall that our characterization of global optimality with respect to (3.1) is based on the Lagrange function and the duality theory. This motivates us to search for local minima of (3.1) from the primal-dual perspective. It has been proved in Di Pillo et al. (2011) that a particular primal-dual based algorithm can converge to a solution that satisfies the necessary optimality Conditions 3.3 and 3.4 for general nonlinear inequality constrained optimization problems. It immediately suggests that their algorithm can be directly applied to solve the optimization problem (3.1), and is guaranteed to find a local minimizer. Nevertheless, the algorithm in Di Pillo et al. (2011) requires to access the Hessian information, which is computationally very expensive for large scale problems. Thus, a more practical primal-dual algorithm is preferred.\nWitnessing the empirical success of Augmented Lagrangian Method (Nocedal & Wright, 2006), a first-order primal-dual method for general constrained optimization problem, we propose to use the augmented Lagrangian method to solve the inequality constrained optimization problem (3.1), as displayed in Algorithm 1. More specifically, we introduce a slack variable ξ = [ξ1, . . . , ξd1+d2 ]\n> to transform the inequality constraints in (3.1) into equality ones. Define the augmented Lagrange function L̃ as follows\nL̃(Z, ξ,λ, µ) = G(Z) + d1+d2∑ i=1 λici(Z) + µ 2 d1+d2∑ i=1 c2i (Z),\nwhere ci(Z) = hi(Z) + ξ2i . At each iteration of Algorithm 1, we solve the minimization sub-problem in line 2 based on gradient descent with respect to Z and ξ, and update the dual variable λ and the penalty parameter µ as suggested in Nocedal & Wright (2006). Here, we let h(Z) = [h1(Z), . . . , hd1+d2(Z)] > in line 3.\nAlgorithm 1 Augmented Lagrangian Method\nInput: Augmented Lagrangian function L̃; parameters T, ξ0,λ0, µ0 > 0 and ρ ≥ 1; initial estimator Z0\n1: for t = 0, . . . , T do 2: Solve ( Zt+1, ξt+1 ) = argminZ,ξ L̃(Z, ξ,λt, µt)\n3: λt+1 = λt + µt ( h(Zt+1) + ξ 2 t+1 ) 4: µt+1 = ρµt 5: end for\nOutput: ZT+1\nAs will be seen in the next section, we demonstrate through numerical experiments that the primal-dual based Algorithm 1 can efficiently solve the constrained nonconvex optimization problem (3.1) given enough observations."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we provide simulation results of the primaldual based method, as discussed in Section 5, for matrix completion and one-bit matrix completion. Under random initialization, we compare our primal-dual based Algorithm 1 (Primal-Dual) with existing gradient-based methods, including vanilla gradient descent (GD), projected gradient descent (Proj-GD) and perturbed gradient descent (Jin et al., 2017) (Perturb-GD). We remark that GD and Proj-GD have been proposed in Ma et al. (2017) and Zheng & Lafferty (2016) for matrix completion respectively, but they all require a specially designed initialization procedure. Here, we are interested in evaluating the performance of all these algorithms with random initialization. All of the aforementioned algorithms are implemented in Matlab, and all the following experimental results are based on the optimal parameters, which are selected by cross validation and averaged over 20 trials."
  }, {
    "heading": "6.1. Matrix Completion",
    "text": "We generate the observed data matrix according to the uniform observation model (2.3). In particular, the un-\nknown low-rank matrix X∗ ∈ Rd1×d2 is generated via X∗ = U∗V∗>, where each entry of U∗ ∈ Rd1×r and V∗ ∈ Rd2×r is generated independently from standard Gaussian distribution, and we scale them to ensure max{‖U∗‖2,∞, ‖V∗‖2,∞} ≤ α, where α = 2. The noise matrix E is set as 0 in the noiseless case, while under the noisy setting, we generate each element of the noise matrix E from i.i.d. centered Gaussian distribution with variance σ2 = 0.25. Note that due to random initialization, the initial estimators may not satisfy the incoherence constraint. In the sequel, we are going to evaluate the recovery performance of different algorithms under the noiseless setting, and investigate the statistical rate of our method.\nTo begin with, we compare the sample complexities required by the aforementioned algorithms under the setting d1 = 120 and d2 = 100 with sample size n and rank r varied. We say the final estimator X̂ successfully recover the ground truth matrix X∗, if the relative error ‖X̂−X∗‖F /‖X∗‖F ≤ 10−3. Figures 1(a)-1(d) illustrate the recovery probability of different methods. Here, the white block indicates successful recovery and the black block denotes failure. As for GD, all the cases have phase transition around n = 4rd. While our method has phase transition around n = 2.5rd under all the settings, and similar results are observed in Figure 1(b) for Proj-GD and Figure 1(c) regarding Perturb-GD. These results suggest that the sample complexity for all these methods could be linear in both d and r.\nNext, we compare the convergence rate of different methods under the setting d1 = 2000, d2 = 1500, r = 20 with sampling rate p = |Ω|/(d1d2) chosen from {15%, 25%, 35%}. The experimental results in terms of the averaged Frobenius norm error ‖X̂−X∗‖F / √ d1d2 versus the number of data passes are demonstrated in Figures 1(e)-1(g). It can be seen that our primal-dual based algorithm achieves lower estimation error than GD after the same number of data passes, especially when the observed sample size is small. On the other hand, compared with Proj-GD and Perturb-GD, our method achieves comparable performance, which verifies the effectiveness of Algorithm 1 for solving nonconvex optimization problem (2.5) to find a local minimum.\nFinally, we study the statistical rate of our method under the following settings: (i) d1 = 100, d2 = 80, r = 2; (ii) d1 = 120, d2 = 100, r = 3; (iii) d1 = 140, d2 = 120, r = 4. The results are displayed in Figure 1(h). In detail, the vertical axis represents the estimation error ‖X̂−X∗‖2F /(d1d2), and the horizontal axis is the rescaled sample size n/(rd log d). The results show that the estimation error and the rescaled sample size align well under different settings, which suggests that the statistical rate of our method is O(rd log d/n)."
  }, {
    "heading": "6.2. One-Bit Matrix Completion",
    "text": "We generate the data matrix Y based on probability model (2.7) with f(Xij) = Φ(Xij/σ), where Φ is the cumulative distribution function of the standard Gaussian distribution, and we choose σ = 0.5 as the noise level. For the unknown low-rank matrix X∗ = U∗V∗> with rank r, we follow the same generative procedure as in Davenport et al. (2014); Bhaskar & Javanmard (2015); Ni & Gu (2016). More specifically, U∗ ∈ Rd1×r,V∗ ∈ Rd2×r are randomly generated from a uniform distribution on [−1/2, 1/2], and are scaled properly such that the incoherence constraint max{‖U∗‖2,∞, ‖V∗‖2,∞} ≤ α is satisfied, where we set α = 1. In addition, we sample the observed index set Ω according to the uniform sampling model.\nTo demonstrate the effectiveness of Algorithm 1, we com-\npare our method with existing gradient-based algorithms including GD, Proj-GD and Perturb-GD with random initialization. In particular, we compute the logarithm of the averaged estimation error ‖X̂ − X∗‖F / √ d1d2 and plot it with the number of data passes for different methods, which are illustrated in Figures 2(a)-2(c) under the setting d1 = 2000, d2 = 1500, r = 20 with varied sampling rate. These results again confirm that with random initialization, the primal-dual based algorithm can recover the unknown low-rank matrix X∗ successfully. In addition, Proj-GD demonstrates a sharp decrease in estimation error after the first several data passes, which is due to the simple but effective projection mechanism.\nIn addition, we investigate the statistical rate for one-bit matrix completion based on our algorithm. Figure 2(d) plots the averaged estimation error ‖X̂ − X∗‖2F /(d1d2) versus the rescaled sample size n/rd log d under different settings, which suggests that our primal-dual based approach achieves statistical rate with order O(rd log d/n)."
  }, {
    "heading": "7. Conclusions and Future Work",
    "text": "In this paper, we proposed a primal-dual based framework to characterize the global optimality for nonconvex lowrank matrix recovery with incoherence constraints. Based on duality, we proved that the optimization landscape of such problem is well-behaved. We further applied a primaldual based algorithm to solve the nonconvex optimization problem and demonstrated its effectiveness via simulations.\nThere are still some open problems along this line of research. For example, how to prove the optimization guarantees for the primal-dual based algorithm? Another question is how to generalize our framework to other constrained nonconvex optimization beyond incoherence constraints. We hope this work can act as the first step towards understanding the global geometry of general constrained nonconvex optimization problems."
  }, {
    "heading": "Acknowledgement",
    "text": "We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."
  }],
  "year": 2018,
  "references": [{
    "title": "Finding local minima for nonconvex optimization in linear time",
    "authors": ["N. Agarwal", "Z. Allen-Zhu", "B. Bullins", "E. Hazan", "T. Ma"],
    "venue": "arXiv preprint arXiv:1611.01146,",
    "year": 2016
  }, {
    "title": "Compressive multiplexing of correlated signals",
    "authors": ["A. Ahmed", "J. Romberg"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Convex relaxations of structured matrix factorizations",
    "authors": ["F. Bach"],
    "venue": "arXiv preprint arXiv:1309.3117,",
    "year": 2013
  }, {
    "title": "Convex sparse matrix factorizations",
    "authors": ["F. Bach", "J. Mairal", "J. Ponce"],
    "venue": "arXiv preprint arXiv:0812.1869,",
    "year": 2008
  }, {
    "title": "Optimal sample complexity for matrix completion and related problems via ell 2-regularization",
    "authors": ["Balcan", "M.-F", "Y. Liang", "D.P. Woodruff", "H. Zhang"],
    "venue": "arXiv preprint arXiv:1704.08683,",
    "year": 2017
  }, {
    "title": "1-bit matrix completion under exact low-rank constraint",
    "authors": ["S.A. Bhaskar", "A. Javanmard"],
    "venue": "In Information Sciences and Systems (CISS),",
    "year": 2015
  }, {
    "title": "Dropping convexity for faster semi-definite optimization",
    "authors": ["S. Bhojanapalli", "A. Kyrillidis", "S. Sanghavi"],
    "venue": "arXiv preprint,",
    "year": 2015
  }, {
    "title": "Global optimality of local search for low rank matrix recovery",
    "authors": ["S. Bhojanapalli", "B. Neyshabur", "N. Srebro"],
    "venue": "arXiv preprint arXiv:1605.07221,",
    "year": 2016
  }, {
    "title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization",
    "authors": ["S. Burer", "R.D. Monteiro"],
    "venue": "Mathematical Programming,",
    "year": 2003
  }, {
    "title": "A max-norm constrained minimization approach to 1-bit matrix completion",
    "authors": ["T. Cai", "Zhou", "W.-X"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Exact matrix completion via convex optimization",
    "authors": ["E.J. Candès", "B. Recht"],
    "venue": "Foundations of Computational mathematics,",
    "year": 2009
  }, {
    "title": "The power of convex relaxation: Nearoptimal matrix completion",
    "authors": ["E.J. Candès", "T. Tao"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 2010
  }, {
    "title": "Accelerated methods for non-convex optimization",
    "authors": ["Y. Carmon", "J.C. Duchi", "O. Hinder", "A. Sidford"],
    "venue": "arXiv preprint arXiv:1611.00756,",
    "year": 2016
  }, {
    "title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
    "authors": ["Y. Chen", "M.J. Wainwright"],
    "venue": "arXiv preprint arXiv:1509.03025,",
    "year": 2015
  }, {
    "title": "1-bit matrix completion",
    "authors": ["M.A. Davenport", "Y. Plan", "E. van den Berg", "M. Wootters"],
    "venue": "Information and Inference,",
    "year": 2014
  }, {
    "title": "A primal-dual algorithm for nonlinear programming exploiting negative curvature directions",
    "authors": ["G. Di Pillo", "G. Liuzzi", "S. Lucidi"],
    "venue": "Numerical Algebra, Control and Optimization,",
    "year": 2011
  }, {
    "title": "Escaping from saddle points-online stochastic gradient for tensor decomposition",
    "authors": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "Matrix completion has no spurious local minimum",
    "authors": ["R. Ge", "J.D. Lee", "T. Ma"],
    "venue": "arXiv preprint arXiv:1605.07272,",
    "year": 2016
  }, {
    "title": "No spurious local minima in nonconvex low rank problems: A unified geometric analysis",
    "authors": ["R. Ge", "C. Jin", "Y. Zheng"],
    "venue": "arXiv preprint arXiv:1704.00708,",
    "year": 2017
  }, {
    "title": "Recovering low-rank matrices from few coefficients in any basis",
    "authors": ["D. Gross"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2011
  }, {
    "title": "Low-rank and sparse structure pursuit via alternating minimization",
    "authors": ["Q. Gu", "Z. Wang", "H. Liu"],
    "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Towards faster rates and oracle property for low-rank matrix estimation",
    "authors": ["H. Gui", "Q. Gu"],
    "venue": "arXiv preprint arXiv:1505.04780,",
    "year": 2015
  }, {
    "title": "Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing",
    "authors": ["B. Haeffele", "E. Young", "R. Vidal"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2007
  }, {
    "title": "Understanding alternating minimization for matrix completion",
    "authors": ["M. Hardt"],
    "venue": "In FOCS, pp. 651–660",
    "year": 2014
  }, {
    "title": "Fast matrix completion without the condition number",
    "authors": ["M. Hardt", "M. Wootters"],
    "venue": "In COLT, pp",
    "year": 2014
  }, {
    "title": "Fast exact matrix completion with finite samples",
    "authors": ["P. Jain", "P. Netrapalli"],
    "venue": "arXiv preprint,",
    "year": 2014
  }, {
    "title": "Low-rank matrix completion using alternating minimization",
    "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"],
    "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
    "year": 2013
  }, {
    "title": "Provable efficient online matrix completion via non-convex stochastic gradient descent",
    "authors": ["C. Jin", "S.M. Kakade", "P. Netrapalli"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "How to escape saddle points efficiently",
    "authors": ["C. Jin", "R. Ge", "P. Netrapalli", "S.M. Kakade", "M.I. Jordan"],
    "venue": "arXiv preprint arXiv:1703.00887,",
    "year": 2017
  }, {
    "title": "Matrix completion from a few entries",
    "authors": ["R.H. Keshavan", "S. Oh", "A. Montanari"],
    "venue": "In 2009 IEEE International Symposium on Information Theory,",
    "year": 2009
  }, {
    "title": "Nuclearnorm penalization and optimal rates for noisy low-rank matrix completion",
    "authors": ["V. Koltchinskii", "K. Lounici", "Tsybakov", "A. B"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "Gradient descent only converges to minimizers",
    "authors": ["J.D. Lee", "M. Simchowitz", "M.I. Jordan", "B. Recht"],
    "venue": "In Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Near optimal compressed sensing of sparse rank-one matrices via sparse power factorization",
    "authors": ["K. Lee", "Y. Wu", "Y. Bresler"],
    "venue": "arXiv preprint arXiv:1312.0525,",
    "year": 2013
  }, {
    "title": "Symmetry, saddle points, and global geometry of nonconvex matrix factorization",
    "authors": ["X. Li", "Z. Wang", "J. Lu", "R. Arora", "J. Haupt", "H. Liu", "T. Zhao"],
    "venue": "arXiv preprint arXiv:1612.09296,",
    "year": 2016
  }, {
    "title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
    "authors": ["Loh", "P.-L", "M.J. Wainwright"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution",
    "authors": ["C. Ma", "K. Wang", "Y. Chi", "Y. Chen"],
    "venue": "arXiv preprint arXiv:1711.10467,",
    "year": 2017
  }, {
    "title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
    "authors": ["S. Negahban", "M.J. Wainwright"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise",
    "authors": ["S. Negahban", "M.J. Wainwright"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "A unified framework for high-dimensional analysis of mestimators with decomposable regularizers",
    "authors": ["S. Negahban", "B. Yu", "M.J. Wainwright", "P.K. Ravikumar"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Non-convex robust pca",
    "authors": ["P. Netrapalli", "U. Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Optimal statistical and computational rates for one bit matrix completion",
    "authors": ["R. Ni", "Q. Gu"],
    "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Sequential quadratic programming",
    "authors": ["J. Nocedal", "S.J. Wright"],
    "year": 2006
  }, {
    "title": "Provable non-convex projected gradient descent for a class of constrained matrix optimization problems",
    "authors": ["D. Park", "A. Kyrillidis", "S. Bhojanapalli", "C. Caramanis", "S. Sanghavi"],
    "venue": "arXiv preprint arXiv:1606.01316,",
    "year": 2016
  }, {
    "title": "Finding low-rank solutions to matrix problems, efficiently and provably",
    "authors": ["D. Park", "A. Kyrillidis", "C. Caramanis", "S. Sanghavi"],
    "venue": "arXiv preprint arXiv:1606.03168,",
    "year": 2016
  }, {
    "title": "Nonsquare matrix sensing without spurious local minima via the burer-monteiro approach",
    "authors": ["D. Park", "A. Kyrillidis", "C. Caramanis", "S. Sanghavi"],
    "venue": "arXiv preprint arXiv:1609.03240,",
    "year": 2016
  }, {
    "title": "A simpler approach to matrix completion",
    "authors": ["B. Recht"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
    "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"],
    "venue": "SIAM review,",
    "year": 2010
  }, {
    "title": "Fast maximum margin matrix factorization for collaborative prediction",
    "authors": ["J.D. Rennie", "N. Srebro"],
    "venue": "In Proceedings of the 22nd international conference on Machine learning,",
    "year": 2005
  }, {
    "title": "Estimation of high-dimensional low-rank matrices",
    "authors": ["A. Rohde", "Tsybakov", "A. B"],
    "venue": "The Annals of Statistics,",
    "year": 2011
  }, {
    "title": "Maximum-margin matrix factorization",
    "authors": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2004
  }, {
    "title": "Complete dictionary recovery over the sphere",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "arXiv preprint arXiv:1504.06785,",
    "year": 2015
  }, {
    "title": "A geometric analysis of phase retrieval",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "In Information Theory (ISIT),",
    "year": 2016
  }, {
    "title": "Guaranteed matrix completion via nonconvex factorization",
    "authors": ["R. Sun", "Luo", "Z.-Q"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2015
  }, {
    "title": "Low-rank solutions of linear matrix equations via procrustes flow",
    "authors": ["S. Tu", "R. Boczar", "M. Soltanolkotabi", "B. Recht"],
    "venue": "arXiv preprint arXiv:1507.03566,",
    "year": 2015
  }, {
    "title": "A unified computational and statistical framework for nonconvex low-rank matrix estimation",
    "authors": ["L. Wang", "X. Zhang", "Q. Gu"],
    "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "A unified variance reductionbased framework for nonconvex low-rank matrix recovery",
    "authors": ["L. Wang", "X. Zhang", "Q. Gu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Speeding up latent variable gaussian graphical model estimation via nonconvex optimization",
    "authors": ["P. Xu", "J. Ma", "Q. Gu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1941
  }, {
    "title": "A unified framework for nonconvex low-rank plus sparse matrix recovery",
    "authors": ["X. Zhang", "L. Wang", "Q. Gu"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2018
  }, {
    "title": "A nonconvex optimization framework for low rank matrix estimation",
    "authors": ["T. Zhao", "Z. Wang", "H. Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements",
    "authors": ["Q. Zheng", "J. Lafferty"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent",
    "authors": ["Q. Zheng", "J. Lafferty"],
    "venue": "arXiv preprint arXiv:1605.07051,",
    "year": 2016
  }, {
    "title": "Global optimality in low-rank matrix optimization",
    "authors": ["Z. Zhu", "Q. Li", "G. Tang", "M.B. Wakin"],
    "venue": "arXiv preprint arXiv:1702.07945,",
    "year": 2017
  }, {
    "title": "The global optimization geometry of nonsymmetric matrix factorization and sensing",
    "authors": ["Z. Zhu", "Q. Li", "G. Tang", "M.B. Wakin"],
    "venue": "arXiv preprint arXiv:1703.01256,",
    "year": 2017
  }],
  "id": "SP:88371ff032baadc135d104e54bf31283c62a832f",
  "authors": [{
    "name": "Xiao Zhang",
    "affiliations": []
  }, {
    "name": "Lingxiao Wang",
    "affiliations": []
  }, {
    "name": "Yaodong Yu",
    "affiliations": []
  }, {
    "name": "Quanquan Gu",
    "affiliations": []
  }],
  "abstractText": "We propose a primal-dual based framework for analyzing the global optimality of nonconvex lowrank matrix recovery. Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality. We illustrate the applicability of the proposed framework to matrix completion and one-bit matrix completion, and prove that all these problems have no spurious local minima. Our results not only improve the sample complexity required for characterizing the global optimality of matrix completion, but also resolve an open problem in Ge et al. (2017) regarding one-bit matrix completion. Numerical experiments show that primaldual based algorithm can successfully recover the global optimum for various low-rank problems.",
  "title": "A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank  Matrix Recovery"
}