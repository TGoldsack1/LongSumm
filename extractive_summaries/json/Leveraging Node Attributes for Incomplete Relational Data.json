{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Relational learning from network data, particularly with probabilistic methods, has gained a wide range of applications such as social network analysis (Xiang et al., 2010), recommender systems (Gopalan et al., 2014b), knowledge graph completion (Hu et al., 2016b), and bioinformatics (Huopaniemi et al., 2010). Generally speaking, the goal of relational learning is to discover and analyse latent clusters of entities (i.e., community detection), and predict missing links (i.e., link prediction).\nThe standard approach for modelling relational data is latent factor analysis via matrix factorisation and its variations. Among the existing approaches, Non-negative Matrix Factorisation (NMF) and the Stochastic Block Model (SBM) are prominent foundational methods. NMF is usually used to model relationships between two sets of entities such as users and movies in collaborative filtering (Mnih & Salakhutdinov, 2008). While developed independently, SBM (Wang & Wong, 1987; Nowicki & Snijders, 2001) can be viewed as an extension of NMF that introduces\n1Faculty of Information Technology, Monash University, Australia. Correspondence to: He Zhao <he.zhao@monash.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\na block matrix to capture the interactions between latent factors. There have been many Bayesian extensions of these two methods, relaxing the assumptions and/or introducing extra components, such as the Infinite Relational Model (IRM) (Kemp et al., 2006), the mixture membership stochastic block model (MMSB) (Airoldi et al., 2008), and the non-parametric latent feature models (NLFM) (Miller et al., 2009). Poisson Factorisation (PF) (Dunson & Herring, 2005; Zhou et al., 2012), is a popular version of NMF which models count data with convenient statistical properties (Gopalan et al., 2014b; 2015). Combining the ideas of PF and SBM, the infinite Edge Partition Model (EPM) (Zhou, 2015) and its extensions (Hu et al., 2016b) have proven successful for relational networks.\nWhen a network has less data, relational learning becomes more difficult. One extreme case is the cold-start problem (Lin et al., 2013; Sedhain et al., 2014; Zhang & Wang, 2015), where a node has no observed links, making suggestion of links for that node even more challenging. In such cases, it is natural to appeal to side information such as node attributes or features. For instance, papers in citation networks are often associated with categories and authors, and users in Facebook or Twitter are often asked to provide information such as age, gender and interests. It is reasonable to assume that nodes having similar attributes are more likely to relate to each other (i.e., homophily, Nickel et al., 2016). Thus, node attributes serve as important complementary information to relational data.\nThere are few Bayesian probabilistic relational models that are able to leverage side information. For example, NLFM uses a linear regression model to transform the features of each node into a single number, which contributes to link probabilities. However, side information in NLFM cannot directly influence the latent factors, which gives little support for community detection. As an extension of MMSB, the Non-parametric Meta-data Dependent Relational (NMDR) model (Kim et al., 2012) incorporates attributes into the mixed-membership distribution of each node with the logistic-normal transform, which results in non-conjugacy for inference. Fan et al. (2016) further developed this idea in the Node information Involved Mixture Membership model (niMM), where side information is integrated in a conjugate way. Although these models demonstrate improvement using side information, they\nscale quadratically in the number of nodes and the incorporation of side information is often complicated.\nSeveral recent methods (Gopalan et al., 2014a; Acharya et al., 2015; Hu et al., 2016a) extend PF with side information using the additivity of the Poisson and gamma distributions/processes. With improved scalability, the Structural Side Information Poisson Factorisation (SSI-PF) (Hu et al., 2016a) models directed unweighted networks with node labels, such as citation networks with papers labelled with one of several categories. However, its performance remains untested when a node has multiple attributes. Moreover, undirected networks are not handled by SSI-PF.\nIn this paper we present the Node Attribute Relational Model (NARM)1, a fully Bayesian approach that models large, sparse, and unweighted relational networks with arbitrary node attributes encoded in binary form. It works with Poisson gamma relational models to incorporate side information. Specifically, we propose the Symmetric NARM (Sym-NARM) for undirected networks, an extension of EPM (Zhou, 2015) and the Asymmetric NARM (Asym-NARM) for directed networks, an extension of PF (Zhou et al., 2012). The proposed models have several key properties: (1) Effectively modelling node attributes: the proposed models are able to achieve improved link prediction performance, especially where training data are limited. (2) Fully Bayesian and conjugate: the inference is done by efficient, closed-form Gibbs sampling which scales linearly in the number of observed links and takes advantage of the sparsity of node attributes. It makes our models scalable for large but sparse relational networks with large sets of node attributes. (3) Flexibility: the proposed models work on directed and undirected relational networks with flat and hierarchical node attributes."
  }, {
    "heading": "2. The Node Attribute Relational Model",
    "text": "Here we focus on modelling unweighted networks that can be either directed (i.e., the relationship is asymmetric) or undirected. Assume a relational network with N nodes is stored in a binary adjacency matrix Y ∈ {0, 1}N×N where yi,j = 1 indicates the presence of a link between nodes i and j. If the relationship described in the network is symmetric, then yi,j = yj,i, and if asymmetric, possibly yi,j 6= yj,i. Node attributes are encoded in a binary matrix F ∈ {0, 1}N×L, where L is the total number of attributes. Attribute fi,l = 1 indicates attribute l is active with node i and vice versa. Although our models incorporate binary attributes, categorical attributes and real-valued attributes can be converted into binary values with proper transformations (Kim et al., 2012; Fan et al., 2016; Hu et al., 2016a).\n1Code available at https://github.com/ ethanhezhao/NARM/"
  }, {
    "heading": "2.1. The Symmetric Node Attribute Relational Model",
    "text": "Sym-NARM works with undirected networks. Its generative process is shown in Figure 1. Instead of modelling the binary matrix Y directly, it applies the Bernoulli-Poisson link (BPL) function (Zhou, 2015) using an underlying latent count matrix X. One first draws a latent count xi,j from the Poisson distribution and then thresholds it at 1 to generate a binary value yi,j . This is shown in Eqs. (1)- (3). Analysed in (Zhou, 2015; Hu et al., 2016b;a), BPL has the appealing property that if yi,j = 0, then xi,j = 0 with probability one. Thus, only non-zeros in Y need to be sampled, giving huge computational savings for large sparse networks, illustrated in Section 3 and Section 5.4.\nThe latent matrix X is further factorised into K latent factors with a non-negative bilinear model: X ∼ Poi(ΦΛΦT ) where Φ ∈ RN×K+ and Λ ∈ RK×K+ . Φ is referred to as the node factor loading matrix where φi,k models the strength of the connection between node i and latent factor k. As in SBM, the correlations of the latent factors are modelled in a symmetric matrix Λ, referred to as the block matrix. Following (Zhou, 2015), we draw Λ from a hierarchical relational gamma process (implemented with truncation as a vector of gamma variables) , shown in Eqs. (8) and (9).\nOne appealing aspect of our model is the incorporation of node attributes on the prior of φi,k (i.e., gi,k). Shown in Eq. (5), gi,k is constructed with a log linear combination of fi,l. hl,k is referred to as the kth attribute factor loading of attribute l, which influences gi,k iff attribute l is active with node i (i.e., fi,l = 1). bk acts as an attribute-free bias for each latent factor k. hl,k and bk are gamma distributed with mean 1, hence if attribute l does not contribute to latent factor k or is less useful, hl,k is expected to be near 1 and to have little influence on gi,k. The hyper-parameter µ0 controls the variation of hl,k.\nThe intuition of our model is: if two nodes have more common attributes, their gamma shape parameters will be more similar, with similar node factor loadings, resulting in a larger probability that they relate to each other. Moreover, instead of incorporating the node attributes directly into the node factor loadings, Sym-NARM uses them as the prior information using Eq. (4), which results in a principled way of balancing the side information and the network data. In addition, different attributes can contribute differently to the latent factors. For example, the gender of an author may be much less important to co-authorship with others than the research fields. This is controlled by the attribute factor loading hl,k in our model."
  }, {
    "heading": "2.2. The Asymmetric Node Attribute Relational Model",
    "text": "Extending the Beta Gamma Gamma Poisson factorisation (BGGPF) (Zhou et al., 2012), Asym-NARM works on di-\nrected relational networks with node attributes incorporated in a similar way to Sym-NARM. Figure 2 shows its generative process. Here the latent count matrix X is factorised as X ∼ Poi(ΦΘ), where Φ ∈ RN×K+ and Θ ∈ RK×N+ are referred to as the factor loading matrix and the factor score matrix respectively. Similar to SSI-PF, the node attributes are incorporated on the prior of Φ."
  }, {
    "heading": "2.3. Incorporating Hierarchical Node Attributes",
    "text": "Relational networks can be associated with hierarchical side information (Hu et al., 2016a). For example, in a patent citation network, patents can be labelled with the International Patent Classification (IPC) code, which is a hierarchy of patent categories and sub-categories. Suppose the second level attributes are stored in a binary matrix F′ ∈ {0, 1}L×M whereM is the number of attributes in the second level. Our models can be used to incorporate hierarchical node attributes via a straightforward extension: re-\nplace hyper-parameter µ0 in Eq. (6) with µl,k = ∏M m δ f ′l,m m,k . This extension mirrors what is done for first level attributes."
  }, {
    "heading": "3. Inference with Gibbs Sampling",
    "text": "Both Sym-NARM and Asym-NARM enjoy local conjugacy so the inference of all latent variables can be done by closed-form Gibbs sampling. Moreover, the inference only needs to be conducted on the non-zero entries in Y and F. This section focuses on the sampling of hl,k (bk), the key variable in the proposed incorporation of node attributes. The sampling of the other latent variables is similar to those in EPM and BGGPF, detailed in (Zhou, 2015;\nZhou et al., 2012). As the sampling for hl,k is analogous in Sym-NARM and Asym-NARM, our introduction will be based on Asym-NARM alone.\nWith the Poisson gamma conjugacy, the likelihood for gi,k with φi,k marginalised out is:\np(gi,k | xi,·,k) ∝ (1− qk)gi,k Γ(gi,k + xi,·,k)\nΓ(gi,k) (19) where xi,·,k = ∑\nj xi,j,k and xi,j,k is the latent count. The gamma ratio in Eq. (19), i.e., the Pochhammer symbol for a rising factorial, can be augmented with an auxiliary variable ti,k:\nΓ(gi,k+xi,·,k) Γ(gi,k)\n= ∑xi,·,k\nti,k=0 S xi,·,k ti,k g ti,k i,k where S x t in-\ndicates an unsigned Stirling number of the first kind (Chen et al., 2011; Teh et al., 2012; Zhou & Carin, 2015).\nTakingO(xi,·,k), ti,k can be directly sampled by a Chinese Restaurant Process with gi,k as the concentration and xi,·,k as the number of customers:\nti,k ← ti,k + Bern (\ngi,k gi,k + i′\n) for i′ = 1 : xi,·,k (20)\nwhere Bern(·) is the Bernoulli distribution. Alternatively, for large xi,·,k, because the standard deviation of ti,k is O( √\nlog xi,·,k) (Buntine & Hutter, 2012), one can sample ti,k in a small window around the current value (Du et al., 2010).\nWith the above augmentation and Eq. (15), we get: p(G,H | x:,·,:,T,F) ∝ (21) N∏ i=1 K∏ k=1 S xi,·,k ti,k e − log ( 1 1−qk ) gi,k · L∏ l=1 K∏ k=1 h ∑N i=1 fi,lti,k l,k\nRecall that all the attributes are binary and hl,k influences gi,k only when fi,l = 1. Extracting all the terms related to\nhl,k in Eq. (21), we get the likelihood of hl,k:\np ( hl,k ∣∣∣∣ gi,khl,k , t:,k, f:,l ) ∝ (22)\ne −hl,k log\n( 1\n1−qk )∑N i=1:fi,l=1 gi,k hl,k h ∑N i=1 fi,lti,k\nl,k\nwhere gi,khl,k is the value of gi,k with hl,k removed when fi,l = 1. The likelihood function above is in a form that is conjugate to the gamma prior. Therefore, it is straightforward to yield the following sampling strategy for hl,k:\nhl,k ∼ Ga(µ′, 1/ν′) (23)\nµ′ = µ0 + N∑ i=1:fi,l=1 ti,k (24)\nν′ = 1/µ0 − log (1− qk) N∑\ni=1:fi,l=1\ngi,k hl,k\n(25)\nPrecomputed with Eq. (15), gi,k can be updated with Eq. (26), after hl,k is sampled.\ngi,k ← gi,kh\n′ l,k\nhl,k for i = 1 : N and fi,l = 1 (26)\nwhere h′i,k is the newly sampled value of hi,k.\nTo compute Eqs. (24)-(26), we only need to iterate over the nodes that attribute l is active with (i.e., fi,l = 1). Thus, the sampling for H takes O(D′KL) where D′ is the average number of nodes that an attribute is active with. This demonstrates how the sparsity of node attributes is leveraged. As the mean of xi,·,k is D/K, sampling the tables T ∈ NN×K takes O(ND) which can be accelerated with the window sampling technique explained above.\nWe show the computational complexity of our and related models in Table 1. The empirical comparison of running speed is in Section 5.4. By taking advantage of both network sparsity and node attribute sparsity, our models are more efficient than the competitors, especially on large sparse networks with large sets of attributes."
  }, {
    "heading": "4. Related work",
    "text": "Compared with the node-attribute models such as NMDR and niMM whose methods result in complicated inference, our Sym-NARM is much more efficient on large sparse networks, illustrated in Table 1.\nThe most closely related model to our Asym-NARM, also extending the BGGPF algorithm, is SSI-PF. But it uses the gamma additivity to construct the prior of node factor loadings with the sum of attribute factor loadings. Our model has several advantages over SSI-PF: (1) The derivation of Gibbs sampling of SSI-PF requires that each column of Θ is normalised (Eq. (18)). This limits the application of SSI-PF to other models such as EPM which is an unnormalised model. (2) Shown in Table 1, Asym-NARM enjoys more efficient computational complexity. (3) Shown\nin Section 5, our model is more effective especially when a node has multiple attributes.\nThere are also models that extend PF and collective matrix factorisation (Singh & Gordon, 2008) to jointly factorise relational networks and document-word matrices such as (Gopalan et al., 2014a; Zhang & Wang, 2015; Acharya et al., 2015). Our NARM models incorporate general node attributes (not only texts) as the priors of the factor loading matrix in a supervised manner, rather than jointly modelling the side information in an unsupervised manner.\nAnother related area is supervised topic models such as (Mcauliffe & Blei, 2008; Ramage et al., 2009; Lim & Buntine, 2016). The Dirichlet Multinomial Regression (DMR) model (Mimno & McCallum, 2012) is the most related one to ours. It models document attributes on the priors of the topic proportions with the logistic-normal transform. For comparison, we propose DMR-MMSB, extending MMSB with the DMR technique to incorporate side information on the mixed-membership distribution of each node."
  }, {
    "heading": "5. Experiments",
    "text": "In this section we evaluate Sym-NARM and Asym-NARM with a set of the link prediction tasks on 10 real-world relational datasets with different sizes and various kinds of node attributes. We compare our models with the stateof-the-art relational models, demonstrating that our models outperform the competitors on those datasets in terms of link prediction performance and per-iteration running time. We report the average area under the curve of both the receiver operating characteristic (AUC-ROC) and precision recall (AUC-PR) for quantitatively analysing the models. Moreover, we perform qualitative analysis by comparing the link probabilities estimated by the compared models."
  }, {
    "heading": "5.1. Link Prediction on Undirected Networks",
    "text": "For the link prediction task on undirected network data, we compared our Sym-NARM with two models that do\nnot consider node attributes, EPM (Zhou, 2015), a stateof-the-art relational model, and iMMM (Koutsourelakis & Eliassi-Rad, 2008), a non-parametric version of MMSB,\nand two node attribute models, niMM (Fan et al., 2016), a non-parametric relational model which has been demonstrated to outperform NMDR (Kim et al., 2012), and DMRMMSB, our extension to MMSB using the Dirichlet Multinomial Regression (Mimno & McCallum, 2012). SymNAMR was implemented in MATLAB on top of the EPM code and we used the code released by the original authors for EPM and niMM. iMMM was implemented by Fan et al. (2016) as a variant of niMM.\nThe description of the four datasets used is given below:\n• Lazega-cowork: This dataset (Lazega, 2001) contains 378 links of the co-work relationship among 71 attorneys. Each attorney is associated with attributes such as gender, office location, and age. After discretisation and binarisation, we derived a 71× 18 binary node attribute matrix with 497 non-zero entries. • NIPS234: This is a co-author network of the 234 authors with 598 links extracted from NIPS 1-17 conferences (Zhou, 2015). We merged all the papers written by the same author as a document, and then trained a LDA model with 100 topics. The 5 most frequent topics were used as the attributes, which gives us a 234 × 100 attribute matrix with 1170 non-zero entries. • Facebook-ego: The original dataset (McAuley & Leskovec, 2012) was collected from survey participants of Facebook users. Out of the 10 circles (i.e., friend lists), we used the first circle that contains 347 users with 2519 links. Each user is associated with 227 binary attributes, encoding side information such as age, gender, and education. We got a 347×227 binary node attribute matrix with 3318 non-zero entries. • NIPS12: NIPS12 was collected from NIPS papers in vols 0-12. It is a median-size co-author network with 2037 authors and 3134 links. Similar to NIPS234, we used the 5 most frequent topics as the attributes for each author. We got a 2037×100 binary node attribute matrix with 10185 non-zero entries."
  }, {
    "heading": "5.1.1. EXPERIMENTAL SETTINGS",
    "text": "For each dataset, we varied the training data from 10% to 90% and used the remaining in testing. For each proportion, to generate five random splits, we used the code in the EPM package (Zhou, 2015) which splits a network in terms of its nodes. The reported AUC-ROC/PR scores were averaged over the five splits. We used the default hyper-parameter settings enclosed in the released code for EPM, niMM and iMMM. For our Sym-NARM, we set µ0 = 1 and all the other hyper-parameters the same as those in EPM. Note that the models in comparison except DMR-MMSB are non-parametric models. For Sym-NARM and EPM, we set the truncation level large enough for each dataset: Kmax = 50, 100, 256 for Lazega-\ncowork, Facebook-ego and NIPS234, NIPS12 respectively. For DMR-MMSB, we varied K in {5, 10, 25, 50} and reported the best one. Following (Zhou, 2015), we used 3000 MCMC iterations and computed AUC-ROC/PR with the average probability over the last 1500. The performance of iMMM and niMM on NIPS12 and DMR-MMSB on Facebook-ego and NIPS12 are not reported as the datasets are too large for them given our computational resources."
  }, {
    "heading": "5.1.2. RESULTS",
    "text": "The AUC-ROC/PR scores are reported in Figure 3. Overall, our Sym-NARM model performs significantly better than niMM, iMMM, and DMR-MMSB on all the datasets, and EPM on 3 datasets (except Facebook-ego with large training proportions). It is interesting that the performance of EPM on Facebook-ego gradually approaches ours when more than 30% training data were used. Note that Facebook-ego is much denser than the others, which means the network information itself could be rich enough for EPM to reconstruct the network and the node attributes contribute less. However in general, when relational data are highly incomplete (with less training data), our model is able to achieve improved link prediction performance.\nTo illustrate how side information helps, we qualitatively compared our model with EPM and niMM by estimating the link probabilities on NIPS234, shown in Figure 4. With 20% training data, EPM does not give a meaningful reconstruction of the original network, but it starts to with more data presented. The similarity of the authors’ topics in Figure 4e matches the original network, demonstrating the usefulness of the topics, but with some error. Using the topics as the authors’ attributes, our Sym-NARM achieves reasonably good reconstruction of the network with only 20% training data, further improving with 80% training data. Although niMM uses the same node attributes, its performance is not as good and is even outperformed by EPM with 80% training data."
  }, {
    "heading": "5.2. Link Prediction on Directed Networks",
    "text": "Here we compared our Asym-NARM (implemented in MATLAB on top of the BGGPF code) with two models that do not consider node attributes, BGGPF (Zhou et al., 2012) and iMMM, and three node-attribute models, niMM, SSIPF (Hu et al., 2016a) and DMR-MMSB. We used the following four datasets:\n• Lazega-advice: This dataset is a directed network with 892 links of the advice relation among the attorneys. The node attributes are the same as in Lazega-cowork. • Citeseer: This dataset2 contains a citation network with 2http://linqs.umiacs.umd.edu/projects/ /projects/lbc/index.html\n4591 links of 3312 papers, labelled with one of 6 categories. For each paper, we used both the category label and the presence/absence of 500 most frequent words as two separate attribute sets. We got a 3312 × 500 word attribute matrix with 65674 non-zero entries. • Cora: This dataset2 contains a citation network with 5429 links of 2708 papers in machine learning, labelled with one of 7 categories. Similar to Citeseer, we used both the category label and the 500 most frequent words as two separate attribute sets. We got a 2708×500 word attribute matrix with 39268 non-zero entries. • Aminer: The Aminer dataset (Tang et al., 2009) contains a citation network with 2555 papers labelled with 10 categories and 5967 links. We further collected information of each paper via the Aminer’s API, including the authors’ names (2597 unique authors), abstract, venue, year, and number of citations. For the abstract, we extract the 5 most frequent topics for each paper in a similar way to NIPS234. In total, we prepared two sets of attributes: the labels and the others formed with the combination of all collected information."
  }, {
    "heading": "5.2.1. EXPERIMENTAL SETTINGS",
    "text": "For fair comparison, we generated training/testing data with the code in the SSI-PF package, which splits a network in terms of its links. We used the default hyper-parameter settings of BGGPF, SSI-PF, and niMM, provided by the original authors. Kmax was set to 50 on Lazega-advice and 200 (same as (Hu et al., 2016a)) on all the other three datasets. For our Asym-NARM, we set µ0 = 1 and the\nother hyper-parameters the same as those used in (Zhou et al., 2012; Hu et al., 2016a). Following the suggestion of Hu et al. (2016a), we used 1500 MCMC iterations in total and the last 500 samples to compute the AUC-ROC/PR scores. Since Citeseer, Cora, and Aminer are already too large for niMM, iMMM, and DMR-MMSB to produce results in reasonable time given our computational resources, we reported their performance only on Lazega-advice."
  }, {
    "heading": "5.2.2. RESULTS",
    "text": "Shown in Figure 5a, Asym-NARM gains better results in terms of AUC-ROC/PR on Lazega-advice in most of the training proportions. Overall, the node-attribute models perform better than the models that do not consider node attributes, showing the usefulness of node attributes. On the other three datasets, we used different sets of attributes to study how different attributes influence the performance of Asym-NARM and SSI-PF.\nIn general, Asym-NARM performs better than SSI-PF regardless of which set of attributes is used. The performance of SSI-PF approaches ours in Citeseer with the labels as attributes (indicated by “-l”). But the gap between SSI-PF and our model becomes larger when the words are used as attributes (indicated by “-w”). In Cora, SSI-PF with the words does not perform as well as its non-node-attribute counterpart, BGGPF, indicating it may not be as robust as our model with large sets of attributes. To investigate this, we varied the number of the most frequent words from 10 to 500 for Asym-NARM and SSI-PF on Citeseer and Cora. With more words, the AUC-ROC/PR score of SSI-PF de-\ngrades increasingly. We further checked the prior of the node factor loadings in SSI-PF (the variable that incorporates node attributes and corresponds to gi,k in our model) and found that the coefficient of variation of each node’s prior drops dramatically, indicating with more words, SSIPF is failing to use the supervised information in the words."
  }, {
    "heading": "5.3. Link Prediction with Hierarchical Node Attributes",
    "text": "Here we used two datasets with hierarchical node attributes: (1) Cora-hier: a citation network with 1712 papers and 6308 links extracted from the original Cora dataset3. The papers are labelled with one of 63 sub-areas (first level) and each sub-area belongs to one of 10 primary areas (second level), such as “machine learning in artificial intelligence” and “memory management in operating systems”; (2) Patent-hier: a citation network with 1461 patents and 2141 links from the National Bureau of Economic Research where the hierarchical International Patent Classification (IPC) code of a patent is used as attributes.\nThe AUC-ROC/PR scores in Figure 6 show that our AsymNARM with hierarchical attributes outperforms the others, which demonstrates leveraging hierarchical side information is beneficial to link prediction. Although SSI-PF also models the hierarchical attributes, its performance in these two datasets is not comparable with our model’s."
  }, {
    "heading": "5.4. Running Time",
    "text": "In this section, we compare the running time of the models for directed networks (all implemented in MATLAB and running on a desktop with 3.40 GHz CPU and 16GB RAM). Using 80% data for training, the running time for Asym-NARM, SSI-PF, and niMM on Aminer with different sets of node attributes is reported in Table 2. Note DMR-MMSB did not complete with “Authors” and “All” due to our computational resources. Asym-NARM is about 10 times faster than SSI-PF with all the attributes and about\n3https://people.cs.umass.edu/˜mccallum/ data.html\n2 times faster with the labels. Thus Asym-NARM is more efficient, especially with large sets of attributes, supporting the complexity analysis in Table 1."
  }, {
    "heading": "6. Conclusion",
    "text": "As a summary of the experiments, Asym/Sym-NARM achieved better link prediction performance with faster inference. While EPM, a non-node-attribute model, performed well on nearly complete networks, it degraded with less training data. niMM and DMR-MMSB, extensions to MMSB with the logistic-normal transform, had similar results to Sym-NARM but scaled inefficiently. SSI-PF’s performance and scalability were not as good as Asym-NARM in the presented cases with flat and hierarchical attributes and it was less effective with larger numbers of attributes.\nThus NARM is a comparatively simple yet effective and efficient way of incorporating node attributes, including hierarchical attributes, for relational models with Poisson likelihood. This leads to improved link prediction and matrix completion for less complete relational data of both directed and undirected networks. With the efficient inference, our models can be used to model large sparse relational networks with node attributes.\nNARM can easily be extended to multi-relational networks such as (Hu et al., 2016b) and topic models with document and word attributes, which is left for our future work."
  }],
  "year": 2017,
  "references": [{
    "title": "Gamma process Poisson factorization for joint modeling of network and documents. In Machine Learning and Knowledge Discovery in Databases",
    "authors": ["A. Acharya", "D. Teffer", "J. Henderson", "M. Tyler", "M. Zhou", "J. Ghosh"],
    "venue": "European Conference,",
    "year": 2015
  }, {
    "title": "Mixed membership stochastic blockmodels",
    "authors": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "A Bayesian view of the Poisson-Dirichlet process",
    "authors": ["W. Buntine", "M. Hutter"],
    "venue": "arXiv preprint arXiv:1007.0296v2 [math.ST],",
    "year": 2012
  }, {
    "title": "Sampling table configurations for the hierarchical Poisson-Dirichlet process. In Machine Learning and Knowledge Discovery in Databases",
    "authors": ["C. Chen", "L. Du", "W. Buntine"],
    "venue": "European Conference,",
    "year": 2011
  }, {
    "title": "A segmented topic model based on the two-parameter Poisson-Dirichlet process",
    "authors": ["L. Du", "W. Buntine", "H. Jin"],
    "venue": "Machine Learning,",
    "year": 2010
  }, {
    "title": "Bayesian latent variable models for mixed discrete outcomes",
    "authors": ["D.B. Dunson", "A.H. Herring"],
    "year": 2005
  }, {
    "title": "Learning nonparametric relational models by conjugately incorporating node information in a network",
    "authors": ["X. Fan", "D. Xu", "R. Yi", "L. Cao", "Y. Song"],
    "venue": "IEEE transactions on cybernetics,",
    "year": 2016
  }, {
    "title": "Content-based recommendations with Poisson factorization",
    "authors": ["P. Gopalan", "L. Charlin", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Bayesian nonparametric Poisson factorization for recommendation systems",
    "authors": ["P. Gopalan", "F.J. Ruiz", "R. Ranganath", "D.M. Blei"],
    "venue": "In 17th International Conference on Artificial Intelligence and Statistics,",
    "year": 2014
  }, {
    "title": "Scalable recommendation with hierarchical Poisson factorization",
    "authors": ["P. Gopalan", "J.M. Hofman", "D.M. Blei"],
    "venue": "In 31st Conference on Uncertainty in Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Non-negative matrix factorization for discrete data with hierarchical sideinformation",
    "authors": ["C. Hu", "P. Rai", "L. Carin"],
    "venue": "In 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Topic-based embeddings for learning from large knowledge graphs",
    "authors": ["C. Hu", "P. Rai", "L. Carin"],
    "venue": "In 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Multivariate multi-way analysis of multisource data",
    "authors": ["I. Huopaniemi", "T. Suvitaival", "J. Nikkilä", "M. Orešič", "S. Kaski"],
    "venue": "Bioinformatics, 26(12):i391–i398,",
    "year": 2010
  }, {
    "title": "Learning systems of concepts with an infinite relational model",
    "authors": ["C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda"],
    "venue": "In 21st National Conference on Artificial Intelligence,",
    "year": 2006
  }, {
    "title": "The nonparametric metadata dependent relational model",
    "authors": ["D.I. Kim", "M. Hughes", "E. Sudderth"],
    "venue": "In 29th International Conference on Machine Learning,",
    "year": 2012
  }, {
    "title": "Finding mixedmemberships in social networks",
    "authors": ["Koutsourelakis", "P.-S", "T. Eliassi-Rad"],
    "venue": "In AAAI Spring Symposium: Social Information Processing,",
    "year": 2008
  }, {
    "title": "The collegial phenomenon: The social mechanisms of cooperation among peers in a corporate law partnership",
    "authors": ["E. Lazega"],
    "venue": "Oxford University Press on Demand,",
    "year": 2001
  }, {
    "title": "Bibliographic analysis on research publications using authors, categorical labels and the citation network",
    "authors": ["K. Lim", "W. Buntine"],
    "venue": "Machine Learning,",
    "year": 2016
  }, {
    "title": "Learning to discover social circles in ego networks",
    "authors": ["J.J. McAuley", "J. Leskovec"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Supervised topic models",
    "authors": ["J.D. Mcauliffe", "D.M. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2008
  }, {
    "title": "Nonparametric latent feature models for link prediction",
    "authors": ["K. Miller", "M.I. Jordan", "T.L. Griffiths"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Topic models conditioned on arbitrary features with Dirichlet-multinomial regression",
    "authors": ["D. Mimno", "A. McCallum"],
    "venue": "In 24th Conference on Uncertainty in Artificial Intelligence,",
    "year": 2012
  }, {
    "title": "Probabilistic matrix factorization",
    "authors": ["A. Mnih", "R. Salakhutdinov"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2008
  }, {
    "title": "A Review of Relational Machine Learning for Knowledge Graphs",
    "authors": ["M. Nickel", "K. Murphy", "V. Tresp", "E. Gabrilovich"],
    "venue": "Proceedings of the IEEE,",
    "year": 2016
  }, {
    "title": "Estimation and prediction for stochastic blockstructures",
    "authors": ["K. Nowicki", "T.A.B. Snijders"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2001
  }, {
    "title": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora",
    "authors": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"],
    "venue": "In 2009 Conference on Empirical Methods in Natural Language Processing: Volume",
    "year": 2009
  }, {
    "title": "Social collaborative filtering for cold-start recommendations",
    "authors": ["S. Sedhain", "S. Sanner", "D. Braziunas", "L. Xie", "J. Christensen"],
    "venue": "In 8th ACM Conference on Recommender Systems,",
    "year": 2014
  }, {
    "title": "Relational learning via collective matrix factorization",
    "authors": ["A.P. Singh", "G.J. Gordon"],
    "venue": "In 14th ACM SIGKDD International Conference on Knowledge discovery and data mining,",
    "year": 2008
  }, {
    "title": "Social influence analysis in large-scale networks",
    "authors": ["J. Tang", "J. Sun", "C. Wang", "Z. Yang"],
    "venue": "In 15th ACM SIGKDD International Conference on Knowledge discovery and data mining,",
    "year": 2009
  }, {
    "title": "Hierarchical Dirichlet processes",
    "authors": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2012
  }, {
    "title": "Stochastic blockmodels for directed graphs",
    "authors": ["Y.J. Wang", "G.Y. Wong"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1987
  }, {
    "title": "Modeling relationship strength in online social networks",
    "authors": ["R. Xiang", "J. Neville", "M. Rogati"],
    "venue": "In 19th International Conference on World Wide Web,",
    "year": 2010
  }, {
    "title": "A collective Bayesian Poisson factorization model for cold-start local event recommendation",
    "authors": ["W. Zhang", "J. Wang"],
    "venue": "In 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2015
  }, {
    "title": "Infinite edge partition models for overlapping community detection and link prediction",
    "authors": ["M. Zhou"],
    "venue": "In 18th International Conference on Artificial Intelligence and Statistics,",
    "year": 2015
  }, {
    "title": "Negative binomial process count and mixture modeling",
    "authors": ["M. Zhou", "L. Carin"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }, {
    "title": "Betanegative binomial process and Poisson factor analysis",
    "authors": ["M. Zhou", "L. Hannah", "D.B. Dunson", "L. Carin"],
    "venue": "In 15th International Conference on Artificial Intelligence and Statistics,",
    "year": 2012
  }],
  "id": "SP:98c4131f5c3676f81f0395e1519b91eaf976cfa7",
  "authors": [{
    "name": "He Zhao",
    "affiliations": []
  }, {
    "name": "Lan Du",
    "affiliations": []
  }, {
    "name": "Wray Buntine",
    "affiliations": []
  }],
  "abstractText": "Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the stateof-the-art link prediction results, especially with highly incomplete relational data.",
  "title": "Leveraging Node Attributes for Incomplete Relational Data"
}