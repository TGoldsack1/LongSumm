{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Gaussian graphical models (Lauritzen, 1996) (GGM) have been widely used in the field of statistical machine learning. The goal is to estimate the precision matrix, which captures the conditional dependency relationship among marginal variables of high dimensional random vectors. One typical application of Gaussian graphical models is to study the conditional independence among the genes at the expression level in genomics, and to estimate the gene regulatory network. In recent years, it has been noticed that one can elaborate a GGM model with additional side information for better estimation accuracy. For example, genetic variants have been shown to have great potential influence on gene expression (Brem & Kruglyak, 2005; Cheung & Spielman, 2002), yet directly applying Gaussian graphical model to gene expression data would neglect such a fact, hence may hinder us from revealing the intrinsic gene regulation relationships. On the other hand, utilizing such genetic variant\n1Department of Computer Science, University of Virginia, Charlottesville, VA 22904, USA 2Department of Computer Science, University of California, Los Angeles, CA 90095, USA 3School of Computer Science, Carnegie Mellon University University, Pittsburgh, PA 15213, USA. Correspondence to: Quanquan Gu <qgu@cs.ucla.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nas side information to adjust the estimation of precision matrix could lead to better accuracy.\nIn fact, this adjusted precision matrix estimation problem can be reformulated as a problem of jointly estimating multivariate regression matrix and the precision matrix. Cai et al. (2012a) proposed a two-stage covariate-adjusted precision matrix estimation method which first estimates the regression coefficient matrix and then estimates the precision matrix based on the estimated regression coefficient matrix. Some other work targets at simultaneously estimating both the regression coefficient matrix and the precision matrix (Yin & Li, 2011; Lee & Liu, 2012; Rothman et al., 2010). Following the literature of this line of research, we briefly introduce the model as follows: given data vectors {yi} n i=1 2 Rm and side information vectors {xi}ni=1 2 Rd, assume that\nyi = ⇤ xi + ✏i, (1.1)\nwhere ⇤ 2 Rd⇥m is the unknown regression coefficient matrix, and ✏i 2 Rm is the error vector. We assume {xi}ni=1 and {✏i}ni=1 are independent from each other and the error vector {✏i}ni=1 follows a multivariate normal distribution with zero mean and covariance ⌃⇤. Therefore, given xi, we have yi|xi ⇠ N( ⇤xi,⌃⇤). Let ⌦⇤ = ⌃⇤ 1 be the corresponding precision matrix that characterizes the conditional dependency structure among the data vectors {yi}ni=1. More specifically, ⌦⇤ij = 0 implies that i-th and j-th variables are conditionally independent given the covariates and other response variables.\nTo estimate ⌦⇤, we construct the following conditional likelihood function corresponding to our model (1.1): `( ,⌦) =\nnY\ni=1\n(2⇡) m 2 ⌦ 12 exp\n\n(yi xi)>⌦(yi xi)\n2\n,\nwhere |⌦| denotes the determinant of ⌦. The corresponding negative log-likelihood function can be written as (neglect the constants):\nfn( ,⌦) = log |⌦|+ 1 n tr ⇥ (Y X )⌦(Y X )> ⇤ ,\n(1.2)\nwhere X = [x1, . . . ,xn]> 2 Rn⇥d and Y = [y1, . . . ,yn]> 2 Rn⇥m. In many applications such as eco-\nnomics and genomics, the number of parameters dm+ m 2 is often much larger than the number of observations n, which imposes great challenges on the model estimation. Thus one common assumption is that both ⇤ and ⌦⇤ have certain structures. In this paper, without loss of generality, we assume both ⇤ and ⌦⇤ are row sparse. Specifically, we assume ⇤ and ⌦⇤ belong to the following classes respectively:\nV(s⇤1) :=\n⇢ 2 Rd⇥m : max\n1id\nmX\nj=1\n1{ ij 6= 0}  s ⇤ 1 ,\nU(s⇤2) := ⇢ ⌦ 2 Rm⇥m : k⌦k1  M,\nmax 1im\nmX\nj=1\n1{⌦ij 6= 0}  s ⇤ 2 .\nTherefore, k ⇤k0,0 = ds⇤1 and k⌦⇤k0,0 = ms⇤2, where k · k0,0 denotes the number of nonzero entries in a matrix. Note that the constraint that k⌦k1  M is a common condition in the literature on precision matrix estimation (Cai et al., 2012a;b). Under this assumption, we propose a cardinality constrained maximum likelihood estimator as follows:\nmin ,⌦\nlog |⌦|+ 1\nn tr (Y X )⌦(Y X )>\nsubject to k k0,0  s1, k⌦k0,0  s2, (1.3)\nwhere s1 and s2 are tuning parameters which control the sparsity of and ⌦ respectively. Note that even though our ultimate goal is to estimate ⌦⇤, since the likelihood function is also related to ⇤, it is thus also important to ensure that is close enough to ⇤, so that it would not downgrade the accuracy in estimating ⌦⇤.\nThe proposed estimator in (1.3) poses great challenges for both optimization and statistical analysis. The sample loss function in (1.2) is not jointly convex in and ⌦. This together with the nonconvex cardinality constraints make our estimator a highly non-convex optimization problem. Moreover, statistical analysis becomes quite challenging for such a non-convex estimator in such a finite sample scenario. Many previous studies along this line of research (Yin & Li, 2011; Rothman et al., 2010; Lee & Liu, 2012) are only able to characterize the asymptotic performance of their estimators. To the best of our knowledge, Cai et al. (2012a) is the only work with non-asymptotic performance guarantee for such a nonconvex model, yet it only analyzes the statistical error and does not include any optimization analysis or guarantees on the statistical estimators, which makes it less practical. To overcome these challenges, we propose an alternating gradient descent algorithm for solving the nonconvex optimization problem in (1.3). We summarize our contributions as follows:\n• We propose a practical algorithm which is easy to implement and fast to compute under strict run-time analysis. Therefore, our algorithm is much faster and closer to the real world situations than existing estimator based algorithms.\n• We show that the proposed algorithm is guaranteed to converge to the true precision matrix ⌦⇤ at a linear rate. In particular, the statistical rate of the estimator from our algorithm actually matches the minimax optimal up to a logarithmic factor.\n• To the best of our knowledge, this is the first work to analyze the non-asymptotic optimization performance guarantee of the covariate-adjusted precision matrix estimation model. This sheds some light on how this model works in real world scenarios.\nThe remainder of this paper is organized as follows: in Section 2, we briefly review existing work that is relevant to our study. We present the algorithm in Section 3, and the main theory in Section 4. In Section 5, we compare the proposed algorithm with existing algorithms on both synthetic data and real datasets. Finally, we conclude this paper in Section 6.\nNotation. Let [n] denote the set of {1, . . . , n}. For random variable X , we define the sub-Gaussian norm as kXk 2 = supp 1 p 1/2(E|X|p)1/p. For a vector x 2 Rd, define kxk2 = qPd i=1 x 2 i . For a matrix A 2 Rm1⇥m2 , we denote by max(A) and min(A) the largest and smallest eigenvalue of A respectively. We define supp(A) as the index set of nonzero entries of A, and supp(A, s) as the index set of the top s entries of A in terms of magnitude. We use A⇤k to denote the k-th column of matrix A and Ajk the (j, k)-th element of A. For a pair of matrices A,B with commensurate dimensions, hA,Bi denotes the trace inner product on matrix space that hA,Bi := trace(A>B). We also use various norms for matrices, including spectral norm kAk2 = maxkuk2=1 kAuk2,\nFrobenius norm kAkF = qPm1\nj=1 Pm2 k=1 A 2 jk, infinity\nnorm kAk1,1 = max1jm1,1km2 |Ajk|, kAk1,1 =Pm1 j=1 Pm2 k=1 |Ajk|, kAk1 = max1jm1 Pm2 k=1 |Ajk| and |Ak1 = max1km2 Pm1\nj=1 |Ajk|. kAk0,0 =Pm1 j=1 Pm2 k=1 1{Ajk 6= 0} denotes the number of nonzero entries in A. For S 2 [m1] and T 2 [m2], we define AST to be the submatrix of A, which is obtained by extracting the appropriate rows and columns in S and T respectively. Also C,C 0, . . . represent absolute constants which could be of different values in different places."
  }, {
    "heading": "2 Related Work",
    "text": "A large body of literature has been devoted to the precision matrix estimation in Gaussian graphical models (GGM)\nsuch as `1,1 norm constrained sparse GGM (GLasso) (Friedman et al., 2008; Ravikumar et al., 2011; Rothman et al., 2008), distributed GGM (Xu et al., 2016), robust GGM (Wang & Gu, 2017), sparse tensor-variate GGM (Xu et al., 2017b) and latent variable GGM (Chandrasekaran et al., 2010; Ma et al., 2013; Xu et al., 2017a). In addition, Wang et al. (2016a); Xu & Gu (2016) studied the faster rate of GGM and its variants with noncovex penalties.\nThe approach of utilizing additional side information for GGM is first proposed by Yin & Li (2011), in which the model is called conditional Gaussian graphical model (CGGM). Along this line of research, there exist two families of methods in general. The first family of methods (Rothman et al., 2010; Lee & Liu, 2012; Yin & Li, 2011) simultaneously estimates the regression coefficient matrix and precision matrix using alternating optimization algorithms. However, all the theoretical results (Rothman et al., 2010; Lee & Liu, 2012) for alternating minimization algorithms are based on the assumption that there exists a local minimizer that possesses certain good properties, while neither of them is guaranteed to find such a satisfying local minimizer. The second family of methods is identified by their two-step optimization procedure (Cai et al., 2012a), in which the regression coefficient matrix is estimated first and the precision matrix estimation is built based on the estimated regression coefficient matrix. However, the two-step approaches cannot fully utilize the interdependency between the regression coefficient matrix and the precision matrix in the estimation process, which often leads to a sub-optimal solution.\nAnother line of research which is closely related to ours is call conditional Gaussian random fields (Wytock & Kolter, 2013), sometimes also been called conditional Gaussian graphical model (CGGM) (Sohn & Kim, 2012; McCarter & Kim, 2016; Zhang & Kim, 2014) or partial Gaussian graphical models (pGGM) (Yuan & Zhang, 2014). This model may seem similar to ours, yet we want to emphasize that it is fundamentally different from ours. The key difference is that their model does not make the sparsity assumption on\n1. Due to this different sparsity assumption, their model is convex while ours is not. Yet the sparsity assumption on is beneficial in modeling real world data.\nIn terms of nonconvex optimization technique, Yuan et al. (2013); Jain et al. (2014); Chen & Gu (2016) proposed and analyzed the gradient descent algorithm with hard thresholding for cardinality constrained optimization problems. However, these algorithms are limited to single optimization variable case. Jain & Tewari (2015) proposed an alternating minimization algorithm for two regression models (i.e., pooled model and seemingly unrelated regression model)\n1In their model, the sparsity assumptions are made on some other related matrices, please refer to their papers for more details\nin classical regime. Chen & Banerjee (2017) follows the same model and analyze the statistical guarantee in terms of Gaussian width. Yet their proof technique is specific to the particular regression models they studied, and cannot be extended to our model. In addition, alternating minimization has also been analyzed for other models such as matrix factorization (Jain et al., 2013; Arora et al., 2015; Zhao et al., 2015; Zheng & Lafferty, 2015; Chen & Wainwright, 2015; Tu et al., 2015; Wang et al., 2016b; 2017), robust PCA (Gu et al., 2016; Zhang et al., 2018), phase retrieval (Candès et al., 2015; Chen et al., 2017) and latent variable models (Balakrishnan et al., 2014; Wang et al., 2015; Zhu et al., 2017). Yet none of these algorithms and theories can be directly extended to our problem."
  }, {
    "heading": "3 The Proposed Algorithm",
    "text": "In this section, we present a gradient descent based optimization algorithm for solving the proposed estimator in (1.3). The key motivation of the algorithm is that the objective function in (1.3) is bi-convex, i.e., it is convex with respect to (resp. ⌦) when the other variable is fixed. Therefore, we propose to optimize the target objective function by performing gradient descent with respect to and ⌦ alternatingly. The details about the proposed algorithm is shown in Algorithm 1.\nAlgorithm 1 Alternating Gradient Descent with Hard Thresholding\n1: Input: Number of iterations T , sparsity s1, s2, step size ⌘1, ⌘2. 2: for t = 0 to T 1 do 3: Update :\n(t+0.5) = (t) ⌘1r1fn (t),⌦(t) , (t+1) = HT ( (t+0.5), s1)\n4: Update ⌦: ⌦ (t+0.5) = ⌦(t) ⌘2r2fn (t),⌦(t) ,\n⌦ (t+1) = HT (⌦(t+0.5), s2)\n5: end for 6: Output: b = (T ), b⌦ = ⌦(T )\nIn Algorithm 1, (t+0.5) and ⌦(t+0.5) are the outputs of gradient descent update. Note that in Algorithm 1, r1fn denotes the gradient of fn with respect to , and r2fn denotes its gradient with respect to ⌦. The hard thresholding procedure (Yuan et al., 2013; Jain et al., 2014) right after gradient descent update is for ensuring the sparsity of the parameters after the gradient descent update. Specifically,\n[HT (A, s)]ij = ⇢ Aij , if (i, j) 2 supp(A, s) 0, otherwise .\n(3.1)\nIn other words, the hard thresholding step preserves the largest s1 and s2 entries in (t+0.5) and ⌦(t+0.5) respectively in terms of magnitudes and sets the rest to zero. This\ngives rise to (t+1) and ⌦(t+1). Recall that s1 and s2 are tuning parameters that control the sparsity level.\nAlgorithm 1 provides an efficient way to solve the nonconvex problem using gradient descent with hard thresholding. Yet it requires that the initial estimators (0) and ⌦(0) to fall into the contraction region in order to work. In order to obtain a good pair of initial estimators (0) and ⌦(0), we introduce the initialization algorithm in Algorithm 2.\nAlgorithm 2 Initialization 1: Input: Regularization parameters , ⌦ and u 2: init = ST (X>X+ ✏ I) 1X>Y,\n3: S = (Y X )>(Y X )/n 4: ⌦init = ST S+ ✏⌦I) 1, ⌦ 5: (0) = HT ( init, s1), ⌦(0) = HT (⌦init, s2)\nIn Algorithm 2, we apply the closed form solution for the elementary estimator for linear regression (Yang et al., 2014a) and a revised version of elementary estimator for graphical models to obtain initial estimators init and ⌦init. Here is ST stands for the soft thresholding operator which is defined as follows:\n[ST (A, )]ij = sign(Aij) ·max(|Aij | , 0). (3.2)\nAlgorithm 2 ensures that the initial estimators (0) and ⌦(0) are sufficiently close to ⇤ and ⌦⇤ respectively and thus falls into the contraction region. While Algorithm 1 guarantees the model parameters’ convergence inside the contraction region. By combining Algorithm 1 and Algorithm 2, we ensure that our nonconvex optimization algorithm will converge to the true parameters."
  }, {
    "heading": "4 Main Theory",
    "text": "Before we present the main results, we first lay out a series of assumptions, which are essential for establishing our theory. Assumption 4.1. There exist some ⌫ 1 such that the maximum and minimum eigenvalues of ⌦⇤ satisfy\n1/⌫  min(⌦ ⇤)  max(⌦ ⇤)  ⌫,\nwhere ⌫ is an absolute constants which do not depend on m.\nNote that the same assumption has also been made in (Lee & Liu, 2012; Wang, 2013; Cai et al., 2012b). Assumption 4.2. Let {xi}ni=1 be the rows in X. Each xi is a sub-Gaussian random vector. In addition, let ⌃⇤X = n 1E[X>X]. There exists ⌧ 1 such that\n1/⌧  min(⌃ ⇤ X)  max(⌃ ⇤ X)  ⌧,\nand\nk⌃ ⇤ 12 X xik 2  K, for all i = 1, . . . , n\nwhere ⌧,K are absolute constants independent of n, d.\nAssumption 4.2 states that the minimum eigenvalue of the population covariance matrix of the predictors is bounded away from zero. This assumption is mild and has been widely made in the literature of multivariate regression (Obozinski et al., 2011; Lounici et al., 2009; Negahban & Wainwright, 2011). Also since k⌃⇤ 1 2\nX xik 2 is bounded for all i = 1, . . . , n, it immediately implies that kxik 2  p ⌧k⌃\n⇤ 12 X xik 2 is also bounded.\nNow we are going to present our main theorem. To simplify the technical analysis, we focus on the resampling version of Algorithm 1, which is illustrated in Algorithm 3 in the supplementary material. The key idea of resampling (or sample splitting) (Hansen, 2000; Balakrishnan et al., 2017) is to split the whole dataset into T pieces and use a fresh piece of data in each iteration. The main propose for resampling is to remove the statistical dependencies between iterates. Theorem 4.3. Under Assumptions 4.1 and 4.2, let R := min 1/(⌫⌧2), 1/(4⌫2⌧), p ⌫/⌧ ,M . Suppose the initial value (0) and ⌦(0) satisfy max{k (0) ⇤kF , k⌦(0) ⌦ ⇤ kF }  R. Let the sparsity parameters satisfy s1\n1 + 4/(1/⇢ 1)2 ds⇤1, s2\n1 + 4/(1/⇢ 1)2 ms⇤2,\nwhere\n⇢ = max ⇢ 1 2 2R⌫⌧2\n⌫2⌧2 + 1 , 1\n2 8⌧⌫2R\n16⌫4 + 1\n.\nAnd suppose the sample size n satisfies that\nn CM2T max{⌫⌧ds⇤1,ms ⇤ 2} log(dmT )\n(1 p ⇢)2R2\n. (4.1)\nLet ⌘1 = ⌫⌧/(⌫2⌧2 + 1), ⌘2 = 8⌫2/(16⌫4 + 1), for all t 2 [T ], we have with probability at least 1 2/d C 0/m that\nmax k\n(t)\n⇤ kF , k⌦ (t) ⌦ ⇤ kF\n C 00M max\np ⌫⌧ds⇤1, p ms⇤2\n1 p ⇢\ns log(dmT )\nn/T | {z }\nStatistical Error + R · ⇢t/2| {z }\nOptimization Error\n, (4.2)\nwhere C,C 0, C 00 are absolute constants. Remark 4.4. Conditions in Theorem 4.3 imply that the sparsity parameters s1 and s2 should be chosen to be sufficiently large but meanwhile in the same order as the true sparsity level ds⇤1 and ms⇤2 respectively. This ensures that the extra error caused by hard thresholding step can be upper bounded. Moreover, we can observe that the definition of R indeed guarantees that ⇢ < 1. Therefore our proposed algorithm indeed converges.\nRemark 4.5. In Theorem 4.3, the result suggests that the estimation error is bounded by two terms: the optimization error term (i.e., the second term on the right hand side of (4.2)), which decays to zero at a linear rate, and the statistical error term (i.e., the first term on the right hand side of (4.2)), which characterizes the the unavoidable estimation error in Algorithm 3 when the optimization error term goes to zero as T goes to infinity.\nIn the next corollary, we show the statistical error achieved by our proposed method matches the minimax lower bound. Corollary 4.6. Under the same assumptions and conditions as in Theorem 4.3, suppose s⇤1 satisfies s⇤1  ms⇤2/(d⌫⌧), and if we choose the number of iterations T = C log n for sufficiently large C such that the optimization error term is dominated by the statistical error term, then we have\nkb⌦ ⌦⇤kF  C 0M\np log n\n1 p ⇢\nr ms⇤2 logm\nn ,\nwhere C,C 0 are some absolute constants.\nComparing with the minimax lower bound, which is in the order of O M p\nms⇤2 logm/n (Cai et al., 2012b), our bounds on ⌦⇤ matches the minimax lower bounds aside from an additional logarithmic term p log n. Such a logarithmic factor is introduced by the resampling step in Algorithm 3, since we only utilize n/T samples within each iteration. We expect that it is an artifact of our proof technique, and such a logarithmic factor can be eliminated by directly analyzing Algorithm 1, which however requires extra technical effort for the analysis. Theorem 4.7. Under Assumptions 4.1 and 4.2, let R := min 1/(⌫⌧2), 1/(4⌫2⌧), p ⌫/⌧ ,M . Suppose the sample size n satisfies\nn Cmax\n⇢ ⌧(ds⇤1) 2\nR3 , M2⌫2ms⇤2 R2 ,\n5 r M6⌫3⌧5(ds⇤1) 3(ms⇤2) 4\nR6\nlog(dm),\nthen the output from Algorithm 2 satisfies\nmax{k (0) ⇤kF , k⌦ (0) ⌦ ⇤ kF }  R.\nRemark 4.8. Notice that for initialization we can also choose to adopt multivariate Lasso for estimating init and graphical Lasso for initializing ⌦init, which is also provable and could lead to a bit better sample complexity. The price to pay is higher computational complexity, since elementary estimators have closed-form solutions. Yet in experiments part, we observe that the current initialization mechanism can also provide accurate enough initial estimators which satisfy the requirements from Algorithm 1 under the same sample complexity constraint."
  }, {
    "heading": "4.1 Runtime Complexity Analysis",
    "text": "We compare the run-time complexity in terms of the optimization error for all baseline algorithms in Table 1. Note that none of baseline algorithms actually proved the linear rate of convergence. As a consequence, their run-time complexity can only be written as per-iteration complexity times the outer iteration To. For MRCE, in each iteration, it requires to solve a coordinate descent sub-problem which will take at least 1/ p ✏ inner loops to converge. For Capme, it only performs one outer iteration, yet since it adopts Dantzig selector and CLIME estimator in their framework, it will take at least 1/✏ inner loops to converge, which is quite slow. Alt-NCD adopts a convex problem formulation and is claimed to be much scalable especially when their memory is limited. Yet in terms of run-time complexity, in each iteration it also requires to alternatingly solve a coordinate descent problem which takes at least 1/ p ✏ inner loops to converge. By securing a linear rate of convergence and only required to solve one step gradient update inside each iteration, our proposed algorithm clearly enjoys better run-time complexity comparing with all these baselines."
  }, {
    "heading": "5 Experiments",
    "text": "In this section, we will present numerical results on both synthetic and real datasets to verify the performance of the proposed algorithm in Algorithm 1. We compare our algorithm with several state-of-the-art baseline algorithms:\n• Multivariate regression with covariance estimation (MRCE) by Rothman et al. (2010);\n• Covariate-adjusted precision matrix estimation (Capme) by Cai et al. (2012a)\n• Alternating Newton coordinate descent algorithm (AltNCD) by McCarter & Kim (2016).\nNote that McCarter & Kim (2016) adopted different problem formulations (convex rather than bi-convex) with the rest of the algorithms including ours. Also, in McCarter & Kim (2016) it presented a block coordinate descent version for solving limited memory case, which we do not compare with."
  }, {
    "heading": "5.1 Synthetic Data",
    "text": "In each replication of each model, we generate an n ⇥ d predictor matrix X with rows drawn independently from a multivariate normal distribution N(0, I) as in Cai et al. (2012a). On the other hand, the data matrix Y is generated from model (1.1), i.e., yi|xi ⇠ N( ⇤xi,⌦⇤ 1) where the precision matrix ⌦⇤ is generated using the following sparse models: (1) Hub graph; (2) Band graph; (3) Cluster graph.\nWe compare the performance on synthetic data with three different settings: (1) n = 500,m = 500, d = 500 (2) n = 1000,m = 1000, d = 1000 (3) n = 1000,m = 1500, d = 1500. Each setting is repeated for 10 times. The averaged estimation error for both and ⌦, and also the running time for all settings precision matrix types are reported in Tables 2, 3, 4. We can observe that in terms of the estimation error of both and ⌦, our proposed algorithm achieves the best accuracy and also the fastest running time comparing with the state-of-the-art algorithms. To be more specific, MRCE achieves the least accurate ⌦ estimation with mediocre running time. Capme achieves relatively high accuracy on ⌦ estimation, yet it is not quite scalable\nwith a large amount of time needed. Even comparing with Alt-NCD, which adopts a much easier (convex) problem formulation, our algorithm still outperforms it with clear advantage. Also due to different assumptions on ⇤, the estimation error for Alt-NCD is the least accurate.\nFigure 1 illustrates the scaling of the estimation error for ⇤ and ⌦⇤ respectively. The x axis of these graphs is the rescaled sample size. This result support our conclusion that our estimator by Algorithm 1 achieves O p\nds⇤1 log(dm)/n statistical estimation error for ⇤, and O p ms⇤2 logm/n statistical estimation error for ⌦⇤.\nFigure 2 demonstrates the support recovery results of ⌦⇤ under three different graph structure of the precision matrix. We use receiver operating characteristic (ROC) curves to compare the support recovery performance of our proposed algorithm with other baselines algorithm. From figure 2 we can see that our proposed algorithm outperforms all other baselines."
  }, {
    "heading": "5.2 eQTL analysis on Yeast Data",
    "text": "We demonstrate the effectiveness of our proposed method by applying it on an eQTL dataset (yeast) from Brem & Kruglyak (2005), which contains the expression measurements of 5,740 transcripts measured on 112 yeast segregants grown from two yeast parent strains: BY4716 (BY) and RM11-1a (RM), with dense genotype data on 2,956 markers. Our goal is to decode the gene regulatory relationships. Here we choose to analyze the gene set selected from yeast cell cycle Saccharomyces Cerevisiae pathway provided by the KEGG database (Ogata et al., 1999). We implemented our method to analyze the 92 genes involved in the cell cycle pathway, along with 787 markers. We utilize five-fold cross validation to select the optimal parameters. As a result, our\nproposed covariate adjusted precision estimation method identifies 102 links among 92 genes, and find around 1000 nonzero entries for coefficient matrix, suggesting that a lot of gene expression levels are affected by genetic variants.\nFigure 3 displays the gene network recovered by our proposed method. Although the estimated gene network may not fully recover the cell-cycle pathway due to some inherent limitations, such as the lack of observations and the missing data for gene expression in our datasets, we find that our method reveals meaningful observations. For instance, gene\nCDC28 (the catalytic subunit of the cyclin-dependent kinase) is connected with genes MIH1, MCM1, ORC2, TPD3, CDC5, BUB2, implying a strong interaction mechanism between CDC28 and those genes.\nWe also implement other baseline methods: MRCE, AltNCD, Campe on the same dataset for direct comparisons. In detail, we choose the optimal parameters for these methods by five-fold cross validation, and the gene networks obtained by the baseline methods can be found in Supplemental Materials. We found that other methods cannot recover meaningful results as compared to the known KEGG pathway."
  }, {
    "heading": "5.3 eQTL analysis on GTEx Data",
    "text": "The Genotype-Tissue Expression (GTEx) project generated RNA-seq expression data for a large number of human tissues (as of February 2018, there are 11688 samples in more than 53 tissues) (Lonsdale et al., 2013). By analyzing global RNA expression within individual tissues and treating the\nexpression levels of genes as quantitative traits, variations in gene expression that are highly correlated with genetic variation can be identified as expression quantitative trait loci (eQTLs). Here we choose to focus on the whole blood dataset due to its relatively large number of samples. We specifically look at 19 genes that are related to GATA1, which is a key TF regulator in blood cells, from the Erythroid (K562) network (Neph et al., 2012). In the dataset, we also randomly add 33 other genes from the Erythroid (K562) network. The total number of genes selected is 53. We also select 333 SNPs from the dataset, 15 of which are known to be significantly related to one of genes selected above.\nFigure 4 describes the gene regulation network (GRN) that supports the results for different algorithms including ours, on GTEx dataset. Comparing with the ground truth, we can see that our proposed algorithm is the only one that nearly recovers the upper block structure. Note that GATA1 is numbered 20 in the plot, it is clear that our proposed algorithm achieves better performance in identifying the gene regulation relationship for the GATA1 regulatory network. In Table 5 we report the F1 score for recovering the GRN, and also the number of non-zero elements for a fair com-\nparison. It also shows the clear advantage of our proposed algorithm over other baselines.\nFigure 5 demonstrates the gene-SNP regulation relationship recovery results. We found that even though all the algorithms cannot fully recover all the significant gene-SNP pairs, our proposed algorithm still outperforms all other baselines. Note that Alt-NCD method does not successfully recover any significant gene-SNP pairs, possibly due to the fact that it adopts a different problem formulation (did not assume sparsity on ), which is less intuitive towards this regression task. Taken together, these results demonstrate the potential of our proposed algorithm in identifying important gene regulatory relationships by jointly considering gene-gene interaction and variant-gene relationships."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper, we presented a gradient descent algorithm with hard thresholding for joint multivariate regression and precision matrix estimation in the high dimensional regime, under cardinality constraints. It attains a linear convergence to the true regression coefficients and precision matrix simultaneously, up to a near optimal statistical error. Compared with existing methods along this line of research, the proposed algorithm out-performs the baseline algorithm in both accuracy and running time. Thorough experiments on synthetic datasets support our theory and the real world eQTL experiments on yeast and GTEx dataset shows the promising potential of applying our proposed algorithm in biological studies."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1618948, IIS-1652539 and IIS-1717206. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."
  }],
  "year": 2018,
  "references": [{
    "title": "Simple, efficient, and neural algorithms for sparse coding",
    "authors": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"],
    "venue": "arXiv preprint arXiv:1503.00778,",
    "year": 2015
  }, {
    "title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
    "authors": ["S. Balakrishnan", "M.J. Wainwright", "B. Yu"],
    "venue": "arXiv preprint arXiv:1408.2156,",
    "year": 2014
  }, {
    "title": "Statistical guarantees for the em algorithm: From population to samplebased analysis",
    "authors": ["S. Balakrishnan", "M.J. Wainwright", "B Yu"],
    "venue": "The Annals of Statistics,",
    "year": 2017
  }, {
    "title": "The landscape of genetic complexity across 5,700 gene expression traits in yeast",
    "authors": ["R.B. Brem", "L. Kruglyak"],
    "venue": "Proceedings of the National Academy of Sciences of the United States of America,",
    "year": 2005
  }, {
    "title": "Covariate-adjusted precision matrix estimation with an application in genetical genomics",
    "authors": ["T.T. Cai", "H. Li", "W. Liu", "J. Xie"],
    "year": 2012
  }, {
    "title": "Estimating sparse precision matrix: Optimal rates of convergence and adaptive estimation",
    "authors": ["T.T. Cai", "W. Liu", "H.H. Zhou"],
    "venue": "arXiv preprint arXiv:1212.2882,",
    "year": 2012
  }, {
    "title": "Phase retrieval via wirtinger flow: Theory and algorithms",
    "authors": ["E.J. Candès", "X. Li", "M. Soltanolkotabi"],
    "venue": "Information Theory, IEEE Transactions on,",
    "year": 1985
  }, {
    "title": "Robust wirtinger flow for phase retrieval with arbitrary corruption",
    "authors": ["J. Chen", "L. Wang", "X. Zhang", "Q. Gu"],
    "venue": "arXiv preprint arXiv:1704.06256,",
    "year": 2017
  }, {
    "title": "Alternating estimation for structured high-dimensional multi-response models",
    "authors": ["S. Chen", "A. Banerjee"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
    "authors": ["Y. Chen", "M.J. Wainwright"],
    "venue": "arXiv preprint arXiv:1509.03025,",
    "year": 2015
  }, {
    "title": "The genetics of variation in gene expression",
    "authors": ["V.G. Cheung", "R.S. Spielman"],
    "venue": "Nature genetics,",
    "year": 2002
  }, {
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "year": 2008
  }, {
    "title": "Low-rank and sparse structure pursuit via alternating minimization",
    "authors": ["Q. Gu", "Z.W. Wang", "H. Liu"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Sample splitting and threshold estimation",
    "authors": ["B.E. Hansen"],
    "year": 2000
  }, {
    "title": "Alternating minimization for regression problems with vector-valued outputs",
    "authors": ["P. Jain", "A. Tewari"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Low-rank matrix completion using alternating minimization",
    "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"],
    "venue": "In STOC, pp",
    "year": 2013
  }, {
    "title": "On iterative hard thresholding methods for high-dimensional m-estimation",
    "authors": ["P. Jain", "A. Tewari", "P. Kar"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Graphical Models",
    "authors": ["S.L. Lauritzen"],
    "year": 1996
  }, {
    "title": "Simultaneous multiple response regression and inverse covariance matrix estimation via penalized gaussian maximum likelihood",
    "authors": ["W. Lee", "Y. Liu"],
    "venue": "Journal of multivariate analysis,",
    "year": 2012
  }, {
    "title": "Stochastic variance reduced optimization for nonconvex sparse learning",
    "authors": ["X. Li", "T. Zhao", "R. Arora", "H. Liu", "J. Haupt"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
    "authors": ["Loh", "P.-L", "M.J. Wainwright"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "The genotype-tissue expression (gtex) project",
    "authors": ["J. Lonsdale", "J. Thomas", "M. Salvatore", "R. Phillips", "E. Lo", "S. Shad", "R. Hasz", "G. Walters", "F. Garcia", "N Young"],
    "venue": "Nature genetics,",
    "year": 2013
  }, {
    "title": "Taking advantage of sparsity in multi-task learning",
    "authors": ["K. Lounici", "M. Pontil", "A.B. Tsybakov", "S. Van De Geer"],
    "venue": "Proc. Computational Learning Theory Conference,",
    "year": 2009
  }, {
    "title": "Alternating direction methods for latent variable gaussian graphical model selection",
    "authors": ["S. Ma", "L. Xue", "H. Zou"],
    "venue": "Neural computation,",
    "year": 2013
  }, {
    "title": "Large-scale optimization algorithms for sparse conditional gaussian graphical models",
    "authors": ["C. McCarter", "S. Kim"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Simultaneous support recovery in high dimensions: Benefits and perils of blockregularization",
    "authors": ["S.N. Negahban", "M.J. Wainwright"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2011
  }, {
    "title": "Circuitry and dynamics of human transcription factor regulatory",
    "authors": ["S. Neph", "A.B. Stergachis", "A. Reynolds", "R. Sandstrom", "E. Borenstein", "J.A. Stamatoyannopoulos"],
    "year": 2012
  }, {
    "title": "Introductory lectures on convex optimization, volume 87",
    "authors": ["Y. Nesterov"],
    "venue": "Springer Science & Business Media,",
    "year": 2004
  }, {
    "title": "Highdimensional union support recovery in multivariate",
    "authors": ["G. Obozinski", "M.J. Wainwright", "M.I. Jordan"],
    "year": 2011
  }, {
    "title": "Kegg: Kyoto encyclopedia of genes and genomes",
    "authors": ["H. Ogata", "S. Goto", "K. Sato", "W. Fujibuchi", "H. Bono", "M. Kanehisa"],
    "venue": "Nucleic acids research,",
    "year": 1999
  }, {
    "title": "Highdimensional covariance estimation by minimizing 1-penalized log-determinant divergence",
    "authors": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B Yu"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2011
  }, {
    "title": "Sparse permutation invariant covariance estimation",
    "authors": ["A.J. Rothman", "P.J. Bickel", "E. Levina", "J Zhu"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2008
  }, {
    "title": "Sparse multivariate regression with covariance estimation",
    "authors": ["A.J. Rothman", "E. Levina", "J. Zhu"],
    "venue": "Journal of Computational and Graphical Statistics,",
    "year": 2010
  }, {
    "title": "Joint estimation of structured sparsity and output structure in multiple-output regression via inversecovariance regularization",
    "authors": ["Sohn", "K.-A", "S. Kim"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2012
  }, {
    "title": "Low-rank solutions of linear matrix equations via procrustes flow",
    "authors": ["S. Tu", "R. Boczar", "M. Soltanolkotabi", "B. Recht"],
    "venue": "arXiv preprint arXiv:1507.03566,",
    "year": 2015
  }, {
    "title": "Introduction to the non-asymptotic analysis of random matrices",
    "authors": ["R. Vershynin"],
    "venue": "arXiv preprint arXiv:1011.3027,",
    "year": 2010
  }, {
    "title": "Joint estimation of sparse multivariate regression and conditional graphical models",
    "authors": ["J. Wang"],
    "venue": "arXiv preprint arXiv:1306.4410,",
    "year": 2013
  }, {
    "title": "Robust gaussian graphical model estimation with arbitrary corruption",
    "authors": ["L. Wang", "Q. Gu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Precision matrix estimation in high dimensional gaussian graphical models with faster rates",
    "authors": ["L. Wang", "X. Ren", "Q. Gu"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "A unified computational and statistical framework for nonconvex low-rank matrix estimation",
    "authors": ["L. Wang", "X. Zhang", "Q. Gu"],
    "venue": "arXiv preprint arXiv:1610.05275,",
    "year": 2016
  }, {
    "title": "A unified variance reductionbased framework for nonconvex low-rank matrix recovery",
    "authors": ["L. Wang", "X. Zhang", "Q. Gu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "High dimensional em algorithm: Statistical optimization and asymptotic normality",
    "authors": ["Z. Wang", "Q. Gu", "Y. Ning", "H. Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Sparse gaussian conditional random fields: Algorithms, theory, and application to energy forecasting",
    "authors": ["M. Wytock", "Z. Kolter"],
    "venue": "In International conference on machine learning,",
    "year": 2013
  }, {
    "title": "Semiparametric differential graph models",
    "authors": ["P. Xu", "Q. Gu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Communication-efficient distributed estimation and inference for transelliptical graphical models",
    "authors": ["P. Xu", "L. Tian", "Q. Gu"],
    "venue": "arXiv preprint arXiv:1612.09297,",
    "year": 2016
  }, {
    "title": "Speeding up latent variable gaussian graphical model estimation via nonconvex optimizations",
    "authors": ["P. Xu", "J. Ma", "Q. Gu"],
    "venue": "Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Efficient algorithm for sparse tensor-variate gaussian graphical models via gradient descent",
    "authors": ["P. Xu", "T. Zhang", "Q. Gu"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Elementary estimators for high-dimensional linear regression",
    "authors": ["E. Yang", "A. Lozano", "P. Ravikumar"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Elementary estimators for graphical models",
    "authors": ["E. Yang", "A.C. Lozano", "P.K. Ravikumar"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "A sparse conditional gaussian graphical model for analysis of genetical genomics data",
    "authors": ["J. Yin", "H. Li"],
    "venue": "The annals of applied statistics,",
    "year": 2011
  }, {
    "title": "Partial gaussian graphical model estimation",
    "authors": ["Yuan", "X.-T", "T. Zhang"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2014
  }, {
    "title": "Gradient hard thresholding pursuit for sparsity-constrained optimization",
    "authors": ["Yuan", "X.-T", "P. Li", "T. Zhang"],
    "venue": "arXiv preprint arXiv:1311.5750,",
    "year": 2013
  }, {
    "title": "Learning gene networks under snp perturbations using eqtl datasets",
    "authors": ["L. Zhang", "S. Kim"],
    "venue": "PLoS computational biology,",
    "year": 2014
  }, {
    "title": "A unified framework for nonconvex low-rank plus sparse matrix recovery",
    "authors": ["X. Zhang", "L. Wang", "Q. Gu"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2018
  }, {
    "title": "A nonconvex optimization framework for low rank matrix estimation",
    "authors": ["T. Zhao", "Z. Wang", "H. Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements",
    "authors": ["Q. Zheng", "J. Lafferty"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Restricted eigenvalue conditions on subgaussian random matrices",
    "authors": ["S. Zhou"],
    "venue": "arXiv preprint arXiv:0912.4045,",
    "year": 2009
  }, {
    "title": "High-dimensional variance-reduced stochastic gradient expectation-maximization algorithm",
    "authors": ["R. Zhu", "L. Wang", "C. Zhai", "Q. Gu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }],
  "id": "SP:20cc8b64867643ac9311028ec3bb7c4074c5eb28",
  "authors": [{
    "name": "Jinghui Chen",
    "affiliations": []
  }, {
    "name": "Pan Xu",
    "affiliations": []
  }, {
    "name": "Lingxiao Wang",
    "affiliations": []
  }, {
    "name": "Jian Ma",
    "affiliations": []
  }, {
    "name": "Quanquan Gu",
    "affiliations": []
  }],
  "abstractText": "We propose a nonconvex estimator for the covariate adjusted precision matrix estimation problem in the high dimensional regime, under sparsity constraints. To solve this estimator, we propose an alternating gradient descent algorithm with hard thresholding. Compared with existing methods along this line of research, which lack theoretical guarantees in optimization error and/or statistical error, the proposed algorithm not only is computationally much more efficient with a linear rate of convergence, but also attains the optimal statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real data support our theory.",
  "title": "Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization"
}