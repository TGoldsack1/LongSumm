{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2268–2277 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2268"
  }, {
    "heading": "1 Introduction",
    "text": "The ability to identify discourse patterns and narrative themes from language is useful in a wide range of applications and data analysis. From a perspective of language understanding, learning such latent structure from large corpora can provide background information that can aid machine reading. For example, computers can use such knowledge to predict what is likely to happen next\n∗*Work done while first author was an intern at Microsoft Research\nin a narrative (Mostafazadeh et al., 2016), or reason about which narratives are coherent and which do not make sense (Barzilay and Lapata, 2008). Similarly, knowledge of discourse is increasingly important for language generation models. Modern neural generation models, while good at capturing surface properties of text – by fusing elements of syntax and style – are still poor at modeling long range dependencies that go across sentences (Li and Jurafsky, 2017; Wang et al., 2017). Models of long range flow in the text can thus be useful as additional input to such methods.\nPreviously, the question of modeling discourse structure in language has been explored through several lenses, including from perspectives of linguistics, cognitive science and information retrieval. Prominent among linguistic approaches are Discourse Representation Theory (Asher, 1986) and Rhetorical Structure Theory (Mann and Thompson, 1988); which formalize how discourse context can constrain the semantics of a sentence, and lay out ontologies of discourse relation types between parts of a document. This line of research has been largely constrained by the unavailability of corpora of discourse relations, which are expensive to annotate. Another line of research has focused on the task of automatic script induction, building on earlier work in the 1970’s (Schank and Abelson, 1977). More recently, methods based on neural distributed representations have been explored (Li and Hovy, 2014; Kalchbrenner and Blunsom, 2013; Le and Mikolov, 2014) to model the flow of discourse. While these methods have had varying degrees of success, they are largely opaque and hard to interpret. In this work, we seek to provide a scalable model that can extract latent sequential structures from a collection of documents, and can be naturally visualized to provide a summary of the learned semantics and discourse trajectories.\nIn this work, we present an approach for extract-\ning and visualizing sequential structure from a collection of text documents. Our method is based on embedding sentences in a document in a 3- dimensional grid, such that contiguous sentences in the document are likely to be embedded in the same order in the grid. Further, sentences across documents that are semantically similar are also likely to be embedded in the same neighborhood in the grid. By leveraging the sequential order of sentences in a large document collection, the method can induce lexical semantics, as well as extract latent discourse trajectories in the documents. Figure 1 shows a conceptual schematic of our approach. The method can learn semantic similarity (across XY planes), as well as sequential discourse chains (along the Z-axis). The parameters and latent structure of the grid are learned by optimizing the likelihood of a collection of documents under a generative model. Our method outperforms state-of-the-art generative methods on two tasks: predicting the outcome of a story and coherence prediction; and is seen to yield a flexible range of interpretable visualizations in different domains of text. Our method is scalable, and can incorporate a broad range of features. In particular, the approach can work on simple tokenized text.\nThe remainder of this paper is organized as follows. In Section 2, we briefly summarize other related work. In Section 3, we describe our method\nin detail. We present experimental results in Section 4, and conclude with a brief discussion."
  }, {
    "heading": "2 Related work",
    "text": "Building on linguistic theories of discourse and text coherence, several computational approaches have attempted to model discourse structure from multiple perspectives. Prominent among these are Narrative Event Chains (Chambers and Jurafsky, 2008) which learn chains of events that follow a pattern in a unsupervised framework, and the Entity grid model (Barzilay and Lapata, 2008), which represents sentences in a context in terms of discourse entities occurring in them and trains coherence classifiers over this representation. Other work extends these using better models of events and discourse entities (Lin et al., 2011; Pichotta and Mooney, 2015). Louis and Nenkova (2012) use manually provided syntactic patterns for sentence representation, and model transitions in text as Markov probabilities, which is related to our work. However, while they use simple HMMs over discrete topics, our method allows for a richer model that also captures smooth transition across them. Approaches such as Kalchbrenner and Blunsom (2013); Li et al. (2014); Li and Jurafsky (2017) model text through recurrent neural architectures, but are hard to interpret and visualize. Other approaches have explored applications related to modeling narrative discourse in context of limited tasks such as story cloze (Mostafazadeh et al., 2016) and identifying similar narratives (Chaturvedi et al., 2018).\nFrom a large scale document-mining perspective, the question of extracting intra-document structure remains largely underexplored. While early models such as LDA completely ignore ordering and discourse elements of a documents, other methods that use distributed embeddings of documents are opaque (Le and Mikolov, 2014), even while they can in principle model sequential structure within a document. Methods such as HMM-LDA (Griffiths et al., 2005) and Topics-over-time (Wang and McCallum, 2006) address the related question of topic evolution in a stream of documents, but these approaches are too coarse to model intra-document sequential structure. In terms of our technical approach, we build on previous research on gridbased models (Jojic and Perina, 2011), which have previously been used for topic-modeling for images and text as unstructured bags-of-features."
  }, {
    "heading": "3 Sequential CG model",
    "text": "In this section, we present our method, which we call Sequential Counting Grids, or Sequential CG. We first present our notation, model formulation and training approach. We discuss how the method is designed to incorporate smoothness and sequential structure, and how the method can be efficiently scaled to train on large document collections. In Section 3.2, we present a mixture model variant that combines Sequential CG with a unigram language model."
  }, {
    "heading": "3.1 Model description",
    "text": "We represent a document as a sequence s of sentences, s = {s1, s2 . . . sD}, where D represents the number of sentences in the document. In general, we assume each sentence is represented as a multiset of features si = {cz}i, where ciz represents the count of the feature indexed by z in the ith sentence in the sequence.1\nThe Sequential CG consists of a 3-D grid G of size Ex × Ey × Ez , where Ex, Ey and Ez denote the extent of the grid along the X, Y and Z-axes (see Figure 1). Let us denote an index of a position in the grid by an integer-valued vector i = (ixiyiz). The three components of the index together specify a XY location as well as a depth in the grid. The Sequential CG model is parametrized by two sets of parameters, πi,z and Pij. Here, πi,z represents a multinomial distribution over the vocabulary of features z for each cell in the grid G, i.e.∑\nz πi,z = 1 ∀ i ∈ G. To induce smoothness across XY planes, we further define histogram distributions hi,z , which average the π distributions in a 2-D neighborhood Wi (of size specified by W = [Wx,Wy]) around the grid position i. This notation follows Jojic and Perina (2011).\nhi,z = 1\nWxWy ∑ i′∈Wi πi′,z (1)\nThe generative model assumes that individual sentences in a document are generated by h distributions in the grid. Movements from one position i to another j in the grid are modeled as transition probabilities Pij. The generative process consists of the following. We uniformly sample a starting location i1 in the grid. We sample words in the first\n1These may simply consist of tokens (words, entities and MWEs) in the sentence, but can include additional information, such as sentiment or event annotations, or other discrete sentence-level representations\nsentence s1 from πi1, and sample the next position i2 from the distribution Pi1,:, and so on till we generate sD. The alignments I = [i1, i2 . . . iD] of individual sentences in a document with positions in the grid are latent variables in our model.\nGiven the sequence of alignments I for a document, the conditional likelihood of generating s is given as a product of generating individual sentences:\np(s| I) = D∏ d p({cdz}| id) = D∏ d=1 ∏ z (hid,z) cdz\n(2)\nSince the alignments of sequences to their positions in the grids I are latent, we marginalize over these to maximize the likelihood of an observed collection of documents S := {st}Tt=1. Here, T is the total number of documents, and t is an index over individual documents. Using Jensen’s inequality, any distributions qtI over the hidden alignments It provide lower-bounds on the data log-likelihood.\n∑ t log p(st|π) = ∑ t log (∑ I p(st, I|π) ) = ∑ t log (∑ I qtI p(st|I)p(I)) qtI\n) ≥ −\n∑ t ∑ I qtI log q t I\n+ ∑ t ∑ I qtI log ( p(s|I, π)p(I))\n(3)\nHere, qtI denotes a variational distribution for each of the data sequences st. The learning algorithm consists of an iterative generalized EM procedure (which can be interpreted as a block-coordinate ascent in the latent variables qtI and the model parameters π and P). We maximize the lower bound in Eqn 3 exactly by setting qtI to the posterior distribution of the data for the current values of the parameters π (standard E step). Thus, we have\nqtI ∝ p(s|I)p(I)\n= [ D∏ d=1 ∏ z (hid,z) cdz(t) ][ D∏ d=2 Pid−1,id ] (4)\nWe do not need to explicitly compute the posterior distribution qtI = p(I|s) at this point, but only use it to compute the relevant expectation statistics in the M-step. This can be done efficiently, as we\nsee next. In the M-step, we consider qtI as fixed, and maximize the objective in terms of the model parameters π. Substituting this in Eqn 3, and focusing on terms that depend on the model parameters (π and P), we get\nL(π,P) ≥ ∑ t ∑ I qtI log ( p(s|I, π)p(I)) +Hq\n= ∑ t ∑ I qtI (∑ d ∑ z cdz(t) log hid,z\n+ ∑ d logPid−1,id )\n= ∑ t ∑ I EqtI [∑ d ∑ z Iitd=ic d z(t) log hid,z ] + ∑ t ∑ I EqtI [∑ d Iitd−1=i,itd=j logPij ]\n(5)\nMaximizing the likelihood w.r.t. P leads to the following updates for the transition probabilities:2\nPij = ∑ t ∑ d P (i t d−1 = i, i t d = j)∑\nt ∑ d P (i t d−1 = i)\n(6)\nHere, the pairwise state-probabilities P (itd−1 = i, itd = j) for adjacent sentences in a sequence can be efficiently calculated using the ForwardBackward algorithm. In Equation 5, rewriting the term containing h in terms of π using Eqn 1 (and ignoring constant terms WxWy), we get:\n∑ t ∑ I EqtI [∑ d ∑ z Iitd=ic d z(t) log ∑ i′∈Wi πi′,z ] = ∑ t ∑ I ∑ d P (itd = i) ∑ z cdz(t) log ∑ i′∈Wi πi′,z\n(7)\nThe presence of a summation inside of a logarithm makes maximizing this objective for π harder. For this, we simply use Jensen’s inequality introducing an additional variational distribution (for the latent grid positions within window Wi ), and maximize the lower bound. The final M-step update for π becomes:\nπi,z ∝ (∑\nt ∑ d cdz(t) ∑\nk|i∈Wk\nP (itd = k)\nhk,z\n) πi,z\n(8) 2Since the optimal value for the concave problem∑\nj yj log xj s.t. ∑ j xj = 1 occurs when x ∗ j ∝ yj\nAs before, the state-probabilities P (itd = i) can be computed using the Forward Backward algorithm.\nIntuitively, the expected alignments in the E-step are distributions over sequences of positions in the grid that best explain the structure of documents for the current value of Sequential CG parameters. In the M-step, we assume these distributions embedding documents into various parts of the grid as given, and update the multinomial parameters and transition probabilities. Modeling the transitions as having a Markov property allows us to use a dynamic programming approach (Forward Backward algorithm) to exactly compute the posterior probabilities required for parameter updates. We note that at the onset of the procedure, we need to initialize π randomly to break symmetries. Unless otherwise stated, in all experiments, we run EM to 200 iterations.\nCorrelating space with sequential structure: The use of histogram distributions h to generate data forces smoothness in the model along XY planes due to adjacent cells in the grid sharing a large number of parameters that contribute to their histograms (due to overlapping windows). On the other hand, in order to induce spatial proximity in the grid to mimic the sequential flow of discourse in documents, we constrain the transition matrix P (which specifies transition preferences from one position in the grid to another) to a sparse banded matrix. In particular, a position i = (ix, iy, iz) in the grid can only transition to itself, its 4 neighbors in the same XY plane, and two cells in the succeeding two layers along the Z-axis ( (ix, iy, iz+1) and (ix, iy, iz+2)). This is enforced by fixing other elements in the transition matrix to 0, and only updating allowable transitions.\nAs an important note about implementation details, we observe here that the Forward-Backward procedure (which is repeatedly invoked during model training) can be naturally formulated in terms of matrix operations.3 This allows training for the Sequential CG approach to be scalable for large document collections.\nIn our formulation, we have presented a Sequential CG model for a 3-D grid. This can be adapted to learn 2-D grids (trellis) by setting Ey = 1. In our experiments, we found 3-D grids to be better\n3To explain, if fd1×G are forward probabilities for step d, and Od+1G×G are observation probabilities for step d + 1, fd+1 = fd ×P ×Od computes forward probabilities for the next step in the sequence\nin terms of task performance and visualization (for a comparable number of parameters)."
  }, {
    "heading": "3.2 Mixture model",
    "text": "The Sequential CG model described above can be combined with other generative models (e.g., language models) to get a mixture model. Here, we show how a unigram language model can be combined with Sequential CG. The rationale behind this is that since the Sequential CG is primarily designed to explain elements of documents that reflect sequential discourse structures, mixing with a context-agnostic distribution can allow it to focus specifically on elements that reflect sequential regularities. In experimental evaluation, we find that such a mixture model shows distinctly different behavior (see Section 4.1.1). Next, we briefly describe updates for this approach.\nLet µz denote the multinomial distribution over features for the unigram model to be mixed with the CG. Let βz be the mixing proportion for the feature z, i.e. an occurrence of z is presumed to come from the Sequential CG with probability βz , and from the unigram distribution with probability 1 − βz . Further, let αtz be binary variable that denotes whether a particular instance of z comes from the Sequential CG, or the unigram model. Then, Equation 2 changes to:\np(s| I, α) = ∏ z,d ( (hid,z) cdzβz )αtz( µc d z z (1−βz) )1−αtz Since we do not observe αtz (i.e., which distribution generated a particular feature in a particular document), they are additional latent variables in the model. Thus, we need to introduce a Bernoulli variational distribution qαzt . Doing this modifies relevant parts (containing qαzt) of Equation 5 to:\n∑ t ∑ I qtI (∑ z qαzt log ( βz ∏ d h cdz(t) id,z ) + (1− qαzt) log (( 1− βz)µ ∑ d c d z z\n) + ∑ d logPid−1,id ) +Hqαzt\n(9)\nThis leads to the following additional updates for estimating qαzt (in the E-step)\n4 and βz (in the Mstep). 4Since the optimal value for the concave problem∑ j xj log yj xj s.t. ∑ j xj = 1 occurs when x ∗ j ∝ yj\nqαzt = exp\n(∑I i P (i t d=i)c d z(t) log hid,z ) βz\nexp (∑I i P (i t d=i)c d z(t) log hid,z ) βz+µ ∑ d c d z z (1−βz)\nIn the M-step, βz can be estimated simply as the fraction of times z is generated from the Sequential CG component.\nβz = ∑ t qαzt∑ t Iz"
  }, {
    "heading": "4 Evaluation",
    "text": "In this section, we analyze the performance of our approach on text collections from several domains (including short stories, newswire text and biographies). We first qualitatively evaluate our generative method on a dataset of biographical extracts from Wikipedia, which visually illustrates biographical trajectories learned by the model, operationalizing our model concept from Figure 1 in real data (see Figure 2). Next, we evaluate our method on two standard tasks requiring document understanding: story cloze evaluation and sentence ordering. Since our method is completely unsupervised and is not tailored to specific tasks, competitive performance on these tasks would indicate that the method learns helpful regularities in text structure, useful for general-purpose language understanding."
  }, {
    "heading": "4.1 Visualizing Wikipedia biographies",
    "text": "We now qualitatively explore models learned by our method on a dataset of biographies from Wikipedia.5 For this, we use the data previously collected and processed by Bamman and Smith (2014). In all, the original dataset consists of extracts from biographies of about 240,000 individuals. For ease of training, we trained our method on a subset of the 50,0000 shortest documents from this set. The original paper uses the numerical order of dates mentioned in the biographies to extract biographical templates, but we do not use this information. Figure 2 visualizes a Sequential CG model learned on this dataset for on a grid of dimensions E = 8 × 8 × 5, and a histogram window W of dimensions 3× 3 . In general, we found that using larger grids leads to smoother transitions and learning more intricate patterns including hierarchies of trajectories, but here we show a model with a\n5For all our experimental evaluation, we tokenize and lemmatize text using the Stanford CoreNLP pipeline, but retain entity-names and contiguous text-spans representing MWEs as single units\nsmaller grid for ease of visualization. Here, the words in each cell in the grid denote the highest probability assignments in that cell. Larger fonts within a cell indicate higher probabilities.\nWe observe that the method successfully extracts various biographical trajectories, as well as capture a notion of similarity between them. To explain, we observe that the lower-right part of the learned grid largely models documents about sportpersons (with discernable regions focusing on sports like soccer, American football and ice-hockey). On the other hand, the left-half of the grid is dominated by biographies of people from the arts and humanities (inlcuding artists, writers, musicians, etc.). The top-center of the grid focuses on academicians and scientists, while the top-right represents biographies of political and military leaders. We note smooth transitions between different regions, which is precisely what we would expect from the use of the smoothing filter that incorporates parameter sharing across cells in the method. Further, as we go across the layers in the figure, we note the biographical trajectories learned by the model across the entire grid. For example, from the grid, the life trajectory of a football player can be visualized as being drafted, signing and playing for a team, and eventually becoming a head-coach or a hall-of-fame inductee."
  }, {
    "heading": "4.1.1 Effects of mixing",
    "text": "The Sequential-CG method can be combined with other generative models in a mixture model, fol-\nlowing the approach previously described in Section 3.2. A major reason to do this might be to allow the base model to handle general content, while allowing the Sequential-CG method to focus on modeling context-sensitive words only. Here, we empirically characterize the mixing behavior for different categories of words.\nFigure 3 shows the mixing proportion of different words when the Sequential-CG model is combined with a unigram model. In the figure, the X-axis corresponds to words in the dataset with decreasing frequency of occurrence, whereas the Yaxis denotes the mixing proportions βz learned by the mixture model. We note that the mixture model learns to explain frequent as well as the long-tail of rare words using the simple unigram model (as seen from low mixing proportion of Sequential-CG method). These regimes correspond to (1) stopwords and very common nouns, and (2) rare words respectively. In turn, this allows the SequentialCG component to preserve more probability mass to explain the intermediate content words. Thus, the Sequential-CG component only needs to model words that reflect useful statistical sequential patterns, without expending modeling effort on background content (common words) or noise (rare words). For the long tail of infrequent words, we observe that Sequential CG is much more likely to generate verbs and adjectives, rather than nouns. This is as we would expect, since verbs and adjectives often denote events and sentiments, which can\nbe important elements in discourse trajectories."
  }, {
    "heading": "4.2 Story-cloze",
    "text": "We next evaluate our method on the story-cloze task presented by Mostafazadeh et al. (2016), which tests common-sense understanding in context of children stories. The task consists of identifying the correct ending to a four-sentence long story (called context in the original paper) and two possible ending options. The dataset for the task consists of a collection of around 45K unlabeled 5-sentence long stories as well as 3742 5-sentence stories with two provided endings, with one labeled as the correct ending. For this task, we train our method on grids of dimension 15× 15× 6 (E), and histogram windows W of size 5× 5 on the unlabeled collection of stories. At test time, for each story, we are provided two versions (a story-version v consists of the provided context c, followed by a possible ending e1, i.e. v = [c, e] ). For prediction, we need to define a goodness score Sv for a story-version.\nIn the simplest case, this score can simply be the log-likelihood log pSCG(v) of the story-version, according to the Sequential-CG model. However, this is problematic since this is biased towards choosing shorter endings. To alleviate this, we define the goodness score by discounting the log-likelihood by the probability of the ending e itself, under a\nsimple unigram model.\nSv = log pSCG(c, e)− log puni(e)\nThe predicted ending is the story-version with a higher score. Table 1 shows the performance of variants of our approach for the task. Our baselines include previous approaches for the same task: DSSM is a deep-learning based approach, which maps the context and ending to the same space, and is the best-performing method in Mostafazadeh et al. (2016). GenSim and N-gram return the ending that is more similar to the context based on word2vec embeddings (Mikolov et al., 2013) and n-grams, respectively. Narrative-Chains computes the probability of each alternative based on eventchains, following the approach of Chambers and Jurafsky (2008).\nWe note that our method improves on the previous best unsupervised methods for the task. This is quite surprising, since our Sequential-CG model in this case is trained on bag-of-lemma representations, and only needs sentence segmentation, tokenization and lemmatization for preprocessing. On the other hand, approaches such as Narrative-Chains require parsing and eventrecognition, while approaches such as GenSim require learning word embeddings on large text corpora for training. Further, we note that predicting the ending without normalizing for the probability of the words in the ending results in significantly weaker performance, as expected. We train another\nvariant of Sequential-CG with the sentence-level sentiment annotation (from Stanford CoreNLP) also added as a feature. This does not improve performance, consistent with findings in Mostafazadeh et al. (2016). We also experiment with a variant where we perform Brown clustering (Brown et al., 1992) of words in the unlabeled stories (K = 500 clusters), and include cluster-annotations as features for training the method. Doing this explicitly incorporates lexical similarity into the model, leading to a small improvement in performance. Finally, a mixture model consisting of the Sequential-CG and a unigram language model leads to a further improvement in performance. The performance of our unsupervised approach on this task indicates that it can learn discourse structures that are helpful for general language understanding.\nThe story-cloze task has recently also been addressed as a shared task at EACL (Mostafazadeh et al., 2017) with a significantly expanded dataset, and achieving much higher performance. However, we note that the proposed best-performing approaches (Chaturvedi et al., 2017; Schwartz et al., 2017) for this task are all supervised, and hence not included here for comparison.\nFigure 4 shows examples where the model correctly identifies the ending. These show a mix of behavior such as sentiment coherence (identifying dissonance between ‘wonderful surprise’ and ‘stolen’) and modeling causation (police being called after being suspected)."
  }, {
    "heading": "4.3 Sentence Ordering",
    "text": "We next evaluate our method on the sentence ordering task, which requires distinguishing an original\ndocument from a version consisting of permutations of sentences of the original (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). For this, we use two datasets of documents and their permutations from Barzilay and Lapata (2008), which are used as standard evaluation for coherence prediction tasks. These consist of (i) reports of accidents from the National Transportation Safety Bureau (we refer to this data as accidents), and (ii) newswire reports about earthquake events from the Associated press (we refer to this as earthquakes). Each dataset consists of 100 training documents, and about 100 documents for testing. Also provided are about 20 generated permutations for each document (resulting in 1986 test pairs for accidents, and 1955 test pairs for earthquakes). Documents in accidents consist of between 6 and 19 sentences each, with a median of 11 sentences. Documents in earthquakes consist of between 4 and 30 sentences each, with a median of 10 sentences.\nSince the datasets for these tasks only have a relatively small number of training documents (100 each), we use Sequential-CG with smaller grids (3×3×15), and don’t train a mixture model (which needs to learn a parameter βz for each word in the vocabulary). Further, we train for a much smaller number of iterations to prevent overfitting (K = 3, chosen through cross-validation on the training set). During testing, since provided article pairs are simply permutations of each other and identical in content, we do not need to normalize as needed in Section 4.2. The score of a provided article is simply calculated as its log-likelihood. The article with higher likelihood is predicted to be the original.\nTable 2 shows performance of the method compared with other approaches for coherence prediction. We note that Sequential-CG performs com-\npetitively with the state-of-the-art for generative approaches for the task, while needing no other annotation. In comparison, the HMM based approaches use significant annotation and syntactic features. Sequential-CG also outperforms several discriminative approaches for the task. In Figure 5 we illustrate the learned discourse trajectories in terms of the most salient features in each sentence. Words in bold are those identified by the model to be most context-appropriate at the corresponding point in the narrative. This is done by ranking words by the ratio between their probabilities (π:,z) in the grid weighted by alignment locations of the document (qtI), and unigram probabilities."
  }, {
    "heading": "5 Conclusion",
    "text": "We have presented a simple model for extracting and visualizing latent discourse structure from unlabeled documents. The approach is coarse, and does not have explicit models for important elements such as entities and events in a discourse. However, the method outperforms some previous approaches on document understanding tasks, even while ignoring syntactic structure within sentences. The ability to visualize learning is a key component of our method, which can find significant applications in data mining and data-discovery in large text collections. More generally, similar approaches can explore a wider range of scenarios involving sequences of text. While here our focus was on learning discourse structures at the document level, similar methods can also be used at other scales, such as for syntactic or morphological analysis."
  }],
  "year": 2018,
  "references": [{
    "title": "Belief in discourse representation theory",
    "authors": ["Nicholas Asher."],
    "venue": "Journal of Philosophical Logic, 15(2):127–189.",
    "year": 1986
  }, {
    "title": "Unsupervised discovery of biographical structure from text",
    "authors": ["David Bamman", "Noah Smith."],
    "venue": "Transactions of the Association for Computational Linguistics, 2:363–376.",
    "year": 2014
  }, {
    "title": "Modeling local coherence: An entity-based approach",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Computational Linguistics, 34(1):1–34.",
    "year": 2008
  }, {
    "title": "Classbased n-gram models of natural language",
    "authors": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."],
    "venue": "Computational linguistics, 18(4):467–479.",
    "year": 1992
  }, {
    "title": "Unsupervised learning of narrative event chains",
    "authors": ["Nathanael Chambers", "Daniel Jurafsky."],
    "venue": "ACL, pages 789–797. The Association for Computer Linguistics.",
    "year": 2008
  }, {
    "title": "Story comprehension for predicting what happens next",
    "authors": ["Snigdha Chaturvedi", "Haoruo Peng", "Dan Roth."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1603–1614.",
    "year": 2017
  }, {
    "title": "Where have I heard this story before?’ : Identifying narrative similarity in movie remakes",
    "authors": ["Snigdha Chaturvedi", "Shashank Srivastava", "Dan Roth."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
    "year": 2018
  }, {
    "title": "Integrating topics and syntax",
    "authors": ["Thomas L Griffiths", "Mark Steyvers", "David M Blei", "Joshua B Tenenbaum."],
    "venue": "Advances in neural information processing systems, pages 537–544.",
    "year": 2005
  }, {
    "title": "Graphbased local coherence modeling",
    "authors": ["Camille Guinaudeau", "Michael Strube."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 93–103.",
    "year": 2013
  }, {
    "title": "Multidimensional counting grids: Inferring word order from disordered bags of words",
    "authors": ["Nebojsa Jojic", "Alessandro Perina."],
    "venue": "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 547–556. AUAI Press.",
    "year": 2011
  }, {
    "title": "Recurrent convolutional neural networks for discourse compositionality",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "ACL 2013, page 119.",
    "year": 2013
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc V. Le", "Tomas Mikolov."],
    "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 2126 June 2014, pages 1188–1196.",
    "year": 2014
  }, {
    "title": "A model of coherence based on distributed sentence representation",
    "authors": ["Jiwei Li", "Eduard Hovy."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048.",
    "year": 2014
  }, {
    "title": "Neural net models of open-domain discourse coherence",
    "authors": ["Jiwei Li", "Dan Jurafsky."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209.",
    "year": 2017
  }, {
    "title": "Recursive deep models for discourse parsing",
    "authors": ["Jiwei Li", "Rumeng Li", "Eduard H Hovy."],
    "venue": "EMNLP, pages 2061–2069.",
    "year": 2014
  }, {
    "title": "Automatically evaluating text coherence using discourse relations",
    "authors": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
    "year": 2011
  }, {
    "title": "A coherence model based on syntactic patterns",
    "authors": ["Annie Louis", "Ani Nenkova."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157–1168. As-",
    "year": 2012
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C Mann", "Sandra A Thompson."],
    "venue": "Text-Interdisciplinary Journal for the Study of Discourse, 8(3):243–281.",
    "year": 1988
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
    "authors": ["Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen."],
    "venue": "Proceedings of NAACL-",
    "year": 2016
  }, {
    "title": "LSDSem 2017 shared task: The story cloze test",
    "authors": ["Nasrin Mostafazadeh", "Michael Roth", "Annie Louis", "Nathanael Chambers", "James Allen."],
    "venue": "Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Seman-",
    "year": 2017
  }, {
    "title": "Learning statistical scripts with LSTM recurrent neural networks",
    "authors": ["Karl Pichotta", "Raymond J Mooney."],
    "venue": "AAAI.",
    "year": 2015
  }, {
    "title": "Scripts, plans, goals and understanding: an inquiry into human knowledge structures",
    "authors": ["Roger C Schank", "Robert P Abelson."],
    "venue": "Erlbaum.",
    "year": 1977
  }, {
    "title": "The effect of different writing tasks on linguistic style: A",
    "authors": ["Roy Schwartz", "Maarten Sap", "Ioannis Konstas", "Leila Zilles", "Yejin Choi", "Noah A Smith"],
    "year": 2017
  }, {
    "title": "Steering output style and topic in neural response generation",
    "authors": ["Di Wang", "Nebojsa Jojic", "Chris Brockett", "Eric Nyberg."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2140–2150, Copenhagen,",
    "year": 2017
  }, {
    "title": "Topics over time: A non-markov continuous-time model of topical trends",
    "authors": ["Xuerui Wang", "Andrew McCallum."],
    "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06, pages 424–",
    "year": 2006
  }],
  "id": "SP:f9196480457bca23771da88d3f20ce177c8d036c",
  "authors": [{
    "name": "Shashank Srivastava",
    "affiliations": []
  }, {
    "name": "Nebojsa Jojic",
    "affiliations": []
  }],
  "abstractText": "We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2or 3-D spatial grids, in which one or two coordinates model smooth topic transitions, while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualizable and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method can capture discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method can capture biographical templates from Wikipedia, and is competitive with state-ofthe-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.",
  "title": "A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text"
}