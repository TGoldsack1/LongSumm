{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2017 Conference, pages 231–240, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Natural Language Generation (NLG) plays a critical role in a Spoken Dialogue System (SDS), and its task is to convert a meaning representation produced by the dialogue manager into natural language sentences. Conventional approaches to NLG follow a pipeline which typically breaks down the task into sentence planning and surface realization. Sentence planning decides the order and structure of a sentence, followed by a surface realization which converts the sentence structure into final utterance. Previous approaches to NLG still rely on extensive hand-tuning templates\n?Corresponding author.\nand rules that require expert knowledge of linguistic representation. There are some common and widely used approaches to solve NLG problems, including rule-based (Cheyer and Guzzoni, 2014), corpus-based n-gram generator (Oh and Rudnicky, 2000), and a trainable generator (Ratnaparkhi, 2000).\nRecurrent Neural Network (RNN)-based approaches have recently shown promising results in NLG tasks. The RNN-based models have been used for NLG as a joint training model (Wen et al., 2015a,b) and an end-to-end training network (Wen et al., 2016c). A recurring problem in such systems requiring annotated corpora for specific dialogue acts∗ (DAs). More recently, the attentionbased RNN Encoder-Decoder (AREncDec) approaches (Bahdanau et al., 2014) have been explored to tackle the NLG problems (Wen et al., 2016b; Mei et al., 2015; Dušek and Jurčı́ček, 2016b,a). The AREncDEc-based models have also shown improved results on various tasks, e.g., image captioning (Xu et al., 2015; Yang et al., 2016), machine translation (Luong et al., 2015; Wu et al., 2016).\nTo ensure that the generated utterance represents the intended meaning of the given DA, the previous RNN-based models were conditioned on a 1-hot vector representation of the DA. Wen et al. (2015a) proposed a Long Short-Term Memorybased (HLSTM) model which introduced a heuristic gate to guarantee that the slot-value pairs were accurately captured during generation. Subsequently, Wen et al. (2015b) proposed an LSTMbased generator (SC-LSTM) which jointly learned the controlling signal and language model. Wen et al. (2016b) proposed an AREncDec based generator (ENCDEC) which applied attention mechanism on the slot-value pairs. ∗A combination of an action type and a set of slot-value pairs). E.g. inform(name=’Piperade’; food=’Basque’).\n231\nAlthough these RNN-based generators have worked well, however, they still have some drawbacks, and none of these models significantly outperform the others in solving NLG tasks. While the HLSTM cannot handle cases such as the binary slots (i.e., yes and no) and slots that take don’t care value in which these slots cannot be directly delexicalized, the SCLSTM model is limited to generalize to the unseen domain, and the ENCDEC model has difficulty to prevent undesirable semantic repetitions during generation.\nTo address the above issues, we propose a new architecture, Encoder-Aggregator-Decoder, an extension of the AREncDec model, in which the proposed Aggregator has two main components: (i) an Aligner which computes the attention over the input sequence, and (ii) a Refiner which are another attention or gating mechanisms to further select and aggregate the semantic elements. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences. We conduct comprehensive experiments on four NLG domains and find that the proposed method significantly outperforms the previous methods regarding BLEU (Papineni et al., 2002) and slot error rate ERR scores (Wen et al., 2015b). We also found that our generator can produce high-quality utterances with correctly ordered than those in the previous methods (see Table 1). To sum up, we make two key contributions in this paper:\n• We present a semantic component called Aggregator which is easy integrated into existing (attentive) RNN encoder-decoder architecture, resulting in an end-to-end generator that empirically improved performance in comparison with the previous approaches.\n• We present several different choices of attention and gating mechanisms which can be effectively applied to the proposed semantic Aggregator.\nIn Section 2, we review related works. The proposed model is presented in Section 3. Section 4 describes datasets, experimental setups and evaluation metrics. The results and analysis are presented in Section 5. We conclude with a brief of summary and future work in Section 6."
  }, {
    "heading": "2 Related Work",
    "text": "Conventional approaches to NLG traditionally divide the task into a pipeline of sentence planning and surface realization. The conventional methods still rely on the handcrafted rule-based generators or rerankers. Oh and Rudnicky (2000) proposed a class-based n-gram language model (LM) generator which can learn to generate the sentences for a given dialogue act and then select the best sentences using a rule-based reranker. Ratnaparkhi (2000) later addressed some of the limitation of the class-based LMs by proposing a method based on a syntactic dependency tree. A phrase-based generator based on factored LMs was introduced by Mairesse and Young (2014), that can learn from a semantically aligned corpus.\nRecently, RNNs-based approaches have shown promising results in the NLG domain. Vinyals et al. (2015); Karpathy and Fei-Fei (2015) applied RNNs in setting of multi-modal to generate caption for images. Zhang and Lapata (2014) also proposed a generator using RNNs to create Chinese poetry. For task-oriented dialogue systems, Wen et al. (2015a) combined two TNNbased models with a CNN reranker to generate required utterances. Wen et al. (2015b) proposed SC-LSTM generator which proposed an additional ”reading” cell to the traditional LSTM cell to learn the gating mechanism and language model jointly. A recurring problem in such systems lacking of sufficient domain-specific annotated corpora. Wen et al. (2016a) proposed an out-of-domain model which is trained on counterfeited datasets by using semantic similar slots from the target-domain dataset instead of the slots belonging to the outof-domain dataset. The empirical results indicated\nthat the model can obtain a satisfactory results with a small amount of in-domain data by finetuning the target-domain on the out-of-domain trained model.\nMore recently, attentional RNN encoderdecoder based models (Bahdanau et al., 2014) have shown improved results in a variety of tasks. Yang et al. (2016) presented a review network in solving the image captioning task, which produces a compact thought vector via reviewing all the input information encoded by the encoder. Mei et al. (2015) proposed attentional RNN encoderdecoder based model by introducing two layers of attention to model content selection and surface realization. More close to our work, Wen et al. (2016b) proposed an attentive encoderdecoder based generator, which applied the attention mechanism over the slot-value pairs. The model indicated a domain scalability when a very limited proportion of training data is available."
  }, {
    "heading": "3 Recurrent Neural Language Generator",
    "text": "The recurrent language generator proposed in this paper is based on a neural net language generator (Wen et al., 2016b) which consists of three components: an encoder to incorporate the target meaning representation as the model inputs, an aggregator to align and control the encoded information, and a decoder to generate output sentences. The generator architecture is shown in Figure 1. While the decoder typically uses an RNN model, there is a variety of ways to choose the encoder because it depends on the nature of the meaning\nrepresentation and the interaction between semantic elements. The encoder first encodes the input meaning representation, then the aggregator with a feature selecting or an attention-based mechanism is used to aggregate and select the input semantic elements. The input to the RNN decoder at each time step is a 1-hot encoding of a token† and the aggregated input vector. The output of RNN decoder represents the probability distribution of the next token given the previous token, the dialogue act representation, and the current hidden state. At generation time, we can sample from this conditional distribution to obtain the next token in a generated sentence, and feed it as the next input to the RNN decoder. This process finishes when a stop sign is generated (Karpathy and FeiFei, 2015), or some constraint is reached (Zhang and Lapata, 2014). The network can generate a sequence of tokens which can be lexicalized‡ to form the required utterance. †Input texts are delexicalized in which slot values are replaced by its corresponding slot tokens. ‡The process in which slot token is replaced by its value."
  }, {
    "heading": "3.1 Gated Recurrent Unit",
    "text": "The encoder and decoder of the proposed model use a Gated Recurrent Unit (GRU) network proposed by Bahdanau et al. (2014), which maps an input sequence W = [w1,w2, ..,wT ] to a sequence of states H = [h1,h2, ..,hT ] as follows:\nri = σ(Wrwwi + Wrhhi−1) ui = σ(Wuwwi + Wuhhi−1)\nh̃i = tanh(Whwwi + ri Whhhi−1) hi = ui hi−1 + (1− ui) h̃i\n(1)\nwhere: denotes the element-wise multiplication, ri and ui are called the reset and update gates respectively, and h̃i is the candidate activation."
  }, {
    "heading": "3.2 Encoder",
    "text": "The encoder uses a separate parameterization of the slots and values. It encodes the source information into a distributed vector representation zi which is a concatenation of embedding vector representation of each slot-value pair, and is computed by: zi = oi ⊕ vi (2) where: oi and vi are the i-th slot and value embedding, respectively. The i index runs over the given slot-value pairs. In this study, we use a Bidirectional GRU (Bi-GRU) to encode the sequence of slot-value pairs§ embedding. The Bi-GRU consists of forward and backward GRUs. The forward GRU reads the sequence of slot-value pairs from left-to-right and calculates the forward hidden states (−→s1 , ..,−→sK). The backward GRU reads the slot-value pairs from right-to-left, resulting in a sequence of backward hidden states (←−s1 , ..,←−sK). We then obtain the sequence of hidden states S = [s1, s2, .., sK ] where si is a sum of the forward hidden state −→si and the backward one←−si as follows:\nsi = −→si +←−si (3)"
  }, {
    "heading": "3.3 Aggregator",
    "text": "The Aggregator consists of two components: an Aligner and a Refiner. The Aligner computes the dialogue act representation while the choices for Refiner can be varied. §We treat the set of slot-value pairs as a sequence and use the order specified by slot’s name (e.g., slot area comes first, price follows area). We have tried treating slot-value pair sequence as natural order as appear in the DA, which even yielded worse results.\nFirstly, the Aligner calculates dialogue act embedding dt as follows:\ndt = a⊕ ∑\ni αt,isi (4)\nwhere: a is vector embedding of the action type, ⊕ is vector concatenation, and αt,i is the weight of i-th slot-value pair calculated by the attention mechanism:\nαt,i = exp(et,i)∑ j exp(et,j)\net,i = a(si,ht−1)\na(si,ht−1) = v>a tanh(Wasi + Uaht−1)\n(5)\nwhere: a(., .) is an alignment model,va,Wa,Ua are the weight matrices to learn.\nSecondly, the Refiner calculates the new input xt based on the original input token wt and the DA representation. There are several choices to formulate the Refiner such as gating mechanism or attention mechanism. For each input token wt, the selected mechanism module computes the new input xt based on the dialog act representation dt and the input token embedding wt, and is formulated by:\nxt = fR(dt,wt) (6)\nwhere: fR is a refinement function, in which each input token is refined (or filtered) by the dialogue act attention information before putting into the RNN decoder. By this way, we can represent the whole sentence based on this refined input using RNN model.\nAttention Mechanism: Inspired by work of Cui et al. (2016), in which an attention-overattention was introduced in solving reading comprehension tasks, we place another attention applied for Refiner over the attentive Aligner, resulting in a model Attentional Refiner over Attention (ARoA).\n• ARoA with Vector (ARoA-V): We use a simple attention where each input token representation is weighted according to dialogue act attention as follows:\nβt = σ(V>radt) fR(dt,wt) = βt ∗ wt\n(7)\nwhere: Vra is a refinement attention vector which is used to determine the dialogue act attention strength, and σ is sigmoid function to normalize the weight βt between 0 and 1.\n• ARoA with Matrix (ARoA-M): ARoA-V uses only a vector Vra to weight the DA attention. It may be better to use a matrix to control the attention information. The Equation 7 is modified as follows:\nVra = Wawwt βt = σ(V>radt)\nfR(dt,wt) = βt ∗ wt (8)\nwhere: Waw is a refinement attention matrix.\n• ARoA with Context (ARoA-C): The attention in ARoA-V and ARoA-M may not capture the relationship between multiple tokens. In order to add context information into the attention process, we modify the attention weights in Equation 8 with additional history information ht−1:\nVra = Wawwt + Wahht−1 βt = σ(V>radt)\nfR(dt,wt,ht−1) = βt ∗ wt (9)\nwhere: Waw,Wah are parameters to learn, Vra is the refinement attention vector same as above, which contains both DA attention and context information.\nGating Mechanism: We use simple elementwise operators (multiplication or addition) to gate the information between the two vectors dt and wt as follows:\n• Multiplication (GR-MUL): The element-wise multiplication plays a part in word-level matching which learns not only the vector similarity, but also preserve information about the two vectors:\nfR(dt,wt) = Wgddt wt (10)\n• Addition (GR-ADD):\nfR(dt,wt) = Wgddt + wt (11)"
  }, {
    "heading": "3.4 Decoder",
    "text": "The decoder uses a simple GRU model as described in Section 3.1. In this work, we propose to apply the DA representation and the refined inputs deeper into the GRU cell. Firstly, the GRU reset and update gates can be further influenced on the\nDA attentive information dt. The reset and update gates are modified as follows:\nrt = σ(Wrxxt + Wrhht−1 + Wrddt) ut = σ(Wuxxt + Wuhht−1 + Wuddt)\n(12)\nwhere: Wrd and Wud act like background detectors that learn to control the style of the generating sentence. By this way, the reset and update gates learn not only the long-term dependency but also the attention information from the dialogue act and the previous hidden state. Secondly, the candidate activation h̃t is also modified to depend on the DA representation as follows:\nh̃t = tanh(Whxxt + rt Whhht−1 +Whddt) + tanh(Wdcdt)\n(13)\nThe hidden state is then computed by:\nht = ut ht−1 + (1− ut) h̃t (14) Finally, the output distribution is computed by applying a softmax function g, and the distribution is sampled to obtain the next token,\nP (wt+1 | wt, wt−1, ...w0, z) = g(Whoht) wt+1 ∼ P (wt+1 | wt, wt−1, ...w0, z) (15)"
  }, {
    "heading": "3.5 Training",
    "text": "The objective function was the negative loglikelihood and simply computed by:\nF (θ) = − T∑\nt=1\ny>t log pt (16)\nwhere: yt is the ground truth word distribution, pt is the predicted word distribution, T is length of the input sequence. The proposed generators were trained by treating each sentence as a minibatch with l2 regularization added to the objective function for every 10 training examples. The pre-trained word vectors (Pennington et al., 2014) were used to initialize the model. The generators were optimized by using stochastic gradient descent and back propagation through time (Werbos, 1990). To prevent over-fitting, we implemented early stopping using a validation set as suggested by Mikolov (2010)."
  }, {
    "heading": "3.6 Decoding",
    "text": "The decoding consists of two phases: (i) overgeneration, and (ii) reranking. In the overgeneration, the generator conditioned on the given\nDA uses a beam search to generate a set of candidate responses. In the reranking, the cost of the generator is computed to form the reranking score R as follows:\nR = − T∑\nt=1\ny>t log pt + λERR (17)\nwhere λ is a trade off constant and is set to a large value in order to severely penalize nonsensical outputs. The slot error rate ERR, which is the number of slots generated that is either redundant or missing, and is computed by:\nERR = p+ q N\n(18)\nwhere: N is the total number of slots in DA, and p, q is the number of missing and redundant slots, respectively. Note that the ERR reranking criteria cannot handle arbitrary slot-value pairs such as binary slots or slots that take the dont care value because these slots cannot be delexicalized and matched."
  }, {
    "heading": "4 Experiments",
    "text": "We conducted an extensive set of experiments to assess the effectiveness of our model using several metrics, datasets, and model architectures, in order to compare to prior methods."
  }, {
    "heading": "4.1 Datasets",
    "text": "We assessed the proposed models using four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in (Wen et al., 2015b) which contain around 5K utterances and 200 distinct DAs. The Laptop and TV datasets have been released by Wen et al. (2016a). These datasets contain about 13K distinct DAs in the Laptop domain and 7K distinct DAs in the TV. Both Laptop and TV datasets have a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs. As a result, the NLG tasks for the Laptop and TV datasets become much harder."
  }, {
    "heading": "4.2 Experimental Setups",
    "text": "The generators were implemented using the TensorFlow library (Abadi et al., 2016) and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1. The hidden layer size was set to be 80 for all cases, and the generators were trained with a 70% of dropout rate. We perform 5 runs with different random initialization of the network and the training is terminated by using early stopping as described in Section 3.5. We select a model that yields the highest BLEU score on the validation set as shown in Table 2. Since the trained models can differ depending on the initialization, we also report the results which were averaged over 5 randomly initialized networks. Note that, except the results reported in Table 2, all the results shown were averaged over 5 randomly initialized networks. The decoder procedure used beam search with a beam width of 10. We set λ to 1000 to severely discourage the reranker from selecting utterances which contain either redundant or missing slots. For each DA, we over-generated 20 candidate utterances and selected the top 5 realizations after reranking. Moreover, in order to better understand the effectiveness of our proposed methods, we (1) trained the models on the Laptop domain with a varied proportion of training data, starting from 10% to 100% (Figure 3), and (2) trained general models by merging all the data from four domains together and tested them in each individual domain (Figure 4) ."
  }, {
    "heading": "4.3 Evaluation Metrics and Baselines",
    "text": "The generator performance was assessed by using two objective evaluation metrics: the BLEU score and the slot error rate ERR. Both metrics were computed by adopting code from an open source benchmark NLG toolkit¶. We compared our proposed models against three strong baselines from the open source benchmark toolkit. The results have been recently published as an NLG benchmarks by the Cambridge Dialogue Systems Group¶, including HLSTM, SCLSTM, and ENCDEC models."
  }, {
    "heading": "5 Results and Analysis",
    "text": ""
  }, {
    "heading": "5.1 Results",
    "text": "We conducted extensive experiments on the proposed models with varied setups of Refiner and\n¶https://github.com/shawnwun/RNNLG\ncompared against the previous methods. Overall, the proposed models consistently achieve the better performances regarding both evaluation metrics across all domains.\nTable 2 shows a comparison between the AREncDec based models (the models with ] symbol) in which the proposed models significantly reduce the slot error rate across all datasets by a large margin about 2% to 4% that are also improved performances on the BLEU score when comparing the proposed models against the previous approaches. Table 3 further shows the stable strength of our models since the results’ pattern stays unchanged compared to those in Table 2. The ARoAM model shows the best performance over all the four domains, while it is an interesting observation that the GR-ADD model with simple addition operator for Refiner obtains the second best performance. All these prove the importance of the proposed component Refiner in aggregating and selecting the attentive information.\nFigure 3 illustrates a comparison of four models (ENCDEC, SCLSTM, ARoA-M, and GR-ADD) which were trained from scratch on the laptop dataset in a variety of proportion of training data, from 10% to 100%. It clearly shows that the BLEU increases while the slot error rate decreases as more training data was provided. Figure 4 presents a comparison performance of general models as described in Section 4.2. Not surprisingly, the two proposed models still obtain higher the BLEU score, while the ENCDEC has difficulties in reducing the ERR score in all cases. Both the proposed models show their ability to generalize in the unseen domains (TV and Laptop datasets) since they consistently outperform the previous methods no matter how much training data was fed or how training method was used. These indicate the relevant contribution of the proposed component Refiner to the original AREncDec architecture, in which the Refiner with gating or attention mechanism can effectively aggregate the information before putting them into the RNN decoder.\nFigure 5 shows a different attention behavior of the proposed models in a sentence. While all the three models could attend the slot tokens and their surrounding words, the ARoA-C model with context shows its ability in attending the consecutive words. Table 4 shows comparison of responses generated for some DAs between different models.\nThe previous approaches (ENCDEC, HLSTM) still have missing and misplaced information, whereas the proposed models can generate complete and correct-order sentences."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "We present an extension of an Attentional RNN Encoder-Decoder model named EncoderAggregator-Decoder, in which a Refiner component is introduced to select and aggregate the semantic elements produced by the encoder. We also present several different choices of gating and attention mechanisms which can be effectively applied to the Refiner. The extension, which is easily integrated into an RNN Encoder-Decoder, shows its ability to refine the inputs and control the flow information before putting them into the RNN decoder. We evaluated the proposed model on four\ndomains and compared to the previous generators. The proposed models empirically show consistent improvement over the previous methods in both BLEU and ERR evaluation metrics. In the future, it would be interesting to further investigate hybrid models which integrate gating and attention mechanisms in order to leverage the advantages of both mechanisms."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported by JSPS Kakenhi Grant Number JP15K16048 and the grant for the research cooperation between JAIST and TIS. The first author would like to thank the Vietnamese Government Scholarship (911 project)."
  }],
  "year": 2017,
  "references": [{
    "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
    "authors": ["Martın Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"],
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473 .",
    "year": 2014
  }, {
    "title": "Method and apparatus for building an intelligent automated assistant",
    "authors": ["A. Cheyer", "D. Guzzoni."],
    "venue": "US Patent 8,677,377. https://www.google.com/patents/US8677377.",
    "year": 2014
  }, {
    "title": "Attention-overattention neural networks for reading comprehension",
    "authors": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."],
    "venue": "arXiv preprint arXiv:1607.04423 .",
    "year": 2016
  }, {
    "title": "Jurčı́ček. 2016a. A contextaware natural language generator for dialogue systems",
    "authors": ["Ondřej Dušek", "Filip"],
    "year": 2016
  }, {
    "title": "2016b. Sequenceto-sequence generation for spoken dialogue via deep syntax trees and strings",
    "authors": ["Ondřej Dušek", "Filip Jurčı́ček"],
    "year": 2016
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Li Fei-Fei."],
    "venue": "Proceedings of the IEEE Conference CVPR. pages 3128–3137.",
    "year": 2015
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1508.04025 .",
    "year": 2015
  }, {
    "title": "Stochastic language generation in dialogue using factored language models",
    "authors": ["François Mairesse", "Steve Young."],
    "venue": "Computational Linguistics .",
    "year": 2014
  }, {
    "title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter."],
    "venue": "arXiv preprint arXiv:1509.00838 .",
    "year": 2015
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov"],
    "year": 2010
  }, {
    "title": "Stochastic language generation for spoken dialogue systems",
    "authors": ["Alice H Oh", "Alexander I Rudnicky."],
    "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3. Association for Computational Linguistics, pages 27–",
    "year": 2000
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th ACL. Association for Computational Linguistics, pages 311–318.",
    "year": 2002
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP. volume 14, pages 1532–",
    "year": 2014
  }, {
    "title": "Trainable methods for surface natural language generation",
    "authors": ["Adwait Ratnaparkhi."],
    "venue": "Proceedings",
    "year": 2000
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3156–3164.",
    "year": 2015
  }, {
    "title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking",
    "authors": ["Tsung-Hsien Wen", "Milica Gašić", "Dongho Kim", "Nikola Mrkšić", "Pei-Hao Su", "David Vandyke", "Steve Young."],
    "venue": "Proceedings",
    "year": 2015
  }, {
    "title": "Multi-domain neural network language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young."],
    "venue": "arXiv preprint arXiv:1603.01232 .",
    "year": 2016
  }, {
    "title": "Toward multidomain language generation using recurrent neural networks",
    "authors": ["Tsung-Hsien Wen", "Milica Gašic", "Nikola Mrkšic", "Lina M Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young"],
    "year": 2016
  }, {
    "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "PeiHao Su", "David Vandyke", "Steve Young."],
    "venue": "Proceedings of EMNLP. Association for Computa-",
    "year": 2015
  }, {
    "title": "A networkbased end-to-end trainable task-oriented dialogue system",
    "authors": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."],
    "venue": "arXiv preprint arXiv:1604.04562 .",
    "year": 2016
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["Paul J Werbos."],
    "venue": "Proceedings of the IEEE 78(10):1550–1560.",
    "year": 1990
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."],
    "venue": "ICML. volume 14, pages 77–81.",
    "year": 2015
  }, {
    "title": "Review networks for caption generation",
    "authors": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W Cohen", "Ruslan R Salakhutdinov."],
    "venue": "Advances in Neural Information Processing Systems. pages 2361–2369.",
    "year": 2016
  }, {
    "title": "Chinese poetry generation with recurrent neural networks",
    "authors": ["Xingxing Zhang", "Mirella Lapata."],
    "venue": "EMNLP. pages 670–680.",
    "year": 2014
  }],
  "id": "SP:af814339692593a037f0891841d9849bfc1b4568",
  "authors": [{
    "name": "Van-Khanh Tran",
    "affiliations": []
  }, {
    "name": "Le-Minh Nguyen",
    "affiliations": []
  }, {
    "name": "Satoshi Tojo",
    "affiliations": []
  }],
  "abstractText": "Natural language generation (NLG) is an important component in spoken dialogue systems. This paper presents a model called Encoder-Aggregator-Decoder which is an extension of an Recurrent Neural Network based Encoder-Decoder architecture. The proposed Semantic Aggregator consists of two components: an Aligner and a Refiner. The Aligner is a conventional attention calculated over the encoded input information, while the Refiner is another attention or gating mechanism stacked over the attentive Aligner in order to further select and aggregate the semantic elements. The proposed model can be jointly trained both text planning and text realization to produce natural language utterances. The model was extensively assessed on four different NLG domains, in which the results showed that the proposed generator consistently outperforms the previous methods on all the NLG domains.",
  "title": "Neural-based Natural Language Generation in Dialogue using RNN Encoder-Decoder with Semantic Aggregation"
}