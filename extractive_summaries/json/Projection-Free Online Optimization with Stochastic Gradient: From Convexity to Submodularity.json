{
  "sections": [{
    "text": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of nonconvex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projectionfree algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal O( √ T ) adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an O(T 2/3) stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel “lifting” framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments.\n1Yale Institute for Network Science, Yale University, New Haven, CT, USA 2Department of Electrical Engineering, Yale University 3Department of Computer Science, Yale University 4Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA. Correspondence to: Lin Chen <lin.chen@yale.edu>."
  }, {
    "heading": "1. Introduction",
    "text": "As the amount of collected data becomes massive in both size and complexity, algorithm designers are faced with unprecedented challenges in statistics, machine learning, and control. In the past decade, online optimization has provided a successful computational framework for tackling a wide variety of challenging problems, ranging from non-parametric regression to portfolio management (Calandriello et al., 2017; Agarwal et al., 2006). In online optimization, a large or complex optimization problem is broken down into a sequence of smaller optimization problems, each of which must be solved with limited information. This framework captures many real-world scenarios in which standard optimization theory does not apply. For instance, a machine learning application cannot feasibly process terabytes of data at a single time; rather, subsets of data may be handled in a sequential fashion. Another example is when the true objective function is the expectation of an unknown distribution of functions, and may only be accessible via samples, as is the case for problems in online learning and control theory (Xiao, 2010; Wang & Boyd, 2008).\nOnline convex optimization, a branch of online optimization that considers sequentially minimizing convex functions, has proved particularly useful for statistical and machine learning applications. Online convex optimization has enjoyed much success in these areas because most offline machine learning techniques utilize the existing theory of convex optimization. As in the offline setting, gradient methods are a popular class of algorithms for online convex optimization due to their simplicity; however, they require projections onto the constraint set, which involve solving a quadratic program in the general case. These projections are infeasible for large scale applications with complicated constraints such as matrix completion, network routing problems, and maximum matchings. Online projection-free methods have been proposed and are much more efficient , replacing a projection onto the constraint set with a linear optimization over the constraint set at each iteration (Hazan & Kale, 2012; Garber & Hazan, 2013). However, these projection-free methods require exact gradient computations, which may be prohibitively expensive for even moderately sized data sets and intractable when a ar X iv :1 80 2.\n08 18\n3v 4\n[ st\nat .M\nL ]\n1 4\nJu n\n20 18\nclosed form does not exist. Thus, there is a huge need for online convex optimization routines that are projection-free and also robust to stochastic gradient estimates.\nWhile convex programs may be efficiently solved (at least in theory), there is a growing number of non-convex problems arising in machine learning and statistics. Notable examples include nonnegative principle component analysis, low-rank matrix recovery, sigmoid loss functions for binary classification, and the training of deep neural networks, to name a few. Understanding which types of non-convex functions may be efficiently optimized and developing techniques for doing so is a pressing research question for both theory and practice. Recently, continuous DR-submodular functions have been proposed as a broad class of non-convex functions which admit efficient approximate maximization routines, even though exact maximization is NP-Hard (Bian et al., 2017). These functions capture many real-life applications, such as optimal experiment design, non-definite quadratic programming, coverage and diversity functions, and continuous relaxation of discrete submodular functions. Recent works (Chen et al., 2018) have proposed methods for online continuous DR-submodular optimization; however, these too require either expensive projections or exact gradient computations.\nOur contributions In this paper, we present a suite of projection-free algorithms for online optimization that use stochastic estimates of the gradient and leverage the averaging technique (Mokhtari et al., 2018a;b) to reduce their variance. This includes\n• Meta-Frank-Wolfe, the first projection-free algorithm for adversarial online optimization which requires only stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves optimal O( √ T ) regret and (1− 1/e)-regret\nbounds for convex and submodular optimization, respectively.\n• One-Shot Frank-Wolfe, a simpler projection-free algorithm for stochastic online optimization which requires only a single stochastic gradient estimate in each round. This simpler algorithm achieves O(T 2/3) regret and (1 − 1/e)-regret bounds for the convex and submodular case, respectively.\n• A novel class of algorithms for online discrete submodular optimization which are based on lifting discrete functions to the continuous domain, applying our methods with an extremely efficient sampling technique, and using rounding schemes to produce a discrete solution.\nFinally, to demonstrate the effectiveness of our algorithms,\nwe tested their performance on an extensive set of experiments and measured against common baselines."
  }, {
    "heading": "2. Related Work",
    "text": "The Frank-Wolfe algorithm, also known as the conditional gradient descent, was originally proposed for the offline setting in (Frank & Wolfe, 1956). The framework of online convex optimization was introduced by Zinkevich (2003), in which the online projected gradient descent was proposed and proved to achieve an O( √ T ) regret bound. However, the projections required for such an algorithm are too expensive for many large-scale online problems. The online conditional gradient descent was the first projection-free online algorithm, originally proposed in (Hazan & Kale, 2012). An improved conditional gradient algorithm was later designed for smooth and strongly convex optimization which achieves the optimal O( √ T ) adversarial regret bound (Garber & Hazan, 2013). However, both of these algorithms can perform arbitrarily poorly if supplied with stochastic gradient estimates. Lafond et al. (2015) proposed an online Frank-Wolfe variant for the any-time stochastic online setting that converges to a stationary point for nonconvex expected functions . While convergence is an important property of the any-time methods, arbitrary stationary points do not yield approximation guarantees for general non-convex functions.\nJohnson & Zhang (2013) introduced the variance reduction technique for accelerating stochastic gradient descent. It was independently discovered by Mahdavi et al. (2013). Allen-Zhu & Hazan (2016) applied this technique to nonconvex optimization. Hazan & Luo (2016) devised a projection-free stochastic convex optimization algorithm based on this technique. Mokhtari et al. (2018a;b) proposed the first sample-efficient variance reduction technique for projection-free algorithms that does not require increasing batch sizes. Their method achieves the tight (1 − 1/e) approximation guarantee for monotone and continuous DRsubmodular functions. Although these variance reduction techniques have enjoyed success in the offline setting, they have yet to be as extensively applied in the online setting that we consider in this paper.\nIn the discrete domain, Streeter & Golovin (2009) studied the online maximization problem of monotone submodular set functions subject to a knapsack constraint and introduced the meta-action technique. In a celebrated work, Calinescu et al. (2011) proposed an (offline) method for maximizing monotone submodular set functions subject to a matroid constraint by working in the continuous domain via the multilinear extension, then rounding the fractional solution. By combining the meta-action and lifting techniques, Golovin et al. (2014) presented an algorithm whose (1− 1/e)-regret is bounded by O( √ T ). The lifting method therein relies\non an expensive sampling procedure that does not scale favorably to large applications.\nBach (2015) demonstrated connections between continuous submodular functions and convex functions in the context of minimization. Building upon the continuous greedy algorithm of (Calinescu et al., 2011), Bian et al. (2017) proposed an algorithm that achieves a (1− 1/e)-approximation guarantee for maximizing monotone continuous DR-submodular functions subject to down-closed convex constraints. Projected gradient methods were investigated in (Hassani et al., 2017) and were shown to attain a 1/2-approximation ratio for monotone continuous DR-submodular functions. Very recently, Chen et al. (2018) borrowed the idea of metaaction (Streeter & Golovin, 2009) and proposed several online algorithms for maximizing monotone continuous DR-submodular functions. However, each of these methods either requires an expensive projection step at each iteration or cannot handle stochastic gradient estimates."
  }, {
    "heading": "3. Preliminaries",
    "text": "In this work, we are interested in optimizing two classes of functions, namely convex and continuous DR-submodular. To begin defining continuous submodular functions, we first recall the definition of a submodular set function. A real-valued set function f : 2Ω → R+ is submodular if\nf(A) + f(B) ≥ f(A ∪B) + f(A ∩B)\nfor all A,B ⊂ Ω. The notion of submodularity has been extended to continuous domains (Wolsey, 1982; Vondrák, 2007; Bach, 2015). Consider a function f : X → R+ where the domain is of the form X = ∏n i=1 Xi and each Xi is a compact subset of R+. We say that f is continuous submodular if f is continuous and for all x,y ∈ X , we have f(x) + f(y) ≥ f(x ∨ y) + f(x ∧ y) where x ∨ y and x ∧ y are component-wise maximum and minimum, respectively. Note that we have defined both discrete and continuous functions to be nonnegative on their respective domains. For efficient maximization, we also require that these functions satisfy a diminishing returns condition (Bian et al., 2017). We say that f is continuous DR-submodular if f is differentiable and\n∇f(x) ≥ ∇f(y)\nfor all x ≤ y. The main attraction of continuous DRsubmodular functions is that they are concave in positive directions; that is, for all x ≤ y,\nf(y) ≤ f(x) + 〈∇f(x),y − x〉\n(Calinescu et al., 2011; Bian et al., 2017). A function f is monotone if f(x) ≤ f(y) for all x ≤ y. A function f is L-smooth if ‖∇f(x)−∇f(y)‖≤ L‖x− y‖ for all x,y.\nWe now provide a brief introduction to online optimization, referring the interested reader to the excellent survey of (Hazan et al., 2016). In the online setting, a player seeks to iteratively optimize a sequence of functions f1, . . . fT over T rounds. In each round, a player must first choose a point xt from the constraint set K. After playing xt, the value of ft(xt) is revealed to the player, along with access to the gradient∇f . Although the player does not know the function ft while choosing xt, they may use information of previously seen functions to guide their choice. The situation where an arbitrary sequence of functions f1, . . . , fT is presented is known as the adversarial online setting. In the adversarial setting, the goal of the player is to minimize adversarial regret, which is defined as\nRT , T∑\nt =1\nft(xt)− inf x ∈K T∑ t =1 ft(x)\nfor minimization problems and analogously defined for maximization problems. Intuitively, a player’s regret is low if the accumulated value of their actions over the T rounds is close to that of the single best action in hindsight. Indeed, this is a natural framework for data-intensive applications where the entire data may not fit onto a single disk and thus needs to be processed in T batches. The algorithm designer would like to devise a scheme to process the T batches separately in a way that is competitive with the best single disk solution.\nA slightly different formulation known as stochastic online setting is when the functions are chosen i.i.d. from some unknown distribution ft ∼ D. In this case, the player seeks to minimize stochastic regret, which is defined as\nSRT , T∑\nt =1\nf(xt)− T · inf x ∈K f(x)\nwhere f(x) = Eft∼D[ft(x)] denotes the expected function. This is a natural framework for many statistical and machine learning applications, such as empirical risk minimization, where the true objective is unknown but pairs of data points and labels are sampled. While the stochastic setting appears “easier” than the adversarial setting (in the sense that any strategy for the adversarial settings applies to stochastic settings and obtains a potentially lower regret), the strategies designed for the stochastic setting may be much simpler and more computationally efficient. For both adversarial and stochastic settings, a strategy that achieves a regret that is sublinear in T is considered good andO( √ T ) regret bounds are optimal for convex functions in both settings. Although convex programs can be efficiently solved to high accuracy, general non-convex programs cannot be efficiently exactly optimized, thus necessitating another definition of regret.\nThe α-regret is defined as\nα-RT , α sup x ∈K T∑ t =1 ft(x)− T∑ t =1 ft(xt)\nfor adversarial maximization problems, and may be analogously extended to other scenarios. Intuitively, α-regret compares a player’s actions with the best α-approximation to the optimal solution in hindsight. This is appropriate when the objective functions do not admit efficient optimization routines, but do admit constant-factor approximations, as is the case with continuous DR-submodular functions.\nNearly all optimization methods for both offline and online settings use first order information of the objective function; however, exact gradient computations can be costly, especially when the objective function is only readily expressed as a large sum of individual functions or is itself an expectation over an unknown distribution. In this case, stochastic estimates are usually much more computationally efficient to obtain via sampling or simulation. In this work, we assume that once a function ft is revealed, the player gains oracle access to unbiased stochastic estimates of the gradient, rather than the exact gradient. More precisely, the player may query the oracle to obtain a random linear function ∇̃f(x) such that E[∇f(x)−∇̃f(x)] = 0 for all x. This computational model captures commonly used mini-batch methods for estimating gradients, among other examples. In this work, we make a few main assumptions that allow our algorithms to be analyzed.\nAssumption 1. The constraint setK is convex and compact, with diameter D = supx,y∈K‖x − y‖ and radius R = supx∈K‖x‖.\nAssumption 2. In the adversarial setting, each function ft is L-smooth and in the stochastic setting, the expected function f is L-smooth.\nAssumption 3. In the adversarial setting, the gradient oracle is unbiased E[∇ft(x)−∇̃ft(x)] = 0 and has a bounded variance E[‖∇ft(x)−∇̃ft(x)‖2] ≤ σ2 for all points x and functions ft. In the stochastic setting, the gradient oracle is unbiased E[∇f(x) − ∇̃ft(x)] = 0 and has a bounded variance E[‖∇f(x)− ∇̃ft(x)‖2] ≤ σ2 for all points x and functions ft.\nWe remark that in the stochastic setting and under mild regularity conditions, unbiasedness of the gradients E[∇ft(x)− ∇̃ft(x)] = 0 implies unbiasedness E[∇f(x)−∇̃ft(x)] = 0 in Assumption 3 because f(x) = Eft∼D[ft(x)] Moreover, upper bounds on the variance terms E[‖∇f(x) − ∇ft(x)‖2] ≤ σ2a and E[‖∇ft(x)− ∇̃ft(x)‖2] ≤ σ2b yield a variance bound of E[‖∇f(x) − ∇̃ft(x)‖2] ≤ σ2a + σ2b , by the triangle inequality."
  }, {
    "heading": "4. Main Results",
    "text": "We now present two algorithms for online optimization of convex and continuous DR-submodular functions in the adversarial and stochastic settings. Unlike previous work, these methods are projection-free and require only stochastic estimates of the gradients, rather than exact gradient computations. In both algorithms, the main computational primitive is linear optimization over a compact convex set. In addition, we remark that both algorithms can be converted into an anytime algorithm that does not require the knowledge of the horizon T via the doubling trick; see Section 2.3.1 of (Shalev-Shwartz et al., 2012)."
  }, {
    "heading": "4.1. Adversarial Online Setting",
    "text": "Algorithm 1 combines the recent variance reduction technique of (Mokhtari et al., 2018a) along with the use of online linear optimization oracles to minimize the regret in each round. An online linear optimization oracle is an instance of an online linear optimization (minimization/maximization in the convex/DR-submodular setting, respectively) algorithm that optimizes linear objectives in a sequential manner. Both the variance reduction in the stochastic gradient estimates and the online linear oracles are crucial in the algorithm, as just one technique is not enough to get sublinear regret bounds in the adversarial setting. At a high level, our algorithm produces iterates xt by running K steps of a Frank-Wolfe procedure, using an average of previous gradient estimates and linear online optimization oracles in place of exact optimization of the true gradient. After a point xt is played in round t, our algorithm queries the gradient oracle ∇̃ft at K points. Then, the gradient estimates are averaged with those from previous rounds and fed as objective functions into K linear online optimization oracles. The K points chosen by the oracles are used as iterates in a full K-step Frank-Wolfe subroutine to obtain the next point xt+1. A formal description is provided in Algorithm 1.\nThere are only a few differences in Algorithm 1 for convex and submodular optimization. First, the online oracles should be minimizing in the case of convex optimization and maximizing in the case of submodular optimization. Second, the initial point x1 may be any point in K for convex problems but should be set to 0 for submodular problems (even if K is not down-closed). Finally, the update rule is\nx (k+1) t ← (1− ηk)x (k) t + ηkv (k) t\nfor convex problems and\nx (k+1) t ← x (k) t + ηkv (k) t\nfor submodular problems. We now provide a formal regret bound.\nAlgorithm 1 Meta-Frank-Wolfe Input: convex set K, time horizon T , linear optimization\noracles E(1) . . . E(K), step sizes ρk ∈ (0, 1) and ηk ∈ (0, 1), and initial point x1 Output: {xt : 1 ≤ t ≤ T} 1: Initialize online linear optimization oracles E(1) . . . E(K)\n2: Initialize d(0)t = 0 and x (1) t = x1 3: for t← 1, 2, 3, . . . , T do 4: v(k)t ← output of oracle E(k) in round t− 1 5: x(k+1)t ← update(x (k) t ,v (k) t , ηk) for k = 1 . . .K 6: Play xt = x (K+1) t , then obtain value ft(xt) and unbiased oracle access to∇ft 7: d(k)t ← (1 − ρk)d (k−1) t + ρk∇̃ft(x (k) t ) for k = 1 . . .K 8: Feedback 〈v(k)t ,d (k) t 〉 to E(k) for k = 1 . . .K 9: end for\nTheorem 1 (Proof in Appendices B and C). Suppose Assumptions 1 - 3 hold, the online linear optimization oracles have regret at most RET , and the averaging parameters are chosen as ρk = 2(k+3)2/3 . Then for convex functions f1, . . . , fT and step sizes ηk = 1k+3 , the adversarial regret of Algorithm 1 is at most\n4TDQ1/2\nK1/3 +\n4T\nK\n( M + LD2\n3 log(K + 1)\n) + 4\n3 RET\nin expectation, where M = max1≤t≤T [ft(x1)− ft(x∗)] and Q , max{42/3 max1≤t≤T ‖∇ft(x1)‖2, 4σ2 + 3(LD)2/2}. For monotone continuous DR-submodular functions f1, . . . , fT and step sizes ηk = 1K , the adversarial (1− 1/e)-regret of Algorithm 1 is at most\n3TDQ1/2 2K1/3 + LD2T 2K +RET\nin expectation, where Q , max{max1≤t≤T ‖∇ft(x1)‖242/3, 4σ2 + 6L2R2}.\nFrom Theorem 1, we observe that by setting K = T 3/2 and choosing a projection-free online linear optimization oracle with RET = O( √ T ), such as Follow the Perturbed Leader (Cohen & Hazan, 2015), both regrets are bounded above by O( √ T ). We remark that the expectation in Theorem 1 is with respect to the stochastic gradient estimates."
  }, {
    "heading": "4.2. Stochastic Online Setting",
    "text": "In the stochastic online setting, where functions are sampled i.i.d. ft ∼ D, we can develop much simpler algorithms that still achieve sublinear regret. Algorithm 2 works without\ninstantiating any online linear optimization oracles and requires only a single stochastic estimate of the gradient at each round. Indeed, because the functions are not arbitrarily chosen, variance reduction along with one Frank-Wolfe step suffices to achieve a sublinear regret bound.\nAlgorithm 2 One-Shot Frank-Wolfe Input: convex set K, time horizon T , step sizes ρt ∈ (0, 1)\nand ηt ∈ (0, 1), and initial point x1 Output: {xt : 1 ≤ t ≤ T}\n1: d0 ← 0 2: for t← 1, 2, 3, . . . , T do 3: Play xt, then obtain value ft(xt) and unbiased oracle access to ∇ft 4: dt ← (1− ρt)dt−1 + ρt∇̃ft(xt) 5: vt ← arg maxv∈K〈dt,v〉 6: xt+1 ← update(xt,vt, ηt) 7: end for\nThe differences in Algorithm 2 for convex and submodular optimization are similar to those in Algorithm 1. Namely, the update rules are the same and the initial point x1 may be arbitrarily chosen from K for convex optimization, and set to 0 for submodular optimization.\nTheorem 2 (Proof in Appendices D and E). Suppose Assumptions 1 - 3 hold and the averaging parameters are chosen as ρt = 2(t+3)2/3 . Then for a convex expected function f and step sizes ηt = 1t+3 , the stochastic regret of Algorithm 1 is at most\n4M log(T + 1) + 6Q1/2DT 2/3 + 4\n3 LD2 log2(T + 3)\nin expectation, where M = f(x1) − f(x∗) and Q , max{42/3‖∇F (x1)‖2, 4σ2 + 3(LD)2/2}. For expected functions f which are monotone continuous DR-submodular and step sizes ηk = 1K , the stochastic (1 − 1/e)-regret of Algorithm 2 is at most\n(1− 1/e)M + 3DQ 1/2\n10 (3T 2/3 + 2T−1) +\nLD2\n2 .\nin expectation, where M = f(x∗) − f(0) and Q , max{‖∇f(0)‖242/3, 4σ2 + 6L2R2}"
  }, {
    "heading": "4.3. Lifting Methods for Discrete Online Optimization",
    "text": "One exciting application of our online continuous DRsubmodular optimization algorithms is a new approach for online discrete submodular optimization. While previous methods could only handle knapsack constraints (Streeter & Golovin, 2009) or required expensive sampling procedures (Golovin et al., 2014), our continuous methods can\nbe applied to the discrete setting to handle general matroid constraints and computationally cheap sampling procedures.\nSuppose f1, . . . fT are nonnegative monotone submodular set functions on a ground set Ω with matroid constraint I and f̄1, . . . f̄T are corresponding multi-linear extensions with matroid polytope K ⊂ [0, 1]n. A discrete procedure that uses our continuous algorithm is as follows: at each round t, the online continuous algorithm produces a fractional solution xt ∈ K, which is then rounded to a set Xt ∈ I and played as the discrete solution. The value ft(Xt) is revealed and the player is granted access to the discrete function ft. Then, the player supplies the continuous algorithm with a stochastic gradient estimate ∇̃f̂t obtained by a single function evaluation, as\n∂ft(x)\n∂xi = E[f(R ∪ {i})− f(R)], ∀i ∈ [n], (1)\nwhere R is random subset of [n] \\ {i} such that for every j ∈ [n] \\ {i}, the event j ∈ R happens with an independent probability of xj . Because a lossless rounding scheme is used, the discrete player enjoys a regret that is no worse than that of the continuous solution. Provably lossless rounding schemes include the pipage rounding (Ageev & Sviridenko, 2004; Calinescu et al., 2011) and contention resolution (Vondrák et al., 2011).\nMost discrete submodular maximization algorithms that go through the multi-linear extension require a gradient estimate with high accuracy. In order to do this, they appeal to a concentration bound, which requires O(n2) evaluations of the discrete function for independently chosen samples. In stark contrast, our algorithms can handle stochastic gradient estimates and thus require only a single function evaluation, finally making continuous methods a reality for large-scale online discrete optimization problems. The framework of the one-sampling lifting method is illustrated in Fig. 1.\nAs an example, we present in Algorithm 3 how to use Meta-Frank-Wolfe as an online maximization algorithm of submodular set functions. According to Theorem 1, the (1− 1/e)-regret of Algorithm 3 is bounded by 3TDQ 1/2\nK1/3 +\nLD2T 2K +R E T , whereRET is the regret of E(k) up to horizon T . If one sets E(k) to an online linear maximization algo-\nrithm with regret bound O( √ T ) and sets K = T 3/2, the (1− 1/e)-regret is at most O( √ T ).\nAlgorithm 3 Meta-Frank-Wolfe for online discrete submodular maximization Input: matroid constraint I, time horizon T , linear opti-\nmization oracles E(1) . . . E(K), step sizes ρk ∈ (0, 1) and ηk ∈ (0, 1), and initial point x1 Output: {Xt : 1 ≤ t ≤ T} 1: Initialize online linear optimization oracles E(1)...E(K),\nsetting the constraint set to the matroid polytope of I 2: Initialize d(0)t = 0 and x (1) t = x1 3: for t← 1, 2, 3, . . . , T do 4: v(k)t ← output of oracle E(k) in round t− 1 5: x(k+1)t ← update(x (k) t ,v (k) t , ηk) for k = 1 . . .K 6: xt ← x(K+1)t 7: play Xt ← round(xt), obtain value ft(Xt) and observe the function ft 8: Sample ∇̃f̄t(x(k)t ) for k = 0, . . . ,K − 1 9: d(k)t ← (1 − ρk)d (k−1) t + ρk∇̃ft(x (k) t ) for k =\n1 . . .K 10: Feedback 〈v(k)t ,d (k) t 〉 to E(k) for k = 1 . . .K 11: end for"
  }, {
    "heading": "5. Experiment",
    "text": "In this section, we test our online algorithms for monotone continuous DR-submodular and convex optimization on both real-world and synthetic data sets. We find that our algorithms outperform most baselines, including projected gradient descent, when supplied with stochastic gradient estimates. All code was written in the Julia programming language and tested on a Macintosh desktop with an Intel Processor i7 with 16 GB of RAM. No parts of the code were optimized past basic Julia usage. A list of all algorithms to be compared in this section is presented below.\n• Meta-Frank-Wolfe is Algorithm 1. We compare the variance-reduced meta-Frank-Wolfe algorithm and the analogue without variance reduction, denoted MetaFW w/ VR and Meta-FW w/o VR, respectively.\n• One-shot Frank-Wolfe is Algorithm 2. We compare the One-shot online Frank-Wolfe algorithm with and without variance reduction, denoted OS-FW w/ VR OS-FW w/o NVR, respectively.\n• Regularized online Frank-Wolfe is referred to as the online conditional gradient algorithm in (Hazan et al., 2016). It has a regularizer term when computing the gradient. Thus we term it the regularized online FrankWolfe algorithm and denote it as Regularized-OFW.\n• Online projected gradient ascent (OGA) follows the direction of the projected gradient. Its 1/2-regret is at most O( √ T ) for online monotone continuous DR-\nsubmodular maximization if the step size is set to Θ(1/ √ t) on the t-th iteration (Chen et al., 2018). Note that OGA is not a projection-free algorithm. In the setting of convex minimization, we use online projected gradient descent instead (denoted by OGD).\n• When we perform experiments on discrete submodular maximization problems using our lifting method, we also compare the above algorithms with the Online Greedy algorithm (Streeter & Golovin, 2009)."
  }, {
    "heading": "5.1. Online DR-Submodular Maximization",
    "text": "In order to test the performance of algorithms for online maximization of monotone continuous DR-submodular functions with stochastic gradient estimates, we conducted three sets of experiments on real-world datasets. We approximate the (1− 1/e)-regret by running an offline Frank Wolfe maximization to produce a solution that is a (1−1/e) approximation to the optimum.\nJoke Recommendations (Continuous) The first set of experiments is to optimize a sequence of continuous facility location objectives on the Jester dataset (Goldberg et al., 2001). It contains ratings of 100 jokes from 73,421 users and the rating range is [−10, 10]. We re-scale the rating range into [0, 20] so that all ratings are nonnegative. Let Ruj be user u’s rating of joke j. All users are splitted into disjoint batches B1, B2, . . . , BT , each containing B users. The facility location objective is defined as ft(X) = ∑ u∈Bt maxj∈X Ruj , ∀X ⊆ [J ], where J = 100 is the total number of jokes and [J ] = {1, 2, 3, . . . , J}. Its multilinear extension is given by f̄t(x) = ∑ u∈Bt ∑J l=1Rujluxjlu ∏l−1 m=1(1 − xjmu ), ∀x ∈ [0, 1]J , where j1u, j 2 u, . . . , j J u is a permutation of 1, 2, . . . , J such that Ruj1u ≥ Ruj2u ≥ . . . ≥ RujJu (Iyer et al., 2014). In this experiment, the sequence of objective functions to be optimized is {f̄1, f̄2, . . . , f̄T }. The stochastic gradient is obtained by the sampling method given in Eq. (1) with only one sample for each coordinate of the gradient. We set the constraint set to {x ∈ [0, 1]J : 1>x ≤ 1} and choose B = 5. We present the results in Fig. 2(a). Meta-FW w/ VR attains the smallest regret. The counterpart without variance reduction Meta-FW w/o VR is inferior to Meta-FW w/ VR in terms of the regret. OS-FW w/ VR outperforms OSFW w/o NVR, which suggests that the variance reduction technique improves the performance of the algorithms.\nJoke Recommendations (Discrete) In the second set experiments, we consider online maximization of discrete submodular functions. The problem set up is the same\nas before, but instead of evaluating regret of the multilinear extensions, we round solutions using pipage rounding and evaluate the regret on the discrete submodular functions. We set the batch size B to 40 and we recommend 10 jokes for users. The results are illustrated in Fig. 2(b). We observe that Meta-FW w/ VR outperforms all other algorithms again. The projected algorithm OGA is second to Meta-FW w/ VR. Online Greedy appears only better than Regularized-OFW. The experiment result show that the continuous algorithms designed under the framework of the lifting method perform better than the discrete algorithms.\nTopic Summarization We consider the problem of selecting news documents in order to maximize the probabilistic coverage of news topics (El-Arini et al., 2009; Yue & Guestrin, 2011). We applied the latent Dirichlet allocation to the corpus of Reuters-21578, Distribution 1.0, set the number of topics to 10, and extracted the topic distribution of each news document. We sample T batches of news documents from the corpus and denote them by B1, B2, . . . , BT , where each batch contains 50 randomly sampled documents. For each batch Bi, we define the probabilistic coverage function as follows fi(X) = 1 10 ∑10 j=1[1 − ∏ a∈X(1 − pa(j))], ∀X ⊆ Bi, where pa(·) is the topic distribution of news document a. Its multilinear extension is f̄i(x) = 110 ∑10 j=1[1− ∏ a∈X(1− pa(j)xa)], ∀x ∈ [0, 1]50, see (Iyer et al., 2014). The sequence of objective functions that the algorithms are expected to maximize is f̄1, f̄2, . . . , f̄T . As in the experiments on joke recommendations, the stochastic gradient is obtained by the sampling method given in Eq. (1) with only one sample for each coordinate of the gradient. The constraint set is {x ∈ [0, 1]50 : 1>x ≤ 45}. We show the (1−1/e)-regret of the algorithms in Fig. 2(c). Again, MetaFW w/ VR exhibits the lowest regret than any other algorithm. Its non-variance-reduced counterpart Meta-FW w/o VR is second to it. OS-FW w/ VR outperforms OS-FW w/o NVR, which confirms the improvement brought by the variance reduction technique."
  }, {
    "heading": "5.2. Online Convex Minimization",
    "text": "The next two sets of experiments test the performance of the algorithms for online minimization of convex functions with stochastic gradient estimates. For these experiments, the regret is computed by obtaining the offline solutions with a Frank-Wolfe solver.\nStochastic Cost Network Flow The fourth set of experiments is a minimum stochastic cost flow in a directed network. A directed graph G = (V,E) with source s ∈ V , sink v ∈ V , and edge capacities c : E → R+ is known to the player. A flow is a function x : R|E|+ → R+ that satisfies the capacities on each edge 0 ≤ x(e) ≤ c(e) and obeys the\n0\n1000\n2000\n3000\n4000\n0 25 50 75 100 Iteration index\n1 −\n1 e −r\neg re\nt\nMeta−FW w/ VR Meta−FW w/o VR OGA OS−FW w/ VR OS−FW w/o VR Regularized−OFW\nMeta-FW w/ VR\nMeta-FW w/o VR\nOGA\nRegularized-OFW\nOS-FW w/ VR OS-FW w/o VR\n(a) Continuous facility location on Jester dataset\n0\n500\n1000\n1500\n2000\n0 100 200 300 400 Iteration index\n1 −\n1 e −r\neg re\nt\nMeta−FW w/ VR Meta−FW w/o VR OGA Regularized−OFW Online greedy\nMeta-FW w/ VR\nMeta-FW w/o VR OGA\nOnline greedy\nRegularized-OFW\n(b) Discrete facility location on Jester dataset\n0\n100\n200\n300\n400\n0 250 500 750 1000 Iteration index\n1 −\n1 e −r\neg re\nt\nMeta−FW w/ VR Meta−FW w/o VR OGA OS−FW w/ VR OS−FW w/o VR Regularized−OFW\nMeta-FW w/ VR Meta-FW w/o VR\nOGA\nRegularized-OFW\nOS-FW w/ VR\nOS-FW w/o VR\n(c) News recommendation in Reuters corpus\nconservation laws for all vertices z,\n∑ {z,r}∈E x(r) =  a z = s −a z = v 0 otherwise\nfor some fixed a ≥ 0. In each round t, a convex cost function on the flow ft : R|E| → R+ is drawn from a distribution, unknown to the player. The goal is to minimize the stochastic regret of the flows chosen. Linear optimizations for this problem may be implemented as combinatorial network flow algorithms. We used the directed Zachary Karate network with 34 nodes and 78 arcs (Zachary, 1977). We set all edge capacities to 1 and cost functions are of the form f(x) = ∑ e∈E wex(e)\n2 where we ∼ Unif[100, 120]. The results are presented in Fig. 2(d). Meta-FW w/ VR attains the lowest regret among all baselines. Again, the regret of Meta-FW w/o VR is larger than the variance-reduced MetaFW w/ VR. Similarly, OS-FW w/ VR also outperforms OS-FW w/o NVR.\nMatrix Completion In the online convex matrix completion problem, one would like to construct a low rank matrix X ∈ Rm×n that well-approximates a given matrix\nM ∈ Rm×n on observed entries OB ⊆ [m]× [n]. The convex relaxation is minTrace(X)≤k ∑ (i,j)∈OB(Xi,j−Mi,j)2. In the online setting, observed entries of the matrix arrive in T batches, OB1, OB2, . . . OBT , each of size B. In each round, we construct a low-rank matrix to minimize the total regret over the T rounds. Although projection involves a full singular value decomposition, linear optimization here is simply a calculation of the largest singular vectors of (X − M)OB , see Chapter 7 of (Hazan et al., 2016). In our experiment, M is a rank 10 matrix with m = n = 50, and B = 100. We illustrate the results in Fig. 2(e) and the computational time is shown in Fig. 2(f). Meta-FW w/ VR is only second to OGD. However, OGD is the slowest algorithm due to the computationally expensive projection operations and its computational time is five times that of Meta-FW w/ VR. The non-variance-reduced Meta-FW w/o VR is inferior to Meta-FW w/ VR in terms of regret."
  }, {
    "heading": "Acknowledgments",
    "text": "AK was supported by AFOSR YIP (FA9550-18-1-0160). CH was supported in part by NSF GRFP (DGE1122492) and by ONR Award N00014-16-1-2374."
  }, {
    "heading": "B. Proof of Theorem 1: Convex Case",
    "text": "We begin by examining the sequence of iterates x(1)t ,x (2) t , . . . ,x (K+1) t produced in Algorithm 1 for a fixed t. By definition of the update and because ft is L-smooth, we have\nft(x (k+1) t )− ft(x∗) = ft(x (k) t + ηk(v (k) t − x (k) t ))− ft(x∗)\n≤ ft(x(k)t )− ft(x∗) + ηk〈∇ft(x (k) t ),v (k) t − x (k) t 〉+ η2k\nL 2 ‖v(k)t − x (k) t ‖2\n≤ ft(x(k)t )− ft(x∗) + ηk〈∇ft(x (k) t ),v (k) t − x (k) t 〉+ η2k\nLD2\n2 .\nNow, observe that the dual pairing may be decomposed as\n〈∇ft(x(k)t ),v (k) t − x (k) t 〉 = 〈∇ft(x (k) t )− d (k) t ,v (k) t − x∗〉+ 〈∇ft(x (k) t ),x ∗ − x(k)t 〉+ 〈d (k) t ,v (k) t − x∗〉.\nWe can bound the first term using Young’s Inequality to get\n〈∇ft(x(k)t )− d (k) t ,v (k) t − x∗〉 ≤\n1\n2βk ‖ft(x(k)t )− d (k) t ‖2 + 2βk‖v (k) t − x∗‖2\n≤ 1 2βk ‖ft(x(k)t )− d (k) t ‖2 + 2βkD2\nfor any βk > 0, which will be chosen later in the proof. We may also bound the second term in the decomposition of the dual pairing using convexity of ft, i.e. 〈∇ft(x(k)t ),x∗ − x (k) t 〉 ≤ ft(x∗)− ft(x (k) t ). Using these upper bounds, we get that\n〈∇ft(x(k)t ),v (k) t − x (k) t 〉 ≤\n1\n2βk ‖ft(x(k)t )− d (k) t ‖2 + 2βkD2 + ft(x∗)− ft(x (k) t ) + 〈d (k) t ,v (k) t − x∗〉.\nUsing this upper bound on the dual pairing in the first inequality, we get that\nft(x (k+1) t )− ft(x∗)≤ (1−ηk)(ft(x (k) t )−ft(x∗))+ηk\n[ 1\n2βk ‖ft(x(k)t )−d (k) t ‖2 +2βkD2 +〈d (k) t ,v (k) t −x∗〉+ηk\nLD2\n2\n] .\nNow we will apply the variance reduction technique. Note that\n‖∇ft(x(k+1)t −∇ft(x (k) t )‖≤ L‖x (k+1) t − x (k) t ‖≤ Lηk‖x (k) t − v (k) t ‖≤\nLD\nk + 3\nWhere we have used that ft is L-smooth, the convex update, and that the step size is ηk = 1k+3 . Now, using Theorem 3 with G = LD and s = 3, we have that\nE[‖ft(x(k)t )− d (k) t ‖2] ≤ Qt (k + 4)2/3 ≤ Q (k + 4)2/3 .\nWhere Qt , max{‖∇ft(x1)‖242/3, 4σ2 + 3(LD)2/2} and Q , max{42/3 max1≤t≤T ‖∇ft(x1)‖2, 4σ2 + 3(LD)2/2} Thus, taking expectation of both sides of the optimality gap and setting βk = Q 1/2\n2D(k+4)1/3 yields\nE[ft(x(k+1)t )]− ft(x∗) ≤ (1− ηk)(E[ft(x (k) t )]− ft(x∗)) + ηk\n[ 2Q1/2D\n(k + 4)1/3 + 〈d(k)t ,v (k) t − x∗〉+ ηk\nLD2\n2\n] .\nNow we have obtained an upper bound on the expected optimality gap E[ft(x(k+1)t )] − ft(x∗) in terms of the expected optimality gap E[ft(x(k)t )]− ft(x∗) in the previous iteration. By induction on k, we get that the final iterate in the sequence, xt , x (K+1) t , satisfies the following expected optimality gap E[ft(xt)]− ft(x∗) ≤ K∏\nk=1\n(1− ηk) [ft(x1)− ft(x∗)] + K∑\nk=1\nηk K∏ j=k+1 (1− ηj) [ 2Q1/2D (k + 4)1/3 + 〈d(k)t ,v (k) t −x∗〉+ ηk LD2 2 ] (2)\nRecall that the Frank Wolfe step sizes are ηk = 1k+3 . We may obtain upper bounds on product of the form ∏K k=r(1− ηk) by\nK∏ k =r (1− ηk) = K∏ k=r ( 1− 1 k + 3 ) ≤ exp ( − K∑ k=r 1 x+ 3 ) ≤ exp ( − ∫ K+1 x=r 1 x+ 3 dx ) = r + 3 K + 4 ≤ r + 3 K\nSubstituting step sizes ηk = 1k+3 into Eq (2) and using this upper bound yields\n(3)E[ft(xt)]− ft(x∗) ≤ 4\nK [ft(x1)− ft(x∗)] + K∑ k=1 ( 1 k + 3 · k + 4 K )[ 2Q1/2D (k + 4)1/3 + 〈d(k)t ,v (k) t − x∗〉+ LD2 2(k + 3) ]\nWhich may be further simplified by using (\n1 k+3 · k+4 K ) ≤ 43K to obtain\nE[ft(xt)]− ft(x∗) ≤ 4\nK [ft(x1)− ft(x∗)] +\n4\n3K K∑ k=1 [ 2Q1/2D (k + 3)1/3 + 〈d(k)t ,v (k) t − x∗〉+ LD2 2(k + 3) ] ,\nAs before, we can obtain the following upper bounds using integral methods:\nK∑ k =1 1 k + 3 ≤ log ( K + 3 3 ) ≤ log(K + 1) and K∑ k =1\n1 (k + 3)1/3 ≤ 3 2\n( (K + 3)2/3 − 32/3 ) ≤ 3\n2 K2/3\nSubstituting these bounds into Eq (3) yields\nE[ft(xt)]− ft(x∗) ≤ 4\nK [ft(x1)− ft(x∗)] +\n4Q1/2D\nK1/3 +\n4LD2 log(K + 1)\n3K +\n4\n3K K∑ k=1 〈d(k)t ,v (k) t − x∗〉.\nNow, we can begin to bound regret by summing over all t = 1 . . . T to obtain\nT∑ t =1 E[ft(xt)]− T∑ t =1 ft(x ∗) ≤ 4 K T∑ t=1 [ft(x1)− ft(x∗)] + 4TQ1/2D K1/3\n+ 4TLD2 log(K + 1)\n3K +\n4\n3K T∑ t=1 K∑ k=1 〈d(k)t ,v (k) t − x∗〉\nRecall that for a fixed k, the sequence {v(k)t }Tt=1 is produced by a online linear minimization oracle with regretRET so that\nT∑ t =1 〈d(k)t ,v (k) t − x∗〉 ≤ T∑ t=1 〈d(k)t ,v (k) t 〉 −min x∈K T∑ t=1 〈d(k)t ,x〉 ≤ RET .\nSubstituting this into the upper bound and using M = max1≤t≤T [ft(x1)− ft(x∗)] yields\nT∑ t =1 E[ft(xt)]− T∑ t =1 ft(x ∗) ≤ 4TDQ 1/2 K1/3 + 4T K ( M + LD2 3 log(K + 1) ) + 4 3 RET\nNow, setting K = T 3/2 and using a linear oracle withRET = O( √ T ) yields\nT∑ t =1 E[ft(xt)]− T∑ t =1 ft(x ∗) ≤ 4 √ TDQ1/2 + 4√ T ( M + LD2 3 (log T 3/2 + 1) ) + 4 3 RET\n= O( √ T )."
  }, {
    "heading": "C. Proof of Theorem 1: DR-Submodular Case",
    "text": "Using the smoothness of ft and recalling x (k+1) t − x (k) t = 1 Kv (k) t , we have\n(4)\nft(x (k+1) t ) ≥ ft(x (k) t ) + 〈∇ft(x (k) t ),x (k+1) t − x (k) t 〉 −\nL 2 ‖x(k+1)t − x (k) t ‖2\n= ft(x (k) t ) + 〈\n1\nK ∇ft(x(k)t ),v (k) t 〉 −\nL\n2K2 ‖v(k)t ‖2\n≥ ft(x(k)t ) + 1\nK 〈∇ft(x(k)t ),v (k) t 〉 −\nLD2 2K2 .\nWe can re-write the term 〈∇ft(x(k)t ),v (k) t 〉 as\n(5) 〈∇ft(x(k)t ),v (k) t 〉 = 〈∇ft(x (k) t )− d (k) t ,v (k) t 〉+ 〈d (k) t ,v (k) t 〉\n= 〈∇ft(x(k)t )− d (k) t ,v (k) t 〉+ 〈d (k) t ,x ∗〉+ 〈d(k)t ,v (k) t − x∗〉 = 〈∇ft(x(k)t )− d (k) t ,v (k) t − x∗〉+ 〈∇ft(x (k) t ),x ∗〉+ 〈d(k)t ,v (k) t − x∗〉.\nWe claim 〈∇ft(x(k)t ),x∗〉 ≥ ft(x∗) − ft(x (k) t ). Indeed, using monotonicity of ft and concavity along non-negative directions, we have\n(6) ft(x\n∗)− ft(x(k)t ) ≤ ft(x∗ ∨ x (k) t )− ft(x (k) t )\n≤ 〈∇ft(x(k)t ),x∗ ∨ x (k) t − x (k) t 〉 = 〈∇ft(x(k)t ), (x∗ − x (k) t ) ∨ 0〉 ≤ 〈∇ft(x(k)t ),x∗〉.\nPlugging Eq. (6) into Eq. (5), we obtain\n(7)〈∇ft(x(k)t ),v (k) t 〉 ≥ 〈∇ft(x (k) t )− d (k) t ,v (k) t − x∗〉+ 〈d (k) t ,v (k) t − x∗〉+ (ft(x∗)− ft(x (k) t )).\nUsing Young’s inequality, we can show that\n(8)〈∇ft(x (k) t )− d (k) t ,v (k) t − x∗〉 ≥ −\n1\n2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 −\nβ(k)\n2 ‖v(k)t − x∗‖2\n≥ − 1 2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 − β(k)D2/2\nThen we plug Eqs. (7) and (8) into Eq. (4), we deduce\nft(x (k+1) t )≥ ft(x (k) t )+\n1\nK\n[ − 1\n2β(k) ‖∇ft(x(k)t )−d (k) t ‖2−β(k)D2/2+〈d (k) t ,v (k) t −x∗〉+(ft(x∗)−ft(x (k) t ))\n] − LD 2\n2K2 .\nEquivalently, we have\n(9)ft(x ∗)− ft(x(k+1)t ) ≤ (1− 1/K)[ft(x∗)− ft(x (k) t )]\n− 1 K\n[ − 1\n2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 − β(k)D2/2 + 〈d (k) t ,v (k) t − x∗〉\n] + LD2\n2K2 .\nApplying Eq. (9) recursively for 1 ≤ k ≤ K immediately yields\nft(x ∗)− ft(x(k+1)t ) ≤ (1− 1/K)K [ft(x∗)− ft(x (1) t )]\n+ 1\nK K∑ k=1 [ 1 2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 + β(k)D2/2 + 〈d (k) t ,x ∗ − v(k)t 〉 ] + LD2 2K .\nRecall that the point played in round t is xt , x (K+1) t , the first iterate in the sequence is x (1) t = 0, and that (1− 1/K)K ≤ 1/e for all K ≥ 1 so that\nft(x ∗)− ft(xt) ≤\n1 e [ft(x ∗)− ft(0)] + 1 K K∑ k=1 [ 1 2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 + β(k)D2/2 + 〈d (k) t ,x ∗ − v(k)t 〉 ] + LD2 2K .\nSince ft(0) ≥ 0, we obtain\n(10)(1− 1/e)ft(x∗)− ft(xt) ≤ 1\nK K∑ k=1 [ 1 2β(k) ‖∇ft(x(k)t )− d (k) t ‖2 + β(k)D2/2 + 〈d (k) t ,x ∗ − v(k)t 〉 ] + LD2 2K .\nIf we sum Eq. (10) over t = 1, 2, 3, . . . , T , we obtain\n(1− 1/e) T∑\nt=1\nft(x ∗)\n− T∑\nt=1\nft(xt) ≤ 1\nK K∑ k=1\n[ 1\n2β(k) T∑ t=1 ‖∇ft(x(k)t )− d (k) t ‖2 + β(k)D2T/2 + T∑ t=1 〈d(k)t ,x∗ − v (k) t 〉\n] + LD2T\n2K .\nBy the definition of the regret, we have T∑\nt =1\n〈d(k)t ,x∗ − v (k) t 〉 ≤ RET .\nTherefore, we deduce\n(1− 1/e) T∑\nt=1\nft(x ∗)− T∑ t=1 ft(xt)\n≤ 1 K K∑ k=1\n[ 1\n2β(k) T∑ t=1 ‖∇ft(x(k)t )− d (k) t ‖2 + β(k)D2T/2\n] + LD2T\n2K +RET .\nTaking the expectation in both sides, we obtain\n(11) (1− 1/e) T∑ t=1 E[ft(x∗)]− T∑ t=1 E[ft(xt)]\n≤ 1 K K∑ k=1\n[ 1\n2β(k) T∑ t=1 E[‖∇ft(x(k)t )− d (k) t ‖2] + β(k)D2T/2\n] + LD2T\n2K +RET .\nNotice that ‖∇ft(x(k)t )−∇ft(x (k−1) t )‖ ≤ L‖v (k) t ‖/T ≤ LR/T ≤ 2LR/(k+ 3). By Theorem 3, if we set ρk = 2(k+3)2/3 , we have\n(12)E[‖∇ft(x (k) t )− d (k) t ‖2] ≤ Qt (k + 4)2/3\n≤ Q (k + 4)2/3 ,\nwhere Qt , max{‖∇ft(0)‖242/3, 4σ2 + 6L2R2} and Q , max{max1≤t≤T ‖∇ft(x1)‖242/3, 4σ2 + 6L2R2}.\nPlugging Eq. (12) into Eq. (11) and setting β(k) = (Q1/2)/(D(k + 3)1/3), we deduce\n(1− 1/e) T∑\nt=1\nE[ft(x∗)]− T∑\nt=1\nE[ft(xt)] ≤ TDQ1/2\nK\nK∑ k=1\n1 (k + 4)1/3 + LD2T 2K +RET\nSince ∑K\nk=1 1 (k+4)1/3 ≤ ∫K 0 dx (x+4)1/3 = 32 [(K + 4) 2/3 − 92/3] ≤ 32K 2/3, we have\n(1− 1/e) T∑\nt=1\nE[ft(x∗)]− T∑\nt=1\nE[ft(xt)] ≤ 3TDQ1/2 2K1/3 + LD2T 2K +RET ."
  }, {
    "heading": "D. Proof of Theorem 2: Convex Case",
    "text": "Let f(x) = Eft∼D[ft(x)] denote the expected function. Because f is L-smooth and convex, we have\nf(xt+1)− f(x∗) = f(xt + ηt(vt − xt))− f(x∗)\n≤ f(xt)− f(x∗) + ηt〈∇f(xt),vt − xt〉+ η2t L\n2 ‖vt − xt‖2\n≤ f(xt)− ft(x∗) + ηt〈∇f(xt),vt − xt〉+ η2t LD2\n2 .\nAs before, the dual pairing may be decomposed as\n〈∇f(xt),vt − xt〉 = 〈∇f(xt)− dt,vt − x∗〉+ 〈∇f(xt),x∗ − xt〉+ 〈dt,vt − x∗〉.\nWe can bound the first term using Young’s Inequality to get\n〈∇f(xt)− dt,vt − x∗〉 ≤ 1\n2β ‖f(xt)− dt‖2 + 2β‖vt − x∗‖2\n≤ 1 2β ‖f(xt)− dt‖2 + 2βD2.\nfor any β > 0, which will be chosen later in the proof. We may also bound the second term in the decomposition of the dual pairing using convexity of f , i.e. 〈∇f(xt),x∗ − xt〉 ≤ ft(x∗)− f(xt). Finally, the third term is nonpositive, by the choice of vt, namely vt = arg minv∈K〈dt,v〉. Using these inequalities, we now have that\nf(xt+1)− f(x∗) ≤ (1− ηt) (f(xt)− f(x∗)) + ηt ( 1\n2β ‖f(xt)− dt‖2 + 2βD2\n) + η2t LD2\n2 .\nTaking expectation over the randomness in the iterates (i.e. the stochastic gradient estimates), we have that\n(13)E[f(xt+1)]− f(x∗) ≤ (1− ηt) (E[f(xt)]− f(x∗)) + ηt ( 1\n2β E[‖f(xt)− dt‖2] + 2βD2\n) + η2t LD2\n2 .\nNow we will apply the variance reduction technique. Note that\n‖∇f(xt+1)−∇f(xt)‖≤ L‖xt+1 − xt‖≤ Lηt‖xt − vt‖≤ LηtD\nwhere we have used that f is L-smooth, the convex update, and the diameter. Now, using Theorem 3 with G = LD and s = 3, we have that\nE[‖f(xt)− dt‖2] ≤ Q\n(t+ 4)2/3 ,\nwhere Q , max{42/3‖∇f(x1)‖2, 4σ2 + 3(LD)2/2}. Using this bound in Eq (13) and setting β = Q 1/2\n2D(t+4)1/3 yields\nE[f(xt+1)]− f(x∗) ≤ (1− ηt) (E[f(xt)]− f(x∗)) + ηt 2Q1/2D\n(t+ 4)1/3 + η2t\nLD2\n2 .\nBy induction, we have\nE[f(xt+1)]− f(x∗) ≤ t∏\nk=1\n(1− ηk)M + t∑\nk=1\nηk t∏ j=k+1 (1− ηj) ( 2Q1/2D (k + 4)1/3 + ηk LD2 2 ) ,\nwhere M = f(x1)− f(x∗). Recall that the step size is set to be ηt = 1t+3 . As in Appendix B, we can obtain the bounds∏t k=1(1 − ηk) = ∏t k=1(1 − 1 k+3 ) ≤ exp(− ∑t k=1 1 k+3 ) ≤ exp(− ∫ t+1 1 dx x+3 ) = 4/(t + 4) and similarly ∏t j=k+1(1 −\n1 j+3 ) ≤ k+4 t+4 . Using these bounds as well as the choice of step size ηt = 1 t+3 in the above yields\nE[f(xt+1)]− f(x∗) ≤ 4M\nt+ 4 + t∑ k=1 ( 1 k + 3 · k + 4 t+ 4 )( 2Q1/2D (k + 4)1/3 + 1 k + 3 LD2 2 )\n= 4M\nt+ 4 +\n4\n3(t+ 4) t∑ k=1 ( 2Q1/2D (k + 4)1/3 + 1 k + 3 LD2 2 )\nwhere the second inequality used (\n1 k+3 · k+4 (t+4) ) < 43(t+4) . As before in Section B, using the inequalities ∑t k=1 1 k+3 ≤\nlog(t+ 1) and ∑t\nk=1 1 (k+3)1/3 ≤ 32 t 2/3 in the above yields\nE[f(xt+1)]− f(x∗) ≤ 4M\nt+ 4 + 4Q1/2D\nt2/3\nt+ 4 +\n4 3 LD2 log(t+ 1) t+ 4 . (14)\nTo obtain a regret bound, we sum over rounds t = 1, . . . T to obtain\nT∑ t=1 E[f(xt)]− Tf(x∗) ≤ 4M\n( T∑\nt=1\n1\nt+ 4\n) + 4Q1/2D ( T∑\nt=1\nt2/3\nt+ 4\n) + 4\n3 LD2\n( T∑\nt=1\nlog(t+ 1)\nt+ 4\n)\nUsing the integral trick again, we obtain the upper bounds ∑T\nt=1 1 t+4 ≤ log(T + 1), ∑T t=1 t2/3 t+4 ≤ 3 2T\n2/3, and∑T t=1 log(t+3) t+4 ≤ log 2(T + 3). Substituting these bounds in the regret bound above yields\nT∑ t=1 E[f(xt)]− Tf(x∗) ≤ 4M log(T + 1) + 6Q1/2DT 2/3 + 4 3 LD2 log2(T + 3) = O ( T 2/3 )"
  }, {
    "heading": "E. Proof of Theorem 2: DR-Submodular Case",
    "text": "Since f is L-smooth, we obtain\nf(xt+1) ≥ f(xt) + 〈∇f(xt), 1\nT vt〉 −\nL 2 ‖ 1 T vt‖2\n≥ f(xt) + 1\nT 〈∇f(xt),vt〉 −\nLD2\n2T 2\n= f(xt) + 1\nT 〈dt,vt〉+\n1 T 〈∇f(xt)− dt,vt〉 −\nLD2\n2T 2\n≥ f(xt) + 1\nT 〈dt,x∗〉+\n1 T 〈∇f(xt)− dt,vt〉 −\nLD2\n2T 2\n= f(xt) + 1\nT 〈∇f(xt)− dt,vt − x∗〉+\n1 T 〈f(xt),x∗〉 −\nLD2 2T 2 .\nIn the last inequality, we used the fact that vt = arg maxv∈K〈dt,v〉. Similar to Eq. (6) in Appendix C, we have 〈f(xt),x∗〉 ≥ f(x∗)− f(xt). Again, Young’s inequality gives 〈∇f(xt)− dt,vt − x∗〉 ≥ −12 (βt‖vt − x\n∗‖2 + ‖f(xt)− dt‖2/βt). Therefore, we deduce\nf(xt+1) ≥ f(xt)− 1\n2T (βt‖vt − x∗‖2 + ‖f(xt)− dt‖2/βt) +\n1 T (f(x∗)− f(xt))−\nLD2\n2T 2\n≥ f(xt)− 1\n2T (βtD\n2 + ‖f(xt)− dt‖2/βt) + 1\nT (f(x∗)− f(xt))−\nLD2 2T 2 .\nRe-arrangement of the terms yields\nf(x∗)− f(xt+1) ≤ (1− 1/T )(f(x∗)− f(xt)) + 1\n2T (βtD\n2 + ‖f(xt)− dt‖2/βt) + LD2\n2T 2 .\nRecalling that (1− 1/T )T ≤ 1/e and f(x1) = f(0) ≥ 0, we have\nf(x∗)− f(xt+1) ≤ (1− 1/T )t(f(x∗)− f(x1)) + 1\n2T t∑ i=1 (βiD 2 + ‖f(xi)− di‖2/βi) + LD2 2T\n≤ 1 e f(x∗) + 1 2T t∑ i=1 (βiD 2 + ‖f(xi)− di‖2/βi) + LD2 2T ,\nwhich in turn yields\n(1− 1/e)f(x∗)− f(xt+1) ≤ 1\n2T t∑ i=1 (βiD 2 + ‖f(xi)− di‖2/βi) + LD2 2T .\nTaking expectation in both sides gives\n(1− 1/e)E[f(x∗)]− E[f(xt+1)] ≤ 1\n2T t∑ i=1 (βiD 2 + E[‖f(xi)− di‖2]/βi) + LD2 2T .\nNotice that ‖∇f(xt)−∇f(xt−1)‖ ≤ L‖vt‖/K ≤ LR/K ≤ 2LR/(k + 3). By Theorem 3, if we set ρi = 2(i+3)2/3 , we have\nE[‖f(xi)− di‖2] ≤ Q\n(i+ 4)2/3\nfor every i ≤ T and Q = max{‖∇f(0)‖242/3, 4σ2 + 6L2R2}. If we set βi = Q 1/2\nD(i+4)1/3 , we have\n(1− 1/e)E[f(x∗)]− E[f(xt+1)] ≤ t∑\ni=1\nDQ1/2 (i+ 4)1/3T + LD2 2T ≤ 3DQ 1/2t2/3 2T + LD2 2T\nsince ∑t\ni=1 1 (i+4)1/3 ≤ ∫ t 0 1 (x+4)1/3 dx = 32 [(x+ 4) 2/3]t0 ≤ 32 [x 2/3]t0 = 3 2 t 2/3.\nTherefore we have\n(1− 1/e)TE[f(x∗)]− T∑\nt=1\nE[f(xt)]\n= (1− 1/e)E[f(x∗)]− f(0) + T−1∑ t=1 [(1− 1/e)E[f(x∗)]− E[f(xt)]]\n≤ (1− 1/e)E[f(x∗)]− f(0) + T−1∑ t=1 [ 3DQ1/2t2/3 2T + LD2 2T ] .\nSince ∑T−1\nt=1 t 2/3 = 1 + ∑T−1 t=2 t 2/3 ≤ 1 + ∫ T 1 t2/3dt = 35T 5/3 + 25 , we conclude\n(1− 1/e)TE[f(x∗)]− T∑\nt=1\nE[f(xt)] ≤ (1− 1/e)E[f(x∗)]− f(0) + 3DQ1/2\n10 (3T 2/3 + 2T−1) +\nLD2\n2 = O(T 2/3)."
  }],
  "year": 2018,
  "references": [{
    "title": "Algorithms for portfolio management based on the newton method",
    "authors": ["A. Agarwal", "E. Hazan", "S. Kale", "R.E. Schapire"],
    "venue": "In Proceedings of the 23rd International Conference on Machine Learning,",
    "year": 2006
  }, {
    "title": "Pipage rounding: A new method of constructing algorithms with proven performance guarantee",
    "authors": ["A.A. Ageev", "M.I. Sviridenko"],
    "venue": "Journal of Combinatorial Optimization,",
    "year": 2004
  }, {
    "title": "Variance reduction for faster non-convex optimization",
    "authors": ["Z. Allen-Zhu", "E. Hazan"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Submodular functions: from discrete to continous domains",
    "authors": ["F. Bach"],
    "venue": "arXiv preprint arXiv:1511.00394,",
    "year": 2015
  }, {
    "title": "Guaranteed non-convex optimization: Submodular maximization over continuous domains",
    "authors": ["A. Bian", "B. Mirzasoleiman", "J.M. Buhmann", "A. Krause"],
    "year": 2017
  }, {
    "title": "Second-order kernel online convex optimization with adaptive sketching",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Maximizing a monotone submodular function subject to a matroid constraint",
    "authors": ["G. Calinescu", "C. Chekuri", "M. Pál", "J. Vondrák"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Online continuous submodular maximization",
    "authors": ["L. Chen", "H. Hassani", "A. Karbasi"],
    "venue": "In AISTATS, pp. to appear,",
    "year": 2018
  }, {
    "title": "Following the perturbed leader for online structured learning",
    "authors": ["A. Cohen", "T. Hazan"],
    "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,",
    "year": 2015
  }, {
    "title": "Turning down the noise in the blogosphere",
    "authors": ["K. El-Arini", "G. Veda", "D. Shahaf", "C. Guestrin"],
    "venue": "In SIGKDD,",
    "year": 2009
  }, {
    "title": "An algorithm for quadratic programming",
    "authors": ["M. Frank", "P. Wolfe"],
    "venue": "Naval Research Logistics (NRL),",
    "year": 1956
  }, {
    "title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization",
    "authors": ["D. Garber", "E. Hazan"],
    "venue": "arXiv preprint arXiv:1301.4666,",
    "year": 2013
  }, {
    "title": "Eigentaste: A constant time collaborative filtering",
    "authors": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"],
    "venue": "algorithm. information retrieval,",
    "year": 2001
  }, {
    "title": "Online submodular maximization under a matroid constraint with application to learning assignments",
    "authors": ["D. Golovin", "A. Krause", "M. Streeter"],
    "venue": "Technical report,",
    "year": 2014
  }, {
    "title": "Gradient methods for submodular maximization",
    "authors": ["H. Hassani", "M. Soltanolkotabi", "A. Karbasi"],
    "venue": "arXiv preprint arXiv:1708.03949,",
    "year": 2017
  }, {
    "title": "Projection-free online learning",
    "authors": ["E. Hazan", "S. Kale"],
    "venue": "In ICML, pp",
    "year": 2012
  }, {
    "title": "Variance-reduced and projection-free stochastic optimization",
    "authors": ["E. Hazan", "H. Luo"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Monotone closure of relaxed constraints in submodular optimization: Connections between minimization and maximization",
    "authors": ["R. Iyer", "S. Jegelka", "J. Bilmes"],
    "venue": "In Uncertainty in Artificial Intelligence (UAI),",
    "year": 2014
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["R. Johnson", "T. Zhang"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "On the online Frank-Wolfe algorithms for convex and non-convex optimizations",
    "authors": ["J. Lafond", "Wai", "H.-T", "E. Moulines"],
    "venue": "arXiv preprint arXiv:1510.01171,",
    "year": 2015
  }, {
    "title": "Mixed optimization for smooth functions",
    "authors": ["M. Mahdavi", "L. Zhang", "R. Jin"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Conditional gradient method for stochastic submodular maximization: Closing the gap",
    "authors": ["A. Mokhtari", "H. Hassani", "A. Karbasi"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
    "authors": ["A. Mokhtari", "H. Hassani", "A. Karbasi"],
    "venue": "arXiv preprint arXiv:1804.09554,",
    "year": 2018
  }, {
    "title": "An online algorithm for maximizing submodular functions",
    "authors": ["M. Streeter", "D. Golovin"],
    "venue": "In NIPS, pp",
    "year": 2009
  }, {
    "title": "Submodularity in combinatorial optimization",
    "authors": ["J. Vondrák"],
    "venue": "PhD thesis, Charles University,",
    "year": 2007
  }, {
    "title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes",
    "authors": ["J. Vondrák", "C. Chekuri", "R. Zenklusen"],
    "venue": "In STOC,",
    "year": 2011
  }, {
    "title": "Fast model predictive control using online optimization",
    "authors": ["Y. Wang", "S. Boyd"],
    "venue": "IFAC Proceedings Volumes,",
    "year": 2008
  }, {
    "title": "An analysis of the greedy algorithm for the submodular set covering problem",
    "authors": ["L.A. Wolsey"],
    "year": 1982
  }, {
    "title": "Dual averaging methods for regularized stochastic learning and online optimization",
    "authors": ["L. Xiao"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2010
  }, {
    "title": "Linear submodular bandits and their application to diversified retrieval",
    "authors": ["Y. Yue", "C. Guestrin"],
    "venue": "In NIPS, pp",
    "year": 2011
  }, {
    "title": "An information flow model for conflict and fission in small groups",
    "authors": ["W.W. Zachary"],
    "venue": "Journal of anthropological research,",
    "year": 1977
  }, {
    "title": "Online convex programming and generalized infinitesimal gradient ascent",
    "authors": ["M. Zinkevich"],
    "venue": "In ICML, pp",
    "year": 2003
  }],
  "id": "SP:388c1432b679364851080a1b8c1905f4e84eff2f",
  "authors": [{
    "name": "Lin Chen",
    "affiliations": []
  }, {
    "name": "Christopher Harshaw",
    "affiliations": []
  }, {
    "name": "Hamed Hassani",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of nonconvex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projectionfree algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal O( √ T ) adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an O(T ) stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel “lifting” framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments. Yale Institute for Network Science, Yale University, New Haven, CT, USA Department of Electrical Engineering, Yale University Department of Computer Science, Yale University Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA. Correspondence to: Lin Chen <lin.chen@yale.edu>.",
  "title": "Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity"
}