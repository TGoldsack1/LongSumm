{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Data analysis problems often involve pre-processing raw data, which is a tedious and time-demanding task due to several reasons: i) raw data is often unstructured and largescale; ii) it contains errors and missing values; and iii) documentation may be incomplete or not available. As a consequence, as the availability of data increases, so does the interest of the data science community to automate this process. In particular, there are a growing body of work which focuses on automating the different stages of data pre-processing, including data cleaning (Hellerstein, 2008), data wrangling (Kandel et al., 2011) and data integration and fusion (Dong & Srivastava, 2013).\nThe outcome of data pre-processing is commonly a structured dataset, in which the objects are described by a set of attributes. However, before being able to proceed with the predictive analytics step of the data analysis process, the data scientist often needs to identify which kind of variables (i.e., real-values, categorical, ordinal, etc.) these attributes represent. This labeling of the data is necessary to select the appropriate machine learning approach to ex-\n1University of Cambridge, Cambridge, United Kingdom; 2Uber AI Labs, San Francisco, California, USA. Correspondence to: Isabel Valera <miv24@cam.ac.uk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nplore, find patterns or make predictions on the data. As an example, a prediction task is solved differently depending on the kind of data to be predicted—e.g., while prediction on categorical variables is usually formulated as a classification task, in the case of ordinal variables it is formulated as an ordinal regression problem (Agresti, 2010). Moreover, different data types should be pre-processed and input differently in the predictive tool—e.g., categorical inputs are often transformed into as many binary inputs (which state whether the object belongs to a category or not) as number of categories; positive real inputs might be logtransformed, etc.\nInformation on the statistical data types in a dataset becomes particularly important in the context of statistical machine learning (Breiman, 2001), where the choice of a likelihood model appears as a main assumption. Although extensive work has focused on model selection (Ando, 2010; Burnham & Anderson, 2003), the likelihood model is usually assumed to be known and fixed. As an example, a common approach is to model continuous data as Gaussian variables, and discrete data as categorical variables. However, while extensive work has shown the advantages of capturing the statistical properties of the observed data in the likelihood model (Chu & Ghahramani, 2005a; Schmidt et al., 2009; Hilbe, 2011; Valera & Ghahramani, 2014), there still exists a lack of tools to automatically perform likelihood model selection, or equivalently to discover the most plausible statistical type of the variables in the data, directly from the data.\nIn this work, we aim to fill this gap by proposing a general and scalable Bayesian method to solve this task. The proposed method exploits the latent structure in the data to automatically distinguish among real-valued, positive realvalued and interval data as types of continuous variables, and among categorical, ordinal and count data as types of discrete variables. The proposed method is based on probabilistic modeling and exploits the following key ideas:\ni) There exists a latent structure in the data that capture the statistical dependencies among the different objects and attributes in the dataset. Here, as in standard latent feature modeling, we assume that we can capture this structure by a low-rank representation, such that conditioning on it, the likelihood model factorizes for both number of objects and attributes.\nii) The observation model for each attribute can be ex-\npressed as a mixture of likelihood models, one per each considered data type, where the inferred weight associated to a likelihood model captures the probability of the attribute belonging to the corresponding data type.\nWe derive an efficient MCMC inference algorithm to jointly infer both the low-rank representation and the weight of each likelihood model for each attribute in the observed data. Our experimental results show that the proposed method accurately discovers the true data type of the variables in a dataset, and by doing so, it fits the data substantially better than modeling continuous data as Gaussian variables and discrete data as categorical variables."
  }, {
    "heading": "2. Problem Statement",
    "text": "As stated above, the outcome of the pre-processing step of data analysis is a structured dataset, in which a set of objects are defined by a set of attributes, and our objective is to automatically discover which type of variables these attributes correspond to. In order to distinguish between discrete and continuous variables, we can apply simple logic rules, e.g.. count the number of unique values that the attribute takes and how many times we observe these attributes. Moreover, binary variables are invariant to the labeling of the categories, and therefore, both categorical and ordinal models are equivalent in this case. However, distinguishing among different types of discrete and continuous variables cannot be easily solved using simple heuristics.\nIn the context of continuous variables, given the finite size of observed datasets, it is complicated to identify whether a variable may take values in the entire real line, or only on an interval of it, e.g., (0,∞) or (θL, θH). In other words, due to the finite observation sample, we cannot distinguish whether the data distribution has an infinite tail that we have not observed, or its support is limited to an interval. As an illustrative example, Figures 2(d)&(f) in Section 4 show two data distributions that, although at a first sight look similar, correspond respectively to a Beta variable, which therefore takes values in the interval (0, 1), and a gamma variable, which takes values in (0,∞). In the context of discrete data, it is impossible to tell the difference between categorical and ordinal variables in isolation. The presence of an order in the data only makes sense given a context. As an example, while colors in M&Ms usually do not present an order, colors in a traffic light clearly do. Similarly, we cannot easily distinguish between ordinal data (which take values in a finite ordered set) and count data (which take values in an infinite ordered set with equidistant values) due to two main reasons. First, similarly to continuous variables, since datasets contain a finite number of examples, it is difficult to tell whether we have observed the finite set of possible values of a variable, or simply a finite subsample of an infinite set. Second, we would\nneed access to exact information on whether its consecutive values are equidistant or not, however, this information depends on how the data have been gathered. For example, an attribute that collects information on “frequency of an action” will correspond to an ordinal variable if its categories belong to, e.g., {“never”, “sometimes”, “usually”, “often”}, and to a count variable if it takes values in {‘‘0 times per week”, “1 time per week”, . . .}. Previous work (Hernandez-Lobato et al., 2014) proposed to distinguish between categorical and ordinal data by comparing the model evidence and the predictive test loglikelihood of ordinal and categorical models. However, this approach can be only used to distinguish between ordinal and categorical data, and it does so by assuming that it has access to a real-valued variable that contains information about the presence of an ordering in the observed discrete (ordinal or categorical) variable. As a consequence, it cannot be easily generalizable to label the data type of all the variables (or attributes) in a dataset. In contrast, in this paper we proposed a general method that allows us to distinguish among real-valued, positive real-valued and interval data as types of continuous variables, and among categorical, ordinal and count data as types of discrete variables. Moreover, the general framework we present can be readily extended to other data types as needed."
  }, {
    "heading": "3. Methodology",
    "text": "In this section, we introduce a Bayesian method to determine the statistical type of variable that corresponds to each of the attributes describing the objects in an observation matrix X. In particular, we propose a probabilistic model, in which we assume that there exists a low-rank representation of the data that captures its latent structure, and therefore, the statistical dependencies among its objects and attributes. In detail, we consider that each observation xdn can be explained by a K-length vector of latent variables zn = [zn1, . . . , znK ] associated to the n-th object and a weighting vector bd = [bd1, . . . , b d K ] (with K being the number of latent variables), whose elements bdk weight the contribution of k-th the latent feature to the d-th attribute in X. Then, given the latent low-rank representation of the data, the attributes describing the objects in a dataset are assumed to be independent, i.e.,\np(X|Z, {bd}Dd=1) = D∏ d=1 p(xd|Z, bd),\nwhere we gather the latent feature vectors zn in a N ×K matrix Z. For convenience, here zn is a K-length row vector, while bd is a K-length column vector. The above model resembles standard latent feature models (Salakhutdinov & Mnih, 2007; Griffiths & Ghahramani, 2011), which assume known and fixed likelihood models p(xd|Z, bd). In contrast, in this paper we aim to infer the statistical data type\n(or equivalently, the likelihood model) that better captures the distribution of each attribute in X. To this end, here we assume that the likelihood model of the d-th attribute in X is a mixture of likelihood functions such that\np(xd|Z, {bd`}`∈Ld) = ∑ `∈Ld wd` p`(x d|Z, bd` ),\nwhere Ld is the set of possible types of variables (or equivalently, likelihood models) to be considered for this attribute, and the weight wd` captures the probability of the likelihood function ` in the d-th attribute of the observation matrix X. Note that, the above expression is a valid likelihood model as long as ∑ `∈Ld w d ` = 1 and each p`(x d|Z, bd` ,Ψd` ) is a normalized probability density function or probability mass function for, respectively, continuous and discrete variables. Hence, under the proposed model, which is is illustrated in Figure 1a, the likelihood factorizes as\np(X|Z, {bd`}) = D∏ d=1 ∑ `∈Ld wd` p`(x d|Z, bd` ). (1)\nWe place a Dirichlet prior distribution on the likelihood weights wd = [wd` ]`∈Ld , and similarly to (Salakhutdinov & Mnih, 2007), assume that both the latent feature vectors zn and the weighting vectors bdj are Gaussian distributed with zero mean and covariance matrices σ2zI and σ 2 b I, respectively. Here, I denotes the identity matrix of size equal to the number of latent features K.\nMoreover, we consider the following types of data for, respectively, continuous and discrete variables: • Continuous variables:\n1. Real-valued data, which takes values in the real line, i.e., xdn ∈ <. 2. Positive real-valued data, which takes values in the positive real line, i.e., xdn ∈ <+. 3. Interval data, which takes values in an interval of the real line, i.e., xdn ∈ (θL, θH), where θL, θH ∈ < and θL ≤ θH . • Discrete variables: 1. Categorical data, which takes values in a finite\nunordered set, e.g., xdn ∈ {‘blue’, ‘red’, ‘black’}. 2. Ordinal data, which takes values in a finite or-\ndered set, e.g., xdn ∈ {‘never’, ‘sometimes’, ‘often’, ‘usually’, ‘always’}. 3. Count data, which takes values in the natural numbers, i.e., xdn ∈ {0, . . . ,∞}.\nWe remark that the main goal of this paper is to determine the types of variables that better capture each attribute in the observed matrix X, which in our method translates to inferring the likelihood weights wd. However, solving this inference problem in an efficient way is a challenging task for several reasons. First, we need to jointly infer all the latent variables in the model, i.e., the low-rank representation\nof the data (which includes the latent feature matrix Z and the corresponding weighting vectors {bd`}{`∈Ld|d=1,...,D}) and the likelihood weights {wd}Dd=1. Second, we need to do so given a heterogeneous (and non-conjugate) observation model, which combines D different likelihood models, corresponding each of them to a mixture of likelihood functions and coupled through the latent feature matrix Z. Additionally, these likelihood functions do not only correspond to either a probability density function or a probability mass function depending on whether we are dealing with a continuous or a discrete variable, but also each mixture combines likelihood functions with different supports. For example, while real-valued data lead to a likelihood function with the real line as support, interval data only accounts for a segment of the real line. Similarly, both categorical and ordinal data assume a finite support, while count data requires an infinite-support likelihood function.\nIn order to allow for efficient inference, we exploit the key idea in (Valera & Ghahramani, 2014) to propose an alternative and equivalent model representation (shown in Figure 1b), which efficiently deal with heterogeneous likelihood functions. In this alternative model representation, we include for each observation xdn as many Gaussian variables (or pseudo-observations) ydn` ∼ N (znbd` , σ2y) as the number of likelihood functions in Ld, and assume that there exists a transformation function over the variables ydn` which maps the real line< into the support of the likelihood function `, Ω`, i.e.,\nf` : < 7→ Ω` y → x . (2)\nNote that, if we condition on the pseudo-observations the latent variable model behaves as a conjugate Gaussian model, allowing for efficient inference of the latent feature matrix Z and the weighting vectors {bd`}. Additionally, we include a latent multinomial variable sdn ∼ Multinomial(wd) which indicates the type of variable (or likelihood function) that the observation xdn belongs to. Then, given sdn, we can obtain the observation x d n as\nxdn = fsdn(y d nsdn + udn), (3)\nwhere udn ∼ N (0, σ2u) is a noise variable. We gather the likelihood assignments sdn in a N ×D matrix S."
  }, {
    "heading": "3.1. Likelihood functions",
    "text": "In this section, we provide the set of transformations to map from the Gaussian pseudo-observations ydn` into the types of data defined above, specifying also the six likelihood functions that our method will account for."
  }, {
    "heading": "3.1.1. CONTINUOUS VARIABLES",
    "text": "In the case of continuous variables, we assume that the mapping functions f` are continuous invertible and differentiable functions, such that we can obtain corresponding likelihood function (after integrating out the pseudoobservation ydn`) as\np`(x d n|zn, bd` , sdn = `) = 1√ 2π(σ2y + σ 2 u) ∣∣∣∣ ddxdn f−1` (xdn) ∣∣∣∣\n× exp { − 1\n2(σ2y + σ 2 u)\n(f−1` (x d n)− znbd` )2\n} ,\nwhere f−1` is the inverse function of the transformation f`(·), i.e., f−1` (f`(v)) = v. Next, we provide examples of mapping functions that allow us to account for real-valued, positive real-valued, and interval data.\n1. Real-valued Data. In order to obtain real-valued observations, i.e., xdn ∈ <, we need a transformation over ydn that maps from the real numbers to the real numbers, i.e., f< : < → <. The simplest case is to assume that x = f<(y + u) = y + u, and therefore, each observation is distributed as xdn ∼ N (znbd<, σ2y + σ2u). Nevertheless, other mapping functions can be used, e.g., we will use in our experiments the transformation\nx = f<(y + u) = w(y + u) + µ,\nwhere w and µ are parameters allowing attribute rescaling, and tuneable by the user.\n2. Positive Real-valued Data. As an example of a function that maps from the real numbers to the positive real numbers, i.e., f<+ : < 7→ <+, we consider\nx = f<+(y + u) = log(1 + exp(w(y + u))).\nwhere w allows attribute rescaling.\n3. Interval Data. As an example of a function the maps from the real numbers into the interval (θL, θH), i.e., f<+ : < 7→ (θL, θH), we consider the transformation\nx = fInt(y + u) = θH − θL\n1 + exp(−w(y + u)) + θL,\nwhere w, θL and θH are user hyperparameters.1 1In our experiments, we assume θL = arg minn(xdn)− and θH = arg maxn(x d n)+ , where → 0 is a user hyper-parameter. We set the rescaling parameter w = 2/max(xd) for the three continuous data types."
  }, {
    "heading": "3.1.2. DISCRETE VARIABLES",
    "text": "1. Categorical Data. Now we account for categorical observations, i.e., each observation xdn can take values in the unordered index set {1, . . . , Rd}. Hence, assuming a multinomial probit model, we can write\nx = fcat(y) = arg max r∈{1,...,Rd} y(r),\nwhere in this case there are as many pseudo-observations as number of categories and each pseudo-observation can be sampled as ydncat(r) ∼ N (znbdcat(r), σ2y) where bdcat(r) denotes the K-length weighting vector, which weights the influence the latent features for a categorical observation xdn taking value r. Note that, under this likelihood model, we need one pseudo-observation ydncat(r) and a weighting vector bdcat(r) for each possible value of the observation r ∈ {1, . . . , Rd}. Under the multinomial probit model, we can obtain the probability of xdn taking value r ∈ {1, . . . , Rd} as (Girolami & Rogers, 2005)\npcat(x = r|zn, bdcat, sdn = cat)\n= Ep(u) [ Rd∏ r′=1 r′ 6=r Φ ( u+ zn(b d cat(r)− bdcat(r′)) )] ,\nwhere Φ(·) denotes the cumulative density function of the standard normal distribution and Ep(u)[·] denotes expectation with respect to the distribution p(u) = N (0, σ2y). 2. Ordinal Data. Consider ordinal data, in which each element xdn takes values in the ordered index set {1, . . . , Rd}. Then, assuming an ordered probit model, we can write\nxdn = ford(y d nord) =  1 if ydnord ≤ θd1 2 if θd1 < y d nord ≤ θd2\n... Rd if θdRd−1 < y d nord\nwhere again ydnord is Gaussian distributed with mean znb d ord and variance σ 2 y , and θ d r for r ∈ {1, . . . , Rd − 1} are the thresholds that divide the real line into Rd regions. We assume the thresholds θdr are sequentially generated from the truncated Gaussian distribution θdr ∼ T N (0, σ2θ , θdr−1,∞), where θd0 = −∞ and θdRd = +∞. As opposed to the categorical case, now we have a unique weighting vector bdord and a unique Gaussian variable y d nord for each observation xdn, and the value of x d n is determined by the region in which ydnord falls.\nUnder the ordered probit model (Chu & Ghahramani, 2005b), the probability of each element xdn taking value r ∈ {1, . . . , Rd} can be written as\npord(x d n = r|zn, bdord, sdn = ord)\n= Φ\n( θdr − znbdord\nσy\n) − Φ ( θdr−1 − znbdord\nσy\n) .\n3. Count Data. In count data each observation xdn takes non-negative integer values, i.e., xdn ∈ {0, . . . ,∞}. Then, we assume\nxdn = fcount(y d n) = bg(ydn)c,\nwhere bvc returns the floor of v, that is the largest integer that does not exceed v, and g : < → <+ is a monotonic differentiable function, in our experiments g(y) = log(1 + exp(wy)). We can thus write the likelihood function as\npcount(x d n|zn, bdord, sdn = count) =\nΦ\n( g−1(xdn + 1)− znbdcount\nσy\n) − Φ ( g−1(xdn)− znbdcount\nσy ) where g−1 : <+ → < is the inverse function of the transformation g(·)."
  }, {
    "heading": "3.2. Inference Algorithm",
    "text": "Here, we exploit the model representation in Figure 1b to derive an efficient inference algorithm that allows us to infer all the latent variables in the model, providing as output the likelihood weights wd, which determine the probability of the d-th attribute in X belonging to each of the above data types. Algorithm 1 summarizes the inference.\nSampling low-rank decomposition. In order to sample the latent feature matrix Z and the associated weighting vectors {bd`}, we condition on the pseudo-observations such that we can efficiently sample the feature vectors as zn ∼ N ( µdz ,Σz ) , where\nΣz = (∑d\nd=1 ∑ `∈Ld b d ` (b d ` ) > + σ−2z I )−1 and µz =\nΣz (∑N n ∑ `∈Ld b d `y d n` ) . Note that this step involves a matrix inversion of size K (the number of latent features) per iteration of the algorithm. Similarly, the weighting vectors can be sampled as bd` ∼ N ( µd` ,Σb ) , where Σb =(\nσ−2y Z >Z + σ−2b I )−1 and µd` = Σb (∑N n z > n y d n` ) . Since Σb is shared for all {bd`} with ` ∈ Ld and d = 1 . . . , D, this step also involves one matrix inversions of size K per iteration of the algorithm. Sampling pseudo-observations. Given the low-rank decomposition and the likelihood assignments S, we can sample each pseudo-observation ydn` from its prior distribution if sdn 6= `, and from its posterior distribution if sdn = `. In the case of continuous variables, the posterior distribution of the pseudo-observation can be obtained as\np(ydn`|xdn, zn, bd` , sdn = `) = N ( ydn ∣∣∣∣µ̂y, σ̂2y) , where µ̂y = ( (znb d ` )\nσ2y +\nf−1` (x d n)\nσ2u\n) σ̂2y, and σ̂\n2 y =(\n1 σ2y + 1σ2u\n)−1 .\nIn the case of discrete variables, the posterior distribution of the pseudo-observation can be computed as follows.\nAlgorithm 1 Inference Algorithm. Input: X Initialize: S, {bd`} and {ydn`} 1: for each iteration do 2: Update Z given {bd`} and {ydn`}. 3: for d = 1, . . . , D do 4: for ` ∈ Ld do 5: for n = 1, . . . , N do 6: Sample {ydn`} given xdn, Z, {bd`} and sdn. 7: end for 8: Sample {bd`} given Z and {ydn`} . 9: for n = 1, . . . , N do 10: Sample sdn given xdn, Z and {bd`}. 11: end for 12: end for 13: Sample wd given S. 14: end for 15: end for Output: Likelihood weights wd.\n1. For categorical observations: p(ydncat(r)|xdn = T, zn, bdcat, sdn = cat)\n= { T N (znbdcat(r), σ2y,maxj 6=r(ydncat(j)),∞), r = T T N (znbdcat(r), σ2y,−∞, ydncat(T )), r 6= T\nIn words, if xdn = T = r we sample y d nr from a truncated Normal distribution with mean znb d cat(r), variance σ 2 y and truncated on the left by maxj 6=r(y d ncat(j)). Otherwise, we sample from a truncated Gaussian (with same mean and variance) truncated on the right by ydncat(r) with r = x d n. Note that sampling from the variables ydncat(r) corresponds to solve a multinomial probit regression problem. Hence, to achieve identifiability we assume, without loss of generality, that the regression function fRd(zn) is identically zero, and thus, we fix bd` (Rd) = 0.\n2. For ordinal observations: p(ydnord|xdn = r, zn, bdord, sdn = ord) = T N (ydnord|znbdord, σ2y, θdr−1, θdr ).\nNote that in this case, we also need to sample the values for the thresholds θdr with r = 1, . . . , Rd − 1 as\np(θdr |ydnord) = T N (θdr |0, σ2θ , θmin, θmax), where θmin = max(θdr−1,maxn(y d nord|xdn = r)) and θmax = min(θ d r ,minn(y d nord|xdn = r+ 1)). In words, each θdr is constrained to be between θ d r−1 and θ d r+1, as well as to ensure that the pseudo-observations ydnord associated to the observations xdn = r and x d n = r+ 1 fall respectively at the left and at the right side of θdr . Since in this ordinal regression problem the thresholds {θr}Rdr=1 are unknown, we set θ1 to a fixed value in order to achieve identifiability.\n3. For count observations: p(ydncount|xdn, zn, bdcount, sdn = count) = T N (ydncount|znbdcount, σ2y, g−1(xdn), g−1(xdn + 1)),\nwhere g−1 : <+ → < is the inverse function of g, i.e., g−1(g(y)) = y. Therefore, ydncount is sampled from a Gaussian truncated on the left by g−1(xdn) and on the right by g−1(xdn + 1).\nSampling likelihood assignments. In order to improve the mixing properties of the sampler, when sampling {sdn} we integrate out the pseudo-observations {ydn`}. Then, the posterior probability of each observation being assigned to the likelihood model ` can be obtained as\np(sdn = `|wd,Z, {bd`}) = wd` p`(x d n|zn, bd` )∑\n`′∈Ld w d `′ p`′(x d n|zn, bd`′)\n.\nSampling likelihood weights. We assume the prior distribution on the vector wd to be a Dirichlet distribution with parameters {α`}`∈Ld . Then, by conjugacy, we can sample wd given the likelihood assignments S from a Dirichlet distribution with parameters {α` + ∑ n δ(s d n == `)}`∈Ld .\nScalability. The overall complexity of Algorithm 1 is O(NDLmax + K3) per iteration, where N is the number of objects, D the number of attributes, Lmax the maximum number of considered data types (or likelihood models) and K the size of the low-rank representation. In all of our experiments, we ran the MCMC for 5000 iterations, which lasted 10-100 minutes depending on the dataset."
  }, {
    "heading": "4. Evaluation",
    "text": ""
  }, {
    "heading": "4.1. Experiments on synthetic data",
    "text": "In this section, we show that the proposed method is able to accurately discover the true statistical type of variables in synthetic datasets, where we have perfect knowledge of the distribution from which the data have been generated.\nFirst, we focus on continuous variables by generating univariate datasets with 1, 000 observations sampled from a known probability density function, which corresponds to i) a Gaussian distribution when considering real-valued data; ii) a Gamma distribution for positive real-valued data; and iii) a (scaled) Beta distribution for interval data lying in the interval (0, θL) where θL takes values 0.1, 1 or 100. Figure 2 shows the distribution, by means of a boxplot,2 of the inferred likelihood weights wd for 10 independent simulations of Algorithm 1 with 500 iterations on 10 independent datasets generated with the parameters detailed in the figure. Reassuringly we observe that the proposed method identifies interval data as the most likely type of data for the three considered Beta distributions; moreover, as the tail of the Beta distribution increases, so does the weight given to the positive real-valued variables. This effect can be explained by the finite size of the dataset, since it is hard to determine whether the variable is limited to values smaller than θL, or we simply have not observed them in the finite set of observations. A similar effect occurs when applying our method to data sampled from Gamma (Figure 2(e)-(h)) and Gaussian (Figure 2(i)-(l)) distributions. Here, we observe that in addition to, respectively, positive real-valued and real-valued data types, our model finds that the variable may also be of interval data type. This effect is larger for Gaussian variables, since in this example the Gaussian is a more heavy-tailed distribution than the Gamma.\n2In a boxplot, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the 10th and 90th percentiles.\nNext, we study whether the proposed model is able to disambiguate among different discrete types of variables, particularly, among categorical, ordinal and count data. To this end, we generate three types of datasets of size 1, 000. In the first type we account for categorical data by sampling a multinomial variable with R categories, where the probability of the categories is sampled from a Dirichlet distribution. Then, for each category we sample a multidimensional Gaussian centroid that corresponds to the mean of the multivariate Gaussian observations that complete the dataset. To account for ordinal observations, we first sample the first variable in our dataset from a uniform distribution in the interval (0, R), which we randomly divide into R categories that correspond to the ordinal variable in our dataset. Finally, to account for count data we first generate a Gamma variable sampled from Γ(α, α/4), and then generate the counting variable in the dataset by taking the floor of the Gamma variable. For both categorical and ordinal data, we generate 10 independent datasets for each value of the number of categoriesR ∈ {3, . . . , 10}, and for count data we generate another 10 datasets for each value of α ∈ {2, . . . , 8}. Figure 3 summarizes the likelihood weights obtained for each type of datasets (i.e., for each type of discrete variable) after running on each dataset 10 independent simulations of Algorithm 1 with 500 iterations for different model complexity values, i.e., for different numbers of latent feature variables K = 1, . . . , 10. In this figure we can observe that we can accurately discover the true type of discrete variable robustly and independently of the assumed model complexity K. We also observe on\nthe top row of Figure 3(a)-(b) that i) as the number of categories R in the discrete variable decreases, the harder is to distinguish between ordinal and categorical data, i.e., to find out whether the data takes values in a ordered set or in an unordered set; and ii) as R in ordinal data increases, the ordinal variable is more likely to be identified as count data. Both of these effects are intuitively sensible."
  }, {
    "heading": "4.2. Experiments on real data",
    "text": "In this section, we evaluate the performance of the proposed method on seven real datasets collected from the UCI machine learning repository (Lichman, 2013). Table 1 summarizes theses datasets by providing the number of objects and attributes in the dataset, as well as how many of these attributes are discrete.\nIn order to quantitatively evaluate the performance of the proposed method, we select at random 10% of the observations in each dataset as a held-out set and compare the predictive performance, in terms of average test log-likelihood per observation, of our method with a baseline method. The baseline method corresponds to a latent feature model in which all the continuous variables are modeled as realvalued data and the discrete variables as categorical data. Figure 4 shows the obtained results for our method (solid line) and the baseline (dashed line) for several values of the model complexity (i.e., the number of latent features K) averaged over 10 independent runs of the corresponding inference algorithms. Here, we observe that i) both methods provide robust results with respect to the number of variables K; and ii) our method clearly outperforms the baseline in all the datasets, except for the Student dataset where the baseline performs slightly better. In other words, this figure shows that by taking into account the uncertainty in the statistical types of the variables, we provide a better fitting of the data.\nAdditionally, Table 2 shows the list of (non-binary) attributes in the Adult and the German datasets together with the data types with larger inferred likelihood weights,3 i.e., the discovered statistical data types. Here, the number in parenthesis corresponds to the observed number of categories in discrete data. The very heterogeneous nature of these datasets explains the substantial gain observed in Figure 4. Moreover, Table 2 shows some expected results, e.g.,\n3In cases in which two data types present very similar likelihood weights (< 10% difference), we display both of them.\nNumber of Latent Variables (K)\nFigure 4. [Real Data] Comparison between our model (solid) and the baseline (dashed) in terms of average test log-likelihood per observation evaluated on a held-out set containing 10% of the observations in each dataset.\nmarital status and race are identified as categorical, while the age is of count data type for both datasets. However, other results might seem surprising. For example, the duration (in months), which one would expect it to be count data, is identified as ordinal; or the a priori categorical attributes native country and job are inferred to be ordinal.\nIn order to better understand these results, we show the histograms of several variables in these datasets and the associated inferred likelihood weights. Figure 5 shows the histograms of two continuous variables, length and weight of the Abalone dataset, which take only positive real values, but are assigned to different data types (respectively, to real-valued and positive real-valued data). This can be explained by the fact that, while the distribution of the length presents large tails, the distribution of the weight is clearly truncated at zero. Additionally, Figure 6(a)-(b) shows two discrete variables, the duration (in months) and the age in German data, which based on the documentation are expected to be count data. However, our model assigns the duration to ordinal data. This result can be explained by the irregular distribution that this variable has. In count data the distance between every two consecutive values should be roughly the same (there is the same distance from “1 pen” to “2 pens” as from “2 pens” to “3 pens”, that is 1 pen), resulting therefore in smooth probability mass functions. We found in Figure 6(c)-(d) that while the number of credits and the job variables can be a priori thought as re-\nspectively count and categorical data, they are both inferred to be ordinal data. In the case of the number of credits, this can be explained by the small (finite) number of values that the variable takes, while in the case of the job, this assignment can be explained by the labels of its categories, i.e., {unskilled non-resident, unskilled resident, skilled employee and highly qualified employee}, which clearly represent an ordered set.\nFrom these results, we can conclude that i) our model accurately discovers the true statistical type of the data, which might not be easily extracted from its documentation; and by doing so, ii) it provides a better fit of the data. Moreover, apparent failures are in fact sensible when data histograms are carefully examined."
  }, {
    "heading": "5. Conclusions",
    "text": "In this paper, we presented the first approach to automatically discover the statistical types of the variables in a dataset. Our experiments showed that the proposed approach accurately infers the data type, or equivalently likelihood model, that best fits the data.\nOur work opens many interesting avenues for future work. For example, it would be interesting to extend the proposed method to account for other data types. We would like to include directional data, also called circular data, which arise in a multitude of data-modelling contexts ranging from robotics to the social sciences (Navarro et al., 2016). Moreover, since the proposed method can be seen as a likelihood selection method, it would be interesting to study how to incorporate our framework in any statistical machine learning tool, where the likelihood model, instead of being fixed a priori, would be inferred directly from the data jointly with the rest of the model parameters."
  }, {
    "heading": "Acknowledgement",
    "text": "Isabel Valera acknowledges her Humboldt Research Fellowship for Postdoctoral Researchers, which funded this research during her stay at the Max Planck Institute for Software Systems. Zoubin Ghahramani acknowledges support from the Alan Turing Institute (EPSRC Grant EP/N510129/1) and EPSRC Grant EP/N014162/1, and donations from Google and Microsoft Research.\nThe code implementing the proposed method, as well as the scripts that reproduce the experiments presented in the paper, are publicly available at: https://github.com/ivaleraM/DataTypes"
  }],
  "year": 2017,
  "references": [{
    "title": "Analysis of ordinal categorical data, volume 656",
    "authors": ["A. Agresti"],
    "year": 2010
  }, {
    "title": "Bayesian model selection and statistical modeling",
    "authors": ["T. Ando"],
    "venue": "CRC Press,",
    "year": 2010
  }, {
    "title": "Statistical modeling: The two cultures (with comments and a rejoinder by the author)",
    "authors": ["L. Breiman"],
    "venue": "Statistical science,",
    "year": 2001
  }, {
    "title": "Model selection and multimodel inference: a practical information-theoretic approach",
    "authors": ["Burnham", "K. P", "D.R. Anderson"],
    "venue": "Springer Science & Business Media,",
    "year": 2003
  }, {
    "title": "Gaussian processes for ordinal regression",
    "authors": ["W. Chu", "A. Ghahramani"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Gaussian processes for ordinal regression",
    "authors": ["W. Chu", "Z. Ghahramani"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2005
  }, {
    "title": "Big data integration",
    "authors": ["Dong", "X. Luna", "D. Srivastava"],
    "venue": "In Data Engineering (ICDE),",
    "year": 2013
  }, {
    "title": "Variational Bayesian multinomial probit regression with Gaussian process priors",
    "authors": ["M. Girolami", "S. Rogers"],
    "venue": "Neural Computation,",
    "year": 2005
  }, {
    "title": "The Indian buffet process: an introduction and review",
    "authors": ["T.L. Griffiths", "Z. Ghahramani"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Quantitative data cleaning for large databases",
    "authors": ["J.M. Hellerstein"],
    "year": 2008
  }, {
    "title": "Learning the semantics of discrete random variables: Ordinal or categorical",
    "authors": ["J.M. Hernandez-Lobato", "J.R. Lloyd", "D. Hernandez-Lobato", "Z. Ghahramani"],
    "venue": "In NIPS Workshop on Learning Semantics,",
    "year": 2014
  }, {
    "title": "Research directions in data wrangling: Visualizations and transformations for usable and credible data",
    "authors": ["S. Kandel", "J. Heer", "C. Plaisant", "J. Kennedy", "F. van Ham", "N.H. Riche", "C. Weaver", "B. Lee", "D. Brodbeck", "P. Buono"],
    "venue": "Information Visualization,",
    "year": 2011
  }, {
    "title": "The multivariate generalised von mises: Inference and applications",
    "authors": ["Navarro", "A. KW", "J. Frellsen", "R.E. Turner"],
    "venue": "arXiv preprint arXiv:1602.05003,",
    "year": 2016
  }, {
    "title": "Probabilistic matrix factorization",
    "authors": ["R. Salakhutdinov", "A. Mnih"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2007
  }, {
    "title": "Bayesian non-negative matrix factorization",
    "authors": ["Schmidt", "M. N", "O. Winther", "L.K. Hansen"],
    "venue": "In International Conference on Independent Component Analysis and Signal Separation,",
    "year": 2009
  }, {
    "title": "General table completion using a Bayesian nonparametric model",
    "authors": ["I. Valera", "Z. Ghahramani"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2014
  }],
  "id": "SP:f3d604f816ae94ddb8dbab6c5cfe983f30cb0359",
  "authors": [{
    "name": "Isabel Valera",
    "affiliations": []
  }, {
    "name": "Zoubin Ghahramani",
    "affiliations": []
  }],
  "abstractText": "A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of realworld data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.",
  "title": "Automatic Discovery of the Statistical Types of Variables in a Dataset"
}