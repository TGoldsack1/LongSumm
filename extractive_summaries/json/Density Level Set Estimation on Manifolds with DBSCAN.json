{
  "sections": [{
    "heading": "1. Introduction",
    "text": "DBSCAN (Ester et al., 1996) is one of the most popular clustering algorithms amongst practitioners and has had profound success in a wide range of data analysis applications. However, despite this, its statistical properties have not been fully understood. The goal of this work is to give a theoretical analysis of the procedure and to the best of our knowledge, provide the first analysis of density levelset estimation on manifolds. We also contribute ideas to related areas that may be of independent interest.\nDBSCAN aims at discovering clusters which turn out to be the high-density regions of the dataset. It takes in two hyperparameters: minPts and ε. It defines a point as a core-point if there are at least minPts sample points in its εradius neighborhood. The points within the ε-radius neighborhood of a core-point are said to be directly reachable from that core-point. Then, a point q is reachable from a core-point p if there exists a path from q to p where each point is directly reachable from the next point. It is now clear that this definition of reachable gives a partitioning of\n1Google. Correspondence to: Heinrich Jiang <heinrich.jiang@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe dataset (and remaining points not reachable from any core-point are considered noise). This partitioning is the clustering that is returned by DBSCAN.\nThe problem of analyzing DBSCAN has recently been explored in (Sriperumbudur & Steinwart, 2012). Their analysis is for a modified version of DBSCAN and is not focused on estimating a fixed density level. Their results have many desirable properties, but are not immediately applicable for what this paper tries to address. Using recent developments in topological data analysis along with some tools we develop in this paper, we show that it is now possible to analyze the original procedure.\nThe clusters DBSCAN aims at discovering can be viewed as approximations of the connected components of the level sets {x : f(x) ≥ λ} where f is the density and λ is some density level. We provide the first comprehensive analysis in tuning minPts and ε to estimate the density level set for a particular level. Here, the density level λ is known to the algorithm while the density remains unknown. Density level set estimation has been studied extensively. e.g., (Carmichael et al., 1968; Hartigan, 1975; Polonik, 1995; Cuevas & Fraiman, 1997; Walther, 1997; Tsybakov et al., 1997; Baıllo et al., 2001; Cadre, 2006; Willett & Nowak, 2007; Biau et al., 2008; Rigollet & Vert, 2009; Maier et al., 2009; Singh et al., 2009; Rinaldo & Wasserman, 2010; Steinwart, 2011; Rinaldo et al., 2012; Steinwart et al., 2015; Chen et al., 2016; Jiang, 2017). However approaches that obtain state-of-art consistency results are largely unpractical (i.e. unimplementable). Our work shows that in actuality, DBSCAN, a procedure known for decades and has since been used widely, can achieve the strongest known results. Also, unlike much of the existing work, we show that DBSCAN can also recover the connected components of the level sets separately and bijectively.\nOur work begins with the insight that DBSCAN behaves like an ε-neighborhood graph, which is different from, but related to the k-nearest neighbor graph. The latter has been heavily used for cluster-tree estimation (Chaudhuri & Dasgupta, 2010; Stuetzle & Nugent, 2010; Kpotufe & von Luxburg, 2011; Chaudhuri et al., 2014; Jiang & Kpotufe, 2017) and in this paper we adapt some of these ideas for ε-neighborhood graphs.\nCluster-tree estimation aims at discovering the hierarchical tree structure of the connected-components as the levels vary. Balakrishnan et al. (2013) extends results by Chaudhuri & Dasgupta (2010) to the setting where the data lies on a lower dimensional manifold and provide consistency results depending on the lower dimension and independent of the ambient dimension. Here we are instead interested in how to set minPts and ε in order to estimate a particular level and provide rates on the Hausdorff distance error. This is different from works on cluster tree estimation which focuses on how to recover the tree structure rather than recovering a particular level. In that regard, we also require density estimation bounds in order to get a handle on the true density-levels and the empirical ones.\nDasgupta & Kpotufe (2014) gives us optimal highprobability finite-sample k-NN density estimation bounds which hold uniformly; this is key to obtaining optimal level-set estimation rates under the Hausdorff error. Much of the previous works on density level-set estimation, e.g. (Rigollet & Vert, 2009) provide rates under risk measures such as symmetric set difference. These metrics are considerably weaker than the Hausdorff metric; the latter is a uniform guarantee. There are such bounds for the histogram density estimator. This allowed Singh et al. (2009) to obtain optimal rates under Hausdorff metric, while having a fully adaptive procedure. This was a significant breakthrough for level set estimation, as discussed by Chazal et al. (2015). We believe this to be the strongest consistency results obtained thus far. However, a downside is that the histogram density estimator has little practical value. Here, aided with the desired bounds on the k-NN density estimator, we can actually obtain similar results to Singh et al. (2009) but with the clearly practical DBSCAN.\nWe extend the k-NN density estimation results of Dasgupta & Kpotufe (2014) to the manifold case, as the bulk our analysis is about the more general case that the data lies on a manifold. Density-based procedures perform poorly in high-dimensions since the number of samples required increases exponentially in the dimension– the so called curse of dimensionality. Thus, the consequences of handling the manifold case are of practical significance. Since the estimation rates we obtain depend only on the intrinsic dimension, it explains why DBSCAN can do well in high dimensions if the data has low intrinsic dimension (i.e. the manifold hypothesis). Given the modern capacity of systems to collect data of increasing complexity, it has become ever more important to understand the feasibility of practical algorithms in high dimensions.\nTo analyze DBSCAN, we write minPts and ε in terms of the d, unknown manfold dimension; k, which controls the density estimator; and λ, which determines which level to estimate. We assume knowledge of λ with the goal of es-\ntimating the λ-level set of the density. We give a range of k in terms of n and corresponding consistency guarantees and estimation rates for such choices. We then adaptively tune d and k in order to attain close to optimal performance with no a priori knowledge of the distribution. Adaptivity is highly desirable because it allows for automatic tuning of the hyper-parameters, which is a core tenet of unsupervised learning. To solve for the unknown dimension, we use an estimator from Farahmand et al. (2007), which we show to have considerably better finite-sample behavior than previously thought. More details and discussion of related works is in the main text. We then provide a new method of choosing k such that it will asymptotically approach a value that provides near-optimal level set estimation rates."
  }, {
    "heading": "2. Overview",
    "text": "We start by analyzing the procedure under the manifold assumption. The end of the paper will discuss the fulldimensional setting. The bulk of our contribution lies in analyzing the former situation, while the analysis of the latter uses a subset of those techniques.\n• Section 3 proves that the clusters returned by DBSCAN are close to the connected components of certain ε-neighborhood graphs (Lemma 2). This is significant because these graphs can be shown to estimate density level sets.\n• Section 4 introduces the manifold setting and provides supporting results including k-nearest neighbor density estimation bounds (Lemma 5 and Lemma 6) that are useful later on.\n• Section 5 provides a range of parameter settings under which for each true cluster, there exists a corresponding cluster returned by DBSCAN (Lemma 7 and Lemma 8), and a rate for the Hausdorff distance between them (Theorem 1).\n• Section 6 shows how one can apply DBSCAN a second time to remove false clusters from the first application, thus completing a bijection between the estimates and the true clusters (Theorem 2).\n• Section 7 explains how to adaptively tune the parameters so that they fall within the theoretical ranges. The main contributions of this section are a stronger result about a known k-nearest neighbor based approach to estimating the unknown dimension (Theorem 3) and a new way to tune k to approach an optimal choice of k (Theorem 4).\n• Section 8 gives the result when the data lives in RD without the manifold assumption."
  }, {
    "heading": "3. The connection to neighborhood graphs",
    "text": "This section is dedicated towards the understanding of the clusters produced by DBSCAN. The algorithm can be found in (Ester et al., 1996) and is not shown here since Lemma 1 characterizes what DBSCAN returns.\nWe have n i.i.d. samples X = {x1, ..., xn} drawn from a distribution F over RD. Definition 1. Define the k-NN radius of x ∈ RD as\nrk(x) := inf{r > 0 : |X ∩B(x, r)| ≥ k},\nwhere B(x, r) denotes the Euclidean ball of radius r centered at x. Let G(k, ε) denote the ε-neighborhood level graph of X with vertices {x ∈ X : rk(x) ≤ ε} and an edge between x and x′ iff ||x− x′|| ≤ ε. Remark 1. This is slightly different from ε-neighborhood graph, which includes all vertices. Here we exclude vertices below certain empirical density level (i.e. rk(x) > ε).\nThe next definition is relevant to DBSCAN and is from (Ester et al., 1996) but in the notation of Definition 1.\nDefinition 2. The following is with respect to fixed ε > 0 and minPts ∈ N.\n• p is a core-point if rminPts(p) ≤ ε. • q is directly density-reachable from p if |p − q| ≤ ε\nand p is a core-point.\n• q is density-reachable from p if there exists a sequence q = p1, p2, ..., pm = p such that pi is directly densityreachable from pi+1 for i = 1, ..,m− 1.\nThe following result is paraphrased from Lemmas 1 and 2 from (Ester et al., 1996), which characterizes the clusters learned by DBSCAN.\nLemma 1. (Ester et al., 1996) Let C be the clusters returned by DBSCAN(minPts, ε). For any core-point x, there exists C ∈ C with x ∈ C. On the other hand, for any C ∈ C, there exists core-point x such that C = {x′ : x′ is density-reachable from x}.\nWe now show the following result relating the εneighborhood level graphs and the clusters obtained from DBSCAN. Such an interpretation of DBSCAN has been given in previous works such as Campello et al. (2015).\nLemma 2 (DBSCAN and ε-neighborhood level graphs). Let C be the clusters obtained from DBSCAN(minPts, ε) on X . Let K be the connected components of G(minPts, ε). Then, there exists a one-to-one correspence between C and K such that if C ∈ C and K ∈ K correspond, then\nK ⊆ C ⊆ ∪x∈KB(x, ε) ∩X.\nProof. Take any K ∈ K. Each point in K is a core-point and by Lemma 1 and the definition of density-reachable, each point in K belongs to the same C ∈ C. Thus, K ⊆ {x ∈ C : rk(x) ≤ ε}. Next we show that K = {x ∈ C : rk(x) ≤ ε}.\nSuppose there exists core-point x ∈ C but x /∈ K and let y ∈ K. By Lemma 1, there exists core-point c ∈ C such that all points in C are directly reachable from c. Then there exists a path of core-points from x to c with pairwise edges of length at most ε. The same holds for c to y. Thus there exists such a path of core-points from x to y, which means that x, y are in the same CC of G(minPts, ε), contradicting the assumption that x /∈ K and y ∈ K. Thus, in fact K = {x ∈ C : rk(x) ≤ ε}. The result now follows since C consists of points that are at most ε from its core-points.\nWe can now see that DBSCAN’s clusterings can be viewed as the connected components (CCs) of an appropriate - neighborhood level graph. Using a neighborhood graph to approximate the level-set has been studied in (Rinaldo & Wasserman, 2010). The difference is that they use a kernel density estimator instead of a k-NN density estimator and study the convergence properties under different settings."
  }, {
    "heading": "4. Manifold Setting",
    "text": ""
  }, {
    "heading": "4.1. Setup",
    "text": "We make the following regularity assumptions which are standard among works on manifold learning e.g. (Baraniuk & Wakin, 2009; Genovese et al., 2012; Balakrishnan et al., 2013).\nAssumption 1. F is supported on M where: • M is a d-dimensional smooth compact Riemannian\nmanifold without boundary embedded in compact subset X ⊆ RD.\n• The volume of M is bounded above by a constant. • M has condition number 1/τ , which controls the cur-\nvature and prevents self-intersection.\nLet f be the density of F with respect to the uniform measure on M .\nAssumption 2. f is continuous and bounded."
  }, {
    "heading": "4.2. Basic Supporting Bounds",
    "text": "The following result bounds the empirical mass of Euclidean balls to the true mass under f . It is a direct consequence of Lemma 6 of Balakrishnan et al. (2013).\nLemma 3 (Uniform convergence of empirical Euclidean balls (Lemma 6 of Balakrishnan et al. (2013))). Let N be a minimal fixed set such that each point inM is at most distance 1/n from some point in N . There exists a universal\nconstant C0 such that the following holds with probability at least 1− δ. For all x ∈ X ∪N ,\nF(B) ≥ Cδ,n √ d log n\nn ⇒ Fn(B) > 0\nF(B) ≥ k n + Cδ,n\n√ k\nn ⇒ Fn(B) ≥\nk\nn\nF(B) ≤ k n − Cδ,n\n√ k\nn ⇒ Fn(B) <\nk n .\nwhere Cδ,n = C0 log(2/δ) √ d log n, Fn is the empirical distribution, and k ≥ Cδ,n. Remark 2. For the rest of the paper, many results are qualified to hold with probability at least 1−δ. This is precisely the event in which Lemma 3 holds.\nRemark 3. If δ = 1/n, then Cδ,n = O((log n)3/2).\nNext, we need the following bound on the volume of the intersection Euclidean ball and M ; this is required to get a handle on the true mass of the ball under F in later arguments. The upper and lower bounds follow from Chazal (2013) and Lemma 5.3 of Niyogi et al. (2008). The proof is given in the appendix.\nLemma 4 (Ball Volume). If 0 < r < min{τ/4d, 1/τ}, and x ∈M then\nvdr d(1− τ2r2) ≤ vold(B(x, r) ∩M) ≤ vdrd(1 + 4dr/τ).\nwhere vd is the volume of a unit ball in Rd and vold is the volume w.r.t. the uniform measure on M .\n4.3. k-NN Density Estimation\nHere, we establish density estimation rates for the k-NN density estimator in the manifold setting. This builds on work in density estimation on manifolds e.g. (Hendriks, 1990; Pelletier, 2005; Ozakin & Gray, 2009; Kim & Park, 2013; Berry & Sauer, 2017); thus, it may be of independent interest. The estimator is defined as follows\nDefinition 3 (k-NN Density Estimator).\nfk(x) := k\nn · vd · rk(x)d .\nThe following extends previous work of Dasgupta & Kpotufe (2014) to the manifold case. The proofs can be found in the appendix.\nLemma 5 (fk upper bound). Suppose that Assumptions 1 and 2 hold. Define the following which charaterizes how much the density increases locally in M :\nr̂( , x) := sup { r : sup\nx′∈B(x,r)∩M f(x′)− f(x) ≤\n} .\nFix λ0 > 0 and δ > 0 and suppose that k ≥ C2δ,n. Then there exists constant C1 ≡ C1(λ0, d, τ) such that if\nk ≤ C1 · C2d/(2+d)δ,n · n 2/(2+d),\nthen the following holds with probability at least 1− δ uniformly in > 0 and x ∈ X with f(x) + ≥ λ0:\nfk(x) <\n( 1 + 3 · Cδ,n√\nk\n) · (f(x) + ),\nprovided k satisfies vd · r̂( , x)d ·(f(x)+ ) ≥ kn−Cδ,n √ k n . Lemma 6 (fk lower bound). Suppose that Assumptions 1 and 2 hold. Define the following which charaterizes how much the density decreases locally in M :\nř( , x) := sup { r : sup\nx′∈B(x,r)∩M f(x)− f(x′) ≤\n} .\nFix λ0 > 0 and 0 < δ < 1 and suppose k ≥ Cδ,n. Then there exists constant C2 ≡ C2(λ0, d, τ) such that if\nk ≤ C2 · C2d/(4+d)δ,n · n 4/(4+d),\nthen with probability at least 1 − δ, the following holds uniformly for all > 0 and x ∈ X with f(x)− ≥ λ0:\nfk(x) ≥ (\n1− 3 · Cδ,n√ k\n) · (f(x)− ),\nprovided k satisfies vd · ř( , x)d · (f(x) − ) ≥ 4 3 ( k n + Cδ,n √ k n ) .\nRemark 4. We will often bound the density of points with low density. In low-density regions, there is less data and thus we require more points to get a tight bound. However, in many cases a tight bound is not necessary; thus the purposes of is to allow some slack. The higher the , the easier it is for the lemma conditions to be satisified. In particular, if f is α-Hölder continuous (i.e. |f(x) − f(x′)| ≤ Cα|x− x′|α), we have r̂( , x), ř( , x) ≥ ( /Cα)1/α."
  }, {
    "heading": "5. Consistency and Rates",
    "text": ""
  }, {
    "heading": "5.1. Level-Set Conditions",
    "text": "Much of the results will depend on the behavior of level set boundaries. Thus, we require sufficient drop-off at the boundaries, as well as separation between the CCs at a particular level set. We give the following notion of separation. Definition 4. A,A′ are r-separated in M if there exists a set S such that every path from A to A′ intersects S and supx∈M∩(S+B(0,r)) f(x) < infx∈A∪A′ f(x).\nDefine the following shorthands for distance from a point to a set, the intersection of M with a neighborhood around a set under the Euclidean distance, and the largest Euclidean distance from a point in a set to its closest sample point.\nDefinition 5. d(x,A) := infx′∈A |x − x′|, C⊕r := {x ∈ M : d(x,C) ≤ r}, rn(C) := supc∈C d(c,X).\nWe have the following mild assumptions which ensures that the CCs can be separated from the rest of the density by sufficiently wide valleys and there is sufficient decay around the level set boundaries.\nAssumption 3 (Separation Conditions). Let λ > 0 and Cλ be a CCs of {x ∈ M : f(x) ≥ λ}. There exists Čβ , Ĉβ , β, rs, rc > 0 and 0 < λ0 < λ such that the following holds:\nFor each C ∈ Cλ, there exists AC , a connected component of Mλ0 := {x ∈M : f(x) ≥ λ0} such that:\n• AC separates C by a valley: AC does not intersect with any other CC in Cλ; AC and Mλ0\\AC are rsseparated by some SC .\n• C⊕rc ⊆ AC .\n• β-regularity: For x ∈ C⊕rc\\C, we have\nČβ · d(x,C)β ≤ λ− f(x) ≤ Ĉβ · d(x,C)β .\nRemark 5. We can choose any 0 < β < ∞. The βregularity assumption appears in e.g. (Singh et al., 2009). This is very general and also allows us to make a separate global smoothness assumption.\nRemark 6. We currently characterize the smoothness w.r.t. the Euclidean distance. One could alternatively use the geodesic distance on M , dM (p, q). It follows from Proposition 6.3 of Niyogi et al. (2008) that when |p − q| < τ/4, we have |p − q| ≤ dM (p, q) ≤ 2|p − q|. Since the distances we deal in our analysis with are of such small order, these distances can thus essentially be treated as equivalent. We use the Euclidean distance throughout the paper for simplicity.\nRemark 7. For the rest of this paper, it will be understood that Assumptions 1, 2, and 3 hold.\nWe can define a region which isolates C away from other clusters of {x ∈M : f(x) ≥ λ}. Definition 6. XC := {x : ∃ a path P from x to x′ ∈ C such that P ∩ SC = ∅}."
  }, {
    "heading": "5.2. Parameter Settings",
    "text": "Fix λ > 0 and δ > 0. Let k satisfy the following\nKl · (log n)2 ≤ k ≤ Ku · (log n)2d/(2+d) · n2β ′/(2β′+d),\nwhere β′ := min{1, β}, and Kl and Ku are positive constants depending on δ, Čβ , Ĉβ , β, τ, d, ||f ||∞, λ0, rs, rc which are implicit in the proofs later in this section.\nThe remainder of this section will be to show that DBSCAN(minPts, ε) with\nminPts = k, ε =\n( k\nn · vd · (λ− λ · C2δ,n/ √ k) )1/d will consistently estimate each CC of {x ∈ M : f(x) ≥ λ}. Throughout the text, we denote Ĉλ as the clusters returned by DBSCAN under this setting."
  }, {
    "heading": "5.3. Separation and Connectedness",
    "text": "Take C ∈ Cλ. We show that DBSCAN will return an estimated CC Ĉ, such that Ĉ does not contain any points outside of XC . Then, we show that Ĉ contains all the sample points inC. The proof ideas used are similar to that of standard results in cluster trees estimation; they can be found in the appendix. Lemma 7 (Separation). There exists Kl sufficiently large andKu sufficiently small such that the following holds with probability at least 1− δ. Let C ∈ Cλ. There exists Ĉ ∈ Ĉλ such that Ĉ ⊆ XC . Lemma 8 (Connectedness). There exists Kl sufficiently large and Ku sufficiently small such that the following holds with probability at least 1 − δ. Let C ∈ Cλ. If there exists Ĉ ∈ Ĉλ such that Ĉ ⊆ XC , then C⊕rn(C) ∩X ⊆ Ĉ. Remark 8. These results allow C to have any dimension between 0 to d since we reason with C⊕rn(C), which contains samples, instead of simply C."
  }, {
    "heading": "5.4. Hausdorff Error",
    "text": "We give the estimation rate under the Hausdorff metric. Definition 7 (Hausdorff Distance).\ndHaus(A,A ′) = max{sup x∈A d(x,A′), sup x′∈A′ d(x′, A)}.\nTheorem 1. There existsKl sufficiently large andKu sufficiently small such that the following holds with probability at least 1 − δ. For each C ∈ Cλ, there exists Ĉ ∈ Ĉλ such that\ndHaus(C, Ĉ) ≤ 2 · (4λ/Čβ)1/β · C2/βδ,n · k −1/2β .\nProof. For Kl and Ku appropriately chosen, we have Lemma 7 and Lemma 8 hold. Thus we have for C ∈ Cλ, there exists Ĉ ∈ Ĉλ such that\nC⊕rn(C) ∩X ⊆ Ĉ ⊆ ⋃\nx∈XC∩X\nfk(x)≥λ− C2δ,n√ k λ\nB(x, ε) ∩M.\nDefine r̄ := (\n4λ·C2δ,n Čβ · √ k\n)1/β . We show that dHaus(C, Ĉ) ≤ r̄,\nwhich involves two directions to show from the Hausdroff\nmetric: that maxx∈Ĉ d(x,C) ≤ r̄ and supx∈C d(x, Ĉ) ≤ r̄.\nWe start by proving maxx∈Ĉ d(x,C) ≤ r̄. Define r0 = r̄/2. We have\nr0 = 1\n2 ( 4 · C2δ,n Čβ · √ k )1/β ≥ ( k vdnλ0 )1/d ≥ ε,\nwhere the first inequality holds when Ku is chosen sufficiently small, and the last inequality holds because λ0 < λ − C 2 δ,n√ k λ. Hence r0 + ε ≤ r̄. Therefore, it suffices to show\nsup x∈(XC\\C⊕r0 )∩X fk(x) < λ− C2δ,n√ k λ.\nWe have that for x ∈ (XC\\C⊕r0/2) ∩ X , f(x) ≤ λ − Čβ(r0/2)\nβ := λ′. Thus, for any x ∈ (XC\\C⊕r0) ∩X and letting = λ′ − f(x), we have\nr̂( , x) ≥ r0/2 ≥ (4λ0Cδ,n/( √ k · Čβ))1/β/2.\nFor Ku chosen sufficiently small, the last equation will be large enough (i.e. of order (k/vdnλ)1/d) so that the conditions of Lemma 5 hold. Thus, applying this for each x ∈ (XC\\C⊕r0) ∩X , we obtain\nsup x∈(XC\\C⊕r0 )∩X fk(x) <\n( 1 + 3\nCδ,n√ k\n) (λ− Čβ(r0/2)β).\nWe have the r.h.s. is at most λ − C 2 δ,n√ k λ for Ku chosen appropriately and the first direction follows.\nWe now turn to the other direction, that supx∈C d(x, Ĉ) ≤ r̄. Let x ∈ C. Then there exists sample point x′ ∈ B(x, rn(C)) by definition of rn and we have that x′ ∈ Ĉ. Finally, rn(C) ≤ r̄ for Kl sufficiently large, and thus |x′ − x| ≤ r̄. The result follows.\nRemark 9. When taking k ≈ n2β′/(2β′+d), we obtain the error rate of dHaus(C, Ĉ) ≈ n−1/(2β+d·max{1,β}), ignoring logarithmic factors. When 0 < β ≤ 1, this matches the known lower bound established in Theorem 4 of Tsybakov et al. (1997). However, we do not obtain this rate when β > 1. In this case, the density estimation error will be of order at least n−1/(2+d) due in part to the error from resolving the geodesic balls with Euclidean balls. This does not arise in the full dimensional setting, which will be described later."
  }, {
    "heading": "6. Removal of False Clusters",
    "text": "The result of Theorem 1 guarantees us that for each C ∈ Cλ, there exists Ĉ ∈ Ĉλ that estimates it. In this section, we\nshow how a second application of DBSCAN (Algorithm 1) can remove the false clusters discovered by the first application of DBSCAN with no additional parameters. This gives us the other direction, that each estimate in Ĉλ corresponds to a true CC in Cλ, and thus DBSCAN can identify with a one-to-one correspondence each CC of the level-set.\nAlgorithm 1 DBSCAN False CC Removal As in Section 5.2, let minPts = k and\nε =\n( k\nn·vd·(λ−λ·C2δ,n/ √ k)\n)1/d .\nDefine ε̃ := (\nk\nn·vd·(λ−λ·C2δ,n/ 3√ k)\n)1/d .\nLet Ĉλ be the clusters returned by DBSCAN(minPts, ε). Let D̂λ be the clusters returned by DBSCAN(minPts, ε̃). Let C̃λ be the clusters obtained by merging clusters from Ĉλ which are subsets of the same cluster in D̂λ . Return C̃λ.\nWe state our result below. The proof is less involved and is in the appendix.\nTheorem 2 (Removal of False CC Estimates). Define γ = λ− supx∈M\\(∪C∈CλXC) f(x), which is positive. There exists Kl sufficiently large and Ku sufficiently small depending on γ in addition to the constants mentioned in Section 5.2 so that the following holds with probability at least 1− δ. For all Ĉ ∈ C̃λ, there exists C ∈ Cλ such that\ndHaus(C, Ĉ) ≤ 2 · (4λ/Čβ)1/β · C2/βδ,n · k −1/2β ."
  }, {
    "heading": "7. Adaptive Parameter Tuning",
    "text": "In this section, we show how to obtain the near optimal rates by estimating d and adaptively choosing k such that k ≈ n2β′/(2β′+d) without knowledge of β."
  }, {
    "heading": "7.1. Determining d",
    "text": "Knowing the manifold dimension d is necessary to tune the parameters as described in Section 5.2. There has been much work done on estimating the intrinsic dimension as many learning procedures (including this one) require d as an input. Such work in intrinsic dimension estimation include (Kegl, 2002; Levina & Bickel, 2004; Hein & Audibert, 2005). Pettis et al. (1979) and more recently Farahmand et al. (2007) take a k-nearest neighbor approach. We work with the estimate of a dimension at a point proposed in the latter work:\nd̂(x) = log 2\nlog(r2k(x)/rk(x)) .\nThe main result of Farahmand et al. (2007) gives a highprobability bound for a single sample X1 ∈ X . Here we\ngive a high-probability bound under more mild smoothness assumptions which hold uniformly for all samples above some density-level given our new knowledge of k-NN density estimation rates. This may be of independent interest. Theorem 3. Suppose that f is α-Hölder continuous for some 0 < α ≤ 1. Choose λ̄0 > 0 and δ > 0. Then there exists constants C1, C2 depending on δ, Cα, α, τ, d, λ̄0 such that if k satisfies\nC1 · (log n)2 ≤ k ≤ C2 · n2α/(2α+d),\nthen with probability at least 1− δ,\n|d̂(x)− d| ≤ 20d · ||f ||∞ · Cδ,n√ k ,\nuniformly for all x ∈ X with fk(x) ≥ λ̄0.\nProof. We have for x ∈ X such that if fk(x) ≥ λ̄0, then f(x) ≥ λ0 := λ̄0/2 by Lemma 5 for C1 chosen appropriately large and C2 chosen appropriately small.\nd̂(x) = log 2\nlog(r2k(x)/rk(x)) =\nd log 2\nlog 2 + log(fk(x)/f2k(x)) .\nWe now try to get a handle on fk(x)/f2k(x) and show it is sufficiently close to 1. Applying Lemma 5 and 6 with =\nCδ,n√ k f(x) and C1, C2 appropriately chosen so that the conditions for the two Lemmas hold (remember that here we have r̂( , x), ř( , x) ≥ ( /Cα)1/α), we obtain\nfk(x) f2k(x) ≥ (1− 3Cδ,n/\n√ k)(1− Cδ,n/ √ k) · f(x)\n(1 + 3Cδ,n/ √ k)(1 + Cδ,n/ √ k) · f(x)\n≥ 1− 9 · Cδ,n√ k ,\nwhere the last inequality holds when C1 is chosen sufficiently large so that Cδ,n/ √ k is sufficiently small. On the other hand, we similarly obtain (for C1 and C2 appropriately chosen):\nfk(x) f2k(x) ≤ (1 + 3Cδ,n/\n√ k)(1 + Cδ,n/ √ k) · f(x)\n(1− 3Cδ,n/ √ k)(1− Cδ,n/ √ k) · f(x)\n≤ 1 + 9 · Cδ,n√ k .\nIt is now clear that by the expansion log(1 − r) = −r − r2/2 − r3/3 − · · · , and for Kl chosen sufficently large so that Cδ,n/\n√ k is sufficiently small, we have∣∣∣∣log( fk(x)f2k(x) )∣∣∣∣ ≤ 10 · Cδ,n√k . The result now follows by combining this with the earlier established expression for d̂(x), as desired.\nRemark 10. In Farahmand et al. (2007), it is the case that α = 1; under this setting, we match their bound with an error rate of n1/(2+d) with k ≈ n2/(2+d) being the optimal choice for k (ignoring log factors)."
  }, {
    "heading": "7.2. Determining k",
    "text": "After determining d, the next parameter we look at is k. In particular, to obtain the optimal rate, we must choose k ≈ n2β′/(2β′+d) without knowledge of β. We present a consistent estimator for β.\nWe need the following definition. The first characterizes how much f varies in balls of a certain radius along the boundaries of the λ-level set (where ∂Cλ denotes the boundary of Cλ). The second is meant to be an estimate of the first, which can be computed from the data alone. The final is our estimate of β.\nDr = inf x0∈∂Cλ sup x∈B(x0,r)\n|λ− f(x)|\nD̂r,k = min x0∈X\nB(x0,r)∩X 6=∅\nmax x∈B(x0,r)∩X\n|λ− fk(x)|\nβ̂ = logr(D̂r,k)\nThe next is a result of how D̂r,k estimates Dr. Lemma 9. Suppose that f is α-Hölder continuous for some 0 < α ≤ 1. Let k = b(log n)5c and r = 1/ √ log n. Then there exists positive constants C̃ andN depending on d, τ, α, Cα, λ0, ||f ||∞, rc such that when n ≥ N , then the following holds with probability at least 1− 1/n.\n|Dr − D̂r,k| ≤ C̃/(log n)2.\nProof sketch. Suppose that the value of Dr is attained at x0 = p and the value of D̂r,k is attained at x0 = q. Let y, z be the points that maximize |λ− f(x)| on B(p, r) and B(q, r), respectively. Let ŷ, ẑ be the sample points that maximize |λ−fk(x)| on B(p, r) and B(q, r), respectively. Now, we have\nDr − D̂r,k = |λ− f(y)| − |λ− fk(ẑ)| ≤ |λ− f(z)| − |λ− fk(ẑ)| ≤ |f(z)− fk(ẑ)| ≤ max{f(z)− fk(z), fk(ẑ)− f(ẑ)}.\nNow let z′ be the closest sample point to z inB(q, r). Then,\n≤ max{f(z′)− fk(z′), fk(ẑ)− f(ẑ)}+ |f(z)− f(z′)| + |fk(z)− fk(z′)| ≤ max\nx∈X,f(x)≥λ0 |f(x)− fk(x)|\n+ Cα|z − z′|α + |fk(z)− fk(z′)|.\nOn the other hand, we have D̂r,k −Dr = |λ− fk(ẑ)| − |λ− f(y)| ≤ |λ− fk(ŷ)| − |λ− f(y)| ≤ |f(y)− fk(ŷ)| ≤ max{f(y)− fk(y), fk(ŷ)− f(ŷ)}.\nLet y′ be the closest sample point to y in B(p, r). Then, ≤ max{f(y′)− fk(y′), fk(ŷ)− f(ŷ)}+ |f(y)− f(y′)| + |fk(y)− fk(y′)| ≤ max\nx∈X,f(x)≥λ0 |f(x)− fk(x)|\n+ Cα|y − y′|α + |fk(y)− fk(y′)|.\nThus it suffices to bound maxx∈X,f(x)≥λ0 |f(x) − fk(x)|, |y− y′|, |z− z′|, |fk(y)− fk(y′)|, |fk(z)− fk(z′)|. First take δ = 1/n and use Lemma 5 and 6 for maxx∈X,f(x)≥λ0 |f(x)− fk(x)|. Using Lemma 3, we can show that rn := |y − y′| . (log n/n)1/d. Next we bound |fk(y) − fk(y′)|. y′ ∈ X so we have guarantees on its fk value. Note that rk(y′) − rn ≤ rk(y) ≤ rk(y′) + rn. Let rk = rk(y′). This implies that fk(y′)(rk/(rk + rn))\nd ≤ fk(y) ≤ fk(y′)(rk/(rk − rn))d. Now since rk ≈ (k/n)1/d, we have |fk(y) − fk(y′)| . log n/k. The same holds for the bounds related to z, z′.\nTheorem 4 (β̂ → β in probability). Suppose f is αHölder continuous for some α with 0 < α ≤ β′. Let k = b(log n)5c and r = 1/ √ log n. Then for all > 0,\nlim n→∞\nP ( |β̂ − β| ≥ ) = 0.\nProof. Based on the β-regularity assumption, we have for r < rc:\nČβr β ≤ Dr ≤ Ĉβrβ .\nCombining this with Lemma 9, we have with probability at least 1− 1/ √ n that\nČβr β − C̃/(log n)2 ≤ D̂r,k ≤ Ĉβrβ + C̃/(log n)2.\nThus with probability at least 1− 1/n,\nβ − β̂ ≥ log(1− C̃/(D̂r,k · (log n 2))) log r − log Ĉβ log r\nβ − β̂ ≤ log(1 + C̃/(D̂r,k · (log n 2)))\nlog r + log Čβ log r .\nIt is clear that these expressions go to 0 as n→∞ and the result follows.\nRemark 11. We can then take k = nβ̂′/(2β̂′+d) with β̂′ = min{1, β̂− 0} for some 0 > 0 so that β̂′ < β′ for n sufficiently large and thus k lies in the allowed ranges described in Section 5.2 asymptotically. The settings of ε and MinPts are implied by this choice of k and our estimate of d."
  }, {
    "heading": "7.3. Rates with Data-driven Tuning",
    "text": "Putting this all together, along with Theorems 1 and 2, gives us the following consequence about level set recovery with adaptive tuning. It shows that we can obtain rates arbitrarily close to those obtained as if the smoothness parameter β and intrinsic dimension were known.\nCorollary 1. Suppose that 0 < δ < 1 and f is αHölder continuous for some 0 < α ≤ 1 and suppose the data-driven choices of parameters described in Remark 11 are used for DBSCAN. For any > 0, there exists\nN ,δ,f ≡ N( , δ, f) and Cδ ≡ Cδ(δ, f) such that the following holds. If n ≥ N ,δ,f , then with probability at least 1− δ simulatenously for each C ∈ Cλ, there exists Ĉ ∈ Ĉλ such that\ndHaus(C, Ĉ) ≤ Cδ · n− 1 2β+dmax{1,β}+ .\nMoreover, using Algorithm 2, there is a one-to-one correspondence between Cλ and Ĉλ."
  }, {
    "heading": "8. Full Dimensional Setting",
    "text": "Here we instead take f to be the density of F over the uniform measure on RD. Let\nminPts = k, ε =\n( k\nn · vD · (λ− λ · C2δ,n/ √ k)\n)1/D ,\nwhere k satisfies\nKl · (log n)2 ≤ k ≤ Ku · (log n)2D/(2+D) · n2β/(2β+D),\nand Kl and Ku are positive constants depending δ, Čβ , Ĉβ , β, τ,D, ||f ||∞, λ0, rs, rc.\nThen Theorem 1 and 2 hold (replacing d with D in Algorithm 1) for this setting of DBSCAN and thus taking k ≈ n2β/(2β+D) gives us the optimal estimation rate of O(n−1/(2β+D)). A straightforward modification of Corollary 1 also holds. This is discussed further in the Appendix."
  }, {
    "heading": "9. Conclusion",
    "text": "We proved that DBSCAN can obtain Hausdorff level-set recovery rates of Õ(n−1/(2β+D)) when the data is in RD, and Õ(n−1/(2β+d·max{1,β})) when the data lies on an embedded d-dimensional manifold. The former rate is optimal up to log factors and the latter matches known ddimensional lower bounds for 0 < β ≤ 1 up to log factors. Moreover, we provided a fully data-driven procedure to tune the parameters to attain these rates.\nThis shows that the procedure’s ability to recover density level sets matches the strongest known consistency results attained for this problem. Furthermore, we developed the necessary tools and give the first analysis of density levelset estimation on manifolds, let alone with a practical procedure such as DBSCAN.\nOur density estimation errors however cannot converge faster than Õ(n−1/(2+d)), which is due in part to the error from resolving geodesic balls with Euclidean balls. Thus it remains an open problem whether the manifold level-set rates are minimax optimal when β > 1."
  }, {
    "heading": "Acknowledgements",
    "text": "The author is grateful to Samory Kpotufe for insightful discussions and to the anonymous reviewers for their useful feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Convergence rates in nonparametric estimation of level",
    "authors": ["Baıllo", "Amparo", "Cuesta-Albertos", "Juan A", "Cuevas", "Antonio"],
    "venue": "sets. Statistics & probability letters,",
    "year": 2001
  }, {
    "title": "Cluster trees on manifolds",
    "authors": ["S. Balakrishnan", "S. Narayanan", "A. Rinaldo", "A. Singh", "L. Wasserman"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Random projections of smooth manifolds",
    "authors": ["Baraniuk", "Richard G", "Wakin", "Michael B"],
    "venue": "Foundations of computational mathematics,",
    "year": 2009
  }, {
    "title": "Density estimation on manifolds with boundary",
    "authors": ["Berry", "Tyrus", "Sauer", "Timothy"],
    "venue": "Computational Statistics & Data Analysis,",
    "year": 2017
  }, {
    "title": "Exact rates in density support estimation",
    "authors": ["Biau", "Gérard", "Cadre", "Benoı̂t", "Pelletier", "Bruno"],
    "venue": "Journal of Multivariate Analysis,",
    "year": 2008
  }, {
    "title": "Kernel estimation of density level sets",
    "authors": ["Cadre", "Benoıt"],
    "venue": "Journal of multivariate analysis,",
    "year": 2006
  }, {
    "title": "Hierarchical density estimates for data clustering, visualization, and outlier detection",
    "authors": ["Campello", "Ricardo JGB", "Moulavi", "Davoud", "Zimek", "Arthur", "Sander", "Jörg"],
    "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
    "year": 2015
  }, {
    "title": "Rates for convergence for the cluster tree",
    "authors": ["K. Chaudhuri", "S. Dasgupta"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Consistent procedures for cluster tree estimation and pruning",
    "authors": ["K. Chaudhuri", "S. Dasgupta", "S. Kpotufe", "U. von Luxburg"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2014
  }, {
    "title": "An upper bound for the volume of geodesic balls in submanifolds of euclidean spaces",
    "authors": ["F. Chazal"],
    "year": 2013
  }, {
    "title": "Convergence rates for persistence diagram estimation in topological data analysis",
    "authors": ["Chazal", "Frédéric", "Glisse", "Marc", "Labruère", "Catherine", "Michel", "Bertrand"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "Density level sets: Asymptotics, inference, and visualization",
    "authors": ["Chen", "Yen-Chi", "Genovese", "Christopher R", "Wasserman", "Larry"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2016
  }, {
    "title": "A plug-in approach to support estimation",
    "authors": ["A. Cuevas", "R. Fraiman"],
    "venue": "Annals of Statistics,",
    "year": 1997
  }, {
    "title": "Optimal rates for k-nn density and mode estimation",
    "authors": ["S. Dasgupta", "S. Kpotufe"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "A densitybased algorithm for discovering clusters in large spatial databases with noise",
    "authors": ["M. Ester", "H. Kriegel", "J. Sander", "X. Xu"],
    "year": 1996
  }, {
    "title": "Minimax manifold estimation",
    "authors": ["Genovese", "Christopher", "Perone-Pacifico", "Marco", "Verdinelli", "Isabella", "Wasserman", "Larry"],
    "venue": "Journal of machine learning research,",
    "year": 2012
  }, {
    "title": "Intrinsic dimensionality estimation of submanifolds in rd",
    "authors": ["Hein", "Matthias", "Audibert", "Jean-Yves"],
    "year": 2005
  }, {
    "title": "Nonparametric estimation of a probability density on a riemannian manifold using fourier expansions",
    "authors": ["Hendriks", "Harrie"],
    "venue": "The Annals of Statistics,",
    "year": 1990
  }, {
    "title": "Uniform convergence rates for kernel density estimation",
    "authors": ["Jiang", "Heinrich"],
    "venue": "International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Modal-set estimation with an application to clustering",
    "authors": ["Jiang", "Heinrich", "Kpotufe", "Samory"],
    "venue": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2017
  }, {
    "title": "Intrinsic dimension estimation using packing numbers",
    "authors": ["Kegl", "Balazs"],
    "year": 2002
  }, {
    "title": "Geometric structures arising from kernel density estimation on riemannian manifolds",
    "authors": ["Kim", "Yoon Tae", "Park", "Hyun Suk"],
    "venue": "Journal of Multivariate Analysis,",
    "year": 2013
  }, {
    "title": "Pruning nearest neighbor cluster trees",
    "authors": ["S. Kpotufe", "U. von Luxburg"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Maximum likelihood estimation of intrinsic dimension",
    "authors": ["Levina", "Elizaveta", "Bickel", "Peter J"],
    "year": 2004
  }, {
    "title": "Optimal construction of k-nearest-neighbor graphs for identifying noisy clusters",
    "authors": ["Maier", "Markus", "Hein", "Matthias", "von Luxburg", "Ulrike"],
    "venue": "Theoretical Computer Science,",
    "year": 2009
  }, {
    "title": "Finding the homology of submanifolds with high confidence from random samples",
    "authors": ["Niyogi", "Partha", "Smale", "Stephen", "Weinberger", "Shmuel"],
    "venue": "Discrete & Computational Geometry,",
    "year": 2008
  }, {
    "title": "Submanifold density estimation",
    "authors": ["Ozakin", "Arkadas", "Gray", "Alexander G"],
    "venue": "pp. 1375–1382,",
    "year": 2009
  }, {
    "title": "Kernel density estimation on riemannian manifolds",
    "authors": ["Pelletier", "Bruno"],
    "venue": "Statistics & probability letters,",
    "year": 2005
  }, {
    "title": "An intrinsic dimensionality estimator from near-neighbor information",
    "authors": ["K. Pettis", "T. Bailey", "A. Jain"],
    "venue": "IEEE Transactions on PAMI,",
    "year": 1979
  }, {
    "title": "Measuring mass concentrations and estimating density contour clusters-an excess mass approach",
    "authors": ["Polonik", "Wolfgang"],
    "venue": "The Annals of Statistics,",
    "year": 1995
  }, {
    "title": "Fast rates for plug-in estimators of density level",
    "authors": ["P. Rigollet", "R. Vert"],
    "venue": "sets. Bernoulli,",
    "year": 2009
  }, {
    "title": "Generalized density clustering",
    "authors": ["Rinaldo", "Alessandro", "Wasserman", "Larry"],
    "venue": "The Annals of Statistics,",
    "year": 2010
  }, {
    "title": "Stability of density-based clustering",
    "authors": ["Rinaldo", "Alessandro", "Singh", "Aarti", "Nugent", "Rebecca", "Wasserman", "Larry"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Adaptive hausdorff estimation of density level sets",
    "authors": ["Singh", "Aarti", "Scott", "Clayton", "Nowak", "Robert"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "Consistency and rates for clustering with dbscan",
    "authors": ["Sriperumbudur", "Bharath K", "Steinwart", "Ingo"],
    "year": 2012
  }, {
    "title": "Adaptive density level set clustering",
    "authors": ["I. Steinwart"],
    "venue": "In 24th Annual Conference on Learning Theory,",
    "year": 2011
  }, {
    "title": "Fully adaptive density-based clustering",
    "authors": ["Steinwart", "Ingo"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "A generalized single linkage method for estimating the cluster tree of a density",
    "authors": ["Stuetzle", "Werner", "Nugent", "Rebecca"],
    "venue": "Journal of Computational and Graphical Statistics,",
    "year": 2010
  }, {
    "title": "On nonparametric estimation of density level sets",
    "authors": ["Tsybakov", "Alexandre B"],
    "venue": "The Annals of Statistics,",
    "year": 1997
  }, {
    "title": "Granulometric smoothing",
    "authors": ["Walther", "Guenther"],
    "venue": "The Annals of Statistics, pp",
    "year": 1997
  }, {
    "title": "Minimax optimal levelset estimation",
    "authors": ["RM Willett", "Nowak", "Robert D"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2007
  }],
  "id": "SP:d00e335537ed123be6eebf72815a7098b44af379",
  "authors": [{
    "name": "Heinrich Jiang",
    "affiliations": []
  }],
  "abstractText": "We show that DBSCAN can estimate the connected components of the λ-density level set {x : f(x) ≥ λ} given n i.i.d. samples from an unknown density f . We characterize the regularity of the level set boundaries using parameter β > 0 and analyze the estimation error under the Hausdorff metric. When the data lies in R we obtain a rate of Õ(n−1/(2β+D)), which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown ddimensional manifold in R, then we obtain a rate of Õ(n−1/(2β+d·max{1,β})). Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or β.",
  "title": "Density Level Set Estimation on Manifolds with DBSCAN"
}