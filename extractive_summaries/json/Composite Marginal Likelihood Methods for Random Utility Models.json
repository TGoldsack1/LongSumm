{
  "sections": [{
    "heading": "1. Introduction",
    "text": "How to model rank data and how to make optimal statistical inferences from rank data are important topics at the interface of statistics, computer science, and economics. Random utility models (RUMs) (Thurstone, 1927) are one of the most widely-applied statistical models for rank data. In an RUM, each alternative ai is parameterized by a utility distribution µi. Agents’ rankings are generated in two steps. In the first step, a latent utility ui for each alternative ai is generated from µi. In the second step, the alternatives are ranked w.r.t. their utilities ui in descending order. The logit model and the probit model, which are very popular in statistics and economics, both have random utility interpretations.\nWhile providing better fitness to the rank data (Azari Soufiani et al., 2012; Zhao et al., 2018b), general RUMs are computationally hard to tackle due to the lack of closed-form formulas for the likelihood function. The only known exception is the Plackett-Luce model (Plackett, 1975; Luce, 1959),\n1Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA. Correspondence to: Zhibing Zhao <zhaoz6@rpi.edu>, Lirong Xia <xial@cs.rpi.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhich is the RUM with Gumbel distributions. RUMs, especially the Plackett-Luce model, have been widely applied to model and predict human behavior (McFadden, 2000), where the standard case of discrete choice models can be viewed as the Plackett-Luce model restricted to top choices. Other notable recent applications include elections (Gormley & Murphy, 2008), crowdsourcing (Pfeiffer et al., 2012), recommender systems (Wang et al., 2016), preference elicitation (Azari Soufiani et al., 2013b; Zhao et al., 2018a), marketing (Berry et al., 1995), health care (Bockstael, 1999), transportation (Bhat et al., 2007), and security (Yang et al., 2011).\nRecently there has been a growing interest in designing faster and more accurate algorithms for RUMs. Many algorithms in previous work share the following rank-breakingthen-optimization architecture. First, rank data are converted to pairwise comparison data. Second, based on the pairwise comparisons, various optimization algorithms are designed to estimate the ground truth (Negahban et al., 2012; Azari Soufiani et al., 2013a; 2014; Chen & Suh, 2015; Khetan & Oh, 2016b;a).\nPairwise data are often obtained from rank data by applying rank-breaking, which allows for a smooth tradeoff between computational efficiency and statistical efficiency (Azari Soufiani et al., 2013a; 2014; Khetan & Oh, 2016b;a). Given m alternatives, a rank-breaking scheme is modeled by a weighted undirected graph G (see Figure 1 for an example) over {1, . . . ,m} (the vertices are positions in a ranking), such that for any ranking R over the m alternatives and any distinct i1, i2 ≤ m, we obtain gi1i2 (the weight on the edge {i1, i2} in G) pairwise comparisons between alternatives at positions i1 and i2 of R.\nOur Contributions. By leveraging the celebrated composite marginal likelihood (CML) methods (Lindsay, 1988; Varin, 2008), we propose a novel and flexible rank-breakingthen-CML framework. Given an RUM, our framework, denoted by RBCML(G,W), is defined by a weighted rankbreaking graph G and a CML-weight vectorW = {wi1i2 : i1, i2 ≤ m, i1 6= i2}, which contains one non-negative weight for each pair of alternatives (ai1 , ai2). We note that both G andW are the algorithm designer’s choices. Given rank data P , we compute ~θ to maximize the following com-\nposite log-likelihood function. CLLM(~θ, P ) = ∑ i1 6=i2 (κi1i2wi1i2 ln pi1i2( ~θ))\nHere ~θ represents the parameters of RUM. Given G, κi1i2 is the percentage of pairwise comparisons ai1 ai2 in the data. pi1i2(~θ) is the probability of ai1 ai2 under RUM with ~θ, which is the total probability of generating a ranking with ai1 ai2 given ~θ. We note that the RBCML framework is very general because any combination of G andW can be used. A breaking graph G is uniform, if all edges have the same weight. Let Gu denote the breaking graph whose weights are all 1. A CML-weight vectorW is symmetric, if for all i1 6= i2, we have wi1i2 = wi2i1 . W is uniform, if all weights are 1, denoted byWu.\nTheoretical contributions. For convenience we let position-k breaking denote the breaking that consists of all unit-weight edges between position k and all positions after k. E.g. the position-1 breaking consists of all unit-weight pairwise comparisons in positions {(1, 2), (1, 3), . . . , (1,m)}. A weighted union of position-k breakings is a breaking that has the same weight (possibly zero) for each k. An example is shown in Figure 1, which is the union of 1/3 position-1 breaking and 1/2 position-2 breaking. Our theoretical results carry the following message about “good\" RBCMLs.\nWe should use RBCML(G,W) with connected and symmetricW . For Plackett-Luce model, we should use a breaking G that is the weighted union of multiple position-k breakings. For RUMs with symmetric utility distributions, we should use Gu.\nThe message is established via a series of theorems (Theorems 1, 2, 5, 8, and 9). Theorems 1 and 2, which prove that strict log-concavity is preserved under convolution and under marginalization, are of independent interest.\nAlgorithmic contributions. Experiments on synthetic data for Gaussian RUMs, where each utility distribution is Gaussian, show that RBCML(Gu,Wu) achieves better statistical efficiency and computational efficiency than the GMM algorithm by Azari Soufiani et al. (2014). For the Plackett-Luce model, we propose an RBCML with a heuristicWH . We compare our RBCML for the Plackett-Luce model with the consistent rank-breaking algorithm by Khetan & Oh (2016b) and the I-LSR algorithm by Maystre & Grossglauser (2015) via experiments on synthetic data and show that our RBCML provides a tradeoff between statistical efficiency and computational efficiency.\nRelated Work and Discussions. Our RBCML framework leverages the strengths of rank breaking and CML. The major advantage of CML is that often marginal likelihood functions are much easier to optimize than the full likeli-\nhood function. However, for RUMs, even computing the marginal likelihood may take too much time, as CML needs to count the number of pairwise comparisons between alternatives in the rankings, which takes O(m2n) time, where m is the number of alternatives and n is the number of rankings. Therefore, standard CML becomes inefficient when m or n are large. RBCML overcomes such inefficiency by applying rank-breaking. The computational complexity of rank-breaking can be O(kmn) for any k ≤ m. Often a tradeoff between computational efficiency and statistical efficiency must be made.\nRBCML generalizes the algorithm proposed by Khetan & Oh (2016b), which focused on the Plackett-Luce model and whose optimization technique turns out to be CML with Wu.1 The comparison between RBCML and other related work is summarized in Table 1.\nOur theorems on strict log-concavity of composite likelihood function generalize Hunter (2004)’s result, which was proved for Plackett-Luce with Gu andWu. Our results can be applied to not only otherW’s under Plackett-Luce, but also other RUMs where the PDFs of utility distributions are strictly log-concave, e.g. Gaussians. Technically, proving our results for general RUMs is much more challenging due to the lack of closed-form formulas for the likelihood function. Another line of previous work proved (non-strict) log-concavity for special cases of RBCML (Azari Soufiani et al., 2012; Khetan & Oh, 2016a;b). Again, our theorems are stronger because (1) our theorems work for a more general class of RBCML, and (2) strict log-concavity is more desirable than log-concavity because the formal implies the uniqueness of the solution.\nThe key step in our proofs is the preservation of strict logconcavity under convolution (Theorem 1) and marginalization (Theorem 2). Surprisingly, we were not able to find these theorems in the literature, despite that it is well-known that (non-strict) log-concavity and strong log-concavity are preserved under convolution and marginalization (Saumard & Wellner, 2014). Our proofs of Theorems 1 and 2 are based on a careful examination of the condition for equality in the Prékopa-Leindler inequality proved by Dubuc (1977). We believe that Theorems 1 and 2 are of independent interest.\nXu & Reid (2011) provided sufficient conditions for general CML methods to satisfy consistency and asymptotic normality. Unfortunately, some of the conditions by Xu & Reid (2011) do not hold for RBCML. Therefore, we derive new proof of consistency and asymptotic normality for RBCML.\nKhetan & Oh (2016b;a) provide sufficient conditions on rank-breakings for CML with Wu to be consistent under\n1Khetan & Oh (2016b)’s algorithm works for special partial orders. In this paper, we only focus on comparisons between RBCML and their algorithms restricted to linear orders.\nthe Plackett-Luce model. It is an open question what are all consistent rank-breakings for CML, even withWu. We answer this question for Plackett-Luce (Theorem 8), as well as a large class of other RUMs (Theorem 9), and for all W’s."
  }, {
    "heading": "2. Preliminaries",
    "text": "Let A = {a1, a2, · · · , am} denote the set of m alternatives. Let L(A) denote the set of all linear orders (rankings) over A. A ranking R ∈ L(A) is denoted by ai1 ai2 . . . aim , where ai1 is ranked at the top, ai2 is ranked at the second position, etc. We write a R b if a is ranked higher than b in R. Let P = {R1, R2, . . . , Rn} denote the collection of n rankings, called a preference profile.\nDefinition 1 (Random utility models (RUMs)) A random utility modelM over A associates each alternative ai with a utility distribution µi(·|~θi). The parameter space is Θ = {~θ = {~θi|i = 1, 2, . . . ,m}}. The sample space is L(A)n. Each ranking is generated i.i.d. in two steps. First, for each i ≤ m, a latent utility ui is generated from µi(·|~θi) independently, and second, the alternatives are ranked according to their utilities in the descending order. Given a parameter ~θ, the probability of generating R = ai1 ai2 . . . aim is\nPrM(R|~θ) = ∫ ∞ −∞ ∫ ∞ uim · · · ∫ ∞ ui2 µim(uim |~θim) · · ·\nµi1(ui1 |~θi1)dui1dui2 · · · duim\nIn this paper, we focus on the location family, where the shapes of the utility distributions are fixed and each utility distribution µi is only parameterized by its mean, denoted by θi. Let πi denote the distribution obtained from µi(·|θi) by shifting the mean to 0. For the location family, we have πi(ui|θi) = π(ui − θi). Because shifting the means of all alternatives by the same distance will not affect the distribution of the rankings, w.l.o.g. we let θm = 0 throughout the paper. Moreover, we assume that the PDF of each utility distribution is continuous and positive everywhere. We further say that an RUM is symmetric if the PDF of each utility distribution is symmetric around its mean. We use Gaussian RUMs to denote the RUMs where all utility distributions are Gaussian.\nFor any combination of m probability distributions π1, . . . , πm whose means are 0, we let RUM(π1, . . . , πm) denote the RUM location family where the shapes of utility distributions are π1, . . . , πm. For any probability distribution π whose mean is 0, let RUM(π) denote the RUM where the shapes of all utility distributions are π.\nGiven a profile P and a parameter ~θ, we have PrM(P |~θ) = ∏n j=1 PrM(Rj |~θ). Because all utilities are drawn independently, the probability of pairwise comparison is PrM(ai1 ai2 |~θ) =∫∞ −∞ ∫∞ ui2 µi1(ui1 |~θ)µi2(ui2 |~θ)dui1dui2 .\nExample 1 (Plackett-Luce model as an RUM) Let µi(·|θi) be the Gumbel distribution where µi(xi|θi) = e−(xi−θi)−e\n−(xi−θi) . For any ranking R = ai1 ai2 . . . aim , we have PrPL(R|~θ) = ∏m−1 t=1 e θit∑m\nl=t e θil\n. The probability\nof ai1 ai2 under the Plackett-Luce model is PrPL(ai1 ai2 |~θ) = e θi1\ne θi1 +e θi2 .\nA weighted (rank-)breaking G = {gii′ : i < i′ ≤ m} can be represented by a weighted undirected graph over positions {1, . . . ,m}, such that for any gii′ > 0, there is an edge between i and i′ whose weight is gii′ . We say that G is uniform, if all weights are the same. Let Gu denote the the uniform breaking where all weights are 1. For any 1 ≤ k ≤ m− 1, the position-k breaking is the graph where for any l > k, there is an edge with weight 1 between k and l. For any ~θ ∈ Rm−1, any weighted rank-breaking G, any pair of alternatives ai1 , ai2 , let Gai1 ai2 (R) = gii′ such that ai1 and ai2 are ranked at the ith position and the i\n′th position in R, respectively. Given a profile P , we define\nκi1i2 = ∑n j=1 Gai1 ai2 (Rj)\nn , and let κ̄i1i2 = E[κi1i2 |~θ]. We note that κi1i2 is a function of the preference profile. κ̄i1i2 is the expected κi1i2 value for perfect data given ~θ, which means that it is a function of the ground truth parameter ~θ.\nExample 2 Let m = 3, n = 2. The profile P = {a1 a2 a3, a3 a2 a1}. Let G = {g12 = g13 = 13 , g23 = 1 2} as shown in Figure 1 (a). Then we have κ12 = κ13 = 1 3/n = 1 6 , κ23 = 1 2/n = 1 4 , κ32 = κ31 = 1 3/n = 1 6 , κ21 = 1 2/n = 1 4 .\nPosition 1\nPosition 2\nPosition 3\n \n \n \n1 1\n2 2\na1 a2\na3\nPosition 1\nPosition 2\nPosition 3\n \n \n \n1 1\n2 2\na1 a2\na3\n(a) G. (b)W .\nFigure 1. A rank-breaking G and a CML-weight vectorW ."
  }, {
    "heading": "3. Composite Marginal Likelihood Methods",
    "text": "LetW = {wii′ : ai, ai′ ∈ A} denote a CML-weight vector. We say thatW is symmetric, if for any pair of alternatives ai, ai′ , we have wii′ = wi′i > 0. We say thatW is uniform, if all wii′ ’s are equal. LetWu denote a uniformW .\nWe note that vertices inW corresponds to the alternatives while vertices in G corresponds to positions in a ranking. For example, vertex i inW corresponds to ai, while vertex i in G corresponds to the ith position in a ranking.\nExample 3 A symmetricW is shown in Figure 1 (b), where w12 = w21 = 1 and w23 = w32 = 2.\nGiven G andW , we propose the rank-breaking-then-CML framework for RUMs, denoted by RBCML(G,W), to be the maximizer of composite log-marginal likelihood, which is defined below.\nDefinition 2 (Composite marginal likelihood for RUMs) Given an RUMM, for any preference profile P and any θ, let pi1i2(~θ) = PrM(ai1 ai2 |~θ). The composite marginal likelihood is CLM(~θ, P ) = ∏ i1 6=i2(pi1i2(\n~θ))κi1i2wi1i2 . The composite log-marginal likelihood becomes:\nCLLM(~θ, P ) = ∑ i1 6=i2 κi1i2wi1i2 ln pi1i2( ~θ) (1)\nWe let RBCML(G,W)(P ) = arg max~θ CLLM(~θ, P ). For the Plackett-Luce model the composite (log-)marginal likelihood has a closed-form formula.\nDefinition 3 (CML for Plackett-Luce) For any ~θ and preference profile P , the composite marginal likelihood for the Plackett-Luce model is CLPL(~θ, P ) =∏ i1<i2 ( e θi1\ne θi1 +e θi2 )κi1i2wi1i2 ( e\nθi2 e θi1 +e θi2 )κi2i1wi2i1 . The\ncomposite log-marginal likelihood is\nCLLPL(~θ, P ) = ∑ i1<i2 (κi1i2wi1i2θi1 + κi2i1wi2i1θi2\n− (κi1i2wi1i2 + κi2i1wi2i1) ln(eθi1 + eθi2 )) (2)\nThe first order conditions are, for all i, ∂CLLPL( ~θ,P ) ∂θi =∑\ni′ 6=i(κii′wii′ − (κii′wii′ + κi′iwi′i) eθi eθi+eθi′ ).\nExample 4 Continuing Example 2 and Example 3,\nCLLPL(~θ, P ) = 1\n6 θ1 +\n1 4 θ2 − ( 1 6 + 1 4 ) ln(eθ1 + eθ2)\n+ 1\n2 θ2 − (\n1 2 + 1 3 ) ln(eθ2 + 1)\nBy solving the first order conditions, we have eθ1 = 1 and eθ2 = 1.5. So the outcome of RBCML is θ1 = 0, θ2 = ln 1.5. We recall that θ3 = 0 in this paper."
  }, {
    "heading": "4. Preservation of Strict Log-Concavity",
    "text": "Definition 4 (Log-concavity and strict log-concavity) A function f(~x) > 0 is log-concave if ∀0 < λ < 1, we have f(λ~x + (1 − λ)~y) ≥ f(~x)λf(~y)1−λ. If the inequality is always strict, then f is strictly log-concave.\nTheorem 1 (Preservation under convolution) Let f(x) and g(x) be two continuous and strictly log-concave functions on R. Then f ∗ g is also strictly log-concave.\nProof: The proof is done by examining the equality condition for the Prékopa-Leindler inequality. Let h = f ∗ g, namely, for any y ∈ R, h(y) = ∫ R f(y − x)g(x)dx. Because f and g are continuous, so does h. To prove the strict log-concavity of h, it suffices to prove that for any different y1, y2 ∈ R, h(y1+y22 ) > √ h(y1)h(y2).\nSuppose for the sake of contradiction that this is not true. Since log-concavity preserves under convolution (Saumard & Wellner, 2014), h is log-concave. So, there exist y1 < y2 such that h(y1+y22 ) = √ h(y1)h(y2). Let Λ(x, y) = f(y − x)g(x). We further define\nH(x) = Λ(x, y1 + y2\n2 ) = f( y1 + y2 2 − x)g(x)\nF (x) = Λ(x, y1) = f(y1 − x)g(x) G(x) = Λ(x, y2) = f(y2 − x)g(x)\nBecause (non-strict) log-concavity is preserved under convolution, Λ(x, y) is log-concave. We have that for any x ∈ R, H(x) ≥ √ F (x)G(x). The Prékopa-Leindler inequality asserts that∫ R H(x)dx ≥ √∫ R F (x)dx ∫ R G(x)dx (3) Because h(y1+y22 ) = ∫ RH(x)dx, h(y1) = ∫ R F (x)dx,\nh(y2) = ∫ RG(x)dx, and h( y1+y2 2 ) = √ h(y1)h(y2), (3) becomes an equation. It was proved by Dubuc (1977) that: there exist a > 0 and b ∈ R such that the following conditions hold almost everywhere for x ∈ R (see the translation of Dubuc’s result in English by Ball & Böröczky (2010)). 1. F (x) = aH(x+ b), 2. G(x) = a−1H(x− b).\nThe first condition means that for almost every x ∈ R,\nf(y1 − x)g(x) = af( y1 + y2\n2 − x− b)g(x+ b)\n⇐⇒ g(x) g(x+ b)\n= a f(y1+y22 − x− b)\nf(y1 − x) (4)\nThe second condition means that for almost all x ∈ R, f(y2 − x)g(x) = a−1f(y1+y22 − x + b)g(x − b) ⇐⇒ g(x−b) g(x) = a f(y2−x) f( y1+y2 2 −x+b) . Therefore, for almost all x ∈ R,\ng(x)\ng(x+ b) = a f(y2 − x− b) f(y1+y22 − x)\n(5)\nCombining (4) and (5), for almost every x ∈ R we have\ng(x)\ng(x+ b) = a f(y2 − x− b) f(y1+y22 − x) = a f(y1+y22 − x− b) f(y1 − x) (6)\nBecause f(x) is strictly log-concave, for any fixed c 6= 0, f(x+c) f(x) is strictly monotonic. Because y1 6= y2 and y2−x− b− (y1+y22 − x) = y1+y2 2 − x− b− (y1− x) = y2−y1 2 − b, we must have that y2−y12 − b = 0, namely b = y2−y1\n2 . Therefore, (6) becomes g(x)\ng(x+ y2−y1 2 ) = a for almost every\nx ∈ R, which contradicts the strict log-concavity of g. This means that h = f ∗ g is strictly log-concave.\nTheorem 2 (Preservation under marginalization) Let h(x, y) be a strictly log-concave function on R2. Then∫ R h(x, y)dx is strictly log-concave on R.\nAgain, the proof is done by examining the equality condition for the Prékopa-Leindler inequality. All missing proofs can be found in the supplementary material."
  }, {
    "heading": "5. Strict Log-Concavity of CML",
    "text": "For any profile P , let G(P ) denote the weighted directed graph where each represents an alternative. For any 1 ≤ i 6= i′ ≤ m, the weight on the edge from i to i′ is κii′ . A weighted directed graph is (weakly) connected, if after removing the directions on all edges, the resulting undirected graph is connected. A weighted directed graph is strongly connected, if there is a directed path with positive weights between any pair of vertices. Given any pair of weighted graphs G1 and G2, we let G1 ⊗ G2 denote the weighted graph where the weights on each edge is the multiplication of the weights of same edge in G1 and G2.\nTheorem 3 Given any profile P , the composite likelihood function for Plackett-Luce, i.e. CLPL(~θ, P ), is strictly logconcave if and only if W ⊗ G(P ) is weakly connected. arg max~θ CLPL(\n~θ, P ) is bounded if and only ifW ⊗G(P ) is strongly connected.\nThe proof is similar to the log-concavity of likelihood for BTL by (Hunter, 2004). For general RUMs we prove a similar theorem.\nTheorem 4 Let M be an RUM where the CDF of each utility distribution is strictly log-concave. Given any profile P , the composite likelihood function forM, i.e. CLM(~θ, P ), is strictly log-concave if and only ifW ⊗G(P ) is weakly connected. arg max~θ CLM(\n~θ, P ) is bounded if and only if W ⊗G(P ) is strongly connected.\nProof sketch: It is not hard to check that whenW ⊗G(P ) is not connected, there exist ~θ(1) and ~θ(2) such that for any 0 < λ < 1 we have CLLPL(~θ(1), P ) = CLLPL(~θ(2), P ) = λCLLPL(~θ(1), P ) + (1−λ)CLLPL(~θ(2), P ), which violates strict log-concavity. Suppose W ⊗ G(P ) is weakly connected, it suffices to prove for any i1 6= i2, Pr(ai1 ai2 |~θ) is strictly log-concave. We can write this as an integral over ui2 − ui1 : Pr(ui1 > ui2 |~θ) = ∫∞ 0 Pr(ui2 − ui1 = s|~θ)ds.\nLet π∗i2(·|~θ) denote the flipped distribution of πi2(·|~θ) around x = s, then we have π∗i2(s − x|~θ) = πi2(s + x|~θ). Further we have Pr(ui1 > ui2 |~θ) =∫∞\n0 ∫∞ −∞ πi1(x|θi1)πi2(x+ s|θi2)dxds = ∫∞ 0 πi1 ∗ π∗i2ds. By Theorem 1, πi1 ∗ π∗i2 is strictly log-concave. Then we prove that tail probability of a strictly log-concave distribution is also strictly log-concave.\nThe proof for boundedness is similar to the proof of a similar condition for BTL by Hunter (2004)."
  }, {
    "heading": "6. Asymptotic Properties of RBCML",
    "text": "Given any RUM M and any parameter ~θ, we define ELLM(~θ) = E[CLLM(~θ,R)] and let ∇ELLM(~θ) be the gradient of ELLM(~θ), whose ith element is∇iELLM(~θ) =∑ i′ 6=i( κ̄ii′wii′\npii′ ( ~θ)\n∂pii′ ( ~θ)\n∂θi + κ̄i′iwi′i\npi′i( ~θ)\n∂pi′i( ~θ)\n∂θi ). Let H(~θ, P ) be\nthe Hessian matrix evaluated at ~θ. And let H0(~θ0) denote the expected Hessian of CLLM(~θ, P ) at ~θ0, where ~θ0 is the ground truth parameter.\nTheorem 5 (Consistency and asymptotic normality) Given any RUM M, any ~θ0 and any profile P with n rankings. Let ~θ∗ be the output of RBCML(G,W). When n→∞, we have ~θ∗ p−→ ~θ0 and √ n(~θ∗ − ~θ0) d−→ N(0, H−10 (~θ0)Var[∇CLLM(~θ0, R)]H −1 0 ( ~θ0)) if and only if ~θ0 is the only solution to\n∇ELLM(~θ) = ~0, (7)\nProof: The “only if\" direction is straightforward. The solution to (7) is unique because CLLM(~θ, P ) is strictly concave. Suppose ~θ1, other than ~θ0, is the solution to (7), then\nwhen n → ∞, ~θ1 will be the estimate of RBCML(G,W), which means RBCML(G,W) is not consistent.\nNow we prove the “if\" direction. First we prove consistency. It is required by Xu & Reid (2011) that for different parameters, the probabilities for any composite likelihood event are different, which is not true in our case. A simple counterexample is θ(1)1 = 1, θ (2) 1 = 2, θ (1) 2 = θ (1) 3 = θ (2) 2 = θ (2) 3 = 0. Then Pr(a2 a3|~θ(1)) = Pr(a2 a3|~θ(2)).\nBy the law of large numbers, we have for any , Pr(|CLLM(~θ, P ) − ELLM(~θ)| ≤ /2) → 1 as n → ∞. This implies limn→∞ Pr(CLLM(~θ∗, P ) ≤ ELLM(~θ∗) + /2) = 1. Similarly we have limn→∞ Pr(ELLM(~θ0) ≤ CLLM(~θ0, P ) + /2) = 1. Since ~θ∗ maximize CLLM(~θ, P ), we have Pr(CLLM(~θ0, P ) ≤ CLLM(~θ∗, P )) = 1. The above three equations imply that limn→∞ Pr(ELLM(~θ0)− ELLM(~θ∗) ≤ ) = 1.\nLet Θ be the subset of parameter space s.t. ∀~θ ∈ Θ , ELLM(~θ0)− ELLM(~θ) ≤ . Because ELLM(~θ) is strictly concave, Θ is compact and has a unique maximum at ~θ0. Thus for any > 0, limn→∞ Pr(~θ∗ ∈ Θ ) = 1. This implies consistency, i.e., ~θ∗ p−→ ~θ0.\nNow we prove asymptotic normality. By mean value theorem, we have 0 = ∇CLLM(~θ∗, P ) = ∇CLLM(~θ0, P ) + H(α~θ∗ + (1 − α)~θ0, P )(~θ∗ − ~θ0), where 0 ≤ α ≤ 1. Therefore, we have √ n(~θ∗ − ~θ) = −H−1(α~θ∗ + (1 − α)~θ0, P )( √ n∇CLLM(~θ0, P )). Since ∇CLLM(~θ0, P ) = 1 n ∑n j=1∇CLLM(~θ0, Rj), by the central limit theorem, we have √ n∇CLLM(~θ0, P ) d−→ N(0,Var[∇CLLM(~θ0, R)])\nBecause ~θ∗ p−→ ~θ0 and H is continuous, we have H(α~θ∗ + (1 − α)~θ0, P ) p−→ H(~θ0, P ). Since H(~θ, P ) = 1 n ∑n j=1H( ~θ,Rj), by law of large numbers, we have H(~θ, P ) p−→ H0(~θ0). Therefore, we have\n√ n(~θ∗ − ~θ) = −H−10 (~θ0)( √ n∇CLLM(~θ0, P )),\nwhich implies that Var[ √ n(~θ∗ − ~θ)] = H−10 ( ~θ0)Var[∇CLLM(~θ0, R)]H−10 (~θ0)."
  }, {
    "heading": "7. Consistency of RBCML",
    "text": "Formal proofs of theorems in this section depends on a series of lemmas, which can be found in the appendix. The full proofs can also be found in the appendix.\nTheorem 6 RBCML(G,Wu) is consistent for PlackettLuce if and only if the breaking is weighted union of positionk breakings.\nProof sketch: The “if\" direction is proved in (Khetan & Oh,\n2016b). We only prove the “only if\" direction by induction on m. When m = 2, the only breaking is the comparison between the two alternatives. The conclusion holds.\nSuppose it holds for m = l, then when m = l + 1, we first prove a lemma which says that by restricting G to any set of continuous positions, the theorem must hold for the subgraph. Then, we focus on G[2,m], which is the subgraph of G on {2,. . . ,m}. G[2,m] must be a weighted union of position-k breakings. Then we focus on G[1,m−1]. The only remaining case is to prove that the weight on edge {1,m} is the same as the weight on edges {1, i} for all i ≤ m− 1.\nSuppose for the sake of contradiction this is not true, then we can subtract a weighted union of position-k breakings from the graph, so that the remaining graph has a single edge {1,m}. We then prove that such an single-edge breaking is inconsistent by proving that (7) is not satisfied, which leads to a contradiction.\nTheorem 7 Let π1, π2, . . . , πm denote the utility distributions for a symmetric RUM. Suppose there exists πi s.t. (1) (lnπi(x))′ is monotonically decreasing, and (2) limx→−∞(lnπi(x))\n′ →∞. Then, RBCML(G,Wu) is consistent if and only if G is uniform.\nProof sketch: Define the single-edge breaking G1 = {g1m = 1}. We first prove RBCML(G1,Wu) is not consistent. Then we prove the theorem by induction on m. m = 2 is trivial because the only breaking is uniform. For m = 3, we first prove that the single-edge breaking G1 = {g13 = 1} is not consistent. Suppose the breaking is G = {g12 = x, g23 = y, g13 = z}. Let G∗ = {g12 = y, g23 = x, g13 = z}. We prove that RBCML(G∗,Wu) is consistent forM∗, which is the RUM obtained fromM by flipping the shapes of the utility distributions. BecauseM is symmetric, we haveM∗ = M. Then we prove that RBCML(G + G∗,Wu) is consistent. If x+ y < 2z, We subtract (x+ y)Gu from G + G∗ and get a consistent breaking (2z − (x+ y))G1, which is a contradiction. For the case where x+ y = 2z we use the premise in the theorem statement to directly prove that the breaking is inconsistent.\nSuppose the theorem holds for m = k. When m = k + 1, W.l.o.g. we let π2 satisfy the conditions that (lnπi(x))′ is monotonically decreasing and limx→−∞(lnπi(x))′ →∞. Let θ1 = L, θm = −L, and θ2 = . . . = θm−1 = 0. So when L→∞, with probability goes to 1, a1 is ranked at the top and am is ranked at the bottom. We then focus on G[2,m] and G[1,m−1]. By induction hypothesis, G[2,m] (respectively, G[1,m−1]) is either uniform or empty. If G[2,m] is empty, then G[1,m−1] is also empty. Because G is nonempty, we must have G = CG1, where C > 0. This is a contradiction. If G[2,m] is uniform but G is not uniform, then the single edge breaking G1 must be consistent, which is a contradiction.\nCorollary 1 Theorem 7 holds for any RUM with symmetric distributions where any single distribution is Gaussian.\nThe following two theorems give stronger characterizations by leveraging Theorems 6 and 7.\nTheorem 8 RBCML(G,W) for Plackett-Luce is consistent if and only if G is the weighted union of position-k breakings andW is connected and symmetric.\nTheorem 9 Let π be any symmetric distribution that satisfies the condition in Theorem 7. Then RBCML(G,W) is consistent for RUM(π) if and only if G is uniform andW is connected and symmetric.\nThe proofs for Theorems 8 and 9 are similar. The “if\" direction can be proved by verifying that the ground truth parameter is the solution to (7). For the “only if\" direction, we first prove that consistency of RBCML(G,W) implies consistency of RBCML(G,Wu), which further implies G is the weighted union of position-k breakings for PLs (Theorem 6) or uniform breaking for RUMs (Theorem 7). Given this condition on G, we prove that W must be connected and symmetric."
  }, {
    "heading": "8. The RBCML Framework",
    "text": "The asymptotic covariance of RBCML depends on G and W . The optimal G and W depend on the ground truth parameter ~θ02, which is exactly what we want. To tackle this problem, we propose the adaptive RBCML framework, guided by our Theorems 8 and 9 and shown as Algorithm\n2Khetan & Oh (2016b) proposed a breaking G, which is not a function of ~θ0.\n1. In this algorithm, G andW are iteratively updated given the estimate of ~θ from the previous iteration. Algorithm 1 Adaptive RBCML Input: Profile P of n rankings, the number of iterations T , the heuristics of breaking G(~θ) and the weightsW(~θ). Output: Estimated parameter ~θ∗. Initialize ~θ(0) = ~0\n1: for t = 1 to T do 2: Compute G(~θ(t−1)) andW(~θ(t−1)). 3: Estimate ~θ(t) using G(~θ(t−1)) and W(~θ(t−1)) by maximizing (1) (or (2) for Plackett-Luce) 4: end for\nNo efficient way of computing the optimal G(~θ) andW(~θ) is known since the asymptotic covariance is generally hard to compute, where an expectation is taken over m! rankings. How to efficiently compute the optimal G andW is a promising future direction. In the experiments of this paper, we use Gu andWu for Gaussian RUMs since Gu is the only consistent breaking. For the Plackett-Luce model, we use the G proposed by Khetan & Oh (2016b) and a heuristic W(~θ) (See Section 9)."
  }, {
    "heading": "9. Experiments",
    "text": "We compare RBCML with state-of-the-art algorithms for both Gaussian RUMs (GMM algorithm by Azari Soufiani et al. (2014)) and the Plackett-Luce model (the I-LSR algorithm by Maystre & Grossglauser (2015) and the consistent rank-breaking algorithm by Khetan & Oh (2016b)). In both experiments, we generate synthetic datasets of full rankings over m = 10 alternatives. The ground truth parameter is generated uniformly at random between 0 and 5 and shifted\ns.t. θ10 = 0. For Gaussian RUMs, the utility distribution of ai is N(θi, 1). The results are averaged over 50000 trials.\nMetrics. We measure statistical efficiency by n × MSE, where n is the number of rankings in the dataset. We use n×MSE rather than the standard MSE, because it is easier to see the difference between algorithms w.r.t. the former. The reason is that n×MSE approaches a positive constant as n → ∞, due to asymptotic normality of RBCML. We use running time to measure computational efficiency of each algorithm.\nGaussian RUMs. We use a one-step (T = 1 in Algorithm 1) RBCML(Gu,Wu) for Gaussian RUMs and the results are shown in Figure 3. We use uniform breaking rather than other breakings because it is the only consistent breaking according to our theoretical results.\nWe observe that our RBCML outperforms the GMM algorithm by Azari Soufiani et al. (2014) w.r.t. both statistical efficiency and computational efficiency.\nThe Plackett-Luce Model. We use a two-step (T = 2 in Algorithm 1) RBCML, where the first step is exactly the algorithm by Khetan & Oh (2016b) (denoted by K-O Breaking). In the second step, we still use the breaking by Khetan & Oh (2016b) but propose a heuristicW(~θ). For any pair of alternatives ai1 and ai2 , we let wi1i2 = wi2i1 = 1 |θi1−θi2 |+4\n. The intuition is that we should put a higher weight on the pair of alternatives that are closer to each other. Moreover, we use the output of the first step as the starting point of the second step optimization to improve computational efficiency.\nThe results are shown in Figure 2. We use 2-LSR to denote the two-iteration I-LSR algorithms by Maystre & Grossglauser (2015). LSR (one-iteration I-LSR) results are not\nshown because of the high n×MSE and runtime for large n. The “CR bound\" line is n times the trace of Cramér-Rao bound (Cramér, 1946; Rao, 1945), which is the lower bound of the covariance matrix of any unbiased estimator. Because Cramér-Rao bound decreases at the rate of 1/n, the CR bound line is horizontal. Since RBCML is not necessarily unbiased, the Cramér-Rao bound is not a lower bound for RBCML.\nWe observe that on datasets with large numbers of rankings (“ \" means “is better than\"): • Statistical efficiency: 2-LSR RBCML K-O Breaking. • Runtime: K-O Breaking RBCML 2-LSR.\nBeyond the experiments. We have only shown the RBCML with simple G andW . Other configurations of G andW can potentially have better performances or achieve other tradeoffs. Exploring RBCMLs for Gaussian RUMs, the Plackett-Luce model, as well as other RUMs is an interesting direction for future work."
  }, {
    "heading": "10. Summary and Future Work",
    "text": "We propose a flexible rank-breaking-then-compositemarginal-likelihood (RBCML) framework for learning RUMs. We characterize conditions for the objective function to be strictly log-concave, and for RBCML to be consistent and asymptotically normal. Experiments show that RBCML for Gaussian RUMs improve both statistical efficiency and computational efficiency, and the proposed RBCML for the Plackett-Luce model is competitive against state-of-the-art algorithms in that it provides a tradeoff between statistical efficiency and computational efficiency. For future work we plan to find efficient ways to compute optimal choices of G andW , and to extend the algorithm to partial orders."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank all anonymous reviewers for helpful comments and suggestions. This work is supported by NSF #1453542 and ONR #N00014-17-1-2621."
  }],
  "year": 2018,
  "references": [{
    "title": "Random utility theory for social choice",
    "authors": ["Azari Soufiani", "Hossein", "Parkes", "David C", "Xia", "Lirong"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2012
  }, {
    "title": "Generalized method-of-moments for rank aggregation",
    "authors": ["Azari Soufiani", "Hossein", "Chen", "William", "Parkes", "David C", "Xia", "Lirong"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS), Lake Tahoe, NV,",
    "year": 2013
  }, {
    "title": "Preference Elicitation For General Random Utility Models",
    "authors": ["Azari Soufiani", "Hossein", "Parkes", "David C", "Xia", "Lirong"],
    "venue": "In Proceedings of Uncertainty in Artificial Intelligence (UAI),",
    "year": 2013
  }, {
    "title": "Computing Parametric Ranking Models via RankBreaking",
    "authors": ["Azari Soufiani", "Hossein", "Parkes", "David C", "Xia", "Lirong"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning, Beijing,",
    "year": 2014
  }, {
    "title": "Stability of the prékopaleindler",
    "authors": ["Ball", "Keith", "Böröczky", "Károly"],
    "venue": "inequality. Mathematika,",
    "year": 2010
  }, {
    "title": "Automobile prices in market",
    "authors": ["Berry", "Steven", "Levinsohn", "James", "Pakes", "Ariel"],
    "venue": "equilibrium. Econometrica,",
    "year": 1995
  }, {
    "title": "Flexible model structures for discrete choice analysis",
    "authors": ["Bhat", "Chandra R", "Eluru", "Naveen", "Copperman", "Rachel B"],
    "venue": "Handbook of Transport Modelling,",
    "year": 2007
  }, {
    "title": "The Use of Random Utility in Modeling Rural Health Care Demand: Discussion",
    "authors": ["Bockstael", "Nancy E"],
    "venue": "American Journal of Agricultural Economics,",
    "year": 1999
  }, {
    "title": "Spectral MLE: Top-k rank aggregation from pairwise comparisons",
    "authors": ["Chen", "Yuxin", "Suh", "Changho"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Mathematical methods of statistics",
    "authors": ["Cramér", "Harald"],
    "year": 1946
  }, {
    "title": "Critère De Convexité Et Inégalités Intégrales",
    "authors": ["Dubuc", "Serge"],
    "venue": "Annales de l’institut Fourier,",
    "year": 1977
  }, {
    "title": "MM algorithms for generalized BradleyTerry models",
    "authors": ["Hunter", "David R"],
    "venue": "In The Annals of Statistics,",
    "year": 2004
  }, {
    "title": "Computational and statistical tradeoffs in learning to rank",
    "authors": ["Khetan", "Ashish", "Oh", "Sewoong"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Data-driven rank breaking for efficient rank aggregation",
    "authors": ["Khetan", "Ashish", "Oh", "Sewoong"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Composite likelihood methods",
    "authors": ["Lindsay", "Bruce G"],
    "venue": "Contemporary Mathematics,",
    "year": 1988
  }, {
    "title": "Individual Choice Behavior: A Theoretical Analysis",
    "authors": ["Luce", "Robert Duncan"],
    "year": 1959
  }, {
    "title": "Fast and accurate inference of plackett–luce models",
    "authors": ["Maystre", "Lucas", "Grossglauser", "Matthias"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Iterative ranking from pair-wise comparisons",
    "authors": ["Negahban", "Sahand", "Oh", "Sewoong", "Shah", "Devavrat"],
    "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2012
  }, {
    "title": "Adaptive Polling and Information Aggregation",
    "authors": ["Pfeiffer", "Thomas", "Gao", "Xi Alice", "Mao", "Andrew", "Chen", "Yiling", "Rand", "David G"],
    "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),",
    "year": 2012
  }, {
    "title": "The analysis of permutations",
    "authors": ["Plackett", "Robin L"],
    "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
    "year": 1975
  }, {
    "title": "Information and accuracy attainable in the estimation of statistical parameters",
    "authors": ["Rao", "C. Radhakrishna"],
    "venue": "Bull Calcutta. Math. Soc.,",
    "year": 1945
  }, {
    "title": "Log-concavity and strong log-concavity: A review",
    "authors": ["Saumard", "Adrien", "Wellner", "Jon A"],
    "venue": "Statistics Surveys,",
    "year": 2014
  }, {
    "title": "A law of comparative judgement",
    "authors": ["Thurstone", "Louis Leon"],
    "venue": "Psychological Review,",
    "year": 1927
  }, {
    "title": "On composite marginal likelihoods",
    "authors": ["Varin", "Cristiano"],
    "venue": "Advances in Statistical Analysis,",
    "year": 2008
  }, {
    "title": "RankingOriented Collaborative Filtering: A Listwise Approach",
    "authors": ["Wang", "Shuaiqiang", "Huang", "Shanshan", "Liu", "Tie-Yan", "Ma", "Jun", "Chen", "Zhumin", "Veijalainen", "Jari"],
    "venue": "ACM Transactions on Information Systems,",
    "year": 2016
  }, {
    "title": "On the robustness of maximum composite likelihood estimate",
    "authors": ["Xu", "Ximing", "N. Reid"],
    "venue": "Journal of Statistical Planning and Inference,",
    "year": 2011
  }, {
    "title": "A cost-effective framework for preference elicitation and aggregation",
    "authors": ["Zhao", "Zhibing", "Li", "Haoming", "Wang", "Junming", "Kephart", "Jeffrey", "Mattei", "Nicholas", "Su", "Hui", "Xia", "Lirong"],
    "venue": "In Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI-18),",
    "year": 2018
  }, {
    "title": "Learning mixtures of random utility models",
    "authors": ["Zhao", "Zhibing", "Villamil", "Tristan", "Xia", "Lirong"],
    "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence",
    "year": 2018
  }],
  "id": "SP:c4a7f897a7a9aa11120691eb9a68d15f3b5c4406",
  "authors": [{
    "name": "Zhibing Zhao",
    "affiliations": []
  }, {
    "name": "Lirong Xia",
    "affiliations": []
  }],
  "abstractText": "We propose a novel and flexible rank-breakingthen-composite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model. We characterize conditions for the objective function of RBCML to be strictly log-concave by proving that strict log-concavity is preserved under convolution and marginalization. We characterize necessary and sufficient conditions for RBCML to satisfy consistency and asymptotic normality. Experiments on synthetic data show that RBCML for Gaussian RUMs achieves better statistical efficiency and computational efficiency than the state-of-the-art algorithm and our RBCML for the Plackett-Luce model provides flexible tradeoffs between running time and statistical efficiency.",
  "title": "Composite Marginal Likelihood Methods for Random Utility Models"
}