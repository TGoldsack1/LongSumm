{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2116–2125 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2116"
  }, {
    "heading": "1 Introduction",
    "text": "Recurrent Neural Networks (RNNs), in particular Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997), have demonstrated remarkable accomplishments in Natural Language Processing (NLP) in recent years. Several tasks such as information extraction, question answering, and machine translation have benefited from them. However, in their vanilla forms, these networks are constrained by the sequential order of tokens in a sentence. To mitigate this limitation, structural (dependency or constituency) information in a sentence was exploited and witnessed partial success in various tasks (Goller and Kuchler, 1996; Yamada and\n1Our code for experiments on the SICK dataset is publicly available at https://github.com/amulyahwr/ acl2018\nKnight, 2001; Quirk et al., 2005; Socher et al., 2011; Tai et al., 2015).\nOn the other hand, alignment techniques (Brown et al., 1993) and attention mechanisms (Bahdanau et al., 2014) act as a catalyst to augment the performance of classical Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) models, respectively. In short, both approaches focus on sub-strings of source sentence which are significant for predicting target words while translating. Currently, the combination of linear RNNs/LSTMs and attention mechanisms has become a de facto standard architecture for many NLP tasks.\nAt the intersection of sentence encoding and attention models, some interesting questions emerge: Can attention mechanisms be employed on tree structures, such as Tree-LSTMs (Tai et al., 2015)? If yes, what are the possible tree-based attention models? Do different tree structures (in particular constituency vs. dependency) have different behaviors in such models? With these questions in mind, we present our investigation and findings in the context of semantic relatedness tasks."
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 Long Short-Term Memory Networks (LSTMs)",
    "text": "Concisely, an LSTM network (Hochreiter and Schmidhuber, 1997) (Figure 1) includes a memory cell at each time step which controls the amount of information being penetrated into the cell, neglected, and yielded by the cell. Various LSTM networks (Greff et al., 2017) have been explored till now; we focus on one representative form. To be more precise, we consider a LSTM memory cell involving: an input gate it, a forget gate ft, and an output gate ot at time step t. Apart from\nthe hidden state ht−1 and input embedding wt of the current word, the recursive function in LSTM also takes the previous time’s memory cell state, ct−1, into account, which is not the case in simple RNN. The following equations summarize a LSTM memory cell at time step t:\nit = σ(wtW i + ht−1R i + bi) (1)\nft = σ(wtW f + ht−1R f + bf ) (2)\not = σ(wtW o + ht−1R o + bo) (3)\nut = tanh(wtW u + ht−1R u + bu) (4)\nct = it ut + ft ct−1 (5)\nht = ot tanh(ct) (6)\nwhere:\n• (W i, W f , W o, Wu) ∈ RD x d represent input weight matrices, where d is the dimension of the hidden state vector and D is the dimension of the input word embedding, wt .\n• (Ri, Rf , Ro, Ru) ∈ Rd x d represent recurrent weight matrices and (bi, bf , bo, bu) ∈ Rd represent biases.\n• ct ∈Rd is the new memory cell vector at time step t.\nAs can be seen in Eq. 5, the input gate it limits the new information, ut, by employing the element wise multiplication operator . Moreover, the forget gate ft regulates the amount of information from the previous state ct−1. Therefore, the current memory state ct includes both new and previous time step’s information but partially.\nA natural extension of LSTM network is a bidirectional LSTM (bi-LSTM), which lets the sequence pass through the architecture in both directions and aggregate the information at each time step. Again, it strictly preserves the sequential nature of LSTMs."
  }, {
    "heading": "2.2 Linguistically Motivated Sentence Structures",
    "text": "Most computational linguists have developed a natural inclination towards hierarchical structures of natural language, which follow guidelines collectively referred to as syntax. Typically, such structures manifest themselves in parse trees. We investigate two popular forms: Constituency and Dependency trees."
  }, {
    "heading": "2.2.1 Constituency structure",
    "text": "Briefly, constituency trees (Figure 2:a) indicate a hierarchy of syntactic units and encapsulate phrase grammar rules. Moreover, these trees explicitly demonstrate groups of phrases (e.g., Noun Phrases) in a sentence. Additionally, they discriminate between terminal (lexical) and non-terminal nodes (non-lexical) tokens."
  }, {
    "heading": "2.2.2 Dependency structure",
    "text": "In short, dependency trees (Figure 2:b) describe the syntactic structure of a sentence in terms of the words (lemmas) and associated grammatical relations among the words. Typically, these dependency relations are explicitly typed, which makes the trees valuable for practical applications such as information extraction, paraphrase detection and semantic relatedness."
  }, {
    "heading": "2.3 Tree Long Short-Term Memory Network (Tree-LSTM)",
    "text": "Child-Sum Tree-LSTM (Tai et al., 2015) is an epitome of structure-based neural network which explicitly capture the structural information in a sentence. Tai et al. demonstrated that information at\na parent node can be consolidated selectively from each of its child node. Architecturally, each gated vector and memory state update of the head node is dependent on the hidden states of its children in the Tree-LSTM. Assuming a good tree structure of a sentence, each node j of the structure incorporates the following equations.:\nh̃j = ∑\nk∈C(j)\nhk (7)\nij = σ(wjW i + h̃jR i + bi) (8)\nfjk = σ(wjW f + hkR f + bf ) (9)\noj = σ(wjW o + h̃jR o + bo) (10)\nuj = tanh(wjW u + h̃jR u + bu) (11) cj = ij uj + ∑\nk∈C(j)\nfjk ck (12)\nhj = oj tanh(cj) (13)\nwhere:\n• wj ∈ RD represents word embedding of all nodes in Dependency structure and only terminal nodes in Constituency structure. 2\n• (W i, W f , W o, Wu) ∈ RD x d represent input weight matrices.\n• (Ri, Rf , Ro, Ru) ∈ Rd x d represent recurrent weight matrices, and (bi, bf , bo, bu) ∈ Rd represent biases.\n2wj is ignored for non-terminal nodes in a Constituency structure by removing the wW terms in Equations 8-11.\n• cj ∈ Rd is the new memory state vector of node j.\n• C(j) is the set of children of node j.\n• fjk ∈ Rd is the forget gate vector for child k of node j.\nReferring to Equation 12, the new memory cell state, cj of node j, receives new information, uj , partially. More importantly, it includes the partial information from each of its direct children, set C(j), by employing the corresponding forget gate, fjk.\nWhen the Child-Sum Tree model is deployed on a dependency tree, it is referred to as Dependency Tree-LSTM, whereas a constituency-treebased instantiation is referred to as Constituency Tree-LSTM."
  }, {
    "heading": "2.4 Attention Mechanisms",
    "text": "Alignment models were first introduced in statistical machine translation (SMT) (Brown et al., 1993), which connect sub-strings in the source sentence to sub-strings in the target sentence.\nRecently, attention techniques (which are effectively soft alignment models) in neural machine translation (NMT) (Bahdanau et al., 2014) came into prominence, where attention scores are calculated by considering words of source sentence while decoding words in target language. Although effective attention mechanisms (Luong et al., 2015) such as Global Attention Model (GAM) (Figure 4) and Local Attention Model (LAM) have been developed, such techniques have not been explored over Tree-LSTMs."
  }, {
    "heading": "3 Inter-Sentence Attention on Tree-LSTMs",
    "text": "We present two types of tree-based attention models in this section. With trivial adaptation, they can\nbe deployed in the sequence setting (degenerated trees)."
  }, {
    "heading": "3.1 Modified Decomposable Attention (MDA)",
    "text": "Parikh et al. (2016)’s original decomposable intersentence attention model only used word embeddings to construct the attention matrix, without any structural encoding of sentences. Essentially, the model incorporated three components:\nAttend: Input representations (without sequence or structural encoding) of both sentences, L and R, are soft-aligned.\nCompare: A set of vectors is produced by separately comparing each sub-phrase of L to subphrases in R. Vector representation of each subphrase in L is a non-linear combination of representation of word in sentence L and its aligned sub-phrase in sentence R. The same holds true for the set of vectors for sentence R.\nAggregate: Both sets of sub-phrases vectors are summed up separately to form final sentence representation of sentence L and sentence R.\nWe decide to augment the original decomposable inter-sentence attention model and generalize it into the tree (and sequence) setting. To be more specific, we consider two input sequences: L = (l1, l2....llenL), R = (r1, r2....rlenR) and their corresponding input representations: L̄ = (l̄1, l̄2....l̄lenL), R̄ = (r̄1, r̄2....r̄lenR); where lenL and lenR represents number of words in L and R, respectively."
  }, {
    "heading": "3.1.1 MDA on dependency structure",
    "text": "Let’s assume sequences L andR have dependency tree structures DL and DR. In this case, lenL and lenR represents number of nodes in DL and DR, respectively. After using a Tree-LSTM to encode tree representations, which results in: D\n′ L = (l̄ ′ 1,\nl̄ ′ 2....l̄ ′ lenL ), D ′ R = (r̄ ′ 1, r̄ ′ 2....r̄ ′ lenR\n), we gather unnormalized attention weights, eij and normalize them as follows:\neij = l̄ ′ i(r̄ ′ j) T (14)\nβi = lenR∑ j=1 exp(eij)∑lenR k=1 exp(eik) ∗ r̄′j (15)\nαj = lenL∑ i=1 exp(eij)∑lenL k=1 exp(ekj) ∗ l̄′i (16)\nFrom the equations above, we can infer that the attention matrix will have a dimension lenL\nx lenR. In contrast to the original model, we compute the final representations of the each sentence by concatenating the LSTM-encoded representation of root with the attention-weighted representation of the root 3:\nh ′′ L = G([l̄ ′ rootL ;βrootL ]) (17)\nh ′′ R = G([r̄ ′ rootR ;αrootR ]) (18)\nwhere G is a feed-forward neural network. h ′′ L and h ′′ R are final vector representations of input sequences L and R, respectively."
  }, {
    "heading": "3.1.2 MDA on constituency structure",
    "text": "Let’s assume sequences L and R have constituency tree structures CL and CR. Moreover, assume CL and CR have total number of nodes as NL (> lenL) and NR (> lenR), respectively. As in 3.1.1, the attention mechanism is employed after encoding the trees CL and CR. While encoding trees, terminal and non-terminal nodes are handled in the same way as in the original TreeLSTM model (see 2.3).\nIt should be noted that we collect hidden states of all the nodes (NL and NR) individually in CL and CR during the encoding process. Hence, hidden states matrix will have dimension NL x d for tree CL whereas for tree CR, it will have dimension NR x d; where d is dimension of each hidden state. Therefore, attention matrix will have a dimension NL x NR. Finally, we employ Equations 14-18 to compute the final representations of sequences L and R."
  }, {
    "heading": "3.2 Progressive Attention (PA)",
    "text": "In this section, we propose a novel attention mechanism on Tree-LSTM, inspired by (Quirk et al., 2005) and (Yamada and Knight, 2001)."
  }, {
    "heading": "3.2.1 PA on dependency structure",
    "text": "Let’s assume a dependency tree structure of sentence L = (l1, l2....llenL) is available as DL; where lenL represents number of nodes in DL. Similarly, tree DR corresponds to the sentence R = (r1, r2....rlenR); where lenR represents number of nodes in DR.\nIn PA, the objective is to produce the final vector representation of tree DR conditional on the hidden state vectors of all nodes of DL. Similar to\n3In the sequence setting, we compute the corresponding representations for the last word in the sentence.\nthe encoding process in NMT, we encode R by attending each node of DR to all nodes in DL. Let’s name this process Phase1. Next, Phase2 is performed where L is encoded in the similar way to get the final vector representation of DL.\nReferring to Figure 5 and assuming Phase1 is being executed, a hidden state matrix, HL, is obtained by concatenating the hidden state vector of every node in tree DL, where the number of nodes inDL = 3. Next, treeDR is processed by calculating the hidden state vector at every node. Assume that the current node being processed is nR2 of DR, which has a hidden state vector, hR2. Before further processing, normalized weights are calculated based on hR2 and HL. Formally,\nHpj = stack[hpj ] (19)\nconpj = concat[Hpj , Hq] (20)\napj = softmax(tanh(conpjWc+b)∗Wa) (21)\nwhere:\n• p, q ∈ {L,R} and q 6= p\n• Hq ∈ Rx x d represents a matrix obtained by concatenating hidden state vectors of nodes in tree Dq; x is lenq of sentence q.\n• Hpj ∈Rx x d represents a matrix obtained by stacking hidden state, hpj , vertically x times.\n• conpj ∈ Rx x 2d represents the concatenated matrix.\n• apj ∈ Rx represents the normalized attention weights at node j of tree Dp; where Dp is the dependency structure of sentence p.\n• Wc ∈R2d x d and Wa ∈Rd represent learned weight matrices.\nThe normalized attention weights in above equations provide an opportunity to align the subtree at the current node, nR2, in DR to sub-trees available at all nodes in DL. Next, a gated mechanism is employed to compute the final vector representation at node nR2.\nFormally,\nh ′ pj = (x−1)∑ 0 ((1− apj) ∗Hq + (apj) ∗Hpj) (22)\nwhere:\n• h′pj ∈ Rd represents the final vector representation of node j in tree Dp\n• ∑(x−1)\n0 represents column-wise sum\nAssuming the final vector representation of tree DR is h ′ R, the exact same steps are followed for Phase2 with the exception that the entire process is now conditional on tree DR. As a result, the final vector representation of tree DL, h ′ L, is computed.\nLastly, the following equations are applied to vectors h\n′ L and h ′ R, before calculating the angle\nand distance similarity (see Section 4).\nh ′′ L = tanh(h ′ L + hL) (23)\nh ′′ R = tanh(h ′ R + hR) (24)\nwhere:\n• hL ∈ Rd represents the vector representation of tree DL without attention.\n• hR ∈Rd represents the vector representation of tree DR without attention."
  }, {
    "heading": "3.2.2 PA on constituency structure",
    "text": "Let CL and CR represent constituency trees of L and R, respectively; where CL and CR have total number of nodes NL (> lenL) and NR (> lenR). Additionally, let’s assume that trees CL and CR have the same configuration of nodes as in Section 3.1.2, and the encoding of terminal and nonterminal nodes follow the same process as in Section 3.1.2. Assuming we have already encoded all NL nodes of tree CL using Tree-LSTM, we will have the hidden state matrix, HL, with dimension NL x d. Next, while encoding any node of CR, we consider HL which results in an attention vector having shape NL. Using Equations 19-22 4, we retrieve the final hidden state of the current node. Finally, we compute the representation of sentence R based on attention to sentence L. We perform Phase2 with the same process, except that we now condition on sentence R.\nIn summary, the progressive attention mechanism refers to all nodes in the other tree while encoding a node in the current tree, instead of waiting till the end of the structural encoding to establish cross-sentence attention, as was done in the decomposable attention model.\n4At this point, we will consider Cq and Cp instead of Dq and Dp, respectively, in Equations 19-22. Additionally, x will be equal to total number of nodes in the constituency tree."
  }, {
    "heading": "4 Evaluation Tasks",
    "text": "We evaluate our models on two tasks: (1) semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and (2) paraphrase detection for question pairs (Quora, 2017)."
  }, {
    "heading": "4.1 Semantic Relatedness for Sentence Pairs",
    "text": "In SemEval 2012, Task 6 and SemEval 2014, Task 1, every sentence pair has a real-valued score that depicts the extent to which the two sentences are semantically related to each other. Higher score implies higher semantic similarity between the two sentences. Vector representations h\n′′ L and h ′′ R\nare produced by using our Modified Decomp-Attn or Progressive-Attn models. Next, a similarity score, ŷ between h\n′′ L and h ′′ R is computed using the\nsame neural network (see below), for the sake of fair comparison between our models and the original Tree-LSTM (Tai et al., 2015).\nhx = h ′′ L h ′′ R (25)\nh+ = |h ′′ L − h ′′ R| (26)\nhs = σ(hxW x + h+W + + bh) (27)\np̂θ = softmax(hsW p + bp) (28)\nŷ = rT p̂θ (29)\nwhere:\n• rT = [1, 2..S]\n• hx ∈Rd measures the sign similarity between h\n′′ L and h ′′ R\n• h+ ∈ Rd measures the absolute distance between h\n′′ L and h ′′ R\nFollowing (Tai et al., 2015), we convert the regression problem into a soft classification. We also use the same sparse distribution, p, which was defined in the original Tree-LSTM to transform the gold rating for a sentence pair, such that y = rT p and ŷ = rT p̂θ ≈ y. The loss function is the KLdivergence between p and p̂:\nJ(θ) =\n∑m k=1KL(p\nk||p̂kθ) m + λ||θ||22 2 (30)\n• m is the number of sentence pairs in the dataset.\n• λ represents the regularization penalty."
  }, {
    "heading": "4.2 Paraphrase Detection for Question Pairs",
    "text": "In this task, each question pair is labeled as either paraphrase or not, hence the task is binary classification. We use Eqs. 25 - 28 to compute the\npredicted distribution p̂θ. The predicted label, ŷ, will be:\nŷ = arg maxyp̂θ (31)\nThe loss function is the negative log-likelihood: J(θ) = − ∑m k=1 y k log ŷk\nm + λ||θ||22 2 (32)"
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Semantic Relatedness for Sentence Pairs",
    "text": "We utilized two different datasets:\n• The Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al. (2014)), which contains a total of 9,927 sentence pairs. Specifically, the dataset has a split of 4500/500/4927 among training, dev, and test. Each sentence pair has a score S ∈ [1,5], which represents an average of 10 different human judgments collected by crowd-sourcing techniques.\n• The MSRpar dataset (Agirre et al., 2012), which consists of 1,500 sentence pairs. In this dataset, each pair is annotated with a score S ∈ [0,5] and has a split of 750/750 between training and test.\nWe used the Stanford Parsers (Chen and Manning, 2014; Bauer) to produce dependency and constituency parses of sentences. Moreover, we initialized the word embeddings with 300- dimensional Glove vectors (Pennington et al., 2014); the word embeddings were held fixed during training. We experimented with different optimizers, among which AdaGrad performed the best. We incorporated a learning rate of 0.025 and regularization penalty of 10−4 without dropout."
  }, {
    "heading": "5.2 Paraphrase Detection for Question Pairs",
    "text": "For this task, we utilized the Quora dataset (Iyer; Kaggle, 2017). Given a pair of questions, the objective is to identify whether they are semantic duplicates. It is a binary classification problem where a duplicate question pair is labeled as 1 otherwise as 0. The training set contains about 400,000 labeled question pairs, whereas the test set consists of 2.3 million unlabeled question pairs. Moreover, the training dataset has only 37% positive samples; average length of a question is 10 words. Due to hardware and time constraints, we extracted 50,000 pairs from the original training while maintaining the same positive/negative\nratio. A stratified 80/20 split was performed on this subset to produce the training/test set. Finally, 5% of the training set was used as a validation set in our experiments.\nWe used an identical training configuration as for the semantic relatedness task since the essence of both the tasks is practically the same. We also performed pre-processing to clean the data and then parsed the sentences using Stanford Parsers."
  }, {
    "heading": "6 Results",
    "text": ""
  }, {
    "heading": "6.1 Semantic Relatedness for Sentence Pairs",
    "text": "Table 1 summarizes our results. According to (Marelli et al., 2014), we compute three evaluation metrics: Pearson’s r, Spearman’s ρ and Mean Squared Error (MSE). We compare our attention models against the original Tree-LSTM (Tai et al., 2015), instantiated on both constituency trees and dependency trees. We also compare earlier baselines with our models, and the best results are in bold. Since Tree-LSTM is a generalization of Linear LSTM, we also implemented our attention models on Linear Bidirectional LSTM (BiLSTM). All results are average of 5 runs. It is witnessed that the Progressive-Attn mechanism combined with Constituency Tree-LSTM is overall the strongest contender, but PA failed to yield any performance gain on Dependency Tree-LSTM in either dataset."
  }, {
    "heading": "6.2 Paraphrase Detection for Question Pairs",
    "text": "Table 2 summarizes our results where best results are highlighted in bold within each category. It should be noted that Quora is a new dataset and we have done our analysis on only 50,000 samples. Therefore, to the best of our knowledge, there is no published baseline result yet. For this task, we considered four standard evaluation metrics: Accuracy, F1-score, Precision and Recall. The Progressive-Attn + Constituency Tree-LSTM model still exhibits the best performance by a small margin, but the Progressive-Attn mechanism works surprisingly well on the linear bi-LSTM."
  }, {
    "heading": "6.3 Effect of the Progressive Attention Model",
    "text": "Table 3 illustrates how various models operate on two sentence pairs from SICK test dataset. As we can infer from the table, the first pair demonstrates an instance of the active-passive voice phenomenon. In this case, the linear LSTM and vanilla Tree-LSTMs really struggle to perform.\nHowever, when our progressive attention mechanism is integrated into syntactic structures (dependency or constituency), we witness a boost in the semantic relatedness score. Such desirable behavior is consistently observed in multiple activepassive voice pairs. The second pair points to a possible issue in data annotation. Despite the presence of strong negation, the gold-standard score is 4 out of 5 (indicating high relatedness). Interestingly, the Progressive-Attn + Dependency Tree-\nLSTM model favors the negation facet and outputs a low relatedness score."
  }, {
    "heading": "7 Discussion",
    "text": "In this section, let’s revisit our research questions in light of the experimental results.\nFirst, can attention mechanisms be built for Tree-LSTMs? Does it work? The answer is yes. Our novel progressive-attention Tree-LSTM model, when instantiated on constituency trees,\nsignificantly outperforms its counterpart without attention. The same model can also be deployed on sequences (degenerated trees) and achieve quite impressive results.\nSecond, the performance gap between the two attention models is quite striking, in the sense that the progressive model completely dominate its decomposable counterpart. The difference between the two models is the pacing of attention, i.e., when to refer to nodes in the other tree while encoding a node in the current tree. The progressive attention model garners it’s empirical superiority by attending while encoding, instead of waiting till the end of the structural encoding to establish cross-sentence attention. In retrospect, this may justify why the original decomposable attention model in (Parikh et al., 2016) achieved competitive results without any LSTM-type encoding. Effectively, they implemented a naive version of our progressive attention model.\nThird, do structures matter/help? The overall trend in our results is quite clear: the tree-based models exhibit convincing empirical strength; linguistically motivated structures are valuable. Admittedly though, on the relatively large Quora dataset, we observe some diminishing returns of incorporating structural information. It is not counter-intuitive that the sheer size of data can possibly allow structural patterns to emerge, hence lessen the need to explicitly model syntactic structures in neural architectures.\nLast but not least, in trying to assess the impact of attention mechanisms (in particular the progressive attention model), we notice that the extra mileage gained on different structural encodings is different. Specifically, performance lift on Linear Bi-LSTM > performance lift on Constituency Tree-LSTM, and PA struggles to see performance lift on dependency Tree-LSTM. Interestingly enough, this observation is echoed by an earlier study (Gildea, 2004), which showed that tree-based alignment models work better on con-\nstituency trees than on dependency trees. In summary, our results and findings lead to several intriguing questions and conjectures, which call for investigation beyond the scope of our study:\n• Is it reasonable to conceptualize attention mechanisms as an implicit form of structure, which complements the representation power of explicit syntactic structures?\n• If yes, does there exist some trade-off between the modeling efforts invested into syntactic and attention structures respectively, which seemingly reveals itself in our empirical results?\n• The marginal impact of attention on dependency Tree-LSTMs suggests some form of saturation effect. Does that indicate a closer affinity between dependency structures (relative to constituency structures) and compositional semantics (Liang et al., 2013)?\n• If yes, why is dependency structure a better stepping stone for compositional semantics? Is it due to the strongly lexicalized nature of the grammar? Or is it because the dependency relations (grammatical functions) embody more semantic information?"
  }, {
    "heading": "8 Conclusion",
    "text": "In conclusion, we proposed a novel progressive attention model on syntactic structures, and demonstrated its superior performance in semantic relatedness tasks. Our work also provides empirical ingredients for potentially profound questions and debates on syntactic structures in linguistics."
  }],
  "year": 2018,
  "references": [{
    "title": "Semeval-2012 task 6: A pilot on semantic textual similarity",
    "authors": ["Eneko Agirre", "Mona Diab", "Daniel Cer", "Aitor Gonzalez-Agirre."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main",
    "year": 2012
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR’2015.",
    "year": 2014
  }, {
    "title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity",
    "authors": ["Johannes Bjerva", "Johan Bos", "Rob Van der Goot", "Malvina Nissim."],
    "venue": "Proceedings of the 8th International Workshop on Semantic Eval-",
    "year": 2014
  }, {
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."],
    "venue": "Computational linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750.",
    "year": 2014
  }, {
    "title": "Dependencies vs",
    "authors": ["Daniel Gildea."],
    "venue": "constituents for tree-based alignment. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2004
  }, {
    "title": "Learning task-dependent distributed representations by backpropagation through structure",
    "authors": ["Christoph Goller", "Andreas Kuchler."],
    "venue": "Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE.",
    "year": 1996
  }, {
    "title": "Lstm: A search space odyssey",
    "authors": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutnı́k", "Bas R Steunebrink", "Jürgen Schmidhuber"],
    "venue": "IEEE transactions on neural networks and learning systems",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment",
    "authors": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh."],
    "venue": "Proceedings of the 8th International Workshop on Semantic Evalu-",
    "year": 2014
  }, {
    "title": "Illinois-lh: A denotational and distributional approach to semantics",
    "authors": ["Alice Lai", "Julia Hockenmaier."],
    "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334.",
    "year": 2014
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "Comput. Linguist., 39(2):389–446.",
    "year": 2013
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1508.04025.",
    "year": 2015
  }, {
    "title": "A sick cure for the evaluation of compositional distributional semantic models",
    "authors": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"],
    "venue": "In LREC,",
    "year": 2014
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur P Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "arXiv preprint arXiv:1606.01933.",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Dependency treelet translation: Syntactically informed phrasal smt",
    "authors": ["Chris Quirk", "Arul Menezes", "Colin Cherry."],
    "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 271–279. Association for Computa-",
    "year": 2005
  }, {
    "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
    "authors": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "NIPS, volume 24, pages 801–809.",
    "year": 2011
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1503.00075.",
    "year": 2015
  }, {
    "title": "Towards universal paraphrastic sentence embeddings",
    "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "arXiv preprint arXiv:1511.08198.",
    "year": 2015
  }, {
    "title": "A syntaxbased statistical translation model",
    "authors": ["Kenji Yamada", "Kevin Knight."],
    "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.",
    "year": 2001
  }, {
    "title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment",
    "authors": ["Jiang Zhao", "Tiantian Zhu", "Man Lan."],
    "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages",
    "year": 2014
  }],
  "id": "SP:a119857d72d1bfa5333018d01112b1238663a488",
  "authors": [{
    "name": "Amulya Gupta",
    "affiliations": []
  }, {
    "name": "Zhu Zhang",
    "affiliations": []
  }],
  "abstractText": "With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectionalLSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).1",
  "title": "To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness"
}