{
  "sections": [{
    "heading": "1. Introduction",
    "text": "This paper gives new, precise definitions of the ANSISQL isolation levels [6]. Unlike previous proposals [13, 6, 8], the new definitions are both correct (they rule out all bad histories) and implementation-independent. Our specifications allow a wide range of concurrency control techniques, including locking, optimistic techniques [20, 2, 5], and multi-version mechanisms [9, 24]. Thus, they meet the goals of ANSI-SQL and could be used as an isolation standard.\nThe concept of isolation levels was first introduced in [13] under the name Degrees of Consistency. The goal of this work was to provide improved concurrency for workloads by sacrificing the guarantees of perfect isolation. The work\nResearch of A. Adya and B. Liskov was supported in part by the ARPA of the Department of Defense under contract DABT63-95-C-0005, monitored by Fort Huachuca US Army Intelligence Center, and by NSF under Grant IIS-98-02066. This research was done when Atul Adya was at MIT. Research of P. O’Neil was supported by the NSF under Grant IRI-97-11374.\nin [13] and some refinements suggested by [11] set the stage for the ANSI/ISO SQL-92 definitions for isolation levels [6], where the goal was to develop a standard that was implementation-independent. However, a subsequent paper [8] showed that the definitions provided in [6] were ambiguous. That paper proposed different definitions that avoided the ambiguity problems, but, as stated in [8], these definitions were simply “disguised versions of locking” and therefore disallow optimistic and multi-version mechanisms. Thus, these definitions fail to meet the goals of ANSI-SQL with respect to implementation-independence.\nThus, we have a problem: the standard is intended to be implementation-independent, but lacks a precise definition that meets this goal. Implementation-independence is important since it provides flexibility to implementors, which can lead to better performance. Optimism can outperform locking in some environments, such as large scale, widearea distributed systems [2, 15]; optimistic mechanisms are the schemes of choice for mobile environments; and Gemstone [22] and Oracle [24] provide serializability and Snapshot Isolation, respectively, using multi-version optimistic implementations. It is undesirable for the ANSI standard to rule out these implementations. For example, Gemstone provides serializability even though it does not meet the locking-based rules given in [8].\nThis paper presents new implementation-independent specifications that correct the problems with the existing definitions. Our definitions cover the weaker isolation levels that are in everyday use: Most database vendors and database programmers take advantage of levels below serializability levels to achieve better performance; in fact, READ COMMITTED is the default for some database products and database vendors recommend using this level instead of serializability if high performance is desired. Our definitions also enable database vendors to develop innovative implementations of the different levels using a wide variety of concurrency control mechanisms including locking, optimistic and multi-version mechanisms. Furthermore, our specifications handle predicate-based operations correctly\nat all isolation levels. Thus, the paper makes the following contributions:\nIt specifies the existing ANSI isolation levels in an implementation-independent manner. The definitions are correct (they rule out all bad histories). They are also complete (they allow all good histories) for serializability; in particular, they provide conflictserializability [9]. It is difficult to prove completeness for lower isolation levels, but we can easily show that our definitions are more permissive than those given in [8].\nOur specifications also handle predicates correctly in a flexible manner; earlier definitions were either lockbased or incomplete [8].\nOur approach can be used to define additional levels as well, including commercial levels such as Cursor Stability [11], and Oracle’s Snapshot Isolation and Read Consistency [24], and new levels; for example, we have developed an additional isolation level called PL-2+, which is the weakest level that guarantees consistent reads and causal consistency with respect to transactions. Details can be found in [1].\nOur definitions are given using a combination of constraints on transaction histories and graphs; we proscribe different types of cycles in a serialization graph at each isolation level. Our graphs are similar to those that have been used before for specifying serializability [9, 19, 14], semantics-based correctness criterion [4], and for defining extended transaction models [10]. Our approach is the first that applies these techniques to defining ANSI and commercial isolation levels. Our specifications are different from the multi-version theory presented in [9] since that work only describes conditions for serializability whereas we specify all ANSI/SQL-92 and other commercial isolation levels for multi-version systems. Furthermore, unlike our specifications, their definitions do not take predicates into account. Our work is also substantially different from the definitions presented in [8] since our specifications handle multi-version systems, optimistic systems and also deal with predicates in a correct and flexible manner at all isolation levels.\nRelaxed correctness conditions based on semantics and extended transaction models have been suggested in the past [10, 4, 17, 7]. By contrast, our work focuses on specifying existing ANSI and commercial isolation levels that are being used by large numbers of application programmers.\nThe rest of this paper is organized as follows. Section 2 discusses prior work in more detail. Section 3 shows that the current definitions are inadequate and motivates the need for our work. Section 4 describes our database model. Section 5 provides our definitions for the existing ANSI isolation levels. We close in Section 6 with a discussion of what we have accomplished."
  }, {
    "heading": "2. Previous Work",
    "text": "The original proposal for isolation levels [13] introduced four degrees of consistency, degrees 0, 1, 2 and 3, where degree 3 was the same as serializability. That paper, however, was concerned with locking schemes, and as a consequence the definitions were not implementation-independent.\nHowever, that work, together with the refinement of the levels provided by Date [11], formed the basis for the ANSI/ISO SQL-92 isolation level definitions [6]. The ANSI standard had implementation-independence as a goal and the definitions were supposed to be less constraining than earlier ones. The approach taken was to proscribe certain types of bad behavior called phenomena; more restrictive consistency levels disallow more phenomena and serializability does not permit any phenomenon. The isolation levels were named READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE; some of these levels were intended to correspond to the degrees of [13].\nThe work in [8] analyzed the ANSI-SQL standard and demonstrated several problems in its isolation level definitions: some phenomena were ambiguous, while others were missing entirely. It then provided new definitions. As with the ANSI-SQL standard, various isolation levels are defined by having them disallow various phenomena. The phenomena proposed by [8] are:\nP0: w1[x] ... w2[x] ... (c1 or a1) P1: w1[x] ... r2[x] ... (c1 or a1) P2: r1[x] ... w2[x] ... (c1 or a1) P3: r1[P] ... w2[y in P] ... (c1 or a1)\nProscribing P0 (which was missing in the ANSI-SQL definitions) requires that a transaction T2 cannot write an object x if an uncommitted transaction T1 has already modified x. This is simply a disguised locking definition, requiring T1 and T2 to acquire long write-locks. (Long-term locks are held until the transaction taking them commits; shortterm locks are released immediately after the transaction completes the read or write that triggered the lock attempt.) Similarly, proscribing P1 requires T1 to acquire a long writelock and T2 to acquire (at least) a short-term read-lock, and proscribing P2 requires the use of long read and write locks.\nPhenomenon P3 deals with the queries based on predicates. Proscribing P3 requires that a transaction T2 cannot modify a predicate P by inserting, updating, or deleting a row such that its modification changes the result of a query executed by an uncommitted transaction T1 based on predicate P; to avoid this situation, T1 acquires a long phantom read-lock [14] on predicate P.\nThus, these definitions only allow histories that would occur in a system using long/short read/write item/predicate locks. Since locking serializes transactions by preventing certain situations (e.g., two concurrent transactions both\nmodifying the same object), we refer to this approach as the preventative approach.\nFigure 1 summarizes the isolation levels as defined in [8] and relates them to a lock-based implementation. Thus the READ UNCOMMITTED level proscribes P0; READ COMMITTED proscribes P0 and P1; the REPEATABLE READ level proscribes P0 - P2; and SERIALIZABLE proscribes P0 - P3."
  }, {
    "heading": "3. Restrictiveness of Preventative Approach",
    "text": "We now show that the preventative approach is overly restrictive since it rules out optimistic and multi-version implementations. As mentioned, this approach disallows all histories that would not occur in a locking scheme and prevents conflicting operations from executing concurrently.\nThe authors in [8] wanted to ensure that multi-object constraints (e.g., constraints like x+ y = 10) are not observed as violated by transactions that request an isolation level such as serializability. They showed that histories such as H1 and H2 are allowed by one interpretation of the ANSI standard (at the SERIALIZABLE isolation level) even though they are non-serializable: H1: r1(x, 5) w1(x, 1) r2(x, 1) r2(y, 5) c2 r1(y, 5) w1(y, 9) c1 H2: r2(x, 5) r1(x, 5) w1(x, 1) r1(y, 5) w1(y, 9) c1 r2(y, 9) c2 In both cases, T2 observes an inconsistent state (it observes invariant x+ y = 10 to be violated). These histories are not allowed by the preventative approach; H1 is ruled out by P1 and H2 is ruled out by P2.\nOptimistic and multi-version mechanisms [2, 5, 9, 20, 22] that provide serializability also disallow non-serializable histories such as H1 and H2. However, they allow many legal histories that are not permitted by P0, P1, P2, and P3. Thus, the preventative approach disallows such implementations. Furthermore, it rules out histories that really occur in practical implementations.\nPhenomenon P0 can occur in optimistic implementations since there can be many uncommitted transactions modifying local copies of the same object concurrently; if necessary, some of them will be forced to abort so that serializability can be provided. Thus, disallowing P0 can rule out optimistic implementations.\nCondition P1 precludes transactions from reading updates by uncommitted transactions. Such reads are disallowed by many optimistic schemes, but they are desirable\nin mobile environments, where commits may take a long time if clients are disconnected from the servers [12, 16]; furthermore, reads from uncommitted transactions may be desirable in high traffic hotspots [23]. For example, in history H1, if T2 reads T1’s values for both x and y, it can be serialized after T1:\nH10 : r1(x, 5) w1(x, 1) r1(y, 5) w1(y, 9) r2(x, 1) r2(y, 9) c1 c2\nThe above history can occur in a mobile system, but P1 disallows it. In such a system, commits can be assumed to have happened “tentatively” at client machines [12, 16]; later transactions may observe modifications of those tentative transactions. When the client reconnects with the servers, its work is checked to determine if consistency has been violated and the relevant transactions are aborted. Of course, if dirty reads are allowed, cascading aborts can occur, e.g., in history H10 , T2 must abort if T1 aborts; this problem can be alleviated by using compensating actions [18, 26, 19].\nProscribing phenomenon P2 disallows a modification to an object that has been read by an uncommitted transaction (P3 rules out a similar situation with respect to predicates). As with P0, uncommitted transactions may read/write the same object concurrently in an optimistic implementation. There is no harm in allowing phenomenon P2 if transactions commit in the right order. For example, in history H2 given above, if T2 reads the old values of x and y, the transactions can be serialized in the order T2; T1:\nH20 : r2(x, 5) r1(x, 5) w1(x, 1) r1(y, 5) r2(y, 5) w1(y, 9) c2 c1\nThe real problem with the preventative approach is that the phenomena are expressed in terms of single-object histories. However, the properties of interest are often multiobject constraints. To avoid problems with such constraints, the phenomena need to restrict what can be done with individual objects more than is necessary. Our approach avoids this difficulty by using specifications that capture constraints on multiple objects directly. Furthermore, the definitions in the preventative approach are not applicable to multi-version systems since they are described in terms of objects rather than in terms of versions. On the other hand, our specifications deal with multi-version and single-version histories.\nThe approach in [8] only allows schemes that provide the same guarantees for running and committed transactions (a lock-based implementation does indeed have this\nproperty). However, many optimistic mechanisms provide weak guarantees to transactions as they run while providing strong guarantees such as serializability for committed transactions. Our definitions allow different isolation guarantees for committed and running transactions; in this paper, we only present guarantees for committed transactions."
  }, {
    "heading": "4. Database Model and Transaction Histories",
    "text": "We now describe our database model, transaction histories, and serialization graphs. We use a multi-version model similar to the one presented in [9]. However, unlike [9], our model incorporates predicates also. Furthermore, we allow predicate behavior that is possible in non-locking based systems."
  }, {
    "heading": "4.1. Database Model",
    "text": "The database consists of objects that can be read or written by transactions; in a relational database system, each row or tuple is an object. Each transaction reads and writes objects and indicates a total order in which these operations occur.\nAn object has one or more versions. However, transactions interact with the database only in terms of objects; the system maps each operation on an object to a specific version of that object. A transaction may read versions created by committed, uncommitted, or even aborted transactions; constraints imposed by some isolation levels will prevent certain types of reads, e.g., reading versions created by aborted transactions.\nWhen a transaction writes an object x, it creates a new version of x. A transaction Ti can modify an object multiple times; its first modification of object x is denoted by xi:1, the second by xi:2, and so on. Version xi denotes the final modification of x performed by Ti before it commits or aborts. A transaction’s last operation, commit or abort, indicates whether its execution was successful or not; there is at most one commit or abort operation for each transaction.\nThe committed state reflects the modifications of committed transactions. When transaction Ti commits, each version xi created by Ti becomes a part of the committed state and we say that Ti installs xi; the system determines the ordering of xi relative to other committed versions of x. If Ti aborts, xi does not become part of the committed state.\nConceptually, the initial committed state comes into existence as a result of running a special initialization transaction, Tinit. Transaction Tinit creates all objects that will ever exist in the database; at this point, each object x has an initial version, xinit, called the unborn version. When an application transaction creates an object x (e.g., by inserting a tuple in a relation), we model it as the creation of a visible version for x. Thus, a transaction that loads the\ndatabase creates the initial visible versions of the objects being inserted. When a transaction Ti deletes an object x (e.g., by deleting a tuple from some relation), we model it as the creation of a special dead version, i.e., in this case, xi is a dead version. Thus, object versions can be of three kinds — unborn, visible, and dead; the ordering relationship between these versions is discussed in Section 4.2.\nIf an object x is deleted from the committed database state and inserted later, we consider the two incarnations of x to be distinct objects. When a transaction Ti performs an insert operation, the system selects a unique object x that has never been selected for insertion before and Ti creates a visible version of x if it commits.\nWe assume object versions exist forever in the committed state to simplify the handling of inserts and deletes, i.e., we simply treat inserts/deletes as write (update) operations. An implementation only needs to maintain visible versions of objects, and a single-version implementation can maintain just one visible version at a time. Furthermore, application transactions in a real system access only visible versions."
  }, {
    "heading": "4.2. Transaction Histories",
    "text": "We capture what happens in an execution of a database system by a history. A history H over a set of transactions consists of two parts — a partial order of events E that reflects the operations (e.g., read, write, abort, commit) of those transactions, and a version order, , that is a total order on committed versions of each object.\nEach event in a history corresponds to an operation of some transaction, i.e., read, write, commit, or abort. A write operation on object x by transaction Ti is denoted by wi(xi) (or wi(xi:m)); if it is useful to indicate the value v being written into xi, we use the notation, wi(xi, v). When a transaction Tj reads a version of x that was created by Ti, we denote this as rj(xi) (or rj(xi:a)). If it is useful to indicate the value v being read, we use the notation rj(xi, v).\nThe partial order of events E in a history obeys the following constraints:\nIt preserves the order of all events within a transaction including the commit and abort events.\nIf an event rj(xi:m) exists in E, it is preceded by wi(xi:m) in E, i.e., a transaction Tj cannot read version xi:m of object x before it has been produced by Ti. Note that the version read by Tj is not necessarily the most recently installed version in the committed database state; also, Ti may be uncommitted when rj(xi:m) occurs.\nIf an event wi(xi:m) is followed by ri(xj) without an intervening event wi(xi:n) in E, xj must be xi:m. This condition ensures that if a transaction modifies object x and later reads x, it will observe its last update to x.\nThe history must be complete: if E contains a read or write event that mentions a transaction Ti, E must contains a commit or abort event for Ti.\nA history that is not complete can be completed by appending abort events for uncommitted transactions in E. Adding these events is intuitively correct since any implementation that allows a commit of a transaction that reads from an uncommitted transaction Ti can do so only if it is legal for Ti to abort later.\nFor convenience, we will present event histories in examples as a total order (from left to right) that is consistent with the partial order.\nThe second part of a history H is the version order, , that specifies a total order on versions of each object created by committed transactions in H; there is no ordering of versions due to uncommitted or aborted transactions. We also refer to versions due to committed transactions in H as committed versions. We impose two constraints on a history’s version order for different kinds of committed versions:\nthe version order of each object x contains exactly one initial version, xinit, and at most one committed dead version, xdead.\nxinit is x’s first version in its version order and xdead is its last version (if it exists); all committed visible versions are placed between xinit and xdead.\nWe additionally constrain the system to allow reads only of visible versions:\nif rj(xi) occurs in a history, then xi is a visible version.\nFor convenience, we will only show the version order for visible versions in our example histories; in cases where unborn or dead versions help in illustrating an issue, we will show some of these versions as well.\nThe version order in a history H can be different from the order of write or commit events in H. This flexibility is needed to allow certain optimistic and multi-version implementations where it is possible that a version xi is placed before version xj in the version order (xi xj) even though xi is installed in the committed state after version xj was installed. For example, in history Hwrite order,\nHwrite order: w1(x1) w2(x2) w2(y2) c1 c2 r3(x1) w3(x3) w4(y4) a4 [x2 x1]\nthe database system has chosen the version order x2 x1 even though T1 commits before T2. Note that there are no constraints on x3 (yet) or y4 since these versions correspond to uncommitted and aborted transactions, respectively. Note also that the naming of transactions does not indicate their commit order, e.g., in history Hwrite order, T2 is serialized before T1."
  }, {
    "heading": "4.3. Predicates",
    "text": "We now discuss how predicates are handled in our model. We assume that predicates are used with relations in a relational database system. There are three types of modification operations on relations: updates, inserts and deletes; inserts and deletes change the number of tuples in a relation.\nIn our model, the database is divided into relations and each tuple (and all its versions) exists in some relation. As before, unborn and dead versions exist for a tuple before the tuple’s insertion and after its deletion. An important point to note here is that a tuple’s relation is known in our model when the database is initialized by Tinit, i.e., before the tuple is inserted by an application transaction. Of course, this assumption is needed only at a conceptual level. In an implementation, the system need not know the relation of all tuples that will be created in the system; it just needs to know a tuple x’s relation when x is inserted in the database.\nA predicate P identifies a Boolean condition (e.g., as in the WHERE clause of a SQL statement) and the relations on which the condition has to be applied; one or more relations can be specified in P. All tuples that match this condition are read or modified depending on whether a predicate-based read or write is being considered.\nDefinition 1 : Version set of a predicate-based operation. When a transaction executes a read or write based on a predicate P, the system selects a version for each tuple in P’s relations. The set of selected versions is called the Version set of this predicate-based operation and is denoted by Vset(P).\nThe version set defines the state that is observed to evaluate a predicate P; as discussed later, P’s Boolean condition is applied on the versions in Vset(P) to determine which tuples satisfy P. Since we select a version for all possible tuples in P’s relations, this set will be very large (it includes unborn and possibly dead versions of some tuples). For convenience, in our examples we will only show visible versions in a version set; to better explain some examples, we will sometimes also show some unborn and dead versions.\nOur approach of observing some version of each tuple allows us to handle the phantom problem [14] in a simple manner. Of course, this does not constrain implementations to perform these observations; e.g., an implementation could use an index."
  }, {
    "heading": "4.3.1 Predicate-based Reads",
    "text": "If a transaction Ti performs reads based on a predicate P (e.g., in a SQL statement), the system (conceptually) accesses all versions in Vset(P). Then, the system determines which tuples match predicate P by evaluating P’s Boolean condition on the versions in Vset(P); tuples whose unborn\nand dead versions were selected in the previous step do not match. If the system reads the matched versions as part of the query, these reads show up as separate events in the history. Thus, a query based on a predicate P by Ti is represented in a history as ri(P: Vset(P)) ri(xj) ri(yk) : : :, where xj , yk are the versions in Vset(P) that match P, and Ti reads these versions. If Ti does not read the matched objects, the events ri(xj) and ri(yk) do not show up in the history, e.g., Ti could simply use the count of tuples that matched P.\nFor example, suppose transaction Ti executes the following SQL query:\nSELECT * FROM EMPLOYEE WHERE DEPT = SALES;\nThis query (conceptually) accesses a version of every visible tuple in the Employee relation (e.g., x1 and y2) and the unborn/dead versions of other tuples in this relation (e.g., zinit). Suppose that version x1 matches the predicate and y2 does not match; recall that unborn versions such as zinit cannot match the predicate. This predicate-based read could be shown in a history as ri(Dept=Sales: x1; y2) ri(x1); here, we do not show unborn or dead versions in the version set. Note that the read of x1 shows up as a separate event in the history; if Ti had just determined the number of tuples matching the predicate (using SELECT COUNT), the event ri(x1) would not have been included.Thus, the history only shows reads of versions that were actually observed by Ti."
  }, {
    "heading": "4.3.2 Predicate-based Modifications",
    "text": "A modification based on a predicate P is modeled as a predicate-based read followed by write operations on tuples that match P. (Although this approach is weaker than the one used in [1], it models the behavior of commercial databases.) For example, suppose transaction Ti executes the following code for the employee database discussed above: UPDATE EMPLOYEE SAL = SAL + $10 WHERE DEPT=SALES;\nSuppose that the system selects versions, x1, y2, and zinit for this operation. If x1 matches the predicate but y2 and zinit do not, the following events are added to the history: ri(Dept=Sales: x1; y2) wi(xi).\nIf the predicate-based write deletes objects, dead versions are installed for all the matching tuples (i.e., these tuples are deleted). Thus, if a transaction Ti deletes all employees from the Sales department in the above scenario, the following events are added to the history: ri(Dept=Sales: x1; y2) wi(xi, dead). Note that the events for deletes and updates are similar. However, there is a difference: in the deletion example, xi is a dead version (for illustrative purposes, we have shown the value “dead” being put in xi) and cannot be used further whereas in the update case, xi can be used later.\nInserts are handled in a similar manner. For example, consider the following statement that copies employees whose commission exceeds 25% of their salary into the BONUS table (this statement is executed by transaction T1):\nT1: INSERT INTO BONUS SELECT NAME, SAL, COMM FROM EMP WHERE COMM > 0.25 * SAL;\nHere is a possible history for T1’s execution in our model:\nHinsert: r1(comm > 0.25 * sal: x0, z0) r1(x0) w1(y1) c1\nIn this history, x0 matches the predicate-based query; therefore it is read by T1 to generate tuple y1 that is inserted into the Bonus table."
  }, {
    "heading": "4.4. Conflicts and Serialization Graphs",
    "text": "We first define the different types of read/write conflicts that can occur in a database system and then use them to specify serialization graphs. We define three kinds of direct conflicts that capture conflicts of two different committed transactions on the same object or intersecting predicates. For convenience, we have separated the definitions of predicate-based conflicts and regular conflicts."
  }, {
    "heading": "4.4.1 Read Dependencies",
    "text": "Read dependencies occur when one transaction reads a relevant version produced by some other transaction. We use the following definition for specifying read-dependencies:\nDefinition 2 : Change the Matches of a Predicate-Based Read. We say that a transaction Ti changes the matches of a predicate-based read rj(P: Vset(P)) if Ti installs xi, xh immediately precedes xi in the version order,and xh matches P whereas xi does not or vice-versa. In this case, we also say that xi changes the matches of the predicate-based read.\nThe above definition identifies Ti to be a transaction where a change occurs for the matched set of rj(P: Vset(P)).\nDefinition 3 : Directly Read-Depends. We say that Tj directly read-depends on transaction Ti if it directly itemread-depends or directly predicate-read-depends on Ti.\nDirectly item-read-depends: We say that Tj directly itemread-depends on Ti if Ti installs some object version xi and Tj reads xi.\nDirectly predicate-read-depends: Transaction Tj directly predicate-read-depends on Ti if Tj performs an operation rj(P: Vset(P)), xk 2 Vset(P), i = k or xi xk, and xi changes the matches of rj(P: Vset(P)).\nIf Tj performs a predicate-based read rj(P: Vset(P)), it read depends on Ti if Ti performs a write that is “relevant” to Tj’s read, i.e., Ti is a transaction before Tj that changed the matches of Tj’s read. Note that all tuples in the version set of a predicate-based read are considered to be accessed, including tuples that do not match the predicate. The versions that are actually read by transaction Tj show up as normal\nread events. Other versions in the version set are essentially ghost reads, i.e., their values are not observed by the predicate-based read but read-dependencies are established for them as well.\nThe rule for predicate-read-dependencies captures the idea that what matters for a predicate is the set of tuples that match or do not match and not their values. Furthermore, of all the transactions that have caused the tuples to match (or not match) for rj(P: Vset(P)), we use the latest transaction where a change to Vset(P) occurs rather than using the latest transaction that installed the versions in Vset(P). This rule ensures that we capture the minimum possible conflicts for predicate-based reads. For example, consider the history:\nHpred read: w0(x0) c0 w1(x1) c1 w2(x2) r3(Dept=Sales: x2, y0) w2(y2) c2 c3 [x0 x1 x2, y0 y2]\nHere, transaction T0 inserts object x in the Sales department, T1 changes x’s department to Legal, and T2 changes the phone number of x but not its department. Transaction T3 selects all employees in the Sales department. In this case, even though T3’s version set contains x2, we add a predicateread-dependency from T1 to T3 because T2’s update of x is irrelevant for T3’s read. Note that this history is serializable in the order T0, T1, T3, T2."
  }, {
    "heading": "4.4.2 Anti-Dependencies",
    "text": "An anti-dependency occurs when a transaction overwrites a version observed by some other transaction.\nTo define anti-dependencies, it is useful to define what it means to overwrite a predicate-based operation.\nDefinition 4 : Overwriting a predicate-based read. We say that a transaction Tj overwrites an operation ri(P: Vset(P)) if Tj installs xj such that xk 2 Vset(P), xk xj , and xj changes the matches of ri(P: Vset(P)).\nNow we can define anti-dependencies.\nDefinition 5 : Directly Anti-Depends. Transaction Tj directly anti-depends on transaction Ti if it directly item-antidepends or directly predicate-anti-depends on Ti.\nDirectly item-anti-depends: We say that Tj directly itemanti-depends on transaction Ti if Ti reads some object version xk and Tj installs x’s next version (after xk) in the version order. Note that the transaction that wrote the later version directly item-anti-depends on the transaction that read the earlier version.\nDirectly predicate-anti-depends: We say that Tj directly predicate-anti-depends on Ti if Tj overwrites an operation ri(P: Vset(P)), i.e., Tj installs a later version of some object that changes the matches of a predicatebased read performed by Ti.\nRead-dependencies and anti-dependencies are treated similarly for predicates, i.e., we add an edge whenever a predicate’s matched set is changed. The difference between item-anti-depends and predicate-anti-depends is also similar. For item-anti-depends, the overwriting transaction must produce the very next version of the read object, while for predicate-anti-depends it simply produces a later version that changes the matched tuples of the predicate.\nThe definition for predicate-anti-depends handles inserts and deletes. For example, consider the employee database scenario described in Section 4.3 that contains visible versions of two tuples x and y. Suppose Ti executes a query that selects all Employees in the Sales department, and the query’s version set contains versions x1 and y2 (along with unborn/dead versions of other tuples), and x1 is in Sales and y2 is not. A later transaction Tj will directly predicateanti-depend on Ti if Tj adds a new employee to the Sales department, moves y to Sales, removes x from Sales, or deletes x from the database.\nIn a two-phase locking implementation (for providing serializability), if a transaction T1 performs a read based on predicate P and T2 tries to insert an object x covered by P’s predicate lock, T2 is delayed till T1 finishes. In our model, T1 reads xinit and T2 creates a later version x2. If T2 changes the matches by T1’s read, T2 predicate-anti-depends on T1. Note that T1’s predicate read-locks delay T2 even if T2 does not change the objects matched by P. Our definitions are more flexible and permit implementations that allow T2 to proceed in such cases, e.g., precision locks and granular locks [14]."
  }, {
    "heading": "4.4.3 Write Dependencies",
    "text": "Write dependencies occur when one transaction overwrites a version written by another transaction.\nDefinition 6 : Directly Write-Depends. A transaction Tj directly write-depends on Ti if Ti installs a version xi and Tj installs x’s next version (after xi) in the version order.\nNote that there is no notion of predicate-write-depends since predicate-based modifications are modeled as queries followed by writes on individual tuples."
  }, {
    "heading": "4.4.4 Serialization Graphs",
    "text": "Now we can define the Direct Serialization Graph or DSG. This graph is called “direct” since it is based on the direct conflicts discussed above. In the graph we will denote direct read-dependencies by wr\nTi ! Tj , direct write-dependencies\nby ww Ti ! Tj , and direct anti-dependencies by rw\nTi ! Tj . Figure 2 summarizes this notation and reviews the definitions for direct dependencies.\nDefinition 7 : Direct Serialization Graph. We define the direct serialization graph arising from a history H, denoted by DSG(H), as follows. Each node in the graph corresponds to a committed transaction and directed edges correspond to different types of direct conflicts. There is a read/write/anti-dependency edge from transaction Ti to transaction Tj if Tj directly read/write/anti-depends on Ti.\nA DSG does not capture all information in a history and hence it does not replace the history,e.g., a DSG only records information about committed transactions. The history is still available if needed, and in fact, we use the history instead of the DSG for some conditions.\nAs an example, consider the following history:\nHserial: w1(z1) w1(x1) w1(y1) w3(x3) c1 r2(x1) w2(y2) c2 r3(y2) w3(z3) c3 [x1 x3, y1 y2, z1 z3]\nFigure 3 shows the DSG for this history. As we can see, these transactions are serializable in the order T1; T2; T3.\nIt is also useful to have additional dependency relations:\nDefinition 8 : Depends. A transaction Tj directly depends on Ti if Tj directly write-depends or directly read-depends on Ti. We say that Tj depends on Ti in H if there is a path from Ti to Tj in DSG(H) consisting of one or more dependency edges."
  }, {
    "heading": "5. New Generalized Isolation Specifications",
    "text": "We now present our specifications for the existing ANSI isolation levels. We developed our conditions by studying the motivation of the original definitions [13] and the problems that were addressed by the phenomena in [8]. This enabled us to develop implementation-independent specifications that capture the essence of the ANSI definitions, i.e.,\nwe disallow undesirable situations while allowing histories that are permitted by a variety of implementations.\nLike the previous approaches, we will define each isolation level in terms of phenomena that must be avoided at each level. Our phenomena are prefixed by “G” to denote the fact that they are general enough to allow locking and optimistic implementations; these phenomena are named G0, G1, and so on (by analogy with P0, P1, etc of [6]). We will refer to the new levels as PL levels (where PL stands for “portable level”) to avoid the possible confusion with the degrees of isolation given in [8, 13]."
  }, {
    "heading": "5.1. Isolation Level PL-1",
    "text": "Disallowing phenomenon P0 ensures that writes performed by T1 are not overwritten by T2 while T1 is still uncommitted. There seem to be two reasons why this proscription might be desirable:\n1. It simplifies recovery from aborts. In the absence of this proscription, a system that allows writes to happen in place cannot recover the pre-states of aborted transactions using a simple undo log approach. For example, suppose T1 updates x (i.e., w1(x1)), T2 overwrites x, and then T1 aborts. The system must not restore x to T1’s pre-state. However, if T2 aborts later, x must be restored to T1’s pre-state and not to x1.\n2. It serializes transactions based on their writes alone. For example, if T2 updates an object x and T1 overwrites x, there should not be another object y in which the reverse occurs, i.e., all writes of T2 must be ordered before or after all writes of T1.\nThe first reason does not seem relevant to all systems. Instead, it is based on a particular implementation of recovery, and other implementations are possible. For example, the Thor system [21] maintains temporary versions of objects for an uncommitted transaction Ti and discards these versions if Ti aborts.\nSerializing transactions based on writes is a useful property since it ensures that updates of conflicting transactions\nare not interleaved. This property is captured by phenomenon G0 and we define PL-1 as the level in which G0 is disallowed:\nG0: Write Cycles. A history H exhibits phenomenon G0 if DSG(H) contains a directed cycle consisting entirely of write-dependency edges.\nFor example, history Hwcycle Hwcycle: w1(x1, 2) w2(x2, 5) w2(y2, 5) c2 w1(y1, 8) c1\n[x1 x2, y2 y1]\nis disallowed by PL-1 because the updates on x and y occur in opposite orders, causing a cycle in the graph. Figure 4 shows the DSG for this history.\nOur PL-1 specification is more permissive than Degree 1 of [8] since G0 allows concurrent transactions to modify the same object whereas P0 does not. Thus, non-serializable interleaving of write operations is possible among uncommitted transactions as long as such interleavings are disallowed among committed transactions (e.g., by aborting some transactions).\nThe lock-based implementation of PL-1 (long writelocks) disallows G0 since two concurrent transactions, Ti and Tj , cannot modify the same object; therefore, all writes of Tj either precede or follow all writes of Ti.\nNote that since predicate-based modifications are modeled as queries followed by normal writes, PL-1 provides weak guarantees for such updates. For example, consider the following history in which transaction T2 increments the salaries of all employees for which “Dept = Sales”, and T1 adds two employees, x and y, to the Sales department. Hpred update: w1(x1) r2(Dept=Sales: x1; yinit) w1(y1)\nw2(x2) c1 c2 [xinit x1 x2, yinit y1]\nThe updates of transactions T1 and T2 are interleaved in this history (x’s salary is updated but y’s salary is not). This interleaving is allowed at PL-1 since there is no write-dependency cycle in the DSG (there is a write-dependency edge from T1 to T2 since x1 x2)."
  }, {
    "heading": "5.2. Isolation Level PL-2",
    "text": "If a system disallows only G0, it places no constraints on reads: a transaction is allowed to read modifications made by committed, uncommitted, or even aborted transactions. Proscribing phenomenon P1 in [6] was meant to ensure that T1 updates could not be read by T2 while T1 was still uncommitted. There seem to be three reasons why disallowing P1 (in addition to P0) might be useful:\n1. It prevents a transaction T2 from committing if T2 has read the updates of a transaction that might later abort.\n2. It prevents transactions from reading intermediate modifications of other transactions.\n3. It serializes committed transactions based on their read/write-dependencies (but not their antidependencies). That is, if transaction T2 depends on T1, T1 cannot depend on T2.\nDisallowing P1 (together with P0) captures all three of these issues, but does so by preventing transactions from reading or writing objects written by transactions that are still uncommitted. Instead, we address these three issues by the following three phenomena, G1a, G1b, and G1c.\nG1a: Aborted Reads. A history H shows phenomenon G1a if it contains an aborted transaction T1 and a committed transaction T2 such that T2 has read some object (maybe via a predicate) modified by T1. Phenomenon G1a can be represented using the following history fragments:\nw1(x1:i) : : : r2(x1:i) : : : (a1 and c2 in any order) w1(x1:i) : : : r2(P: x1:i, ...) : : : (a1 and c2 in any order)\nProscribing G1a ensures that if T2 reads from T1 and T1 aborts, T2 must also abort; these aborts are also called cascaded aborts [9]. In a real implementation, the condition also implies that if T2 reads from an uncommitted transaction T1, T2’s commit must be delayed until T1’s commit has succeeded [9, 14].\nG1b: Intermediate Reads. A history H shows phenomenon G1b if it contains a committed transaction T2 that has read a version of object x (maybe via a predicate) written by transaction T1 that was not T1’s final modification of x. The following history fragments represent this phenomenon:\nw1(x1:i) : : : r2(x1:i) : : : w1(x1:j ) : : : c2 w1(x1:i) : : : r2(P: x1:i; :::) : : : w1(x1:j) : : : c2\nProscribing G1b ensures that transactions are allowed to commit only if they have read final versions of objects created by other transactions. Note that disallowing G1a and G1b ensures that a committed transaction has read only object states that existed (or will exist) at some instant in the committed state.\nG1c: Circular Information Flow. A history H exhibits phenomenon G1c if DSG(H) contains a directed cycle consisting entirely of dependency edges.\nIntuitively, disallowing G1c ensures that if transaction T2 is affected by transaction T1, it does not affect T1, i.e., there is a unidirectional flow of information from T1 to T2. Note\nthat G1c includes G0. We could have defined a weaker version of G1c that only concerned cycles with at least one read-dependency edge, but it seemed simpler not to do this.\nPhenomenon G1 captures the essence of no-dirty-reads and is comprised of G1a, G1b and G1c. We define isolation level PL-2 as one in which phenomenon G1 is disallowed. Proscribing G1 is clearly weaker than proscribing P1 since G1 allows reads from uncommitted transactions. The lockbased implementation of PL-2 disallows G1 because the combination of long write-locks and short read-locks ensures that if Ti reads a version produced by Tj , Tj must have committed already (i.e., G1a, G1b not possible) and therefore Tj cannot read a version produced by Ti (i.e., G1c not possible).\nOur PL-2 definition treats predicate-based reads like normal reads and provides no extra guarantees for them; we believe this approach is the most useful and flexible. Other approaches, such as requiring that each predicate-based operation is atomic with respect to other predicate-based operations, are discussed in [1]."
  }, {
    "heading": "5.3. Isolation Level PL-3",
    "text": "In a system that proscribes only G1, it is possible for a transaction to read inconsistent data and therefore to make inconsistent updates. Although disallowing phenomenon P2 prevents such situations (e.g., H2 presented in Section 3), it also prevents legal histories such as H20 (which is also discussed in Section 3) and hence,disallows many optimistic and multi-version concurrency control schemes. What we need is to prevent transactions that perform inconsistent reads or writes from committing. This is accomplished by the following condition:\nG2: Anti-dependency Cycles. A history H exhibits phenomenon G2 if DSG(H) contains a directed cycle with one or more anti-dependency edges.\nWe define PL-3 as an isolation level that proscribes G1 and G2. Thus, all cycles are precluded at this level. Of course, the lock-based implementation of PL-3 (long read/writelocks) disallows phenomenon G2 also since two-phase locking is known to provide complete serializability.\nProscribing G2 is weaker than proscribing P2, since we allow a transaction Tj to modify object x even after another uncommitted transaction Ti has read x. Our PL-3 definition allows histories such as H10 and H20 (presented in Section 3) that were disallowed by the preventative definitions.\nThe conditions given in [9] provides view-serializability whereas our specification for PL-3 provides conflictserializability (this can be shown using theorems presented in [9]). All realistic implementations provide conflictserializability; thus, our PL-3 conditions provide what is normally considered as serializability."
  }, {
    "heading": "5.4. Isolation Level PL-2.99",
    "text": "The level called REPEATABLE READ or Degree 2.99 in [6] provides less than full serializability with respect to predicates. In particular, it uses long locks for all operations except predicate reads for which it used short locks, i.e., it ensures serializability with respect to regular reads and provides guarantees similar to degree 2 for predicate reads. Thus, anti-dependency cycles due to predicates can occur at this level.\nWe define level PL-2.99 as one that proscribes G1 and G2-item:\nG2-item: Item Anti-dependency Cycles. A history H exhibits phenomenon G2-item if DSG(H) contains a directed cycle having one or more item-antidependency edges.\nFor example, consider the following history:\nHphantom: r1(Dept=Sales: x0, 10; y0, 10) r1(x0, 10) r2(y0, 10) r2(Sum0, 20) w2(z2, 10) w2(Sum2, 30) c2 r1(Sum2, 30) c1\n[Sum0 Sum2, zinit z2]\nWhen T1 performs its query, there are exactly two employees, x and y, both in Sales (we show only visible versions in the history). T1 sums up the salaries of these employees and compares it with the sum-of-salaries maintained for this department. However, before it performs the final check, T2 inserts a new employee, z2, in the Sales department, updates the sum-of-salaries, and commits. Thus, when T1 reads the new sum-of-salaries value it finds an inconsistency.\nThe DSG for Hphantom is shown in Figure 5. This history is ruled out by PL-3 but permitted by PL-2.99 because the DSG contains a cycle only if predicate anti-dependency edges are considered."
  }, {
    "heading": "5.5. Mixing of Isolation Levels",
    "text": "So far, we have only discussed systems in which all transactions are provided the same guarantees. However, in general, applications may run transactions at different levels and we would like to understand how these transactions interact with each other. This section discusses how we model such mixed systems.\nIn real database systems, each SQL statement in a transaction Ti may be executed atomically even though Ti is\nexecuted at a lower isolation level. Mixed systems in which individual SQL statements are executed atomically are discussed in [1].\nIn a mixed system, each transaction specifies its level when it starts and this information is maintained as part of the history and used to construct a mixed serialization graph or MSG. Like a DSG, the MSG contains nodes corresponding to committed transactions and edges corresponding to dependencies, but only dependencies relevant to a transaction’s level or obligatory dependencies show up as edges in the graph. Transaction Ti has an obligatory conflict with transaction Tj if Tj is running at a higher level than Ti, Ti conflicts with Tj , and the conflict is relevant at Tj’s level. For example, an anti-dependency edge from a PL-3 transaction to a PL-1 transaction is an obligatory edge since overwriting of reads matters at level PL-3.\nEdges are added as follows: Since write-dependencies are relevant at all levels, we retain all such edges. For a PL-2 or PL-3 node Ti, since reads are important, readdependencies coming into Ti are added. Similarly, we add all outgoing anti-dependency edges from PL-3 transactions to other nodes.\nNow we can define correctness for a mixed history:\nDefinition 9 : Mixing-Correct. A history H is mixingcorrect if MSG(H) is acyclic and phenomena G1a and G1b do not occur for PL-2 and PL-3 transactions.\nIt is possible to restate the above definition as an analog of the Isolation Theorem [14]:\nMixing Theorem: If a history is mixing-correct, each transaction is provided the guarantees that pertain to its level.\nThe above theorem holds at the level of a history and is independent of how synchronization is implemented 1. Note that the guarantees provided to each level are with respect\n1As stated in [14], this does not imply that a PL-3 transaction observes a consistent state since lower level transactions may have modified the database inconsistently; if we want a PL-3 transaction to observe a consistent state, lower level transactions must update the database consistently even if they observe an inconsistent state.\nto the MSG. The reason is that an MSG considers the presence of transactions at other levels whereas a DSG is simply constructed with all edges. An MSG is useful for determining correctness if PL-1 and PL-2 transactions “know” what they are doing whereas a DSG ensures correctness without making any assumptions about the operations of lower level transactions.\nA mixed system can be implemented using locking (with the standard combination of short and long read/write locks). But it can also be implemented using other techniques. For example an optimistic implementation would attempt to fit each committing transaction into the serial order based on its own requirements (for its level) and its obligations to transactions running at higher levels, and would abort the transaction if this is not possible. An optimistic implementation that is mixing-correct is presented in [1]."
  }, {
    "heading": "5.6. Discussion",
    "text": "We summarize the isolation levels discussed in this section in Figure 6.\nThese levels are defined to impose constraints only when transactions commit; they do not constrain transactions as they run, although if something bad happens (e.g., a PL3 transactions observes an inconsistency), they do force aborts. Analogs of the levels that constrain executing transactions are given in [1]; these definitions use slightly different graphs, containing nodes for committed transactions plus a node for the executing transaction."
  }, {
    "heading": "6. Conclusions",
    "text": "This paper has presented new, precise specifications of the ANSI-SQL isolation levels. Unlike previous proposals, the new definitions are implementation-independentand allow a wide range of concurrency control techniques, including locking and optimism. Furthermore, our definitions handle predicates in a correct and flexible manner at all isolation levels. Thus, they meet the goals of the ANSI-SQL standard.\nThe paper also specified the behavior of systems that allow mixing of levels: users are allowed to choose the level for each transaction they run, and the system guarantees that each transaction is provided with the constraints of its own level, even when some transactions are running at lower levels.\nOur approach is applicable to other levels in addition to the ones discussed in the paper. We have developed implementation-independent specifications of commercial isolation levels such as Snapshot Isolation and Cursor Stability, and we have defined a new level called PL-2+; the details can be found in [1]. PL-2+ is the the weakest level that guarantees consistent reads and causal consistency; it is useful in client-server systems [3, 1] and broadcast environments [25].\nAll of our definitions are implementation independent. This makes them suitable for use as an industry standard, since they do not preclude clever but unconventional implementations that either exist today or may be developed in the future. Instead they provide implementors with the opportunity to choose the best performing concurrency control mechanism for their environment."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank Chandra Boyapati, Miguel Castro, Andrew Myers, and other members of the Programming Methodology Group for their comments. We are grateful to Dimitris Liarokapis and Elizabeth O’Neil for carefully reading the paper and helping us improve the specifications. We would also like to thank Phil Bernstein, Jim Gray, and David Lomet, for their helpful comments."
  }],
  "year": 1999,
  "references": [{
    "title": "Weak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions",
    "authors": ["A. Adya"],
    "venue": "PhD thesis,",
    "year": 1999
  }, {
    "title": "Efficient Optimistic Concurrency Control using Loosely Synchronized Clocks",
    "authors": ["A. Adya", "R. Gruber", "B. Liskov", "U. Maheshwari"],
    "year": 1995
  }, {
    "title": "Lazy Consistency Using Loosely Synchronized Clocks",
    "authors": ["A. Adya", "B. Liskov"],
    "venue": "In Proc. of ACM Principles of Dist",
    "year": 1997
  }, {
    "title": "Consistency and Orderability: Semantics-Based Correctness Criteria for Databases",
    "authors": ["D. Agrawal", "A.E. Abbadi", "A.K. Singh"],
    "venue": "ACM TODS,",
    "year": 1993
  }, {
    "title": "Distributed Multi-version Optimistic Concurrency Control with Reduced Rollback",
    "authors": ["D. Agrawal", "A.J. Bernstein", "P. Gupta", "S. Sengupta"],
    "venue": "Distributed Computing,",
    "year": 1987
  }, {
    "title": "Semantics-Based Concurrency Control: Beyond Commutativity",
    "authors": ["B.R. Badrinath", "K. Ramamritham"],
    "venue": "ACM TODS,",
    "year": 1992
  }, {
    "title": "A Critique of ANSI SQL Isolation Levels",
    "authors": ["H. Berenson", "P. Bernstein", "J. Gray", "J. Melton", "E. O’Neil", "P. O’Neil"],
    "venue": "In Proc. of SIGMOD,",
    "year": 1995
  }, {
    "title": "Concurrency Control and Recovery in Database Systems",
    "authors": ["P.A. Bernstein", "V. Hadzilacos", "N. Goodman"],
    "venue": "Addison Wesley,",
    "year": 1987
  }, {
    "title": "Synthesis of Extended Transaction Models using ACTA",
    "authors": ["P. Chrysanthis", "K. Ramamritham"],
    "venue": "ACM TODS,",
    "year": 1994
  }, {
    "title": "An Introduction to Database Systems. Addison- Wesley",
    "authors": ["C.J. Date"],
    "venue": "Fifth edition,",
    "year": 1990
  }, {
    "title": "The Dangers of Replication and a Solution",
    "authors": ["J. Gray", "P. Helland", "P. O’Neil", "D. Shasha"],
    "venue": "In Proc. of SIGMOD,",
    "year": 1996
  }, {
    "title": "Granularity of Locks and Degrees of Consistency in a Shared Database",
    "authors": ["J. Gray", "R. Lorie", "G. Putzolu", "I. Traiger"],
    "venue": "In Modeling in Data Base Management Systems. Amsterdam: Elsevier North-Holland,",
    "year": 1976
  }, {
    "title": "Transaction Processing: Concepts and Techniques",
    "authors": ["J.N. Gray", "A. Reuter"],
    "year": 1993
  }, {
    "title": "Optimism vs. Locking: A Study of Concurrency Control for Client-Server Object-Oriented Databases",
    "authors": ["R. Gruber"],
    "venue": "PhD thesis,",
    "year": 1997
  }, {
    "title": "Disconnected Operation in the Thor Object-Oriented Database System",
    "authors": ["R. Gruber", "F. Kaashoek", "B. Liskov", "L. Shrira"],
    "venue": "In IEEE Workshop on Mobile Comp. Systems,",
    "year": 1994
  }, {
    "title": "Apologizing Versus Asking Permission: Optimistic Concurrency Control for Abstract Data Types",
    "authors": ["M.P. Herlihy"],
    "venue": "ACM TODS,",
    "year": 1990
  }, {
    "title": "Disconnected Operation in the Coda File System",
    "authors": ["J.J. Kistler", "M. Satyanarayanan"],
    "venue": "Proc of the ACM Symp. on Operating Sys. Principles,",
    "year": 1991
  }, {
    "title": "Database System Concepts",
    "authors": ["H. Korth", "A. Silberschatz", "S. Sudarshan"],
    "year": 1997
  }, {
    "title": "On Optimistic Methods for Concurrency Control",
    "authors": ["H.T. Kung", "J.T. Robinson"],
    "venue": "ACM TODS,",
    "year": 1981
  }, {
    "title": "Safe and Efficient Sharing of Persistent Objects in Thor",
    "authors": ["B. Liskov", "A. Adya", "M. Castro", "M. Day", "S. Ghemawat", "R. Gruber", "U. Maheshwari", "A. Myers", "L. Shrira"],
    "venue": "In Proc. of SIGMOD,",
    "year": 1996
  }, {
    "title": "Development of an Object-Oriented dDBMS",
    "authors": ["D. Maier", "J. Stein", "A. Otis", "A. Purdy"],
    "venue": "In Proc. of OOPSLA,",
    "year": 1986
  }, {
    "title": "The Escrow Transactional Method",
    "authors": ["P. O’Neil"],
    "venue": "ACM TODS,",
    "year": 1986
  }, {
    "title": "Efficient Concurrency Control for Broadcast Environments",
    "authors": ["J. Shanmugasundaram", "A. Nithrakashyap", "R. Sivasankaran", "K. Ramamritham"],
    "year": 1999
  }, {
    "title": "Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System",
    "authors": ["D. Terry"],
    "venue": "In Proc. of SOSP, Copper Mountain Resort, CO,",
    "year": 1995
  }],
  "id": "SP:27682071ccc226220fdfafaed42d35826309d692",
  "authors": [{
    "name": "Atul Adya",
    "affiliations": []
  }, {
    "name": "Barbara Liskov",
    "affiliations": []
  }, {
    "name": "Patrick O’Neil",
    "affiliations": []
  }],
  "abstractText": "Commercial databases support different isolation levels to allow programmers to trade off consistency for a potential gain in performance. The isolation levels are defined in the current ANSI standard, but the definitions are ambiguous and revised definitions proposed to correct the problem are too constrained since they allow only pessimistic (locking) implementations. This paper presents new specifications for the ANSI levels. Our specifications are portable; they apply not only to locking implementations, but also to optimistic and multi-version concurrency control schemes. Furthermore, unlike earlier definitions, our new specifications handle predicates in a correct and flexible manner at",
  "title": "Generalized Isolation Level Definitions"
}