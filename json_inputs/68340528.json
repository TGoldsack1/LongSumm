{
  "sections": [{
    "text": "KEYWORDS Fake news detection; joint learning; social media mining\nACM Reference Format: Kai Shu, Suhang Wang, and Huan Liu. 2019. Beyond News Contents: The Role of Social Context for Fake News Detection. In The Twelfth ACM International Conference on Web Search and Data Mining (WSDM ’19), February 11–15, 2019, Melbourne, VIC, Australia. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3289600.3290994"
  }, {
    "heading": "1 INTRODUCTION",
    "text": "People nowadays tend to seek out and consume news from social media rather than traditional news organizations. For example, 62% of U.S. adults get news on social media in 2016, while in 2012, only\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM ’19, February 11–15, 2019, Melbourne, VIC, Australia © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5940-5/19/02. . . $15.00 https://doi.org/10.1145/3289600.3290994\n49 percent is reported seeing news on social media1. However, social media is a double-edged sword for news consumption. The quality of news on social media is much lower than that of traditional news organizations. Large volumes of fake news, i.e., news with intentionally false information, are produced online for a variety of purposes, such as financial and political gain [2, 13].\nFake news can have detrimental effects on individuals and the society. First, people may be misled by fake news and accept false beliefs [18, 21]. Second, fake news could change the way people respond to true news2. Third, the wide propagation of fake news could break the trustworthiness of entire news ecosystem. Thus, it is important to detect fake news on social media. Fake news is intentionally written to mislead consumers, which makes it nontrivial to detect simply based on news content. To build an effective and practical fake news detection system, it is natural and necessary to explore auxiliary information from different perspectives.\nThe news ecosystem on social media provides abundant social context information, which involves three basic entities, i.e., publishers, news pieces, and social media users. Figure 1 gives an illustration of such ecosystem. In Figure 1, p1, p2 and p3 are news publishers who publish news a1, . . . ,a4 and u1, . . . ,u6 are users who have engaged in sharing these news pieces. In addition, users tend to form social links with like-minded people with similar interests. As we will show, the tri-relationship, the relationship among publishers, news pieces, and users, contains additional information to help detect fake news.\nFirst, sociological studies on journalism have theorized the correlation between the partisan bias of publisher and the veracity degree of news content [8]. The partisan bias means the perceived bias of the publisher in the selection of how news is reported and covered [6]. For example, in Figure 1, p1 is a publisher with extreme left partisan bias and p2 is a publisher with extreme right partisan bias. To support their own partisan viewpoints, they have high degree to distort the facts and report fake news pieces, such as a1 and a3; while for a mainstream publisher p3 that has least partisan bias, he/she has a lower chance to manipulate original news events, and is more likely to write a true news piece a4. Thus, exploiting the partisan bias of publishers to bridge the publisher-news relationships can bring additional benefits to predict fake news.\nSecond, mining user engagements towards news pieces on social media also help fake news detection. Previous approaches try to aggregate users’ attributes to infer the degree of news veracity by assuming that either (i) all the users contribute equally for learning feature representations of news pieces [10]; or (ii) user features are\n1http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms2016/ 2https://www.nytimes.com/2016/11/28/opinion/fake-news-and-the-internet-shellgame.html?\ngrouped locally for specific news and the global user-news interactions are ignored [4]. However, in practice, these assumptions may not hold. On social media, different users have different credibility levels. The credibility score, which means “the quality of being trustworthy” [1], has a strong indication of whether some user is more likely to share fake news or not. Those less credible users, such as malicious accounts or normal users who are vulnerable to fake news, are more likely to spread fake news. For example,u2 and u4 are users with low credibility scores, and they tend to spread fake news more than other highly credible users. In addition, users tend to form relationships with like-minded people [25]. For example, user u5 and u6 are friends on social media, so they tend to post those news that confirm their own views, such as a4. Therefore, incorporating the user credibility levels to capture the user-news interactions has potentials to improve fake news prediction.\nMoreover, the publisher-news relationships and user-news interactions both provide new and different perspectives of social context, and thus contain complementary information to advance fake news detection. In this paper, we investigate: (1) how to mathematically model the tri-relationship to extract feature representations of news pieces; and (2) how to take advantage of tri-relationship modeling for fake news detection. Our solutions to these two challenges results in a novel framework TriFN for fake news detection problem. Our main contributions are summarized as follows: • We provide a principled way to model tri-relationship among publishers, news pieces, and users simultaneously; • We propose a novel framework TriFN, which exploits both user-news interactions and publisher-news relations for learning news feature representations to predict fake news; and • We conduct extensive experiments on two real-world datasets to assess the effectiveness of TriFN."
  }, {
    "heading": "2 PROBLEM STATEMENT",
    "text": "Let A = {a1,a2, ...,an } be the set of n news pieces, and U = {u1,u2, ...,um } be the set ofm users on social media posting these news pieces.We denoteX ∈ Rn×t as the bag-of-word featurematrix\nof news pieces, where t is the dimension of vocabulary size. We use A ∈ {0, 1}m×m to denote the user-user adjacency matrix, where Ai j = 1 indicates that user ui and uj are friends; otherwise Ai j = 0. We denote the user-news interaction matrix as W ∈ {0, 1}m×n , where Wi j = 1 indicates that user ui has shared the news piece aj ; otherwise Wi j = 0. It’s worth mentioning that we focus on those user-news interactions in which users agree with the news. For example, we only consider those users who share news pieces without comments, and these users share the same alignment of viewpoints with the news items [12]. We will introduce more details in Section 3.3. We also denote P = {p1,p2, ...,pl } as the set of l news publishers. In addition, we denote B ∈ Rl×n as the publishernews publishing matrix, and Bk j = 1 means news publisher pk publishes the news article aj ; otherwise Bk j = 0. We assume that the partisan bias labels of some publishers are given and available (see more details of how to collect partisan bias labels in Sec 3.4). We define o ∈ {−1, 0, 1}l×1 as the partisan label vectors, where -1, 0, 1 represents left-, neutral-, and right-partisan bias.\nSimilar to previous research [10, 32], we treat fake news detection problem as a binary classification problem. In other words, each news piece can be true or fake, and we use y = {y1; y2; ...; yn } ∈ Rn×1 to represent the labels, and yj = 1 means news piece aj is fake news; yj = −1 means true news. With the notations given above, the problem is formally defined as,\nGiven news article feature matrix X, user adjacency matrix A, user social engagement matrix W, publisher-news publishing matrix B, publisher partisan label vector o, and partial labeled news vector yL , we aim to predict remaining unlabeled news label vector yU ."
  }, {
    "heading": "3 A TRI-RELATIONSHIP EMBEDDING FRAMEWORK",
    "text": "In this section, we present the details of the proposed framework TriFN for modeling tri-relationship for fake news detection. It consists of five major components (Figure 2): a news contents embedding component, a user embedding component, a user-news interaction embedding component, a publisher-news relation embedding component, and a semi-supervised classification component.\nIn general, the news contents embedding component describes the mapping of news from bag-of-word features to latent feature space; the user embedding component illustrates the extraction of user latent features from user social relations; the user-news interaction embedding component learn the feature representations of news pieces guided by their partial labels and user credibilities; The publisher-news relation embedding component regularize the feature representations of news pieces through publisher partisan bias labels; The semi-supervised classification component learns a classification function to predict unlabeled news items."
  }, {
    "heading": "3.1 News Contents Embedding",
    "text": "We can use news contents to find clues to differentiate fake news and true news. Recently, it has been shown that nonnegative matrix factorization (NMF) algorithms are very practical and popular to learn document representations [20, 28, 38]. It can project the newsword matrix X to a joint latent semantic factor space with low dimensionality, such that the news-word relations are modeled as the inner product in the space. Specifically, giving the newsword matrix X ∈ Rn×t , NMF methods try to find two nonnegative matrices D ∈ Rn×d+ and V ∈ Rt×d+ , where d is the dimension of the latent space, by solving the following optimization problem,\nmin D,V≥0\n∥ X − DVT ∥2F + λ(∥D∥ 2 F + ∥V∥ 2 F ) (1)\nwhere D and V are the nonnegative matrices indicating low dimension representations of news pieces and words. Note that we denote D = [DL ; DU ], where DL ∈ Rr×d is the news latent feature matrix for labeled news; while DU ∈ R(n−r )×d is the news latent feature matrix for unlabeled news. The term λ(∥D∥2F + ∥V∥ 2 F ) is introduced to avoid over-fitting."
  }, {
    "heading": "3.2 User Embedding",
    "text": "On social media, people tend to form relationships with like-minded people, rather than those users who have opposing preferences and interests. Thus, connected users are more likely to share similar latent interests in news pieces. To obtain a standardized representation, we use nonnegative matrix factorization to learn the users’ latent representations. Specifically, giving user-user adjacency matrix A ∈ {0, 1}m×m , we learn nonnegative matrix U ∈ Rm×d+ by solving the following optimization problem,\nmin U,T≥0\n∥Y ⊙ (A − UTUT )∥2F + λ(∥U∥ 2 F + ∥T∥ 2 F ) (2)\nwhere U is the user latent matrix, T ∈ Rd×d+ is the user-user correlation matrix, Y ∈ Rm×m controls the contribution of A, and ⊙ denotes the Hadamard product operation. Since only positive links are observed in A, following common strategies [19], we first set Yi j = 1 if Ai j = 1, and then perform negative sampling and generate the same number of unobserved links and set weights as 0. The term λ(∥U∥2F + ∥T∥ 2 F ) is to avoid over-fitting."
  }, {
    "heading": "3.3 User-News Interaction Embedding",
    "text": "We model the user-news interactions by considering the relationships between user features and the labels of news items. We have shown (see Section 1) that users with low credibilities are more likely to spread fake news, while users with high credibilities are\nless likely to spread fake news. To measure user credibility scores, we adopt the practical approach in [1]. The basic idea in [1] is that less credible users are more likely to coordinate with each other and form big clusters, while more credible users are likely to from small clusters. Specifically, the credibility scores are measured through the following major steps: 1) detect and cluster coordinate users based on user similarities; 2) weight each cluster based on the cluster size. Note that for our fake news detection task, we do not assume that credibility scores are directly provided, but inferred from widely available data, such as user-generated contents. By using the method in [1], we can assign each user ui a credibility score ci ∈ [0, 1]. A larger ci indicates that user ui has a higher credibility, while a lower ci indicates a lower credibility score. We use c = {c1, c2, ..., cm } to denote the credibility score vector for all users.\nFirst, high-credibility users are more likely to share true news pieces, so we ensure that the distance between latent features of high-credibility users and that of true news is minimized,\nmin U,DL≥0 m∑ i=1 r∑ j=1 Wi jci (1 − 1 + yLj 2 )| |Ui − DLj | |22 (3)\nand (1 − 1+yLj2 ) is to ensure we only include true news pieces (i.e., yLj = −1), and ci is to adjust the contribution of user ui to the loss function. For example, if ci is large (high-credibility) and Wi j = 1, we put a bigger weight on forcing the distance of feature Ui and DLj to be small; if ci is small (low-credibility) and Wi j = 1, than we put a smaller weight on forcing the distance of feature Ui and DLj to be small.\nSecond, low-credibility users are more likely to share fake news pieces, and we aim to minimize the distance between latent features of low-credibility users and that of fake news,\nmin U,DL≥0 m∑ i=1 r∑ j=1 Wi j (1 − ci )( 1 + yLj 2 )| |Ui − DLj | |22 (4)\nand the term ( 1+yLj2 ) is to ensure we only include fake news pieces (i.e., yLj = 1), and (1 − ci ) is to adjust the contribution of user ui to the loss function. For example, if ci is large (high-credibility) and Wi j = 1, we put a smaller weight on forcing the distance of feature Ui and DLj to be small; if ci is small (low-credibility) and Wi j = 1, then we put a bigger weight on forcing the distance of feature Ui and DLj to be small.\nFinally, We combine Eqn 3 and Eqn 4 to consider the above two situations, and obtain the following objective function,\nmin U,DL≥0 m∑ i=1 r∑ j=1 Wi jci (1 − 1 + yLj\n2 )| |Ui − DLj | |22︸ ︷︷ ︸\nTrue news\n+ m∑ i=1 r∑ j=1 Wi j (1 − ci )( 1 + yLj\n2 )| |Ui − DLj | |22︸ ︷︷ ︸\nFake news\n(5)\nFor simplicity, Eqn 5 can be rewritten as,\nmin U,DL≥0 m∑ i=1 r∑ j=1 Gi j | |Ui − DLj | |22 (6)\nwhere Gi j = Wi j (ci (1 − 1+yLj2 ) + (1 − ci )( 1+yLj\n2 )). If we denote a new matrix H = [U; DL] ∈ R(m+r )×d , we can also rewrite Eqn. 6 as a matrix form as follows,\nmin U,DL≥0 m∑ i=1 r∑ j=1 Gi j | |Ui − DLj | |22 ⇔ minH≥0 m∑ i=1 r+m∑ j=1+m Gi j | |Hi − Hj | |22\n⇔ min H≥0 m+r∑ i, j=1 Fi j | |Hi − Hj | |22 ⇔ minH≥0 tr(H T LH)\n(7) where L = S − F is the Laplacian matrix and S is a diagonal matrix with diagonal element Sii = ∑m+r j=1 Fi j . F ∈ R(m+r )×(m+r ) is computed as follows,\nFi j =  0, i, j ∈ [1,m] or i, j ∈ [m + 1,m + r ] Gi(j−m), i ∈ [1,m], j ∈ [m + 1,m + r ] G(i−m)j , i ∈ [m + 1,m + r ], j ∈ [1,m]\n(8)"
  }, {
    "heading": "3.4 Publisher-News Relation Embedding",
    "text": "Fake news is often written to convey opinions or claims that support the partisan bias of news publishers. Thus, a good news representation should be good at predicting the partisan bias of its publisher. We obtain the list of publishers’ partisan scores from a well-known media bias fact-checking websites MBFC 3. The partisan bias labels are checked with a principled methodology that ensures the reliability and objectivity of the partisan annotations. The labels are categorized into five categories: “left”, “left-Center”,“leastbiased”,“right-Center” and “right”. To further ensure the accuracy of the labels, we only consider those news publishers with the annotations [“left”,“least-biased”, “Right”], and rewrite the corresponding labels as [-1,0,1]. Thus, we can construct a partisan label vectors for news publishers as o. Note that we may not obtain the partisan labels for all publishers, so we introduce e ∈ {0, 1}l×1 to control the weight of o. If we have the partisan bias label of publisher pk , then ek = 1; otherwise, ek = 0. The basic idea is to utilize publisher partisan labels vector o ∈ Rl×1 and publisher-news matrix B ∈ Rl×n to optimize the news feature representation learning. Specifically, we optimization following objective function,\nmin D≥0,q\n∥ e ⊙ (B̄Dq − o)∥22 + λ∥q∥22 (9)\nwhere we assume that the latent feature of news publisher can be represented by the features of all the news it published, i.e., B̄D. B̄ is the normalized user-news publishing relation matrix, i.e., B̄k j = Bk j∑n j=1 Bk j\n. q ∈ Rd×1 is the weighting matrix that maps news publishers’ latent features to corresponding partisan label vector o.\n3https://mediabiasfactcheck.com/"
  }, {
    "heading": "3.5 Proposed Framework - TriFN",
    "text": "We have introduced how we can learn news latent features by modeling different aspects of the tri-relationship. We further employ a semi-supervised linear classifier term as follows,\nmin p ∥ DLp − yL ∥22 + λ∥p∥22 (10)\nwhere p ∈ Rd×1 is the weighting matrix that maps news latent features to fake news labels. With all previous components, TriFN solves the following optimization problem,\nmin D,U,V,T≥0,p,q\n∥X − DVT ∥2F + α ∥Y ⊙ (A − UTU T )∥2F\n+ βtr(HT LH) + γ ∥e ⊙ (B̄Dq − o)∥22 + η∥DLp − yL ∥22 + λR\n(11)\nwhere R = (∥D∥2F + ∥V∥ 2 F + ∥U∥ 2 F + ∥T∥ 2 F + ∥p∥ 2 2 + ∥q∥22 ) is to avoid over-fitting. The first term models the news latent features from news contents; the second term extracts user latent features from their social relationships; and the third term incorporates the user-news interactions; and the fourth term models publisher-news relationships. The fifth term adds a semi-supervised fake news classifier. Therefore, this framework provides a principled way to model tri-relationship for fake news prediction."
  }, {
    "heading": "4 AN OPTIMIZATION ALGORITHM",
    "text": "In this section, we present the detail optimization process for the proposed framework TriFN. If we update the variables jointly, the objective function in Eq. 11 is not convex. Thus, we propose to use alternating least squares to update the variables separately. For simplicity, we user L to denote the objective function in Eq. 11. Next, we introduce the updating rules for each variable in details.\nUpdate D. Let ΨD be the Lagrange multiplier for constraint D ≥ 0, the Lagrange function related to D is,\nmin D ∥X − DVT ∥2F + βtr(H T LH) + γ ∥e ⊙ (B̄Dq − o)∥22\n+ η∥DLp − yL ∥22 + λ∥D∥2F − tr(ΨDD T )\n(12)\nandD = [DL ; DU ] andH = [U; DL].We rewrite L = [L11, L12; L21, L22], where L11 ∈ Rm×m , L12 ∈ Rm×r ,L21 ∈ Rr×m , and L22 ∈ Rr×r ; and X = [XL ,XU ]. The partial derivative of L w.r.t. D as follows,\n1 2 ∂L ∂D = (DVT − X)V + λD + γ B̄T ET (EB̄Dq − Eo)qT\n+ [ βL21U + βL22DL + η(DLp − yL)pT ; 0 ] − ΨD\n(13)\nwhere E ∈ Rl×l is a diagonal matrix with {ek }lk=1 on the diagonal and zeros everywhere else. By setting the derivative to zero and using Karush-KuhnTucker complementary condition [3], i.e., ΨD (i, j)Di j = 0,we get,\nDi j ← Di j √ D̂(i, j) D̃(i, j)\n(14)\nD̂ = XV + γ ( B̄T ET EoqT )+ + γ ( B̄T ET EB̄DqqT )− + [ η ( DLppT )− + η ( yLpT )+ + β(L21U)− + β(L22DL)−; 0\n] D̃ = DVT V + λD + γ ( BT ET EB̄DqqT )+ + γ ( B̄T ET EoqT\n)− + [ β(L21U)+ + β(L22DL)+ + η ( DLppT )+ + η ( yLpT )−; 0] (15) where for any matrix X, (X)+ and (X)− denote the positive and negative parts of X, respectively. Specifically, we have (X)+ = ABS (X)+X\n2 and (X)− = ABS (X)−X\n2 , ABS(X) is the matrix with the absolute value of elements in X.\nUpdate U, V and T. The partial derivative of the Lagrange objective function w.r.t. U and updating rule are as follows,\n1 2 ∂L ∂U = α(Y ⊙ (UTUT − A))UTT + α(Y ⊙ (UTUT − A))T UT + λU − ΨU + β(L11U + L12DL)\n(16)\nUi j ← Ui j √ [ Û ] (i, j)[\nŨ ] (i, j)\n(17)\nÛ = α(Y ⊙ A)UTT + α(Y ⊙ A)T UT + β(L11U)− + β(L12DL)−\nŨ = α(Y ⊙ UTUT )UTT + α(Y ⊙ UTUT )T UT + λU + β(L11U)+ + β(L12DL)+\n(18) The partial derivatives of the Lagrange objective w.r.t V and updating rule are,\n1 2 ∂L ∂V = (DVT − X)T D + λV − ΨV (19)\nVi j ← Vi j\n√ [ XT D ] (i, j)[\nVDT D + λV ] (i, j)\n(20)\nThe partial derivative of the Lagrange objective w.r.t T and the updating rule are,\n1 2 ∂L ∂T = αUT (Y ⊙ (UTUT − A))U + λT − ΨT (21)\nTi j ← Ti j\n√ [ αUT (Y ⊙ A)U ] (i, j)[\nαUT (Y ⊙ UTUT )U + λT ] (i, j)\n(22)\nUpdate p and q. Optimization w.r.t p and q are essentially least square problems. By setting ∂L∂p = 0 and ∂L ∂q = 0, the closed from solutions of p and q are as follows,\np = (ηDTLDL + λI) −1ηDTLyL q = (γDT B̄T EB̄D + λI)−1γDT B̄T Eo (23)\nWhere I is an identity matrix, and E ∈ Rl×l with ek ,k = 1, . . . , l on the diagonal and zeros everywhere else."
  }, {
    "heading": "4.1 Optimization Algorithm of TriFN",
    "text": "Wepresent the details to optimize TriFN inAlgorithm 1.We first randomly initialize U,V,T,D, p, q in line 1, and construct the Laplacian matrix L in line 2. Then we repeatedly update related parameters through Line 4 to Line 8 until convergence. Finally, we predict the labels of unlabeled news yU in line 10. The convergence of Algorithm 1 is guaranteed because the objective function is nonnegative\nAlgorithm 1 The optimization process of TriFN framework Require: X,A,B,W,Y, o, yL ,α , β ,γ , λ,η Ensure: yU 1: Randomly initialize U,V,T,D, p, q 2: Precompute Laplacian matrix L 3: repeat 4: Update D with Eqn 14 5: Update U with Eqn 18 6: Update V with Eqn 20 7: Update T with Eqn 22 8: Update p,q with Eqn 23 9: until convergence 10: Calculate yU = Sign(DU p)\nand in each iteration it will monotonically decrease the objective value, and finally it will converge to an optimal point [15]."
  }, {
    "heading": "4.2 Complexity Analysis",
    "text": "Themain computation cost comes from the fine-tuning variables for Algorithm 1. In each iteration, the time complexity for computing D is O(nd +nld2+rd +rm+n2). Similarly, the computation cost for V is approximately O(tnd), for U is O(m4d3 +md), for T is about O(m4d3 +m2d2). To update p and q, the costs are approximately O(d3 +d2 +dr ) and O(d2ln +d3 +dl). The overall time complexity is the sum of the costs of initialization and fine-tuning."
  }, {
    "heading": "5 EXPERIMENTS",
    "text": "In this section, we present the experiments to evaluate the effectiveness of the proposed TriFN framework. Specifically, we aim to answer the following research questions: • Is TriFN able to improve fake news classification performance bymodeling publisher partisan and user engagements simultaneously? • How effective are publisher partisan bias modeling and user engagement learning, respectively, in improving the fake news detection performance of TriFN? • Can the proposed method handle early fake news detection when limited user engagements are provided?"
  }, {
    "heading": "5.1 Datasets",
    "text": "We utilize one of the comprehensive fake news detection benchmark dataset called FakeNewsNet [31, 32]. The dataset is collected\nfrom two platforms with fact-checking: BuzzFeed and PolitiFact, both containing news content with labels and social context information. News content includes the meta attributes of the news (e.g., body text), and social context includes the related user social engagements of news items (e.g., user posting/sharing news in Twitter). The detailed statistics of the datasets are shown in Table 1."
  }, {
    "heading": "5.2 Experimental Settings",
    "text": "To evaluate the performance of fake news detection algorithms, we use the following metrics, which are commonly used to evaluate classifiers in related areas: Accuracy, Precision, Recall, and F1. We randomly choose 80% of news pieces for training and remaining 20% for testing, and the process is performed for 10 times and the average performance is reported. We compare the proposed framework TriFN with several state-of-the-art fake news detection methods. Existing methods mainly focus on extractingdiscriminative features and feed them into a classification algorithm to differentiate fake news. Next, we introduce several representative features as follows, • RST [26]: RST stands for Rhetorical Structure Theory, which builds a tree structure to represent rhetorical relations among the words in the text. RST can extract style-based features of news by mapping the frequencies of rhetorical relations to a vector space 4. • LIWC [23]: LIWC stands for Linguistic Inquiry and Word Count, which is widely used to extract the lexicons falling into psycholinguistic categories. It’s based on a large sets of words that represent psycholinguistic processes, summary categories, and part-of-speech categories. It learns a feature vector from a psychology and deception perspective 5. • Castillo [4]: Castillo extract various kinds of features from those users who have shared a news item on social media. The features are extracted from user profiles and friendship network. We also include the credibility score of users inferred in Sec 3.3 as an additional social context feature. • RST+Castillo: RST+Castillo represents the concatenated features of RST and Castillo, which include features extracted from both news content and social context. • LIWC+Castillo: LIWC+Castillo represents the concatenated features of LIWC and Castillo, which consists of feature information from both news content and social context.\nNote that for a fair and comprehensive comparison, we choose the above feature extraction methods from following aspects: 1) only extract features from news contents, such as RST, LIWC; 2) only construct features from social context, such as Castillo; and 3) consider both news content and social context, such as RST+Castillo, LIWC+Castillo"
  }, {
    "heading": "5.3 Performance Comparison",
    "text": "We evaluate the effectiveness of the proposed framework TriFN for fake news classification. We determine model parameters with cross-validation strategy, and we repeat the generating process of training/test set for three times and the average performance is reported. We first perform cross validation on parameters λ ∈ 4The code is available at: https://github.com/jiyfeng/DPLP 5The readers can find more details about the software and feature description at: http://liwc.wpengine.com/\n{0.001, 0.01, 0.1, 1, 10}, and choose those parameters that achieves best performance, i.e., λ = 0.1. We also choose latent dimension d = 10 for easy parameter tuning, and focus on the parameters that contribute the tri-relationship modeling components. The parameters for TriFN are set as {α = 1e − 4, β = 1e − 5,γ = 1,η = 1} for BuzzFeed and {α = 1e − 5, β = 1e − 4,γ = 10,η = 1} for PolitiFact.\nWe test the baseline features on different learning algorithms, and choose the one that achieves the best performance (see Table 2). The algorithms include Logistic Regression (LogReg for short), Naïve Bayes (NBayes), Decision Tree (DTree), Random Forest (RForest), XGBoost, AdaBoost, and Gradient Boosting (GradBoost). We used the open-sourced xgboost [5] package and scikit-learn [22] machine learning framework in Python to implement all these algorithms. To ensure a fair comparison of features, we ran all the algorithms using default parameter settings. We also show the performances for each learning algorithm and report the average performance on both datasets. Due to the space limitation, we only show the results of F1 score (Table 3 and Table 4). We observe similar results for other metrics in terms of average performance. Based on Table 2, Table 3, and Table 4, we have following observations:\n• For news content based methods RST and LIWC, we can see that LIWC > RST for both best performance and average performance, indicating that LIWC can better capture the linguistic features in news contents. The good results of LIWC demonstrate that fake news pieces are very different from real news in terms of choosing the words that reveal psychometrics characteristics. • In addition, social context based features are more effective than news content based features, i.e., Castillo > RST and Castillo > LIWC . It shows that social context features have more discriminative power than those only on news content for predicting fake news. • Moreover, methods using both news contents and social context perform better than those methods purely based on news contents, and those methods only based on social engagements, i.e., LIWC +Castillo > LIWC or Castillo and RST + Castillo > RST or Castillo. This indicates that features extracted from news content and corresponding social context have complementary information, and thus boost the detection performance. • Generally, for methods based on both news content and social context (i.e., RST+Castillo, LIWC+Castillo, and TriFN), we can see that TriFN consistently outperforms the other two baselines, i.e.,TriFN > LIWC+Castillo andTriFN > RST+ Castillo, in terms of all evaluation metrics on both datasets. For example, TriFN achieves average relative improvement of 4.72%, 5.84% on BuzzFeed and 5.91%, 4.39% on PolitiFact, comparing with LIWC+Castillo in terms ofAccuracy and F1 score. It supports the importance to model tri-relationship of publisher-news and news-user to better predict fake news."
  }, {
    "heading": "5.4 Assessing Impacts of Users and Publishers",
    "text": "In previous section, we observe that TriFN framework improves the classification results significantly. In addition to news contents, we also captures user-news interactions and publisher-news relations.\nNow, we investigate the effects of these components by defining three variants of TriFN: • TriFN\\P - We eliminate the effect of publisher partisan modeling part γ ∥e ⊙ (B̄Dq − o)∥22 by setting γ = 0. • TriFN\\S -We eliminate the effects of user social engagements components α ∥Y ⊙ (A − UTUT )∥2F + βtr(H\nT LH) by setting α , β = 0. • TriFN\\PS -We eliminate the effects of both publisher partisan and user social engagements, by setting α , β ,γ = 0. The model only consider news content embedding.\nThe parameters in all the variants are determined with crossvalidation and the best performances are shown in Figure 3, we have following observations: • When we eliminate the effect of user social engagements component , the performance of TriFN\\S degrades in comparison with TriFN. For example, the performance reduces 5.2% and 6.1% in terms of F1 and Accuracy metrics on BuzzFeed, 7.6% and 10.6% on PolitiFact. The results suggest that social engagements in TriFN is important.\n• We have similar observations for TriFN\\P when eliminating the effect of publisher partisan component. The results suggest the importance to consider publisher-news relations through publisher partisan bias in TriFN. • Whenwe eliminate both components in TriFN\\PS, the results are further reduced compared to TriFN\\S and TriFN\\P. It also suggests that components of user-news and publisher-news embedding are complementary to each other.\nThrough the component analysis of TriFN, we conclude that (i) both components can contribute to the performance improvement of TriFN; (ii) it’s necessary to model both news contents and social engagements because they contain complementary information."
  }, {
    "heading": "5.5 Early Fake News Detection",
    "text": "Early detection of fake news is very desirable to restrict the dissemination scope of fake news and prevent the future propagation on social media. Early fake news detection aims to give early alert of fake news, by only considering limited social context within a specific range of time delay of original news posted. Specifically, we change the delay time in [12, 24, 36, 48, 60, 72, 84, 96] hours. From Figure 4, we can see that: 1) generally, the detection performance is getting better when the delay time increase for those methods using social context information, which indicates that more social engagements of users on social media provide more additional information for fake news detection; 2) The proposed TriFN consistently achieves best performances on both datasets for accuracy and F1, which demonstrate the importance of embedding user-news interactions to capture effective feature representations; and 3) Even in the very early stage after fake news has been published, TriFN can already achieve good performance. For example, TriFN can achieve\nF1 score more than 80% within 48 hours on both datasets, which shows promising potentials to combat fake new at the early stage."
  }, {
    "heading": "5.6 Model Parameter Analysis",
    "text": "The proposed TriFN has four important parameters. The first two areα and β , which control the contributions from social relationship and user-news engagements. γ controls the contribution of publisher partisan and η controls the contribution of semi-supervised classifier. We first fix {α = 1e − 4, β = 1e − 5} and {α = 1e − 5, β = 1e − 4} for BuzzFeed and PolitiFact, respectively. Then we vary η as {1, 10, 20, 50, 100} andγ in {1, 10, 20, 30, 100}. The performance variations are depicted in Figure 5. We can see i) when η increases from 0, eliminating the impact of semi-supervised classification term, to 1, the performance increase dramatically in both datasets. These results support the importance to combine semi-supervised classifier to feature learning; ii) generally, the increase of γ will increase the performance in a certain region, γ ∈ [1, 50] and η ∈ [1, 50] for both datasets, which easy the process for parameter setting. Next, we fix {γ = 1,η = 1} and {γ = 10,η = 1} for BuzzFeed and PolitiFact, respectively. Then we vary α , β ∈ [0, 1e−5, 1e−4, 1e−3, 0.001, 0.01]. We can see that i) when α and β increase from 0, which eliminate the social engagements, to 1e − 5, the performance increases relatively, which again support the importance of social engagements; ii) The performance tends to increase first and then decrease, and it’s relatively stable in [1e − 5, 1e − 3]."
  }, {
    "heading": "6 RELATEDWORK",
    "text": "We briefly introduce the related work about fake news detection on social media. Fake news detection methods generally focus on using news contents and social contexts [32, 40].\nNews contents contain the clues to differentiate fake and real news. For news content based approaches, features are extracted as linguistic-based and visual-based. Linguistic-based features capture\nspecific writing styles and sensational headlines that commonly occur in fake news content [24], such as lexical and syntactic features. Visual-based features try to identify fake images [9] that are intentionally created or capturing specific characteristics for images in fake news. News content based models include i) knowledgebased: using external sources to fact-checking claims in news content [17, 37], and 2) style-based: capturing the manipulators in writing style, such as deception [7, 27] and non-objectivity [24]. For example, Potthast et al. [24] extracted various style features from news contents and predict fake news and media bias.\nIn addition to news content, social context related to news pieces contains rich information to help detect fake news. For social context based approaches, the features mainly include user-based, post-based and network-based. User-based features are extracted from user profiles to measure their characteristics and credibilities [4, 14, 34, 39]. For example, Shu et al. [34] proposed to understand user profiles from various aspects to differentiate fake news. Yang et al. [39] proposed an unsupervised fake news detection algorithm by utilizing users’ opinions on social media and estimating their credibilities. Post-based features represent users’ social response in term of stance [10], topics [16], or credibility [4, 36]. Network-based features [29] are extracted by constructing specific networks, such as diffusion network [14] etc. Social context models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions towards the news to infer news veracity [10]. Propagation-based models assume that the credibility of news is highly related to the credibilities of relevant social media posts, which several propagation methods can be applied [10]. Recently, deep learning models are applied to learn the temporal and linguistic representation of news [11, 30, 35]. Shu et al. [33] proposed to generate synthetic data for augmenting training data to help improve the detection of clickbaits. It’s worth mentioning that we can not directly compare the propagation-based approaches, because we assume we only have user actions, e.g., posting the\nnews or not. In this case, the propagation signals inferred from text are the same and thus become ineffective.\nIn this paper, we are to our best knowledge the first to classify fake news by learning the effective news features through the tri-relationship embedding among publishers, news contents, and social engagements."
  }, {
    "heading": "7 CONCLUSION AND FUTUREWORK",
    "text": "Due to the inherent relationship among publisher, news and social engagements during news dissemination process on social media, we propose a novel framework TriFN to model tri-relationship for fake news detection. TriFN can extract effective features from news publisher and user engagements separately, as well as capture the interrelationship simultaneously. Experimental results on real world fake news datasets demonstrate the effectiveness of the proposed framework and importance of tri-relationship for fake news prediction. It’s worth mentioning TriFN can achieve good detection performance in the early stage of news dissemination.\nThere are several interesting future directions. First, it’s worth to explore effective features and models for early fake news detection, as fake news usually evolves very fast on social media; Second, how to extract features to model fake news intention from psychology’s perspective needs further investigation. At last, how to identify low quality or even malicious users spreading fake news is important for fake news intervention and mitigation."
  }, {
    "heading": "8 ACKOWLEDGMENTS",
    "text": "This material is based upon work supported by, or in part by, the NSF #1614576 and the ONR grant N00014-16-1-2257."
  }],
  "year": 2018,
  "references": [{
    "title": "Measuring User Credibility in Social Media",
    "authors": ["Mohammad Ali Abbasi", "Huan Liu"],
    "venue": "In SBP",
    "year": 2013
  }, {
    "title": "Social media and fake news in the 2016 election",
    "authors": ["Hunt Allcott", "Matthew Gentzkow"],
    "venue": "Technical Report. National Bureau of Economic Research",
    "year": 2017
  }, {
    "title": "Information credibility on twitter",
    "authors": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete"],
    "venue": "In Proceedings of the 20th international conference on World wide web",
    "year": 2011
  }, {
    "title": "Xgboost: A scalable tree boosting system",
    "authors": ["Tianqi Chen", "Carlos Guestrin"],
    "venue": "In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining",
    "year": 2016
  }, {
    "title": "Framing bias: Media in the distribution of power",
    "authors": ["Robert M Entman"],
    "venue": "Journal of communication 57,",
    "year": 2007
  }, {
    "title": "Syntactic stylometry for deception detection",
    "authors": ["Song Feng", "Ritwik Banerjee", "Yejin Choi"],
    "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume",
    "year": 2012
  }, {
    "title": "Media bias in the marketplace: Theory",
    "authors": ["Matthew Gentzkow", "Jesse M Shapiro", "Daniel F Stone"],
    "venue": "Technical Report. National Bureau of Economic Research",
    "year": 2014
  }, {
    "title": "Faking sandy: characterizing and identifying fake images on twitter during hurricane sandy",
    "authors": ["Aditi Gupta", "Hemank Lamba", "Ponnurangam Kumaraguru", "Anupam Joshi"],
    "venue": "In Proceedings of the 22nd international conference on World Wide Web",
    "year": 2013
  }, {
    "title": "News Verification by Exploiting Conflicting Social Viewpoints in Microblogs",
    "authors": ["Zhiwei Jin", "Juan Cao", "Yongdong Zhang", "Jiebo Luo"],
    "venue": "In AAAI",
    "year": 2016
  }, {
    "title": "Says Who?: How News Presentation Format Influences Perceived Believability and the Engagement Level of Social Media Users",
    "authors": ["Antino Kim", "Alan R Dennis"],
    "year": 2017
  }, {
    "title": "Prominent features of rumor propagation in online social media",
    "authors": ["Sejeong Kwon", "Meeyoung Cha", "Kyomin Jung", "Wei Chen", "Yajun Wang"],
    "venue": "In Data Mining (ICDM),",
    "year": 2013
  }, {
    "title": "Algorithms for non-negative matrix factorization",
    "authors": ["Daniel D Lee", "H Sebastian Seung"],
    "venue": "In Advances in neural information processing systems",
    "year": 2001
  }, {
    "title": "Detect rumors using time series of social context information on microblogging websites",
    "authors": ["Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong"],
    "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management",
    "year": 2015
  }, {
    "title": "Web-based statistical fact checking of textual documents",
    "authors": ["Amr Magdy", "Nayer Wanas"],
    "venue": "In Proceedings of the 2nd international workshop on Search and mining user-generated contents",
    "year": 2010
  }, {
    "title": "When corrections fail: The persistence of political misperceptions",
    "authors": ["Brendan Nyhan", "Jason Reifler"],
    "venue": "Political Behavior 32,",
    "year": 2010
  }, {
    "title": "Text mining using non-negative matrix factorizations",
    "authors": ["V Paul Pauca", "Farial Shahnaz", "Michael W Berry", "Robert J Plemmons"],
    "venue": "In Proceedings of the 2004 SIAM International Conference on Data Mining",
    "year": 2004
  }, {
    "title": "The Russian “Firehose of Falsehood",
    "authors": ["Christopher Paul", "Miriam Matthews"],
    "venue": "Propaganda Model. RAND Corporation",
    "year": 2016
  }, {
    "title": "Scikit-learn: Machine learning in Python",
    "authors": ["Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"],
    "venue": "Journal of machine learning research 12,",
    "year": 2011
  }, {
    "title": "The development and psychometric properties of LIWC2015",
    "authors": ["James W Pennebaker", "Ryan L Boyd", "Kayla Jordan", "Kate Blackburn"],
    "year": 2015
  }, {
    "title": "A Stylometric Inquiry into Hyperpartisan and Fake News",
    "authors": ["Martin Potthast", "Johannes Kiesel", "Kevin Reinartz", "Janek Bevendorff", "Benno Stein"],
    "year": 2017
  }, {
    "title": "Towards news verification: Deception detection methods for news discourse",
    "authors": ["Victoria L Rubin", "N Conroy", "Yimin Chen"],
    "venue": "In Hawaii International Conference on System Sciences",
    "year": 2015
  }, {
    "title": "Truth and deception at the rhetorical structure level",
    "authors": ["Victoria L Rubin", "Tatiana Lukoianova"],
    "venue": "Journal of the Association for Information Science and Technology 66,",
    "year": 2015
  }, {
    "title": "Document clustering using nonnegative matrix factorization",
    "authors": ["Farial Shahnaz", "Michael W Berry", "V Paul Pauca", "Robert J Plemmons"],
    "venue": "Information Processing & Management 42,",
    "year": 2006
  }, {
    "title": "Studying Fake News via Network Analysis: Detection and Mitigation",
    "authors": ["Kai Shu", "H. Russell Bernard", "Huan Liu"],
    "venue": "CoRR abs/1804.10233",
    "year": 2018
  }, {
    "title": "FakeNewsTracker: a tool for fake news collection, detection, and visualization",
    "authors": ["Kai Shu", "Deepak Mahudeswaran", "Huan Liu"],
    "venue": "Computational and Mathematical Organization Theory",
    "year": 2018
  }, {
    "title": "FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media",
    "authors": ["Kai Shu", "Deepak Mahudeswaran", "Suhang Wang", "Dongwon Lee", "Huan Liu"],
    "year": 2018
  }, {
    "title": "Fake News Detection on Social Media: A Data Mining Perspective. KDD exploration newsletter (2017)",
    "authors": ["Kai Shu", "Amy Sliva", "Suhang Wang", "Jiliang Tang", "Huan Liu"],
    "year": 2017
  }, {
    "title": "Understanding User Profiles on Social Media for Fake News Detection",
    "authors": ["Kai Shu", "Suhang Wang", "Huan Liu"],
    "venue": "IEEE Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE",
    "year": 2018
  }, {
    "title": "Toward computational fact-checking",
    "authors": ["You Wu", "Pankaj K Agarwal", "Chengkai Li", "Jun Yang", "Cong Yu"],
    "venue": "Proceedings of the VLDB Endowment 7,",
    "year": 2014
  }, {
    "title": "Document clustering based on nonnegative matrix factorization",
    "authors": ["Wei Xu", "Xin Liu", "Yihong Gong"],
    "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval",
    "year": 2003
  }],
  "id": "SP:627d5471b580bd598c62136b1023b60535b4fac0",
  "authors": [{
    "name": "Kai Shu",
    "affiliations": []
  }, {
    "name": "Suhang Wang",
    "affiliations": []
  }, {
    "name": "Huan Liu",
    "affiliations": []
  }],
  "abstractText": "Social media is becoming popular for news consumption due to its fast dissemination, easy access, and low cost. However, it also enables the wide propagation of fake news, i.e., news with intentionally false information. Detecting fake news is an important task, which not only ensures users receive authentic information but also helps maintain a trustworthy news ecosystem. The majority of existing detection algorithms focus on finding clues from news contents, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. Therefore, we need to explore auxiliary information to improve detection. The social context during news dissemination process on social media forms the inherent tri-relationship, the relationship among publishers, news pieces, and users, which has potential to improve fake news detection. For example, partisan-biased publishers are more likely to publish fake news, and low-credible users are more likely to share fake news. In this paper, we study the novel problem of exploiting social context for fake news detection. We propose a tri-relationship embedding framework TriFN, which models publisher-news relations and user-news interactions simultaneously for fake news classification. We conduct experiments on two real-world datasets, which demonstrate that the proposed approach significantly outperforms other baseline methods for fake news detection.",
  "title": "Beyond News Contents:The Role of Social Context for Fake News Detection"
}