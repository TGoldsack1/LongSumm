{
  "sections": [{
    "heading": "CCS CONCEPTS",
    "text": "• Information systems → Online analytical processing engines; Relational database query languages; Data analytics; Data warehouses;"
  }, {
    "heading": "ACM Reference format:",
    "text": "Rajesh Bordawekar and Oded Shmueli. 2017. Using Word Embedding to Enable Semantic Queries in Relational Databases. In Proceedings of DEEM’17, Chicago, IL, USA, May 14, 2017, 4 pages. DOI: http://dx.doi.org/10.1145/3076246.3076251"
  }, {
    "heading": "1 INTRODUCTION",
    "text": "We begin with a simple observation: there is a large amount of untapped latent information within a database relation. This is intuitively clear for columns that contain unstructured text. But even columns that contain di erent types of data, e.g., strings, numerical values, images, dates, etc., possess signi cant latent information in ∗Work done while the author was visiting the IBM T. J. Watson Research Center.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. DEEM’17, Chicago, IL, USA © 2017 ACM. 978-1-4503-5026-6/17/05. . . $15.00 DOI: http://dx.doi.org/10.1145/3076246.3076251\nthe form of inter- and intra-column relationships. The usual way to utilize this information is using SQL and extensions, such as text extensions, or User De ned Functions (UDFs) to handle exotic data types. However, these extensions are rather limited in their smarts. Speci cally, SQL queries rely on value-based analytics to detect patterns. In addition, the relational data model neglects many interor intra-column relationships. Thus, the traditional SQL queries lack a holistic view of the underlying relations, and thus are unable to extract and exploit semantic relationships that are collectively generated by tokens in a database relation.\nIn this paper, we introduce and explore a new class of business intelligence queries, called Cognitive Intelligence (CI) queries, that extract information from a database based, in part, on the relationships among database tokens encoded as vectors produced by a machine learning method. We use an idea from the Natural Language Processing domain, Distributed Representation via Word Embedding, to associate a vector to each database-associated token, and these vectors are then used during the query execution. We rst discuss various aspects of the word embedding approach and illustrate novel capabilities of the CI queries such as semantic similarity queries, predictive queries using tokens not present in a database, and analogy queries. Finally, we enumerate key performance issues and outline future plans."
  }, {
    "heading": "2 USING THEWORD EMBEDDING MODEL",
    "text": "The idea of word embedding is to x a d-dimensional vector space and for each word in a text corpus (i.e., collection of documents), associate a dimension d vector of real numbers that encodes the meaning of that word [2]. Meaning of a word is determined by other words in its neighborhood. Thus, for a given text corpus, the nal meaning of a word re ects collective contributions of neighborhood words for di erent appearances of the word in the corpus. Two words are closely related or have similar meaning if they appear often within close proximity of of the same words. If two words have similar meaning, their word vectors point in very similar directions, i.e., the cosine similarity between their vectors is high (vector similarity is measured as a cosine of the angle between two vectors and can vary from 1.0 to -1.0).\nOver the last few decades, a number of methods have been introduced for computing vector representations of words in a natural language [2]. Recently, a neural network based approach, word2vec [7, 8], has gained prominence as the vectors it produces appear to capture syntactic as well semantic properties of words. There are alternative mechanisms for producing vectors of similar quality, for example GloVe [9] (we use word2vec as the method for constructing vectors from database tokens, although, we could use GloVe as well). The vectors produced by word2vec seem to capture\ncloseness of words and have been tested convincingly in a number of benchmark tests. These vectors seem to capture syntactic (e.g., present-past, singular-plural) as well as semantic closeness of words. One surprising application of word-embedding vectors is their usage in solving inductive reasoning problems such as computing analogies [10, 11], e.g., king to man is what to woman? (answer: queen) by using vector algebra calculations [6].\nIn the database context, vectors may be produced by either learning on the database itself or using external text sources. For learning from a database, a natural way of generating vectors is to apply the word embedding method to a token sequence generated from the database: each row would correspond to a sentence and a relation would correspond to a document. Thus, vectors enable a dual view of the data: relational and meaningful text. Word embedding then can extract latent semantic information in terms of word (and, in general, token) associations and co-occurrences and encode it in word vectors. Thus, the vectors capture rst inter- and intraattribute relationships within a row (sentence) and then aggregate these relationships across the relation (document) to compute the collective semantic relationships. The encoded semantic information can be then used in querying the database.\nIn this work, we propose to build a cognitive database system by integrating word embedding techniques and capabilities into a traditional database system [3]. We are currently developing a Spark [1] based implementation (supporting both Python and Scala) of our cognitive database system. We are using Spark DataFrames API and exploiting Spark SQL functionalities in Spark version 2.1 for building and querying our cognitive database system. Figure 1 presents the three key phases in the end-to-end execution ow of a cognitive database. The rst, optional, training phase takes place when the database is used to train the model: a database table data is rst texti ed into a meaningful text format. Note that, which rows are texti ed can be controlled using standard relational operations (e.g., by de ning a view). Next, we use a modi ed version of the word2vec algorithm to learn vectors for the words (database tokens) in the extracted text. This phase can also use an external source, e.g., Wikipedia, as the source for text for model training. The result is a set of low-dimensional (say, of dimension 200) vectors, each representing a word. We use word as a synonym to token although some tokens may not be valid words in any natural language. Following vector training, the resultant vectors are stored in a relational system table (phase 2). At runtime, the SQL query execution engine uses various UDFs that fetch the trained vectors from the system table as needed and answer CI queries (phase 3). These UDFs compute distances between vectors in a semantic vector space using the cosine distance metric to determine contextual semantic similarities between corresponding source database tokens. The similarity results are then used in the relational query execution, thus enabling the relational engine to exploit latent semantic information for answering enhanced relational queries.\nThe power of CI queries goes far beyond analytical capabilities present in current relational systems. All well-known commercial and open source (e.g., Spark or MADlib[5]) database systems have built-in analytics capabilities. However, such systems view the database as a repository of features for the analytics libraries. Systems based on statistical relational learning models combine\nprobabilistic graphical models and rst-order logic to encode uncertain rst-order logic rules based on known information [12]. In contrast, the cognitive database learns a neural network model that infers the meaning of the database tokens by extracting latent features from the database data. The meaning is a re ection of the context (i.e., the relational view) used for training, and is encoded as vectors in a d dimensional space (d is usually 200). These vectors are then used to perform semantic matching on database tokens, rather than syntactical (value) matching. The vector-based query execution enables the cognitive database to support inductive reasoning queries such as analogies. Thus, the CI queries augment the capabilities of the traditional relational BI queries and can also be used in conjuction with the existing SQL operators (e.g., ROLLUP or CUBE [4]). In summary, we believe this work is a step towards enpowering database systems with built-in AI capabilities."
  }, {
    "heading": "3 CI QUERIES IN PRACTICE",
    "text": "Broadly, there are two classes of cognitive intelligence queries: similarity and prediction queries. Similarity queries on a database use a model that is trained using the database being queried. The prediction queries use a model that is externally trained using an unstructured data source or another database. Both classes of queries use UDFs to compute similarity. A UDF takes as input either a pair of tokens or a pair of sets (or sequences) of tokens (either constant tokens or ones associated with query variables); it then fetches vectors that correspond to these tokens and computes and returns numeric similarity value between 1.0 and -1.0 based on these vectors. A user can then control the results of the CI query by using a numerical bound for similarity result value as a predicate for selecting the eligible rows. The similarity value captures the semantic closeness between the relational variables: higher is the similarity value, closer are the two database variables. The similarityUDF() computes similarities between a pair of tokens by computing the cosine distance on the corresponding vectors. For sets and sequences, the individual pair-wise similarity values are then aggragated to generate the resultant value. In case of sequences, computation of the nal similarity value takes into account the ordering of tokens: di erent pair-wise distances contribute di erently to the nal value based on their relative ordering. For example, two food items, a chicken_item consisting of (chicken, salt), is not very similar to a corn_item consisting of (corn, salt), although both contain salt (however, the corn_item is closer to chicken_item than a wheat_item that contains (wheat, sugar)).\nThe rst example (Figure 2) illustrates an SQL CI query that identi es similar customers by comparing their purchases. Assume that sales is a table that contains all customer transactions for a grocery store and whose sales.purchase column contains all items purchased during a transaction. The current query uses a UDF, similarityUDF(), that computes similarity match between two sets of vectors, that correspond to the items purchased by the corresponding customers. Unlike the food item scenario, the purchased item list can be viewed as an unordered bag of items; and individual pair-wise distances contribute equally to the nal result. A modi ed version of the query (not shown) can identify similar customers based on their overall purchasing pattern as evidenced in a number of rows. The word embedding model creates"
  }, {
    "heading": "Text",
    "text": "SELECT X.custID, X.name, Y.custID, Y.name, similarityUDF(X.purchase, Y.purchase) AS similarity FROM sales X, sales Y similarityUDF(X.purchase, Y.purchase) > 0.5 ORDER BY X.name, similarity LIMIT 10\nFigure 2: Example of an CI similarity query: nd similar customers based on their purchased items\nSELECT P1.type, P2.type FROM Products P1, Products P2 WHERE analogyUDF(P1.type, ’peanut-butter’, ’jelly’, P2.type) = (SELECT MAX (analogyUDF(P3.type, ’peanut-butter’, ’jelly’,P2.type)))"
  }, {
    "heading": "FROM Products P3",
    "text": "Figure 3: Example of an CI analogy query\na vector for each customer name that captures the overall purchases made by that customer. Then, the customers with similar purchase patterns would have vectors that are close using the cosine distance metric. The pattern observed in this query can be applied to other domains as well, e.g., identifying patients that are taking similar drugs, but with di erent brand names or identifying food items with similar ingredients, or nding mutual funds with similar investment strategies. In all these scenarios, the relational database tokens are matched based on their semantic similarities, not by their values. This is a fundamental change from the current relational model which uses value based matching for the core relational operations (i.e., select, project, and join). In a cognitive database these relational operations can be strengthened with semantic matching.\nA unique capability of word-embedding vectors is that one can use them to answer inductive reasoning questions that require an individual to reason from part to whole, or from particular to general [11]. An example of inductive reasoning is computing analogies; e.g., king to man is like queen to woman. Analogy can be viewed as a special type of similarity, where two pairs of entities that share similar relationships, have similar distance between their\nSELECT X.number, X.name, similarityUDF(X.purchase, ‘allergenic’) AS similarity"
  }, {
    "heading": "FROM sales X similarityUDF(X.purchase, ‘allergenic’) > 0.3",
    "text": "LIMIT 10\nFigure 4: Example of a prediction query: nd customers that have purchased allergic items\ncorresponding vectors [6, 8]. CI queries enable exposing such relationships within a relational table. For example, the following query (Figure 3) identi es pairs of products that relate to each other like tokens peanut butter and jelly (both are product types). The key is that if product p1 relates to product p2 like peanut butter to jelly, their associated vectors ~p1, ~p2, ~peanut − butter and ~jelly are likely to satisfy that the vector di erences ( ~peanut − butter - ~jelly) and ( ~p1 - ~p2) are cosine-similar (we use the UDF analogyUDF() to compute the di erences). Potential answers to this query may include (chips, salsa) or (pancake, maple syrup). The analogy capabilities of CI queries have several applications in the enterprise space, e.g., associating customers with either most-common or least-common purchases in a given domain (e.g., books, electronics, etc.)\nThe nal use case provides an illustration of using an external text source for querying a database (Figure 4). The example assumes that we have built a word embedding model using an external source such as Wikipedia. Let’s assume that the external knowledge source has a document that lists all allergenic fruits, e.g., list of allergenic fruits include strawberries, rapsberries, blueberries,... The model will create vectors for all these words and the vector for the word allergenic will be closer to the vectors of strawberries, rapsberries, and blueberries. Now, we can import this model and use it to query the sales database to nd out which customers have bought items that may be allergenic, as de ned by the external source. As Figure 4 shows, the similarityUDF() UDF is used to identify those purchases that contain items similar to allergenic, such as strawberries. This example demonstrates a very powerful ability of CI queries that enables users to query a database using a token (e.g., allergenic) not present in the database. This capability can be applied to di erent scenarios in which recent, updatable information, can be used to query historical data. For example, a model\nbuilt using a FDA recall notices could be used to identify those customers who have purchased medicines similar to the recalled medicines.\nThe key characteristic of the CI queries is that these queries are executed, in part, using the vectors in the word embedding model. If the word embedding model is generated using the database being queried, it captures meaning in the context of the associated relational table, as speci ed by the relational view. If a model is rebuilt using a di erent relational view, a CI query may return di erent results for the new model. For every token in the database, irrespective of its type, the model generates a single-precision oating point vector of dimension d . In other words, similarity calculations over SQL variables of di erent types always use uniformly typed vectors. This enables the database system to seamlessly analyze data of di erent types (e.g., text, numeric values, and potentially, images) using a single CI query. We are currently working on extending our infrastructure to support additional data types such as numeric values, SQL Dates, and images.\nNone of the existing relational systems supports the type of queries outlined in this section. It is conceivable that one may obtain similar results by exporting data from a database table and analyzing it externally. However, our approach provides seamless integration of the word embedding model into the existing standard SQL infrastructure, including the query optimizer, thus extending capabilities of existing systems to support smarter SQL queries.\nAlthough we used hypothetical scenarios to demonstrate our ideas, CI queries are applicable to a broad class of domains. These include retail, customer care, log analytics, healthcare, genomics, semantic search over documents (patent, legal, or nancial), healthcare informatics, and human resource management."
  }, {
    "heading": "3.1 Implementation Details",
    "text": "In our end-to-end Spark implementation, we use word2vec to train the model using either a dataset exported from a database (e.g., a CSV le) or text documents from an externel corpus (e.g., wikipedia). Alternatively, we can load a pre-trained model directly. The Spark program loads the source database and the model into dataframes. We also build additional hash-based indices to accelerate vector computations. In an end-to-end system, there are three spots where performance can become a concern. First, in the training phase, if the vectors have to be trained from the raw database data, depending on the size of the text used, the vector training time can be very long. However, this training phase may not always be necessary as the system can use pre-trained vectors, and if the vectors need to be trained, the training (or retraining for new data), can be implemented as a batch process. Performance of training can be further improved by using GPUs. Second, the cost of acceessing the trained vectors from the system tables has to be as small as possible as it impacts query execution time. Finally, the execution cost of a CI query is dependent on the performance of the distance function. In many cases, we may need to compute distances among a large number of vectors (e.g., for analogy queries). The distance calculations can be accelerated either using CPU’s SIMD capabilities or using accelerators such as GPUs. This is a focus of our current work on accelerating key Spark ML libraries and we will provide details in subsequent publications."
  }, {
    "heading": "4 CONCLUSIONS AND FUTUREWORK",
    "text": "We describe the enhanced querying capabilities resulting from having each database token associated with a vector that captures its syntactic as well as its semantic characteristics. Vectors are obtained by a learning algorithm operating on database-derived text. We use these vectors to enhance database querying capabilities. In essence, these vectors provide another way to look at the database, almost orthogonal to the structured relational regime, as vectors enable a dual view of the data: relational and meaningful text. We thereby introduce and explore a new class of queries called cognitive intelligence (CI) queries that extract information from the database based, in part, on the relationships encoded by these vectors.\nWe are implementing a prototype system on top of Apache Spark [1] to exhibit the power of CI queries. We are currently extending our infrastructure to support more complex query patterns that use di erent data types, e.g., numeric values and images. We are also working on accelerating model training and vector distance calculations using GPUs, and developing new techniques for incremental vector training. We believe CI queries are applicable to a broad class of application domains including healthcare, bio-informatics, document searching, retail analysis, and data integration. We are currently working on applying the CI capabilities to some of these domains."
  }],
  "year": 2017,
  "references": [{
    "title": "A Neural Probabilistic Language Model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin"],
    "venue": "Journal of Machine Learning Research",
    "year": 2003
  }, {
    "title": "Enabling Cognitive Intelligence Queries in Relational Databases using Low-dimensional Word Embeddings",
    "authors": ["Rajesh Bordawekar", "Oded Shmueli"],
    "venue": "CoRR abs/1603.07185",
    "year": 2016
  }, {
    "title": "Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub- Totals",
    "authors": ["Jim Gray", "Surajit Chaudhuri", "Adam Bosworth", "Andrew Layman", "Don Reichart", "Murali Venkatrao", "Frank Pellow", "Hamid Pirahesh"],
    "venue": "Data Mining and Knowledge Discovery",
    "year": 1997
  }, {
    "title": "The MADlib analytics library: or MAD skills, the SQL",
    "authors": ["Joseph M. Hellerstein", "Christopher Re", "Florian Schoppmann", "Daisy Zhe Wang", "Eugene Fratkin", "Aleksander Gorajek", "Kee Siong Ng", "Caleb Welton", "Xixuan Feng", "Kun Li", "Arun Kumar"],
    "venue": "Proc. VLDB Endow. 5,",
    "year": 2012
  }, {
    "title": "Linguistic regularities in sparse and explicit word representations",
    "authors": ["Omer Levy", "Yoav Goldberg"],
    "venue": "In Eighteenth Conference on Computational Natural Language Learning",
    "year": 2014
  }, {
    "title": "word2vec: Tool for computing continuous distributed representations of words",
    "authors": ["Tomas Mikolov"],
    "year": 2013
  }, {
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"],
    "venue": "In 27th Annual Conference on Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "GloVe: Global Vectors for Word Representation",
    "authors": ["Je rey Pennington", "Richard Socher", "Christopher D. Manning"],
    "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    "year": 2014
  }, {
    "title": "A model for analogical reasoning",
    "authors": ["D.E. Rumelhart", "A.A. Abrahamson"],
    "venue": "Cognitive Psychology 5,",
    "year": 1973
  }, {
    "title": "Unities in inductive reasoning",
    "authors": ["R.J. Sternberg", "M.K. Gardner"],
    "venue": "Journal of Experimental Psychology: General 112,",
    "year": 1983
  }],
  "id": "SP:5c6d93741b89a82fb8e8a20ccde734623f4adfa2",
  "authors": [{
    "name": "Rajesh Bordawekar",
    "affiliations": []
  }, {
    "name": "Oded Shmueli",
    "affiliations": []
  }],
  "abstractText": "We investigate opportunities for exploiting Arti cial Intelligence (AI) techniques for enhancing capabilities of relational databases. In particular, we explore applications of Natural Language Processing (NLP) techniques to endow relational databases with capabilities that were very hard to realize in practice. We apply an unsupervised neural-network based NLP idea, Distributed Representation via Word Embedding, to extract latent information from a relational table. The word embedding model is based on meaningful textual view of a relational database and captures inter-/intra-attribute relationships between database tokens. For each database token, the model includes a vector that encodes these contextual semantic relationships. These vectors enable processing a new class of SQLbased business intelligence queries called cognitive intelligence (CI) queries that use the generated vectors to analyze contextual semantic relationships between database tokens. The cognitive capabilities enable complex queries such as semantic matching, reasoning queries such as analogies, predictive queries using entities not present in a database, and using knowledge from external sources.",
  "title": "Using Word Embedding to Enable Semantic eries in Relational Databases"
}